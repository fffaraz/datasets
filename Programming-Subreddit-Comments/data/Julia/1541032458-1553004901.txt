It describes on the README how to do it: https://github.com/JuliaPlots/Makie.jl#precompilation . Makie.jl is really pioneering the full precompilation, so YMMV but the demos show it gets rid of all of the JIT lag.
Thank you for the ideas! I'll look into Makie.jl, I haven't seen that one yet. I may have to PM you about GSoC, I wanted to do it last year implementing a "tidy" version of a bayesian inference package in R. I'm very interested in doing one in Julia. 
I don't know if anyone is working on [Tree-sitter support](https://blog.github.com/2018-10-31-atoms-new-parsing-system/#language-support) but that'd be nice to have. I use Atom for julia development, and I think that's what most people use for julia so that'd be a big boost to improving julia development
Hey man! I know you haha. I'll check that out!
You could help implement a good [CMAES](https://en.wikipedia.org/wiki/CMA-ES), it's one of [the best](https://github.com/jonathanBieler/BlackBoxOptimizationBenchmarking.jl) black-box optimization algorithm but we don't have a good Julia version. I have one that works and is compatible with Optim.jl but it doesn't perform as well as the Python version for some reason, which would need to be investigated. I also saw a paper recently claiming that this kind of evolutionary algorithms outperforms stochastic gradient descent for deep learning, that would be interesting to test out. I also have an [IDE](https://github.com/jonathanBieler/GtkIDE.jl) written in Julia if you are into that kind of things. Something that I have and that I would like to see in other editors is [method extraction](https://github.com/jonathanBieler/GtkIDE.jl/blob/master/src/Refactoring.jl), my version mostly works but it has some bugs. I would also like to have some kind of fuzzy auto-completion in the editor; you type the begging of a line and you can fully autocomplete it based on past lines you typed/codebase. I'm not sure that would work though. Maybe there's some better packages now but [LaTeX.jl](https://github.com/rened/LaTeX.jl) could use an upgrade. 
Ha I didn't even look at the username, small world haha
One good place to start is by searching GitHub issues. Here are all nearly 18k open issues in Julia repositories: https://github.com/search?l=&amp;q=language:Julia+state:open&amp;state=open&amp;type=Issues You could try also including `label:good-first-issue` but not all authors/maintainers do a good job of labeling their issues.
`kron(Matrix(I,4,4),randn(4,4))`
Like any language, learn by doing. As long as you're able to get things done, keep doing that and you'll become gradually more fluent, and less reliant on having to "translate". Further, googling for alternate, simplified solutions to the things you're trying to accomplish is another great way of learning the particular quirks of the language. 
look at the source of the standard libraries. like for example follow the implementation of an enumerator, or take a look how the promotion system works. they are all pure julia, and quite awesome.
Phrasing!
It might be worthwhile taking a look at Rosetta Code to see how various problems have been solved using Julia. There's no guarantee that these are good solutions, of course: https://rosettacode.org/wiki/Category:Julia
Looks like Sugar and Transpiler haven't been ported to 1.0 indeed. You can maybe open an issue on Transpiler and ask Simon Danisch what's are his plan for this, but in my experience if you really need something you better fix it yourself.
For some packages it's easy, you just load the package in v0.7 and fix the deprecation warnings. But from this it looks like it involved more work: https://github.com/SimonDanisch/Sugar.jl/pull/31
I managed to speak with Danisch. He said: "Simon Danisch [2:34 PM] it's not an easy fix sadly sugar.jl needs a thorough update since Julia's IR has changed, so it needs quite a different design" :(
Thank you! That looks nice :) Is this for all iterators? 
Yup, `reduce()` applies to all iterable sequences. https://docs.julialang.org/en/v1.0/base/collections/#Base.reduce-Tuple{Any,Any} Also, looks like my example above is now out of date. The new syntax is: `reduce(+, i for i in 1:10 if false; init=0)`
https://github.com/JuliaLang/julia/issues/27766 This has been discussed before, the main issue seems to be around returning a *zero* of the correct type, and the type inference/instability issues around it. 
What does the if false bit do? 
Aha thanks
push doesn't work with dictionaries. Try a["sdf"] = [1, 2] The Key-value pair will be added to a.
Yes "()" was missing. Thank you :)
push! does work with dictionaries (TIL) julia&gt; a = Dict{String, Array{Int64,1}}() Dict{String,Array{Int64,1}} with 0 entries julia&gt; julia&gt; push!(a, "sdf" =&gt; [1, 2]) Dict{String,Array{Int64,1}} with 1 entry: "sdf" =&gt; [1, 2] 
I stand corrected. 
Is it me, or is the error message quit misleading?
https://julialang.org/community/
What's an error message that might be clearer?
That is not the error message I'm seeing in OP's screenshot!
thanks for the suggestions so far. I might add: Another reason why I really want to learn julia is that I think julia will be very future proof. It's just in it's baby shoes and I think if could be cutting edge for a long time. And another important reason is: My ocleagues barely do any coding. They're ms windows point and click type of guys but have at least a tiny bit of ambition. So because julia has an easy syntax and is so modern, i think its the language i could get them with.
i really like Plots.jl. I actually might look into their github and see if i can contribute anything. I suck at julia but it could be nice practice. A ggplot like environment that is not as heavy as gadfly would also be nice.
Revise looks promising. Would be nice if it works in atom.
gadfly, conceptually is so amazing! It uses Julia's extremely-lazy-ahead-of-time compiler to compiles code tailor made for your plot... Maybe there are some ways for the compilation to be optimized, but I think that would not be a beginner project.
that's exactly what it says! It's just easy to miss the ::Type that's sitting in there.
oh that's really wierd. I would have expected it to be a 0-element array of Nothing.
I don't use an IDE. I have a text editor, I have a shell, I have a REPL. I do mostly reading from the web &amp; Excel workbooks, parsing &amp; processing, storing in SQLite, &amp; then writing results into another Excel - I get that to make the charts :)
I think what you are looking for is the OnlineStats package. It supports the incremental creation 
Sorry, I meant to say for reference, this is what Python does, but accidentally missed that crucial detail.
I have to second your comment #1. I use python and Julia, and while Julia is great in so many ways its sluggishness at startup (and also when plotting) as compared to python is a major drawback that makes me pull it up only when I think it's absolutely necessary.
No, since it's the only plotting package I can correctly run on my dev box, I'm definitely thinking of gadfly.
Both Gadfly and Plots.jl can make better use of `@nospecialized` to reduce the first plotting times. However, Makie.jl is shaping up to really replace them all. I like it already. For me, I haven't moved over yet because their recipe system still isn't robust enough to develop the DiffEq recipes for, but once it is I'll do a pretty hard switch to Makie. 
Makie has a Cairo backend now.
Cool, thanks for the tip!
What version of Julia are you using? Newer versions are much faster than old. What do you have in your .juliarc startup file? If you're always loading something like PyPlot, it will take longer. What computer and OS are you using? This could help others understand if your startup time is reasonable.
&gt; I was told (can't remember where) that it's about to get better I think siriusfrz was referring to this part. Makie.jl is intended to replace most of the other plotting packages in time.
I also prefer VS Code but have not had much success with the Julia extension. JuliaPro (basically Atom+Juno in one package) works fairly well, and on macOS it keeps everything self contained.
Thanks very much. I never would have guessed that scoping in julia is this weird. I normally write a lot of c and a bit of python and i just applied python style to julia. I now see the error in my ways. 
This is awesome and you're awesome! I spent a stupid amount of time trying to do something very similar and ultimately shelved it due to time constraints and frustration. What I was trying to achieve was to label slices of a vector (like "named views" into the vector, rather than individual entries) to have a more elegant/verbose replacement for `ArrayPartition` in DiffEq. E.g. calling `u.M` could give you a matrix shaped view on elements that are stored serially in `u`, while `u.x` could be a vector and `u.p` could be an individual parameter (Ref). That would make it very convenient to work with e.g. differential equations in which vectors or matrices evolve over time. I appreciate your blogpost as well, since there's a couple of really nice tricks in your code I didn't think of (e.g. using the type `Val{s}` in a `@generated` function `getindex`, instead of just using `s::Symbol` and hoping for constant propagation, is really elegant). I also never have a clue which methods are actually currently needed to implement an array type, so your code serves very nicely as a reference!
PackageCompiler.jl might help...
&gt; Yes, I know I could use the REPL, but then I'll need to copy (somehow) my code into an actual editor. Try an IJulia notebook?
Nice, thanks for your post. Great usage of `@generated`. There's a typo in the not generated `getindex` function. The last line should read `x.__x[idx]` without the `$`.
Why not both? I was coding scientific computing in Python and enjoyed its rich libraries. Currently I use Julia because of DiffEq package, which the best of the world.
How do you get around the huge start up time in julia? I'm starting to write some small scripts, and the 10-15 s compilation time DaraFrames and CSV is a killer in my development work flow. 
I primarily run biological dynamic models with lots of ordinary differential equations. I have to admit the startup time is pretty slow when I run the models or plot the results for the first time. So I keep Julia sessions open as long as possible to avoid long compile times.
I can't speak to those packages, but I've found on 1.0 that the packages I use are still slow, but faster (i.e. DiffEqs packages and Plots.jl). Things are going in the right direction -- Makie.jl is moving forward as a new plotting library, and it can be pre-compiled, supposedly eliminating this issue. Hopefully now that 1.0 is out the language developers will dedicate some effort to fixing this issue over the next year. As is mentioned below, the best way to deal with this is to just keep a long running notebook or console open for doing calculations.
I've just updated to 0.7. I've read that there are some issues about errors and warnings in 1.0 v 0.7. I first tried it out at 0.3, and I've read a few blogs saying that start up tone is now a focus. 
Ah. I'll give that a go. I hadn't thought of separating the using from the import. 
Maybe it helps to update julia and CSV package to the latest version. Not all package could be completely precomputed, unfortunately. https://stackoverflow.com/questions/52475931/how-to-decrease-the-time-taken-to-open-a-large-csv-file-in-julia-using-csv-and
If you have enough code, you can put all the functions into a library and turn precompilation on for it. That should improve startup time somewhat. Also, I have managed to statically compile a decently complex piece of code I have (though with no binary dependencies) and that got rid of the compilation lag entirely.
Use [Revise.jl](https://github.com/timholy/Revise.jl) Startup is a negligible concern. I do something similar to [this workflow](https://medium.com/@Jernfrost/my-new-workflow-with-julia-1-0-99711103d97c)
Definitely Julia
I feel like you're doing things wrong somehow. Are you testing your code by, for example, writing it to a jl file and then running it against a csv file using the command line? As in julia code.jl test.csv. If so, that would be pretty slow. Ideally you just want to load the DataFrames and CSV modules just once while you are doing development and testing. Jupyter notebook is probably the best way but the following works if you only use the REPL too. I'll describe the simple way and the more complex way. 1. Write a function doanalysis("test.csv", "output.csv"). Just run the function in the REPL every time you change it. No need to reload CSV. 2. Install Revise.jl (https://timholy.github.io/Revise.jl/stable/). Create your own package and define the doanalysis(input,output) function in your package. Every time you modify the function, call revise() and the function gets automatically updated in the REPL. 
Yeah, don't use anything before .7 -- a number of language features changed with .7/1.0... I ended up going to 1.0 despite the lack of depreciation features as some packages seemed to have jumped from .6 to 1.0 directly, and wouldn't run on .7.
Nailed the prob that keeps me in Python!
That package is dope! I wonder how many it drew to Julia alone, by virtue of being a best-in-class package.
It's been well known since the beginning. You're right; a couple of lines, or a few thousand, and you're OK. It also goes against the principle of making a program good at one thing. If I make a script to do some data manipulation, and then plot something, but then call that program a few hundred times, I'm compiling everything a few hundred times. Hopefully they finally fix the problem.
Not all packages support precompliation.
What do you mean? The code is only compiled the first time, or if you change the *types* of the inputs.
Run &gt; julia analysis.jl data.csv The run &gt; julia analysis.jl data.csv again. You still encounter the same start up time. 
You don't say what kind of errors you get. Does it fail to include or get called or whatever ? Error messages will help. If it's a path problem you can used println(pwd()) to find out where the code is running from.
Is there a way to load packages by path from my hard disk. As I understand it I would have to tell Julia the path to my packages in some pre-loading script. Nothing with relative paths like in Python. If I were to create packages I would want them to work just from scratch with any newly installed Julia without my customers having to edit a file i.e. without me having to explain this procedure to any customer every time again and again. If that does not work I would have to keep using the include statement which means long start-up time, I assume. 
&gt;Is there a way to load packages by path from my hard disk. Yes. `]dev "path/to/package"`.
Thank you for your answer. &gt; ]dev "path/to/package" But does this work in the script. This looks like Repl stuff. &gt; Drop it in .julia/dev? I would want that "just works", where my users do not have to do such things, like copy a package somewhere. By the way I am mainly talking about WINDOWS i.e. WINDOWS users that are not used to such things. &gt; That's dev and add from path are for. But that has to be done by the user. I want something that works in the script with a freshly installed Julia, nothing done to it. 
Yeah it's possible. What you do is add your code directory to JULIA_LOAD_PATH (variable name might be slightly different) with "push!" and then just load your module like normal with the "using" command.
&gt; But does this work in the script. This looks like Repl stuff. You can always use the pkg string macro to run any Pkg command in a script: `using Pkg; pkg"dev \"path/to/package\""`. 
Thank you. I will try it.
Thank you. I will try it.
I tried the following: using Pkg pkg"dev \"./Vinci\"" using Vinci and got a lot of erros: e:\src\vinci_q_julia1&gt;c:\Julia-1.0.1\bin\julia.exe HelloWorld.jl Updating registry at `C:\Users\sue\.julia\registries\General` Updating git-repo `https://github.com/JuliaRegistries/General.git` Cloning git-repo `./Vinci` ERROR: LoadError: failed to clone from ./Vinci, error: GitError(Code:ERROR, Class:Net, unsupported URL protocol) Stacktrace: [1] pkgerror(::String) at C:\cygwin\home\Administrator\buildbot\worker\package_win64\build\usr\share\julia\stdlib\v1.0\Pkg\src\Types.jl:120 [2] #clone#2(::Nothing, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::String, ::String) at C:\cygwin\home\Administrator\buildbot\worker\package_win64\build\usr\share\julia\stdlib\v1.0\Pkg\src\GitTools.jl:107 [...] Julia seems to expect the package to be in some git repository. So, it seems it does not work with just files (in the necessary directory structure) on the hard disk. 
btw exactly this problem is already covered, don't remember the name, but it translates to code that is actually better, i.e. ((w3 * x + w2) * x) + w1
I probably should switch to generated. I'll have a look into it. I thought that was for dealing with Expr objects. I need to read up. As for the array input, it does get similar results that way in the toy example but for larger polynomials it doesn't. I should probably switch that around and explain more clearly, but I was mostly just putting together correct @eval usage and how to check that it is doing what you expect as that currently isnt well covered in the docs. I'll definitely tweek this to be more correct though, thanks for the help :)
@generated needs to be at a global scope to define functions it seems from testing with it and the docs. But who knows...
Because I write batch scripts to iterate over all of my files, because I distribute code to other researchers and they can't be expected to treat this particular little program as it's own special snowflake. This has worked for all of my java programs, why does julia need to break this paradigm? 
No precompile message. As far add I can tell, DataFrames and CSV don't support precompilation. 
Horner's method, implemented in `Base.@evalpoly`. 
When you looking at the LLVM IR, those `oob` branches are actually about index bounds checking. If you do `@inbounds` on the line with the accesses it's probably much simpler. Not only that, but you should pull the construction of the weights array `[0.2,0.5,1.2]` outside of the loop since the heap allocation is likely effecting the benchmark. I'm not sure much of a difference holds after those. Simple methods on an array which output scalars are surprisingly well-optimized. You'd need to do a bit more intense math, plus put some scalar functions which get constant propagated, in order to see a big difference in a case like this. 
In v1.0 all packages are precompiled, it's just that precompilation doesn't solve everything, it's not a full compilation to machine code. &gt;No other language that I've used asks you to compile a program everytime you run it. It's because Julia is compiled just in time, not interpreted or compiled ahead of time. That said it should be possible to cache (precompile) more code and save it to the disk automatically, but that hasn't been done yet. For now you would need to use PackageCompiler. &gt;Until the start up time is fixed, it seems that julia isn't for me. Maybe, at the moment Julia isn't good for small tools that need to start fast and do a simple task. Currently it's more suited to computationally intense workload or exploratory data analysis, where startup time doesn't matter. That said there's nothing wrong with leaving a REPL open and including your file, it literally take two key strokes. 
Because Julia has its own use-cases and agenda. And it mostly involves running things from the REPL. At least until full compilation is possible at some point in the future. &amp;#x200B; I'm sad to say, but it seems like Julia might not suit your needs at the current point. Other use-cases simply has higher priority.
Yes, that is an odd statement. 0-based indexing naturally has its advantages in some fields, but it was particularly 'mathematical applications' that led to the decision to use 1-based indexing.
Well, but that would require our WINDOWS user to install Git, require us to provide a Git server for the users to be able to get our Git repo and the willingness of the "normal" WINDOWS user to do such things, which is quite low to non-existing. So I have to stay with the include statement and keep on lamenting that it does not work with packages and Julia's startup time is enormous. Quite some reasons to keep on using and recommending Python :-/
Yeah, this was mentioned. I was discussing it in the Julia slack yesterday evening and it seems that while most constants propagate into the function normally, in the case of arrays they do not. This is a problem that needs fixing as it is present in other mathematical languages. I am aware of @inbounds but I am not looking to optimise this code really, only demonstrate some metaprogramming features of Julia through one specific optimisation (which is more for the benefit of encoding data than actual speed).
I think you're a bit confused about what `include` actually does. From the documentation: Base.include Evaluate the contents of the input source file in the global scope So it takes the file, and tries to evaluate it as Julia code. However, your `.toml` file is not valid Julia code. From the TOML.jl readme TOML.parsefile(filename::AbstractString) Parses a TOML AbstractString or IO stream into a nested Array or Dict. To access it, you'd do something like import TOML file = "/path/to/file.toml" parsed = TOML.parsefile(file) readtimeout = parsed["HTTPConfig"]["readtimeout"]
Nope. It's too slow to start (needs to do final stages of compilation) and it is geared towards numerical computing with general programming being a nice add-on.
What do you really mean by "shell scripts"? I mean, I know what a shell script is but what would you suggest the typical ask is? I run Julia every day in a shell by hand to do various administrative tasks - scrape data from a website, update a SQLite database, combine it with data stored in colleagues Excel spreadsheets and generate a report in another Excel spreadsheet from it for distribution. 
in my limited experience, for such "orchestration" tasks, i would prefer python, even if the language is inferior. it has lower "footprint", significantly larger existing code base and connectivity, and actually faster for this purpose. kinda pity, because i keep banging my head against the table and ask "srsly? why does it need to be so difficult?" python feels like bash compared to julia.
Shell scripts that are used in CI/CD jobs. I have used Ansible extensively and it in general can significantly reduce dependence on bash scripts, but at times I want to write a simple shell script in a language that doesn‚Äôt teleport me into the black hole of existential crisis like bash does.
Strange, maybe I'm just constantly changing structure definitions. I don't know why it seems like I frequently still need to restart the REPL even when using it. Does it require a restart if one is editing two modules, the second depending on the first? That is also a common workflow for me.
Thanks, I'll have to look into that!
It doesn't require running from the REPL, it just (currently) requires that when running the same code repeatedly you don't keep reloading Julia. One could certainly write a script that iterates through files, loading data from them and calling some functions on this data. That would presumably avoid the issue of repeatedly re-loading Julia, but doesn't require an interactive workflow. 
Thanks for commenting. u/Dutsj's comment above really helped. Be well.
I also take versions of python where you guys had no problems compiling stuff with PyPlot etc. &amp;#x200B;
I still can't wrap my head around generated functions... Do you have an easier example to digest generated functions? What can a generated function do that a normal function cant...
I'd say no. &amp;#x200B; Less than 100 LOC -&gt; bash More than 100 LOC -&gt; python &amp;#x200B; Using julia to replace bash scripts sounds like a bad idea imho. As /u/mobius-eng said, it's for numerical computing! &amp;#x200B; (note: it is possible to make very robust shell scripts!!)
If you can compile it, then Julia might work.
&gt;python feels like bash compared to julia. Can you elaborate?
why not python for everything?
the startup time would definitely be an issue for many shell scripts, so I'm glad you mention it. However, in terms of "general programming being a nice add-on", what feature does Python or bash have that make them better suited for general programming than julia?
Generated functions are a function on the types of the input. Basically, Julia lets you create functions which act differently dependent on the input types, this is multiple dispatch. However, what if you wanted to programmatically determine what a dispatch would be using one code for generating all dispatches? That's a generated function. 
No, no restart required. You can even track and modify Base and Compiler functions (https://timholy.github.io/Revise.jl/latest/#What-Revise-can-track-1)
Which julia packages do you use for excel tasks? There is like too many of them ü§î
Why do people keep making a big deal of the 1- vs. 0-indexing thing? I mean, is driving on the right an objectively better choice than driving on the left? Seems like the same thing to me.
I wrote something which scrapes the Google Maps API to get a list of travel distances from a matrix of travel locations, with cached results to minimize trips to the API. This is something I would normally run in Ruby or Python scripting... I wrote it in julia because 1) I can't figure out how to install ruby anymore 2) it's way prettier than python and 3) i don't care that it takes about 10s to start up.
my favorite "why not python": scary nonlocal effects. ``` def f(a = []): return a val = f(a) val.append("hi mom") f(a) ``` I would say less than 25 LOC -&gt; bash 25-200 LOC -&gt; python 200+ LOC -&gt; Go for CLI, Elixir for servers, Julia for number crunching, Rust for anything lowlevel
Don't! The start up time will kill you. At least wait until they fix it.
I don't know that that will ever be changed entirely. 
I just use whatever comes in Conda.jl. It works great and most packages CI against that.
Better data structures and idioms.
Honestly, I love Julia but I'd probably pick Ruby or Fish or csh or something if I wanted to get away from bash. I think your scripts will get more verbose in Julia. The shorthand of shell syntax is pretty convenient when you're launching subprocesses with interpolated arguments and redirecting output and checking return codes.
Very much debatable. 
Have a look at [XLSX.jl](https://github.com/felipenoris/XLSX.jl), there are also references to other related packages. I've also used [Taro.jl](https://github.com/aviks/Taro.jl), but it's not Julia 1.0 ready yet.
Do you have any examples? Flexible and powerful data structures appear to be one of Julia's greatest strengths. And what idioms?
There might at some point be an interpreter for Julia, which could greatly improve responsiveness. And also possibly some static compilation.
Do what you want, but imho if you want to have a few commands with some variables, bash is the way to go, it's silly to think of something else when you already have the right tool for the job! Also, bash (or sh) is installed everywhere, not python. And you have this issue of v2 or v3, and dependencies (and how to manage them). Whereas bash is straightforward and will just work as expected.
does bash even have default arguments for functions? While I agree it's a blemish of python that default argument objects are created once, and not on every function invocation, it doesn't sound right to call it a nonlocal effect, or to say that python in general has nonlocal effects.
You can call that function in a totally different file twice, remote file A expects the default, remote file B, packaged in a private pip deployment, has a place where a different developer has carelessly used the return call of a the packaged function and mutated it in a function that calls the default, three calls in.
If plotting is the issue Gaston.jl is a good interface for GNUplot
What are the options to initialize SIMD vectors other than listing all of their elements: ``` a = Vec{4, Int32}((1,2,3,4)) ``` I'm looking for something along these lines: ``` a = Vec{32, Int8}((1:32)) ```
It needs to be some kind of tuple to be inferable. Here's the options: https://github.com/eschnett/SIMD.jl/blob/master/src/SIMD.jl#L92-L122
How come the 3x3 is slower and will it always require complex code similar to \`**matmul3x3\`**
Does the SIMD.jl library replace the @simd macro?
Oh okay so there's more fine grained control but it requires more knowledge of what you're doing. Cool
This post perfectly illustrates why benchmarks are boring for comparing Julia to other languages: Even Python can be made fast enough (with numba and numpy). *Speed* is not why we should choose Julia.
Looks promising! I don't quite understand what you did. What are the macros you are using? and what does it mean using `$var` when passing a variable to a function? I hope you have some time to explain us what you did! 
I'd say, focus on metaprogramming ability, math-friendly syntax, and barrier-free use of *both*, vectorized code and imperative loops. Personally, I come from a Python background, and the biggest hurdle for using Python in science is its rigid syntax. `numpy.exp(1j*numpy.pi)` is just not a very convenient syntax. This is necessary, since modules do not change the global namespace in Python--which is a good rule for software development, but not very convenient for scientific notations. Another example is pandas, which has a *query* method, like `dataframe.query('trial&gt;=50')`, which is a shorthand for `dataframe[dataframe.trial &gt;= 50]`. This is necessary, since more complex expressions in the latter form would duplicate the `dataframe` several times. But encoding such a thing as a *string*, with no syntax checks, is just sad. In Julia (or R), you would be able to write these things much more cleanly. As for the point about vectorization, many algorithms are more cleanly expressed in a vector notation, whereas others are most easily expressed as loops. Python and R and Matlab can do vectors, but loops are very slow. C can do loops, and has to use loops even for vectors and tensors. With Julia, you are free to choose the most natural way of expressing your code. But then, I am not nearly as proficient in Julia as I am in Python. But because of that, I see performance arguments as silly. I *can* make any Python code fast if I want to. Performance is not a problem I need a solution for. Syntax however, is.
Julia has a package system too, and loading multiple packages into the global namespace often causes collisions. The only way around this is to explicitly mention the module name. If you're sure that you're namespace would be clean, you can just import everything from numpy and not bother with namespaces. However they exist for a good reason, and you can always use aliases like np for numpy and df for dataframes.
I thought this post gave great insight about how speed is actually achieved in different languages (or what is preventing them). It's probably the best Julia benchmarking article I've read. The code examples and discussion about the LLVM vs JVM was particularly interesting.
Julia has namespaces, true. However, basic constants like e, i, pi are built-in and basic functions like exp, abs, sin are readily available as well. Furthermore, libraries can extend these functions for new data types without the namespace prefix (only partly possible in Python, e.g abs but not exp). Finally, Julia can define new operators, which yet again don't need a namespace prefix.
See the page that defines the plot functions: http://docs.juliadiffeq.org/latest/basics/plot.html You can do things like `plot(sol,vars=(2,4,6))` to view solution variables 2 v 4 vs 6 as a 3D timeseries. 
this is how you catch these errors. you should pay attention to the ::Type{...} part. it looks innocent, but this crucial. it indicates that the parameter passed was not a value but a type. if the parameter is an Int, you would see ::Int64, but if the parameter is *the type* Int, you see ::Type{Int64}. in your case, G is not an array, but an *array type*. it is because you forgot to call a constructor, you just used the type. as an analogy: N = 1 N = Int your G falls into the second category
If you are using Julia, you can plot through Julia with Plots. It has a few backends including PyPlot (https://docs.juliaplots.org/latest/examples/pyplot/)
[removed]
I think the best benchmark is personal; how fast is the simplest implementation of the types of codes I want to run. Can I get good/great performance without writing lots of extra code for optimization (i.e. going to Fortran, C or Cython plugins)? For linear algebra many languages now have good vector-based interfaces to essentially the same c/fortran routines, so if that is my application I can use pretty much any language. For lots of other types of math problems I would either need to roll my own code or spend lots of time trying to find if anyone has written a library in that language (and then learning how to use it, and seeing if it actually helps on my problem). It is in this case that Julia really starts shining. Once one learns to watch for, and avoid, type instabilities, Julia allows for a high-level code (like Python) that offers good C-like performance. 
`G` is an array of arrays, go `G[i]` is an array. `G[i][j,k]` is the `j,k` element of array `i`. Note in these scenarios, using `VectorOfArray` from RecursiveArrayTools.jl can be very helpful. https://github.com/JuliaDiffEq/RecursiveArrayTools.jl#vectorofarray You'd just do `VectorOfArray(G)` and you have its interface, which does: ``` A[i] # Returns the ith array in the vector of arrays A[j,i] # Returns the jth component in the ith array A[j1,...,jN,i] # Returns the (j1,...,jN) component of the ith array ```
It's worth learning, both to keep in your back pocket on the off chance you run into the two language problem as well as broaden your algorithmic thinking. Julia's syntax and notation is minimal, like Python, but it's code philosophy is different enough to warrant serious thinking about HOW to structure and write your code. In my case, it offered me the swift kick I needed to start writing my machine learning algorithms in a more functional style, for easy QA and testing, as well as more provable I/O, even though I was working in Python. That said, Julia is still a relatively young language, and unless you're in dire need of the speed or numeric algorithms it offers (a lot of cutting/bleeding edge numerics research is done with Julia), you should still be okay learning and using just Python for the next few years, at least. 
if you are still feeling your python is improving an an almost daily basis, then, no, keep at python.
that's a tough one. as i see it, julia is up one league in clarity and expressiveness. it is a completely different game and different experience. however, it only matters if develop libraries or complex software, and you learn how to do it right. for a programmer, i would without a doubt suggest taking a look. but for some only interested in getting some particular job done, it is a harder sell.
think about it the other way around, what is the value of G[1] ? 
In that example, the length is inferrable because it is explicitly given as `Vec{32, Int8}` constructor. To load data into a SIMD `Vec`, one can use `vload(Vec{N,T}, xs, i)` to load `N` values from the vector `xs` starting at index `i`.
include("./file.jl") may be what you're looking for
And if I change the file afterwards, should I include it again? 
I tried. My file looks like module SomeModule #some code end And when I press ‚Äúrun‚Äù, REPL throws an error saying I should add the package SomeModule.
Yes. And the code will run again from line 1
I will try and learn it at a relaxed pace then. It certainly won't hurt me knowing another relevant language in applied AI/ML in any case. Thanks for sharing your experience.
The good thing about my current role is that I will continuously rely on Python so hopefully, that improves my experience it for a long time. 
I only have an interest in applying ML/AI techniques. I will leave the development of these techniques to people smarter than me :)
I have never had an experience with C or those so-called fast languages so I don't really see or appreciate their 'fastness' as opposed to Python. If a language allows me to run scripts faster than Python (which I feel is blazing fast), then I'm all up for it. I will gradually learn Julia. Any recommended resource for learning Julia?
I particularly like [this tutorial](https://lectures.quantecon.org/jl/index_postgrad.html). It goes into a bit more depth into how to optimize your code and exploit the type system than most other introductory tutorials I've seen, but it still keeps it really approachable for first-time users. It also doesn't have much to do with quantitative econ. Only issue is I think it's written with Julia 0.6 so here and there you might have to make some minor code modifications. 
The `Base.Cartesian` \[[link](https://docs.julialang.org/en/v1.0/devdocs/cartesian/#Base.Cartesian-1)\] module is supposed to help in the generation of generic functions for operations on arrays. I''ve been playing around with it in an attempt to familiarise myself with both the use of macros/generated functions in Julia. The performance gain from use of these macros is not trivial, but it does have a learning curve. The two macro invocations expand to: julia&gt; @macroexpand @ntuple(4, op) = (+,-,*,/) :((op_1, op_2, op_3, op_4) = (+, -, *, /)) julia&gt; @macroexpand @nany(4, i -&gt; op_i(a,b) == c) :($(Expr(:||, :(op_1(a, b) == c), :(op_2(a, b) == c), :(op_3(a, b) == c), :(op_4(a, b) == c)))) As for the `$var` in the `@btime` macro, as I understand it interpolates the values of a, b and c into the benchmarking macro so that you don't measure any allocations/evaluations that are not actually part of the function, i.e. you only benchmark the function itself. I would recommend looking into how Julia interpolates values into quoted expressions for a more complete explanation \[[link](https://docs.julialang.org/en/v1.0/manual/metaprogramming/#Interpolation-1)\]. &amp;#x200B;
Basically, what I do is have a REPL open with vim on top of it in a tmux session. Then, when I need the interactivity of the REPL, I use that and when I want to write long functions, glue things together, I put them in the source and include the file in my REPL session. I have no problem with this since coming from R, I often had to "source" my files. &amp;#x200B; Or you could use Juno but I found it to be somewhat slow and limiting when using only a keyboard. With Juno, I think you need to press shift+enter to source the file but it is also easy from the repl. I just press ctrl+R then type inc and hit enter. They both are the same amount of work for me. 
not perfect though because if you change the signature of a function, the old one will not go away, they will be two methods. fine in most cases, but still.
You might find something like Numba or cython useful. Julia is cool too, though!
Certainly not ideal, but in attempting to replicate my R setup I've played with [vim-slime](https://github.com/jpalardy/vim-slime) to get the convenient link between vim and the REPL. There's a post showing how to set it up here: [http://www.serhatcevikel.com/?p=56](http://www.serhatcevikel.com/?p=56) I do wish that there was just a straight clone of [NVim-R](https://github.com/jalvesaq/Nvim-R), though. That's almost perfect for me.
Thank you. I think I remember you as a contributor to the nbody package? Big fan of it!
Doesn't really work. It says I need to add a package with the name of my module (unless I'm doing something wrong)
so i haven‚Äôt done much julia since 0.7/1.0 came out. But when i used this workflow, i needed to have a module (which is basically the same thing as a package) in a particular folder which i told julia is where special packages are. I think with the updates you have to do something like ‚Äú] init‚Äù to create the folder structure 
I use include a lot with the REPL. Revise.jl was really a game changer for package development. 
https://github.com/JuliaTPU/XLA.jl
No you don't need to do that. Personally I put everything in a module in a single file (not a package) and then use \`includet(myfilename)\` to get Revise to load it and track changes in it.
With Gadfly you should be able to do: p = plot(...) draw(SVG("foo.svg", 6inch, 4inch), p) You can also save in png and pdf, but last time I checked there was an issue on v1.0. http://gadflyjl.org/stable/man/backends/
With Plots use the savefig function. You can do png or svg
Recently I have been watching quite a few videos on the problems with OOP (I can try and find the links if it is important). The general gist of the videos are that it is a lot easier to understand code when data is just data, and not some sort of black box object to interact with. I think Julia, with it multiple dispatch system, fits in quite well with the entire "Data Orientated Programming" style. Write out your generic functions, then provide methods for your specific types if the generic method is not sufficient. Now onto the code you supplied: Julia already has the `getfield`, `setfield!`, `getproperty` and `setproperty!` functions which you can use to get/set fields in a struct. If there are any specific things that need doing for your custom data types, just ~~override these functions~~ provide the methods (the power of multiple dispatch strikes again). (`getproperty` and `setproperty!` actually define what happens when the syntax `obj.a` is used, so you don't even need to write out the more verbose function names!) &amp;#x200B;
I've made an example with simple functions because it was the easiest thing that came into my mind... In my usual work flow I ask the user (usually myself) a set of informations which are the free parameters of my model. I then pass those parameters implicitly to a series of helper functions. A function, which is to be callable by the user, uses the results provided by the helper functions and returns a value which is the "result" of my data analysis. I then check if the result is "good" according to certain criteria. What I need to do is to repeat almost the same analysis with different initial conditions, thus the idea of making a type containing those initial conditions and also all the intermediary numerical results which need to be shared between the functions contained in the same module. I don't know if I managed to explain my work flow... 
x.m(a) is just a syntax for m(x, a) the person that sold the world the idea that the former is somehow more logical is a criminal. it will take a while to clean up the mess. however, there is more than syntax to it. most languages bind methods to a type. that sort of warrants the special syntax, as it shows this connection. in julia, methods are not bound to a single type, the are bound to all the types in the parameters. consider a method that takes a matrix and a vector. is this a method of the vector or the matrix? logically it belongs to both types. also julia is duck typed. usually you want to write methods in type-agnostic way, and don't care too much about what you get as long as it knows what's needed. as a general remark: you should not try to reproduce features from other languages. leave them at the door, and see what you are offered by julia. it might turn out that you don't even need that type at all. a mutable object is particularly suspect. if you have many of these types, there is something wrong with your approach. maybe you should show a specific example.
As soon as I have time I will try to write a clearer example of what I'm doing (in python) so that I can get better suggestions, thank you! 
From what I understand, some global state needs to be shared between the functions in the module you are creating. Julia functions should not change global variables. This unfortunately leads to rather poorly performing code. There are hacks around this, but I'm not sure which are still work, nor what problems they contain. Note that how performant code is, is subjective - or at least how far you want to take optimization is subjective. So maybe altering global variables is not much of a problem. What you could definitely do is create a type that stores the intermediary results from the calculations and pass it in as an argument to the function. You could even specify it as the default argument to the function if you don't want to write it out each time. There is an `__init__()` function available for a module that evaluates when the module is loaded for the first time in the current Julia process. You could use that to initialize a global variable in the module namespace that is passed as an default argument to your data analysis functions. With all the above done, you could create an array of your initial conditions and map your data analysis function over that array. Now you should be evaluating different initial conditions, with each successive function call having access to the previous intermediate results in a performant manner.
Data encapsulation and polymorphism are not exclusive to OOP. :) Programming is all about managing complexity through abstractions. In the dark days of slow computers, abstractions were computationally costly and no one used them (actually it is not true, languages with grate and efficient abstraction support were around long ago: e.g., common lisp and ML - both old, both fast, both blew your typical OOP language (e.g., java, C++, python, whatnot) in terms of abstractions out of the water). Then computers became faster and code bases became huge... Abstractions became necessity! And historically OOP approach became popular (why -whole different story). It is good for some problems (e.g., development of gui interface) and bad for others (e.g., some people around consider scientific computing one of those areas). &amp;#x200B; Answering your question! If you want be a master of structuring your program :) in any language, and since you decided to learn julia (which is very close to lisp in some ways), I would advice you to read Structure and Interpretation of Computer Programs (SICP). Nice book, not as complicated as some think, and in my opineon very applicable nowadays. Here is a nice link: [http://sarabander.github.io/sicp/html/index.xhtml](http://sarabander.github.io/sicp/html/index.xhtml) &amp;#x200B; This is mostly related to dynamical languages. If for some reason you would like to learn more about statically typed languages, poke internet about Haskell/Ocaml. &amp;#x200B; Last but not least, if you want to see more of OOP: smalltalk or at least ruby. &amp;#x200B; With all that, you would not have any problems structuring your julia program (SICP would be quite enough - Haskell and smalltalk would be just another addition of your understanding of abstractions).
In that case you would write a model type and pass it to your function. Don't use globals unless you really need to. m = fit(m::MyModel, data)::MyModel You can write help functions: params(m) #getter params(m,x...) #setter You can check `MultivariateStats.jl` for an example of that type of interface: https://multivariatestatsjl.readthedocs.io/en/stable/pca.html Typically you want to reuse common functions like fit, mean, etc. 
Thank you! I'll look into that! I have a few more questions if this doesn't bother you. When you said to avoid globals, do you mean "avoid writing functions which rely on global variables" or something else? "global" variables defined inside a module are considered global in this regard? If I define a type inside a module and then create an object (I don't know if it is the correct term) of this type inside the main program, is it considered global? Is it alright to make all functions of a certain module require as the first parameter a variable of type MyModel? Should one object of type MyModule contain every variable that needs to be shared between the functions of said module (the same way in python one would use self.var)? Thank you again for your help! 
Thank you very much for the useful insight! 
Thank you very much for your help, you have given me much to think about! 
give a use case. what kind of meaningful change you can apply to a module to get another module?
Yes, even though `Float64 &lt;: Real`, `Array{Real,1}` is not a subtype of `Array{Float64,1}`. You can use a parametric type, function something(Array{T,1}) where {T&lt;:Real} .... end
&gt; function something(Array{T,1}) where {T&lt;:Real} Yeah, works. Thank you very much.
few comments: 1. Array{Real, 1} is an actual type, which means an array of an undefined real subtype. that is, a heterogeneous list that is limited to real subtypes. you can use such a type as parameter, and it works, albeit the performance will be not very good. 2. are you sure you need type restriction? as a rule of thumb, you only want to specify a type if you want to specialize. that is, you have two methods for different types. even so, you probably want minimum restriction, like function something(a&lt;:Array) ... function something(a) ... to handle arrays separately from non-array types. the reason for it is that the user might come up with their own types, which you didn't even think of, and they can't use it because you banned it. are you sure your method does not work for complex numbers?
In general you are sure right. Thank you. But in my special case, I need Real for real.
The module (or library) to operate for example needs a some container library to operate, this library provide generic interface like: create container, modify container. etc. There are various possible implementations of this container, which has different performance characteristics (neither is better). So by switching your library dependence on different modules, without changing the code, you would change data structures it uses. Or if some library which you rely on is broken, you can easily substitute it for different one. This is the only one simplest show case. To know more, I asked here, so people will share their idea. Even if modules which are already written are not composible - i.e., there is different interfaces, it is easy to write intemidiate glue module which will change the interface. In essence, this feature promotes composability further - on level of modules, not only on the level of functions.
That is only an example. I do not know Ada, but I had in mind ML (e.g., Ocaml) functors. This parametrization, is the key to structure most of MLs programs, moreover, it also allows to achive ad hoc polymorphism. So this feature maybe not as crucial for Julia as it is for example Ocaml. But the main idea of course is to have generic modules, which behavior is parametrtized by other modules, which opens whole new area how to structure large programs. Maybe in some cases, it would be more convinetn for polymorphism then multiple dispatch. For example if you need not only one polymorphic function, but rather a set of behaviors - like polymorphic data structures+functions together. Or something like this. What you had in mind? 
Do you have a use case that wouldn't be solvable using metaprogramming? 
hehe. It is not a fair question. everything language specific feature can be solved via metaprograming (probably except some optimization). Look at CL community - they produce such magery, it is scary. Nevertheless, features are added into reflective languages for convenience. Look at Racket - it has the most sophisticated metaprogramming capabilities! yet, they constantly add more things to the language to make it better. Few examples, contracts, their module system, typed racket, etc. 
a general module-module function would take any module and give a new module. i don't exactly see the use case for this. what you outline here takes a module template, which is not a module in itself, and takes some parameter, like a type or perhaps a module, and combines them into a module. this has a different input type. it can be done, the question is whether you want it or there are other, better ways. i think majority of these cases can simply be solved by duck typing. like you pass any kind of enumerable or indexable types, and it just works with that. it is more interesting if the methods don't get the containers as inputs. but rather, we either expect them to return a specific type, or we want them to use that type internally. the julia way is to take the type as parameter, or in case of your own types, perhaps parametrize the type with a type. something like struct MyType{ContainerType} ... and then instantiate them with MyType{MyCont}(...) and so you store an extra value, which can be used in methods. function dosomething(mt::MyType{T}) where T = use T and mt as you need so the true question is whether we need this as module level, as in, 1, multiple types and functions take a common parameter, and 2, do we need modules as parameters, and not types
I think that use case is already covered by abstract type with multiple dispatch, e.g. #abstract mpdule defining the interface abstract type Container end things(c::Container) = error("not implement") how_many_thing(c::Container) = length(things(c)) #module 1 struct C1 &lt;: Container x::Float64 end things(c::C1) = c.x #module 2 struct C2 &lt;: Container x::Float64 y::String end things(c::C2) = (c.x,c.y) #using module1 how_many_thing(C1(1)) #using module2 how_many_thing(C2(1,""))
no and no 
No real encapsulation as far as I'm aware. You can mimic interfaces and some aspects of inheritance using the type system, though since Julia is a dynamic languages nothing can be enforced. Julia just doesn't have many object oriented features that you find in languages like Java.
That worked for me: `using PyCall` `pyimport_conda("sklearn.neighbors.kde", "scikit-learn") #download the package you need` `@pyimport sklearn.neighbors.kde as py_kde # import it` `kde = py_kde.KernelDensity[:fit](py_kde.KernelDensity(kernel="gaussian", bandwidth=0.2), m_train)` \[:function\_name\] (object, args...) is the why you use function `py_kde.KernelDensity[:score_samples](kde, hcat(m_dens[i,2:end]...) )[1]`
I believe because the for cycle's scope is out of the scope that contains it. Using 'global A = 8' should fix it.
This doesn't change it `global A = 7` `begin` `for i in 1:10` `A = 8` `end` `@show A` `end`
So, I've always wondered about this as well and decided to look it up. Maybe this will help. https://stackoverflow.com/questions/51930537/scope-of-variables-in-julia
The A inside the loop and the A outside of the loop are different variables. The one outside of the loop is a global variable, while the one inside the loop is a local variable (local to the loop). You need to tell the loop that you're talking about the global A when you tell it to assign the new value. You can bring any information you want into the loop from the outside using global variables, but any assignments that are done inside the loop are done to local variables by default.
If you wrap this code in a function it'll behave as you originally expected. Julia on the REPL operates in global scope.
As most people already said, it's because of the global scope of A outside the loop, and the local scope inside the loop. This only happens if you dump code into the global scope, if you put it into a function then you avoid global scope entirely and that solves the 'problem': julia&gt; function foo(a::Int) println(a) for i in 1:10 a = 8 end println(a) end foo (generic function with 1 method) julia&gt; foo(10) 10 8 I hesitate to call this a problem since it works as designed, you could end up with a lot of bizarre issues. Depending on what version of Julia you're on, you may or may not get a few deprecation warnings running the code you showed: julia&gt; A = 7 7 julia&gt; for i in 1:10 A = 8 end ‚îå Warning: Deprecated syntax `implicit assignment to global variable `A``. ‚îÇ Use `global A` instead. ‚îî @ none:0 julia&gt; @show A A = 8 8 
 julia&gt; c(s::String) = 5 c (generic function with 1 method) julia&gt; Base.hash(x::String) = c(x) julia&gt; hash("a") 5 
I think in general developers prefer(red) this (now interim) solution to the previous hard/soft scope problem... https://github.com/JuliaLang/julia/pull/19324 ... because it improved performance and reduced coding errors. But the more interactive-focused users didn't like the new solution any more than they liked the earlier solution.
Perhaps, I'm a bit late to the party, but ... Have you seen the QuantEcon.jl package ([https://quantecon.org/quantecon-jl](https://quantecon.org/quantecon-jl))? It seems pretty relevant to me (without knowing details of what functionality you need).
So all in all there was an improvement.
Thank you guys. You are always helpful!
You cannot write a custom hash for `String` because it already has an hash defined for it. You can create your own type that wraps a `String` and define the hash on that type. ``` julia&gt; struct MyString str::String end julia&gt; Base.hash(s::MyString, h::UInt) = hash(0xFFFFFF, h) julia&gt; Base.isequal(s1::MyString, s2::MyString) = s1.str == s2.str julia&gt; hash(MyString("foo")) 0xf6af40ff0a1b116f julia&gt; hash(MyString("foobar")) 0xf6af40ff0a1b116f ```
enumerate through the rows one at a trime and you'll find the one with a problem.
Please don't use "girlfriend" to talk about Julia. It's a programming language, i.e. an "it". The community guidelines are pretty clear about this. 
I can't provide an answer, but I'd recommend asking this kind of question to the julia discourse, or the slack #helpdesk channel [https://julialang.org/community/](https://julialang.org/community/)
I think this is type invariance described in that section of the docs: https://docs.julialang.org/en/v1/manual/types/#Parametric-Composite-Types-1 Basically even though String &lt;: Union{Symbol,T} where T is true Vector{String} &lt;: Vector{Union{Symbol,T} where T} is false. You can see is the than this: Vector{Float64} &lt;: Vector{Real} If you replace String by Float64 and `Union{Symbol,T} where T` by Real. The solution is to rethink you types and methods. It looks too complicated, but it's hard to tell more without knowing what you are trying to do exactly.
You want build(suptokens::Vector{&lt;:Vector{&lt;:Token{&lt;:Any}}}) Because typeof("A") &lt;: Token{&lt;:Any} is true, but typeof(["A"]) &lt;: Vector{Token{&lt;:Any}} is false. Whereas, typeof(["A"]) &lt;: Vector{&lt;:Token{&lt;:Any}} is true. The issue is that ["A"] is a Vector{String}, not a Vector{Token}. However, String is a subtype of Token, so ["A"] is a Vector of a subtype of Token. Alternatively, if you only want build to work with types that are exactly Vector{Vector{Token{T}}} function build_strict(suptokens::Vector{Vector{Token{T}}}) where T suptokens end and then be sure to pass it a Vector{Vector{Token{T}} instead of Vector{Vector{String}} atokenvec = Vector{Token{String}}(undef,1) atokenvec[1] = "A" btokenvec = Vector{Token{String}}(undef,1) btokenvec[1] = "B" build_strict([atokenvec, btokenvec]) 
I haven't tried slime, but vimcmdline will send a line across to the repl for me. I miss nvim-R's functions though.
Yep, and several people on the developer slack are doing the same 
Thank you. I actually read this part of docs and there I found about the `:&lt;` operator which should be used exactly in those situations.
Thank you very much, that seems reasonable (if somehow too complicated and not really nice). I'm to the Haskell way of things, where I would write simply build :: [[a]] -&gt; Model But thank you nonetheless.
I for one think it's a poor design choice - really what's the benefit? Also, it works differently on Jupyter notebooks. Having code work differently in different environments is obviously bad.
No I don't think so. That thread is for hardcore Julia developers. I really think the design choice is counter-intuitive and difficult for beginners (myself being a beginner). I find myself having to force code into a function during iterative development, which isn't always very efficient
Thanks for the link! Had no idea this was a thing
Yes. Leaderboard?
Do you really need the typing though ? Just do: build(suptokens) 
The main/easy/obvious way to do any sort of OOP style programming in Julia would be via composition based methods exploiting the type system where the type structures you create define smaller objects that you can assemble into larger ones and then create functions that only operate on specific types or on different types in different ways via multiple dispatch. 
Yes
`freq_array = []` that array is not typed and so everything will be uninferred. Do this with `Float64[]`.
Wow, that slayed it. Took all of 5 seconds.
There are a some extra performance traps in your code: 1. You are using non-constant global variables, specifically `lines`. Instead, make sure that you send `lines` as an input argument to `part2`. 2. You have a type instability in `run_sum`. You initialize it to an `Int`, but it subsequently changes into a `Float64`. Make sure to initialize it to `0.0` instead. 3. You should use BenchmarkTools.jl when timing your code: &amp;#8203; &gt;] add BenchmarkTools &gt; using BenchmarkTools &gt; @btime part2($lines) &amp;#x200B;
Ok, I figured it out. My helper function, currently is picking out the value of duplicate whose original occurs first in the input array. So, if `a = [1,2,2,1,3]` my function will output 1, although what I really want is 2 (because 2 is the first repeated value). I'm not sure if there's a way out of this for me. My revised code is as follows, and is terribly slow. function return_duplicated(array::Array{Float64,1}) for i in 2:length(array) array_before = array[1:i-1] if count(j-&gt;(j == i),array_before) &gt; 1 return i break end end end The only options left are (1) case my array into a data-frame and use the `nonunique` method (I think).
I would definitely use dict instead of array for keeping track of frequencies.
Please use a `Set` instead of a `Vector` for this. Just change `freq_array = []` to `freq_array = Set{Int}()`.
Wouldn't Set be much more efficient?
I mean efficient in terms of cheking whether we have reached the same frequency already. In Dict it will be O(1), while in Set or Array it will be O(n).
First of all stop using an array to track the seen frequencies, this is giving you horrible time complexity which is why your code is so slow. The best solution for this problem is: iterate over the frequency changes. Compute the current sum, if the sum has been seen before (i.e. it's present in the set), then break, else add the current sum to the set. A dictionary works for this as well, all we want is for the checking if we have seen an item before as fast as possible (with a set or a dict O(1))
You can simplify the code, in your loop you first check whether the key exists, if it does you increment and if it doesn't you create it. You then go on to read that value just after. Instead of all this, check if the dictionary has the key, if so great we're done break from the loop, if not then just add the key (with an arbitrary value, doesn't matter) and continue looping. This also means you can use a set instead of a dictionary
`Dict{String, Integer}` doesn't initialize a new dictionary, you're setting m to be the type. Put a pair of parentheses after the type to call the empty constructor. Also, there's no reason the dictionary shouldn't accept 0 or negatives as keys, can you post the code you were using to try and do that, and the error you were getting?
`m = Dict{String, Integer}` is a type rather than an object. To initialize an empty dictionary you need to use `Dict{String, Integer}()`. Also, `Integer` is an abstract type which includes unsigned integers, 8 bit integers, etc. It would be better to use `Int64` or `Int`, which is alias for 32bit/64bit integers for 32bit/64bit systems. 
fun fact: [https://github.com/JuliaLang/julia/blob/d789231e9985537686052db9b2314c0d51656308/base/set.jl#L3](https://github.com/JuliaLang/julia/blob/d789231e9985537686052db9b2314c0d51656308/base/set.jl#L3)
[https://github.com/kleinschmidt/adventofcode2018](https://github.com/kleinschmidt/adventofcode2018)
`Set`s are pretty much `Dict{eltype, Nothing}`
So, I edited the code as per the suggestions. &amp;#x200B; **1st version, using a Set** function part2(data::Array{Float64}) freq_set = Set{Float64}() freq_found = 0 run_sum = 0.0 while freq_found == 0 for freq in data run_sum += freq a if run_sum in freq_set return run_sum freq_found += 1 break else union!(freq_set, run_sum) end end end end @btime part2(lines) Benchmarking results: 4.639 ms (33 allocations: 3.00 MiB) &amp;#x200B; **2nd version, using a Dict** function part2_dict(data::Array{Float64}) freq_dict = Dict{Float64,Float64}() freq_found = 0 run_sum = 0.0 while freq_found == 0 for freq in data run_sum += freq if haskey(freq_dict, run_sum) return run_sum freq_found += 1 break else freq_dict[run_sum] = freq end end end end @btime part2_dict(lines) 4.930 ms (37 allocations: 5.67 MiB) &amp;#x200B; Are these what you'd expect? Any further improvements I can make to the code?
I had asked a similar question. My code using a dictionary is here [https://www.reddit.com/r/Julia/comments/a235dp/julia\_code\_takes\_too\_long/eawgzqe](https://www.reddit.com/r/Julia/comments/a235dp/julia_code_takes_too_long/eawgzqe) &amp;#x200B; Might be helpful.
That clarifies things! I think that also solves the issue with the keys for some reason - I must have been initialising it wrong. Many thanks.
Didn't know that. Thanks!
Yep good point! I've tidied it up a bit now. Didn't realise readdlm was returning a 2D float until I made the function types explicit. This runs in nearly 0.01ms now. function part1b1(d::Array{Int,1})::Int m = Dict{Int, Bool}() x = 0 flag = false while flag == false for i in d if haskey(m, x) flag = true break else m[x] = true end x += i end end return x end function part1b2(d::Array{Int,1})::Int m = Set{Int}() x = 0 flag = false while flag == false for i in d if in(x, m) flag = true break else push!(m, x) end x += i end end return x end @time readdlm("./input", Int)[:,] |&gt; part1b1 @time readdlm("./input", Int)[:,] |&gt; part1b2 Any further suggestions? 
Ha ha, our code is so similar! (just posted a Set and Dict version further up). Some minor differences, like you've done it in Float, I've kept to Int. You used `union`, I used `push!`. Didn't know you could just use `in` to check if in set.
Your code seems faster though, like by a factor of 40. Any idea why?
wtf Python takes 2 minsÔºüWhat CPU are you using? Cuz my Julia code only takes 10 secs to find the duplicate. How many cycle do you have to go through your input.txt before running into first duplicate sum?
I was an idiot and didn't using the dictionary in Python. Once I shifted to dictionary the code finished in 3.6 ms. The Julia version with a dict takes around 5 ms.
You can probably do this with finite state automata by first building an automaton to recognise all strings in the sequence and then counting how many times verticies are visited.
You're repeating work that you've already done as diff(a, b) == diff(b, a) hence your outer loop stays over the whole array, but the inner loop can only be over the subset current item to end of array (instead of whole array). 
not answering your questions though, but to make this faster, I would: for i in 1:length(data)-1 for j in i+1:length(data) and then using data[i] and data[j] instead of line and j
I did try to replace `[i for i in line] .== [i for i in j]` with `split(line,"") .== split(j, "")` But that only worsened the speed, and the memory allocation. \&gt; You're repeating work that you've already done as diff(a, b) == diff(b, a) hence your outer loop stays over the whole array, but the inner loop can only be over the subset current item to end of array (instead of whole array). This is an interesting point. How can I incorporate this optimization? Something like `index(line) &gt; index(j)`
Well now you know what your problem is. Does line .!= j not perform what we want? For that see the other guy's comment, he noted the same optimisation
Ah! this looks cool. Will try it out.
`line .!= j` will still admit the following redundant indices (line = 1, j = 4) and (line = 4, j = 1).
Have a look at https://github.com/samuelpowell/Spinnaker.jl if happen to have a PointGrey camera.
The collect can be replaced by a Substring t = "abcdefghj" s = SubString(t, 5, 7) # offset (start + n) and end (start + num chars - 1) "efg" 
are you running my code locally on your machine? I would have thought union would be more computationally intensive than push, but I'll try running your code on my machine first...
Thanks for the comment. I'm trying to achieve something like this: Given two strings a and b, extract their common elements: `a = "abcdfg"` `b = "abcelkg"` The characters in common for a and b are a,b,c, and g-- which corresponds to the index array \[1,2,3,7\]. Long story short, the substring need not be composed of consecutive characters.
No encapsulation. We all trust each other here.
Yes, and Go made a design decision to drop Sets.
Have you come from R? In R, vectorised code is much faster than looped code, and the general advice in R is not to loop, but to vectorise the analysis. But that's because control flow / loops in R is really slow. The 'vectorised' code in R is usually just C/C++/fortran code which is looped.
Yes, came from R. Got tired of the slow loops in R.
*Some* things are faster in Julia using for loops but other times there are built in functions that are somehow magically faster. I usually end up trying it both ways to see which is faster.
In general, to get maximum performance in Julia, you should use loops. Vectorized expressions can sometimes give equal performance, but often cause excessive allocations. Vectorizing your code is useful in 'slow' languages such as Python, Matlab and R. But it's all loops underneath. You have forgot to type your output vector here: string_ret = [] Write `string_ret = String[]` for better performance. 
If you‚Äôre familiar with R, just FYI that JuliaCall is a really useful library for calling Julia from R (equally reticulate for Python and Rcpp for C++). Or you can use RCall from Julia if you want to go the other way. My point is that you don‚Äôt have to isolate yourself to one language as most languages have convenient ways to use other languages - allowing a best of both worlds approach. 
A major "gotcha" with vectorized code like this is that you'll end up creating a lot of unnecessary temporary arrays. Notice how much more memory the vectorized version allocates (75,000 allocations vs. 12, for nearly 40 MiB vs. 1 KiB). That's probably where most of the speed difference is coming from.
I have no idea what this means, but it sounds awesome. Any chance of an example?
install on connected system, put on usb stick, copy. you dont need root, it lives in a self contained directory tree. You need to keep the same directory strcture though, full paths get baked into dependencies e.g. the conda python &amp;#x200B; same goes for Windows (which i had to do for a long time because my work pc is locked down and I didn't have the windows management framework so Pkg.add didn't work) &amp;#x200B;
you just need the generic binary for your architecture, and all the packages are really directories living in \~/.julia
It's air-gapped for a reason. Please talk to your system administrator first.
julia version 1.0.1 
if all else fails, put it in a docker container
This is due to Julia‚Äôs scoping rules when working with global variables. In your case a is a global variable so if you want to use it within the while block you have to use the keyword ‚Äúglobal‚Äù when doing the assignment. So ‚Äúglobal a = ...‚Äù. There should be a segment in the documentation about variable scope and scoping rules. This is mostly an issue with global variables, you wouldn‚Äôt see this problem within a function for example. The use of non constant global variables is usually discouraged for performance reasons. Generally speaking, most things in Julia should take place within functions and not at the global scope
And you use maps to implement Sets too. type void struct{} var empty void setOfInts := make(map[int]void, 16) 
I have way too much time on my hand tonight, and collected some links related to this... Github: https://github.com/JuliaLang/julia/issues/28789 Reddit * https://www.reddit.com/r/Julia/comments/a1mi5c/why_cant_i_access_the_a_in_the_forloop/ * https://www.reddit.com/r/Julia/comments/9vxkw6/weird_behaviour_with_for_loops/ Discourse * https://discourse.julialang.org/t/another-possible-solution-to-the-global-scope-debacle/15894 * https://discourse.julialang.org/t/new-scope-solution/16707 * https://discourse.julialang.org/t/confused-about-global-vs-local-scoping-in-for-loops-in-1-0/16318 * there are more Stackoverflow * https://stackoverflow.com/questions/52187073/does-scoping-in-julia-1-0-0-with-for-loops-make-sense-to-beginners * https://stackoverflow.com/questions/51930537/scope-of-variables-in-julia?noredirect=1&amp;lq=1
On the point about memory allocation: All the strings are the same length, so you could pre-allocate two character arrays and overwrite the values as needed.
julia is not ideal for pure functional programming. for example, currying is not automatic, if you have f(x,y) then f(x) is not a function, but you can make it so by explicitly defining f(x) = y -&gt; f(x, y) also, lazy evaluation is not a thing in julia unless you (or the library) explicitly says so. there is a large collection of enumerators and iterators which gives you lazy evaluation, but plain simple code will not. or not necessarily, because a lot of common constructs are automagically converted to lazy structures. but you need to be vigilant. basically, one could say julia has what it takes to be functional, but you need to be explicit about it.
Those are a funny two examples. Automatic currying and lazy evaluation are popular in the ML family of languages but there are other more or less purely functional languages without either of those things such as Clojure, Wolfram Language and Erlang. 
how do you define a functional language, and how that definition does not fit to borland pascal?
Cubature.jl
It's one of those packages that takes a function as an argument and returns the integral. I don't have a function, I have its values on a grid.
I'm also very interested in this. I'm still not very experienced at Julia but one aspect of it that was liberating is how it does away with classes and inheritance with multiple dispatches. Coming from C++, I found that a lot more elegant. But at the same time, coming from R, I was expecting much stronger functional style capabilities with better support for piping but even the piping operator is as of now really underdeveloped and you can only do the most basic tasks with it. 
Interpolations.jl will give you a function you can integrate over.
I agree, multiple dispatch covers ad-hoc polymorphism, but this is on a function level. When constructing libraries, one maybe might want to have those things on whole namespaces level. 
have no idea how borland pascal works - it was too long ago. But as far as I am concerned truly functional language has a) first class functions (including lambdas) b) lexical scope, to be able to do closures, etc. So both checks here with julia. I would also expect to see TCO in trully functional language and julia does not have it :( so it makes arbitrary control flow with function calls impossible. currying and lazy evaluation are not a markers of true pure functinanl language. For example Idris, Ocaml, Purescript are strict by default, but no less functional then say Haskell. Moreover, racket is very functional in nature, but you have to write currying explicitly (it is much easier due to lisp syntax). So I guess, if you would need easy cyrrying in julia, you would probably need to write some macroses. &amp;#x200B; p.s. If I am not mistaken, pascal does not have lambdas and closures, right?
newer versions of pascal do, but i wish they didn't. as i see it, you can use "functional" in two ways: 1, restrictive as in the lack of mutable objects and 2, having features that makes functional style viable and not a pita. i'm not an expert in functional languages or theory. but as i see it there is a shift toward the 2nd interpretation. as opposed to enforce, they enable. people keep saying me that currying is not needed, which is possible, but then why it is one of the most important topics when people teach lambda calculus or category theory? same goes for lazy evaluation. if you study haskell, it is very important, but it is suddenly not so important if you study another language deemed functional.
&gt;newer versions of pascal wow! I never new it evolves :) I used to program turbo pascal and then delphi when I was like 10-13 years old =D So someone still is using it? that is interesting news. Well I would somewhat disagree, because one of the most functional languages I know is scheme :) and it has currying via macroses. As I understand, currying style like in haskell is just a syntactical beauty, if you have first class functions, you have possibility to do currying by default. You just can make it convenient in your language. I do not know much of category theory except some blog posts, but curring is not a core feature of it. Morphism in category theory are just arrows between nodes which obay composition. And you can manipulate them any way you like. In math in general, you can apply your functions any way you want, so I would not say it is a core concept. Same goes for lambda calculus, you just do it often there. But mathematicians used partial function application long before lambda calculus was invented. It is just a way you can manipulate functions. about lazy evaluation - of course it is important for haskell, Haskell started as a project to investigate lazy evaluations in functional language, but functional languages were widely used before it. last, your distinction between 1 and 2 is essentially pure vs non pure functional languages. Which has nothing to do with currying. p.s. sorry for my english.
maybe i'm so focused on these because i started with cryptol, which is similar to haskell in these regards.
&gt;cryptol never heard of it. Nice that functional languages are becoming more and more widespread. By the way, I was not trying to bash currying and lazy evaluations. They are nice to have. I used to be a Haskell fun and I still like it quite a bit. My biggest frustration with Haskell is that I can not use it for my work =D
Ok thanks, that sounds like a good idea actually
The syntax of array indexing is pretty much universal. Array(i, j) is always interpreted as row i, column j. The difference between row-major and column-major is how an array is represented in 1-dimension. Under the hood, the array is stored as a sequence of values indexed linearly. Row-major stores the values in the first row consecutively (same row index, column index changes), followed by the second row, etc. Column-major stores the values in the first column consecutively (row index changes, same column index), followed by the second column, etc. Same pattern for higher-order arrays. Row-major indexes by making values in the last dimension contiguous, whereas column-major indexes by making the first dimension contiguous. Row-major (last-dimension contiguous) 1: (1, 1, 1) 2: (1, 1, 2) 3: (1, 1, 3) Column-major (first-dimension contiguous) 1: (1, 1, 1) 2: (2, 1, 1) 3: (3, 1, 1)
you can use Channels for a kind of lazy evaluation function inf(ch) i = 1 while true put!(c, i) i = i + 1 end end for ever in Channel(inf) println(ever) end Russ Cox (now of Go fame) writes about CSP style from his days (and mine) in Plan9 https://swtch.com/~rsc/thread/ 
You can use `eachindex` to check the "natural" order of an array: A = [(i,j,k) for i=1:3,j=1:3,k=1:3] for i in eachindex(A) @show A[i] end A[:] also works.
See some of the packages here: https://github.com/JuliaLang/julia/issues/5571#issuecomment-205754539
So I did some time-testing and found that it is fastest to index in the way I thought it would with column-major, but that it doesn't really matter that much so long as the most rapidly changing index is the first dimension.
For this one I actually did the parsing in awk, as it seemed more suited, but I'm sure I'll be proven wrong. This is the data: #1 @ 249,597: 20x15 #2 @ 192,174: 10x21 #3 @ 734,527: 23x10 #4 @ 165,232: 27x27 #5 @ 834,22: 17x12 ... the awk script: #!/usr/bin/awk -f BEGIN{ printf "number,x1,y1,x2,y2\n" } { split($0, a, " ") printf substr(a[1], 2, length(a[1])-1) "," split(a[3], b, ",") d = substr(b[2], 1, length(b[2]) - 1) printf b[1]+1 "," d+1 "," split(a[4], c, "x") printf b[1]+c[1] "," d+c[2] printf "\n" } which gave me this output: number,x1,y1,x2,y2 1,250,598,269,612 2,193,175,202,195 3,735,528,757,537 4,166,233,192,259 5,835,23,851,34 ... julia script: using CSV using DataFrames d = CSV.read("./input_post_awk", delim=",") cloth = zeros(maximum(d[:,:x2]),maximum(d[:,:y2])) for i in 1:size(d)[1] cloth[d[i,:x1]:d[i,:x2],d[i,:y1]:d[i,:y2]] += 1 end sum(cloth .&gt; 1) for i in 1:size(d)[1] e = cloth[d[i,:x1]:d[i,:x2],d[i,:y1]:d[i,:y2]] .&gt; 1 if sum(e) == 0 print(d[i,1]) end end I find the indexing of the 'cloth' a bit ugly, but it seems to work. I'm a big fan of dataframes, but it takes a while to load, and given the values are all integers, probably should have just been imported as an array. Does anyone know the best way to load a csv into an array? Any suggestions are welcome. TIA.
Wait, is that a phone? Can you provide more info?
Not OP, but I code on my phone by ssh'ing (or rather mosh'ing) to my laptop. I used JuiceSSH while I was on Android, now I use Blink on iOS. 
Not the most elegant but gets the job done. ``` function parse_input(line::String) offset = match(r"\d+,\d+", line).match left, top = parse.(Int, split(string(offset), ',')) dimensions = match(r"\d+x\d+", line).match width, height = parse.(Int, split(string(dimensions), 'x')) squares = [(left + i, top + j) for i=1:width, j=1:height] return reshape(squares, (length(squares))) end function puzzle3(datadir::String=joinpath(@__DIR__, "data")) instructions = readlines(normpath(joinpath(datadir, "3.txt"))) claims = parse_input.(instructions) squareCounts = countmap(reduce(vcat, claims)) partOne = count(values(squareCounts) .&gt; 1) partTwo = 0 for i in 1:length(claims) claim = claims[i] if all([squareCounts[k] for k in claim] .== 1) partTwo = i break end end return Dict("Part One" =&gt; partOne, "Part Two" =&gt; partTwo) end export puzzle3 ```
&gt; Does anyone know the best way to load a csv into an array? You can use `DelimitedFiles` and `readdlm`. For example `readdlm("input_post_awk", ',', Int, skipstart=1)`.
It's pretty slow, but it works and isn't too complicated: struct Claim id::Int leftoffset::Int topoffset::Int width::Int height::Int end const edge = 1000 function checkclaim(claim, fabric) for i in claim.leftoffset+1:claim.leftoffset+claim.width for j in claim.topoffset+1:claim.topoffset+claim.height if fabric[i, j] &gt; 1 return false end end end return true end function findnooverlap(claims, fabric) for claim in claims thisone = checkclaim(claim, fabric) if thisone return claim.id end end # Return -1 if failure return -1::Int end function parseclaim!(line, fabric) id = parse(Int, split(line, "@")[1][2:end]) lo, to = parse.(Int, split(split(split(line, "@")[2], ":")[1], ",")) w, h = parse.(Int, split(split(line, ":")[2], "x")) for i in lo+1:lo+w for j in to+1:to+h fabric[i, j] += 1 end end return Claim(id, lo, to, w, h) end function main() # Process the claims fabric = zeros(Int, edge, edge) claims = open("input.txt", "r") do file claims = Array{Claim, 1}() for line in eachline(file) push!(claims, parseclaim!(line, fabric)) end claims end # Puzzle 1 println(sum(fabric .&gt; 1)) # Puzzle 2 println(findnooverlap(claims, fabric)) end main() &amp;#x200B;
Yup indeed! It's running Julia locally in a neovim buffer, with julia-vim and vimcmdline as my 'IDE' on a 1st gen Pixel, on Termux. I posted about getting it running [previously](https://www.reddit.com/r/Julia/comments/8rci8l/numerical_computing_in_my_pocket/) - in the comments are a link to the repository with the Julia binary. Base-julia seems to work fine, but I had difficulties installing the DataFrames package. I did a more in-depth post about getting R running which was [a bit trickier](https://www.reddit.com/r/rstats/comments/8rnxz2/instructions_for_r_onthego/).
Try something along the lines of: https://en.m.wikipedia.org/wiki/Levenshtein_automaton
thanks for posting! I like the regex - the parsing is really neat. you've also pointed out countmap to me, which looks pretty useful.
Yeah sorry `countmap` is part of `StatsBase` (in case you didn't know). It's super useful for all of the AoC problems because almost all of them involve counting. 
Take a look at pyjulia: [https://github.com/JuliaPy/pyjulia](https://github.com/JuliaPy/pyjulia) &amp;#x200B; &amp;#x200B;
Just to add a meaningless comment, Julia has addressed all the things I wish other languages could have, and then blown me out of the water with even more. Kudos.
Just tried it and I couldn't get it to work. It seems that it doesn't play well with Python on Ubuntu and virtualenvs.
Gender this, don't gender that, you must say this, you can't say that. &amp;#x200B; Fascism. Fascism everywhere. Sponsored by feminism, SJW's and affiliated imbecilles.
Why not store the cloth claims as a sparse array or dictionary? Really there is no reason to stores mostly zeros.
A language with this "feature" is fundamentally flawed. my 2 ¬¢
 for i in 1:size(d)[1] e = cloth[d[i,:x1]:d[i,:x2],d[i,:y1]:d[i,:y2]] .&gt; 1 if sum(e) == 0 print(d[i,1]) end end Could you tell me what this bit of the code is doing? I get that \`e\` is a boolean row-vector. What is the print command doing?
Here's my code. I've been working on this for over a day, and it's still incorrect. I'd appreciate all help. &amp;#x200B; # Read and parse data into an array file_3 = open("data/input3.txt") lines_3 = readlines(file_3) len = 1337 wid = 4 parse_array = Array{Int64,2}(undef,len,wid) # "#1 @ 935,649: 22x22" reg_pattern = r"^.*@\s(\d+),(\d+):\s(\d+)x(\d+)$" for i in 1:len for j in 1:4 parse_array[i,j] = parse(Int64,match(reg_pattern, lines_3[i])[j]) end end # Convert the offets into a coordinate system coord_array = hcat(parse_array[:,1] , parse_array[:,1] + parse_array[:,3] , 1000 .- parse_array[:,2], 1000 .- parse_array[:,2]- parse_array[:,4]) + ones(Int64,size(coord_array)) # Adding an offset to handle corner cases.I get some entries with a x1 coordinate of zero. So I offset them by 1 to make later indexing easier. So far, so good. Here's where things go wrong. function fun(ref_array) m = maximum(ref_array[:]) punch_array = zeros(Int64, m,m) len = size(ref_array)[1] for i in 1:len A = ref_array[i,:] punch_array[A[4]:A[3],A[1]:A[2]] = punch_array[A[4]:A[3],A[1]:A[2]] .+ 1 # Subset the zero matrix using the [y1:y2, x1:x2] matrix, and add 1. end return sum(punch_array .&gt; 1) end fun(coord_array) I don't know why this doesn't output the right answer. What am I missing?
It's faster in Python still? Did you type your set? 
It looks like the Youtube integration is messed up and played the playlist instead of the video. The video is found by clicking on the link, or by using https://www.youtube.com/watch?v=_TK_s3uThWA
Chris is awesome as always.
You can delete your own posts
d is a dataframe with each row as each claim of the cloth. cloth is a 2d array which counts the number of claims for each square of cloth. I check the area of the claim on the 2D array that is cloth, to see if any values are more than 1 (i.e. claimed more than once). If that is true, then e will contain at least one true. I then sum e (which converts booleans into 1 and 0), to see if we have any trues in that claim. If there are no trues then that claim is unique, and I print the claim number from that line of dataframe d.
Thanks for sharing this! I love that regex. A couple of things: have you got `len` correct? rather than give an explicit value, better to take `length` of lines_3. I'm not 100% sure about your conversion from the parse_array to the coord_array. I see you've flipped the one axis by subtracting from 1000, and you've added one. That shouldn't be a problem - but I wonder if your claims are 1 too big on each dimension? I.e. a 5x4 claim might start at the 10th column and go to the 15th column - that's a difference of 4 rather than 5. Hope that helps?
yes, a sparse array would have been better. I can see a dictionary approach works just as well - another solution also posted in this thread. I guess with a 2d array you have the convenience of being able to index the rectangle directly without having to use two loops. Apart from that, I don't see much difference. I suspect that a dictionary approach is faster, given all the advantages of the hashtable.
My friend said it‚Äôs probably faster with an array, but uses much more memory. Both are cool though!
He's the one that posted it!
So?
Oh I thought it was weird that you didn't just say " You are awesome as always". Carry on then. 
The whole thing with `flag` and `break` makes the function a bit less readable than necessary. You don't need to do work so hard to get to the bottom of the function. Just use `return`: function part1b3(d) m = Set(0) x = 0 while true for i in d if (x += i) in m return x else push!(m, x) end end end end &amp;#x200B;
Wow, this is beautiful. \`countmap\` literally kills this problem. Thanks for sharing. &amp;#x200B;
maybe :)
Thanks that is a lot clearer - far more sensible!
Use an `Int` instead of `Integer` since `Integer` is an abstract type that can be any type of integer `Int8(0), Int16(0), UInt32(0)` etc. using `Int` will give you better performance.
They explicitly mention it doesn‚Äôt work well with Ubuntu + Julia &gt; 0.7 - did you check the troubleshooting guide for workarounds?
I tried python-jl but I couldn't manage to change Pycall's python path to my virtualenv instead of the system's python.
What code are you running in the REPL and what are you running in the file? 
Putting it inside a file but not inside a function or a module (not top-level) will steal define a global variable. This is not the point being made there. The point is put your code inside functions and if you want to access a global variable, take it as a function parameter (or type annotate it when you use one, not recommended though). [The section below](https://docs.julialang.org/en/v1/manual/performance-tips/#Measure-performance-with-[@time](@ref)-and-pay-attention-to-memory-allocation-1) gives two functions the first of which accesses a global variable and therefore is slow, the second takes a function parameter instead and the caller sends the global variable as a function argument. The function in the second example can be optimized and therefore is fast. In both, the random array x is defined as a global variable. So the issue is relying on a global variable in your function body and not creating a global variable in REPL. The point is put your code in functions and the functions (body) should not rely on a global variable if you want to get higher performance. 
I'm running this code: [https://pastebin.com/8zFi41Zr](https://pastebin.com/8zFi41Zr). Running it line by line in the REPL gives the same results as in the documentation, while running the file yields 5K and 3K allocations for the functions (instead of like 5 for the second one).
I think I get the idea but when I run this file: [https://pastebin.com/8zFi41Zr](https://pastebin.com/8zFi41Zr) it doesn't seem to work. The second function sum\_arg does not depend on a global variable as x is taken as argument but the number of allocations is still 3K for sum\_arg. In the REPL, running this code line by line yields 5 allocations for sum\_arg.
Note that the performance tip only applies to *non-constant* globals, so you could mark it `const`. But do also note that it's a little dangerous to do so for benchmarking purposes, as it could cause the compiler to optimize away code that *would* be needed if `x` were not constant (constant-folding), so the benchmark scenario would not be realistic. For accurate benchmarking, using the `BenchmarkTools` package is heavily recommended: using BenchmarkTools @btime sum_arg(x) setup = (x = rand(1000)) which results in BenchmarkTools.Trial: memory estimate: 0 bytes allocs estimate: 0 -------------- minimum time: 965.421 ns (0.00% GC) median time: 1.322 Œºs (0.00% GC) mean time: 1.352 Œºs (0.00% GC) maximum time: 4.244 Œºs (0.00% GC) -------------- samples: 10000 evals/sample: 19 Be sure to read the manual for this package: [https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md](https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md).
This is from the section I referenced in my message: &gt;On the first call (`@time sum_global()`) the function gets compiled. (If you've not yet used `@time` in this session, it will also compile functions needed for timing.) You should not take the results of this run seriously. So you are looking at the measurement that includes compilation. You need to run your function again with `@time` to see how your *compiled* function performs. @time loop_over_global() @time loop_over_global() # compare this @time sum_arg(x) @time sum_arg(x) # with this As long as you pass a variable of the same **type** as `x` to your `sum_arg(x)`, julia will use your *compiled* function `sum_arg(x)` in the subsequent calls. I would also suggest you to take a look at `BenchmarkTools` and `@btime`.
Lazy.jl is your answer for pipin. It provides waaaay more piping abilities than R does. 
Thanks. I should really look into lazy.jl. I just assumed that the work on that package has been abandonned in favor of native solutions which are somewhat lacking. But it seems it's still the best option right now.
I haven't touch Julia in a few months, but my evaluation then was that it wasn't ready for 'general purpose' yet just due to it's slow startup time. Its speed over Perl or python is lost for small scripts due to its startup. I also found it to be lacking in speed in the strings department. So much so that again Perl or python could do it just as fast. I don't remember the source but there was a comment from Karpinski along the lines of "we know they are slow, and we'll fix it some day". So, long story short, it's not good enough to replace a general purpose scripting language for writing scripts yet ... For me as of a few months back. 
I'm very excited for when those few sticking points are fixed though and the ecosystem around the 1.0 release has matured :)
I worked on [Discord.jl](https://github.com/PurgePJ/Discord.jl) for making chat bots, does that count?
what's data science? :)
I thought Julia was more popular for Physics-related tasks especially those that involves partial differential equations. In the data science community Julia is not used at all.
The intention with Julia was not to build a general purpose language. It is and will be a language for technical computing.
Oh man, ok I see it now. Sorry and thanks. I'll definitely read some more and check out the BenchmarkTools!
Thank you, I'm going to read this!
No. Julia is intended to be a general purpose language. Technical computing was the original focus, but general purpose is the goal. They even removed the tagline "A fresh approach to technical computing" from the REPL. 
I'm not sure, but I'll know it when I see it! 
It has some pretty popular deep learning libraries 
For my next project at work I‚Äôll be replacing Python with Julia because KNet and (especially) Flux look way better than what most of Python has to offer. Julia is slowly gaining attention in data science. The physics and differential equation stuff is rightfully popular but there‚Äôs no reason to stop there.
DifferentialEquations.jl is what caused me to switch from Python last week (at least for that application).
I'm a heavy participant in the Kaggle community and Julia is not used at all. Most popular frameworks are Keras, PyTorch then Tensorflow then H2O.ai with some mxnet mixed in-between.
I think you need to run a `@everywhere` block defining your functions again. You can also use this to evaluate some code on a specific worker: remotecall(Core.eval, 2, :( f() = 1 ))
A Lapalissade.
Can't you use `@everywhere` again? Putting your functions in a file and then issuing `@everywhere include("myparallelfunctions.jl")` would be the way to go. If you are using a notebook Also see this SO answer [https://github.com/ChrisRackauckas/ParallelDataTransfer.jl](https://github.com/ChrisRackauckas/ParallelDataTransfer.jl)
Could be due to its lack of support in kaggle notebooks. 
I tried the package as well, but it's not complete and reliable. For example the place function is with acker implemented and works not so well. I hope that we as community will work on that :)
I had the same problem with it! 
Kaggle was sort of dominated by scikit-learn until recently. As Julia usage picks up generally we'll likely start seeing that taking place in Kaggle as well. One thing I do wish Julia had was more probabilistic programming variety (right now the only viable one I know about is Turing, which still breaks a lot and only supports MCMC). 
Have you read about performance optimization in the Julia docs? I see threads like this now and then and they usually end with a performance tip from the docs that will speed Julia up!
I'll try giving it a shot! Thank you 
[Here's the link.](https://docs.julialang.org/en/v1/manual/performance-tips/). Actually, Julia has especially good docs in general üëå.
If you want to use vectorized syntax then the dot broadcasting is the way to go. I‚Äôm not seeing the slowdown you are experiencing on my machine. Julia is slightly ahead of numpy for me. As a general rule: When benchmarking Julia you want to avoid using global variables, which x is if defined on the prompt. The benchmark macro can do this by prepending the variable by a ‚Äú$‚Äù. You also seem to be using an different version of Julia than 1.0 since your range function throws an error for me if called like that 
I am using Julia version 1.0.2
Have you tried `collect`ing the `range`? So you get an actual array as in linspace instead of a linear range object. Otherwise it will iterate through range object to find the next value. Also make sure that both python input and julia input have the same values. Both numpy and julia uses `libm`. There should not be a significant difference. Numpy also calls libm's sin for each element in the array one by one under the hood. Vectorization in python is not black magic. For many functions, there is an underlying C library function that has loops in it. If you want higher speed, you may want to try mkl library which has trigonometric functions that take a vector. If you have an Intel cpu, perhaps it is better optimized than libm. Never tried though. 
People suffering social consequences for being assholes? It must be fascism!
Hm, im on 1.0.2 as well and range(0,100,length=1000) throws a method error for me
A talk at JuliaCon 2018 "[Low Level Systems Programming in High Level Julia](https://www.youtube.com/watch?reload=9&amp;v=AaQ7XuAR2yY)" shows the versatility of Julia. See also a Hacker News [discussion](https://news.ycombinator.com/item?id=17725555) on general-purpose use of Julia. 
&gt; Have you tried collecting the range? So you get an actual array as in linspace instead of a linear range object. Was about to recommend that.
I'm not sure that I uderstand your question correctly, but ControlSystems uses the Plots package, so if you want to modify the limits of your plot try the xlims=(0,6) argument. I didn't try it, just an idea.
Haven‚Äôt worked with it myself but [here is an older example of Julia being embedded in an Xcode project](https://github.com/AlesTsurko/AUJulia). I assume you‚Äôd include the .dylibs from Julia the [same way you would with those from R](http://www.brianrhall.net/rss/linkingxcodecandrtocreateplots)
Funny that you post aujulia I came across it as well (I also have a pending pull request in that project). I talked to Ales (the creator) and he mentioned how much of a pain it was to setup and how fragile it is. I have reasons to believe that some things have changed that broke the same approach for newer versions of Julia. It does work for 0.4 though.
I think that's the answer. When I compare while `collect`ing before I get ~10 micro second for numpy and ~12 for Julia
&gt; Both numpy and julia uses libm Julia does not use the system libm, so it can be different. 
Same. I'm in the middle of porting a bunch of Python scripts to Julia. Once I get a real handle on the rel√®vent Julia packages, I'm going to be working primarily in Julia going forward. 
Yes, now it appears the syntax is range(0, stop=100, length=1000) The only required parameter is the initial value. The stop and length are optional, but one of them is required. 
Remember to interpolate global variables with \`$\` when benchmarking: @benchmark sin.($x) It doesn't make a big difference here, (reducing runtime from 11 to 10us on my computer), but sometimes it makes a world of difference, and also a more accurate number for the allocations.
Looks like you've got encoding errors in the embedded code. struct C &amp;lt;: MetaClass end 
I‚Äôm trying to fix the encoding in the code blocks. If anyone has any advise on using Julia with Crayon in Wordpress, lmk.
Nice article. I think the stack overflow comes from the fact that there are two parameters in the Node outer constructor, and it simply calls itself. If you want to have a default constructor, I think defining something like Node() = Node(0, nothing) would work (assuming you make next::Union{Node, Nothing})
Yes, their DataFrame.jl package was broken for 1.5 years when I started ML. I actually started with Julia first before using Python ;) https://github.com/mratsim/MachineLearning_Kaggle/tree/master/Kaggle%20-%20001%20-%20Titanic%20Survivors
Also I guess using sum with the indices isn't a comprehension. 
It's creating a copy of the array before summing it, as the section isn't contiguous. You can use views to get around the copying, this should perform similar to your version where you loop over the array
What do you mean the section isn't contiguous? As in it's not all in a row if it were flattened? Views also was a bit faster, but not quite as fast as the traversal. 188.477826 seconds (34.06 M allocations: 1.192 GiB, 0.13% gc time)
You're using non-constant global variables. See [https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-global-variables-1](https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-global-variables-1). You may also want to use BenchmarkTools, [https://github.com/JuliaCI/BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl).
It actually took longer with all local vars, half as much allocation though. 204.209767 seconds (9.05 M allocations: 552.067 MiB, 0.03% gc time) # Find Largest function findLargest(input::Array{Int64}) largestArea = 0 largestX = 0 largestY = 0 largestSquare = 0 for numSquare = 1:300 for i = 1:300-numSquare+1 for j = 1:300-numSquare+1 thisArea = sum(view(input,i:i+numSquare-1,j:j+numSquare-1)) if thisArea &gt; largestArea largestArea = thisArea largestX = i largestY = j largestSquare = numSquare end end end end println("$largestX,$largestY,$largestSquare") end @time findLargest(fuelCells) I'll check out BenchmarkTools
Yes exactly, it's not next to each other in memory. Of course you don't need that for summing, and your version where you loop over the array is clear and efficient
Makes sense. I still wonder why it doesn't match C's performance, though. Writing the same code, it took about 30 seconds versus 100 in Julia.
You're putting the entire code inside a function right? If you're using variables from the global scope you might want to declare them as constants
Check out the performance guide on https://docs.julialang.org, at the very least you need to chuck that all in a function and get rid of the global keyword. 
Yeah that‚Äôs what I did in the comment a little lower
My guess would be then (I'm not at a computer so I can't check) that the function isn't typesafe, try using `@code_warntype` failing that check the rest of the tips in the guide. 
Ohhh, someone joking, or even speaking in a way a few pre-butt-hurt snow-flakes consider offensive ? Quick, ostracize him, deplatform him, shut him up or kill him, he's a hater and he's worse than hitler. That's how "inteligent" you PC censors are.
As usual Poe's law(?) is in effect, I can't tell if you're serious or this is over the top satire/trolling. Buuuuut maybe consider that there have always been all kinds of things you could say that general society could ostrasize you for? Think about the whole football players kneeling for the national anthem controversy, you can bet your ass that would have been way more intense 50 years ago. Is everyone who gets worked up over that a snowflake? You're not mad about there being rules about what you can/can't say, you're just mad that the rules have changed to not favor straight white men over everyone else. 
&gt; You're not mad about there being rules about what you can/can't say, you're just mad that the rules have changed to not favor straight white men over everyone else. You dumb piece of ____. Freedom of speech does not favor anyone, you cretin! I'm advocating for freedom and equality for all, you are just too dumb to nitice. HOW DUMB YOU PEOPLE ARE ????
So over the top troll, then? 
So, still no defendable argument ?
Hey this is Advent Of Code :D I also did it in Julia, took a whole 30 seconds to compute ;-; I have found a more efficient algorithm and I might want to try implement it. https://www.geeksforgeeks.org/print-maximum-sum-square-sub-matrix-of-given-size/
Yeah this is the na√Øve approach. The much better solution uses partial sums. I just was wondering why this particular bit of code was so slow. 
Adding zlim=(0,5) at least lets you see the color differences and the shape a little better. I'd suggest trying the Makie package if you haven't already. It's much better at handling 3D plots and you can zoom and rotate them. Granted I struggle to get Makie to work half the time, especially on my work computer so I can't really help much more than pointing you towards it at the moment.
Thanks! This package looks great and has a much more useful documentation.
Afaik Makie supports Julia 1.0 fully. But in general, it is still a work in progress. Some fancier things may not be easy to do or change in the future. 
On the github page for Makie.jl they mention adding Makie#master which works for me on 1.0.2. [https://imgur.com/a/sUa53nS](https://imgur.com/a/sUa53nS) f(x,y) = abs(gamma(x+ im * y)) x = y = range(-5, 5, length=1000) function lim(x) if x &gt;= 6 return 6 else return x end end z = map(lim,[f(a,b) for a=x,b=y]) surface(x,y,z, colorrange=(0,5),colormap=:Purples, shading=true) &amp;#x200B; &amp;#x200B;
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/HCNhfxs.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ebniyvm) 
Thanks for the info, may I ask how you managed to get Makie working? I tried pkg&gt; add Makie#master But when I run your code it doesn't recognise colormap and complains about overriding surface from the main library or something. &amp;#x200B; The plot looks good though! I think with a little experimentation with Makie it will be perfect, if I can reach that point.
It sounds like there's a conflict between libraries you have activated. If you've also loaded Plots or another library that has a function named `surface` in the same session the two functions will conflict. Try restarting Julia. If that still doesn't work it's possible it is a platform issue, I ran my code on OSX. 
Yep that was it! Thanks for all the help.
You can also just call Mackie explicitly like in Python. Mackie.surface
Best be explicit: function f(arr::Vector{T}) where T x = Vector{T}(...) ....
Cool, thank you. So that way T stands for the type of the argument vector: very elegant.
You're very welcome! I typed this on my phone so I can't be 100% sure the syntax is correct. Julia is great since the types can be used as first class citizens in the function body and the compiler will usually figure it out and generate optimized code!
I believe T needs to be in {}, but double check me. It might not be a bad idea to limit things T can be to actually make the function work for everything you can call zeros() on - which are Numbers. This is cool because it can work for complex and real. &amp;#x200B; Just as before, function f(arr::Vector{T}) where {T&lt;:Number} x = zeros(T,3) .... That way, T must be a number.
You don't need the where clause to be in braces unless there's more than one type variable.
There's a few different questions here. Some have already been answered, but I thought I'd just collect all the answers in one post. &amp;#x200B; For benchmarking, I'll use the following function: \`benchmark(x, y) = x' \* y + sum(x) + sum(y)\`. Note the adjoint after the first x. &amp;#x200B; \## Question: How element type affect vector speed Normally, the benchmarking function is fast: \`\`\` using BenchmarkTools x, y = rand(100000), rand(100000) # These are Float64 benchmark(x, y) # run once first for compilation @btime benchmark(x, y) \`\`\` &amp;#x200B; This takes 66.7 ¬µs, allocating basically nothing. &amp;#x200B; We can then use Vector{Any} instead: &amp;#x200B; \`\`\` a, b = Vector{Any}(x), Vector{Any}(y) @btime benchmark(a, b) \`\`\` &amp;#x200B; This takes 9714 ¬µs, allocating 9.16 MiB. So to answer one of your quesitons, yes, using Vector{Any} will make your code slow and memory hungry! &amp;#x200B; You can also restrict the vector type to something smaller than \`Any\`. For example, you can restrict it to either \`Float64\` or \`Int32\` by doing: &amp;#x200B; \`\`\` a, b = Vector{Union{Float64, Int32}}(x), Vector{Union{Float64,Int32}}(y) @btime benchmark(a, b) \`\`\` &amp;#x200B; This takes 831 ¬µs, allocating nothing. It seems \`Any\` is really bad, union types are somewhat bad, and specific types are the best. &amp;#x200B; &amp;#x200B; \## Question: How types in function affect speed. &amp;#x200B; However, don't be misled. These two functions have exactly the same speed! &amp;#x200B; \`\`\` benchmark1(x::Vector{Float64}, y::Vector{Float64}) = x' \* y + sum(x) + sum(y) benchmark2(x::Any, y::Any) = x' \* y + sum(x) + sum(y) \`\`\` &amp;#x200B; The reason is that although \`benchmark2\` \*can\* take any type, when you pass it e.g. a \`Vector{Float64}\`, it will still specialize, compiling to the same instructions as \`benchmark1\`. This means you can safely use \`benchmark2\` (or the original \`benchmark\`) with no performance issues. If you really want you function to ONLY take Float64 or Complex{Float64}, you can do: &amp;#x200B; \`benchmark3(x::Vector{T}, y::Vector{T}) where {T&lt;:Union{Float64,Complex{Float64}}} = x' \* y + sum(x) + sum(y)\`. &amp;#x200B; Again, this will perform just fine. &amp;#x200B; \## Initializing vectors &amp;#x200B; You can initialize zeros with any type you want: &amp;#x200B; \`zeros(Complex{Float64}, 5)\`. &amp;#x200B; You can even combine this with a paramtric function: &amp;#x200B; \`\`\` function test(x::Vector{T}) where {T} # This is the same as where {T&lt;:Any} y = zeros(T, length(x)) return benchmark(x, y) end \`\`\` &amp;#x200B; Again, with no performance penalty. The reason we have \`undef\`, is if you want to save time by not setting the array to zero or any other value: &amp;#x200B; \`x = Vector{Float64}(undef, 5) # create 5 Float64 values, any values\` &amp;#x200B; However, the time difference is usually not important. When I tested it, initializing 10000 Float64 took 7 ¬µs with \`zeros\` and 1.4 ¬µs using \`undef\`. 
Thank you, very informative. I learned to program in Pascal a long time ago (Turbo Pascal for CP/M, anyone?), and I am not a programmer. Whenever I use a language that allows you to be not strict with types I worry I will do something really inefficient.
Instead of doing \`factorial(30)\`, you should do \`factorial(big(30))\`. &amp;#x200B; The issue comes from the fact that the size of 30! is way too big to be held in a single 64 bit integer value, so it has to use the big integer type. However, the version of factorial that returns \`BigInt\` type also takes \`BigInt\` as the argument.
It's actually best to NOT limit your function to types. What if someone implements zero() for some non-number type for which an"additive identity" makes sense? Then your function could potentially work, but it won't because it's been limited. Julia isn't Haskell, and you shouldn't treat it like Haskell.
It's really easy to just write the loop nPr(n, r) = begin p = one(n) for k=r+1:n p *= k end p end I think there's a few corner cases to also handle like if `r &gt; n`, or negative numbers, or non integers, but you get the idea.
Interesting. Isn‚Äôt it an intent thing though? Like even though one could add an implementation of zero(), if the intent was to only create an array of numbers, the type of T should be limited to numbers. I have been having several heated discussions about Julia vs Haskell the past few days with my roommate, so this is certainly interesting.
Or just prod(r+1:n)
If you're working with really large numbers and don't need 100% accuracy, there's always [Stirling's Approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation): nPr(n, r) = nPr(promote(n, r)...) nPr(n::T, r::T) where {T} = reduce(*, (n-r+1):n, init=one(T)) nPr2(n, r) = nPr2(promote(n, r)...) nPr2(n::T, r::T) where {T} = exp(n*log(n) - (n - r)*log(n - r) - r + 0.5 * log(n / (n - r))) and testing it out on a variety of numbers: julia&gt; nPr(6, 2) 30 julia&gt; nPr2(6, 2) 30.208155117866866 julia&gt; nPr(30, 20) 7324805792373932032 julia&gt; nPr2(30, 20) 7.350360351273633e25 julia&gt; nPr(60, 50) -7504967304036220928 # overflowed julia&gt; nPr(60, big"50") 2293040981244871622651108130297443887994425872564276331693473792000000000000 julia&gt; nPr2(60, 50) 2.309013930409499e75 julia&gt; nPr2(60, big"50") 2.309013930409475559528844892952756860185591671173736176415153817056155920788719e+75
Sure. It's hard to be sure, though. Is an element of a galois field a number or not? This isn't even a theoretical thing. I was playing around with optimizing algorithms for Reed Solomon Erasure coding, which uses generalized gaussian elimination and using Julia's matrix solver was way better than writing my own matrix solver. The default linalg library lu also does not make any assumptions about the number type you give, so I only had to implement zero, times and plus on my galois field type.
Sure, but then when do you think one should limit type on their function? If all their functions were of type T, wouldn‚Äôt the performance suffer? Where is the line drawn?
Bt
No performance regression: You take the hit at compile time. There's something like 500 implementations on * in the standard library, doesn't seem to be a big deal. You'll be in trouble if you're using it in global scope and need to look up the dispatch table every time, but if it's in a function you should be okay.
https://discourse.julialang.org/c/community/jobs
Very interesting. I guess I never thought of the dispatch being a scope issue, but that totally makes sense. I‚Äôm still new to Julia, but I have been really enjoying it so far. I‚Äôm an electrical engineer, so I‚Äôm not a career programmer, just an enthusiast. I have been writing PDE solvers for electromagnetics problems, and I have been very impressed with its performance. I‚Äôve done my fair share of programming in ‚Äútraditional‚Äù languages which has made me very interested in Julia from the design perspective.
thanks for this!
big() is useful, thx
I will say there isn't much there though. Julia is still a very niche language without a ton of support from the community. I think you'd be hard pressed to find a full-time position where you'll be working 100% in Julia.
https://julia-companies.org/
I found my current job by searching for Julia on Indeed.
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 452.9 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about job search advice. But, I'm only ~23% sure of this. Let me know if I'm wrong! Have you checked out Forbes, LiveCareer, TalentWorks? They've got some great resources: * https://www.forbes.com/sites/karstenstrauss/2017/03/07/job-hunting-tips-for-2017/#794febea5c12 * https://www.livecareer.com/quintessential/15-job-hunting-tips * https://talent.works/automate-your-job-search
Turing.jl is a great probabilistic programming library. We use it in DiffEqBayes.jl for nonlinear probabilistic programming where the nonlinearity is a differential equation describing some biological or physical system. 
Thanks Chris! Turing is indeed really cool, and I plan to play around with DiffEqBayes over the holidays. However, are you aware of (or have a good idea of) how you could use such tools in order to do probabilistic inference over a model that simulates programs (by generating Julia expressions) ?
Write the Turing.jl model as a function on expressions. If you somehow have a way to put a likelihood on the expressions in terms of a Distributions.jl distribution, you're golden. 
4 and counting!
All the Julia dev's I've seen have been hired by people meeting them up at local meetups and the like.
The only reason why dispatch is an issue in global scope is because the variables in global scope have to carry type information with them because the type could be unstable due to code that appears later. Since dispatch occurs on function call, at compile time the type information gets resolved, and unless your code introduce type instability (like `x = y &gt; 0.0 ? y : 0` when y is a float) you should be ok
How would you form a prior over expressions though? Would you just use a uniform distribution over expressions you would use in your program? Or is there some neat way of creating something akin to a Dirichlet process over programs? 
That's up to you as the modeler. If you have infinitely many possible expressions though, uniform might not work :).
I realized a more accurate answer makes use of the log-gamma function (i.e. where ln[Œì(n+1)] = ln[n!]). `lgamma` is provided by the [SpecialFunctions](https://github.com/JuliaMath/SpecialFunctions.jl) package, and using it gives answers which are much closer than Stirling's Approximation gives: ``` julia&gt; nPr3(n,k) = exp(lgamma(n + 1) - lgamma(n - k + 1)) nPr3 (generic function with 1 method) julia&gt; nPr3(6, 2) 30.000000000000004 julia&gt; nPr3(30, 20) 7.309657732919742e25 julia&gt; nPr3(60, 50) 2.2930409812449054e75 ```
wow, cool. very interesting
Are you also aware of the efforts at [https://github.com/tpapp/skeleton.jl](https://github.com/tpapp/skeleton.jl)?
A quick search on GitHub revealed [MuKanren.jl](https://github.com/latticetower/MuKanren.jl), a minimal implementation of miniKanren. On a first glance it seems to be compatible with the miniKanren reference implementation.
smth like this? [https://github.com/habemus-papadum/LilKanren.jl](https://github.com/habemus-papadum/LilKanren.jl)
Yes, exactly what I was looking for. Thank you!
Yes, although to be honest, this project seems a little harder to understand for a beginner. It should produce sufficient understanding and examples though. Thank you!
I found out about the JULIA_DEPOT_PATH environment variable. Changed my life.
That's good to know about! Doesn't that set the *global* package path, instead of per-project? For example, running: ``` (PkgName)&gt; add SomePkg ``` Will add a package to your *project*, if run after `activate`ing it. What else does `JULIA_DEPOT_PATH` change?
I was not, thanks for sharing!
It's like GO_PATH. All packages are installed in that location. I like it with the 'dev' command to clone something so that you can work on it. The fundamental problem I had was working on multiple interdependent packages. With JULIA_DEPOT_PATH i actually enjoy my workflow. 
Sorry, but I might need more information. Are you iterating over indices for some Claim?
I updated the post to hopefully be more informative. I also could post my Rust implementation of the same thing if needed
more info: https://discourse.julialang.org/t/julia-1-0-3-is-now-available/18810 Downloads: https://julialang.org/downloads/
Somehow Opensuse loaded that update yesterday evening
Looks like there is one at the end of the list: https://julialang.org/learning/ 
[removed]
I highly recommend the documentation as well as the Julia Box tutorials. https://docs.julialang.org/en/v1/ https://juliabox.com/
That might be helpful, yes.
[removed]
``` #[derive(Debug)] struct Claim { id: u32, x_inches: u32, y_inches: u32, x_size: u32, y_size: u32, } impl Claim { fn new(id: u32, x_inches: u32, y_inches: u32, x_size: u32, y_size: u32) -&gt; Claim { Claim { id: id, x_inches: x_inches, y_inches: y_inches, x_size: x_size, y_size: y_size, } } fn iter_points(&amp;self) -&gt; IterPoints { IterPoints { claim: self, pt_x: self.x_inches, pt_y: self.y_inches, } } } struct IterPoints&lt;'a&gt; { claim: &amp;'a Claim, pt_x: u32, pt_y: u32, } impl&lt;'a&gt; Iterator for IterPoints&lt;'a&gt; { type Item = (u32, u32); fn next(&amp;mut self) -&gt; Option&lt;(u32, u32)&gt; { if self.pt_y &gt;= self.claim.y_inches + self.claim.y_size { self.pt_y = self.claim.y_inches; self.pt_x += 1; } if self.pt_x &gt;= self.claim.x_inches + self.claim.x_size { return None; } let (px, py) = (self.pt_x, self.pt_y); self.pt_y += 1; Some((px, py)) } } ```
In Julia, the following code ``` for i in x # stuff end ``` is a shorthand for writing ``` it = iterate(x) while it !== nothing i, state = it # stuff it = iterate(x, state) end ``` Hence, you need to define two `iterate` methods for your object: `iterate(x::MyType)` to get the first iteration, and `iterate(x::MyType, state)` for the rest. These methods should return `nothing` when the iterator is done, else a `(result, state)` tuple.
ThinkJulia is on 1.0 and is available online: https://benlauwens.github.io/ThinkJulia.jl/latest/book.html O'Reilly is working on a print version: http://shop.oreilly.com/product/0636920215707.do
[removed]
Thanks!
Thanks! Will they have a ebook version or it's only web?
Thanks!
DataFrames 0.15 introduced `mapcols`. You should be able to do this: `dt = diff(df.time); mapcols(c -&gt; diff(c) ./ dt, df)`.
The second snippet has the loop in row major order while Julia is column major. 
Can you indent your python code please?
I am sorry but I dont know what it means to indent the code. I wrote it in the post and I can send you the .git link to add in Julia, in order to have the .py code: [https://github.com/prasang7/Transportation-Problem.git](https://github.com/prasang7/Transportation-Problem.git) The code is in [vogels\_apprx.py](https://github.com/prasang7/Transportation-Problem/blob/master/vogels_apprx.py) I really appreciate your help
/r/domyhomework If you don't know what indenting is you need to learn that for python. I would chuck this code out and try and solve the problem your own way using Julia. 
Here's a sneak peek of /r/DoMyHomework using the [top posts](https://np.reddit.com/r/DoMyHomework/top/?sort=top&amp;t=year) of the year! \#1: [Online precalc exam $70](https://np.reddit.com/r/DoMyHomework/comments/8sahbt/online_precalc_exam_70/) \#2: [$250 general online anatomy and organic chemistry classes, each 250](https://np.reddit.com/r/DoMyHomework/comments/8sah7q/250_general_online_anatomy_and_organic_chemistry/) \#3: [Summer General physics with labs 1 online !! $350](https://np.reddit.com/r/DoMyHomework/comments/8s9h8s/summer_general_physics_with_labs_1_online_350/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
That's too much to ask someone to do on reddit
Yeah, I know. But, I have tried to solve the problem in Julia (Voggel approximation method) and I haven¬¥t been able to do it, so I tried to look for an already programmed code online and I found that one. Sadly, I don¬¥t know Python and that¬¥s why I¬¥ve been looking for help.
You should post this on Stack Overflow. They love helping with stuff like this.
You are trying to solve an optimisation problem. Julia has a package called Jump that may have the algorithm you are looking for. 
Google found something on Rosetta Code which might be the same thing: [Vogel's Approximation](https://rosettacode.org/wiki/Vogel%27s_approximation_method#Julia)
I know this is the Julia subreddit...but for what you've described, I think R would suit you very well. I haven't applied it in a "job" setting, but I've done a number of data projects around my campus ranging in complexity and scope and R has been a perfect tool for each one. Ggplot2 is a fantastic visualization package that I miss when working in other languages, the tidyverse is absolutely incredible for munging/cleaning data, and if you need more advanced statistical techniques then you can pretty much count on them already having been implemented in an R package by a statistician. Also, since you have to make reports, R has RMarkdown which is an incredible resource for making nice, clean reports in html, pdf, or Word format. The only downside to R you may find is that it is a bit quirky imo. It was the first language I learned, so I never really thought about it until I learned Python... but it's definitely a bit different than other languages. But depending on your preferences it might not necessarily be a bad thing.
&gt; I found that Python is really bad for calculating the weighted median. R is great for this. Can you define 'bad' in this context? If you mean slow, why not write it in c / c++ and call it from python? I wouldn't swap out a language for a singular function.
Thanks Don. Yes I agree with you. From what I've seen so far R ggplot and tidyverse are great. RStudio is also fantastic (better than Emacs or Jupyter in the browser). I also find code R quirky. And Julia code much easier to write and read. 
Thanks! Well I had to create a pivot table from a large dataset, using the weighted median (not mean) as agg func and I tried with Python and it was not an easy task, at least for a beginner like me. I tried to use WeightedStats [https://pypi.org/project/weightedstats/](https://pypi.org/project/weightedstats/), but after 100s of rows, it failed (it throws an error I can't debug, and the error arises at a random number of rows. It works fine for like the first 50 summary rows) . &amp;#x200B; In R I created the table using a single line, like this: &amp;#x200B; summarytable = data %&gt;% group\_by(GroupingVariable) %&gt;% summarise(median(Variable1,WeightVariable), median(Variable2,WeightVariable), median(Variable3,WeightVariable)) &amp;#x200B; And R provides also other weighted median functions, like matrixStats::weightedMedian &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
I am not familiar with that package, however it may be an issue with that particular package (and if that's the case issue a bug report ticket!). Native python is notoriously slow compared to other 'lower-level' languages, and for numerical computations the default choice would be numpy (which yields massive speed ups due to the backend). Judging from the link you provided, the module also provided wrappers for numpy. Regardless, the advantage of python being that it is really beginners friendly, and with the combination of cython allows you to go deep into optimization without major changes. I hear that julia offers similar shifts in performance, so that maybe an interesting alternative. Especially as it matures. My recommendation for a beginner is to stick to one language and focus on developing your problem solving skills rather than hopping from one language to the other. In the end it is only a matter of syntax anyway. Each language has its benefits in terms of performance / ease of use, but it all starts out with how to solve a particular problem. 
Thanks! :-)
That's a sensible recommendation. R is mature, and has great support for data analytics and visualization. We should always support Julia for what it's good at, and, at least for now, number-crunching and numerical analytics is where it's at. It's developing to be much more soon, but there's no reason to simply dispense other working, proven, tools.
Wow, that is amazing, the only problem is that when I run the code, it says UndefVarError bitpack not defined. Is the some package I must add in order to use bitpack?
If using large data sets, why not use data.table instead of dplyr?
same here. I am learning Julia, its a more advanced language, its fast, but R with all its quirks is mature with a mountain of libraries. Until I am much better in Julia, R would be my choice over Julia except in case speed is really not sufficient
there is Atom/Juno for Julia. But in my learning phase, I am using Jupyter 
Just learning the basics at this moment! &amp;#x200B;
Yes I used both, Atom/Juno and Jupyter. Atom/Juno is really slow, don't know why. 
I like learn julia the hard way: https://scls.gitbooks.io/ljthw/content/
Atom-Juno is slow. In my computer VS Code, R Studio feel much faster. How do you make Julia work in Jupyter Notebook. I have Anaconda installation. how do I get Julia in Jupyter?
I think this has something to do with Atom and not Julia. Too bad Julia extension for VS Code doesn't support latest Julia version. I think It would have been better if Julia team officially support Julia extension for vs code instead of Juno.
If you can write your own equations there is no issues with python at all. But if you are after speed and parallelization go with Julia. I would go with R if you do not want to write stuff and get stuff out of the box but it is a bit slower than python. The other option send data through something like apache thrift from one language to another the disadvantage to this is the overhead for apache thrift.
I remember installing IJulia within Julia, but thats about it that I remember. In short, I followed what was on the internet and it worked
that's another disadvantage of the new thing, The infrastructure around. For R there are plenty solutions. For Julia few. 
Google suggests this https://github.com/JuliaLang/julia/search?utf8=‚úì&amp;q=bitpack I suggest running code under v0.7 before v1.0, to catch these deprecations (of which there were many)...
Just because they are delusional, does not mean the rest of us are. Julia is primarily for scientific computing.
IJulia Docs: [https://github.com/JuliaLang/IJulia.jl](https://github.com/JuliaLang/IJulia.jl) start Julia prompt **option A)** Pkg.add("IJulia") **option B)** type \] to enter Pkg manager mode then type 'add IJulia' , use 'CTRL+C' to exit pkg mode
1. Why? 2. If you want to interact with Python libraries, look at https://github.com/JuliaPy/pyjulia ... You can call Python functions almost as if you were in Python. I've had very good success with it. 3. If you want to call JavaScript, see #1. Your best bet might be waiting for WASM to support garbage collection and going that route, as Julia uses LLVM and that supports WASM as a target. Look hard again at #1 and happy transpiling! :-) 
Julia to WebAssembly (Wasm) : * https://nextjournal.com/sdanisch/wasm-julia And with Julia 1.1 there will be much better support * https://github.com/JuliaLang/julia/pull/29341
Hey awesome!
Hey awesome!
I want to use Julia for my next AI related projects, because it looks good for them! If not, I'll be using Golang. I'm looking at Julia though because of its performance, and the idea that a higher-level language should be much much much less code for the type of code I want to write :).
You can use [BigInt](https://docs.julialang.org/en/v1/manual/integers-and-floating-point-numbers/index.html#Arbitrary-Precision-Arithmetic-1) to get Python-like behavior, but I don't know if there's a way to set it as the standard integer type in the REPL.
Hmm thank you! I really appreciate the workaround you've offered. I hope there is a way to change the default behaviour though, now, or in the future :).
Here's how you can use [ReplMaker.jl]() define your own REPL mode that uses BigInts and BigFloats by default: using ReplMaker using MacroTools: postwalk Big(x) = x Big(x::AbstractFloat) = BigFloat(x) Big(x::Integer) = BigInt(x) function Big_parse(s) postwalk(Big, Meta.parse(s)) end initrepl(Big_parse, prompt_text="BigJulia&gt; ", prompt_color = :red, start_key='&gt;', mode_name="Big-Parser-Mode") If you evaluate the above code, it will make a new REPL mode that you can switch to using the `&gt;` key (and backspace to return to the regular REPL mode) which should act the exact same except Ints and Float64s get promoted to their Big versions. BigJulia&gt; 2^64 + 1 == 18446744073709551617 true You can also use the above definitions to make a macro that turns enclosed code into its big version like so: macro Big(expr) postwalk(Big, expr) end and it'd be used like julia&gt; @Big 2^64 + 1 == 18446744073709551617 true
Wow this is exactly what I need! Thanks you!!
"for a REPL" is not a thing. there could a REPL for a purely logical language which does not even understand numbers. there could be a REPL for machine code.
Out of interest, what do you do where you're frequently using Integers that are larger than 2^64?
Normally, the performance critical stuff is done in C, C++ or Fortran when you use Python. I say normally because I did write my own [tensor + machine learning + deep learning](https://github.com/mratsim/Arraymancer) library from scratch due to Python lacking perf and to ease transition to production (not in Julia though). Depending on what kind of AI you want to do you might not escape Python, especially reinforcement learning.
Hmm thanks, I'll build benchmarks for my purposes and measure trade-offs!
I can't think of examples for every day (it's 6am, very late for me) but today I was simply trying to show someone why their massive number would overflow, and explain bits. However the number was too big (which I found ironic). Other times I will usually be checking stuff like min/max values in various use-cases. While I'm not always outside of the signed 64 bit limit, it does feel like at least 25% of the time. Often enough that I don't use it at all.
It was somehow jarring to see them without the Viking hat and fedora. 
Any $$$ involved ?
LibPQ.jl for sure.
Check out data.table too. I've switched from tidyverse to that. You might like it too. 
Any introductory book suggestion that uses data.table instead of tidyverse? I'm reading [R for Data Science](https://r4ds.had.co.nz/) but it uses tidyverse. 
I'm not sure. There is a thread on Stackoverflow comparing data.table with dplyr: https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly data.table and the tidyverse can also be used together. ggplot+data.table is a great combo. With a data.table tutorial it won't be hard to translate tidyverse code to data.table code. Data.table basically has only one operation, after all :)
Seems like a very basic thing to have to import for a numerical computing language.
 using Distributed; addprocs(3); @everywhere using DistributedArrays res = (x -&gt; sleep(1)).(distribute([1, 2, 3, 4, 5])) 
You technically can't from a SMP sense. Eigenspace's answer is a multi processing solution.
so is this language allowed to be embedded into other markup languages like HTML? 
Thank you :)
What do you mean by SMP?
https://docs.julialang.org/en/v1/manual/parallel-computing/index.html 
The full error when I don't provide an initial value is julia&gt; reduce((x,y) -&gt; x == y ? 0 : 1, a3, dims=2) ERROR: MethodError: no method matching reducedim_init(::typeof(identity), ::getfield(Main, Symbol("##45#46")), ::Array{Int64,2}, ::Int64) Closest candidates are: reducedim_init(::Any, ::Union{typeof(+), typeof(add_sum)}, ::Union{AbstractArray{Complex{Float16},N} where N, AbstractArray{Complex{Float32},N} where N, AbstractArray{Complex{Float64},N} where N, AbstractArray{Complex{Int128},N} where N, AbstractArray{Complex{Int16},N} where N, AbstractArray{Complex{Int32},N} where N, AbstractArray{Complex{Int64},N} where N, AbstractArray{Complex{Int8},N} where N, AbstractArray{Complex{UInt128},N} where N, AbstractArray{Complex{UInt16},N} where N, AbstractArray{Complex{UInt32},N} where N, AbstractArray{Complex{UInt64},N} where N, AbstractArray{Complex{UInt8},N} where N, AbstractArray{Float16,N} where N, AbstractArray{Float32,N} where N, AbstractArray{Float64,N} where N, AbstractArray{Int128,N} where N, AbstractArray{Int16,N} where N, AbstractArray{Int32,N} where N, AbstractArray{Int64,N} where N, AbstractArray{Int8,N} where N, AbstractArray{UInt128,N} where N, AbstractArray{UInt16,N} where N, AbstractArray{UInt32,N} where N, AbstractArray{UInt64,N} where N, AbstractArray{UInt8,N} where N}, ::Any) at reducedim.jl:171 reducedim_init(::Any, ::Union{typeof(+), typeof(add_sum)}, ::AbstractArray, ::Any) at reducedim.jl:109 reducedim_init(::Any, ::Union{typeof(*), typeof(mul_prod)}, ::Union{AbstractArray{Complex{Float16},N} where N, AbstractArray{Complex{Float32},N} where N, AbstractArray{Complex{Float64},N} where N, AbstractArray{Complex{Int128},N} where N, AbstractArray{Complex{Int16},N} where N, AbstractArray{Complex{Int32},N} where N, AbstractArray{Complex{Int64},N} where N, AbstractArray{Complex{Int8},N} where N, AbstractArray{Complex{UInt128},N} where N, AbstractArray{Complex{UInt16},N} where N, AbstractArray{Complex{UInt32},N} where N, AbstractArray{Complex{UInt64},N} where N, AbstractArray{Complex{UInt8},N} where N, AbstractArray{Float16,N} where N, AbstractArray{Float32,N} where N, AbstractArray{Float64,N} where N, AbstractArray{Int128,N} where N, AbstractArray{Int16,N} where N, AbstractArray{Int32,N} where N, AbstractArray{Int64,N} where N, AbstractArray{Int8,N} where N, AbstractArray{UInt128,N} where N, AbstractArray{UInt16,N} where N, AbstractArray{UInt32,N} where N, AbstractArray{UInt64,N} where N, AbstractArray{UInt8,N} where N}, ::Any) at reducedim.jl:
It should be okay to just do [x[i, 1] == x[i, 2] ? 0 : 1 for i = 1:size(x)[1]] 
Doesn't use `reduce`, but this should work fine: convert.(Int, a3[:,1] .== a3[:,2])
Oof that works too, thanks
Could you explain how this works? The answer's actually flipped [1 1 0 1] instead of [0 0 1 0], but I don't know how to tweak it. What does `convert.` and `.==` do?
Adding a `.` in front of a function allows it to be applied element-wise to a vector. So `.==` will do the comparison for each element and `convert.()` will apply `convert`to each element. You could use `.!=` if you want zeros and ones switched.
Convert is the way you get a different type, so for example `convert(Float64, 1)` gives you a float `1.0`. The other things are using "dot broadcasting", so the `convert. ()` broadcasts the call to the entire array. The `.==` part gives you a vector of booleans (it compares each element of the array, as opposed to just `==` which would ask if the arrays were equal to each other and would return a single Boolean). This works because converting a Boolean to an `Int` gives you 1 (for true) or 0 (for false). If it's backwards, you can change it to `.!=`
Just FYI you can do `size(x, 1)` instead of `size(x)[1]`.
\^This. Misread the OP, fixed the comment. Thanks!
I've used LibPQ without any trouble
Thanks, will try that then
For most purposed JuliaDB should not be thought of as a database package, but as a distributed dataframes package. Of course dataframes are kinda like databases. 
 For MySQL I use MySQL.jl. For SQLite I use SQLite.jl. For PostgreSQL I use LibPQ.jl. There is also JDBC.jl for JDBC and ODBC.jl for ODBC. If you are using different database systems, you might want to check out [Octo.jl](https://github.com/wookay/Octo.jl/). 
Sorry about the formatting, not sure how to do it better
&gt;`convert.(Int, a3[:,1] .!= a3[:,2])` This allocates two temporary arrays, which is not needed. You can use \`view\`s to speed things up: Int.(@views x[:,1] .!= x[:,2]) It still allocates more than the comprehension, but the runtime is similar.
It's probably a bad idea to assume that X exists, what are you trying to do ?
Ah, interesting! I had used `@time` to benchmark both my solution and the comprehension in the REPL (running a few times each to allow for compiling) and found my solution had many fewer allocations, but using `@benchmark` (from BenchmarkTools) I see now that the comprehension does in fact perform better, at least on my machine. Good catch!
Nice !
Well, I have a global variable that I set in the `startup.jl` file and I want to be able to use this variable throughout the entire code/project I am generally having trouble trying to convert from julia 0.6 to julia 0.7 and it has to do with some difference between `include("A.jl")` and `using A` So far (julia 0.6), I started `Revise` and push!ed my project paths to `LOAD_PATH`, all in the `.juliarc.jl` / After which I was able to do `using A`, `using B` etc. in any module that I have, which was found thanks to `LOAD_PATH` and reloaded during dev thanks to `Revise` \- All worked quite well If I try the same with julia 0.7 (moving `.juliarc.jl` to `startup.jl`), `using A` is not working properly, e.g. as stated in the post (but also other related issues), however `include("A.jl")` does work &amp;#x200B; Appreciate any help - I have tried posting about this in discourse, with no help at all
The package system changed a bit. I would turn your code into proper packages, then you can add them using `]dev path_to_package` and use them from anywhere. If you need some packages to be configurable I would either create another package that contains all the options, or just a file (could be a json or toml file but also a julia one) that you can load in your packages.
I see - I was also thinking that this is what I might have to do It's just that the entire project is so big and would contain so many packages - The overhead of creating packages seems large But maybe I just have to bite the bullet :)
I was correcting an erroneous statement about the purpose and intension of Julia. It *is* primarily for scientific computing, but it is *intended* to be general purpose. Even if the creators of Julia *were* delusional, my remark would be correct. Your comment makes no sense, and is also needlessly rude.
You don't need to turn everything into packages, it's fine to have a bunch or scripts. But if you have something that many scripts depends on, it makes sense to make a package out of that. Also any kind of functionality that is relativity atomic and independent from the rest goes well into a package, it should simplify your work.
Thanks for the advice, will give it a try
From my understanding this shouldn't be empty. However union types are apparently currently not considered to be abstract types (Though the docs state they are). There is a todo comment about that here: [https://github.com/JuliaLang/julia/blob/099e826241fca365a120df9bac9a9fede6e7bae4/base/reflection.jl#L471](https://github.com/JuliaLang/julia/blob/099e826241fca365a120df9bac9a9fede6e7bae4/base/reflection.jl#L471) And `subtypes` returns an empty array for non abstract types: [https://github.com/JuliaLang/julia/blob/099e826241fca365a120df9bac9a9fede6e7bae4/stdlib/InteractiveUtils/src/InteractiveUtils.jl#L212](https://github.com/JuliaLang/julia/blob/099e826241fca365a120df9bac9a9fede6e7bae4/stdlib/InteractiveUtils/src/InteractiveUtils.jl#L212)
Damn. I can't find a tracking issue either.
Perfect time to open one then. 
Your are allocating a new vector in every iteration (with append!(y, [...])). Here is a version without unnecessary allocations: function transform(seq::String) x = Vector{Float64}(undef, 1 + 2*length(seq)) y = Vector{Float64}(undef, 1 + 2*length(seq)) x[1] = 0 y[1] = 0 running_value = 0.0 i = 2 @inbounds for character in seq if character == 'A' a = running_value + 0.5 b = running_value elseif character == 'C' a = running_value - 0.5 b = running_value elseif character == 'T' a = running_value - 0.5 b = running_value - 1 running_value -= 1 elseif character == 'G' a = running_value + 0.5 b = running_value + 1 running_value += 1 else a = running_value b = running_value end x[i] = (i-1)/2 y[i] = a i += 1 x[i] = (i-1)/2 y[i] = b i += 1 end return hcat(x, y) end
Not sure what the cause of the problem there is, or if this'll be a suitable solution for you, but [DataFrames.jl](https://github.com/JuliaData/DataFrames.jl) is the native package in Julia if you want to use Pandas-like DataFrames. I'd recommend using that instead of Pandas.jl which is just a wrapper for the python package. 
Note that you should probably use [BioSequences.jl](https://github.com/BioJulia/BioSequences.jl) to handle sequence data. Strings aren't the fasted I think. Also if you need to read FASTA files. 
Maybe the file doesn't exits ? Try `isfile(url)`.
Sweet! That gets me down to ~50ms on 1x10^6 letters.
What is the difference between a `BioSequence{DNAAlphabet{4}}` and a `String`? When I convert the DNA sequence string into a BioSequence and call the `data()` method on it, I get a `62500-element Array{UInt64,1}`. Is it packing the data more efficiently into the array (16 letters per Int)?
DNAAlphabet{4} uses 4 bits encoding, you can also use DNAAlphabet{2} that uses only two bits, see: https://biojulia.net/BioSequences.jl/latest/sequences/bioseq.html#Using-a-more-compact-sequence-representation-1 I don't know if that really makes a difference in terms of performance, but using strings always feels dirty.
Why do you need to `hcat(x, y)` at the end? The python code just returns a tuple. If, instead, I just do return (0.0:0.5:length(seq), y) I cut the runtime from 23us to 10us for a 1million long string. 
thanks. the reason i choose Pandas.js because this is the first time i have tried Julia (switch to Julia from Python), and i want to use some method of pandas such as drop(). Navite DataFrames also has those function liked Pandas.jl ? Such as drop() ? 
sure it exits mate, because i tried CSV.jl before pandas.read\_csv
The weird thing is that when i try those code on my PC, it works fine, just have to edit a little bit in FILEPATH. !? 
&gt; Your code allocates a new vector in every iteration with append!(y, ...) Not exactly, array lists grow exponentially whenever they reach max capacity. This means array lists only reallocate log(N) times for N insertions which results in an amortised O(1) insertion complexity.
I clarified the source of the allocations. I meant to refer to the part [...; ...] of the append! calls.
A BioSequence will automatically check that it's properly formatted (no weird characters). Yes, it also packs the data more efficiently. They are also very fast, should be faster to iterate over than a String. I'd use a BioSequence with a two-bit Alphabet.
If you're switching then I'd recommend trying to use native packages wherever you can, if you switch and then just end up calling Python functions anyway then have you really switched? üòâ DataFrames.jl can (as far as I know) do, at least, everything Pandas can. You can look through the documentation [here](juliadata.github.io/DataFrames.jl/stable). Closest thing to drop would be [deletecols!](juliadata.github.io/DataFrames.jl/stable/lib/functions.html#DataFrames.deletecols!) and [deleterows!](juliadata.github.io/DataFrames.jl/stable/lib/functions.html#DataFrames.deleterows!) 
Not sure, it's likely a problem due to Pandas.jl having to install its own copy of python, with its own copy of Pandas, which then has to be called through Julia. There are a fair few points where that might fail. 
Right, but the second argument to his append! calls are newly created arrays.
Newly created arrays of length 2, hardly an expensive task.
When done a million times, it is. I'd wager those allocations are taking most of the time of his code.
Out of interest, why are you calling `string` on every character?
OH MANNNN, you really saves my day. Ability to solve optimization mathematic model with JuMP is my main point to switch to Julia. I must also solve one for my thesis which is unable to run with CVXPY but JuMP can (following the instruction of paper) &amp;#x200B; Once again, Thanks so muchhh 
Its a weird thing, anw thank you so much mate. 
No problem. If you need more help post here again, or even better post on the [Julia Discourse](https://discourse.julialang.org/) forums since they're way more active. 
Often what I do there is have a running_value per thread / pass, then add them all up at the end which often transforms the hard to multithread thing into easy. 
 str = String(rand("ATGC", 10^6));
Keep in mind some of the really useful tools like \`code\_warntype\` and \`@btime\` from benchmarktools. 
My mistake -- didn't know that you can use single quotes for characters
&gt; I don't think you can simd this code Yep, didn't realize that. &amp;#x200B;
Wow I had no idea that \`hcat\`ing made such a difference. Why is that?
hcat allocates a contiguous chunk of memory to fit both arrays, the copies everything into it. Returning a tuple of the two things doesn't need to copy anything since it just holds the pointers to the original data. 
I couldn't help but try to optimize this. I work with biological sequences too, and can't miss an opportunity to show off BioJulia. &amp;#x200B; First, I timed your Python code using the \`%timeit\` command in Jupyter. It took 292 ms on my computer. Second, I copy-pasted your Julia code. It takes 65 ms - significantly faster, but only by a factor of 4. Julia should be much, much faster than Python at this task. My first real attempt was much similar to that of /u/cafaxo, except using a BioSequence rather than a String. using BioSequences function test1(seq) y = Vector{Float64}(undef, length(seq)*2 + 1) running_value = 0.0 @inbounds y[1] = running_value @inbounds for i in eachindex(seq) base = seq[i] if base == DNA_A y[i &lt;&lt; 1] = running_value + 0.5 y[i &lt;&lt; 1 + 1] = running_value elseif base == DNA_C y[i &lt;&lt; 1] = running_value - 0.5 y[i &lt;&lt; 1 + 1] = running_value elseif base == DNA_T running_value -= 1 y[i &lt;&lt; 1] = running_value + 0.5 y[i &lt;&lt; 1 + 1] = running_value elseif base == DNA_G running_value += 1 y[i &lt;&lt; 1] = running_value - 0.5 y[i &lt;&lt; 1 + 1] = running_value else y[i &lt;&lt; 1] = running_value y[i &lt;&lt; 1 + 1] = running_value end end return y end Several things to notice here: 1) I preallocate the array with uninitialized values instead of growing it. 2) I use a BioSequence (both faster and more safe than a String) 3) I use \`@inbounds\` liberally 4) I use bitshifts to calculate the correct positions of the arrays. Probably doesn't matter, since Julia would convert multiplication by two to bitshifts anyway. This completes in 7.6 ms. We're now almost 40 times faster than the Python code. &amp;#x200B; But we can do better. This code is full of branches (i.e. if/else statements), which are slower CPU operations than most others. Can we figure out a way to skip those? Well, there are only 16 possible nucleotides (the 15 IUPAC ones plus a gap), so the values of \`y\[i &lt;&lt; 1\]\` and \`y\[i &lt;&lt; 1 + 1\]\` can just be looked up in an array. Even better, because it's so small, it can be looked up in a tuple. &amp;#x200B; My second attempt is thus: using BioSequences const rv = (0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0) const rv2 = (0.0, 0.5, -0.5, 0.0, -0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0) function test2(seq) y = Vector{Float64}(undef, length(seq)*2 + 1) running_value = 0.0 @inbounds y[1] = running_value @inbounds for i in eachindex(seq) base = seq[i] running_value += rv[reinterpret(UInt8, base) + 1] a = running_value + rv2[reinterpret(UInt8, base) + 1] y[i &lt;&lt; 1] = a y[i &lt;&lt; 1 + 1] = running_value end return y end Now there are no branches in the loop. Inspecting the code generated (using \`@code\_native test2(seq)\` reveals that allocating the array and determining the length of the bioseq are expensive operations, but there are only done once. The generated code in the loop consists only of small efficient CPU operations. This takes 1.58 ms. This is &gt;180x times faster than the original Python solution and only \~6 clock cycles per base. We probably can't improve any further. &amp;#x200B; &amp;#x200B;
Gobbedyret is right, repeatedly allocating those arrays is very expensive: julia&gt; @btime for i in 1:10^6 x = [2,3] end 33.986 ms (1000000 allocations: 91.55 MiB) &amp;#x200B;
Very nice! Would you mind sharing a fully working example, that is, including how to generate the `seq` input?
Depends on where you want to get it from. I made random one: using BioSequences # Here, I make a random DNA string seq = randdnaseq(1_000_000) # Here, I get the first DNA seq from a FASTA file seq = open(FASTA.Reader, "/home/jakni/Downloads/example/contigs.fna") do file record = iterate(file)[1] return sequence(record) end &amp;#x200B;
I meant just getting some data to be able to run the function. The rand-version would be the typical way.
Then just use the randdnaseq function as shown in the example above :)
My comment was made before the edit, he originally said it was because of append! and not the [...] but it is good to know that arrays aren't as efficient as I thought they were, is there any library that allows for stack allocated arrays?
StaticArrays 
Wow! There's a lot of [deep magic](https://narnia.fandom.com/wiki/The_Deep_Magic) going on here. This is going to give me a lot to chew on as I dive deeper into Julia. Thanks!
Yes, StaticArrays.jl may be what you're looking for. But they are only efficient up to roughly a size of 100 elements. Beyond that, they start to get slow.
Can you give some context? Julia code is usually wrapped in functions. What would be the input and output data of your code?
Please use the .jl file extension for proper syntax highlighting. The first step would be to remove non-const global variables (such as light), as type inference will not work for them. This makes code involving such variables extremely slow. Also, place all code inside a function (see lines 84-94) and take compile time into account for the first run.
It looks to me like the idiomatic thing to do would be a for loop here. 
Thanks!
What did profiling point at? 
Mostly the inter function, there's a lot of memory allocations too. I'm not sure if there's a type instability somewhere or it's just allocating all the hits objects.
Here's one possible way: function f(u) out = reverse(u) @. out = -2 * max(out, 0) - u + 1 end x1, x2 = -2.0:0.20001:2.0, -2.0:0.20001:2.0 f_u = reduce(hcat, (f([x, y]) for x in x1, y in x2)) u = hcat(x1, x2) Some remarks * Never `collect` ranges unless you have good reason * Splatting is expensive, especially for arrays * Vectors are normally constructed as `[1, 2]` not `[1; 2]`. The output is the same and the performance similar, but since you asked about 'idiomatic' * No need to allocate a vector `[1, 1]` for adding, it's better to use `.+ 1` * Consider using StaticArrays instead of length-2 vectors * `@.` puts broadcasting dots in all the right places, and makes the operations in-place and fused * Don't be afraid of using loops, they are idiomatic in Julia, and will normally give the very best performance * Concise function names in lowercase is preferred. `f(u)` is a lot clearer than `fOfuFunc(u)`. Or perhaps something more descriptive.
Amazing! Exactly what I wanted! Thank you so much!
Sick! Thank you so much!
If I dot everything, wouldn't I lose some flexibility. The \[1 ; 1\] column vector may not always be a \[1 ; 1\] ... I ended up parameterizing and needing to change the vector down the road.
Should I be doing something to avoid allocating such objects? On .NET I'd define them as value types... 
Yes, please read the comment at the gist: https://gist.github.com/jdh30/28a7397bdce3762948e026122faf610e#gistcomment-2803445
Splatting: The three dots in `f(x...)` &amp;#x200B; Not sure I understand the addition question, but `.+ 1` works with all types of AbstractVectors. I'm pretty sure it's always better than `+ [1, 1]`.
Ooh, thanks.
I'm doing + with other vectors, [1, 2], [1.1, 2.3], etc..
&gt; `ifelse()` is recommended over `? :` when calculating both branches is inexpensive Why is this so?
Apparently in many cases, the compiler can figure out to remove the branch entirely when using `ifelse`.
If you're passing in a vector to add to the expression, then just do function f(u, v) out = reverse(u) @. out = -2 * max(out, 0) - u + v end and it will work with both vector and scalar `v`, but scalar will be faster.
I think you need to do include. You need a file ‚ÄòPkgName.jl‚Äô in src folder. Inside PkgName.jl you need the lines ‚Äòinclude(‚ÄúA.jl‚Äù)‚Äô and similarly for B.jl Then you should do using PkgName in the repl.
I guess you'll need to run `using .PkgName` or `using Main.PkgName` after the include statement, as the module will be loaded in the Main namespace
 home = raw"C:\Users\matt\Documents\power" if ! ("$home\\GitHub\\power" in LOAD_PATH) for lib in readdir("$home\\GitHub") push!(LOAD_PATH, "$home\\GitHub\\$lib") end end 
If you're coming from python, you should be aware that code loading in Julia doesn't work the same way. It doesn't just load all of the code in a given folder. Take a look at the packages section of the docs - I learned a lot from that.
As KeScoBo said, you'll want to look at the packages section of the documents. To include all functions from A.jl, you would run `include("A.jl")` with a path appropriate to your working directory. This way, if you modify A.jl after you've included it in another live session, you can re-include it. &amp;#x200B; On the other hand, for package management, you'd run `Pkg.generate("A")` which would create a package folder A in your packages folder. It includes .yaml, .gitignore, license, and [README.md](https://README.md) files as well as /src and /test folders. Inside /src you should see an A.jl. A.jl should have include statements for other .jl files in /src. For example, if B.jl is part of the A.jl package, A.jl should have the line `include("B.jl")`. A.jl also needs export statements for each command that you want to be able to use when you type `using A`, including those that were included from B.jl
Try x = [i for i in a:b if i != c]
you can do this, but rather ugly Iterators.flatten((1:3, 5:10))
use parens, no need to create an array
There you create an actual array and allocate memory. Better use (i for i in a:b if i!=c)
setdiff(1:10,4)? Easier to read, not sure about efficiency.
I did a quick run through to remove type instabilities and add StaticArrays. It runs faster but I didn't really test it or profile it or anything. Probably a few more things you could do to make it faster. https://pastebin.com/rAXEpbSz
There's no way in Julia currently to enforce const-ness of a function.
Wonderful, thank you.
Well another way to put it is, a function is always a global const.
Please elaborate on your use case. I find that in a lot of cases when people talk about constants they are talking about immutability. 
Yea immutability is what I'm talking about (coming from a C background). I basically want `func(const type *)` from C, i.e. an immutable reference parameter.
RemindMe! 2 days
I will be messaging you on [**2019-01-11 21:09:48 UTC**](http://www.wolframalpha.com/input/?i=2019-01-11 21:09:48 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Julia/comments/ae5bew/how_to_pass_a_large_struct_as_a_const_parameter/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Julia/comments/ae5bew/how_to_pass_a_large_struct_as_a_const_parameter/]%0A%0ARemindMe! 2 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ednv6r8) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
By default structs in Julia are immutable. You have to [explicitly make them mutable](https://en.m.wikibooks.org/wiki/Introducing_Julia/Types#Mutable_structs).
Yea I did that. I do want this particular structure to be mutable for some functions.
In this case, it causes problems. The branch *isn't* removed - correctly - but `ifelse` evaluates both arguments. Converting `g` to a `UInt8` throws an exception if it is NaN, and sometimes it is.
Here's my go. [https://gist.github.com/nbaum/e99f4d33b94b6e191836b34efc5320fb](https://gist.github.com/nbaum/e99f4d33b94b6e191836b34efc5320fb) **Make the globals constant.** Non-const globals are bad for performance. When Julia compiles a function that references one, it cannot generate efficient code because it does not know if the global's value will be of the same type when the function is called. (It is not yet possible to declare the type of a global.) **Use tuples instead of vectors.** The size of a tuple is part of its type, which helps Julia generate more efficient code. It can avoid consing up new vectors, and skip bounds checks for indices which are known to be in-bounds. It can also unroll the loop in `dot`. **Move non-intersection check to** `raytrace`**.** Rays which don't intersect propagate a NaN all the way back to the main loop, but the shadow test still runs. Dropping out of `raytrace` early produces a speed-up proportional to how many pixels don't intersect the shape. Bit of a cheat, since this isn't a language-specific speed-up. **Some very marginal improvements** These changes might produce detectable speed-ups in a proper test setup, but they're high-hanging fruit. Julia isn't yet smart enough to know `sqrt(eps(1.0))` is constant. It knows `eps(1.0)` is, but computes the sqrt every single time. A little macro trickery computes the value once, when the function is compiled. In `unitise`, sqrt checks if its argument is negative, although we know it can't be. The `@fastmath` macro removes the check - along with IEEE-compliant maths, but it doesn't affect the output in this case. `raytrace` is type-unstable because it can return a Float64 or a literal 0, which is an Int. This means the function that calls it has to deal with checking which it is.
Fascinating. Thank you so much! :-)
Oh wow, that's a nasty sounding bug.
It's not technically a bug - ifelse is functioning exactly as intended. The primary use case for `ifelse` is arrays. Suppose `ifelse.(mask, original, override)`, where `mask` is an array of Booleans. The result is an array containing elements from `original` where `mask` is true, and from `override` where `mask` is not. In this case, not evaluating all arguments doesn't really make sense.
&gt; Although of course, the code that uses ifelse wrongly does have a bug! Right. That's what I meant. Provided you regard `ifelse` as a function call like any other I suppose the consequences are obvious so it isn't as bad as I'd thought. 
Revise.jl does this
Thanks, Looks like an active library with good documentation. Two welcome signs. &amp;#x200B;
 using Module ?
I'm just starting to look at the Julia ecosystem now so I can't really say, but Revise.jl does fit the bill. There is an example here: [https://github.com/timholy/Revise.jl/issues/194](https://github.com/timholy/Revise.jl/issues/194) &amp;#x200B;
To be clear, Revise helps you to update the code of existing, already loaded, modules. So if it's what you want to do it's fine.
That's because `Cell[]` is a value and not a type. Write `grid::Vector{Cell}` instead of `grid=Cell[]`. 
thanks a lot for your reply. That helped a lot. yeah the courseGrain was a typo. I am a Julia noob(as you may have guessed) can you tell me how i can use the constructor to append to the vector? Thanks a lot!
Ahh, that was actually not clear from what I read. I'll look into module in that case to deploy new code 
 struct Cell color::Int64 location::Int64 Cell(color::Int64,location::Int64) = new(color,location) Cell()=new(0,0) end struct Grid dimensions::Int64 timestep::Int64 grid=Vector{Cell} courseGrain=Vector{Cell} Grid(dimensions,timestep,grid_cell,courseGrain_cell) = new(dimensions,timestep,[grid_cell],[courseGrain_cell]) end although you don't need a constructor at all if all you are doing is binding values, only ones for the special cases struct Cell color::Int64 location::Int64 Cell()=new(0,0) end struct Grid dimensions::Int64 timestep::Int64 grid=Vector{Cell} courseGrain=Vector{Cell} end new_grid = Grid(1, 2, [Cell()], [Cell()]) in fact, because of the joys of multiple dispatch and type inference, you don't even need types either, this will produce the same native code struct Cell color location Cell() = new(0,0) end struct Grid dimensions timestep grid courseGrain end new_grid = Grid(1, 2, [Cell()], [Cell()]) &gt; typeof(new_grid.grid) Array{Cell,1} 
No, you should absolutely type the fields in the struct definition. Not doing that destroys performance. It's not necessary to specify types in functions for performance, but that is not the case for structs.
yes, you're right. I have been labouring under a misapprehension. I ran @code_llvm on both versions and the typeless one was about twice the size with plenty of branching 
1, my approach would be to first try to use a regular function, and if the optimizer can't handle it, and keeps compiling unnecessary fluff, then take things in your hand. you can imagine for example that you want different behavior for Integer types, you can just do an if, and the compiler will throw that part away as necessary. but sometimes it might hurt inlining or unrolling. sometimes you can't use a single if, because you want to do a loop based on a type parameter, like the size of an NTuple. you might end up actual code injected, even if it could have been done in compile time. tl;dr, you can customize compilation. 2, i don't think you can limit parameters this way. but you can always use duck typing, i.e. just ignore the problem, and let the user use any type. if the type does not support indexing, an error will be raised. i don't really like limiting types the way you did, because i might implement a type that supports getindex, but not a vector subtype, and so i can't use your function. at one point, i devised a little tool to test if a certain method exists. unfortunately it does not seem to work anymore, at least not on getindex. this was the code: @generated function require(method, args...) t = Base.to_tuple_type(args) t = Tuple{method, t.parameters...} if ccall(:jl_method_exists, Cint, (Any, Any), method.name.mt, t) == 0 :(error("$method is required")) end end function myfunc(a) require(feature, a) ... end and it would throw an error if feature(a) does not exist.
Wow, hard to imagine a more relevant code snippet than that! Thank you so much for your response. Both answers are very informative and helpful. I don't take any of my code seriously enough to look at the machine code it generates, so I'm getting the impression that if I can do everything I want with normal duck-typed functions then I should just be happy with those. &amp;#x200B; With regard to the limit parameters, I do see your point about using duck typing, although I personally like putting in those parameters because they make the function signature more informative. But I probably should just get better at writing the little help snippets above functioins, which would make the limiting redundant anyway. &amp;#x200B; Thanks again for the response!
a snippet is not so relevant if it does not work :)
As far as I know, you can't really do something like this with Blink -- it interacts with a local Electron window and can't be used to serve content over the web.
Blink is actually designed to do this (the Electron install is optional). You can serve a Blink page without launching the window and connect to it either via a normal browser or from another machine; you can even use a Blink page as Mux middleware and embed it in a larger site. Unfortunately I don't remember specific steps for any of this, so it'd probably take a bit of poking through the code to work out. Or if you (OP) open an issue someone who's worked on it more recently might be able to help out.
https://github.com/zyedidia/SFML.jl https://github.com/NHDaly/PaddleBattleJL https://github.com/jayschwa/Quake2.jl And also the OpenGL stack.
I‚Äôm curious - are there any features in Julia that is advantageous for making games and such?
Fast loops and GPU bindings
On the other hand, slow startup time and GC pauses
As someone who has done *C++* in games for quite a while, Im super interested in it. The team cares about speed, which is always a plus. They support macros, which makes building DSLs very nice. Multiple dispatch is great for gameplay code. Rarely run code can be slow and nice looking, while fast hot path code can be typed specifically or even drop to C or assembly if needed. Some silly things .: Whitespace is not meaningful, which is a plus for me. They use *end* for scopes, which makes my *Pascal* brain happy. Having a GC is always a bit problematic. Hiccups in games are death. For an indie game you can probably get away with it, but I wouldnt be able to recommend it for anything else. And, if Im required to do crazy gymnastics to get around it, that reduces the reason to use the language in the first place. 
How does minecraft handle the GC hiccups in Java? It's not that severe is it? Otherwise it would be pretty annoying
Im not sure, I havent seen the source. Most similar java projects use object pools and such, but if Im going to do stuff like that, Id rather use a language like C++ that gives me tools to make nice APIs around it. *Julia* has enough tools to do something similar for me, which is partially why Im excited for it. 
Be aware that SFML.jl currently need to be ported from Julia 0.6 to 0.7/1.0 &amp;#x200B; You can probably be also interested in [https://github.com/JuliaGL/GLFW.jl](https://github.com/JuliaGL/GLFW.jl) [https://github.com/cprogrammer1994/ModernGL](https://github.com/cprogrammer1994/ModernGL) &amp;#x200B; &amp;#x200B;
Broadly speaking, Julia's compiler works a lot like Rust's compiler, and I'd expect them to get very similar results for a well-written sort algorithm. We can do some fancy kinds of specialisation on data in Julia, but it's pretty explicit when this is happening, as opposed to the kind of compiler magic that something like V8 pulls off. So the short answer is "no", and we probably just have better sorting algorithms (or ones that happen to work well for your use case).
Julia's sorting algorithm just does quicksort by default, and insertion sort for very small arrays. You can manually set the algorithm you want; base includes an option for mergesort. I think there were discussions on adding more algorithms but last time I checked, base didn't have options for other algorithms like timsort or radixsort, might be wrong.
Pattern-defeating quicksort seems to be a good option today. But it all comes down to cache/branching optimization of the code. (The only reason that insertion sort is the fastest n smaller 25 is how Intel built the caches.) 
There are sorting algorithms in SortingAlgorithms.jl and a fast string sort exists in SortingLab.jl(https://github.com/xiaodaigh/SortingLab.jl)
JuliaAcademy just started and is in beta. I think they're free while in beta, so just dive in.
Very cool. I have been enjoying using Flux for machine learning and look forward to trying the Flux class.
thanks for the write up, cool stuff!
Thanks 
Finally some easily accessible pdf doc! [https://raw.githubusercontent.com/JuliaLang/docs.julialang.org/assets/julia-1.1.0.pdf](https://raw.githubusercontent.com/JuliaLang/docs.julialang.org/assets/julia-1.1.0.pdf)
probably best to isolate the exact error from your text and visit [github](https://github.com/JuliaGraphics/Cairo.jl/issues), where you *may * get some specific help... But Cairo isn't widely supported by the community, tbh.
i've been doing related tasks to this while working on a ColorSchemes revamp, so perhaps I'll dig something out tomorrow or whenever...
And a whopping 1200 pages doc at that :)
I have no clue at all on understanding that output, but would check if I had all devel packages https://stackoverflow.com/questions/2358801/what-are-devel-packages
It could be lack of privileges. Are you running as root? I suggest to post this question in [Julia Forums](https://discourse.julialang.org/).
Actually finding the release notes is not incredibly straightforward, so [here they are](https://github.com/JuliaLang/julia/blob/v1.1.0/NEWS.md).
It looks like that's what I'm gonna have to do, thanks. Also, I really thought it'd be better supported considering all the packages that require it (at least according to [Julia Observer](https://juliaobserver.com/packages/Cairo)).
I thiiiiink I just tried this (it turns out I didn't have cairo-devel installed), but still no luck...
I think I'd tried running as root and as my account in multiple shells, but I've never had any luck. I'll probably post it in the forums at some point. 
Well perhaps I exaggerate, it's still used, but on Mac it uses Homebrew which has been problematic over the last few years. It's not central to the core activity areas of Julia (ML, Maths, etc) and people have often reported over the years that they have problems installing Cairo. 
Part of me would like to be able to print the whole thing and read it like a book, but I'd feel bad for the trees...
there may be more than one devel needed. I remember from building R it is always a search what is missing. Its not always obvious either.
I must admit I'm guilty of giving up finding them :] Thank you my fellow redditor.
Awesome, thank you! I really don't have an idea how reasonable this is, so I'm curious to see what your related stuff is.
It will be at https://github.com/JuliaGraphics/ColorSchemeTools.jl eventually, but it's not yet working.
I just discovered this language use 1-indexing. Shame on you!
As 1-indexing is the natural standard for mathematics, and in this sense Julia is a Science oriented language, I can't really relate to what you're saying, sorry.
academics and their pointy hats, trying to school an entire field of technology with "theory is above practice"
How to tell someone is a bad programmer ? S/He cares about 1-indexing vs 0. 
no. because tuesdays! actually transistors and pepperoni. That makes me right!
Seriously don't say something like that in an interview, it's a huge red flag.
should be working (at alpha-level) now... [docs](https://juliagraphics.github.io/ColorSchemeTools.jl/latest/makingschemes/)
This is beyond amazing. I can't believe that you put all of this together in \~4 days. And also way more flexible then anything I was even hoping to be able to do. Thank you so much. I'm building it and going to start messing with it right now.
https://i.imgur.com/NyyIYey.jpg Ooooh noooe, I'm out of a job now!
Perhaps not what you are looking for, but fonts such as [Iosevka](https://github.com/be5invis/Iosevka/blob/master/README.md) have ‚Äúprogramming ligatures‚Äù. Most modern editors have options to enable those (e.g. Sublime Text, VS Code, Atom).
In general i prefer face to face workshop then online real time training then online slef-paced training.
AFAIK, it is not possible without modifying [the parser](https://github.com/JuliaLang/julia/blob/e90981ba0f9d1733d394c02ae254940449111a8a/src/julia-parser.scm#L114). \`-&gt;\` is a special form operator and not a function name. Perhaps, you can define a macro that switches \`‚Ü¶\` to \`-&gt;\`, but I doubt it makes things easier or more readable for you.
These are very good and quite fast as well definitely worth a look.
Yeah, Fira Code is also a good ligatured font for programming. Makes - &gt; actually look like an arrow. 
not judging, but I have never understood obsession with unicode in your code 
I like to use the Greek alphabet when writing math stuff. Looks really good and is easier to follow when reading with a paper.
I do the same but then kick myself when I'm trying to search where I defined \beta or whatever :s
Do note that in my browser (Firefox on Android), your operator is just a box. Part of the point of plain text is portability. Adding dependencies on fonts that aren't universally deployed puts that at risk. To be fair, I don't actually know how far of the beaten track you have to be to not have that glyph. Maybe it's just me.
Hasklig, too, is quite good. I prefer Fira Code, though.
I think you should add a table in the Readme showing your benchmark times (specifying your hardware) 
FYI, your Julia installation has the html manual in /Contents/Resources/julia/share/doc/julia/html/en/index.html. You can create an alias to it for offline reading. I think it's a little more attractive than pdf :)
You could break down the code into manageable functions to start and possibly expose it as a package. After that you could write tests for each function to make sure they do what they say using the testing framework Julia provides. We can't just debug your code for you unfortunately. You're going to have to design code that is testable and easy to understand. If you don't understand the code you wrote, we sure can't.
Hi, and thanks for the reply. I have added my benchmark results. In brief, C/C++/Rust always have approximately the same performance with rust slightly faster, no matter how to implement the algorithm. If a sorting operation to an array is involved in the algorithm, Julia's performance is abouth 50% of C/C++/Rust. If we omit the sorting operation, Julia's performace is almostly same as C/C++/Rust !!! &amp;#x200B; One other notable thing is that when I benchmarking Julia, type annotation is still significant. A totally no type annotation program is significantly slower than a type annoted version and a generic-style one. &amp;#x200B; Julia is much faster than other counterparts: C#, Scala, Haskell, Python. Note that I implement the algorithm with pure the benchmarking languages, and ensure that as little FFI calls as possible is involved. So that no numpy is used for python for example. This helps to benchmark the limitation of the language itself. &amp;#x200B;
Suppose you have a table with columns named x and y: push!(rows(t), (x=6, y=1.0))
I see you're building a pharmacometrics library. Any idea for what estimation algorithms that will be implemented/available for use? (FOCE, SAEM, MPEM, non-parametric, full Bayesian/Stan etc.)
We will be releasing with the standard likelihood approximations (LaplaceI, Laplace, FOCEI, FOCE, FOI, FO), and Bayesian via Hamiltonian Monte Carlo. Our focus up until now has been mostly on simulation, so the stuff like SAEM and MPEM are planned for the summer. Then there are research additional projects for other estimation techniques which are being developed directly in PuMaS, and those of course will release when they are ready.
Super cool. Thanks! I was pretty stoked to see references to PBPK/QSP being on the horizon and a part of the plan. I currently use SADAPT which uses MC-PEM primarily (though it has every other algorithm a pharmacomatrician could want, really) and is Fortran-based. Very robust system imo. However, not super easy to generate simulations in, so I typically outsource to other software. I'm excited to see the potential for something open-source, fast, and in a language that's easier to read/write. I look forward to seeing the project progress. 
I can try helping if you comment your code and give me an outline of what you are trying to do.
To add to what umib0zu said I would create two `update!` containing everything coming after the `while !(t == days)` and remove/refactor the `if neutral != true` if-else block (to remove these code duplications).
this worked perfectly!, thanks for your help
funny nugget: when you load a saved table, remember to cast it with table(load("here/was/your/table/saved"))
&gt; One other notable thing is that when I benchmarking Julia, type annotation is still significant. A totally no type annotation program is significantly slower than a type annoted version and a generic-style one. Type annotations on fields in structs is important for performance. Type annotations on function arguments is not. See https://docs.julialang.org/en/v1/manual/performance-tips/index.html#Type-declarations-1.
Yes, I benchmarked the code again with the annotations on fields and leave function arguments not annotated, then the performance significantly improved. 
Possibly this is not of great interest in this subreddit, but there is some low-hanging fruit for improving the Haskell version. If I make the point type strict with data Point a=Point !a !a deriving Show and compile versions of the top level functions with specialized types, e.g. using {-# SPECIALIZE integrate :: (Double-&gt;Double)-&gt;[Double]-&gt;Double-&gt;Double #-} then I get a factor of 3 speedup. Full specialization is very important for performance in Haskell, which I think is probably similar to Julia. It happens automatically if you use a function with a specific type in a given module, but not necessarily across modules. Usually the use of lists in Haskell as data structures is not great for numerical code, but that would be a more substantial change.
I dabble a bit in Haskell, would you mind detailing what specializing means/entails here?
Same thing as monomorphization.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskell] [Numerical benchmark where Haskell seems to significantly under perform.](https://www.reddit.com/r/haskell/comments/aneyng/numerical_benchmark_where_haskell_seems_to/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I thought GHC would take care of that automatically?
Hell yes I found the community that fits my values. Thanks for the writeup!
For cross module stuff, that can only happen if the function body (or more precisely, its unfolding) is exposed in the module's interface file. This will only happen if GHC chooses to expose the body (depending on various heuristics such as size) or you use an Inlineable or Inline pragma.
Thanks for the explanation
I love free beta stuff.
I really like to see this kind of thing, but one thought: picking a larger problem size (so that the C version takes at least a few seconds) might be a good idea, especially if some of the languages being compared have fixed set up costs, which get amortized over running code. it may also help reduce the relative error due to the kernel scheduler, interrupts, etc. on the other hand, my impression from a quick look is that you aren't timing total executable runtime, but you are using timing embedded in the program which probably eliminates most of the set up costs, and others like JIT time may be amortized since it looks like you are doing multiple runs? still, 87ms is pretty short compared to the duration of OS events like a scheduler quantum or a disk read.
I am not sure what you are asking. Julia is indeed JIT compiled and not interpreted. 
Whether compiled, or not, is a classification of the Implementation, not the language. For example C has interpreters and compilers, as do many many other languages. julia includes a compiler and an interpreter, but you can't actually tell which is being used. the julia compiler is not an ahead-of-time compiler, it is a just in time compiler. Which might feel like an interpreter from a UX standpoint, since it has e.g. a REPL. 
Julia is JAOT. Just ahear of time
Funnily enough one of the guys from PyPy described Julia to one of the core devs as ‚ÄúAOT compiled at runtime‚Äù, and it kind of is and so much more. LLVM provides C/C++/Rust grade performance optimisation and the compiler tracks further usages of code which means other runtime optimisations (that aren‚Äôt possible or guaranteed with fully AOT languages) are possible. So you‚Äôre actually kind of right.
I have heard "extremely lazy ahead if time"
So is what Julia does significantly different from other JITs? 
So real. This word is funny.
I think it could much more efficient if there's a concatenate operation for UnitRanges.
I think transpiling from Julia to Python makes no sense(just write a RTS that provides dynamic dispatching might be sufficient to finish this), but the opposite direction could be extremely useful. https://github.com/JuliaCN/Py2Jl.jl
it is dynamically typed, and efficiently jitted (large and important portions of the code can be jitted)
Attention: @when macro is added today while Julia Mirror haven't sync with the latest yet. If you want to try *when destructuring*, you can wait for less than one day or install MLStyle.jl via `add https://github.com/thautwarm/MLStyle.jl#master`. 
The formatting of the post seems broken.
Thanks. Did you mean the formatting of printing the table? It's generated by DataFrames.jl and goes well within a PC, but broken at all on Android.
It doesn't render well with Reddit Markdown, as multiple spaces are collapsed to single spaces.
Well, I'll fix it later.
I took the training on Introduction to Machine Learning and Artificial Intelligence, and the self-paced material in [https://academy.juliabox.com/](https://academy.juliabox.com/) for 'Deep learning with Flux' and 'Foundations of Machine Learning' is very very similar. If you do both of those, you won't be missing out on much from the live training
Thank you very much! 
I meant it looks like: https://i.imgur.com/oleWodu.png 
Ooooh, that's terrible! I'll fix it once I get home!üòÇ
Now it's fixed. Markdown formatting of old reddit doesn't support code-block quoted with 3 backticks.
[https://docs.julialang.org/en/v1/manual/performance-tips/index.html](https://docs.julialang.org/en/v1/manual/performance-tips/index.html)
Thank you, however my code is basically 2 + 2
If you run the @code_native macro you can inspect the assembly to see what‚Äôs adding the overhead
Thank you. I will try this
Assuming you're comparing the "performance" of a compiled C program with that of `julia -e 1+1`, that's a nonsensical comparison. Julia needs lots and lots of initialization, e.g., to load and initialize LLVM, libraries, the code loading environment, etc. None of that is relevant to the performance of the code that will be executing, yet it is contained in those perf measurements.
Are you taking the JIT overhead into account when you compile? Also, are you wrapping the code you want to benchmark in a function? There are some things to be careful about if you really want to see what performance Julia could reasonably give you. I don't really know what your needs are, but I can sum billions of numbers on my computer in Julia in basically zero time, so unless you care about code taking 1e-5 seconds instead of 1e-6, the addition of numbers might not be the most relevant thing to consider when you're trying to assess how different languages will do real tasks. Further, for a lot of tasks that might actually be a bottle neck---like, say, a matrix factorization---the julia arrays will get passed to a C iibrary and then C is doing all the 2+2 stuff anyway. Again, though, I don't know your use case. So I apologize if you know what I'm saying and know that in your case the CPU cycles are actually what is most relevant for benchmarking.
Thats what I thought. I was hoping there was a way to compile the julia code to get rid of the overhead, since the point of my project is to show how programming a specific algorithm in FPGA and synthesizing specialized hardware can make (for example) calculating fibonacci numbers much much faster. However I might just have to code the algorithm in assembly and then synthesize the hardware for it instead of going the route round julia. It would be fun to have it in the text as a comparsion from a relative high level language, to assembly to synthesized hardware.
No I was not. I am trying to figure out a way to measure the performance counting the cycles since I will synthesize hardware for an algorithm at a later point and the easiest way to compare two implementations without them being hardware related is counting the cycles as far as I can tell. I will of course expand the algorithm at a later point, I will not be comparing 2+2. I will however probably be calculating a fibonacci sequence.
If you're trying to compare the implementations of an algorithm, wouldn't it make sense to implement it fully instead of relying on `1+1`?
Of course, this was just a simple start point. No one flew to the moon before they did something simpler first. 
I guess I'm a little confused about the purpose of this. you say: &gt; [MLStyle.jl](https://github.com/thautwarm/MLStyle.jl) is a package for Julia to provide some practical ML infrastructures like pattern matching, ADTs/GADTs and the way to extend pattern matching. I've done a bit of OCaml programming so all of that sounds good. However, your examples seem to be manipulating the program's AST. Are there simpler examples which do ML-style pattern matching on types,etc?
Interesting! That sounds neat and I see why you are interested in such low-level diagnostics. I really don't know anything about CPU cycles, and I imagine benchmarking something that is so fast is probably pretty hard. But if you wrap code in a function and be sure that it has been compiled, that is probably a reasonable next step. Maybe it would look like ``` function f() for j in 1:1_000_000_000 2+2 end end # cause f to get JIT-compiled: f() # actually do benchmarks on this one: f() ``` I don't know if it is easy to add your cycle counter in a script-like file like that, but a lot of other benchmarks people do in the language have a template that is vaguely like that.
What's your project? Verilog synthesis vs Julia? You care about bandwidth here, not startup costs - after all, this is the sort of application an FPGA is tailored for. Measure units of work per second - bin off startup costs. Easiest way to is to simply up the number of work items (diluting the startup cost). This will give you a better benchmark.
Yes in general I will compare high level (Julia) , arm v7 assembly (or x86) and synthesis by vhdl. Thanks a lot for the tip. I will try to find an algorithm that is somewhat easy to implement in vhdl and still includes a significant workload. Do you have any tips as to which algorithms or applications may be suited?
Cool thanks. What I have Ben doing so far is using perf. A Linux process benchmarking tool. 
If you're looking for that, you'd better check the discourse and the Julia Slack team that are very active! :)
Might be Julia JiTing things at startup? 30s is annoying, sure, but if your main workload normally takes hours it is a drop in the ocean if the rest of it is faster. I can't really offer much help though, as I haven't touched Julia since .3 and am only now getting back into it. Best of luck! 
How about the examples in docs? Typ patterns and advanced type patterns are what you want! https://thautwarm.github.io/MLStyle.jl/latest/syntax/pattern/ I just focus on AST manipulations for it's the title of article... Of course, there's another reason why I don't write an article about general use of pattern matching: it seems that many friends who write Julia don't know why to use pattern matching. To achieve the same functionalities, code base of OOP and FP could be so distinct from each other that I cannot make an attractive comparison in very few codes to show why to use MLStyle.jl. However, manipulating ASTs can show how MLStyle.jl simplifies your codes tremendously and promises you better performance and readability, like https://github.com/queryverse/Query.jl/pull/238/files
I'm a little confused. It seems you've identified that time to first plot is slow because of Julia startup time, package load time, and function compilation, and that subsequent plotting is fast. Cool. If the Python plots take several hours because of processing large datasets, and Julia plots could take (several hours) / (Python-Julia speedup factor) + 30 seconds ‚âà 30 minutes, does that 30 second startup time matter that much? Anyway, I seem to misunderstand your needs, so it's maybe worth mentioning that if you want to avoid compilation time entirely your options are basically keep a long-running Julia process (on each person's machine? On a server? Etc.) or maybe check out [PackageCompiler](https://github.com/JuliaLang/PackageCompiler.jl)
Just out of curiosity, what plots do you make with that much data? And do you know where the bottleneck is in Python? Are you using matplotlib?
You can try using the gr backend for plots, like so: using Plots; gr() . That should at least be faster than the default pyplot backend. Or you can look into the standalone GR.jl. I have to admit that I've had problems with this library so far, unfortunately the documentation seems not to be complete yet. You can use some of the python documentation though. As the previous commenters said you will always have a startup overhead in Julia compared to python, but that should be insignificant compared to the speedup you get when running hour-long scripts.
I think you're confused, the slowness is not because of your code but because it has to compile the entire Plots package every time you call your jl script. It's a known limitation, and right now the only way is to either leverage an interactive REPL session running to avoid the recompilation every time, or try to compile your script ahead of time using Package Compiler (but it seems super complicated)
The most convenient way to share the results is via Jupyter or something similar. This allows you to save the result of the executed script. Basically, you have to recompile Plots each time the script is run and this is slow. (There will be upcoming work to make this faster, but nothing really stable and workable right now. ) Feel free to ask if you need anymore help. 
I will check out PackageCompiler, thank you for the recommendation! It is not plausible to keep the long running Julia process on each computer, the visualization portion of our work is infrequent, but we almost always need it in a time crunch, which is why we're looking at Julia. Is the Julia speedup factor really that significant? If so, that's awesome and can't wait to get working with it.
This was my concern, if that 30s was per plot then it wouldn't be worth going through the transcription phase. I was hoping there was a way to have the GR, Plotly, etc. backend compiled in the background to get rid of that.
We plot large datasets for research done in high-performance computing. We have traces for about two dozen models and then we analyze the raw data and plot our analysis. Yes we use matplotlib, and it's very slow relative to what we hope Julia can reduce the time to.
Can't make promises, it's highly problem dependent, etc. But one data point for you is that I recently cloned a simple python package to Julia and saw a 70x speedup. On the other hand, if your problem plays nicely with numpy and pandas there may be little speedup since those libraries are already optimized.
Something like sum(E_Grid[j,i] for j in peakW) should work, let me know if this is not that case.
That worked, thank you so much. Do you know if there is a similar syntax to use if you wanted to do the same thing without a sum ie: **@constraint(ed,CurE\_peak\[i\] &gt;= E\_Grid\[ peakW ,i\])** so to have CurE\_peak be 1 value that is larger than all the values in E\_Grid during peak hours. 
If I am not wrong, for j in peakW @constraint(ed,CurE\_peak\[i\] &gt;= E\_Grid\[j ,i\]) end &amp;#x200B;
&gt;That also worked! &gt; &gt;Thanks again boss, ya saved me a whole lota time. 
&gt;I was hoping there was a way to have the GR, Plotly, etc. backend compiled in the background to get rid of that. It does compile it the first time you call it :) And then, the subsequent times (in a same script) it won't be compiling again.
Thanks those examples help.
Innocent question to help me understand your challenge... Which part of your python script is slow? The data crunching / analysis part or the visualizing analysis results part (aka the plotting)? Knowing that, you couldd look at different options to solve... E.g. if it is only the analysis part that is slow, you could move that to Julia, and feed the results into the python plotting part via zermq or similar... Or you could consider pre-calculating analysis offline (possibly still in Julia), save results to a db/file and plot on demand based on those pre-calculated numbers... Or... 
Our analysis is actually done in Go. The files are too large to complete in python with our resources. We only use python for visualization
A possible approach to identify (well... estimate) the 'function specific' cycles could to run a baseline simple thing and get the cyclecount for that (your 1+1 could serve for this), then run your actual function and get the cyclecount for that. And then subtract the two... It's not going to be exact, but it should give you a better view (assuming my suggestion is feasible of course)
Thanks for clarifying that for me. So I guess the 15-20Gb file is the result of the Go-based analysis... How many datapoints is that (order of magnitude)? Do you always need to plot them all? Or only a part of them (based on what user requests)? Can you pre-compute some slicing/dicing factors for that... 
That‚Äôs a great idea. Thank you!
Hundreds of thousands, sometimes as many as tens of millions. Our user is ourselves, we normally plot virtually everything but our end product are generally scientific papers. 
Looks like you and I do very similar jobs :) I meant to ask what kind of plots you do, if it's scatter plots or 3D scenes with raytracing or histograms or whatever. Paraview won't help, right? As other people mentioned, you should identify the bottleneck. I expect you can gain something reading and writing compressed output as opposed to CSV, for example.
This looks suspiciously like code I've been working on :) I worked on a project to allocate energy resources to help balance a grid. I've finished the project, but made a lot of JuMP code in the processes. I'd be keen to see what you are up to, and maybe help.
You could try plotting with Makie.jl. It has instructions on their GitHub site for how to compile it with package compiler.
Plotting is slow in Julia. I have been exploring the language as well, lured by the claims that it is a fast language. But plotting is really bad, doubly so if you are trying to plot a large dataset with many points. As a bonus, have you checked memory usage as well? You might stumble with a few nasty surprises there. 
Julia is great for long running computationally heavy tasks, but as of version 1.x I don't recommend using it for developing small interactive scripts that parses a command-line argument and quickly perform some small task. The startup overhead is pretty significant. There are many nice visualization libraries available in Julia (GR, PGFPlot, PlotlyJS, Vegalite, Gadfly, Plots) but again none of them are as mature as Matplotlib/Seaborn yet. Plots.jl is one of the heaviest (read slow startup) one. I would recommend directly using `GR.jl` which is extremely fast... You may have to write more code as compared to Plots.jl as it does not offer fancy higher level functions like Plots does. But it would be pretty fast... And it does provide man popular chart types out of the box `scatter`, `plot`, `histogram` etc.. The code below runs in 4s on my laptop... It's not exactly same as your code but a quick derivative. ``` using GR GR.inline("svg") println("hello") x = 1:10 y = rand(10) plot(x,y) savefig("p1.svg") scatter(x,y) savefig("p2.svg") plot(x,y,xlabel="This one is labelled",title="Subtitle") savefig("p3.svg") histogram(x) savefig("p4.svg") println("Complete") ```
You need to talk with your supervisor or equivalent how you want to measure this properly and what the goals are. Every language using any type of compiler will compile it to an add instructions on the cpu. It is a useless benchmark. 
Maybe you should do the plotting in javascript, I mean matplotlib it's pretty crummy (don't @ me). Also probably don't use csv files as a backend, load it into bigquery or something. 
As far as I know, GR is the default backend for Plots.
Thank you! There's a strong need of this kind of books with code updated to julia 1.0.
You really mean you plot all those (sometimes millions) data points as individual points on 1 plot? Wow, challenging indeed...
Perfect timing! We really need more of this.
Thanks a lot, I've just started learning Julia, and this will be a really useful resource. Couldn't I suggest using thicker font and a bigger size? for those who would like to read the book on devices like iPad and ereader. &amp;#x200B; Many thanks!
I don't think its the lack of OOP thats prevents people from using it. It may not be pure OOP but it sure acts like OOP. Instead of writing `object.foo()` you just have `foo(object)` that's specialized for `object`. 
That's is like writing functions. There is no inheritance structure right?
It's as OOP as C
Structs in Julia are types with inheritance, and they can contain data or functions. So you could write OOP style in Julia using structs if you like
What you're talkin about her abstract type which are basically like interfaces in Java right? That's not object oriented in terms of inheritance. 
Does Julia have simple plot interfaces like python say matplotlib? do you know what is causing your lag in Python is it simply the parsing of the CSV, memory footprint? Just curious
It seems like you might be able to use C++ to convert the data to hdf5 format if it's the parsing that's taking a long time then you could plot individual data pieces and only read the data required from the h5 file. H5 format allows the data to stay on disc compressed while still giving random access to the file data. Many high-level languages have support of the interfaces to the h5 format. if you use a fast language to convert it to that then you can plot it and whatever you want likely. Of course I don't know a whole lot about the structure of your data or what's in the CSV
Oops, you're right. I was thinking of defining inner constructors for the abstract type, but abstract types can't have methods
People have used metaprogramming to do inheritance of concrete struts in Julia. You can get the language to do whatever you want. ... But should you? OOP is often treated like the end all be all, but it has lots of flaws. In particular, there's no principled reason why the first argument to a function (the implicit this pointer) should be special compared to all the other arguments. Letting go of that is very freeing, and a lot of things that get awkwardly crammed into an OOP style become very elegant instead. PS: I still don't know if a Square is a Rectangle in an OOP sense. 
Here is a similar question I asked in golang. There's a comment by someone named "tenF0ld" that might be good info for you too. I'm asking more questions to try to understand pros/cons. https://www.reddit.com/r/golang/comments/aqyjei/oop_go_ever/?utm_source=reddit-android 
C isn't object oriented... 
I know. But people attempt to do stuff through void* pointers and such but there's really no interfaces.
Here is a similar question I asked in golang. There's a comment by someone named "tenF0ld" that might be good info for you too. I'm asking more questions to try to understand pros/cons. https://www.reddit.com/r/golang/comments/aqyjei/oop_go_ever/?utm_source=reddit-android 
Julia would be a very ugly language is OOP was added to it. It's a good thing it's early creators had some sense of taste. Otherwise I can see how a Julia could've been a JIT language with OOP constructs everywhere.
You have type inheritance and you can define methods at any level of the type hierarchy like you are used to in OOP languages. That is most of the power of object inheritance except without the methods being owned by the same thing that owns the data (the object).
Some of the Julia creators have described Julia as having an abstraction of OOP via the type system and multiple dispatch. I see absolutely no interest among core developers in C++-style OOP.
I feel like there has to be drawbacks to this in practice the problem is is that it's hard for me to visualize without actually doing stuff in the language so what are the drawbacks?
It really sounds very much like C to me.
Many thanks! We need more of these kind of books
Its actually the opposite. Because they didnt go with object.foo() [and instead use the typical foo( object )], they naturally extend their dispatch to multiple dispatch. You can get around needing multiple dispatch, but every time you basically end up implementing a worse version of it with a crappy API. For the problems MI solves, it solves them very well. And give what other folks have said, you can do everything **C**++ can do, and more. (Though, I havent written a big application in **J**ulia yet, so I dont know where the warts are)
I don't think there's really any serious drawbacks. It's a bit weird at first but most people end up loving it it seems. Julia type system with multiple dispatch is very powerful. Here's a dumb example: abstract type Humans end abstract type Animals end struct Cat &lt;: Animals end struct Person &lt;: Humans end pets(h::T,a::Vector{K}) where {T &lt;: Humans, K &lt;: Animals} = "$T has $(length(a)) $K" julia&gt; pets(Person(),fill(Cat(),3)) "Person has 3 Cat" There's some limitations though, for example there's no multiple inheritance at the moment (in Base at least, I think there's some packages that might do it). 
&gt; abstract types can't have methods Methods are owned by functions, types don't have methods. But you can have abstract type in your methods of course (e.g. `f(x::Real)`).
As a longtime game developer Im super interested in it. Multiple dispatch is stricl...&lt;reading paper on MD&gt;... ahh yes. Multiple dispatch is strictly stronger than single dispatch. It makes some gameplay code very nice. Collision is an often brought up example which games directly use. Im excited to see what other systems will work well with it. My only issue is the **G**arbage **C**ollector. However, since **J**ulia has macros, there are likely ways to get decent looking APIs that are still fast dont cause wierdnesses. 
I believe its more powerful than anything you can do in Java or C#. C++ has MI as you said, which means it can probably do things that Julia cant. Though, that said, C++'s MI is kinda funky to use. Scala has a really slick MI implementation. 
I don't see Julia ever becoming object oriented. People who are will versed in Julia would see object oriented programming as a downgrade with essentially no benefits compared to Julia's multiple dispatch and abstract types. I would agree with them. As to whether this difference to Python prevents newbies from learning Julia, I don't think so. Julia's system is simpler than Python's - perhaps not internally, but to the user, it appears simpler. The object model, the way inheritance works and multiple dispatch is so much simpler.
The thing that stopped me from adopting it was having to wait 9 years for a package to load.
Haha... Relatively speaking?. It always seems quick enough.
Except you get all the advantages that you usually get out of inheritance.
Why did I sing out that whole song
Do do do do do dooo lmao 
Have you tried Python's DataShader package? Not sure if it will work in your case but it is designed to plot very large datasets. 
In principle I agree with this sentiment, but given recurring questions and discussions in the Julia discourse and in my work place, there are some legitimate use cases for OOP that need to be implemented differently in Julia, and for which it's very unclear how to achieve the desired effect. Unfortunately some answers are always "You shouldn't desire this effect". The most prominent example is interfaces, without which composition (which should replace inheritance after all) always feels hacky to me (and many others): https://github.com/JuliaLang/julia/issues/6975 It's a pity that this couldn't be sorted for 1.0, and we are getting many packages where extensibility is dependent on good documentation.
Good point, you are right about the poorly-defined interfaces of abstract types. I've struggled with that myself. &amp;#x200B; In my experience, though, it's much easier to keep track of an abstract type hierarchy than OOP-style inheritance (e.g. designing an interface for abstract types in Julia is easier than designing a class that is isheritance-friendly). On the other hand, traits may become more common in Julia soon, which might make the type hierarchy more messy.
thank you! it takes me a while to find the github link ( couldn't find it in the book) [https://github.com/h-Klok/StatsWithJuliaBook](https://github.com/h-Klok/StatsWithJuliaBook)
Thanks! Actually, the code in GitHub is still not updated, but will shortly be updated. 
I just published this post, and I was looking for some feedback and discussions on parallel &amp; distributed programming in Julia! The post is written in [Org mode](https://orgmode.org/), and the data and code are all hosted on this [GitHub repo](https://github.com/phrb/phrb.github.io/tree/master/res/intro_parallel_julia).
Nice post. You mention that not specifying the argument types for `naive_gemv` can impact performance. This is not true. Type annotations for function arguments are only used to dispatch to the most specific method that matches the arguments that were passed in. Native code is (by default) compiled for each unique tuple of concrete argument types with which a generic function is called. One thing I found strange is that normally, BLAS `gemv` updates `y`; there's no separate `output` argument. That's the case for `naive_gemv`, but not the other ones. To actually make this fast, there's a whole bunch of stuff you can do in terms of cache locality before resorting to any type of parallelism, but I guess that may not really be the point of the post. This talk by Kiran Pamnany may be of interest to you: [https://www.youtube.com/watch?v=YdiZa0Y3F3c&amp;list=PLP8iPy9hna6SgIyQrKUtx7bXOyuc4Gz3B&amp;index=2](https://www.youtube.com/watch?v=YdiZa0Y3F3c&amp;list=PLP8iPy9hna6SgIyQrKUtx7bXOyuc4Gz3B&amp;index=2). For the `@threads` version, you probably shouldn't use `@threads` in both the inner and outer loop, because you run into the problem with naive nested parallelism described by Kiran in his talk. Note that this argument may change with the new `partr` changes (depth-first scheduling) coming in soon. But still, you ideally want each thread to have a sizeable portion of work to do, and the number of columns is probably already going to be (much) higher than the number of cores.
I think it makes more sense to let each shark be a \`Shark\`, instead of having \`Shark\` contain a list of members. Like this: struct Shark name::String end name(s::Shark) = s.name function sing(shark::Shark) for _ in 1:3 println(name(shark), " shark doo doo doo doo doo doo") end println(name(shark), " shark!") end sharks = [Shark(name) for name in ["Baby", "Daddy", "Mommy", "Grandpa", "Grandma"]] foreach(sing, sharks) &amp;#x200B;
Thanks for reading and commenting! I really though that not specifying types would impact performance. My reasoning was that some type inference would have to be made every time, but it makes sense that after the first call there would be no overhead. I'll update the post with your feedback! I added the separate output array to make the coding a little bit simpler, but I agree it's closer to the definition of GEMV to overwrite `y`. Despite that, `naive_gemv` does not overwrite it as well, as far as I understand it, it allocates a new vector inside the function, is that right? Thanks for the link to the talk, I'll check it out! Yes, the number of cores is much smaller that the number of threads. I distributed only the outer loop in `processes_gemv!` because of that, and I should have done the same in `threads_gemv!`.
Very nice post! Lots of good info. I was wondering if you ran the BLAS version with multihtreading as well? [OpenBLAS](https://github.com/xianyi/OpenBLAS) has a multithreading option export OPENBLAS_NUM_THREADS=$num_threads which AFAIK is not used by default.
Thanks for reading! I did use Julia [BLAS threads](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#BLAS-Functions-1), configured by calling `BLAS.set_num_threads(n)` to get `n` threads.
Awesome! I must have missed that part!
You didn't miss it, I left it out but I think I should add it! Given your question, others might be interested. If you want to see all details, you can check the code I used for benchmarking [https://github.com/phrb/phrb.github.io/blob/master/res/intro_parallel_julia/benchmark.jl](here)!
good point, noted. I wanted to make it somewhat Julia specific so I wanted to use a type of some kind. I would imagine there is probably a way to do this much more clever with metaprogramming but I went the easy route.
There's no need for metaprogramming here. Just a straightforward type that you then put into an array (instead of putting the array into the type.)
Is the actual book available on the web?
Yes, there is a link in the README
Yes. A draft is available for next few months (till publication): [https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf](https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf) 
Ok, thanks guys!
Is there an estimated publication date? I'd be interested in getting a copy.
Hopefully before the end of the year with Springer. Anyway, happy to get feedback now about the online draft version. &amp;#x200B;
thanks a lot, this is excellent work.
Thanks. can‚Äôt wait to check it out.
[https://julialang.org/blog/2018/12/ml-language-compiler](https://julialang.org/blog/2018/12/ml-language-compiler)
Doesn't Julia implementation of multiple dispatch make OOP a nonstarter? 
If you're in SF on Sun, join us for the Julia answer: https://www.meetup.com/Bay-Area-Julia-Users/events/259079031/
No. How much OOP have you done in the past?
Lots of this is interesting talk in here. I still haven't put my perspective on there. Essentially there are a lot of people who really like functional programming on there that use go and Julia. https://www.reddit.com/r/golang/comments/aqyjei/oop_go_ever/?utm_source=reddit-android I am very very productive in OOP. This is the way I think and I've been doing it for years in coming from somebody who started in procedural programming. I think there are only a limited number of cases where you really need multiple dispatch probably. It's tough to compare them because python is not a language which is close to Julia or C++ in terms of speed, but you can make any argument any type and python has named arguments. also it's lack of typing actually speeds up development especially when you're talking about projects that are not large and you're just trying to write scripts or you have a simple class structure a greatly simplifies and speeds up development which is why it's so popular. For instance, I could write a Texas hold'em program with way less code and way faster then anybody could in C++ or Julia or Go (note the class structure in a game like this would be pretty simple) Most of the knocks that people were giving other languages that really like Julia or Go are people who for whatever reason just don't seem to understand how powerful and useful OOP is... Or most of them just seem to not like it. after becoming fluent in object-oriented programming any time I've had to go back to a language which is procedural I keep thinking to myself man this is a real pain in the butt I could do this so much more easily with a class. One of the things that I really like about python is that it's usually a mix of both. I know that each of these languages has its uses and if you're talkin about raw speed really they're both like C on steroids (those syntactically they're much simpler and there's other stuff they can do that C cannot do easily). I suppose I probably will not find out unless I do back-end server development or something else that requires raw speed. If it does require raw speed. I'll probably just stick with what I know which is C++, or C# is easy. Python can be sped up with C++ or Java. 
I can answer 2 out of 3 of those questions: 1. `struct` is basically the same as `immutable`, `mutable struct` replaces `type` (but you usually just want to use `struct` 3. I am on mobile and I already forgot what question 3 was brb
`immutable` is now `struct` and `type` is now `mutable struct`. They work just like they did in earlier Julia versions, just with a slightly more consistent terminology. To create a type alias, (or any other kind of alias), you just do: ``` const MyAlias = ComplicatedTypeIDontWantToWriteOutEveryTime ``` 
It's recommended to run your old (&lt;= v0.6) code first in version 0.7. You should see lots of helpful messages and warnings about what's deprecated/changed. If you go straight to v1., you won't see these messages, you'll just get the errors. Otherwise, v0.7 and v1.1 are very similar, so when you've updated the code, run it in v1.1.
You can use LateX codes for the logical symbols in the Julia REPL.
But does Julia has features to program Boolean arithmetic symbolically?
It does have a layer for sympy, which is feature filled but, being python, kinda slow. Rewrite.jl would likely do the job. &amp;#x200B; [https://www.youtube.com/watch?v=53AS7uxBMRs](https://www.youtube.com/watch?v=53AS7uxBMRs) &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Thank You! Both of these changes make sense once you laid them out, I'll update the subreddit if everything works out
Thanks, in your `do end` block example, could you explain it as you're passing println("hello world") to f() as the anonymous function of the do end block which represents the function g? Im just trying translate the logic into words to help me better understand
so I'm down "1 error" left. I'm thinking most likely I have some more errors hidden somewhere, but one thing at a time. the current block I'm struggling with is this ` context("indent by current LEVEL") do original_STDOUT = stdout (out_read, out_write) = redirect_stdout() system_output = @async String(read(out_read)) context("intended") do close(out_write) close(out_read) redirect_stdout(original_STDOUT) # current LEVEL is 2 expected_str = string(FactCheck.INDENT^2,"&gt; intended\n") @fact wait(system_output) --&gt; expected_str end end` error is: ` Failure :: (line:272) :: intended :: fact was false Expression: wait(system_output) --&gt; expected_str Expected: " &gt; intended\n" Occurred: nothing` I'm having trouble figuring out what it's trying to accomplish with the STDOUT. What are redirect_stdout statements accomplishing, and why does one output to out_read, out_write, but the other returns nothing? Best I can figure is one is calling the function `redirect_stdout` the other is calling the method. But why are the `out_write` and `out_read` being closed before the second `redirect_stdout` call? It looks to me like it makes sense that nothing is being returned since you're closing the pipes (are they pipes?) before anything happens. But I'm wondering if the @async call plays a role here. As you can tell, I'm on the edge of my understanding here
If I understand correctly, shouldn't the 1 in the lower left corner be NA too?
Does this help? a = DataFrame(:x1 =&gt;[1,4,1],:x2=&gt;[2,8,6],:x3=&gt;[3,2,9]) f(x) = x&lt;=5 ? missing : x for n in names(a) a[n] = f.(a[n]) end 
&gt;a = DataFrame(:x1 =&gt;\[1,4,1\],:x2=&gt;\[2,8,6\],:x3=&gt;\[3,2,9\]) f(x) = x&lt;=5 ? missing : x for n in names(a) a\[n\] = f.(a\[n\]) end Yes :-) Thank you for spotting this. I corrected my question.
Wow! Thank you again, this worked!
Note: `NA` isn't a thing in julia. There's `nothing` or `missing` or `NaN` depending on what you want. But I'm not sure I understand why you want to convert these values to recommend one or the other. Can you explain a bit more why you want to do this? 
Ok, I think I understand what is supposed to happen - @async will starting looking for outputs to stdout and conitnue executing through the code. When context("intended") is called it's supposed to print out ` !CONFIG[:compact] &amp;&amp; println(INDENT^LEVEL, "&gt; ", desc)` on this line. I think the stdout stream is supposed to catch this print statement and compare to the expected string, but for some reason this is not printing / being caught. 
Nice! Maybe you're already aware of that, but I guess I'd better mention it just in case: some functions mentioned in the book have been deprecated in DataFrames recently. This includes `readtable`, `head`/`tail` and `showcols`. Sorry for that, but we did it now precisely to avoid breaking books and user code later. Hopefully DataFrames 1.0 isn't too far now.
You are right, NA is not a thing in Julia - that was just to make it easier for me to post. I understand that could have confused readers, sorry for that. I was working with an ML problem, which contained, as a raw data, more than 20 variables. I was looking at their correlation matrix (which is the `DataFrame` above), and wanted a quick way to spot those with more than 0.8 as the corelation coefficient. Although I could have used a plot for this, I just thought instead of having a plot with many variables, I would just visually see the matrix with all coefficients &lt;0.8 made as "missing" and just see other numbers. So, basically, the resultant `DataFrame` was not for using further - it was just for a visual aid. 
Thanks. Aware of some, not all. Our chap 4 needs some updates anyway. Will do.
Ah, got it. And yes, making a minimal working example is definitely the right thing to do. Hopefully one of the other comments answers your question about how, I think missing is a good bet for your use case since iirc when is printed it has a darker font do the numbers you want will stand out even more.
[removed]
You should have a look at [https://discourse.julialang.org/t/julia-code-formatter-code-beautifier/11485](https://discourse.julialang.org/t/julia-code-formatter-code-beautifier/11485)
How about creating a new one?
:D
*Wooo* It's your **3rd Cakeday** _trichoplax! ^(hug)
Cant u just drag it out?
You can make separate windows but they will each run their own separate Julia instance.
It's one of the most frustrating things in Atom. I haven't found a solution either. 
Atom (like VS Code) uses an underlying framework called Electron. That‚Äôs where the limitation comes from , based on my understanding. I tried using Atom/Juno and found it very slow. Are there any tricks or tips to make that better , would be good to know 
Don't give up. Looks like a good project.
It's for parallel/distributed computing on multiple machines, which you likely won't need for quite some time if you're just starting out. 
Calling R from Julia is pretty straightforward using RCall. This is a nice introduction: [http://luiarthur.github.io/usingrcall](http://luiarthur.github.io/usingrcall)
Thanks, that really enhances the possible use-cases. R have a package for almost everything stats-related.
Entitled prick can‚Äôt be bothered with [looking up package for arbitrary indexing](https://julialang.org/blog/2017/04/offset-arrays), which has been a thing for quite a while.
Thanks, I couldn‚Äôt figure out why the @async read command wasn‚Äôt working, but I was able to get builds to succeed and packages all in order. It won‚Äôt be merged into the general repository for Julia in Favor of the Test package, but you can clone the repo and use it locally if needed 
That's what I figured, I'm not just starting out with programming though, only programming in Julia
I did not know that you can define a function ‚Äúfunc‚Äù using just ‚Äúdo‚Äù and ‚Äúend‚Äù. This is amazing!!! `@inline function PDFP_setDefaultPrecision(prec::Int)` `global DEFAULT_PRECISION[] = prec &lt; 0 ? 0 : prec` `end` &amp;#x200B; `function PDFP_temporarily_setDefaultPrecision(func::Function,prec::Int)` `olddefaultprec = PDFP_getDefaultPrecision()` `PDFP_setDefaultPrecision( prec )` `func()` `PDFP_setDefaultPrecision( olddefaultprec )` `end` &amp;#x200B; And using it like this &amp;#x200B; `julia&gt; println("Default Precision is ",PDFP_getDefaultPrecision() )` `Default Precision is 16` &amp;#x200B; `julia&gt; PDFP_temporarily_setDefaultPrecision(4) do` `global result = inv([ PDFP(1) PDFP(2); PDFP(3) PDFP(4) ])` `end;` &amp;#x200B; `julia&gt; result` `2√ó2 Array{PDFP,2}:` `PDFP(1, 0, [2, 0, 0, 0]) PDFP(0, 0, [1, 0, 0, 0])` `PDFP(0, 0, [1, 5, 0, 0]) PDFP(1, -1, [5, 0, 0, 0])` &amp;#x200B;
I realise I'm very late to the party here, but I'm trying to learn a little Julia and thought this was an interesting problem to try. Please note, this is the first code I've ever written in Julia, so there may be basic errors. &amp;#x200B; Here are my results: [https://gist.github.com/chr1sj0nes/b34fa546e15fca7a2d957d9bd9348b75](https://gist.github.com/chr1sj0nes/b34fa546e15fca7a2d957d9bd9348b75) &amp;#x200B; **TL;DR** * \~640Œºs single threaded - \~350Œºs with 8 threads. * Don't use double precision when single precision will suffice. * Modern CPUs can do several arithmetic operations at the same time, if you can express the problem as vector operations (SIMD).
Really interesting, good read! I didn't know single precision on a 64-bit CPU could make such a big difference - that is essentially a free optimization right there.
For me this is certainly not the reason. I'm rarely using OOP features in python either. While I'm interested in Julia from a curiosity standpoint, my number one reason (apart from learning overhead) for staying with python is PyTorch. And I generally tend to not use language bindings whenever possible but rather I follow the "When in Rome..." principle. And number two is that I can't get rid of C++ because I'd again have to wrap the pytorch C++ inference or tensorflow lite or whatever in Julia, only that I then encounter new issues as my code is running on Android, iOS, blackberry etc. And the C++ code compiles everywhere without issues. Objective-C++ made the wrap easy on iOS. JNI on Android. The REST APIs are done with django or whatever, python in any case. Adding another language to the mix is a large effort and just doesn't seem worth it. Alone the mental overhead I already have with objective c and Java and python and C++ is a major PITA by itself. 
Salty nerd coming 1 month later to behave like a condescending horse on his high horse. I'm sure you code like a horse.
I cant see from looking at that code what it's trying do so. My experience of "case" is a switch but you have a constant as the parameter to case - (1) in the first instance But is the point of your post about the case construct or the fact you can build it? 
&gt;But is the point of your post about the case construct or the fact you can build it? Yes, it's about introducing a case construct with changing Julia parser! If you're confused with this \`case\`, just think about a \`switch-case\` in c! Like this &amp;#x200B; switch(weekday) { case Monday : // dosomething case Tuesday : // dosomething ... } &amp;#x200B; In Julia, you can do above stuffs with &amp;#x200B; case(weekday) do Monday =&gt; # dosomething Tuesday =&gt; # dosomething ... end &amp;#x200B;
Calm down there, Mr. Hands.
I do miss a case/switch. if .. else if ... else ... end always seems so messy. I tend to use a vtable : actions = Dict{String, Function}() actions["Monday"] = ()-&gt;dosomething actions["Tuesday"] = ()-&gt;dosomething .... get(actions, day, ()-&gt;())() 
AFAIK, that's the way how Guido told Python programmers to do switch-case. For you said you do miss a case/switch, I'd suggest you to have a try with MLStyle.jl. And you can copy the codes from the first link I've posted above, to support such a natural case syntax.
I'll see if I like it next time it comes around
If you make the case/switch a language feature, the language can then implement it how it wants. C and C++ will switch between if/then, tables like above, and one other way to implement them depending on what the switch looks like. 
Julia has multi-processing capabilities built in. Imagine sitting in a computer lab, firing up Julia on every machine, and manually kicking off some computation. Julia provides a set of abstractions to automate that: one master session can send data and code to many workers to distribute a computation and execute it in parallel. The machine file tells Julia how to set up this cluster. Many more details are here: https://docs.julialang.org/en/v1/manual/parallel-computing/
SLAP FIGHT!
Have you tried [Jupyter](https://jupyter.org) ? You write your code in cells and you can run one cell or all of them. It‚Äôs really nice, very interactive. Cells can be text, images, latex equations and more. 
I tried Jupyter for Python once, but I like normal IDEs more than the web based Jupyter. Mostly, I prefer the Atom IDE because here I have a variable workspace like in Matlab
I guess you are looking for this [http://docs.junolab.org/latest/man/basic\_usage.html#Cell-Evaluation-1](http://docs.junolab.org/latest/man/basic_usage.html#Cell-Evaluation-1)
yep, that's it! Thank you
In Atom go in Julia-&gt;Settings and open Keybindings. Search for "run". You will see a list of available commands there you will see various shortcuts, one of them will be julia-client-run-cell.. &amp;#x200B; You can read how to define cells here: [https://docs.junolab.org/latest/man/basic\_usage.html](https://docs.junolab.org/latest/man/basic_usage.html) 
I was recently thinking about how to write tests for Julia code. This will be useful, thanks !
Looks like evaluating code restart the session, so you have to reload packages each time. That's a bit of a problem.
Julia's functionality is spread out over many packages, and I'm having a bit of trouble using the packages on this Repl.
Interesting. We should probably preinclude some of the top packages? Is there a list of the top packages I can look at?
It does restart the session on every editor run (as opposed to a console run). In most repls that's the expected behavior otherwise the environment gets polluted. I wonder why Julia doesn't do a better job at caching and basically make the second run a noop! Maybe we can precompile some top packages. 
https://juliaobserver.com/packages I would install DataFrames, Distributions, Flux, JuMP, DifferentialEquations, JSON.
You could try to compile the packages into the system image: https://github.com/JuliaLang/PackageCompiler.jl I don't know how well that work. But easier solution imo would be that running code would just send it to the repl, with another button to restart the session. That's how editors usually work.
Last updated a month short of 3 years ago. Hasn't Julia changed quite a bit since then?
Actually I get similar results on a numerical integration example. I try to compute an integral at 1e4 points using a distributed for cycle. with 1 proces it takes ~1s and with 4 workers it takes ~1e-7s, which is strange in my opinion... I will try again to understand if the single proces version of the algorithm is bugged and eventually post my code here on reddit for comparison 
Weird, I get the expected 10sec runtime when running this on my machine, could you add a print of `myid()` and/or `j` just before the call to `sleep()` to see what's going on ? (Also note that your function is not type-stable as `sum` is initially an Int but then changes to a Float if the inner loop is ran at least once)
Making that change results in this: `julia&gt; a = @spawn MyFunc.ArraySum(A[:,1:10],1);` `julia&gt; From worker 3: 1` `From worker 3: 2` `From worker 3: 3` `From worker 3: 4` `From worker 3: 5` `From worker 3: 6` `From worker 3: 7` `From worker 3: 8` `From worker 3: 9` `From worker 3: 10` This suggests that the function is actually being called in the statement. I had assumed that @spawn worked similarly to the `client.submit()` of Dask, where the expression isn't evaluated until it is fetched. But it looks like that's not true. Does `@spawn &lt;expression&gt;` begin evaluating immediately on the worker? &amp;#x200B;
Yup, that's what the docs suggest :) `help?&gt; @spawn` `Create a closure around an expression and run it on an automatically-chosen process, returning a Future to the result.` I'm not familiar with Dask but what you were expecting probably is @task `help?&gt; @task` `Wrap an expression in a Task without executing it, and return the Task. This only creates a task, and does not run it.` &amp;#x200B;
Are you running this as a script? Or are you calling each integration subsection in the REPL like I did above?
Ah okay! Thanks! The confusion was the "returning a Future to the result". In Dask, a future is sort of a "future value". It gets returned immediately, and is only calculated when you run Future.result().
I run it altogether. I don't use spawn and fetch though, as for this kind of problem (and expacially in my case) I think it is easier to use `@distributed for` and SharedArray to store the computation results, as suggested in the Julia docs 
I will post my code by this night 
Just in the off chance you weren‚Äôt aware, Julia also has proper thread based parallelism (@threads) in addition to multi-process based parallelism (multiprocess and similar packages in Python, and what you‚Äôre doing here). Also, as I learnt recently, instead of going size(a)[1] You can actually go: size(a,1) Where the second parameter is your dimension.
I've made a Jupyter notebook with the results I've collected. Turns out that @distributed was running asynchronously, thus returning an execution time which was not accurate. # Parallel computing in Julia In this notebook it is shown how parallel computation can speed up by order of magnitudes complex computations For this example, `QuadGK`, `Distributed`, `BenchmarkTools` and `SharedArrays` are required, if not already installed please run the following code: ```julia using Pkg Pkg.add("QuadGK") Pkg.add("Distributed") Pkg.add("BenchmarkTools") Pkg.add("SharedArrays") ``` ```julia using Distributed using BenchmarkTools CPUcores=4 addprocs(CPUcores) # One should add n workers, where n is the number of available CPU cores print(workers()) ``` [2, 3, 4, 5] I now need to load the library required for computing the integral (`QuadGK`) and the lib for `SharedArray`s. Since every process need to be able to calculate integrals and operate on arrays, I load the libraries `@everywhere` ```julia @everywhere using QuadGK @everywhere using SharedArrays ``` Define the Euler $\Gamma$ function: $$\Gamma(z)=\int_{0}^{\infty} x^{z-1}e^{-x}$$ ```julia @everywhere Œì(z)=quadgk(x-&gt;x^(z-1)*exp(-x),0,Inf32) ``` We shall now create 2 ```SharedArray```s. Such arrays can be accessed and modified by multiple processes simultaneously and in efficient fashion. They behave like regular arrays if no multiprocessing is required. ```julia npoints=10000 z = SharedArray(rand(range(1,stop=30, length=10000), npoints)); a = SharedArray(zeros(npoints)); # results array ``` Define a function used to fill `a` whith zeros ```julia function reset_a!() @distributed for i=1:10000 global a[i]=0 end end ``` reset_a! (generic function with 1 method) I define two functions will compute $\Gamma(z)$ at n points between 1:30 (up to 10000 points). The first one is parallelized, the second one not &lt;span style="color:red"&gt;Caution:&lt;/span&gt; @synch is needed for the benchmark but usually it is not. Tasks can by run asynchronously or synchronously. &lt;br&gt; A synchronous routine waits for all the tasks to finish before returning a results, while an asynchronous computation returns instantaneously a `Future` object, which will contains the results of the computation once it is done. &lt;br&gt; Thus, in order to know the total compute time, it is preferable to run a synchronous task. ```julia function test_me_distributed(n) @sync @distributed for i=1:n global a[i]=Œì(z[i])[1] end end function test_me_distributed_async(n) #an asynchronous task to show the results of an asynchronous computation @distributed for i=1:n global a[i]=Œì(z[i])[1] end end function test_me(n) for i=1:n global a[i]=Œì(z[i])[1] end end ``` test_me (generic function with 1 method) ```julia @time test_me_distributed_async(10000) @time test_me_distributed(10000) @time test_me(10000) ``` 0.033074 seconds (15.39 k allocations: 831.626 KiB) 3.739367 seconds (197.61 k allocations: 9.708 MiB, 0.16% gc time) 2.994496 seconds (33.94 M allocations: 712.996 MiB, 3.60% gc time) As we can see, an async task requires virtually no time, but the computation is still running when a future result is returned. ```julia res1=0. n=100 reset_a!() for i=1:n tmp=@timed test_me_distributed(10000) global res1+=tmp[2] end print("elapsed time with parallelism: $(res1/n) seconds") ``` elapsed time with parallelism: 0.28937014896999996 seconds ```julia #%% res2=0. reset_a!() n=100 for i=1:n tmp=@timed test_me(10000) global res2+=tmp[2] end print("elapsed time single process: $(res2/n) seconds") ``` elapsed time single process: 0.8324613159400001 seconds ```julia print("speed-up factor: $(res2/res1)") ``` speed-up factor: 2.8768043936221783 
How.about adding MLStyle.jl to the list of Functional Programming?
Decibans?
It's one tenth of a Hartley. I assume that clears everything up. 
Inspired *by*, not *too.* &amp;#x200B; This looks good.
If anyone what to learn Julia. This is a great site: [https://app.codesignal.com/signup/NaqMfsKPZ6h7Av5mH/main](https://app.codesignal.com/signup/NaqMfsKPZ6h7Av5mH/main)
Very clever code here! 1) the "\~" character is assigned "length", so read "\~a" as "length(a)". This just saves a few characters. 2) length(a)\*(length(a+1)/2) is a fast way to sum all the numbers from 1 to length(a): [https://betterexplained.com/articles/techniques-for-adding-the-numbers-1-to-100/](https://betterexplained.com/articles/techniques-for-adding-the-numbers-1-to-100/) 3) Since we know from 2) what the sum WOULD be if there was no missing label, subtract the actual sum. Since the only difference in sums is that 2) includes the missing label and sum(a) does not, subtracting sum(a) gives you what that missing label was. &amp;#x200B; Don't write code like this if you are working on a real project with a team, or if you want to understand it later on, but for puzzles like this it's kind of fun to try to figure out what it means. I solve puzzles for fun though, don't make me do it at my day job :)
Thank you! That's really clever
What's the point of \~ here? Wouldn't `missingnumber(arr) = (n = length(arr); n * (n + 1) / 2 - sum(arr))` be much more legible and clear? What is the *julic* way?
&gt;What is the julic way? I feel like the analog to "pythonic" for Julia is *julian*. At least that's what I've seen on Slack and Discourse a few times.
Fair enough.
I agree, this is definitely better. But also, use \`div\`, not \`/\`, since the latter will produce a float.
There's nothing wrong with using clever tricks, as long as they are clearly documented. Especially if they are considerably more efficient.
Thanks for posting this!! Been looking to play around with Julia more and this is a great way to. 
reminds me of this from Jurassic Park https://www.youtube.com/watch?v=9nazm3_OXac
I think that means it isn't registered. You can try adding it directly with ]add https://github.com/JuliaPlots/StatsPlots.jl.git Although you may want to double check that StatsPlots supports 1.1 
`]update` first
In case anyone is confused (like I was), `&gt;&gt;` is the right bit shift operator and the effect of bit shifting a binary number to the right by 1 bit is equivalent to dividing the number by 2.
That's right. My point was that using `div(n, 2)` would create an `Int64` even when `n` is e.g. an `Int8`, while `n &gt;&gt; 1` will preserve the type of `n`. It's a bit irrelevant, though, since if `n` is the length of the vector, then it will be an `Int64` anyway. There is, BTW, a nice way of achieving the the same as above, without clever formulae, and still with the same execution speed. Here's a clearer (imho) version of the original code, that makes it obvious what's going on: function findskipped(arr) Nmax = length(arr) expectedsum = sum(0:Nmax) # this is super fast actualsum = sum(arr) skippedval = expectedsum - actualsum return eltype(arr)(skippedval) end The key is that ranges in Julia are smart, so that `sum(0:Nmax)` is translated under the hood to `div(Nmax*(Nmax+1), 2)` with identical performance. The last line converts the output back to the same type as the input vector, just because it seemed more appropriate.
+++ for this question, does exists also qt library as with python? 
I would definitely call it a GPPL, http://genieframework.com/ is written in julia for example.
The language itself is great for any of that, but in practice startup/load time is too much for CLIs and baseline memory usage is too much for running servers (if you have a cheap vps like me). 
What happens with memory? how much can it be?
It's not because you can that you should ;)
Julia uses 138M as soon as it starts, 220M when I load a few decent-sized packages, and my server has 1GB. My other applications are in the &lt;10M range.
I don‚Äôt think so. As much as I love Julia, I don‚Äôt think it lends itself towards desktop application development as there isn‚Äôt currently a way to make a distributable binary.
Oh that's too mucho why does it happen?
Yes I use it every day for whatever I need to do - whcih is mostly SQL / Excel / Web Scraping
There is a package that provides qml bindings. None that do direct qt widget design.
There's a great post on the Julia forums covering exactly this topic: https://discourse.julialang.org/t/not-only-for-technical-computing-changing-the-narrative-around-the-usecase-for-julia/19784
Julia the language is definitely a great general purpose programming language, it is positioned right in the middle between dynamic languages like Python but with the ability to write high performance "low level" code without leaving the language or giving up it's high level constructs, plus lisp-like metaprogramming system and a great type system that allows you to create extensions to the language that are just as performant and integrated as the ones in the standard library (which are also written in Julia). There are some pretty demanding stuff being developed like real time robotics ( http://www.juliarobotics.org/ ) and HPC ( https://www.nextplatform.com/2017/11/28/julia-language-delivers-petascale-hpc-performance/ ). Julia the tooling is still not as mature as languages that are typically used in this domain but it's moving forward. There are works on creating debuggers/interpreters for julia 1.0, to (pre)-compile the language to avoid recompiling the code every time you run a program (which causes the delay, especially since LLVM is kind of a slow compiler) and to create a binary ( https://github.com/JuliaLang/PackageCompiler.jl ). Since the compiler also infers a lot about the code in the language can also allow in the future better static analysers. But for now, the lack of binaries and recompiling every time you run the script makes the language more appropriate for long running processes (such as scientific computation). Julia the library ecosystem is still very biased towards scientific computation and research, though we are seeing more variation such as web frameworks being created, though currently that kind of application is much more commonly used everywhere, which could attract people who will help building the surrounding ecosystem, such as web and gui programming tools. So for now I'd say if you want to write a pure web or gui application, you should look for mature languages with extensive and well tested libraries for those uses. If you want a single language that is really good at scientific computing/machine learning that can also do those things even if not as good yet, then you should try Julia.
Thanks, appears to be in some sort of weird dependency hell. Trying to add it directly, I get the error message concerning Widgets.jl &gt;restricted to versions 0.5.0-* by StatsPlots but as far as I can tell, that version doesn't exist, only 0.4.3+ that I have already installed. Maybe not yet up to speed with 1.1.0? At any rate, I can still install and use the predecessor StatPlots.jl with most of the functionality. As a new user of Julia, I really like the language, but these not infrequent glitches with versioning are a real pain :)
Thanks, already tried that, to no avail.
How does it related to cython? I haven't checked Julia in a while, but having native low level functions is tempting for me..
IMO a web framework is actually in line with Julia's mission to be the scientific language.
By "low level" I just meant kind of like C (minus pointer arithmetic and malloc/free, can use the formethough the former you use iterators and you can at the very least create code with all the data on the stack instead of the heap) or Fortran, but not assembly (you could sort of go down to the LLVM but it would probably not be pleasant). The FFI with C/Fortran is pretty easy to use though, since the Julia types are mostly similar: https://docs.julialang.org/en/v1/manual/calling-c-and-fortran-code/index.html#Mapping-C-Types-to-Julia-1 which is similar to Cython (though you can't fully compile Julia for now). Julia compiler does type inference, so even if you write a code like python it will always try to optimize the best it can. So if a variable is always an int (like Int64, same as C), even if you don't write type hints, it will consider it an int. Julia has multiple dispatch as it core concept, which means that functions can work differently depending on the type they get (which is decided at compile time), so if the compiler knows the variable it will generate code that is effectively that is basically the same as it would be in a language like C. But if you use more dynamic features (such as having a list including Strings and Ints), the compiler will default as an Array{Any}, and it will use heap allocated generic boxes that need to be asked every time what type it is making it very dynamic but very slow. Avoiding this last scenario is what the Julia community calls type stability (though Julia optimizes well small unions such as Union{Int64, Nothing} for nullable types). People usually annotate types though. And while Julia is interpreted, the compiler always run static code. It's what some people call the just ahead of time compilation: when you call a function, the compiler will compile every possible path given the variable types and it will only run the static code. That's what allows the LLVM to compile code with the same level of optimizations as static languages. And it's also what people feel the most when using Julia as a scripting/interactive language, and which hopefully will improve (for example options to compile the least possible, making it closer to normal JIT languages, or by pre-compiling, making it closer to static languages).
What packages do you use for that kind of workload if I may ask?
SQLite, ExcelReaders, XlsxWriter, DataArrays, Pycall -&gt; requests that's about it for external packages 
There's Gtk bindings are works reasonably well.
I think Julia ships with all kind of stuff, a compiler, a REPL, BigInts, etc. So it makes a large binary. It might be possible to slim it down by removing stuff but I don't know if it's easy to do right now. 
You can compile your project, package all the dependencies and make a distributable binary with it. It's not completely straightforward to do but it's possible. https://github.com/NHDaly/ApplicationBuilder.jl
First of all thanks for the long reply. I read somewhere that in julia you could write similar code to what happens in cython, i.e. you call some magic syntax, and statically type the code. It offers a native approach to writing cython like code. Back when it first released I was tempted by the way they promoted it, I did write some basic code in julia. However, as a grammar snob I have a huge distate for matlab like syntax (forcibly had to write matlab for years). Consequently, I never really got into julia as it reminded me too much of it. I know this is way too silly. I love the way functions could be written to, i.e. lamda functions, unicode is nice too!
TIL. Cool. I'll check this out, thanks!
It looks like Matlab but it's just superficially, a little below it feels more like a Lisp to me. You can even go the more functional way with pattern matching (https://kmsquire.github.io/Match.jl/latest/#Alternatives-and-Guards-1), LINQ (http://www.queryverse.org/Query.jl/stable/linqquerycommands/), transducers (https://github.com/tkf/Transducers.jl). What led me to try the language recently was seeing the more advanced stuff made with the language in https://docs.google.com/presentation/d/1IiBLVU5Pj-48vzEMnuYEr9WQy9u2oi6Yk_8F3sgGkvQ/preview#slide=id.p
slide 28 is interesting :D 
are you using JuliaPro? If you are, you might benefit from googling "how to use general registry instead of juliapro registry"
Yes, Julia is a good General Purpose programming language, when more general purpose packages come into existence.
Out of curiousity, what have you found deficient in HTTP.jl that's caused you to reach for python requests?
Long version: My corporate terminal is locked down so I can't run installers without permission and my Julia work is unauthorised. We were on Win7 when I needed the HTTP stuff and I couldn't Pkg.add. I had to have a mirrored environment at home and manually copy my Julia environment. We're on Win10 now and I can use Pkg.add again. I started to move my code to HTTP but found the cookie management challenging and gave up because all I was working towards was the same result but in Julia. TL;DR cookies
Actually I am looking at julia's [benchmarks](https://julialang.org/benchmarks/), it's kinda odd that they use native python to compare numeric computation stuff for all but two methods. If true, the spread is pretty sick for julia lang tho
*Wooo* It's your **1st Cakeday** spkane31! ^(hug)
Do you think it's an actual ergonomics thing or just a learning curve of doing things differently than with requests? If you think the cookie management is a weak point of the package, it'd be worthwhile to open an issue describing your workflow and why it's difficult with the current cookie management. If you're happy with your current solution using pycall, there's of course almost no reason to switch, and I don't think anyone would recommend you do such a pointless task.
I did try, and iirc the documentation focuses on the server. Single Method actions were easy but managing sessions and updating client cookies got hairy quickly and the documentation didn't deal with that, I found myself trying to make sense of the source code but it is spread across too many modules to grok. It just wasn't worth the work to get what I had already. Maybe one day I will return to it, it shouldn't be so hard. I wrote one in shell script! https://plan9.io/sources/contrib/maht/rc/httplib.rc
I went ahead and opened https://github.com/JuliaWeb/HTTP.jl/issues/398. If you get a chance to go back and review the specifics of why you had trouble with it, I'm sure the maintainers would appreciate a comment on that issue with more details.
That's a good idea and I will follow it up. 
Don't think so. If you find one, let us know... If you write one, let us know. But the only package that can do moving 2D/3D graphics with cameras and stuff is probably Makie...
Makie, I'll check it out. 
I'd say both codes are suboptimal. The "clever" code always sums all the numbers even when we could get away with just looking at the first couple of numbers.
I'm reading the official documentation instead, which is surprisingly well-written and clear. Then I'll go through Flux's examples and probably read most of Flux's source code.
Something like this? [https://github.com/ajkeller34/Unitful.jl](https://github.com/ajkeller34/Unitful.jl)
It's already in there. Check out https://github.com/ajkeller34/Unitful.jl
There is a library for this: https://ajkeller34.github.io/Unitful.jl/stable/ If you need, it is possible to define other units and if you need astrophysical units look at https://github.com/JuliaAstro/UnitfulAstro.jl 
that's the spirit, man!
I think you're running this in the REPL, which is special function xscope() x = ones(1) for i in 1:1 y = sum(x) x = ones(1) end y I don't have a REPL available so I cant test it right now
You may find some people interested on the Slack channel [https://julialang.slack.com/](https://julialang.slack.com/) &amp;#x200B; &amp;#x200B;
Repl behaves wierdly with global variables. you could pu everything in a function and run it, or my favourite, put it in a let block (let ... end) whih is basically a code block. Let blocks in general are quite nice and you should check them out. I think its also possible to write "global x" or something inside the for loop and it should work
Ah! You are correct! Thank you so much for the help. Is there any workaround though? I do scientific programming and like being able to "step through" my functions easily using the Run Block function of juno to double check that my code is doing the computations I think it is. It's a bit annoying to no longer have that option.
There is a long discussion about it if you care: https://discourse.julialang.org/t/new-scope-solution/16707/227 In which they made an heuristic for the REPL in a PR: https://github.com/JuliaLang/julia/pull/30843#issuecomment-458379049 In that version, since you read a variable before writing in it, it's automatically considered global. As it is now you should use global.
The discussion is helpful, thanks. For now, I think popping in let blocks where I need them is a decent enough workaround. 
There are many GitHub projects with open issues. That's always a great way to contribute.
If you know that this is supposed to be a diagonal matrix, then probably the best way is to wrap it in a `Diagonal` wrapper. Then it will behave like a diagonal matrix, without having to copy data or overwrite the old. So like this: using LinearAlgebra D = Diagonal(A) This also has the advantage that subsequent calculations may have implementations optimized for diagonal matrices. Otherwise, you can take a look at `diagind` in `LinearAlgebra` to find a solution, or perhaps consider setting values below a certain threshold to zero: A[abs.(A) .&lt; thres] .= 0 This would contain a safety in that too large off-diagonal values are not erroneously zeroed. But if you know for certain that the result should be structurally diagonal, go with the first solution.