Printing to a PTY should be thread safe in 1.2 and most IO should be thread safe in 1.3.
Why not extend the Nanosoldier package instead of making something from scratch? &gt; Even though highly inspired by Nanosoldier, this has a completely unique workflow. The workflow looks pretty much the same.
This was easier. We asked about Nanosoldier and couldn't figure out how to make it trigger our Gitlab runners.
I thought that Julia should be faster than Haskell when running code. But it is actually the same speed. I specifically was interested in Julia for fast scripting of dynamic code (not precompiled). Is there a faster way to run this one liner?
how about running it again?
What you're measuring here is compilation time + runtime: Julia is just in time compiled. For very short running programs, compilation time dominates a lot. In Haskell you should compare this to running ghc + program execution
Just a FYI, you can use `BenchmarkTools` and `@btime` to more accurately benchmark code (e.g., benchmark without the JIT compile costs.)
In Haskell there is "runghc" which compiles a program, then runs it. So yes, I compared to that way of running Haskell.
Yes, I am more interested in running via Interpreter if that is working. I mean ideally one would have a JIT that runs slow, then optimizes if performance is needed.
I want to run code on the fly, like "runghc" and hugs does. Haskell compile time is so slow, so I thought Julia is faster. But Julia is more a compiled language, so even slower than Haskell compilation times. Python does the same thing in 0.021 seconds instead of 0.3 seconds (totally different languages anyways).
`@btime` works by averaging over multiple runs. So the first time JIT cost is included, just amortized. Play around with the interpreter, see how it feels. The `time to first plot` problem is indeed a problem, and people are working on it, but it mainly binds in the context of using packages with expensive function calls. Not so much for base ops.
Julia is particularly bad for short command line tool, precisely because of loading and compilation times. If you plan to use it like that maybe look for another language.
Python interprets. If I run in Atom: @time A=[1,2];print(A) I get 0.000004 seconds (6 allocations: 304 bytes) [1, 2]
Ok. It may be that Julia is just slower for a very small program like this. It's not exactly the typical program people write in Julia: usually these are long-running (often numerical) programs where startup / compile time is insignificant.
well, if you want speed on first run, your options are limited. there are precompile options with julia, but it is way more complex. julia shines when you do lengthy computations. when the algorithm runs for 15 seconds, the 2 seconds compile time is acceptable.
What is positive about Julia is the startup time seems once per command line. Consider this: `eval(Meta.parse("A=[1,2];print(A)"))` `eval(Meta.parse("B=[2,3];print(B)"))` `eval(Meta.parse("C=[3,4];print(C)"))` Using Haskell, this takes 3x times longer than in Julia. Because each "eval" call in Haskell takes like 0.3 seconds.
good job I don't write programs to just print a byte
Use fezzik.jl to compile a new sysimage.
This ray? https://github.com/ray-project/ray/blob/master/README.rst Hadn't heard of it but looks neat. Would also be interested in hearing about this
Flux is great imho, but its immaturity as a package keeps popping up. Most of the functions are still undocumented, there's little in the way of tutorials, and I personally dislike some design decisions (like the Flux.train! loop). As long as it lacks a large amount of people working on it, which Tensorflow and Pytorch have, it will be restricted. &amp;#x200B; That being said, it's a very well done library, and the ideas Mike Innes and others have outlined in recent blog posts (check the [Flux.ml](https://Flux.ml) site) on differentiable programming etc. are very nice. I look forward to it picking up more steam.
I'm not familiar with GHC internals, but Julia compilation might be slower because it is "compiled" twice, from Julia to LLVM IR and from LLVM to Machine Code, and the LLVM isn't built for interpreted languages so compilation speed is not really number one priority (and some other languages that use it also has some notoriously slow compilation speed such as Rust). And Julia side of the compilation is also quite complex (multi-stage compilation with user defined hooks at multiple levels, plus type inference and huge combination of methods due to heavy use of multiple dispatch). The JIT you're referring to are tracing JIT, which profile code as they are interpreted and finds hot loops to compile. It's one strategy, not necessarily the ideal for every situation. Traditional JITs just compile the entire method as you call them. And then there is the Julia JIT, which will compile the method you call, then every method that this method call recursively until there is nothing more to compile, and then run it.
Most of the base features can be used through the Distributed module, maybe with an additional Clustermanager.
I'm not sure if it's also you asking here, but if not there is a comparison with Dagger.jl here: https://discourse.julialang.org/t/python-ray-project-alternative-in-julia/25811
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/artificial] [Is Flux better than PyTorch? Go](https://www.reddit.com/r/artificial/comments/c851gs/is_flux_better_than_pytorch_go/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Thanks! I've been using Elixir at work so I'm used to println debugging thread-safely! Of course, numerical performance in BEAM is not good (right tools for the right job, and all that). One side note, you can write a "safe println" (which I did) by adding those locks as part of a function, but if you call something like `safe\_println("foo $bar")` you can still get a segfault (or at least I seemed to have) due to the memory operations of the string interpolation. I had to reach for using the splat operator: ``` function safe_println(v...) global printlock lock(printlock) println(v...) unlock(printlock) end ```
What? Are you sure? I very much have the impression that compile time is excluded. I cannot find this stated explicitly in the manual, but it appears implied. At least allocations used by the compiler is *not* included, since you frequently get memory/allocs estimates of 0Bytes/0Bytes. Including compilation overhead would be counterproductive and unexpected, imho.
This is *exactly* the worst use-case for Julia: short scripts run from the terminal. You will not be happy with performance in this scenario, and I don't think Julia is the right choice, not now. Development of pre-compilation tools and an interpreter will likely improve this in the future, but for now, running code from the REPL (if possible) gives a much better experience.
I would very very much want to see some benchmarks with e.g. deep convnets to see how the JIT compares to pytorch + cudnn. Most likely not a fair conparison, but still...
`Flux.train!` is optional, and kind of a disincentivized leftover. You can just design your own train loops (a few lines of code), and I think most are encouraging new users to do exactly that. A deprecation warning might follow, who knows.
&gt; As long as it lacks a large amount of people working on it, which Tensorflow and Pytorch have, it will be restricted. This would require a large company to adopt it, because it's almost purely academic right now, so it's surviving off grants. If Flux gains some traction on the academic end, though, it might break into mainstream applicationg. Julia is a hot-topic in numerical sciences, it could attract these people and dominate the niche of numerical approaches in neural networks (`DiffEqFlux.jl`, for example). Also, due to Julia's design, Flux is well suited to take a leading role in the development of Differentiable Programming.
Look at the section [here](https://github.com/JuliaCI/BenchmarkTools.jl/blob/bbf8f544b4ac081af0ee27423c80828eb8589957/doc/reference.md) on `warmup`.
Knet has some benchmarks, performances seems very comparable: http://denizyuret.github.io/Knet.jl/latest/tutorial/#Benchmarks-1 I don't know how Flux compares to Knet though.
People should also have a look at Knet, it's more low level but it's great, the tutorials in particular are awesome, e.g. https://denizyuret.github.io/Knet.jl/latest/cnn/
Julia lacks a central repository. Installing packages from github makes it impossible to use it at company level due to obvious security issues. Until there is something comparable to CRAN or PyPi, it won‚Äôt be used at scale in a company (source : been there)
The warmup section seems to say that JIT overhead *isn't* (normally) incorporated in the results, not that it's amortized.
I'm not sure why github is less secure than CRAN or PyPi or any other repository?
Right, I edited my comment above. Although, if your benchmark is set up to use 1 evaluation (1 sample, with 1 eval/sample), as can sometimes be the case for benchmarking expensive calls, then you are explicitly paying this cost. But you're right, it's not amortized/averaged.
It‚Äôs less secured because you can‚Äôt easily open download julia repos while still preventing other github downloads. In the other hand you can easily setup a pypi / cran / conda forge / docker hub mirror with something like Artifactory. It downloads packages to a company repo, analyzes and approve them, and then makes everything available inside the company firewall with a conventional pip install. I don‚Äôt think it‚Äôs possible in Julia right now. If it is I would like to know! I am not saying that‚Äôs a bug, but it sure prevents my employer from using it
&gt;It downloads packages to a company repo, analyzes and approve them, and then makes everything available inside the company firewall I'm fortunate to not be working at a large corporation behind a firewall any longer (what a pain in the ass those firewalls were), but even when I was working at the megacorp I was able to download Julia packages from github within Julia's package manager. IIRC it was a matter of setting the http\_proxy environment variable. What does this "analyze and approve" mean? Is the analysis automated? &gt;It‚Äôs less secured because you can‚Äôt easily open download julia repos while still preventing other github downloads. I'm not entirely sure what this means. Why would you want to prevent some github downloads? As I said above, I worked at a large tech mega corp and there wasn't a problem with downloading anything from github that you wanted to download.
It depends on your work environment. I need to be able to work in virtual machines containing highly sensitive (aerospace) data, I can‚Äôt simply download form github. If I am on my laptop sure, I switch to a public wifi, download anything I want and take responsibility. But unfortunately that‚Äôs not enough to make Julia work at scale in our production environments. Don‚Äôt get me wrong, I‚Äôd love to use Julia at work if I can find a way to make it work I am not 100% certain of the verification part of artifactory, but basically it‚Äôs a local copy of repositories that are deemed safe. We can‚Äôt set it up to pull from github.
Multiple dispatch. Native AST and type systems have been done before (in languages like LISPs, for example), but as far as I'm aware no language has ever implemented multiple dispatch quite like Julia has and it makes for some really cool design patterns (such as [Holy Traits](https://github.com/JuliaLang/julia/issues/2345#issuecomment-54537633)). As an aside does anyone know if there's a decent write up anywhere about Holy Traits, I can't seem to find one (which is weird since it's such a fundamental part of the standard library now).
x) It has multiple dispatch, which is pretty unique for languages. x) C# has complete introspection both at compile time and runtime as do the LISPs of course. x) It generates type specific code for functions. So, in your example function f(x) 2*x end Would generate a different function for ints and for floats (unless I misunderstood what I was reading).
CLOS has multiple dispatch, but I agree its super rare.
A few things: 1. I think it's a mistake to look for some kind of "killer app." Maybe you're asking instead about overarching design decisions or philosophies or other choices. But when you say: &gt; what does julia offer ? what's is its key feature ? it sounds like you're buying a toaster. 2. I'm not sure that the snippet of code you gave actually runs. But, you could be talking about _generic programming_ (that is, writing "type-agnostic" code which "just works" for a variety of inputs; e.g., regardless of whether `x` is a matrix or a scalar.) 3. The closest thing to a recurring motif that Julia has is probably (as has been said by others) _multiple dispatch._ Everything else (the ability to view the AST, the fact that types are organized into a sensible hierarchy, the fact that you can define your own types and place them inside that hierarchy, the fact that the language is written largely in itself, the fact that it's JIT compiled ...) is either icing on the cake or intricately bound up with the decision to use multiple dispatch. Loosely speaking, multiple dispatch says that the behavior of a call `f(x, y, z)` depends on the types of the inputs. _All_ the inputs, not just (say) the first one (as with Java.) This means we can have, for ex., one method for `1 + 1`, and another for `1.0 + 1.0`, and another for `1.0 + 1`, and another for `1.0 + 1//1`, and... you see where this is going.
&gt; As an aside does anyone know if there's a decent write up anywhere about Holy Traits, I can't seem to find one (which is weird since it's such a fundamental part of the standard library now). No, and I would be interested as well. I went looking for something at one point expecting a nice part by Chris at least, but never found something.
Sorry I wasn't being very clear... I meant actually coding GPU kernels by leveraging Julia's JIT compiler as in e.g. here: [https://mikeinnes.github.io/2017/08/24/cudanative.html](https://mikeinnes.github.io/2017/08/24/cudanative.html) &amp;#x200B; It would be really great to write whatever RNN architecture or weird convolutional layer as a for loop and just have that compiled into a fast CUDA kernel. &amp;#x200B; PyTorch's torchscript tries to do something like that ([this](https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/) is a very nice post) , but that's basically just another hack on top of Python I guess...
i would say it is an intersection of multiple dispatch, JIT compilation, and duck typing. the result is an extremely flexible, natural but fast language. compile times are a bitch though, and precompiling is not possible in general.
The "essential" of Julia is it's goal: "We want a language that‚Äôs open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that‚Äôs homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled." https://julialang.org/blog/2012/02/why-we-created-julia All the combination of features we see today (multiple dispatch, macros, broadcast, the JIT, the base library) are following this vision. As a personal answer, what got me into the language was it's amazing extensibility shown in: https://docs.google.com/presentation/d/1IiBLVU5Pj-48vzEMnuYEr9WQy9u2oi6Yk_8F3sgGkvQ/preview#slide=id.p
This has them in there: &amp;#x200B; [http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/](http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/) &amp;#x200B; Essentially you just dispatch in two parts. Put type information into a type through a function, like \`has\_A(::MyType) = HasA()\`. And now do: &amp;#x200B; \`\`\` function depends\_on\_A(x::MyType) if has\_A(x) \_dispatch1(x) else \_dispatch2(x) end \`\`\` &amp;#x200B; Notice that since this is all type information, depends\_on\_A(x) will compile to be the same as \`\_dispatch1(x)\` with all other checks gone, so there's no efficiency loss by doing this. That means that any compile-time resolvable function can be used as a form of dispatch (and a macro like in SimpleTraits.jl can make this patterning easier). &amp;#x200B; So for example, if you want to dispatch (at compile-time, no dynamic dispatching) on 64-bitness of a number representation, you can do something like &amp;#x200B; \`\`\` is\_64bit(x) = false is\_64bit(x::Float64) = true is\_64bit(x::Int64) = true f(x) = is\_64bit(x) ? \_64bitf(x) : \_non64bitf(x) \_64bitf(x) = 2x \_non64bitf(x) = 3x \`\`\` &amp;#x200B; which compiles to \`2x\` if \`x\` is a 64 bit number, or \`3x\` if it's not. The interesting thing about trait dispatch is notice that anyone can extend it. So you make your own number type in your package and notice that the default \`false\` isn't the right thing to do, so in your package you add: &amp;#x200B; \`\`\` is\_64bit(x::MyNumber) = true \`\`\` &amp;#x200B; and now it takes that other path. So people can just write generic codes, querying information about types, compile specialized versions based on said type information, and let users who make new types add dispatches to make their type take the most optimized (or correct) path through a generic program.
&gt;\[Note: the original THTT required that you used types, like \`is\_64bit(x) = Is64Bit()\`, to propagate the information. However, with v1.0 we have constant propagation, so these compile-time constants will propagate and be used by the compile very reliably, and you can double check the outputted LLVM IR and see that these all resolve during compile time.\] This is super interesting - I definitely looked at this last pre-1.0 and that bit of it seemed more complicated than seemed worth it at the time (and I was really confused by your post up until reading this :-D). This is much simpler, thanks for the explanation! And tagging [u/seamsay](https://www.reddit.com/user/seamsay/) so they see it too. PS - your code blocks aren't working. I'm guessing you're using the new fancy-pants editor but expecting the old markdown editor. This has been screwing me up for a while.
I like this!
Thanks :D
Oh, like how C++ does it. Thanks for the description.
The syntax would look something like this `foo(x::Vararg{AbstractNetwork,N} where N)` As an example: function bar(x::Vararg{Real,N} where N) return sum(x) end gives the following: bar(1,2.0,4//2) # 5.0 bar(1,2.0,1.0im) # Gives MethodError
Brilliant! Thanks a million!
This or `bar(networks::AbstractNetwork...)`
Is there a preference between the two, this looks cleaner for sure.
Which one is supposed to be more correct? And how does Julia compare to Python?
Verlet, the main benefit of Euler is that it's really easy to implement and quick to compute. Well it depends. We decided to not use numpy which gives julia a great advantage in terms of speed as well as simplicity (we implemented a Vector class ourselves and used lots of loops/iteration in Python.). You really do notice that this is the kind of thing Julia is made for. Had we used numpy I think they would've been kind of similar with the Julia code being a bit nicer to read. As for animations: Not really comparable because python is a live plot and Julia saves as gif but Julia seems really slow here. Apart from that: i fucking hate both docs as well as error messages in julia - doesn't even come close to python. Docstrings are nicer in Julia because of markdown, unittests aren't great with either language imo. I also noticed that a lot of the fancy features of Julia lend themselves to producing bugs.
I was wondering about the `Flux.train!` thing too. Do you know if, when writing your own training loop, can you still use the pre-defined Adam optimizer on the model parameters, like how in torch you explicitly define the optimizer, call backward() on the loss, and then call step()?
The label [good first issue](https://github.com/JuliaLang/julia/labels/good%20first%20issue) in the Julia Github repository is one possible place to go.
I think instead of `good-first-patch` you want [`good first issue`](https://github.com/JuliaLang/julia/labels/good%20first%20issue) label in the Julia Github repository is one possible place to start.
&gt; i fucking hate both docs as well as error messages in julia - doesn't even come close to python. Docstrings are nicer in Julia because of markdown, unittests aren't great with either language imo. I also noticed that a lot of the fancy features of Julia lend themselves to producing bugs. Upvote for that.
What do you find limiting about python that Julia makes easier?
Building complex heterogeneous performant models. Building libraries around them that allow others to easily build them. Have all of this code work seamlessly with the rest of the ecosystem.
Another thing you can do is [edit the manual](https://julialang.slack.com/archives/C67TK21LJ/p1562200677423500).
It's a small package I've made a while ago, it's not super complete but it can still be interesting. The idea is to test "real-life" tasks instead of micro-benchmarks. Plus it runs on travis.
Can I just ask what the status of bifurcation diagrams is? Does it come as part of DifferentialEquations.jl these days? I've been struggling to get [PyDSTool](https://github.com/JuliaDiffEq/PyDSTool.jl) working.
You can use DiffEq's stuff with Bifurcations.jl: [https://tkf.github.io/Bifurcations.jl/dev/api/](https://tkf.github.io/Bifurcations.jl/dev/api/) we should update the docs on that.
Nice benchmarks! Interesting to see how optimized R's sort function is. Tells you a lot about the priorities haha
The data.table package creator contributed their very optimized radix sort to base R at some point.
Great thanks, I'll give that a go!
The vectorized pdf functions are also pretty fast, but there's no vectorized multinomial. So it's fast until you hit one of the corner case that isn't.
You should use data.table for R instead of data.frame.
[https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl/blob/master/src/Benchmarks/DataFrames/DataFrames/DataFrames.R](https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl/blob/master/src/Benchmarks/DataFrames/DataFrames/DataFrames.R) &amp;#x200B; I don¬¥t think this is a fair representation for R - no one in their right mind will be using read.csv to read anything that depends on speed. fread from data.table is the thing everyone uses.
Strange that Julia's `sleep` function is so much off. Is the timing accurate?
Neat with the Genomics one. I'm impressed with the R code! It looks like `BAM.quality` is poorly implemented in BioAlignments.jl (have a look!). Switching that function out with a better one, it takes about 18% less time.
I'm sure plenty of people (e.g. me) uses it, since that's often what you find in tutorials online, but yes I should use data.table.
It doesn't seem very precise: julia&gt; @benchmark sleep(0.01) BenchmarkTools.Trial: memory estimate: 192 bytes allocs estimate: 5 -------------- minimum time: 10.811 ms (0.00% GC) median time: 11.728 ms (0.00% GC) mean time: 15.160 ms (0.00% GC) maximum time: 20.337 ms (0.00% GC) -------------- samples: 329 evals/sample: 1
What problem does `BAM.quality` has ? I'm using it a lot so I'd love to know ;)
It uses that weird (slow) list comprehension to return Int8s rather than UInt8, but because the return value of the function is annotated to Vector{UInt8}, it then converts it back to UInt8 again
File an issue in the repo.
What operating system?
Strang is just awesome.
Windows 7. Maybe it's better on other systems.
I don't remember the exact discussions, but yes, I think timing on windows is a challenge.
According to the plugin page, it had Julia REPL support already. https://iodide-project.github.io/docs/language_plugins/ I haven't tried it but that seems to suggest you can pipe text in and out and process it with Julia.
Interesting! Julia working in Browser would be nice.
It's even worse on MacOS. Unfortunately, I cannot provide the results from where I am now.
It'll be nice to do make browser-based of biophysical and pharmacological models. That's a domain where many of the users of such an application might not be technically included, but the underlying mathematics is parameter fitting on differential equations, so we want to mix the "easy web" with "fast differential equations", i.e. Julia + Web browser :).
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rfelectronics] [Marconi.jl : A Julia Library for RF\/Microwave Engineering](https://www.reddit.com/r/rfelectronics/comments/cbjetr/marconijl_a_julia_library_for_rfmicrowave/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
What a flashback to the times of my studies. We had a similar library back then, made for HP41 calculators. We had 2 port, microstrip and other stuff I forgot in the past 20 years in it. From time to time I google to see if I find an equally powerful RF library for modern use but not too much shows up. Which is surprising to me as there is still a yearly wave of students who have to learn all that stuff and it is a good problem domain to learn a new programming language, for example.
Yeah right now most of the library is plug and chug formulas, but I hope to fully leverage Julia for non-linear analysis and advanced calibration routines soon. If you have anything that you'd like to see in this, let me know!
nice logo! :)
Thanks! My GF helped with her sick Illustrator skills.
For robotics there is [https://github.com/JuliaRobotics](https://github.com/JuliaRobotics) , with RigidBodyDynamics and RigidBodySim. There was a really nice talk at the last JuliaCon on this: [https://www.youtube.com/watch?v=dmWQtI3DFFo](https://www.youtube.com/watch?v=dmWQtI3DFFo)
There is a ROS repo. I thinks its just ROS1 though.
I‚Äôm thinking less ‚Äúwrapper around another language‚Äôs library‚Äù which isn‚Äôt of much use if one isn‚Äôt doing a big part of the project in Julia already and more ‚Äúground up all-Julia‚Äù library :)
This looks promising, I hope they continue to add functionality. I‚Äôd really like a Julia OpenCV, though...
I did a lot of my PhD experiments in Julia. I was investigating ways to fiddle about with various low level tracking processes to improve performance in the presence of time-varying motion blur. The big advantage for Julia over python for me was that I could much about with how eg SIFT matching works without having to implement it in low-level C/C++, then wrap for python.
Hey. Just FYI, I'm much more active on Discourse than on reddit. A RigidBodySim.jl `GUI` can be constructed from a `MeshCatMechanisms.MechanismVisualizer` ([this constructor](http://www.juliarobotics.org/RigidBodySim.jl/latest/details/#RigidBodySim.Visualization.GUI)). The `MeshCatMechanisms.MechanismVisualizer` constructor will [call `MechanismGeometries.visual_elements`](https://github.com/JuliaRobotics/MeshCatMechanisms.jl/blob/dfbbc2d3aae837b5699e594073735b675a9b9ef2/src/visualizer.jl#L11-L21). So you can create a `MechanismVisualizer` first and then create a `RigidBodySim.Visualization.GUI` from it.
Thank you for your response! I ended up creating a visual model of the mechanism by using LightXML.jl. Do you know which filename extensions are supported to use on the .urdf file? I have only tried .obj this far, but does it support for .stl or .STEP for example?
I think the JuliaImages packages (see [https://juliaimages.org/](https://juliaimages.org/)) are probably the closest thing to a pure Julia OpenCV. I haven't used either much so I don't know what features are currently missing compared to OpenCV, but the JuliaImages code is generally of high quality.
I'm not convinced it'd make sense from a maintainability perspective to actually re-implement the whole ROS communication protocol in pure Julia, but using e.g. CXXWrap.jl or CXX.jl to call roscpp instead of using PyCall.jl to call rospy for better efficiency would be nice. For fast message passing needs in the JuliaRobotics packages we currently use LCMCore.jl (bindings to LCM, [https://github.com/lcm-proj/lcm](https://github.com/lcm-proj/lcm)). This is a much lighter weight solution than ROS, but of course it's not helpful if you're already in ROS land.
I have nothing more than a very casual, lay interest in robotics (I almost didn‚Äôt click on the link), but that was excellent. Glad I watched it.
These versions mean the package version numbers. I don't know how to solve it, but have seen a plenty of issue like yours on discourse, try to search for it. These posts often link to one discourse thread, althought I couldn't find it now.
I think ] update will resolve it
What I‚Äôm worried about is that without a large ecosystem no one would really be using my Julia code. When I write research code the first motivation is to quickly and efficiently do experiments, but a close second is the dissemination of the code to the research community along with the research paper describing it. This is why people appreciate (and use and cite) work more often is (Python) code is available, doing all your research in Matlab for example (I still see papers using MatConvNet...) these days is a recipe for most of the community ignoring it or at least not building off it.
Especially these days
Currently just `.obj`, but support for `.stl` and `.dae` is almost ready (see https://github.com/rdeits/meshcat/pull/48, https://github.com/rdeits/MeshCat.jl/pull/104, https://github.com/JuliaRobotics/MeshCatMechanisms.jl/pull/32). Three.js itself (the framework underlying meshcat) doesn't have a `.STEP` loader, I believe, so that's farther in the future. In the mean time, I've always been pretty happy with meshlab as a tool to convert meshes from one format to another.
&gt; Anyways, it's already been proven that the code that was benchmarked was bad because someone wrote canonical Julia code and in some cases it was about 13x faster. Please show some URI so we can see what "someone wrote". &gt; So this is a good repo to show how far off the mark the original benchmarks were: https://github.com/KristofferC/BenchmarksGame.jl Do you mean the programs in that repo are *better* ? If so, [the authors are free to contribute them](https://github.com/KristofferC/BenchmarksGame.jl/issues/37) to the benchmarks game for measurement.
The Julia programs on the benchmarks game website have been [updated several times](https://salsa.debian.org/groups/benchmarksgame-team/-/issues?scope=all&amp;utf8=%E2%9C%93&amp;state=all&amp;search=julia).
\&gt; Do you mean the programs in that repo are *better* ? &amp;#x200B; Yes, they are simpler, written in a more standard Julian style and quite a bit faster. I am not sure of an axis where they aren't better. &amp;#x200B; \&gt; If so, [the authors are free to contribute them](https://github.com/KristofferC/BenchmarksGame.jl/issues/37) to the benchmarks game for measurement. &amp;#x200B; How do they get added? I don't know the process to suggest to them how to do it.
]Please contribute your better programs.](https://salsa.debian.org/benchmarksgame-team/benchmarksgame/blob/master/README.md)
&gt; How do they get added? https://discourse.julialang.org/t/julia-programs-now-shown-on-benchmarks-game-website/17722
That looks interesting but it doesn't seem to be very actively developed. I suppose it would take a large organization to adopt Julia for CV applications before the ecosystem really got started.
Indeed the timing looks much better than at the beginning, on par with Fortran in most cases.
I tried to understand these messages many times but they never made much sense. Last time I had one I ]dev the package and it just worked‚Ñ¢. Probably not the best practice though.
unfortunately \`\] update\` and \`\] resolve\` didn't work. I would just like to understand the problem but those error messages are the worst error messages in julia IMO.
``using Pkg`` ``Pkg.update()`` Worked for me. Maybe download the newest binaries
&gt;using Pkg; &gt; &gt; &gt; &gt;Pkg.update() interesting. I used my julia 1.2.0-rc1 installation ( I think rc2 is out) and it works. Weird. Thanks
Why are you working with v1.2.0...
And yet, you can see from the run logs (at the bottom of the source code pages) that most of *those Julia programs predate the comments* in this discussion.
V1.1 didn't work üòÇ
Thanks, I'll relay this back to the authors of the repo and hopefully we'll get this updated.
Most likely, with the new julia version you didn't have ColorVectorSpace restricted to version 0.7.0. You should be able to confirm that with `] st`, my guess is that the version is 0.6.0 on julia 1.2.0.
I think that's a sound motivation. Lots of research code is built in the manner you aspire to, and forms part of larger ecosystems. It's worth thinking about the maintainence of the code you create, too. One option with your research code is to build a reliable, stand-alone demonstrator which can be built and run from first principles, by anyone. There are tools for doing this sort of thing, like docker, or the various configure-my-linux systems such as ansible. This sort of thing should require relatively little maintenance. You'll have to version-pin your dependencies, or update your code as APIs change. Another option is to find an existing open-source codebase, like opencv, and creating your research work as an extension to that codebase. In this case, someone else will do the maintenance for you, assuming you can convince them to take the pull request. Using an existing codebase does place restrictions on the kind of things you can do (see my earlier example.) If the community you're part of has an existing body of work in python, and the way your work will integrate is relaitvely clear, then you can probably just write some python modules and share them on github etc, but you'll still have to maintain things as the field moves on, if your work continue to be used. For my work, it didn't really make sense to integrate with opencv or numpy, because the problems were relaitvely niche. If my experiemnts had been more successful, I probably would have been re-implementing the algorithms into my sponsor's proprietary tracking systems. I guess the only person who can really answer your question is you. Think carefully about your requirements, the software ecosystem you're working in, and what's going to happen to your code once you stop working on it. Ask questions about those things and you'll work out what's best for you.
We just released new package versions with the \`.stl\` and \`.dae\` support.
Documentation
Just going with a little more detail, for a neutral and updated learning resource about the language itself, the manual is currently the best resource: https://docs.julialang.org/en/v1/index.html And for HTTP services, you could use for reference: https://github.com/JuliaWeb/HTTP.jl https://github.com/GenieFramework/Genie.jl/ https://github.com/wookay/Bukdu.jl
I ask the same question myself several times. I don't think Julia is ready for this, since it misses the port for a lot of the libraries used in robotics, thus making python/c++/matlab still a better choice. However I am sure the language is flexible enough for providing these libraries, we still miss the community interested in them. I started looking at the available options for porting OpenCV. As you will have realized by now, all the current implementation are not maintained anymore. We now have Cxx.jl working again, but I think using it is not the right solution. Instead, I would like to fork the current python generator (included in OpenCV itself) in order to generate the Julia binding. Given the very extensible support for metaprogramming in Julia, this should be a feasible (tough complex) task. But it can be more powerful (i.e. natively support Julia types and reduce the binding performance cost) and probably more maintainable. &amp;#x200B; My focus is now on [https://github.com/JuliaInterop/libcxxwrap-julia](https://github.com/JuliaInterop/libcxxwrap-julia). If someone is interested and wants to contribute to this solution, let's talk!
I'll do you one (or a few) better. We have to get individual *modules* approved as well, for each "major release number*. Oh, and anything that's version 0.* is considered beta and requires VP approval of business justification. You'll note that quite a few Julia packages still fall under "beta." Each distribution has to be separately approved, e.g. PyPy, SciPy, Cython, Jython and modules for each distribution require approval through manager, desktop support, cyber security, cyber research, and then counterintelligence. Oh, and separate approvals required for 32/64-bit OS, and for each of the various OS types: Windows 7, Windows 10, MacOS, and Linux. Also separate approvals for networked and non-networked machines. We got Atom denied because, "We already have three text editors approved (WordPad, Notepad, Notepad++). Use one of the approved editors instead." Upon request, I once had to write an approval for renewing Matlab licenses. Buyer had discovered that we could instead "just use FORTRAN, C/C++, Python, Perl, Excel, VBA, or Pascal" so I ended up writing a mini-thesis about why we needed to renew Matlab.
I need to save this to remember how lucky I am. I work at s startup and I'm pretty much the defacto software developer/data analyst automation expert, so I can just install anything I want at my discretion. Installed Julia on a whim a hapf a year or so back and have been playong around with whatever I can get to work. Sorry for your trouble with Juno (I love Juno myself), but congrats on getting Julia approved!
That's horrifying. I wonder, could you use Docker to circumvent these approval problems?
Yes
Tried that route, that requires official IT support to standup a server (if using a server based tool). Docker was rejected as a desktop top software due to a lack of (a) IT experience with Docker, and (b) trust, and (c) ability of Docker for circumventing the approval process.
And?
I install plain atom, then the Uber-juno package in atom and things work fine. Also in a corporate env by the way. The one thing that is stopping me for doing a full POC is the web stack. I want something like plotly dash.
Beaten to the punch! As a fun side-project I've been working on a Python pixel-universe, which has been a lot of fun (I've included collisions as well, but it's all Euler's method. Side note: I wasn't even aware there were different methods -- about to set off on a reading spree) . And, as a beginner to Julia, I thought it would be the perfect thing to try recreate in Julia. So this is awesome! &amp;#x200B; Also, thanks for actually documenting your code. It's still scarily rare these days.
Especially these days
Have you tried the Julia Pro version? That comes with Juno set up for you. However, you do need to ensure your machine has the right security certificates set up, along with the appropriate proxy settings in your atom .aprmc file.
Mind sharing your Python Code? I usually do python so it would be quite interesting to me :D Yeah that's also what my Buddy and I thought :D we did Python first and then tried to recreate it in Julia The docstrings with Julia are kinda cool - If you load the code into the repl and type `? function_name ` you get the docstring with rendered markdown. And yeah I used a lot of my old code later on and struggled with a lack of comments so I try to document at least the nontrivial stuff. The inbetween comments are by my buddy - I usually don't do a lot of those (got in the habit of mostly omitting them via PEP8 and honestly don't really miss them)
That was the next step. I decided against asking for JuliaPro for the first foray, so to speak, because it might have been a little more complicated in terms of getting vetted from a legal perspective (user/license agreements and all that). But I did mention it to my IT contacts, I just didn‚Äôt officially ask for it yet. For the record, I know nothing about security certificates, proxy settings, etc. All that stuff just frustrates me, and I usually don‚Äôt have to deal with it. I will have to get knowledgeable about it now I guess.
Thanks. Despite having to go through the approvals process, which was annoying, people have now at least heard of Julia and there is some excitement about it. Despite not being able to use it at work (I‚Äôve only been playing around with it at home, and love Juno too btw), I‚Äôve been banging the Julia drum pretty loud. It‚Äôs interesting really...it went from skepticism and resistance at first, to ‚Äúwhat do you need to get started?‚Äù I‚Äôm remaining cautiously optimistic.
Same here but at a gov facility... Though this year they started to require approval of each *minor* version. Also... with the insanity of having to go through an approval process for each module I tried getting JuliaPro approved. Yeah, right. :-/ Last week the approval came through! Totally shocked. So it should only be another month or two before it can be installed. (much better than the bad old days where the time from initial request to install was usually around 18 months)
Man, I hope so. Julia is just a fantastic language, and I think it offers a lot more than Python, but still as easy to get into as Python. Course, I also just hate Python for various reasons, so Im not exactly unbiased.
I would say no currently it has the potential to be but currently they need to have much better documentation and more support
There is quite a way to go before that's true, if it ever will be. My first experience with Julia: download and install V1.0. Follow the highest ranked example on Google for 'getting started'. Oops, none of the libraries I need to import are compatible with 1.0. And the much-hyped package manager can't seem to figure that out. Maybe, just maybe, don't hype your 1.0 release until there is a chance people other than core developers can do something useful with it. Next grab a tutorial. "Julia is terrific and lets you just write your code mathematically!. Before we get started, here's a tutorial on LaTex syntax for expressions." Jesus Christ! Dumping people in to LaTex as a pre-requisite is, frankly, insane. That was my experience. I haven't been back because everything I've read since then has been self-congratulatory bull that says the 1.0 launch was great.
Also: Error messages are terrible in Julia imo (not that they're great in Python, but they're better than Julia)
Depends on what you're doing. Python makes the simple thing easy, Julia is much easier once you get beyond the simple part. While it's undeniable that if you want to just open up the terminal and plot a simple function, Python has some really nice syntax and it's beautifully easy, to say that means Python is easy is obscuring how difficult it was to write that library in Python. For example, let's say you're solving nonlinear equations \`f(x)=0\` and you want to use a GPU, and notice the library isn't GPU compatible. How hard is it to get the library to work with the GPU? If you're using NLsolve.jl, you can bang a way and get a development prototype in an hour for something like this, just changing type signatures and removing a few checks that might be unnecessary on your problem. It might not be a release-ready version, but there you go, GPU-based trust region Newton. Now try doing that with SciPy... SciPy is C or Fortran code underneath the hood. You'd have to rewrite those to be CUDA kernels, and so you really can't reuse most of the existing code since it has calls to non-GPU SciPy routines... and at that point it's usually much easier to start from scratch. Another example is with code generation. Julia has macros for metaprogramming with, and Julia is represented in Julia. So you see code generation all over the place. One example is ModelingToolkit.jl, which can [automatically change a numerical function to a symbolic one](https://github.com/JuliaDiffEq/ModelingToolkit.jl/pull/146), then generate the analytical solution to the Jacobian, and then generate an ODE problem which uses this analytical solution. Building this is just standard usage of Julia types, and type information tells the system what is a dependent variable or parameter. It all works with syntax highlighting builds Julia functions from Julia. Etc. So the ModelingToolkit.jl library which writes this doesn't have to do fancy work. Now think about "metaprogramming" libraries in Python. First of all, since Python functions are slow, most of the ones that I know (PyDSTool, JITCode, etc.) don't build Python code, but actually build C code. PyDSTool is an example where you write your ODE definitions as strings: "dx/dt = alpha\*x\*y". PyDSTool then parses the string, tries to understand the context of the pieces, and then tries to build a C function for that ODE, and then interface with a Fortran ODE solver (radau). So not only does building PyDSTool in Python mean the authors lose get syntax highlighting or error checking (because all of the user code is writing strings), but it also means that they have to build a compiler and interface between 3 languages. Python library writers impress me all of the time with all of the fancy footwork they've done to make libraries work, but since I don't want to work that hard I use Julia. Another example is performance engineering. PyTorch wanted to have users of their library get fast code, so they [wrote a JIT compiler and a type system](https://www.youtube.com/watch?v=DBVLcgq2Eg0) as part of writing a neural network package. Meanwhile, Flux.jl [defines just a few functions](https://github.com/FluxML/Flux.jl/tree/master/src) and calls it a day, and still routinely [outperforms PyTorch](https://github.com/boathit/Benchmark-Flux-PyTorch) and [outperforms Tensorflow](https://www.juliabloggers.com/deep-learning-exploring-high-level-apis-of-knet-jl-and-flux-jl-in-comparison-to-tensorflow-keras-2/). What's Flux.jl's secret? Flux.jl uses the standard Julia compiler, and lets the compiler writers optimize the compiler. Most people don't write libraries, so most people won't care about these details. But the productivity difference for library writers is so large that over time I see more and more library developers moving to Julia (I moved to Julia for exactly this reason). So at least at the level of developing libraries, it's clear that Julia is easier to use than Python, which hopefully will translate to higher level usage over time as the tooling improves.
Ahhh, interesting. I was more talking about how it uses gradual typing, how a lot of similar libraries allow scientific computing, but didnt consider the sort of new developer experience. Coming from C++ Im used to crappy experiences in regards to that. Fairly good point though. Preleasing a pre-release to 1.0 for your library authors to make sure everything works.
&gt; Preleasing a pre-release to 1.0 for your library authors to make sure everything work They actually did that, but then didn't wait for the library ecosystem to catch up. Plus they didn't / still don't have the practice of correctly labelling the version requirements in libraries to avoid un-necessarily declaring a version incompatible.
Ahh, interesting. From my reading, it sounds like thats something Rust did correctly. Do you happen to know if thats true?
I haven't really followed Rust much, as I'm completely un-interested in 'a better C++'. The Rust folks certainly have their hype mechanism cranked up to 10, so that makes me a little hesitant to take the things I hear about it at face value.
All I can say is I downloaded Julia about a month ago, am by no means am an expert developer in python, but i had it up and running in probably less time than an anaconda/python download would take, was writing/running interesting scripts in a few minutes, and found lots of interesting tutorials from DifferentialEquations.jl and Flux.jl that worked and gave me insights into the syntax. Plots.jl worked well too. If I were just starting out I might of even thought it was easier than python to start messing around with.
That's nice, it's too bad it has taken most of a year for that to be the case. Plotting definitely didn't work out of the box in August of last year. I'm fairly sure the 'easier than anaconda download' point is true in any significant way.
Yeah if you follow some of the DifferentialEquations.jl tutorials they do plotting using Plots.jl and it worked for me. Seems quite comparable to matplotlib (im fact they have a matplotlib backend as an option). I was using the Juno IDE which comes with the Julia Pro download so I guess I would recommend that.
This. The documentation is really spotty and the package environment is still pretty rough - Julia doesn't have the kind of package-for-everything kind of environment that Python does yet - but in terms of raw mathematical processing it's way easier, and has a lot of potential for more general usage.
I'm always getting confused with how to run python code. I also never understood how to build a package/include files. So I would say Julia is easier, but maybe that's just me.
For me I think its a huge language win to be able to start your code in Julia, then as you need faster code, stay in Julia rather than moving to C. Which is similar to why I like C++ really.
Once Jetbrains creates an IDE as awesome for Julia, that will help a lot.
PyCharm is really amazing for sure. But.... 1. There is a Julia plug-in for IntelliJ now. I think it works with all JetBrains IDEs. I haven't tried it and it might still be a bit rough (eg you have to type a few things into your Julia prompt to connect the IDE to it), but I think it's supposed to have the basics including plotting support. 2. Atom is pretty awesome. Old school debugging support isn't anywhere near what any top IDE provides, but it's not a huge problem in practice due to other really nice aspects of doing Julia dev, incl. stuff like Revise.jl.
The 1.0 launch was really rushed and rough. I think the core team wanted to force package maintainers to support 1.0 ASAP by deprecating everything else. It might have been the fastest way to get package ecosystem updated, but there were significant sacrifices as a result, such as out of date examples, and nobody realizing the new scoping rules were a mess until after 1.0 was released. Since 1.1 I think it's been pretty good, with steady non breaking improvements since.
It's kinda like the rust plugin within clion/ intellij.... which is pretty decent, but there really isn't a lot of benefits of using the rust plugin vs using VIM with some plugins. I'm glad there are plugins so that I can at least get my nice syntax highlighting, search and replace features, and color scheme automatically, but it really isn't the same as having a full featured IDE which is specialized for a certain language. I'm not an atom fan also. Reaching the big leagues means there needs to be a IDE that is built specifically around the Julia paradigm.
The documentation of Julia itself ([https://docs.julialang.org/en/v1/](https://docs.julialang.org/en/v1/)) is actually super detailed (the PDF has like 1200 pages). Sometimes I wish there was a shorter manual for beginners... &amp;#x200B; It's true though that many Julia packages need better documentation.
Yeah, I agree. This has been recently discussed in [https://github.com/JuliaLang/julia/issues/32470](https://github.com/JuliaLang/julia/issues/32470). Suggestions are always welcome!
Yeah, the steady breakage in the 0.x era was annoying. Now it is quite stable (core-wise), but Julia packages still break a lot. I guess there will be the same issue again, when Julia 2.0 gets released.
The Julia documentation is super detailed (I've actually read the pages on methods and types several times from start-to-finish, as I would the book), but I think there are some pretty irritating flaws in how the documentation at-large is organized. One big bugbear for me as a new user is that I don't actually know what functions I can call on standard data types. For example, I'm working with a Vector{Float}. What functions are provided for me and which ones do I have to write myself? I know about `push!`...can I insert into the middle? Pop? Easily concatenate two arrays? Extend an existing array? As far as I know there's no one place where all this information can be looked up, and it really makes it a drag when you're trying to figure out if you need to write your own functions to do X or if there's something already written. In most OO languages, these methods would be bundled into the documentation of their classes, but since Julia dispatches on all members, it's not even clear whether there's a correct way to put all these functions in one place...
Give me a few hours to clean things up. I'll throw it on Github with a notebook in a day or so. I'll PM you and reply here. Re: Julia docstrings, they're super weird coming from Python (being written _outside_ the function), and getting to grips with multiple dispatch was super interesting as well (and something I've grown to love since).
I really like kenos suggestion. In general an orientation on the error messages of rust is a good idea since they're one of the things the language is usually praised for (we of course have the added difficulty of handling overloaded functions in Julia)
Sounds good :D Yep the fact that they're outside is a bit weird but it's that way in lots of other languages (Rust, Haskell, C#,...) and I actually prefer it. I haven't yet delved into multiple dispatch but it's essentially function overloading isn't it?
I doubt 2.0 will be as big of a change. 1.0 purposely flooded all breaking changes everyone could think of, because that was going to be the first stable version and so every single "I would like this syntax instead" issue was discussed. There will be some breaking changes in 2.0, but I doubt people will rename \`mul!\` or change \`Iterators\`.
Calling C,Fortran,C++ and even Python is so! Much easier in Julia. Really, Julia has its clunky parts sure but it's really an amazing language.
Julia's type system and polymorphic function binding damn feels like C++ though. I used to develop C++ libraries with template dark magic stuff, Julia feels like the bright side of those things.
That‚Äôs a troll. Check their comment history, immature jokes and questionable views on things in general. There‚Äôs apparently a sub called onewordanswers where people ask questions and get shitty one word answers. So I guess that‚Äôs an instance of that. It‚Äôs sad that the moderators of this sub aren‚Äôt diligent enough to remove low quality content like that. I know the core Julia community (a serious, intelligent, passionate group of people) mostly hangs out on Discourse and Github, but if it‚Äôs to have a presence on Reddit (/u/ChrisRackaukas is pretty active here, ffs) it needs to do a better job of filtering out trolls and providing help/advice to newcomers. Letting that shit slide isn‚Äôt good for the language and the community. Imagine being a complete newbie and coming across this kind of thing. It might turn someone completely off if they see enough of it. The moderators of subs like /r/badeconomics and /r/AskHistorians don‚Äôt stand for the kind of thing. BE once banned someone for commenting on a female politician‚Äôs physical appearance and then doubling down when confronted. If /r/Julia wants to be taken seriously and to appeal to the Reddit userbase that is interested in Julia, then it needs to step its game up.
Documentation is good but not good enough, IMHO. There should be more examples, and metaprogramming needs a lot more documentation and explanation. I find myself often looking at Julia libraries' code for lot of things which could have been documented but I have to find out myself. That being said, [discourse.julialang.org](https://discourse.julialang.org) is your best friend ;-)
That's a good point, and it's especially hard because of parametric types (for Vector you'd have to also look for methods in AbstractArray, and for Float it can be T&lt;:AbstractFloat or T&lt;:Number), since showing all of them in one place could make it too crowded and hard to reason which cover your specific concrete type - methodswith(Vector{Float64}, supertypes=true) has 761 entries with just Base imported. Though I think a browser based methodswith within the documentation would be pretty good when you have to look at an extensive list of functions that use your concrete type (which you can just ctrl-f to look for the functionality you want). Other libraries won't have nearly as many definitions as Base so I feel it's not as much of a problem compared to just being carefully written and the general issue of discoverability.
I have asked to be a moderator of the community since no one seems to be doing moderating... but I was denied because I am in the community (I can dig up the exact quote). So the moderators are not members of the Julia community, deny people from the Julia community from moderating, and have a preference for it to stay that way. I am not sure if that's normal for a subreddit because I am not a redditor, but that seems silly to me. Julia has [Community Stewards](https://julialang.org/community/stewards/) who manage the other forms of communication, but I am not sure how to formally ask Reddit to allow them on the moderating team. This has come up a few times when porn submissions caused the Julia News Twitter bot to spam the linked submissions, and the response times were quite slow, leading to requests to take down the bot... it's a problem.
Multiple dispatch and function overloading behave almost the same, but multiple dispatch happens at *run time* while function overloading happens at *compile time*, which allows your program to have really flexible and dynamic behaviour, like in [this simple example](https://stackoverflow.com/questions/1801216/what-is-the-difference-between-multiple-dispatch-and-method-overloading).
Yeah it does! :) Ive done my share of SFINAE and such (long before nice thingslike enable_if) and when I read what Julia would do, I got super excited about it. Jitting down polymorphic functions into fast type aware functions is pretty awesome. Doing all that automatically so you can quickly write code is even better.
Huh thought they were both runtime features because I had a mental connection between overloading and dynamic dispatch. TIL - thanks :D
&gt;There‚Äôs apparently a sub called onewordanswers where people ask questions and get shitty one word answers. So I guess that‚Äôs an instance of that. No, in this instance I gave a low effort response to a low effort question. This could easily just be googled and didn't need to be a thread. One could argue that flooding the subreddit with vapid threads is as damaging to the community as a troll, since people get annoyed and unsubscribe. OP could have given us more context about what exactly he's looking for, what he's trying to do, maybe where he's looked already, but didn't. So a poorly thought out question got a poorly thought out answer.
&gt; I gave a low effort response to a low effort question There‚Äôs really no excuse for low effort posts in serious subs. This is a place where people come to learn (and in one instance meme) and support each other. You provided nether of that. It doesn‚Äôt matter how high or low effort OP‚Äôs question is ‚Äî he or she deserves an answer if anyone has one. &gt; This could easily just be googled and didn't need to be a thread. One could argue that flooding the subreddit with vapid threads is as damaging to the community as a troll Julia is a relatively new language. Not everything is google-able. Going on Reddit or Discourse and posting a question related to your field of interest is completely justified. Even if there are resources available elsewhere, it‚Äôs still our imperative to help someone in need who is trying to become a member of the community. There is nothing wrong with ‚Äúvapid‚Äù threads ‚Äî someone new could still learn something. Your response is completely immature, judgmental, and unjustified.
&gt; but I was denied because I am in the community That‚Äôs one of the silliest things I‚Äôve ever read. Ever. I know about the Community Stewards, but they‚Äôre mostly a Discourse thing from what I gather. Stefan Karpinski and Viral Shah should try to reach out to the Reddit admins and see if they can extend they‚Äôre stewardship to here, along with you ‚Äî you‚Äôre clearly a Redditor now, whether you like it or not ;)
I understand the rationale, which was that since I am deep in the community I may be (and I am) biased. I can see their point, but I believe that the moderators should be the people who care so much that they are reading every post. Sooner or later one of the Stewards should become a moderator though, otherwise this subreddit should be very upfront about not being "official".
I just heard back from the main BE mod. Check out https://www.reddit.com/r/redditrequest/wiki/faq if you haven‚Äôt already. Not sure how helpful that‚Äôs going to be, but you should give it a shot. Honestly, that line of logic doesn‚Äôt make *that* much sense to me. Yes, you‚Äôre biased, sure, whatever, but we‚Äôre here to learn and support each other. The very nature of the sub is that it‚Äôs ‚Äúbiased‚Äù. If you‚Äôre dealing with Python vs Julia talk, then yea...coming to /r/Julia might not give you the most objective view. Neither will going to /r/Python. But that‚Äôs not why I‚Äôm here. I already decided that I‚Äôm going to learn Julia, so I‚Äôm here for Julia-focused help and discussion. And I really don‚Äôt want to see that kind of low-effort, troll nonsense. You are more than anyone here qualified to moderate this sub. You and a handful of others who tend to be quality posters/commenters. No offense to the current mod team, but if they have no interest in moderating this sub properly, they should the reins off to someone who has that desire. /rant
Lol so fragile, how do you get through the day? No one is getting hurt or offended by my original comment. Anyone (besides you, apparently) would think "well shit" and wait for a real answer, or ask somewhere else.
In Python, I can just put a breakpoint in Pycharm and debug my code. Wasn't able to do it with Julia so far in neither PyCharm nor Juno. It will surely be as easy to use as Python, it's just not as easy to use yet.
Does it work if you launch the julia repl? I.e. is it a julia problem or a vs code problem?
Yeah, Julia's documentation is pretty low level. Thorough perhaps, but not informative if you don't already know what any of it means. You need to have better than basic coding skills to really parse it. That limits Julia's utility as a *toolbox*. It's great, I'm sure, for people who are interested in making or porting libraries, but if just want to do an analysis or create a model of something, the how-do-I-do-X? type of documentation and, perhaps more importantly, archive of answered how-to questions, just doesn't exist for a lot of fields yet.
Updating my vscode worked
Actually, while \`.stl\` seems to be working fine, there are still some issues with \`.dae\` which should be resolved by [https://github.com/rdeits/meshcat/pull/58](https://github.com/rdeits/meshcat/pull/58).
Check out this [link](https://docs.julialang.org/en/v1/stdlib/Random/index.html). Use the `RandomDevice` RNG type and you should get access to the OS‚Äôs random number generator, which (if I understand your link correctly) is what Python‚Äôs `getrandom()` uses.
Cool!
Julia could do pretty good in this area, most current performance issues with Julia is with the JIT warm up (time to first plot), but with a long living process such a web service it will reach easily the point in which everything is already compiled so that's not really an issue. Outside of that, maybe just memory use, especially compared to small binaries like Go. And with the better multithreading in 1.3 you can manage the computational resources even better. And you get a lot of stuff already to use like JuliaDB for parallel data analysis and Flux/K.net for ML, which could replace more complex architecture like using spark/tensorflow cluster (though good support for those and other would be great to have as well). Outside of performance, I particularly feel Julia's metaprogramming/multiple dispatch can provide an environment just as good for other tasks like it current does for math. I really like Elixir's Ecto for database management, which effectively creates a clean and intuitive DSL for working with SQL (when compared to the object chaining approach from SQLAlchemy or the message passing OOP approach of ActiveRecord), and Julia has an even more powerful metaprogramming support (though Ecto/Phoenix got a lot more than just using a powerful language to get to this point). And having the option to get the query results directy into a dataframe/juliadb for powerful analytics in the native language (or clean DSLs) just as easily would give something extra that other frameworks can't easily replicate. I'm trying to find time and will power to do some serious work on the database scene on Julia (the most important aspect right now is some unified way of interfacing with databases, like Python's DB-API or R DBI), which unfortunately doesn't seem like a current large priority for the community (which can already do more complex analytics without the need of a sql database), but it's quite important in the web development/services scene.
I'm also keeping an eye on Genie.jl as a potential Django replacement, because of the reasons you and /u/fborda mentioned. I feel like Julia in general has a lot of promise in this area, but the right tools just need to be developed and fleshed out some more. &amp;#x200B; On a side-note, does anyone know how to get a Genie.jl project running inside a Docker container? I run all of my Django projects in Docker, but so far I haven't had success getting a Genie.jl project to work correctly.
Hi what would you say I should definitely use Julia for over Python/R?
You can set breakpoints in Juno with the new interpreter and debugger that was released in March. [https://julialang.org/blog/2019/03/debuggers](https://julialang.org/blog/2019/03/debuggers)
Probably everywhere where python is applied, which is great
If your objective is just doing data science work the fastest way possible, it's hard to compete with those much older and/or more popular languages that have almost everything already implemented (and in fast languages like C/C++/Fortran) and with books/stackoverflow answers for every little thing you would normally do. And Python in particular has vast support for web development unlike Julia right now. What Julia provides is being able to do everything you want with a single language. You don't need to avoid using the language native tools to use languages within languages to do anything without killing performance (like being forced to use the vectorized functions of numpy/panda/tensorflow instead of just using a python loop and data types, or tidyverse/data.tables for R). You also don't need to write in C/C++/Fortran and use the FFI when you really need top performance and your task wasn't already solved for you. And while "pure" Python can be made fast (though usually not as fast as Julia even restricting the language), the "simple" aspect is lost when you have to deal with multiple environments such as Cython and Numba. And Julia having all the environment written in Julia (and the paradigm of the language, based on one of the most powerful languages which is Common Lisp), gives Julia an amazing interoperability, where the ML library can interact with the Differential Equations Library, and with the custom Array libraries, and with the Graph library, and the DataFrame library, and with the code you create, creating something that is more than the sum of the parts. While in python for example, if you want to do differentiation with numpy you basically have to create a new numpy (like google jax).
Better than Python. Run as FCGI server you can use all the multi-core features. Not stress tested it with how many concurrent clients it can handle.
It makes a lot more sense than Python too. When you port Python to WASM you have... Python. Which is nice but not that useful tbh. Then you have to port all the libs by hand. When you port Julia you get 90% of the Julia ecosystem for free as it's just Julia.
Does it require any setup? I was trying to set some breakpoints and start the debugger, but all I ended up with was an error message mentioning missing signatures and suggesting me to run `using Revise`, but that did not help.
Your second example gives a matrix with uninitialized off-diagonal elements, which will give you nasty bugs. Use z = fill(0.0, x, x) instead. If you're interested, see this discussion: [https://discourse.julialang.org/t/eye-in-julia-0-7/9820](https://discourse.julialang.org/t/eye-in-julia-0-7/9820) &amp;#x200B; Also, I should add the disclaimer that it is very rare to need the in-memory dense identity matrix. Usually you can structure your code to only need I itself.
"for i in x" does not go from 1 to x. you need 1:x for that.
The second one doesn't really give a good result though: julia&gt; ident2(3) 3√ó3 Array{Float64,2}: 2.30567e-314 2.30567e-314 2.30568e-314 2.30567e-314 2.30567e-314 2.30587e-314 2.30567e-314 2.30567e-314 1.0
Thanks, I typed it wrong here, I'll edit. In my script I have 1:x.
Yea you're right, I made an edit in the post.
almost
Good point. With the fill command it brings it to about 32.464 ns which is still an improvement. Can you elaborate on your last point? For example, I need to make a series of computations where I subtract a dense float matrix from an identity matrix and I'm trying to optimize it.
&gt; I need to make a series of computations where I subtract a dense float matrix from an identity matrix and I'm trying to optimize it. You would just loop over the diagonal and subtract 1 from them. No need to materialize the identity matrix.
Excellent thanks! I need to get better at this sort of thing, I appreciate the help
You probably never need to explicitly create an identity matrix, it's a waste of memory. There's already an implicit one in LinearAlgebra: using LinearAlgebra rand(3,3) + I https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#The-uniform-scaling-operator-1
I think Julia also has a DiagonalMatrix type, wich functions as a mqtrix, but only stores the diagonal. Not entirely sure though and would look at the source code for it.
[https://discourse.julialang.org/t/julias-growth-is-making-free-juliabox-unsustainable/19441](https://discourse.julialang.org/t/julias-growth-is-making-free-juliabox-unsustainable/19441)
1. Loops are generally fast in Julia. 2. Have you profiled your actual code? There's almost no way creating identity matrices is your main performance problem. You're either doing something really trivial in which case performance doesn't really matter, or you've got bigger design issues with your code which creates way more identity matrices than you need.
I really appreciate the work thats being put into DifferentialEquations.jl with extra resources like this. Great job!
If you have a n x n Hermitian matrix, then it is positive semi-definite iff its eigenvalues are greater or equal to 0. A quick test would be to find the eigenvalues and test to see if they are all &gt;0.
Is that a fast and robust way? What's the complexity of calculating the eigenvalues of a Hermitian?
I'm assuming that you are interested in a numerical method. The usual way is to attempt to perform a Cholesky decomposition. This will work if and only if the matrix is positive definite. Julia has functions for doing that.
Yes posdef checks whether a matrix is positive definite via the cholesky decomposition. But I need positive semi-definite.
Next on the TODO list is to provide a ready made Docker container for Genie with the various dependencies - so this will provide an opportunity to dive into the issue and have an "officially supported" Docker setup. But till then, can you please open an issue on GitHub and let me know what the problem is? Thanks
It's O(n^3), which is probably not what you're looking for. It's much much easier to calculate the largest or the smallest eigenvalue using the power method. I believe that's what `eigmin` uses, but I would check the documentation.
In that case there may be no way around doing an eigen-decomposition and checking that all eigenvalues are &gt;= 0.
 This would be sweet! Having docker makes testing stuff out a lot easier (&amp; reporting bugs easier).
 Is this something you're hypothesising about or something that is actually supported? I thought proper stable multithreading was coming in julia 1.3?
 I'd definitely recommend checking out the previous JuliaCon videos on YouTube.
Not a strictly Julia tutorial, but you could work through \[this Gluon thing\]([https://gluon.mxnet.io](https://gluon.mxnet.io)) with the Julia MXNet package.
For machine learning applications you have: https://github.com/MikeInnes/diff-zoo (creating a differentiation library from scratch) Multiple models implemented in Flux: https://github.com/FluxML/model-zoo For statistics and more theory: https://people.smp.uq.edu.au/YoniNazarathy/julia-stats/StatisticsWithJulia.pdf https://github.com/h-Klok/StatsWithJuliaBook https://github.com/StatisticalRethinkingJulia/StatisticalRethinking.jl And something about Julia's programming paradigm: https://www.youtube.com/watch?v=HAEgGFqbVkA
I am hypothesising but I meant compared to Python's GIL problem rather than any Julia problem. Python servers run one instance per request to get round this so rely on the concurrency of the web server. Julia's start up time problem makes it unsuitable for that, hence my FCGI idea (or a native web server but that is not adviseable to roll yourself unless you are battle hardened at doing that on the public internet). OTOH number of concurrent requests is a platform feature on how it implements book-keeping and switching. So Erlang vs Java will give you different results.
Not exactly ML tutorial but I found this helpful. https://lectures.quantecon.org/jl/
fluxml.ai
If you're interested in machine learning + the real power of Julia, take a look at neural differential equations. An introductory blog post can be found at [https://julialang.org/blog/2019/01/fluxdiffeq](https://julialang.org/blog/2019/01/fluxdiffeq) . The [DiffEqFlux.jl repository](https://github.com/JuliaDiffEq/DiffEqFlux.jl) is about 150 lines of code which allows for generating neural ordinary differential equation (neural ODE), neural stochastic differential equation (neural SDE), neural delay differential equation (neural DDE), neural differential-algebraic equation (neural DAE), and neural (stochastic) partial differential equation (neural PDE and neural SPDE) models for fitting nonlinear dynamics to data. It's generated through the differentiable programming aspects of Julia (ForwardDiff.jl, Tracker.jl, and very soon Zygote.jl) mixed with the [DifferentialEquations.jl](http://docs.juliadiffeq.org/latest/) library which encompasses a large set of differential equations. There's no neural ODE library that works for stiff ODEs and GPUs in other languages yet, and ODEs are just the warmup for the rest of these equations. So this is quite an "only in Julia" thing all built by Julia's type system and compilation power.
You have the function isposdef() in LinearAlgebra.jl
What's your use case? How large is the matrix? Is it Hermitian? You can do a Cholesky, like others have said, or a diagonalization or a power method etc. but which one is faster will depend on the size, sparsity and structure of the matrix. Because this is computationally expensive, it's often better to do this at the same time as other steps (like do a Cholesky, than use the Cholesky in a solve step) or to setup the problem in such a way that you don't have to check (eg if you are doing quantum mechanics, ensure that operations on the density matrix are positive instead of testing the density matrix for positivity).
eigmin might be faster but probably only for large ish matrices. It's also O(n¬≥) I believe because matrix matrix multiplication is O(n¬≥).
There is a function in `LinearAlgebra` called `isposdef`, which presumably does what other people are describing doing manually here. This probably doesn't make sense for performance-intensive applications, but if you're just looking for a quick test in the REPL I think it is very acceptable.
Very cool! I have just started learning about Julia and high performance computing, so I found this paper to be extremely helpful as a bit of a review.
Enjoy! :)
Sweet! I‚Äôve been looking forward to this since I saw the ISOP newsletter. I‚Äôll have to benchmark it against some standard (and maybe a couple less than standard) datasets I have to get an idea of the workflow. Given the possibility of bring my modeling and simulation work together in a single language is also nice (no more hacking some Perl script together to convert between things).
Excellent
I see pieces of things we do in nonmem, winnonlin and SAS. How to convince this is just as good? How to ensure its stable and validated? There is a reason R has not made it in that field, and that is not because SAS is the better language.
beautiful! I'm guessing you are forced to use Win7 at work(like me). How did you do it in the end?
Was it hard to install?
Let me start on the math part of the answer. We have a validation suite which does a pretty much combinatoric check on the event handling system against NONMEM (steady state dosing yes/no, rates yes/no/matching ii/multiple of ii/between 2-3x ii/etc., modeled lags yes/no, bioav, ...) where we match NONMEM with sqrt(eps) relative tolerance. The one caveat there is that we only match NONMEM to that when it does steady state dosing to 10^-5 relative tolerance, because we match the analytical solution to 10^-14 relative tolerance while NONMEM is 10^-5 relative tolerance because of how NONMEM does early exits in its analytical solution loop. We will be opening up the repo along with our test suite with automatic verification plots on that so anyone can take a look at exactly how we're testing. But essentially, we are able to use arbitrary precision arithmetic and analytical solutions (because we are using DifferentialEquations.jl) and are able to show that the simulator converges closer to analytical results than NONMEM (but both are far more accurate than data, going about 5 decimal places out, so this enhanced precision is more of an interesting detail than a practical one). There is a more interesting question with parameter fitting. This is where NLMIXR tends to have issues and where NONMEM or Winnonlin tends to have be much more robust. We are doing extensive testing of the parameter estimation to continually make it more robust, but the core trick is that automatic differentiation really enables more accurate gradients. We described in our [investigation of sensitivity analysis](https://arxiv.org/abs/1812.01892) how automatic differentiation is faster than the traditional sensitivity analysis approaches, lets us optimize with respecet to any possible parameters in the model, all while being accurate. There's still some work left to do here, but the robustness of the optimization is a key factor and Andras Noack and Patrick Mongenson are looking deep into this. This highlights one major difference from NLMIXR in that it's not a one man project, but instead we are pairing with Julia Computing to use the extra manpower to make it a complete software, with a support staff and looking to get the right verifications and FDA approvals for adoption. As a project led by [Joga Gobburu and Vijay Ivaturi](https://www.pharmacy.umaryland.edu/centers/ctm/people/biographies/), of course this is an interest of ours that our team has expertise in, given that Joga was the Director of Pharmacometrics at the FDA who essentially created the FDA verification structure for pharmacometrics that stands today. But I think that looking at those other systems as competitors is missing the point. Most of what I set out to create Pumas for is because those systems are too limited for the types of things I am interested in. I want to make the models be stochastic differential equations with high order adaptive integrators to better capture the internal stochasticity and be more accurate. I wan to GPU parallelize the integration. I want a true solution to machine learning integraion. NONMEM and Winonlin are good solutions to traditional pharmacometrics, but we are going far beyond that, with traditional pharmacometrics as the core but single component of next generation pharmacometrics.
I can't imagine, unless win7 has any major quirks (which it doesn't AFAIK)
I see your point, and I do believe you can outperform those. I work in a CRO for clinical trials, we use winnonlin because its expected. There is no secret in a AUC that stops us from calculating in SAS or R or even Excel. Our customers expect winnonlin. Perhaps within farmaceutical companies they use such things to develop. Though I don't know how Julia will be liked in the corporate straitjacket. Note from our perspective validated has two aspects. Besides what you say, we have to ensure our installation of software is validated and returns the expected results. We work on a validated centralized SAS partly for that reason. We don't upgrade until its all validated and approved again. The new version is already on the server, but not validated for production.
&gt;Note from our perspective validated has two aspects. Besides what you say, we have to ensure our installation of software is validated and returns the expected results. We work on a validated centralized SAS partly for that reason. We don't upgrade until its all validated and approved again. The new version is already on the server, but not validated for production. That's why Pumas is being developed as part of the Pumas-AI company with partnership from Julia Computing. Pumas-AI has expertise in validation of pharmacometric software and is currently working on the validation suite, and [Julia Computing has products like JuliaTeam + Deployment](https://juliacomputing.com/products/juliateam) for ensuring proper validated deployment of new versions to internal servers. We have already received validation to be used internally at the FDA (where today you can download JuliaPro and install Pumas). Our team will continue this push to be a true Julia pharmacometric solution.
Windows 7/Windows Server 2012 users also require Windows Management Framework 3.0 or later. Prepare for fun, especially on non-administrative accounts. I had to install it at home with same directory structure, copy to USB, and then copy it at work. Pkg was also tough because of Github's SSL version insistence but I think that's cured now. Then you have to get Python working for PyCall, which can mean fiddling with paths in the source files.
ouch
It's a great initiative! Already can see lots of compelling points compared to something like Stan + Torsten. Probably it would be the most interesting to the researchers in PKPD (QsP) community rather practitioners due to excessive regulations in the field
QSP is where I am targeting, which is why GPU acceleration and function based interfaces to support PDEs is in there. Watch Yingbo's JuliaCon talk on accelerated stiff ODE solvers for QSP.
Yes, though we‚Äôre migrating to Win10 at the moment. My switch is due in a month or two, and I can‚Äôt wait to do it all again. How I did it? I called the help desk. Two guys there were extremely helpful and dedicated in helping me resolve the issue. One of them, who was on the security side of things, had me open and close Atom repeatedly as it tried to find the packages, while he monitored my internet traffic. He finally hunted down all the URLs that the proxy was hitting. That guy was amazing ‚Äî we were on the phone for hours. One thing that helped is that we manually grabbed the Juno 0.2.0 folder from Github and placed it in the .atom/packages folder. That way, every time I opened Atom, the Juno script would start running trying to grab all the components.
Once I got approval to explore Julia and Juno, I had to get temporary admin rights to my computer. Installing Julia and Atom themselves was not hard. Juno was the hard bit, see my other comment and my last post here.
Also, interesting discussion on Hacker News about the paper, with the author chiming in: [link](https://news.ycombinator.com/item?id=20477873.
When you first load a library in Julia it does some pre-compilation, which compiles a small subset of what you would be actually using and caches it (I don't know if pre-compilation actually compiles anything to machine code, maybe it compiles to an intermediate language or bytecode). Then, when you run first run a function it compiles the code to machine code for the particular set of types you are using. But every time you restart Julia, it will need to recompile that function again for that particular set of types. So, yeah Julia does a lot of recompilation when you restart Julia. Due to this, a good way to code in Julia is to have a repl or jupyter notebook constantly running so that you minimize the number of times you have to restart Julia and use the Revise.jl package.
Julia does not compile the same thing twice (within the same session, if you call something from the shell twice or restart the repl it will compile it again), but it may compile the same function over and over again. What happens is that every time you write a function (especially untyped or parametric functions), you're actually writing a set of functions for large amount of type combinations that may not even exist yet (which makes it effectively infinite). You can see it in other languages like C++, in which abusing templates creates indefinitely long compile times, and in Julia every function without concrete types as arguments is infinitely polymorphic. The compiler can only know what versions of a function to compile (since it can't compile everything without wasting a lot of space and time and still failing whenever you define/import a new valid type) once it can infer all the types that it will ever be called with (which it will do whenever it evaluates something in the global namespace, such as the REPL, eval or at startup), and since it's a dynamic language it can never fully know it covered it all. Of course, the compiler could be a lot less aggressive with the polymorphism (for example assuming every type will always be Any instead of each possible concrete type such as Int64), but then it would not be able to create the optimal code for every function (competing with languages like C in speed, and instead it would compete with Python). Solving that problem is the next priority after multithreading (1.3) for the compiler devs which could be handled by allowing the code to be interpreted when needed (no JIT lag, but worse performance over time), better caching and better precompilation. In general you can also adjust your workflow so you don't need to close the REPL (so it will eventually only compile the small differences) or experiment with stuff like the [PackageCompiler](https://github.com/JuliaLang/PackageCompiler.jl) to precompile more of the code before starting.
&gt;Revise.jl interesting, i've never heard of this package! &amp;#x200B; I guess i'm curious why the functions you compile can't be cached either?
Are you implying giving your functions very specific types allows them to be permanently cached?
In the same way as if you had a function with no type annotation and only called it with a single type ever (and everything inside was type stable). Which is kinda why some libraries that have small variations in arguments have less compilation issues than libraries that have lots of options (like plotting libraries). Using overly restrictive types is not idiomatic code in Julia, the language is built to be extremely flexible and polymorphic, so you can write generic algorithms that work with whatever types or contexts you want. And permanently cached within the session, it will not keep it after you close the program (unlike using an extension like PackageCompiler).
No,a generic function will be compiled for each combination of types passed as an argument. If you have two different types you are trying to use a function on, it will compile once for each whether you defined your function generically or defined two functions, one for each type.
&gt; how to get a Genie.jl project running inside a Docker container? I asked this on Quora because if I run a website with Genie.jl, it will definitly be with Docker too... https://www.quora.com/unanswered/How-woul-you-get-a-Julia-Genie-jl-project-running-inside-a-Docker-container
One of my favorite language features.
I finally figured it out, and I at least got the default example genie project working in a docker container. The biggest thing is that you have to make sure that the Genie app is running on host 0.0.0.0 instead of the default host 127.0.0.1 You also just need to throw these files inside your Genie project folder, and then use "docker-compose up" to run the genie app. **Dockerfile**: FROM julia ENV NAME genie COPY ./ /genie WORKDIR /genie RUN julia -e 'using Pkg; pkg"activate ."; pkg"instantiate"; using Genie; Genie.loadapp(); Genie.startup()' **docker-compose.yml**: version: '3' services: genie: container_name: genie-app build: . command: julia --color=yes --depwarn=no -i bootstrap.jl -l 0.0.0.0 -p 8000 s volumes: - ./:/genie ports: - "8000:8000" I had issues deploying onto my cloud server that only has 1GB RAM. The server would run out of memory while instantiating the project. To work around this I built the image from the Dockerfile on my home computer, pushed the image to Docker Hub and then replaced the "build: ." line in the docker-compose.yml to "image: imagename" which works on a server with limited memory, but isn't the ideal way to deploy the project.
You opened a parenthesis for your return statement (you dont need to btw), but you never closed it.
ahah, that was just a typo in the post, thank you though!
The functions you compile are cached in memory, just not persistently between sessions.
Sounds like you have a very helpful IT service! The last place I worked allowed me to use linux on my work machine. They had no interest in maintaining it, so the deal was I could do what I liked, but I couldn't ask for help. I suppose it was fair enough. Though one time I put in a ticket about a problem getting a certificate off a Windows machine, and they went off on one thinking I was asking for help with my linux machine. Which was quite funny. I politely listened to our head of IT going off on what was clearly a pre-prepared speech about taking your horse to the train station before pointing out the issue was nothing to do with linux.
Have you tried https://github.com/hfp/libxsmm ?
I'd suggest being conscientious about nomenclature of what you're shooting for around regulatory buy-in &amp;#x200B; \&gt; looking to get the right verifications and FDA approvals for adoption &amp;#x200B; as Joga says, "the FDA has never, and will never, be in the business of blessing/approving particular software." &amp;#x200B; Likewise, I can't imagine you are going for pure Part 11 compliance. &amp;#x200B; Point is, the FDA will accept stuff from Julia, R. excel, SAS, etc. If you want to build supportive evidence via testing suites or quality docs, that will be to generally be to appease industry QA groups, not the regulatory agencies themselves. Furthermore, if that was not the case, what you're (accidentally) implicitly suggesting is that things done in Julia, but not through the Julia Computing group directly, with the manpower to make "complete software", are not sufficient for regulatory use.
Great results! Every article I see about Julia really blows me away. It‚Äôs great to be part of such a lively community filled with so many talented people!
I very much doubt these results. Eigen is mature enough that it should be doing what is most ideal for the CPU. I bet they screwed up their benchmark and introduced copies or something.
Verified that this is the case in juno. Also found similar trouble when attempting to run the module file with Julia. It seems like it may be related to how readline interacts with the REPL when you are calling it as part of a module (I'm something of a noob myself). Anyone know what kind of module structure we are missing to proceed through the calls? I'm getting a method error that seems to indicate that it thinks the strings we enter in answer to the prompt are attempts to make a call on some unknown method instead of being a string reply to the prompt.
Good thing it's fully reproducible since all of the code is ran. Please let us know where they messed up so we can rely it back to the author! Otherwise, it looks pretty clean to me.
seems like the kind of thing you want plots for, not giant tables, or at least the tables should be appendix-like material
urandom(n) = String(rand(RandomDevice(), UInt8, n))
Some background information [here](https://discourse.julialang.org/t/user-console-input-read-readline-input/15031/15). Your code works in the Atom/Juno REPL, but there's some complex interaction between a file and the REPL that prevents the two communicating easily. I suppose it could be argued that, as opposed to C (don't know about Java), having an interactive REPL means you don't have to write code just to get user input...
It sounds like "the easy and natural way of doing X is wrong and you should not use it, there's an unnatural and more complicated way Y you should use it instead". Julia should provide a way to somehow preserve that state and allow to use it in a way \`julia script.jl\` maybe with something like \`julia --keep-julia-runtime-between-runs script.js\`
Very interesting, and for once the library is generic and integers are not left on the side like for regular Julia/Python/R matrix multiplication. &amp;#x200B; Beyond padded integers, if you focus on small matrices, you might want to store them in Morton layout. All high performance BLAS implementations, including on GPU, perform packing/swizzling before running the matrix multiplication kernel. &amp;#x200B; On CPU: \- the pack\_A and pack\_B, in this [high-performance matrix multiplication course](http://apfel.mathematik.uni-ulm.de/~lehn/sghpc/gemm/page02/index.html) \- my [packing implementation in Nim](https://github.com/numforge/laser/blob/7c539820c4419c96a499b693ea95502ea36e8938/laser/primitives/matrix_multiplication/gemm_packing.nim) (it reaches similar speed as OpenBLAS and MKL-DNN GEMM implementation without assembly) \- [Halide BLAS swizzling](https://github.com/halide/Halide/blob/fa25a98119c9a8dc4a3b4f758569966c5bd50b35/apps/linear_algebra/src/blas_l3_generators.cpp#L57-L73) (also reaches OpenBLAS speed without assembly) And on GPU: \- NVIDIA's [CUTLASS swizzling](https://github.com/NVIDIA/cutlass/blob/b5cab177a9e8f7c803f60d1ad4124f2c094142af/CUTLASS.md#threadblock-rasterization). Cutlass reaches CuBLAS speed except on Tensor Cores without assembly. &amp;#x200B; The layout change (with zero-padding) will allow triple for-loops that uses 100% of the CPU core as it will be memory/prefetcher friendly. &amp;#x200B; For reference this is my implementation in Nim of GEMM with prepacking: [https://github.com/numforge/laser/blob/master/laser/primitives/matrix\_multiplication/gemm\_prepacked.nim](https://github.com/numforge/laser/blob/master/laser/primitives/matrix_multiplication/gemm_prepacked.nim) &amp;#x200B; I didn't consider the small matmul use-case though, it was more for batch matrix multiplication.
Yes there will be a livestream in all rooms, on the JuliaLang youtube channel: [https://www.youtube.com/channel/UC9IuUwwE2xdjQUT\_LMLONoA](https://www.youtube.com/channel/UC9IuUwwE2xdjQUT_LMLONoA)
I'm excited :)
Some sessions are marked as non recorded, they would be streamed only?
That would be really bad. The DiffEq workshop or "Writing a package -- a thorough guide" should *definitely* be recorded for posterity. I don't think this could be right.
I cannot find any livestream anywhere! Any help would be appreciated.
Workshops are not streamed but some are recorded and will be put online one hour after the end. Talks starting tomorrow will be streamed live
Oh, good!
Maybe it's just some quirk of the pretalx web system (look at this talk for example https://pretalx.com/juliacon2019/talk/8SBXLD/). Will hope that most of stuff would be recorded!
Your post seems to imply that the Julia developers are foolish, and never thought of this. *Of course* this is a desired feature, the reason it is not available *yet* is that it is definitely *not* "easy", at least not when you are the one who has to implement it. It is easy to whine and complain (over free software, might I add), not so easy to design and build.
This is great! I love that the authors explain in details about how they leverage Julia's type system to design this package.
I have the impression that Julia libraries are normally highly generic. Is it really the case that they they normally ignore integers, or did you mean just Python/R?
Julia is generic in general but for linear algebra just like R and Python, it relies on BLAS, a set of routines implemented in C, Fortran or most often pure Assembly. The BLAS standard only requires float32, float64, complex64 and complex128 and only Intel MKL added accelereted integer routines (and this was quite recently). BLAS stands for basic linear algebra subroutines and notably include GEMV and GEMM (GEneralized Matrix-Vector and GEneralized Matrix-Matrix Multiplication) which are non-trivial to optimize. The LAPACK standard which include stuff like QR, Cholesky and LU Decomposition, eigenvectors,... is built on top of BLAS so it's really important that BLAS is fast. AFAIK Julia, R, Python uses BLAS for floats and complex but fall back to triple for-loop in case of integer. The performance difference is very important as you can see in the screenshot of my reworked integer matmul: https://github.com/mratsim/Arraymancer/pull/368. Note that this is an implementation without SIMD vectorization, I have one with SIMD which improves speed by 2x to 4x depending on int32 or int64 and Avx512 support.
BLAS, yes. But those are not written in Julia, and your remark appeared to be targeted at Julia (and R and Python) matrix libraries in general.
half of the workshops were livestreamed today the VoDs will be on the channel https://www.youtube.com/user/JuliaLanguage/
I've lost too much time in the past reading benchmarks like these, and then realizing they were full of shit.
Unrelated, can you inform me as to the possibilities of Julia? Does it have the potential to write games in? Can you make websites with it?
Compared to R and Python, Julia's main appeal (and marketing) is high performance in a high-level language. I.e. Julia has the tools to provide much fast integer matrix multiplications than 3 na√Øve for-loop. Now, I think it's mostly useful for image processing for convolution/blurring/edge-detection/... without converting the 0-255 integer color channels to float, but filters in that case are small (3x3) so that may be covered by OP libraries.
I wonder why not all were recorded. Very sorry to miss some of those.