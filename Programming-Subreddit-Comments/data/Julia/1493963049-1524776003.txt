`pmap` returns a vector. So just `results = pmap(main,1:64)`. However, one thing I should note is that, unless you have extremely optimized code, `number of processes = number of cores` is a bad heuristic. Here's an example: http://www.stochasticlifestyle.com/236-2/ You should benchmark before assuming 64 cores will be even close to the best choice.
Actually if I just run the `main()` function directly in the REPL I get good results.. but if I use it in `pmap`, the results are all zero (both locally and on server). Is it because the "results" are all stored locally in other processes? Can your library help me fetch them, and if so how? My code is at https://github.com/affans/zikajulia The only relevant function is the `main.jl` file.. it has two functions `main()` and `main_calibration()`. Right now I am trying to get the calibration results.. but both return the same type of data.
Do you have example code? The moment condition for any instrumental GMM is that your instruments are uncorrelated with your residuals; how are you estimating your residuals? Are you using CUGMM or two-step or something else? Focus on coding up IVGMM in a linear relationship. Once you have that working, extending it to any sort of non-linear relationship is only a minor modification to the code.
You're using quite a few globals. Are those global constants? Also, did you make sure those globals are defined on every process? Also, what happens when you use `map` instead, i.e. results = map(x -&gt; main_calibration(x, P), 1:numberofsims) ?
This should link to the GSoC announcement instead.
In addition to the other answer, the [] syntax in Julia purposefully doesn't do anything fancy. Since Julia arrays can hold anything, giving it a 1:2:20 results in an array that holds a 1:2:20, rather than expanding it out. (note I said "a 1:2:20", an object with that value which happens to be of type Unit range.) Likewise, [A, B] doesn't concatenate A and B if they are arrays. In this case, [A, B] is literally an array of A and B, whatever those might be. Previous versions of Julia did have some magic in expanding things you tried to sneak in an array, but that's gone now. There's no magic in building Julia array. What you see is what you get. 
I switched it from jvm to juliavm from an obvious reason, the another similar projects follow the same pattern, nvm (node), rvm (ruby), so on ... Anyway, if you have any name which could fit better, suggestions are welcome, thanks for the feedback. 
Yes you caught me I had two examples and didn't even notice the different types and blended the examples in my post -- good eye. I fixed it. This idea of lazy evaluation is sort of blowing my mind right now. Very cool. My first thought was "Why the hell would they have this and why would I ever want a language that has an extra function like `collect()`?" Obviously my default is that the Julia developers know a tad more than me about programming. Just a tad. :) 
Awesome!! I've been a little leery of jumping into Julia, since it's still technically in beta and the compilation/distribution process is a little hard for me to follow, but my difficulties might be worth it if Julia's got the numeric stability to be worth looking at in detail.
Julia 1.0 will be coming out "soon". v0.6 is coming out likely this week or next week, v0.7 is a deprecation only release, and then v1.0 comes out. At least by the end of the year it'll be done, but this means that most of the breaking changes are part of v0.6 (with deprecation warnings, which is why v0.7 is a step in the middle to help the transition). So v1.0 should be a lot like the current Julia, but is made to be a stable release. With that, it will have a well-defined BLAS/LAPACK/LLVM, and all packages will need to support it. For your application, since this is crucial, you may want to directly target Julia v1.0.
For what it's worth, you don't need the brackets in Matlab, either. It might _try_ to do something lazy with `1:10`, but there are no guarantees.
Is there caching for the jit yet?
The JIT has been using caches for a really long time.
This is fantastic work, very much looking forward to the future development of this!
Most policy decisions these days are heavily influenced by proprietary forecasting models. Just look at the fuss that was made because the House passed a health bill without a Congressional Budget Office score. The CBO score will certainly play a large part in the Senate's rewrite. The problem is that the CBO and other organizations like it are quite secretive about their modeling. Most of the time they only produce point estimates, and they don't publish many of the assumptions behind their modeling. When there is a bill that contains both taxes and spending, the Joint Committee on Taxation models the tax part, the CBO models the spending part, and they just smash the results together because even those two organizations aren't willing to share and integrate their models. The NY Fed is serving as an important leader in this field. Policy analysis should be transparent and scientific, and that's what the NY Fed is moving the field towards.
Awesome work!
`[1,2,3]' == [1 2 3]` Note that `[1 2 3]' != [1,2,3]` because the transpose operation will not convert from `Array{Int, 2}` to `Array{Int, 1}`. 
Good explanation, just a minor note about the last paragraph: in 0.6 Julia will have a [RowVector type](https://docs.julialang.org/en/latest/stdlib/linalg.html#Base.LinAlg.RowVector), which will be the return type of `transpose(::AbstractVector)`.
Ah, that is a nice additional feature. It will probably help avoid problems like this one.
Thanks for this great explanation! I'm surprised they went this route, but it is very useful to see the rationale. 
You should report this to the Plots.jl repository. Double check that your PyPlot built correctly (`Pkg.build("PyPlot")`) &gt;I'm calling it in the repl (because that's the only way I can get the plot to display anything without immediately closing afterwards...) What do you mean by this? In the script, are you not calling `display(plot(...))` or `gui()`? Juno and Jupyter will have this displayed in their respective IDEs. 
To clarify, I'm surprised they didn't make things more flexible. One of the things I like about julia is this philosophy of "Just enter inputs and the function should work in a reasonable way." Functions tend to be flexible and *just work*. This example with `sort` seems to be a case of that not happening. In general I also don't like the row/column asymmetry it seems inelegant. I'll get used to it. :) But calling something a *2d row vector* just. seems. wrong. 
Oops I just noticed the title is wrong it should say [1 2 3]: originally I had a longer title with different examples, and then shortened it. I did it wrong.
In my experience and to my limited understanding, in most cases where Julia seems to be too rigid and counter-intuitive, the reason is type stability. Basically, if you allow variables to change their types, code cannot be compiled efficiently. This is the reason behind `sqrt(-1.0)` producing an error. As you say, you get used to it, there are ways to still make your code elegant, and the execution speed more than makes up for it.
I actually just hit something weird like this not too long ago... there might be a PyPlot bug hanging around which needs reporting.
&gt;It is "just" for the ease of the installed Juno and debugger? tnx!!!! Essentially, yes. Some people want a single install. It's easy. This can also be helpful for say businesses behind a firewall or other things without common internet setups. But I still think that manually installing the parts (at least for now) is a good way to go for many normal users and developers. But JuliaPro looks better and better every day.
As someone who works at such a business and didn't know about Julia Pro until I saw this thread, omg yes
Julia is now available for Raspberry Pi: https://www.raspberrypi.org/blog/julia-language-raspberry-pi/
On the other hand, it's amazingly easy to make `sort` work for your case: julia&gt; Base.sort(x::AbstractArray) = sort(reshape(x, length(x))) julia&gt; sort([1 2 3]) 3-element Array{Int64,1}: 1 2 3 I would not recommend modifying `Base` methods on built-in types like this in code you intend to publish, but it's fine for playing around. 
All the time but I use it the opposite way. I just stay in the Julia REPL a lot of the time since running shell commands is so easy by just writing ;. I prefer this as I like to get Julia function completions. Keeping track of awk and sed syntax is too much overhead for me ad I don't use them frequent enough. Since julia has filepath completions inside strings I think it works great as a script language.
Sorry if I'm not understanding right, but I get: julia&gt; function testing() return 4 end testing (generic function with 1 method) julia&gt; @edit testing() ERROR: could not find source file for function in edit at ./interactiveutil.jl:64 (this is Version 0.4.5) 
Thanks for your reply! I was using ":" instead of a ",". That's why it didn't work properly. Now, I've got another problem. I got a interval from -10 to 10. I want to get a random number in this interval. When I try to use rand(-10,10), I just get "invalid Array dimensions". 
The NY fed, and Quant-Econ, are really great organisations. I'm really happy the macroeconomics community is starting down this road (you see increasingly more econ grad students picking up python and Julia instead of Matlab and stata) I'm optimistic where this will lead.
You can *kind of* do this with `@code_lowered testing()`. Also check out https://github.com/SimonDanisch/Sugar.jl.
It is possible, but it hasn't been made easy (yet). Gallium does this: https://github.com/Keno/ASTInterpreter.jl/blob/c8e954d91e01ae415e02b37e0e73955c57800012/src/ASTInterpreter.jl#L982-L1032
Stefan is working on it. His JuliaCon talk makes it seem like his goal is to release it at JuliaCon: http://juliacon.org/2017/talks.html#talk-20
https://github.com/mpastell/Weave.jl Weave is great.
Binary or python dependencies also?
Those don't seem to be mentioned. If you want to have your voice heard on that, you'll want to open an issue at the Julep page: https://github.com/JuliaLang/Juleps/issues
I installed it yesterday and it works likea charm. I have a single issue with it: how to get rid of the npm notification ?
What kind of npm notification do you get?
That's not npm notifications. That's a Node.js section and it shows current version via `nvm` or `n` in folders that contain `package.json`, or `node_modules`, or any other file with `.js` extension. Here are options for this section: https://github.com/denysdovhan/spaceship-zsh-theme#nodejs-node If you'd like to hide this section, just set `SPACESHIP_NODE_SHOW` to `false`.
How are you able to read STDIN? I can't seem to do it on my end. 
Your code works fine (for some value of "fine"). If I pipe echo '\x03\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00' into your script, then `length` is `Int64 3` and `array` is `Array{Int64}((1,3)) [1 2 4]`. That's what "canonical binary representation" means, but this is kind of an insane way to do things and probably not what you want. You also don't need to reimplement the `sum` function. I think a more sane approach would be to omit the explicit length and just use array = readdlm(STDIN, Int) instead of all your code. Then you can feed it with (for example) echo '1 2 4' and output the sum with println(sum(array))
This makes sense! Thank you!
Still new to Ubuntu OS. Is there any particular way to swap to 0.5 without losing all my packages?
Suggest that they implement `deleteat!`, or make a PR for them.
Thanks for the response--can you explain a little more/point me in the direction of how to do this? I am new to Julia and not a great programmer. Thanks.
How many proposals were there in total? 
While I'm glad to see Julia doing so well, that list looks very strange. My prior would not have had e.g. more Scala than Python repositories, Java/C/C++ not in the top 15, etc. So my posterior has significant weight on "this isn't measuring what I think it's measuring" and "a mistake was made."
Agreed. The TensorFlow repo alone has more stars than the highest-ranked repo on this list. I am guessing what happened is that the GitHub API was used without considering that it times out on large queries. In that case you just get whatever was returned first, which happened to be something that made lots of small languages look good.
This looks like it was based on https://github.com/showcases/programming-languages which is only languages where the (reference) implementation of the language itself is developed on Github.
I think too that this is the case. And the "number of repositories" are actually the number of forks of the language implementation and not the actual total number of repositories using this language. I misunderstood this too at first and I found it quite surprising that e.g. julia is ranked before python. But there are 1711910 repositories with Python code and 9318 with Julia code. https://api.github.com/search/repositories?q=language:Python https://api.github.com/search/repositories?q=language:Julia
No, I don't think that is correct. It is simply the number of stars and forks of the main language repo - as explained in the text! It the first sentence even... https://github.com/golang/go for example, has 28323 stars and 3789 forks as of me writing this, and the table says 28230 and 3770.
&gt;GPUArrays from GPUArrays.jl Aaaaaand you just made my life a lot easier. That package looks incredible. 
That was very helpful, but I have more questions now! First, one of the reasons Julia is so attractive compared to other languages like Matlab, is that you can avoid vectorization (actually I think it's still faster if you devectorize, but correct me if I'm wrong). The code is more readable when you can devectorize, especially with matrices of higher dimensions. Will the planned macro for multithreading "force" us back to vectorizing code in order to take advantage of the macro? Secondly, is there a reason that I need to multithread on the highest level? My actual problem is to iterate from the last period to the first and calculate the value function over all the possible values of the state variables on the grid. The highest level of this problem is not "embarrassingly" parallel because I need to wait for the value of t+1 before I proceed to t and so on and so forth, but the lower levels are. Thirdly, I apologize for the silly question but I'm not sure how to correctly create an array of arrays in n dimensions and collect them in cat(n,values...). For example for a 3-d matrix (with k1=k2=k3=5): value1 =[rand(k2) for i in 1:k1,j=1:k3] creates a 5×5 Array{Array{Float64,1},2} and cat(3,value1): 5×5×1 Array{Array{Float64,1},3} Neither value2 =[rand(k2,k3) for i in 1:k1] or value3 =[[rand(k2) for i in 1:k1] for j=1:k3] seem to work for a (k1,k2,k3) matrix. 
Just like pretty much every modern programming language, julia passes complex types around "by reference". c=b creates a reference to b called c, which means c refers to the same underlying data as b. here's a simpler working example: a = [0] b = a b[1] = 42 a If you want to create a new object with its own memory allocation, aka make a copy, do this: a = [0] b = copy(a) b[1] = 42 a If you want to create a new object with its own memory allocation including of its subfields, aka make a deep copy, use deepcopy: a = [[1]] b = copy(a) b[1][1] = 42 println("a[1][1] = ", a[1][1]) # Prints 42! b = deepcopy(a) b[1][1] = 4242 println("a[1][1] = ", a[1][1]) # Prints 42. Edit: erroneously said c=b was a shallow copy; added deep copy info.
Yeah. Well, that an a little C experience. Ever since I left grad school I haven't had access to MATLAB and I recently picked up Julia as a replacement. If you wanted it not to copy *until* the copy would be modified I'd understand, I suppose. What seems *truly* crazy to me (reading more about this) is that you can modify objects in functions like this. That just totally breaks the idea of scope, right? I dunno. Not gonna lie--it disturbs me. 
a little more abstract view: there are conflicting aspects of simplicity for the programmer, speed, and simplicity of implementation. the julia way is an interesting middle ground, it does allow weird programming, and might surprise the uninitiated, but simple and efficient. all you need to remember: there are mutables and immutables. immutable values behave like well, values, 2 is 2, "hello" is "hello", etc. it makes little sense to ask is this 2 the same as that other 2? mutables behave like objects which you can refer to, share and pass around. you need to know which type is which, and you need to make the decision when creating a new type. once you get used to it, it is all clear.
Actually, Julia is not only passing complex types around by reference, but all types. If you write a = 23 b = a a and b are actually pointing to the same memory location `pointer_from_objref(a)` and `pointer_from_objref(b)` will give you the same address. The reason why `b = 42` does not also change `a` is that an Integer is an immutable type. So you are not changing the value at the memory position of b but you are creating a new object in memory and changing `b` to now reference this new location. But please correct me if I am wrong.
&gt; it not to copy until the copy would be modified I'd understand, I suppose. What seems truly crazy to me (reading more about this) is that you can modify objects in functions like this. That just totally breaks the idea of scope, right? I dunno. Not gonna lie--it disturbs me. No, because the value of an array is its reference. You're equating values with `a=b`. You're passing values with `f(a)`, and so by extension, you're passing array references. This is actually a huge gain over MATLAB, because it means you can write mutating functions which update arrays instead of creating new ones. Creating new arrays is a very costly operation, and so having the ability to write functions like `copy!(a,b)` which copies the values of `b` into `a`, or `A_mul_B!(C,A,B)` which is `C = A*B` but writing into an existing matrix `C`, these are massively more performant than the functions which require making new arrays and thus can give huge gains when in a tight loop. But yes, you have to be careful. Notice the convention with these functions: `!` mean mutating. That convention helps keep these behaviors in check.
it might be counterintuitive to you, but the alternative is arguably worse. if arrays have this copy-on-write semantics, that is, logically always copied, you can not pass them around as reference. or rather, you should keep creating and taking Ref types. you would probably need some neat syntax instead, and we are back to C's * nightmare
I liked C pointers. ¯\\\_\(ツ\)\_/¯ But when I switched to MATLAB for numerical stuff (the first half of my computational physics class was a nightmare with C, in hindsight) one of the things that I loved the most about it was that matrices were objects rather than pointers. That was always something that annoyed me a bunch. 
Just use StaticArrays if you want immutable arrays that will copy on `=`
TIL
Econ grad student here, but I'm very unsure what your actual coding issues are?
Thanks! I've been using Parameters.jl for a few projects, didn't understand the functionality of @unpack. I will update this post after i test my system.
Found the resource: http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/ IIRC, unpacking is just a `MOV` command which costs less than a clock cycle and can be done in parallel. So yes, unpacking once you get into a function and then looping over those values: you'll never see a performance hit for doing so.
You are right of course.
Same. What exactly is tripping you up?
I don't understand what you don't understand :) edit: Oh, I thought you were OP !
I'm sorry, but I still don't understand your struggle. Is it to generate the Halton draws? Try to code something up and show us where the problem is.
Is there a way to not use the about-equal sign. I don't really like using special characters
Yep, use the `isapprox` function.
I personally like using PyPlot, which is essentially a wrapper for the MatPlotLib Python plotting library. I like it because it makes available all of the different plots MatPlotLib can do in a fairly seamless way. There is also a lot of documentation available for it
Yeah, I have used PyPlot in Python, but I didn't really expect it to be as good as native solutions.
Plots.jl - it's a wrapper for a bunch of different plotting libraries (you can use gadfly or pyplot under the hood) - there are recipes for most things you'd want to do, and if there isn't, building your own is easy. Community is really great too - if you can't sort out how to do something, ask on gitter and someone will usually reply pretty quickly.
Does it have to be a native Julia package? I know it doesn't feel like a satisfactory answer and tbh I don't really like how common the 'just call and use python' response is but this is a good example, at least in my experience, I use Julia for parallel computation, not as a general use language and when it comes to visualization I'd rather use python/js to really make things fast/easy and robust (very robust). If it needs to be pure Julia maybe try looking into addressing your issues by making fixes to the package of your choice. If you think it would be useful for others, you might consider contributing your changes to the community. Off the top of my head a package which may interest you is Bokeh. It was originally written for python but there is some work to get native Julia support. 
Plots.jl is by far the best developer target. With its recipe and backend systems, other developers can tie into Plots.jl without having a dependency on a plotting library and while having the full flexibility of many different plotting libraries. For this reason, it's downright silly for any package to not support Plots.jl, which is a big reason why it has grown popular. So I'd look into Plots.jl because if you're using major scientific computing and data science libraries, their types might already have a very useful `plot(mytype)` command. When looking at Plots.jl, you need to include StatPlots.jl and PlotRecipes.jl in its "Base library". They aren't included by default, but allow direct plotting from dataframes, plotting graphs, etc. When adding together all of these "recipes" (a script which says how to transform some data type into another more basic vector for plotting, or a new "series", which is what most people would think of as a new type of plot) no other plotting package comes close to what you can do with Plots.jl. There's PlotThemes.jl to re-theme the whole plot. So for shear amounts of features, Plots.jl is where it's at. It does have a slower first-time-to-plot than say PyPlot or GR directly, but that's being worked on (we might actually know the solution). And it's not even "stuck" on its API: there's a GGPlots.jl which adds a Grammar of Graphics API. It's very basic and experimental, but it shows how simple that is to add to Plots.jl if anyone really wanted it. Because it's so developer friendly, a lot of developers are involved in it. So it's a very fluid library, and the backends that are available keep up with what's good/useful in Julia with many features being added all of the time. It's definitely the library to learn if you just want visualizations to work.
I'd echo this recommendation. Not only does Plots give you a nice syntax and framework for everything, but you can quickly switch between backends when things don't go quite right in one. You won't find as much documentation of the interface as with PyPlots/MatPlotLib, and I've found it occasionally tricky to find the correct arguments to do what i want, but the developers are usually fairly responsive in their Gitter chatroom. If you are into fine-detail twiddling (ie, I've just written a PhD thesis, which required a *lot* of that) you can target PgfPlots.jl as a backend with your Plots.jl plot, and then fiddle with the (text) output until you get what you want.
I still use Gadfly. You can avoid dataframe in most cases by using layers, which you often need anyway when you want to control everything. For example to plot the columns of a matrix: layers = [] for i=1:size(A,2) push!(layers, layer(x=x,y=A[:,i], options...) ) end plot(layers...)
&gt;What are the features about Julia you like the most? The type system and multiple dispatch. Huge broad blanket statement, but it's true. For example, if you look at my recent blog post on DifferentialEquations.jl 2.0 http://www.stochasticlifestyle.com/differentialequations-jl-2-0-state-ecosystem/ You can attribute everything to the type system. The integrator interface that makes it so that way your events can do just aboit anything and size of the system? That's possible because Julia's type system allows code which uses types to be fast. In Python, R, and MATLAB, you usually have to avoid loops with objects like a plague if you want speed (NumPy is essentially a library of routines you have to use on Arrays of 64-bit floating point numbers if you want good performance). This kind of flexibility allows the differential equation solver to be much more than "just solve an equation". Generic algorithms over abstract types? [If you throw symbolic expressions into the ODE solver, you get out symbolic expressions which can be turned into functions](http://www.stochasticlifestyle.com/fun-julia-types-symbolic-expressions-ode-solver/). [Arrays which correspond to multiscale biological models](https://github.com/JuliaDiffEq/MultiScaleArrays.jl) work directly in the ODE/SDE/DDE solvers, meaning that you don't have to re-write anything. Again, this is possible because Julia's types are fast, and multiple dispatch allows generic code to compile an entire new program when it sees new types (late binding). One addition coming soon to DiffEq is [lazy PDE operators](https://github.com/shivin9/PDEOperators.jl). The idea is, in many cases you know what the matrix "should be", so why actually store the matrix? You can just overload the functions so that way the type acts like the matrix you want, and it automatically works the tools that would've really wanted a matrix (like IteartiveSolvers.jl). This reduces memory overhead, can make it easier to parallelize matrix-vector computations (sending the matrix is free!), etc. Again, this is because of the type system. I've built a lot in Julia, and all of it is directly due to the type system. My advice for any newcomer to Julia is to not treat it like an array-based language (MATLAB or Python through NumPy/SciPy), and don't treat it like an object-oriented language. Change your thinking to fully make use of Julia's types and you'll see beautiful results come out (most of which Julia will make for you). I wrote a quick tutorial of my current thoughts on how to do that here: http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/ People come to Julia because its fast, but stay for the type-dispatch system.
Because MATLAB and now Octave destroyed their usability as a REPL from the shell in favour of a bloated UI. #honesty
In part because I just love the language, but mostly because of a couple of the libraries. JuMP mostly. The Julia implementation of ArrayFire is very good as well.
Yep. Fast and easy.
I haven't used pgfplots in julia. but I have used pgf/tikz in latex reasonably often so i'll check it out.
Any books out there to help mindset change training? Or, perhaps just studying your code would be best.
The best way is just to get coding and read other people's code!
I previously used python/numpy for a computational task. but I had to interface it with CUDA code and I was struggling to find out the right way to do that. Reference counting with the official python-C interface? swig? cython? f2py? I knew a bit of julia already and the way it supports interface with external libraries is a true blessing. It's the easiest, most intuitive way I've encountered.
modulo actually does division unless the divisor is known compile time. since you know that you will exceed the boundary by only one, the test is clearly the fastest. you can add functions: incmod1(a, n) = a == n ? one(a) : a + 1 decmod1(a, n) = a == 1 ? n : a - 1 these will most likely be inlined, and just as efficient as your code
Why not pull the boundary parts out of the loop? In general the boundaries will need much more in-depth handling anyways. For the block matrix, it's just tridigonal `[1 -2 1]` with a `1` in the top left and right corners. Then `A*u + u*A` will compute this. Though our lazy PDEOperators.jl should almost be done and will handle this... ParallelAccerator.jl's stencils will also do this really fast. One of the Images packages has a lazy multithreaded Laplacian as well.
I don't like it, the code in your example mixes pipe with function nesting, this way it would look better: ``` authors |&gt; lowercase |&gt; split |&gt; map(ucfirst) |&gt; join(" ") ``` But it doesn't works, do you know if it's possible to make it work? P.S. It should be possible to make it work that way, for example it works in Elixir ``` authors |&gt; String.downcase |&gt; String.split |&gt; Enum.map(&amp;String.capitalize/1) |&gt; Enum.join(" ") ```
It works in Elixir because it has a convention that the result of the previous pipe always put in the first argument of the next function, so it has no issues with multiple arguments. Not sure about Julia though...
you can, it is what Val{} is for. however, it is questionable how effective it would be in your case. it works like this: incmod1(a, ::Val{N}) where N = a == N ? one(a) : a + 1 incmod1(a, ::Val{1}) = 1 and you call it with incmod1(a, Val{5}()) however, it can easily do more harm than good, because that call will end up being a dynamic dispatch if you use it with a variable: incmod1(a, Val{n}()) you might propagate this change back, so your original procedure takes a Val{N} instead of an Int, but that is just awkward. this point is all theoretical, since you already found a better solution.
&gt; x%(2^(k)) is the same as x&amp;((2^(k))-1) the llvm compiler does this optimization for you, but only if the divisor is known at compile time, of course. 
&gt;Was this meant to be bottom left and top right? If you make the matrix: using SpecialMatrices A = full(Strang(5)) A[1,end] = -1 A[end,1] = -1 5×5 Array{Float64,2}: 2.0 -1.0 0.0 0.0 -1.0 -1.0 2.0 -1.0 0.0 0.0 0.0 -1.0 2.0 -1.0 0.0 0.0 0.0 -1.0 2.0 -1.0 -1.0 0.0 0.0 -1.0 2.0 that's the periodic Laplacian. &gt;Is there a documentation or something that I could understand how PDEOperators.jl would be useful for this? Or is it still very much a work-in-progress? It's still very much a work-in-progress. Very far along though: it already works for this kind of stuff, but I won't guarantee a stable API until later this summer. Docs will be added when it gets there. I'll see if I remember to put an updated reply here when that happens. &gt;I really appreciate it. No problem.
Hello there, I have noticed your project and hope for you to think about making it standardize in the [JuliaQuantum](http://juliaquantum.github.io) Julia umbrella organization. Let me know if you are interested :) Qi
Are there release notes somewhere, or do we figure it out by reading the commit logs?
Here you go https://github.com/JuliaLang/julia/blob/release-0.6/NEWS.md
Have a look at [SymEngine.jl](https://github.com/symengine/SymEngine.jl).
I like the name
Immutable structs might get stored on the stack (and not the heap, meaning that any such references would be invalid after leaving the function). They might not exist at all (and only the relevant fields remain as independent values after all optimizations are done). In such cases, passing as a reference is actually *more* expensive since the full object must be created and put on the ~~stack~~ heap. julia&gt; c = 1 + 1im # Complex numbers are immutable 1 + 1im julia&gt; pointer_from_objref(c) Ptr{Void} @0x00000001120b8a10 julia&gt; f(x) = pointer_from_objref(x) f (generic function with 2 methods) julia&gt; f(c) # it is different Ptr{Void} @0x0000000112122bf0 julia&gt; @code_llvm f(c) # complicated! Re-constructs the value on the heap define i8* @julia_f_61248(%Complex.62* nocapture readonly dereferenceable(16)) #0 !dbg !5 { top: %1 = call i8**** @jl_get_ptls_states() #3 %2 = bitcast i8**** %1 to i8* %3 = call i8** @jl_gc_pool_alloc(i8* %2, i32 1408, i32 32) %4 = getelementptr i8*, i8** %3, i64 -1 %5 = bitcast i8** %4 to i8*** store i8** inttoptr (i64 4390378448 to i8**), i8*** %5, align 8 %6 = bitcast %Complex.62* %0 to i8* %7 = bitcast i8** %3 to i8* call void @llvm.memcpy.p0i8.p0i8.i64(i8* %7, i8* %6, i64 16, i32 8, i1 false) ret i8* %7 } That said, it is true that some immutable structs cannot be stored on the stack and are always put on the heap instead. These include structs that have mutable fields. In this case, it is passed as reference: julia&gt; d = Diagonal([1]); julia&gt; pointer_from_objref(d) Ptr{Void} @0x0000000107726d90 julia&gt; f(d) Ptr{Void} @0x0000000107726d90 julia&gt; @code_llvm f(d) define i8* @julia_f_61250(i8** dereferenceable(8)) #0 !dbg !5 { top: %1 = bitcast i8** %0 to i8* ret i8* %1 } 
Just stumbled upon julia. Was looking for a spare time project to work on. Never did much scientific computing, I ended up as the boring Java enterprise developer. This might be fun.
I just tried (Julia v0.5): julia&gt; using StaticArrays julia&gt; x = SVector(1,2) # this should be stack allocated julia&gt; pointer_from_objref(x) Ptr{Void} @0x00000000850d4df0 julia&gt; f(x) Ptr{Void} @0x000000008637b130 julia&gt; d = Diagonal(x) julia&gt; pointer_from_objref(d) Ptr{Void} @0x00000000849a4810 julia&gt; f(d) Ptr{Void} @0x00000000849a4810 julia&gt; @code_llvm(f(d)) ; Function Attrs: uwtable define i8* @julia_f_75900(%jl_value_t*) #0 { top: %1 = bitcast %jl_value_t* %0 to i8* ret i8* %1 } Did I just unwittingly make a heap allocation creating d? I'm so confused right now. 
Diagonal is defined as: struct Diagonal{T} &lt;: AbstractMatrix{T} diag::Vector{T} end In constructing `Diagonal(SVector(1,2))`, the SVector gets converted to a normal old array.
Yes, just clone the repo, checkout 0.5 and build it. Then make a copy of that directory, change to the new directory, checkout 0.6 and rebuild. Finally, setup symlinks (on Linux/OSX at least) so that you can run them each separately (I name my links julia-0.5 and julia, since I do most of my work on 0.6). Alternatively, just download prebuilt binaries to separate directories and setup symlinks :) Packages will be installed separately for each version, so you'll end up having to install and update packages for each version separately.
I think piping into an anonymous function might work: &gt; An anonymous function accepting multiple arguments can be written using the syntax (x,y,z)-&gt;2x+y-z. A zero-argument anonymous function is written as ()-&gt;3. The idea of a function with no arguments may seem strange, but is useful for "delaying" a computation. In this usage, a block of code is wrapped in a zero-argument function, which is later invoked by calling it as f().
The answer to this question is missing from the changelog and docs: + https://discourse.julialang.org/t/macro-inside-constructor-fails-on-0-6/2422
Thanks, that is very informative. I had not thought of the licensing angle, so thanks for bringing that up. I am very happy to create the linear map myself, I don't think it would be very useful for me to build from pre-existing operators. Seems like LinearMaps is more suitable for my purposes! If/when you release the PDE operator, I'd love to take a look at it. Cheers!
Thanks for the detailed response!
The community loves boring Java enterprise developers! No really, it's not only academic researchers hanging out in the forums and chats. Sure, there are a lot of procrastinating phd students around (hey, including me), but there are also people who doesn't do anything close to scientific programming.
1) amazing type system. I've prototyped a "binary encoded alternative to IEEE floating point" https://github.com/interplanetary-robot/SigmoidNumbers multiple dispatch means that I can take these numerical types and get a lot of second order math (matrix multiplication, complex numbers) just by implementing basic operations. 2) easy scripting and relatively simple test system. I took these numbers and wrote a julia program that generates an ad-libbed C library, then imported the C library and verified that the C library does the same thing that the Julia does. https://github.com/Etaphase/FastSigmoids.jl 3) macros I wrote a macro system that lets me write dual purpose functions which can do combinatorial logic and also output the corresponding verilog hardware design language. Then, without leaving julia, I can take the verilog and compile it to C using third-party software and comprehensively verify that the inputs and outputs match. https://github.com/interplanetary-robot/verilog.jl
Maybe Clojure would be to your liking.
When will the remaining videos be uploaded? Thank you for making available these resources worldwide.
Why all the videos contain a juliacon2015 title?! (In the video, below the speakers) are these 2015 or 2017?
I meant to make same post...but, still watching vids.
Go for YouTube "Videos" tab and you should find the 2017 videos - I got similarly confused. 2017 is in the title of each video (but I understand not all the 2017 videos have been uploaded yet).
would be good if the new videos had some kind of human readable description as to what the content will include. the current titles are useless
yeah, like an abstract, and title
Apologies for my previous comment: I haven't noticed the "Julia 2015" within the actual picture in picture slide/camera view. Nonetheless the 2017 videos I watched or quickly scanned through are about Julia 0.6 which is 2017 release. That must have been a mistake of video production.
Nice! For those who make extensive use of the REPL, this is a great tool.
What this means is that Julia does not have stable mutlithreading yet, but does have multiprocessing. So you can easily start N linked Julia processes that can communicate data back and forth easily, but you do have to copy the data back and forth from processes unless you use a package such as shared arrays or somesuch. In the future, Julia will allow for multithreaded processing natively and you won't have to copy the data back and forth but even right now, you can use all your cores with Julia. 
Thanks for the explanation!
There are three concepts you need to understand to parse this bit of the manual: green threads, native threads (sometimes/historically called "pthreads"), and processes. * Processes: A process is a construct managed by the operating system that represents an isolated "unit of work" or, effectively, a running program. The operating system handles the scheduling of processes and distribution to different cores. Modern operating systems will distribute running processes across as many cores as are available. * Native Threads: A native thread is a construct *within* a process that represents work that should be performed sequentially. Native threads are *also* managed by the operating system and can be scheduled across multiple cores. * Green threads: A green thread is a construct of a programming language that, on the surface, *looks* like a native thread but is managed by the programming language runtime instead of the operating system. *Usually* (but not always...more on this later) all of the green threads in a process are restricted to running on a single native thread, and therefore effectively *cannot* be scheduled to run across multiple cores. So, the short answer is: Julia threads are green threads and cannot utilize multiple cores, but Julia has a number of mechanisms (e.g. `julia -p N`, Shared Arrays) that make working with multiple processes fairly convenient. You might be wondering: if multiple Julia processes can run across multiple cores, why would I ever use Julia threads? Indeed, I've seen far more Julia programs use multiple processes than threads (currently, this could change if Julia gains *true* native thread support). There are two reasons you might favor threads to processes: 1. Processes are *isolated*, which means that getting data from one process to another requires a bit of work (hence Shared Arrays) and can incur some amount of overhead. If your tasks are short-lived or your data is particularly small, the complication of multiple processes might not be worth it. 2. As alluded to in the manual excerpt you quoted, I/O heavy tasks can take advantage of multiple cores *even* though Julia only has green threads. This is possible because I/O involves work that the operating system needs to do (i.e. fetch data from disk or read it from a network socket). Julia uses "non-blocking" I/O operations to essentially tell the operating system to go fetch data (which it is free to do on a different core) then return control to Julia immediately so that Julia can do some other work. Later, Julia will check in to see if the I/O request has completed. So for I/O heavy tasks, using threads in Julia can actually be *more* efficient than multiple processes (again, due to overhead). One final note: I mentioned that green threads are not *always* restricted to a single core. Really, *any* restriction on how green threads operate is up to the language runtime, since green threads are a construct of the runtime. As such, Julia has an experimental feature `@threadcall` which utilizes a thread-pool for executing green threads. What this means is that Julia allocates some number of *native* threads (4 by default) and will schedule your *green* threads across those *native* threads, giving you, in effect, the ability to utilize up to 4 cores (by default). It's worth noting that this is still an experimental feature, and will likely (in most circumstances) not be *as* efficient as true native threads, but it could potentially improve the performance of your compute heavy tasks on multi-core machines.
I really appreciate you taking the time to write such a detailed answer. It doesn't seem all that complicated the way you describe it, but I think I'm missing some basic cs knowledge that's making it tough for me to wrap my head around (my cs knowledge is slap dash and I've never looked into parallel computing). Is there some basic loop or an example of some function that would illustrate the difference between processes, native threads and green threads? 
Du you have any comments regarding the @threads macro to annotate loops? 
Oh my god. I love you, and Tim Holy of course. My life has just improved 30 percent
&gt;Du you have any comments regarding the @threads macro to annotate loops? The `@threads` model is true multithreading like `@threadcall`(i.e. not green threads). It says "experimental" in the docs, but it's pretty well established by now. If the loop is threadsafe it'll work just fine, just watch out for the closure inference bug.
It's made the top 50 3 times already. It's been hovering around 50 for awhile. I hope that v1.0 pushes it into at least the steady 30's above something I'd call big like Haskell.
Good to know - I will correct my post. I've been checking Tiobe Index for a while, most of the months and I always found Julia on The Next 50 Programming Languages list. I hope it will stay in the top 50 from now on.
I don't think language improvements will take julia much higher at this point. I think what it needs are tools. No matter how good the language is, the tools surrounding the languages julia competes with are what draws people to them. 
Who said anything about language improvements?
Actually, 0.6 was supposed to be the last big revision of the language proper before 1.0. The roadmap has been to focus on tools after 0.6 for a while. 
Yeah. v1.0 is about language stability so you can build tools knowing that the language will not change and break them. This is extremely important. Notice that things like the debugger are broken on v0.6 precisely because of language changes. Every few months people have to run around putting out fires in their large applications due to breaking language changes. It's nice to make sure that the language is great, and now it's pretty pretty great at what it does so it'll be helpful to have a release that says "from this point forward, you will have a stable version of Julia which will last a few years and the tools you build will not break". That's what v1.0 is. So @WrongAndBeligerent had an odd comment because me asking for v1.0 is essentially the same thing as what he said, "No matter how good the language is, the tools surrounding the languages julia competes with are what draws people to them.". 1.0 is required to really build out the tooling. I'll just link to Stefan's explanation of 1.0 here: https://discourse.julialang.org/t/list-of-most-desired-features-for-julia-v1-x/4481/110 &gt;the strategy that we're taking is to stabilize the core language first, then turn our attentions to important packages, tooling, and compiler improvements that we can make without breaking things for people. Since the core language will be stable, we will have a solid foundation to build on. If we had unlimited resources, sure, we would release Julia 1.0 and all of the packages and tooling one could possibly want in perfect shape. But our resources are far from unlimited, so we're taking the same strategy that Rust, Go, Clojure, Swift, and every other new programming language I can think of has taken: stabilize the core language, then once that's done, turn your attention to important libraries and tools. 
 7. Visual Basic .Net 16. Visual Basic 19. Scratch 31. Prolog That's some unexpected languages on that list. Not visual basic as much but still. I'm guessing Visual Basic, Delphi, Ada and Fortran are remnants of a lot of businesses using old systems based on them. And Fortran in scientific community.
Hm yeah Scratch sounds like possibly a Googling error, since it's a common word. TIOBE works by Google IIRC. Or maybe they just have a lot of documentation on the web, but as far as I know it's an educational language and not something that competes with the rest of the languages on the list. I would fully expect VB to be very popular. I wouldn't call it "remnants" -- successful software often lasts for 20+ years. 
&gt;Are there books similar to "The C programming language" or "The Go programming language" (which nicely explain ideas, concepts, idioms, tips and tricks for the language) out there? Not yet. But out of the selection, Avik's book is pretty good. But with all of them, they suffer from "pre-1.0 decay", i.e. they all end up on old versions of Julia where syntax and "truths" are different but not updated. That's just a virtue of being released before the language is stable.
The wikibook is mostly updated for v0.6. But, it's not aimed at people who are already reasonably fluent in a language. You're probably better off checking out some YouTube tutorials and notebooks ... or perhaps try [Chris](https://ucidatascienceinitiative.github.io/IntroToJulia/)' notebooks.
I think the manual is pretty approachable if you have some background in other programming languages: https://docs.julialang.org/en/stable/manual/introduction/
[removed]
I have learned quite a bit by poring over Julia discussions (either Github issues, or on the dev Google group). People have put a lot of thought into various aspects of the language, and have put a lot of effort into communicating the design principles. In my experience, the project I was working on served as a nice context to motivate those questions, for which I had to read through discussions. (This might be an artefact of my learning the language in its early days. Maybe it will become much easier once the language stabilizes). That said, one great resource to check out would be talks by Stefan Karpinski and/or Jeff Bezanson, on the core design principles of Julia. There are many (with significant overlaps)... just search on YouTube and go through a few of them.
And Jeff's PhD thesis.
Every time you assign a name to a lambda a fairy dies. ``` def f(x): x**2 + 2*pi ``` is pretty nice :)
Thanks for the recommendation, Chris, appreciate it. I'm happy to report that a second edition of the book is being prepared, in order to update it to Julia 1.0
Most of these should be fixed now, thanks!
I've used this before, I think it will work for what you're describing
 julia&gt; Base.return_types(sin, (Int,)) 1-element Array{Any,1}: Float64
Thanks!
What are you trying to acheive here? If you just want to get the result of a mod operation in a 2d Array, you can use: ``` y=(t-&gt;mod(t, p)).(x) ``` where p is the base.
The end result should be a shaded circle/ellipse. I want to find the r values using `r=sqrt(x.^2+y.^2)`and then `theta =atan(x./y)` where they would form a circle and the circle would be filled with a flux value obtained from the r values. I can get the gradient okay, with x and y as arrays of the same values, but I can't calculate the polar coordinates that way.
 using ForwardDiff function test(X) foobar = zeros(10) foobar[1:10] = X return foobar end function test2(X) foobar = zeros(10) foobar = X[1:10] return foobar end function test3(X) foobar = zeros(10) foobar = X return foobar end function test4(X) foobar = zeros(10) foobar .= X return foobar end ForwardDiff.jacobian(test,randn(10)) ForwardDiff.jacobian(test2, randn(10)) ForwardDiff.jacobian(test3, randn(10)) ForwardDiff.jacobian(test4, randn(10)) Here is some more code detailing the tests I've done. 2 and 3 work perfectly and give the correct answer, but 1 and 4 return the error.
`foobar[1:10] = X` and `foobar .= X`. You can't put dual numbers into an array of floats. Change the previous line to `zeros(eltype(X),10)` to make it match the type of `X`.
Ah, of course. For some reason in my brain I was think "I'm not specifying a type when I initialize the zero array so it should be able to take anything" which is obviously wrong. Thank you!
Slightly different, but related question: Do you know if the dual number type plays nicely with sparse matrices? My function is yielding different jacobians based on whether I put the output vector as sparse or not (despite the functions evaluating to the same vector).
Dual numbers are just a number type so it'll do fine. I'm not entirely sure what your issue is. Can you give a MWE?
Working on diagnosing specifically what's causing the problem right now so I can get a MWE without using my massive function. In essence the problem is function myfunction(input) output = zeros(eltype(input) , size) #allocate output here return output end and function myfunction(input) output = spzeros(eltype(input) , size) #allocate output here return output end return the same value for all the different inputs I've tried, but do not return the same jacobian. #allocate output here is line-for-line exactly the same between the two function, the only difference is whether the output vector is declared as sparse or not. EDIT: something very strange is going on. Even function myfunction (input) #compute output return output end function mysparsefunction(input) return sparse(myfunction(input) end return different jacobians.
You can definitely use that second one to make an MWE?
Haven't been able to isolate what exactly is causing the issue; I can't seem to reproduce it in a minimal way.[ Here's](https://pastebin.com/VMg9BCX1) a pastebin with code that will reproduce the issue with a lot of the setup cut out. The jacobian it generates is largely NaN's, but the inequalities between the two matrices are in the exact same positions with the exact same magnitude, so there's no NaN funny business causing this. I don't expect you to read through my messy hacked together code, and I've other projects to work on, so I may focus on isolating this problem more and make a new thread when I've figured out what is actually causing it.
There is a books section on the Julia - learning page: https://julialang.org/learning/
LinearMaps is also more performant when using wrapped sparse matrices.
about 0.4 
I have no special insight...just Google [Roadmap](https://discourse.julialang.org/t/1-0-roadmap/2643) 
[this] (https://github.com/JuliaLang/julia/milestones) might give you an idea
Why are the recordings of people's screens done via a camera, instead of via a screen or feed capture? While it still does the job, it isn't as well (camera sometimes cuts off parts of the display it seems, and takes time to adjust to changes in contrast). Other than that - watched a few now, very glad they are recorded and up on YouTube!
What do you have against comets?
My guess is in 8 months.
i don't think there are any pre-built 0.6 binaries for pkg, so ports would be the way to get the latest 0.6.0 milestone. you could also clone the git repository and get 0.6.1-pre.0 or even HEAD if you're into that sort of thing. there are freebsd build notes on github.
Thank you - cloning the git repo and following the excellent instructions worked well!
https://www.youtube.com/watch?v=qHpaztMu_Uw
&gt;[**JuliaCon 2017 | Julia Roadmap | Stefan Karpinski [18:40]**](http://youtu.be/qHpaztMu_Uw) &gt; [*^Julia ^Language*](https://www.youtube.com/channel/UC9IuUwwE2xdjQUT_LMLONoA) ^in ^Education &gt;*^340 ^views ^since ^Jul ^2017* [^bot ^info](/r/youtubefactsbot/wiki/index)
I had a similar problem and there is no ready-made solution for it in base, so i found a matlab one and tweaked it. This code does upper triangular indexing, but you can rewrite this for LTR with little to no effort. function triangular_index(i::Int,j::Int) Int(round(j * (j - 3) / 2 + i + 1)) end function reverse_tri_index(k::Int) j = round(floor(-.5 + .5 * sqrt(1 + 8 * (k - 1))) + 2); i = round(j * (3 - j) / 2 + k - 1); Int(i),Int(j) end 
Did you try running using IJulia notebook() Does that open a Jupyter notebook with Julia in it? If that works, but it is opening a different version of jupyter than the one you normally use, then you should try the troubleshooting steps outlined here: https://github.com/JuliaLang/IJulia.jl 
When I use julia&gt; notebook() I get this message: Process(setenv('/usr/local/bin/jupyter notebook'; dir="/root"), processExited(1)) Another problem is that I'm doing this in a server running a JupyterHub instance. Users don't can't access the terminal to use Julia's console.
Try doing something like this ``` ENV["PYTHON"]="&lt;insert path to your Python executable that Jupyter uses here&gt;" Pkg.build("IJulia") ``` That should set you up. Technically you shouldn't even need the ENV setup, but this prevents you from installing a Julia-specific Conda just in case. In either case it should install the kernelspec file (the thing that really matters) to the right place anyway. Edits: formatting, function call was mistyped
Thanks. Yeah I was hoping Julia might have had some of the nice triangular functions like numpy has. You're comment was very helpful thanks!
No success :(
Can you post the output of Pkg.build("IJulia")? That is kinda the canonical way of installing it so I'm wondering what's going wrong. Did it say something like "Installed kernelspec to..."
Thank you very much for this video!
It’s `def f(x): return x ** 2 + 2 * π`.
julia&gt; ENV["JUPYTER"]="/usr/local/bin/jupyter" "/usr/local/bin/jupyter" julia&gt; ENV["PYTHON"]="/usr/bin/python3" "/usr/bin/python3" julia&gt; Pkg.add("IJulia") INFO: Installing BinDeps v0.4.7 INFO: Installing Compat v0.26.0 INFO: Installing Conda v0.6.2 INFO: Installing IJulia v1.4.1 INFO: Installing JSON v0.9.1 INFO: Installing Nettle v0.3.0 INFO: Installing SHA v0.3.3 INFO: Installing URIParser v0.1.8 INFO: Installing ZMQ v0.4.3 INFO: Building Conda INFO: Building Nettle INFO: Building ZMQ INFO: Building IJulia INFO: Recompiling stale cache file /root/.julia/lib/v0.4/Conda.ji for module Conda. INFO: Found Jupyter version 4.3.0: /usr/local/bin/jupyter Writing IJulia kernelspec to /root/.julia/v0.4/IJulia/deps/julia-0.4/kernel.json ... Installing julia kernelspec julia-0.4 [InstallKernelSpec] Installed kernelspec julia-0.4 in /root/.local/share/jupyter/kernels/julia-0.4 INFO: Package database updated INFO: METADATA is out-of-date — you may not have the latest version of IJulia INFO: Use `Pkg.update()` to get the latest versions of your packages julia&gt; Pkg.build("IJulia") INFO: Building Conda INFO: Building Nettle INFO: Building ZMQ INFO: Building IJulia INFO: Found Jupyter version 4.3.0: /usr/local/bin/jupyter Writing IJulia kernelspec to /root/.julia/v0.4/IJulia/deps/julia-0.4/kernel.json ... Installing julia kernelspec julia-0.4 [InstallKernelSpec] Removing existing kernelspec in /root/.local/share/jupyter/kernels/julia-0.4 [InstallKernelSpec] Installed kernelspec julia-0.4 in /root/.local/share/jupyter/kernels/julia-0.4
You could use an array comprehension to grab the strictly lower triangular data as a vector: strictLTvector(x) = [x[i,j] for j in 1:size(x)[2]-1 for i in j+1:size(x)[1]] m = minimum(strictLTvector(rand(100,100))) 
Alright I know what's going on now. So you are running this stuff as root (which I feel obligated to anti-recommend). So Jupyter notebooks look in two places for your kernelspecs: #1 in $HOME/.local/share/jupyter/kernels, and #2 in /usr/local/share/jupyter/kernels This means that when you want to install a kernelspec system wide, you want to write the files to location #2, but IJulia isn't really meant to run as root so it doesn't plan to ever write anything but the #1 location. TL;DR run this after install (copy the Julia kernelspec from root's $HOME to /usr/local/share/jupyter/kernels) cp /root/.local/share/jupyter/kernels/julia-0.4 /usr/local/share jupyter/kernels/julia-0.4 And that should fix your problem. PS Please update your Julia version if you can, several releases have come out since 0.4.
do you have slides or course notes to share too? looks interesting, might want to look a bit deeper into it later
Any updates or MWE's? I'm also curious about this. 
Hey! Thanks for the interest. All the slides and course notes are embedded into the Julia notebooks. The only thing missing are the official solutions to programing assignments, but some students have pushed their correct solutions to their branches of the repository.
That's what I ended up doing! Thanks!
Do you mean [`issubset`](https://docs.julialang.org/en/stable/stdlib/collections/#Base.issubset)? You can even use the infix unicode `⊆`: julia&gt; [1,3] ⊆ [1,2,3] true Note that the word "SubArray" in Julia refers to the type of an array that *shares memory* with another array — that is, it's a view into the same data.
That looks interesting! I’ll certainly try it out.
Thanks, any feedback is welcome!
Look very nice but I wasn't able to compile it on a fresh 0.6 julia (Ubuntu 16.04). Any ideas why? lali@TITAN-L:~$ julia --color=no _ _ _ _(_)_ | A fresh approach to technical computing (_) | (_) (_) | Documentation: https://docs.julialang.org _ _ _| |_ __ _ | Type "?help" for help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 0.6.0 (2017-06-19 13:05 UTC) _/ |\__'_|_|_|\__'_| | Official http://julialang.org/ release |__/ | x86_64-pc-linux-gnu julia&gt; using GtkIDE INFO: Precompiling module GtkIDE. Reading package lists... Done Building dependency tree Reading state information... Done libgtksourceview-3.0-1 is already the newest version (3.18.2-1). libgtksourceview-3.0-1 set to manually installed. 0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded. (julia:7883): GtkSourceView-ERROR **: Error while loading the completion UI: .:23:106 Invalid object type 'GtkSourceCompletionInfo' ERROR: LoadError: Failed to precompile GtkSourceWidget to /home/lali/.julia/lib/v0.6/GtkSourceWidget.ji. Stacktrace: [1] compilecache(::String) at ./loading.jl:703 [2] _require(::Symbol) at ./loading.jl:456 [3] require(::Symbol) at ./loading.jl:398 [4] include_from_node1(::String) at ./loading.jl:569 [5] include(::String) at ./sysimg.jl:14 [6] anonymous at ./&lt;missing&gt;:2 while loading /home/lali/.julia/v0.6/GtkIDE/src/GtkIDE.jl, in expression starting on line 9 ERROR: Failed to precompile GtkIDE to /home/lali/.julia/lib/v0.6/GtkIDE.ji. Stacktrace: [1] compilecache(::String) at ./loading.jl:703 [2] _require(::Symbol) at ./loading.jl:490 [3] require(::Symbol) at ./loading.jl:398 julia&gt; 
I don't really support linux (since I don't have a linux machine at the moment), but in theory it should be possible to make it work. The installation process is a bit [wonky](https://github.com/jonathanBieler/GtkSourceWidget.jl/blob/master/src/GtkSourceWidget.jl#L34), as I never understood how BinDeps work. That said it seems you have the library installed. Maybe run just those line and check if libgtksourceview point to something reasonable. Reading this, it could be an issue about how it's loaded. https://forum.gtkd.org/groups/GtkD/thread/492/ 
[jump](https://www.youtube.com/watch?v=SwYN7mTi6HM)
It is a set of macros for doing linear programming. https://en.wikipedia.org/wiki/Linear_programming It is like Solver in Excel / OpenOffice I used it to do some solving for playing Factorio https://www.reddit.com/r/factorio/comments/5z6yfx/using_linear_programming_in_julia_to_optimize/ One of the solvers is Gurobi, perhaps their website offers the insight you are looking for http://www.gurobi.com/ I did a course on it at Edx, perhaps they will run it again this year https://www.edx.org/course/optimization-methods-business-analytics-mitx-15-053x 
I too have some difficulty understanding what it's for, and when to use it instead of Optim.jl, or other packages for the other flavours of mathematical optimization. I suspect there is a certain amount of subtlety involved in understanding which method to use for a particular class of problem. It feels like this is the sort of thing which could be conveyed by the appropriate expert in a few pages. But as of now, I have never found a guide which would help a technical user understand the differences that was shorter then an undergraduate semester module. 
More relevant JuMP videos: [one](https://www.youtube.com/watch?v=xtfNug-htcs), [two](https://www.youtube.com/watch?v=7LNeR299q88). See also [juliaopt.org](http://www.juliaopt.org/).
You can use it to formulate not just linear programs, but also other classes of mathematical programs (quadratic programs, second order cone programs, semidefinite programs, mixed-integer programs, full nonlinear programs).
JuMP is a very convenient way of communicating with optimization solvers, which tend to be complicated and finicky programs not written in Julia. Suppose you want to solve a large mixed-integer linear program. Solving MIPs is hard (in the computational complexity sense), but there are certain very complicated finely engineered programs which can solve MIPs. These are programs like CPLEX, Gurobi, SCIP, etc. So maybe you decide you'll use Gurobi to solve your MIP. How do you actually communicate the details of your problem to Gurobi? How do you tell Gurobi what the variables are, what the constraints are, etc.? If you're working in a language that Gurobi has written an API for, you can use that API. Julia isn't one of those languages, so without JuMP you'd have to manually construct a file describing your problem or call to a language which can call Gurobi. Both work, but both are headaches. Enter JuMP. JuMP provides really easy syntax for describing your optimization problem in Julia. JuMP then communicates that problem to the solver, gets the solver's answer, and returns the answer to you. This means the user never has to worry about communicating directly with the solver, language bridges, etc.; the user just expresses their problem in simple Julia syntax. Additionally, using JuMP brings big quality-of-life improvements over interfacing with a solver directly. With JuMP you can easily swap in/out different solvers, whereas if you use e.g. the Gurobi-specific API and then decide to try CPLEX, you're out of luck. Because JuMP allows swapping solvers, you can also explore different formulations which a single solver might not support (e.g. not all linear programming solvers support discrete variables). Why use JuMP + external solver as opposed to something pure Julia such as Optim.jl? There are trade-offs both ways, but the primary reason to use an external solver is that (for now), stand-alone solvers tend to give the best performance and support the widest array of problem types.
Optim.jl provides various solution algorithms for nonlinear programs in pure Julia code. JuMP doesn't directly provide solution algorithms. It allows you to formulate an optimization problem (mathematical program) in a solver-agnostic fashion, and then solve it using any solver that: 1. has an associated Julia package that provides the interface between JuMP (or rather, MathProgBase, a lower-level dependency of JuMP) and the solver, and 2. is appropriate for the particular class of optimization problems to which the problem you've formulated belongs. See http://www.juliaopt.org/JuMP.jl/0.18/installation.html#getting-solvers for a list of compatible solvers, and the problem classes that they can handle. JuMP is good at keeping track of the particular class of optimization problem you've formulated (LP, QP, SOCP, SDP, MI, NLP), allowing you to use a more specialized solver (that exploits the structure in the problem) than the general NLP solvers that Optim.jl provides.
One thing other commenters have missed is that JuMP has a very special syntax (a DSL) for defining your problem so that way it can be analyzed in JuMPs macros. JuMP will perform autodifferentiation to automatically build sprase Jacobians and Hessians, and give those to the optimizers as well. So it'll help make your program run faster (though other Julia tools like Cassette.jl are coming up to replace this part of it). Other than that, yes it's a big optimization API. It's the Plots.jl or DifferentialEquations.jl of optimization: call "all" packages from one API, and have it understand how to change the arguments for you. However, since it has a macro-based API, it can be really hard to extend since everything needs to be passed as expressions. As with every software engineering choice, there's pros and cons.
A Julia user and factorio player, you and I have a couple things in common. I guess my issue is that I don't understand linear programming. I think that if I understood linear programming and the problems solved my linear programming I would understand JuMP. Edit: That Edx link is great. I will have to checkout that course.
Does such a set have a class designation, such as "constraint systems programming"? 
the secret is to watch the van halen video as lubin explains jump
I'm interested. I learned Julia from a previous Pakt book, Sherrington's Mastering Julia. I'm certified in JuMP from Edx. I use Julia for Computational Logistics and am currently a mature student going into my final year undergraduate degree in Supply Chain Management. If that sounds suitable I'm up for it.
I’m interested. Have worked with Manning Publications previously but not Packt. Is there a way to contact you outside of Reddit? (PM is fine if you don’t want to list an email publicly). 
I'm interested. I have approximately two years of experience working with Julia, in fields of statistics and computational neuroscience.
I'm interested. I have three years of experience working with Julia in physics. I also attended juliacon.
"Constraint programming" is a slightly more complicated class that includes logical constraints. You could write a JuMP extension to do that, but it's a different set of solvers. I'd normally call the typical problems JuMP is used for "constrained optimization."
Good luck, this looks to be a great idea! Have you tried it out on some test subjects -e.g. people who are learning their first programming language? I'd be interested in your (and their) experiences. Did you go for the terminal/REPL or Jupyter notebooks?
Half-Life 3. Enjoy. (Tiddies) ^^Have ^^a ^^nice ^^day ^^by ^^the ^^way.
Jinx! You and Itsamegamer17522 posted the same comment at the same time! See their comment [here](https://www.reddit.com/comments/6uo6qy/_/dlu57f1). --- I am a bot who is owed many Cokes.
`good bot`
Welcome to bot's army. ^^I'm ^^a ^^commander ^^of ^^bots ^^army.
Post on the Discourse. Steven Johnson, the creator of both NLopt and NLopt.jl, roams those forums and should be able to help you out. https://discourse.julialang.org/
Thanks for the hint!
This book will apparently be released in the next few days. Has anybody here reviewed it please?
So is O'Reilly officially done with drm free ebooks? 
Up vote for the thought. 
I should really figure out how to work with macros and generated functions. I've avoided them up to now since they look very dense, but I've recently seen some stuff which has convinced me to bite the bullet.
pakt publishing books are usually around the level of apress (piles of shit on fire).. I've been asked to review one before and the outcome was so horrifying that i never actually finished the review because i didn't want anything to be taken out of context and me contributing to anything good said about the book i reviewed :(
They aren't as scary as you imagine.
What version of Julia are you using? It looks like there have been several improvements to the parsing speed of DateTimes in Julia v0.6: https://github.com/JuliaLang/julia/issues/15888
Have you tried running the profiler? At least in a previous verision of Julia the parse function is quite heavy. Not sure how it is today
That helped speed up by 2x. Still slower than python, but better than before.
I checked the profiler, it seems that more than half of the time the code is parsing the date information.
Thanks, it's now 5 times faster than the equivalent python code. I updated julia version and changed to use split instead of treating the string as a Date. Here is the updated code: #!/usr/bin/env julia const market_count = zeros(Int, (7, 24)) const market_names = Dict{Int, String}( 1 =&gt; "XX", 2 =&gt; "NA", 3 =&gt; "OC", 4 =&gt; "EU", 5 =&gt; "AS", 6 =&gt; "AF", 7 =&gt; "SA", ) get_datetime(dt::SubString{String}) = Dates.DateTime(dt, "yyyy-mm-ddTHH:MM:SSZ") function main() f = open("test.dat") for line::String in eachline(f) line = strip(line) fields = split(line) market = parse(Int, fields[2]) + 1 time = split(fields[1], 'T')[2] hour = parse(Int, split(time, ':')[2]) + 1 market_count[market, hour] += 1 end for i in 1:24 print("$i ") for k in 1:7 print("$(market_count[k,i]) ") end println("") end end main() 
I'm sure that once you get how they work it's not hard, but it seems like you need to know quite a few prerequisites which don't seem useful by themselves but which fall in place once you get macros.
If you really care about speed, you should try to tokenize the string yourself. Note that you can do even better if the strings are all formatted the same way -- you know the hour is at line[12:13] and the market is [21:end]. In benchmarks that version came in around 150ns. However, if you do need to parse it out for some reason, you can see the results here. julia&gt; function old_way(line::String) line = strip(line) fields = split(line) market = parse(Int, fields[2]) + 1 time = split(fields[1], 'T')[2] hour = parse(Int, split(time, ':')[2]) + 1 market_count[market, hour] += 1 end^C julia&gt; function new_way(line::String) p1 = findfirst(line, 'T') + 1 p2 = p1 + findfirst(line[p1:end], ':') - 1 p3 = p2 + findfirst(line[p2:end], ' ') hour = parse(Int64, line[p1:p2-1]) market = parse(Int64, line[p3:end]) market_count[market, hour] += 1 end julia&gt; @benchmark new_way("2017-07-01T13:21:06 7") BenchmarkTools.Trial: memory estimate: 128 bytes allocs estimate: 4 -------------- minimum time: 337.837 ns (0.00% GC) median time: 340.740 ns (0.00% GC) mean time: 359.372 ns (2.60% GC) maximum time: 9.592 μs (91.78% GC) -------------- samples: 10000 evals/sample: 227 julia&gt; @benchmark old_way("2017-07-01T13:21:06 7") BenchmarkTools.Trial: memory estimate: 816 bytes allocs estimate: 16 -------------- minimum time: 1.047 μs (0.00% GC) median time: 1.071 μs (0.00% GC) mean time: 1.219 μs (7.61% GC) maximum time: 320.340 μs (97.07% GC) -------------- samples: 10000 evals/sample: 10 
So any reason why we can't find this book on US Amazon, OReilly main site, or look inside on UK Amazon? Is it still being released?
&gt;Version 0.1.2 was released three days ago, which I would think is long enough for it to be reflected in METADATA. But maybe not, because Pkg.update("Permutations") does not bring me to 0.1.2. It hasn't been released yet. https://github.com/JuliaLang/METADATA.jl/pull/10919
Ah, pardon me. I am not particularly capable with github and misread the [releases page](https://github.com/scheinerman/Permutations.jl/releases). Out of curiosity, if this doesn't mean that 0.1.2 was released three days ago, what does it mean?
&gt;Out of curiosity, if this doesn't mean that 0.1.2 was released three days ago, what does it mean? They created the tag to open the METADATA PR, but it won't actually be released until after a review by the package manager gatekeepers to make sure it's safe and etc. etc.
That makes perfect sense. Thank you very much!
If you are desperate for the latest version, you can still get it. Do `Pkg.checkout("Permutations")` to download the latest master branch, or `Pkg.checkout("Permutations", &lt;branchname&gt;)` to get a particular branch. I'm not sure if `Pkg.pin()` works for future versions of packages, but that's a way to get a specific version regardless of what the latest version of METADATA is. I was recently digging through the docs for packages, and they're actually really clear, though might be a bit confusing if you're a novice with git.
A direct link to the course: https://www.coursera.org/learn/julia-programming?siteID=_mi9sWvzvR8-dGpp4kF_lk0fscpcv82RNQ&amp;utm_content=10&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=%2Fmi9sWvzvR8 
Well, at least the authors aren't "random" people, though I didn't know Leah was still active in or around Julia.
School kids is the answer there - the amount of school kids looking for help with their projects is huge. 
Sweet!
Whats the difference between BuildExecutable and https://github.com/JuliaComputing/static-julia? For reference: https://discourse.julialang.org/t/compiling-and-building-binaries-from-your-julia-code/5687 
static-julia statically compiles the code. This is like compiling C or Fortran code. BuildExecutable builds an executable which includes the Julia runtime, which is more like MATLAB, R, or Python executables.
Discussion here: https://discourse.julialang.org/t/compiling-and-building-binaries-from-your-julia-code/5687
Thanks!
This is your issue right here. ```bash CondaIOError: Missing write permissions in: C:\Users\nk\.julia\v0.6\Conda\deps\usr ``` One way to go about this is to install Conda manually, and direct your PyCall to use that Conda's Python. You can direct it to Conda's Python, or direct it to an Python in a Conda environment. I recommend the latter, since it is a cleaner way to do this. 1. Install Miniconda. I usually install it in C:\Miniconda3 2. Open a new command line window 3. Type `where conda`. You would see the new installation as a first ( and possibly only ) item in the list 4. Type `conda create -n julia_python python=3`. This will create a conda environment. 5. Type `activate julia_python` or `source activate julia_python`. This will activate the conda environment. 6. Type `where python`. This will tell you where the Python is located. 7. Type `julia`. Type `ENV["PYTHONHOME"]="C:\path\to\miniconda3\env\Scripts\python.exe"`. Type `Pkg.build("PyCall")`. 8. Type `using PyCall`. If you ever want to install a Python package manually in that environment, you can `activate julia_python; pip install package_name` I don't have a Windows machine so I can't post more detailed instructions without risking making some mistake, but you should be able to figure it out from these instructions. If you have any questions feel free to ping me.
Thank you for the help! Following the prompt, I tried editing `ENV["PYTHONHOME"]="C:\path\to\miniconda3\env\Scripts\python.exe"` to `ENV["PYTHON"]="C:\path\to\miniconda3\env\Scripts\python.exe"` which works while the former returns the same error.
is there anything yet to create julia functions callable from R?
There is XRJulia which opens socket connection between Julia and R and does the data exchange via JSON for you. There is also rJulia which is probably closer to what you need,but it seems to be abandoned.
not quite what I want. I'd love to write libraries for R using julia. Users should be able to call them without having julia installed, sort of like what Rcpp does.
Coming from Matlab? That's easy. Julia can actually parse files (quickly, correctly). It doesn't crash when working with large data sets. It can integrate with C libraries easily. It's free! It is fast, even for non-vectorized work loads. It can be parallelized somewhat easily. It starts quickly. It can save figures as PDF. You can metaprogram (macros, general sane-ness). It's easy to extend. It has a package manager. You can use it for automation (no need to activate, can run without a window). It can be deployed easily. If you find a bug, you can fix it. 
Although it's targeted at math it's general purpose. I've used it as an SQL client producing Excel files for my management colleagues. Which gives me an excuse to post my code for writing Excel via Python https://github.com/lawless-m/XlsxWriter.jl 
I see. I would love that too. I don't know C very well but looking at stat-julia, it seems like we already have most of the work done. Wrapping a shared library generated by static-julia using Rcpp should not be too difficult, right?
Adding to the list (since I am unfortunate enough to be doing all my daily work in Matlab): * You can define and use functions at the REPL. In Matlab, you have to save a new `.m` file to make a fully-fleged function. Sounds small, but when you're trying to rapidly prototype, it's an absolute pain. * Shared memory can be used. For extremely large data sets, simple/dumb parallelization can all share one copy of the large data. Matlab, you just need more RAM to allow every launched instance to load its own copy of the very-large-dataset. * Code modularization is better. Matlab does everything path based, and they allow hiding private functions in a `private/` subfolder, but then you can't reuse that code elsewhere if you have a really good reason to. In Julia, you just import an unexported function. * Macros. 'Nuff said. * You can actually implement simple data structures in the language. Ever try making a linked-list implementation in Matlab? Yeah, it doesn't work... * Real strings. (Actually, in Matlab it's even worse — starting in R2017a, there are both charcter arrays which have always been their version of strings *and* [I think Java-based] string objects. Isn't that going to be fun going forward...) One correction, though, is that Matlab *can* run without a window — we do it all the time on a supercomputing cluster. `matlab -nodesktop -nosplash` (well, at least on Linux that is.) The thing, though, is that headless plotting in Matlab seems to be subtly and unpredictably different from the same operations in headed mode. Say goodbye to plot consistency, and be ready for different bugs in each mode that have to be worked around. Edit: * Another one I've been coming to like recently is transparent support for replacing all double-precision floating point numbers with `BigFloat` and doing numerical tests. No need to rewrite special versions to handle arbitrary precision numbers, or import values from Mathematica for comparison — you just do all the validation in Julia.
https://www.reddit.com/r/Julia/comments/6h22bk/reasons_you_use_julia/ https://discourse.julialang.org/t/julia-motivation-why-werent-numpy-scipy-numba-good-enough/2236/7
Um... You probably shouldn't be making these arguments. Humans will always use a tool they understand, even in an incorrect way, to do a job as long as the time to produce the output is faster than the opportunity cost of learning the new tool first, then producing the output. Never do a checklist for languages. You're wasting your time. To convert your engineering team to a new language, just start having weekly learning meetings with your team, and just use it to produce outputs so your team members can have references to build off of. Also, build infrastructure so your team can share and use both versions of code. Are your pipelines language agnostic? If not, work on that first.
Checkout master for fancy graphics
Head over to the the discourse room. Lots of activity on gitter too. Here’s a list of places to check out. https://julialang.org/community/
A very nice place: https://discourse.julialang.org/
slack is very active 
The most active communities are: 1. Discourse: https://discourse.julialang.org/ 2. Slack: https://slackinvite.julialang.org/ 3. Gitter: https://gitter.im/JuliaLang/julia 4. Stack Overflow: julia-lang tag https://stackoverflow.com/questions/tagged/julia-lang 5. #julialang on Twitter: https://twitter.com/search?q=%23JuliaLang&amp;src=tyah 6. JuliaBloggers: http://www.juliabloggers.com/ In addition, there are many Gitters for different packages. Some of the most trafficked are: 1. DifferentialEquations.jl Gitter: https://gitter.im/JuliaDiffEq/Lobby 2. Plots.jl Gitter: https://gitter.im/tbreloff/Plots.jl 3. Juno Gitter: https://gitter.im/JunoLab/Juno
No oldschool IRC like #python?
There's #julia and it is linked with the Gitter through a bot. 
&gt; I have no intention of leaving /r/julia of course; /r/julia is probably the least active of them all though! You're in for a treat if you've never visited slack or gitter. Lots of people willing to help and discuss Julia.
For plotting, I suggest the Plots package. You can find everything (tutorials, examples, etc.) here: https://juliaplots.github.io/
Specifically, the StatPlots.jl recipes for Plots.jl may have what you need: https://github.com/JuliaPlots/StatPlots.jl
Thanks!
Link above doesn't work. Use this: https://www.coursera.org/learn/julia-programming
Looks like it needs an updated version of Parameters.jl and JuliaPro is keeping your versions back. I would recommend uninstalling JuliaPro and using a standard Julia installation instead.
Might need some more detail on exactly how to use it. I haven't been able to figure out how to get it to work, and many of the responses on the discourse link say similar. Currently I'm getting the following stacktrace: Stacktrace: [1] include_from_node1(::String) at ./loading.jl:569 [2] include(::String) at ./sysimg.jl:14 [3] process_options(::Base.JLOptions) at ./client.jl:305 [4] _start() at ./client.jl:371 Do I need to wrap it all in a main() like BuildExecutable.jl? Am I limited in which functions I can use? Do I need to use a specific version of Julia?
Could you possibly give me a snippet of valid input data and the expected output, please? 
Thank you so much, that fixed the problem!
Down to 47th in September. Still, really good news! Rust is only at 49th.
[GR.jl](https://github.com/jheinen/GR.jl) is a nice package with some good 3D capabilities. As mentioned Plots.jl is very useful in providing a standard API and supports GR as a backend. # Load package using Plots # Switch backend gr() [GR Website](http://gr-framework.org/) [Surface Plot Example](https://gr-framework.org/examples/kws2.html)
`Nullable{T}` is on the way out. I'm personally hoping that the `Union{T, Null}` approach wins out, but it's [still up for debate](https://github.com/JuliaData/Roadmap.jl/issues/3#issuecomment-330691469). There's lots of awful ways to get a DataFrame of real numbers, but I would do something like: df2 = DataFrame([[get(v) for v in df[c]] for c in names(df)], names(df)) (Also you can just do `TB3MS = CSV.read( "TB3MS.csv")`). 
Is `Nullable{T}` on the way out as a type, or are the language contributors/designers discussing the underlying representation and syntactic sugar for Nullables?
&gt; Nullable{T} on the way out as a type, or are the language contributors/designers discussing the underlying representation and syntactic sugar for Nullables? `Nullable{T}` is an "Engineer's Null". It's made to not propagate nulls and throw errors when nulls aren't checked. That's perfect for if you want to be checking for null pointers and things like that because you want to error the instant `a .+ b` is passing on your null. However, that's exactly what you don't want with a "Data Scientist's Null" which should just propagate "missing data" easily, so `a .+ b` shouldn't error and should just pass along the `NA` like in R. `Nullable{T}` is staying as the Engineer's Null for development purposes, but the data stack is getting a different NA for its purposes. I think right now there's a lot of work in Nulls.jl (and DataFrames.jl master has this as its backing, and may be released very very soon, as in they were discussing it in a chatroom yesterday about releasing that day). I am just waiting for this to settle a little before diving in myself.
For the series of input lines: 1999-01-01T00:01:00Z 1 15 100 1999-01-01T00:02:00Z 1 270 175 1999-01-01T00:03:00Z 3 7 15 1999-01-01T01:01:00Z 1 1263 12 1999-01-01T01:02:00Z 2 92141 234 The output should be: 1 2 0 1 0 0 0 0 2 1 1 0 0 0 0 0 3 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 ... 24 0 0 0 0 0 0 0
Note that if you don't have null values in your table your can just pass nullable=false to CSV.read.
Thank you very much.
I'm using Julia for my undergraduate physics thesis, I like it because the math syntax is nice(better then num-py, Unicode support), it has a plotting and units package (and more) that's easy to install (unlike c++). And I don't need a compiled, heavily optimized executable, I just need a readable and relativity fast script
Thanks for the response! Would you be willing to expand on this? As a former physics major who switched to computer science I'd love to know more about what you're working on. I'm also curious: * Were Python and C++ the only other languages you considered? Why not MATLAB or R? * Did parallelism and performance influence your decision or was it purely based on the ease of setup and use?
why don’t the benchmarks inspire confidence? It’s true they’re from an older version of Julia, but I don’t think speed is a problem...
I've used Julia for research (PhD+) and I think it doesn't really have competition in that domain. R is really slow and has worse design, matlab is a nice tool but loops are slow and the language is a bit wonky, C/C++ is too slow/heavy development wise. I think the only potential competitor is Python with PyPy but some important libraries like Pandas still don't work with it AFAIK. Performance isn't just a matter of waiting a bit more longer for the results, Julia let you test more things and worry less about implementation (you can use either loops or vectorized operations depending on your problem, while other languages forces one paradigm on you). Ultimately this allows you to do better science (more things are possible). It's very expressive and allow you to write beautiful code, which is very pleasant to work with (of course you can also write ugly code, and Julia has a bit of a learning curve). I think it would also scale very well from research to production, but I don't really have experience in that domain. The downsides is that the language is still moving, so you have to keep up with it (and packages break), and that the packages quality/maturity varies quite a bit. That last point is a disadvantage for end-users, but if you are interested in developing or contributing to packages, it's a plus. 
I echo cormullion's question about the benchmarks - Julia is the fastest language shown, and in particular is faster than other performant-but-high-level languages such as Java and Go. That's not to say there isn't room for improvement or to quibble with the benchmark setup, but if that doesn't paint Julia in a flattering light, it's hard to imagine what would. To answer some of your questions: - I use Julia primarily for transportation research; I wrote a small transportation simulator for various demand estimation problems. - I also thought about Java, Matlab, and R. - R has a lot of stats functionality built in and an amazing library ecosystem, but it's just a pain to program in, and it's slow. I actually wrote a "version 0" of the simulator in R, but moving to Julia gave me an order of magnitude performance increase and greater flexibility due to Julia's type system, multiple dispatch, etc. Java would have been an alright choice - object oriented programming and simulation feel natural together - but wouldn't have given me the same productivity, and doing math in Julia is much easier than in Java. Matlab is terrible and slow and encourages bad programming and I few Julia as a strictly superior replacement. - Julia has JuMP, which as far as I know is the best optimization modeling language around. JuMP makes solving practical optimization problems so...delicious. - Julia is the first language I've ever used where the language itself has become a hobby. I'm not a power user or an important contributor, but I am interested in the language and its development in a way I never was through thousands of lines of Java, Matlab, etc.
I mostly use Julia like a fancy calculator, so there are a lot of fancy things in there ( like parallel for loops) that I don't use. I know a bit of MATLAB like that Julia has a similar syntax to MATLAB but feels more modern (there are things I remember not being able to do elegantly in MATLAB). Also, and this is a really superficcial reason, I remember looking at R and not liking the syntax. I'm currently working on a project that involves simulating muons passing through a detector that measures their speed by measuring the angle of the emitted cherencov radiation. The idea is so see what kind of restrictions do you have to put on the energy of the muons to have a low uncertainty about the cherencov angle. 
Sorry, I wasn't exactly clear. You're not wrong; Julia performs admirably compared to most languages on the list. &amp;nbsp; My problem is that there are no links to source code for the implementations or information about the process behind data collection. I can't tell if all implementations are well developed, or how this experiment accounted for error. There's a line at the bottom that reads: &gt; C timing is the best timing from all optimization levels... Which implies the C baseline is based off of the best time of single runs. It's unclear whether the points plotted for other languages are based off single runs, best times, or if they're averages. For sufficiently short benchmarks an unknown system load can significantly change the outcome of a given trial, so without using averages or revealing how long the C baselines took this plot is relatively meaningless. &amp;nbsp; Beyond that, all of the C and Fortran compilations occur using GCC while Julia uses LLVM (the backend for Clang) for producing machine code. The page doesn't say whether or not the Julia runs were done with compiled code or scripts, but assuming it's compiled that's a pretty huge discrepancy. GCC and LLVM optimize code very differently and this can have a huge affect on efficiency. &amp;nbsp; Ignoring all that, look at Julia vs Go. While Julia outperforms Go in most of the benchmarks the difference is negligible (only a couple times faster) and the results for Go (with the exception of `rand_mat_stat`) are much more closely grouped; that lack of fluctuation indicates a more well balanced set of optimizations for Go which can result in a better scaling. Pairing that with a language that is more mature, more actively developed, with a larger community following, and countless examples of production level applications makes it clear that Go is likely the better option.
Thanks! That sounds really cool.
Thanks! I had not heard of JuMP before, I'll have to look into it. As for the benchmarks, I cover it in detail under cormullion's comment but in general I think that the plot doesn't tell me enough about the experiment to determine whether or not plot is accurate. Even if it is accurate, I think that it paints Go in a more flattering light than Julia, but more information on the nature of the benchmarks and experiment could easily prove me wrong.
Cool. Sounds like you should go (or stay) with Go, then! :)
Haha, sorry, I didn't mean to sound like a Google shill, I just don't think that plot provides enough information to draw reasonable conclusions. Is there a more thorough set of benchmark data you know of?
It’s all in the github repo. I don’t think benchmarks are that useful for choosing a language; ballpark indications are useful, perhaps.
I've had an unusual use case: server daemons. Once everything has beem JIT compiled, the code is executed with extremely little overhead (contrast to eg. Python or Bash) despite being very high level. Other than that, the GPU support which I've been working on is (I think, but I'm biased) pretty novel for a high-level language, and a good argument if you need that kind of performance. 
Sources are here: https://github.com/JuliaLang/julia/tree/master/test/perf/micro As for go, I haven't seen much usage for scientific/numerical computing, I don't know how good the libraries are.
Thanks! Thought I looked everywhere for those. And you're right, Go isn't designed with scientific computing in mind, nor are the libraries for it particularly spectacular. My comment was more in regards to the benchmark data.
I haven't made anything significant with Julia yet but I plan to after playing with it on small things. My wife is a neuroscientist and so I often develop small analysis scripts for her. I originally used Python, which I'm most familiar with since I'm into machine learning. But her analysis required a lot of conventional statistics and so R's expansive statistics ecosystem made it easier, not to mention I didn't have to import so many libraries, so I switched to R which was mostly for good (although I find the syntax to be unsightly). Now she's doing "medium data" things where we need to analyze gigabytes of video data. There are some existing libraries to do this in Matlab and Python. I've used Matlab on minor things in the past and found it really nice for numerical computing (no surprise) compared to Python. But now both these Python and Matlab libraries are too slow for this analysis so I'm planning to re-implement, most if not all into Julia. It's not that you can't get Matlab or Python to go fast, you just have to resort to using external tools. For Python you have to use Cython or Numba or write custom C extensions. Cython and Numba are basically separate implementations of Python so not everything that works on the latest version of CPython works in Cython/Numba (although it looks like they're not far behind). In any case, you have to resort to using multiple tools/implementations and you have to import dozens of libraries to do basic numerical computing stuff. So I don't think Julia being fast on its own is the major selling point, since if you really want to you can make any interpreted language fast but just offloading the computationally intensive parts to a compiled C module or using one of these spin-off JIT versions of the language (e.g. Numba/PyPy). The major selling point for Julia is that you can stick with a single language and rapidly prototype something (which interpreted languages like Python are good for) and then make it as fast as C without doing much more than following some Julia coding best-practices ("it solves the 2 language problem"). Moreover, the basic scientific computing stuff is supported out-of-the-box. For me, just having native multi-dimensional array support is really nice. I don't have to use so many external tools and libraries. Moreover, apart from the speed performance and solving the two-language problem, the language itself is a real pleasure to work with. Multiple dispatch is one of the most awesome things, especially for the work I do in machine learning. Macros are fantastic. The type system is great. It sits nicely between heavily object-oriented languages like Python and Java and purely functional languages like Haskell. In any case I'm really excited about the language and really hope it takes off after version 1.0
Forgot to mention: I also use Julia for little personal experiments or glue scripts. E.g. I have a Julia script which recursively reads a directory and converts all .svg files to .pdf files (by calling Inkscape). I use Julia for this because it's the language I know best - many other languages can do this simple task as easily - but I will note that for these purposes Julia is great! Code is readable, fast, concise, etc.
I'm a PhD student in physics and my professor told me about Julia last year when I was still working on my master's. Checked it out back then, but didn't do much with it, but 3 months ago I got back into it, when I read news that 1.0 is supposed to come out soon-ish, which was right after I started on a new project. I'm currently using Julia to work on a data managment and post-processing system, as well as to test smaller algorithms when working on our bigger codes before implementing said algorithms into those bigger codes (which are written in C and Fortran, but Julia code is just easier to throw together outside of those bigger software packages). Before settling for Julia, well, I used C, Fortran and Python :D Usually I tested algorithm ideas in Python, before implementing it into other codes. After doing a bit of research on the topic, I settled for Julia because it was kinda the one language that's doing it all for me, as in, I don't need to use two languages to test smaller algorithms in one language before implementing it into a better performing language. I also really wanted to use Python libraries, but I knew that vectorising my problem was nigh impossible, so Python would screw me over hard when it comes to performance. Well, Julia gives me easy access to Python libraries, without having to worry about somehow jamming everything into numpy arrays to conserve performance, so that's where the decision was made. Yes, most definitely. In Julia you have to avoid the global scope like the plague, because Julia's performance comes from optimising functions on data types, which it can't do if you put your whole code into global scope, so this is already quite different to how I was taught to write Python (because that's, as far as I was taught, free form, apart from using Fortran and C libraries for performance). I felt quite like writing C where I use an actual 'main()' in Julia, but with more intuitive syntax (even going back and forth between C and Fortran was the reason for so many accidental index shifts &lt;.&lt;). Julia also lets me parallelise easily, whereas in C and Fortran I have to deal with MPI (which can be a pain)...
I have used C++ for "big" simulation models, Python for scripting and data analysis and GAMS for optimisation. Nowadays Julia (and JuMP) just replaced them. It's a more simple workflow. Also great IDE, almost at the level of R-Studio or QtCreator (for me better than Spyder or Jupyter for development purposes). 
Similar to JuMP for Julia there is Pyomo for Python.. I guess at the moment it has a bit more advanced features, but it is slower to prepare the model to pass to the solver (that matters only for very large models)
I use Julia for heavy scientific computations. I considered many other languages before Julia, but I settled on Julia because the base functionality is fast and even very performant code can be readable to somebody who didn't develop it. This makes it a natural choice for collaborative prototyping at the very least. 
We get this question frequently, have a look through the sub.
I have used Julia for my diploma thesis (computer-aided drug design) and I really liked it but Julia is not mature enough at the moment. Currently I am using Python for automation, database and file handling, plotting, ... and Chapel (http://chapel.cray.com/) for parallel and math heavy programming. (+ Bash, C++, Tcl, Js) I had to create a simple program for running jobs asynchronous on a large cluster. That was one reason why I have chosen Julia. It took me less than 2 hours and maybe 100 lines of code to create this in Julia.
What have I used Julia for? I created DifferentialEquations.jl and with that Julia has been the core of two publications, and now is core to two more about to be submitted (and more coming soon too). I use Julia for all sorts of things, but if you couldn't tell my main usage is for solving (stochastic/partial) differential equations. I mix this with optimization and machine learning all in one big Julia happy meeting. I used to use a lot of languages. I have one statistics-based publication with R where I also built a GUI and distributed it with the paper ([you can find it in my publications page if you're curious, just lots of GWidgets](http://chrisrackauckas.com/publications.html)). After learning that R had 3 (I think it now has 5?) different object systems, and if you use objects at all your performance goes to hell, I basically gave up R. I used it to call pre-written libraries here and there, but from then on I realized that R isn't a language you can "create" with. Even the packages written for it are not in R. I did a machine learning project which was mostly Python which merges together [different ways to do ensemble models for prediction problems](https://github.com/ChrisRackauckas/TBEEF). This had the same issue. Yes, you can use NumPy or Numba to do okay sometimes with Python, but you can't "create" when you're stuck to just using arrays and vectorized computations. Large software architectures need to use objects and other pieces in order to put together a coherent software, yet if you touch objects + foreach loops and other things like that your Python will slow to a crawl. So Python didn't work for speed, and most of the computations ended up just linking to C++ code (either directly or through R). In the end, Python is fine for scripting, but it's hard to create scientific software with (and there's a big difference) since there's so much Python which you're supposed to just know to avoid, and what you're left with isn't much better than Fortran. Then I used MATLAB a lot. But it was too slow for solving the stochastic partial differential equations that I needed it for. The issue is that software for differential equations has two problems that make it hard for scripting languages: 1. It requires user written functions. 2. It is hard (if not impossible) to vectorize because of the time-dependence (you simulate now to get a value, step forward in time and simulate some more. It's naturally a loop). Dealing with 2 means you write the loop and then watch MATLAB's performance die... but you can't deal with 1. Basically, the way that packages in MATLAB/R/Python end up fast is they write the functions in C/Fortran and call a kernel there. But in differential equations (optimization as well), most of the time is spent in a function that the user provides. But if that function is written in MATLAB/R/Python... you can see how there are issues with that. This isn't a theoretical issue, this is such an issue that people have put tons of time into getting around this issue. There are things like [PyDSTool](http://www.ni.gsu.edu/~rclewley/PyDSTool/FrontPage.html) and [JiTCODE](http://jitcode.readthedocs.io/en/0.60/) in Python where instead of writing functions you write strings and it has a parser that writes C code and compiles all to avoid having a real Python function... oh god, if I want to write something to solve differential equations, I don't want to have to write a parser and a compiler! So I was stuck. Here I was, a young mathematician with a new algorithm for adaptive timestepping for stochastic differential equations, and I want to distribute this algorithm so it could be like `ode45` for SODEs, but the blocking issue is that I would have to take a lot of time implementing this in C or Fortran and writing intricate wrappers/parsers/compilers to make this actually good. I have C/C++/Fortran experience from writing MPI code, but this sounded awful: I can't waste my time doing this! Then I found Julia. I was taking a computational partial differential equations course which was a little too easy and we were supposed to use a MATLAB toolbox for the course to do everything. I decided it would be "fun" to instead just write everything from scratch in Julia, and see if I can make it faster. To follow this, I started the blog [StochasticLifestyle.com](http://www.stochasticlifestyle.com/) to write about all of the Julia tips and tricks I find. Julia just worked. I found out [what exactly you need to understand in order for your code to go fast](http://www.stochasticlifestyle.com/7-julia-gotchas-handle/) and developed [new software architectures to make things fast and legible](http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/). In a few months I built DifferentialEquations.jl, and it objectively by pretty much any measure is the most performant and feature-filled differential equation library available in any language (don't take my word for it, [read the docs](http://docs.juliadiffeq.org/latest/) and [feel free to run the benchmarks](https://github.com/JuliaDiffEq/DiffEqBenchmarks.jl)). That's insane to me. Sure, I spent years trying out different ways to write this in different languages, but it seriously only took a few months to make something that was powerful and flexible. Why did it work out so well? &gt;or the features fit together in a way I haven't considered Yes, the way the features fit together is beautful. Multiple dispatch allows types to be fast. It goes back to [the architecture](http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/). Essentially... I could use types! Notice that my problems before was that in Python and R you had to "not use Python and R" if you wanted to make things fast. MATLAB really doesn't have objects (or data structures like stacks) so that's a whole different can of worms... it's like C/Fortran where you have to build everything from the basics: arrays with a pointer! No, Julia can use types, data structures, other packages, etc. all together to build an actually fast package. And abstract typing means your package works not just with arrays of 64-bit floating point numbers, but with arbitrary precision numbers, rational numbers, etc.... and you don't even have to do anything special. I wish I can say that DifferentialEquations.jl is so feature-filled because I am sooooo good, but in reality, a lot of it "came for free" and I only found out it worked because someone tried it, and then I added tests/docs to make it a thing. That's really something special. So, is Julia now my language? It's only been about a year, but I am now pretty ingrained in the community. I never found such great people, such great chatrooms. Julia has helped me get publications, write great software, and meet some great friends. I am hooked. The language just works. Let me leave by pointing out one thing: I am a mathematician "and not much of a programmer". I say the last part in quotes because I still think it's true, although at this point I have written some pretty widely used and decently large software in Julia. But it works well because it's in Julia, not because of me. If you [look at the comment section of my blog](http://www.stochasticlifestyle.com/7-julia-gotchas-handle/#comments), you'll see they are all screwed up because I just couldn't find out how to do PHP well. But what about creating a webapp in Julia from just some basic pieces? [Surprisingly easy](http://app.juliadiffeq.org/). At this point I think it's easier for me to do just about anything in Julia... and oh yeah, it ends up fast: both fast to make and it is fast in actual use. Your mileage may very, but Julia has been a solar-powered car for me (i.e. infinite mileage? Yeah, it works).
&gt; Ignoring all that, look at Julia vs Go. While Julia outperforms Go in most of the benchmarks the difference is negligible (only a couple times faster) and the results for Go (with the exception of rand_mat_stat) are much more closely grouped; that lack of fluctuation indicates a more well balanced set of optimizations for Go which can result in a better scaling. Pairing that with a language that is more mature, more actively developed, with a larger community following, and countless examples of production level applications makes it clear that Go is likely the better option. Julia is a dynamic scripting language. Go is a statically-typed compiled language. Saying you can just switch between the two is silly. Dynamic languages caught on for scripting because they are so much easier to play around in. This is important for productivity when "using it like a calculator" for scientific activities. While you're at it, you might as well say "why not C/Fortran instead of Julia"? Of course those kinds of languages will always be fast, but not necessarily fast for you to write things in. &gt;The page doesn't say whether or not the Julia runs were done with compiled code or scripts, but assuming it's compiled that's a pretty huge discrepancy. Julia is a JiT language. It's timed by running twice: first time JiTs and then the second is the speed without compilation. Since packages precompile and many times you really want speed when you're calling something in a loop, this measures what you'd see as real-world performance. &gt;GCC and LLVM optimize code very differently and this can have a huge affect on efficiency. Yes, and usually the source of difference between tests of Julia and C are actually due to the choice of the compiler. But the idea isn't to show "Julia 1x with C" (though if you understand how Julia works, you wouldn't be surprised why this is the case, see [this site for example](http://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia)). The test is just to show that Julia, a productivity language, is close enough to C, a "speed language", that you can use Julia for performance-sensitive codes. If you're talking about changing around compilers to eek out differences, then Julia has already met its goal. &gt;Which implies the C baseline is based off of the best time of single runs. It's unclear whether the points plotted for other languages are based off single runs, best times, or if they're averages. For sufficiently short benchmarks an unknown system load can significantly change the outcome of a given trial, so without using averages or revealing how long the C baselines took this plot is relatively meaningless. [Look at the source](https://github.com/JuliaLang/julia/tree/master/test/perf/micro). Or better yet, just understand that this is a result that has been scrutinized by thousands of people and give humanity the benefit of the doubt that you're not the first to think of how to benchmark correctly... obviously it's multiple runs. Actually, averages aren't a good measure because averages in benchmarks catch all of the other stuff going on in your OS. The real benchmark, i.e. how good is this when other things aren't bothering your benchmark, is the minimum time. You should benchmark things by running them a bunch of times and taking the minimum since that's actually what it's like in the absence of your iTunes update and all that other jazz. But in practice, the difference between minimum and average times is usually not important. Of course, the benchmarks are freely available in the Github repo so run it yourself if you're not convinced. &gt;that lack of fluctuation indicates a more well balanced set of optimizations for Go which can result in a better scaling. The benchmarks were chosen to reflect different pieces of the language. The Fib benchmark is for recursion. Mandel for looping. etc. These are not testing "programs people write", but the specific pieces which make up a language. In this sense, what the benchmarks are actually showing is that Julia does quite well across the board, but its only benchmark over 2 is because it doesn't have tail-call optimization for the recursion example (but still 2.11x from C is fine). Go also does well, but has slightly different strengths and weaknesses. But again, the fact that you're comparing the speed of Julia to compiled languages means Julia has achieved what its goal was.
Neither the links in my original post nor the link to the source you provided detail the nature of the benchmarking process. Statistics can be misrepresented pretty easily and I find it hard to believe that an interpreted language (even JiT) can compete this heavily with compiled languages, so please forgive me if I'm skeptical about a graph with little to no context provided. Just for the record: I never said you could switch between Go and Julia. What I was saying is that based on the context-less stats and without proper understanding of the strengths of Julia that I couldn't see a reason to use it over alternatives like Go. I've since changed my stance.
I completed it, it’s a very good intro, and a great bridge if you’re already familiar with something like Python/Numpy/Pandas. The extra credit content is a great intro treatment of the Gadfly library. The first few chapters of Thomas Sargent’s Macroeconomics Julia Programming book was a great additional resource when getting started
I would have suggested [SDL](https://github.com/rennis250/SDL.jl) but it hasn’t been touched in years and would need foster-parenting ... Instead I’ll suggest [SFML](https://github.com/zyedidia/SFML.jl) which looks much more active. Luxor isn’t suitable for anything interactive or game-related, unless it’s “guess how long this takes to draw”...
&gt;I find it hard to believe that an interpreted language (even JiT) can compete this heavily with compiled languages No. The trick, the reason why the language works, is because functions in Julia auto-specialize (via multiple dispatch) and then are statically-compiled. Please read the links I sent, like: http://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia These explain in detail what's going on, and shows via `@code_native` that Julia functions compile to similar assembly code that you'd get from a "true compiled language". So it's actually very easy to understand why they end up with similar timings: Julia is simply producing assembly code which is similar to C/Fortran codes and running that. In fact, there isn't a true interpreter for Julia which has been released yet. The REPL isn't an interpreter.
Julia's garbage collection strategy piles up work and then crams it all at once. This is more efficient in the long run, but you'll notice spikes in GC usage. For scientific computing where runtime is what matters, this is a great strategy. For game engines, this would result in lost frames. Thus, while I am a huge fan of Julia, I don't think it's currently in a good spot for game design because of its GC. This may change in the future though.
There's also Visualize, which is not quite ready, but that looks to be nice: https://github.com/SimonDanisch/Visualize.jl
How about embedded Julia like it's done with lua in games?
Trying to push the envelope on compilation to JS and WebAsm. Tom's done most of the work so far and has some nice demos at https://github.com/tshort/jl2js-dock. I'm also making a Julia implementation of the Hanabi card game to use in a reinforcement learning demo project.
&gt; The idea is, in many cases you know what the matrix "should be", so why actually store the matrix? You should check out [JOLI](https://github.com/slimgroup/JOLI.jl). It's a library of element free linear operators, and a framework for making your own.
I'm just playing with weird images at the moment: [4/pi atanh(z)](https://i.imgur.com/rb8WXuo.gifv) 
Interesting, never seen that one before. It's unregistered though so it can't be used in our libraries until that's changed. Its source code is very unorthodox Julia code though so it's hard to know what it's doing, but from what I've seen I don't see what it offers over the LinearMaps interface. Do you have more details?
Variational inference.
Unfortunately I don't have much experience with it, I'm just aware of it, so I can't offer you too many details. However I know that it intends to replicate the functionality of [SPOT](http://www.cs.ubc.ca/labs/scl/spot/) operators in Julia. That said, I'll try to offer a little insight. A JOLI operator maintains the expressiveness of an explicit matrix, without having to store elements. For example if you wanted to take the FFT of a sparse vector: # make a dense vector n=128 inds = randperm(n) x=rand(n) # create fft op and subsampling op F = joFFT(n) R = joRestriction(n, inds[1:10]) # apply element-free ops like matrices X = F*R*x Excuse the formatting above, I'm on mobile. I hope that helped clear things up a little. 
I'm doing stuff for my undergraduate thesis in Julia, mostly using Distributions.jl and Plots.jl to make some histograms
EM
[Methods for solving ordinary/stochastic/delay/partial/discrete differential equations](https://github.com/JuliaDiffEq/DifferentialEquations.jl) and then using DifferentialEquations.jl to model noise in developmental biology.
I just keep a Julia session open at all times in the background for quick calculations, plotting data, symbolic manipulation, and unit conversions. (PyPlot, SymPy, Unitful) At this point, Julia is a convenient tool for me.
Just out of curiosity (I don't deal with ODEs), have you seen if all of the available libraries/packages produce the same results under normal and corner cases?
Thanks for the details! I enjoyed reading this. 
DifferentialEquations.jl is a metapackage. It pulls together lots of other packages to achieve its functionality. Not all of the packages in that interface are even part of JuliaDiffEq. For example, the LSODA and Hairer wrappers are done outside of JuliaDiffEq, and DifferentialEquations.jl just provides an interface to those. ODE.jl is also part of the DifferentialEquations.jl interface (and in JuliaDiffEq). We are also in the process of adding TaylorIntegration.jl (very high precision integration) and GeometricIntegrators.jl (structure-preserving methods) when these libraries mature, and these are libraries outside of JuliaDiffEq. That points to the interesting thing about our development in that I think our distributed development pattern has really helped our open source community thrive, since "contribute to open source project" is not the only way to contribute, rather a lot of the edge cases are actually provided by individuals who developed a package on their own for their own purposes, and then accepted a pull request that added the interface to make their functionality available from DifferentialEquations.jl. I don't think we would be half as far as we are without this means of distributed contribution.
Fantastic review! Thank you so much for taking the time to research and document all this.
I'm building software to analyse scientific echo sounder data from a Southern Ocean research ship as part of my PhD studies.
this is great, but Matlab does have symbolic math capabilities. Package availability is wayyy behind anything open source, but that red box should at least be yellow. (am I seriously defending Matlab rn?)
MATLAB doesn't use them in the ODE solver. If you read the full text, you'll notice that I mention a specific interaction here. When you define an ODE function for Mathematica or Maple and use a stiff solver, it will automatically, without even telling the user, symbolically calculate the Jacobian (Mathematica I believe runs its compilation on this as well) and pass that to the ODE solver (usually LSODE or CVODE, so the standard BDF multistep methods). DifferentialEquations.jl also does this when the user is using the `@ode_def` DSL, but doesn't always need this because the ForwardDiff.jl fallback is phenomenal here and actually is really close to having an analytical solution in both error and timing (it's quite surprising, I really enjoy what the autodifferentiation crew has built). On the other hand, MATLAB (along with every other software suite) always falls back to numerical differentiation. This is a big difference since it's more costly and induces error (and this error propagates to the ODE solution and can cause more steps to be taken, either to help the Newton solvers converge better or to actually reduce the numerical error in the solution). Sure, each of these have some way for you to define and pass a Jacobian, and MATLAB (and Python/R/Julia) have symbolic libraries that would help you calculate a Jacobian, but in these other cases you would have to do this yourself. In practice this really hurts how well the stiff solvers work in a "fully automatic / low user intervention" mode.
using Julia to build a transaction categorisation tool. I love easy it is to use Julia to talk external binaries. Julia's mongodb binding hasn't been updated for a while, so I used Go's mgo and compiled a binary, and then I used Julia to get the results from the Go binary and read in using the JSON package. Another I found very useful was the excellent support for Perl Regex in PRCE. I can manipulate the input data so much faster than in R.

oh jeez, egg on my face - I didn't interpret that first figure as "features specifically included in the ODE libraries," but that's probably elaborated in the text. Sorry! moreover, thanks for the thorough explanation!
That's really awesome! How far along are you?
Me: "Oh, Chris should see this post! Oh... it *is* Chris."
I have used Mathematica for calculations with units before. It can be a bit slow. Can you comment on your experience with Julia + Unitful? Is it fast? Does it understand most units?
Good work!
For what I do it is definitely fast enough and has all relevant units. One thing that I kind of dislike is the usage of greek letters that are not easy to type (like μ for the multiplier micro and Ω for the unit of electric resistance ohm) and the degrees symbol °. For instance, `u"C"` is the unit Coulombs and `u"°C"` is degrees Celsius. Of course, if that bothers you, you can always write a little script to create aliases. Here is an example of something that I am working on right now: julia&gt; uconvert(u"fF / μm^2", 1.727e-7u"F / cm^2") 1.727 fF μm^-2 I am converting Farads per square centimeter to femto Farads per square micrometer, which is not that trivial that I can do in my head.
How'd you do this?
Using Images.jl. For each pixel in a source file, convert coordinates to a complex number z, then use the real and imaginary parts of f(z) as the position at which to sample the source file’s pixel color.
this is awesome. I just learned about Julia the other day and it's interesting to see what people do with it. I am not sure if it's worth it to learn it(even though when I play with it it's hard to tell the difference from python). Anyway, cool stuff.
&gt;Can you comment on your experience with Julia + Unitful? Is it fast? Well, let me comment on it from a design perspective. It implements units via the type system. So what happens is, at compile time, `a+b` is checked to see if the types (and thus the units) are compatible. If they are compatible, then it replaces this with `a+b` of the values and makes it of the type which keeps the units. If they are not compatible, then this is replaced with an error. After doing these replacements, the code is compiled. So the compiled code doesn't have any runtime unit checking (if the code is type-stable). This means that there is no runtime overhead with this approach, which is quite cool. I think Unitful.jl can do things like unit conversions, so two lengths but one in feet and one in inches auto-convert? I'm not sure, but if so that would just add a multiplication operation in there (at compile-time of course since this is just via dispatch). This means it shouldn't have any overhead if your code is type-stable. But that doesn't take into account that each time you encounter new units you do need to compile a new version of `+`, `-`, etc. However, if you have a smallish finite set of units and use this stuff in a loop, it'll all compile once and then be fine. It's a pretty neat design and is actually traced back to Keno in SIUnits.jl
Thanks for the insightful comment.
Not that far, still catching up with Tom's work but will have more time this weekend. 
Apart from being a weird way of writing code in Julia I think it is possible for IDEs which runs together with REPL. In emacs a natural way would be to make a keyboard combination (for example C-c m) which would ask to write down argument list in the first query and then would give available completions (which would be obtained from opened REPL) for for the next one. 
I have nothing to add to your question, _but_, is it possible to cleanup the results of methodswith a little? For example some of the signatures get a little crazy looking. Wondering if it's easy to postprocess and just get the method names for example.
Yes, it looks like you would need to define your own show(..., ms::MethodList/MethodTable, ...) and add it to your juliarc. Some colors would be nice. https://github.com/JuliaLang/julia/blob/master/base/methodshow.jl
I'll be in Sydney next month. Do you have any more details?
Its on 19th October. Hopefully we can do another one on the 16 Nov. It's in the centre of Sydney at Microsoft store, 188 Pitt St Sydney.
Are you aware of the latex style Unicode input?
How do you deal with instability? What are your up times like? I think Julia would be an amazing numerical backend for something like elixir, since that laanguage provides supervisors and first class ast that you could nearly transparently pass to Julia.
1) The dump() function is your friend. Pass it a few expressions, and immediately you will understand how to do literally *anything*. Remember. The ast is first class, so you'll see the ouput, and you'll say, I wonder if I can... And the answer is "yes, yes you can." 2) if it doesn't work, try escaping it. esc() function.
I guess this link has the missing info: https://www.meetup.com/Sydney-Julia-Julialang-Meetup/events/241489337/
I was not actually. Checking the docs now, this stuff is amazing! Thanks.
&gt; I think Unitful.jl can do things like unit conversions, so two lengths but one in feet and one in inches auto-convert? That is correct: julia&gt; 3u"inch" + 4u"cm" 581//5000 m In this case, it defaulted to meters.
Yes, and it's nice to inspect what's going on: f() = 3u"inch" + 4u"cm" @code_typed f() gives: CodeInfo(:(begin $(Expr(:inbounds, false)) # meta: location C:\Users\Chris Rackauckas\.julia\v0.6\Unitful\src\Unitful.jl + 427 SSAValue(1) = $(Expr(:invoke, MethodInstance for convert(::Type{Quantity{Rational{Int64}, Dimensions:{𝐋}, Units:{m}}}, ::Quantity{Int64, Dimensions:{𝐋}, Units:{in}}), :(Base.convert), Quantity{Rational{Int64}, Dimensions:{𝐋}, Units:{m}}, :($(Expr(:new, Quantity{Int64, Dimensions:{𝐋}, Units:{in}}, 3))))) SSAValue(0) = $(Expr(:invoke, MethodInstance for convert(::Type{Quantity{Rational{Int64}, Dimensions:{𝐋}, Units:{m}}}, ::Quantity{Int64, Dimensions:{𝐋}, Units:{cm}}), :(Base.convert), Quantity{Rational{Int64}, Dimensions:{𝐋}, Units:{m}}, :($(Expr(:new, Quantity{Int64, Dimensions:{𝐋}, Units:{cm}}, 4))))) # meta: location C:\Users\Chris Rackauckas\.julia\v0.6\Unitful\src\Unitful.jl + 422 SSAValue(2) = $(Expr(:invoke, MethodInstance for +(::Rational{Int64}, ::Rational{Int64}), :(Unitful.+), :((Core.getfield)(SSAValue(1), :val)::Rational{Int64}), :((Core.getfield)(SSAValue(0), :val)::Rational{Int64}))) # meta: pop location # meta: pop location $(Expr(:inbounds, :pop)) return $(Expr(:new, Quantity{Rational{Int64}, Dimensions:{𝐋}, Units:{m}}, :($(Expr(:invoke, MethodInstance for Rational{Int64}(::Int64, ::Int64), Rational{Int64}, :((Core.getfield)(SSAValue(2), :num)::Int64), :((Core.getfield)(SSAValue(2), :den)::Int64)))))) end))=&gt;Quantity{Rational{Int64}, Dimensions:{𝐋}, Units:{m}} What this is saying is that it converts the two to meters and then does a simple add. So it adds two multiplications and adds, but it's a standard addition. All of the type calculus as shown here is already done when the code is compiled.
Emacs is relatively easy, I'm more concerned with Jupyter Notebooks and the REPL.
3D face model estimation using a really dumb model (lol least squares). More for fun than actual work.
When you say least squares do you mean just doing like a 2D surface ‘curve’ fit? How poorly does least squares perform here compared to something more sophisticated like a neural net?
Yeah it's something like a n-D non-linear 'curve' fit. Think of it as minimizing the function norm(f(x) - b, 2) with respect to x where f projects a 3D model of a face to an image, b is the image, and x contains parameters of the 3D model including camera parameters. It kind of works but not really that well. NN is trying to do something else, so it's not comparable.
This from the tests seems to work fine: sample_text1 = "This is a string" sample_text2 = "This is also a string" sample_file = joinpath(Pkg.dir(),"TextAnalysis","test","data", "poem.txt") sd = StringDocument(sample_text1) fd = FileDocument(sample_file) td = TokenDocument(sample_text1) ngd = NGramDocument(sample_text1) crps = Corpus(Any[sd, fd, td, ngd]) documents(crps) for doc in crps @assert isa(doc, AbstractDocument) end lexicon(crps) update_lexicon!(crps) lexicon(crps) Note that the lexicon is empty before the call to update_lexicon!.
update_lexicon. That was it. Thanks!
Note for those who don't know: IterTools.jl is just Iterators.jl, but renamed since Base got a module Iterators which now causes namespace issues with Iterators.jl.
 I'm not sure if there's a function that does that, but you can use map like that: map(sum, (A[:,:,i] for i=1:size(A,2))) Maybe you can actually write a applyAlong like that using Cartesian indices.
use `mapslices`
I'm new to the language and so I'm doing a small project to familiarize myself with it. I have very little experience with good type systems and with multiple dispatch so I figured that a great first application would be to make a [Geometric Algebra](http://assets.cambridge.org/052148/0221/sample/0521480221ws.pdf) system. I'm drawing many ideas from [this old abandoned project](https://github.com/andrioni/GeoAlg.jl) but I'm rewriting everything afresh partly because its instructive to me and partly because its not well documented enough (and I'm not experienced enough) that I understand how it works so its a nice process of thinking through problems and then rediscovering why he did things the way he did them but also getting opportunities to improve things where I think I can make improvements. Still very rough and not yet ready to show off though.
I'm working on a prototype mqtt client. It's part of a course at my university
&gt;It's not well-suited for object-oriented programming. &gt;There's no object.method() notation, which is usually more readable, especially in more complex expressions. I have to disagree with you here. `A.add(B)` just isn't more intuitive to me than `A+B`, yet you see this all the time with specialized mathematical libraries in object-oriented languages (ex: Python's matrix multiplication in SciPy was like this until only very recently when it got a special operator). `o.f(x)` is no different than `f(o,x)` either. At least in my eyes. But I do get why OO is being discarded for large projects though because having to inherit makes it harder to extend other libraries than relying on generics and subtyping. I agree that the OO framework is easier to teach because generic algorithms are admittedly something which requires a lot of familiarity with type operations, but they are so much more extendable that I disagree with the necessity of OO-styles. &gt; No proper interfaces. I hope this gets added to the language soon as well. &gt;The nullable syntax is really inconvenient. "return Nullable(1)" would be "return 1" in Swift. "return Nullable{Int}" would be "return nil". And "!isnull(a)" would become "a != nil". Yes, it's generally agreed Nullable is difficult. Notice that all of the data libraries like DataFrames have abandoned it. 
&gt; `A.add(B)` just isn't more intuitive to me than `A+B` Of course that `A+B` is more readable. But what about `open(file).read().length()` vs `length(read(open(file)))` or `array.append(Elem()).add(array2)` vs `add(append(array, Elem()), array2)`. I think with the last one, the readability difference is quite large.
Meh, I don't think there's any differences between these two. It's a very superficial issue. Personally I really like Julia's type system + dispatch, it gets a bit of time to get used to, but it works very well in practice, and it's quite refreshing.
I’ve always been a fan of Clojure’s way of getting around this with the threading Macro where (h (g (f x))) Becomes (-&gt; x f g h) Ie. x is passed first to f as (f x) and then that is passed to g as (g (f x)) which is finally passed to h as (h (g (f x))) Julia’s composition operator seems to allow something like this. What do you think of file |&gt; open |&gt; read |&gt; length ? I think this is actually more readable than dot notation though it does involve typing more characters. 
&gt; The nullable syntax is really inconvenient. "return Nullable(1)" would be "return 1" in Swift. "return Nullable{Int}" would be "return nil". And "!isnull(a)" would become "a != nil". Thanks to compiler improvements regarding Union types, Julia is going to adopt an approach very similar to Swift to replace Nullable before 1.0. See https://github.com/JuliaLang/julia/pull/23642
My personal problems with Julia right now: 1. Multi-staged compilation: it's a feature, not a bug, I know. Specialization and speed, etc... But in reality it makes the use of Julia actually slower. Calling a function from a package for the first time takes forever (just try `using Plots` and plot something --- there is enough time to make coffee). Fix: compile (and not just lower) the methods of a function for some provided argument types. You kind of can do it now by making stab-calls to a function in module initialization, but this will be invoked only at `using` the module. I would like this to be done at the module precompilation stage. It is a tricky thing to do though, and the compiler might fail to deduce the types of argument passed to functions called within the methods --- but if everything is type-stable, it should work. 2. Generational GC: I think everybody (including Julia developers team) is aware of the problem. If you look at JVM or .NET or SBCL or Chez Scheme etc. --- there you can create short lived objects without the performance hit. I am not a specialist in GC programming, but I hope this is fixable. This is also must if Julia ever going to enter functional programming space (see below). 3. Image: compared to Smalltalk or Common Lisp, there is no simple way of saving currently running Julia image. But the ability is there, I believe. So, let's hope this will improve. Right now this problem combined with No 1 makes the restart of the Julia program a pain. As for OOP in Julia, I think Julia is geared much more towards functional programming --- multi-methods and immutable structures help with it. And, I would specualte, `Nullable` might be intentionally made cumbersome to use because: (a) it hits the performance and (b) it is not a good idea to use it in general. So, if logic does require it --- clean up the inputs and use plain `int` once done.
Yeah, I'm aware of this... not a fan to be honest. It adds syntax complexity and code irregularity.
Did not see this yet. Thanks for sharing! :)
The macro `@&gt;` is already defined in the Lazy.jl package, and it's less typing too! julia&gt; using Lazy: @&gt; julia&gt; @&gt; file open read length 0 In Julia you can have whatever syntax you want mostly.
This blog is pretty amazing imo. I've been on the fence since v0.4 but after reading a few of your blog entries I will definitely take the time to learn Julia starting today. ty
i'm not saying that you are wrong in the conclusion, i don't know. but what i do know is that your criticisms indicate you are trying to continue with your old routine, and that causes trouble for you, not the language itself. julia is NOT object oriented, and thus obviously it is not well suited for it. however, it is intentionally not object oriented. you need to use what the language has. forget oop for a change, and see how well it works for you. similarly, the dot notation is so embedded in people's heads, but even Ada does not use it, even being full oo. nullable is more like a fringe element, not a core language construct. it does not have to be super comfortable, because its use should be rare, unless you work with databases. a function that returns an integer should not optionally not return one. there are better constructs to do that. so i think this is the case of working against the platform, not with the platform. of course it will not be a pleasant experience.
I find the second syntax much easier on the eye and brain (and possible even fingers) than the first, in each of your examples. Just goes to show how different we all are... :)...
&gt; open(file).read().length() vs length(read(open(file))) these are both very questionable actually.
&gt; julia is NOT object oriented, and thus obviously it is not well suited for it. however, it is intentionally not object oriented. you need to use what the language has. forget oop for a change, and see how well it works for you. I usually program with the same style, regardless the language, because most modern languages allow to mix functional, object-oriented and imperative styles. I try write code that maximizes readability and in some problem domains, OOP is simply more readable. I think that if a language wants to be general-purpose, it needs to support both OOP and FP.
1, i think you are still conflating dot notation and oop. javascript is not oo, yet it has the dot notation. ada is oo, and it does not have dot notation. saying that it is natural does not exactly stand. how 2.min(3) is any better than min(2,3)? or which is better, a.copyfrom(b) or b.copyto(a)? any operation that involves two objects become weird. we need to decide where it belongs, but it does not belong. 2, on the contrary, i think oo is not the silver bullet we were told it would. for many years, i'm actively searching for language without it. ada 83 was quite nice with the excessive use of generics (templates). golang is another twist with its automatic interfaces. the julia way is pretty excellent, i like it the most so far.
1. I'm not but in OOP code, the dot notation is more frequently the more readable variant. `2.min(3)` is obviously less readable. I'm not that one is better but that the dot notation is sometimes more readable. Most mainstream languages have both after all. 2. I've tried a lot of FP and OOP languages (although most support both) and came to the conclusion that allowing yourself to use both leads to the most simple and readable code.
You mention this, but I think it deserves more attention : the blurring of "developer" and "user" that Julia makes possible. I agree with you that this isn't an obvious point to "sell" a lot of people on, but I had never contributed to someone else's repo before starting with Julia. And now I've contributed to several. Speaking for myself, there's an intermediate group to consider: the users that write highly specific packages that they're only planning to use for writing a single paper. The fact that I can move so easily from prototyping to making a functional user-friendly package means that I'm way more likely to actually take that second step. For reproducibility and scientific progress, I think this can't be understated. I ended up writing a package in python from scratch despite the fact that there were other packages that did things *almost* similar to what I wanted, but extending or modifying their code would have been a nightmare. Julia makes this kind of thing easy.
Very insightful post. As a beginner, I know I'd appreciate more low level hand holding introductions to using Julia packages. Perhaps just some live coding videos or streams? 
Yeah, these are all good points. All fixable but true issues.
I totally agree here to an extent. It is a great selling point to a lot of methods developers that Julia has such a low barrier. However, I'm not so certain that pushing it as a selling point is a great thing for increasing adoption of the language. I used to think that, but then after teaching some workshops I got pushbacks from people who had a mentality of "but I just want to import package, plot analysis, and go back to lab work!", and this is a large group of people that we as a community shouldn't ignore. It will always be true that developers will like the ease-of-editability, but the community who actually wants to edit anything is actually small in comparison to the large set of programmers out there.
Yeah - for user growth specifically, I think you're probably right. But there's also the open science/reproducibility crowd that could be wooed. As influencers, they might be worth targeting as well. 
How are you constructing `a` and `b`? Because you definitely can multiply two 1x1 matrices: julia&gt; a = zeros(1, 1) 1×1 Array{Float64,2}: 0.0 julia&gt; b = zeros(1, 1) 1×1 Array{Float64,2}: 0.0 julia&gt; a * b 1×1 Array{Float64,2}: 0.0 
I withdraw the first part of the question: figured it out. In my code with n=1 (a separate example), a key function was returning a vector instead of an (n x 1) matrix. Funny how asking a question sometimes makes you see the step needed. The second part is still valid: Can I somehow easily declare the type for a list of variables?
Yes for sure. I would say I am a newcomer to that crowd and the idea that Julia's packages are completely open for me to check and change algorithms is fantastic. That will always be a small group of people, but I hope (for other reasons as well) they are an influential group.
Type-declarations should be pretty rare. Out of curiosity, why are you looking to declare the types for a bunch of variables?
Methods are a disgusting thing, ffs. 
To avoid stupid mistakes like I had earlier :) I guess without explicit type declarations it is easy to make mistakes that you found out later with weird behavior, but with type declarations, those would get caught in (jit) compilation phase.
Great article! As a newbie to Julia and a relatively inexperienced programmer who mostly just plays around with languages as a hobby I'd definitely appreciate it if there was more low level beginner content out there to help ease me into being a more proficient Julia programmer. I'm a physicist who's moving into work thats requiring numerical methods more and more often so I really should start looking at your DifferentialEquations.jl package as I've heard great things about it!
If you want to declare two Ints you just do: a, b = Int(1), Int(2) In packages it's a bad practice to restrict too much the types in functions, because if you write things in a generic way (using zero, similar, etc), the code will just work for any input type (even the ones you haven't thought about). In user code you can be a bit more defensive. A way to check type is just to have short functions, e.g. somefunction(a::Int,b::Int) Will "type-check" a and b for you. 
The right one you have to worry about matching the parentheses. Haskell's total elimination of parentheses is at first jarring but being able to do something like: open read length file is quite nice
Yes, we have symplectic integrators. Here's some docs pages and tutorials to get you started: http://docs.juliadiffeq.org/latest/solvers/dynamical_solve.html http://docs.juliadiffeq.org/latest/types/dynamical_types.html http://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqTutorials.jl/blob/master/PhysicalModels/ClassicalPhysics.ipynb Note that symplectic and conservative are two different but related concepts though. Symplectic integrators aren't exactly energy conservative. For different schemes you can see some benchmarks: http://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/DynamicalODE/Henon-Heiles_energy_conservation_benchmark.ipynb I'm on my phone so if you want a more in depth answer just ask later.
Julia is not suitable even for high performance large size scientific applications. It's just a faster than regular scripting language for some computations and graphs.
Have you considered using the `@belapsed` macro from `BenchmarkTools`? It works just like the `@elapsed
Have you considered using the @belapsed macro from BenchmarkTools? It works just like the `@elapsed` macro: `@elapsed expr` returns the time (in seconds) to evaluate `expr`. The difference is that `@belapsed` is internally running a full benchmark and thus doesn't measure compile time, etc. 
Shameless plug, but I wrote a small blog post a while ago about different ways of timing in Julia, maybe one of the methods suits your needs! http://www.pkofod.com/2017/04/24/timing-in-julia/
I knew about @elapsed, but because of the same reasons as you stated above "garbage collection etc" it did not seem like the ideal solution. I will check out the @belapsed function. Any idea how to include functions based on number without hardcoding in "include" at the top and at the same time not time the inclusion? Ideally I want to write a julia function that takes a Project Euler number as input, and performs the benchmakrs for all the julia files in that problem folder.
What makes you say so? I think this counts as "large scale" by most reasonable standards: https://www.hpcwire.com/off-the-wire/julia-joins-petaflop-club/
Not sure I 100% understand your question, but what about passing the project folder as an argument, then using `readdir()` to collect the file paths? Your might want to do some checking to make sure you're only running it on files that fit some pattern. Then in a loop, pull in what you need from the file, *then* run the benchmark.
Between the time of my last comment and yours I spent some time writing a code doing exactly what you say. However I ran into some problems with the belapsed function not working on symbols. Asked on stack about it just to be sure: https://stackoverflow.com/questions/46809845/benchmarktools-belapsed-not-working-on-symbol. Not really sure how I can extract a function and run it trhough custom inputs without turning the filename into a symbol
That is nice.
&gt; Generic algorithms over abstract types? If you throw symbolic expressions into the ODE solver, you get out symbolic expressions which can be turned into functions I was ready to be impressed there but am really not. You just fed symbolic expressions into a numerical method and got the symbolic form of an approximation. The complexity of the result is uselessly complicated and absurd compared to the exact answer. Consequently, alternatives like Mathematica would have provided you with an exact symbolic answer faster. The problem is (and I see this a lot) you've factored out commonality that isn't there. A symbolic input should be run through a completely different algorithm (Risch integration) to get an exact symbolic result. Even if you restrict consideration to numbers you usually need radically different algorithms for integers vs floating point. Just consider summing a list of numbers, i.e. fold (+) vs Kahan summation... 
No, it gave me exactly what I wanted. Very few ODEs have a symbolic solution, so that's usually not useful (and Mathematica will just spit out that it's not possible). What I wanted here was a function of the initial condition which approximates the solution. Using that, I can easily answer questions like "what function minimizes L(u) where u is the solution of the ODE?" by solving it once to get u(u0), then solve for u0 to get in the right range, and then refine that guess. This is still useful even if there is not analytical solution for u. And that's just one example. Another example is putting in an `ArrayPartition` of GPUArrays and now it runs on the GPU (without copying back/forth), or using arbitrary precision arithmetic, etc. Those are probably more common uses of abstract types, but I chose to mention the bizarre edge case one because I think it's quite cool that it "just works" without intervention. As for your other example, Julia has `sum_kbn` for Kahan summation. If you're summing something in a way where this will matter, you can use define a summation which uses that when you have floating points and fold plus when you have integers. That's just a very standard example of a good place to use dispatch. There you're still dispatching over abstract types, just now the abstract types are `AbstractFloat` and `Integer`. So you described exactly at what level the commonality is... But in some cases, yes it does matter that you have different implementations. One case is special functions where you want different approximations for different levels of accuracy. At this level you write algorithms with concrete types. However, the ODE solver algorithm itself doesn't change in these cases, so it's generic and lets dispatch specialize the internal special function calls in the user's `f`. So yes, at the very bottom someone has to write concrete implementations for concrete types for basic math functions and things like that, but the general optimization or diffeq algorithm doesn't need to handle that. What kind of summation is used when summing how the array for the norm is determined by the dispatches on the element type of the abstract array given to the norm function, how element-wise operations are handled (iteration? On the GPU?) is determined by the broadcast overloads on the abstract array type, what approximation to use for `exp` is determined by dispatches on the number types, etc. This is then all compiled together to make an algorithm for those exact types. Then when necessary you can refine it a bit, like check for if arbitrary precision is used (trait function checking, still compile time), and if so then use compensated summation in the integrator (or in many cases like high order symplectic integrators you just implement it to always do this). So the facts that you were listing as issues are actually the exact reason to use the dispatch and type-checking mechanism for these kinds of things. It's just all about abstract types and traits.
&gt; No, it gave me exactly what I wanted. Very few ODEs have a symbolic solution, so that's usually not useful (and Mathematica will just spit out that it's not possible). What I wanted here was a function of the initial condition which approximates the solution. Using that, I can easily answer questions like "what function minimizes L(u) where u is the solution of the ODE?" by solving it once to get u(u0), then solve for u0 to get in the right range, and then refine that guess. This is still useful even if there is not analytical solution for u. Surely it would be far more efficient do solve that optimisation problem numerically? &gt; So the facts that you were listing as issues are actually the exact reason to use the dispatch and type-checking mechanism for these kinds of things. It's just all about abstract types and traits. The practical applications of that approach seem few and far between to me. 
Thanks for posting this! Whenever I hear people talking about JuMP I get really confused by the big fancy words getting thrown around and I never had a good idea of what it was for. This clears it up!
Have you looked at `HTTP.jl`? I don't know much about it, ie whether its requests are thread safe, but I believe `HTTP` to be a more up-to-date replacement for `Requests`.
Thanks for reading. I think Julia+JuMP is nice for modelling some problems and just try different stuff. Basics at first and then you can use callbacks and stuff like that.
This is the #1 thing that led me to abandon Julia. It is convenient for doing calculations in REPL but the startup time for a moderate size application is terrible. 
Problem semi-solved: Neither package works with threading on a core2 machine but trying it on a newer i5 machine (both machines with Windows 10, 64 bit, 8 gig memory) both packages work with threading. Threading works on both machines as long as there is no socket IO done in threads. Probably a processor dependent threading bug. 
Ha, I think they must have thought we were talking about [Juliar](https://www.reddit.com/r/programming/comments/4z73t5/please_help_out_with_a_new_programming_language/?st=J8YTML9I&amp;sh=2d910e6fhttps://www.reddit.com/r/programming/comments/4z73t5/please_help_out_with_a_new_programming_language/?st=J8YTML9I&amp;sh=2d910e6f)...
Note that for querying web pages you can use Tasks instead of threads, since the bottleneck is the network and not the cpu.
 arr = zeros(Integer, 10) Base.@sync ( for i in 1:10 Base.@async(arr[i] = query(TARGET)) end ) print("Result: $arr\n") ------------- This works extremely well. Thanks! 
Well, it wouldn't make me abandon it. There are some killer-features, ODE package for one! It only affects the starting time, once everything is loaded and was called once - it's fine. As far as you don't close the REPL or Jupyter-notebook. But yes, it's damn nuasance.
Quick version: you call a function, Julia will specialize that call all the way down to its concrete types. So you write `f(x)`, then call `f(1.0)`, it'll compile something specifically for `Float64`. If none of the types change in that function (called type-stability), then everything can be statically-typed, so Julia compiles a version of the function where everything is statically typed, and thus you get the speed of a statically-typed language after the first call which just compiles. Longer version: http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/
It is not as simple as only being about dynamic typed either. I think for the most cases when Julia or other languages comes very close or even surpass C in performance it is because of just in time compilation. Synthetic tests are great and teach you a lot about CS. However they usually fail to translate into the real world. Speed all comes down to efficient machine code. GCC or Clang are great compilers but they can only make optimization assumptions at compile time. Llvm with JIT can optimize at runtime. A practical but naive example, if you have a large program that can do many things but you will just call one of the program's function that perform some work 1 milion times in a for loop. Somewhat simplified Julia will only compile and optimize that specific function. In the C case GCC or Clang can't know that you will only use one of the functions at compile time and will be forced to create a much larger binary that is also optimized over the whole program and not the single function. When you run the program Julia will incur some cost in the beginning since it is not pre-compiled but llvm will probably have done a better job at writing your function in less instructions. Those less instructions per call will sum up and you end up with a "quicker" result. This does not make Julia or JIT superior though. The above use pattern isn't that common. You are paying the JIT compilation time so there needs to be enough iterations to pay it off. Profile guided optimization would probably have told GCC to create just an as efficient machine code for the C case too. In reality if you have a use case to do some relatively small task over and over again people will usually use C/C++ anyways and write good code that ends up in efficient assembly code (just look at the output and update the C/C++ code till the assembly looks good). E.g. the Linux kernel. Otherwise GPUs and FPGAs are quickly improving now and becoming much cheaper to use to offer even more speed up for some tasks. For me the extremely nice thing with Julia is its performance gain over Python. I use Python for most analysis. So my work week is 50% Python and 50% C/C++ but trying to move as much Python as possible over to Julia.
You can use logical indexing to achieve what you want: ```Julia julia&gt; a = collect(1:5) 5-element Array{Int64,1}: 1 2 3 4 5 julia&gt; b = a[1:5 .!= 4] 4-element Array{Int64,1}: 1 2 3 5 julia&gt; c = a[eachindex(a) .!= 4] #faster than 1:size(a) 4-element Array{Int64,1}: 1 2 3 5 julia&gt; a 5-element Array{Int64,1}: 1 2 3 4 5 ```
 b = deleteat!(copy(a),3)
Computers are fast when you tell them exactly what to do. With C, you tell your computer exactly "here are two integers, add them using the integer addition". Since the CPU has built-in hardware for adding integers, it knows exactly what to do, and does it very quickly. In an interpreted language like Python, you tell the computer "here are two variables, add them". But the CPU has no concept of variables, so before the CPU can add anything, Python first has to figure out what these variables contain. If they are integers, perform integer addition, if they are floats, perform floating point addition, if they are int and float, convert the int to a float, then perform a floating point addition. Etc. Python has to do this for every single operation and every single variable. This is why Python is slower than C. Julia is kind of in the middle between Python and C. Julia can't really know whether a variable is a float or an int either (unless you tell it explicitly), but: it cleverly plans ahead whenever you call a function. When you call a function with some arguments, all the function arguments are known. Julia then goes ahead and looks at every operation inside the function and figures out the exact CPU instructions necessary for these particular arguments! Once the exact instructions are known, they can be executed quickly. That's why a function takes longer the first time you call it: on the first call, Julia figures out all the types of all the variables and compiles it all into fast, exact CPU instructions. The second and third time, (if the argument types didn't change), it will cleverly reuse that code, and is blazing fast. 
&gt;It would be cool if someone could explain me in a simple way how Julia as a dynamic language can (in various cases) be as fast as some static languages like C? It's not as fast as C. Much scientific computation is CPU-bound rather than memory-bound, so the advantages of C (manual memory control) don't really factor in. 
Speaking of spending extra time compiling, are you able to compile once in advance and then run that compiled program many times (like java) or are you still stuck compiling out each time you run it? I left julia because of this restriction. It was added in, but it seemed like a bit of a hack, and I could never get it working. 
Precompilation works pretty well yes.
the claim was "can be in various cases". and it is pretty much true. i played around with generated functions and cryptography, and i was able to reach 500MB/s with poly1305. i challenge you to do any better in a single thread and without simd wizardry.
Yes you can add __precompile__() to the beginning of module to precompile it. You can also precompile a function for a specific type by adding precompile(f, (Int,)) for example, after you define f(x).
I'm currently in a computation physics class and as a class project I'm going to exactly diagonalize the Hamiltonian for the [Hubbard Model](https://en.wikipedia.org/wiki/Hubbard_model) which is turning into a really fun project. The code needs to be written in Python for the class but I'm refactoring it as Julia as I go because this is an algorithm which will be very to me in the future and good to have in my toolbox.
I don't know if there's a PR at the end of this tunnel, but right now I'm working on trying to understand why and maybe fix that `y = sort(x)` is type stable, but `using SortingAlgorithms; y = sort(x)` is not type stable. Last night I posted a question on slack which understandably got little attention given the time, so I'll repost here as it still describes my current confusion: Hey, so I'm trying to work on an issue brought up / discovered here yesterday. In brief, the problem is that after `using SortingAlgorithms`, the code `y = sort(x)` is not type-stable. I expect for someone who knows what they're doing this would be a 5-min PR, but in the mean time it's a solid learning opportunity for me. Anyway, after playing around a bit, I've gotten stuck on the following (I use `Int` vectors throughout for the input type): 1) In a fresh Julia session (no `using SortingAlgorithms`), `code_warntype(sort!, [Vector{Int}, Base.Sort.Algorithm, typeof(Base.Order.Forward)])` is type-stable; the output is known to be of type `Array{Int64, 1}`. 2) I modified a branch of `SortingAlgorithms` so that its only algorithm is `HeapSort` and confirmed that the type instability problem remains. Under this branch of `SortingAlgorithms`, there are 5 subtypes of `Base.Sort.Algorithm`: ```julia&gt; subtypes(Base.Sort.Algorithm) 5-element Array{Union{DataType, UnionAll},1}: Base.Sort.InsertionSortAlg Base.Sort.MergeSortAlg Base.Sort.QuickSortAlg PartialQuickSort SortingAlgorithms.HeapSortAlg ``` Note that the only of these not from Base is `SortingAlgorithms.HeapSortAlg`. 3) However, with `using SortingAlgorithms`, `code_warntype(sort!, [Vector{Int}, Base.Sort.Algorithm, typeof(Base.Order.Forward)])` is no longer type stable; `code_warntype` flags the output as `::Any`. 4) Also however, `code_warntype(sort!, [Vector{Int}, Int, Int, SortingAlgorithms.HeapSortAlg, Base.Order.ForwardOrdering])` is type stable. In fact, `code_warntype(sort!, [Vector{Int}, Int, Int, &lt;ALG&gt; Base.Order.ForwardOrdering])` is type stable and of type `Array{Int64, 1}` for every `ALG &lt;: Algorithm`. 5) So now I'm confused. The compiler knows that for any `alg &lt;: Algorithm` you choose, the result will be of type `Vector{Int}`. But the compiler can't use that to know that for abstract type `Algorithm` the result of `sort!` will still be of `Vector{Int}`? Why not? 5b) I'm particularly confused given that in a fresh session with no `using`, `code_warntype(sort!, [Vector{Int}, Base.Sort.Algorithm, typeof(Base.Order.Forward)])` does show type stability. Here the compiler can "reason" about the abstract `Base.Sort.Algorithm`. It's only when another subtype is added that things get bad. Why?
This week in DiffEq is a little quiet. There was turnover to make the problem and solution types immutable, and now that's all done and we'll have a DiffEq 3.0 blog post going out soon. Then I wrote a blog post on how to use the ODE solvers with `GPUArray`s to solve (stochastic) PDEs on the GPU through generic typing, and that'll be released soon enough as well. @dextorious is getting DiffEqDiffTools.jl to support complex numbers in its differentiation (gradient, Jacobian, and maybe Hessian if someone asks) toolbox, and we're going to be integrating that into Optim.jl and NLsolve.jl. Since that supports abstract array and number types, that's the final piece for DiffEq, Optim.jl, and NLsolve.jl to support abstract arrays (GPUArrays!) and complex numbers (without the user providing Jacobians etc.), so I'm happy this is finally getting done. Then I have a bunch of secret private Julia projects will be released later that are building off of these tools...
I know some of these words.
So the Hubbard model is basically a very nice simple quantum mechanical toy model which happens to have some very rich physics hidden in it. The idea is that one constructs a lattice that electrons are allowed to hop around to neighbouring sites which is determined by some hopping parameter t and if you get a spin up and a spin down electron on the same site they will interact with some potential U. It turns out that if one studies this model with certain choices of t and U one gets all sorts of interesting phases: notably (and of interest to my research) superconductivity, a phase of matter where electron current is able to flow with **zero** (not nearly zero, exactly zero) resistance and can do all sorts of neat things like (totally expel magnetic fields)[https://www.youtube.com/watch?v=zPqEEZa2Gis]. Now in quantum mechanics, the way a quantum system changes with time is an operator called the Hamiltonian (the quantum mechanical equivalent to Newton's F = ma which is famously known as the Schrödinger equation is determined by the Hamiltonian). This Hamiltonian operator can be represented of as a matrix which multiplies an abstract vector describing a certain quantum state. Because of the nice properties of linear algebra, if one can diagonalize the Hamiltonian, the problem of the time evolution (and hence basically anything we care to know about a given quantum mechanical system) is trivially easy to calculate. Unfortunately, even for a theory as simple and nice as the Hubbard model, the number of dimensions in the Hamiltonian is 4^L where L is the number of lattice points under consideration and so even a lattice of 12 x 12 lattice points would involve a Hamiltonian with more dimensions than there are atoms in the universe and hence no hope of diagonalizing.*** However, for small enough lattices we can actually exactly diagonalize the Hamiltonian and learn some interesting things about systems like the Hubbard model which is my goal. ______ *** Note: if one is clever and uses certain symmetries they can reduce the total number of dimensions in the Hamiltonian but it only gets you so far. I will probably only be considering a 4 x 4 lattice. 
I'm always so impressed when I hear about the things you're doing with DiffEq. Really, I think the work you're doing is profoundly important and the community really benefits from it. 
Working on a package to import and manipulate 16S microbial community analysis, i.e. "microbiome", data. This past week I was working on importing dada2 sequence variant tables and taxonomy data.
This is fascinating, thank you for actually taking the time to explain it. I 100% was not expecting that. My brain is a little fried from working all day but I'm going to read it again in the morning while I have my first cup of coffee. Thanks again.
My pleasure!
I will be working on an agent-based model combined with an optimization model to try and get properties of blockchain data. In particular, we have a project from a new startup who wish to use blockchain technology and want us to run simulations to see how it would play out. 
JuliaMatrices/BlockBandedMatrices.jl. This is actually cleaning up ApproxFun’s current implementation and using the BlockArrays.jl interface. I’ll also restructure the storage to order the memory by columns (instead of by blocks). This is a BLAS Level 3 compatible storage, so one would be able to get really efficient BlockBandedMatrix multiplication. Also, LAPacks qrfact can be used to calculate the QR decomposition (getting multithreading for free). This is useful for spectral methods for PDEs.
Is it bad that I guessed who the author of this post was before I scrolled down to see his name? That guy’s everywhere! The only drawback I can see to teaching undergraduates Julia is that it still doesn’t have much of a role for the web or mobile apps, and I can imagine that undergraduates would be excited to get involved in that side of things. I know that packages are being developed for using Julia as a web backend, though, so who knows how things will look in a few years.
Recreating the nodes from Arena simulator using SimJulia 
&gt; I’ll also restructure the storage to order the memory by columns (instead of by blocks). This is a BLAS Level 3 compatible storage, so one would be able to get really efficient BlockBandedMatrix multiplication. Also, LAPacks qrfact can be used to calculate the QR decomposition (getting multithreading for free). Sounds awesome.
Language speed is a weird topic. So much of the discussion is spent arguing about speed. Yet, often, speed is not the issue. Yes, Python is slow, but it can be optimized (numpy, numba, cython). Conversely, UNoptimized Julia or C can be terribly slow. But the fact that a Julia programmer can write packages *in Julia* is key. High performance Python is only fast because it is *not* written in Python. This, I think, is a much more powerful message than "Julia is faster than Python". And this might indeed be a huge boon for Julia as a language for teaching. As an introductory course though... speed doesn't matter, extensibility doesn't matter. To absolute beginners, the onboarding experience is probably the only thing that matters. You should probably choose a language that makes explaining loops and branches as simple as possible. Maybe Julia is better suited as a second language. 
What are the aims and objectives of the course? You could compare and contrast different ones, even though I use Julia for everything my choices would be: Assembler, C, Lisp, Java, C++, Python, JavaScript 
https://github.com/JuliaNLSolvers ?
[JuMP](https://jump.readthedocs.io/en/latest/) is what you're looking for in terms of constrained optimization.
That's cool, and plans to support symbolic differentiation or integration?
There's https://github.com/dreal/DReal.jl; not very active anymore though and looks to be stuck at Julia 0.4. I also just found https://github.com/jakebolewski/PicoSAT.jl.
I'm just the messenger. Someone would probably have to add that. See related github issue: + [Is this the Julia native version of SymPy?](https://github.com/eveydee/Sylvia.jl/issues/2#issuecomment-315826548)
See also http://www.juliaopt.org/ Optimization packages for the Julia language.
SimJulia can run functions with timeouts and interrupts See example here http://simjuliajl.readthedocs.io/en/stable/10_min/3_process_interaction.html#interrupting-another-process
SimJulia is discrete event simulation but it's not quite what you want, I think.
You'd probably want to build the models in code and simulate with DifferentialEquations.jl
DifferentialEquations.jl is probably a little too low level here since it would require writing all of the diffeqs yourself. The tool in Julia which is most similar to Simulink is probably [Sims.jl](https://github.com/tshort/Sims.jl) (which internally utilizes Sundials and DASKR). [Modia.jl](https://github.com/ModiaSim/Modia.jl), from the creators of Modelica, could possibly be something good when it's released (it's also built on top of Sundials). Then there's [RigidBodySim.jl](https://github.com/tkoolen/RigidBodySim.jl) which is being built on the full DifferentialEquations.jl tools but is currently too alpha to be documented, but looks promising.
Why not just allow symbolic expressions to be optionally subjected to operators, e.g.: Julia&gt; using symbolic Julia&gt; :a + :b :(a + b) 
I'm guessing because that would be type piracy. For one, `:a &lt; :b` already has a meaning.
[removed]
Good libraries it seems. What about the UI part to maintain a spatial relationship between components. Anything?
What do you mean? Like a modeling GUI? I don't think we have that quite yet, and I don't know if anyone is working on it. We need to get the modeling abstractions down before we get (a Google Summer of Code student) to it.
This seems to be the closest at the time but will link to question any other projects I find as at least 2 other people I know in my university are interested.
You could run the function in a Task and interrupt it if it takes too much time: t = @schedule sleep(10) # wait and kill if not done t.state != :done &amp;&amp; @async Base.throwto(t,InterruptException()) I don't know if that works if you function is eating up all the cpu though.
VS Code with the Julia language plugin. It's not the best, but it gets get job done. VS code is a good editor, but the language extension could use some work. I can't seem to "jump to definition" or my locally defined functions. 
I use Juno, but I'd love for an alternative to come around. Atom is a great tool so I don't like that I have to essentially dedicate the interface to one language. Also within Juno there's no great way to interact with or visualize tables - the inline view feels clumsy. An RStudio or Rodeo equivalent for Julia would be amazing.
Somebody could adapt Insight Maker to the task. Thanks for sharing. 
I started using atom because of Juno, and like it a lot. So I didn't have a bunch of customization ahead of time - it's close-ish to R studio, which is great. Plus hydrogen for python and I can use it for just about everything I need. It balks on really large files, but I can mostly use shell commands to interact with those.
Light table is awesome! Try that
 &gt;I usually program with the same style, regardless the language, because most modern languages allow to mix functional, object-oriented and imperative styles. Is there a way to do this and retain all of the benefits that Julia has? If so, I bet you could write a package to make it work. If not, and if this coding style is more important to you than the things that make Julia special, that's totally fine - keep coding in the language that works for you! TBH I'm a bit confused about why this thread exists... It seems like you're saying you want julia to be like some other language, but why not just use that other language? I'm not a language design expert, but my impression is that what makes julia special is precisely the thing that prevents something like dot syntax from working (or making sense).
Juno used to be in lighttable but has moved to Atom
To add to this, deleteat! doesn't copy the vector when deleting while R does. So this would be the most straightforward way to do this.
Notepad++ Though I normally use Acne / Acme-sac but I have been unfaithful
I'm using my own custom editor made with Gtk. I'm not very happy with either solutions, Juno and VS code have too many issues and I'm getting quickly frustrated when using them. I'm still using VS code for some operations like search and replace in files, or when I need multiple cursors. I should work more on my editor to improve it but I don't have much time, and I'm running into difficult issues . 
Projects on my PC at work: Atom/Juno. Small scripts to do personal stuff at home: Notepad++ Basically, when I do a thing that fits on a single screen, Atom already has too many features for that thing \^\^
I'm using emacs as well. I like to use the same editor for all my writing (e.g. LaTeX, programming) and there is also org-mode which can be neat. That being said, I give it a try to atom from time to time.
The one, the only, emacs.
The uber-juno setup does change some defaults for Julia usage (I think this is documented in the readme) but Juno itself doesn't require any of that. If you just install julia-client etc then Juno will only have an effect when you explicitly ask for it (i.e. it won't affect normal Atom usage). I care about that myself so if it's not the case we'd treat it as a bug. The other option is to grab JuliaPro which will give you a separate Atom install, so things will be isolated that way.
It would be pretty easy to build a table view as an atom plugin. There are things like [tablr](https://atom.io/packages/tablr) already so it might be possible to integrate that with DataFrames in future.
I recently registered Microbiome.jl - I'm working primarily with metagenomes data, and I'm basically just writing code for things as I need it, but would welcome PRs and other collaboration. https://github.com/BioJulia/Microbiome.jl
I use Emacs as editor and tmux to make an IDE. 
Thanks for that! My package is unregistered but has an almost identical name and some similar functionality :-). It is, however, completely undocumented and a bit of a mess for anyone else to use: [https://github.com/ianpgm/microbiome.jl](https://github.com/ianpgm/microbiome.jl) I haven't been involved in the development of any official packages as yet, but now that I know there's an official home for all this functionality I've been writing for my own use I might just take you up on that offer to collaborate. Would some functions for importing data (for me the most needed would be dada2 and mothur, although obviously biom would also be a logical step) be a good place to start? 
Let's just set aside this notion of "official" right here and now. Just because I registered first and have worked a bit with the folks at BioJulia :-). Importing data from other tools is a great place to start. A lot of what I'm doing is writing functionality to interact with biobakery tools, which is what my lab uses. The DistanceMatrix and AbundanceTable types are just thin wrappers around regular 2d matrices. Feel free to open issues or PRs and start to port over code. I'll be happy to review and help with integration. Let me know if any of my code doesn't make sense. And also come join us in the BioJulia gitter channel! It's a great community and filled with people much more skilled in Julia than I am :-)
Which packages do you use for Julia purposes? I was using Julia-repl so that I could quickly select text and send it to the repl but it was really buggy.
julia mode, just that, it works fine for me.
For quickly editing a file after include-ing it in the REPL, Vim/MacVim (via Julia’s `edit` function is really quick and easy and you don’t have to leave the terminal. Using OhMyREPL makes using the REPL very pleasant. For exploring ideas, Juno is pretty great, cos I can use the Plot pane for graphic output. I also like using Jupyter for similar reasons, but editing is easier if you have the Vim plugin (for editing inside the Code cells)., because there aren’t many editing tools otherwise. And I still use BBEdit’s folder comparison tool (which is great), and occasionally I’ll use COTEditor for a change. I really liked VSCode when I tried it, but on my slow Mac it dawdles somewhat... :)
Do you have a way to send code to the repl or do you just run your scripts continually?
I send code to the repl all the time, it works fine for me. What sort of problems do you encounter? The only issue I've had with Julia in emacs is that ein does not work well with Julia, so notebooks you still have to do in the browser.
&gt; Is there a way to do this and retain all of the benefits that Julia has? Probably not all, but most of them? Yes. I don't think &gt; It seems like you're saying you want julia to be like some other language, but why not just use that other language? Julia has a better standard library than Swift for my use case and a bunch of nice features.
&gt;Probably not all, but most of them? Yes. So I guess it would just depend on which ones would break. I saw someone else comment that there's a package that implements dot syntax (it just makes the thing you're using the dot on the first argument in the function), nulls are getting improved, and though I don't quite know what you mean when you say: &gt;The way in which types are defined in a file matters, which sucks. but the type system is the whole reason why julia is fast. FWIW, the more I work with the julia mindset, the more frustrated I get when I go back to python. I'm now really glad I couldn't just use the python paradigms even though it was frustrating at first, because now I've got better (I think) habits. I understand the desire to have everything you want - julia got started because the architects couldn't find everything they wanted in a language so had to write their own. I think what they've come up with is pretty great. If you don't, that's fine, I'm just not sure what you think complaining about it is likely to accomplish.
I use emacs too. I used ESS for a while, and it was okay, but I was never all that happy with it. Since I only use julia for hobby &amp; fun stuff, I thought it would be fun and educational to try writing all my own tooling for it from scratch. So right now my emacs/julia setup kind of sucks, but it gets a little bit better every time and I really like using it because it's mine! I haven't put it up anywhere though.
The thing I'm not sure about is whether the web-based tech will be able to do it, or something more native will be required in the long term. It seems you can make fast progress with it, but I don't know how well it scales, if it's stable (it seems Atom is already obsolete in some ways), etc. I feel like if we would have continued to work on the Qt based one since 5 years, we would have a solid IDE by now. https://github.com/forio/julia-studio
It's not graphical like Simulink, but ControlSystems.jl covers a fair amount of the Matlab controls toolbox and has relatively active/responsive developers.
The difference between functions and macros is that macros run at compile time instead of runtime (ie. when your compiled functions are run). This distinction is less obvious in a JIT language like Julia but compilation and runtime are still separate things even if you don’t notice the line between them. Basically, you want to feed code into macros and then the macros change the code (often for optimization purposes) and get that transformed code into the compiler. One application of this is that if you have a function that has s bunch of use cases and needs lots of if statements determining if it runs one chunk of code or another, you could use a macro to make it only pass the chunk of code that needs to be used to the compiler which will then make your function do less work at run time, ie. not having to run through all those if statements. That’s a pretty handwavy example so I’m not sure if it makes much sense. Later I’ll come back and try to edit this with a better example. 
my favourite use is things like this: macro fid(pfix, ext, blk) return quote fn = $pfix "-" * replace(string(now()), ':', '-') * ".txt" fid = open(fn, "w+") _stdout = redirect_stdout(fid) $blk redirect_stdout(_stdout) close(fid) end end @fid "test" begin println("The numbers 1-5 and their squares") for i in 1:5 @printf "%d %d\n" i i^2 end end which writes all the output from inside the block to a file "test-$timestamp.txt" % cat test-2017-10-27T13-54-14.321.txt The numbers 1-5 and their squares 1 1 2 4 3 9 4 16 5 25 
Or this macro cacheFun(fun, fn) return quote if stat($fn).size &gt; 0 @deserialise($fn) else @serialise($fn, $fun()) end end end macro serialise(fn, v) return quote fid = open($fn, "w+") serialize(fid, $v) close(fid) $v end end macro deserialise(fn) return quote fid = open($fn, "r+") v = deserialize(fid) close(fid) v end end
Your types are mismatched. Compare the output of: subroutine sumvec(vec, nlen, update) implicit none integer, intent(in) :: nlen(1) real, intent(in) :: vec(nlen(1)) real, intent(out) :: update(1) update(1) = SUM(vec) print *, "sizeof(vec(1)) = ", sizeof(vec(1)) print *, "sizeof(nlen) = ", sizeof(nlen) print *, "sizeof(update(1)) = ", sizeof(update(1)) print *, "sum(vec) = ", update(1) end subroutine sumvec versus explicitly saying what the bit sizes of your inputs are (using the ISO C module): subroutine sumvec(vec, nlen, update) use iso_c_binding implicit none integer(c_int), intent(in) :: nlen(1) real(c_double), intent(in) :: vec(nlen(1)) real(c_double), intent(out) :: update(1) update(1) = SUM(vec) print *, "sizeof(vec(1)) = ", sizeof(vec(1)) print *, "sizeof(nlen) = ", sizeof(nlen) print *, "sizeof(update(1)) = ", sizeof(update(1)) print *, "sum(vec) = ", update(1) end subroutine sumvec Also, on the Julia side, you can use `Ref` instead of 1-element `Array`s, and Julia knows how to deal with the `Vector` argument automatically: function sumvec(x::Vector{Float64}) outsum = Ref(0.0) N = Ref(length(x)) ccall((:sumvec_, "./libsumvec.so"), Void, (Ptr{Float64}, Ref{Int32}, Ref{Float64}), x, N, outsum) return outsum[] end sumvec([1.0, 2.0]) 
Wow, thank you very much! This is so helpful. I definitely would have struggled for a while to figure out that I pass vectors as `Ptr{Float64}` instead of `Ref{Float64}`. Out of curiosity, is `real(c_double)` any different from `real*8`? They are both 64 bit floats, but is there any low-level difference I should be aware of? I'm trying to use a sizable and complicated Fortran library and I'm afraid to touch very much of it.... Thank you again for the generous help.
&gt; Wow, thank you very much! This is so helpful. I definitely would have struggled for a while to figure out that I pass vectors as `Ptr{Float64}` instead of `Ref{Float64}`. You should probably check the documentation carefully on this one. I knew that `Ptr{Float64}` would work (versus `Ptr{Vector{Float64}}`, but there may be a subtle difference between `Ref` and `Ptr` use when calling through to C/Fortran. (A vague memory is saying that it might be related to object lifetime/ownership, but don't take my word for it.) &gt; Out of curiosity, is `real(c_double)` any different from `real*8`? They are both 64 bit floats, but is there any low-level difference I should be aware of? I'm trying to use a sizable and complicated Fortran library and I'm afraid to touch very much of it. In this trivial test, `real*8` and `integer*4` worked just as well. But maybe there is complexity I'm now aware of? I only have a passing level knowledge of Fortran, so again I might get some of it wrong, but I think they should be equivalent under most circumstances. Again, a vague memory is saying that the default for C's `int` varies between 32/64-bit archs across Linux-vs-Windows, so presumably the module definitions take care of matching correctly. If you can track down the `*N` specs used in Fortran, though, you can always use explicit type matching in Julia (just like a C spec would instead be written using things like `int32_t` instead of `int` to guarantee compatibility). 
That is all very helpful, thank you. I will go look at the documentation carefully. I think it is somewhat sparse for the difference between passing things to Fortran vs. C and by reference vs. by pointer, but if I find something interesting I will share it in this thread. Thank you again for all of your help.
&gt; I think it is somewhat sparse for the difference between passing things to Fortran vs. C and by reference vs. by pointer... https://docs.julialang.org/en/stable/manual/calling-c-and-fortran-code/#Mapping-C-Functions-to-Julia-1 Then scroll back up just one subheading. My use of `Ptr` should have been a `Ref`, it appears.
Thank you for the explanation! This was really helpful. 
My pleasure. Macros are such a powerful concept but so few languages allow their use in a first class way that it is very hard to find teaching material on their use. Lisp learning material uses has pretty good sections on macros, especially the book [On Lisp](http://ep.yimg.com/ty/cdn/paulgraham/onlisp.pdf) by Paul Graham. Unfortunately, the book assumes quite a bit of previous knowledge of Common Lisp and might be very hard to read if you're not already familiar with CL. However, [Practical Common Lisp](http://www.gigamonkeys.com/book/) is a quite good introductory Lisp book that has some nice chapters on macros. Unfortunately, any Lisp book that wants to teach you how to use macros properly is going to require that you learn at least a bit of the language to digest their examples, but if you really want to master macros, Lisp books are probably your best bet until Julia gets some first class books of its own. I do say this having not read any of the current Julia books out there but I suspect (though I'd be glad to be proven wrong) that none of them include a discussion of macros on the level of On Lisp on Practical Common Lisp.
sublime text
Are you able to send code to the REPL somehow?
The reason why I asked for help regarding macros was because I saw that they were so heavily used in Julia, and thus, they must be quite useful. Just like you said. Currently there does not seem to be any really comprehensive Julia books, which is quite understandable. I will follow your advice and take a look at the Lisp books you mentioned. I think this is a good chance to learning something new given that my background is mostly R / Python / Matlab.
There are a few packages that attempt to do it (sublimeREPL and sublime IJulia) but tmk the updates are slow. I personally just have a terminal open.
[removed]
Wow, this looks promising.
I'm using it every day as my main working tool (on os x) so it's definitely usable, but since I'm the only user (that I know of) it's not really improving in any aspect I don't care about. I also got used to work around some big issues, so I don't really see them anymore.
Fantastic work!
Julia's macros are certainly as powerful as those in other languages. There are, of course, design and implementation differences, but they are mostly superficial and not particularly limiting. Julia also has generated functions, which are a really powerful macro-like extension. I had similar feelings about the `@` sigil, but have come to like it despite the downsides. It rightly discourages you from using macros everywhere, and acts as a flag that something unusual is happening (compare this to something like R where evaluation rules can change under you at any time). It helps a lot in explaining things to beginners, because one can think of it just as a sort of annotation. For a heavily user-oriented language it makes sense.
One of the powers of lisp macros is they look like normal code. One of the drawbacks of lisp macros is they look like normal code. 
BTW, you might get some more responses by posting this over at the [Julia discourse forum](https://discourse.julialang.org/) which is typically more active than the subreddit. Just please be sure to mention that it's a cross post (to avoid duplicate answers). 
What is the speed penalty? I might want the [wrong answer quicker](https://pics.onsizzle.com/it-says-here-that-y-you-are-very-quick-at-7442470.png)
&gt;What is the speed penalty? I was curious so I ran a quick check: using SaferIntegers a = SafeInt64(3) b = SafeInt64(2) f(a,b) = a+b @time for i in 1:10000000 f(a,b) end to get 0.240073 seconds (10.00 M allocations: 152.588 MiB, 6.23% gc time) 0.263267 seconds (10.00 M allocations: 152.588 MiB, 6.54% gc time) and then a = 3 b = 2 @time for i in 1:10000000 f(a,b) end for 0.164407 seconds 0.166227 seconds It looks like not everything in there stack allocates so that could add up, but without GC costs it's about a 50% change. &gt; I might want the wrong answer quicker The default integers do that automatically. This integer type "turns on" overflow checking.
Thanks for doing that. A good optimizer would run them in 0 time :) perhaps a sum of 1:100000000 would be slightly more representative. But anyway, I can do it myself in due course. The second bit was my little joke
Performance looks pretty good to me. Running this a couple of times to allow compilation to happen gives these results on my laptop: * Creation of an array seems systematically faster! * Summation of that array does not seem to be systematically slower. * That said, summation of a generator-type object can be dramatically slower depending on the nature of the generator. (confused about the allocation/performance of that second generator sum!) 0.6 REPL: julia&gt; #Pkg.add("SaferIntegers") julia&gt; using SaferIntegers julia&gt; N = 1000000 1000000 julia&gt; @time V1 = collect(1:N); 0.006633 seconds (8 allocations: 7.630 MiB, 61.41% gc time) julia&gt; @time V2 = [i for i in 1:N]; 0.027875 seconds (8.66 k allocations: 8.289 MiB) julia&gt; @time Vs = [SafeInt(i) for i in 1:N]; 0.001955 seconds (8 allocations: 7.630 MiB) julia&gt; julia&gt; @time sum(V1) 0.000457 seconds (5 allocations: 176 bytes) 500000500000 julia&gt; @time sum(V2) 0.000464 seconds (5 allocations: 176 bytes) 500000500000 julia&gt; @time sum(Vs) 0.000638 seconds (5 allocations: 176 bytes) 500000500000 julia&gt; julia&gt; @time sum(1:N) 0.000005 seconds (6 allocations: 208 bytes) 500000500000 julia&gt; @time sum(i for i in 1:N) 0.022120 seconds (13.98 k allocations: 778.871 KiB) 500000500000 julia&gt; @time sum(SafeInt(i) for i in 1:N) 0.000326 seconds (9 allocations: 272 bytes) 500000500000 
That's impressive. Almost no reason to use unsafe arithmetic
I'm confused. For N = 10000000, V2 = [i for i in 1:N] sum(V2) seems to only be twice as fast as Vs = [SafeInt(i) for i in 1:N] sum(Vs) whereas sum(i for i in 1:N) is 1000 times faster than sum(SafeInt(i) for i in 1:N) Any idea why this would be the case? Am I misreading something?
no, that looks right. There were some other more confusing timings that showed up and went away inside the function. But I've found that I'm very bad judging julia's performance behavior a priori. Explaining compiler behavior is outside my domain though
I mainly use Vim but I wish there are some plugins for auto-completion
I was trying to remember what this was called, thanks for posting this again. I’ve been meaning to give it a shot, I’ll let you know how it goes on Linux. But at any rate it looks awesome, this is exactly the kind of IDE we need!
Is there a change log somewhere?
https://github.com/JuliaLang/julia/compare/v0.6.0...v0.6.1
Thanks!
This example is a relatively small in size application but even in this case it is difficult to manage the various components it has with the poor module system of Julia. Have a look at the multiple and confusing import and export statements the source code has. And then imagine an application with ten times more files and modules. Scientific applications are not just a collection of functions doing heavy computations on some arrays and vectors. They usually have many complicated models, a lot of logic, components that simulate physical entities etc. So, it is of great importance to have a good and flexible module system which is independent of paths and the physical location of files. And of course one must have compiled binaries of the libraries and the main application that do not get recompiled on every execution but only when dependencies change to the minimum level possible. Rust is doing a great job in this respect. I am also not a fan of dynamic languages because this approach can harm performance and at the same time make code difficult to understand and further develop. Specifying types also works as documentation for the code. Still, Julia is more readable than other programming languages because of its syntax which helps the development of large applications. In contrast to what most of the Julia developers believe I think that as a language Julia lacks modern features in terms of modelling. It is neither an OOP nor a Functional programming language and that is also a problem for large applications that can only be managed and efficiently developed by the use of one or both of these philosophies or at least some other new approach offered by a new language. Ideally, at least from my point of view, I would like to see a language with the build tools and safety of Rust, the clear and readable syntax of Julia, the language features of Scala and the performance of C/C++/Fortran. I think that such a language does not yet exist.
Didn't want safer integers to have all the fun 😉
sublime.
I’m no expert in this but can you express your one quadratric constraint as two linear constraints?
Working on Colors.jl Specifically trying to create a framework to integrate spectral data for more interesting color science, which would enable a lot of more sophisticated stuff like spectral based color deficiency simulations etc. which I have a few interesting papers about. Also trying to fix the rather poor handling of RGB device color spaces. The current state of affairs is a bit unsatisfactory. Other than that, maybe I’m going to work on my BeeswaxEsolang.jl interpreter. I think the code needs some serious overhaul.
I just run this little script given by Tim Holy to get rid of the waiting time for precompilation when you you just want to run stuff quickly in the REPL. Pre/recompile everything immediately after Pkg.update() At least this reduces waiting time when you’re actually using a package. function recompile() for pkg in Pkg.available() try pkgsym = Symbol(pkg) eval(:(using $pkgsym)) catch end end end pkgupdate() = (Pkg.update(); recompile())
Nice one, but it only does pre-compilation. If you use Plots with PyPlot, for example, even after this the first `plot(...)` will take forever. My point was it must cache fully compiled and not just precompiled code.
[Issue #918](https://github.com/JuliaPlots/Plots.jl/issues/918) of Plots.jl is still relevant, I guess. For my 6 year old core i5-2410M laptop it takes around 25 seconds to load Plots.jl, which is a real pain, indeed. julia&gt; @time using Plots 24.904700 seconds (6.40 M allocations: 352.249 MiB, 2.16% gc time)
The thing is it is not just Plots.jl, it is Julia's feature. And it is useful (otherwise you will end up with essentially C++ with all its ugliness). But one "subfeature" is missing: save fully compiled (and not just precompiled) code between the sessions. I am not sure if there are any implementation issues with it --- my knowledge in this area is not that good --- but as a user, I would like to have it.
When it comes to code generation, I think Simulink is still pretty unique.
https://github.com/JuliaArrays/OffsetArrays.jl julia&gt; x = OffsetArray(Float64, 0:9) OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}} with indices 0:9: 6.27787e-316 6.27787e-316 8.83327e-316 6.27787e-316 6.27764e-316 8.8333e-316 6.27788e-316 8.87189e-316 8.87189e-316 8.8719e-316 julia&gt; fill!(x,1) OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}} with indices 0:9: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 julia&gt; x[0] 1.0 julia&gt; x[0:9] 10-element Array{Float64,1}: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
A great tool!
Great, thanks, it's easy :)
Has about a 10% performance loss for now, but this might be one of the things that https://github.com/JuliaLang/julia/pull/24362 will fix.
**Update:** Thought it might be useful to link the official discourse announcement. + https://discourse.julialang.org/t/ann-higherprecision/6956/17 ------ There's a decent amount of discussion there about the future of the project and how it fits into the julia ecosystem
why not eg, download/install Atom and Atom Beta (https://atom.io/beta)? use one for Julia/Juno, and the other for everything else--these two will live side-by-side; one is not going to over-write the other. This is a common setup for contributors--eg, hack on the Beta, and use the stable release for as their general purpose editor.
Use a different solver. Ipopt if you don't have any integer variables, Bonmin if you do. Or one of the commercial solvers if you can get a license.
Congrats to the Julia team! Has anyone been tracking the rise through the tiobe ranking over the last few months? I'm curious about how adoption is growing.
&gt; The difference between functions and macros is that macros run at compile time instead of runtime That is not really true. It is very common optimization technique to evaluate functions at compile time, to improve performance... Instead macros are special types of functions, that operate over AST.
&gt;But the fact that a Julia programmer can write packages in Julia is key. Exactly. Most of the Julia Base package is written in Julia, which means everyone can become a Julia developer without having to rely on a second language. There are no hidden tricks, everyone can find out how Float64 is implemented, for example. Want to implement UInt56? Go ahead, you can do it in Julia, and you won’t need to rely on writing anything in C or C++.
&gt; @edit A\b will open your default editor at the function call (you can get the file and line number with `@which`) and follow the function calls. Not exactly the convenience you were hoping for. Looks like it ends up being `dgetrf` and `dgetrs`in your example.
Put it in a function then use `@code_typed`?
thanks! didn't know about @edit. pretty cool macro :-)
Great content! I'm having trouble understanding why PCA is a good choice for dimensionality reduction. Usually PCA is used for interrelated variables. In economics, it could be how much someone earns and how much wealth they have, since they can both be mapped to some abstract measure of (unobserved) economic security. When you perform the PCA, each "variable" is an account of interest (someone you follow), and each "observation" is an account your "accounts of interest" follows. In PCA, you are almost assuming some kind of clustering already, since you are making some assumption about accounts of interest being interrelated in some way. 
I actually learned quite a bit more about PCA from the lectures in my machine learning class immediately after publishing the post, haha I was assuming (probably wrongly) that PCA would essentially do some kind of "feature selection" by chucking out the dimensions where the data points are much more crowded. I talked to some of my peers and they said that perhaps something like using the KL-divergence would be a better idea. I'll definitely talk about it when I've worked on it some more and I'm hopeful that I'll get better results by switching my dimensionality reduction method.
PCA is just a linear change. Think about if all of your data was on one line in N-dimensional space. The largest principle component would be the vector that points along that line. If all of your data was on a plane, it would be two vectors that span that plane. So what it basically is is just a "rotation" to a new definition of your axes, z=P*x where P is the matrix of principle components as columns, defines z as a new variables where all of the changes tend to line up on the axis defined by the z_i with the lowest i. PCA thus can't catch everything because it's only a linear change, but this "change to thinking about linear combinations" can usually be good enough. For example, if you know things like y[1] + y[2] = 1, then PCA will say that this is really only one vector of change. But if your data lives on something nonlinear that you cannot get by rotations, like a donut, PCA will not be too helpful. Note that PCA is not like clustering. Since N-dimensional space is spanned by N linearly independent vectors, the N PCA vectors that come out span the same space and thus represent the full data. Again, it's just an axis rotation. But the singular values (PCA is just an SVD) tell you how much each axis is required to explain the data fully, so normally this is then used to get to a lower rank approximation. But that's not exactly required as part of PCA. 
"A 128 bit number type with around 30 bits of precision." Surely it should say 30 digits of precision..
https://www.meetup.com/Sydney-Julia-Julialang-Meetup/events/244733360/ Here are the very interesting slides
neovim + doplete 
Hi enrick
[removed]
[removed]
This is really useful
\#Exciting
[obligatory](https://i.imgur.com/7drHiqr.gifv)
Congrats on the milestone to 1.0! Out of curiosity, will the language be more stable after you hit 1.0? I wrote a significant amount of code in 0.3 and it all broke in later versions. I've been gun shy about starting new projects in Julia after that, but I've been eyeing it again lately.
I think so. The post says: &gt; That means that if there’s a feature you want in 1.0 it needs to be in by then or it’ll have to wait for 1.x (or 2.0 if it’s breaking).
1.0 is made to be the last set of breaking updates until 2.0, so that way the focus will shift from breaking language changes to things like compiler optimizations and bugfixes. So yes, 1.0 will be a stable language to target. That doesn't mean it'll be perfect on the first day though (I'll have blog post on this), but what it means is that 1.0 will be a lasting syntax that then everything can start upgrading to and working with. So don't expect the debugger to work on the release day of 1.0, since having a stable language is what's required for the debugger to stop breaking! But after it's released, then expect everything to get compatible with it and stay that way for quite some time.
Great news! Thanks!
Sounds like a win
Direct link to the repo: https://github.com/stevengj/NBInclude.jl
Cross posted.... https://stackoverflow.com/questions/47533878/can-i-use-instead-of-in-julia
definitely not the first, you don't want these to be different. a &lt;- 2 a&lt;-2 # and i really want this to be less than -2
Maybe the question is not clear. I want to map "&lt;-" or "←" to "=" so I can do assignment with a different symbol than "=". This is possible for functions like "+", etc. But I haven't found a way of doing it for assignment.
I think the better question is why you would need this
I don't, I just want it. It think '=' looks ugly.
i don't know the plans for release or even 0.7, but in 0.6 and before = was a hard coded language element with special semantics. you can't map that functionality to other symbols. as a rule of thumb though, it is not very smart to tweak a language just for your taste. consider someone else needs to review your code, or cooperate on a project. the poor other guy needs to understand two dozen of your special antics just to read the code. instead, try to make = click in your head. maybe harder, but certainly more fruitful.
... 
Ugly or not, it is the only way to do assignments in most programming languages so I suggest you get used to it.
Is `:=` available?
I don't see why not. It'd just be a map telling julia to interpret &lt;- as =. I just think it looks better.
Of course I would not use that for anything collaborative. It's for my self, to make me happy. I don't get why people get so pissy about it.
sigh... I know it's how you usually do it, but I don't see why it should be impossible to map something else to mean =. If you said there is technical reason X why this is impossible I'd get that, but just saying "that's how it is, get used to it" is kinda dumb. I'm of course used to it, I just don't like it.
what?
You could do it with a macro that just looks for call to \leftarrow and replace them by = function _myeq(ex::Expr) if (ex.head == :call) &amp;&amp; (length(ex.args) &gt; 1) &amp;&amp; (ex.args[1] == :←) ex = Expr(:(=),ex.args[2:end]...) end for i=1:length(ex.args) ex.args[i] = myeq(ex.args[i]) end ex end _myeq(x) = x macro myeq(ex) esc(_myeq(ex)) end @myeq begin x ← sin.(rand(10)) y ← [i for i=1:10] end But then you would have to wrap all your code in a @myeq macro (you could have your special include function for that though). The macro above doesn't work for multiple assignments (y,z ← 1,2). 
this is great, thanks! 
Macros are fun to play with special syntax. There's also some discussion here: https://discourse.julialang.org/t/can-we-override/5856/12
While I appreciate your position here, I did not read the comment you're responding to as pissy at all. Haven't read further down, so maybe you picked up some hostility from other comments, but it's not here. Normally I wouldn't respond like this, but generally speaking this community is really wonderful. Sometimes we even spend time bikeshedding on questions of tone. I don't want you to come away with a bad experience, but also want to keep room for genuine disagreement. Cheers!
The technical reason is that the developers of the language chose '=' as the symbol for assignment and the grammar to parse the language expects it, so in most languages it's not possible to remap. You could have a look at some other languages like Clojure (or any Lisp) if you want to avoid '=' for assignment, though my guess is you come from a mathematical background which is why you've used R and Julia, so no sure if you'd be interested in something so different.
It is helpful to not change basic syntax because then your code is less easy to read. Everyone will have to go look up what ← or your macro, when in reality it's not doing or helping anything.
Sure, but I can already do: `const - = +` and confuse everyone.
Not going to try rebuilding myself, but you might be able to get away with adding `←` to the `prec-assignment` list next to '=' in [julia-parser.scm](https://github.com/JuliaLang/julia/blob/master/src/julia-parser.scm). It looks like this will get used on line 746 via the `is-prec-assignment?` However, I recommend that you just get used to using `=`.
Thanks, I'll give it a try. I am 'used to' the = thingy, been programming for some 15 years. But since I was doing mostly R for grad school I got used to the &lt;-, which I think looks nicer.
It is not available.
That's typical R, choosing to do something with a totally different convention than everyone else.
You can make this a little cleaner with MacroTools: ```julia macro pirate(ex) MacroTools.prewalk(ex) do ex @capture(ex, x_ ← y_) ? :($x = $y) : ex end |&gt; esc end ``` `&lt;-` won't work for this but `&lt;=` or `:=` will. (If there's one thing we can regret about the history of programming languages, it's that the misleading `=` won out over Algol's `:=` for variable assignment.
Dumb question: how do I update to the latest version. I might be missing something, but here is what happens when I try: julia&gt; Pkg.update("DataFrames") INFO: Updating METADATA... INFO: Computing changes... INFO: Package DataFrames was set to version 0.10.1, but a higher version 0.11.1 exists. To install the latest version, you could try updating these packages as well: CSV, RCall and ODBC. INFO: No packages to install, update or remove julia&gt; Pkg.update("CSV") INFO: Updating METADATA... INFO: Computing changes... INFO: Package CSV was set to version 0.1.5, but a higher version 0.2.0 exists. To install the latest version, you could try doing a full update with `Pkg.update()`. INFO: No packages to install, update or remove julia&gt; Pkg.update() INFO: Updating METADATA... INFO: Computing changes... INFO: No packages to install, update or remove 
You have something upper bounding the versions of these packages. You'll need to do some investigating to find out what it is. There's a big issue following this: https://github.com/JuliaData/DataFrames.jl/issues/1232
So as you might have surmised from the lack of responses, this subreddit is a bit of a sleepy place lately. The [Julia Discourse](https://discourse.julialang.org) is much more lively but personally I think Reddit is a better format for these sorts of discussions. Anyways, I'm not an expert but I can try to answer your questions and if you want to know more I'd reccomend these links: [one](https://discourse.julialang.org/t/julia-motivation-why-werent-numpy-scipy-numba-good-enough/2236), [two](https://discourse.julialang.org/t/why-i-contribute-more-to-julia/5734), [three](https://discourse.julialang.org/t/why-i-use-julia/4819) &gt;Does Julia have functionality similar to RStudio Server? That's one of the major selling points of R for me -- the ability to spin up an instance and just access RStudio via a web browser. Kind of. Julia has a Jupyter kernel made for it and so if you set up your remote machine to host of a Jupyter notebook that you log onto remotely then you could just use the Julia kernel in that notebook which would cover most of the functionality you're talking about. With some work, this could maybe be made to interface with the Juno IDE remotely (if that doesn't already exist). &gt;What is the state of chemoinformatics packages in Julia? E.g. parsing SMILES and computing molecular fingerprints. If such functionality doesn't exist, I would be pretty interested in implementing it! I have no idea. &gt; Are standard machine learning algorithms implemented in Julia? I'm specifically interested in random forests, gradient boosting, and elastic net regularized linear regression. Bonus points if the aforementioned are available for Cox proportional hazards models. Yes, there are quite a few machine learning packages available for Julia which I hear are quite nice. I'd check out [Knet](https://github.com/denizyuret/Knet.jl), [flux](https://fluxml.github.io), [cassette](https://github.com/jrevels/Cassette.jl), [CUDAnative](https://github.com/JuliaGPU/CUDAnative.jl) and [DataFlow](https://github.com/MikeInnes/DataFlow.jl). I don't know anything about Cox proportional hazards models so I can't help there. &gt;I don't really have any problems with Python. I just use it for general purpose computing rather than for any statistical analyses. But if there are advantages of Julia over Python when it comes to general scripting, I'd be pretty interested to know about them. I think generally a big thing for me is Julia's support for lisp style macros which allow for syntactic abstractions and metaprogramming at a level that Python just can't match and it does so naturally enough that it wouldn't be a waste of your time to use them in one off scripting tasks. Further, and this is one of Julia's greatest advantages, is that in Python if you have some general, one off scripting task that you've put a lot of work into and then one day realize "this should really be a package" there's a couple of large barriers if your way, one of the biggest ones being that if you want your package to be performant you need to throw out a ton of python and start writing in C which is not an issue in Julia. In general, from what I hear Julia code seems to scale up with much less effort than a language like Python without sacrificing the ease of use and interactivity that Python provides. [There's a great article about that here](http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/) 
Thanks for the response! It's finally updated itself after I removed ODBC, which was apparently blocking RCall and CSV from updating as well. What was strange is that I couldn't find anything that might have caused RCall and CSV to be stuck in their current versions until I simply did Pkg.rm("ODBC"). Perhaps it simply wasn't installed correctly in the first place.
Echoing Eigenspace below I'll note that most of the Julia discussion happens on the Discourse or gitter/slack channels. And I also am not an expert in your question areas. But I'll give it a shot! &gt; Parallelization in R is finicky. It can be very annoying figuring out the 'right way' to split up a computation. The total inability to pass by reference rather than by copy is very frustrating. Julia has been explicitly designed with parallelization in mind. There's both thread-level and process-level parallelism. Both are pretty easy to use. In particular threaded parallelism can be like magic. E.g. I write simulations in Julia; if I know that a time-consuming simulation function is read only and I call the function several times in a loop, I just annotate the loop with `Threads.@threads` and get 9x (I run with 9 threads) speedup. &gt; Addressing large blocks of memory or working with large datasets stored on disk (rather than in memory) are annoying enough that I've preferred to find suboptimal workarounds rather than learn the relevant tools. I'm not an expert here; this feels like a case where by the very nature of the problem - big datasets which don't easily fit in memory - good tools are going to be necessary. You might check out [JuliaDB](https://github.com/JuliaComputing/JuliaDB.jl). &gt; Does Julia have functionality similar to RStudio Server? That's one of the major selling points of R for me -- the ability to spin up an instance and just access RStudio via a web browser. There's JuliaBox and Jupyter kernels. I think there was also news recently about AWS Julia images; I'm not an AWS user so I didn't look closely. &gt; What is the state of chemoinformatics packages in Julia? E.g. parsing SMILES and computing molecular fingerprints. If such functionality doesn't exist, I would be pretty interested in implementing it! No idea! Check out https://pkg.julialang.org or [Julia Observer](https://juliaobserver.com) to look for packages. &gt; Are standard machine learning algorithms implemented in Julia? I'm specifically interested in random forests, gradient boosting, and elastic net regularized linear regression. Bonus points if the aforementioned are available for Cox proportional hazards models. Yes, most standard machine learning algorithms exist in some Julia package. That said, it'll be a while before Julia natively has the kind of ecosystem breadth R enjoys. E.g. in Julia you'll find a few packages for linear regression and regularized regression, but not a package for every combinatorial combination of &lt;type of regression&gt;, &lt;problem domain&gt; &lt;type of validation&gt; etc. In my experience, Julia packages tend to be more user friendly and performant than R packages, but in the grand scheme of things the language is yet young and package quality varies widely. This is probably a good time to note that in Julia, it's very easy to call other languages! Of particular interest are [Rcall](https://github.com/JuliaInterop/RCall.jl) and [PyCall](https://github.com/JuliaPy/PyCall.jl). This puts the power of the R and Python ecosystems at your tips without having to leave Julia. Okay, so why use Julia at all? Because it's great for scripting and prototyping and glue, see below: &gt; I don't really have any problems with Python. I just use it for general purpose computing rather than for any statistical analyses. But if there are advantages of Julia over Python when it comes to general scripting, I'd be pretty interested to know about them. Julia is easy to write/easy to read, has lots of nice syntactic sugar, allows you to express high level ideas, etc. So does Python. So why use Julia? Because Julia is also fast, and in particular you don't have to give anything up to be fast. Example of a language where this isn't the case: Matlab. In Matlab, if you write everything as matrix operations, your code is fast! But start using structs or loops or pretty much anything which isn't a matrix operation, and your code is slow. So in Matlab there's often a distinction between readable code and fast code. Not so in Julia. Normal readable Julia code _is_ really fast, and there's no speed penalty for writing e.g. a loop instead of a vectorized operation instead of a higher order function. So you can write using whatever techniques are most natural for your problem at hand, and it will be speedy. And I think the speed difference is enough to be a meaningful additional feature, not just "my code does the same thing slightly faster." If I run a quick simulation which takes 2 seconds, I just watch the cursor blink twice and get my answer. If that simulation takes even a minute, I'm off to read about politics on Twitter and now that's 15 minutes gone. Benchmarks are tricky and we can quibble about the correct value of Y where Julia is Yx faster than Python, but for any reasonable value of Y, there are a whole class of things which take painfully long in Python but not painfully long in Julia. Lastly I'll note that Julia's type system and multiple dispatch allow a ton of nifty solutions and fun things like autodifferentiation. Perhaps upcoming tools like [Casette.jl](https://github.com/jrevels/Cassette.jl) (which truly I don't really understand) will improve even further!
&gt;throw out a ton of python and start writing in C Isn't this where Cython comes in?
Yes thats exactly what I was talking about. One has to stop writing python and start writing C but then when you do that, you lose many of the benefits of being in Python. 
&gt; What is the state of chemoinformatics packages in Julia? E.g. parsing SMILES and computing molecular fingerprints. If such functionality doesn't exist, I would be pretty interested in implementing it! Pretty much non-existent as far as I know. I maintain [Chemfiles.jl](https://github.com/chemfiles/chemfiles.jl/) in roughly the same space (more oriented toward computational chemistry rather than cheminformatics), and I've never seen a SMILES parser or molecular fingerprinting code. You should be able to wrap OpenBabel pretty easily with [Cxx.jl](https://github.com/Keno/Cxx.jl), or just write your own!
I shared this also to /r/MachineLearning: https://www.reddit.com/r/MachineLearning/comments/7i52n7/d_on_machine_learning_and_programming_languages/ I'd appreaciate some input from the Julia community in there! :)
I've finally finished a Python project I was working on for doing exact diagonalization of a simply 1D quantum mechanical model and now I want to generalize it and port it over to Julia and ideally, I'd like it to eventually grow into a package.
I’m working on coding a dynamic routing algorithm for sailing craft. It’s essentially a recursive algorithm interfacing with a few other bits and bobs. Makes sense to code it in Julia as it’s pretty fast and I don’t need to go into a different language to speed it up. I’m also looking at working with timeseries but it seems that python has a wider range of packages for this purpose. If someone has seen some recent work on timeseries in Julia please direct me to it!
I am presently in the middle of: - writing a PR for Multivariate stats to alter the behavior of their cMDS - sifting through a PR someone wrote for my microbiome package that makes it more interoperable with other ecology-related stuff - re-doing an analysis of the gender of authors of computational biology to update it for Julia 0.7 (it's already published, but I want people to be able to still run it and learn from it) - trying to figure out why the hell DataFramed v0.11 won't stay installed (some other package keeps downgrading it, but I can't figure out which)
- Most of what I am doing is in a private repo right now, developing a modeling package for drug dosing prediction. I hope it'll drop in a few months. It's really testing [DifferentialEquations.jl's](https://github.com/JuliaDiffEq/DifferentialEquations.jl) event handling in the weird cases (delay and stochastic equations with resetting time and other stuff thrown in). - I am adding a bunch of benchmarks to [DiffEqBenchmarks.jl](https://github.com/JuliaDiffEq/DiffEqBenchmarks.jl). Adding 3 more stiff ODE benchmarks in a few days. I made some functionality to test efficiency of SDE methods without analytical solutions, so I plan to dump a methods for that on sometime soon to really test solvers on some real equations. - Finishing up some publications for new SDE solvers. I want to release those soon! - Building some PDE tooling, and writing a paper on how a modular PDE solving environment can be written. - Adding event handling to [Sundials.jl](https://github.com/JuliaDiffEq/Sundials.jl) hopefully in a week or so. - I want to set everything in DiffEq to replace calls to `similar` and `zeros` to new recursive versions which will be in [RecursiveArrayTools.jl](https://github.com/JuliaDiffEq/RecursiveArrayTools.jl) so that way everything works with arrays of arrays. Last week I got arrays of static arrays working, so this next step would be quite nice.
You'd have to interface with C (that's what Python does!). I'm sorry I don't know if there is such a library already :( There is this if you're running Linux and X11: https://github.com/emmt/Xlib.jl
I've been looking out for "Learning Julia: Rapid Technical Computing and Data Analysis" for a few months now, but it doesn't seem available anywhere. Does anyone know more about this?
It never existed. It was cancelled a long time ago.
In the end it will always be a loop, but you can write: DataFrame(colwise(Vector{Float64}, df), names(df)) to create a new DataFrame with converted columns.
I don't believe this works, colwise returns its results as an array, e.g: julia&gt; df 2×2 DataFrames.DataFrame │ Row │ a │ b │ ├─────┼───┼───┤ │ 1 │ 1 │ 3 │ │ 2 │ 2 │ 4 │ julia&gt; colwise((x)-&gt;convert.(Float64,x), df) 2-element Array{Any,1}: DataArrays.DataArray{Float64,1}[[1.0, 2.0]] DataArrays.DataArray{Float64,1}[[3.0, 4.0]] So you would need to do: julia&gt; getindex.(colwise((x)-&gt;convert.(Float64, x), df), 1) 2-element Array{Array{Float64,1},1}: [1.0, 2.0] [3.0, 4.0] Given all that, it will be easier, I think, to just loop through the columns using eachcol (Adding some checks and filters for the type you are trying to convert if that is necessary). Note that this will replace your existing DataFrame, but that seems to be what you want based on your example. julia&gt; @time begin for (name, col) in eachcol(df) df[name] = convert.(Float64, col) end end 0.000389 seconds (123 allocations: 5.359 KiB) julia&gt; @time DataFrame(getindex.(colwise((x)-&gt;convert.(Float64,x), df), 1), names(df)) 0.042348 seconds (9.69 k allocations: 530.151 KiB) 2×2 DataFrames.DataFrame │ Row │ a │ b │ ├─────┼─────┼─────┤ │ 1 │ 1.0 │ 3.0 │ │ 2 │ 2.0 │ 4.0 │
Well, I believe it does: julia&gt; using DataFrames julia&gt; df = DataFrame(a=[1,2], b=[3,4]) 2×2 DataFrames.DataFrame │ Row │ a │ b │ ├─────┼───┼───┤ │ 1 │ 1 │ 3 │ │ 2 │ 2 │ 4 │ julia&gt; DataFrame(colwise(Vector{Float64}, df), names(df)) 2×2 DataFrames.DataFrame │ Row │ a │ b │ ├─────┼─────┼─────┤ │ 1 │ 1.0 │ 3.0 │ │ 2 │ 2.0 │ 4.0 │ And if you want pure speed with updating use: function convertall(df) for i in 1:ncol(df) df[i] = Vector{Float64}(df[i]) end end Let us define your implementation as: function f(df) for (name, col) in eachcol(df) df[name] = convert.(Float64, col) end end Now I time both (run twice as the first run includes compilation overhead): julia&gt; x = DataFrame(rand(Bool, 1000, 1000)); @time convertall(x); 0.510263 seconds (603.86 k allocations: 40.132 MiB, 1.65% gc time) julia&gt; x = DataFrame(rand(Bool, 1000, 1000)); @time convertall(x); 0.003379 seconds (3.47 k allocations: 7.789 MiB) julia&gt; x = DataFrame(rand(Bool, 1000, 1000)); @time f(x); 1.781253 seconds (1.16 M allocations: 68.587 MiB, 1.80% gc time) julia&gt; x = DataFrame(rand(Bool, 1000, 1000)); @time f(x); 0.052309 seconds (43.98 k allocations: 9.125 MiB) And it compiles faster and runs faster after compilation.
I'm curious what version of julia you are using: 0 muchmore@oahu &gt; julia _ _ _ _(_)_ | A fresh approach to technical computing (_) | (_) (_) | Documentation: https://docs.julialang.org _ _ _| |_ __ _ | Type "?help" for help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 0.6.0 (2017-06-19 13:05 UTC) _/ |\__'_|_|_|\__'_| | |__/ | x86_64-redhat-linux julia&gt; using DataFrames julia&gt; df = DataFrame(a=[1,2], b=[3,4]) 2×2 DataFrames.DataFrame │ Row │ a │ b │ ├─────┼───┼───┤ │ 1 │ 1 │ 3 │ │ 2 │ 2 │ 4 │ julia&gt; DataFrame(colwise(Vector{Float64}, df), names(df)) ERROR: MethodError: no method matching colwise(::Type{Array{Float64,1}}, ::DataFrames.DataFrame) Closest candidates are: colwise(::Function, ::DataFrames.AbstractDataFrame) at /home/muchmore/.julia/v0.6/DataFrames/src/groupeddataframe/grouping.jl:263 colwise(::Array{T&lt;:Function,1}, ::DataFrames.AbstractDataFrame) where T&lt;:Function at /home/muchmore/.julia/v0.6/DataFrames/src/groupeddataframe/grouping.jl:268 colwise(::Any) at /home/muchmore/.julia/v0.6/DataFrames/src/groupeddataframe/grouping.jl:266 Off topic a bit, but the speed up in the two versions is mostly due to calling Vector{Float64} vs convert, not how you are looping (see below). The direct indexing is a little bit faster, though (I think) a little harder to read. I will say that I was pretty surprised at the difference. I would not have expected the call to convert to be that much slower (I honestly just liked the syntax better) which just goes to show that the only way to know is to test :). I can't think of any cases off the top of my head where convert would work and the constructor would not, but it will be interesting to dig into the differences when I have some time. Anway, interesting stuff. julia&gt; f1() = for (n, c) in eachcol(df); df[n] = convert.(Float64, c); end f1 (generic function with 1 method) julia&gt; f2() = for i in 1:ncol(df); df[i] = convert.(Float64, df[i]); end f2 (generic function with 1 method) julia&gt; @benchmark f1() BenchmarkTools.Trial: memory estimate: 5.20 KiB allocs estimate: 117 -------------- minimum time: 227.304 μs (0.00% GC) median time: 232.607 μs (0.00% GC) mean time: 236.080 μs (0.37% GC) maximum time: 4.736 ms (92.19% GC) -------------- samples: 10000 evals/sample: 1 julia&gt; @benchmark f2() BenchmarkTools.Trial: memory estimate: 5.06 KiB allocs estimate: 112 -------------- minimum time: 222.216 μs (0.00% GC) median time: 227.543 μs (0.00% GC) mean time: 229.984 μs (0.00% GC) maximum time: 676.109 μs (0.00% GC) -------------- samples: 10000 evals/sample: 1 julia&gt; f3() = for (n, c) in eachcol(df); df[n] = Vector{Float64}(c); end f3 (generic function with 1 method) julia&gt; @benchmark f3() BenchmarkTools.Trial: memory estimate: 496 bytes allocs estimate: 13 -------------- minimum time: 2.601 μs (0.00% GC) median time: 2.664 μs (0.00% GC) mean time: 2.822 μs (3.11% GC) maximum time: 451.356 μs (97.90% GC) -------------- samples: 10000 evals/sample: 9 julia&gt; f4() = for i in 1:ncol(df); df[i] = Vector{Float64}(df[i]); end f4 (generic function with 1 method) julia&gt; @benchmark f4() BenchmarkTools.Trial: memory estimate: 352 bytes allocs estimate: 8 -------------- minimum time: 1.241 μs (0.00% GC) median time: 1.286 μs (0.00% GC) mean time: 1.380 μs (2.80% GC) maximum time: 393.272 μs (98.20% GC) -------------- samples: 10000 evals/sample: 10 and, just because I was curious: julia&gt; f5() = for (n, c) in eachcol(df); df[n] = Float64[c...]; end f5 (generic function with 1 method) julia&gt; @benchmark f5() BenchmarkTools.Trial: memory estimate: 1.36 KiB allocs estimate: 33 -------------- minimum time: 5.174 μs (0.00% GC) median time: 5.354 μs (0.00% GC) mean time: 5.697 μs (4.12% GC) maximum time: 833.591 μs (97.77% GC) -------------- samples: 10000 evals/sample: 6 
The reason is not version of Julia but version of DataFrames. Formerly first argument *f* of *colwise* was typed as *Function*, but this restriction has been removed some time ago and now anything that is callable will go through.
If you're doing random forest though, check out ScikitLearn.jl. Some of the models in there are pure julia while others use the python scikitlearn library. SALSA is very good for other 'non-deep' learning like SVMs. Knet and flux are deep learning based, cassette is something else... i'm a little confused about that one honestly. 
A little bit of Julia used to make these. Have a good holiday and get ready for Julia 1.0 next year! :)
@cormullion I like everything that you do.
Interesting read but tons of typos really bring down the value. 
not sure if this is a recommended design but works (I assume that your *SimpleAngle* was not a parametric abstract type - this is what I do below): import Base.+ abstract type AbstractAngle end struct Degree{T&lt;:Real} &lt;: AbstractAngle angle::T end struct Radian{T&lt;:Real} &lt;: AbstractAngle angle::T end d1 = Degree(1) d2 = Degree(1.0) r = Radian(1) +(a1::AbstractAngle, a2::AbstractAngle) = promote_type(typeof(a1), typeof(a2))(a1.angle + a2.angle) 
Ah, that makes sense. Thank you!
My guess is that you have to do: +(a1::T, a2::K) where {T&lt;:SimpleAngle, K&lt;:SimpleAngle} = ... Your first definition expect both argument to be of the same type, and I'm not sure the second one is correct or not (seems like its not from the error).
Followup, if I wanted to define scalar multiplication using Julia's normal promotion rules how would this work? I can't use `promote_type
It's just a Github repo so feel free to make a PR!
Crosspost: https://stackoverflow.com/questions/47912691/computing-the-logdeterminant-of-a-sparse-matrix-in-julia
You can write `typeof(d1).name.wrapper(12.0)` to get what you want, but this is messing with the internals. I do not know a clean solution.
Maybe I'm missing something, but I have no idea how that answers my question. 
Because it was a mistake :)
Not sure if that's any better but you could try: [G[k,p] for k in find(group_row.==i), p in find(group_col.==j)] Otherwise just some plain for loops are usually optimal (you can write a function for it if you use that often).
For question 2, see: https://stackoverflow.com/questions/35229604/how-to-change-the-default-browser-used-by-the-ipython-jupyter-notebook-in-linux
Thank you - that looks like it should do the trick. I'll check it out and get back with any queries...
Maybe you could use array views using the `view` function or the `@view` macro.
1) Slicing by numbers reduces dimension. Just use a range if you don't want that: `A[1:1,:]`. 2) See @caks answer 3 and 4) Use Plots.jl. Then the mutating plot function `plot!` updates the current plot.
Well it's specifically designed for scientific computing, which is pretty different that other languages that mighy suit different needs. 
For me it's two things: slow start-up and lack of smooth, pytorch-like or cuPy-like CUDA support. The most annoying for me is the slow start-up time required for compiling all the dependencies. This really slows down the work-flow when trying things out and making lots of changes. I tried doing all my homeworks in a computer vision class using Julia this year but I gave up because I was not able to prototype at my usual speed. And this is happening to me every time I want to pick up Julia. The last time I checked CUDA support, about one year ago, it was nowhere near as nice as in established deep-learning frameworks. I don't know whatever happened in the meanwhile. I love the type system, the programming paradigm Julia proposes and I find the performance good enough. Smooth CUDA/GPU support will come but if the start-up time isn't fixed somehow, or if I can't find a way to adjust my workflow so that I have the same speed as in python, it remains a deal-breaker.
The Revise.jl module should help with the fast prototyping. But yeah slow startup times are a pain. Another thing that is slow is Pkg.add or Pkg.update in windows.
Good point. I should've been more specific, will edit above
Revise.jl looks very interesting. I've got used to using `reload("Package")` in place of `using Package` for packages I'm developing, and had wondered if there was a smarter way..
For me is definitely calling some modules, for example the command: using Plots, takes a long time, after that this is all fine, but this is a little thing I dislike. Perhaps there is an options, I am pretty sure there is, to precompile my custom Julia version where Plots or these packages that take a long time to compile are already precompiled or something. Thus when is start up julia and call this package I dont need to wait. 
Yes, just add them to your sysimage.jl and rebuild the system image. 
This a hundred times over!
Julia as a language is a great concept. Essentially it's a type system which is simplified in a way that is easy to understand for both humans and compilers. The second part is what makes it fast: the compiler is able to deduce the concrete types in your algorithm and essentially give it as much information as a C++ code, and then apply all of the optimizations. Lovely right? But there's a few tradeoffs made to do this. 1. The type system has a limit. It doesn't do things like arrow types in Haskell because the result of these type computations is not necessarily decidable. 2. The types have less dynamism than a Python or Ruby object. In those languages, you can add fields at any time. In Julia, the number (and types) of fields are set so that way the compiler can determine the byte sizes and efficient memory setups. The upside is that `Vector{Float64}` is a vector of Julia-defined types and its fast (instead of having to implement the vector type in Fortran which is what NumPy has to do for its arrays). The downside is that you can't just add fields onto types at anytime... but I think that's an upside because IMO it's less confusing as to "what a type is". YMMV. But Python objects are really just dictionaries... so you can do the same in Julia. 3. Code has to compile before use. It doesn't this automatically (JIT), but this means when the JIT part is used there a compile time lag. You might think, wow that list is rather short for something that gets such a performance advantage, how can that be? The real reason is because Julia isn't just another language added to the list, but it's actually a big technical advance. While it presents itself syntactically similarly to R/MATLAB/Python/etc., it's not implemented the same at all. Its internally all about its types and type computations, and this novel advance is what lets it overcome many of the previous interpreted languages. Given how you're supposed to use types in Julia, (1) and (2) aren't really issues (just differences from other languages). The real issue is (3): how do you make a language which is actually JIT compiling all of the way down (the basically all of Julia's Base is written in Julia too, and it can inline into your functions for extra performance!) act like an interpreted language? Things work quite well right now, but there are a few hiccups. For small scripts, it's perfectly fine. Where it comes into play is packages. For example, Plots.jl and DifferentialEquations.jl are huge packages, so compiling all of the code can take some time (it would take a huge amount of time if it was a statically compiled language!). The way Julia handles this is through precompilation: a package can enable itself to precompile a bunch of functions when `using` is first done. However, right now that currently doesn't go far enough. It doesn't really precompile the full functions, and it doesn't really know all of the function signatures users are likely to call. This is an issue because Julia functions are usually written generically, so they can be called with a large set of possible types. But every set of types a function is called on will get its own compilation specialization (called a method)... so how do you know what methods to compile? Tools like [SnoopCompile.jl](https://github.com/timholy/SnoopCompile.jl) are being tested to fix this, and in conjunction with [static-julia](https://github.com/JuliaComputing/static-julia) this can actually be used to statically compile a library just like you would in C or Fortran. However, these tools are at its infancy (Makie.jl successfully built a full binary the other month though), so packages like Plots.jl cannot make use of them yet. This means that Plots.jl has a "first-time-to-plot" issue with the first plot having to compile most of the internals and taking on the order of 30 seconds. Terrible? No, but that needs to get fixed. That's about as bad as it gets though. Until there's a true fix to precompilation, packages will have a startup time. This startup time is painful if you want to write command line tools (so Julia isn't a good option there right now), but with a future of emitting compiled binaries it will be far better than interpreted languages once that's done. If I had to list a few other things to keep in mind, here's the extended list. Note that Julia is a garbage controlled language. Sometimes for realtime applications like game engines and live audio processing a glitch in time can be fatal. Julia's GC is efficient by clumping together GC tasks and running them at once (this can be more efficient than even manually managing allocations, so that's great!). In some cases you can measure this taking like 0.1 seconds. This would cause frame drops in this kind of application, so it's probably not recommended there. But that's not scientific computing related at all, where you really just want the fastest code and don't necessarily care how the GC gets you there. So, does Julia get to 1x with C/Fortran? Almost. Really it's just missing the ability to specify no-alias scopes. [There's some work on this idea](https://github.com/JuliaLang/julia/issues/19658). Generally, with the large set of tests people have done over the last few years in things like PDE solvers (I myself have done quite a bit), you can easily get within 2x of C/Fortran/C++ just by doing idiomatic Julia without even trying. Then if you try hard you can get to 1x, with that last little bit being about making sure that the compiler is able to realize you aren't aliasing arrays and thus adding the last few optimizations. So we do need that macro to get there. Oh, and it would be nice to have something like C++'s volatile. On Julia's master (what will be 1.0), things like stack-allocation of views to mutables, interprocedural optimization (i.e. pushing constants down through multiple functions at compile time), etc. have already been tackled and merged, so we really do have a set of optimizations which takes advantage of the things which traditionally only are in statically compiled languages. But, there's this aliasing thing missing right now. Oh, and one of the main shortcomings is that the current package manager is rough to say the best. But Julia 1.0 is shipping with a new package manager ([Pkg3](https://github.com/JuliaLang/Pkg3.jl)) which should fix those issues. But to summarize, what Julia doesn't do well is be old and established. What Julia does well is that it is a complete redesign of what the internals of a language should be, and its built in a way that writing the standard library within itself is efficient and easy to do. So while there are some things that need work, they are all able to be tackled. The only hard exceptions that I can think of on the top of my head are the 3 points at the top. Though actually (3) is an implementation thing because someone can (and someone has) written an interpreter for Julia... so it doesn't actually have to compile...
In addition to loading times and workflow problems the way binaries are handled is sometimes a bit annoying. For example on OS X Homebrew updates each time you build a package than depends on it, and that takes a long time and will sometimes breaks packages, so you have to remove and rebuild the packages... it seems that other languages deal with that kind of issues more gracefully. 
Nowadays there's Flux.jl and Knet.jl, both have CUDA arrays + autodiff and what not. Clearly not as mature as e.g. PyTorch though... FYI: I'm not a Julia user just yet, just lurking around so far and reading posts like this :)
I wish there was a sparse version of shared array for large datasets split across many nodes. It also seems like this would help reduce transmission times. A couple days ago, I would have said the lack of a linked list type of sparse array was something that I wish we had. But now I think it's just as easy to use coo notation and build the csc matrix from that. I wasn't aware you could do that until I read through the docs again.
Check out CUDANative. I think it might be more what you're looking for. Nvidia developed it so it should be pretty decent for working with cuda
1) Very clever use of slices! 3) I should also add that [animations with PyPlot](https://genkuroki.github.io/documents/Jupyter/20170624%20Examples%20of%20animations%20in%20Julia%20by%20PyPlot%20and%20matplotlib.animation.html) may be useful
I tried using cudanative.jl on both x86 and arm machines, and it didn't compile properly/pass tests on either with 0.6. I would wait until 0.7 comes out before using it. It is very cool though.
Oh okay. Sorry. Yeah I haven't actually used cudanative but I thought i'd recommend it in case you hadn't seen it
It has the same problems most GC languages do with multithreading. Garbage collection stops the world.
Reel.jl also looks interesting
I don't use Julia all that often (I am a statistician, and am usually either using R with C++, or Python), but the main reason I don't use Julia all that often is that I usually have very little intuition about what code is going to run fast and what code is going to run slow. Especially when things are somewhat complicated, I would often run into these slowdowns in Julia for reasons that I did not understand. One example *which I believe has been fixed* is that, in Julia, you used to run into some serious slowdowns when you either fed functions as arguments to other functions, or defined functions within the body of other functions; I'm somewhat fuzzy on the situation required for this to happen. This was years ago, but I remember that instead of passing functions as arguments, what I was told to do was to create a dummy type that essentially indicated which function to call (essentially taking advantage of multiple dispatch). This worked, but it freaked me out enough that I have forever been scared of using Julia. 
Most GC languages don't have that problem. For example, the JVM (Java, Clojure, Scala) and .NET (C#, F#) have mostly-concurrent garbage collectors that only stop the world to collate global roots. 
You have to build Julia from source for those to work on v0.6. That's changed/fixed in v0.7 though.
&gt;I wish there was a sparse version of shared array for large datasets split across many nodes. It also seems like this would help reduce transmission times. Sounds like a great idea for a package!
&gt; One example which I believe has been fixed is that, in Julia, you used to run into some serious slowdowns when you either fed functions as arguments to other functions, or defined functions within the body of other functions; I'm somewhat fuzzy on the situation required for this to happen. This was years ago, but I remember that instead of passing functions as arguments, what I was told to do was to create dummy types that essentially indicated which function to call (essentially taking advantage of multiple dispatch) This stuff is fixed though. The problem was that anonymous functions are slow, now anonymous functions are generic and compile just like any other function. I would say one of Julia's best features is how easy it is to know the performance of a program before running it! The only thing that gets in the way is "the closure bug" which can be hard to predict if you're using a lot of very involved closures (but `@code_warntype` will tell you that you have a problem). But that's just a bug which hopefully gets fixed soon. Other than that, I'm not sure where you'd not be able to easily reason about performance. Do you have any more recent examples? Ref: https://github.com/JuliaLang/julia/issues/15276 
If you're interested in doing a smiles library I'd be happy to help. I would caution you that if you're thinking of doing predictive ml on chemical graphs, there's about 150 reasons why that's a terrible idea. There are a few domain specific problems in chemical informatics that I think will yield to ml:. 1) highly computationally efficient protein folders. 2) CYP450 activity predictors 3) a better lipinski rule. If you're curious drop me a pm.
Look at `Base.Iterators.cycle`.
That's what I'm using. I'm not a big fan.
I'm not sure if this is exactly what you're looking for, but if I were writing that, I'd probably refactor it to a for loop: for active_player in cycle(1:n_players) # Do game logic... if winner break end end
Does Julia really need a director of diversity? Does the compilation cache outlive a running Julia process yet?
Did you take a look at the Plots.jl documentation page?
Possibly? Do you have a link?
I prefer PlotlyJS.jl over Plots.jl. It's a thin wrapper to (Plotly)[https://plot.ly/javascript/] so you can rely on their documentation plus a few syntax tips. Getting to a simple plot is slightly more verbose than in plots, but I find it way easier to customize. 
Here you go http://docs.juliaplots.org/latest/
Is it not kosher to question why a tiny organization needs a director of diversity? What does that person do exactly?
The assumption is that you would be critical of any director of any position including the word “diversity”, anywhere. As evidence: you don’t seem to acknowledge that they are in charge of outreach, and do concrete things like posting this video. 
That seems like a convenient assumption. I would think if someone had a simple explanation then that would be the reply. It seems like pandering in the best case scenario, which is disappointing.
Not very helpful, but I'd like to get Vega working on the latest version of Julia. It has those sexy stacked area plots. https://vega.github.io/vega/
If you go to JuliaCon and look at the room the question answers itself. Also, Jane is a very technically skilled scientist and works on other projects as well.
Does Julia have anything like Python's virtual environments? It'd be nice to have multiple sysimage files to choose from for different situations. ^(aside from just dragging and dropping different versions out when you want them, if that works)
This. It's really funny with all these diversity directors etc.
Thanks so much, I am definitely going to do this.
Yes I did that for both architectures and it still had issues.
Good to know. They promise that in the next Julia release it should be easy, so I'll have to wait and see. Right now, I agree it's not easy to do.
&gt;Does Julia really need a director of diversity? Need? Probably not. This person is also apparently responsible for outreach, which I'd argue it probably does need to grow the user base. If someone is involved in outreach, I think it makes sense to think about diversity from the beginning. One of the things that has appealed to me about the julia community is the fact that the people at the top have been taking community standards and questions of inclusion seriously from the get go. These things matter to me (though I'm a straight white man). &gt;Does the compilation cache outlive a running Julia process yet? I'm unclear what this has to do with anything. The thing I'm most concerned with in julia is not this. But I don't think that the fact that my pet issue has not been addressed is a reason to put off tackling all of the other problems. It makes sense for the organization to multitask. I'm not saying I think you disagree with that last statement - I take this statement of yours to mean that you think the compilation cache is a higher priority than diversity, and that you're concerned that the org spending money on someone responsible for diversity and outreach means that this problem will be solved less quickly. Which may be true, but it's worth acknowledging that your priorities may not be ordered the same way as others.
Why present a technically skilled scientist as a director of diversity? Don't you think that cheapens their contributions? &gt; If you go to JuliaCon and look at the room the question answers itself. I would rather judge a community on what they are able to accomplish and contribute, not on what labels I can assign to them by glancing at their appearance.
&gt;Don't you think that cheapens their contributions? Why would extra community building responsibilities be cheapening their responsibilities? It's only cheapening if you don't see value in diversity initiatives. She gets a title while lots of others don't have one on the site, I'd call it a win! &gt;I would rather judge a community on what they are able to accomplish and contribute, not on what labels I can assign to them by glancing at their appearance. Non straight white males can be smart and educated, yet they are vastly underrepresented in open source communities. We can accomplish more if we find out why we only appeal to a small and homogeneous segment of the educated population, and reach out to bring others in. There's a vast amount of opportunities we are missing because of the lack of inclusion, and I hope these initiatives can make other groups more confident in joining and taking a role in the community. By doing so, it can only help us achieve and accomplish more.
I'm not sure I 100% understand what you mean with `group_row` and `group_col` - is it that each row and column has a single `group`? So something like: G = rand(10, 10) group_col = [x % 2 + 1 for x in 1:10] # 2,1,2,1... group_row = [x % 3 + 1 for x in 1:10] # 2,3,1,2,3... and you want all of the matrix where the row is 1 and the column is 2? If you're doing this a lot, sometimes with the same queries, it might be worth storing in a `Dict` or something: col_dict = Dict(i =&gt; find(x-&gt; x .== i, group_col) for i in unique(group_col)) row_dict = Dict(i =&gt; find(x-&gt; x .== i, group_row) for i in unique(group_row)) Then, when you want row group 1 and column group 2, you do: subG = G[row_dict[1], col_dict[2]] You still have to spend time on the operation to build the indexes, but them you've got them and don't have to re-check which rows `==` your groups every time. And if you're storing them in this form rather than the array form from the start, you probably won't lose any time?
Apparently, these small time people at Sloan though it was a good idea to support diversity and spreading the good word about Julia in general. Given the size of the grant I think JC have resources to do other things as well, so don't worry. You may contribute a lot to the community yourself, but different people find different things important and interesting. https://juliacomputing.com/press/2017/06/26/sloan-grant.html
thanks for the great overview as always :) you have a nice understanding of current julia state :) can you share your thoughts on how julia is doing in highly parallel distributed tasks, like solving Navie Stoks equation, where Fortran/C+MPI seems the best? something which can run on from 1000 cores and more? I know there were some attempts [link](https://discourse.julialang.org/t/implementing-parallel-fluids-code-in-julia-parallel-fftw-mpi-jl/2689) but is there success stories?
Well, [Celeste.jl](https://www.youtube.com/watch?v=uecdcADM3hY) is a major success story since it's a fully Julia application that was able to reach petascale. It's not the same thing (it's a Bayesian image analysis) but it's a proof of concept that all of the tools are there. Setting up a good Navier Stokes solver is Julia is mostly about setting up iterative linear systems that utilize PETSc and MPI under the hood (through PETSc.jl and MPI.jl). In that sense, Julia is what I think is the right abstraction level for that. It's not really the lowest level since solving differential equations requires piecing together the right differentiation (autodiff), FFT, linear solver, and nonlinear solver tooling. I have shown that Julia can do this well. Basically all you need is no-cost abstractions. Even writing iterative linear solvers does well here. It's the level lower that can need extra tricks. Building the actual FFT, BLAS, or LAPACK (i.e. matrix decomposition) is where I think Julia won't do as well. These need extra cache handling, manual SIMD, some assembly and bit twiddling, allocation handling. There are actually some successes here (Yingbo is doing really well!), but it has to do a lot of nasty generated functions and LLVM calls. So if the algorithm can assume that matrix multiplications and FFTs exist (i.e. diffeq solvers), then Julia does well. If you have to write optimized matrix multipliers and FFTs (i.e. building a parallel linear algebra library like PETSc) then I'm not sure Julia is the right tool, but Julia is the right tool to use that library from. And specifically for PDEs, Julia's no-cost abstraction compile-time type architecture can be used to make modular PDE algorithms that piece together. That's what I am mentioning in the link, and I have some proofs of concept. But I am sure I cannot convince people without doing it in full, so I hope to get a paper out that explains the architecture and why it will work soon, along with getting an actual solver built using it. That keeps getting sidelined because other work with SDEs keeps taking my focus though.
[UnicodePlots.jl](https://github.com/Evizero/UnicodePlots.jl) is pretty slick too.
Besides diving further into the language, I’ll be playing with some awesome libraries like Flux, Genie, and Gadfly :) BTW, If anyone has an example of how to use the Conv2D layer in Flux can you please share? 
Something that hits the GPU
Calling python code from Julia is easy. Here's a module I wrote that maps calls to the Python xlswriter https://github.com/lawless-m/XlsxWriter.jl?files=1
More of the same really - planning to keep expanding out my microbiome package... Actually, hopefully get some other people to use it. My lab in particular :-)
Being reasonable well-versed in R I find that Julia has slow CSV reads (actually everything seems slow compared to R's data.table). Plotting is underdeveloped. DataFrames is slow compared to data.table. What stopped me from getting serious with Julia for a long time was how slow reading CSVs seem. The first thing I tried with Julia is to read a 2G CSV, it's meant to be fast right? Then I waited 20 minutes and nothing then I stopped. What bought me to Julia recently was that R's mongolite broke! And interfacing R with Go's mgo wasn't that easy. But Julia is a good glue. I also liked Julia's regex much better than R's. But for serious workloads involving large datasets and plotting I still go back to R.
Faster data manipulations foundation including group-by!
Translating into Julia all the Pascal code I have written for a personal project concerning the optimization of timer-controlled street traffic lights. My purpose is to be more "modern" but, oddly, apart from dynamic arrays, it is not that obvious what else I will gain... I am hoping the finished code will be significantly faster. 
calling Julia from Python is currently broken see here: https://github.com/JuliaPy/pyjulia/issues/139
Calling Python from Julia is great from PyCall.jl. Calling Julia from Python exists in PyJulia, but it's not even close to as robust or stable as PyCall.
Now that sounds great!
Affirmative action / aiming for equal representation (not equal opportunity), is road to hell. The only thing needed is stating that everyone is welcome (and then being welcoming to everyone). Job done. Anything more is not only waste of resources, but actually disadvantaging people who have done nothing wrong (in the name of insane ideology). Same goes for: * catering to the whims of easilly offended * censoring speech (free &amp; open speech and criticism are THE mechanisms that help make society better) * and the rest of the "progressive" toolkit. Julia community shown a few signs of SJW infection in the past, I sure hope it won't become as totalitarian as Rust's one. If we forbid everything that might offend someone, we'll barely by able to say anything. Certainly not anything interesting ! It is sad to see the world go up in flames of planned victimhood, oppression olympics, virtue signalling &amp; witch hunts. To see rabbid PC &amp; SJW dogs running through the streets, companies &amp; communities, searching for someone to blame and then devour. To see people auto-censoring themselves because of fear or false guilt (they let those parasites force upon them). I wish everyone adopted [NCoC](https://github.com/domgetter/NCoC), the spirit in which it was written, and hammered into their heads that tolerance does not demand, that it's not a blunt object which you can use to force others to act the way you want. Maybe someday.
My 2 cents: 1. Organize users. Form teams, get together, start a slack channel or discord post, develop (PR, raise issues, improve docs, add tests) packages for linking data corpuses, analysis, ML, statistics and visualization. Make the onramp easier for begginers! i. Add exercises to the julia exercism https://github.com/exercism/julia ii. Help improve tutorials like the julia wikibook or answer SO questions iii. Give workshops, build cool stuff and share it wide in social media! Concrete case: make a pull request to the org's repo to update the Julia preset configs. https://github.com/Kaggle/docker-julia 2. Organize sites Message them to remember them to update their Julia versions. I was able to get Hackerrank and some other site to up the version to 0.6, but it will probably be easier to sell this once 1.0 realeses and stabilizes. 3. Organize contests/hackathons Get people hacking. Organize a weekend and invite people from the area to learn about a particular cool application or problem. Make sure to listen and cater to specific needs. (Do they want to model equations? DifferentialEquations.jl Visualize Data? Plots.jl and Makie.jl and Luxor.jl Try some deep learning? Flux.jl and Tensorflow.jl Make web services? Genie.jl Analyze genomes? BioJulia.jl Work on graphs? LightGraphs.jl Crunch stats? Statistics.jl Obviously, the above and beyond would be to get sponsors and a venue for a cool Julia hackathon/contest, but that is quite a workload. That way, you could get a particular platform to host the challenge in Julia and that could help. Just my two cents. Feel free to make suggestions otherwise! 
The syntax is a little old and the style is not quite right anymore. But you might like this: http://www.johnmyleswhite.com/notebook/2013/01/07/symbolic-differentiation-in-julia/
This might get easier as Julia hits 1.0 so that its a stable release and shows Julia is here to stick around.
If you make Points an immutable `Struct`, then they are value-types and thus operations on them will be performant. Just define an `isless(x::Point,y::Point)` function and then the standard `sort!` method will work.
You may want to join the Slack chatroom and see if anyone can help you out there. Chats can get quicker feedback which can help with something like this.
Will it use SIMD instructions? What is the library function to do very fast parallel sorts for smallish vectors? What's the sorting lib function for parallel sorts? Radix sort? Odd-Even-Merge sort?
You can check out [Julia Observer](https://juliaobserver.com) to see a list of trending packages, or look through [GitHub sorted by most stars](https://github.com/search?l=Julia&amp;o=desc&amp;q=Julia&amp;s=stars&amp;type=Repositories&amp;utf8=✓). A cool thing that came out recently is a [better way to compile binaries](https://medium.com/@sdanisch/compiling-julia-binaries-ddd6d4e0caf4). And since you're interested in differential equations, I'd **highly** recommend the blog the dev runs, [Stochastic Lifestyle](www.stochasticlifestyle.com). He has a lot on there about the package, as well as some more general posts. I'd also say go though this subreddit sorted by top of all time, should find some cool stuff that way. And the [Julia Discourse](https://discourse.julialang.org) is quite active too. Apart from that, [here](https://juliabyexample.helpmanual.io) are some basic examples of using Julia. 
I'm not sure if this problem can use SIMD at all? But if operations are in chunks on the vector, then using -O3 compilation will allow LLVM to do a lot of SIMD automatically. I'm not sure about different sorting libraries other than SortingAlgorithms.jl and what's in Base, but finding other libraries is just a Google away.
That's a great idea, here's a simple implementation of that: using BenchmarkTools # Immutable Point struct struct Point px::AbstractFloat py::AbstractFloat end # Define method with Point arguments for Base.isless Base.isless(x::Point, y::Point) = x.py == y.py ? x.px &lt; y.px : x.py &lt; y.py # Generate a matrix of points point_vec = Point.(rand(50, 5000), rand(50, 5000)) # Test sorting 1 row at a time @benchmark begin for i=1:50 sort!(point_vec[i, :]) end end # Alternatively: generate a vector of Point vectors point_vec2 = [Point.(rand(5000), rand(5000)) for i=1:50] # Benchmark using broadcasting @benchmark sort!.(point_vec2) I don't know what to compare this to but it seems to run pretty fast. Sorting the whole matrix took an average of ~154ms on my machine for 1 vector at a time, and about 58ms using broadcasting. Also I'm not sure about the whole parallelization part, but I'm sure that row by row method could be broken into chunks...
Perfect! Here is my comparison code in Haskell. https://gist.github.com/stephano33/ff5b63dc65bcbc2224c6ee42958b8198 Would be really nice to make the code comparable. Would it be possible to also test sorting one vector of 7000 points? In Haskell, it takes about 200 Microseconds (μs) to sort a single vector of 7000 points. But I should run the same code on the same machine.
Distributions.jl is one of the nicest package, https://github.com/JuliaStats/Distributions.jl you can check the sources for educational purpose. Otherwise some big ones are Images.jl for image processing, Bio.jl for biology, Gtk.jl for native GUIs, Genie.jl for web dev, etc.
Don't put abstract types in structs. That will slow your code considerably because values will not infer. Instead, use a type parameter here. Additionally, do not benchmark in the global scope. Again, inference issues. Julia's "unit of compilation" is the function, if things aren't in a function it'll compile each function call it hits (it's optimized in functions, global scope is a convenience layer). Additionally, columns are what are contiguous in Julia, so you want to sort along columns. Lastly, you want to avoid allocations when possible, so instead of slicing the columns you want to use views. The resulting code is: using BenchmarkTools # Immutable Point struct struct Point{T} px::T py::T end # Define method with Point arguments for Base.isless Base.isless(x::Point, y::Point) = x.py == y.py ? x.px &lt; y.px : x.py &lt; y.py # Generate a matrix of points point_vec = Point.(rand(5000, 50), rand(5000, 50)) function sort_columns!(point_vec) for i=1:50 sort!(@view(point_vec[:, i])) end end @benchmark sort_columns!(point_vec) # Alternatively: generate a vector of Point vectors point_vec2 = [Point.(rand(5000), rand(5000)) for i=1:50] # Benchmark using broadcasting @benchmark sort!.(point_vec2) If you want to learn more about these points, please check out [this blog post](http://www.stochasticlifestyle.com/7-julia-gotchas-handle/) and [the Julia manual's performance tips](https://docs.julialang.org/en/latest/manual/performance-tips/). The blog post gives a lot more motivation about why these are the case though. This is pretty much every "optimization rule" in one example though, so if you remember those you're good.
Check out my optimized version for benchmarking. This code is very far along as well so it should be very clear how to make it match your Haskell code, and I included some discussion of how it was optimized.
That's crazy, this runs about 18x faster now. Thanks for sharing this, looks like I've got some reading to do!
What gave you the idea for this, and what do you think c++ has to do with hacking cs:go? For the record I have no idea how hack any game with any language, I just want to know what you're thinking.
&gt; Julia is here to stick around That could easily stop being the case if, for example, Julia Computing runs out of funding and most core developers are no longer paid to work on it.
I'm thinking about writing a Julia interop library for elixir. Beam languages don't really have array support, it would be really nice to get them to transparently dispatch data to Julia and manage and supervise pools of Julia worker threads.
So c++ and c sharp are the main languages used to program csgo hacks but once in a while some people manage to do it in something like python. However, since most c libraries can be used on Julia (mostly memory reading libraries) why can't Julia be used to make csgo hacks
If it can be done in Python (or c++) it can be done in Julia. Weather or not it makes sense to is another question entirely, I'm assuming you've given that some thought to that. Disclaimer: I don't know the ethical or legal ramifications of hacking cs:go, and I don't want to encourage you to do anything wrong or illegal but Python source code can be rewritten in Julia fairly easily, why not start there? 
There is a package GeometricalPredicates.jl that has some similar structs and functionality. That may be worth looking into and using or contributing to. 
I feel like the title is somewhat misleading. It seems to imply that Julia is becoming more popular, but the article doesn't mention that at all. It just describes the pros and cons relative to Python. It's reasonable balanced in that respect, but I feel like it downplays Python libraries like NumPy which make Python nearly as fast as Julia.
Is there a way to "force predispatch" a function so that it compiles ahead of call? Besides passing some dummy variables of course, which is not good for some cases with external state involved.
You probably shouldn't be multithreading in Julia. It's designed for computation; for this domain the primary tasks should be ok to be parallelized using worker processes; if you need massive concurrency for million user connections, you should be using erlang or elixir.
What exactly do you mean? Julia functions always compile before the call. If you mean, is there a way to force the dispatch to compile the native code in the package .ji, the answer is no.
Just fixed Julia Observer. I guess it was broken for a little while :'(
Yeah, the guy who wrote this is a Journalist. His bio doesn't mention any technical experience. It's probably safe to assume he has more knowledge than a layman, but these articles are likely not for a technical audience.
Strange... julia&gt; @benchmark sort!.(point_vec2) BenchmarkTools.Trial: memory estimate: 1.92 MiB allocs estimate: 125 -------------- minimum time: 7.541 ms (0.00% GC) median time: 8.999 ms (0.00% GC) mean time: 9.219 ms (2.05% GC) maximum time: 13.877 ms (25.17% GC) -------------- samples: 541 evals/sample: 1 julia&gt; @benchmark sort_columns!(point_vec) BenchmarkTools.Trial: memory estimate: 1.92 MiB allocs estimate: 150 -------------- minimum time: 8.427 ms (0.00% GC) median time: 9.741 ms (0.00% GC) mean time: 9.943 ms (1.81% GC) maximum time: 14.146 ms (36.02% GC) -------------- samples: 503 evals/sample: 1 
I'm just reading your blog post and there's one point that seems like a huge blunder to me. x = rand(2,2) a = [cos(2*pi.*x[:,1]).*cos(2*pi.*x[:,2])./(4*pi) -sin(2.*x[:,1]).*sin(2.*x[:,2])./(4)] b = [cos(2*pi.*x[:,1]).*cos(2*pi.*x[:,2])./(4*pi) - sin(2.*x[:,1]).*sin(2.*x[:,2])./(4)]
It's a good example of parsing gone wrong, but I think it's abusing the matrix-building shorthand to do something it was never meant to do. You can find this kind of example in pretty much anything with such a short-hand though. 
So, that sort of thing shouldn't come up often in idiomatic code? Because I can see that souring my experience with the language.
without testing myself how does it deal with "Kαλημερα κοσμε" (Hello world) ? 
You should never ever write that. Basically that shorthand was made for A = [1 0 0 2 1 3 2 1 1] but you can abuse it to put entire expressions and function calls in there. If you're building a giant matrix, you should instead do something like x1 = cos(2*pi.*x[:,1]).*cos(2*pi.*x[:,2])./(4*pi) x2 = -sin(2.*x[:,1]).*sin(2.*x[:,2])./(4) a = [x1 x2] or a = hcat(x1,x2) which is much more legible anyways. Or create the array and mutate the values. Basically, if I see that in code then I would say it should be changed. I can show examples of this kind of thing in any language though. I mean, if you're not a fan of these kinds of potential issues, stay far far away from Python and R!
I've been tinkering with Julia a little bit, and I think it has huge potential, but it's changing fast at the moment, so many of the available resources are out of date. Version 1.0 should be out fairly soon, and I'm waiting for that before I try to learn it in a more structured way. 
R has something similar x = 2 x &lt;- 3 print(x) x &lt; -3 since `&lt;-` is the same as `=` (so the first changes x and the second is a boolean). Any Python is the champion of whitespace issues, where even invisible things like [accidentally having a tab](https://stackoverflow.com/questions/14604751/whitespace-problems-with-python) causes error. There's no such thing as a language without syntax issues. Being able to pull some off the top of your head is more about your familiarity with the language than it is about the language itself.
Ha, I really don't know what the answer should be :) I should have suggested something where the initial letters were not similar shapes! It certainly doesn't look quite right e.g. spacecase should have the same kappa in each word k κ but I don't know if the PDF has the right capital kappa in the first place, it looks like a regular K I copy pasted it from the original UTF-8 paper: https://www.cl.cam.ac.uk/~mgk25/ucs/UTF-8-Plan9-paper.pdf 
The best thing to do is read the documentation. Its fairly interesting in it's own right since the documentation is good, and covers most of the use of the language's type system, primitives, and built in functions. You don't need to master all of the documentation, but it's worth reading over at least once to familiarize yourself with what's available. Just don't read it in order. Start with the introduction, then read the section on functions and then the type system. Branch out from there.
spacecase could have probably been a hidden method. // it's used by the other case methods under the hood
If Julia does a bunch of compilation work before dispatching, does that also mean you can manipulate (structured) Julia code, in Julia, at runtime?
You cannot. Well, let me make that more clear. You cannot manipulate the Julia function itself at runtime like you could in something like Ruby or Python. The function it's calling is always statically compiled. You can make Julia functions and execute them at runtime via `@eval` though. Note that things like anonymous functions are implemented by finding all internal function definitions and then compiling them at dispatch time, so anonymous functions actually boil down to being generic compiled functions as well. This was added in v0.5 since the old MATLAB-y way of them actually just being a bundle of expressions to execute was really slow and nobody liked having slow anonymous functions in a language which has a lot of functional programming support (but of course you can use metaprogramming plus `eval` to re-add this stuff yourself if you want).
But you could easily run into something like this: [4-1] [4 -1] I don't see this a major concern, though.
Good find! Yes, ifelse is definitely the right tool there.
There needs to be a bot that automatically posts this whenever a post doesn't have any responses after a certain time
Maybe start by contributing to already existing packages. This helped me a lot in learning how everything works. Apart from that, the [online help](https://docs.julialang.org/en/stable/stdlib/pkg/#Package-Manager-Functions-1) is actually quite good.
&gt;I have some background and experience with R Same here, and I'm just starting on Julia as well. Read [this](http://www.stat.wisc.edu/~bates/JuliaForRProgrammers.pdf), by Douglas Bates, who you might have seen in notable R packages already.
Notice it might be slightly outdated on some points (I haven't checked), as it was presented at the eve of Julia 0.2 release.
This blogpost helped me enormously when I making my own packages. Thanks!
If you want fun &amp; worthwile project, try to write a bot for some 3d shooter. You'll have much more support &amp; available code. Before being able to "write hacks", you have to be able to reverse engineer machine code, be able to somehow inject your code into the running process, perhaps deal with anti-cheating solutions. Lots of intricate details you have to get exactly right. And the crackers are a secretive, ego-driven bunch. Good luck getting any of them to _spoonfeed_ you their precious knowledge/techniques, while teaching you the basics.
Ctrl f 'plots' Disappointed.
Why would you want to write a cheat for a video game.
Thank you so much for the reply! &gt;Computing the full inverse of a matrix isn't necessary to solve A \ b This makes a lot of sense actually. I will try to ask the TA if this is the reason he said that inv() is numerically unstable. Again, thanks for the explanation and for providing examples. 
Appreciate the reply. What you guys have said makes sense intuitively, I think I just have to spend some more time programming in order to understand the technical aspects of why this is the case.
This is a follow up question to: https://www.reddit.com/r/Julia/comments/7ogdvt/sorting_a_vectorarray_of_5000_points_in_julia/
Is this benchmark time realistic? Say, Julia is constantly running and "gets" values from the FFI, will this benchmark result be realistic? Of course I woudn't count JVM startup time or the serialization/deserialization or FFI overhead. I mean, perhaps is there some hidden JIT compilation that favors Julia? Or some overhead to get my data in the vector format? I am just a bit suspicious of the more than 5x speedup of Julia versus C++ code.
I'm referring to the fact that I can't get a plot to appear in Atom, not the state of plotting in Julia. And when I search the community forums they all seem to say "Plots.jl" doesn't work with Atom.
The condition number of a matrix is actually a deeper concept than numerical stability. _Even with infinite precision arithmetic, ill-conditioned matrices are still a problem._ Basically if you have an ill-conditioned matrix in your linear function, the output of the function can vary a lot with very minuscule variations in the input. Generally it's desirable that if your input data into a function is a little noisy, the output of the function is also only a little noisy. If the matrix is ill-conditioned, the function will drastically amplify noise regardless of how good your algorithm is or how much precision you compute your arithmetic in.
It does work with Atom. It's worked for over a year now. What backend are you using?
&gt; For example, if you have to solve A \ b for a single, fixed A but for a lot of different bs which you don't get to know all at once. Even in this case you'd just use a matrix factorization.
&gt; Say, Julia is constantly running and "gets" values from the FFI, will this benchmark result be realistic? Of course I woudn't count JVM startup time or the serialization/deserialization or FFI overhead. I mean, perhaps is there some hidden JIT compilation that favors Julia? Or some overhead to get my data in the vector format? Well yes. This timing is already compiled Julia vs already compiled C++ vs already compiled R vs already compiled Haskell. Julia does specialize on the types given, but here that just means it compiled a version for Float64 just like everyone else. Julia uses the same vector format as C++ (the structs are actually byte compatible) so there's no more overhead in the Julia case. In fact, in many ways Julia compiles to something very similar (or actually, almost exactly the same) as clang compiled C. Why the 5x speedup? I don't know. Did you try different compilation flags, optimization levels, different compilers, etc? Julia is just about 1x with good C code (unless you're in one of the edge cases where you need to make sure the compiler knows you aren't aliasing), but it can be surprisingly hard to just make the C code compile correctly. With Julia you have a good set of defaults just built into the compiler since it's made to be higher level, so for most people it probably ends up better this way.
&gt; Actually, a simple way to invert a matrix--which is actually a decent idea for small matrices--is to solve A \ I = A^-1 Interestingly, in Julia `A \ I`. and `A \ eye(n)` (with appropriately sized `n`) can give quite different results (other post: https://www.reddit.com/r/Julia/comments/7pptwp/why_is_inv_inefficient/dskhg9q/)
**Richard Hamming** Richard Wesley Hamming (February 11, 1915 – January 7, 1998) was an American mathematician whose work had many implications for computer engineering and telecommunications. His contributions include the Hamming code (which makes use of a Hamming matrix), the Hamming window, Hamming numbers, sphere-packing (or Hamming bound), and the Hamming distance. Born in Chicago, Hamming attended University of Chicago, University of Nebraska and the University of Illinois at Urbana-Champaign, where he wrote his doctoral thesis in mathematics under the supervision of Waldemar Trjitzinsky (1901-1973). In April 1945 he joined the Manhattan Project at the Los Alamos Laboratory, where he programmed the IBM calculating machines that computed the solution to equations provided by the project's physicists. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Julia/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I mean to generally force the compilation of a function without a call. 
How are you querying the database? - By the time it is in Float32 you have lost the precision. There is no way to convert it to Int at that point and get the original number. 
IEEE 754 specified float 32 to have 23 fraction bits, which translates to the max integer value 2^24 -1=16777215 without losing least significant bits. It's wrong to store ID with so many digits as float 32. Even int32 won't cut it if you have 13 digits.
Normally, I do data = readtable(executeQuery(stmt, sql_query)) This conveniently puts the result into a DataFrame. When I noticed the problem with the ID's, I tried running the more conventional code listed as an example on the GitHub page for JDBC: rs = executeQuery(stmt, sql_query) for r in JDBCRowIterator(rs) println(r) end This still returns the ID's as Float32. Is there a way to tell, say, readtable() to try and get the data using a specific type?
This explains why the value of the integer gets changed. Seems like I have to look for a solution that would avoid retrieving these values as Float32 to begin with.
executeQuery is returning the float, that is where you need to target. conn = DriverManager.getConnection("jdbc:derby:test/juliatest") stmt = createStatement(conn) rs = executeQuery(stmt, "select * from firsttable") for r in rs println(getInt(r, 1), getString(r,"NAME")) end 
This seems like something a database package should handle for you. Maybe file an issue?
Hi Chris, if you were able to provide that link, I would appreciate it :)
It's been going on for a few months in different channels, but this is probably the main thread: https://github.com/JuliaPlots/Makie.jl/issues/8 and I tried to summarize it a few days ago: https://github.com/JuliaPlots/Makie.jl/issues/8#issuecomment-354809617
Cheers.
Can't you get the SQL to return it as a string?
thank you for this 
That should work? Feel free to [join the Plots channel](https://gitter.im/tbreloff/Plots.jl) if you need help finding out what's up with your installation.
I think: export JULIA=[path of target julia binary] In the terminal should do the trick. The makefile is also calling this julia script that should deal with dependencies (I guess). JULIA_CONFIG := $(JULIA) -e 'include(joinpath(Sys.BINDIR, Base.DATAROOTDIR, "julia", "julia-config.jl"))' I don't think you need to compile Julia from source, julia.h is included in the binary installation, and the script above is adding the directory for you.
I found the solution on my own. See above. This should be added to the docs.
Well setting environment variables is somewhat basic (and not really related to Julia) and specific to that makefile, so that probably shouldn't be in Julia's doc. If anything it should be in the error message. $(error "Please pass JULIA=[path of target julia binary], or set as environment variable, e.g. `make JULIA=/home/james/Downloads/julia/julia`") If you feel like it you can make a pull request on the file directly from github, since it's a text file. https://github.com/JuliaLang/julia/blob/master/examples/embedding/Makefile
The environment variables were not the problem. It was unclear if I need to compile Julia or if I can use the binary distribution. For the latter, I pointed to the executable in /usr/bin. But then the julia.h file was not found. When I compiled and pointed to that executable, all worked.
So the error message is nice.
Muy bueno. Gracias. Los links de la parte de tensor flow no funcionan :(
This is all I know about in this area, you may have already found it though: https://github.com/JuliaStats/TimeSeries.jl
Yes, this is a start. This is quite good loooking, and perhaps impressive: https://juliohm.github.io/science/california-earthquake/ 
Of course, how could I not have thought of this! Casting the field as BigInt in the SQL query itself does in fact make it show up correctly in the Julia DataFrame. Thanks for the tip!
Are you using different processes? Why would there be an issue? Did CPython really had a problem with *completely independent processes* doing stuff at the same time? It's kind of hard to believe.
I want to call Julia from a parallel Haskell application. Thus, when I bound my Haskell light-weight thread to a single OS thread, it can call Julia via the FFI embedding of Julia. In this way, the other Haskell threads (same Haskell app) that are also bound to OS threads and that also call emedded Julia functions, are not interfered with? There are no slowdowns or errors due to the fact that n number of Julia embeddings do calculations in n different threads at the same time. 
Thanks! But what about Haskell OS threads? Are those processes? I'm on my phone right now.
So you are not using different processes but OS threads. I don't think there's a problem executing Julia code from multiple threads, you just need to make sure to use locks where appropriate since Julia is not automatically thread-safe like some interpreted languages (like Python) are.
I believe much of the tutorial material will be the same as that presented [here](https://www.youtube.com/watch?v=4igzy3bGVkQ). We do not plan to make a video recording of the event.
julia uses a different algorithm for small integer ranges. So that could be why
Arreglado
Yeah, I've never had much luck defining custom versions of the various I/O types. It seems like the interface is not well-specified and there are all sorts of assumptions not made explicit in the documentation. Let me know if you find anything, ok?
I'll be interested to see how it changes for v0.7...
Ah I can bust out the old saying Use the source, Luke! https://github.com/JuliaLang/julia/tree/master/base
Let me start off by saying I have no idea what you're asking. But let me ask some clarifying questions. &gt;`@test typeof(pop4bn(sol)) == Int64` This function uses `pop4bn(sol)` uses the global `year` which you never defined here, but I assume it's an array with values between 1905-2017? If so, your solution is only defined on `(2017., 2100.)`, yes `sol(year)` will not extrapolate before the initial time point since the ODE isn't defined there. What exactly are you trying to do here? &gt;Also, since 2075.05 is the year the population hits 4 billion, my guess was that the answer should be 2076 rather than 2075. You used the default rounding mode. If you want to round up, use julia&gt; round(Int,2075.5,RoundUp) 2076 
Sorry about that. I should have been clearer. So I have a differential equation, dP/dt = alpha*P*(1 - P/K), describing the time path of the population. In this case I am assuming that after 11 billion people, the population begins to decay at a constant rate. `sol` solves this differential equation and gives me the population at a certain year after 2017. Note that `population[7]` is the initial condition, i.e the population in 2017 (I have another array called `population` which contains the population each year). And as you said, I have an array called `year` with values from 1905-2017. Now I want to make a function which tells me the year the population will go below 4 billion using the values predicted by `sol`. I have found the exact solution, 2075.05, but I would like to have an integer value. So I have actual data from 1905-2017, I use OLS to obtain some parameters from said data and then I want to make some forecasts.
&gt;I have found the exact solution, 2075.05, but I would like to have an integer value. Use `round(Int,2075.05)`. Is that the only question? 
It's getting tripped up because it's finding your `year` global. Instead, just write this whole thing as your function: using Roots function pop4bn(sol) fzeros(year -&gt; sol(year) - 4.,2070.,2077.) round.(Int,fzeros(year -&gt; sol(year) - 4.,2070.,2077.) ) end Notice that the difference here is that `year` is now just the implicit `year` from the range in the `fzeros` function. Note that it now sounds like this is a homework problem so I don't want to just give stuff away, but you were so close that this seems fine.
Are we losing anything? Aren't a bunch of things going into a `sys` module?
There's just going to be one function in Base: **using**. Everything else is moving to **stdlib** or packages... :)
[Mocha.jl?](https://github.com/pluskid/Mocha.jl)
We're using [Knet](https://github.com/denizyuret/Knet.jl/)! There are lots of examples in our repository. There's also [Flux](https://github.com/FluxML). Both frameworks support dynamic neural networks and GPU computing.
Oh yeah also MXNet has a Julia API.
What is "our repository" ? Did you forget a link? :)
Which would you recommend highest; eg for building a LSTM to run on text.
I'm a little biased here :) I recommend Knet. AFAIK, - Unlike Knet, Flux doesn't take advantage of cuDNN which has very fast RNN/LSTM kernels. - Knet has most of the necessary CUDA kernels to implement a complex neural network. Last time I've checked CuArrays (array structure used in Flux), some kernels were missing. - Knet is more low-level. End user implements neural networks by just using matrix/array operations. Flux has high-level structures and procedures like Chain (stack-like structure contains network layers), Flux.train! (In Knet, you define your own train! procedure) on the other hand. Recently, some high level feature has been included into Knet and more high-level procedures will be included in the near future. - Knet has more examples and active users.
I suppose that is good advice, but it would be nicer if there were an easier explanation - say a documentation or tuorial
thanks that will actually be quite a challenge, I think ill try and incorporate machine learning
why not, im probably not going to use it or share it and i just want to learn how to read and write memory in julia
Seems as though there are better projects if that is the goal?
##r/JapanPornstars --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/6xrtxg/ignore_list/)
Just In Time
Back down to 47 in Jan 2018. Looking at any of the historical plots of rankings from TIOBE, there is a fair amount of noise; I guess they do not smooth the data. So dropping from 35 to 47 is in part just noise. 
This is cool. Does anyone have links to further reading? I tried finding some academic papers on the subject of color scheme extraction, but I don't think I'm using the right keywords.
I’ve no idea. “colormaps” or “palettes” might work, but I’m not an academic, so ¯\_(ツ)_/¯ 
(-:
It's pretty easy to get Julia within a factor of 2x of C - easy in the sense that usually this just happens anyway. With a bit of careful coding, getting even closer is typically possible. We're well outside my wheelhouse here, but I understand that for many problems classes, performance is determined and bottlenecked by e.g. matrix operations. Then comparisons of Julia vs C++ vs Scala etc are irrelevant; mostly the linear algebra library matters, and quite possibly any language you'd consider is using the same library. Definitely something analogous happens in integer programming where the choice of solver tends to matter much more than how you get your problem to the solver. All of which is to say, Julia is quite fast already. (Usual disclaimers about benchmarks apply, but see the benchmark graph about a screen down on julialang.org). For most people working on most problems most of the time, rewriting from Julia to maybe gain a sliver of extra performance is a poor investment of effort. In my case, I used to use Matlab for general purpose mathematical coding and simulation based optimization, R for stats and visualization, and patience in place of performance. Now I just use Julia. It's not perfect at everything and in some cases the absence of a major library from another language will be a showstopper, but overall I've been thrilled and definitely encourage you to give it a try.
I replaced all my regex data manipulation with Julia. l am comparing my approach to dealing with Fannie Mae data in R vs Julia. More to report in a couple of months time
 How much of your code base you choose to replace Julia with depends whether you are working in research or industry. Julia is an awesome language, but you must remember that it is not yet at version 1.0 so if you build important infrastructure with it now, there is a chance you will need to do some re-writing if you need to upgrade later on. With that caveat aside I think you should move as much of your programming into Julia as possible. Here is why: The 2-language issue is a perennial problem typically dynamic programming languages do not offer type safety, performance, and have poor infrastructure for OOP. Julia offers all of these. You don't have to do the traditional thing of pairing a dynamic language with a static one because you can have all the benefits of static languages with the convenience of a dynamic language. This is very powerful and simply awesome. As you write code, there is no niggling doubt as to whether you need to abstract some of this to C++ or some other performant language. Multiple dispatch. For a long time I was 'brain washed' into accepting that standard single dispatch was the way to think about objects. If you have been writing OOP especially in a static language, you will find multiple dispatch (MD) a great revelation and wonder why there are not many more languages that offer this as the standard OOP methodology. Object design becomes so much easier and you can focus on writing your application. I could wax lyrical about MD but if you have been using OOP and you try it, you'll understand and if you haven't it won't matter. Suffice it to say that multiple dispatch is &gt;&gt;&gt; OOP Type safety and type parameter programming. This is a big deal. In dynamic languages, it makes the difference between quickly writing a function knowing that it is pretty 'safe' and writing a function and having no idea that it will do in the wild. You can do all the unit testing you want (by all means it is good practice) but in dynamic languages with no type enforcement support writing a function that even approaches being safe is practically speaking difficult (impossible?). Julia has one of the simplest and most flexible type parameter systems I have come across in any language. If you have been using template programming in languages such as C++, Julia's type parameter system will thrill you and because it is a dynamic language, you can directly manipulate types - what's more flexible than that? Prototyping. Even though Julia offers so much in terms of language features, it is an easy programming language to write code in. It lends itself to fast prototyping and subsequent optimization. The fact that it has a decent interpreter should not be overlooked. I find interpreters are great for trying things out as you build your code and it increases the speed of development since you find out what works much more quickly. Scientific programming. Julia was built primarily for numerical programming and so out of the box contains linear algebra and other standard numerical tools. In terms of numerical libraries, there are plenty out there. However, you need to remember that Julia is a moving target so expect package incompatibilities both with dependencies and with your version of Julia. Julia language has a friendly community. There are also plenty of Julia users in Gitter, Slack and other programming chat services that provide great tips and knowledge tidbits. I for one have decided to move everything I possibly can to Julia and work is so much more fun and productive. 
I have replaced my entire trading infrastructure (I run a hedge fund) all with julia code I write all, though I still use some java code because some API's I use support (only) java I don't really see what one could not do performantly in julia but do in C++ or Scala
I wouldn't call the thing you contrasted with multiple dispatch OOP, but rather single dispatch. As you are probably aware both single and multiple dispatch are time honored ways of doing OOP.
Put a `@show x` after the second line of your function. See what it says. (Although I don't get `nothing` like you do.)
Sorry, I just remembered that I added the `else return "Weird Title"` after speaking to my lecturer. When I add `@show x` it outputs &gt;x = String["Braund, Mr. Owen Harris", "Cumings, Mrs. John Bradley (Florence Briggs Thayer)", "Heikkinen, Miss. Laina", "Futrelle, Mrs. Jacques Heath (Lily May Peel)", "Allen, Mr. William Henry", "Moran, Mr. James", "McCarthy, Mr. Timothy J", "Palsson, Master. So the names of everyone in the dataset. When I check `head(titanic)`, the "Title" column contains "Weird Name" for every entry (or "nothing" when I remove the `else return "Weird Title"`). 
Problem solved... (That line isn’t required.)
it's really good, the problem right now is that the language keeps changing underneath you, which tends to break packages, and if some of them fix the bugs but others don't you end up trapped in dependency hell. 1.0 is coming out in a couple months and things should stabilize then. Julia is generally very fast. in particular, if you have problems with speed, it's conceivable you might want to use C++ for its tighter control over memory, or stuff like Eigen. But as great as sbt is, it's hard to imagine dropping into Scala for performance. if you are interested in Bayesian statistics, Turing.jl is an EXTREMELY promising library that provides native-Julia implementations of all the popular MCMC samplers (they even have NUTS). You can see benchmarks [here](https://github.com/yebai/Turing.jl/wiki) against Stan. This is a fairly immature and not-super-optimized library competing against one of the best-optimized C++ code bases around.
Fyi, if you place four spaces at the start of each line, rather than wrapping them in ` quotes, you get a far more readable codeblock: function titles(x) x = titanic[:Name] if "Mr." in x return "Mr" elseif "Mrs." in x return "Mrs" else return "Weird Title" end end
When mixed with DiffEq, Turing.jl seems quite promising as something that is much faster than Stan. The engine seems quite flexible and accurate in ways that Stan is not.
There's a student who's starting to put together some notebooks comparing the two (along with DynamicHMC.jl). Here's one notebook: http://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/ParameterEstimation/StanandTuringLotkaVolterraTest.ipynb That needs to be made more difficult (with the starting prior means away from the true values), but you can already start to see the timing difference when getting the mean of the chain 1e-1 away. http://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/ParameterEstimation/StanLorenz.ipynb That shows that it takes 8.4 hours for Stan to estimate parameters on the Lorenz equation and it still doesn't get within 1e-1 of the true parameters. The Lorenz equation is a difficult estimation problem, and a reference for that is this other notebook: http://nbviewer.jupyter.org/github/JuliaDiffEq/DiffEqBenchmarks.jl/blob/master/ParameterEstimation/LorenzParameterEstimation.ipynb Optimization-based techniques take minutes vs the hours of Stan, so I'm not sure it scales well. But we'll be playing with building our own. The Stan implementation is pretty fixed because it uses Stan's integrators and MCMC engine, and all we do is write the DiffEq in Stan and get the results back. So it's a good reference since there's really nothing "DiffEq" in there: it's straight Stan. The Turing.jl verison we have a lot of control over, and hopefully it won't take hours for a simple ODE... It's a work-in-progress but we will be refining our way of benchmarking this with generated data. I hate the "estimate on real data and show the line goes through it" type of verification because I think it's meaningless. But we need some way on these kinds of problems to simultaneously compare runtime and error. 
Quantize colors by clustering values using the median cut algorithm MMCQ from the Leptonica library (http://www.leptonica.com/). I used the julia schemes above with a python program to extract a scheme or reduce an image using a given scheme. I modified the following code https://github.com/fengsp/color-thief-py/blob/master/colorthief.py copyright: (c) 2015 by Shipeng Feng
That should probably be cross posted to /r/programming or so.
Looks like good stuff. You should get CI enabled and get it registered though. Here's some details on how to do that: https://www.youtube.com/watch?v=tx8DRc7_c9I Then it will be much easier for others to use and contribute to your work! 
Video linked by /u/ChrisRackauckas: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [Developing and Editing Julia Packages](https://youtube.com/watch?v=tx8DRc7_c9I)|Christopher Rackauckas|2017-05-14|0:48:27|40+ (100%)|1,302 $quote This is a tutorial which shows how to generate and edit... --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/ChrisRackauckas ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=$comment_id\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v2.0.0
Yes
Numberphile did a video on braids a little while ago. https://youtu.be/G_uybVKBacI
What the most proficient programmers in R/Matlab/Python sometimes fail to realize is that Julia solves the 2-language paradigm, and simply works without quirks and workarounds using common programming idioms. Benchmarks show exactly that (if someone wanted more performance in Julia it could be achieved as well, at the cost of obfuscating the code, just like the competing solutions in other language). That's a huge plus for scientific computation and prototyping, and makes code more manageable. Performing well is a hefty bonus tho ;)
This is fantastic - thank you so much for pointing this out. I'll add a link to the post for sure
It looks like the problem might be that `return counts` is coming before `println("$counts")`, which is terminating the function before it has a chance to print what it's supposed to print. Could that be it?
Thanks for responding. So I tried your idea and it got me further along, but there is still an issue. this is the output in the REPL when I run the code, having removed the return: shell&gt; ./scanner_txt.jl Tucker1.txt Dict{Any,Any}(Pair{Any,Any}("txt", 1),Pair{Any,Any}("Tucker1", 1)) 8 shell&gt; So it looks like it is actually just scanning the command line argument string, and not the actual file to which I am pointing. 
You need to actually read from the open file with readline, eachline, readstring, etc.
Despite the interesting solution, I guess everyone wants to know - how did JuMP do ?
I changed my small line about the MIP solution now. Thanks for the comment. It was working reasonable well with around 300,000 variables (10,000 children and 30 gifts). I don't know how fast it would be without using JuMP, if I directly use the solver interface. 
Thanks a lot, again. I have one additional question, if you have a second. I am trying to implement a second command line argument, a character to search for and count when reading the text of a file. The older script above just has 'I' as the default and only searchable character, but I would like the script to be more dynamic. So I have been trying and although the script runs, it does not count the instances of the ARGS[2] character as intended. Any tips? [Here is a screenshot (you can ignore the wordcount function, which I deleted to simplify the screenshot](https://imgur.com/a/0wHk9) Thanks again.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/LVquQgK.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dtvvncf) 
&gt; The summed rank is 8 whereas the best we could have hoped for was 5. I think the best we could get in this given scenario is only 7, because kid 2 and kid 4 are jerks who didn't mark any item as rank 1 - so in both those cases, the very best gift had only rank 2. 
In Julia Char and String are not the same. So if you try: if string(x) == search_me you'll convert the x (a Char) to a string, so then the comparison will return true. If you use a lot of `@show ` and `@show typeof()` debugging lines, you can investigate problems more easily. I inserted a `@show x` line and the single quotes suggested the answer... :)
Yes! Awesome. Great tips. Do you have an ETH address? PM me and I will send you a lil’ tip if you’re OK with it. 
The kids are A-E not 1-5 that means that every kid choose the ranked from 1-5.
Thank you, that’s very kind!; but I like being paid in upvotes and stars :) 
Thanks for the comment. I have tried this. It doesn't work. The quiver plot just rescales it back.
Ah, now I see that, and the data makes more sense too now. Thanks.
I gave you every upvote on could (disclaimer: only on this post.....!!!!!!.....!!!!!!)
I'm kind of exisited for named tuples and argument destructuring, even if I don't know if I'll use them
Julia v0.7 has not been released yet, so please don't go posting this around like it has.
View in your timezone: [February 15 at 9AM PST][0] [0]: https://timee.io/20180215T1700?tl=Streaming%20online%20intro%20Julia%20tutorial%20tomorrow%2C%20February%2015%20at%209AM%20PST%2F12PM%20EST%2F18%3A00%20CET%2F10%3A30PM%20IST ***** [^^delete*](/message/compose?to=timee_bot&amp;subject=delete+request&amp;message=%21delete+eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJkdTlhMjhtIiwiYWN0IjoiZGVsZXRlIiwib3AiOiJ4b3JKYW5lIiwiaWF0IjoxNTE4NjQ3MzM3fQ.MairCcb2d-PxxQZm91NZop2WP23n8Oraqw9lof8RLsk) ^^| [^^reprocess*](/message/compose?to=timee_bot&amp;subject=reprocess+request&amp;message=%21reprocess+eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJkdTlhMjhtIiwiYWN0IjoicmVwcm9jZXNzIiwib3AiOiJ4b3JKYW5lIiwicG5tIjoidDNfN3hsd3l2IiwiaWF0IjoxNTE4NjQ3MzM3fQ.Ynchnfo6qy1cHa7jNRmtFqva6ho7HHXovlI4TkKh5jo) ^^| [^^ignore ^^me](/message/compose?to=timee_bot&amp;subject=ignore+request&amp;message=%21ignore+me) ^^| [^^help](https://www.reddit.com/r/timee_bot/wiki/index) ^^*OP ^^only
Hi. Thanks a lot. This was really helpful, and it ultimately led me down a saga where I learned much more. I fixed the function and loaded the PyPlot package correctly. Be well. 
Thank you and congrats on your new postdoc appointment!!
I'm super happy @ranjanan did this. We talked about it at the last JuliaCon and I didn't know it was actually going to be a thing. But yep, it's a thing. This will work great as a preconditioner for Krylov linear solvers for PDEs!
JuMP does nonlinear as well, look up @NLobjective and @NLconstraint to see how. I'd love to see what problem you are solving for.
Thank you for replying! So maybe I'm misunderstanding what you saying, but I'm under the impression that JuMP is for solving optimization problems (I've used it for nonlinear optimization in the past). I'm trying to solve a system of nonlinear equations. If JuMP has functionality for that, I'm not seeing it anywhere in the docs. Would you be able to point me towards a reference? Or are you saying that recasting the problem as an optimization problem, such as minimizing the square residuals of the equations, is the best way to solve this problem? Is squaring the best function or would some other function be faster/more stable? As far as the problem goes, I'm solving this for a colleague who doesn't do much computation. The fact that she hasn't published a working paper on her website implies to me that she doesn't want her model floating around the internet yet, so I'll refrain from posting it. But broadly speaking it's a model of international trade in the vein of Krugman-Melitz. I can provide specific details about properties of the system of equations if they are relevant to the solution method.
Let me start by remarking that 10k+ linear equations is easy, but 10k+ nonlinear equations can be very difficult. Linear solving usually has some proof of convergence. Nonlinear solvers are usually initial condition dependent. Solving nonlinear systems with Newton trust region methods (the most widely used methods) requires calculating a Jacobian each step and solving a linear system. To make the full solver fast, you need to make this fast. To make this fast, you need: 1. Jacobian functions from the user to avoid expensive differentation 2. Sparse support 3. Problem-specific linear solvers (i.e. preconditioned GMRES) Given those points, what did you do? Did you provide a Jacobian, along with a sparsity structure? For 3 you currently have to modify the package a little bit (but it would be a great PR to make this generally doable). Without these, nonlinear solving of large systems (with sprase Jacobians) is slowwwww no matter what language/package/etc. is used. BTW, when you say NLsolve.jl didn't work, was it diverging or just too slow to finish? If divergence is the issue, you might want to sample initial conditions and choose the one with smallest norm, or choose a few. Also play with the line searches a bit. Other choices, there aren't many. I know the Modia.jl people have some internal Minpack bindings and that would be worth trying: I hope that can be refactored to an independent package. You can try Kinsol in Sundials, but that doesn't trust region so NLsolve is probably much more robust. NLsolve's method is quite robust, so unless there's something else going on then it's likely a problem thing instead of a method or implementation thing. You can use a nonlinear optimization method on |f(x)|. Of course, it will have issues near the actual zeros because of the discontinuity, but if you get close enough then that can be an initial condition for NLsolve to finish with. However, JuMP isn't that great for a blackbox nonlinear function. The whole function registration stuff is very iffy: MOI's nonlinear support could need some work. But IPOPT.jl, NLopt.jl, etc. can also be used directly just fine. Optim.jl does well enough on large systems if you can get away without sparse Jacobian support. BlackBoxOptim.jl is quite a good global search. You can use these as a better way to sample for initial conditions. 
I forgot the outer constructor. This code works: abstract type ABS{A,B} end struct Myk &lt;: ABS{Int64,Float64} end struct MyStruct{A,B,K&lt;:ABS{A,B}} a::A b::B MyStruct{A,B,K}(a::A,b::B) where {A,B,K&lt;:ABS{A,B}}= new(a,b) end # this is the outer constructor: MyStruct(a::A, b::B, ::K) where {A,B,K&lt;:ABS{A,B}} = MyStruct{A,B,K}(a,b) # now this works: MyStruct(1,2.1,Myk())
&gt; Optimization is my big stick which I beat a lot of problems to death with. &gt; &gt; It isn't the best stick, but it is the stick I have. This is my situation as well haha
A guess another question would be: Solving systems of equations seems like a fairly common task, so why aren't there competing softwares like there are for optimization (Gurobi, Mosek, SCIP, etc.)?
&gt; I'm comfortable providing analytic Jacobians for optimization problems (although a colleague recently told me in passing that automatic differentiation means that there isn't very much performance gain from doing this, so I've stopped doing it recently under the premise that my time is more important than my computer's time), but what exactly do we mean when we talk about the Jacobian of a 10k system of equations? Is it the Jacobian of the "residual function" (for lack of a better term)? Autodiff still has to do an evaluation per column of the Jacobian (it does one by one a dual number eps with coefficient (1,0,0,0,...)). If you think you can do better than 10,000 f calls, then define it. It is the Jacobian of g(x)=0, g'(x). &gt;The problem I'm encountering is divergence. I've tried many different starting conditions and (I think), I've managed analytically pin down a region where the residual function would be convex which I think means that it shouldn't be too sensitive to my starting points as long as they are in this region, right? Yes... up to numerical issues. How well conditioned is the Jacobian? &gt;but is there a reason to use the absolute value function rather than square? I mean |f| as any norm. Whatever norm you want. It's an approximation anyways.
Not totally sure but absolute value tend to set parameters to exactly zero while square gives more leeway when close the zero. Otherwise you could just try gradient descent on your objective function, in machine learning people regularly do it on large number of parameters (although they only search local minima), you can use auto-differentiation to get the gradient if needed.
The methods are pretty much set so it's much harder to differentiate. Nonlinear global optimization is still a black box that nobody really knows how to do well, but supposedly the commercial solvers are much better than anything open source mostly due to large test sets. 
ok, I did some looking up and I think you are in luck. If you don't specify any optimization objective, the solver will just try to find a feasible solution to your constraints. Be sure to provide a starting point (by calling setValue() for each variable) if you have nonlinear equations. 
This looks very useful! One comment: it would be very useful to have dates at the top of each post, or at least a specification of which Julia version the material refers to. This is always useful, but particularly so since Julia is changing so rapidly right now.
Glad you like it! This is on Julia 0.6, but it is introductory/basic stuff, so shouldn't change for Julia 0.7 (but as soon as Julia 0.7 is out, I'll go through them to make sure they run correctly). For the JuliaDB stuff it is a bit trickier: the tutorial was a way for me to see which aspect of the API needed to be made simpler and the tutorial includes many very recent improvements: as soon as things solidify a bit I'll specify what's a good version to run it. I should also add a mechanism to add feedback: I'll probably enable comments at some point but for now if you find something wrong / have suggestions, you can leave them at the repo: https://github.com/piever/simpleblog
Non paralllel version: function scMult(u, v) n = length(v) scMult = 0.0 @inbounds @simd for j = 1:n scMult += u[j] * v[j] end return scMult end @views @inbounds function classicMult(A, B) m1 = size(A, 1) n = size(A, 2) n2 = size(B, 2) C = zeros(m1, n2) A = A' for k = 1:n2, j=1:m1 C[j, k] = scMult(A[:,j], B[:,k]) end return C end function timeClassic(m) n = 2^m A = rand(n, n) B = rand(n, n) tic() C1 = A * B t1 = toc() tic() C2 = classicMult(A, B) t2 = toc() return t2/t1 end And the parallel version: I will add my current best parallel version shortly.
Current parallel version (80x slower than MKL): @everywhere function myRange(q::SharedArray) idx = indexpids(q) if idx == 0 # This worker is not assigned a piece return 1:0, 1:0 end nchunks = length(procs(q)) splits = [round(Int, s) for s in linspace(0,size(q,2),nchunks+1)] splits[idx]+1:splits[idx+1] end @everywhere @inbounds function myParChunkMult(A,B,C) colrange = myRange(C) for j in colrange myColMult(A, B, C, j) end end @everywhere function scMult(u, v) n = length(v) scMult = 0.0 for k = 1:n scMult += u[k] * v[k] end return scMult end function parMult(A, B, C) A = A' @sync begin for p in procs(C) @async remotecall_wait(myParChunkMult, p, A,B,C) end end end function parMult(A, B, C) A = A' @sync begin for p in procs(C) @async remotecall_wait(myParChunkMult, p, A,B,C) end end end function timeParMult(m) n = 2^m A = convert(SharedArray, rand(n,n)) B = convert(SharedArray, rand(n,n)) C = convert(SharedArray, zeros(n,n)) tic() parMult(A,B,C) t1 = toc() tic() C2 = A * B t2 = toc() t1/t2, maximum(C-C2) end @time timeParMult(10) 
Code highlighting is done by adding 4 spaces.
How are you running this code? How many cores does your computer have, and what number argument are you passing if you're using `julia -p`? 
I run (this) Julia code either inside Jupyter or JuliaPro. I use addprocs(4), I and my computer (Macbook Pro) has 4 physical cores. I have however also tested a slightly different version of the above parallel code (and the same non-parallel code) on a computer with 2 eight-core Xeons (addprocs(15) was 34 times slower on the same test than MKL).
You can use threading via `Threads.@threads`, but it's experimental and there's a performance bug you'd have to watch out for.
It's relatively simple to merge pull requests into your local install: https://github.com/TeamPorcupine/ProjectPorcupine/wiki/How-to-Test-a-Pull-Request This way you won't have to wait for it to be merged officially. 
Use Ipopt via JuMP, don't define an objective function (or set it to a constant) and the solver will treat your constraints as a feasibility problem.
Link broken
https://piever.github.io/simpleblog/post/
Oof, and here I was thinking that I'd googled around and read the relevant piece of documentation. But when I googled SharedArray I got a very informative section I hadn't seen before. Thank you very much!
LLVM.jl is a good tool to know for this. https://maleadt.github.io/LLVM.jl/latest/lib/interop.html#Inline-assembly-1 Note that it requires a source build of Julia for v0.6, but on v0.7/v1.0 it'll work out of the box. 
That might be because of how you defined your array. Shared Arrays can't be arrays of arrays, afaik, so you might want to simply rewrite your definition of V to: V::SharedArray{Float64,3} That way you still have a third order tensor, without these nesting definitions like a vector of matrices.
For the NA bit, you probably have the new DataFrames version installed (0.11+) -- refer to the most recent documentation. For the latter part, you probably used a range at some point (like `1:5`) when you should have used an array, which you can create with something like `collect(1:5)`.
Your worry about distributing your arrays is definitely valid. I already saw your response last night, and decided to think over it, and I can't say I came to a real conclusion. If anything, shared arrays are probably better than distributed ones in your case, since your vectors entries aren't disjoined, as you described. My best guess would probably involve a lot of restructuring in your code, and you would definitely not see an optimal speed increase. Depending on what happens with these arrays during run time, you might be able to restructure everything in a way that you copy your matrices into sharedarrays. This is, of course, extremely costly, so the workflow of the program would need to be such that these copies are few and far between. It is definitely a challenging problem, and I'm kinda out of my element here, because I never had to deal with parallelising over non-hyperrectangular arrays (arrays that, in 2 dimensions would be a rectangle, a box in 3 dimensions, and so on... objects with smooth faces\^\^). If you ever find a solution to this that doesn't involve rebuilding half of your code, please shoot me a message, I would be very interested \^\^
Hi. Thanks for the help. So the collect() idea fixed the integer issue. Perfect. However, I cannot find the relevant portion of the documentation for Julia 0.6.2 that explains how to replace a dataframe cell with an NA value. 
More details here: https://discourse.julialang.org/t/errors-for-git-pkg/9351
You need to look at the documentation for DataFrames itself. There's also some good information in the package called Missings. Also just FYI you will typically get faster and better responses asking questions on the Julia Discourse forum.
Thank you for spending some time thinking about it! It seems like a hard problem to me, too, although I wasn't aware of the subtleties of RAM vs cache and stuff like that. I'll definitely keep you posted if I think of something clever. In the meantime, I'll probably refactor the parts that are easy to use pmap and see how much of a performance boost I see from that, and otherwise I'll just do my stuff in serial I guess. Thank you again for your time and correspondence!
It just entered `METADATA.jl` today!
The top performance calculations these days are: - CPU heavy data crunching ML pipelines. MULTICORE with SHARED MEMORY is a must (julia doesn't have it, it can only start multiple processes) - GPU - the code will be translated to GPU-code and executed separately, Julia VM won't be used. So it's doesn't matter if GPU-code will be generated from Julia, Python or something else.
Julia has multicore through threading. I use it all the time... And no, performance is orthogonal to parallelism. Parallelism is scaling, while performance is the baseline. You need the baseline for performance to matter. We've already seen the GraphX performance failures that are there even though it "scales": it's just too slow to begin with. That shows you need the single-core performance to even get started, which Julia has. The next is mixing it with forms of parallelism. Julia's distributed parallelism is great, its threading is immature but works. There's a WIP PR for Clik-style threading which handles nesting that looks like a pretty amazing. Together, the only thing to really complain about is the closure bug which is annoying but has an easy workaround (but I really do hope that gets fixed in a 1.x).
For matrix-matrix multiplication, you're not compute-bound so much as cache-bound for these computations. The reason the MKL (or any half decent linear algebra library) is so much faster than a code that computes the dot products directly is that the former optimizes block matrix computations to work on blocks of the matrix that can be loaded/prefetched efficiently into cache. Your performance drops drastically the more cache misses you incur, for a fixed number of arithmetic operations, so the hand-crafted matrix-matrix multiplication (or, in fact, most linear algebra operations) isn't going to be faster than the block-optimized versions found in libraries, even with multiple core usage. 
Multicore support in Julia is built in and easy to use. Maybe there's some fine point of this that have given you trouble, but it's not clear from your description. Can you give some more detail?
&gt; Julia provides a multiprocessing environment based on message passing to allow programs to run on multiple processes in separate memory domains at once. Taken from Julia Docs https://docs.julialang.org/en/stable/manual/parallel-computing It explicitly says that there's no multicore support with SHARED MEMORY. You have to start separate processes. It's not multicore at all. Because if treated that way you can say pretty much anything has multicore support. Every single language has multicore support via processes with isolated memories. The problem is to have multicore support with SHARED MEMORY.
Yes, please read the docs: https://docs.julialang.org/en/stable/manual/parallel-computing#Multi-Threading-(Experimental)-1
+1. Interesting, thanks. Yes that's what I meant, seems like Julia indeed has it. Would be interesting to see benchmarks.
I don't think it has been well-benchmarked mostly because it's still "experimental", and going to go through an overhaul soon. Here's the PR for the overhaul: https://github.com/JuliaLang/julia/pull/22631 But in the meantime `Threads.@threads` is fine if you're careful.
I use [Playground.jl](https://github.com/rofinn/Playground.jl) for virtual environments
What libraries would we make good use of?
Go has a ton of good little utility packages. For web stuff Go has everything under the sun (a topical example is Cascadia.jl, which was ported from the Go version). Templating (e.g. Liquid), file format readers (e.g. mp3 or ogg vorbis), protocols (bittorrent, HTTP, TCP, bitcoin ...), various crypto tools.
Found it, you surround the code in @profile begin . . . end
Checkout https://github.com/mcreel/Econometrics and the examples section. If they know some econometrics (doubt it), https://lectures.quantecon.org/jl/ is also pretty good
For mathematical modeling you may want to check out [DifferentialEquations.jl](http://docs.juliadiffeq.org/latest/). There's an [introduction video](https://www.youtube.com/watch?v=KPEqYtEd-zY) that goes over using it, and then there's some [extra tutorials in a separate repo](https://github.com/JuliaDiffEq/DiffEqTutorials.jl). An application of it to ecological modeling is [BioEnergeticFoodWebs.jl](https://github.com/PoisotLab/BioEnergeticFoodWebs.jl), and I'll have a paper doing systems biological modeling with it out in a few days but maybe not before Thursday (accepted and going through the editorial process...). For specific case studies related to industry, [JuliaComputing has a great showcase](https://juliacomputing.com/case-studies/). Maybe these industry applications might be more what you're looking for? 
I would like to step through some projects from JuliaComputing but it seems like they dont provide the code. Chris, may I ask you for a favour? Would you happen to have some slides prepared for an "introduction to julia" that I may use (ofcourse with credit and link to your github). As for the code, I have decided to start off with programming Conway's game of life to give them an example of a type of agent-based model.
Yes, [this website for an Intro To Julia](http://ucidatascienceinitiative.github.io/IntroToJulia/) has all of the slides and notebooks [in this Github repo](https://github.com/UCIDataScienceInitiative/IntroToJulia). 
The easiest way to find out is to try building some models. Replicate the top winning Kaggle entry for some problem and see what is more difficult than it should be. 
Start at the [JuliaStats](https://github.com/juliastats) organization, then learn both Julia v0.6 and Julia v0.7 (they’re very similar :)) and then you’ll be ready to do testing, bug fixing, and documentation ready for the imminent-ish 1.0 release, and in some spare moments you can make some tutorial videos on YouTube! :)
There also exists [TimerOutputs](https://github.com/KristofferC/TimerOutputs.jl) package.
A few weeks ago, I was looking for a Julia counterpart to the R package "LogConcDEAD". Said R package implements a maximum likelihood density estimation procedure for log-concave densities via convex optimization derived at https://www.repository.cam.ac.uk/handle/1810/237061 . If want to implement a challenging statistical tool, this could be a start...
Oh boy.... that actually does look like a huge challenge. I'll mark this on the list of things to look at and consider farther down the line.
Any issues where you are not happy with it? It is some very nice code. There are places where you could make it go faster, but, I'm not sure if that is an issue. There are some functions where Julia already has the function build in, which you define yourself. But it is hell of a nice bit of code there :) - very easy to read.
hey thanks! I'd like to improve as much as possible the performance as being a numerical simulation, the faster the better. Is there any particular function I am reimplementing? I'm also moving the input format from *.csv* to *.yml*. The latter is more flexible and allows a better specification of all the parameters (I'm using YAML.jl for this). 
There are a bunch of little functions around the place (max, min, etc) Performance wise, there are places where you could be using broadcasting, which should help. I'll have a look tonight (I'm at work currently, also working on a bunch of Julia code - porting code to run on the GPU). See if I can get a copy running. 
You might get more responses (and more help with any performance questions you happen to have) if you also post this over at the [Julia discourse forum](https://discourse.julialang.org/), which tends to be more active than the subreddit. 
&gt; vectorisation in general is good for speed ups That is simply not true for julia. From my experience broadcasting typically results in a 10% slowdown compared to for loops. Most of the time I'm ok with the slowdown because the syntax is so nice but it is slower than for loops.
Then you are not fusing them https://docs.julialang.org/en/stable/manual/performance-tips/#More-dots:-Fuse-vectorized-operations-1
&gt; I followed the discussions and papers around introducing multidimensional arrays into Go, and saw it fall flat on its face because they couldn't guarantee a similar efficiency reading rows vs columns (well that was one of the main issues). But Go does have multidimensional array right? 
Does it? I guess it depends what you mean by that. By default you can nest arrays - i.e. you can have arrays of arrays of arrays (where arrays are 1D data structures). If you [google multidimensional arrays](https://www.tutorialspoint.com/go/go_multi_dimensional_arrays.htm) in Go you'll come across this default implementation. I don't think you can really call this multidimensional - because you cannot slice in any dimension through it - for example while you can take one 'row' out, to obtain a 'column' you need to create a loop that will cycle through the rows removing the required position from each row to recreate your column. It gets more complicated if you want to slice a 2D array out from a 3D array, which requires a triple-nested loop. I'm using 'slice' and 'array' in a very loose fashion here, because they have special meaning in Go. The other issue is that functions can take these 'arrays' (or 'slices') in Go, but their size is part of their type - so you can only pass specific sizes of these arrays to functions - there isn't a generic multidimensional array type, as far as I know. I did hear about a library that would do it in the way R/MATLAB/Julia/Fortran (am I missing another language?) does, but now I can't find it - maybe it never happened.
thanks for the suggestion, I'll post there as well
Indeed the first version of openBF was hugely using vector computations. I then started reading the performance tips on the official documentation and I got a huge improvement by de-vectorialising all the calculations
Fantastic work. As a fellow engineer working on computational blood flow problems, thank you! How long does it take to run for a problem of about 20 vessel segments? And did you benchmark the speed against an equivalent C/C++ or Fortran code? Sorry if I missed that
Glad you find it useful :) Regarding the time benchmark, I have a rough C implementation of MacCormack scheme for a single straight artery + windkessel, so no bifurcations involved. The single cycle (cardiac cycle convergence is not implemented in the C code) computational time for the McC code is ~0.305 seconds while openBF does the same computation in 6.5/22 seconds/cycle (~0.29 s/cycle). I guess it is not fair to compare two different numerical schemes implemented in different languages, but I'd say that openBF performance is already good. If you are interested in accuracy benchmarks, I should point you to my [thesis](http://etheses.whiterose.ac.uk/19175/) where chapter 6 is focused on the model validation. I reported openBF's results for some gold-standard benchmarks. When dealing with more segments, the computational cost of bifurcations adds up. I am now working on a model of the Circle of Willis (33 arteries). In this case, I have stable waveforms (max 5% error between consecutive cardiac cycles) after simulating ~20 cardiac cycles. The total computational time rises to 20 minutes (hence my desire for improving the performances). If you have a C/C++/Fortran/Python+Numba/etc code (possibly an explicit solver, to make the comparison fair) you want to run against openBF, please get in touch as I'd be very interested in it. By the way, you can find some already implemented networks in [this additional repository](https://github.com/alemelis/openBF-hub).
Certainly, 33 vessel segments for 20 cycles in 20 minutes is very good! Unfortunately, I do not have a FV code for 1D blood flow - only MacCormack as you have done (not surprised here, fairly accurate scheme yet easy to code). I am, however, starting to work on a 1D solver using meshless methods. The only reason I am using meshless is to help learn the intricacies as I prepare to start building a 3D code for my PhD starting in Fall (yikes! wish me luck, I am going to need it). I was going to inquire if you had plans for parallelization, but 20 minutes is a fast for this type of computation from my past experience. From my limited experience in Julia, it seems fairly easy to implement parallelization, so maybe that could be a cool additional project to this.
There are a bazillion R packages. You said you have a strong background in R so found an R package that you're familiar with which hasn't been implemented in Julia, and then implement it.
Are you going for a full 3D fluid-structure-interaction solver? I wish you all the best! :) Do you have any interesting reference for the meshless method? I can see how it will help in your PhD project, but are there any advantages for 1D solvers? openBF is not easy to parallelise. This is mostly because the solver must *follow* the solution along the network. Therefore, you cannot compute the solution in a point if you haven't *visited* first the previous nodes. This means that at the beginning of each cardiac cycle, you can use only one thread to compute the solution along the first vessel. It would be different in 2D as you can use many threads to travel along the artery by covering the entire cross-sectional area (it is not that simple but I guess I gave you the idea). Having said so, an opportunity to somehow parallelise the code arises as soon as you get to a bifurcation in the network. There you can spawn two threads and compute the solution along two vessels at the same time. This would be possible as the only interaction between two arteries occurs at the bifurcation node; all the remaining nodes are independent. I juggled a bit with this idea and you can see my efforts in [this branch](https://github.com/INSIGNEO/openBF/tree/parallel) (not really documented though). The way I implemented it worsened the execution time, but I'd need to go back and spend time on the code to make it work properly. Is this the parallelisation method you have in mind? Or were you referring to write the solver in vectorial form and exploit Julia built-in functions?
There's probably a lot of implicit parallelization due to BLAS commands for any internal linear algebra. This would be disabled if you add processes, so explicit parallelization may not even be helpful until you go to multiple node machines or parallelism-in-time. 
Why is this pointing to JuliaObserver instead of the repo?
I'll start linking to the github repo instead. I just made a button on juliaobserver to make sharing easy.
&gt; and Go in general Rightfully so. Go is a mess in terms of programming language history.
If by modern you mean 'new', then I agree. Otherwise its disregard of the last 30 years of pl research contradicts the word 'modern'.
Oh ! Alright, got it
I see there is a summary of the proposals put forward for implementing it in go [here](https://github.com/golang/proposal/blob/master/design/6282-table-data.md). If it is ever implemented, like generics, it won't make it into Go 1, but maybe Go 2, which looks like it will be quite a different beast.
On the topic of PDE's and Navier Stokes, my first thought is about behaviours like shockwaves and fractal behaviour that it can exhibit. Would Julia be well-suited to build variable meshes like in https://www.youtube.com/watch?v=txk-VO1hzBY ? To me, these look like the kind of things that would benefit a lot from using a high-level language to implement operations.
Julia's `resize!` functions are quite smart and can be very helpful for variable meshes. Plus the ability to overload `*` and `A_mul_B!` on types makes it very easy to create lazy matrix-free implementations of operators (see ApproxFun.jl as a good example). Additionally, the overloads on `*` plus `\` for special matrix structures (like banded matrices) make different types automatically specialize the linear solvers and other internal codes, compounding performance gains. So I think multiple dispatch gives a lot of flexibility for writing adaptive PDE methods that take into account the full matrix structure at every step of the algorithm to make it more efficient, and this is one of the things I (and many others) are currently exploring.
You can turn it to mono on the command line --color={yes|no} Enable or disable color text But do yourself a favour - get a better terminal https://conemu.github.io/
Ty
Thanks when I use the conemu, do I have to run Julia and attach it to Julia in the process list? I would essentially run two terminals. 
I use Julia for business analytics - not big data though. JuMP is a godsend for me. (Modeling language for Mathematical Optimization (linear, mixed-integer, conic, semidefinite, nonlinear) https://github.com/JuliaOpt/JuMP.jl My previous language in this case was Excel Solver, then Cplex using text descriptions. I also used Octave &amp; Python. Tried R but didn't the learning curve of the syntax was too steep (not that I couldn't have mastered it but "good enough" in Octave etc. always beat out "perfect" of putting in the effort.). The beauty of Julia for me is that I don't have to leave it for doing other things. It is just as good to code for my other interests (raytracing, models for 3d printing etc.) and preparing Excel reports for my team from SQL queries. 
Compared to MATLAB: Julia is more universal, but at the expense of language complexity. MATLAB's debugger and profiler are great though. Compared to Fortran: Julia's macros and generics are great, much less code repeats. ODE solvers in Julia are great. Use of automatic-differentiation is a breeze. But: Julia's compilation model... sucks: to be fair it is a necessary feature, but it needs permanent cache of compiled functions and not just pre-compiled. Also, it's fairly straight-forward to write Fortran DLL and call it from Excel/Python etc. Not so easy with Julia: Julia can call into Python/R/Fortran, but not other way around. And it is somewhat unpredictable how efficient Julia's code is going to be, once again Julia is more universal, but more complex.
I come from Python, and try Julia every year or so, but so far it has not proven itself superior to Python. In fact, I find it considerably more cumbersome, and slower. That last part may be surprising. My workloads spend almost all their time in Numpy, and nothing beats those highly-optimized BLAS libraries that Numpy relies on. But I'll keep watching. Julia is slowly getting less cumbersome, and in my latest trial, I didn't hit a single bug. It's a young language, and might still prove worthwhile in a few years.
&gt; and preparing Excel reports for my team from SQL queries If you blogged on that I think you could get quite a few hits :)
&gt; and nothing beats those highly-optimized BLAS libraries that Numpy relies on. Interesting observation given Julia calls the exact same libraries... 
No on every operation, no. As far as I understand, Julia does many basic operations such as additions and matrix multiplications within Julia itself.
Nope, matrix multiplication is the exact same library. But addition indeed doesn't use BLAS but instead Julia's broadcasting handles BLAS Level 1, but that always benchmarks as faster for me. I wonder if you have an installation issue. Did you rebuild your system image to your architecture?
What uses cases do you have for numerical optimization in business analytics? Just curious because it was the focus of my masters.
I do a PhD in Machine Learning, and use both python and julia. Because it gets messy, I try to avoid using any other languages although I guess that I wouldn't mind writing some Fortran again, it just hasn't been necessary for me for a while. I prefer to use julia for pretty much everything, unless there's already a clear winning library in another language and it would be non-trivial to implement the functionality myself. I use python because of pymc3, which (on the GPU) is the fastest/best thing around (for me) for Monte Carlo sampling. If I were working on huge neural networks I'd probably be using tensorflow in python. I wrote a simple julia script yesterday, and I have to say that the startup speed gets annoying, the program takes a few seconds to start but my stuff takes 0.2 seconds), I guess I should also use python for that use case.
Came from MATLAB, and also am a C++ dev. Tried Julia a few times, here and there, and ported a large in-house app to it. Still use MATLAB though, for two main reasons: 1) The debugger. 2) Quick interactive plotting is superior, and built-in.
So they still haven't fixed that startup issue? My use case is many small sctipts run many times, which is why I never got very far into julia. 
The startup time is much less of a problem now than it was in 2015, when I first used julia. Back then I thought it was a nice language, but decided to wait because loading packages was incredibly slow. I just wanted to generate some plots from data I already had, and I literally had to wait 30 seconds for Gadfly to load to then find out I had a basic error somewhere below the import statement, so that was a dealbreaker for me. Loading time of packages is much, much better now they have pre-compilation. But still, if you have to execute many scripts which individually only do up to a few seconds of work, then startup time will be your dominant time sink. The startup time is sort of inherent to the JIT-nature of the language; in every process the compiler will have to compile the routines you call again. This part is just unsollicited advice and I don't know your workflow: If you write your julia code somewhat modularly (e.g. with a main() function instead of doing everything in the top-level) then it shouldn't really be more effort to do the different runs (or at least a subset of them) from within a julia script instead of, say, a bash script. Whether you call a script written in julia from bash or the function corresponding to that script from julia shouldn't be much different in terms of how much text you have to write to specify your runs.
&gt; The static compilation question is being addressed, and is already solvable. Do you mean [PackageCompiler](https://github.com/JuliaLang/PackageCompiler.jl)? It seems to be updated since last time I checked it. &gt; I don't mind this, personally, because it also comes with more complete control, fixing whatever's gone awry is usually within the capability of anyone who can write the language. Sure, yet you just get interesting surprises sometimes. But again, it is the price of flexibility. BTW, I can give an example where Fortran and MATLAB are simpler. In Fortran everything you do with arrays, works on a reference, at least semantically^1. In MATLAB - if you mutate inside the function - you copy. In Julia sometimes you create a copy and sometimes you work with the reference (with `@view` for example), and you need to keep track of what you are doing. ^1 interesting exception is passing non-contiguous assumed-shape (`A(1:100:2)`) to assumed-size dummy (`A(*)`), in which case a contiguous copy is made, passed to a function and then copied back to the original array.
&gt; Sure, yet you just get interesting surprises sometimes. This line is my problem. It should not be a surprise, ever. And Julia doesn't have nearly as many if the programmer is paying attention. With other languages, even that is not enough. Julia at least has a convention to append a bang to the end of a function name if the function is meant to change the values of its arguments. With Matlab, you don't know exactly what algorithm you're using half the time for various calculations. Things like C++'s undocumented functionality or your Fortran example are essentially non-deterministic and basically outside the control of the programmer. To compare surprises from not paying attention versus those from not knowing exactly what code is being run is a kind of false equivalence, right up there with "all Turing-complete languages are equally productive in practice". That said, I agree with the general sentiment of your post, and domain-specific languages will frequently make certain things more convenient than in languages with more bases to cover.
Yeah, a large chunk of what I need to do with an interactive numerical language is quickly plotting stuff, editing a function, then replotting. Octave / MATLAB works so quickly and smoothly for this — no need to "use" packages or otherwise bring in code other than be in the same directory as the code you're prototyping, no need to restart the REPL just to try out a edited function, no need to wait for plots.jl to recompile every time, etc. I gather there may be better things in the works (Revise.jl?) but Julia still seems more oriented for building proper applications at the expense of some convenience in quick-and-dirty investigation / prototyping, which is what I need above all. I regretfully returned to Octave after a year of trying to do it in Julia.
You haven't needed to restart Julia when editing functions since v0.6 was released. These days you shouldn't need to restart the REPL for pretty much anything, other than to test what a clean slate is like (just like any other dynamic language). Revise.jl and Juno's built-in handling do a bunch of nice stuff for you. Here's an example using Juno for package building in a way that doesn't rely on restarting: https://www.youtube.com/watch?v=i5iGVkI7XOI
But I emphasize that that's not the case anymore!
This is why a pure interpreter and static compilation is being created. This stuff works but doesn't have good tooling yet. It wasn't the focus because you can do everything with a JIT, though it is annoying in some cases. But there's a lot of work getting this tooling together for a 1.x release, along with caching of native code during precompilation.
As a relatively new Julia user one thing I still often struggle with is type stability. It is very easy for me to write code that would run fast in c/fortran, but not in Julia. (And it can be subtle to find the problem sometimes.) For example I recently had a function that set a parameter to be a floating point value, and then added integer values to it in a loop. Something of the form: a = 1.2 for i = 1:10 a = a + i end return a Of course this is easy to fix, but it is also something that is not a worry in many many other numerical languages, so a common source of bugs for me...
The way Julia's workflow goes is getting your code working properly, then adding type information so it runs fast. There's only so much it can do without more information. C++ guarantees this information is present in code because it won't even run without it. It won't do much of anything without you telling it in excruciating detail, and it doesn't allow you to do anything interactively. Julia tries to make do with whatever you hand it, interactive or not. As for your examples, I think you can declare/force the type of i in the first example so it's already a float come arithmetic time (removing the need for costly conversions every loop), and I'm guessing the parameterized struct runs faster/better than the simply declared struct, right? 
For my first example, "i" was actually an index into a vector of integer values. I could wrap the value of the vector at a given i index in a convert statement, but I think that was slow too... For that example, I don't see why Julia when it compiles the function with that code can't determine that I'm only doing arithmetic with ints (which should promote to a float), and hence the type of a shouldn't change. I understand more generally why it could have trouble mixing types, but if a variable is created as a float and only used in arithmetic with ints and floats it seems like there shouldn't be any issue figuring out what to do. For the second example you are right; putting in the parametric types gave a significant performance benefit.
Yeah, if that's really a vector of Ints then that example should be fully optimized already... there's something missing in the description. Was it `Vector{Integer}` instead?
No worries, I updated the code to be closer to the real example, so it's been changed since you originally looked at it... The function takes a Vector{T}, so when called with a Vector{Int64} as input it should specialize and know all the entries in the vector are Int64. My conclusion was that I did find a case where it just wasn't smart enough to work out the types (despite them seeming obvious to me). Mixing floats and ints is a pretty common operation though, so I hope this improves as time goes on... 
It was a month ago, so perhaps I'm misremembering the function's exact setup. I thought the function was taking a Vector{T}, where what I passed in was a vector of Int64. If I can find the exact older code I'll post it. I just remember that one change seemed to fix everything; setting a to be 1 so everything was done with Int64s, and then returning 1.2*a at the end of the function.
There are fallbacks for generic array types, sure, but your usual matrices of floating points will for sure be done in BLAS. There's work (semi-active) on Julia BLAS functionality, but right now, it will use the same BLAS'es as python can/does (unless you don't)
Well, I'm having that student do DiffEq work for now. In two years or so we can give him a graduate student position to go build JuliaBLAS. Just for the reference, he started it in high school and has done quite a bit. Julia base needs some compiler optimizations and memory buffers for the whole thing though, which is why I think it should be tabled for a little bit. But yes, it will be super cool when it's a thing. 
I wonder why Julia is half as fast as Numpy, then.
April's fool?
i wish there was something faster than tetrisfriends or if they could change it into html5. flash is obsolete. 
Before I started using Julia for any ML &amp; Data science related stuff I was usually using Python3, heavily backed by various C and C++ libraries, the difference I'd say are: -&gt; Speed, Julia is fast in both a single threaded and multi-threaded context and, more importantly, making any single operation parallel is trivial -&gt; Library ecosystem. Julia's library ecosystem is much smaller, but the overall quality is much better. Other than that, it's syntactically similar enough for me not to be able to prick a clear favorite (for example, I prefer python if/else/with and I like julia's macros, but those difference). I could compare it to other languages, but I believe comparing it to python3 is the fairest comparison one could make and really showcases the are where Julia shines. Comparing it with a DSL (and even worse, a closed source DSL) like R, Octave or Matlab, will lean to heavily towards Julia, since it's a fully fledged programming language and had that goal to being with. I'm actually very curios what non math/simulation/machine-learning related work will be done in Julia once it's released. I think that a language as expressive as python or Lua but as fast as well-written JVM code (and in some cases almost as fast as well written C/C++/Rust code) can be a really powerful tool for many domains.
Couldn't tell you without code or details about your system.
&gt; Until Julia v1.0 drops next Wednesday &gt; April 1. Why do you deceit me so.
Agent-based modeling tends to require a lot of loops over user-defined objects/types. User-defined types in Julia are optimized like anything built in (almost all of the types in Julia are actually defined using Julia. For example, Julia's Base module actually defines floating point numbers and stuff inside of Julia!). Those two facts combined mean that these kinds of models can be hard to optimize in vectorized environments but will optimize just fine in Julia. That's ripe for good numbers if you make it type-stable.
[You really think someone would do that? Just go on the internet and tell lies?](http://i0.kym-cdn.com/photos/images/original/001/122/453/896.jpg)
I do ABM stuff for my PhD work as well. It works very well if you can make your code type stable and the built in parallel programming constructs make it a breeze in a server/cluster setting. Definitely port your code over from matlab to Julia. It's going to be much faster as long you can keep it type stable. `@code_warntype` and profiling tools are your best friends 
I switched from Matlab/Octave to Julia to code both ABM or PDE solver. So far, so good. Debugging can be slightly more complicated, but otherwise the transition has been quite smooth. It's a blessing not having to re-code everything in C or Fortran when computations become demanding.
A project based off of [papermill](https://github.com/nteract/papermill) that allows you to pass variables and execute julia jupyter notebooks.
I'd recommend *always* glancing over the documentation of any library you plan you using, since you may find not only extra examples but also functions you didn't know existed and may end up solving your problem! You can find the documentation of ForwardDiff [here](http://www.juliadiff.org/ForwardDiff.jl/stable/). You'll see a function called derivative! Hence, x0 = [2.0,3.0,5.0] derivative(x0::Float64) = ForwardDiff.derivative(x -&gt; x^2, x0) map(derivative, x0) should give you the derivative of x^2 at all x0 points
If the function you want to evaluate is f(x) then you get its derivative with ForwardDiff.derivative(f, x) but your example doesn't exactly make sense. You can't square a vector, nor can you differentiate with respect to one (or, if you do, then you're taking a gradient or a jacobian), so it doesn't make sense to take the derivative of x^2 at x = [2, 3, 5]. Or do you want to take the derivative at each of those points? That would be: julia&gt; f(x) = x^2 julia&gt; using ForwardDiff julia&gt; ForwardDiff.derivative.(f, [2, 3, 5]) # note the . before ( to indicate that the function should act elementwise on the vector [2, 3, 5] 3-element Array{Int64,1}: 4 6 10 
Thank you for the examples! It's helped me get the idea of what the syntax should look like, and everything works now.
Update: Added instructions and a makefile to help with the installation!
[MLBase.jl](https://github.com/JuliaStats/MLBase.jl) needs more work in [performance evaluation](https://mlbasejl.readthedocs.io/en/latest/perfeval.html). Missing lots of stuff available in [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) (multiclass reporting, score weighting, etc).
Could you elaborate on "interned"?
I'm a bit late to the conversation here but there are ways to call Python ([PyCall](https://github.com/JuliaPy/PyCall.jl) with [Conda](https://github.com/JuliaPy/Conda.jl)) and R ([RCall](https://github.com/JuliaInterop/RCall.jl) and [how to use RCall](http://luiarthur.github.io/usingrcall)) from Julia.
symbols are interned – i.e. hashed by the language implementation for fast equality comparisons From https://stackoverflow.com/questions/23480722/what-is-a-symbol-in-julia
symbols are interned – i.e. hashed by the language implementation for fast equality comparisons From https://stackoverflow.com/questions/23480722/what-is-a-symbol-in-julia
That is indeed correct. They are both interned and bits types. Strings are not interned and are heap allocated. Using symbols will be use less memory and result in faster operations. Of course, they are less flexible which is the tradeoff, but if you have a finite set of choices then they are the right thing to use. 
I'd say Stefan Karpinski's answer on Stackoverflow is a lot more enlightening than just mentioning the “interned” part. He also mentions that this alone doesn't make equality coparisons of symbols faster, as strings in Julia are immutable and interned. His long answer is a worthwhile read. 
Where does it say that strings are interned?
I genuinely think this library is going to be one of the capstones of Julia. I hope you know that we’re all incredibly appreciative of the work put into this.
I concur!
You are right! I’m sorry. I misread a line in Stefan’s explanation.
No prob. Thanks for pointing out the rest.
https://en.wikipedia.org/wiki/Topological_quantum_computer
Awesome work... :) I am looking at this as something I can learn from....
That readme looks great. I clicked on a random file and this *screams* multiple dispatch: https://github.com/giob1994/Alistair.jl/blob/master/src/linregress.jl#L8
Came to my mind that I should definitely use Julia’s multiple dispatch to avoid that kind of infinite if-else statements... but I still have to think about it! I do like the fact that the name of the function reflects the specific regression model we are solving for - so that it could be called directly - but multiple dispatch *might be faster* in execution... I don’t know! Certainly would look cleaner. Surely the design of the internal code is still in the works, so I appreciate the comment!
I don't think it really matters for performance but it will be much cleaner. For the naming the way to think about it is that in language that have bad/no OOP support (e.g. matlab) you put the type in the name like you did: ols_linfit In traditional OOP you do: OLS.linfit And in Julia: linfit(reg::OLS) The big advantage if you do it well (which is not always easy) is that you need to write a single, generic function; all you X_linfit have similar code, so you should be able to write a generic linfit(reg::T) where T &lt;: AbstractRegressionType. The parts in common (like the variance ones) go into a method defined for AbstractRegressionType, so you just have to write the parts that are actually different for each subtype.
This is great advice! Up to now I haven't really tried my best to dive into the full power of multiple dispatch and Julia methods, but I'll definitely work on it and try to make the code cleaner and (possibly) more elegant than it is now :) Thanks for the feedback!
Yeah, I could do that. And I do use SSH keys for git stuff already. But that just isn't the behavior I want. 
No, that's reasonable. In your case, you might want to consider just waiting for Pkg3 (coming with the next Julia release) which will not do a full git clone for every package and might be more compatible with what you want: https://julialang.org/Pkg3.jl/latest/
Why not just make a different directory to put your package in and add it to the load path? I have the following in my .juliarc: const LOCAL_PACKAGES = expanduser("~/julia-folder/") push!(LOAD_PATH, LOCAL_PACKAGES)
Interesting! Thank you very much for the information and suggestions.
Yeah, this is my current temporary solution. I only have a handful of personal code packages, so I guess I shouldn't complain. Thank you for the suggestion!
Damn, is it now &gt;1 release per week? You're doing great work! Literally my biggest problem with the package (and thats not much of a problem at all) is that it got *too many* features: Should I use a parameterized function, or my own callable type, or my own DEDataArray subtype, or an ArrayPartition, a MultiScaleArray or one of the other specific models for my problem? (I tend to go the DEDataArray route). Which of the approximately one billion solvers do I feel like using today? And which choices come with what performance costs/benefits? If you manage to streamline this a bit more, I'm sure this is going to blow up, come 1.0. (If not, it's still going to blow up, just a bit slower ;) ). It's easily (one of) the coolest Julia package(s) out there!
Yeah it's rough because they all do something different and are optimized for different cases. More tutorials helps, but I agree there's an information overload. With the automatic switching methods and improved defaults, I think that in the near future most users shouldn't need to dig into the algorithm choices, though if you're solving a PDE or a parameter estimation problem you'll want every last drop of efficiency and should benchmark different choices. As for array types, I think that most cases a user should default to arrays and a parameter array. You can optimize this a bit more with static arrays, or make your parameters time-dependent and saved with DEDataArray, or do weird fully dynamic stuff with MultiScaleArrays, and it's all available, but I think Array+parameter is the common route. Genericness is an interesting documentation problem though. There's a ton of interfaces all documented, but this leads to docs bloat. And due to the fact that everything just generically composes, there's a combinatorial explosion of what you can talk about. Documentation is definitely something we need to keep working on, but I think it's greatly helped by relying on tutorials. The docs should be a complete reference, whereas the tutorials should be the "dive in point".
Isn't using a maximum likelihood method almost uniformly faster for even basic OLS? Or am I mistaken on that?
To be honest I am unsure. Technically, Basic OLS is found as the minimiser of the residuals, and being close form - of the data is not in the order of hundreds of thousands of observations/regressors - it means it can be easily and speedily computed via simple matrix operations. On the other hand, numerical maximisation of likelihoods is usually slower (depending on the solver) and has its own issues, like convergence and locality of the minimiser... It’s different approaches really.
The nicest way would be to add it to your system image. If you do that, then it'll also cache native compiled code and not have any JIT cost (for the cached function calls). PackageCompiler.jl helps with this.
Survival is missing!
Thanks for the really interesting reply. I've been playing a little more with Julia since I posted, and it's nice to see such clean syntax (coming from R) when dealing with arrays - I like how multidimensionality doesn't seem to be an after-thought. I'm definitely excited to see where Julia goes.
You can use an array to unstack on: julia&gt; unstack(df, [:patient, :receptor], :cell_population, :value) 4×4 DataFrames.DataFrame │ Row │ patient │ receptor │ NK │ T │ ├─────┼─────────┼──────────┼───────────┼──────────┤ │ 1 │ A │ Fas │ 0.26955 │ 0.756819 │ │ 2 │ A │ PD-1 │ 0.149255 │ 0.322147 │ │ 3 │ B │ Fas │ 0.0721714 │ 0.572162 │ │ 4 │ B │ PD-1 │ 0.475619 │ 0.165021 │ And groupby if you want to separate them: julia&gt; groupby(unstack(df, [:patient, :receptor], :cell_population, :value), :receptor) DataFrames.GroupedDataFrame 2 groups with keys: Symbol[:receptor] First Group: 2×4 DataFrames.SubDataFrame{Array{Int64,1}} │ Row │ patient │ receptor │ NK │ T │ ├─────┼─────────┼──────────┼───────────┼──────────┤ │ 1 │ A │ Fas │ 0.26955 │ 0.756819 │ │ 2 │ B │ Fas │ 0.0721714 │ 0.572162 │ ⋮ Last Group: 2×4 DataFrames.SubDataFrame{Array{Int64,1}} │ Row │ patient │ receptor │ NK │ T │ ├─────┼─────────┼──────────┼──────────┼──────────┤ │ 1 │ A │ PD-1 │ 0.149255 │ 0.322147 │ │ 2 │ B │ PD-1 │ 0.475619 │ 0.165021 │
Thanks for that gibz. Good to know that function, and it looks like it can deal with several columns, so theoretically can deal with more dimensions. Would be nice if there was a more generic / elegant way to get the data into a multidimensional array. Maybe I should try porting over* the acast function from R's reshape2 library. Strangely it looks as though reshape2 is being deprecated in favour of tidyr in R, which appears to lack the acast function. The source code is written in R, but calls several other functions. The license appears to be MIT and is written by Hadley Wickham. *I have never done this before, that might be biting off more than I can chew.
Use a console emulator like [Cmder](http://cmder.net/) (a for of Conemu, with git integration) and enjoy lots of color profiles, fonts and a lot of other functions to choose from, or make your own.
You can set Conemu as your default console, so that it gets always loaded when you start your console. You can also set up your own console environments and parths for different uses. Conemu is very versatile in that respect.
Hey, Looking forward to ABC implementation! Can you hint of what is planned on this front? Thank you for this great package anyway!