Oh, that's easier! There's two ways I can think of. The easiest one: you can create a `lazyView` function that adds caching to an existing view function; the signature is a little different than the one you're trying. My example asumes you're only interested in caching the view for the latest state: ``` let lazyView (view: 'state -&gt; 'args -&gt; View) : 'state -&gt; 'args -&gt; View = let mutable cached = None fun state args -&gt; match cached with | Some(lastState, lastView) when lastState = state -&gt; lastView | None -&gt; let view = view state args cached &lt;- Some(state, view) view ``` Now you define your cached views like this: ``` let viewA = lazyView (fun state dispatch -&gt; (* ... *) ) ``` Of course, if you wanted to use the same view multiple times inside other views, you could apply `lazyView` for each cached instance of the view instead of globally: ``` let viewA state dispatch = // ... let viewB = let cachedViewA = lazyView viewA fun state dispatch -&gt; // ... ``` You may need type annotations to overcome the *value restriction* if you try to make them generic, though. Plus, you may want some variations of `lazyView` that cached over sequences, etc. If any limitations of this are a problem, or you wanted something more general, you could instead look at the hard way. Look at implementation of [Fabulous' `dependsOn` function](https://github.com/fsprojects/Fabulous/blob/4d48f8085ddb84c2013dce52c0e79d083362435b/src/Fabulous.Core/ViewHelpers.fs#L30) which uses a global cache indexed by a state-key and the `func.GetType()`, which is different for each defined F# function, to identify the calling function. This may be closer to what you want, but I think the mechanism is a little messy.
Thank you, this looks interesting.
&gt; If it was possible to enforce a function to be pure they would be comparable i guess 1. The compiler isn't required to generate equal IL code for two arbitrary functions, so a string comparison of IL code isn't going to happen. 2. The compiler can't perform a code analysis to statically compare the meaning of the code (i.e. check if [the domain and the image](https://en.wikipedia.org/wiki/Domain_of_a_function) of `f` and `g` are equal) because it would need to solve [the halting problem](https://en.wikipedia.org/wiki/Halting_problem). &gt; would you be interested in having this ? No. &gt; is it just me Probably no.
**Domain of a function** In mathematics, the domain of definition (or simply the domain) of a function is the set of "input" or argument values for which the function is defined. That is, the function provides an "output" or value for each member of the domain. Conversely, the set of values the function takes on as output is termed the image of the function, which is sometimes also referred to as the range of the function. For instance, the domain of cosine is the set of all real numbers, while the domain of the square root consists only of numbers greater than or equal to 0 (ignoring complex numbers in both cases). *** **Halting problem** In computability theory, the halting problem is the problem of determining, from a description of an arbitrary computer program and an input, whether the program will finish running (i.e., halt) or continue to run forever. Alan Turing proved in 1936 that a general algorithm to solve the halting problem for all possible program-input pairs cannot exist. A key part of the proof was a mathematical definition of a computer and program, which became known as a Turing machine; the halting problem is undecidable over Turing machines. Turing's proof is one of the first cases of decision problems to be concluded. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/fsharp/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
interesting, will check out the links later..
In my understanding, Elmish is a way of connecting UI and code with a Model-View-Update pattern. You can apply this style to any type of App. No matter it is a desktop app, a website, a mobile app, etc. Avalonia is a UI framework which does lots of tasks. It comes with XAML, a markup language (as a web dev, you can think of it as HTML and CSS somehow), built-in Very basic UI controls (see it as built-in HTML tags), also with complicated UI components that built on top of the basic ones (you can think of it as React components), built-in APIs for interacting with UI and system APIs (like how JS calls browser and modifies DOM). Avalonia.FuncUI is bringing Elmish MVU style to Avalonia apps, so you can write apps in the style you already familiar with (Elmish), and is also better and concise.
The function equality part has been addressed, I'll add this v &gt;Is there a way to prevent F# functions from accessing outer (non global) scope ? If you make it a module function, it can *only* access its parameters plus the global scope. The only other scopes left are class scope (used by methods) and function scope (used by local functions or lambdas). But "global scope" is too broad for your purpose. `DateTime.Now` is in the global scope but a function that uses it isn't pure. Impurity is contagious. To know that a function is pure, every single piece of code it invokes, however indirectly, must be pure. You might have `if stringA &gt; stringB` somewhere in your function and suddenly you're impure because string comparison is culture-aware by default. The only way to have compile-time purity enforcement is to start from the bottom, and annotate the entire standard library and every library you use for purity or non-purity.
I'd love to start using this, but I think using VSCode in Linux will make me a third class citizen. Has anyone done any development work with this, or even just Xamarin, using Linux, VSC, dotnet-core etc?
Oh, good! Reddit decided to delete my .gifs upon submitting.
Whether typescript or f# I have to reload the window every now and then to get the language server back on track. I think I still like vs code more than other options but I'd love it if it didn't poop itself so often.
i hope you have a lovely day stranger
Last week I walked a coworker through installing VS 2019 and creating a simple console F# project with one dependency from Nuget. Literally almost the simplest thing you could do, and it would not work (namespaces not recognized, then link to dependencies broken and code would not compile). Then I had the coworker install Rider and follow the *exact same series of steps*, and of course it worked fine. &amp;#x200B; I'm on Rider now.
If we're gonna bitch about f sharp lets bitch about the fact that the best option in F sharp for database migrations is to do them in C#
Are you using the dotnet core tool directly from the terminal to generate these projects? I've had really bad luck when trying to generate projects through other means, both on linux (manjaro) and windows 10 (latest, non-insiders). The vscode experience for me is alright (not slow, interactive works fine) after the project is setup. I'm not using any outside dependencies though. Another issue I've hit was that, on Windows, installing the dotnet core sdk directly did not work at all, I had to remove it, and install via Visual Studio 2019.
The test project, yes, I used dotnet. The results were the same.
[removed]
I haven't burnt off a day on this for a little while, so maybe the situation has improved, but: it's tragic and kinda amazing to hop back to projects that are over 3 years old and see how much more robust the solution tooling was, and how elegant the scripts were... The combination of .Net Core breakage/churn and related F# tooling breakage/churn has been pretty rough. Despite being *ideal* for the kinds of work I do, and long awaited, in hindsight it would have been much more productive to ignore everything past 2017 and just stick to Mono.
Have you filed issues either in the Ionide, FsAutocomplete, or F# GitHub repositories demonstrating your issue? If not, please do. Thanks!
I'm just trying to use FluentMigrator in F# wish me luck :-)
Hi just trying to use fluentmigrator in f# wish me luck :-), I'm dad.
&gt;Rider MacOS user here... I haven't tried Rider but its crazy that 3er parties could be able to produce better tooling than MS! who invented this technology, probably there is a big lack of resources assigned to F#, Golang support is being maintained by MS, they are really fast solving issues reported in github, that's sad I think. In my case, the only way that worked for me setting up projects has been so far manually (I've tried Ionide tooling and VS2019 with bad experiences): \&gt;dotnet new console -lang F# -o myApp \&gt;cd myApp; paket init Then open it with Ionide, adding external deps manually to paket.dependencies... And editing manually restAPI.fsproj to import modules. I'm just starting so I hope the road won't be so hard as so far.
The sad thing is that .NET Core was made necessarily mostly because .NET 3 onward was turned into a Win32-only, unmaintainable, monolithic mess when they decided that WinFX (WPF, WCF, WinFS etc.) should be dumped into .NET and called .NET 3.5. I even wrote [a petition](https://www.infoworld.com/article/2658821/winfx-name-change-found-irksome--petition-circulates.html) begging them not to do this, and warning of the exact situation we're in now (it doesn't exactly take a prophet to figure out), and the .NET team lead responded by telling me that it was a marketing decision.
I'd love to contribute, and I have in the past, but it's just... Where do I even begin? Like, is a bug report saying "getIonideLogs doesn't work on either of my two machines and I have no idea why" really going to be useful at all?
just want to say you are beautiful
Everything at Microsoft is a marketing decision. That's the problem.
I haven't played around with this yet but it looks like great work. There's definitely a lot that can be done on the DL side to make research faster/more accessible (e.g. it should be a trivial one-liner to extract a layer from a pretrained model, connect it to your own custom layers and have the shapes statically checked in your IDE). I also wonder if computation expressions could be used for custom kernels. I was actually going to explore type providers for something like this - would it be possible to extend your efforts to achieve this? Also it looks like you gave a presentation at Compose - is there a recorded video around? The notes were useful but I always like to listen to the developers.
I think that a chunk of the challenge is that F# was created as the first ground-up .Net first language... Part of the beauty is that it's so informed by the .Net internals and provides such appealing access to its power. But the day-to-day is also reliant on fairly mature parts of the framework that are outside the core VM. That's no problem at all, unless you nuke that framework from orbit and replace it with something that's distincly less capable. I just wanna script again, with ease, and with confidence that I can do the stuff I saw in demo's a decade ago.
I thought C# was the first ground-up .NET language? F# has vestiges of OCaml, and it lacks some of C#'s OOP capabilities. Aside from type providers, what does F# rely on .NET for? I know F# was kind of the test bed for a lot features that are popular in more modern languages, even though it's rarely mentioned in the "Inspired by" sections on Wiki. That was reason enough for me to stick with it for a decade despite the frustrations.
C# was being hammered out in concert with the VM, inheriting a lot from its spiritual predecessors and focused on bootstrapping a sustainable ecosystem. I'm not 100% on all the specifics, but the research around F# is where .Net got generics and a bunch of language features like computation expressions (ie LINQ), and lots of work for the dynamic language runtime was already underway and available. So F#s design is based around a mature understanding of how .Net handles generics, types, libraries, references and contains mature solutions on how to handle the advanced language capabilities as they were designed knowing what they'd be. It restricts itself to a single-pass compiler to service Visual Studio Intellisense within the speed requirements. It exploits the live VM in its tooling (ie FSI), and uses the VMs dynamic nature for basic language capabilities. C#s design preceded most of that. The language itself is quite similar to OCaml, inheriting their mature solutions to functional programming, but is significantly refined and altered for ease of use with/on .Net. As a .Net language it has equivalent/greater OOP capabilites than C#.
That's a good point about C# being a bootstrap language and F# being the first "true" .NET language. Now that you mention it, I remember a Channel9 video where Anders Hejlsberg of the C# team was proudly showing off his Cup&lt;T&gt; mug with the C# 2 launch, talking about the research they did with F#. People back then were saying C# 2.0 was "kind of the real C# 1.0, as tends to be the case with Microsoft."
Hi the video is here: [https://www.youtube.com/watch?v=3zdlQ\_HjKl4](https://www.youtube.com/watch?v=3zdlQ_HjKl4&amp;list=PLNoHgLVTxtaorTczyo8NA3tg_vK8WC5rD&amp;index=20) Type providers are heavily used in Sylvester to provide a convenient syntax for constructing objects e,g `let x = Mat&lt;400, 700&gt;.Rand` I also think computation expressions would be great for things like creating kernels and I mention in the presentation the desire to utilize the meta-programming facilities in F# like type providers and computation expressions for scientific computing.
ATM I am rewriting a feature in some C# code that I inherited that synchronizes a list of sheets in a Revit model with a spreadsheet. Missing sheets in one have to be copied to the other. User can specify a direction for individual properties to be sync'd for matching sheets. The original code sprawled over 4 or 5 projects, and the models had properties that were being mutated at various points along the path. I was having a hard time understanding the separation of concerns between all the different moving parts, and it was not clear who was mutating what. Since I've started going down the FP path I feel that my patience and willingness to unwind these kinds of solutions to debug and understand has dwindled. But the good news is that I think I am close to being done rewriting within a day, and it all lives in a single module now! In a very satisfying moment, I added the final function at the bottom of the module that ties it all together, and I thought, "IT IS DONE." &amp;#x200B; /// Synchronizes sheets between Revit and Excel according to the user selected property sync directions. let sync uiDoc excelPath toExcelProperties toRevitProperties cfg = &amp;#x200B; let revitSheets = getSheetsFromRevit uiDoc let excelSheets = getSheetsFromExcel excelPath cfg &amp;#x200B; mergeSheets revitSheets excelSheets |&gt; syncExcelToRevit uiDoc toRevitProperties cfg |&gt; syncRevitToExcel excelPath toExcelProperties cfg &amp;#x200B; I feel like that's pretty clear! It also pleases me that "Revit" and "Excel" have the same number of letters. :)
&amp;#x200B; Success or failure? You make the call!
Going through Ionide's repo to see if I can help.
Building a library that enables Elm like Apps in F# + Avalonia! https://github.com/JaggerJo/Avalonia.FuncUI Really happy with what I have right now. Feedback is appreciated!
Donâ€™t ever let anyone dull your sparkle :)
Awesome.
I'm writing an elmish layer over the Veldrid framework/IMGUI so I can learn MVU and make vulkan powered applications and games. I'm having fun!
do you have a link to share ? :)
link ? :)
Sorry for the late reply here - quite busy! If getting logs doesn't work, that's definitely the first issue to file. Bar none, getting diagnostic info should work. Separately, any way to reduce it to a small codebase helps tremendously. This goes for both the Ionide repo and the dotnet/fsharp repo. Speaking for the F# repo, since there's already a lot of activity (literally thousands of closed issues!) it might be difficult to find a duplicate, or if something has already been fixed (but not yet released). So feel free to file an issue anyways and we can take care of it from there if it's an issue.
&gt; It restricts itself to a single-pass compiler to service Visual Studio Intellisense within the speed requirements (among other things) Single-pass has little to no bearing on tools. In fact, we're re-architecting much of the internals of the language service to use a completely different model that starts with incremental lexing of source code and leads to immutable snapshots of the known universe that all tools work off. The number of passes a compiler may or may not do is immaterial here. It's just a design decision made a long time ago. I imagine it's for useful reasons, like how file ordering is required so that compile times won't explode.
There are a bunch of independent reasons why a single pass compiler is A Good Thing, but in context I'm almost positive I've seen DSyme specifically point to Intellisense times as one of the and less obvious ways that F# is more bound to .net framework than people think, and as a design criteria that influenced that decision on the compiler... That's based specifically on the impact of exploding of compile times you pointed out. Comparable VM-bound functional languages like Scala that didn't have that design constraint couldn't match the same live tooling functionality of C#. A hard sell for an MS backed language in VS, and a pretty 'useful reason' to match it ;) I hope the situation will be better in 2020, but in 2004 there weren't too many roads to matching C#s in-IDE performance for functional languages, and well worth the sacrifices IMO.
This is so awesome. XAML in F# feels so wrong. Definitely gonna try this out.
great to hear!
&gt; There are a bunch of independent reasons why a single pass compiler is A Good Thing, but in context I'm almost positive I've seen DSyme specifically point to Intellisense times as one of the and less obvious ways that F# is more bound to .net framework than people think, and as a design criteria that influenced that decision on the compiler... I'm unaware of such rationale. These things are independent: requirements of running on the .NET VM are not related to the number of passes in a typechecker. IntelliSense is not related to number of passes in a typechecker. And IntelliSense is not related to requirements of running on the .NET VM. &gt;That's based specifically on the impact of exploding of compile times you pointed out. Single pass and file ordering are also independent. It's entirely possible to build a compiler with multiple passes and achieve the same outcome as we have today. It's also possible to have an "eventually consistent typechecker" that could remove the need for file ordering, probably using multiple passes to some degree. These would also be independent. I just want to reiterate: there were some design decisions at some time that resulted in things like the typechecker doing a single pass, or file ordering being a requirement. The latter has certainly been retroactively been viewed as a feature. But neither outcome is a necessity for IntelliSense tooling nor running on .NET. I don't believe that was ever the case. It's just a consequence of working in a complex area with millions of nuances. Decisions get made and they often are eternal. For fun, you can read about why the C# team forever has to maintain a [3-SAT solver](https://blogs.msdn.microsoft.com/ericlippert/2007/03/28/lambda-expressions-vs-anonymous-methods-part-five/) to resolve overloads; something that I'm sure most would agree isn't exactly necessary aside from not inducing breaking changes.
Have a common test project that does nothing but add those packages, then reference that project.
&gt; These things are independent: requirements of running on the .NET VM are not related to the number of passes in a typechecker. IntelliSense is not related to number of passes in a typechecker. And IntelliSense is not related to requirements of running on the .NET VM. I haven't at all said or implied that they are... I think you're disagreeing with something you've read into my comment, not what was stated :) &gt; But neither outcome is a necessity for IntelliSense tooling nor running on .NET. I don't believe that was ever the case. I highly agree with this statement :) What I said, though, was that a language that was designed after its language services and IDE took their time requirements into consideration... You've called those 'useful reasons', 'nuances', and 'design decision'. I agree, but called it a 'design criteria'. And it's a design criteria that was met.... As you very rightly pointed out, if you do it the traditional way you end up with a compiler with cooler types but that also can't support the dynamic VS language services in 2004. It's something that sets F# apart, for this (among many!), reasons. I can't be bothered to dig for it, but I'm rather sure I've seen it specifically pointed to as part of the design rational for that decision. So unless we disagree that that F# came after .NET and Visual Studio, or that a single pass compiler isn't way faster, or that the F# team didn't consider Intellisense or Visual Studio when designing the language... ... Â¯\_(ãƒ„)_/Â¯
But we donâ€™t accept issue reports from assholes :-)
Having them separated seems like a better approach. What is the problem with having multiple libs installed?
Because I create a project for each challenge, I need to re-install those libraries for every project.
This sounds like a prefect solution, I'll try that. Thank you very much.
Benefit: You can also wrap around a few common test methods. And add more packages like `Swensen.Unquote`. So you only have to add one project reference and have everything set up.
I'm trying to help here, but you're seriously making huge assumptions about people's workflows if you think people don't need working stack traces. If you want to reject issues because you think the person making the report is an asshole, you're doing yourself and your users a huge disservice. I never resorted to name calling either, by the way. So I'm not sure I'm the only asshole here.
I think creating a new project for every challenge is too much. I think a new module is the right approach. About the test, use the "Traits", in NUnit it is the "Category" class attribute. Example \[&lt;Category("challenge\_1")&gt;\]
Porting Fornax to .NET Core and making it a global dotnet tool. `fornax build` works, and all the tests pass. I think the only thing broken now is that Suave is having trouble serving up the site when you run `fornax watch`. Progress here: https://github.com/NatElkins/Fornax/tree/netcore
You can bundle your app with a (stripped version) of .Net Core, self contained and running on its own.
We're talking past each other here. My point is _only_ that single pass typechecking is completely independent of IntelliSense. This statement: &gt; It restricts itself to a single-pass compiler to service Visual Studio Intellisense within the speed requirements (among other things) Is not something I agree with, nor can I find any evidence that this is true.
Out of curiousity, what are your concerns about multiple modules in the same project? I don't know how large the challenges are but unless they are extreme, I would be inclined to try that out - modules (or namespaces) are much quicker to create and more lightweight than projects :-) You can also use [backtick methods](https://cockneycoder.wordpress.com/2017/06/12/when-to-unit-test-in-f/) and modules to help readability of your tests - VS2019's test runner will correctly parse them and categorise them for you. Another option is to keep all tests in one project and all your "real" code in another project (or other projects). This would, however, mean that you need to handle project references and some other bits. An alternative way to go is to look at the Paket project. You only need bind to the dependencies once (at the repository level). Then, adding a the dependencies to the new project is a simple task of copying an existing `paket.references` file from one of your other projects to this one.
Hey, isaac-abraham, just a quick heads-up: **curiousity** is actually spelled **curiosity**. You can remember it by **-os- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Well then let me restate it, because you've actually detailed why it's a necessity on early .Net, and it's a near tautology: &gt; Among other requirements, the speed required by its already in-production IDE influenced F#s selection of an apparently 'restricted' compiler. ... to break it down: "among other things" would be any and all other requirements considered; VS and its language services were obviously considered when writing the base language. As you yourself pointed out, without this kind of restriction compile times "explode". That's a big deal for a squiggly red lines when scripting in VS. I'm gonna apologize for third-hand quotes, I can't be bothered to try and dig up transcripts from talks I've been to where this was touched on, nor the Slack chats where tpetricek touched on the concrete speed requirements, but here's some [corroborating evidence](https://fsharpforfunandprofit.com/posts/type-inference/): &gt; The F# team have said that it is possible to make the compiler more sophisticated, but it would work less well with Intellisense and might produce more unfriendly and obscure error messages. So for now, we will have to live with this limitation. [Or maybe](https://markhneedham.com/blog/2009/05/02/f-stuff-i-get-confused-about/#comment-16153): &gt; I recently asked Don Syme about making multiple source passes to improve the type inference process. His reply was "*Yes, itâ€™s possible to do multi-pass type inference. There are also single-pass variations that generate a finite set of constraints. However these approaches tend to give bad error messages and poor intellisense results in a visual editor.*" I'm not sure what there is to agree about... Good Intellisense information in a visual editor benefits from faster compilation times, and F# aimed to provide "on dot" Intellisense early on. One is obvious, and something I've seen DSyme say. The other is visible in early language demos, and one of the oft-stated reasons we don't have a pure Haskell.Net :)
I wonder if itâ€™s to do with not being able to organise code using subfolders in Visual Studio? Is that still a thing?
Yeah, probably an ideal use case for templates.
Why do you feel privileged to require the tool to fit into *your* workflow? Why do you feel privileged to force me spend my time on the something that fits *your* workflow? Have you done something for the project? Have you done something for F# ecosystem? Have you created issue describing you workflow, suggesting the solution and patiently wait for someone (one of the maintainers or contributors) to discuss and implement the thing? No, you decided to bitch on 1. unrelated issues in the other repository 2. 2 years old closed issue on Ionide repository 3. this subreddit About some small thing that is apparently so important that no one else has ever mentioned it for all those years. And call out people that doesn't agree with you. But yeah, clearly you random Redditor are the person that knows best what users needs. I definitely think that calling something people work on terrible is awesome strategy to get help from those people, well done. &gt; competitor Hey, u/phillipcarter2, you're my deadly enemy, I challenge you to a duel, feel free to choose weapons. ðŸ˜‚
lol
It's a good thing I work on the language/compiler/tools :) &gt; The F# team have said that it is possible to make the compiler more sophisticated, but it would work less well with Intellisense and might produce more unfriendly and obscure error messages. The compiler is currently in a steady state. It has plenty of quirks, but they are known, and generally it plays well with tooling. Making typechecking much more intricate risks bringing it out of that steady state. That's what this statement is saying. The statement is also presupposing that N passes in typechecking would necessarily bubble up partial information to the user. That is one way to do things, but not the only way. &gt; I recently asked Don Syme about making multiple source passes to improve the type inference process. His reply was "Yes, itâ€™s possible to do multi-pass type inference. There are also single-pass variations that generate a finite set of constraints. However these approaches tend to give bad error messages and poor intellisense results in a visual editor." This is also assuming a specific implementation of the tools. I imagine that at the time, when tooling was far less elaborate than it is today, it was assumed that the tools couldn't control the compiler very much. That is, if it emits some weird stuff due to being only partially done, users would see that and get confused, or IntelliSense results would get funky. This might have been true a long time ago, when the F# language service was just a thin veneer over the compiler, and editor tools didn't do much aside from stuff a few things into lists. Today, even without architectural changes, the compiler service can completely control the information that it needs. It's been capable of doing this for quite a long time, actually. So the single pass-ness of the typechecker is immaterial to tooling experiences. It's very easy to simply say, "get the complete typechecking results" and it can make as many passes as it needs to get there. The only reason why this doesn't happen today is because moving the compiler out of the steady state it is in is risky business. There wouldn't really be fundamentally worse performance by using multiple passes. Similarly, error messages are also something the compiler service can control. The _current_ architecture is that the typechecker will yield back error messages as it attempts to unify types. It will also yield them back as it attempts to do post-inference checking for additional semantics (such as `byref` scoping rules). This will be done in a way that is as error-tolerant as possible, but definitely has limitations. If we were to throw partial typechecking results into the mix, that wouldn't really change much. The key difference here is that it's just harder to get right if the typechecker is eventually consistent. Not really a tools thing, just an implementation difficulty thing.
Awesome, let me know if I can help anyhow.
I know who you are, we've interacted on github on several long running issues *painfully* relevant to the topic of this post ;) This is all awesome info, but it's orthogonal to what I stated. "[P]oor Intellisense results in a visual editor" and "work[ing] less well with Intellisense" just aren't commensurate solely with less helpful type messages. And you're not addressing why a single pass compiler was chosen, nor the constraints made by exactly those specific implementations of those specific tools to show info at typing speeds. I'm fully aware there are alternative, robust, approaches to compiler info... But I'm also aware of the compilation consequences of having more powerful, purer, functional type systems and how they impact type resolution. Those capabilities are, after all, why the single pass compiler is considered a restriction compared to more powerful functional languages, and why multi-pass is frequently requested in F#. Those other languages can't do it 2020. Somehow F# managed it by 2004... So when you've got a pure Haskell.Net working fully, wake me up, I'll be all over that. Until that time: .Net languages that restrict important programming constructs specifically in order to speed up compilation are doing so *in part* to support showing type information quickly inside a visual editor.
&gt;Why do you feel privileged to require the tool to fit into your workflow? How about the fact that .fsx is how F# was intended to be used, or that a compiled REPL was the killer feature at the time? You *honestly* think that having to address exceptions in REPL is a rare situation to encounter? Or sending 300-400 lines from an .fsx, and then having to wait close to ten seconds \*just\* for it to be received by FSI? What's the alternative? Move everything into an .fs and compile it? Open the project in VS2019? Stop using FSI? Sure, I can do all that, but I think at that point, I've earned the privilege to complain. &gt;Have you done something for the project? Have you done something for F# ecosystem? Have you created issue describing you workflow, suggesting the solution and patiently wait for someone (one of the maintainers or contributors) to discuss and implement the thing? Recently, only a couple times on the FSharp repo, but a lot more often on the old forum before Github. I've been involved on-and-off for over a decade. What does that have to do with the validity of the criticisms? Or are you just trying to make it personal? &gt;No, you decided to bitch on &gt; &gt;unrelated issues in the other repository You're upset because I had the mind to consider that *maybe* there's something on VSCode's side that could be affecting how slow FSI is right now? And it turns out to be related. It's exacerbated by a common problem, partly, by the fact that a) it's slow because it's going through a terminal, and b) you can't turn off echo. Also, why are you even upset that I brought up a (seemingly) unrelated issue on another repo? Isn't that my own business? Holy cow. &gt;2 years old closed issue on Ionide repository Which continues to be an issue. Should I have blindly opened another one without doing any prior research? &gt;this subreddit Which is the least formal place to talk about F#. Should I have done it all in one place, i.e. on Github? &gt;about some small thing that is apparently so important that no one else has ever mentioned it for all those years. There's a saying: When people stop complaining about your product, that's when you know you have a \*really\* serious problem. It's either because a) people have stopped using your product, b) there are some *really* serious problems that dwarf the serious problems, c) people don't believe it's going to improve anymore. &gt;And call out people that doesn't agree with you. *Very* broadly and anonymously, as "downvoters," because Reddit is known for drive-by downvoting. &gt;But yeah, clearly you random Redditor are the person that knows best what users needs. This random Redditor is also a typical F# user who's been using F# for over a decade. Yes, I know what the user needs. Clearly quite a few people agree with at least some of it. &gt;I definitely think that calling something people work on terrible is awesome strategy to get help from those people, well done. I didn't call your work terrible. I *literally* called the *entire* state of F# tooling horrible, which it is. The tooling for a 15-year-old language shouldn't be this bad. &gt;Hey, u/phillipcarter2, you're my deadly enemy, I challenge you to a duel, feel free to choose weapons. ðŸ˜‚ Cute. You say that like I meant "competitor" to be a bad thing. Stop putting words in my mouth just because you don't like my criticisms.
I remember now why I lost interest in F# a few years ago. It was because of this same guy thrashing on anyone complaining about the UWP situation at the time. This person is just toxic. There is just no point continuing the discussion once he jumps in. I was strongly considering picking F# for my current project, since the language is a perfect fit for the requirements, but seeing how such a big contributor to the tooling is still being childish after all those years (and I just had a peek at his recent comments and other social medias, it kinda got worse actually), that makes me think twice. I won't be using Ionide, that's for sure (cue in some reply along the lines of "good riddance")
&gt; and I just had a peek at his recent comments and other social medias I try not to do that, but you got me curious. [Still putting words in my mouth](https://twitter.com/k_cieslak/status/1146516484568604672). I mean, I know I get heated, but Jesus, I never make it personal.
My current concerns are simply about structuring projects in general. I come from a different background so Visual Studio and F# are foreign lands to me. I think I'll move things in the same project and make tests in a different project that I'll reference (didn't know that was a thing).
currently I am working on a admin UI (Fable / Elmish) for my "Simple Azure Functions Auth Provider". [https://github.com/DieselMeister/Azure.Functions.SimpleAuthProvider](https://github.com/DieselMeister/Azure.Functions.SimpleAuthProvider) A not standard token based auth provider, which can be installed as an azure function and uses the azure storage for the data. I built this for some very small projects, where hosting a full blown identity server is to much. These "Apps" are mostly F# Elmish Clients hosted as static website on an azure storage account with an azure functions backend. They are very cheap to host. :)
haha, it's a private Gihub repo that I'll have to section off and tidy up. But I do want to, I'll let you know!
If youâ€™re using netframework, yes (ish). If youâ€™re using netcore, no!
I don't know if it's relevant here but ocaml compiler is much faster than F#
There's a few elements to consider: total time of compilation from A-to-Z, the impact of incremental changes during scripting on visual coding in a functional language WRT type information and "on dot" time requirements, and the impact of those design criteria on the languages functional features in the context of F#'s positioning and resources in the early 2000's. In the general, single pass compilers are smaller, simpler, faster, and quicker to produce. But they're not the absolute fastest, per se, and more mature compilers can smoke them. By restraining itself to a single pass, and forcing type inference (generally), top to bottom left to right, you're left with something quick to make and reasonably performant, yet blatantly "crippled" WRT to functional programming as practiced in several major functional languages. AFAIK this is much more of an issue for people coming from a Haskell, Scala, or LISP backgrounds. As far as I know we have not seen a live, visual, coding experience for Haskell or OCaml scripting that is on par with VB.Net, C#, or F# 2010-2016ish... If I'm wrong, *please* lemme know! I'm sitting here staring at paragraph after paragraph of noise here on Reddit from MS for opining that F# considered it's tooling in its design, while literally staring at the same running FSI issues that have been going on 3 years now. Code that compiles and fails to execute, with an entire VS 2017 VM &amp; dedicated branch off to the side just to run stable F# code... ... in "Enterprise" tooling... ... for years on end...
Rounding up the remaining bits and pieces for F# 4.7 :)
I used it in a small project and it worked as expected:)
I don't have a stake in this sub-discussion, but I think it's important to keep this in mind: Tooling for a programming language is _incredibly_ difficult and time consuming. Much more than developing a language itself. It's so difficult and time consuming (read: expensive) that only two companies realistically make money out of it (Microsoft and JetBrains), both of whom also benefit from that tooling helping to drive other things that make money (e.g., Azure, partnership with Google for Android, etc.) To give an example, the C# 8, Nullable Reference Types are going to ship. With that, there will be some accompanying IDE support. One feature you might expect to be there is to "gray out" an unecessary `!` operator (called the "dammit" operator), which people will have to sparingly apply to things if the compiler analysis cannot figure out if something is actually non-null. That compiler analysis may improve over time, so a `!` today might be unnecessary tomorrow. Hence, you'd expect to be able to see the `!` gray out, and potentially give you a code fix that removes it! Simple, right? Well, it's way more complicated than that. Because the presence or non-presence of a `!` can completely change the semantics of an entire codebase. To enable graying out a single `!` the IDE would have to analyze the impact of _not_ having that `!` now matter where the variable it's applied to is passed. This can massively impact performance, because all possible areas that could be affected by its nullability now need to be re-analyzed. Whereas before, non-nullability was _asserted_, allowing the IDE to not perform potentially expensive analysis. This intersection of tricky logic and performance generally means one of two things: 1. A feature simply does not exist, or it exists in an intentionally more limited form. 2. A feature exists to its fullest capacity, but impacts performance enough that people can sometimes notice. To give another example of this, but this time for F#, your first issue that you listed - "slow background compile" - isn't slowness at all. It's an intentional delay. This is because when users type code, they vary: sometimes it's very fast/guided by IntelliSense and the resulting code compiles; other times it's very slow and results in errors. If those errors are parse errors, it's cheap. But if they're typechecking errors, it's expensive! The compiler has to figure out if what was typed "works", and that can have downstream impacts due to type inference. Sometimes it's cheap, but other times it's expensive. So this forces a difficult decision. Either: 1. Give back diagnostics immediately, incurring the performance hit on any user who isn't done typing things yet. 2. Delay diagnostics a bit, causing people a usability issue where people might expect an error to show up faster than it does. Just to give an idea of the magnitude being dealt with, tools like Ionide and Rider can ballon to 10GB+ of memory being used on someone's machine, despite an intentional effort to reduce resource usage for things. All of this is to say that statements like this: &gt; The tooling for a 15-year-old language shouldn't be this bad. Generally don't lead anywhere. It doesn't matter what the level of criticism is, who it's coming from, who it's directed at, nor for what reason. Unless a hard dollar amount is attached to the complaint, or someone just got nerd-sniped into something something generally easy, it tends not to accomplish much. All parties just end up more frustrated than they were before.
There actually is a form of "solution-level" dependency management. It is the magical `Directory.Build.props` file. Just put a file with that name in the top level directory of your solution, and every `.fsproj` under or in that directory will have those values inserted into it. All you need is a file that looks like this: `&lt;Project&gt; &lt;PackageReference Include="NUnit" Version="1.0.0"/&gt; &lt;/Project&gt;` If you have other needs that don't need to be shared by everything, but still shared by several, you can name the file anything and just put the following line in the relevant `.fsproj` file(s) : `&lt;Import Project=".. \path\to\file.props/&gt;` Then, when you need to make updates to the multiple projects, you just make the change in the one place. _(On mobile, sorry if formatting is weird.)_
&gt;The tooling for a 15-year-old language shouldn't be this bad. &gt; &gt;Generally don't lead anywhere. That's true. It did cross my mind that I shouldn't say that, especially because it's not really fair, but I kept it in because I was frustrated and the gloves had kind of come off by then. I apologize to you and k\_cieslak. I suspected that VS2019 background compilation was being delayed on purpose to avoid premature errors. You can imagine my surprise when the same compiler turns out to go so much faster in another IDE (text editor, whatever). Is this not something that can be be made adjustable to the user? At least then, we can know what to expect, and it's not just three seconds of wondering if something's wrong all the time. After all, there are occasionally times when it gets really slow for abnormal reasons, or just stops working altogether. I appreciate that the tooling is often more difficult to develop than the language itself -- especially for a language with type inference and an ML-style syntax. Obviously there are a lot of moving parts; as someone else pointed out in this thread, the .NET Core transition has been pretty painful. When I asked if there really is a point in making bug reports, I wasn't asking to be snide. That said, you'll have to forgive me for not being convinced that VS2019 F# has adequate manpower even with Microsoft calling it a first-class citizen, or that Ionide isn't being managed like a hobby project. What really gets me, and what's the reason for the title of my post, is that there are two similar projects that have issues that are totally different in nature but equally disruptive overall. It wasn't until I gave Ionide an honest try last week that I started getting the sense that the problems VS2019 (and Ionide) face aren't entirely technical. I mean, if I saw all three IDEs eat up 10gb of memory, I'd just accept that I need to buy more ram.
Geiler ScheiÃŸ... (in english: awesome!)
&gt;I apologize to you and k_cieslak. Happily accepted (and I don't feel insulted, it's part of being a product owner :)). It's appreciated, though. &gt; That said, you'll have to forgive me for not being convinced that VS2019 F# has adequate manpower even with Microsoft calling it a first-class citizen, or that Ionide isn't being managed like a hobby project. This is a tricky one. We recently invested in Ionide for 3 months to address: * Moving over to LSP * Hammering on performance problems, of which there were many * Stabilizing the project creation experience * Moving towards a more .NET Core-first experience given the path of .NET in general * Novel features along the way (e.g., Info Panel, which is amazing) However, as it is an OSS project run by a small handful of maintainers, having Microsoft come and step in runs the high risk of squashing it. Ionide has grown to ~1/3 of total F# usage, but that usage is quite diverse. I personally believe that it's used much more for smaller projects and ad-hoc scripting than fully-fledged development in a larger solution. But it can be used for all of those. Given Microsoft's storied history with OSS, this is particularly challenging. So we made significant gains: people who simply could not host their solution in Ionide can do it now, and people who could not reasonably run it due to the large memory and CPU usage can now reasonably run their solution in it. But that's all foundational work that you're unlikely to notice if it was already "fine" for you, or you just tried it out recently for the first time. The VS story is also quite frustrating and storied. The team was de-invested in the Windows 8 timeframe and nearly evaporated. One of many, many casualties in that disastrous era of Microsoft's history. It wasn't until mid-VS 2017 timeframe that the team grew to the size it needs to be successful with. This is mostly due to the fact that it's incredibly difficult to find people who are willing and able to work on this stuff in the long-term. But with that comes piles of technical debt. One of the biggest problems was utterly rotten infrastructure, which required a full-time engineer nearly 2 years' worth of work to bring up to par with what we needed. Now we're running into architectural issues that, while not debilitating for most, do inhibit _very large_ F# solutions from working properly. And all the while, F# usage has nearly tripled since mid-2016, which means 3x the amount of people running into problems and filing bugs to address. The notion of "adequate manpower" is a tricky one. We've found that smaller teams typically operate better in the long run than larger ones. It's the mythical man-moth problem. Similarly, the C# group is not much larger than the F# one. But the key difference is they've been the same core group (modulo a few people over the years) since 2010. That's a huge difference than a core group that was evaporated and only recently replaced. That isn't to say the situation is dire or anything - quite the opposite. We have a stable tool set, we can evolve the language and compiler quite freely, we've made massive strides in performance improvements, and OSS activity is quite healthy. But when something has the unfortunate circumstance of needing to start over while inheriting years of non-investment, hard prioritization work is done such that many of the quirky, "annoying but I can live with it" kind of issues are left unaddressed for a long time.
Great write-up and project :)
This is a great news, and the fact it's not only for vs is a plus.
I'd love to see how to get started with absolutely minimal dependencies. Last time I checked it out (Fable 1) it could not run without Paket. It's so often everything at once.
I feel like there is some confusion with regards to different concepts here. you can have OOP in a functional language and discriminated unions in an imperative one
No `paket`. No `dotnet fable`. The only dependencies we start with are `fable`, `babel` and `webpack`. The only thing we call is `npm start`. We try to start as minimal as possible.
Thank you, I will definitely check out Fable soonâ„¢ again. Not sure if I manage to join the live stream, but I'll definitely take a peek to the recording later on.
&gt; Feels a bit cumbersome having all those xxx = None; You can make a Create method that takes optional arguments. &gt; match is really great, but I felt it was probably better to use if in many cases. Yes. `match` is most useful for DUs. &gt; For .NET library fuctions, the choice of f(x) vs x |&gt; f is annoying. How so? &gt; I feel like it may be useful to get an error or warning when using let instead of use to bind an IDisposable That won't work because you use isn't always what you want. Another idea: https://github.com/dotnet/fsharp/issues/4618
&gt; Since checksumPath and localName are both string option I was hoping I wouldn't have to give them an explicit value when defining a Remote if I wanted to keep them as None. Feels a bit cumbersome having all those xxx = None;. The lack of default values *is* something you have to kinda get used to. As u/CSMR250 said, you could make a function to do it. What I would do is to have a function that just takes the mandatory parameters and fills in None for the rest. Then you can use a record expression to fill them in if need be. Something like... ``` let newRemote name remotePath = { name = name remotePath = remotePath checksumPath = None localName = None } And then, for example, to use it to fill in the first one, you could do: ``` updateRemote { newRemote "ArcDPS" "https://www.deltaconnected.com/arcdps/x64/d3d9.dll" with checksumPath = Some "https://www.deltaconnected.com/arcdps/x64/d3d9.dll.md5sum" } ``` Or you could just make a function that takes all 4 parameters and pass in None None for the last two each time to save having to specify the parameter name every time. &gt; When I was first writing updateExists, I was thinking of it as a function I'd run right after. I like to make things functions and pull them outside the outer function scope just to make it easier to reason about things. It's just easier to reason about your program if you can know exactly what inputs your value depends just by looking at the parameters, so you can spend less time staring at and trying to figure it out six months down the line. &gt; I bound a few "constant" values right at the top of the file (ln 7-9). But each value is only used in a function each, maybe it's better to have them only where they'll be used? I would keep them there, since they serve as settings that you won't want to have to go digging for later. &gt; I feel like it may be useful to get an error or warning when using let instead of use to bind an IDisposable... I think the problem with that is that there are legitimate uses for, say, returning a Stream to the caller, where closing the stream on the way out would break it.
&gt; You can make a Create method that takes optional arguments. Cool, I'll look into it. &gt; How so? Just...I don't know which one to use. `|&gt;` is more "idiomatic", alright, but it's also a bit longer and often seems unnecessary...? I don't know. &gt; That won't work because you use isn't always what you want. Another idea: https://github.com/dotnet/fsharp/issues/4618 Ah, that looks good. Thanks for the reply!
Very insightful, thanks! &gt; I like to make things functions and pull them outside the outer function scope just to make it easier to reason about things. It's just easier to reason about your program if you can know exactly what inputs your value depends just by looking at the parameters, so you can spend less time staring at and trying to figure it out six months down the line. Yes, I was thinking that too. I guess I was just a bit worried about polluting the namespace with functions that are only called once inside another function... Though I know it wouldn't be a problem in a tiny program like this, it might be better to keep everything grouped up in a module, which I just learned about :P.
:D Yeah, there's definitely a bit of a trade-off there too. Encapsulation can feel like a bit of an afterthought in functional programming compared to OOP. What I find is that it's easier to pollute lots in the beginning, and then spend a few minutes cleaning up later, by making things private, moving them into modules etc. It's a chore, but a surprisingly quick one, because by the time you're ready to publish your work, it's easy to decide what you want to expose and what you don't.
You can always define functions inside functions.
&gt; https://github.com/dotnet/fsharp/issues/4618 That's really cool. &gt; |&gt; is more "idiomatic", alright, but it's also a bit longer and often seems unnecessary...? I don't use it if it makes it harder to read. I think a safe rule of thumb is to use it if you would write an extension method for it, and otherwise not. I also don't use currying/partial application if a function takes more than one argument of the same type for which the order matters, e.g. `subtract (5,3)` over `subtract 5 3`, because what does `3 |&gt; subtract 5` mean?
[removed]
I love it. I'm a bit behind you! Last week I was learning zeroth order propositional logic, and implementing an FParsec parser to evaluate simple propositions with just Boolean constants I've also done it to cement my understanding of the topic!
Highly misleading comment here, please do not imply that FP cannot be pragmatic. There are an entire set of effects types that can be utilized for â€œusefulâ€ things. Solving problems and receiving blessings for your well-implemented code are not mutually exclusive.
Glad there are two of us. :) Iâ€™m happy to share notes if you get stuck on anything, although Iâ€™m still far from an expert.
that's a moot point. For instance, it's very easy to model functional language with OOP language, good example is F#: functions, closures are just objects. But it may be very difficult to model OOP in FP: Haskell is so limited language that the attempt to do it leads to new language O'Haskell, and if you remember naive article of Peyton-Jones how he tried to simulate OOP with Haskell (when Haskell was still seen as .NET FP language candidate and no any F#), you understand what I am talking about
For coding katas (especially ones that are just uploads to challenge websites), I like to have each problem as an F# script (.fsx), and evaluate some known inputs/outputs for the function in the F# Interactive as I go along. &amp;#x200B; Does that approach work for you, or are you strictly more interested in compiled assemblies and automated tests here?
That sounds like a nice approach as well. I use tests because I want to make it a habit. What environment are you using (IDE/text editor). Do fsx files require project structures as well? Or can they be one-shot files?
I use Visual Studio Code and Ionide for FSI/Kata work. .fsx files need no project structure, and are usually self-contained in a single file. Referencing other scripts or DLLs is done using an #r syntax to not require a project file. I use them for katas specifically to keep them in one file if I ever want to revisit am old approach to a problem.
Thanks, that approach will fit my needs better. I'll use projects for more real applications.
You can always open a GitHub issue and ask the maintainers directly.
How the holy heck haven't I come across this before?
Not familiar with Fable, but it should be as simple as rendering a select tag filled with option tags. You can use map on your list to create a list of option tags for "interpolation" into the select tag. You will need to handle the statefulness of it though. The react docs have an example of this [here](https://reactjs.org/docs/forms.html). Though you may want to translate it to using hooks.
Ahh nice! Thank you!
Right? I'm so confused by the lack of stars vs number. If downloads. And thecompany that runs it has one outdated website. But regular updates...
I mean, I love FP as much as anyone can, and I do agree that itâ€™s a better way to write software, but there are a LOT of assertions in this article with no evidence to back them up. Bad OOP is bad not because itâ€™s OOP, but because itâ€™s bad.
Uhm.... Oop and fp both have their pros and cons
I like FP but the whole, "OOP is completely wrong" is laughable. The overwhelming majority of software that actually does something is OOP.
I'm honestly not sure if this is a parody or not....
Hello, &amp;#x200B; we do have examples with dropdown in Fulma documentation. \- Form with native dropdown: [https://fulma.github.io/Fulma/#fulma/elements/form](https://fulma.github.io/Fulma/#fulma/elements/form) \- Custom Bulma dropdown: [https://fulma.github.io/Fulma/#fulma/components/dropdown](https://fulma.github.io/Fulma/#fulma/components/dropdown)
For example... &gt; Testing Private Methods &gt; &gt; Some people say that private methods shouldnâ€™t be testedâ€¦ I tend to disagree, unit testing is called â€œunitâ€ for a reason â€” test small units of code in isolation. Yet testing of private methods in OOP is nearly impossible. We shouldnâ€™t be making private methodsinternal just for the sake of testability. &gt; &gt;In order to achieve testability of private methods, they usually have to be extracted into a separate object. This, in turn, introduces unnecessary complexity and boilerplate code. Nobody says "you shouldn't unit test private methods". You should be able to test your private methods by calling your public methods. If you have to make your private methods internal to test them, then that's a code smell that your design breaks modularity and/or encapsulation. Bad OOP, as you said. Mutable state is definitely problematic and should be avoided as much as possible. However, I've never seen anyone write a GPU driver in a pure FP language. Sometimes, mutable state is an inescapable fact. The CPU and memory architecture of a computer *is a giant pile of mutable state*. Of course, I do think the world would be better off if every programmer at least practiced with an FP language and learned how to push dealing with mutable state off to the edge cases where it belongs.
&gt; I tend to disagree, unit testing is called â€œunitâ€ for a reason â€” test small units of code in isolation. Yet testing of private methods in OOP is nearly impossible. The equivalent in F# would be to test sub-functions. No one would do that either.
Absolutely strange passage. If you want to test "private" functions in Haskell you can either: \- make them "public" or \- extract them in special module which will export anything (usually called "Internal") and test them through it. And import it in another one module and re-export only some "public" functions/types to client side. So, most canonical FP language (ie, Haskell): &gt;introduces unnecessary complexity and boilerplate code. By the way, no such thing as private/public and other accessibility levels in Haskell at whole lol. But in C# I can just use "internal" access modifier ([https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/access-modifiers](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/access-modifiers)). How it's intelligent. Bravo, Microsoft!
I think that the majority of software that actually does something is procedural. OOP is a passenger at best. Have a look at this https://youtu.be/IRTfhkiAqPw
Much as I think the article is deeply flawed, your argument does not hold water. You say people have done useful things using OOP. Was it because they used OOP, or in spite of them using it? The only thing it shows is that it is possible to develop software using OOP, but nobody argues that in the first place.
â€œThe need for a debugger almost disappears completelyâ€ nope
Appreciate this! Thank you!
Agreed, yet it was upvoted at least 16 times. "Usage implies fitness" is one of the axiomatic beliefs underlying so many of the debates we have in this sub/industry. Yet sadly, we rarely debate it directly.
Almost this entire article is cool kid "x bad y good" speak with like appeal to group mentality, there's very little meat here but a whole lot of straw. See the first few sentences, which are ... farfetched. And the "real-world example" section which contains no example but does lead to an image of a "promiscuous" woman.
Hi, we're the maintainers of Aardvark Platform. &amp;#x200B; Aardvark is a collection of libraries for rendering, maths, computer vision, VR, and building interactive apps in general. We're actively using and maintaining it as a software base for industry projects in a scientific research context (terrestrial surveying, volume data, lighting simulations, etc.) - most of the projects are not publicly available, which is why there are so many downloads but only little public discussion. Aardvark has been maturing for many years, but only recently did we make it open source (which is why we don't have many videos yet :/ ). &amp;#x200B; Aardvark.Rendering is a high performance incremental 3D renderer with dependency tracking ([https://www.researchgate.net/publication/281104016\_An\_Incremental\_Rendering\_VM](https://www.researchgate.net/publication/281104016_An_Incremental_Rendering_VM)). [Aardvark.Media](https://Aardvark.Media) is a non-frustrating GUI library that implements the Elm architecture using HTML+JavaScript (running as a web server). We're thinking about also deploying to the web soon using Fable. &amp;#x200B; We're very enthusiastic about Aardvark and will gladly chat with you! There's a lot of real-world experience behind the project. We're always open for feedback and suggestions, and we'll gladly support you in case you have any questions. Just talk to us: &amp;#x200B; \* Reply to this thread \* Chat with us on gitter: [https://gitter.im/aardvark-platform/Lobby](https://gitter.im/aardvark-platform/Lobby) \* On github \* Contact us directly at: [https://aardworx.com/index.en.html](https://aardworx.com/index.en.html) &amp;#x200B; We'll be hanging out in this thread for a while.
Couldn't agree more. Almost every paragraph starts with the same thing. x bad y good and when everyone is hooked to get examples - " nah, let's skip to next paragraph where I ambiguously explain some stuff which says x bad y good, but i won't give any real life examples". I come from OOP(C#) background, but when I read an article about comparing x,y I need something more than just -"I say so that is why" , I need examples, something to hold on to, not just because some dude said so.
Can you please restore link to c# version?
Strange how that broke...fixed it.
Nice, thank you very much!
Thanks for your response! Interesting paper and website! This is all quite advanced for my roughly 6 months of F# experience and lack of knowledge on graphics pipelines. I'm also really struggling to understand the incremental logic system and translate a real world problem to it to fully understand how it works. How many resources are you planning/committing to document and provide examples for hobbyists like me? Thanks again for the response and I'll definitely keep an eye out!
You should not model `createContactInfo` to take two optional strings because they're not really optional, are they? It's misleading, and it's causing you to duplicate validation code. &amp;#x200B; Option 1: Pass in two strings, `create` them both, and then your function becomes much more simple (it returns if Some/Some, else Error). &amp;#x200B; Option 2: Do you really even need a `createContactInfo` function? It seems like maybe you're creating it as more of an academic exercise. I think it might be overkill. I would really consider just handling this in the consuming code instead of in the domain/model.
From your description it sounds like an elmish context, is that right? If that's the case it might be useful to have the invalid values with their errors in your model, too. The pattern Scott describes is useful the domain logic, but not so much in the UI. From an elmish point of view I'd split the types of ContactInfo you want as a result and the one you have in the input UI. But to get back to your question: try mapping/binding the results, like: ``` | (None, Some e) -&gt; EmailAddress.create e |&gt; Result.map Email ``` That way you can save a few matches and function endings. Minor nitpicks: - Both regexes disallow valid values. E.g. international phone number start with a '+' or 00 and emails can contain spaces. - Types and modules ca have the same name in the same scope, so you don't have to put the email type as T into a module, you can have it above.
First, I agree with u/green-mind that you might not really need `createContactInfo`. Depending on how you take the inputs, it may make more sense to branch and handle validation there. Second, if you really want `createContactInfo` anyway, you could reduce much of your boilerplate using `Option.map` and `Result.map`, and collapsing the two Ok/Error branches: Â´Â´Â´ let createContactInfo phoneInput emailInput = let phoneResultOpt = phoneInput |&gt; Option.map PhoneNumber.create let emailResultOpt = emailInput |&gt; Option.map EmailAddress.create match (phoneResultOpt, emailResultOpt) with | (None, None) -&gt; Error ["A phone number or email address is required"] | (Some pResult, None) -&gt; pResult |&gt; Result.map Phone | (None, Some eResult) -&gt; eResult |&gt; Result.map Email | (Some pResult, Some eResult) -&gt; match (pResult, eResult) with | (Ok p, Ok e) -&gt; Ok (PhoneAndEmail (p, e)) | (Error pMsg, Error eMsg) -&gt; Error (pMsg @ eMsg) | (_, error) | (error, _) -&gt; error Â´Â´Â´ Still, consider if you really need this to be one function. If you could branch before trying to create a ContactInfo, you could do away with the options and just need `.create p |&gt; Result.map ...` and the last match. --- As a side note: Are you following an OCaml guide? In FSharp, we don't use the idiom of `type T` inside a module; there's not much value to it without module functors, which F# doesn't have. Instead, idiomatic F# puts them side by side: ``` type PhoneNumber = PhoneNumber of string module PhoneNumber = (* ... *) ```
I've seen the \`type T\` approach recommended for F# too, by some people. When I ran a small Twitter poll most F# people disliked it (or hadn't heard of it). I too prefer the side-by-side approach. That said I have received feedback that some environment/consuming language combinations have an issue with a type and a module having the same name. Not verified though.
&gt;I've seen the type T approach recommended for F# too To be fair, I came to like it too when I started learning Ocaml not long ago. F# started as "Ocaml on the CLR", but has moved on to fit a bit better with the .NET ecosystem. It took a while for the official style guidelines and the F# community to converge with each other, but I think they mostly do now; I understand the position of those that still have "culture shock", though. &gt;I have received feedback that some environment/consuming language combinations have an issue with a type and a module having the same name. In the past, if the type wasn't generic, you needed an attribute that changed the compiled name of the module in the assembly to avoid collisions. That's done automatically since F# 4.1 I believe; that's the only issue I can think of right now.
&gt;You should not model createContactInfo to take two optional strings because they're not really optional, are they? I might not be understanding you right, but if they're not optional how would I end up with, e.g. the `Phone` option for `ContactInfo`? Should the decision for "what to build" be separated from the validation? That feels wrong, but that's probably my OO background fighting me. It sounds like this is the root of the problem I'm facing. I'm trying to go from "dirty/unvalidated" user input to "clean/validated" domain objects, but I'm not sure where that should be done. What would be different about handling this in the consuming code, as you suggested in option 2? That sounds like a better idea, and if you know of any examples I'd love to check them out.
Still, I've changed the phrasing to better convey that it's just rare and not idiomatic, but not somehow wrong; that's not what I meant.
Not elmish (I had to look that up) although this will be part of a webapp backend, which I'm doing purely as. An exercise to learn more about F#. Thanks for pointing out `Result.map`, I didn't know that existed! I've got a little reading to do. But back to your main suggestion: I gather you're talking about having an input type with basically no validation (in this case just a DTO with phone and email string fields) and then a function to map that to a validated ContactInfo, right? That makes sense, but I'm not sure how it would clean up the branching... &gt;Both regexes disallow valid values. Haha, I knew someone would notice! You're right, this is just for me to play with, so no production environments are in danger ;) &gt;Types and modules can have the same name in the same scope, so you don't have to put the email type as T into a module, you can have it above. I was following Scott Wlaschin's lead on this one. From what I understand, I'll be able to mark the type constructor as private to force use of the `create` method - did I get that wrong? Either way, still good to know that type and module names won't collide!
This is a little learner webapp, so I'll be taking in the input as a JSON object, initially. I'll play with ways of branching before creation, as that does sound like a better approach. I guess I'm still in the OP mindset of an object being responsible for building and validating itself... Good to know about the idiom of putting the type alongside the module! Since that opens up the type to being constructed by anyone, what's the standard way to enforce validation? To answer your question: no OCaml guide, this idea was from the link in my post.
&gt; What would be different about handling this in the consuming code, as you suggested in option 2? That sounds like a better idea, and if you know of any examples I'd love to check them out. You just need to split up concerns a bit. You could have different validation functions for the different cases (extracted from [my suggestion](https://www.reddit.com/r/fsharp/comments/cdb1oj/dealing_with_branching_in_validation/ettke73/)): module ContactInfo = let createFromPhone p = PhoneNumber.create p |&gt; Result.map Phone let createFromEmail e = EmailAddress.create e |&gt; Result.map Email let createFromPhoneAndEmail p e = let phoneResult = PhoneNumber.create p let emailResult = EmailAddress.create e match (phoneResult, emailResult) with | (Ok p, Ok e) -&gt; Ok (PhoneAndEmail (p, e)) | (Ok p, Error eMsg) -&gt; Error eMsg | (Error pMsg, Ok e) -&gt; Error pMsg | (Error pMsg, Error eMsg) -&gt; Error (pMsg @ eMsg) Then, if you happen to end up with the inputs inside two options, like in your example, then just: match (phoneInput, emailInput) with | (None, None) -&gt; Error ["A phone number or email address is required"] | (Some p, None) -&gt; ContactInfo.createFromPhone p | (None, Some e) -&gt; ContactInfo.createFromEmail e | (Some p, Some e) -&gt; ContactInfo.createFromPhoneAndEmail p e If you turn that last snippet into a function, you have your `createContactInfo` again.
Yep, this makes a lot of sense. Now that I see it I feel silly for not having arrived at that while trying to refactor, haha. My takeaway here is that constructing and validating each option separately can be a good way to go, which I'll have to get used to. Thanks!
&gt; Since that opens up the type to being constructed by anyone, what's the standard way to enforce validation? In your example, the case constructors can also be constructed by anyone. The constructors can be used from outside too. To enforce validation, you can hide the case constructors with the `private` modifier, like this: type PhoneNumber = private PhoneNumber of string type EmailAddress = private EmailAddress of string Note that this `private` goes on the right side of the equals. This marks all case constructors as visible only inside the enclosing module (or file, if it lives in a namespace); it also works on records, BTW. This is how encapsulate a union/record in F#, but don't use it more than necessary. For example, there's no need to hide the constructors of ContactInfo, since instances of PhoneNumber and EmailAddress will be validated already. With this, you also lose the ability to pattern match on those types from outside the enclosing module/file. You must then expose functions that perform that pattern matching; in this case, expose functions to extract the phone/email string from PhoneNumber/EmailAddress.
"Domain Modelling Made Functional" is a decent book but it has become a cult within F#. The community needs to stop reading just one book. People want to stop thinking and just find patterns. Results are not always better than exceptions and using them indiscriminately will lead to more complex code. Hiding complexity through encapsulation is often good. Single case DUs are a ridiculous case of complexity for its own sake.
\&gt;I'm also really struggling to understand the incremental logic system and translate a real world problem to it to fully understand how it works. &amp;#x200B; That's basically the reason why Aardvark.Media exists :D We constantly strive for higher levels of abstraction and expressivity, allowing people with the domain knowledge to sketch apps quickly and confidently, while also expressedly remaining fully hackable, so the hardcore low-level folks can do their thing. &amp;#x200B; The incremental system keeps precise track of dependencies and makes the minimal set of updates on the output if an input is changed. We use it to implement our renderer - a layer of abstraction over graphics hardware. We have a couple of nifty features, such as a declarative scene graph supporting actual full dynamism, and \[an awesome language for shaders\]([https://fshade.org/](https://fshade.org/)). &amp;#x200B; [Aardvark.Media](https://Aardvark.Media) is probably the best API for all of this. We employ \[Elm\]([https://elm-lang.org/](https://elm-lang.org/)), an extremely nice architecture in which you define your app in terms of an (immutable) model, an (immutable) view where the UI elements produces messages (as values), and an (immutable) update function that computes a new model from the current model and a message. Media makes sure everything is incremental and efficient automatically in the background, you just declare your HTML Dom (which can contain scene graphs/3D rendering) and updates. \[Check it out\]([https://github.com/aardvark-platform/aardvark.media/tree/master/src/Examples%20(dotnetcore)](https://github.com/aardvark-platform/aardvark.media/tree/master/src/Examples%20(dotnetcore))) &amp;#x200B; This works out quite nicely for us, people are building quality scientific apps that run nicely, and the components are passed around and re-used in new projects. However, we're clearly from a scientific background, so there's no level editor or asset store :( (yet). &amp;#x200B; We always expand documentation as more people use our stuff, but it's actually really not very difficult. \[If you want to try it, check out the template and go.\]([https://github.com/aardvark-platform/template](https://github.com/aardvark-platform/template)) We also \[appear at various functional programming\]([https://www.youtube.com/embed/9XkE0\_4HoOc](https://www.youtube.com/embed/9XkE0_4HoOc)) and\[computer graphics conferences and love to have discussions in person.\]([https://www.youtube.com/embed/vIqdSngwHFQ](https://www.youtube.com/embed/vIqdSngwHFQ)) Lastly, feel free to hang in our gitter chat, we'll gladly support any specific ideas for a project you might have.
&gt; I guess I'm still in the OOP mindset of an object being responsible for building and validating itself... Yes! You hit the nail on the head with that statement. The domain type definitions become more lightweight because they have no responsibility other than modeling what the valid data should look like. The type system allows you to do this in a declarative way such that values have to exist unless they are marked as optional. Now you are free to pass these types as function arguments and you have *zero* validation checks to perform, because that type is validated at its inception by the calling code. By the time it makes its way into your pure domain functions, you are guaranteed that it properly constructed and legit! &amp;#x200B; Another benefit of this is that you avoid the temptation of trying to customize your domain types with too much baggage that is actually use-case restrictive. The domain layer is not necessarily the place to determine what to do if input data is invalid -- that is actually use-case specific to that scenario. All you should care about at the domain level is creating simple pure functions. &amp;#x200B; As an example, let's say that you wanted to implement subscribing to a news letter: User enters their email and clicks "Subscribe to News Letter" button. Consuming code below: &amp;#x200B; &gt;module Subscriptions &gt; &gt;let registerSubscriber (email: EmailAddress) = &gt; &gt;// Do stuff with full confidence that email is valid &gt; &gt; &gt; &gt;let sendMostRecentNewsletter (email: EmailAddress) = &gt; &gt;// Do stuff with full confidence that email is valid &gt; &gt; &gt; &gt;\_\_\_\_\_\_\_\_\_\_\_\_ &gt; &gt; &gt; &gt;module UI &gt; &gt;birthdateTxt &gt; &gt;|&gt; EmailAddress.create &gt; &gt;|&gt; Result.map Subscriptions.registerSubscriber &gt; &gt;|&gt; Result.map Subscriptions.sendMostRecentNewsletter &amp;#x200B; This allows you to create a very declarative "receipe" like pipelining syntax in your consuming (UI) code. Result.map will only call the next step in the pipeline if the input is "Ok". Result.map allows you to maintain a simple pipeline format while still handling the Result type with grace. If not for this pattern, you would end up with ugly C# "triangle of doom" code everywhere with inline if/then, try/catch statements sprinkled everywhere... That is why even the most simple tasks in C# usually end up looking the garbage after all the try/catch and if/then statements are added to make it bullet proof. With F#, look at how simple and declarative the end result is, and yet you are still just as bullet proof -- NAY, even more bullet proof because the types are immutable, so they never need to validated again! Whereas with C#, the data class properties can usually be mutated at ANY point along the way, which is why you see validation being duplicated all over the place (*just to be sure).* &amp;#x200B; Sorry if there are any errors in the code, but hopefully you get the idea.
I used the `type T` approach in one of the first bits of F# code my team ever saw, and I suspect that it played a role in the ban that immediately followed (which was later reversed, thankfully). Whatever "encapsulation" benefits I thought I was receiving were likely offset by the readability issues caused by that pattern. So I think I prefer the side-by-side approach as well for improved readability. (readability &gt; cleverness is a hard won opinion, and I have to scars to prove it). &amp;#x200B; Also, I ran into a hard-to-spot bug in Revit development where I applied attributes to a private type in my module which caused the library to fail in very weird ways. It took my longer than I'd care to admit to figure out that the scope of the record type was causing the issue. My preference is veering away from hiding things in FP land, and I for once feel good about it. (There is something very Gollum-like about encapsulating everything for fear that "they tries to uses our functions, but we hides them so they can't misuses them!!").
On the same line, I typically solve it using [FsToolkit.ErrorHandling](https://demystifyfp.gitbook.io/fstoolkit-errorhandling/) as below ```fsharp #r "./packages/FsToolkit.ErrorHandling/lib/net461/FsToolkit.ErrorHandling.dll" open FsToolkit.ErrorHandling open System.Text.RegularExpressions type PhoneNumber = private PhoneNumber of string with member this.Value = let (PhoneNumber n) = this n static member TryCreate (number : string) = if Regex.IsMatch(number, @"^[2-9][0-9]{2}-[2-9][0-9]{2}-[0-9]{4}$") then Ok (PhoneNumber number) else Error ["Invalid phone number"] type EmailAddress = private EmailAddress of string with member this.Value = let (EmailAddress e) = this e static member TryCreate (email : string) = if Regex.IsMatch(email, @"^\S+@\S+\.\S+$") then Ok (EmailAddress email) else Error ["Invalid email address"] type PhoneNumberAndEmailAddress = { Phone : PhoneNumber Email : EmailAddress } with static member TryCreate ((phone, email) : string * string) = Validation.map2 (fun p e -&gt; {Phone = p; Email = e}) (PhoneNumber.TryCreate phone) (EmailAddress.TryCreate email) type ContactInfo = | Phone of PhoneNumber | Email of EmailAddress | PhoneAndEmail of PhoneNumberAndEmailAddress type ContactInfoDto = { Phone : string option Email : string option } with static member ToContactInfo (dto : ContactInfoDto) = match dto.Phone, dto.Email with | Some p, Some e -&gt; PhoneNumberAndEmailAddress.TryCreate (p, e) |&gt; Result.map PhoneAndEmail | None, Some e -&gt; EmailAddress.TryCreate e |&gt; Result.map Email | Some p, None -&gt; PhoneNumber.TryCreate p |&gt; Result.map Phone | _ -&gt; Error ["A phone number or email address is required"] ``` You may find these [validation examples](https://demystifyfp.gitbook.io/fstoolkit-errorhandling/index/trycreate) useful.
Awesome! I'll take a look, thanks.
Even on Windows I've constantly had trouble getting stuff just to build without breaking. Then again this just seems to be the way with F#.
1. Validation is small task and F# can do much more impressive stuff. It's unfortunate that 50% of threads about F# are about validation just because of the popularity of a certain website. 2. There is no need to do the hacking mentioned in that website to do validation. Here is a simpler version of your code. Note that DUs should not be used if there is just one case. &amp;#8203; type PhoneNumber private (s:string) = member t.Number = s static member TryCreate(s:string) = if System.Text.RegularExpressions.Regex.IsMatch(s, @"^[2-9][0-9]{2}-[2-9][0-9]{2}-[0-9]{4}$") then Some(PhoneNumber(s)) else None type EmailAddress private (s:string) = member t.Email = s static member TryCreate(s:string) = if System.Text.RegularExpressions.Regex.IsMatch(s, @"^\S+@\S+\.\S+$") then Some(EmailAddress(s)) else None type ContactInfo = | Phone of PhoneNumber | Email of EmailAddress | PhoneAndEmail of PhoneNumber * EmailAddress static member FromPhoneAndEmailOpts = function | Some p, Some e -&gt; PhoneAndEmail(p,e) |&gt; Some | Some p, None -&gt; Phone(p) |&gt; Some | None, Some e -&gt; Email(e) |&gt; Some | None, None -&gt; None let createContactInfo (phoneInputOpt:string option) (emailInputOpt:string option) = let p = phoneInputOpt |&gt; Option.bind PhoneNumber.TryCreate let e = emailInputOpt |&gt; Option.bind EmailAddress.TryCreate ContactInfo.FromPhoneAndEmailOpts(p,e)
(This thread is already 2 months old so I'm basically replying here for posterity's sake. :D) To make the point about Reason more explicit, Reason is, basically, a COSMETIC FACELIFT, by Facebook, of good ole' OCaml, and F# was, by admission of its own creator, Dr. Don Syme, modeled HEAVILY after OCaml, so basically, any Reason developer you have on your team is an F# developer just not knowing it yet. :D Also, any .Net house, C# folks, can pickup F# in a breeze, even do OOP with F# syntax and gradually, as they learn to think FP, gravitate towards pure FP in pure F#. It really IS that easy.
&lt;COUGH&gt;Fable&lt;COUGH&gt; &lt;COUGH&gt;Fable-Elmish&lt;COUGH&gt; And just for completeness's sake: Elm and PureScript.
Microsoft is a small company and doesn't have a lot of resources.
I can't find the thread, but Don Syme doesn't like typing directly in FSI. He likes send to interactive from fsx files. So I think they will prioritize requests to improve scripting outside of FSI. [https://github.com/dotnet/fsharp/issues/6484](https://github.com/dotnet/fsharp/issues/6484) [https://github.com/fsharp/fslang-suggestions/issues/542](https://github.com/fsharp/fslang-suggestions/issues/542)
Which makes sense since there is so much rich tooling already in the editor and sending to the interactive window is just a hot key away.
I've been using WPF to display graphics or make multiple "channels" of printfs or receive user input, but the event pump in FSI breaks a lot of keyboard input when you show a non-modal window.
State of the art OO philosophies don't even have this problem. Large-scale OO projects tend to deal with what used to be private methods via composition and DI today. So where I used to would have a `CalculateWidgetTax()` private method, now my class depends on `IWidgetTaxCalculator`. That is tested separately, and can be controlled in the context of whatever is using it. I'm pretty sure if I sat down and wrote a lot of F# using only anti-patterns and smells, I could write an interesting article about how much FP costs. It'd be bad faith. If I had to write an essay on this topic, my personal premise is history's greatest and most common programming disasters boil down to failures to manage complexity. The reason we make new languages is to try and handle that complexity better than the existing languages. But every approach is a tradeoff between different kinds of complexity and if we aren't careful we can use a perfect language to write a garbage pile. Lots of atrocities were committed when OO was still very new. I don't think we really learned how to write good OO software until the mid 2000s. We learned good practices on top of the ashes of the bad practices. I think we also rediscovered a healthy amount of software engineering FP devs had known all along. But I find people still use examples of 1990s OO design as a condemnation of OO in the 2010s. That's like using Windows 95 to criticize Windows 10. I don't have to write 1000 words to support, "Failure to properly maintain project complexity leads to unmaintainable code." And I don't care what language you give me, if I treat my project like a storage shed instead of a garden, pretty soon I'm going to have a mess.
Very well said. In the end, we're dealing with complex issues, even if they can sometimes be stated simply. We try and manage that complexity, but as you said, there are tradeoffs. Cargo-cult programming leads to using patterns badly and can overcomplicate things that shouldn't be quite as complicated. And then there is outright malice by contracting firms and charge-you-for-customization vendors that profit from complexity.