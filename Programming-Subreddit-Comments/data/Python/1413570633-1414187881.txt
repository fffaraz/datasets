Once you get to a certain scale you don't really start to build the apps just with a framework like flask; it becomes much more about load balancing/cacheing/storing your data in a scalable and performant enough way. Flask will happily take you to *and past* the point where you wouldn't be bottlenecked by Flask, you'll be bottlenecked by those sorts of problems.
How many hours a week dedicate an student to your class?.
You should probably check out /r/learnpython. And work on that counting thing ;).
"cd_price" - "money". You can't do mathematical operations on strings. If they are integer variables , try removing the quotes around them.
re: very long functions Use an editor that uses this: https://pypi.python.org/pypi/mccabe . I use https://github.com/klen/python-mode which tells me if my functions are getting too complex. Be warned: when you look at old code it will show up as an error, which will make you want to fix it. Could be time consuming. :) 
I would still be interested in hearing your opinion as soon as you've looked into it! :)
raw_input always creates a "string" object. A string is just text. You can't do math on text- "Hello" - "World" doesn't make any sense. You have to convert the strings into integers.
Yes, do not be mislead by microframework. It means that flask comes with the bare essentials to make a web application and you decide what functionality to add through custom code or flask plugins. It is the opposite of django batteries included philosophy.
"Micro" has more to do with what flask _is_; a router, a templating engine, a wsgi toolkit that abstracts requests and responses, facilities for handling the request lifecycle, and... very little else. It exposes a small, focused, modular api surface, instead of a tightly coupled monolith (I want to say "like Django" since that's what flask is usually compared to, but I've never actually built anything with Django so I couldn't really say). It's a foundation, and you can build whatever size application you want on it, some things just won't be "built in" and you'll need to add them yourself. The beauty of a "micro framework" like flask is that isn't tough to do.
`joblib` is built on top of pickle, and thus has the same security vulnerabilities, as far as I can tell.
a better description of flask is "minimalist" rather than "micro". you can most certainly build a large application with it, but because flask itself is minimalist it will mean bringing in a lot of your components for stuff because there are many things that you will probably need to do that just aren't part of what flask does. 
buy that 20$ ofcourse it's donate :)
I would often up that - write two to throw away. Do it wrong once, do it more correct the second time and the third should be pretty damn good^tm.
I realize that recenty, I agree with you auto click is one of best option
Actually the idea is a one-page site with absolutely no content except by javascript. I only use html (which I really, really hate) as a type of markup because it is the assembly language of browsers. 
I appreciate this and the info below where you elaborated. I have to ask this though: As a learning recreational game Dev, which one should I start with? Is Kivy going to be anymore difficult creating a game? Sometimes I feel I take too much time learning about tools, when I should just start creating and when I need to switch, I'll have a base of knowledge that is applicable elsewhere 
I did not. I'll look into that later, probably.
&gt;especially when its just one value. I actually think the single value case is one of the main reasons to avoid it. People tend to just write stuff like: print("x = %r" % x) without realising that this is often a bug. To be safe, you need to do: print("x = %r" % (x,)) which is already getting a bit clunky - indeed, I'd say it looks more awkward than when you've got multiple values. 
Document your intentions not your actions. Makes it a whole lot easier to come back and correct errors. 
You could also throw your gui update into its own thread to update it in the background while the rest of the app runs.
5 hours a week. 3 hours for labs, 2 hours for lecture.
&gt; For example could you create applications like Reddit or maybe Twitter using Flask? Yes, but you wouldn't use Flask by itself to serve Reddit or Twitter. Sites at those scales have more to do with [caching, load-balancing, data storage and replication](http://www.reddit.com/r/Python/comments/2jja20/is_flask_good_enough_to_develop_large_applications/clc9iir). &gt; I'm not planning to do so as I am just a beginner but I was just curious if you could if you wanted to. If I may, as a beginner I would find a framework I like then use it to build a small app (say a simple blog, to do list, or timekeeping app). Then maybe build it again in another popular framework. Maybe even try another language? When you get far enough along with this, you'll start to understand general web application development, and what kinds of frameworks fit you best.
excellent answer my friend, I came here to say the same thing. I've never really reached the scale where these things became a problem - do you have some resources you'd recommend or a plan for designing / monitoring the pitfalls you mentioned (load balancing/ cacheing / data storage) so that you can actively adapt your application and tech instead of being bottlenecked by these and having to overhaul?
they have a publicly available mailing list where you can ask this question.
You can make a large project into spaghetti code with any size framework. It just depends how much you'd like the framework to do for you initially.
You're addressing app scalability it seems, I thought the OP meant large in terms of the app logic. edit: Sorry, somehow I missed this "For example could you create applications like Reddit or maybe Twitter using Flask?" and I would agree the framework doesn't matter for those kinds of apps, I reckon 90% of the complexity(largeness) is in the scalability requirements. i wouldn't really call them twitter or reddit complex in and of themselves. a complicated(largeness) app IMO would be something like salesforce.com turbotax healthcare.gov just to name a couple. 
This 'one function should have one function' thing is great, and works very well in languages like Java which have better encapsulation and control of APIs, but it's hard to take seriously in python, which is kind of designed against such control. 
Agreed and good points - sometimes it can be ugly and be ok. I think the thing to take away is this: Go back and look at your old code, analyze how it could be better and only re-write if necessary based on your specific needs for that specific code. 
감사합니다! 반가워요
Everyone here has offered great advice. For what I can contribute, I'd add that there are two parts of writing good software: solving problems well, and writing code well. Solving problems well is of critical importance. Writing good code to implement a poor solution makes for poor software. I like [Domain-Driven Design](http://en.wikipedia.org/wiki/Domain-driven_design) for this reason: it emphasizes understanding and solving the problem in terms of the problem itself. Before you think about what language constructs to use in building, say, a Fibonacci number generator, start by defining what a Fibonacci number *is*. Then the answer will usually "shake out" from there. Writing code well is also of high importance, though as noted before the best code for a poor solution is still a poor solution. Similarly, the worst code for a good solution can obscure that good solution to the point of being unrecognizable. Code should convey intent, it should explain to its reader how it works (and as much as possible, *why* it's doing what it's doing). Well-named variables, short functions with a single intent, intent-revealing unit tests...these things all help convey to the reader how your code works. And before you think "I'm the only reader", remember that there are usually at least two people reading any given codebase: Present You and Future You. Present You understands the code by having it all fresh in mind; Future You will need to regain that level of understanding by reading the code. Future You always benefits by being as clear as possible in your code, and keeping the amount of context needed to understand any given chunk of code as small as possible.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Domain-driven design**](https://en.wikipedia.org/wiki/Domain-driven%20design): [](#sfw) --- &gt; &gt;__Domain-driven design__ (__DDD__) is an approach to [software development](https://en.wikipedia.org/wiki/Software_development) for complex needs by connecting the [implementation](https://en.wikipedia.org/wiki/Implementation) to an evolving model. The premise of domain-driven design is the following: &gt; &gt;* Placing the project's primary focus on the core [domain](https://en.wikipedia.org/wiki/Domain_(software_engineering\)) and domain logic. &gt;* Basing complex designs on a model of the domain. &gt;* Initiating a creative collaboration between technical and [domain experts](https://en.wikipedia.org/wiki/Domain_expert) to iteratively refine a conceptual model that addresses particular domain problems. &gt;The term was coined by Eric Evans in his book of the same title. &gt;==== &gt;[**Image**](https://i.imgur.com/rxMM2q5.png) [^(i)](https://commons.wikimedia.org/wiki/File:Maintaining_Model_Integrity.png) --- ^Interesting: [^Object-oriented ^programming](https://en.wikipedia.org/wiki/Object-oriented_programming) ^| [^Agile ^software ^development](https://en.wikipedia.org/wiki/Agile_software_development) ^| [^Behavior-driven ^development](https://en.wikipedia.org/wiki/Behavior-driven_development) ^| [^ECO ^\(Domain ^Driven ^Design)](https://en.wikipedia.org/wiki/ECO_\(Domain_Driven_Design\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+clcfwg8) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+clcfwg8)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Good link, thanks for pointing it out.
If you only want the side effects and not the result, then why oh why are you not just writing a loop? for x in ...: f(x) You can even write it on one line if you want. 
And beyond gaining an understanding of web app development and what each framework provides, hopefully also a sense for how each framework does what it does and leverages the features of the language, e.g. introspection. It's one thing to understand why it's beneficial to "not repeat yourself" but it's another to be able to incorporate that methodology into your own code, and that can be a good way to learn.
if you end up doing a distinct series of things in your one function, you could try naming the steps being taken and refactoring to move well defined blocks of functionality into their own function called from the original. 
Got very excited there for a second but after trying it I found it pretty much unusable in its current form. Code cells were either squashed or truncated, embedded HTML output wouldn't display properly, code completion wouldn't complete anything outside the current cell and larger notebooks wouldn't load at all in some cases. Hopefully they'll devote sufficient time to actually make it usable before the 4.0 release.
I don't mind doing someone else's homework :-) http://pastebin.com/vYWUVGyq Even with some foolproof http://pastebin.com/fKA5mAxi
There are a lot of packages. It takes a while or just don't update everything. The software is just doing exactly what you asked it to do.
After consideration, comprehensions are thought to be the more elegant way of doing much that map used to do before comprehensions. I don't think the community will go back on this myself, so that leaves wrapping/rolling your own. Here's one I did earlier: &gt;&gt;&gt; iterable = range(5) &gt;&gt;&gt; def function(x): return x + x &gt;&gt;&gt; map(function, iterable) &lt;map object at 0x02A440F0&gt; &gt;&gt;&gt; &gt;&gt;&gt; def lmap(*args): return list(map(*args)) &gt;&gt;&gt; iterable = range(5) &gt;&gt;&gt; lmap(function, iterable) [0, 2, 4, 6, 8] &gt;&gt;&gt; Personally I think you miss out by generating the list in most situations instead of feeding a generator forward.
So, this is me, but it works well for me. I use /u/Paddy3188's approach, but from the other direction. I find that once you've written a long and convoluted chunk of code, it's difficult to refactor. Your way of thinking about the problem is circuitous and complex. That's because most programmers learn to write from the ground up. You start at the beginning, keep manipulating and manipulating and eventually get to something that you can use. It's like a winding river finding the path of least resistance. The way I code is the other way around. I write a single function that I can call from a RESTful route. Something ideally without any state. Within that method I first write a series of clearly named calls. def some_task(args): try: data = get_some_data(args) related_data = get_related_data(data, args) result = process_something(data, related_data) catch SomeException: log_exception(SomeException) return result I've been working in ruby more than python lately, so some of that may be horribly wrong. Once I have this structure, I start getting errors about methods not being defined. So I write tests and implement those methods. Either I have a one-liner that works within each method, or I write a named method as if I've already written one. I continue until everything is either one-liners or methods. At some point I have a fully functioning program that's already refactored. Usually I get to a handful of methods that I've already written, and it just works. If your naming convention is solid and obvious, you'll sometimes accidentally use methods you've already written. So long as they're well-named and obvious it's usually OK.
I would highly recommend you look at Pyramid rather than Flask. It has better machinery to scale outward and upward.
Not 100% sure, but I think a lot of Meebo (now owned by google) was developed partially Flask.
many large companies with significant traffic us flask - so in that regard yes.
That seems to sacrifice overall usability + completeness on the altar of individual correctness, so I guess I have to agree that they are not the same.
I've been trying to mess with cython but it's been quite a hassle. If this is as easy as it looks then this is phenomenal. I'll have to give this a try. Thanks for sharing!
You example case works far more elegantly with coroutines and/or generator chains. MCing should be used for dynamic just-in-time initialization of static class members, the classic example being a db api wrapper. Your example on the other hand is just straight from the standard library docs with some scenario that misses the point, so neither is it advanced MC use by any standard. Can't understand how this was voted, probably nobody of those guys who did understand how this stuff works...
Planning for scale may hinder time-to-market, as well. So you have to be very, very sure that your pre-emptive optimization isn't.
This is great news. I save all my working code snips, &amp; do all my initial work, in ipython notebooks, then copy/paste into pycharm.
Interesting, I'm curious as to why it is called out for astrophysical computation as the supported types and functions seem to be pretty generic.
As the saying goes: "premature optimization is the root of all evil".
Absolutely, yes. In fact, Flask is arguably better for a truly large scale app compared to Django. Django is more for medium-sized apps that fit a standard template. The real question is whether to use Flask or Pyramid for a very large project. Both are good choices.
It depends on the nature of your project/product. Some software absolutely must run with very low latency (or high requests per second or high [insert metric here]) to be effective at all, even as a prototype. One such example is a product that sits inline between a customer's server and the Internet, or the reverse. A typical Python-made web app, no, but thinking about an architecture that can be expanded upon and scaled up when necessary can be quite useful and also helpful with future projects even if your current one never reaches a point where it needs to scale up to a great degree.
Bah! Get rid of the double negative! 
Why not https://pypi.python.org/pypi/pyPdf ?
I don't really understand the concept. What are blueprints and why are they better? How were they implemented? I read through that but I still feel a bit confused.
You may want to read [Clean Code](http://www.amazon.ca/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882) !
Cython takes some getting use to, but I've found it to be pretty rock solid. I think it's also the most battle tested of any of the python numerical acceleration methods out there, being used in production in Pandas, Scikit-Learn and Scipy. Numba is also a good option. It's in active development and appears to be improving rapidly.
billsil is exactly right. "conda update --all" is almost certainly faster than upgrading all of anaconda. If you don't believe me, you can try installing anaconda even from miniconda (e.g., by typing "conda install anaconda").
Yes, exactly this. Which is why I want to know how these things should be *accounted for*, not necessarily optimize toward, in the design process of an app so they don't demand a system overhaul down the line to keep the flexibility intact.
I think Flask supports certain kinds of applications very well. It can be added-to to support different kind of applications, but at it's core, the simplicity it provides lends itself to support "thin-middle" web applications very very well. Those kinds of applications are becoming more and more common as people create great front-end frameworks and whe whole Single Page Application movement chugs along. The applications I write these days usually look something like this: [ Front-end framework (Angular / EmberJS / Backbone / jQuery / Vanilla JS) ] ------- "client side" above here ---------------- ------- "server side" from here on ------------- [ Thin API layer (Flask) ] [ Database ] And many larger apps also eschew the same architecture. The reason is, most of the time, if you start using a larger framework on the client-side (or front-end), you often only have to really deal with shuffling data around on the back end. So for a big site like reddit, you can think of it from back to front: 1. They need some database table/file/whatever that stores posts. 2. They need an endpoint that will serve post data (ex. if you send a GET request to the URL "reddit.com/posts", you will receive JSON data for lets say the 50 top posts) 3. They use a front-end framework to do those requests for data asynchronously (using AJAX), and build the page accordingly. Some heavier apps/other web frameworks render the page with data when you hit the endpoint (ex. you go to "reddit.com/posts", and the server responds an entire HTML page, not just data, the posts will be used to create the markup that comes back), and Flask *can* do that but I find it's best used as a very thin middle layer/restful resource layer. Flask is great BECAUSE it's small -- you don't have to hold a lot in your head to get going, and if you really only have a small amount of data to send, it can be &lt;100 lines, and serve high-traffic sites (because the real work will be done by your database, in serving up the data repeatedly, flask is just there to give the front-end app a path to the data it wants) Don't mean to bombard you with a lot of information, but if you're a beginner, here are some things that might help you immensely in getting "good" at writing web apps: 1. Ping/traceroute a website on port 80 2. Take a 30 second look at DNS resolution (if you can't find something that explains it in 30 seconds/a paragraph, ignore this step) 3. Take a 5 minute look at the HTTP protocol 4. Telnet a website on port 80, send it a basic HTTP GET request for / Sorry if most of this is stuff you actually know 
Kivy is really a better choice. I cant think of any reason I'd recommend pygame over it for your purpose. (to be fair it can be useful for prototyping visualizations if youve already gone through the pain of actually making it run on your machine)
I REALLY could have used this last summer
When I was doing astro research in college I had a program that was not seemingly complicated... just really computationally intensive that did spatial statistics on the clustering of stars. It took 5 days to run with my largest sample. I eventually found the numba module which uses a JIT for functions. Reduced computational time to 2 hours. It's not always about complexity as it is speed for brute force computation. Ideally the algorithm could have been tweaked to be nlog(n) instead of n^2 but I was pressed for time
I'm surprised there is no support for the OSX notification center.
Good reference, thanks! I didn't think to search for a Python library to modify PDF's. :-) I looked further into pyPdf, and the GitHub page says it's no longer maintained: https://github.com/mfenniak/pyPdf That repo refers to https://github.com/mstamy2/PyPDF2, which looks perfect. They even show an example of merging documents: https://github.com/mstamy2/PyPDF2/blob/master/Sample_Code/basic_merging.py I've updated my script to use this library: https://github.com/bamos/python-scripts/blob/master/python3/merge-pdfs-printable.py I know performance doesn't really matter, but using ghostscript and pdfinfo on some sample files ran in 3.523 seconds and produced a 1.4MB file. The new method with PyPDF2 took 0.496 seconds and produced a 4.0MB file. I'm still using `ps2pdf` to create a blank page. I could also use http://www.reportlab.com/apis/reportlab/2.4/pdfgen.html and I tried it. However, I couldn't figure out how to create a blank PDF with it in 2 minutes, so I see no reason to change. :-)
I have a few larger PHP based websites that I have been migrating to Flask. So far so good. The very first test I wrote was for speed so I took the nastiest DB stuff and started my testing there. I don't think there was a single test where PHP out performed Flask and my ability to optimize PHP far exceeds my present ability to optimize Python or Flask. Also due to its very nature, scaling quite a bit should be a dream unless you were to pretty much deliberately code in some sort of anti-scaling sort of way. 
One thing that sucks about flask is that creating routes requires an application object to exist. Usually you want to define your routes inside of a module at the top level (as opposed to inside of a function where everything needs to be indented one level), and when you're writing something simple and creating an application object at import time this isn't really a problem. But what if you're using a factory to create your application object? Or what if you want to make re-usable routes? Or... well a lot of other situations where you don't want to tightly couple your routes to an imported module? Your options are to make a factory function and deal with the indent or... use blueprints. At their core, blueprints just record all the routes (and most other things) registered to them, and, when the blueprint is registered on the application, replay all the recorded actions on the application object. It's a way of inverting who's concern it is, from the routes registering themselves on the app to the app registering the routes. Now the app can bind them at a different root, or give them a different endpoint name, or any number of different things. And just as an added note, a reason this is important (to me anyways) is that import side effects are bad practice, and depending on them tends to make your code a completely untestable tangle of spaghetti. Something you just kinda learn over time.
Oh, this makes sense! Thank you. I thought it had to do with performance for handling large numbers of requests.
This worked, thanks a ton!
Mostly seems fine. I had to uninstall homebrew's git and use the platform one because homebrew's couldn't remember my password.
why care? 
&gt; nastiest DB stuff If the query takes 500 ms, it doesn't really matter if the web server layer does it in 22 or 18 ms. 
Upgrade worked fine for me w homebrew's python and git
I'm using it with Brew and everything seems to be working fine.
TL;DR was strings can be wrapped in both double and single quotation marks plus their triple versions. mystring = "gotta use 'single' inside" mystring = 'gotta use "double" inside' mystring = """can "use" 'both' inside""" mystring = '''same 'thing' "here"''' Hope you didn't get overwhelmed by the amount of feedback :P In any case I'm sure you'll find many useful applications of Python in the future and you're off to a good start.
I watched Brandons talk and was total puzzled, cause his code is so clean. Sometimes I read opensource projects and think wow how does it work because there isn't any "code" in it just a bunch of function doing little things. And refactoring... how do you do this 
Well what aspect of python are you thinking of. I have no expirience with java and very little in c++ and ruby. Python is the only language I really "know"
That sounds totally like TDD its like you first write the failing test and then make it green
I disagree. Flask allows for easily replacing key parts of the architecture with other components, for example the orm. Django's architecture is so rigid that components are either fixed or very hard to replace. In this sense, flask is a better choice if scalability is a concern. Also, you can include exactly the parts you need, eliminating bloat. You are right however, that for developrs that are just starting out, an inclusive, non-minimal framework is a better starting point. On the other hand I also think that things like Django's class based views, although helpful, also hide a lot of functionality from the developer. A lot of things automagically hapoen, which make it hard for a beginner to debug or understand. I found that in Django, if I want to deviate from the standard forms or models, it took a lot more effort.
It is a very popular task on Rosetta code, and I am glad that people blog about it. I think it helps popularise both maths and programming as it can be straightforward to follow but gives the newcomer a sense of accomplishment when understood as it is non-trivial.
I would just name the processor with a verb in this case: process_string = MyStringProcessor() So it will be natural to call it: "A test string" == process_string("a test string")
You are missing the order of filters.
Too bad for the license.
I would add a processor factory: def make_string_processor(*filters): return lambda s: reduce(lambda string, func: func(string), filters, s) process_string = make_string_processor(capitalize, upper, strip) result = process_string(" i am a cat help me derp") PS: Many string methods are implemented in *string* module: from string import capitalize, upper, strip PPS: Actually *make_string_processor* is *compose* but I've just realized that python's functools have no *compose* func :(
Yeah I do TDD too.
So say you want to make a report from your app that queries for everyone who responded "No Preference". Now you are hard-coding the magic string "No Preference" in your app. A sloppy programmer changes it to "No preference" because of some new corporate edit on capitalization (it happens). Now things stop working silently. I'd prefer to have a magic symbol in there; define BOOL2014_NOPREF = 4 and query against that. Normalization is mostly a good idea, but limited-range rarely changing values is where I'd draw the line. 
What magic does "attach to process" use? Is the forcible insertion of a DSO in existing process a lá http://pyrasite.com/ , or does it require each process to have a backdoor ready a lá Twisted Manhole? 
I real world, in big /apps/ (programs, products whatever the big boss calls them) you just don't do that. You simply don't relay on constants.py with anything. You don't use integers and then compare them against your somewhere else defined variables. You make new table and link the column against that. That way, when querying the database, you see what, lets say role, the user is having instead of just "1". Sure, if you want to have random integers in your database presenting something else than an integer, go ahead. Good luck with future work on that app.
Bullshit.
Random integers? Constants like that are certainly constrained by your database system unless you use MongoDB. If you have a small set of values and you don't expect them to change for the lifetime of the application, I don't see a problem. Anyway, in the real world you do not expose your database to everyone and anyone. You have a multi-tier system. Your clients use some kind of RPC to talk to your app servers who talk to your database; ideally you expose the same RPC to both your internal applications and external users. Bob from accounting is not going to be allowed to write SQL to screw with your RDBMS. He'll get an service endpoint and will like it. 
Its not manner of value changing, its about multiple applications accessing the same database so that you don't have to define your constants in all of those places. You will have multiple RPCs to talk to that db. Bob from accounting will use one app and Don from marketing will use another app, both talking to same database. You don't want to write the constants multiple time, do you? And it is very likely that moth of those apps are made by YOU or your team. There might be one team to do the database and another to do the app which uses that database.
there's already a pull request fixing given problem, use that fork.
There is nothing wrong with using the occasional dictionary but if the data in the dictionary is central to your program, it is probably a good idea to make this into a class. This because classes are more explicit than dictionaries and *"Explicit is better than implicit".* Explicit in this case means that the members and methods of an object are (or should be) explicitly stated in the class definition, whereas dictionary items will be inserted and deleted at several locations in the program. As and example let's say that I am debugging some unknown code and want to examine a variable. If it's an object I can print it's type, find its class definition and from there see what the object aims to do, and how it can be manipulated. If it's a dictionary, printing it's type will say 'dict' and tell me nothing. Examining the keys and values likely will not help. "What is the purpose of the 'foo' item?" "Should it always be present or is it just present in this case?" When you have a class definition you may realize that some of your functions just manipulate the objects contents. These are better implemented as a method. Take a moment and think of what the responsibility or purpose of this class is, and design it properly. This may help you to make your functions shorter. However, just like functions, classes should be short and have one responsibility. Resist the temptation to make *God classes* that do too much. As some others already said, a function should do one thing only. If you are thinking of a good name for a function and find out that it should be named *does_x_and_y* to properly reflect what it does, consider splitting it up. At least make separate *does_x* and *does_y* functions and call them in *does_x_and_y*. 
I think your "real world" must be a IBM or another serious enterprise where DBAs don't talk to developer except via arcane ticket systems. I'm not really advocating littering your code with magic constants and for an enum I'd typically use a CHECK-enforced CHAR(1). I'm not going to store an auto-generated key and force every to join up with a user_type table so they can translate 'n' to "Normal User" and 't' to "Trial User". If you were storing web URLs (http and https being the only choices) in a table, and wanted to break them up into components because you wanted to do some kind and absolutely wanted another column, would you change the protocol into a surrogate key and force everyone to join up with web_protocols table? 
Very interesting, any chance of a more detailed write up about the structure and lessons learnt? 
&gt; Even in the simplest case you may have a web application adding and modifying the database and then a reporting system reading from the database to aggregate figures or whatever. In this case there really should be a single location where (0, 1, 2, 3) = ('Unknown', 'True', 'False', 'No Preference') is defined. This is why you ETL data in to a specialized reporting data warehouse and not use the same operational database system. Your reporting software/ business intelligence application would point to that, rather than the web app's database. And, again, the cases where you use this pattern, this mapping won't change ever, so you use the documentation of the web app to build out the ETL, and implement a check in the ETL to detect values that are not 0,1,2 or 3 and issue a report to the DBA in the event something like that is in the data so a response for the ETL can be made in the event it did change at some point. Modern ETL software solutions can do this. &gt; Having critical business information like that locked into your web-app code just means it has to be repeated in each system that accesses the database so when a change has to be made now you have multiple places to update or, more likely, forget about updating the second location and only find out about it when your manager starts screaming that the reports are coming through all messed up. Multiple applications shouldn't be accessing an operational database of a web app directly in an actual corporate/enterprise environment. Applications should have a service that serves the data and any application that wishes to consume the data. The mapping would ultimately find itself in such a service layer. The webapp, in a corporate environment, never touches a database. It touches a service layer. You would ETL the data from the database (or even the service layer if your ETL tool supports it and it makes sense to do so) to a different reporting database for reporting, since you will probably want to denormalize the database for user reporting simplification.
OP is talking about twitter and reddit, so I guess I can bring github to the table. Do you thing they have 'f' for free plan, and 'm' for micro and so on? They have student and jobs programs on top of the normal one (git). Do you think they have the plans as CHAR(1) and then have constants to define which is which? Ask anyone who has been developer in big organization. They just don't use chars and integers presenting critical data and the compare against constants.
I tend to redo all my homebrew stuff when I upgrade anyway, was pretty painless aside from XCode 6.1 not being in the App Store at the time
I agree that when starting out something like Django will help avoid NIH syndrome. Otherwise you end up rewriting stuff that is not unique to your app such as user authentication and an admin back-end. Plus you can start out by looking at somebody else's similar app and modifying it or just using it for inspiration. For example there is already a Reddit type app for Django: https://github.com/agiliq/django-socialnews Also for Twitter: https://github.com/gnarula/django-ribbit Flask is good if you want just a few simple views, such as a ReST server, or if the functionality is so custom it doesn't obviously map onto any of the frameworks out there. Phillip.
What Python metaprogramming? There isn't anything. You got https://github.com/lihaoyi/macropy, but thats still AST transformation (no different then what Hy is doing). I'm also not sure if the abilities from macropy can reach the flexibility of Lisp macros because of the python syntax. Which is what actually makes Lisps macros awesome.
In other words: it's a microframework, not a framework for micro apps.
What do you mean, `datetime` support is not easy? It has many unittests specifically directed at datetime support. If you have an example of it failing with datetime, you should raise an issue (preferably with a unittest).
[citation needed]
Writing highly concurrent code.
There's no area where it's really weak; even for computationally intensive tasks there are whole suites of libraries specialised to those tasks. The two things you need to be aware of are that threading is disfavored in favour of other forms of parallelism; and that for performance-critical work you may need to use specialist libraries. (Although there is also pypy, a high-performance JIT implementation suitable for non-numerical high performance work). The reason why Python has so much traction is because it has lots of libraries, is fun to write (unless you love static typing, language-enforced access control, or having to use tortuous patterns), and really has very few downsides. Edit: If you love writing in a recursive style, Python certainly won't handle it well, because it has no optimisations for recursion. That said, it's possible to write recursive functions just fine unless you're working with very large recusive datastructures. Edit 2: ITT people who can't let go of threads as their only concurrency abstraction. Edit 3: ITT people who think type checking is an alternative to unit testing.
I've done a lot of C++, Java, C#, assembly before moving to Python. Pros: 1: No project setup - Amazing for short one off scripts, 1 simple .py file compared to set up Visual Studio project 2: Dictionaries - Really intuitive and easy to use 3: Code size - The lack of boilerplate code, compared to other languages means the code, is around half the size 4: Libraries - An impressive collection of libraries, your problem is probably partially solved already with an existing library Cons: 1: Dynamic variables - Variables are untyped, which means everything is like a template which is great UNLESS you didn't want that. So passing max('a', 'b') or max(1, 2) works exactly as you expect, but if you accidentally pass max(a, b) and a is 'a' and b = 2, then you will get an error at runtime rather than at compile time. 2: Speed - It's fast enough for web apps, file processing, but games are too slow (I tried a multi-layer parallax scrolling game in Pygame and performance was not sufficient). 3: Code protection - You distribute programs as source code, rather than as a binary which may be an issue if you want to avoid releasing the source. Overall language preferences: 68000, Python, C++, C#, Arm, Java, 6502, Z80, 80x86
Python is really convenient and I love using it for its REPL and for small personal projects. I think of it as a jack of all trades kind of language. Some drawbacks would be: * It's really slow. If performance is critical to your program, don't use python. * It's really high level, so you don't want to be writing kernel code with it * Threading (might not give you the performance increases you might expect, due to CPython's Global Interpreter Lock) Of course there are other complaints (dynamic typing, runtime errors, gripes with anonymous functions, using very few keywords and powerful one-liners can make code unreadable, etc.), but those usually have some good counterarguments (e.g., pylint, nested functions, multiprocessing). Like any other language, it has its pros and cons, but it's definitely very powerful and versatile. Good luck exploring Python! :)
Anything with graphics isn't very fun, in my experience, due to a combination of crappy libraries + meh performance. It depends though, there is a sweet spot for simple 2D games where you can get by with pygame and some elbow grease, but 3D is generally too intensive, and stuff that requires a real UI through GTK or something tends to end up with ugly code due to Python's limitations (no true anonymous functions).
Mobile dev
I really like static typing too. Most practices that move runtime errors to compile time errors are ok in my book. Python is still a fun and powerful language, you'll just have to get used to the differences.
Yes, I believe Python has a place in Game programming, but you will need to move any heavy work code over to Cython to make it realistic. 
My biggest complaint with Python is the lack of inline anonymous functions. These are incredibly useful for async, event-based, or anything when a lambda won't cut it. (Yes, I know twisted has a yield syntax. It doesn't allow for complex chains. And frankly, twisted is bloated, complex, and unpythonic.)
Crypto. I really wish there were stdlib symmetric and asymmetric encryption libraries, or at least a stdlib wrapper for OpenSSL. They have to make API calls to OpenSSL for TLS, so why not have a generic crypto API? All the third party libraries are unaudited or still in development[1]. [1] - https://www.youtube.com/watch?v=r_Pj__qjBvA
You might want to check the new ayncio module from newest version of python https://docs.python.org/3/library/asyncio-task.html I had the same problem but this really seems like a fun thing to use. :-) 
That does look oodles better. Unfortunately, my current project is stuck in Py2 because of protocol buffers. Maybe it's time I got a 3rd-party library for it.
Cross mobile platform. When I met Python, I fallen in love with it. It makes my life easier. From working as a script to an app with painless GUI (eg with Kivy). The toys (I mean libraries :v ) are awesome. But ? The bounds, for me and for now, is cross mobile platform. You can hear about Kivy or Py4A. I've used them before, they are good, but not good enough ... yet. Performance, deployment, extensible are pretty painful. But ... again ? Not good enough not means don't use them. You should give them a try. The bounds can be broken by us.
I disagree. It is good to define constants if they are actually constants. It is much better and cheaper to define constants and write DB migrations when they change than to create a new table to hold them. * Store in tables if the meaning can be changed by users/admins. * Store as constant in code if it is a constant. It makes sense to do things like Model.filter(status=Model.PUBLISHED) as compared to Model.filter(status=StatusModel.get(name='published')) If you store in the DB, you've to make sure you maintain integrity of the table. What if a row is deleted or changed? You can't be sure the row with `status="published"` will always be present. It is more work, more fragile and more expensive (more queries). Always define constants/symbols to hold constants. Don't use mutable data structure to hold constants. Never. A constant makes sense to the code. A variable might not.
Collaboration with people who barely know how to code.
All of the apps need to know which string/int to match. They need to know that is should be `where choice.name = "no preference"`. In case of constants they need to know the constant instead of string. Is that really such a huge problem? If the apps are connecting to the same database without any proper API/Protocol/Documentation then that is the problem. Using constants is not a problem. A library should be created to interface the apps with the DB or at least very good documentation. You have to document to look for certain strings anyway.
Whether it is the integer `1` or the character `f` is not really the point. When you define symbols/constants, you abstract that away. It is always PLAN_FREE or PLAN_MICRO. Internally they can be integers or strings or bitstrings or whatever.
Touché. Doesn't that apply to any code which "has to" run on a VM?
But also completely pointless, as everything but the comments can be decompiled.
Anything that lends itself naturally to deep recursion. This is my problem, not Python's. Many years of hacking in Lisp (and more recently Haskell) make my brain reach for recursive structures and and algorithms. Having to re-cast that as loops and comprehensions causes a slight break in flow every time.
Native GUI apps for any platform. There are tons of libs and they all wind up being hard to use (no great GUI builder, it's all manual tweaking) and look/act weird 100% of the time.
Web scripting. It takes too much to get Django or Flask to work, so I just use PHP, since it works right out of the box.
There are also issues of portability (Python isn't conveniently available for some architectures that might be relevant if you do embedded systems work) and disk/non-volatile storage space (Python itself is huge by embedded standards).
There are ways to get around this. I believe DropBox does by distributing bytecode and also using a custom interpreter. But it certainly isn't anything a regular user would be able to do.
No argument with any of this. If you are using python and speed is an issue, you can always use something like cython, HOPE or pypy to speed things up. 
&gt; You distribute programs as source code, rather than as a binary which may be an issue if you want to avoid releasing the source that seems to be a common problem with c# as well although python it's easier to decompile and even modify since you can just modify and put the .py file back in library.zip(if you use something like py2exe) while with c# you have to modify the IL code but it's not that hard
The performance issue is something of a solved issue - pypi for pure python, numpy and its ilk for numerical stuff. I've not heard of anyone who had to rewrite their whole program out of python because of performance. As to threading, it's a crummy way of doing concurrency anyway. Poor threading support is on a par with being unsuitable for kernel code: both are low-level tasks for which you should be using something completely different.
&gt; But also completely pointless It depends on how hard you're trying to protect the source. If you're just trying to provide a little bit of resistance such that the average user doesn't go mucking around in the source and getting any big ideas, distributing it as bytecode is good enough. For the Python/C++ application I work on professionally we do this as we're not terribly concerned about protecting the source from decompilation. If you want more protection then there are obfuscators (although I have not used any of them). If you need even more protection there's things like [Nuitka](http://www.nuitka.net/) (although I haven't used it). Java (along with many other languages) has this same issue in that its bytecode, if not obfuscated, is fairly easy to suss out once decompiled. It may not be a perfect solution but I think it's far from "completely pointless". It just depends on your usecase.
IME, Python is not a good choice for medium-sized or larger projects (more than say a few thousand lines and a handful of files) *if* the project doesn't have a very regular architecture. If you're doing something like a large web site using some sort of framework, it's not really a problem to handle larger projects because of the uniformity. But for a major application with lots of "moving parts" doing different things, the lack of static typing and the generally underpowered and overcomplicated packaging/distribution and module/package systems are big negatives.
They don't use a custom interpreter. They have an early stage project to create a new high-performance compiler.
That's not a virtual machine, that's a physical machine. Also, those chips are used as much for IBM to rent out mainframe cores as anything else.
it *still* won't make me a pizza
Using what abstraction? Working with threads in any language which doesn't offer an enforced shared nothing option is terribly painful.
I'm aware of the things which are out there. They're imperfect and often have backwards APIs that invert a lot of the idioms you would normally use. Other languages are better for concurrency *and that's ok*. Python wasn't really designed from the ground-up to be concurrent, it's better at other things. We shouldn't make it be all things to all people.
Analytical math. Python can't compete with Mathematica. Not that I blame anyone, the languages have widely different focuses.
This. I wanted to make an Android game, thought I'd use PyGame and call the program inside Ren'Py and use Ren'Py to build to keep it simple, but with the quirks involved, I've decided to just start learning Java instead. Python's way of drawing to the screen seems slow, and I don't think I'd be able to do what I have in mind optimally with it, at least not for mobile.
I'm interested in the machinery that's being added to support optional type annotations. I think this will be a really nice feature if it's powerful enough.
&gt; Cons: 1: Dynamic variables - Variables are untyped, The term "dynamic variable" usually means variables which have dynamic scope, rather than no fixed type. I say this only as an aside - your description of python variables is correct. 
Take a look at some of the optional static typechecking stuff in MyPy. I think they're aiming to standardize some of it in Python 3.5? I'm so excited.
&gt; No, it's the fault of the language for not having a type system that protects from doing that. No, it's your fault for not writing appropriate unit tests.
Not using threads.
Trollius is an official backport of asyncio.
This is a meaningless fragment. I'm interested in hearing about your experience, unless you mean you refuse to use anything but threads.
Are you aware of [Kivy](http://kivy.org/#home)? It's a *lot* better than pygame for mobile games (and IMO for just about anything else, but I'm a kivy dev so I could be biased about it). &gt; Python's way of drawing to the screen seems slow Pygame uses sdl1, which is not bad for some things but is vastly slower than a modern opengl pipeline. Kivy's own graphics api is much more modern abstraction of opengl and pushes most computation to the gpu, so it doesn't have these problems. Kivy is not perfect, and still has plenty of its own limitations (as does anything), I don't want to present it as necessarily solving your problems. But I do think python's mobile power should not be judged based on pygame.
Threading isn't a problem, it's just the abstractions we have over threading. Go solves this nicely by having coroutines be first class within the language, and languages like Clojure support them as well as more advanced abstractions that let you think more naturally about the problem you are solving.
As someone who hacks in Lisp (more recently Clojure), I feel like that's more of a problem with Python. Those languages expose powerful idioms for thinking about your computation, and I sometimes feel like I have to dumb down my brain when I move to Python.
&gt; Go solves this nicely by having coroutines be first class within the language i.e. by not using threading as a concurrency abstraction. Also, note that python has first-class coroutines. &gt; Clojure support them as well as more advanced abstractions that let you think more naturally about the problem you are solving. Python also has a number of alternative concurrency models available.
I don't understand how the compiler would know what type of object you intend something to be. We're simply talking about static vs dynamic typing here right? It's not a fault, it's a feature. Like most things it has pluses and minuses to it, but it would be quite a different language if it were statically typed and thus lose a lot of what makes it python. Use the best tool for the job, or if you find yourself into problems like the above, needing features in a project your language doesn't offer, don't *make* it work, re-evaluate your approach, there's probably a much better way suited for your environment.
I didn't say that it didn't try. But you can ask anyone who's wondered through the many different libraries in python, and used Mathematica, they simply can't be compared. Python is clunky, slow, and has you spending way more time working out *how to get your tools to solve your problems* where as, in Mathematica, once you've defined your problem the solutions is immediate, instantly visualised with arbitrary controls and explorative elements there just like that. This however isn't true of python in general which I use for quite a bit of other tasks, and I have great hope that it'll one day start getting closer to competing with Mathematica, but it's just not worth the hassle in the industry when a better tool exists.
I'm *not* using threads. You may need to re-read what I am putting. Almost everything in the standard library is unusable. Multiprocessing is an absolute joke, for example. gevent's API is not really all that different from threads, so it too is pretty sucky. greelet's is a joke in that it puts a lot of complexity if yielding control onto the developer. I shouldn't have to think about switching tasks. Am I writing code or am I pretending to be a scheduler. I'm not sure what other languages you've used but there are languages out there that make concurrency a total non-issue. See: Erlang, Clojure or Go. I'm not hating on Python in any way other than to say the tools which are available pale in comparison to what is available elsewhere. Implying there's no problem with the stuff that's available in Python is wrong-at-best. 
Have you tried import pizza ?
Coroutines in Go are implemented by the language as system threads.
Try qt, it's not perfect but it has pretty decent GUI builder and looks pretty native on all major platforms.
Both of those also work out of the box. Of course they can be complex, but just writing a simple web app doesn't really require anything besides `pip install django` and then writing some code. Deployment can also be easy (see heroku and dokku for examples). I usually have some templates that I use for different projects, including flask and django, which contain more advanced setup. You should look into that if you're often creating new apps and want to avoid implementing, for example, login system every time.
Regarding python code, there is only one real rule: it should run as a stateless process. State must be handled by a separate service, in most cases a database. Note that a service is not a _service_ unless it can be used like one by *any* of your processes living on *any* machine; using a local filesystem to store session ids like PHP does by default is probably the most notable offender to this rule. This seems simple, and *is* simple, and is probably how most people write their python applications, but seriously effects your ability to scale horizontally if it's not followed. Through following it, your application tier should be able to scale linearly with your load out to infinity, and the concerns of scaling move almost entirely to your storage services.
No were talking about good type systems versus none at all. I'd recommend taking a look at haskells type system. It protects from almost everything. I think you're misunderstanding my argument. I'm saying that python is a fine tool, but has some drawbacks. The question asked what the weak points of python are. In my opinion lack of a good type system is a weak point. 
Could you expand on that? I rarely miss the standard higher-order functions in python. Colin Winter's functional package is a good implementation of the basics (although now unmaintained; his website has also been missing for a couple of years).
Why should I write unit tests for something a computer can do for me? Should I write unit tests that the method is binding correctly or that the variable is going to the cpu register? 
&gt; Coroutines in Go are implemented by the language as system threads. That's great. So what? 
I whole heartedly agree. Optional type annotations is my favorite kind of type system. 
&gt; Why should I write unit tests for something a computer can do for me? Your computer can't keep track of all the types in your program. Even if you have a incredibly rich type system, there are valid uses which a type checker will not be able to prove are valid. Also, most statically typed languages (i.e. pretty much any language where all variables must be explicitly type annotated) are far more primitive than that. &gt; Should I write unit tests that the method is binding correctly or that the variable is going to the cpu register? Yes, if that's something you need to have control over, you definitely should write tests for that. 
Well that's a dealbreaker for me
&gt; IMO for just about anything else Kivy is... bad. It "works" but it's not really as good or even easy to use as java, and I hate java. I tried deploying a simple "hello world" app few weeks ago on a clean system and gave up after having to modify buildozer code for it to even work properly, well to work to the part where it just doesn't do anything and provides no debug data. It is better than pygame, but that does not make it good, but then again it's better than anything else that uses python for mobile development.
If you store as constant in your code, it is only your code which knows the entry in database means. If some other program (maybe developed by another team) is accessing your database table, they don't have the constants your CODE has. If your constants where defined in database, there wouldn't be that problem. What if the other team sets your model status to 7 and your constants len() is 4? That would fuck you up, but if you made your database correctly, you could set relations in that way that the value MUST be in the table it is related to. What I'm saying is that you can make relations which must to be valid, or the database wont create it and if object which has relations to another table cannot be deleted without deleting entries on the other end of the relations. You guys should study more SQL...
That's an interesting point, mainly as I'd almost forgotten about analytic math since college. I do a lot of simulation work but it's all with discrete solvers. And in any event all of that winds up being heavily optimized C or Fortran with what we're doing.
&gt; If you find that you can't live without static typing, then Python won't be for you Coming from an originally Matlab background I can definitely live with it, it's just something I've come to enjoy in other languages.
Thanks, I'll try this
If you're looking to replace matlab (essentially) or generally do scientific computing, the community has put a lot of work into that.
I'm sorry you had a bad experience. If you ever want to try it again, let us know what the problem is...if it hasn't already been fixed, we can try to debug it!
&gt; You clearly understand what is going on. I'm not sure what you mean. I pretty much had no idea what he was talking about until his last comment in that thread.
Peculiar. I deployed a test app within minutes. The only thing that really took time was downloading and updating (I think) a buildozer virtual machine, which was **not** included in those aforementioned minutes. :)
&gt;Your computer can't keep track of all the types in your program. Sure it can. In fact it has to in order to be able to run the program. &gt;Even if you have a incredibly rich type system, there are valid uses which a type checker will not be able to prove are valid. Sure, Im sure there are ways to trick almost any type system, but the point is for most uses it works more than fine. &gt;Also, most statically typed languages (i.e. pretty much any language where all variables must be explicitly type annotated) are far more primitive than that. Sure, that doesn't make it right though. And in times when a rich type system would be useful I won't use those languages and I'll list the lack of a rich type system as a downside. &gt;&gt;Should I write unit tests that the method is binding correctly or that the variable is going to the cpu register? &gt;Yes, if that's something you need to have control over, you definitely should write tests for that. So you're saying I should unit test every single variable in a c++/c program? 
You think for the last 6 years of working with Python, I didn't try any library/pattern or anything that helps with concurrency ? It's not fun at all, if you have any sense, you should understand it, there's no point of arguing that it's good or not anymore. Facing the problem makes it better.
&gt; Sure it can. In fact it has to in order to be able to run the program. Nope. Completely untrue. As a trivial demonstration, find the type annotations in assembler. Or, write a whole C program with only void pointers integers. It's possible (it's not pleasant, but it is possible without having the computer keep track of types). &gt; So you're saying I should unit test every single variable in a c++/c program? You definitely should if you need to control the register which every single variable goes into. Of course, you don't need to manually control that, which is why you don't write tests for that. 
There are non-Oracle options: http://www.ajile.com/
Sweet. What's the roadmap to 1.0?
Python really lowers the barrier to programming which means some atrocious looking code.
Well it's interesting you bring that up. At the moment that isn't quite the plan. Initially it looks like my dive into Python is going to be focused on prepping and gluing together an assortment of compiled libraries and executables doing the heavy lifting. And for the foreseeable future, Matlab licenses and toolboxes are easy to procure. However, I've heard there's application of Python in scientific computing, so it will be on my radar to keep an eye on and dabble in as well.
I think more important part of this comment is recursion. Since you cannot do recursion, you cannot really implement recursive data structures with it. So you cannot really use algorithms that are based on those data structures. I remember reading about it some where. They were saying that python borrowing tree algorithms from database backend.
You say as you use Reddit...
what??
connecting to mssql from unix pretty sure it's impossible
&gt; They don't use a custom interpreter. Yes, they do IIRC. Their interpreter has some custom bytecodes, and maybe all other bytecodes are shuffled in meaning, primarily to make it harder to reverse engineer.
At my work I do at least 1 Django project a week or in two weeks. Using PHP in 2014 is just like saying "It takes too much to get Django or Flask to work, so I just use PHP, since it works right out of the box".
Have you tried PyCharm? It's expensive, but absolutely worth it. 
But not *quite* as nice because it can't use the `yield from` syntax.
CPU-bound code isn't usually what's meant by a problem with concurrency. That's usually needing parallelism / efficiency of the language.
This is by design, BTW. Every recursive algorithm can be rewritten into some kind of loop, and Guido (and probably many other Python code devs) think this is the right way to do it, because recursion can easily lead to hard to understand code (not to mention debugging). "Flat is better than nested" One goal of Python is that the code written in it should be easily understandable, so "dumbing down" the code is encouraged. If you still want to use deep recursion in Python, there are tail call optimization decorators that will allow it. Most solutions use a trampoline to accomplish that.
Actually they're not. Go has its own scheduler that manages coroutines. Among other things, that scheduler moves the coroutines among the available threads (afaik you still have to specify to the runtime how many threads you want to use with GOMAXPROCS). Go's coroutines appear clever in theory, but, at least in my limited experience, using them was a pain. They didn't so much abstract the pain of threads away, as provide syntax for dealing with the pain. 
Why can't you do recursion in python? 
Also cf. http://mypy-lang.org/ This is a superset of Python with enforced type annotation semantics. I came across a post on the mailing list where it was suggested that enforcing annotations in this way might be in store for a future Python version. Can’t find the link anymore, I’m afraid.
Yes, I've tried it out and left with frustration. Planning on taking it up again at some point, but I wasn't impressed at the state it was in, I think this was some 3 years ago or so, so a lot might have changed, I haven't been keeping track of development. 
No, it's not a huge burden. But when so many things in Python are so beautifully simple, it kinda sticks out.
It is but using Django or Flask or web2py has nothing to do with it. These frameworks are designed for rapid development and it takes minutes to get them running.
Type inference would be even nicer. Algebraic data types would be cool as well. 
Python definitely has a lot of advantages over PHP, but ease of deployment for a simple page is still not one of them. Heroku and the like have made it closer, but that's still a handful of providers vs the endless hordes of webhosting companies using cPanel. A beginner can just upload their files to the document root and done. Deploying a django/flask app to that same server is probably going to require: 1. Installing a newer version of python from source, since the distros package manager uses something old and breaks if you change the system python. 2. Installing mod_wsgi from source, being careful that it uses the correct python shared library. 3. Modifying httpd.conf to include mod_wsgi, and configure a virtualhost to point to wsgi.py and properly set up python path. For a larger site, or after your comfortable with this, totally worth it. For "Bob's super awesome home page"? No, not really. 
You can definitely "do" recursion in the sense that there's nothing stopping you from using recursion... except the stack depth limitation which will almost certainly prevent you from doing recursion as an alternative to looping over large datasets - and that's what people mean when python doesn't do recursion.
If you've tried others and are happy with PHP, fine. You should still read that blog post, it lists quite a few less known gotchas that are helpful for a PHP programmer to know, IMO.
Good to know. I recently wrote a (very shallow) recursive function in python (maybe 5 or 6 recursive calls, to traverse subnodes in a fairly shallow XML file). I didn't foresee issues with it but recursion is not something I've had to use much.
Will do.
Data storage and validation, e.g. anything you should use a SQL database for.
You can, but each recursive call takes additional space so the depth of the recursion is limited. You can "do recursion" in Python, what Python lacks is called "tail recursion" or "tail calls": https://en.wikipedia.org/wiki/Tail_call
So you're the one killing the internet!
But if the SQL layer had added another 20+ ms more than PHP then I would have been sad. But instead it shaved some time off; not a ton but some time. And by nasty I had some administrative stuff that wasn't very DB efficient. So it would do things like do a query, get 800 results, and then fire off 800 statements to be executed. All that back and fourth would really show any sluggish nature of Python and SQL. But, as I said, happily there wasn't any.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Tail call**](https://en.wikipedia.org/wiki/Tail%20call): [](#sfw) --- &gt; &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), a __tail call__ is a [subroutine](https://en.wikipedia.org/wiki/Subroutine) call performed as the final action of a procedure. If a tail call might lead to the same subroutine being called again later in the call chain, the subroutine is said to be __tail-recursive__, which is a special case of [recursion](https://en.wikipedia.org/wiki/Recursion_(computer_science\)). Tail recursion is particularly useful, and often easy to handle in implementations. &gt;Tail calls can be implemented without adding a new [stack frame](https://en.wikipedia.org/wiki/Stack_frame) to the [call stack](https://en.wikipedia.org/wiki/Call_stack). Most of the frame of the current procedure is not needed any more, and it can be replaced by the frame of the tail call, modified as appropriate (similar to [overlay](https://en.wikipedia.org/wiki/Overlay_(operating_system\)) for processes, but for function calls). The program can then [jump](https://en.wikipedia.org/wiki/Jump_(computer_science\)) to the called subroutine. Producing such code instead of a standard call sequence is called __tail call elimination__, or __tail call optimization__. Tail call elimination allows procedure calls in tail position to be implemented as efficiently as [goto](https://en.wikipedia.org/wiki/Goto) statements, thus allowing efficient [structured programming](https://en.wikipedia.org/wiki/Structured_programming). In the words of [Guy L. Steele](https://en.wikipedia.org/wiki/Guy_L._Steele), "in general procedure calls may be usefully thought of as GOTO statements which also pass parameters, and can be uniformly coded as [machine code] JUMP instructions." (See history for further discussion.) &gt;Traditionally, tail call elimination is optional. However, in [functional programming languages](https://en.wikipedia.org/wiki/Functional_programming_language), tail call elimination is often guaranteed by the language standard, and this guarantee allows using [recursion](https://en.wikipedia.org/wiki/Recursion_(computer_science\)), in particular tail recursion, in place of [loops](https://en.wikipedia.org/wiki/Loop_(computing\)). In such cases, it is not correct (though it may be customary) to refer to it as an optimization. The special case of tail recursive calls, when a function calls itself, may be more amenable to call elimination than general tail calls. &gt; --- ^Interesting: [^Scheme ^\(programming ^language)](https://en.wikipedia.org/wiki/Scheme_\(programming_language\)) ^| [^Goto](https://en.wikipedia.org/wiki/Goto) ^| [^Lisp ^\(programming ^language)](https://en.wikipedia.org/wiki/Lisp_\(programming_language\)) ^| [^Recursion ^\(computer ^science)](https://en.wikipedia.org/wiki/Recursion_\(computer_science\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cld1gpn) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cld1gpn)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I haven't coded for an entire year
* Anything high-performance (systems and real-time code and so on). * Large systems where the absense of decent compile-time type checking would outweigh the other niceties. * Where totally dependency-free portability is required (e.g. C compiler as only build-time dependency, and nothing as run-time dependency).
You're right, the thing people would like to replace is most likely (only) the ORM. Django's ORM is exclusive to Django, so using it sort of ties you to Django for the forseeable future unless you want to revise all of your database interaction. At the company I work for we looked into this problem, because we had a developer that was quite knowledgable in Django. However, I was very much hooked on SQLalchemy as an ORM. I admit we haven't gotten much further than googling it, but the results we got were all along the same lines; you will lose some of the default Django functionality and have a hard time looking up problems you might encounter on the Internet. http://lethain.com/replacing-django-s-orm-with-sqlalchemy/ http://stackoverflow.com/questions/18465197/how-do-i-start-with-django-orm-to-easily-switch-to-sqlalchemy This seems to be quite old information; maybe nowadays the integratin of another ORM is smoother, which would be a big plus for Django.
It's now free for students. https://www.jetbrains.com/estore/students/
I would suggest looking for Django apps to handle CMS and blogging. Something like https://www.djangopackages.com/grids/g/blogs/ might be helpful. This way, you glue several apps together with Django configs and URL routes.
You're in the wrong subreddit. Your condescending tone is pretty unwelcome. 
Unit tests are wonderful, but so is type safety on occasion. These aren't mutually exclusive systems.
Wat.
This doesn't strike me as that weird. Just because something takes a long time to run, doesn't mean it's super-involved code-wise. It's not unusual for me to load a big dataset into memory and start some simple-to-write-but-slow-to-compute analysis on it in a Python terminal in a tmux, then leave it and go do something else for awhile and come check on it later. That kind of one-off chunk of code doesn't even get saved to a file, let alone get tested. It all depends on what you're doing.
Have you tried the [eGenix](http://www.egenix.com/products/python/mxODBC/) product? I used it and it worked quite well.
I really don't see the appeal of anonymous functions.
I'm quite happy about this, actually. IME, code that can't be lambda'ed should be named (but I'm happy to hear a counterexample). 
Even the fastest language or framework will perform like crap when you're doing an N+1 select, which is the name for doing a loop over a result set and issuing another query on each iteration. The pattern itself is a coding error, and it should be fixed, and it doesn't matter if one framework is 20 ms faster or slower to execute it, because that query is just waiting for a bad moment to put your db server CPU at 100% for half a minute, when the first result set gets bigger.
What type of student are you? Are you in a Comp Sci program? I only ask because you may have different needs if enrolled in a CS program. In any event Idle comes with Python. For that reason it is good to have passing knowledge of Idle. What i use a lot though is PyDev an enhancement to Eclipse. I have mixed opinions with respect to Eclipse but PyDev is fantastic. The nice thing about Eclipse is that it supports many other languages and editors. Have a CSV file that needs touch up - no problem as there are editors for that. Need to craft some C++ code - no problem there. HTML editing - no problem again. Eclipse is extremely nice if you aren't focused on one language all the time or find yourself working with a diverse set of files. I also have AquaMacs installed which is EMACS with a Mac GUI grafted on. It runs well if EMACS is your birthright. Me i prefer GUI apps that actually are GUI. Mac Vim exist too. I have it installed to keep AquaMacs from getting lonely. It is good for handling certain files though. My problem with MacVim and AquaMacs is that i have no motivation to learn the interfaces and much prefer a GUI. If you are on the Mac XCode should be installed. I wouldn't call it much of an IDE for Python though. It is free and you should have the native development tools installed. For something completely different look into IPython. It is a new way to look at Python development and could be extremely useful to you if your student career is in a field outside of computer science. By the way this is Interactive Python not Iron Python. 
That's an ASIC that implements the Java machine architecture. In principle you could make one for Python as well. Anything can be made suitable for embedded systems if the hardware is specialized enough. 
I'm assuming the crash I got wasn't intended. I think there is a python 3 requirement, which is unspecified. There's also a ton of required packages, which is annoying. The crash I got Traceback (most recent call last): File "D:\Python27_x86\Scripts\prospector-script.py", line 9, in &lt;module&gt; load_entry_point('prospector==0.6.4', 'console_scripts', 'prospector')() File "D:\Python27_x86\lib\site-packages\pkg_resources.py", line 356, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File "D:\Python27_x86\lib\site-packages\pkg_resources.py", line 2476, in load_entry_point return ep.load() File "D:\Python27_x86\lib\site-packages\pkg_resources.py", line 2190, in load ['__name__']) File "D:\Python27_x86\lib\site-packages\prospector\run.py", line 7, in &lt;module&gt; from prospector import config as cfg, tools, blender File "D:\Python27_x86\lib\site-packages\prospector\config.py", line 6, in &lt;module&gt; from prospector.tools import TOOLS, DEFAULT_TOOLS File "D:\Python27_x86\lib\site-packages\prospector\tools\__init__.py", line 1, in &lt;module&gt; from prospector.tools.dodgy import DodgyTool File "D:\Python27_x86\lib\site-packages\prospector\tools\dodgy\__init__.py", line 5, in &lt;module&gt; from dodgy.run import check_file File "D:\Python27_x86\lib\site-packages\dodgy\run.py", line 12, in &lt;module&gt; r'%(sep)stests?(%(sep)s|$)', File "D:\Python27_x86\lib\re.py", line 190, in compile return _compile(pattern, flags) File "D:\Python27_x86\lib\re.py", line 242, in _compile raise error, v # invalid expression sre_constants.error: unbalanced parenthesis
Reddit is written in python. 
Well duh, I know so many sites use Python. But like I said in another comment, I haven't coded in over a year. If/when I get back into it, I may teach myself to use Python for web.
What about [wagtail](http://wagtail.io/)?
The problem here is lack of documentation and/or collaboration. If a team is going to build up an entire app without even taking to the other team or reading docs then they are going to face bigger problems anyway. The original app should expose an API instead of five direct access to the DB. In case you decide to make your DB *THE* interface, then I can understand storing meanings in the database but in any case the interface needs to be documented and teams need to be in touch. 
And you use Facebook- does that mean now that you're going to write a website in PHP? 
&gt; Just because something takes a long time to run, doesn't mean it's super-involved code-wise. If you're going to run something that can take you 4 hours to run, a couple of tests can save you hours of time. It's not really about the code being complex.
True
There was a talk on MyPy at this years EuroPython. It's quite interesting!
Having rolled one distributed crawler that took days but consumed hundreds of gigabytes before parsing my overall scheme looked like this... Have one coordinator process to send out work * coordinator also puts work stamped with the time it was sent to a worker into a waiting queue invisible to workers * coordinator sweeps unverified work from waiting queue and pushes them back to top of work queue worker nodes run independently * calling back to coordinator to announce work done * provide heart beats to let supervisor process restart when needed * report errors to an error log for inspection. This is all very vague but a queue based on redis or leveldb (ssdb) cheaply replace more expensive queues. Amazon sqs provides alot of the functionality out of the box. Very achievable in vanilla python and lxml or c based libs of your choice. substitute tools of your choice where appropriate
Trivial one-time examples are handled by use of async yield (from). Often in these situations, the function doesn't need to be named because 1. It's only used one time, and 2. Its use is obvious from what it was passed in to. If you're going to have a repeated call back (data updates, progress, etc.), or you need to set up multiple chains of callbacks, the yield model breaks down. There is an argument to be made for naming things. But if you have a pile of one or two line functions and then a bunch of things to chain them together, it becomes ugly.
That wasn't my experience. I found Kivy pretty easy to use and deploy, although I only tried it on Android. It's limitation is the GUI for me but if you want to write games it should be fine.
I think Python can be used nearly anywhere. But R still seems better for pure analytical problems that require more statistical tools than Pandas &amp; co can provide, especially when it comes to fast data visualization. And Go/JavaScript/Scala... are far more advanced at concurrency-related problems, particularly for web programming (yes, we do have Tornado)... The unresolved GIL issue bites us here.
No, type inference at compile time. At the time it's compiled to bytecode. The nice syntax of dynamic languages without the need to annotate every type. 
My time is way more valuable than computer time, and in my experience, my error rate for this kind of thing is low enough that for code that's only ever going to be run once I'm fairly certain that it's a better use of my time for me not to be obsessive about testing and just accept that occasionally I'll lose a little time fixing a bug. In any event, generally if I don't test it, I'll likely litter the thing with print statements so that I can make sure it's doing what it's supposed to do correctly before it's done. For things that are going to be maintained, or live longer than the amount of time it takes to answer a single question about a dataset once, I'm more likely to test. But like I said, it depends. I have no objection to testing in situations where it saves labor, but only to testing for testing's sake.
I agree with what you're saying, but your anecdote is nonsense. &gt; I've seen a device fall over in production using what was generally tried and tested firmware, because Python's runtime only caches a certain number of compiled regular expressions on the fly. *Python's runtime* doesn't cache anything. The caching is done by the `re` library. Not only that, [CPython's `re` tends to be faster than C++'s `regex`](http://stackoverflow.com/questions/14205096/c11-regex-slower-than-python) anyway. Just use `re.compile` and be done with it. You could argue this is bad library design (I would agree), but I could point to bad library design in C++. It doesn't mean the language is inherently *anything*. &gt; Python's runtime couldn't start up and finish checking for all the dynamic environmental and path-related things it does within the total time available to run the entire process. FWIW, using `-S` to disable site packages can help reduce the import delays on certain setups with slow drives. That's not to say the problem goes away, but it can reduce it.
Some things just make more sense as a recursive function, rather than a looped function. The Collatz conjecture, for example, is very easily laid out as recursive. But in Python, you need to lay it out as a loop. Fibonacci numbers, however, I see written as recursive in Python surprisingly often and *that* doesn't make sense to me.
This sounds less like a language feature and more like a compiler optimisation.
&gt; If you still want to use deep recursion in Python, there are tail call optimization decorators that will allow it. Interesting. &gt; Most solutions use a trampoline to accomplish that. What's a "trampoline" in this context?
Good point, but to be fair, it's possible to miss an important test case. Also, test code can have bugs. Furthermore, no test suite/strategy can ever be "complete".
Would you care to elaborate on what's wrong/awkward about multiprocessing? Not looking for a fight, I'm interested in your feedback.
&gt; It's really slow. If performance is critical to your program, don't use python. Seriously, PyPy is fast. It's not C-fast, but it's somewhere between [way faster-than](http://www.cdotson.com/2014/08/nodejs-vs-python-vs-pypy-a-simple-performance-comparison/) and [about the same speed as](http://blog.kgriffs.com/2012/10/23/python-vs-node-vs-pypy.html#id-4) V8. Numpy/SciPy/etc. are also blazingly fast if they fit whatever you're doing.
All one-off one-liners, e.g. where you have to parse a file and perform some calculations. My hand automatically reaches for `awk` or `perl` that can really give you a one-liner.
Creating anything with a GUI.
And if only you could use the two together, then we'd be in business. Preferably along with matplotlib too.
Windows desktop apps
No, since type inference effectively makes the language statically typed. You can't go around changing the type of variables at runtime. Also, type inference may seldom require explicit type annotations for highly polymorphic functions. It definitely affects your program. I just want an interpreted, type safe language so badly. 
It's literally only used in one location. I suppose stuffing it into utils makes sense.
App programming, I tried Kivy and it is almost there. But it just is too finicky and the resulting apps are a bit too big. They are so close. 
Not him, but if we programmed in a pure functional way the problem would be solved. It'd be trivial for the interpreter or compiler to determine which code it can execute in parallel. One approach to the problem, at least.
That particular bit of code is run around once a month and takes 10 seconds, so a rework is scheduled for the 23rd of Never. It was a nice simple test to port first though because it does something vaguely hard on the actual data. 
Fibonacci numbers as recursion make sense mathematically because that's how they are defined (f_n = f_n-1 + f_n-2). But in terms of time complexity...never do that recursively. 
&gt;I'm not saying "thou shall not use PHP", I'm saying "don't use PHP just because you haven't tried other solutions". You can take the high road but I'll just go ahead and say "thou shall not use PHP". 
&gt;but so is type safety on occasion The problem is that those occasions are rather rare and static typing imposes a heavy burden all the time. (adequate) unit tests obviate the need for static typing though.
&gt;Furthermore, no test suite/strategy can ever be "complete". And no static typing can catch all bugs - and most don't even catch many types of type errors.
&gt; Bob from accounting is not going to be allowed to write SQL to screw with your RDBMS. He'll get an service endpoint and will like it. This isn't how the real world works at all. Bob from accounting either gets a read-only DB connection, or you get a request every other week for a different kind of accounting report. I suppose if by "service endpoint" you mean "complete dump of the relevant tables" you could be right.
&gt;I think your "real world" must be a IBM or another serious enterprise where DBAs don't talk to developer except via arcane ticket systems. Yes, large companies where the majority of programmers are employed, hence "real world."
&gt; The problem is that those occasions are rather rare and static typing imposes a heavy burden all the time. Isn’t type checking done only at compile time?
&gt; I've not heard of anyone who had to rewrite their whole program out of python because of performance. I've been on teams that rewrote really hot Python code in C and Go, both with Python C modules and complete application rewrites. One interesting reason was context switching. We ran lots of Python processes to handle message volume and avoid the GIL—more than the number of cores, because there was enough I/O-bound code—but it involved a lot of CPU context switching. When we replaced it with a Go process and let the Go scheduler handle parallelism the switches and the system load went down significantly.
Not on 2.7.5, at least.
And Guido is set against it because it screws up stack traces, among other reasons: http://neopythonic.blogspot.com/2009/04/tail-recursion-elimination.html
Even in a purely functional context, this is a tough problem. For example, one promising but challenging implementation of the idea is for [Data Parallel Haskell](http://www.haskell.org/haskellwiki/GHC/Data_Parallel_Haskell), which has been in development for quite some time already. Those guys have some impressive results already, but it's very much still work in progress. This is way, way beyond what you could achieve with a language like Python, though.
Gui. Appalling every which way. Qt? You're joking right? There are hurdles at every step b Large-scale platform with multiple developers of varying skill. We can't all be the mega nerds - some of us come from client-facing/business. You might write some amazing code on one line. I need to understand and maintain that to fix a bug for the business - but first I've got to understand your 'pythonic' coding? Write it for others to maintain, not so you can show off to other nerds. The evangelistic nature of those pushing Python for large projects in finance. The non-transferability of this to other industries. 3 years writing Python for a bank, using their own proprietary IDE (no shit!), proprietary VCS (no shit!) and propriety DB (really?). Try selling that experience to an other employer. 
This kind of question can be asked of any language. In python's case I think execution speed and very large code bases make it struggle a bit when compared to some other languages. But like other people have said here, there are ways around it. For most things its fine. And when its not fine, there's usually a way to call into some C/C++ library to make things fine again. I think the community has learnt over the years that there's some things python really stuggles with. So rather than spend countless years trying to fix something that might not be fixable, rather make the language play nice with C/C++. This is probably Python's greatest strength. Its ability to pull in foreign libraries and glue building blocks together. Its also it greatest weakness, because it leads to dependancy hell and messy build and deployment procedures. Dont worry too much about what python is bad at. Try rather to think in terms of which building blocks you can glue into your python based solution. 
Bit-twiddling. You know, you need to create/modify a specific pattern of bits, or you need to check a pattern of bits for some pattern. Some languages make it easy, but Python isn't one of them.
&gt; It protects from almost everything. At the expense of adding mounds of complexity, perhaps more than if you wrote the checks yourself. &gt; In my opinion lack of a good type system is a weak point. But that's like saying a weak point of Java is that it abstracts you away from the hardware. That's the core reason for the language's existence. This reminds me of a review of a mini food chopper I read. Under "cons" was listed that you couldn't fit in anything bigger than an onion. Of course you can't - it's a MINI chopper. If you could fit a watermelon in it it would no longer be a mini chopper, would it? A statically typed Python loses duck typing, simple, clean code and lots of other things that are its reasons for existing.
Microsoft has a free Linux driver that I use without issues with Ubuntu and PyODBC, called "Microsoft® SQL Server® ODBC Driver 1.0 for Linux"
This question gets asked so often it should have its own faq entry.
Oh the question can for sure be asked of any language or really anything. I just think knowing something's limitations is as important as knowing the strengths.
&gt;Why should I write unit tests for something a computer can do for me? 1) Because the language DOESN'T DO THAT FOR YOU AND YOU KNOW IT. 2) Because the compiler CAN'T do it for you in the sense that you'll now need to write all sorts of other boilerplate for the compiler to know what you intend, and many time you will write code that a human knows will compiler yet the compiler remains unconvinced of this. Now you find yourself writing code for the compiler rather than for yourself, and in a language that gets rid of stuff like that (such as braces) that's just wrong. 2B) So the real question is always "Why should I write code that I don't need to make the meaning clear". &gt;Should I write unit tests that the method is binding correctly or that the variable is &gt;going to the cpu register? Sigh. You do know that Python isn't a statically typed language, right? And again, should I write code that I don't need just to make a compiler happy? That's the crucial question.
&gt; because there was enough I/O-bound code I take it this was before asyncio? Did the other python async solutions not fit?
&gt; They're imperfect And what isn't? &gt;and often have backwards APIs that invert a lot of the idioms you would normally &gt;use. Normally use... in Python? Don't knock Python because it isn't Java or C#; it's not supposed to be. &gt;Other languages are better for concurrency and that's ok. Python is great at concurrency. Python developers just understand that threads were not originally intended for parallel programming (as Guido pointed out in a PyCon 2012 or 2013 keynote and some people got upset over despite the fact it's true). Processes are a natural, and *safe* means of parallel processing and Python provides rich features for doing this and dealing with message passing and even sharing memory. Few mainstream languages have this high a level of support for parallel programming. 
That makes a lot of sense, thank you!
Composition just means that one object contains another object. It doesn't have special syntax. For example: class Foo: def __init__(self, ...): self.bar = Bar(...) self.other = ... Instances of class `Foo` have a `bar` member that is an instance of class `Bar`. Since everything in Python is an object, that pretty much means you're using composition every time you create an object with any attribute. 
I actually just finished working at a company that does this very thing. If you need to use things which depend on Django ORM, just set them up with their own database, and let them do their Django thing. You can use SQLAlchemy to manage your own model interactions. 
Con #3: Code protection: You can compile to binary: py2exe, cxFreeze, etc., but I find them to be very picky and version-sensitive. This is the worst aspect of Python for me.
/r/programmerhumor Please read the sidebar. This is not the place for jokes.
&gt; what superior model do you have in mind for implementing concurrent/parallel systems? In no particular order: coroutines; asyncio; greenlets; actors.
Citation needed.
I agree it would be nice, but no-one has ever proposed a nice syntax for it.
It's probably worth also looking at the C-side interfaces to the python stuff, to see what they build on (I don't know, I'm an extremely occasional user of the statistical stuff) and if they can integrate with your existing stuff or what. 
http://www.openwall.com/presentations/WOOT13-Security-Analysis-of-Dropbox/slide-13.html
But that is not really Python specific: I wouldn't want to code with someone who can't code in *any* language.
&gt; OK, fair point, but I think we're bordering on quibbling over semantics here. I really don't think I am. The point I'm making is that you could easily write the same caching system in C++ or remove the caching system in Python's `re`. I get that what happened is a real problem, but I think attributing it to Python is misguided. &gt; in that particular case using the JIT compiled version I assume you mean "precompiled" rather than JIT compiled, no? &gt; though not without cost; using that strategy does mean that you always have to precompile your regexes that way, which could mean incurring various overheads you otherwise wouldn't Like what? &gt; If they'd used a language where some sort of metaprogramming precompiled the parser code, or even written a little hand-crafted parser, this kind of issue could not have arisen. If you're at the point where metaprogramming applies, you can just call `re.compile` to compute it at module import. Not that C++ supports precompiling regex *strings*, AFAIK; only regex expressed (awkwardly) though C++ syntax. 
The most naive implementation (recursive, not memoized) will be around O(2^n ) in time and O(n) in space (I think). This is the version I meant to never ever do. Memoized recursive implementation would indeed be O(n) in time and O(n) in space. Not bad. The iterative solution (c=a+b,a=b,b=c) is still O(n) in time but is now constant in space. 
The idea is to do no codeing at all.
But that also means you can't define a class inside another class expecting an instance of Bar to require an instance of Foo, which is workable with it for now until there is need for what I am referring to. Thanks
Research shows is the primary product of an agency. Big trust issues due to that.
Those tools are all primarily useful for concurrency alone, though, not for more general parallel operations. In other words, as substitute for thread-based programming they have limited applicability. I do realise that in your own posts you have only commented on concurrency in this discussion, but threads are widely used for other types of parallel execution as well, and I think it's fair to say (as several posters here have) that this is not Python's strongest area.
It is computationally expensive to determine what is and is not a `datetime` during JSON deserialization if there is no JSON schema (such as a JSON-LD `@context`) to indicate which fields to try and map into a (timezone-aware) `datetime.date` / `datetime.datetime` / `numpy.datetime64` / `arrow`; I didn't mean to imply that it's not a specific limitation of jsonpickle.
Naive implementation really is awful, I agree. Didn't think about the space complexity of the iterative version, thanks! 
Thank you for your time and input!
That, so much. If python had a good type system, it would be the only language. (well nearly) https://www.youtube.com/watch?v=SWTWkYbcWU0 talks about type systems. Just need to fit a good one into python. Maybe, just maybe it would be worth breaking things yet again, maybe it can be done with out breaking compatibility. 
Nope. I've been using it / experimenting with it since March.
Type is simpler to use than building a test. Each gives you a clear way to check something different. (logic vs type) Type can be useful to more than just the programmer. (compiler, ide, tools and so on) 
cpython i would think
&gt; Situations where recursion is favorable Recursion can always be written with a while loop and is usually clearer (and faster).
Why no try something like the Unity engine using Boo?
I've got a very thorough understanding of Celery and am using it in production where I work (with rabbitmq) -- you can message me if this is the route you go.
&gt; It's really slow. If performance is critical to your program, don't use python. Depending on your code, you can make use of numpy and scipy. Very intensive matrix manipulation can certainly be written in python. You just need to vectorize your code. If you rely on lists, your code is going to be slow, but numpy arrays use static typing. Numpy in CPython is faster than PyPy's (limited) numpy and I don't see that changing any time soon.
How did you link to Go? I assume you used a system call. That go-back seems like it'd be really slow.
&gt;It's not fun at all, if you have any sense, you should understand it, there's no point of &gt;arguing that it's good or not anymore. Facing the problem makes it better. Why are people saying that it's "just not good" not able to clearly explain why? The consensus reality is that Python's concurrency is actually higher level than most other languages'. 
celery + rabbitmq + rethinkdb for me :) Good luck for your project 
I like the [confident code](https://www.youtube.com/watch?v=T8J0j2xJFgQ) approach: max(int(a), int(b)) 
It's not worth arguing about anymore; the folks complaining apparently can't explain what specifically the problem is, just that it's "unusable" or "not fun". Meanwhile, Python has a much higher level of parallel programming than most languages do natively. Try programming a language with nothing in the standard library other than low level threading and a standard library that is filled with non-thread safe functions and one comes to appreciate immensely how simple and powerful, and yes, fun, Python's concurrency options are. 
Not sure exactly what your asking, does this work? class Foo(object): class Bar(object): def __init__(self, foo): assert isinstance(foo, Foo) def __init__(self): self.bar = Foo.Bar(self) 
Qt and wx certainly get the job done. &gt; 3 years writing Python for a bank, using their own proprietary IDE (no shit!), proprietary VCS (no shit!) and propriety DB (really?) Now you're just complaining. My company develops it's own proprietary tools. That's normal. I got so annoyed with one of them because it was underfunded yet it hobbled along, so I wrote an open-source knockoff and killed it. Now I get paid to work on it. Companies develop tools because they are cheaper or don't know about an alternative.
use the free one, unless you need the extra stuff for like javascript of django it is just as usefull
I really need to write more tests, any good resources on writing tests?
How in the world are you being down-voted for saying this *in a Python subreddit?!?* A score of -2, so I guess that would be Bjarne Stroustrup, Anders Hejlsberg and Niklaus Wirth downvoting you? I came to Python with 17 years of professional computing experience and yes, once I got past the knee-jerk conventional wisdom such as "it's always better to catch errors as early as possible" and other folk sayings, I did indeed find that I didn't miss it. I'm not the only one: http://blog.codinghorror.com/loose-typing-sinks-ships/ 
Oh snap, you're right -- there really *isn't* a need for any other programming language! Congrats, you win the language wars.
Use [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) to parse the HTML, and `.split` to split the URL: #!/usr/bin/python3 # -*- coding: utf-8 -*- from bs4 import BeautifulSoup html = """ &lt;iframe width="560" height="315" src="//www.youtube.com/embed/YwAd-UzSO0g" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt; """ soup = BeautifulSoup(html) source_link = soup.find('iframe')['src'] video_id = source_link.rsplit('/', 1)[-1] print(source_link, video_id) 
Linux shell scripts. You need a layer of indirection to run system commands, which adds verbosity. It's a fine line, though. Sometimes you start writing a simple script that just calls commands, and as it grows you find that you would really like some high-level constructs like dicts and lists, and then it's time for a proper programming language.
Because I like writing code, and learning programming. Unity is nice, but all the time spent learning it won't translate into any other form of software development. Learning Java and libgdx will. Plus, Boo to my knowledge is .NET based. I'm working in a Linux environment, which makes that, while not impossible, a bit of a headache.
&gt;&gt; The point I'm making is that you could easily write the same caching system in C++ or remove the caching system in Python's re. &gt; You could, but now you're limiting the discussion to quite a specific alternative, viz. using an analogous regex library in C++. I don't see how that's relevant; these approaches completely solve the problem you were having. There may be other approaches, but that doesn't make the approach I'm referring to any less relevant. &gt;&gt; I assume you mean "precompiled" rather than JIT compiled, no? &gt; No, I really meant JIT compiled. `re.compile` is still doing the compilation at runtime Using `re.compile` at import is effectively AOT compilation (although less so than at program compilation). Using `re.search` is JIT compilation. You're moving *away* from JIT compilation. Note that you said &gt; using the JIT compiled version of the regexes would have solved the problem but this implies that `re.compile` is somehow more JIT than `re.search`. &gt; unnecessarily compiling a regex that it turns out you won't use, Considering you wanted compilation when the program compiled, I don't see how that could be an issue. &gt; keeping the precompiled versions around somewhere How else could you avoid recompiling it? &gt; manually sharing them around different places that use the same one Just don't share them, then. The cache will get you 90% of the way to deduplicating if memory is an issue, although it's almost definitely not. &gt; Again you're implying a specific alternative. You were comparing against C++, I was just responding. 
I do!! :) I write my backend code in Node and farm out the work to Celery Workers. It's very fast and scales very well. Using it in production.
Depending on how far down in size you're going; if you need to fit on an 8-bit chip, yeah, python is out. If you're running on a 32-bit arm processor, it's well worth considering. In many embedded use-cases, a $5 to $20 dollar processor isn't prohibitively expensive.
Oh wow, thanks! this is perfect! Looks like i'm going to be looking a lot more into beautiful soup and what else it can do! :D
Anything having to do with interactivity or animated graphics is a pain because of the general lack of good libraries and documentation for them.
How well does it work when you have a lot of dependencies on other open source modules?
Are you talking about compatibility? PyPy is compatible with pretty much everything except stuff that uses C extensions. 
I am generally fine with it, so I can make a plug for my own editor, but not this time ;)
Cool, a few of the modules I use need c extensions, is pypy compatible with some of the more popular ones (like lxml)?
I wrote two articles talking about using Pelican for my blog. Rationale - http://pbpython.com/site-tech-1.html Technical details on install - http://pbpython.com/pelican-config.html So far I've been happy with the choice.
v1.5 beta of the Web Tool is now available: https://www.mediafire.com/folder/8cldkmvzsha1d/Desktop_Website_Tool_v1.5_(beta)
Like you can't use KeyboardInterrupt to terminate you process, and have to use signal to pass the interrupt to the main process. Even when you does that, it rarely works, and you have to implement another parentid checking, the so-called "suicide when parent die"
Ok, I was just curious to see how much effort would be involved to add support to my current project. I'll dig a bit deeper.
Theorem proving. Realtime systems. Mobile app creation. Safety systems. 
I use Python! Now shut up already about it.
But then, you don't need to worry about the size of your ints if you use a long for a bit-set for example.
Is there any way to interpose Hy with regular python? Would this even be useful? I'm imagining a situation analogous to Julia's macros, where you can define a macro and use it with imperative type programming. 
Mainly, stuff I have to distribute. Shipping a whole python runtime, plus modules and everything that my app requires can quickly get absurd. A simple static linked native binary can be much easier to deal with in some cases.
I guess that's a good thing, then, because the inline version would be significantly slower because it contains the `isinstance` check. That said, you can already do what you want: for match in matches: def action(j, f): if isinstance(container, MutableMapping): j[f] = value else: j[f].append(value) act_on_path(json, str(match.full_path), action) Writing act_on_path(json, str(match.full_path), (lambda j, f: if isinstance(container, MutableMapping): j[f] = value else: j[f].append(value) )) isn't really any better IMHO. If you want that kind of stuff, I'd suggest making a `given` decorator: for match in matches: @given(lambda action: act_on_path(json, str(match.full_path), action)) def _(j, f): if isinstance(container, MutableMapping): j[f] = value else: j[f].append(value) which is pretty trivial to write: def given(callback): def call(function): return callback(function) return call and use: def with_callback(f): return 10 * f(1, 2, 3) @given(lambda callback: with_callback(callback)) def result(a, b, c): return a + b + c print("result = ", result) #&gt;&gt;&gt; result = 60 
&gt; I just want an interpreted, type safe language so badly. There's a lot of languages in the ML family that fit the bill here, OCaml and F# being the main contenders.
Unit tests are basically just as much work as static typing. 
Not everything is memorized, non memorized fib SUCKS
&gt; Consequently there is a need for techniques like the cache behind the re library You *don't* need the cache. Just use `re.compile`. &gt; However, doing so typically involves doing part of the work, such as sharing compiled regexes around your code base Why would you do this? Just use them locally. &gt; in a language with a strong and static type system and expressive metaprogramming/macro facilities, the example issue would not arise If that language used a cache, it would. A cache might be useful if you're using a lot of dynamically-created regexes. If Python's `re` didn't have a cache, the example issue would not arise. What you're talking about has literally *nothing* to do with the dynamism of Python. &gt; nor any need to jump through hoops when writing code instead of using the tools naturally The code looks pretty much identical in both languages. I don't know what hoops you're talking about. &gt; There are plenty of other cases where that same relatively high-level language and dynamic-everywhere behaviour will be a potential problem with embedded systems, though. Not arguing with that. &gt; Please remember that we're talking about a field where in some cases even basic programming techniques like recursion and dynamic memory allocation are completely forbidden on the grounds that they can cause unpredictable run-time failures. No, I was responding to your anecdote. As I said "I agree with what you're saying, but your anecdote is nonsense." I would probably never bother with anything more dynamic than, say, Go on an embedded system.
vim
I'm a bit swamped at the moment, but I'd definitely love to share this seeing as there's some interest in it. I'll see what I can do :)
Condescending? All I'm doing is answering the topic with my opinions and then defending them. I think python is a fine language which is why I'm here in the first place, but there are some limitations and I'm just expression how I feel about them.
Huge ass projects that live for 1 year+. The dynamic types will get you. 
there are times (very rarely) when dealing with data where I just think... this would be like 2 lines in SQL instead of all these nested for loops
I think Nikola is a good choice. Http://getnikola.com
Thank you for posting this. I've noticed this as well, and I would add to your list "check out my project X", where X happens to be written in Python but otherwise completely unrelated. I love seeing posts that talk about Python as a language or discussions about specific techniques. These also tend to be the more highly upvoted posts, so it seems I'm not alone.
Algebraic and recursive data types would be a really nice addition but I fear they may break the beginner friendly style of python. Optional type annotations wouldn't really change anything for beginners but would add a lot of ease to writing more complicated systems.
I wish they would add it as an option. It's kind of unfortunate that he's just flat out refusing it.
While you definitely have a point, you may want to look at [python-bitstring](https://code.google.com/p/python-bitstring/), which provides a nice interface to do basically anything ever and then some that you might want to do to bits.
OP, start simple, read the file, break it down, line by line, examine field lengths and recreate the file. Your going to have an input file, the original that you need to read and and new file that you need to create, the output. No need to make it complex at the start. Practice with a small 10 line file, get it printing what you want and then change the prints out writes. Stackoverflow will have some great examples of the syntax you'll need. http://stackoverflow.com/questions/2549746/python-csv-large-file-with-rows-of-different-lengths http://stackoverflow.com/questions/6159900/correct-way-to-write-line-to-file-in-python
PyTools for Visual Studio Pros: Excellent debugger, code insight, package helpers, multiple python run times Cons: windows only love it
Works on 2.7.6 for me.
But you have to understand that the format of reddit causes the most *easily consumable content* to be the most likely to the upvoted, which is why the crap bubbles to the top. Without a concerted effort and rule enforcing, any large subreddit becomes dominated by easy-to-digest, crappy (but funny, don't get me wrong) content. It's not even that people do it intentionally, or for the wrong reasons, it's that reading a complicated, super long article on ... I dunno, signal processing using Python takes ages to read and doesn't garner votes. A joke (although, does this subreddit get jokes? I never see any...) is upvoted immediately by the most casual of users. That being said, the "can you help me with X" posts are always voted to 0, so I agree I don't see why they need to be deleted. A polite "please post on /r/learnpython" is more than sufficient. Although they do tend to clutter up the page.
Concurrency, most specifically CPU bound threading. Never.
OK, we're obviously not communicating here, so perhaps we'll just have to agree to disagree and leave it there. What I'm talking about has everything to do with Python's dynamic nature, in contrast to the alternatives I'm suggesting that would compile right down to optimised native code in advance and therefore would not require either cache support or JIT compilation at run-time to achieve decent performance. From the way you're writing, I'm wondering whether I've been clear enough and you've fully understood what else is possible here. I really do recommend the link to the post about Rust that I mentioned earlier, and then following through to some of the other examples that it links to in turn. You could also look up how the more advanced compiled/statically typed languages handle things like format strings for their equivalent of `printf`, or how they handle parsing very dynamic, loosely structured data formats like JSON and XML. You can do some pretty amazing things at compile time with the current state of the art, and these alternative tools offer essentially all of the advantages of Python's clean, maintainable code with none of the run-time overheads or unpredictability in this kind of area. For embedded work, that is a much better strategy IMHO.
There is no Windows equivalent to ssh, at least nothing that can do what op wants.
Are you running this on IronPython or cpython?
No, doesn't unit testing also deliver type safety for wherever you've got code coverage? The only place I ever get production failures due to type safety is code that hasn't gone through unit-testing. Mostly typically, it's in error-handling for catastrophic conditions (out of disk space) that I didn't feel like adding to a test-harness.
Examples? I just checked the first page and didn't see anything particularly off-topic. In any case, I don't think this is a particularly wide-spread worth worrying about at the present time.
Wait, what? First off, why are you defining one class inside another? Nested classes have their uses, but they're few and far between. Next off, why do you think you would ever need that? That's the most awkward programming requirement I think I've ever heard. That's never even come up for me in java, much less python. 
Grooooss. I think it does what he wants, but why? Why does Bar need to be defined inside Foo? (And of course this flies in the face of duck-typing, but that's another issue altogether) This whole scenario is starting to look more and more like some sort of wierd hyper-OOP style thing. (But again, I wouldn't even do this in the common hyper-OOP languages...)
&gt;the format of reddit causes the most easily consumable content to be the most likely to the upvoted I find this is much more a problem the larger a subreddit gets. Small, community-focussed subreddits tend to consist largely of people who care more and have a larger attention span, and a better sense for good content.
...Everything else?
It has the most important features required for [design by contract](http://en.wikipedia.org/wiki/Design_by_contract). The project is old and clearly not active, but requirements are so simple that it does not matter at this point.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Design by contract**](https://en.wikipedia.org/wiki/Design%20by%20contract): [](#sfw) --- &gt;__Design by contract__ (__DbC__), also known as __contract programming__, __programming by contract__ and __design-by-contract programming__, is an approach for designing [software](https://en.wikipedia.org/wiki/Software). It prescribes that software designers should define [formal](https://en.wikipedia.org/wiki/Formal_methods), precise and verifiable interface specifications for [software components](https://en.wikipedia.org/wiki/Component-based_software_engineering#Software_component), which extend the ordinary definition of [abstract data types](https://en.wikipedia.org/wiki/Abstract_data_type) with [preconditions](https://en.wikipedia.org/wiki/Precondition), [postconditions](https://en.wikipedia.org/wiki/Postcondition) and [invariants](https://en.wikipedia.org/wiki/Invariant_(computer_science\)). These specifications are referred to as "contracts", in accordance with a [conceptual metaphor](https://en.wikipedia.org/wiki/Conceptual_metaphor) with the conditions and obligations of business contracts. &gt; --- ^Interesting: [^Eiffel ^\(programming ^language)](https://en.wikipedia.org/wiki/Eiffel_\(programming_language\)) ^| [^Bertrand ^Meyer](https://en.wikipedia.org/wiki/Bertrand_Meyer) ^| [^Exception ^handling](https://en.wikipedia.org/wiki/Exception_handling) ^| [^Object-oriented ^programming](https://en.wikipedia.org/wiki/Object-oriented_programming) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cldhqvd) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cldhqvd)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
yeah I really don't know what he is asking, I think it relates to nested classes in Java and some miss understanding? 
I have to know what you mean!?!?! 
It works if you reverse the order of arguments in the assert line. But this is most certainly not composition: that means that you gain polymorpism through containing other classes instead of inheriting from them. This...nested class thing is something to be avoided: I can't imagine any good use for it. I highly suggest finding some simple solution, as you almost certainly do not need this kind of code.
It makes a great calculator.
Medium to high level networking code (i.e. sockets and above). Compared to Java or C, using Python just feels like cheating.
ah yes, typos typos
has to be what op is thinking of. also java makes me sad 😔 
Looks like a neat tool! It would be nice to have a http://pre-commit.com/ hook for it. I currently use pre-commit to run autopep8 and flake8 and a few other tools on most of my projects (and make it easy for my coworkers to do the same), but it looks like prospector could replace most of those tools.
Your problem is that you don't have any problems to solve. Go find some.
I use it for data analysis on medium to small data sets (100s of Mb to a few Gb), visualizations, admin automation, and basic game programming.
You may get more relevant google results if you're adding 'mixin' to the search, e.g. http://stackoverflow.com/questions/533631/what-is-a-mixin-and-why-are-they-useful
Mechanical engineers trying (and failing) to write macros in Excel to analyze data. Mechanical engineers struggling to read data out of 100's upon 100's of data files from their test rigs. IT can't support them fast enough, VBA is hell, and Matlab is too expensive. Python is excellent. I use Python at work to query our database, extract a ton of info, generate these big statistical models and predict warranty / reliability of our products, then generate Excel workbooks, charts, etc. Also wrote scripts to edit Excel files, upload data to DB, etc.
&gt; These also tend to be the more highly upvoted posts So where is the problem? Upvote those you like, downvote those you don't, and go on with your day.
I too am surprised by this post. I have not noticed too many "help me" posts or "joke" posts. Current subreddit content is fine with me.
teaching
Maybe you're being sarcastic, but when I need to calculate something I open up `python` instead of something like `bc`.
I'd say this is a most off-topic post I've seen.
I use it for machine vision - color and motion isolation and tracking. There's probably a performance hit over doing it in C++, but it's quicker and easier to prototype. That gets fed to a robot arm simulator (and indeed a robot arm) written in python with pygame. Data visualisation with numpy and matplotlib, etc. Pretty much everything.
I've been in one situation where I couldn't think of a better solution, specifically creating SQLAlchemy schema based on config options. 
Do you mind if I PM you about CV in python? I'm really struggling to get started. 
I'm betting that you are using reddit through RSS. The thing is that the homework-type of questions get down the subreddit page pretty quickly. So, people comming to the site see a filtered reddit. Posts don't get deleted, but you will never see them. It's far too easy to say "nope, never delete posts, I'm against it" while you see 1 crappy post out of 100. When using RSS, you see everyone of these posts.No community filter. And very nice people keep answering these questions instead of suggesting /r/learnpython. I'll see if I can recover and categorize old RSS feeds from this subreddit.
Facebook is to PHP as my house is to Python.
I have to disagree. There are a fair number of problems where recursion is more intuitive than loops (for example traversing a tree-like structure). And I'd argue that for most problems it comes down to your personal background if you find one or the other to be clearer.
PyCharm has the same level of autocomplete features for Python as for something like Java, so that's really not a good argument. Java and C# have fundamentally broken type-systems as they allow anything but primitives to be null. So something like public String getHostname(); does not really provide a `String`, but rather a `String`, `null` or a `RuntimeException`. Conversely public int indexOf(String str); has no representation of an absent value, so we have to resort to conventions like "-1 if it isn't there". This kind of static typing is not really helpful and we have to type an awful lot to make the compiler happy. Languages like Scala (also Swift), Haskell and others have addressed these kind of problems which makes their static typing actually worth the effort. 
As long as you don't mind reading C++ documentation and mentally translating it into Python. Tkinter is the same deal with Tcl (with the added drawback that it makes horrible UIs). There's just no *native* GUI experience for Python, nothing that was actually designed to be used in Python instead of being a thin wrapper around another programming language's GUI. This problem might stop being relevant when desktop apps stop being relevant... but Python doesn't run in your users' web browser either. Anyone who wants to make a sufficiently good GUI for their Python code will have to know two programming languages.
Spot ON!
Refer to zahlman's comment
I am actually learning OOP (coming from C as my first language) and I have started reading some of Alan Kay's writings. What are nested classes like in Java because I am yet to feel drawn towards Java.
You can actually make it generic, and then provide adapters for each of them.
I don't think they were, I think they were referring to it's emulated long ints.
I love call-to-arms posts on my unpopular subreddits, they usually create hilariously unnecessary drama
What do you mean by "otherwise completely unrelated"? Some sort of python based application, rather than a library aimed at other python users? Or would you consider the later unrelated, too?
I also had a not very good experience with kivy since help is very poor except for the IRC where they do not really answer your questions. It has a designer GUI but it is experimental and not ready for use. 
/r/coolgithubprojects (it's in the sidebar, btw)
It isn't, but there are times (like traversing a nested data structure for eg) when it is the cleanest, most "Pythonic" way to implement in code.
Anything involving binary formats, or places where typing matters. Python is great, but realizing that it does all sorts of shenanigans behind the curtain was a bit of a let down. That might just be because I never really learned how to do them in Python - I learned in C, default to it when the going gets tough. Edit: grammar
Looks like you're not the only person to have this error (unless you are the one that reported it!) https://github.com/landscapeio/dodgy/issues/2 I'll dig into it, thanks for the stacktrace :-)
Yes it does
In Python we don't do that, because if you actually do want a closure for some reason (seriously, is making an explicit back-reference to the depended-upon instance such a burden?), you can nest *functions*. Most uses of inner classes (especially anonymous ones) I've seen in Java have been to implement "Command pattern"-type interfaces, and we don't have those in Python, because we don't need a Command pattern, because *functions are first-class objects*. It would help if you showed the exact problem you're trying to solve.
&gt;trust issues code is open source.
Thanks, subscribed.
You also see them by going to the 'new' tab.
I am just learning Python so I want to see what can be done and what can't be done before deciding what I will use it for
I feel you. I'm always a bit uneasy when bit shifting in Python. But on the other hand you have to love 'struct' .
I always wonder if there is something like linq (.net framework) in python, which isn't a list comprehension 
Of the first 20 submissions on the first page 12 of them are self.python submissions. That seems like a pretty high ratio to me.
- Flask comes with minimal set of functionalities like `routing, http request, response, session handling`. - Flask doesn't enforce how to structure your app, as result you need to experiment with few structure and come up one that suits your app. - Flask doesn't come with default authentication backend like Django. This can be useful or time consuming depending on the app. Said all that flask provides limited set of functionality required for web application. It stays backend, authentication agnostic. 
Don't want to annoy you, but if you ever come back: http://shop.oreilly.com/product/mobile/0636920031116.do It was released this year, so it's quite up to date. I'm still at the beginning of the book, but it seems quite good.
&gt; Qt? You're joking right? There are hurdles at every step b Try QML and PyOtherSide. They are much easier than PyQt or PySide because there is more abstraction between UI and logic.
[Understanding the Python GIL (PyCon 2010)](http://pyvideo.org/video/353/pycon-2010--understanding-the-python-gil---82)
Python increases/decreases references almost every time an object is accessed, Java (and PyPy, btw) computes references counts only during GC cycles. It gets stranger when you compare Python to Linux kernel. The kernel relies on refcounting in many places (even in performance critical code like mm) and is still one of the most scalable and concurrent pieces of software in existence. CPython could employ techniques known from the kernel, like atomic operations, memory barriers, wait-free algorithms or RCU to get rid of GIL and keep its deterministic reference counting model without serious performance loss. It's just that nobody tried it yet. Why not? Many people see PyPy team's efforts very promising and simply wait for Armin's brain to do the job. Others (like Django and web folk) are not affected by GIL at all and there are also many people like me who are perfectly content with multiprocess parallelism.
I've used lxml in a pypy project without issue. 
And the static types will limit you. It's a lose-lose either way.
Thanks for the explanation!
I feel the same way about my experience. "I'm retarded?" The packaging still throws a warning saying it doesn't recognize the tests_require field, even though documentation led me to believe it's a valid field. 
Self posts and off-topic posts are completely different things.
&gt; Most while loops are simply taking a recursive function and expressing it iteratively As opposed to taking an iterative operation and expressing it recursively? How can you tell the two apart? &gt; Recusive algorithms are almost always shorter Really? Anything traversing a tree is a pain to do iteratively, but for a typical one- or two-pass iterative calculation? Here's the first example I made up: gradients = ((y2-y1)/(x2-x1) for (x1, y1), (x2, y2) in pairs if x1 != x2) How exactly could you improve on that? Even the verbose mode: def gradients(pairs): for (x1, y1), (x2, y2) in pairs: if x1 != x2: yield (y2-y1) / (x2-x1) is pretty hard to improve on.
Actually no, I just tend to 'hide' posts a lot once I've skimmed them or if they're not relevant to me. A lot of new posts here with upvotes in the 4-10 range aren't really that relevant. I do think it's nice of people to answer questions, but... I also think it encourages bad behaviour (i.e. posting on the wrong subreddits, reducing the quality of reddits overall).
I think we already have that, though. You can read the sidebar just as well as I can, and we already have /r/learnpython for questions about how to use python. I don't think, generally speaking, it's good for all forums to become "general forums". To me, it means that finding what you really need (Whether that's "higher level" articles or beginner help) becomes more troublesome.
The problem it's solving is that of thread safety. Essentially, if you have multiple threads, you need to worry about what happens when two of these are monkeying about with the internals of various objects, otherwise you can end up corrupting the state, leading to nasty problems. There are various solutions to this problem, the most common being *locking* - essentially before you access an object, you need to acquire a lock, which will only be granted to one thread at a time, thus preventing them trampling over the internal state of the object. However, there's some variation in how *granular* your locking is - ie how much is covered with one lock, with python and java's approaches being at different ends of this spectrum. The GIL approach is extremenly coarse grained locking. Essentially, you just have one lock, and acquiring this covers *everything* - all python objects are managed by the same lock. This means that you're basically locking out all threads accessing python interpreter state, which is pretty much everything, except various bits fo C code that temporarily release the lock. The Java approach is more fine-grained locking. Each individual item has its own lock. If you want to modify a dictionary, you acquire a lock specific to that dictionary, modify it, then release the lock. As such, the only threads that are prevented from running are those that are using that same dictionary. There are advantages and disadvantages to both approaches. The obvious disadvantage of the GIL approach is that only one thread can be doing anything with python objects at the same time. This was not such a big deal prior to the prevalence of multi-core systems, since this would have been the case anyway, but in the case where you're got a CPU intensive task, and want to take advantage of multiple cores, it basically means you can't - only one core can work on the problem. The advantage is that there's much lower administrative overhead. You acquire one lock, do a whole bunch of work over potentially dozens of objects, and then release the lock. With fine grained locking, you'll likely be doing dozens of lock/release calls for every little object, which can add up to a significant slowdown, especially in single-threaded code where you're not actually getting the benefit out of the multiple cores, but are still paying the administrative price. How expensive this latter is depends on how many objects you're doing all this locking on, as well as how clever you get in ways to minimise such locks. This is where the reference counting issue comes in, because it inflates the amount of locking required. There have been a few efforts to make python more fine-grained, but generally, the expense of the fine-grained locking has been sufficient to drown out any gain. Java does use this method fairly successfully though, as it has a few advantages over python. Most notably, it's had a lot more work put into optimising it to reduce any impact from the locking. It also doesn't use reference counting, which avoids a lot of locking python requires.
It's a turing complete language, so you can do anything. (Even getting out of the turing tarpit, it can do whatever you want.) Sadly, I wouldn't use it to make an OS, but most things past that level of performance requirements are fine. You shouldn't ask hyperspecific questions about a particular technique to get a feel for a language. Asking about looping, general class definitions, etc. is fine, but this particular question is way too specific to tell you anything worth knowing.
From my frontpage (Having hidden things once I'm done with them), here are som articles I think are inappropriate for /r/python (And better fit for example /r/learnpython), and definitively are now "News about ... python" or artictles about the language: * https://www.reddit.com/r/Python/comments/2jop3r/why_is_there_a_gil/ * https://www.reddit.com/r/Python/comments/2jnr23/what_concurrency_primitives_would_you_choose_to/ * https://www.reddit.com/r/Python/comments/2jnc4y/parsing_a_string/ * https://www.reddit.com/r/Python/comments/2jly46/pycharm_live_templates_where_is_the_xml_file/ * https://www.reddit.com/r/Python/comments/2jjf4f/just_installed_miniconda_updating_all_packages/ Most are downvoted, yes, but I've already hidden most of the ones that got upvotes in the 4-10 range, despite now "belonging" to this subreddit (According to the sidebar).
Which is how I feel. I would prefer that moderators delete such topics and refer the posters to /r/learnpython.
&gt; PyCharm has the same level of autocomplete features for Python as for something like Java, so that's really not a good argument. But if I were to define a function that takes an input 'MyArg' of type 'MyClass', there's no way of specifying that in the function definition, no? So how can the IDE know anything about 'MyArg' or autocomplete anything? Though I certainly agree with your subsequent statement - takes extra effort to get around nullness!
&gt; That being said, the "can you help me with X" posts are always voted to 0, so I agree I don't see why they need to be deleted The reason I feel like they need to be deleted is that not deleting them, especially as the threads do get answers, encourages people to use /r/python as a "noob help" subreddit.
People have tried splitting the GIL up into smaller locks before. The problem is that so far it's always sacrificed single-threaded performance.
&gt; Once you get used to "duck typing", you (likely) won't miss it most of the time. How is the type system in Python different than Matlab's? Learning programming in Matlab and then moving to C# and C++ my initial reaction was, "I have to *tell this thing* what type everything else? Why can't it just figure it out like Matlab does?" But then I came to like the static type system both for autocomplete and making sure things "fit together" appropriately at compile time, e.g. not passing a String when a method expects Double or what have you. I can definitely program equally well either way. Maybe I'm just particularly disappointed now in Matlab's IDE/Editor after having gotten used to Visual Studio :)
Being brand new to this language this looks *extremely* useful. Have some new gold!
Why not make an OS with it? If you get a hardware implementation of the PVM why not?
&gt; IT can't support them fast enough, **VBA is hell**, and Matlab is too expensive. Cannot agree more. Amazes me how many engineers stick to VBA *even when Matlab is available or installed on their machine*. Excel/VBA is so outrageously slow it boggles the mind. To date though the places I've worked have had no issue procuring Matlab licenses. "If it saves time, it's worth the $". I'll be curious to see how Python compares for some number crunching and visualization.
Libraries are generally interesting. Applications themselves seem less relevant. However, that doesn't mean there isn't interesting parts about in the implementation details. But "I wrote an application that scrapes the web in python" is a good example of what I think doesn't belong.
One thing I dislike the most with python is import side-effects. So for me this is the opposite of impressive.
... So your complaint is that they are 'reaching the front page' but the only reason they are is because you're hiding everything else? Not only that, but they are downvoted? So basically your complaint is that the mods aren't removing downvoted content and you're going far enough down that you see it. I have a better idea, once you get to downvoted posts *get off reddit for a bit and do something else until more content arrives*. 
Among other things, I find Python useful: - to write server software and the back end of web sites (it offers a good range of networking and database tools, libraries to handle common data formats, and the ability to call our to other things via C APIs); - to prototype complicated data-crunching algorithms and experiment with new ideas; - as a scripting language (to automate whatever other tools I'm using, instead of relying on OS-specific shell scripts, hacks using makefiles, and so on). I haven't done much GUI work with Python personally, but I have colleagues who find it useful for creating simple GUIs for databases and instrument control/data monitoring applications as well.
&gt; No, doesn't unit testing also deliver type safety for wherever you've got code coverage? No, it doesn't. Unit tests prove *a* case work. Type systems prove *all* cases work. If your system design is relatively uniform, say a large web application set up with a framework like Django or Flask, IME this isn't usually a big problem. Almost everything is going to be fairly uniform and text-based by default. If you're doing more heavyweight computational work, where maybe you've got a variety of user-defined data types and a lot of transformations from one thing to another, then the lack of pre-execution type safety in Python is a *huge* liability, and the kind of scenario /u/Octopuscabbage described is all too plausible.
Thanks :)
Take this with a few grains of salt -- the truth lurks somewhere in here, but I may be embellishing some things :) Turn back the clock a while. Like, a *while*. A couple decades, in fact, to when Python was young and Java didn't exist yet. Python came from the Unix scripting-language tradition, which is important because Unix had support for multitasking, and so anything which really *needed* to do concurrent work could do it by forking new processes. Then Java happened. Java went through several re-targeting phases in its early development, starting out as a platform for interactive TV boxes. Which, notably, didn't have multitasking. And even when it emerged as a general-purpose programming language, it still needed to run on devices/operating systems where traditional multi-process multitasking wasn't available. So in Java, the rule is "I am the Thread thy Concurrency Model; thou shalt have no other Concurrency Models". And, well, Java got kind of popular, and people learned Java, and then branched out into other languages. Like Python. And started saying that Gosling had smashed some stone tablets over their heads and forced them to use threads for everything, so could Python get, like, threads, please? --- OK, so enough with the funny history, because this is the part where we get to why the GIL happened. In order to implement threading safely, there were basically two options: 1. Rewrite a lot of Python's internals and garbage collector in a way that would be backward-incompatible with tons of existing code and extensions to Python, or 2. Bolt on a big lock to make things safe, at the cost of performance in some cases. What do we mean by "some cases"? Well, turns out this big lock could be implemented in a way that wouldn't be terribly noticeable in single-threaded programs, and wouldn't even really be an issue in I/O-bound multi-threaded programs. It would only be a huge problem for CPU-bound multi-threaded programs. And at that time, hardly anybody had the *hardware* to be doing CPU-bound multi-threaded programs, and the things people used threading for were all I/O bound anyway. So it was decided that (2) represented a good trade-off. Fast-forward back to today, and we're all walking around with multi-core computers in our pockets, which is something that was, um, not exactly expected at the time. And since we have the hardware now, lots of people want to write CPU-bound multi-threaded programs. And so over the last few years there's been more and more and more complaining about the GIL as a result. But now there's like a quarter-century of inertia behind it, plus a bunch of actually-hard technical problems (some of which you can find in links from this thread), and so it doesn't seem likely to go away any time soon.
I want to use Python instead of PHP; I'm doing an experiment where I try to remove PHP from my Apache web server and replace everything with Python based tools to see how mature Python as a language is. So far, I'm not having a lot of luck.
So you not the only person to say to use a "static site generator" but that's not what I want; It has to be dynamic just like word-press does. I don;t want to have to upload or regenerate the entire website by hand just to post an update.
Not really. On-topic is news about Python. If your self post contains news then link it properly in the subject and it won't have a self.python domain That being said, the vast majority of self posts are off-topic whether requests for help or attempts to foster general discussion.
 [wrote a script to help you get pizza](http://pastebin.com/GCcBUWMG) should work if you name the file "pizza.py" and import pizza
try kivy, I just started a few days ago, and it's really good
&gt; But if I were to define a function that takes an input 'MyArg' of type 'MyClass', there's no way of specifying that in the function definition, no? So how can the IDE know anything about 'MyArg' or autocomplete anything? Of course there is no static type checking, but if you name your arguments correctly it's usually not a problem to see what goes where. E.g.: If the Argument reads `parser` and there is a BaseParser class and some subclasses in your project/scope, you can just start typing `parser` and it will autosuggest them (even if they don't start with the word). I've found that PyCharm is also aware of what you used previously and will rank suggestions accordingly. 
Thanks! I was looking for something like this the other day. 
Jsonpickle solves this problem by storing type information by default.
&gt; You cannot prove something is correct by testing a limited number of cases. Agreed. But are you are using 'prove correct' in a way I'm not familiar with? Because static typing certainly doesn't prove code is correct - merely that one doesn't have type errors.... It feels like recently over the past two years with the minor backlash against dynamic typing and surge for Scala, Go, etc that quite a number of people are looking at static typing as a silver bullet. And yeah, I have been in discussions with programmers that justifying deferring unit test harnesses until later in the project - "because we've got static typing anyway".
Type annotations in Python was not supposed to check types though. It was thought as a "documentation" feature, to help reading code and IDEs. Are there any tools that do type-checking using those annotations?
This tells me nothing. Relevant citation needed.
Everyone else made it sound too easy. Java manages to get by with fine-grained locking because it has a JIT and is statically typed. Without either, it's not going to work. jRuby has shown that [circumventing this is possible](http://www.mutuallyhuman.com/blog/2012/11/26/speeding-things-up-with-jruby/) in theory, [but you lose safety guarantees](http://deployingjruby.blogspot.co.uk/2013/07/why-jruby-arrays-are-not-threadsafe.html). This is not a compromise that I see CPython making. Further, the rewrite to make this possible (adding a simple GIL and moving to pure GC) is monumental. Considering the number of C extensions that require implementation details, there's no way this is happening. And to note, [Java isn't really thread safe](http://stackoverflow.com/a/9278798/1763356). Python guarantees all integer operations (even those on big integers) are atomic; Java doesn't even guarantee `long`s. So what could Python do? PyPy's Software Transactional Memory approach seems best: run two things in parallel, and if one conflicts with the other just rerun it after the other process is done. This allows you to guarantee just as much as CPython does while remaining parallel for free. The PyPy team also think you might be able to hook it into some kind of event system to make *whole events* atomic. That's a completely different level of safety than anything else being offered, and it's being offered for free. Finally, let's say CPython just removed the GIL and it came at only a 50% performance overhead. Let's say you have a decent computer with 8 cores, and your locking overhead between the threads (which you have because otherwise you'd just use `multiprocessing`) comes to another 25%. You then lose turbo due to all of the extra heat, so knock of another 20% or so in speed. Your speed up is then 8 / (1.5 * 1.25 * 1.2) ≈ 3.6 So your code runs about 4 times as fast... **So what?** CPython is a good 100x slower than, say, Go in the simple case. You've just tripled the complexity of your code, added a massive complexity burden onto CPython and gotten almost nothing from it. Just rewriting the hot loop in Cython or using CFFI to dish out to C would give a way larger performance improvement. C extensions can release the GIL, so you can get parallelism on top of that. Rewriting the whole thing for PyPy with CFFI would be even better. Using `multiprocessing` has more overhead but will probably let you get at least a 2x speed improvement even in the shared-state kind of scenario. Each of these options is somewhere between half as good and way, way better.
After ten-years of maintaining a huge python application I just wasn't able to run into the doomsday/huge-liability scenarios my java colleagues keep warning me about. With about 6 developers and monthly releases, we probably had one type-related bug crash the system a year. Probably 50% chance it was downstream handling a prior very rare error (which didn't have a test in the test-harness).
Yes, but they address the correctness of your program against it's spec, which make them vastly more valuable.
Coroutines are first class in python.
In fairness, this is one of those areas where Guido and the python-devel crowd claim too much. Recursive code is only difficult to understand if you've never worked extensively with recursive code; the same is true of iterative code.
IMPORTANT NOTE: Java is not thread safe "by default" by a design decision. The compiler can allocate a local cache copy by thread which speeds up execution. If you want thread safety on Java you need to use the volatile keyword.
there's a save button on reddit you know
Well, OK, but if we're trading anecdotes then after roughly half that time maintaining a substantial data-crunching application in Python, something implementing lots of complicated proprietary algorithms and user-defined data types, we were spending so much time and money guarding against type errors that we decided to rewrite the entire project in a more suitable language. (Edit: This wasn't the only reason, of course, but it was a major factor.) The problem was not that we were seeing a lot of crashes in the field, because this was a key part of a vital system and so it was extensively reviewed and tested. It was just the overheads of constantly doing grunt work that a stronger, more static type system would do for free. To say that development efficiency increased after the rewrite would be an understatement. As a convenient side effect, our test suite now is about half the size it used to be in terms of lines of code, even though it actually covers a few extra useful scenarios relative to its predecessor. Edit 2: Would you mind sharing a little about the application you're working on where you haven't found type errors to be a problem? I've waited a long time to find someone who actually has maintained a large Python system over a significant period to compare experiences with, and usually about this time I find that their idea of "huge" and mine are several orders of magnitude different in size, or that although their system is big it's just 100 cases of doing the same thing in some framework so there's only a relatively small amount of variation in types being used anyway.
The better question is what *don't* I li... oh.
It's also less useful in a lot of other cases. For example, on some platforms including Windows, starting new threads is relatively cheap but starting new processes is relatively expensive.
I reference this all the time. Good stuff.
Give pyzo a try Www.pyzo.org It comes with a Nice editor (iep) a debugger, many scientific modules (scipy numpy matplotlib... ) Comes with an installer for unix, windows ans osx
I might as well link my unmerged pull request [here](https://github.com/RafeKettler/magicmethods/pull/43).
Not all static typing is created equal. As I said, static typing in "older" languages like Java or C# is often more trouble than it's worth. There is, however, a spectrum of different kinds of "strictness" in static typing. Haskell, for example, expresses IO-Operations as types. This means, that a function which takes a `String` argument and returns a `String` (written as `String -&gt; String`) cannot hit disk, a network or even print to stdout. Printing would have to be something like `String -&gt; IO String`. While this may seem very of restrictive at first, it gives us a lot of guarantees, about what a piece of code does and does not. Functional programming also often uses concepts from math (e.g. algebraic data types), which have mathematically proven properties. Static typing itself does indeed only prove your code obeys certain type rules, but if these rules are provable, this goes a long way to proving correctness of your code. There are languages (idris, agda, coq) that experiment with even stricter systems (dependent types) aiming for "real" provability. As with all very new languages, this is mostly research and it might take some years before we see anything that is usable for mainstream software.
Like I said, not all of them are downvoted. I've already hidden the ones in the 4-10 range that are not.
thanks for the unexpected gold ... :)
I was going to jokingly suggest a /r/pythonmeta for conversations such as this op!
Just wanted to point out that Java has support for many other concurrency models via it's rich library ecosystem. See [Akka](http://akka.io) or [Quark](http://docs.paralleluniverse.co/quasar/) for instance.
But your complaint is that they are reaching the front page. But that's only because you are forcing them to reach the front page. The rest of us haven't noticed it at all. It sounds more like this is an issue for you to resolve rather than have mods do it for you. 
There are other people in this thread with the same problem, but I see what you're saying. My problem is, basically, that these threads are not deleted. I think it dilludes the quality of a subreddit when threads that don't belong there are left to linger, especially since "How do I do X"? threads are getting responses, encouraging people to continue to do this. It's a longer-term devolution either way
I'm in the US. The program evolved out of my master's project. I needed a few pieces that we had at our company, but I made sure to not use them to avoid plagiarism/cheating issues (even though I wrote/bug fixed a fair amount of the sections in the code). When I used a finite element model created by my coworker for that project, I was very explicit in my report about who made it. I used that approach when I decided to open source it. I didn't use a bit of code from the original project and have proof in the log. I realized that I probably needed 4 months of development time before I could make the other program deficient in every aspect as compared to my version (speed, features, usability), so at that point, as long as I made it available to my company, there should be no problems. Also, it wasn't really a sellable product like an IDE. It's an interface to a third party finite element code. They weren't interested due to overhead associated with switching, but that changed about a year later, when they needed some obscure feature and though the code didn't support it, I think it took me a few hours to add. That turned into a big question about what was supported and the right answer is the list of what's not supported is shorter. The code is LGPL and I also told my boss I'd sign a paper allowing the rights to use the software in our proprietary code. They don't want to own it. They just want to use it.
i see, thanks ! 
It's trivial to implement it yourself, in a reusable fashion if you want (or just take any implementation from the internet), if you really need it. For example: http://blog.moertel.com/posts/2013-06-12-recursion-to-iteration-4-trampolines.html 
Sure, but trampolining generally results in slower computations than straight tail recursion. Plus it's not built in and therefore requires more development overhead than other systems 
The go-to answer is that if you really need that much more performance, implement that stuff in C++ in the first place, then call it from Python. By the way, you can also use https://docs.python.org/2/library/sys.html#sys.setrecursionlimit if you don't care that much about performance and just want your stuff to work.
&gt; Would you mind sharing a little about the application you're working on where you haven't found type errors to be a problem? Sure, it was the security data warehouse for a very large service-provider. Python was used for all ETL processing, all automation, a lot of the data automation, all aggregation, all data database unit-testing, and some of the more interesting analytics. This involved doing very heavy validations &amp; transformations on about 300 million rows a day, loading the data warehouse multiple times a minute, and transforming about 20 different types of data (some simple, like Firewall, some very difficult like Asset). Some of the code was used to construct tools and frameworks (such as ETL utilities and database unit-testing utilities), others were in custom applications (such as transforming a specific customer's IDS feed). In spite of the data volumes, we tracked data quality across each row's fields every day. The impact of late discovery of errors, significant downtime, etc was very high. And we ran 24x7 for about 14 years. I'm sure there are far larger and more challenging code bases out there. By but any measure ours involved significant challenge to design and build, and was easier to build and maintain than similar ones built in Java. I suppose I could have built dozens or hundreds of custom types, which could have made things more complex. Especially, if I also choose to avoid unit-testing. However, I choose neither route. Also, if I were to list the top-10 challenges of building this in Python, I suspect that type-checking would even make that list. 
If you want to raise an error, you need to use the `raise` statement. So wherever you are doing your table lookup for the function, if the result is not found, raise the exception there. This post also belongs in /r/learnpython
Interesting, thanks for sharing. Would it be fair to say that most of the code you're talking about was backed by a database in one way or another? I'm wondering whether that brings a certain degree of natural structuring and consistency, which I've seen in larger Python web applications that used a framework but not so much in other Python-related contexts so far. If you're able to say, could you give an idea of how many individual applications you're talking about here, what sort of size (or range of sizes) they are in terms of number of lines/files/some other reasonable metric, and how much commonality or independence they have in terms of sharing code, data structures, etc? I'm trying to get a feel for the scale of the code you've been maintaining and how much it's one big code base vs. lots of smaller but perhaps related/connected ones. I can totally understand preferring Python to Java for that kind of work, BTW. I have used Java on some projects, but generally only when non-technical factors weighed heavily in its favour. As alternatives for Python in the more tricky coding projects I've worked on, I'd be far more likely to use something like C or C++ if performance characteristics needed to be tightly controlled, or oddly enough something like Haskell. I find the latter has a surprising amount in common with Python in terms of the pros and cons for when it's a good choice (relatively expressive language, runs with OK performance but you have to be aware of how you write it, etc.) but a lot depends on the target platform(s) and the maturity of the available Haskell libraries in whatever field I'm working in (which is an area where Python frequently wins today).
 d = {'make list': list, 'make set': set, 'make deque': collections.deque} def raise_notImplemented(): raise NotImplemented() d.get('some_function', raise_NotImplemented)()
To point blank answer the question, yes you could develop both twitter and reddit with Flask. Most of these large websites, and really most websites themselves are quite simple code. The hard part is balancing load and actually running the server side. Flask has very little to do to help or hinder that. 
Erm, doesn't Kivy use a truckload of Pygame?
Pygame is used as a backend for some components on some platforms, but Kivy doesn't depend on it at all. In fact, we'll shortly be moving to our own SDL2 backend by default on the desktop platforms (and android too further in the future), which should fix a lot of problems that defaulting to pygame brings, though kivy already avoids some of pygame's problems simply because it's an optional backend and we already use alternatives where necessary or useful.
This is then just a philosophic difference I have with Python. For many people, recursive solutions are easier to understand and easier to code. With Clojure lazy sequences, they are also performant and space efficient. This also maybe speaks to something I've noticed in the Python community to exclusion of other programming languages: Community members telling people "you don't need that feature". Or, as someone I recently had a conversation about this with said more eloquently: "I don't always agree with Guido and shouldn't have to pretend to"
Well, that's why no one would use a naive recursive implementation. You could write something just as space inefficient iteratively.
this sounds like the Python equivalent of cinch.rb which I love and helped convert me to Ruby. I always used ircutils in Python and it felt clunky, had at least one bug/missing feature that I needed and seemed largely unmaintained. I might give it a whirl for my next IRC project. edit: oo, ecosystem. cinch.rb has a plugin system, but there's not too many plugins. if you can create/foster a good plugin ecosystem, this project could really take off. I **hate** having to re-invent user management in every IRC-related project!
http://pandas.pydata.org/pandas-docs/stable/ecosystem.html
No idea why you're being down voted. This is clearly the decision that the community has made. 
Because I'm snarky! :)
I use PyCharm on a daily basis for my work. It is great. It will really help you when you are learning python because of all of the checking that it is doing while you are typing. Also, it will get you into good habits because it is linting your code as you type ( PEP-8 ). We are really strict within my company about the coding style. Our main engine is written in C/C++ but all of the API and individual checking scripts are written in Python ( more than 200 scripts in total). I would die without an IDE like PyCharm. Just remember one thing. PyCharm is not to be started every time that you want to edit a file. I leave my PyCharm running for months at a time. Here is what I mean. You make one project called 'school' or whatever. You then can add files/directories to it within PyCharm. These new files also get synced with your VCS. 
"Categoricals in Series/DataFrame" Finally!!!
Is there any to use pandas with on disk / out of core data sets? 
http://blaze.pydata.org/docs/v_0_6_5/index.html
Looks cool, but blaze objects cannot be passed to packages in the pydata ecosystem in the same manner as a pandas df (statsmodels etc). What about using numpy memmap? 
I don't think what you're hoping for is possible :). Well written libraries in the pydata ecosystem will use np.asarray() to ensure that they are passed non-subclass numpy arrays. The memmap option is nice, in theory, but in practice the assumption that arrays can be accessed and copied in memory is built in to many packages. There is no magical fix for that, but Blaze should (we hope) make it a lot easier for library authors to write code that generalizes better.
&gt; Would it be fair to say that most of the code you're talking about was backed by a database in one way or another? Well, not like a web-application might be backed by a database. But most of these are definitely focusing on structured data, and often doing "ETL" operations. &gt; could you give an idea of how many individual applications you're talking about here Well, each file type is an individual transform application, these can get very complex, mostly depending on source data characteristics. So, there's about 20 of these applications that vary between 500 &amp; 5000 lines, with a median probably around 1000. There's a lot of code sharing, mostly around shared infrastructure services (logging, business rule auditing, data movement, metadata, etc). The code that's shared is allowed to be more sophisticated, but the custom transform code is deliberately kept more simple to make it easy for non-python, and barely-programmers to read the code to understand the exact business rule implementation if they want. Beyond that there are about a dozen significant various tools, engines, and frameworks we built to make life easier. These probably average around 2000 lines each and are mostly independent codebases. The chief challenges, off the top of my head are probably: * Python was hard pressed to support more than about a billion transforms a day on a 4-core server. To go beyond that would require much more hardware, maybe multiple servers &amp; distribution framework, compiling libraries/functions/classes/apps using shedskin, cython, or another language. * Python packaging was a mess when most of this was being built. It's gotten so much better that it's no longer a huge deal. * Before it was all moved to Linux I had some challenges supporting vastly different versions of Python. * It wasn't Java: much of the rest of the organization we ended up in was a Java team. Anything not-Java had a hard-time getting official sanctioning. On the flip side a number of very good programmers were much more interested in joining the team. * EDIT: and Unicode
Thanks for the explanation! Where does Dynd fit into this? 
Thanks for checking it out! Ecosystem is probably the hardest part- convincing others that the project's not only interesting enough to use, but to extend and enhance :) You're right though, some basic plugins would certainly encourage others to grab a few and try it out. User management sounds like it could cover a lot of ground - did you have an example in mind?
 if key not in d: raise NotImplementedError("missing function for "+key) d[key]() # dispatch function 
http://pandas.pydata.org/pandas-docs/stable/ &gt; **pandas** is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. 
I think the github README for DyND describes it well: https://github.com/ContinuumIO/libdynd DyND is a rewrite and alternative to the core part of numpy itself. It provides in-memory n-dimensional arrays that is supposed to be more easily extended to handle new data types (e.g., categorical and missing data) than numpy itself.
Both! We generally support any Python interpreter you'd like, but we do have some interpreter specific features. For CPython we have specific support for profiling and attach to process which both depend on interpreter specific features. For IronPython we have specific support for the interpreter to understand .NET namespaces via imports and assemblies via clr.AddReference* calls. For both of those we have support for auto-discovering the installed interpreters via registry entries. But you can also add your own custom interpreter. Based upon the configured language version we'll parse the code and offer the appropriate built-ins. As long as the interpreter supports sys.settrace (which all the major implementations do) we'll be able to offer debugging as well.
174161
I am a total novice but enjoy using pandas quite a bit. Can you explain why you find categoricals such an exciting addition? 
If you plan to do the plugin thing, you might wanna check this out: https://github.com/mitsuhiko/pluginbase Came across this sometime ago, haven't used it my self nor have I looked the code but sounds cool.
While not bad per se, it always irks me if authors present something about Python and implicitly refer to the 2.x branch. If anything, by not mentioning a version, it should be clear that you are referring to the latest version. And the tiny single paragraph Appendix for 3.x that is pasted at the end hardly scratches the changes that were made. The author really could do his readers the favor to at least state right away that the guide is based on the 2.x version...
Yep. [Client.init](https://github.com/numberoverzero/bottom/blob/e6727f7b133e7d8ad8cc46dddbfb15cb09e26d6e/bottom/__init__.py#L12) -&gt; [Connection.init](https://github.com/numberoverzero/bottom/blob/e6727f7b133e7d8ad8cc46dddbfb15cb09e26d6e/bottom/__init__.py#L18-L19) -&gt; [asyncio.open_connection](https://github.com/numberoverzero/bottom/blob/e6727f7b133e7d8ad8cc46dddbfb15cb09e26d6e/bottom/__init__.py#L58) I haven't done any deep investigation into `asyncio.open_connection` though, so I'm not sure that there's sufficient validation to mean that ssl=True is worth anything. I'll add an issue to dig into it. Thanks! *Edit*: [created issue](https://github.com/numberoverzero/bottom/issues/6) 
Having a clean way to create a bot without binding it to a destination would be great; I'd love to have my bots be configuration-driven and provide an endpoint or set of endpoints when the bot is run. Ideally, it would be possible to have &gt;1 instance of the bot, too -- so you'd create a bot mapping and then instantiate it for the connections, rather than the current "build one, run one" mode. Something like this: bot = Bot() @bot.on("ping")(lambda message: bot.send("pong", message=message)) client = bot.connect("irc.freenode.org", 6667) loop = asyncio.get_event_loop() loop.run_until_complete(bot.run()) I recognize that there are some complications about coordinating the right instance for the bot reference in that lambda, but once that's solved then you have a reusable bot definition that can be connected to arbitrary endpoints. 
As a backend for desktops, yes. Sadly. It's the #1 cause of crashes and mysterious fail behaviour in kivy on desktops. (fortunately they're in the process of rewriting to use the SDL2 bindings directly; another massive uptick in my books for the kivy maintainers; actually recognizing that pygame is a problem and actively working to no longer use it)
There really is no good solution for this problem with Pandas or many other tools. Doing analysis on outside of membership datasets is tough with Python and R. One way around this is to look at stream processing techniques.
Thanks for the feedback! Injecting config per-client or per-connection has some tradeoffs, and I went back and forth. I love that once a client is created we don't have to keep track of config - it's self-contained and knows what to do whenever it connects. To be fair, it still looks a *bit* weird to call a `connect` method without args. I think, depending on how plugin registration will work, it's possible to handle this cleanly with the current design. Let me know if I've missed the point here. # plugins.py def register_pong_plugin(client): @client.on("ping") def keepalive(message): client.send('pong', message=message) # main.py from bottom import Client from plugins import register_pong_plugin import asyncio endpoints = [ # https://freenode.net/irc_servers.shtml ("holmes.freenode.net", 6665), ("kornbluth.freenode.net", 6666), ("leguin.freenode.net", 6667), ("orwell.freenode.net", 6697) ] clients = [Client(*endpoint) for endpoint in endpoints] for client in clients: register_pong_plugin(client) # run until all clients are done loop = asyncio.get_event_loop() tasks = [client.run() for client in clients] loop.run_until_complete(asyncio.wait(tasks)) This isn't quite as clean as what you've got above, but I don't think that's going to be possible without some serious magic to track current triggering bot (similar to `bottle.request` but without a threadlocal to simplify things). --- Once we start talking about registering plugins, things get even muddier with the single @bot.on -&gt; bot.send style - is importing a plugin module enough to hook it up to a bot? I'm a bit nervous around import-based plugin registration. I'd much prefer explicit `register_foo(thing_to_register_on)` so that readers don't have to chase down imports to see where plugins register. As a counterpoint, https://github.com/mitsuhiko/pluginbase is popular, so maybe I just haven't seen the light. *(edited typo)*
Yes, calling a C routine from python will not be slower than calling it from C. Congratulations.
The downside here is that the nice binding -- decorator-based, app-oriented -- that really makes the API slick is lost partially. Presumably your plugin API will look a bit like that, but even so, that's another layer in the way. Is there truly no connection tracking approach that works reasonably well in this instance? Damn. 
I'm not sure if it fits you but there are two weekly newsletters that I know of. Those are http://pycoders.com/ and http://www.PythonWeekly.com (second one does not work at the moment but I got an email from them lately; I will try to figure out what's wrong with it). If there are other resources I would like to know about them.
Thanks but, You've provided the same link twice. Its not working at the moment though
Sorry, my bad I should have double checked and don't trust copy paste. Fixed one link and added comment on the second.
I see your point - I worked around this with some kind of manager functions who call all the other functions but there is nothing from stopping you to go straight to the "employee" and ask him instead.
but it does mean you can write a Python program that runs fast enough
I give up.
What license is this under? This code is a legal mine-field until you clarify the licensing of your code.
Thanks again. It's always interesting to see real world case studies of what works (or doesn't work) for people doing different things! For comparison if you're interested, the rewritten project I mentioned was probably somewhere in the range of 5-10K lines of code. It was basically one big data processing application, i.e., data in one end, do some computations, and output on the other end with very little UI or interaction with other systems. I don't know whether its code was what you would call "more sophisticated", but it certainly implemented quite a few proprietary algorithms that had some real thought behind them, and some non-trivial data types beyond those Python offers out of the box. For performance reasons, it also used a lot of minor transformations to prepare data prior to running a relatively slow algorithm, and a lot of cacheing, memoization and related techniques. Beyond a certain scale, these were the kinds of areas that we found were taking a lot of care to get right and sometimes a lot of co-ordination between developers and/or documentation for later reference. More static type systems encode that kind of information in the source itself, which for us proved to be a significant advantage. This is also why I find Python applications with more uniform designs don't tend to have these concerns, at least not to the same degree. It's not so much the absolute scale I've found challenging with more dynamic type systems, but rather the variety of types and software architecture you tend to get as some systems grow, even when they are still quite small code bases by general programming standards.
It is the lack of encapsulation that makes Python a half-OOP language with FP bolted on
I think it's because you have defined the wordcounter function after you call it. Move it above the reader function and you should be good
I fixed the code for you. Hope it helps. http://pastebin.com/z55QsCGE
https://github.com/thogaw/pyvenvwrapper
Sure.. and let me preface this by saying, I'm a novice to Python. Coming from R, it's extremely easy to identify as variable in a dataframe as a factor.. there's literally an asFactor() function. I started about a month ago trying to re-work some previous stuff I'd done in R, in IPython with numpy and pandas to learn more about it. I made it all of 5-10 minutes in and ran into not knowing how to identify a variable in a Pandas dataframe as a factor/category. Then I found out you literally could not do it. That's when I gave up, as I knew 0.15 was coming out shortly. So that's why I'm excited about it.. I can now continue on trying to learn more about scientific python with that stumbling block removed. If you're asking why categoricals/factors are important, let me know, I can explain that as well, I just assumed you were asking about it's relevance to myself and pandas. 
Thanks for the reply. Yes, I meant more the latter -- is it just adding a better (faster?) way to deal with nominal / ordinal data? Can you give an example of a use case where this would be *just the thing* and the prior versions would have required a second-rate workaround? Thanks again for your time.
and a decent sized zope/plone community that existed long before django/flask/pyramid for that matter. it depends on where you define your personal bubble. 
Sure.. My interest is in Machine Learning, so i'll explain it that way, though I do add a disclaimer that I'm just getting started in my Masters degree in the Data Science field, so by no means am I an expert in any of this. The categorical / factor data type allows us to convert qualitative variables to quantitative ones for use in predictive models (regression analysis). For example, let's say I want to build a model to predict blood pressure across a range of age groups.. say 10-20, 21-30, 31-40, etc.. I honestly don't even know how I'd use that group membership as a variable in a Pandas dataframe at the moment. Generally you code the variable, (10-21 group = "1", 21-30 group = "2"), etc.. If I were to use those values in a Pandas dataframe and then run it through regression analysis, there was previously no way for me to tell Python that "hey, this "1" value you see here.. it's not really the number 1, it's a coded representation of a group membership".. it would just treat that value of 1 as literally the number 1.. same with the rest of the coded varaibles. The addition of categoricals allows Pandas to tell Python and other operations / packages that the "1" I added in my data is a category, and not literally the value of 1. 
404
Thank you very much! It's very useful in orden to know the scope of a method, variables and improve my code (use of "with as", get method instead of dict[key]) :)
Great! thanks a lot, will test tonight :)
i see that it's nice from a maintenance point of view, but not from a user's. you could enhance usability greatly by having a &lt;noscript&gt;See contents at &lt;a href="nav.htm"&gt;the table of contents&lt;/a&gt;&lt;/noscript&gt;, and everything would be fine both from a search engine / bot / linked data point of view and for users who use noscript or similar.
I agree with you, I was backing up your claim with more anecdotes. 
 l = list(text.split(' ')) there's no need for the list() call, as text.split() already returns a list. 
for me the .dt accessor will reduce the amount of boiler plate a lot. 
Agreed - case studies with admission of went didn't work well are invaluable. I've found that in ETL applications that it dramatically simplifies the applications over the course of time if one can separate the functionality into completely separate programs. It adds a little overhead, but makes the inevitable data changes over time and testing so much easier. Also, our codebase was simple &amp; accessible enough that if someone had questions about a function, and it lacked a proper docstring, it was trivial to look at the code for reference.
Pandas itself wouldn't do much with it, other than modify the data-type to reflect that it's a category. What would use it differently would be the various algorithms for Machine Learning within the scikit-learn library (at least for my purposes). Instead of treating "1","2","3" as the actual numerical value 1,2,3, they would then understand that these are categories, and incorporate that knowledge into the resulting models. (Using the previous example, it would treat all of our blood records with our new category variable = 1 as a group (since 1 represents the age group of 10-20).. rather than x number of individual records).. This is important because I want to compare blood pressure between age groups, and not between individuals. I'm making the assumption that scikit-learn will work with this new datatype in Pandas right out of the box.. I'm hoping that's the case and that scikit-learn doesn't need to be updated as well, but still need to verify that. 
This is awesome. Do you know if there are plans to extend it to parse any more of the 990?
Python Weekly is the best of the 'newsletters' that I've found, but most of the time I've already read their articles here in r/Python
&gt; User management sounds like it could cover a lot of ground - did you have an example in mind? it is! and at pretty much every turn, there's multiple choices available. that said, having **one** common solution is better than none, even if it doesn't satisfy everyone. it will satisfy enough people and provide a jumping off point for further extension. I've thought about this problem a lot, so here's a brain-dump :) **user identification** there's two primary ways of doing this: hostnames or passwords. hostnames tend to feel "archaic" to modern IRC users. you've probably seen this if you've ever used an eggdrop bot. for users with static IPs, ident+hostname is a reasonable approximation of an "authorized identity". however, modern consumer internet uses dynamic IPs and NAT gateways break ident. anyone using commercially-hosted server (bouncer or VPS) is likely to have a static ident+hostname. some networks (Rizon, Freenode) allow for hostname masking when you authenticate with Services. the primary feature of this is it's transparent to the user. if they're connected from a known hostname, they're authenticated. however, to be a complete solution, you have to provide a 1-to-many mapping of "users" to hostnames, since some users may connect from multiple servers. it may also be a good idea to support "hostmasks": adding wildcards or globbing to a hostname can allow you to accomodate users on dynamic networks or make a security trade-off and trust large network segments. for example, many of my friends use Comcast and I might want to trust "*.wa.comcast.net" with some basic privileges on the assumption that anyone with that mask is either a friend or a friend-of-a-friend. "passwords" are a much more initutive solution for non-technical users or IRC rookies. `/msg BotName login username password` and now the bot recognizes you. you'd need some self-service features though like "change password". you might also want to allow registration and "forgot password" functionality. both solutions could be implemented with an ACID database as the datastore. sqlite works great for single-threaded daemons like an IRC bot. data storage could also be implemented via an ORM so MySQL/Postgres could also be supported. extend your user management with a web interface or link user management between applications? **user privileges** again, there's two major ways I've seen this implemented: "access level" or "flags". "access level" is typical some integer value. "commands" have a "access level" as well. the command is allowed if `user.level &gt;= command.level`. "flags" is more granular: users might have "+abcd", allowing them access to commands "a,b,c,d". Rizon Chanserv implements of these: [ACCESS level](http://wiki.rizon.net/index.php?title=ACCESS) and [FLAGS](http://wiki.rizon.net/index.php?title=FLAGS) Eggdrop implements a "flags" system too. there's so many potential problems to solve here. how do commands get defined/managed? ok, that's all for now. ask questions if you'd like :)
A few notes. 1. Reading words from a file and counting words is two separate tasks. I would not couple them together in a single class. Instead, define a function that takes a file name and outputs a list of words. Pass that list of words into the `Counting` class instead. 2. Files should be opened using the `with` statement. Read up on context managers. with open(fname, "r") as f: # do stuff 3. You want a dictionary. It's useful to have `Counting` subclass `dict`. class Counting(dict): # do stuff
It was on Google :(
Thanks for the reply. I actually did conda update --all on a PC with anaconda installed; which then took ages (like all day), and I then went to incorrectly assume it would take ages if I ran conda update -all on a system with fewer python packages installed. When I actually ran conda update --all on a system with just numpy, ipython, scipy, pandas and a few other packages, it ran in quite quickly. TL;DR I made a bad assumption and jumped the gun with this post.
Archives work for me (though some issues are still missing), just took them a while to load - try again maybe? Alternatively, I could forward you the past issues, I've been subscribed to them for a while, they're pretty good. PM me your email and the desired # of past issues if interested.
&gt;"Categoricals in Series/DataFrame" https://en.wikipedia.org/wiki/Categorical_variable http://pandas.pydata.org/pandas-docs/stable/categorical.html http://pandas.pydata.org/pandas-docs/stable/api.html#api-categorical
I would recommend [miniconda3]( http://conda.pydata.org/miniconda.html) Then just: condo create -n myenv python=3 Then if on Linux: source activate myenv to activate it Then: source deactivate to deactivate your environment I think on Windows you can drop "source" from the command To install packages: conda install package_name To list packages installed: conda list
Conda looks very interesting - the ability to install numpy/scipy without tens of C libraries and a working Fortran compiler is certainly a great benefit! Do conda packages usually stay up-to-date with the pypi versions?
Thanks! I forgot to add LICENSE file, it's MIT.
Not even that. They wrote algorithms that are faster than ones you would write.
How is it free if you have to buy Excel? 
Anyone have an ELI5 (or link to a tutorial) on how I would get this code up and running on a Mac? I've hacked javascript before and have done some CS 101 in Java, but I haven't used Python.
You can use an open source alternative.
I think there are a couple improvements that could be made. Using dbms to do that lookup is really fast, but it's memory intensive, so using an SQL database could be better for many people, even though the dbm is faster. There aren't any plans to add more 990 stuff, but I could definitely roll out some code if anyone knew where to get 990s for free.
So the bad news is that I can pretty much guarantee that scikit-learn *cannot* handle categorical data from pandas out of the box... and in fact it's not clear that they ever will -- many scikit-learn functions explicitly require numpy arrays rather than dataframes. But if you're interested in this sort of thing, the right solution is to use the awesome patsy library for coding your dataframe into a design matrix before passing it off to scikit-learn. Patsy bring R's formula mini-language to Python, and it already handles categorical data right, even without the categorical dtype: http://patsy.readthedocs.org/
Let me know if I'm missing something, but I think you need an account for that :-/
awww shit, you're right. My bad.
&gt; Thanks! I forgot to add LICENSE file, it's MIT. Very cool. I will play around with it tonight, unless I fall asleep before. :D Thanks for releasing this. I might contribute a monitor plugin or two.
It's assuming you already have excel, hence the title. 
I keep up with http://planetpython.org via RSS in Feedly... 
Forgot to mention you can still install packages with pip with the environment activated. pip isn't installed by default but all you need to do is just: conda install pip and then you're good to go. 
Why should I use this instead of Cairo directly?
Reading tip: If you want to read something hardcore pro-OOP, make sure to read some of Bertrand Meyer's stuff. I had an Intro to Programming course with him. He came up with Eiffel (programming language). I hated his zealot stance on the topic, but it was very informative. I never fully understood why we **need** enforced encapsulation. We're all adults here, right? 
I've unsubbed from PP a long time ago, like most "planets" is constantly full of crap.
Ok so I downvoted this and I want to give you honest feedback on this! I was "shocked" when I read the title and found "physics engine" and "requires pygame" in the same line. I took a really quick look and I noticed what was wrong: You actually made a game! A physics engine is an *isolated* piece of code that does some calculations and returns numbers. Isolated meaning that the code either doesn't need 3^rd party libraries or only really small ones. In your case you made a small engine and left it in a game (probably the game you tested it on) thus requiring an entire framework. Here are some things you can/should do: * Take all of the physics code, put it in a class and then in a separate .py file * Comment your code if necessary and make sure that the functions are understandable * Import your file into your game and test it * Remove all of the unnecessary code and files and throw them out of the github repo (euclid.py seems very useless) * Create a README.md file on the master branch and **thoroughly** document your engine (how to install it, how to use it, etc) Oh and one more thing: Please take your time while working with code, especially when it comes to an engine. No one is pushing or forcing you to code this. If you don't feel that your code is ready, it probably isn't. The greatest mistakes happen when you're in a rush.
Wow, thanks so much.. I'm going to look into that! :)
Well I won't entirely disagree... as with a lot of sources, that's why I have in Feedly so it's trivial to skim the headlines and descriptions and mark them as read if nothing catches my interest. 
* You posted in the wrong subreddit. Questions go in /r/learnpython as indicated on the sidebar. * Your previous posts in /r/blackhat make it seem pretty likely that you're trying to break into somebody else's e-mail account. * Brute forcing an e-mail login in the way you're describing is almost definitely not going to work. It would probably take you years just to try every combo under ten lowercase alphanumeric characters.
What are the benefits of drawing vector graphics programmatically rather than through programs like illustrator?
Thanks /u/Habitual_Emigrant for your kind offer. I haven't seen any of its issues so I don't know which would I be interested in. I'll try the archive when I get a better internet connection. In the meanwhile I'll enjoy Python Weekly, it looks amazing
Well, one thing is making fractals like the logo in the link. Or maybe you want a complex pattern that you can describe with math. It's easy to implement this with code, and hell to do by hand. This also lets you make changes quickly to iterate on your designs. 
You can write all kinds of (vector, not png) formats from Cairo, check [this](http://zetcode.com/gfx/pycairo/backends/) out.
You're welcome! And I was talking whether you'd want to get, say, last 5 or last 10 issues, not specific numbers :)
&gt; Research shows is the primary product of an agency. Who the heck do you think made Wordpress...
Yes, it's just not implemented. The (very reasonable) difficulty is that it's not just at export time that you decide the format, it's from the beginning with the kind of Cairo surface that you use (SVGSurface, PNGSurface, etc.), and then the coordinates systems are not the same (in a PNG surface you count in pixels, while in svg you don't).
Thanks for pointing our his /r/blackhat past posts. downvoted.
Thanks, very useful these notes! I thought classes were "only" used when you have to do a lot of things inside them, in other cases you could use methods keeping separete different functionalities. I guess, then, that methods are used to separate functionalities related among them and classes to implement different, and not related, things. And I'll have to get used to using the with statement. I'll do it again using your tips.
You can do [these mathematical roses](http://i.imgur.com/rg2GGmK.gifv) for instance :)
Ah, by 'not implemented', you meant in Gizeh. I thought you were saying it wasn't implemented in cairo. I understand now.
This looks like a very cool application...
The post in /r/blackhat was for a session theif for my friend who "wants WIFI where ever he goes" but thank you for referencing me to /r/learnpython and hopefully I can restore this email account of which all info except of the email address (which was memorized) was deleted after our servers crashed and a worker forgot their password.
If you want the entire Scientific Python stack you could download the Anaconda distribution for Python3 and just have everything along with the conda package and environment manager. http://continuum.io/downloads
That is for managing different *pythons*, not specifically different *python virtual environments* which is what virtualenv and the venv module do. The name can be a little confusing for pythoneers, given it's got "env" in it, and the reason is it was forked from its father project called [rbenv](https://github.com/sstephenson/rbenv) which is similarly for managing multiple rubies.
It says 2012 right there at the top...?
Every time I browse /r/python at work, there are at least 3 noob help posts on the front page. That's pretty bad.
Thanks for the thorough explanation!
I've used NodeBox 1 for generating vector images and animations, have a look if you have time: http://nodebox.org/ Export to svg format is supported, so it can be edited with inkscape later if needed.
Thats why I don't want to use wordpress. Oh, and the php cooties of course.
A good rule of thumb is to use classes in situations where you want to do a set of related tasks operating on some internal state. The reason `collections.Counter` in the standard library has a `Counter` class is because it exposes a couple of related methods. For a straightforward task like yours, I'd simply define two functions, one to read words from a file into a list and another to take a list and spit out a dictionary.
[High Scalability](http://highscalability.com/) is always a decent read. It's often better to build your application to be 'decomposable' than try and make it scale as far as you can imagine it might need to up front. By this I mean that if you model your search as a service up front, even if it's just using MySQL full text indexing in the beginning, it's very possible to swap out (say) ElasticSearch later if/when it becomes a bottleneck. But if you do that too early, your operational complexity becomes higher (have to sync both data sources, manage 2 services, either of whose failure can impact your uptime, etc) and isn't really adding much value up front. That's one of the big sells of spending time doing the right unit/integration/functional testing, too; you can do these kinds of service replacements with much higher confidence.
Cool, thanks for the thorough responses, but it sounds like it may simply be above my level right now. I would think you could just make the `1` into `str(1)` and have non-numerical data that still made readable sense to you, to avoid treating it as a number.
Imagine if you need 100 random circles, random colors as well ;) Look up "generative art"
Too bad it rarely works.
Indeed. 2to3 wasn't much of a success. The [future](https://pypi.python.org/pypi/future) module (not to be confused with the `__future__` syntax) has been taking a much better approach. It comes with a `futurize` script that works like 2to3, but it automatically converts your Python 2 into code that runs on both Python 2 and 3. (You can also just modernize your code yourself, without the help of a library, or with a simpler helper library like `six`. But really, if you could, you probably would have done it by now.) I'm told it works well. I wouldn't know firsthand because I don't really write Py2 code anymore. I'm excited about the other command they're developing, `pasteurize`, which takes your pure-Python-3 code and makes it compatible with Python 2. 
I've used it on some pretty large scripts with serious 2.7 dependency and had some great success with it. It can miss a few things sometimes, but it makes for a wonderful baseline for converting scripts doing a lot of the work for you. Sorry to hear you didn't have as much success with it as I have. 
&gt; Coroutines in Go are implemented by the language as system threads. &gt; The Go design docs clearly state that coroutines are given to system threads to perform their work. Goroutines are not system threads, they are limited version of coroutines and coroutines are not threads, nor are they an abstraction of threads. Python has coroutines and those coroutines are not threads. In fact, Python's coroutines are a more complete realization of coroutines than that provided by Go. Go imposes those limits on goroutines specifically to separate the concepts of "concurrent" from "parallel" on the belief that doing so will empower programmers. A program written in Go can use the primitives provided by the language to easily and cheaply create tens of thousands of goroutines and this program can run unaltered whether the runtime operates with a single thread or a thousand threads. The primary problem that, for me, kills the promise of goroutines is that goroutines can still deadlock, whether run in a single or multi-threaded environment. That failure of the abstraction suggests that the limits imposed upon goroutines are not worth the cost of a less than full realization of coroutines. [rob pike on concurrency vs parallelism](https://www.youtube.com/watch?v=cN_DpYBzKso)
Thank you for the feedback :) and sorry, I'm quite new to programming.
If I remember correctly, using scrapy you can ship out eggs of your scrapers to multiple machines running scrapyd. It will also encourage you to write very similar scrapers and process data in generalized pipelines. The architecture should help with scaling, but I'm not very familiar with the platform. I can't vouch for it, but it may be worth investigating.
always work in svg, then export to png?
I actually do. For some reason, I find that my thoughts flow easier with a pencil in my hand than staring at the screen. Usually, I end up just jotting down a general layout of my ideas and then switch to the computer. Sometimes I end up pseudocoding the whole thing on paper. I think it depends on the complexity of the problem. The more complex, the more pencil. 
 /r/learnpython 
If they're supposed to be integers, you need to convert them to integers first. If they're floats, you need to convert them to floats. Alternatively, you can concatenate the strings together.
I don't see enough encouragement to use classes in these comments, so I am going to add my $0.02. It's hard to say for sure that you should be using classes, without seeing your problem space, but you _probably_ should be using classes. Classes are a powerful way of organizing code. A class' purpose is to encapsulate data and functionality into one place. If you find yourself passing around a lot of objects (like dicts or lists) to various functions, and then doing operations on that data, you probably could make that into classes and methods. Classes give you the ability to do inheritance/polymorphism, composition, and aggregation. Technically, I guess you could do composition in a function, but that's so ugly. I think it's better to just give an example, so here's a classic: You want to "write" a car. A car has an engine, drivetrain (technically, the engine is part of the drivetrain, but ignore that), and wheels. Let's say you want to create an "accelerate" functionality. Now, you could write a function that has all those objects (or worse, you could use some other data structure). Your main function could call an engine function with: current speed, gear, and throttle value (all of which need to be stored as variables in your function), and it could return RPMs. Then the main function calls a drivetrain function with the RPMs and gear, and it returns torque, angular velocity, and gear values. Then the main function could call the wheel function with a list of wheels, torque, and angular velocity. This works, but what if you want to use a different size engine? Or different transmission? You could use different functions, or pass around a different engine object, but this is all messy. Let's try a class-based approach instead: There are several ways you could do this. I'm sure others will disagree with my approach, but it's just one approach of many. Let's start with a Car class. When you instantiate your car class, you give it the following class references: Engine, Drivetrain, and Wheel. The Car constructor instantiates 4 Wheels, and passes them to the Drivetrain when instantiating it. Then it passes the Drivetrain to the Engine when instantiating it (this is called "Composition" btw). The Car constructor also initializes an instance variable 'current_speed' to 0. When the Engine constructor executes, it sets the 'current_rpms' to 0. When the Drivetrain constructor executes, it sets 'current_gear' to 1, etc. There is probably a better way to do the above, but I'm just pulling this out of the air right now. But here's the point: When your main function wants to accelerate, it just calls car.accelerate(&lt;throttle-level&gt;). The method 'accelerate' can call engine.throttle(&lt;throttle-level&gt;). Engine adjusts its own RPMs accordingly and calls drivetrain.turn(&lt;rpms&gt;). The drivetrain checks its RPMs and, if necessary, upshifts and sets its gear value (for simplicity, I wont deal with a manual transmission). It then calls wheel.turn(rotation-velocity) on each wheel (let's pretend it's all-wheel drive). Instead of your main function calling all those different functions, you could push that code out to your classes. It's also cleaner because you didn't have to pass around all those state values that are specific to each car component (rpms, gear, etc). Also, if you want to design a specific car, you could create subclasses of all those types. So if you want a Mazda CX5 (my car), instead of passing-in the various car components, you could override the constructor to default to a 4cylinder engine, with a All wheel drivetrain and 6 gears, and 19" wheels. Since you have defined your interface, you Car doesn't need to relearn how to accelerate. Just the engine's throttle method needs to know how to do that. Your Car doesn't need to know wheel velocity based on RPMs, only the transmission/drivetrain 'turn' method needs to change. etc. etc. This is the value of encapsulation and polymorphism: Each piece knows how to handle itself. You define an interface for each component-type, so they become interchangeable. Your code stays clean because each functionality is concise and to the point. When you type "car.accelerate()", I know exactly what you are doing. ...I hope this was useful, and not just the ramblings of a madman. 
if a and b are the strings you should be able to do: c = int(a) + int(b)
X-post: http://www.reddit.com/r/flask/comments/2jto0m/making_flasks_url_for_more_flexible/ The unwritten rule is to denote x-posts.
Iam familiar with the car excample and this made always perfect sense for me but when coding IRL I don't know how to apply this. So to reduce some abstraction: My last/current project was an Energymanagementsystem on Django basis which complie with the ISO 50001. you had to be able to tell how much power you are using and which of your machines uses it. So Django has this Model.py(all Classes) and the View.py (Some classes but mainly functions to hand over datastructurs). And I made a claculation.py where I put all the logic in. So one task was to get all the diffrent sources of energie (Power, Gas, whatsoever) and tell how much was used in the past year, what did it cost, the percentage of each powersource and costs. To do this I made one function (**I began to read some of the recommended books - and know that this was a bad decicion**). It aggregated the data and made a dictionary for each powersource with the asked values and let the function hand over a list of these dictionarys. Iam sure this could be done with a class - but I have no clue how to do it. It has to be something like: list_of_classes = [create_class(element) for element in list_of_powersources]
It's because all the subsequent runs are sorting an already sorted list. The first time is really the only time it's doing any work at all, as when lists are already sorted, bubble-sort basically boils down to scanning through the list once, noticing no swaps were done, and exiting. To do a proper test, you need to recreate your shuffled list (or do the sort on a copy of it) each time, rather than passing the same list, which gets mutated into a sorted one on the first call. Eg. change the logic to: print timeit.repeat("bubble(list[:])", "from __main__ import bubble, list", repeat=3, number=3); and you should get more useful figures. PS: /r/learnpython is probably a better subreddit for these sorts of questions. 
Doh. Should have spotted that. Thanks. (updated the post)
Interesting. In my limited usage of Cython, one pain point I found was constantly needing to profile my program in order to be "sure" that I'd properly assigned types everywhere, since of course the program would compile anyway. Is there any sort of tool, like a kind of linter, or perhaps just some compiler flags, to help with this?
Can you pm me what exactly you are having problems with? I posted a zip with example source code, did you look at that? And I'm also confused on what you mean by the .nomedia files ...
Surprised nobody's said this yet, but: If you're working with a medium to large codebase you didn't write, and it isn't *beautifully* well documented, the lack of type declarations can make life very difficult. You work out what function you need to call, but what types does it accept, and what types does it return? Is that a list? Ah, it's an instance of this class over here...that takes 4 arguments. What are *their* types? etc. If you look at any given piece of Python code, often it seems quite easy to understand what's going on. But to make any changes, you end up looking all over the codebase. Type-declarations are useful documentation that is compiler-guaranteed to be up-to-date. That can really make a difference! So I would say: if you have a codebase that's 10k+ lines, and a lot of developers will have to make small changes, Python is bad. You end up having to maintain type-declarations anyway, but there's no language-level support for ensuring they're correct.
Very good to know! I appreciate it.
Did you overpay for python? Are you afraid you didn't get your money's worth when you purchased the document? You should take it back, to where you bought it from. Show 'em.&lt;/sarc&gt; It's all open source. That's the cost. Your choices are : figure it out yourself, pay for support, ask on a forum, or move onto a product that you think is better - paid for or otherwise. Edit: Also, which would you rather have a) software + no documentation or b) no software. Me? I'd go with B.
Open source is not an excuse for bad documentation. Most part of the Python documentation is great, just some bits are missing. That said, it is an open project, so the OP could contribute with a better documentation and examples.
Did you fix the docs after you figured out the answer?
What would be the best way to learn Cython? Should I start with C? As someone who knows Python fairly well, but has no idea what "Malloc" is.
Since cython is basically just C in a new skin (but all the old undefined behaviour, integer arithmetic gotchas, ... are still there), you would probably want to properly learn C first, yeah.
I think you are right and also a bit angry man, but you must know that it's an open source community, we all help to build a better community. Usually when I find a module lacking documentation, once I found out how to do it I make a pull request to add the information. You should do that to prevent the next one in your situation to waste his time.
His claim makes sense, but yes, some benchmarks would be nice.
Your rant is nothing new [The Python documentation is bad, and you should feel bad.](http://cryto.net/~joepie91/blog/2013/02/19/the-python-documentation-is-bad-and-you-should-feel-bad/) (19 Feb. 2013) reddit post at the time: http://www.reddit.com/r/Python/comments/18ssb6/the_python_documentation_is_bad_and_you_should/
Not a great example. It's unusual to malloc 8 bits because the pointer to the thing is bigger than the thing. You might still do it, but rarely.
Nice and small :) It seems to get stuck playing a sound when anyone scores a point.
True, i'm working on fixing that.
This. Nothing gets better by simply complaining.
I believe [pyjs](http://pyjs.org/) works that way, but I haven't used it.
The point of the post is that he's making use (almost) solely of syntax that cython can convert to pure C *without* the overhead of the python interpreter. It becomes the almost the same as writing a separate C module and calling it from python - the code within that module doesn't get slowed down by the fact that your original function call comes from the python interpreter. I suppose there will necessarily be some small cost for starting the python interpreter and calling the code in the first place, and the cythonised code may be less efficient than hand written C, but compiler optimisations probably make the latter mostly or fully irrelevant and the former is only a very small problem. Cython really is very good at this, so although there are no real benchmarks in the post, I think it's quite believable that (to quote) 'Both the Cython version and the C version are about 70x faster than the pure Python version'.
I looked at pyjs (when it was still called pyjamas) back in 2011 when I worked on rctk. If I understand correctly, it's similar to GWT for java and it actually compiles to javascript. Back then the python supported/expected by pyjs wasn't semantically equivalent to cpython which meant my code didn't always work - I'm not sure if this has improved. Also, your application runs in the browser (which is fine depending on your use case), so no file access, opening of sockets, database access, etc (something that rctk did support since it effectively ran on the server)
Thanks. I wanted to start a similar project a while back. I think I would have no pressure forking the project if I have your blessing.
Very nice.
How do you feel about the following changes? http://pastebin.com/ycmWtpTX I only used pastebin because I don't know how to use git. Using the above code, a comparison can be done in O(1) operations instead of O(N) where N is the maximum number in the range function. My example may need more brackets actually to account for precedence. EDIT* according to this page : https://docs.python.org/3/library/stdtypes.html#range I am wrong, it must use the start, stop and step inputs to work out membership. I really like what you have done with it, I will play it when I get home from work.
Hmmmm, I will implement them soon and see if it helps, if it does I'll add it to the repo. EDIT 8 MINUTES LATER: I added them, made one small change with a value to get it to look right (i also had this issue), they're on the github repo, and you have been credited :) you should really learn git ;) EDIT 1 HOUR LATER: Spent the last 30 minutes debugging my code, found out there was a small typo in your collision system, here is the updated code :) http://pastebin.com/vxWz6PyM notice on line 8, one of them said y1 instead of y2 :)
Can you provide an example where this is used?
Okay so, check out this, the result of the cython -a command: https://rawgit.com/syllog1sm/python-numpy-c-extension-examples/master/lib/sim2.html Click on each line and see the C code that Cython generated for that line. A yellow colouring indicates that the compiler believes it's adding significant overhead. As you can see, there's overhead associated with the Python function-call conventions, but when we're picking values out of the arrays and doing numeric operations on them, everything is "fully C". The C that the Cython compiler generates is fussy, verbose, and a bit odd. But clang and gcc do a fine job of optimising any weirdnesses away, and we end up with a .c file that is just as performant as a .c file you would hand-write.
Thank you very much. All I really want for /r/python is for me to be able to return here, and get great content that will help me become a better python developer. I think too much overlap with f.x /r/learnpython will damage /r/python, as it relates to my goal. Which is why I was hoping for stricter, delete-"happy" moderation. But, I respect your decisions, and by no means is this subreddit already "ruined". I just hope it never happens.
Database is perfect for this kind of thing. I'm not too sure on commonly used Python database systems, but any popular SQL based one should do. You'll want to find out how to create a database and a user table with a student Id and password field. From here, for a simple system, when a student tries to log in it'll compare the student Id they entered to see if a row (student) containing it exists in the database. If it does, you can compare the password stored alongside it with the one entered at login. Implementation wise, I can't help much with Python. I'm gonna assume that you'll have to use a few SQL statements in your code for the database access. I hope this helps in some way, good luck! 
Okay so, I had never written anything in C, and I'd worked in confusion on my PhD supervisor's C++ code for a while, always going "fuck, what does this bit of syntax do". I wish the Cython documentation were better, and if I had money and leisure, I'd work on this. When I was an academic, I often joked to friends that I was being paid to produce public goods, but really the best hours-multiplier impact I could make would be to fix the damn Cython docs. Certainly higher than writing another paper. It's still probably true, but now I'm self-employed. I say: You'll only ever learn a thing by doing something, and doing things in C is going to come with too much over-head. How do I set up my tool-chain? How do I parse command line arguments? How do I print stuff? etc. So, I think you should start working in Cython, not C. So: look up memory management tutorials/explanations, which will refer to C, and understand enough of the C syntax to be able to follow C examples. But when you set out to actually do a thing, use Cython. Also, start by writing some code "fresh", not by writing a wrapper to an exisiting C library. Allocate and deallocate the arrays and structs yourself, don't use numpy or other "sugar". Oh, and make sure you're adding exception declarations, e.g. "except -1", so that you get Python exceptions where possible, instead of just segfaults.
Wow... this is fricking cool! Enable each line of C code to explode into the corresponding assembler, and we'd be pretty close to a full picture there; optimizer notwithstanding. Serious question though: just because you use cdef for all the variable declarations and cymem for the mallocs, it's not necessarily going to be significantly faster than the pure Python equivalent is it? I notice that a lot of the Python code still generates a lot of C that uses Python C API functions. Am I reading that right? IANACP so forgive my ignorance here.
&gt; I had to spend hours digging around for examples and then had to crawl my way deep into the Python tests to try to find out how to drive it. Fuck what a waste of time. Next time, try reading carefully. All the information you need is there. I opened the page with no previous knowledge of how to do what you're trying to do. The first thing I did was ctrl-f for "base64", and the result was: &gt; ## 21.6.17. DataHandler Objects &gt; &gt; `DataHandler.data_open(req)` &gt; &gt; Read a data URL. This kind of URL contains the content encoded in the URL itself. The data URL syntax is specified in RFC 2397. This implementation ignores white spaces in base64 encoded data URLs so the URL may be wrapped in whatever source file it comes from. But even though some browsers don’t mind about a missing padding at the end of a base64 encoded data URL, this implementation will raise an ValueError in that case. Two things to notice: this section is named "DataHandler objects", so this means `data_open` is an instance method. Secondly, the parameter is named `req`, so that means it's a request of some sort. Scroll to the top, ah yes, there's a `Request class`. We're done. That's it: from urllib.request import DataHandler, Request url = 'data:image/png;base64,...' data = DataHandler().data_open(Request(url)).read() Or, if you hadn't found that section, you still must have noticed that `DataHandler` has the word "handler" in its name. Let's ctrl-F for "handler" and read. Hmm: &gt; `urllib.request.build_opener([handler, ...])` &gt; &gt; Return an OpenerDirector instance, which chains the handlers in the order given. Looks good to me. Now what do I do with an `OpenerDirector`? ctrl-f "OpenerDirector" and find: &gt; ## 21.6.2. OpenerDirector Objects &gt; &gt; OpenerDirector instances have the following methods: &gt; &gt; [...] &gt; &gt; `OpenerDirector.open(url, data=None[, timeout])` &gt; &gt; Open the given url (which can be a request object or a string), optionally passing the given data. Arguments, return values and exceptions raised are the same as those of urlopen() (which simply calls the open() method on the currently installed global OpenerDirector). Bingo! We're done: from urllib.request import build_opener, DataHandler url = 'data:image/png;base64,...' data = build_opener(DataHandler).open(url).read() You see, it's not that hard. There's no need to go searching for examples or diving through the source. Read what's written carefully. It took me a few minutes to find these two ways of doing this and verify that they both worked. Being able to read technical documentation is one of the most valuable skills you can develop. 
Whoa, cool! Tx, many useful tips there.
That's not entirely true; complaining in whatever issue tracker exists is often a great gateway to getting docs fixed, at least if done in a civil way.
Ah, yes, I've actually been using Python Tools for VS for a long time, with both Iron and C python. It's a great extension! I was unintentionally vague with this question. What I meant was, are you running your Django site on IronPython or cPython? I've been jonesing to use IronPython for Django for a while...
Could the power sources be classes? Do you have a model for each type of power source? If so, each model could know how to calculate its own cost. I don't know anything about ISO 50001, but let's pretend the following is how your data is structured: You have a model instance (database record) for each powersource. Each powersource reports its usage periodically, which you then store in another table with a foreign key to the powersource. So, to get the power usage of a powersource, you need to sum all of the usage entries over a certain time frame. This is something you could do as a method in Powersource. Let's say you have a powersource instance. You want to see its usage, so you create a method 'usage_over_time()' that takes a date range and returns a usage amount (what is that? kilowatt hours?). That method could simply query the usage table, and summate the usage over the date range. Okay, so that's good, but now you have to create a date range in some other function every time you want to query, but what you really want is always just the last year. In that case, you could create a helper method that is just 'past_year_usage()', which generates a date range ending at now, starting one year ago, which calls 'usage_over_time()'. I don't know how you calculate cost, if that's a constant, or if it varies by time of year. If it varies, I assume you would want to include that cost in the usage record. If it is variable, then your usage method should probably also calculate cost too. Now you still have the problem where you need to aggregate all this data. Well, you have to do that somewhere. A function is a good candidate for that. More than likely, it's probably coming from a view. Certainly your view could query the powersources and loop over them to collect the costs. If you find yourself needing to this in more than one place, you could either create a utility function (like your calculation.py might contain). Or you could use classmethods on a Powersource base class. Since you are aggregating information across objects, it doesn't really make a ton of sense to do that part in a class; however, all calculations pertaining to each individual powersource can (should) be done in a class. 
Yep great article, making th same points and this guy really hits the mark. The most important thing he says is this: "PHP solves this by having examples for every single function and class." Python seems to have mottos and sayings and peps for everything. It probably has a "zen of documentation" which says "never provide a complete working example for every method and property because users might be able to work out what is going on in a matter of seconds".
Yes it does make me angry to have to spend hours finding how to do something that takes three lines. Python itself is a very efficient and productive language when you know how to drive it and it's annoying to have its productivity slashed by poor documentation.
You need to specify the types to reduce the calls to the Python C API and make your program significantly faster. In that example, compute_F (the function that is responsible for most runtime) is compiled to almost pure C code, as you can see by the white lines in the annotated output.
Give me a SIMPLE way to update the docs and sure I'll add examples in sometimes. But force me to become a "contributor" to a heavyweight process, doing diffs and source code checkins and mercurial and signing forms and blah blab blah then no Im not interested in contributing a few examples to the docs.
You can add as many examples as you want, but people will still have to learn how to read and think. It's impossible for examples to cover every use case. What might be a perfect example to one person's special snowflake situation is just noise to scroll past to someone else. And once you pick up those skills, you can solve any problem on your own in a few minutes, rather than getting frustrated and taking hours. 
To some extent isn't this premature optimization though? The other questions to be asked are: 1) Why not just use C? and... 2) Why not use languages like [Genie](https://wiki.gnome.org/action/show/Projects/Genie?action=show&amp;redirect=Genie) or [Haxe](http://haxe.org/)?
You're argiung that the docs should remain without working examples for every method a property because for everyone's good that instead people should have to learn to work it out.
His code doesn't use iterators, I wouldn't call that using python. It's just writing c in a more obscure way.
I'm saying that even if the documentation had an example for every method — which would be completely ridiculous and obnoxious — you would still have to learn to read and think because they still wouldn't cover every possible use case. The primary documentation is the written prose; examples are garnish. You can't live on sprigs of cilantro.
actually the author was imprecise [cymem](https://github.com/syllog1sm/cymem) not really wrapper around malloc &amp;co but PyMem_Malloc &amp;co. This is even better because like this python interpreter know about the allocated part and can be more clever about when doing a garbage collection. The documentation could emphases that it is not a good idea to store a python object in the c structures because python would not know how to resolve the cyclic reference (if i don't mistake)
pyenv supports virtualenvs very well with the virtualenv plugin ;) $ pyenv install 3.4.1 $ pyenv virtualenv 3.4.1 blargh $ pyenv local blargh $ python 
It would be interesting to see how numba or pythran would work for this, since it seems like an ideal use case. Might have a look when I get home. With the intent to keep it in Python and make it fast with less effort, after the fact.
I had [this blog post](http://zach.se/generate-audio-with-python/) bookmarked for later use ... here's to hoping it provides a starting point, at least.
http://rosettacode.org/wiki/Sorting_algorithms/Bubble_sort#Python
This defies the conventional wisdom that you should write slow--&gt;profile--&gt;tune, yes. But think: is it premature optimization to write in C/C++ before you've prototyped in Python? The logic behind the conventional wisdom is that you can't know where your performance problems will come, and so you should gather data first. But, who says we don't know anything? Sometimes we do, sometimes we don't. Guessing where a performance problem will be, and wasting time tuning it, can be expensive. But it's also expensive to replace all of the data structures in your program, because you did it wrong the first time. Engineering is about difficult cost/benefit analyses, that can't be solved by any slogan. So yeah, it could be premature optimization. Depends whether the optimization was *actually* premature :).
Thanks
this looks very useful - I'll certainly try it out! 
SQLite is actually an amazing database system. It's really only extremely large datasets, high volume or high concurrency that it's not suited for.
If nothing else, restarting that if it crashed would have been pretty painful.
You might consider adding a bug to the Python bug tracker, then. http://bugs.python.org/
Does this imply that if Python adds static typing, that the resulting generated Python byte-code could speed up significantly?
True, but it is not a virtualenv alternative itself, which is what I'd take you're implying if all you do is leave the link to it as if it's self-evident.
yeah, that is what I wanted to do, I just don't know how to go about showing that. Nor how to release an open source project or licensing issues that might arise. How do I turn a package into a module that can be installed with PIP like pyzipcode? Pyzipcode is GPL and the R zipcode package is CC BY-SA 2.0.
Awesome. I ordered Python cookbook today by won't be here until Thursday. I think a small sql db is all I need. This is very low usage.
To build off this and answer the two follow up questions: *1) Why not just use C?* /u/syllogism_ 's comment answer this partially. Additionally, you get to keep access to the greater Python ecosystem of tools you may want to use. *2) Why not use languages like Genie or Haxe?* Well, I don't know Genie or Haxe, but the more poignant, albeit similar, question to ask is: Why **not** Python? What problems does Genie and Haxe solve that Python does not? Again, not knowing Genie or Haxe...but... It seems that Genie's claim to fame is C-like performance...but with Cython you are already getting that. Seems a little redundant to learn another language to just get C-like performance. Likewise, Haxe's claim to fame seems to be universal platform deployment. It seems to accomplish this by using a unifying compiler that compiles Haxe code into other various tools. This is an interesting higher level glue-type approach that seems interesting...if severely limiting. But why not just use Python to build a platform agnostic application? I guess it could depend on the goal of the project...
Please post learning questions to /r/learnpython! You'll get a much better reception there. Make sure you search first though, I'm sure this question has been asked. Good luck
It's easier than you think. 1. Create a github account 2. Create a repo 3. Add a LICENSE file with whatever license you feel like (assuming you're not using anyone else's code in your project, in which case it gets more complicated) 4. Add the rest of your project 5. Read a few guides on python packaging/setup.py, do it wrong on your first few tries, eventually get a working package uploaded to pypi At some point in there you would email the maintainers of askgeo.com and show them your github repo and explain your project, and hopefully they'll give you access. 
Indeed. I used the push-based generator idea to create a script that reads a bunch of files with LaTeX syntax in comments, programmatically inserts some more, and makes three output files with different content. It's like Unix pipes but with arbitrary Python values and in the shape of an arbitrary directed graph instead of a line.
sigh. /r/learnpython. 
Post it there? Or look there? Cause looking I couldn't find anything :(
That doesn't sound that out of date. I'd go for it if you don't care about Python 3 support. Shoot, we're still using pyxml at my company and the last version was made for Python 2.4 (I think). It's a horrible library that imports as xml, which overwrites the default xml library, so unless you have a comment, you don't know what is being imported.
Why do you want to use it? Is it to solve a particular problem or just to try it out? I'm with billsil that February isn't that long ago. Are you planning to have other developers on the project? Are you wrangling an old codebase or starting a new one? What do pylint, pyflakes, pychecker, or whatever linter you use think of it? If you have to throw out all of your static analysis tools to embrace macropy, I'd think twice before doing anything that you couldn't easily gut and replace. If you are working with other people, be aware of what their development environments might be. Do you use mock? Try mocking something defined by macropy. If you really need to shoehorn some conditional types in as you tackle an ancient 1m loc codebase, sure, give it a shot, but if you are looking for away to save writing some straightforward code for a budding project, I'd KISS.
"enter the command" means that your are in a REPL where `data` is something like `print repr(data)`. If you just run the file, it will only print what you tell it to. In [1]: class X: ...: def __repr__(self): ...: raise Excpetion('oops') ...: In [2]: X() Out[2]: &lt;repr(&lt;__main__.X at 0x7f487202bb48&gt;) failed: NameError: global name 'Excpetion' is not defined&gt; Your question is well put. Thank you for that. You've clearly shown what you are doing, what you expect to happen, and what actually happened in addition to why you are trying to do what you are asking about, where you are trying to do it, and what you think might be a cause. If you stick around on /r/python or /r/learnpython, you'll see that this is pretty rare.
This should be pretty perfect for Numba. I'd be surprised if it doesn't get up around the same performance as the C code. Cython has an edge on Numba when you need data structures that use pointers. Otherwise, if all you're doing is numeric computation on contiguous arrays, Numba should be able to get you to the same performance, and even a little better if the SIMD stuff kicks in properly.
I'm not sure this is an example of that at all. We're not talking black and white here with grey somewhere between, we're talking about 2 languages that have strengths and weaknesses. Python has syntactic sugar as a strength, and C has its speed. We're not finding a half-way point here, we're taking the good from one, and also the good from another. Separate, they're still both very powerful. Together, they're just as powerful.
Too bad you didn't call it Pyong.
Thanks! The examples are great. 
As soon as this post vanishes I'm renaming the repo.
You need to use ipython notebooks. That is what all the pandas tutorials use. Here are the terminal commands to install and start: pip install ipython ipython notebook 
Check out /r/learnpython it has excellent resources for getting acclimated with the language
This might help: https://stackoverflow.com/questions/10357992/how-to-generate-audio-from-a-numpy-array
I up-voted you, cause it is hard. But, the hardest one, is also the first one. 
In a Django web app I had to generate a huge CSV file. But this used a ton of memory to store it while building and was a bad user experience for download - user didn't get Save As window for a minute until file was built. The improved version used a generator to return the file line by line as it was built. Just a matter of moving code into a function and yielding back each line. Much better user experience since download starts instantly.
To find out if one key belongs to both dictionaries, you could try something like this... for k,v in dict2.iteritems(): if k in list(dict1.keys()): # load dict1[k] # v is dict2[k] # do whatever you want with those values In general, I like to iterate over dictionaries with for k,v in dict.iteritems(): print 'key is ', k print 'value is ', v 
Thanks! Yes, iterators I understand, I was just trying to fit generators into my repertoire :)
is the code still working for anyone? the /api/getMessages doesn't seem to work anymore (/getGreatest still works)
I'm having a bit of a hard time trying to understand what these statements are doing. for k,v in read.dict.iteritems(): if k in list(ref.dict.keys()): k is the key in the ref.dict and v is the corresponding value to that key?
Let me elaborate a bit. Take for example the Read: GGC GAG CCC TCT The read.dict would look something like: GGC: 8 GAG: 6, 11 CCC: ? TCT: 2, 14 I assume this would happen in my case. The correct order given the offset is 8, 11, ?, 14; where the ? has an expected value of 14 because the length of the Read is 3. Each key is offset by the Read length. The value 14 for Key TCT is expected to be 17 and therefore we can assume there was an insertion in the Read. An insertion can refer to as point for which we are at is less than the expectation. Does that make any sense?
I think I follow your example. How many reads do you have exactly? Are we talking millions+ or something smaller? Are the reads all ~20mer or do you have longer sequences? If you have fairly small sequences, I would just load them all into ram and do set comparisons. # perfect matches matching_reads = set_of_reads.intersection(set_of_references) # insertions/deletions/whatever inexact_matches = set_of_reads.difference(set_of_references) If you are gung-ho on your current idea (rather than blast), I think you would be better off dealing with the actual reads rather than doing this trick with the dictionaries - you are going to rely on a nested data structure which requires more bookkeeping or iterating over the lists inside the dictionaries.
You started X windows. 
If you like that, you should check [MoviePy](https://github.com/Zulko/moviepy) from the same author, it's great to manipulate videos in Python!
This has nothing to do with Python, for the record. Terminal is just a command prompt. You can use it to run Python, but you can also use it to run many other programs. It is just a command prompt. Like other people said, typing "x" into the command prompt tries to start "X Windows", which is already running and cannot continue. the "= 34" is interpreted as the command line arguments to "x". The command line arguments are irrelevant, because "x" cannot start. If you want to run python and execute python commands, you first have to run "python", instead of "x". THEN you can start trying python commands.
I'd suggest asking for help in /r/learnpython.
Can you also use c++, or strictly C only?
That is one spammy site. Here's the non spam link: http://chimera.labs.oreilly.com/books/1230000000393/index.html
Thanks for the direct link. It would save us a few clicks. However, I do not agree that the site in question is spammy, as it provides what it claims. This is my opinion. 
I'm no expert. I actually just started learning python a few days ago. I've been watching these great beginner lecture videos that you might find helpful: https://www.youtube.com/watch?v=UQVK-dsU7-Y This is only one of his introduction videos. He has a video for every chapter of his text book, as well as a really helpful free course on his website that follows these YouTube lectures. If you want to skip the introduction stuff and see him teaching the language, just find a video from one of the later chapters and see if it interests you. I have no idea if this will help you in your quest of game design, but it certainly can't hurt. I feel it introduces you to some important concepts and ways of thinking which you can probably put to use when learning other languages. 
Based on your account being brand new, and you already posting three links to this site, you probably already know this, but he's probably referring to [the absurdity of the advertisements](http://i.imgur.com/xP1cMgl.png). Plus, the site is the definition of blogspam. The content is all ripped from elsewhere, and the only reason anyone would go there is because they were somehow pointed there instead of to the page with the actual content they want (the link /u/rubycowgames posted which is at the bottom of the blogspam you posted). The sole purpose is clearly pulling people in from searches and hopefully getting them to click one of the dozen ads that pop up all over the page.
I have **recently** used this to install pygame for Python 3 on OSX 9 (it assumes you have Python 3 already installed - but that's trivial): ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" brew install sdl sdl_image sdl_mixer sdl_ttf portmidi brew tap homebrew/headonly brew install --HEAD smpeg brew install mercurial sudo pip3 install hg+http://bitbucket.org/pygame/pygame Done.
Some years ago I implemented the SOCKS4 protocol for a server. The plan was to use the **asyncore** module. Since the server was (of course) able to accept many simultaneous requests, I created a function that was handling the SOCKS4 protocol, yielding **R** if the function was waiting to read from the socket, and yielding **W** when the function needed to write to the socket. Again, everything was asynchronous, so I wasn't able to "just write to the socket" or "just read from the socket", I had to wait for the asyncore loop to give me the sockets that were ready for that. I still think it was a pretty neat solution to avoid the "callback hell" you usually have with this kind of async programming. I can try to find the code if needed. Unfortunately GitHub was not so popular at the time :P
Our team is creating free weekly news digest in russian http://pythondigest.ru/ We are collecting the best articles and news from reddit, twitter, pythonweekly, pycoders, planet python, informit, ibm dev network e.t.c. (25-35 really interesting links each week) If someone wants to be an editor of english version - feel free to contact me because my english is sucks to write good announces in english.
Yeah, 17 hour old account posting lots of links to the same, ad heavy site. We're not idiots.
We use it internally for a SaaS ERP that's powering a chain of some dozens of stores, and even though that's the best example we have at the time of heavy usage, that one is not public. However, it was recently used for a public consulting project of ours, and that one is public, check it out at www.rocklobby.com (if you inspect a request you'll see it's running netius).
Not that recent but could that help? * http://geocoder.ca/?freedata=1 * https://catalog.data.gov/dataset?q=zipcode&amp;sort=score+desc%2C+name+asc 
I almost feel honored that you created an account only to give me shit for what I wrote. Thank you :)
Hey, that does seem like the sort of error I would make. I think I was wrong to put &lt;= in the code, if we are checking in the range(0,30), Then i think to be correct 0&lt;n &lt; 30, instead of 0&lt;n &lt;= 30. In general, I was hoping this change would improve performance, has that been your observation?
Adblock Plus. 
Interesting, but I'd like to see a properly worked example: input html, source code for a MWE, an explanation of exactly how and why the code works, and a complete listing of the output. That would make it much easier to see if this is easier and better, or just different (and maybe more convoluted). As to the problems, I think that's true, even with django templates.
probably shouldn't be encouraging people to pirate copyrighted material here. O'Reilly often has 50%-off ebook sales, if you're cost-sensitive you can keep an eye on their web site or sign up for email offers... 
As it is pointed out in the article, i'm still experimenting with this. You can check out the POC Django application here : https://github.com/makkalot/django-enlivepy-example it has a todo application example with lots of forms (usage for csrf tokens) in it, which may give more information about the flow. Probably, i will write a second post, which will be more like a detailed step by step tutorial.
Still no match for a direct link to the thing itself.
It's an interesting idea! It provides a nice separation of content &amp; logic, and it's more powerful than a template engine. However, I have two concerns: * Simplicity - the code seems a bit complicated. It would be nice if I could just do something like this for common changes: &gt; html.set('li &gt; a', href=url) * Performance - parsing the entire html, transforming it several times, and then building it again, seems like it would be slower than most templating engines. Of course, the html-tree can be cached to avoid parsing, and perhaps some transformations can be cached as well (and invalidated when the variables change).
http://www.reddit.com/r/learnpython/comments/2jyf45/quick_help_in_comparing_keys_and_values_of_two/ Original poster /u/pyhelp, reposted by /u/helpwalgo for some reason.
What about MacPorts?
Also of you're going to post a question you'll have to format it 
OK, thanks for making us aware of your templating system. I am the author of [HTML::Seamstress](http://search.cpan.org/~tbone/HTML-Seamstress-6.112830/lib/HTML/Seamstress.pm) for Perl and I maintain a list of systems using this style of tmeplating. The style of templating you are using is known as ["push-style templating"](http://perlmonks.org/?node_id=674225) and there are many such systems inside and outside Python. The most widely used one is Python is meld3 in my estimation. Terence Parr wrote the definitive paper on this style of templating (linked to in my perlmonks article). Now some comments about [your article](http://makkalot.github.io/posts/2014/Oct/22/enlivepy-html-transformation/) - * Your link to the github repo in the section "A possible solution to the problem" is broken. * the template attribute of your TodoNavSnippet seems a bit brittle. it seems more flexible for the snippets to operate on HTML/XML trees instead of HTML itself. This way they can be agnostic to where they are being used and will work anywhere. I suppose you can achieve the same effect by overwriting the attribute. But again, I would aim towards snippets working regardless of where the HTML they got comes from. In other words, the snippets seem to be a bit framework-specific or telepathic by using bits of HTML. Also in this code snippet, I wonder if you could chain modifiers of the element like `content(i).set_attr( .. )` instead of two lambda calls? Finally and most importantly, it seems that an introduction to enlivepy without any Django-isms would be much easier for the public to digest initially. I do not know Django and found myself confused by all the discussion of `STATIC_URL` etc. I will be moving over to a new format for the push-style templating databse shortly. 
Thanks for your personal opinion..
We are currently working on a replacement for a DJango app written in AngularJS. While the backend is still written in Python, it is just providing a REST api for Angular to consume.
And Node.js devs are switching to Go. Yes. All of them. Some developers are always chasing the new and shiny. This is great. But many others just keep using known and tested technologies. That's great too. Don't panic, keep coding :-)
This. The client side is JavaScript, the back end is still Django and the whole mature server side ecosystem Python has.
You should really check out docopt. It's really nice. https://github.com/docopt/docopt
Let's imagine: - hey Boss, our team is very efficient with python, we have 3 successful products written in python - good, so? - i wonder if we could rewrite everything from scratch and start learning something else
Javascript apps? Web apps have been going in that direction before Django was even conceived ... The javascript tools and frameworks (and browsers!) are finally getting good enough that building such apps isn't a total exercise in frustration. I'd be curious to hear if anyone thinks that getting such apps up and running is easier for a web dev beginner than compared to something like Django. Companies? My org shares the model for much of it's data in an SQLAlchemy package for use by command-line apps, so re-doing that model in Javascript isn't very appealing and so doing web things in Python still makes the most sense. I think if I said, "Let's do all our pipeline work in Javascript!" at work, I'd get bad looks. Python is always going to be a more pleasing language to write in than Javascript. 
Thanks Soriven... 
Thanks, this is what I thought. Node.js may become more popular but writing Python code is more pleasing..
Yep, that (along with other projects) is kind of what inspired this. I'm in no way competing with or replace projects like that. This is more of a learning platform and gave me a more complex project to work on instead of the typical "Compute X fibonacci series, blah blah blah." :) I'm more looking to see if I'm doing any that's a crazy python no-no or un-pythonic here.
*Dismisses any opposing answers as personal opinions.*
Web app development is just going to continue fracturing. As each new genie comes out of the bottle, the other genies aren't going to go back in. It used to be taken for granted that LAMP was the defacto way to go and the only arguments were between Perl/Python/PHP and PostgreSQL/MySQL. My current side project is using Pyramid (Python) + Discourse (Ember.js + Rails). The Python part of the app works with PostgreSQL and LDAP, so there needs to be a fair bit of Python going on. At that point, it still seems easier to me to continue with the UI in Python land than to try and fob the work off to something JS. Discourse is pretty cool, but man does it have a lot of moving parts under the hood. 
I shit you not... this happens a lot more often than you think, but it's usually the other way around where the boss (potentially new) motivates the team to go in a drastically different direction. 
As someone who manages development at a company; we are NOT considering fully switching from Django/Python to Node.js/Meteor. We have never talked about switching our existing Django apps to anything else. That being said; we have looked at using node.js for future projects; but we'll probably stick with using a Front-end framework and using Django or Flask on the backend, depending on the project. My personal opinion is that I like writing apps in Python and am comfortable with getting a project out the door with it. There is no compelling reason for me to change that. If I knew I could code something up faster/easier in Node.js/meteor, then I would consider switching to it; but so far, that isn't the case. Python/Django work great for us.
Thanks landyman for sharing your opinion and experience.
This question really does not belong here. I would check on /r/redditdev if I were you.
Oh lord. Good luck. PyGame on OSX is **not** hell. It's **worse** than hell. In hell there are no innocents.
Thanks for your comment, about the arguments : - I'm still experimenting with api, but i'm not sure which part seems complicated. Probably for something simple like this it is a little bit overwhelming. The idea is to have a chain of transformations to the selected nodes. What would html.set would return, the transformed nodes ? - Yeah, there are lots of things that can be done about performance, especially on caching.
Thanks for your reply, I was not aware of meld3 library, definitely will check it. Just to make it clear, i don't claim anything that this is something i've invented. As it is pointed out in article, it is something i copied from "enlive" (clojure library). About comments : - Thanks for the report of broken link, it is fixed now. - As you can see TodoNavSnippet inherits from DjangoSnippet so, it is somethong Django related. It seems ok to me to be related to htmls. - About content(i).set_attr( .. ) i'm still not sure about the api, would something like html.select("selction").content(i).set_attr() be more useful ? To answer your question, currently it doesn't work like this it is more general, but for sure you can write a wrapper that does some chaining like in your example. - I use Django a lot myself and it is kind of popular in Python world, thats the only reason why it was chosen. Because, if i had introduced only the enlivepy, people would complain : "Ok it is nice, now how can i use it in my Django app " :) I have a good readme on enlivepy github page : https://github.com/makkalot/enlivepy Can you give more info about this new format ? Thanks.
Mainly, coding practice for me. That's why I posted it here for critique (of which I received none - any idea where to post it to get critique? /r/learnpython?). Other than that, I gladly pay music that's available for money. What I personally don't like are things like "Like these 3 pages on Facebook and enter your mail address to download this 2 minute track". I see how circumventing click-begging may be considered stealing. I personally don't think it is and buy tracks I like.
work at a perl shop (15 year old dotcom). head of tech made a huge push to move to java, even though no one in house had any experience with java. the main driving force was that the perl mods were not maintained well, which is fine and justified for what we were doing. i suggested python because it made more sense going from perl, but meh. 
I think s/he is not a native english speaker and also bad at reddit. They're just thanking people for responding, but qualifying everything with "personal opinion".
Please don't use hard-coded http:// links -- use protocol-relative urls (//www.github.com/etc) instead. Currently, https://makkalot.github.io/posts/2014/Oct/22/enlivepy-html-transformation/ looks like a mess because the stylesheet doesn't load (and https everywhere redirects me to https automagically)
That sounds like the exact same thing that happen at the last company I worked at. I even suggested python but left to work somewhere better.
&gt;The important thing is that cython is not an attempt to "average" two languages, it's a tool to solve a specific problem, and it does work well for that. I agree that Cython is a tool to solve a specific problem, and that it performs that function very well. I think the issue here is that the author is trying to use it as a "best of both worlds" language, as opposed to an easy way to improve Python performance.
I rewrote our internal tools web server from Flask to Node because it was just "better" for what we're doing. Lots of live events, live stats, server network connections, etc. The client side used a lot of websocket technologies, and NodeJS is king in websocket world. However, the heavy lifting resides in Celery tasks -- so I wrote a Celery client for node. Right tool for the job etc etc
Whenever you scroll through source code and see things that repeat -- that's usually an indicator you should be using a function. You could probably cut your LOC count in half, if that's the impressive part of this project.
1422 upvotes 3908 favorites **closed as not constructive** Gotta love SO.
You most certainly can do this in Django, RoR or any other traditional web framework. a RESTful site is just as much a website and any regular HTML one. It is just that the consumer has changed. Both are effectively outputting structured text documents at the end of the day. You still need to take care of all the normal stuff: authentication, authorisation, persistence of data and any complex business logic. Frameworks like Django have all that stuff well and truly covered. Not need to throw all that away and start again.
No. You don't.
Just throwaways. 
&gt; It's likely that the compiler was able to optimize the previous implementation using the same vector instructions. He can't check?
Whether Hugs is a popular implementation is irrelevant. My point is that being "interpreted" is an implementation characteristic, not a language feature. Why do you say you want an "interpreted, type safe language"? What about being *interpreted* is desirable to you?
Popularity doesn't mean constructive dude, come on. 
Yes, I agree with that. I code in Haskell all day and I definitely miss the immediate feedback interpreted languages have. 
Another way you could use this w/o "stealing" from the artist; apparently Soundcloud has a download limit for free accounts, so if you want get songs they previously had available for download... I guess you're "stealing" ad revenue from Soundcloud, but I already do that when I browse w/ adblock...
Interesting indeed. It might be quite nice to be able to have tabbed before/after views when showing the example templates.
Sorry, I can't. The topic is **closed as off-topic**.
I would assume you would like a loop to decide these factors like every hour you would add a certain number of organisms based on the rate of growth. If I were to do this I would make an 'input' function to ask how many organisms do you want and which it prints out the amt of organisms, rate of growth, etc. per 'hour'. If I am correct, you could do something similar to: number = input("Number of organisms wanted: ") then you could do a while loop similar to while number &lt; (put variable for how many organisms there are here): CODE HERE I could whip up an example if you would like if this is what you are asking. Good luck figuring it out and just tell me if you want me to give an example or if this is not right. EDIT: Make sure to type: number = int(number) after the input function so it will be an integer instead of a string to avoid errors. Ex: number = input("What is your age?: ") number = int(number) print(number + 1, "is how old you will be on your next birthday!") 
Your best bet is going to be to switch to numpy. Doing anything in the millions with plain python is probably going to be too slow. Very quick example of what I'm thinking would be an element wise comparison of two arrays. In [1]: import numpy as np import random import string In [2]: ref = [l for l in string.letters[:26]] In [3]: read = [l for l in string.letters[:26]] In [4]: random.shuffle(read) In [5]: ref_ar = np.array(ref) In [6]: read_ar = np.array(read) In [7]: read_ar == ref_ar Out[7]: array([False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False], dtype=bool) In [8]: (read_ar == ref_ar).sum() Out[8]: 2 This would give you how many identical matches were present in a string. I'd recommend translating it first, but I don't know if you need the fidelity.
Seems about right. I'd store the data in an SQLite database if it's more than 100 rows. If it's less, a CSV file. PyQt, PySide or maybe a web application using Django or Flask. What format does the end result have? *Edit: I believe I've used [xlrd](https://pypi.python.org/pypi/xlrd) to get data from Excel spreadsheets, it's quite nice.*
I'm going to guess that the first workbook is, in fact, an overgrown application that takes data and produces a report based on it? If the input is just a few fields, GTK+Glade, wxWidgets, or any number of widget toolkits will suffice. (I only suggest going web-based if you already have a logical place to put it OR the UI is complex enough to justify HTML. A one-form website seems like too much work.) OTOH, if you have to take in tabular data, I suggest taking in a spreadsheet and processing that. In my experience, these people are really good with excel; don't take it away from them.
The simplest solution I've implemented was using HTTP Basic Auth headers with [Flask-HTTPAuth](https://github.com/miguelgrinberg/Flask-HTTPAuth). You set the "Authorization" header in `$http.defaults.headers` and basically forget about it after that. Make sure everything is sent over HTTPS since the HTTP Basic Auth data is plain text. I've also done a system where you POST the password to a `/login` endpoint, which then returns an authorization token generated with the [itsdangerous](http://pythonhosted.org/itsdangerous/) library. These tokens can generated with a timestamp so they expire after a set length of time. The tokens don't need to be stored in the database on the backend. The tokens can be placed in the "Authorization" header like the previous method. Both methods are completely stateless as far as the backend is concerned and are very easy to implement on the frontend.
Who reads the documentation? **/s**
Python docs are great!
Not when the whole project takes two hours to build. Add dependencies and it's three hours.
What's the difference of this from right clicking on an image and clicking save as? Does an arbitrary hurdle make it stealing? In the same sense of people trying to protect images via disabling right clicking with javascript? As in, if I set my browser to not run javascript and now right click save the image did I steal it? Lets say it's a Flash slide show now protecting the image. It's on my screen. I scrape my screen with Sniping tool to save the image. Am I a thief? If I screen scrape and post to imgur to post to reddit am I criminal? 
I have no idea what the fuck you're talking about.... I guess i don't get the contract
You almost certainly have more Haskell experience than I do.. Aren't there was your builds could be made faster, like having pre-built modules and a distributed build system? Even when dealing with interpreted code, you can run into this exact same problem if you have any kind of "build" step, eg: native code libraries that need to be compiled, or pre-processors/code-generators that need to be run. I remember having this problem with some Python projects at a company I used to work at.
yeah! my code currently looks like this * number = int(input("Enter the number of organisms ")) * rate = float(input("Enter the rate of growth ( Number bigger than1)")) * cycleHours = int(input("Enter the number of hours to achieve rate of growth") * totalHours = int(input("Enter the total hours of growth')) for loop will go here! print('The total population is " +str(endPop) + ".") 
I think this is the right way - or, at least, that's how I do it. I also used Ember and Flask, awesome combination in my opinion.
I've used this company in the past: http://www.zip-codes.com/ But you can source this all yourself from free census data. If you use AWS they have the data available for free also, but it's older. Here's a few more sources, some of which are free, others paid: http://www.census.gov/geo/maps-data/data/tiger.html http://download.geonames.org/export/zip/ http://lite.ip2location.com/database-ip-country-region-city-latitude-longitude-zipcode-timezone http://www.boutell.com/zipcodes/ http://www.unitedstateszipcodes.org/zip-code-database/ http://federalgovernmentzipcodes.us/ http://www.zipinfo.com/products/products.htm There's tons of these... You could just do timezone detection with javascript, and use reverse IP to assume the user's zipcode... though that's not wholly accurate. What do you need this data for? **edit** http://www.nist.gov/pml/div688/localtime.cfm#zip NIST recommends this, which is free: http://www.zipinfo.com/search/zipcode.htm
I don't have IP addresses associated with historical records. I'm trying to associate recorded billing zipcodes with timezones and recorded datetimes so I can relate approximate time of transaction (from the purchaser's perspective)
so you stored the dates using user's local time? oh noooo. *facepalm* how critical is this? *edit* so are you showing this data back to the users? on a webpage? or via print? if on a webpage, why not use javascript to use the user's browser's timezone?
no, dates are stored on database time. I want to know what time it was for the purchaser.
I'm not showing the data back to users. I'm analyzing it, looking at consumer purchasing behavior.
ahh so wondering what hours they were browsing, etc... assuming some statistical norm is what you're aiming for, i think using one of these free databases should be more than accurate enough. if you're looking at broad numbers, and aren't worried about unique results, should be fine if you have a pretty sane standard deviation.
yeah, I'll have to look at the other databases you added when I'm at work. Because the exact numbers aren't a huge deal, I did go ahead using the pyzipcodes library. It ends up amounting to about 5% of results without a matching zip and I'm sure a handful of the results amount to data from the counties in Indiana which have changed. I'm just surprised that the source data for such a used library hasn't been updated in 10 years, and I can't find a real alternative. Seems like there could be applications in which that small percentage does matter.
yeah, don't know the source, but it amounts to the same kind of data available in r zipcodes. no timezones or offsets.
the last column is timezone. hit up your local zoneinfo and should give offset. ~ » export TZ='America/New_York' ~ » date +%z -0400 *edit* more details, if you're on windows or something: http://cs.ucla.edu/~eggert/tz/tz-link.htm
My mistake. I had no idea Markdown had a formal syntax. I thought it referred to *any* non-standard text format (actually, I'm not even sure what I thought it meant). &gt;If you're trying to convert reddit Markdown to HTML, you should probably use a Markdown to HTML translator. Isn't that overkill when I just want to have line breaks figured out? I'm not really interested in italicizing or bolding anything. I'll still check it out though.
Did you follow the link? It works for me.
I wasn't sure from your OP whether you were concerned with line breaks only, or whether that was a small part of the larger problem of parsing reddit posts/comments in general. If you only intend to parse line breaks, I don't see an issue with just using message = message.replace('\n\n', '&lt;br /&gt;').replace('\n', '')
hmm interesting. I was trying to date the data based on timezone information, yet and based to the wiki article, this has to be from like mid 2006. pulaski zip codes report America/Indiana/Knox (CST) when they should be America/Indiana/Winamac (EST)
Just to try it out. That makes sense, thanks. 
Which part? Compile-time type checks?
Or you could support your dev community and pay for the ebook.
It was a joke, about how the post on moderation went under moderation. 
No, the answer really isn't very enlightening. They tried putting all "subjective" questions on one dedicated site - of course that's not going to work. That'd be like giving AskReddit sex questions their own subreddit (which I think exists, actually...). The thing is, StackOverflow's guidelines for what's "subjective" or "not constructive* could be a *bit* looser without any significant loss in quality. 
In all seriousness, though, who reads the Python docs cover to cover? :\^) 
what's the question?
For comments, there should be `body` and `body_html` attributes: https://github.com/reddit/reddit/wiki/JSON#comment-implements-votable--created For submissions, there should be `selftext` and `selftext_html` attributes: https://github.com/reddit/reddit/wiki/JSON#link-implements-votable--created 
post this in /r/learnpython
It is standard behavior in markdown. A single line break is considered whitespace; a double line break separates paragraphs and other block-level constructs. The reason for this is that markdown is designed to travel unharmed over channels that have line length limits, such as e-mail; if you re-wrap markdown source to, say, 80 columns, the output should not change. The key thing to keep in mind in order to maintain sanity is that **Plain Text**, **HTML**, and **Markdown** are three separate encodings of your content, and you should *never* mix them. Ever. Whenever you need to go from one encoding to another, make the transition explicit; you need a real parser for all of them, string search-and-replace will not cut it. Trust me, I've been there, and [so have others before me](http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/). So, basically, what you want to do is: 1. Find a markdown implementation that understands reddit's markdown flavor. (Failing that, find one that's close enough, fork it, and patch it to your needs). 2. Get markdown source from reddit's API 3. Feed that source to the markdown parser; this gives you a fragment of HTML. 4. Inject that HTML into your HTML document, or parse it into a DOM (or a soup object or whatever you fancy) for further processing. 5. To turn markdown into plain text, either feed it to a markdown parser that can output plaintext instead of HTML, or convert it to a HTML DOM first, then use DOM methods to extract text from selected elements.
any reason why use Django for REST API? why not Tornado for instance.
Yep. I've kicked off more of a general discussion about "stealing" from artists here, which wasn't my intention :S. Maybe I'll get critique over there.
and also nuke this thread
Why? Some people are having a discussion about what stealing means below your comment. I don't wanna rob them of their opportunity for intellectual debate :P
Not the whole python's doc but in [Standard Libraries' doc](https://docs.python.org/3/library/intro.html) : This means that if you start reading this manual from the start, and skip to the next chapter when you get bored, you will get a reasonable overview of the available modules and application areas that are supported by the Python library. Of course, you don’t have to read it like a novel — you can also browse the table of contents (in front of the manual), or look for a specific function, module or term in the index (in the back). And finally, if you enjoy learning about random subjects, you choose a random page number (see module random) and read a section or two. Regardless of the order in which you read the sections of this manual, it helps to start with chapter Built-in Functions, as the remainder of the manual assumes familiarity with this material.
Surely exceptions can be made though? 9 out of 10 times I've been linked to a stack overflow questions is because it was "subjective" like this one. They bring great value and lots of traffic to the site, I don't understand the uncompromising zealotry towards strictly "objective" questions.
&gt; I'm aware that stealing isn't perfectly analogous between the digital and physical world, but spare me your bullshit. If you download music from someone who's not putting it out there for free, then it's somewhere on the gradient of stealing. I couln't have put in better words. Thanks.
You can practice coding with other stuff... This is a really bad excuse. If you gladly pay for music don't make a script to download it anyway even if the artist didn't explicitely allowed you to do so...
Stop considering soundcloud's side. Consider the artist's side !
I feel the same way. IEEEXtreme only allows numpy, etc. on Machine Learning and Natural Language Processing problems. Luckily, they bump the window for completion up to about 10 seconds for Python compared to 2 and 4 seconds for C/C++ and Java, respectively.
For comprehension seems interesting, will definitely play with it. It is useful to know that it is not only about setting content and setting attributes, there are lots of things that can be done. Check out the readme on enlivepy to see the whole list of transformations.
If its because of security concerns, well then you're a damned either way. Personally I see the "one function only" on many levels. So you could have some get_data_from_db, process_data, write_data_to_file functions which only do one thing on their level, but also a write_out_data_to_file which just puts those three together in maybe only two lines. That way I can think of it doing one thing on a business/use case level. Doing this it you can test all the specific behaviour, corner cases, etc.. for the single function and the business logic on the bigger one. Think of it as in libraries or building APIs on every level. The function that gets the data from the database actually calls some other library which then opens the connection, get a cursor, executes the statement and fetches the data. Just a year ago I would turn of all the pylint warnings about to many variables/statements/branches, but today I get it and for me those are a sign to cut down those functions or methods. 
I'm actually aware of it, but this style doesn't preclude more complex operations, it just makes the common ones simpler to write and nicer to read.
Thanks, but I'm well-versed enough with the standard library at the moment. I don't doubt there's a bunch more I could learn, but looking up on a case-by-case basis has worked well for me so far. 
This is a terrible misrepresentation of the facts. I can make a library to do the exact thing in Java. I like both languages, but this is just a terrible example. Wish I wasn't on mobile, or I'd be more in depth.
This is amusing because I literally landed on this comment after coming from /r/webdev about CSS selectors you dont know and the line was also that nobody reads the CSS specification.
Yeah me to, I soak every little bit that is release by Brandon. And I totally share your feelings about their code. I'm just in the end of a month work of big refactoring on one of my long going projects at work (like 3 years old now but with constantly changes made to it). I won't be as near as good as Brandon in explaining what I did. So bear with me. It started with using py.test and coverage to get a grip on the test coverage of the area that needed refactoring (It was frightening to see that the test that build up through the years actually missed some essential parts of the code). I just added 2 or 3 system test knowing that I would deal with corner cases on function level. From there I just looked at my code seeing things (sometimes only 2 lines long) that formed a unity (e.g. two lines that took a object and tried to find similar ones in the index) and made them into functions. **Always writing tests and running the test suit** (more see below) I was specially concerned about tearing functions out of their class method environment. So instead of making it a new class method or handing over the whole object, I just made the needed attributes arguments of the function. This not only helped me to test these functions without any state of the objects or without any database connections but also lets me see what the functions actually need. I must say sometimes those extractions looked rather ugly because they needed lots of arguments or dependency injections because everything was so tightly coupled. But I think sometimes you just have to tear it out to see where the links are that you need to cut. Also it makes you think of "why the heck does this function needs a database connection after all?? when everying it does it processing the name" and how you could shape it to a better. When I had extracted most of those little functions some of my methods just looked like skeletons only calling three functions and passing the data between them. Others had a lot of glue code in them. It made it clear that this was actually nothing the class should deal with. So I moved this glue upwards into the actually script that uses this class. Also some signatures of the functions looked strange to me when seeing them sitting there pulled out. So I often changed these afterwards (using a good IDE this only takes two seconds, run your tests afterwards and your done) **Testing** Everything said, I could have done nothing of this without proper testing. Actually the months before I was just obsessed with reading and trying everything about testing, tools and strategies. Whenever it was possible I tried to test in the TDD way. But if the code is already there (like most of the time in this case) I just wrote the tests knowing that I would write around the implementation. This is true to some extend but even then you find bugs doing this everywhere. **tl;dr** * write system tests * rip out those tiny little units into functions * tests * get them out of the class/method as much as possible * there will be glue code but maybe thats what actually belongs into you script * **tests**
Sure, for starters you should get used to reading -- it's an important skill for a programmer. A good place to start would be the header on the submission page that clearly indicates that your question belongs in /r/learnpython.
Are you sure you don't already have it? Starting with Python 3.4, pip is included by default with the Python binary installers.
I always thought that only the selector precedence is something nobody learns. What selectors are obscure?
&gt; Can you give more info about this new format ? Yes, it's a google spreadsheet. [Here is the link](https://docs.google.com/spreadsheets/d/1mRK7bbYguN-sSDDJt3pz0WQmyhQcuUZ1W3LkR6ykKhc/edit?usp=sharing) 
Indeed. It's almost just as bad using the python stdlib - https://gist.github.com/kennethreitz/973705 There's a lot to be said for using python over java, depending on the circumstances, but this post says nothing about the pros or cons of either _language_.
I still feel like some of the solutions are a bit 'hackish', but I blame that part on Ember.
Try out the zip-codes.com database... there's a free search on their site. Then you can buy the whole database for like 30 bucks, worth it, if it's accurate at least, I haven't used them in years. This?? https://en.wikipedia.org/wiki/Pulaski%2C_Tennessee That is indeed CST, what makes you think it's EST?
Are there any other books that O'Reilly shares like those two?
Yeah I agree. Sometimes I think "fuck Ember", other times it's "I love Ember"
Wow, this is great ! + works in virtualenv easily unlike some others.
Yes, cookies is the right way to do it. My company builds apps for clients, and this is the way we build all REST APIs, with flask login and session cookies, and flask restful. The http auth header is useful for oauth, but I still strongly prefer cookies for most things.
Celery is awesome and I'm indebted to its developers since I've used it extensively. However, it may also make sense to read about task queues in general since there are other potential options such as RQ and Taskmaster. Here's my curated list of task queue links including many for Celery: http://www.fullstackpython.com/task-queues.html My favorite link from the list is probably Caktus Groups' "Getting Started Scheduling Tasks with Celery": http://www.caktusgroup.com/blog/2014/06/23/scheduling-tasks-celery/
Oh, maybe it wasn't clear in my original comment. We dumped DJango entirely. I guess specifically we are working with OpenStack. The default (Horizon) was written with DJango. We also have a few other projects internally that use Tornado to expose a REST api.
Yes this book is totaly helping!
The examples are written in Java but you will get the big picture. 
That's just unreadable !
[Here is a good explanation plus examples.](http://interactivepython.org/courselib/static/pythonds/Recursion/recursionsimple.html)
thank you i really appreciate it.
thank you 
A much more real-world example is navigating through a file system. def do_folder(folder): for filename in os.listdir(folder): if os.path.isfile(filename): do_stuff_with_file(filename) elif os.path.isdir(filename): do_folder(filename) This isn't a perfect working example, obviously. It's minimized for clarity.
We have a followup blog post as well -- http://www.caktusgroup.com/blog/2014/09/29/celery-production/
Wonderful, thank you so much! That second link hit the nail on the head. I see what kinds of problems Celery is good for, but where I need help is in general system's architecture involving celery. These are good reads! 
Thank you! This is exactly the kind of thing I was after :)
/r/learnpython
Thank youuuuuu 
I am learning python on my own time but I don't have the time&amp;skills to do work on this by myself. I'll look into the things you mentioned, thanks!
PLEASE COPY THE DESCRIPTION OF WHAT THE FUCK YOUR PROJECT IS INTO THE SUBMISSION BECAUSE IT IS RUDE TO ASSUME EVERYONE CARES
i just wanted some feedback on how I can improve :)
/r/learnpython or ask a specific question.
Okay, sorry :-)
Maybe you could put the current script up on GitHub (together with a few items of sample data) so we or our brethren from /r/learnpython can take a look at it and suggest improvements or fixes. 
I don't own a Raspberry Pi or a camera to test this but would like to know how many frames per second one could stream over the Internet using this method? Is it possible to get, like, 30 FPS or more?
25k rows should be small enough to fit into memory, in which case [pandas](http://pandas.pydata.org/) is definitely a good way to go here. It can be an intimidating API, but even if you just learn a few of its features, cleaning up data will be much more convenient than the obvious alternatives. There's a [10 minutes to pandas](http://pandas.pydata.org/pandas-docs/version/0.15.0/10min.html) document to get you started. Pandas can load Excel files directly using xlrd. Using SQLite, or streaming CSV files, are handy techniques if you have data that can't fit in memory, but they are generally more cumbersome, so if your data does fit in memory, I wouldn't bother with them.
How much does lack of composite types and multiple dispatch hamstring things?
[Jython](http://www.jython.org/)
If the data is in Excel and you think you have duplicates, just find them and delete them. 25,000 data points isn't that much to look through. Is there a unique identifier for each piece of data like a bid number or contract number? If so, just highlight that column and click on data &gt; Delete duplicate data. If not, then add a column and create a unique value by concatenating all the prior 10 columns: =CONCATENATE(column1, column2,...). Then copy, paste values, data &gt; delete duplicate data. If that doesn't work then sort the data in the same order as the original data source and go through it page by page brute force. 
Why doing DOM modification on server side when this could be done on the client side? Also, how fast is it? How do you handle caching? But thanks for your contribution. 
To add one to that comment, something like this may work: pop = 5 # population growthrate = 2 # Rate of growth number = int(input("Enter hours elapsed: ") # Hours ph = 1 # Base mumber to count hours while ph &lt; number: ph += 1 # Count for hours rateperhour = growthrate * number # Math for rate of growth for the hour newpop = rateperhour * pop # Find new population pop = newpop # Set population to the new population print (newpop, " People for hour ", number")#Population for that hour print("") #Blank space Haven't tested it, typed it up on my phone, but you might want something similar to that. 
Here you go: http://www.reddit.com/r/raspberry_pi/comments/2k4pm4/raspberry_pi_computer_in_the_car_w_screen/ Ended up posting in raspberry_pi, I figure it was more fitting in that subreddit.
awesome! ill let you know when I compile my code this afternoon :)
Sounds good
I also made this IRC bot for tracking time spent on jobs earlier today https://github.com/blha303/JobClockBot My free time has been very productive today :D
This is being worked on by [guykiesel](https://github.com/guykisel): https://github.com/landscapeio/prospector/issues/56
Although I like the idea of a flask-powered forum, did you HAD to try to emulate the PHPBB design? It's not only ugly, it evocates a whole bunch of bad memories regarding user experience. On the other hand, I really liked that you're going to a plugin-based architecture. All in all, I guess I will definitely keep an eye on it.
That's me :-) 
Well then now you know what you're working on ;-)
Is 'poop' really necessary in the multipart ? 
You can use [pyjnius](http://pyjnius.readthedocs.org/en/latest/quickstart.html#a-minimal-example) to work with java classes from python. It works fine from normal python, no special interpreter or anything. For example, Kivy uses it to call the android java api from python apps. It does have some limitations and annoyances. Whether these are significant to you depends on your project, they may well not be. There are also other similar projects, including at least [jpype](http://jpype.readthedocs.org/en/latest/), but I don't know much about these.
I wrote an analog of this in tornado a couple months back ([source](https://github.com/patrickfuller/camp)) as a websocket replacement to the 3-5 fps motion library, and I really wish I had your article in hand when I wrote it! Have you had any luck with more complex streaming protocols? I tried my hand at transmitting buffered video, but never got it to work.
Nice work, the code looks very clean!
I haven't done much with the more efficient streaming protocols, but I know you can get good frame rate and good quality. The Pi can do a pretty good job at encoding these streams in real time on the hardware encoder, but I don't know of any integrated solution that creates the live HLS stream from that, this will probably end up being a mashup of an encoder plus ffmpeg to do the packaging. Then the Pi just needs to serve those files over HTTP and an HLS aware client can do the rest.
Thats Flux's design, not phpBB one. One used by phpBB is far more aged.
This worked, thanks!
For some reason I thought this was going to be an article about interior designers ... and there was going to be a Q &amp; A session from a user named "baroque". My favorite interview was where the guy told me the main reason they were using Node.js was because Python lacked support for closures. I proceeded to spend most of the interview explaining how JS lacked well though out variable scoping. Along with the fact Python didn't just support closures, but had some fantastic syntactic sugar for functional programming in the form of decorators.
it looks slightly fucked on mobile (safari), but good work.
Fuck tdd
Ever think about using tornado?
You have no idea what you are taking about.
Hell yeah! Modern forum software is what we need. No way I'm going to install any of those v-PHP-BB-Powerboards. Not because of PHP – they're just horrible to deal with.
Unequivocally yes, I've done this to several of our spreadsheets that sound really similar. If there are just a few settings and you want to keep Excel as a front-end, you can call Python scripts from Excel via VBA with Shell() and give cell values as command-line arguments. I haven't used QT, but that would work as well (as long as you can get a Python interpreter on everyone's machine, or the .dlls to run a .exe) For outputting to a new spreadsheet you have a couple of options. One is to use OpenPyXL to write xlsx files. Formatting is a pain in that, so what I typically do is save a Formatting Template spreadsheet, write the new sheet with no formatting, then paste formatting from the template and save. Another way you can do it is write a two-column csv with keys and values, then use VLOOKUP or INDEX-MATCH to bring in the values based on the keys to a formatted spreadsheet (paste the csv to a separate tab, or import it via VBA). Print to PDF or paste values and delete the raw data tab and you're good to go for the client. I've also heard good things about xlwings, but haven't used it. 
none of the links seem to work for me on the first post =/
Miguel, I love your blog. You are amazing. Keep up the awesome work.
Great job, just remember to put a # infront of line 1 and line 7 because they are counted as part of the code and give errors. Worked well otherwise!
I wrote [this](http://sprunge.us/BSVH?python) a couple of years back at my job to help me learn how to understand decorators, as well. Feel free to do what you want with it.
I was wondering the exact same thing. I remember upgrading to Mavericks and had the hardest time getting command line tools to work properly and this caused endless frustration for install new python modules.
I had the exact same thought as your first paragraph before I saw what sub it was in. 
still gives a old-stuff-phpBB-ish vibe. But there're themes, so, I guess it's fine.
The best way to improve is to keep writing code. Pick one of the many open source Python projects in an area you are interested in and start contributing! There nothing like having other more experienced developers looking over your code and critiquing it when it comes to learning and improving. :) Another good idea is to revisit old code you have written and see if there are areas that can be improved. Are your functions broken up into small, easily testable pieces that do one thing well? Are there better ways to organize code? Can you rewrite a for loop that appends to a list as a list comprehension? The list goes on. 
It's what the puush windows client sends. :P It's ignored by the server.
Kind of feels like a return to XSLT. When applied to real-world projects, it becomes more complicated and cumbersome than dedicated template languages.
oh yeah I commented those out ahah thanks for the advice! , super helpful :D
After spending time on reddit, any other type of forum always seems so foreign to me. Looks slick.
&gt;**Problems using virtualenv under Linux** &gt; &gt;When on Linux, Toga uses the system native python GTK+3 bindings for display purposes. However, if you’re using a --no-site-packages virtualenv, the Python bindings for GTK won’t be in your PYTHONPATH. &gt; &gt;Unfortunately, you can’t pip install GTK+ bindings, so you have to use a workaround. To make the system GTK+ bindings available to your virtualenv, symlinking the gi module from the system dist-packages directory into your virtualenv’s site-packages: &gt; $ cd $VIRTUAL_ENV/lib/python2.7/site-packages &gt; $ ln -si /usr/lib/python2.7/dist-packages/gi From: toga's solution to using python GTK+3 [using virtualenv under linux](http://toga.readthedocs.org/en/latest/#problems-using-virtualenv-under-linux)
1. Get a job as python developer. 2. Learn the tools - be good at git, get everything out of your ide etc. 3. Learn other languages - javascript for example. 4. Learn SQL. 5. Learn NOSQL. 6. Do some async things - celery tasks, websocket chat 7. Start writing big project and autogenerate milions of rows to optimize queries for example, add caching etc.
That's a good thought. Old code is a good place to look for ideas. One of my goals is to start contributing to more open source projects. Thanks for the advice.
nice one mate! 
There was divmod newov, there was zope tal. IMO what django or jinja do is the best of both worlds. You can have more kosher approach at templates, sure... What are the benefits, what's the cost? 
Bbcode is horrible. Why not markdown? 
&gt; Get a job as python developer. Considering the crappy code, written in Python, I see at my company, I would rather advise to explore good OSS projects instead. 
&gt; What are some things that advanced Python programmers should learn or know. Libraries, concepts, etc? Depends what you want to do? I use PyQt, wx, matplotlib, numpy, scipy, VTK, pandas (rarely) and then lots of in-house code. I stil haven't gotten to the end of numpy or scipy and I've been working with them for years. &gt; I can write code pretty efficiently Until you vectorize your code or use Cython or SWIG, you haven't written efficient code. Pure Python is not efficient. If you dump your mega calculations off onto a C-based program, then fine, but Python isn't about efficiency. EDIT: people obviously missed my point. Get good at the standard tools. Learn how to take slow code and make it fast, which often just requires using these libraries properly.
In Reagent this is pretty great, but then you get the advantage of being able to do this client AND server side plus you get react. This project doesn't seem to have any of those advantages :(
Give a look a this post: http://www.toptal.com/python/interview-questions
" that internal function also needs to return something." You don't need to return anything. A python 'function' can be procedural only. Also this is python: it forces you to return None implicitly.
No problem. Email me at foxhunter12@gmail.com if you need extra help on other things. I'd be delighted to help!
I do know that. I was only replying to the fact that not all jobs will help improving your Python skills. Apparently people didn't grasp my meaning.
&gt; NodeBB Looks good, but no threaded comments option?
Not that I know of, but could be implemented in a plugin. Though that's what I like about it, not really a fan of threaded comments.
Ah that makes more sense. Easy misunderstanding. 
&gt; Considering the crappy code, written in Python, I see at my company, I would rather advise to explore good OSS projects instead. You should implement some new tools and practices. For example code review, check pep8's and pylints automatically in jenkins, refactor bad code or even tell and explain others what's wrong with their code etc. 
No problem. I think my initial comment was probably feeling more sarcastic than I intended.
Trust me, I've tried. We have all of it. But as, @nocommentingallowed said, it's really a lack of culture for quality and elegant coding practices unfortunately.
Have you toyed with meta-programming (Metaclass, decorator, template, ...) ? Nothing that you "should know" but can be fun.
It'd be nice if there was an API. That'd make it easier to write third-party apps for forums or custom front-ends.
Forum softwares always excite me! I was really sad that to know there are no good/modern forum softwares in Python (see [my old thread](http://www.reddit.com/r/Python/comments/289hb1/why_there_are_no_good_forumbulletin_board/)). It makes me happy that people are coming up with forum software on python. Sadly, I had to migrate from mybb to nodebb. Though my users love it, I am still a beginner at JS and my heart always goes back to Python. I really hope your project grows one day and becomes production ready (to handle large userbase. I own a forum of ~100K registered users, but only 500 or so are active at time. However MyBB/NodeBB handles it really well) I wish you all the best! (also another forum software in python, which comes to my mind: [Misago](http://misago-project.org)), which is also not really production ready.
1. Get a job as python developer. - ✓ 1. Learn the tools - ✓ 1. Learn other languages - ✓ 1. Learn SQL. - ✓ 1. Learn NOSQL. - ✓ 1. Do some async things - **X** 1. Start writing big project and autogenerate milions of rows to optimize queries for example, add caching etc - **X** Not bad for myself, yay!
Hey! I'm the author of flaskbb. I was really suprised from where I got all those stars so I googled 'flaskbb' haha :) I'm using the new design from fluxbb because my designing skills are really really bad. I appreciate every PR/Issue/suggestions ;)
I am not sure if threaded comments fit their mongodb-schema.
I started working on some design patterns in Python a little bit ago, but I know there's a lot of pure CS I should still look into. I think that could be a good way to gauge what I know and expose the things I don't. Thanks!
I'm good with decorators, but I haven't done much with Metaclasses. I have run into them a few times, but never got past the shallow end on it. I'll give them a look.
That's exactly what I was looking for! Lots of good ideas there to keep me going. Thank you!
Why in 2014 would you not make a forum threaded? 
Suggestions for code quality: * In tests, assert (whatever) is good enough. Don't need to do assert (whatever) is true. * Use relative imports in your package, everything is using absolute imports as it stands. * Single quote your strings.
I like markdown, but I think you're wrong about using it to replace BBcode. Markdown was made for a specific purpose (simple, fast markup that is still readable as plain text), and thus lacks several features that are expected in forum software. Support for embedding and formatring images and other media is a good example. Tables are another (though people have tried to add tables to MD). BBcode is a pretty decent half-way point between MD and HTML, and enjoys a pretty good install base. I think it is the correct choice.
&gt; better to simply define classes in python than use namedtuples. They're easier and do basically everything composite types do (plus you can subtype them, which can the Interesting thanks.
This may not be a Python solution, but... Take a look into Google Refine / Open Refine. This might help you get to your goal (after first perhaps using the solution you have right now). HTH
&gt; Meteor, however, restricts you to the kind of stuff that would make a basic to-do app. That is an unsubstantiated claim. 
&gt; Single quote your strings why? why not double quotes?
Why not support both? At least for basic stuff like bold/italic/links it's much more decent than BBCode
I don't see how composite types (just a fancy word for objects, which Python does support) and multiple dispatch (fancy word for overloading, which Python does not) necessarily helps solving a problem. if you're working in numerical problems then I would recommend getting to know NumPy and SciPy and learning the pythonic idioms that those libraries utilize. 
I've used it on a quite big project and recommend it. Nice framework with good dev.
/r/learnpython is more appropriate for asking for help.
Put that list on a paper, stick the paper on a wall, start researching-learning and tick everything you got from there. I'm doing this way and I must say, it helps. Even though it seems like a very subjective way, there's nothing you can lose or go wrong with this :)
Funny, didn't /u/mitsuhiko start on Flask because he wanted to write a forum?
Just create a separate Sandbox project (and package, if you want) to put your script in, and open it along side your MyProject project [using this dialog box](http://imgur.com/9O0bB2I). The package and modules in the Sandbox project can be imported using absolute import statements: from sandbox.myhack import MyClass 
i think i disagree in this case. hes not asking for help w/ python. Hes asking for help on an IDE w/ something beyond the normal use case.
The problem with this approach is that the encoding will not go through the Raspberry Pi GPU encoder, as far as I know. Encoding H.264 in software is very CPU intensive, not something the Pi can do with decent resolution and frame rate. If I had to do this I would use ffmpeg just for the packaging of the HLS stream.
Idea behind Misago is to be able to either run it solo or use it as framework further extending Django with extra features for community sites. So if forums will be important part of your site, you can just install Misago and write few custom apps for it adding features you need (eg. to pull threads from "news" forum to your site's index) or customize premade django apps for use on your site.
Can those libraries provide the capability mentioned here?: http://www.reddit.com/r/Julia/comments/2k79mn/is_there_something_about_julia_that_facilitates/ (easy abstractions mapped to continuous arrays)
So /r/pycharm in that case, if it exists. Not /r/python, either way.
Looks nice but... no Python 3 support? DjangoBB has you beaten there. I like your description though since it fits my needs but I don't use Python 2 in production anymore. :(
Asking a question about something is relevant to the Python community and tools is completely fine. Hit the 'hide' button and move on.
I'm still pretty new to python and I haven't even started with Django. I think GUI might be the best route :/
How to support both? I see there a few problems like, if you quote a post which is written in for example in markdown but you are using bbcode? I chose bbcode in the first place because it is the more common markup language. I personally prefer markdown.
Learn Regular Expressions, they are an indispensable tool! Once learned, you will wonder how you could live without them. 
It would be neat if you converted the cam.py module into a COM interface and instantiated it directly within VBA. That way you could just return the np array as a 2D array of Integers directly to VBA and remove all of the Worksheet calls from Python. This is really easy to do with win32com. Cool demo though!
Wouldn't it make more sense to implement some sort of pre-commit pep8/pylint check? Is that easily possible with git?
I'm not an expert in NumPy and it would be better to ask that community directly. I do know that NumPy very heavily leverages vectorized array operations implemented in C to gain its performance over normal Python, so I'm assuming that contiguous memory arrays are a thing. 
I'm guessing he prefers that symbol-like strings use single quote signs, while messages use double quote signs (and docstrings use triple double quote signs).
No one mentioned double context managers in 2.7 and 3+ with open("a.txt") as a, open("b.txt", "w") as b: for line in a: b.writeline(line)
1. Write a lot of code. 2. No seriously, write a lot of fucking code. Most of it will suck. That's okay, you don't need to release it. My private code repo is about 50x the size of my public releases. 3. Read a lot of code. Read the standard library source, read third party source, read it all, and do your best to understand what is going on. Assuming your interests are reasonably varied, reading and writing a lot of code that suits your interests will eventually get you to where "advanced" programmers are. It helps if you are interested in the sharp edges of Python: Metaclasses, the type system in general, generators, finalizers, context managers, async (asyncore, AsyncIO, or Twisted), the GIL, thread scheduling differences in Python 2.x vs. 3.x, ...
This library initially was a part of email marketing platform. Its idea was that a manager writes a layout using Jinja2 and regular clients use it to render and send messages, using both API and web interface. The code was used to show clients a form that can be used to provide data for a mailing. It also was used for validating user input. Considering that API was used mostly by cron tasks to perform recurring mailings, and a manager could change a layout at any given moment and break compatibility, it was really important to return an error if incoming data doesn’t fit into a layout. Another example — a task scheduler, where tasks are defined by YAML files that rendered by Jinja2 templates (an approach that SaltStack uses). Template context can be thought as task parameters. jinja2schema is used to provide users with a form where they can enter parameters and run a task. I also imagine tools like Cookiecutter can make use of jinja2schema. Currently it requires you to describe all the parameters in cookiecutter.json — jinja2schema would make it possible to infer required parameters directly from the Cookiecutter templates.
&gt; &gt; Multiple dispatch allows us to define functions as collections of methods, where each method is defined for a specific combination of types. &gt; &gt; So a class with some inheritance and then checking if the object is a subclass of the parent? No. Multiple dispatch allows `foo.bar(quux)` to resolve to a different function from `foo.bar(yabba)` (where `quux` and `yabba` denote the types of the arguments). The python solution for this is not particularly elegant. You can use a double visitor dispatch type solution, or explicit type testing inside a method. 
Py3k support isn't going to land in next release, but I am aware there is demand for it and its pretty high on the list.
I just used the same token until it expired, or for the duration of a "session". The token method I described is basically the same as what you implemented, except it doesn't require the client to use cookies, which might be easier to deal with if you aren't using a browser as a client (not your use case though).
Really cool!
You can't use either. You don't have users tell software "uhhh... this post's bbcode". You just make your message parser parse both MD and BBCodes, but from my experience its simpler to "bolt on" BBCode support on MD parser than vice versa.
&gt; No. Multiple dispatch allows foo.bar(quux) to resolve to a different function from foo.bar(yabba) (where quux and yabba denote the types of the arguments). Now it sounds like function overloading. The way you handle that in python is: def f(x): if isinstance(x, list): # do list based stuff elif isinstance(x, tuple): # do tuple stuff else: # crash What Python suffers from is not having static types. However, there are multiple ways around that. You can write C/Fortran extensions and you can use numpy who already wrote C/Fortran extensions for manipulating lots of constant type data (e.g. a list of floats as opposed to a list of floats and ints). &gt; The python solution for this is not particularly elegant. Fine, but its hardly preventing you from doing anything. It's syntactical sugar.
Came here to suggest this. Python code becomes so much nicer after learning FP. At least from my experience.
You're probably at a point where you should worry less about Python and more about different technologies or paradigms. Looking into other completely different languages like some FP language (assuming you haven't yet) will certainly help your code in any other language you already know. The point here being, experiment with a language that is as different as possible from the ones you already know. Other things in case you haven't tried yet: mobile, cloud, distributed architectures, micro-services, map/reduce, etc. Just ideas. 
"Straight" PHP does a lot of what the Python frameworks do. It parses HTTP requests and facilitates returning HTTP responses. It has built-in helpers for sessions, cookies, database interaction, html-related operations, etc. So, you're comparing a language designed for the web (PHP) with a general purpose language that was not designed with web apps in mind (Python). Thus, Python needs a little more library/framework glue on top to make it comparable. It's worth noting that most serious PHP web developers now also use frameworks, because the benefits are pretty hard to deny. The days of slopping HTML and DB queries into a single .php file are over, at least if you're competent and experienced. Similarly, you CAN still write Python CGI code that is crude and uses no framework, but it's going to suck.
First: PHP is not a general purpose language. By using PHP, you start with PHP's web framework. The purpose of a framework is to help deal with a large amount of shared code that happens between a TCP socket and a request handler. But it is absolutely not necessary. You can write everything from scratch starting from the `socket` module. Write your own HTTP parser, the works. Good times. You could also start from `http.server`, twisted's server, or from an asyncio module. They parse HTTP for you and provide classes to deal with http servers. Alternatively, you can use old-fashioned CGI (spawned from a webserver and use environment variables and stdin to get the request) or WSGI (standard API for connecting Python apps to web servers). I [wrote my own](https://github.com/astronouth7303/restracker/blob/master/restrack/server.py) WSGI-based framework for a class once. And then, of course, on top of all this, there's the frameworks. Flask, Django, etc. Unless there's strong reasons not to, people use these.
WSGI does *some* of the things that PHP does by default. Most notably, it provides a single entry point for your application. This is probably the defining feature of non-PHP web applications. It basically forces you to take a more structured approach to web development and strongly encourages you to use a proper library. This is A Good Thing if you're writing an application and A Bad Thing if you just need a simple script. 
You won't believe how complicated it is to retrieve the posts of a topic in NodeBB. This here gets called for _each_ post in a topic which is loaded on a page: https://github.com/NodeBB/NodeBB/blob/master/src/topics/posts.js#L46 This whole project is a complete clusterfuck of hipster software architecture. A forum is probably the best use case of a relational database, but they use Mongo DB which doesn't know shit about relations. Everything is a single document which is referenced by ID and pulled by an individual database query. I can't even properly describe how fucked up this is. Read the code, but only if you got some schnapps to help you endure the sheer hipster overload of it.