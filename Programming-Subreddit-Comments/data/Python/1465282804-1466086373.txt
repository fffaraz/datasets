Check out [Yasuf](https://github.com/sYnfo/Yasuf) for another way of making Slack bots.
Thanks. Do you have any GitHub link to share? :)
I found using environment variables convenient enough. Also these variables are independent from each other to some extent, so to store them in one file would be inconvenient if I'd like to make 4 distributions (32bit, 64bit)x(py27, py35) or change Python version later. Actually I found that most convenient is to write code that is compatible with both Python 2.7 and Python 3.5 because tools that I use (PySide, Enaml) are stable only on Python 2.7 but they would be ported to Pyrhon 3.5 some day. *.bat files and *cmd files are pretty much the same, but there are some differences I found irrelevant in my case. "did you have a compelling reason to use batch files as opposed to implementing those scripts all in python" - the funny part. I simply started using batch scripts from the start. They were the very simple ones. Then I started to add functionality gradually piece by piece. Sometimes I have noticed that batch scripts may not be the better choise for the task they do, but eventually I was able to do everithing I wanted with batch scripts (calling Power Shell is a quite powerful tool).
Cool, thanks! I'm still on 3.4 in part because of PySide not being on 3.5. Don't let PySide keep you on 2.7, but I understand if you're waiting on Enaml. 
There are actually Enaml port to 3.4 and 3.5 but it works well only on PyQt. The old Pyside-Enaml bug can be easily fixed in 2.7 but the fix does not work in this new 3.4-3.5 port. This port is actually 2.7-compatible, and the fix does not work on Python 2.7 either. So they broke Enaml :) I'm going to wait for PySide for 3.5 and then bugreport.
Thanks.. Would be even nicer if you try it :)
Thanks for the input.
I'm more interested in solving the problem correctly. If it happens to be adopted into the mainline one day, great. The PyParallel code as written diverged too much to be suitable for merging straight in, and there are many warts that would need to be addressed first. If it got to the point where gilectomy was up for mainline submission I'd probably be more vocal about the issues with the architectural approach being taken.
Huh - I wonder why? You shouldn't have to declare within a function - the `if` example there works OK, for example.
Yup. I'm in the same boat.
Because you want the if to fire once per request not once per process.
No, I'm just asking if it is possible and if you have a resource that I can reference to try to write the script myself. =)
This is the wrong reddit to post questions, http://www.reddit.com/r/learnpython is meant for exactly this sort of thing. On your question, it doesn't matter, you just need to know the full exact path for `open()` or place it in the same directory as where you run the python interpreter. Last note, your code isn't being formatted correctly, add 4 extra spaces before each line and one line between it and other text
Yes.
yeah it formatted weird when I put it in the comments. I figured it out. I had to do the exact file path...but then threw an error for unicode - so i had to do double backspaces and then it worked fine. Thanks.
Probably you'll find the answer here: https://automatetheboringstuff.com
[I think this sums up my sentiment quite nicely.](https://groups.google.com/forum/#!topic/framework-benchmarks/vtTvwpTeznI) &gt; The “Why Windows?” (or “Why not Linux?”) question is one I get asked the most, but it’s also the one I find hardest to answer succinctly without eventually delving into really low-level kernel implementation details. &gt; &gt; You *could* port PyParallel to Linux or OS X -- there are two parts to the work I’ve done: a) the changes to the CPython interpreter to facilitate simultaneous multithreading (platform agnostic), and b) the pairing of those changes with Windows kernel primitives that provide completion-oriented thread-agnostic high performance I/O. That part is obviously very tied to Windows currently. &gt; &gt; So if you were to port it to POSIX, you’d need to implement all the scaffolding Windows gives you at the kernel level in user space. (OS X Grand Central Dispatch was definitely a step in the right direction.) So you’d have to manage your threadpools yourself, and each thread would have to have its own epoll/kqueue event loop. The problem with adding a file descriptor to a per-thread event loop’s epoll/kqueue set is that it’s just not optimal if you want to continually ensure you’re saturating your hardware (either CPU cores or I/O). You need to be able to disassociate the work from the worker. The work is the invocation of the data_received() callback, the worker is whatever thread is available at the time the data is received. As soon as you’ve bound a file descriptor to a per-thread set, you prevent thread migration &gt; &gt; Then there’s the whole blocking file I/O issue on UNIX. As soon as you issue a blocking file I/O call on one of those threads, you have one thread less doing useful work, which means you’re increasing the time before any other file descriptors associated with that thread’s multiplex set can be served, which adversely affects latency. And if you’re using the threads == ncpu pattern, you’re going to have idle CPU cycles because, say, only 6 out of your 8 threads are in a runnable state. So, what’s the answer? Create 16 threads? 32? The problem with that is you’re going to end up over-scheduling threads to available cores, which results in context switching, which is less optimal than having one (and only one) runnable thread per core. I spend some time discussing that in detail here: https://speakerdeck.com/trent/parallelism-and-concurrency-with-python?slide=24. (The best example of how that manifests as an issue in real life is `make –jN world` -- where N is some magic number derived from experimentation, usually around ncpu*2. Too low, you’ll have idle CPUs at some point, too high and the CPU is spending time doing work that isn’t directly useful. There’s no way to say `make –j&lt;just-do-whatever-you-need-to-do-to-either-saturate-my-I/O-channels-or-CPU-cores-or-both&gt;`.) &gt; &gt; Alternatively, you’d have to rely on AIO on POSIX for all of your file I/O. I mean, that’s basically how Oracle does it on UNIX – shared memory, lots of forked processes, and “AIO” direct-write threads (bypassing the filesystem cache – the complexities of which have thwarted previous attempts on Linux to implement non-blocking file I/O). But we’re talking about a highly concurrent network server here… so you’d have to implement userspace glue to synchronize the dispatching of asynchronous file I/O and the per-thread non-blocking socket epoll/kqueue event loops… just… ugh. Sure, it’s all possible, but imagine the complexity and portability issues, and how much testing infrastructure you’d need to have. It makes sense for Oracle, but it’s not feasible for a single open source project. The biggest issue in my mind is that the whole thing just feels like forcing a square peg through a round hole… the UNIX readiness file descriptor I/O model just isn’t well suited to this sort of problem if you want to optimally exploit your underlying hardware. &gt; &gt; Now, with Windows, it’s a completely different situation. The whole kernel is architected around the notion of I/O completion and waitable events, not “file descriptor readiness”. This seems subtle but it pervades every single aspect of the system. The cache manager is tightly linked to the memory management and I/O manager – once you factor in asynchronous I/O this becomes incredibly important because of the way you need to handle memory locking for the duration of the I/O request and the conditions for synchronously serving data from the cache manager versus reading it from disk. The waitable events aspect is important too – there’s not really an analog on UNIX. Then there’s the notion of APCs instead of signals which again, are fundamentally different paradigms. The digger you deep the more you appreciate the complexity of what Windows is doing under the hood. &gt; &gt; What was fantastic about Vista+ is that they tied all of these excellent primitives together via the new threadpool APIs, such that you don’t need to worry about creating your own threads at any point. You just submit things to the threadpool – waitable events, I/O or timers – and provide a C callback that you want to be called when the thing has completed, and Windows takes care of everything else. I don’t need to continually check epoll/kqueue sets for file descriptor readiness, I don’t need to have signal handlers to intercept AIO or timers, I don’t need to offload I/O to specific I/O threads… it’s all taken care of, and done in such a way that will efficiently use your underlying hardware (cores and I/O bandwidth), thanks to the thread-agnosticism of Windows I/O model (which separates the work from the worker). &gt; &gt; Is there something simple that could be added to Linux to get a quick win? Or would it require architecting the entire kernel? Is there an element of convergent evolution, where the right solution to this problem *is* the NT/VMS architecture, or is there some other way of solving it? I’m too far down the Windows path now to answer that without bias. The next 10 years are going to be interesting, though. &gt; &gt; Regards, &gt; &gt; Trent. 
Yea I'm pretty sure the noise filter in this case is just the ferrite bead or what ever it's called. The heat sinks are pretty common, but generally considered unnecessary. I think it's more for looks but many of the kits include them since they're less than a dollar. Canakits are actually pretty decent you get everything you need except a keyboard(depending which kit you buy) without having to find everything for a decent price. The next kit up from the one posted includes an SD card with NOOBS installed as well. Although with the 3 now having integrated wifi and bluetooth the kits are a little bit more pointless. It looks like the kits changed now but I got my RPI 2, a case, 8gb sd card, hdmi cable, power supply and usb wifi adapter I knew would work with the pi(not all do apparently). For $65 which at the time was within a dollar or two of what it would be to order all the parts separate. Also incredibly weird that you have a computer with no SD-card reader. Plus any OS should be able to handle it as well.
visual studio has spellcheck. pretty sure sublime does too. seems like a risky proposition, if you're the lazy type who does funny abbreviations in your variables, or you name them run-on phrases or anything like that, you could easily break your code all to hell and back. 
You'll get a better response in /r/learnpython.
DynamoDB allows you to specify the field and field type to use for the partition key. Your application would be the one setting it for each record.
Howdy. I was the presenter for this talk. Rather than reply to every thread under this comment, let me summarize my thoughts here. (Disclaimer: just speaking for myself, not on behalf of any organizations I might be claimed to represent.) -- First, it's kind of a dick move to ask "Why do you care about using Python?" at a Python conference. Also, suggesting that because CPython isn't currently good at cpu-bound multithreading, we should just give up and use other languages, with the implicit suggestion that I'm wasting my time, is contemptious. Therefore, also kind of a dick move. Finally, the session chair had already announced we only had time for one question. Spending that for a jokey unhelpful "sick burn" is kind of a dick move. Were there other questions lurking in the room? I don't know. But the room was packed, and I think I spent more than a half hour answering questions to a crowd of about ten people after the session was over. My guess is, if he hadn't jumped up to the microphone, we would have gotten a less dismissive question. -- As observed many times, there's nothing inherent in the design of Python-the-language that prevents it from running multiple threads simultaneously. In fact there are two implementations of the language (Jython, IronPython) that have no such limitation. So fundamentally I disagree with the question's premise. Are there technical challenges involved? Well, yes, obviously. And it's going to take some time, given that nobody is being paid to work on this (that I know of). But it's a little early to declare defeat. -- Finally, giving a talk is a stressful performance. I was a little dazed at the time, and I didn't come prepared with snappy answers. Also I didn't actually hear the question that well; I heard the "Go or Rust" part and guessed at the rest. For better or for worse I said the first thing that popped into my mind.
ty
5- 10 years is a long time to wait. Is your current "skunkworks" project slated to work on linux?
Great post. Relatable use case and a straightforward implementation that should be able to serve as a foray into machine learning for the lightly experienced. 
Howdy. I was the presenter of the talk. No disrespect to trentnelson, and I applaud any attempts to make Python useful for solving problems in new problem domains, but: I'm not particularly interested in PyParallel for a number of reasons. First, it's irrevocably Windows-only. Second, it doesn't seem like it will ever go beyond "alpha-quality"; my aspirations are higher than that. Third, the approach taken doesn't work, and *will never work,* for general-purpose Python code. You must write special custom code to take advantage of PyParallel's parallel processing abilities, with difficult-to-follow new rules (don't share objects outside your thread!) which break your program if they are not strictly obeyed. Fourth, while doing parallel processing, you leak memory like crazy, by design. PyParallel tracks memory use per-thread, and when the thread exits it reclaims all the memory. So each parallel "task" can only live for a relatively short time. This makes PyParallel suitable for only a narrow range of tasks. I'm trying to solve a much harder problem.
Hi Per, Very interesting article. When you talk about accuracy in the final section, how are you measuring this? Presumably you're basing it on the database you built at the start of 1000 customers v. 1000 non-customers. But didn't you train the model on this? So, the accuracy is measured on the training set? 
Hey, thanks! We do a train/test split of the dataset, at 70/30. This means we're training on around 1400 training examples, while testing on the remaining 600. 
Even for a 1920px wide window, you only need 1920 samples, not 15000. Just graph every 100th sample, or reduce your reading rate to something reasonable if you're not using the data for anything else.
Also the refreshing rate of your monitor is unlikely more than 100Hz, so no reason to plot at a higher frequency 
I would try [pyqtgraph](http://www.pyqtgraph.org/) for this. 
I don't, I'm just starting with Python since I was told that it is good for beginners 
Well, then, any of the links on the subreddit sidebar. I haven't read any of them (since I've been doing python for years), but I've been told that Think Python is really good for beginners. Just check them all out.
How did you learn Python?
Huh. Interesting. I use kat.cr
I saw a Slack bot on this subreddit a week or two ago using asyncio - https://medium.com/@greut/a-slack-bot-with-pythons-3-5-asyncio-ad766d8b5d8f#.ub9vjlqgs
Howdy. I'm the author of the gilectomy. Thanks for the link to Greg's revised patch. I hadn't seen that; all I'd seen was [his original patch, as revived by David Beazley back in 2011](http://dabeaz.blogspot.com/2011/08/inside-look-at-gil-removal-patch-of.html). That version made incr/decr safe by performing those operations under a mutex (!), which was never going to be a winner. I'd heard there was a later revision that used InterlockedIncrement under Windows but I hadn't seen it. You seem to have located it and checked it in, which is nice. Where'd you find it? &gt; It was slow then, it's slow now, and it'll be slow in the future. I don't think it's a viable way to solve the problem. Greg's approach, and my approach so far, use naive approaches. There are many other more sophisticated approaches yet to be tried. My intuition is, the main speed regression is in reference count incr/decr. To address that I'm trying "buffered reference counting", a twenty-year old technique described in "The Garbage Collection Handbook". Another core dev wants to try thread-local reference counts. If those don't work, perhaps we'll find other approaches to try. We'll keep measuring, and experimenting, and iterating. Nevertheless, you've already stated that I'm doomed to fail. I wish I had your crystal ball. &gt; In fact, I don't even think it frames the problem correctly. How does simply removing the GIL make my web server serve more clients in parallel, or reduce the runtime of my data processing working? It doesn't. Clearly you understand what the GIL is, and how it affects the execution of code. So there's no need to explain it to you. However, others reading this exchange may not be as well-versed in the issue. My response below is really intended more for them. You use the phrase "the problem" in reference to the GIL; you then dismiss my work as not addressing it. But the GIL creates many problems, and you didn't say which one you had in mind. So I can't sensibly address your concerns. To my way of thinking, "the problem" with the GIL is that only one thread can use the CPython runtime at a time. This precludes using using more than one core. It precludes CPU-bound threads from running in parallel. Removing the GIL fixes that problem. And I've already removed the GIL in my branch--I did it back in April. Of course, I've now replaced that problem with another problem: a major performance regression. This is where we are with the gilectomy right now. I remain optimistic that we can address the regression sufficiently to make the gilectomy "interesting"; I define "interesting" as "when you add cores to Python, you reduce the wall time needed to compute your CPU-bound multithreaded program". But, as stated, apparently you already somehow know that this is impossible. I think Python is a wonderful language. And I think the GIL is not as major an impediment to using multiple cores as most people think. Many people with CPU-bound tasks could restructure their program around a multiprocess solution. But there are definitely people who need multiple *threads* computing code simultaneously, for whom multiprocess won't work. And who would like to use Python but therefore cannot. Moreover, I assert that computers are only going to become increasingly multicore over the foreseeable future. Intel et al find it increasingly difficult to make individual CPUs go faster, so they've decided to start competing via core counts. Right now it's hard to put these extra cores to good use; we only have so many embarassingly-parallel problems. But I further assert that all these cores will create increasing "research pressure" to find ways of putting them to better use. I don't know what shape those techniques will take, but I can be certain that as long as CPython has a GIL it will be increasingly left behind by these new programming paradigms. The best of all possible worlds is one in which people can simply write multithreaded programs in Python, and it will automatically run on multiple cores if they are available. IronPython and Jython already do this. Surely this is desirable in CPython too? Surely you're supportive of me trying to achieve this? &gt; If it got to the point where gilectomy was up for mainline submission I'd probably be more vocal about the issues with the architectural approach being taken. By all means--speak up now. If we're wasting our time exploring the gilectomy, it'd be cruel to save your damning critiques for the 11th hour, after we'd already found enough success that it was being considered for merging.
Hmm, I can't seem to get virtualenv starterbot to execute properly. Running into an error when the command runs: &gt; ImportError: dlopen(/Users/username/Development/Python/slackbot/starterbot/lib/python2.7/lib-dynload/_io.so, 2): Symbol not found: __PyCodecInfo_GetIncrementalDecoder &gt; Referenced from: /Users/username/Development/Python/slackbot/starterbot/lib/python2.7/lib-dynload/_io.so &gt; Expected in: dynamic lookup Python newbie, so I'm sure it's something I'm doing wrong.
The Gilectomy guy is /u/ExoticMandibles. He might be able to say whether this would make things better or worse.
Yes, that's completely expected and [documented](https://docs.python.org/3/reference/executionmodel.html#resolution-of-names) behavior. Assignment is a type of name binding operation, and assigning to a name anywhere in the function binds it as a local variable, unless declared otherwise with `global` or `nonlocal`. It doesn't matter that it's not reachable. The compiler can't perform that kind of analysis. (It would amount to solving the halting problem in the general case; not all examples are trivial.) 
Well I know iMacs do but forgot about MacBooks. It's so weird to me since it seems like it's been a really long time since I've seen a laptop(which might be why) without an SD even some of the T420's do. That really sucks. Seems like a huge oversight.
So. Much. This. Our budding D.S. team has been having a hard time engaging in active discussions with groups in our company to EVEN TRY to see what we can do. It's not as if we're asking for copious amounts of time and effort on their part to work with us, only to define some objectives and meet semi-regularly to see if we're producing anything of merit. This, (technically) watered down I think could be nice use case to reignite a discussion with our marketing dept.
Sorry I meant that sucks about the HDMI port. But yea I hardly use my SD card reader pretty much just for the RPI.
Perhaps it is reasonable when discussing with a friend that you are writing project that is CPU bound and requires threading. And the right answer currently is indeed to use something else for that particular problem. It's totally inappropriate to say it to a person who spends significant amount of his free time so you would be able to respond "you could solve this problem with python"
Haha, good grief Larry, you're being so formal. It's me, c'mon ;-) [Did you see my other response?](https://www.reddit.com/r/Python/comments/4mc634/pycon_2016_pythons_infamous_gil/d3zr7u7?context=3) &gt; Thanks for the link to Greg's revised patch. I hadn't seen that; all I'd seen was his original patch, as revived by David Beazley back in 2011. That version made incr/decr safe by performing those operations under a mutex (!), which was never going to be a winner. I'd heard there was a later revision that used InterlockedIncrement under Windows but I hadn't seen it. You seem to have located it and checked it in, which is nice. Where'd you find it? I honestly can't remember -- I thought it was from Beazley's blog post about the GIL... or maybe I just used that to bisect mail.python.org search dates and pulled the attachment from an e-mail. Regarding all the other stuff, yeah of course we're on the same page -- we want Python to optimally exploit our underlying hardware. If removing the GIL and adding fine-grained locking ultimately wins, great. That's a pretty conventional approach to the problem. And as there are people like you and Greg trying conventional approaches... I can try unconventional ones to see if there are other options for consideration. (Side note: how painful is our string interning when doing this stuff?! Interned strings, everywhere!) 
This attitude does not encourage people to keep developing for free.
`global` isn't affected by scope FWIW, it operates at compile-time.
Yeah, that's what I'm noticing. That's a big part of what confused me. I always thought of `global` as a keyword which executes at runtime, so when it runs into `str(something)` it'd only call the global if the `global` line "executed", which it is not since it's just affecting how it generates bytecode at compile time. I thought determining whether to call a global or local happened at runtime.
Some resources for Flask (I don't use Django myself, because it's too complex for my use case): * [Basic hands-on tutorial](https://github.com/bev-a-tron/MyFlaskTutorial) (I used this to learn Flask.) * The Flask [quickstart page](http://flask.pocoo.org/docs/latest/quickstart/) * The Flask [tutorial](http://flask.pocoo.org/docs/0.11/tutorial/) * Miguel Grinberg's [Mega tutorial](http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world) Here are some links to help you layout and design your webpage. * [Html tutorial](http://htmldog.com/guides/html/beginner/) * [Html forms tutorial](https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/Forms) * [Jinja tutorial](http://jinja.pocoo.org/docs/dev/templates/) * [CSS layout tutorial](http://learnlayout.com/) * [Website with color schemes](http://www.colorcombos.com/) For visualization, I'd suggest just reading the Seaborn/Bokeh/pandas documentation.
Locals and globals result in different code generated. Locals are accessed by a numerical index into a fixed list, whereas globals are looked up as names in dictionaries. This is a performance optimization made possible by the fact that you can determine all the locals of a function through static analysis, and new ones can't be added at runtime as with globals. (Occasionally you'll see code that caches a global with a seemingly redundant local, but it's to save on that overhead difference.) This is also why you can't assign to `locals()`.
Well I learned something new about Python. I thought when you invoked a function it set up some local scope dictionary and that's where locals were stored as they were executed. I didn't know it was a list generated at compile time. So, essentially the ids of locals are equivalent to indices into a list, some integer value. Now it makes a lot more sense, thanks.
&gt; It's highly unlikely OP can (or needs to) do 15kHz's worth of FFTs anyway. Why? 15kHz is nothing. When we were doing hardware sensor selection we were sampling at 1MHz and collected data in 1 minute chunks. It went into hardware filter design, software filter design and the controller design. It's stuff that is bread and butter for Matlab (and hence why we used it at the time). If Python is choking on 15kHz FFTs then it's going to be a hard sell to my group to displace Matlab.
my two favorites: www.cherrypy.org http://flask.pocoo.org/
Lovely piece. Questions: have you looked into what the most important features learned by the RF are? Similarly, have you inspected the misclassifications to see if there's anything in particular in those company descriptions that's tripping up your model?
I've just started learning python and have come to the same realization - the language is the tip of the iceberg! Is there anywhere you'd recommend that teaches the rest of the iceberg of programming? Such as the architecture you mention above? It sounds like all the separate languages are simply slightly different paths to get to the same point. But there's so much jargon I keep learning, it would be nice to understand all that extra jargon too!
All I could recommend you are the texts I picked up in college. I haven't looked for any other resources. That being said: http://www.amazon.com/Computer-Organization-Design-Fifth-Architecture/dp/0124077269 This text is really only useful after learning some discrete math, but is THE book to learn algorithms: http://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844 This one wasn't given to me in college, but at my first job. Really opened my eyes to OOD &amp; software architecture: http://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612 
Nice! I've always wanted to do something like this.
There's also [Pyramid](http://docs.pylonsproject.org/projects/pyramid/en/latest/)
What you described is not actually what you want to do. You don't need a real time 15khz constantly updating feed of voltage values. What you really want is similar to what an oscilloscope shows: a voltage versus time plot showing just a few wavelengths of whatever signal you're looking at. To plot that, you first need to calculate the wavelength of your signal (ie, do your fft, pick the maximum, or maybe a local maximum depending on your specific signal needs). Then you only need 15k/wavelength_signal data points to plot one wavelength. This only gets you a small stationary plot segment. You want something like this that updates, but you don't have to do it at the full 15khZ rate, you could update this plot just a few times a second. That gives plenty of time to calculate what wavelength scale you really want to be looking at or even do some averaging over several sample segments
If you don't get anything good here, you might try /r/learnpython, which is intended specifically for those new to Python and might be more amicable to your question. 
Personally I am really enjoying async python. The web server aiohttp is a little lower level, but its awesome. There is also growler which is kinda like flask, but built on top of aiohttp
You can use the same FFT library that Matlab uses (fftw through something like pyfftw), and get roughly the same performance. As a general rule, aside from some really esoteric domain-specific toolboxes and simulink, python can do everything Matlab can do (and more).
I recommend a book about a light web framework(Flask) [Flask Web Development: Developing Web Applications with Python](http://www.amazon.com/Flask-Web-Development-Developing-Applications/dp/1449372627/ref=la_B00J23SQ34_1_1?s=books&amp;ie=UTF8&amp;qid=1465351916&amp;sr=1-1)
distribution is a pack of your codes ready for your clients. It can be combined with your IDE. For example, you pack up your code as a distribution and upload your code to somewhere you place your code(such as Google App Engine)
https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-7 It just started
[Image](http://imgs.xkcd.com/comics/ten_thousand.png) [Mobile](https://m.xkcd.com/1053/) **Title:** Ten Thousand **Title-text:** Saying 'what kind of an idiot doesn't know about the Yellowstone supervolcano' is so much more boring than telling someone about the Yellowstone supervolcano for the first time\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/1053#Explanation) **Stats:** This comic has been referenced 7166 times, representing 6.3010% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d40cuom)
I loved your talk, thanks so much for putting that together. I can't wait to see where the GILectomy goes. I was wondering, do you think that a hypothetical GIL-free version of Python would be backwards compatible with the current v3.x? Because it doesn't look like it would be, at least to me. Obviously not an expert. If you're making compatibility-breaking changes wouldn't it be easier to change how namespaces work in Python than to deal with all this complicated locking?
+1 what are the most importante features/words?
up now.
Using multiple processes is just fine, unless/until you have data that needs to shared and updated amongst them. With multiple processes you'd have to copy the data and updates to all relevant processes (suitably coordinated). Using multiple threads in the same process means the data is "just there" - easy to access and update for all the code. But the GIL means that only one thread at a time can be running Python code within a process. It also controls when execution switches from one thread to another. Again this won't be a problem if the code running is doing non-Python things (eg database work or waiting for the network) and releases the GIL during, but if you want your Python code to run concurrently in a process then the CPython GIL prevents that. Some folks end up deeply affected by the GIL, while others aren't. That is why you see opinions ranging from this is the single most important thing Python must fix to remain relevant, through why the big deal given alternatives that work for others.
You only need to plot at the monitor's update rate (even old SGI beasts only did 240Hz refresh), and you can only plot as many points as you have pixels. 
Idk why I didn't think of that. Thank you!
That's what came to mind for me as well. The term to google is [ARC](http://clang.llvm.org/docs/AutomaticReferenceCounting.html)(automatic reference counting). One important difference is that Swift/Objective C have proper compilers, so have pretty good understanding of object lifetime. And, now that I've reviewed the link, I now recall how complicated ARC is. It gives me a new appreciation for Rust.
This looks really good! Is there a way to just scrape the stream from a site without the annoying ads and popups?
Ah, I see the distinction. Thanks for that. 
I'd use either aiohttp or falcon framework, flask context makes stuff unnecessarily hard.
So your loop right now is as follows? while 1: tweet_something() sleep(3600) It sounds like your tweet_something function is probably taking 10-20 seconds to run. So it tweets until 10 seconds past the hour, sleeps until 10 seconds past the next, tweets until 20 seconds past the hour, and so on. If you want to stay in the Python library, you might want to look into [sched](https://docs.python.org/3/library/sched.html). 
cron job is the right answer, but for fun: import datetime tweet_time = datetime.datetime.now() while True: if tweet_time &lt; datetime.datetime.now(): print("Tweet!") tweet_time += datetime.timedelta(hours=1) You can sleep (and occupy fewer CPU cycles) when you're dead!
But is it even faster? PyPy needs to emulate the CPython C extension API (it's recommended that you use CFFI for fast cross-interpreter C extensions for python).
I seconds this. We use it to graph 50kHz signals routinely.
Thanks! If you are interested I also just a similar python script that gets you live standings for major soccer leagues (EPL, La Liga, Bundesliga...etc) https://github.com/jctissier/Soccer-Football-League-Standings 
Thanks for the tip! i'll go make those changes
Datetime or cron as suggested, I have a script that should execute every 30 seconds, but the sleep is 28 since the program takes 2:)
Will do when I get home from work - please don't laugh!
Cron is nice if you are on linux and have access to the config file. If you don't, use asyncio.sleep(1), check if an hour has past since the last stuff, then if yes, run it. 
Run your twitter request in a sub process/thread. Make main thread a scheduler with correction. Twitter thread/process listens on queue. Scheduler process issues request at that time. 
I have to test this, it's great, thanks! Is there a planned (roughly) date for a production ready release? 
Thanks! I am looking at the descriptions it misclassifies, and sometimes it's easy to see why, whereas other times harder to crack why it misclassified. It's very good at filtering away telecom, broadband and mobile companies, but sometimes it does stupid mistakes, such as disqualifying shipping lines. I'm not sure how I'd 'debug' the random forest algorithm. Especially as it has 5K features. It's very much like a black box. Do you have any suggestions? 
we've posted the next parts of the article, in case it's interesting for you - https://7webpages.com/blog/writing-online-multiplayer-game-with-python-and-asyncio-part-3/
&gt; predict.proba Good idea! I'll do that for sure.
 import time while True: while int(time.strftime('%M')) != 0: time.sleep(1) else: do_the_things() This ensures that your task only runs when the minute is XX:00 This way, you don't need a cronjob. You can do everything inside the same python process, which might be more attractive in some cases.
Bottle is simple and awesome. It's like Flask but much simpler and easier to use (not to mention coming in a single file!). Use (monkey-patched) gevent as the back-end server and you get all the benefits of async I/O with none of the ugly drawbacks (you just write idiomatic Python code and let all the asynchronous parallel magic happen in the background, as it should).
just leave the program open? 
Thanks for the response, the reason to remove the 'for' loops is for homework question, i cannot have any loops. The professor showed up once weeks ago how to do it without a loop, but i can't find the notes (or perhaps i never took them) 
colon (:) is missing `while i != 0:` is correct
You're trying to use python2 to run python3 code. Either change input to raw_input or use the newer version of python.
Thank you. I still have to learn the differences...
Well, it's up to the OS to load a particular process by whatever scheduling mechanism it uses right? So it wouldn't be guaranteed, but it's also not guaranteed if you poll in a tight loop that your process won't get interrupted by the scheduler and put to sleep for some amount of time that may be even farther off the target than time.sleep() would have left you, depending how CPU starved the system is. Or am I missing something?
No idea. I'll run some tests on a heavily loaded system to see which method is better.
Looking forward to this. I'm constantly being given weird collections of 20GB+ delimited text to try and turn into something sensible with my very dodgy Python scripts. And there's the 2TB of IIS logs sitting on a hard drive on the shelf that I try to avoid having to run ad hoc searches over.
I just released it this morning. Feel free to give your opinions and suggestions. Cheers
C++
+1 for Codecademy
You need the data or a screenshot of the table would work?
I haven't tried it myself, but [bokeh](http://bokeh.pydata.org/) seems to have a large chunk of functionality devoted to streaming data. You could ask this question on the bokeh mailing list - They're quick and very helpful!
I imagine it like a completely exhausted horse running laps on an empty track hoping every round that THIS TIME the trainer will be waiting with an apple.
Let the UNIX philosophy flow through you. Good… Now speak after me: "One tool, one task, one tool, one task…"
It's disappointing that the benchmark didn't include comparisons to the standard csv module, or for writing csv files. These omissions incline me to think that it's slow at these tasks.
Thanks! :)
While you could send emails/gmail from python, doing so would require python to be running somewhere. Not easy when your computer is turned off. There are many non-python plugins for gmail, a google search for "gmail delayed send plugin" gives you a top result that should help you. Personally, I prefer to have a small device (a raspberry pi) running 24/7 and let that handle delayed sending of emails. In python, of course.
its has to be the data. need to return the description based on register values!
Awesome explanation! Thank you!
One hundred percent this. Even on windows, I'm pretty sure there *must* be a task scheduler easily accessible somewhere. I do love python, but "I want to run a script every hour" seems something that a task manager should do, not the script itself. Let's not reinvent the wheel, *even if the wheel is not in Python*.
Switch to SAX
post it here https://pl.python.org/forum/index.php?board=9.0
How does this differ from joblib's Memory? https://pythonhosted.org/joblib/memory.html
&gt; Could it be that the speed benefits come into play for larger files? This makes no sense at all. Unless its multithreading its a serial IO constraint.
Install poppler and use pdf2text and see if you can, in paper and pencil, reconstruct the table from the output.
He may be on a hosting not providing access to cron.
Agree, it's a bit suspicious. However, I think a challenge with the standardlib CSV module is that the benchmark would be implementation dependent. I.e., you want to ultimately (almost always) convert it to a NumPy array or pandas DataFrame to be useful; aside from read speeds, I think for large datasets converting Python objects to these structures could be a memory bottleneck
You are my favorite person today, thank you so much!!
I just see this benchmarks were done on machines with SSDs (see [whitepaper](https://deads.gitbooks.io/paratext-bench/content/machine_setup.html)). That's what I thought, however, nowadays, we still store the majority of our datasets on HD ... because they are large ... I'd expect a smaller difference between the tools on HDs but still a nice tool!
Returns to parallelization always increase with computational difficulty. The overhead of doing an easy task in parallel can make it slower than doing it on a single processor.
The idea is great. The problem is that it is server side only, so doesn't solve the universal rendering problem.
4GB/s isn't just a single ssd, but probably 8 ssds in a raid array
I have tested extensively, both are the same and concat/append at the same rate. I even have this documented in my pandas extension: # NOTE: concat same speed as append...
remote or on-site?
This is very important. I've been worrying for a long time about the consequences of executing all that untrusted code from PyPI. And yet at the same time its such a critical resource. I think a good first step would be to introduce a 'flag as spam or malicious' button on each package, in the same way that [Atom](https://atom.io/packages) does.
Actually, scratch that. You should occupy all *n* cores in the machine in a busy loop, just to make *sure* you got the tweet sent. First one to hit the deadline kills the others, then sends the tweet. It's just not good enough to only occupy 100% of a *single* core in this day and age.
FYI, the more appropriate place for this is in /r/learnpython.
Til append has evolved. regardless, concat (list,ignore_index=true,axis=0) is quite snappy. When you say slow, what are you comparing it to?
I think that's right for analysis. But I also see a lot of work done with Python on data preparation, transformation, validation &amp; cleansing - where DataFrames can be too slow on large volumes.
Again, I did read the dozen other comments, and also clearly mentioned there are other libraries, but this is the simplest and easiest solution. How would do it using sched? There doesn't seem to be a "repeating" schedule. Would he have to set a "constant" number of schedules in the future? How many would he, thousands? Would he have to re-add more once they run out? I'm sure it would definitely be more than the 2 lines of code I have above.
Pretty interesting that it was Guido van Rossum who did the port too
How does the numpy stuff work now? Does it still require using the separate numpypy package? And can I use things like Pandas?
Alright! We've just released xonsh v0.3.4 (available from conda or pip, or from Github master), which I believe should fix this issue. The relevant changes are: we no longer inherit the `$PROMPT` variable from the parent environment, and we now fail in a more graceful way if the `$PROMPT` is malformed. A full changelog is available [here](http://xon.sh/previous/changelog.html#v0-3-4). If you get a chance to try it out, please let me know whether this fixed those problems, and also generally feel free to let us know if you have any other feedback!
Alright! We've just released xonsh v0.3.4 (available from conda or pip, or from Github master), which I believe should fix this issue. The relevant changes are: we no longer inherit the `$PROMPT` variable from the parent environment, and we now fail in a more graceful way if the `$PROMPT` is malformed. A full changelog is available [here](http://xon.sh/previous/changelog.html#v0-3-4). If you get a chance to try it out, please let me know whether this fixed those problems, and also generally feel free to let us know if you have any other feedback!
Alright! We've just released xonsh v0.3.4 (available from conda or pip, or from Github master), which I believe should fix this issue. The relevant changes are: we no longer inherit the `$PROMPT` variable from the parent environment, and we now fail in a more graceful way if the `$PROMPT` is malformed. A full changelog is available [here](http://xon.sh/previous/changelog.html#v0-3-4). If you get a chance to try it out, please let me know whether this fixed those problems, and also generally feel free to let us know if you have any other feedback!
Alright! We've just released xonsh v0.3.4 (available from conda or pip, or from Github master), which I believe should fix this issue. The relevant changes are: we no longer inherit the `$PROMPT` variable from the parent environment, and we now fail in a more graceful way if the `$PROMPT` is malformed. A full changelog is available [here](http://xon.sh/previous/changelog.html#v0-3-4). If you get a chance to try it out, please let me know whether this fixed those problems, and also generally feel free to let us know if you have any other feedback!
As already stated... no display updates at 15000 frames per second. Also, your display is probably not capable of showing 15000 pixels in a row. 1. You don't want to update your graph on every sample. 2. You don't want to show 15000 dots. If I was doing this, I would decide on a refresh rate of the graph... like 15 hz, collect latest 1000 samples, and refresh the graph. Also, this will give me enough time to find the fft of these 1000 samples and show that along side the graph. If 15hz seems a little low, we can bump it up to 30 hz... but going above that has diminishing returns. 30 hz is usually good enough, and refreshing faster than 60hz has no benefits at all.
I mean, you have 4 lines of code above, but I take your meaning. You would use the classic pattern of having a function schedule itself in an event loop. Basically, you would turn this: def tweet_forever(): while True: start = time.time() tweet_function() duration = time.time() - start time.sleep(3600 - duration) Into this: def tweet_forever(): manager.enter(3600, 0, tweet_forever) tweet_function() Or, even more precisely, this: def tweet_forever(now): tweet_function() next_tweet = now + 3600 manager.enterabs(next_tweet, 0, tweet_forever, [next_tweet]) The latter one is about as optimally precise as it's possible to be, within the limitations of `time.sleep`'s precision, though it does assume that the `tweet_function` will never take longer than 3600 seconds. Where `manager` is an instance of the [`sched.scheduler`](https://docs.python.org/3/library/sched.html#sched.scheduler) class
Here is one way to do it. Convert the image into a graph, where each white pixel is a node (ignore black pixels) and 2 nodes share an edge if they are adjacent in the image (touching vertically, horizontally, or diagonally). Now, use a graph library (networkx) to separate this graph into connected components - each star will be a separate component (if stars are not overlapping). For each component, you can find the graph diameter (method in networkx library), which, in case of circular stars also corresponds to the diameter of the star in pixels in the input image. 
Overall a good list. There are a couple on there that I wasn't aware of that potentially look interesting. I would probably have substituted holoviews for gleam on the list since they seem to have similar purposes, but holoviews is still being maintained while gleam apparently hasn't been maintained for a few years now.
Tom Augspurger is one of the main pandas contributor. [Here](http://nbviewer.jupyter.org/github/tomaugspurger/modern-pandas/blob/master/modern-4-performance.ipynb) he says concat is faster than append.
Cool!
Swift is the object storage portion of Openstack. Documentation says it can be installed standalone without the full Openstack installation. https://wiki.openstack.org/wiki/SwiftInstall http://docs.openstack.org/developer/swift/development_saio.html Edit: Python client https://pypi.python.org/pypi/python-swiftclient Node.js support https://www.npmjs.com/browse/keyword/openstack
ParaText does not support writing CSV files. The scope of our benchmarks focused solely on reading. The standard CSV module `csv.reader` is row-based. It does not populate a data frame or a collection of columns. We only included readers that have this capability.
Fantastic, great to hear about someone's first experience. Especially given that it was such a positive one!
It's the same for all package managers and other open-network-fetching resources; the attack surface is huge, from MITM to typosquatting... Pip has no "buttons"; you would need some sort of web-based feedback form. By the time you've realized your mistake, setup.py has already run anyway, and your server is now a spambot. The main action item really is for archive managers to periodically review typosquatter candidates, either manually or in a semiautomated way. If necessary, make people jump through more hoops when you upload or register your package name. And for god's sake don't run pip as root, that's just asking for trouble.
Way to read into the post (thumbs up), people probably only down voted you because your post was a bit long, and so they *couldn't be bothered*. We should all have a greater appreciation when someone qualifies a cognitive bias in their post *before* it's ever pointed out.
Which library does this? :P https://i.imgur.com/Ch5u5KW.png
I saw you walking around, glad you had a productive and fun time!
Thanks a lot! I've also looked into Ceph and Minio. To be a bit more specific, I am working on a networking middleware for games. I need to handle uploading of game stats (level/experience) as a binary file and save it to the database (and later on retrieve it). I'm running multiple RabbitMQ workers so I need a database for storing these... I can't just save them on disk.
I encountered warehouse. It is basically finished since several years, but nobody uses it. It seems to be a dead birth.
isn't this the exact problem with open source though? 
What do you mean?
Supporting Python libraries out of the box is the right way to go. I believe that this updates will increase PyPy adoption and contributors list.
Well, he is wrong.
I like Leather...no comment. ... Except... "It's my plot and I need it now!"
I'm curious how you think that would prevent a typo-based attack. What downstream packagers (like debian, red hat, et al) provide is a controlled environment with vetted packages where typo versions can't be included.
Warehouse is under active development.
How big do you think the files might get? Postgres BLOB storage might be able to handle it as well. https://wiki.postgresql.org/wiki/BinaryFilesInDB
There are a lot of problems with open source, too many ways to do things isn't one of the ones I would have picked though. I would have said documentation or lack thereof.
each file would be about ~10kb... 10kb * 500 000 users = 5 GB total?
Lol C-
Are there any good libraries for 3D visualization? I've got some really niche geological profiles I'd like to plot, but not much luck doing so.
Sure, if you do the "wake up once per minute" strategy then your accuracy is limited to one minute, but you'll never be more than one minute off, which is adequate for many tasks. The idea is just that you are never looking for an exact time. You just look to see if there's any new work to do everytime you wake up. So if deadline &lt; current_time, do the thing. If not, ignore it. On average, you'll have 30 seconds error in your trigger time. (Or, half of the fixed sleep period, which is 30 seconds with a one minute sleep.) It's not perfect, but if it's adequate, a more complicated technique doesn't get you anything.
&gt; Perhaps the most interesting difference is that the reference count is not stored on the object itself, but rather separately. And they make two simple optimisations. Firstly objects with a reference count of 1 (by far the most common value) do not have an entry for their reference count. Your facts are *partially* true. This may have been true in the 32-bit world, but for most cases today, it's not. Your conclusion is backwards. With ARM64, there's enough unused bits in a pointer that the `isa` pointer carries 19 bits of refcount. It only falls back to the external hash table if you have more than 2^19 (about half a million) references to your object (and then it sets the 20th bit to indicate this). I think the story is similar with x64, but due to more bits already being claimed, the inline refcount is only 8 bits. Still, that's enough for many/most application objects. In fact, it's *using tagged pointers* that is (always) the optimization. Not having to keep hitting an external hash table improves retain/release performance by about 50%, due to spatial locality. In a sense, Python is already ahead of the game here, because Python objects don't need to be single C words, so it stored refcounts inline from day 1. If Python moved refcounts into a shared external structure, that would hurt performance. The one piece of the Cocoa refcount system that *might* help Python is bit-packing. Python uses Py_ssize_t (ssize_t, the signed version of size_t -- see PEP 353) for refcounts, which is usually mostly 0's. It's possible that by combining that with the class pointer (and overflowing into an external hash table when needed), you could save enough memory to improve your cache hit rate. Interestingly, that's the major performance bottleneck with the Gilectomy project, as well, which suggests it's a promising thing to try. But that's not at all a sure bet. Unlike Cocoa, which is basically just a refcount and a class pointer on top of C, the Python runtime has lots of other interesting functionality. (For one thing, ARC never deals with collecting cycles. It's your responsibility as a programmer never to create strong cycles.) It's not at all obvious to me that saving a word of memory from every Python object would significantly improve performance. It might, but it might not. The idea that Cocoa is faster than CPython because it uses an external shared hash table for refcounts is just backwards. Apple moved away from that, for the first half-million cases, as soon as they had the bits to spare.
Just clarifying: on Python data viz libraries that specialize in producing interactive graphics? So minus libraries like matplotlib, for instance?
You could try out Mayavi (http://docs.enthought.com/mayavi/mayavi/index.html) or VisPy (https://vispy.readthedocs.io/en/latest/). Plotly also has some 3d options: http://blog.plot.ly/post/101360048217/7-plotly-graphs-in-3d-stocks-cats-and-lakes
First, homework questions go on r/learnpython, as noted in the sidebar. To your idea, think about lists of lists. Each inner list is a row, and the values in each inner list correspond to columns, like this: [[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]
Try plotly
I was more thinking of d3.js type python packages when making the above comment. Since I am new to Python I am only aware of few that have beautiful interactive graphics. My boss (non-techie) once said, you are right if your chart looks beautiful.
Ignoring C++? Have you tried using http://doc.pypy.org/en/latest/cppyy.html
Which may very well be beneficial. Changing the refcount invalidates the cache for that area of memory. If the refcount of the object lives externally to the rest of the guts of the object, that means an incref won't invalidate the object itself and you can get to work immediately without waiting to refetch it. One PhD I know claims that my "buffered reference count" approach should be modified to store the refcounts externally for exactly this reason.
I'm talking specifically about the pandas. I use it on a daily basis. Concatenation lists with pandas is the way to go
Awesome +1 to that. For those looking for a weekly Python/Django Newsletter check out http://importpython.com/newsletter/ Just sending out the 76th issue of the newsletter today. disclaimer - I curate it.
Sure. All you really need is the Bottle documentation's [Greenlets to the Rescue](http://bottlepy.org/docs/dev/async.html#greenlets-to-the-rescue) section. The first line and last line are basically the only thing you need to use gevent instead of the default WSGI reference server from the standard library. That's pretty much all you need to handle many requests in parallel if they do things like network I/O and IPC (e.g. talking to a database)—the primary benefit of asynchronous programming. If you want to do multiple sub-tasks in parallel (such as making requests to other web APIs while handing a request), you'll want to use [gevent's API](http://www.gevent.org/contents.html) to start and wait for greenlets (which behave similarly to threads but don't preempt each other unless you explicitly request it or they hit operations that would block).
If I followed this method and used IPython, I would just run the commands in Terminal (once everything has been installed), ya?
Yeah that's why I don't use Matlab. 
I'm using EPD free right now and have no problems with it. Conversely you can just work right out of the Terminal as he states in the intro to the book.
&gt; just &gt; opengl I've found OpenGL to be extremely stressful.
With Numpy, Scipy, and Matplotlib support, this is becoming very interesting! How much slower are these than their CPython brethren? Also, is Numpypy compatible with CScipy and CMatplotlib?
Yeah, I meant "just" as in opengl is raw, not easy. Like, whatever they want is definately possible with opengl.
We use [APScheduler](https://apscheduler.readthedocs.io/en/latest/) for a pythonic approach to scheduling on a big project that keeps track of a network of radio stations and their programs. It's been running flawlessly for about a year. I don't know sched that well, but I'd say APScheduler is to Requests what sched is urllib. APScheduler allows the ability to save to database (redis, mongo, SQLAlchemy), so if your program or system goes down it won't lose state and will execute anything that was hanging. Database integration and triggers make it very easy to fit with your other work. It handles time zones. Also, it is thread safe, which I believe sched is not. My only complaint is that they did a major API change right after we wrote all our code. Totes inconsiderate. 
I kinda like the looks of PyQtGraph and currently using it to plot data from the serial port. Seems useful for plotting a lot of data at the same time.
This post is probably better suited to /r/LearnPython. If you just want the functionality, you could look at a packaged like PyBDSM to extract source sizes. Your best bet for strategy is to use the coordinates of the centre of the mask and fit a 2D gaussian around that point. The FWHM of the gaussian will give you the radius of the star.
Have you checked out [Astropy](http://www.astropy.org/)? I'm a bit out of practice along the astro front, but perhaps they have some modeling support that would assist with what you're trying to do (which I assume by "radius," you mean the apparent width of each star in the field in the image, versus actually attempting to calculate each star's radius based on other data you have about the stars' temp/luminosity). I know Astropy has FITS file support, but since you didn't mention difficulty in dealing with that, I'm guessing you have that part handled.
You can use easy_install to get pip and use pip from there.
Run SIFT (or other similar algorithm) on the printed image. Run SIFT on the webcam frames, Calculate matches, between the 2 sets of sift points. Calculate a homography from the matched points. I think there are examples of this very thing in the opencv docs. I'm on mobile right now and can't find them. 
How does that work, using Flask and D3?
I currently work in PyCharm using the IdeaVim plugin (https://plugins.jetbrains.com/plugin/164?pr=clion) which allows you to work Vim-like inside of Pycharm, but it has... quirks (copy pasting in search bars, for instance, is a hassle). But it works quite fine otherwise. To get started with Vim, look up vimtutor, it should be built-in in the install.
This is great! I am about to start working on something that has some overlap. Any chance you've cleaned the code to put it up on github yet? Cheers
Regardless of your opinion of what code *should be* written in, many of the useful libraries in Unix *are* written in C. And a C FFI is also useful for interfacing with code written in other languages that can present C-style externs, like Rust and C++. It's the lingua franca of Unix, whether you like it or not. Also, CPython's primary low-level FFI is the C-Python API. So that's what most of the Python extensions are written in. PyPy wants to be compatible with existing extensions.
Flask generates some json (or csv, tsv - up to you) data, puts it in /static and D3 loads the data from there and makes charts from it. 
I have been using Vim for just under 2 weeks now and I was up and running with basic editing in about a day or 2 with this: https://scotch.io/tutorials/getting-started-with-vim-an-interactive-guide
You missed the r/python meetup :P
You should have a look at Kivy https://kivy.org/docs/guide/android.html
Just get another terabyte of RAM then! It's cheap, right? What do you mean you can't afford another 10 servers?
&gt;sometimes I think people writing webservices in Python really have no idea that the scientific community even exists. Sometimes I think web developers really have no idea ANY other kind of programming exists, FTFY. There are many other kinds of applications that need to be built and maintained. This is one of my gripes about the new school Javascript-everywhere movement. . . nodejs is not a perfect solution for every problem. Nor is Python or any other language or tool. I've gone to meetups off and on for a number of years, I still remember the first Python meetup I attended - after the meetup, the organizers asked for feedback on what could be improved and one of the attendees, who was clearly a Scipy/Numpy/Pandas kind of guy, complained that "there are too many web dev types at the meetup." I thought that was funny but bitchy yet it illustrates the somewhat fractured nature of the "Python community". Let's not get started on the Python 2/3 schism. . . :) EDIT: and yes, I do have a problem with Javascript. I don't hate it but I refuse to pretend that it would even exist on the backend if we weren't all essentially forced to use it for browser coding. . . I am hoping that Web Assembly changes that once and for all.
Hi, have you seen GCD? I very much would like that approach to multithreaded programming. Celery and GCD have very similar approach. If you can assume, as a language designer that each task is atomic, locks stop being such a constraint - you copy non-shared memory and the end programmer has to worry about access to shared objects.
Hi there! I made the switch to using Vim as an IDE for my python/c++ work a while back and haven't looked back. So the tricky thing is that Vim isn't an IDE on its own, it's just an editor with fantastic plugin support. In some cases there are dedicated IDE programs that can be more intuitive--I use vim because it's fast, I'm used to its shortcuts and I do a lot of work on servers over SSH, so a full graphical application isn't always useful for me. That said, I've tried a number of other IDEs and editors (sublime, atom, pycharm, eclipse, netbeans) and, in my opinion, none of them hold a candle to Vim. I use [YouCompleteMe](https://github.com/Valloric/YouCompleteMe), which is a plugin for Vim that gives it semantic autocomplete for c++ and python, along with c# and rust and a few others. ~~**An important note on YCM:** It really only works on Unix-based systems (Mac OS X or Linux). If you're on windows you're gonna have a bad time with it.~~ Edit: I stand corrected! According to /u/deto [YCM works on Windows](https://www.reddit.com/r/Python/comments/4na5hj/discussion_using_vim_as_ide_for_python/d42e4yn) Aggravatingly, YCM doesn't have very intuitive install documentation. If you decide you want to use it I'd be happy to give a quick writeup on how to get it up and running. [Here's a screenshot](https://imgur.com/C9wYpCj) of my editing setup, which uses Vim with YCM and Byobu, a terminal multiplexer (which is a fancy way of saying "I can have multiple windows open in my terminal). On the left is Vim with YCM--I have the autocomplete shade up, showing all the members of the Random class. Upper right I have htop, which is partially to look cool. Bottom right I've a linker error, which isn't exciting, but my point is with byobu I can edit and compile and debug super easily--Basically, I've made an IDE for myself. The upshot of this is that most of these utilities are accessible via SSH on servers, so I can use my department's beefy servers to do all the heavy lifting, and make sure that my assignments will compile on the grading systems. If you've got questions about any of it I'd be happy to help! Cheers and happy hacking! Edit: I realize I didn't include a screenshot of the actual python side of YCM--[So here's one!](https://imgur.com/touDsqi) As you can see it offers introspection similar to the c++ side, showing you members of various classes, along with autocomplete.
https://youtu.be/NqdpK9KjGgQ This is a pycon talk about using python for Android and iOS. It is possible, but requires some tricks.
If you have a good tutorial I'd like to see it - I dabbled with Cython's `nogil` without much success...
That does make things clearer, thanks - and I agree, I don't actually think we really need to get rid of the GIL at all, but instead make tools to make parallel code possible. What I think you do miss though is that IO isn't the only reason for wanting threading, in the scientific community many more things are CPU and RAM bound and you really want to be able to operate on shared data in parallel - it's a bit tragic seeing your 32-core workstation chug away using just one core. I think the tools to make this possible are within reach, but I they probably won't be the same tools used in web programming.
Maybe next year!
https://github.com/alex-sherman/deco 
&gt; Do people use coroutines? Yes, but not in production code. I may be opinionated, but I've done concurrency in many languages and never ever have I seen anything less readable than coroutines. I don't agree. I use python 3.5 coroutines in production code and, for me, is very readable.
The problem is that celery only solves half the problem. Yes, you can shoot off a celery task to go execute code without blocking your current process. As far as that goes, celery does an A+++ job. And for many many applications, that is all the asynchronous functionality that is required. However, what if you need a callback? Go execute this code asyncrhonously, so I don't have to block, and then whenever you happen to finish that, return to me the result so I can do something else with it. There is no way for celery to do this asynchronously. Your current process would have to block/loop and use polling to check your celery result store to wait for the results to appear. Thus, defeating the purpose of doing the work asynchronously to begin with. If you can find a way to fire callback functions asynchronously, you've got it solved. But celery doesn't do that, and the GIL is going to get in your way.
Holoviews and vispy both support nice-looking interactive plots.
Doesn't PyQt5 work as well? Not positive but I remember reading about all the ways you can package the final product.
Did you ever try some of asyncio libraries? [http://asyncio.org](http://asyncio.org)
Kivy is good and useful coz you can create one UI for desktop and mobile but if you want serious mob-dev like freelance of fulltime, you better begin from something more native for those platforms. Smth like SDK(Java) or NDK(C++) for Android or Unity(C#)/Unreal(C++) for games and so on. Kevy still have some problems but in most cases not so big for personal projects.
It actually is quite doable :)
How big of a team works on that code base?
What code base? mine?
That was a whole lot of great info thanks for providing with such great explanation. Just installed YouCompleteMe just messing around to get my ideal config. 
Yes, ou are right, also it depends on the type of instance. Also the task you are running, for example, a crawler would not need too much CPU but it takes time to get each http response. So, if you need a lot of CPU, choose a c4, if you need crawler, t2, and so on. In any case, just for curiosity, I created 2 t2.micro and made 2 test: For 1 hour the proposal task in a t2.micro machine. (heavy CPU load) and here we have the results: * 0-10 min: 1456 repetitions * 10-20 min: 368 repetitions * 20-30 min: 406 repetitions * 30-40 min: 376 repetitions * 40-50 min: 368 repetitions * 50-60 min: 415 repetitions Very bad performance. What about a crawler? I made a script which collects html code (no analize nor storage it). 30 minutes: * 0-10 min: 2591 repetitions * 10-20 min: 2588 repetitions * 20-30 min: 2586 repetitions In this case, it remains constant. 
I used to try `YouCompleteMe` once, but make vim start very very slow. So, I switched to `vim-jedi` instead, it add about 2, 3 second for vim to start but still faster than `YouCompleteMe`(~10s). Do you have this problem?
There is no discussion. VIM is not an IDE. Just use a real IDE.
&gt; find a way to fire callback functions asynchronously Um, pass the "callback" with the task? That's why it's called a callback, "Call be back when you are done". "Return to **me** the result" is not an asynchronous callback, it is a block and wait for sub routine to return. An asynchronous call back is "do this, and when done call this, oh and on error call this", the caller continues on / never gets the return result (directly) I'm using call/callback in broader sense, it could be implemented as REST api endpoint RPC, putting something in task queue, etc.
I've never had my vim startup time be noticible with YCM, no--certainly nothing on the order of ~10s. I think YCM relies on Jedi for the python completions under the hood. Seems odd that even Jedi would slow your start time to anything noticeable...
Was not aware of this! Thanks. Looks nice. Lines are a little thick but that can easily be changed :P 
+1 for tmux/byobu--I was so excited when I learned I could multiwindow in the terminal!
It doesn't sound like you've tried Celery before. Try it :)
Enter `plt.style.available` to see the other available styles. There are several now, including some from Seaborn.
so, 1 person team?
Understood, but when you're actually doing true multiprocessing/threading you probably want to share the same memory space, rather than having to marshal objects back and forth. It's fine for many cases, but can become a major bottleneck for many applications. Though in those cases usually people write in C and do a python wrapper around it (less than ideal in my opinion, but meets the need pretty well).
To me, addressing the GIL is about getting better parallelism while preserving the simplicity of Python. There's numerous ways, such as Celery, to get parallelism with different levels of added complexity. 
Yes. Not everyone works in a team.
Seems like you do not know what an "integrated development environment" is. And thats okay, most people new to python are mostly clueless for some reason.
There are ways to make it work pretty nicely. If you install Cygwin and set it up correctly, you can right-click + "Bash Prompt Here" to open Bash in any folder. And soon Bash/Ubuntu is going to be built-in on Windows anyways. The only thing I dislike is not being able to run a Vim+Tmux combo. You can do this in Cygwin, but the main reason I want to use this is to run a python REPL in one of the splits, and I use Windows-compiled Python instead of Cygwin-compiled Python because with the latter I could never seem to get the BLAS/LAPACK/ATLAS numerical libraries to link properly with Numpy.
Or: "Celery fixed _my_ Problem", or: "Surely everyone writes web applications" The GIL hamstrings parallelism, not concurrency. What you've described is a distributed system; you've introduced a ton of new failure conditions. In my world the GIL is a big problem. Why? Because it makes it hard to leverage my resources. 8 core and 16 core servers are common. If I want to write Python code, and my problem is not solved with some package that's already done the legwork doing the meat of my problem in C (numpy, pandas, etc), I simply can't use them from a single process. People find that frustrating, and I don't blame them. So because of the GIL, I have to run 16 copies of my process per box, and a queue server, and some other daemon, which can all break independently. My processes can't share any memory directly. They can't share connections to other resources. I have to pay serialsation and copying costs for them to communicate. But it's no problem because the API is clean? There's a big vibe of "I don't personally see the need for this therefore it isn't useful." Nobody uses coroutines in production? Unreal. 
I am not a super advanced user who worries about virtual environments, etc. I really just need the ones you mentioned. And when I do call an external or imported module, I often do it manually with a copy for easier portability. Also, I too work in just a terminal and text editor (flipping between vim and TextWrangler). So, with that said, I can say that for me, it hasn't mattered much. I have Canopy on my work laptop (but through work, I have a higher license than the free ones) and Anaconda on my home macs. I do not really notice a difference. I think the only time it mattered for me was when I was updated matplotlib and I had to do `easy_install` at home and used `pip` at work. There were some issues but they were also related to trying to do this at work and with our firewall 
I don't understand why you don't like Javascript. It was a terrible scripting language tied to the browser, but it's ridiculously improved since that time. Improved enough that people find it useful outside of the browser now. If you're upset that people like to use the tools they already know instead of finding the "best" tool, I expect that you'll be upset for a long time. I'm afraid you sound like a hipster who's upset that his favorite ~~hat~~ language has become trendy. I think "trendy" is good for us, and there is room for everyone.
If you find something interesting, I hope you'll report back, either with a new blog post or just commenting in this thread. =)
I don't know if it is worth it to switch from PyCharm to VIM. If you are productive in PyCharm, then I'm not sure why you'd switch to VIM. I've been a long time VIM user. However, if you are interested in learning VIM, then go for it, but it does have a steep learning curve. Anyway, I've been using [Python mode](https://github.com/klen/python-mode), and it's really nice.
how?? I am hearing it for the first time. :)
Callback style hell is exactly what coroutines and asyncio yield were designed to avoid, because it ends up even worse looking. Pipelines are better but only work cleanly for a subset if problems and require extra divisions of your code. 
Its an additional security. If the package can't be validated with your preinstalled keychain, the package manager warns that the package is untrusted.
VIM is a text editor, but I was just pointing out the fact that you specifically came into a thread asking how to use VIM as part of a workflow and you just responded with: just don't do it. You are not being helpful is what I'm saying. 
I mean.. one day.. when we get web assembly, this would probably be viable. Right now there arent any good solutions.
Python doesn't have an equivalent. Nor does it have an equivalent of C++'s non-pointer stack values. All identifiers are references in Python. Not to gripe, you should really be asking stuff like this in /r/learnpython 
whats the difference between blender an maya? Thanks a lot btw
Ok thanks
Hey p4rosq, the script scans multiple subreddits and also some web scraping for live scores and stats. 
Huh? Numpy, numba and dask are all python. You just install them through pip or conda, import them and use them like any other library. CPython is implemented in C and designed to be extended through c, and that's part of the concept behind python, so to say that C extensions aren't valid is silly IMO.
&gt; It was a terrible scripting language tied to the browser, but it's ridiculously improved since that time. When it stops spitting out 'undefined' everywhere instead of performing any sort of rational error handling, I'll take it seriously.
In many applications copying that much memory is not an efficient option.
Just wrote a quick post [here](http://www.treo.co.uk/blog/2016/python-memory-sharing-numpy/), hope it helps!
Just wrote a quick post [here](http://www.treo.co.uk/blog/2016/python-memory-sharing-numpy/), hope it helps!
You can't really say it's readable as you are the one who wrote it.
I use coroutines everywhere in Scala and most places in JavaScript. ES2017 async/await is amazing. Code using monads can also be converted into coroutines in any language that supports them. FYI a future is a monad.
For anyone doing scientific python I strongly recommend the Anaconda python distribution. You get super good bindings for numpy out of the box and most scientific package can just be installed with conda install in a second. Also numba is super sweet :-P
Nice!! I'll test it out here soon :)
Can I choose what I want to install or I have no choice and have to install everything?
So, a lot of people are saying that I don't have experience with celery or that it does have callbacks. Both things are wrong. I have been using celery for years, and it doesn't have "real" callbacks. In celery, a callback works like this: task.apply(..., link=some_other_celery_task()) That doesn't help the problem. Consider this example: You have a program that is running a GUI application. You want to asynchronously process some data, because it's going to take awhile. In the meantime, you don't want to lock up the GUI. You want to let the user do other things while this is happening. CPU has more than one core, so go for it. Whenever that processing happens to be done, you want to display the results in the GUI immediately. In celery, all the work is done by celery workers. Celery workers don't have access to the GUI of your programs' main process. They are separate processes running elsewhere. They might even be on another machine. How can they call back your main process to get the GUI updated? Or maybe your main process is going to resort to locking and polling for that data to be ready, defeating the purpose entirely. Now compare that to something like JavaScript/jQuery function processData() { $.ajax({ url : 'example.com', type: 'GET', success : updateGUI, }) } The ajax request happens asyncrhonously. After you fire off that HTTP request, your code does not stop, it just keeps right on going. But when that HTTP response comes back, the updateGUI callback fires. And that callback, unlike a celery task, is within the context of your original "process". It has access to the DOM. If javaScript followed the celery model, that would be like having the updateGUI callback get executed in some other browser tab that knows nothing about the tab it came from.
it's a little late, but I always recommend people install [miniconda](http://conda.pydata.org/miniconda.html) which installs a bare bones version of Anaconda (basically it installs Python, conda, conda-env, and pip), but with access to the conda package manager you can then: conda install scikit-image and you're all set!
so just replace all of: for poster in posters: rel_path = poster['file_path'] url = "{0}{1}{2}".format(base_url, max_size, rel_path) poster_urls.append(url) with: posters[0] ?
I'm programming with reactjs and "undefined" is the only thing js says when I make a syntax mistake or a I forget to initialize a variable. It's useless. Now I really appreciate python's tracebacks. 
Hint: try using a Timer :) Edit: You can, and I actually did wrote, a Thread subclass that wakes up and does something once every X seconds. It's a very neat solution if you need to wrap a function with a timeout. Since all you need to get back the result of a Celery task is that task's ID - which is an int, then you can pass that int to your thread to poll celery backend asynchronously for results of processing. 
Anyone want to share their vimrc?
This is like saying vectorization primitives and autovectorization is unnecessary in your C compiler because you can just write inline assembly, and C was designed to make that easy. It's technically true you can do that, but it doesn't mean vectorization of C code isn't also a tremendously useful thing to have. Maybe 1 out of 100 programs will actually bother to drop down to the lower level. The difference between "it's technically possible" and "it's easy and we do it by default" are the difference between Numpy being fast, and all the other 50 libraries I use being fast. It's neat that you can write a library for Python in C that bypasses the GIL, but after a couple decades, I can still count on the fingers of one hand the number of Python libraries I've used that actually do. 
I don't think any other language in the world has anything quite like C++ pointers.
actually, how could i change it save the url for the image instead of the image to a folder. ultimately, i would like to put this url into a database for a website, but i could figure that out, even if you just guide me into getting it into a text file or similar. thanks!
That is a nice abstraction on multiprocessing/threading for sure! I may be misremembering this, but isn't that just a syntactic sugar on multiprocessing? IIRC this still requires object serialization to share, not sharing the memory (which unfortunately keeps us in a similar spot :()
If you really need mutable pointers, you have to wrap a value in either a one-element list or an object.
You can just return the url from the link above. Or print that url to a file of your choosing too.
Python is not written in Python! Therefore you use c functions from python every time you use a built in function. CPython is extended through C, and you can use numpy.sum just like you use the built in sum, and they both use c code.
Does a twisted-style `inlineCallbacks` count as a coroutine? If so, I think it could be said to make code *more* readable. Edit: called inlineCallbacks the wrong thing
At least it decreases the pain, hopefully over the next couple of years the story will shift a little more!
This is what frustrates me the most about Javascript. The language has actually come quite a long way and is actually almost usable these days. The horrible silent failure default behavior makes it quite painful.
Consider also voice software. https://www.youtube.com/watch?v=8SkdfdXWYaI
&gt;The difference between "it's technically possible" and "it's easy and we do it by default" are the difference between Numpy being fast, and all the other 50 libraries I use being fast. No, the GIL is not 100% responsible for the speed difference between Python and C. Right now, the GIL actually makes Python faster than not having it. I don't even know how to estimate how much faster Python would be without the GIL and with some other solution instead, but even assuming Python could just magically be GIL-less and each thread was the full speed of a current python thread regardless of how parallelize-able your code is, you'd get a 4-8x speed improvement maximum. But using C or FORTRAN is over 100x faster. That 4-8x faster is a magical best case scenario, and there would need to be some thread safety overhead no matter what solution is used and your code is not actually all that parallelizable. You'd have to go to some effort to make your code parallelizable, and that effort would look a lot like using numpy arrays and numba.vectorize (which will run your code on your GPU anyway and blow GIL-less python out of the water). Python is slow because it is dynamic and flexible. Even if concurrency were free in python, people would still use Numpy, Cython, etc, because having well structured arrays of simple static data types is just plain faster.
I find *explicit* coroutines highly readable. There is an obvious `yield`, `yield from`, or `await` which signifies that something asynchronous is happening, but otherwise it reads the same as normal blocking code. There's no confusion about a mess of callbacks. That's not to say anything and everything should be made a coroutine. I find a lot of libraries building on `asyncio` take this too far (why would I care to `await` closing a connection?). But this is not a readability problem, at least.
Hey, thanks. Sometimes you just need different keywords and there it all is. Any free alternatives to SIFT? 
That would imply a manageably small keychain, which is not a possibility in an environment like PyPI.
No Pandas?
It's still a pretty terrible programming language. Better than say, PHP, but that's not saying much. I'll admit there's neat stuff that happens in JS land, but even then 90% is reinvention of something that another community discovered a long time ago, but JS devs act like they came up with it. Flux/Redux for example. Event stores have been around for a *long time* but most of what i read on these libraries is the second coming off Christ. 
That is an excellent point. Looks like you could add it if you are feeling generous: [Github source](https://github.com/IPGP/scientific_python_cheat_sheet)
I gotta say. Well done. How lucky is that student to have a teacher who has gone out of their way to support his learning. Thanks for being awesome /u/awizardisneverlate
Try new things by all means, but don't let anyone convince you that you *need* to use Vim to be a serious developer. Use whatever works for you.
That looks really interesting! It might be too much for a beginner programmer, but I'll see if I can get a copy of Dragon to test it out with. Thanks!
[**@1st1**](https://twitter.com/1st1): &gt;[2016-06-09 22:16:48 UTC](https://twitter.com/1st1/status/741031316322410496) &gt;Did you know that PEP 492 had a flaw \(\_\_aiter\_\_ design\)? And that we are fixing it in 3.5.2? [*python.org*](https://www.python.org/dev/peps/pep-0492/#api-design-and-implementation-revisions) ---- [^[Mistake?]](/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=/4nd9bb%0A%0APlease leave above link unaltered.) [^[Suggestion]](/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/joealcorn/TweetPoster) [^[Issues]](https://github.com/joealcorn/TweetPoster/issues) 
Any reason you prefer anaconda over [canopy](https://www.enthought.com/products/canopy/package-index/)?
&gt; the web framework for humans *sigh*
Wow, thank you for taking the time to post this.
Our you can route json to a url. Let's say you had a database and wanted to visualize data returned from queries. You could point d3.json to a local url and have flask jsonify a python dictionary. See Miguel Grinberg's APIs with flask tutorials, or the section in his flask book.
Have you seen dask? It handles the heavy liftiting of parallizing some types of code. &gt; My processes can't share any memory directly. They can't share connections to other resources. I have to pay serialsation and copying costs for them to communicate. Preach, this is my problem. I need parallelism with maxed out CPU and shared memory. It's just not possible with the GIL and I've had to fallback on crazy setups/services and queuing systems to solve what would be a simple task in a shared memory threading system. The fact is it's 2016 and shared memory multi-threading should not be a second-class citizen.
Ok, step 2: 1) Post the exact command that you are entering in the console. 2) Post the exact error msg. 3) Are you running Vista? There are some known bugs regarding setx and Vista, which might be causing your problem but that is unlikely. 
Drop the C:&gt; and try again. The "C:&gt;" in the wiki is only trying to indicate that you are in the console... On your screen, you will notice that each line in the console is prefixed by C:\path\to\current\folder&gt; . 
&gt; Ipython is extremely helpful in scientific endeavors. If you are not scientifically trained.
It still says syntax error, though it "points" to the end of the word "path" now.
Took this hoping it'd help: http://imgur.com/DbJmhoq
This is VERY awesome. I find myself forgetting fundamentals pretty frequently 
What does that even mean? 
MKL is the linear algebra routines backend and has nothing to do with shared memory. It does simd and parallelization though. Numpy, regardless of backend, can take a piece of shared memory for its buffer. 
Awesome thanks.
Right--yeah. That much I've done. That part is working. I really appreciate your breaking down the simple issues for me... helps a lot. So now I've just trying to figure out what the problems are with cygwin and notepad++.
[Selenium](http://www.seleniumhq.org/docs/03_webdriver.jsp) can probably fill your need.
What limitations?
This is kind of silly, and there's no reason to prefer one way or the other, but does anyone else do `from matplotlib import pyplot as plt` rather than `import matplotlib.pyplot as plt`?
Check out [VIM and Python - a Match Made in Heaven](https://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/).
I'm a from matplotlib import pylab person. I blame ipython's pylab inline. 
It will blow over. . . Javascript is where Ruby was about 10 years ago. It's actually good, no language should dominate forever.
Nope, that is not the right place to paste that. You need to find the the .bashrc file. It should be in your user folder (the top directory of Documents, Music, etc) or potentially in the same folder where you installed cygwin to.
Finally, I am dying to go to bed. Think of cygwin as a different version of the Windows command prompt, which -- I guess -- the Enthought python distribution requires to work (I don't use Enthought so this an educated guess only). I presume that they are making you use the Enthought python distribution because it comes with a lot of extra modules that are not included in your standard python distribution. These extra modules add functionality to your python environment and some of these are presumably required for your psychopy module. Psychopy is a python module. Without a python distribution and presumably a number of other modules it would not run on your computer. Notepad++ is a text editor that unlike the (windows included) plain old notepad does not mangle your code (specifically, plain old notepad does not respect whitespace which is important in python code). Also notepad++ is a decent code editor that has extra features such as syntax highlighting that make coding with it easier. It is not the best text editor but it isn't bad either and by far the easiest to learn (for a windows user). Should coding become an important part of your life, you should look into other text editors such as emacs (emacs is life!), vim (the devil incarnate), or sublime (probably the closest to notepad++ but still a step up). 
generators: x = [for i in firstn(10)] should be: x = [i for i in firstn(10)]
Pair programming
I make it a goal to write up a much more simple version of one of these for each language I learn. This is amazing. Can you do more?
Plus as written it demonstrates a list comprehension and not a generator (well... kind of) there's a difference between them that I think could be expressed more clearly Edit: spelling 
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Ding ding you win the thread
Why I hate Twitter! No useful info in most twits. 
Well I have been using the Python.org package and pip3. Sadly there seems to be a significant issue here as I keep getting permission errors upon the next update. This leads me to suspect somebody is doing something wrong with the iPython /Jupyter notebook suite. At least at the moment it seems to be related to their files. What is strange here is that I normally use HomeBrew for everything else and for some reason didn't for my Python 3 installation. So I can't comment on HomeBrew for Python 3 installation other than to say it might have been a good idea in my case. By the way the Python 3 / Jupyter issue seems to exist under Windows also as I've gotten the same sort of permission errors when trying to update. So something seems to be inherently wrong with using pip3 to update Jupyter. Either that of techniques I've used for years are at fault. 
Depending how bad it is, maybe a better alternative is an ergo keyboard. When I hurt my wrists it was great albeit they weren't that badly off. 
Don't bother. He is a troll. I am a PhD student and IPython Notebooks (or Jupyter) made my life so much easier. Inline matplotlib and so much more!
I agree, anything else is too simplistic (drag and drop programming teaches you `if`s and `while`s, but all the fine intricacies you need for a proper project aren't available) or too hard to learn for a temporary injury (voice recognition software). Depending on how strict your syllabus is (it is very flexible in german Universities), he may also just take another class instead of this one and do this class next year.
I use and love Ipython notebooks. But what's that 'inline matplotlib'? never heard of it
There are people programming without hands, so it might be hard, but not impossible. If this is the passion of this student, why kill it because of technical limitations? I'm no expert in this. How about special keyboards? Voice input? Touch screens?
also http://weppy.org/docs/0.7/foreword#why-not 
I do use coroutines in production code :(...
Good question. I think it is out of habit and has not been thought through. 
Uf da. autocorrect strikes again.
I found this, it might be helpful http://dsp.stackexchange.com/questions/1288/what-are-some-free-alternatives-to-sift-surf-that-can-be-used-in-commercial-app
It seems [2to3](https://docs.python.org/2/library/2to3.html) does that already.
Very nice application of Prolog! It's great to see Prolog applied in such reasoning contexts and logical derivations.
eeer.. too long.. after proper optimization it should run in 2.7 seconds
I don't mind the phrase if it's true. But this looks as abstract as any other web framework. Only humans willing to get over the learning curve will use it. My suggestion for anyone who wants to program for humans is to read [The Design of Everyday Things](https://www.amazon.com/Design-Everyday-Things-Donald-Norman/dp/1452654123).
Read the sticky. 
I think she thought that me sayiing we could use python was showing off. Since she didnt know it, she immdiately thought it was harder. There was another time the team was doing something in excel, I refused to let them and did it in python. That had a different VP and he was very happy with the work and happy i saved the team time.
Out of curiosity, why do you want to make the change? While the original plan was definitely to deprecate `%` formatting at some point, at this point it looks as if that's not going to happen, and with the introduction of f-strings it's pretty clear that the python devs are still not really satisfied with the state of string formatting/interpolation in python. As I see it, `format` offers some improvements over `%` formatting for more complex use cases, but is also more verbose, and for simple cases is no more readable. So for simple use cases (which should be the large majority of the time) `%` formatting may actually be the superior choice. Given the lack of clear direction here, I'd say it's a poor use of energies to try to change everything to use `format`.
Your post history is gross. 
This is not The Facebook you dipshit.
In the dict examples thrid line: shouldnt it be c = [value for key, value in **a**.items()] since a is the dict and b is only a single item from the list assigned in the previous line? 
What does that even mean?
That means you are either retarded or just stupid. Take your stupid shit back to The Facebook FFS.
I believe they have a license for open source projects.
This is what I was looking for. But well, it is a machine for research so no problem installing everything. Thanks for the thip :)
yep, you're right.
Projecting so hard. 
Dictionaries don't track order. 
While many of the things you say are true (and I'm not sure who downvoted you for it), I'm not sure how they're relevant. I never made the crazy claim that the GIL is "100% responsible for the speed difference between Python and C". You're attacking a straw man. I'm also not sure where the "4-8x speed improvement maximum" comes from. Are you assuming computers have at most 4 cores? The Gilectomy guy said in his presentation that he has a 28-core workstation at home. You can go to your local Apple store and walk out with a 12-core Mac. High core count machines are no longer just found in supercomputers. &gt; Python is slow because it is dynamic and flexible. I don't know where this canard came from, either. It seems to be a popular meme. I'd say Common Lisp is even more dynamic and flexible, and it runs circles around Python. Clojure is, too, and it makes it easy to use as many cores as I have. Even Javascript is several times faster than Python these days, and I don't think anyone would claim it lacks in dynamicism or flexibility. Single-threaded Python is slow primarily because it's a dynamic language that doesn't have a JIT. We have good evidence for this: Pypy is solidly beating Python in performance in basically every category today. &gt; Even if concurrency were free in python, people would still use Numpy, Cython, etc, because having well structured arrays of simple static data types is just plain faster. I don't know about "people", but I use Numpy because it has the algorithms I need already implemented. That's why I use all the Python libraries I use, even though 97% of them are pure Python and have worse performance than if I took time to implement them myself with an eye to performance. I don't care about performance, which is why I'm using Python in the first place. I just want something that works. 
&gt; Given the lack of clear direction here, I'd say it's a poor use of energies to try to change everything to use format. I think that's why aquacash5 is looking for a tool to do this automatically.
Nope. Just trying to understand you and your claims.
I'm not a programmer by trade, I'm a data scientist. Honestly in my experience that is probably the biggest subsection of python users. BI scripting and data science.
I think you would benefit more from looking up lists, loops, and file IO in Python 2.7's documentation. I assure you it's not hard (it's almost as trivial as print statements), and you'll benefit from learning to read docs. Here's the link I always reference: https://docs.python.org/3/tutorial/inputoutput.html If you can't make heads or tails of it through the docs or StackOverflow, hit me up with a message for help.
Is your friend trying to help you learn how to program, or trying to help you get a job as a programmer? These are two very different things. It sounds like your friend is attempting to push their opinions on you. At your stage, I don't think they can help you with higher level questions like this. I don't think anybody can. This is the most important advice I can give you: Pick a language. A single language. Don't create a roadmap. Don't plot out your ascent to competence. Just pick a single language, and learn to program in it. Learn about software development, not the programming language. Don't switch languages until you're comfortable solving programming problems in the one you started in. If you keep iterating like this, you're gonna be stuck in analysis paralysis.
/r/learnpython
This is why I have switched to spaces as well. Cutting and pasting within the editor is fine but between windows/apps can be screwy with tabs. It is a shame because I have always preferred tabs. 
Do you know if VimCompleteMe works with Jedi-Vim? I'm thinking, its possible that if Jedi-Vim is populating the OmniComplete results, VimCompleteMe should just be able to use the list, but I don't feel like installing just to find out.
String formatting says hi
No, you're not. You're fairly typical of someone that knows what code is. But people that use excel generally don't think of themselves as 'coders'. They also don't use LaTeX and look at you funny if you do. Life goes on.
I would probably start a graphic contest to customise the logo with the *humans* word changed with the most ridiculous categories people can suggest, and relevant graphics. Just to emphasise the *ironical* concept behind the *humans* word ;) 
Nice
Vim should really come with a warning, like you find on cigarette packages and alcohol advertisements. When it's all set up and you've learnt it, then yes it's completely lovely. And it's also the editor I recommend in CLI environments. *However*, and bear with me, vim isn't really optimal for usage as an everyday desktop IDE. PyCharm for example is a hundred times more convenient if what you want to do is program in Python. I feel like Vim and Emacs have become very glorified and sort of a novelty. People put themselves through the hell that is learning Vim, just so they can feel that they have mastered it. My main point and message here being; If you just want to code something in python, dont believe vim would be easy or good for you. It takes months of practice and configuring to get a workflow and an installation that you're comfortable with, a workflow/setup you could easily get after a 10 minute install of any popular desktop GUI editor.
Like this one: https://github.com/mrocklin/multipledispatch 
This is great and all, but what about integrated Docker features? PyCharm accomplishes all these features and more with minimal setup. Of course, it takes some time to learn and configure PyCharm to work with minimal mouse interaction, but it is possible.
Hm. Or use another ide and get all that too? The title got me thinking there was some mystery sauce ingredient that made python and vim in particular more powerful than other combinations. There isn't. I'm a little annoyed at that but it really didn't take that long to skim over the article either. 
Totally agree. I am a daily Vim user and I think that the bullshit Vim tough-guy swordfights have spilled over into the programming world for no good reason. PyCharm is bad ass and if that'd existed when I had to suffer through learning Vim, I would never have bothered. Same with sublime text. 
&gt; are adamant that VIM is not an IDE which this article works to refute Yeah, I've always hated this attitude when I see it on here. x: "Vim's not an IDE!" y: "Well...I'm using it like one" x: "Stop it, you're doing it wrong" y: "Why?" x: "It's not supposed to be an IDE" y: "And who decided that?" I mean, clearly its a matter of opinion and taste. So convince me why the alternative is better, but don't bother delivering edicts on how things "are meant to be" as if this is some religion.
Is it both wrists? I wonder is there a way of typing one-handed on a keyboard's number pad like how you would type on a phone. Ideally with predictive text, but even without that you can type reasonably fast. I had a quick look but can't find anything. I would be surprised if it didn't exist somewhere.
So do you think I should learn an other language after Python like html?
Frankly, I feel like I am so far into vim, it would be a poor investment to learn PyCharm. I have dabbled with it, but I haven't been able to learn it like vim. I haven't been pleased with the documentation either. 
Does weppy require less documentation compared to other web frameworks? That would be a good goal IMO. In the book it uses a door with the word "Push" as an example of a design that can be improved. Even one word of documentation can be too much. I prefer to work on code rather than documentation so this suits me. 
How hard was it to make it Material?
You will need to, but don't plan for it and don't be in such a hurry to learn new languages.
Alright thanks 
Can you fly up and down through the directory tree in PyCharm like you can with the CLI and VIM? That's the thing I really find useful. I don't really want to have to use finder and PyCharm. I want to use one and only one thing.
I can't always control what environment I'm gonna be programming on that's why I like Vim. I used to use Textmate and then I had to stop working on Macs for a particular company. Since then Vim is the only choice. IDE's sometimes require lots of resources and slow downs can occur that never happens with Vim unless you add a shit ton of plugins to basically make it an ide which I have learned is not always wise.
I totally agree that thinking about encoding should happen at the boundaries - but why the hack is ``print()`` considered not to be there? And by chosing the default system encoding as default when it comes to IO, you encourage people to forget about that! This leads to subtle errors when running a program on different platforms... sadly that python devs have made the same mistake as the Java devs (long time ago for their excuse). I would agree that it is nice to have some string type, which enforces one specific unicode aware encoding (what often is simply called *Unicode*). But then the best thing you can do is to make this obvious by really forcing people to explicitly define input and output encodings when they reach the boundaries! (I would of course suggest utf-8 as default encoding :-) )
&gt; Want to refactor rename all my variables? Boom two keys. Jump to function declaration etc etc. eh? what? ofc you can do all this in vim as well. 
&gt;a workflow/setup you could easily get after a 10 minute install of any popular desktop GUI editor. I agree with your larger point, but the 10 minute workflow an IDE provides is nowhere near comparable to the workflow of someone with a personalized Vim workflow.
Well, normally speaking a clipboard should just store what you give it, and get it back without alterations - put in tabs, tabs come back out; put in spaces, spaces come back out. Text editors do convert between tabs and spaces, but if you select tabs in your editor, copy them to the clipboard, and paste them back in, no conversion should happen, except for auto-formatting, but that should always follow the convention that your current document is using, or else reformatting parts of it would also break things, which just means you have your autoformatter set to a convention that doesn't match the document. Am I making any sense here?
&gt; You lose all the great features that an IDE provides for you. Want to refactor rename all my variables? Boom two keys. Jump to function declaration etc etc. What makes you think those things aren't possible in Vim? I have been programming full time in Vim for 4 years and I can tell you my workflow is more efficient than anybody I've worked with so far. 
The IdeaVim plugin for PyCharm (and all other jetbrains IDEs) is something you *need* to try out. You can even pull your bindings from your .vimrc with it. The plugin is maintained by jetbrains employees, and is always up to date w/ the latest IDE.
Everyone on here talking about PyCharm needs to check out [IdeaVim](https://plugins.jetbrains.com/plugin/164), a plugin that lets you use vim movements/binds/etc almost exactly like vim. You can even add "source ~/.vimrc" to your ~/.ideavimrc and get all of your binds in IdeaVim for free. You can even set up vim-style binds for IDE-specific features like refactoring. I cannot recommend IdeaVim enough. Bonus points for being maintained by JetBrains employees so it will probably not ever go stagnant. It is by far the best Vim plugin for any IDE or editor.
I found the opposite to be true. I started using PyCharm at my boss' recommendation, and installed IdeaVim. I struggled with it for several months, constantly frustrated at IdeaVim seeming to always do the wrong thing and not supporting movements and commands I'm accustomed to in Vim. I've since stopped using PyCharm entirely in favor of NeoVim with Jedi, and I find I'm much happier.
Thanks for this. Creating all these rst files always felt unpythonic.
&gt; and installed IdeaVim You can't really install PyCharm and expect it to work exactly like Vim. You're basically saying "PyCharm is bad because it isn't Vim". I mean each to their own, but you should at least give it an honest effort.
Cool, would you really recommend vim as a python IDE in front of PyCharm though, to someone who have used neither? I also use vim daily, although more as a general text editor than a programming environment (for configuration files, light coding, writing readmes/specifications/etc). I'm a software engineer (and former system developer &amp; network engineer). Thanks for actually discussing the subject, there are often too many fanboys and flaming going on when you try to discuss these things :P
One of the best days in my linux life was when i discovered nano.
&gt; People put themselves through the hell that is learning Vim, just so they can feel that they have mastered it. &gt; I can confirm, I've been tempted to dabble with it just based on how cool it sounds from the way people describe it. For better or worse, I've never really had the time to commit to it though. 
You could consider using something like `func.__module__ + '.' + func.__qualname__` as an index to `_registry` rather than `func.__name__`, that way using polymorphism in multiple modules and/or classes won't collide.
Seconding Beautiful Soup.
[Pyramid](http://docs.pylonsproject.org/en/latest/docs/pyramid.html) has incredibly well written and well structured documentation. I haven't used Pyramid for anything big yet, but anytime I looked into their code or docs, I learned something.
&gt; Quite a lot of Python nowadays runs on ARM and any GIL access solution would need to be portable to other platforms. No, it is *desirable* that GIL removal be portable to other platforms. But right now anything goes. Maybe you have the engineering resources to commit to creating a cross-platform solution, but right now I'm trying to prove that it's even possible.
I totally agree—if all you want to write is Python (or other JetBrains languages). I think Vim shines if you want to use one editor for everything, particularly documentation, configuration files, and small scripting languages that don't leverage the capabilities of the IDE. You have to weigh the benefit of "one editor to rule them all" with the cost of either not having language-specific features or having to manually configure them. I don't think this is a good state of affairs, though. I'm waiting for the day I can use Neovim embedded in IDEs so I can avoid having to mess with Vim emulation.
I'm happy with Sublime, but I find it very tempting to switch to emacs just to annoy all the militant vim advocates on the internet.
Amazing. I didn't really follow the math, but the message of looking for connections in unexpected places, of looking to the old masters for guidance and of creativity through diverse teams was immensely inspirational.
Numba integration is better - nothing can beat a single @jit annotation in temrs of integration. For explicit loops, Numba and Pythran achieve the same performance level. Pythran supports more Python/Numpy calls but it does not have a fallback to Python mode as Numba have. Numba can take advantage of GPU, and Pythran cannot. Pythran does not handle methods or user classes while Numba does.
Of course, you're right. None of this is required. I was just pointing out there are a few billion ARM devices out there that might want to be able to run GIL-free python some day :-)
If you have Jedi installed, it can do quite a few intelligent refractors. If you dive additionally use rope-vim, you can actually use all, if not more, of the refactoring features you expect from an IDE.
Jedi-Vim is the plugin by the maker of Jedi. It comes with ready-to-go goto definition functionality, virtual-env support, python documentation support, etc.
Thank god someone is saying it. I would absolutely _never_ use VIM if I had access to a computer with Pycharm, which is 99.9% of the time. I don't really see any reason to use something like VIM aside from trying to intentionally be old-school, or if you're working directly from the CLI. And even then, if you are writing quick scripts, it is much simpler to just use nano instead of trying to figure out VIM for the first time.
Ok, I use idea at work so I can imagine how the "integration" works :/ ... even for java/groovy I prefer to run the stuff "manually" from command line over configuring run and debug configs in IDEA
Early on, I liked nano a lot. The only problem was that it isn't prevalent (or so it seems). That was the biggest reason why I made myself start learning vim. By no means am I a vim expert, but I'm able to do what I need to do, and I feel that I'm getting better. Plus it's pretty cool learning some features.
You're obviously a pico fan. /s
I use PyCharm, IntelliJ, and PHPStorm at work. Most one-off stuff I will definitely do from the commandline, but when hard problems come up there is nothing quite a graphical debugger to help me sort stuff out. One huge thing PyCharm seems to have over everything else (though I haven't checked WingIDE yet for this) is the tight integration with the PEP 484 Type Hinting. Makes writing typed Python3 a lot nicer. The docstring type annotations work well in python2 code as well so you can make sure all the appropriate completions show up.
They're totally different tools and goals though. Vim is a text editor, a way to write text efficiently. Pycharm is an ide focused on helping you develop Python, so not necessarily editing it, but debugging and refactoring. Text editors suck at that, and ide's suck at editing text. There's definitely a ton of pride in learning vim or emacs but don't make the mistake of thinking there's nothing beyond that. It's like saying there's no reason to ride a motorcycle after riding a car because there's no AC on a bike.
You and every other person who use vim all believe the same thing ... No point arguing.
Sure. I use eslint in Vim (via Syntastic) for as-I-type syntax checking and linting. I also have a gulp task that that runs my Jest tests, and I have that piped in to run after my main browserify task (but actually running via watchify). I use gulp-util beep at the end of the test run, and also in case of a failure. That way I get feedback without looking at the tests run. (One beep means, things are good. Two beeps in succession means the test action bombed out (due to one error or another)). I've seen other file-watching browserify gulp setups, but this one is working for me right now. It can be sort of a chore setting it up because there are so many versions of everything, and they don't necessarily line up. There's definitely a component of "tweak / check, tweak / check, tweak / check" when getting Gulp set up properly. I think this is a major downside to building via Node, but it's still vastly superior to other non-JS build tools in my opinion. (For example, eslint pulls its config out of package.json so it's trivial for the team to share a lint config) Edit: I want to add that this was for a C# MVC4 project at my last job, but we kept the client side super separated and distinct from the web layer. At my new job, I'm fighting with a super slow Ruby on Rails asset pipeline, and I really miss my old Gulp flow!
Hey! Thanks for replying. I looked around and found this code to be working: import urllib.request #Set as appropriate userAgent = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36' req = urllib.request.Request('http://www.imdb.com/list/export?list_id=ratings&amp;author_id=ur56484104', headers={'User-Agent' : userAgent}) response = urllib.request.urlopen(req) #Save the file f = open('a.csv', 'wb') f.write(response.read()) f.close() Now, this code is working with other urls with jpgs and mp3s, but not this particular url that I have used in the example. Can you help me here? Gives a 404 error, but downloads perfectly fine in a browser.
so which timeit method does IPython use when calling the timeit magic?
Everything's possible especially with the right plugins but doesn't mean it's more efficient.
That URI gives me a 404 error in the browser too. Maybe it requires being logged in, or maybe it requires a cookie or referer header to be set.
The standard timeit method.
Huh, I'm sooo dependent on that timeit magic ...well, dependent is maybe a tad bit exaggerated. I use it routinely... wonder if there is a way to use the perf.timeit as a drop in replacement in IPython.
Thanks!
Thank you!
And once again, the "`unicode` is great" people *still* think that NFC is enough.
I guess my point is that binary isn't really used unless you're trying to deal with large problems. As such, it's important to use an efficient method. I'd say it's far more important than making it as simple as possible. Numpy for example is not simple for a beginner, but is worth learning because it's so useful. &gt; Nothing fancier than something along the lines of "in" or "startswith" is needed. How do you know when you get a byte that goes into a float vs. a double vs. a int vs. a long vs. a string? You need to pack the binary data into an int/float/string. The point of binary is that it's highly structured, so you don't need to guess and that you can mass read data. Determining the file type, that is simple. That's not really reading a binary file. Reading a 2 GB image, that requires some efficiency and just in/startwith is going to be slow. That would be a nightmare. For some meta data, it's fine.
I see no point in hyper-optimizing vim this way. It's for editing text. It's supposed to be quick to start up and simple to use; these plugins make it complicated. If you want to start managing git repos, traversing project and directory structures, and wrangling virtual environments in one tool, a nice pointy-clicky IDE does that out of the box. Incidentally, the complexity (do everything-ness) of IDEs is the reason can't get into PyCharm. I've already spent time learning how to code, edit in vim, work with a git repo, and manage virtual environments- and IDEs ask me to learn *their* special way to do all those. Why did I learn those tools in the first place? I don't see the value in learning a tool that just does what I already know how to do.
That was very educating. Thanks man. As a person who is just starting out in web scraping and stuff, it really helps when you receive such a good explanation. I'll look into login methods and devtools :)
If you have a list you can remove the `None` objects from it by doing: your_list = [item for item in your_list if item is not None] To read more about the syntax, it is called "list comprehension".
You're completely right, and it's also the point I'm trying to make. Vim is splendid as a text editor, but not at all as an IDE.
On the other hand, it's much easier to debug programs in PyCharm. They have both their pros and cons. 
Yeah, it's a no-brainer. With that setup you get a nice debugger and other nice IDE features, and the ability to use Vim keybindings. I use the same setup both at work and for my own projects.
Oh, I wasn't talking about NFKC, I was talking about cases that *can't* normalized to a single codepoint. As an English-readable example, s̶t̶r̶i̶k̶e̶t̶h̶r̶o̶u̶g̶h̶. But there are plenty of languages where this sort of thing is required for ordinary words. And *then* there's the interesting question of "what is a palindrome in an RTL-aware world?" ... but let's not get into that. Supporting grapheme clusters is the *minimum* that is necessary (and the `unicode` class doesn't help with that).
Even simpler is to just use the built-in filter: your_list = filter(None, your_list) which will remove None values and just leave the strings. (If using Python3, filter will return an iterator, not a list - if you want a list, do: your_list = list(filter(None, your_list)) 
Yeah. Palindromes in RTL languages (or worse, palindromes with embedded direction-control characters) are out of scope. Also I mentioned the shortcoming of things that don't have a single-codepoint composed form.
Yeah, and a[::-1] to get the reverse of a numpy array.
This is correct, but I think the OP's filtering of `"None"` and `"none"` in addition to `None` is extraneous - if you look at the pasted data sample, the values are indeed just `None`, not strings.
The file that you're reading from the internet may have a line ending on it, like "r0.7\n". I would use: currentVersion = site.read().strip() Also; you may want to either strip out the '#' or use a list splice to get the localVersion. Something like: localVersion = lines[1][1:].strip() This may be useful if the version number has another digit at some point.
Yes, I know. I just wanted to check the output of `is` to make sure I wasn't going crazy. (I am *fairly* high) :)
Stripping the site content worked well. Thanks! I was loading `localVersion` into another string and using `.replace('#','')` instead. I guess I'll revisit that. 
I'll stick with atom.
Sorry I'm away from my computer so I can't go into to much detail but look up into importing the make transform function. You can create a table which decodes or encodes hex values and then just input your table into this function. Hope this helps!
Oh... don't take me wrong. Your solution is completely adequate, and if its not, it can be made easily so through the measures you are proposing. I was pointing out the fact that there are times when the problem is more complicated than it looks on first glance, and there are corner cases lurking in corners.
The time I spent learning vi, 25 years ago, has paid off hundredfold. The basics aren't that hard, and it's there on every Unix machine.
PRAISE I had been using vim exclusively for years. It's a great "I need an editor right now" editor, and if you're going to be doing any ops work, having vim in your pocket for quick on-the-fly modifications is like a super-power. I set it up on Windows, I set it up on Ubuntu, I set it up on Mac, I had a gigantic vundle configuration and I saved my config file in github so that I could easily deploy it on arbitrary computers. But I wanted more! I wanted tab-completion! I wanted integrated pep8! I wanted my editor to highlight which files had changed from the last Git commit! And once I had to start custom-compiling in order to get it to edit python with any level of alacrity, I was like "nope nope nope, this is not worth it, I'm just going to pay for PyCharm." And then I installed IdeaVIM and it felt _good_. Significantly better than the Vim mode in Atom. Dammit. :3
Thank you.
Embedding neovim this times a hundred. Atom had a plugin for it like 5 months ago (didnt work well, but it might now!), but for IDEs it might be too far off. Maybe VS Code sometime soon
Spaces means it also looks the same no matter what editor you use
&gt; constantly frustrated at IdeaVim seeming to always do the wrong thing and not supporting movements and commands I'm accustomed to in Vim. I'd really appreciate it if you give a few examples?
You're right. I only looked at the code, not the file.
&gt; And then there's the interesting question of "what is a palindrome in an RTL-aware world?" This gets to the heart of the problem, and basically forces the interviewer to facepalm and say "just compare the bytes going forward and going backward". Checking palindromes is one of those things that seems like an easy/fun problem in Computer Science 101, but it turns out it never happens (or anything even remotely like it) in real life. Personally, I'd probably add this to my "hang up now" list of questions.
A popular way to do this is with an .editorconfig file, which is supported by pretty much every editor under the sun. If OP's editor is auto-formatting pasted text differently than the rest of the document, it means either: - there's no .editorconfig, or their editor is not obeying it, or: - the file was improperly formatted to begin with 
&gt; When it's all set up and you've learnt it, then yes it's completely lovely. Nope, not even close. I genuinely hate you vim acolytes. You cant even use vi?
Because someone proficient at vim is much faster at editing text than someone proficient at pretty much any else.
Do you have a local file named twilio.py, which could be masking the installed twilio module?
Works for me: Python 3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37) [MSC v.1900 64 bit (AMD64)] on win32 Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; z = list("abc") &gt;&gt;&gt; z ['a', 'b', 'c'] &gt;&gt;&gt; z.remove('b') &gt;&gt;&gt; z ['a', 'c']
Works for me in 3.5. What's the error it returns?
Agreed. Thanks:-)
That's not really true. Yes, to get tip programming with typical stuff like syntax highlighting and such, there's no learning and no fuss. But There's a whole bunch of other stuff that you need to learn; at least if you want to be as productive as possible. Edit: not sure why I got downvoted. I've been using jetbrains stuff for several years now, and it's taken a while to really master it and become highly efficient. Lots of key shortcuts to remember, lots of little utilities that you have to learn even exist and how to use, etc. They both require some considerable saddle time before you'll be truly competent.
Yes. All your project files are in the side bar. You can fully navigate without ever touching the mouse.
youcompleteme, jedi, neocomplete, syntastic.
Comparing nano to vim. Hah.
Thanks Caleb for the Package - I have just recently started using Vim-Conda.. &gt; This works fine if the same major version of python is used in both cases, but if, say, the vim python is 2.7, but your conda env is 3.5, then you don't get completions for the new stuff in the env stdlib. This is not a problem with me, as I use Python 3.5 all over..I could not properly set YouCompleteMe to support Conda and there were lot of Comments on their Issues page - that they are not going to support Conda - so I have not seen their Implementation. I am happy with how I am setup and dont find any problem as such. 
this seems a bit silly, timeit is meant to be used to compare apples to apples, so it doesn't matter as much what gets in your way, as long as you can find the fastest (which is done in two parts, 1. removing other loads on your system, 2. trying a bloody lot of times over and over to see which gets lower) Average is great if you are wondering how something runs on your system on average, but its a bit less useful for comparing, unless there are other issues there (e.g. something unreasonably hoisting up the times) edit: oh also if warmup is something worth worrying about, I'd say probably run that up to like, 20 times (as sometimes warmup in other interpreters can be a bit silly in certain situations)
Indeed. I use vim without any plugins. I also have very few custom keybindings. I just set a few of the basic settings like tab-replacement and search wrapping and I'm good to go. My shell and other command-line tools (grep, etc.) complete my IDE. I'm also experimenting with using [xonsh](http://xon.sh/) for that shell, which would give me the Python integration I don't really need in vim. There are a few kinks though (I've been running into problems with pipelines).
What specifically about it? 
So, I use a couple of plugins (most importantly easymotion) for search. So lets imagine that I'm editing a big python file, and I want to jump to some arbitrary place on my screen. I'm in the upper left corner, and I want to move to the bottom right over the name `_SECRET_KEYBINDING_LOCATION`. Now in pycharm I'd cursor over. It would take a second or two. In vim, I go `/SEC&lt;CR&gt;j` which, with easymotion will highlight all occurances of `SEC` on the screen, allowing me to pick the one I want (with a 1-2 keystroke combination) and jump to it. This is probably a wash on time, if you're using a single monitor desktop, but if you have 2, or are on a laptop, it can be noticeably faster to search this way. I can also jump to an arbitrary word or paragraph with ` w` or ` p` respectively. (I really, really like easymotion, it basically solves the one problem with vim in terms of not having a mouse) In writing this I also learned that easymotion can now support fuzzy-search and improved incremental searching. See: - https://github.com/easymotion/vim-easymotion - https://github.com/haya14busa/incsearch.vim - https://github.com/haya14busa/incsearch-fuzzy.vim What's more important to me is the lack of context switching. Search, no matter how, whether its for one word or all instances or close or far (well, excluding the same line) is on `/`. If I want to search for something off the screen, its `/&lt;WORD&gt;&lt;CR&gt;nnnnnn` and I'll jump to each one. I'll probably at some point want to go back and figure out how to get `:%s/old/new/g` bound to just `/` via some magic, but that might not be possible. Then, it does all the stuff that other things do just as well. Jump to next error is bound to `:ln' by default, I moved it to ` ]`, That to me is quicker than F2. F2 requires me to move my hand off the keyboard, meh.
The first four paragraph. Victor Stinner is a python core developer.
&gt; the comparison is more like a sedan to a formula one racecar. You sound like an old gentoo-chump.
Depends on the specific kind of job but some basics I use more often than I thought I would, and what I would ask for if I had to hire somebody: - File IO and CSV parsing - Byte encoding and Unicode pain. - Basic web applications for ChartJS / D3. Flask etc. - Sphinx and reStructured Text. Markdown if that's your flavor. - Command Line scripting and argument parsing. - Logging. Logging. Logging. LOGGING! So widely ignored but so incredibly useful. Of course there are libraries that make some of these things easier but the more native Python you know the better you can handle weird and esoteric scripts that people before you have cooked up.
https://plugins.jetbrains.com/plugin/7086?pr=
Yes; if you release it as GPL, or don't release it at all, then you can use PyQt for free. Otherwise, you need a commercial license from Riverbank (which is not very expensive, all considered).
install elpy once you do switch
&gt;There are many different ways to install VIM on Windows. Start with the official docs. wtf
Uhm... "some analysis" is really a bit vague to say anything about a whole process but here are some libraries that might help you: [nltk](http://www.nltk.org/howto/stem.html) is a library that does stemming (and other cool stuff). [sklearn](http://scikit-learn.org/stable/index.html) has lots of stuff for natural language processing and good tutorials.
[nltk](http://www.nltk.org) is the library you want, and reading the book should be a good introduction to the field as well as how to use the library :)
Debugging and looking things up are IME concerns that are much more relevant to coding than writing fast. (Unless you were referencing 'write a quick script', in which case never mind)
It is. It's similar to adding `print()` statements to part of your code to see what's going on and *how* it is going on. The only difference is the output goes to a designated file instead of on the screen.
r/learnpython
Built something non-trivial with it that is being used by a non-trivial number of people. I see a lot of junior developers think that learning a language is an abstract concept where if you have read up on a certain number of concepts and think you understand them, then that's enough to put it on your résumé. In the context of the workplace, a language is not simply a set of concepts. You only really know a language once you've used it for real projects. If I see Python on somebody's résumé, I expect to be able to talk to them about projects they have built with it. If you haven't built anything real with Python, then you can't truthfully claim to be a Python programmer. If that's you, then stop thinking about concepts, and start building things. You'll soon discover which concepts are necessary. 
`nano` is installed by default on every Linux system I've touched. That said...I just install `vim`.
So you're saying it took you 17 years to navigate backwards by word in your editor? That took me 2 days to figure out in pycharm? Stop glorifying vim, it's a very good cli editor, nothing more, nothing less. I'm using it myself (and have been for about 10 years), but it's not magically "amazingly powerful" just because its unneccesarily difficult to master. 
&gt;&gt; the comparison is more like a sedan to a formula one racecar. &gt;You sound like an old gentoo-chump. And? As a Gentoo user, I don't muck with CFLAGS or LDFLAGS. But USE flags are incredibly helpful and powerful.
Sure, I can use `vi`. But why would I want to, if I have `vim`?
Good for them! Setting an example for progressive development. 
&gt; I don't see the value in learning a tool that just does what I already know how to do. Productivity. PyCharm gives me the tools to do things faster, and with less action on my part. For example, working with Git. Sure, I have learned the Git command line, and it's super useful to know and understand how that works. But now I know it. I don't want to use the Git CLI for every-day mundane usage. I just want to commit shit to a repo. The VCS stuff within the JetBrain products makes that ridiculously easy on my part, and I don't have to type anything. Have you ever had to move some changes to a different branch, or make a separate commit, or stash a select few files, or something like that? How long did that take you on the command line? For me, that would take mere seconds as I click some checkboxes. How long does it take you to switch branches, or merge branches, or push/pull, etc? All within one click away for me. While I do understand and appreciate that a full-blown IDE is not for everyone, I often wonder if ignorance and/or stubbornness is a big part of that sometimes. I used to use plain jane text editors, like ST2, for everything. It worked fine, I could write code. But once I started using IDE's my productivity just skyrocketed. Code completion, code navigation, context-aware stuff, database tools, VCS tools, testing tools, refactoring tools, syntax highlighting, code generation, deployment tools, etc. I use all of these things every single day, and I couldn't do my job as effectively if I didn't have access to these tools. I find it hard to believe that one could match the productivity and efficiency of an IDE by only using simple editors. So, that just seems strange to me, choosing to ignore things that make your job easier.
&gt;So you're saying it took you 17 years to navigate backwards by word in your editor? That took me 2 days to figure out in pycharm? I'm saying I never really *needed* to. Ctrl-(left arrow) works in most GUI environments. And that's what I did in those. But when I reflexively type `^/&lt;txt at change enter&gt;&lt;enter&gt;`, to jumo to an arbitrary point on the line, and it's that much faster, why navigate by word? Seriously, the only reason I learned `b` was so I could write `d3b` to delete the last three words in three keystrokes. So, not for navigation, but for using the navigation command as part of a compound command. &gt;Stop glorifying vim, it's a very good cli editor, nothing more, nothing less. I'm using it myself (and have been for about 10 years), but it's not magically "amazingly powerful" just because its unneccesarily difficult to master. I'm not glorifying `vim`, here, you are; you're the one saying it's power depends on a difficulty to master. It doesn't. You learn its grammar, and the rest falls into place *as you need it.* Its grammar is pretty simple; *&lt;VERB&gt;-&lt;OBJECT&gt;*. Learn the verbs and objects as you need them. Don't assume you have to have memorized all possible sentences before you're fluent in it; that's like assuming you have to know every word in English to be a fluent English speaker.
Totally agree. I am embarrassed how long it took my stubborn old brain to come around to this idea. 
I recommend using the built-in logging module for logging.
&gt; Vim and Emacs have become very glorified and sort of a novelty In the world of editors, Vim and Emacs have become what sniper rifles are in FPS games. Even if other weapons are infinitely more practical in certain situations, people still want to use sniper rifles exclusively because sniper rifles are cool.
Nope. You, sadly, have convinced yourself that something is harder than it is.
I've switched to emacs after being tired of Pycarm's java-eat-all-available-RAM thing. Now i can take part in those holywars :)
I've been using VIM for pretty much as long as I have been using computers (so, since the late '90s). It's my goto editor. I've edited quite a bit of python code in it. But I wouldn't consider myself a vim power user. I almost never customize it. I normally use it for one file at a time, and primary do copy/pasta using the mouse (from some other app into vim or vice versa). Now that I do a lot more programming than I did in my sysadmin days, I use PyCharm as my IDE. Every once in a while, I'll encounter one of these tutorials and think "yes, I use ViM, I should totally do this!". I could leave all my code on my desktop and be more effective coding remotely. So I'll get vundle installed, and I'll start mucking with my vimrc file. But it gets frustrating. One of the first things that get frustrating is when you start mucking with auto-indent settings. For example, on this tutorial itself, once you even enable `filetype plugin indent on` suddenly, you can no longer take a block of code from that page and paste it into vim. So you start having to look up commands like `set noai` and `set paste`. Sure, that's just one small bit, but it's one new thing I have to research, and one more step in things I do daily. All the changes keep adding up. 
For internal projects at work I've taken it a step further and decided this year all the new Python code I'll write will be Python 3.5+ The typing module as well as the concurrent programming features built into the standard library (concurrent.futures and asyncio) are enough for me to not want to use Python 2 anymore. Some coworkers are already complaining since it is not installed by default on OSX and RHEL but I'd rather spend my time showing them how trivial it is to install it on both (homebrew and IUS respectively, hopefully EPEL soon) instead of supporting 2 and 3.
In `docs/user_guide/plugin_development/basics.rst`, there is an out-of-date reference to "three sections" as [python] is no longer a config heading. (my small contribution!) And really nice to read about dropping Py2 support! 
Every single question about an IDE on /r/python ends up with you claiming vim uber alles. Its not even an IDE.
The python community is global and incredibly varied. You will get basically the same opinions as the overall population, but the distribution of opinions here might be slightly shifted from that. Also, this post is not python related, and I don't think anyone really cares to discuss US presidential politics instead. Save it for the water cooler.
&gt; - flask has been replaced by bottle (sorry flask no py3 support, no future) http://flask.pocoo.org/docs/0.11/python3/ I get that Flask may not have supported Python 3 at the time, but it does now. I'm not saying the authors should go back and redo all the work to use Flask but at least the second half of the comment from the changelog should probably be updated.
A lot of people are pointing out there are better IDEs out for pure Python programming; a large part of my job involves Python, but the servers on which I have to write my code don't have a lot of fancy IDE capability (and installing them yourself can be an access / permissions nightmare). Customizing the hell out of VIM is actually the easiest solution for me.
Indeed I gonna add an "edit: ". I vaguely remember that at that time the project was not only incompatible with Python 3 but *opposed* to it and this was what triggered the comment IIRC.
the pandas datetime library would meet your requirements, set a date range for those specific frequencies (weekly, daily, custom), then can apply mask to a single date range. http://pandas.pydata.org/pandas-docs/stable/timeseries.html 
If you want to go the conservative route Gary Johnson may be what you're after. Though I agree with the others that this isn't really the place for politics. Not convinced the office water cooler is either...
What about writing tests? 
That's a really great way to put it. I have a couple project ideas that I like that I don't think are really that relevant to a workplace but they give me a good foot in the door to other projects. 
Same here. We're launching new microservices as 3.5-native. The old code can stay on 2.7 until we get around to it, but there's no point in starting new projects with it.
I find it harder to mentally parse your version. It is longer and has an additional unneeded statement that makes it more complex.
I tried PyCharm and I really wanted to like it. But it didn't fit my mindset. It felt like it was just a GUI of everything I do on the command line. And the advantages it brought, I don't typically find myself using. For example, finding all subclasses of something is neat, but I've only seen used once, and that's something that can imitated with grep, and if I need to open all those files, I can just capture that output and open them with vim. Iunno. PyCharm isn't for me despite how many people have told me just how much more productive I'd be if I used it -- it isn't, I used it for two weeks and wrote a fraction of the code I normally would. I'm sure some of it was, "It's new, so it's bad" but I just found myself not getting the point too often. 
http://prntscr.com/bf420c what does this mean
What version of Python did you install? Can you post a link to the script you're trying to run?
Sure, but this guy is describing the practice as a code smell. So the particular example isn't that relevant as he's making the generalisation that this is a bad thing to do.
Good point. Come to think of it, I think the reason I do it with `from` is similar structure to other nested imports. If I wasn't using an alias I would do `from matplotlib import pyplot` rather than `import matplotlib.pyplot as pyplot`. So it makes sense to do it the same way with an alias. If I want to switch between using an alias and not using an alias, I don't also switch the structure of my import statement. I just realized this is probably my motivation. I don't think it's a *good* reason, readability is more important, so I'll probably switch to the traditional way.
&gt; It felt like it was just a GUI of everything I do on the command line. You say that like its a bad thing. 
It is for me. Maybe not other folks. 
Thanks! I always forget those lol
Meanwhile at my work I'm considered a freak for not doing everything in some combination of power c and perk. I've been trying to get 3.4 installed on a production server and the admins insist 2.6 is fine.
For using a CLI editor for the first time? Yeah. Nano is a bit simpler imo.
To expand on this answer, you need to make sure that the item in your list has a equivalent representation as the string you are using in remove. So, if your list is a bunch of custom objects, they would need an __eq__ method that can be called, since that is the basis the list structure uses for determining to a item is in it (equality). Also, the remove function removes the first instance of the item found, not all instances, so you may need to consider that as well. (On mobile in case formatting is bad).
That is pretty great. Thank You 
Oh dear : Python 2.6 to end support with 2.6.9 in October 2013 "getting hacked or stumble on a bug or an incompatible dependency that will never ever get fixed" is apparently fine by your admin ?
The python ecosystem is so large that is impossible, even for an experienced Python engineer to cover it all, so Irecommend to specify your area of expertise (e.g. Scientific, Web, Textual analysis). If you're a beginner be honest and say "Python (beginner)'
Those are definitely on my list as I want to become a data scientist. 
That's a good point. My focus would be data scientist. 
RHEL, for one, still provide security support for Python 2.6, even if the CPython core developers do not.
Well they are running 2.6.6 to be exact. It is a problem.
THAT'S COOL! Thank you for the idea!
Why does it seem like people have forgotten how to use google?
LOL
One uses a package manager, the other does not. 
The nice thing about standards is that there are so many of them to choose from.
Honestly I've seen and gotten the opposite advice. If you're familiar with it put it on your resume. Your work experience will say what you've spent the most time on. The caveat of course is it depends on what jobs you're going for. Look at the job description, if it doesn't say fluent in Python, the job isn't primarily about writing Python, and you do have a language that you're strong in, then you should be fine.
Yeah, by default it seems that whenever you run something in PyCharm it automatically appends the root directory of the project onto sys.path. One big mistake to avoid is not learning to use pdb properly -- PyCharm's got all the fancy visual breakpoints and helpers for debugging, which is nice, but it won't help you if you need to debug something you've ssh'd into. 
Right? The `unittest` module is more important than any of the above 6 points. You could have memorized everything about those, but if you've never written a unit test then I'd prefer someone who has.
The conversation would try to find out if they understood the core concepts (from having used them before), and knew where and how to apply them -- things like classes, builtin data structures, generators, and decorators. Basically, if you can get to it without "import", it's probably fair game. Unless it's obscure crap that nobody really uses (memoryview is neat but I've never seen it used in real life). I'm not terribly interested in trivia, as long as they know where to look it up. (Knowing which builtin data structures are mutable is important. Knowing the exact name of the string method to convert to title-case is not.) Beyond that, Python is a pretty mainstream language, so general programming skill is more important than Python-specific skills. Someone who's great at designing classes and assembling unit tests in C# or Lisp or Rust or is probably going to be great at it in Python, too.
Is this surprising? There's at least that many HTML parsers in C, and both HTML and C are more-standardized and better-documented than Python or Markdown. 
Unsurprising. Armin Ronacher is not a Python 3 fan by any means. To be fair, Flask supported Py3 long before Django did.
This is from 5 years ago but it probably includes most of his grievances: http://lucumr.pocoo.org/2011/12/7/thoughts-on-python3/ Edit - if you mean why Flask supported Py3 before django, i suspect it's just a matter of complexity and size: django is just so much bigger than Flask in terms of pure codebase as well as in terms of backward-compatibility requirements.
Very simple strategy using PIL: 1. Read image from file 2. Loop through and call getpixel() on each pixel to get its colour 3. Compare its r/g/b to your desired colour. If it falls outside some threshold of your desired colour, use some formula to convert to greyscale (e.g. set rgb to the average of the 3 values) and use putpixel to replace that pixel 4. Save image There are probably much more efficient ways, but this is a simple way to get what you want.
Data scientist sounds glamorous doesn't it but unless you are very, very clever and ridiculously shit hot at maths and logic and I mean shit hot, you will never be a data scientist, you'll just be a data analyst with an overblown job title. What I am trying g to say is that in reality, the workplace needs far fewer data scientists than it needs coders and analysts. Some career advice if you are pursuing that route. No 1. Make sure you are data literate. 
Yeah, data scientist is the end goal. Data analyst pays okay and seems to be the intern version of data scientist. I want to play around with data and do cool things with it, not manage databases and run basic reports.
How to make it run at launch in my terminal? I've tried: ``` if [ -z "$XONSH_STARTED" ]; then export XONSH_STARTED=true; xonsh; fi ``` in .bashrc, to no avail. Any suggestions? **Edit** Nevermind, anser is here: http://xon.sh/linux.html#additional-setup
From http://pythonhosted.org/vlermv/acid/#locks &gt; Consistency and isolation could be improved with locks, which could be implemented inside of the vlermv module. This hasn’t been an issue for me because I have been able to design my directory structure such that the race condition scenarios never occur. WOW!
I haven't had to deal with big files myself. But from what I learned from others, if performance really degrades a lot turning off syntax highlighting can make a massive change and make it work very well even with huge files. The one problem I know about with vim is that it has big problems with very long lines due to the internal data structures used. For example using big minified JS files that are just a couple insanely long lines can be bad performance wise.
That's an order of magnitude less than the number of markdown dialects, isn't it?
Good point!
Enough that you can search stackoverflow for the solution to a problem and know it when you see it.
I use [Django REST Framework](http://www.django-rest-framework.org/). It's a bit more explicit, thus easier to understand/debug.
Don't listen to this guy, DRF is a garbage fire
How beginner are you? If you can, you can start with scraping stuff out of the internet. Get as much data as you can, then visualize it and try and find similarities etc
Guys, how to re-run the first run configuration?
Did you create an issue on github?
Wanted to get confirmation first. Looks like it's already there. See my edit.
RHEL supports Py34 and Py35, just use the SCL packages: https://www.softwarecollections.org/en/scls/rhscl/rh-python34/
[removed]
 if x = 4: is a SyntaxError in python.
It really depends on how green you are. If you feel up to it I'd suggest trying to pull data from weather.gov and make your own little weather logging app. Then update it with a graphical user interface using tkinter. 
I know this has been a while ago, but AFAIK, xonsh works fine on Windows (with Python 3.4+). A couple of our core devs use Windows, and I have installed xonsh on Windows before (both within Cygwin and without) and haven't had any problems. If you want to give it a shot and you do happen to run into trouble, we have a mailing list and chat rooms on IRC and Gitter.im if you have questions.
Depending on your definition of simple, maybe use this and change it to something your group finds interesting. http://www.danielforsyth.me/analyzing-a-nhl-playoff-game-with-twitter/
Python needs tests more than statically typed languages. With statically typed languages the compiler will catch certain types of errors, but with python you don't have this benefit.
I just did a project through uni to build a very simple chat room program that runs in the terminal. Be good to learn python prgramming and get an understanding on socket programming (communicating with other programs over a network), python data structures and potentially SQLlite databases if you want to extend it further. I can give you the project brief and starter code (which does some of the socket stuff for you). I've completed it so I could answer any questions you have if you get stuck? 
You could try playing through [INJECTION](http://schilcote.itch.io/INJECTION) (disclaimer: I am the developer of this game.) It's a bit like Untrusted if you've ever played that; basically there are puzzle levels and you have to edit the game world via the Python REPL to solve the levels. So for example, in the first level you're trapped in a cell and have to find out what attribute on the password-protected door stores the password and change it. Eventually you work your way up to writing functions to drive a robot through a maze, and reprogramming a laser turret that was shooting at you to protect you from incoming rocket launcher shots instead.
Is there something in your life (or work) that you do all the time? Start there. For me, the best motivation is solving a real world problem.
Write a wrapper script that runs a command and parses the output into a Python data structure.
In his [Beyond Pep8](https://www.youtube.com/watch?v=wf-BqAjZb8M) talk, Raymond Hettinger gives a pretty brilliant example of wrapping a foreign API to make the result "pythonic". Things like giving collection wrappers`__len__` and `__getitem__` methods so it can be an iterator.
Which is where the whole thing about `/` always being search comes in. I don't want to press F6 to search sometimes, and CTRL+F to search other times, depending on what I want to search for. That's kind of silly. So I press `/` to search, and I find things.
[Turtle](http://openbookproject.net/thinkcs/python/english3e/hello_little_turtles.html) is a fun python package to play around with that really helps to visualize how programs and programming works. You control turtles and can move them around your screen.
So I had the same dilemma once I started running hundreds of windows through hundreds of loops. PhantomJS is the tool you desire, it lets you run Selenium headless using the PhantomJS() webdriver. Full functionality, with even better speeds.
&gt;samples from a binary file I have to read often Right, if OP is constantly opening up the file during the loop, I'd hate to say that maybe the code is due for some optimization? Although, if this is not the case: maybe your loop is too far outside of your processing to benefit from JIT optimization? ex: def main(): do_something() for i in range(100000): main() vs def main(): for i in range(100000): do_something() main() I've had problems with do_something() if it does anything fun with libraries (e.g. for processing binaries) or for IO that takes a non-trivial time to read. Granted, I know that I know nothing, so grain of salt?
Thank you, this is exactly what I'm looking for! 
No I don't.
On the same note - When I started coding, I was frustrated, that my school's weekly schedule was only available on their website. So I build a quick scraper with BeautifulSoup and put it in my calendar with Google' calendar API for Python.
Automate something in your home. I bought a raspberry pie, connected it to a sensor and a radio transmitter and hooked up a table fan to a radio receiver. Now I have an automatic temperature-controlled fan.
"robut? Tell them I hate them."
Really? I made a calculator in c++ for Linux terminal, kept on adding useful features and it is one of the most useful programs for me now. 
That looks super cool, can u give me a github link, or tutorial on how u dit it ?
Run it from the command line. This way at least you will be able to see the error message.
+1, great tool. Allowed me to scrape pages that weren't scrapable otherwise.
The best libraries for what exactly? Why the Chinese libs specifically, and why no Beautifulsoup, matplotlip, etc? There are a lot of redundant libraries, but that is a good thing. We need people to attempt improvements. Maybe they won't come out on top every time, but we still need the attempts. Out of one of those attempts will come an improvement to a current library and it will take its place as 'the best.'
Good... for you?
Can also run selenium in a docker container but that may be overkill for this simple use case 
Thanks for your replies! Here's my best-library-standard: Project star, code update freq, project member are always considered, google trend is a good factor too. However, many factors are hard to be quantitative (or there isn't any available data), like expansibility/ease-of-use/performance/community-activity, etc. So you can see the best projects are chosen by hand now, and a little "based on experience". I've used Python for many years, and I've known some good libraries which I'm sure they're the best, but some others I'm not so sure. You can create a PR if you are interested in this project, and I will keep the list always clean and best.
Perhaps have them write a simple text-based RPG. I taught a friend C++ by helping him write one, and it looked something like this; [Please pardon the language - we were high school kids!](https://puu.sh/ppNCH/0349fb26d4.jpg) Variables like health, XP, level are all easy to understand, and the importance of math becomes clear when you write the level-up curves. The use of objects (in our case, we just used ``struct``) is clear when it comes to separating Enemies and Players, and functions named like ``levelUp()`` or ``gainXP()`` make their purpose clear too. A simple game like that pictured allows for all of those elements of programming to fit together in a way that just makes sense.
It's impossible to say without seeing the code, but chances are that you're reading the file bound - in this case no JIT (or native) would make it any faster. Post the example somewhere and we can have a discussion
That was entertaining.
I belive You want to write wrapper on existing rest API so i reccomend You to watch: "Laura Rupprecht – Describing Descriptors – PyCon 2015" for handling api response. The second thing i recommend is to look at Hammock source code https://github.com/kadirpekel/hammock https://github.com/kadirpekel/hammock . You should also know how generative classes work http://derrickgilland.com/posts/introduction-to-generative-classes-in-python/ .
I would have thought from the way I was reading that way more time was spent building data structures than performing actual device IO. But maybe I'm overestimating the low level buffering performance. I can throw the data into a SpooledTemporaryFile tomorrow and see how that goes.
Binary blobs from a PLC programming environment. 
Are they tied to a specific code build? Are the blobs build artifacts? Are they stored off your local box? Restore from backups Tested? I only ask b/c it sucks to have a false sense of security w/ custom backup solutions Only to have the backups fail for some reason :(. Also if they are reproducible build artifacts it may be better to just consider storing the code using a source control tool instead of storing a bunch of binary blobs which can always be reproduced.
I'm confused, what exactly are you talking about? Could you provide more information as to the context of your post. Links? 
If they're gamers you might want to start off on the book, [Learn to Program with Minecraft](https://www.nostarch.com/programwithminecraft). It's aimed at kids, but isn't condescending and seems like a good introduction to Python 3. I poked around at it a bit to evaluate it and believe there is a lot of merit to it. It's fun and gives the reader encouragement to get creative by applying what they've learned in different ways. A game environment is also an excellent place to learn to think about Objects as... objects. I know a lot of people struggle with OOP when they start, but in a game where objects are *tangible* it gives your brain a kickstart into thinking this way. I personally learned C# and OOP from programming mods for Asheron's Call (an old turn of the century MMORPG). My brain already understood that a health potion was an object; it made sense that it would have its own set of attributes and methods of use. Likewise, a monster is just an object that can run around (**M**obile **Ob**ject, which gamers already refer to as a **mob** dating back to MUDs). So a monster object has its own set of attributes and methods. 
It was new news to me. xkcd graphs, Comic Sans fonts, corporate ladder here I come!
I'm pretty sure this list will be much more popular if you rebrand it as "recommended first choices", not "best and only". For almost all of these categories, I can think of a use case where the library you suggest would be a worse choice than some other, lesser-known library. For example, for general machine learning, `scikit-learn` might be a good source of solid implementations for many tasks, but it just does not compare in breadth to using RPy2+caret. For deep learning, [`theano` beats `tensorflow` in a certain class of models (e.g. LSTM)](https://medium.com/@sentimentron/faceoff-theano-vs-tensorflow-e25648c31800#3fea). For the "concurrency and networking" category, we're recently tested [uvloop](http://magic.io/blog/uvloop-blazing-fast-python-networking/) and it runs circles around gevent in our use case. And it goes on.
Reasonable. I'll update the desc later.
I am also new to python and currently I'm making a program to open up a hotspot. The softwares that I get online have Ads and annoying shit so I have decided to make my own hotspot software. I'm also building a program to start/stop services that take too much RAM and CPU. Service name has to entered manually. Both are in windows.
Assuming the API is purely RESTful, [Slumber](http://slumber.readthedocs.io/en/v0.6.0/) is my go-to client wrapper. 
care to elaborate?
I had a blast with turtle!! I was about to give up on programming altogether, but everything just clicked for me during the turtle module of my class.
I wanted to do something just like this. But with a window AC unit. Mind sharing some pointers?
Check out http://www.cs.armstrong.edu/liang/py/test.html
The GIL also is no problem for I/O bound threading. So in that scenario you don't even need to worry and use celery and friends, you simply use threads.
No lie i immediately started using it for work. It is shit for bar charts, or for charts with more than 5 lines, but everything else it looks freaking amazing. So impressive. Now i wonder if Randy is pissed someone reverse engineered his style with so few lines of code...
Thanks - but I think this seems to be a little wonky for getting red only: from PIL import Image import sys im = Image.open("test.png") pixdata = im.load() for y in xrange(im.size[1]): for x in xrange(im.size[0]): if pixdata[x,y][0] &lt; 50 and pixdata[x,y][1] &gt; 25 and pixdata[x,y][2] &gt; 25: r,g,b,a = pixdata[x,y] brightness = int(round(0.299 * r + 0.587 * g + 0.114 * b)) pixdata[x,y] = (brightness,brightness,brightness,a) im.save("output.png") 
yes - somehow i have to find a way to find a way to express the pure range of red numerically for RBG
 Python 3.5.1 (default, Jan 22 2016, 08:52:08) [GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)] on darwin &gt;&gt;&gt; import matplotlib.pyplot as plt &gt;&gt;&gt; plt.xkcd() &lt;matplotlib.rc_context object at 0x10e98fba8&gt; &gt;&gt;&gt; This is probably embarrassing, but is there something else I was supposed to do?
Thanks, knew it was going to be embarrassing.
are you saying you like the scatterplot look? just asking, as I don't know what plots you make
You have this model already. Just use concurrent.futures. In addition your examples are I/O bound, so in your case GIL does not really stand in your way. The GIL is a problem for situations that your code is CPU bound. Also processes, threads and coroutines are orthogonal concepts and you can combine them together. For example recently used asyncio together with threading. I set up multiple asyncio loops, one per thread. I probably would be fine with a single thread, but use threads to separate different components.
I haven't personally used it, so I can't speak for how well it works, but I believe this is what [pex](https://pex.readthedocs.io/en/stable/index.html) was designed to do.
I'm a Linux guy and I've basically replaced my bash scripting with Linux. Helped me learn the os module and also get comfortable with more complicated syntax. I don't need to do that anymore because of xonsh but it was still useful. Also making stupid little cli things to automate common tasks. Edit: for the record, I've been a Linux user for a long time, long before I started programming. This really helped me appreciate things more. It might not be exactly what OP is looking for but maybe it'll inspire other users. 
C does Edit: also [rust does](https://doc.rust-lang.org/book/raw-pointers.html), although you should acid using them. And of course an assembler, but that's mostly why C has it in first place.
I use d3nv and bootstrap dateselect field for various charts. Haven't found any dashboarding apps also. 
what exactly is slow? I see zero difference between this and zsh with oh_my_zsh or bash with git prompt.
How many Project Euler problems are worth doing? They seem to be very different than building a small project.
Alright, but how do I make my graphs go hilariously off the chart? So far I've only ever seen matplotlib's "inside-the-box" experience.
I second number 2. A simple bash wrapper would suffice perfectly. If they only need to run it once you can just `pip install module` in your script. If they are going to be running it multiple times then some logic to check if it exists would probably be better.
You could be missing setuptools: apt-get install python3-setuptools
He definitely has a font for automated rendering, but you can generally tell when it's used. (It's only used in the 'interactive' ones.) I think he hand letters all of his "normal" strips.
Your website looks really cool, love the style :) 
I haven't tested that scenario! I'm not using threading so I imagine this won't work as smoothly as I hoped. Will update shortly. 
Your secret is safe with me ;)
You really should ask questions like this in /r/learnpython In Python 3, you can supply an 'end' argument to the print function to replace the default ending character, a newline. This can be an empty string. In Python 2, you can use system.stdout.write to have precise control of output. You can do this in Python 3 as well, of course.
https://docs.python.org/2.7/library/functions.html#print `print("foobar", end="")`
You really should ask questions like this in /r/learnpython In Python 3, you can supply an 'end' argument to the print function to replace the default ending character, a newline. This can be an empty string. In Python 2, you can use system.stdout.write to have precise control of output. You can do this in Python 3 as well, of course.
quite vague, but something with pygame
Python does seem like a good option for what you are doing. The [built in csv module](https://docs.python.org/3/library/csv.html) will probably help a lot if you have to parse different csv formats.
Oh man. No idea. Congratulations, you've just made me disappointed in a thing i thought was amazing!! *edit: just like my parents...
Yall need to start working in ipython yo. No inline plotting is wack. Incidentally... How do i reset it back to nonxkcd? I only know plt.rcdefault() but that sets it to some other ugly setting and not the nice one i start out with in ipython
Is there a relevant xkcd for this?
I've found installing from requirements files to be unbearably buggy. Dependencies being handled incorrectly seems to be more rule than exception. 
Exactly - that dashboard would be a massive project.
Here's the talk video: https://www.youtube.com/watch?v=5gy6svL1DuU
#1350 (Lorenz) has a font in Randall's handwriting style. Does that count?
In that case: name = 'ME' day = None with open('output.txt') as f: for line in f: line = line.strip() # remove the ugly new lines if name in line: break # stop the loop if 'day' in line: day = line print name, day, f.next() # print the day and the next line after your name Hope this helps a bit!
Thanks, will try tomorrow and report back! 
I'm currently working on a program that needs to take in user input and run eval on it. How does your game safely check if the user input is valid? Did you manually code in removing portions of malicious looking code based on keywords, and then running it?
Next time you should ask in /r/learnpython but since I'm here anyway, i'll give an answer anyway. As /u/DefinitelyNotRed has already said: print("foobar", end="") does the job. So these two lines: print("Loading---", end="") print("-", end="") will print: Loading---- But the next thing you print will be on the same line added to the end so don't forget to do this print("\n", end="") or print("") What's also fun to do is this: print("Loading\r", end="") print("Loading-\r", end="") print("Loading--\r", end="") These will all be printed on the same line because of the "\r" charachter (carriage return). This means that the cursor will be set to the begin of the line so the next characters that get printed will overwrite on its current line. 
Sure! 
Thanks!
Post the top post of all time from a random subreddit every $TIME?
You would have to post the output you got. Perhaps using HSV would be a better way to get red values, as said by /u/kindall 
I have no idea. It worked out of the box for me. I doubt its a font thing as youre not getting the line distortion. You may have some overriding settings, or an old version of python/anaconda/matplotlib
Not sure if it helps but im on a mac in case it is a font thing
Take a popular news account(CNN, Fox, etc) and repost their tweets after passing it through [this filter](https://xkcd.com/1288/). Should get some interesting results :)
[Image](http://imgs.xkcd.com/comics/substitutions.png) [Mobile](https://m.xkcd.com/1288/) **Title:** Substitutions **Title-text:** INSIDE ELON MUSK'S NEW ATOMIC CAT [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/1288#Explanation) **Stats:** This comic has been referenced 246 times, representing 0.2151% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d46q1cd)
I've made 3 twitter bots: 1. Plays tic tac toe with someone: https://twitter.com/tic_tweet_toe 1. Tweets the HN front page articles: https://twitter.com/hn_frontpage 1. Tweets the current Aturan date, from the Kingkiller Chronicles: https://twitter.com/aturancalendar Ideas: 1. Plays minesweeper with a user 1. Someone can register their city, and tweets at them once day the weather forecast
Reconstructed? Did randall really make a new font just for his comics? I feel like he would have just downloaded one from the internet
That's even better than what I came up with and it shows that python's functions are objects too! My original improvement was `-~int()` to get 1, which was slightly shorter than `len(" ")`.
It's pretty awesome. I always hated the default git logs. Thanks :) .
There are other ways to do it, but this is the best way. 
From what you've said, I'd certainly go with Pandas for this - it's basically Python's answer to spreadsheets. The downside is that while Python aims to have "one obvious way to do it", Pandas is fairly specialised - it does *many* things, and it's not always clear at first which you need. Still absolutely worth it though, especially for systems you'll keep working with over time. My suggestion for how to approach this: do it in small parts, which can be connected up later. You have the advantage of a working system, so there's no rush and you can test things well before use! The parts I can see are: - Identify which format the `.csv` file is in (manually) - Run the converter for that format - Upload converted data - Query as required Basically any of these could be done separately; I suggest starting by writing a few converters, a tool to check that a file is correctly converted, and maybe an upload function.
I dont think it capitalizes letters for you mate. Didnt notice that in any of my tests. Try it on ipython maybe?
Good to know ! I'll fix that. `super().__init__()` is just an habit, so I never forget to call it if I change my class hierarchy
Really though, you should just make a git repository for it and include a requirements.txt file and be done with it. 
Make a program that uses iteration to find the cubic root of any number (negative and postive)
Will do, thanks
Unfortunaly there are already a number of mature projects doing this: click, begins, docopt, etc.
OK, I don't personally care about these three features, but I'm upvoting because I'm increasingly using and appreciating the work put into flake8.
A bit hard at first but with the [material design lite](https://getmdl.io/) library and some daring choices it went really well.
Converting client specific csv data into something useful and standardized is a huge part of what we do at work. Some general suggestions: - avoid client specific code or settings as far as humanly possible. Much better to run all data through unnecessary normalization steps than to have lots of spaghetti formats. This also has the advantage of being able to handle customers changing their file with no warning and for no reason :P - don't depend on the order of columns in the CSV. Depend on the headers but only after heavy normalization (non-braking space is a killer, as is zero width space). - when something doesn't match print/trace the repr of the column header/data. See previous point. - it's ok to build a system that just handles the easy 80% first then slowly eat the rest (we made the mistake of building a system that should be able to do everything the old legacy system could do. Much of that code was thrown away or made obsolete by better ideas)
[removed]
No idea what you're doing but this sounds like you need to rethink your basic design. If you're trying to remote control something there are two valid options: 1. Create a client/server setup (or client/client) where you run a small local app that will send complete "commands" to your remote application (e.g. using a REST API, depends on what it's doing). 2. Log onto the remote with SSH and use the app as if it were local. Or am I misunderstanding what you're trying to achieve? Other than that r/learnpython will be more likely to give answers to questions like this.
It's been 19 hours so I doubt I'm spoiling anything here, I solved it by using a try/catch and division by 0.
Unfortunately, it doesn't support LaTeX so I can't write math expressions in labels.
JFYI, Debian already has RFCs in default repositories. Simply install the `doc-rfc` metapackage and read them just like the regular man pages.
Well, it's not impossible to do it in matplotlib using [annotations](http://matplotlib.org/users/annotations_guide.html), though I would recommend it only for simple stuff like your examples.
So I re-profiled my code and found that it wasn't IO bound, but I was instead (stupidly) constructing a complicated struct.unpack() format string on every read. I memo-ized that formatting string, and boom, things went much better for the interpreted python. I'll check PyPy again tomorrow to see if it now makes a performance benefit on top of the optimization blunder.
Sounds like you are not cpu bound, and its in fact the io that's taking the bulk of your time.
[removed]
For the differences see [here]( https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-2-release-candidate-1). It is also at the "full changelog" link in the OP link.
Why are you using Python for this? Does it need to be integrated into an existing Python workflow? Matplotlib supports [LaTeX text rendering](http://matplotlib.org/users/usetex.html), but I don't think that Matplotlib is really designed for arbitrary graphs like this. +1 to the [Tikz](http://www.texample.net/tikz/) suggestion. It has a learning curve and it's as quirky as the rest of LaTeX, but if you need this level of fine control you'll definitely be able to achieve it. [Graphviz](http://graphviz.org/) is simpler to use. There's a [hacky way to insert LaTeX snippets into dot graphs](http://brighten.bigw.org/projects/ladot/), as well as the [Graphviz to LaTeX converter](https://dot2tex.readthedocs.io/en/latest/) and [Python to Graphviz converter](https://github.com/erocarrera/pydot) that /u/farsass mentioned. I suggest that you try multiple approaches to render your simplest graph, and decide what works best for you. 
http://docopt.org
I'm into data science and machine learning so I use matplotlib a lot. I generate eps files from python and then import them into my LaTeX papers. I don't like (La)TeX very much so I try to avoid packages like TikZ whenever I can. I wish there was something as powerful as TikZ for python.
You might want to look at eval for the '=' one. EDIT: nvm, it doesn't seem to have eval
Maybe check out the products from rogue amoeba. 
Thanks to: ademus4, tdammers, DefinitelyNotRed, TheBlackCat13, the_hoser, and TiLorm.
I wrote something similar a little while back called [defopt](http://defopt.readthedocs.io/en/latest/). All of the projects /u/desmoulinmichel mentioned are similar but don't quite have the level of simplicity I think you and I are looking for.
I'm glad there are other projects doing this as well, but I think this is still unique. Docopt still makes you write the documentation code, and has its own special syntax. Click is certainly a nicer interface to argparse, but you still need to specify information about each of your arguments individually. Begins is closest to what I'm trying to do, but is still focusing on the documentation side of the parser, and not the ability to run arbitrary python functions (especially those without string arguments) from the command line. I haven't seen any other library that uses type hints as per PEP 484 as the basis for automatically generating a parser, and that's the interface to parsing I've always wanted. Thanks for the other references, though!
Wow. Its all so consistent! This just makes him even more impressive
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, this post has been removed as it is not directly related to the Python programming language. It might be more topical on /r/programming, /r/coding, or /r/technology. Cheers, /r/Python mods
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
It's meant to be fairly flat. You can "nest" sub-modules across a folder tree using "\_\_init\_\_.py" files (one per each folder/nested folder), but for most projects that isn't necessary. Scipy and Numpy use this nesting for some of their "toolbox" sub-modules (like machine learning and the like), but most of their most-used tools are at that "top level", just a folder with a single "\_\_init\_\_.py" script. My rule of thumb is, if you can reasonably fit your sub-module into a single .py file, keep things top-level. If you have multiple .py files to describe a sub-module (more like a one-class/function per file organization), then make it a folder sub-module and call it out in the higher folder's init file. Python is flexible enough to allow most code organization philosophies. If you find one isn't working for you, you should consider reorganizing, in this case, condensing your codebase into fewer/less nested files/modules. edit: markdown is not my native tongue.
and so, I shall show the light http://docs.python-guide.org/en/latest/writing/structure/
Just wanted to tell you that it works perfectly like I wanted. Thank you very much!
Escape your dunders. `\_\_init\_\_.py` or `\_\_init__.py` will show up as \_\_init__.py in markdown.
Put all your python into a 'src' subdir and then if you have sprinkling of modules and packages, it's not getting mixed in with other project assets. In your setup.py import this using setuptools: from setuptools import setup, find_packages And then in the setup() function call declare your subdir name: package_dir={'': 'src'}, packages=find_packages('src'), 
All of them are worth doing, they're challenging math problems. That said, a lot of them are more about the math than programming, by which I mean that after a certain point, the problems become difficult to brute-force in code. Think days and days (or years and years!!!) of runtime for a naive algorithm. I recommend picking and choosing the problems you find interesting. Some require a little lateral thinking and math knowledge to catch on an efficient solution, and many times you'll build a small library of simple utility functions along the way, to re-use parts of one solution to use in another.
Most of web development is database and network bound, so Python is plenty fast there. Sure, you can write it in C, but you already have super fast WSGI servers and caching and whatnot written in C.
Nice idea, I can see myself using that.
I discovered this from a [Pycon talk](https://www.youtube.com/watch?v=dgnikoiau68). You run `wormhole some_file.txt`, it gives you a short, readable code to pass on to the recipient. That code is enough to identify the transfer and encrypt it. In the talk, the author suggests that he wrote the tool to demonstrate a less familiar cryptographic algorithm called PAKE, for Password Authenticated Key Exchange. This lets two machines with a small secret key (like a password) securely generate a bigger key, the kind you'd need to do symmetric encryption. What else might you do with that ability?
During the sprints, there was talk about adding a command to easily share public key credentials for ssh and add them to known_hosts and authorized_keys, possibly pgp too. 
Try this #!/usr/bin/env python import subprocess import os # Do not display output FNULL = open(os.devnull, 'w') def pingNetwork(): network = raw_input('Enter network i.e [192.168.2.0]: ')[0:-1] starting_ip = raw_input('Starting IP: ') ending_ip = raw_input('Ending IP: ') for i in range(int(starting_ip), int(ending_ip)): try: subprocess.check_call(['ping', '-c', '1', network + str(i)], stdout=FNULL,stderr=FNULL) except (OSError, subprocess.CalledProcessError): print "[-] DOWN {}{}".format(network,i) else: print "[+] UP {}{}".format(network,i) pingNetwork() 
Web apps are almost entirely I/O bound, and the speed of an interpreted language (Python, Perl, PHP, Ruby, Java etc.) matters much less in this scenario, as (until you hit the big time) it's the smallest part of the equation. But even with that said, Python web apps can be super fast: look at python3 + uvloop + asyncio - you can do 30,000 odd request per second, if you're serving straight from memory. If you have to access a database, or disk files, it will be far slower.
Argh. Duh. Thanks. Edited.
I'm not clear on what the motivations for some of these are, particularly for dictionaries. &gt; Dict({1: 2, 3: 4}).reverse() # =&gt; Dict({2: 1, 4: 3}) Dictionaries aren't ordered and therefore cannot be "reversed" as the word is generally used in Python. &gt; Dict({1: 2, 3: 4}).reduce(fun=lambda acc, k, v: acc + k + v, acc=0) # =&gt; 10 Probably clearer as: from itertools import chain dct = {1: 2, 3: 4} sum(chain(dct.keys(), dct.values())) &gt; Dict({1: 2, 3: 4, None: "go away"}).remove_empty(filter_keys=True) # =&gt; Dict({1: 2, 3: 4}) Probably clearer as: {k: dct[k] for k in dct if k} &gt; Dict({1: 2, 3: 4, 2: 3}).remove_empty(fun=lambda x: x!=2) # =&gt; Dict({1: 2, 3: 4}) The method name becomes misleading for this use case, given that it's not removing empty keys, it's filtering out keys equal to 2. EDIT: Clarifying my issues with `reverse`.
&gt; Dict({1: 2, 3: 4}).reverse() # =&gt; Dict({2: 1, 4: 3}) &gt; Dictionaries aren't ordered and therefore cannot be reversed. Then what would you call flipping the keys and values? Reverse sounds fine to me.
You can turn off browser security in your browser using flags. It will only work for you, but it does allow you to automate the process for local instances. http://stackoverflow.com/questions/3102819/disable-same-origin-policy-in-chrome
`invert` would be my first choice. Since `reverse` is a method on a basic datatype (and `reversed` is a built-in function to achieve the same result but not in place), I would hesitate to use it for different functionality. Inverting a dictionary is a fairly uncommon use case, and can be easily done: dict(reversed(item) for item in dct.iteritems()) 
I completely understand your criticism, some of these functions may not be apparantly useful depending on what Python project one works on. As for me, all of the functions in hawkweed are just things that I needed at one point or another and had to implement myself manually - which is what you do in every one of your examples. Let me exemplify my point with your reimplementation of `remove_empty`(a name I am far from happy with). I would argue that the function states more clearly what it does (filtering) as opposed to a comprehension, because although I like comprehensions I see myself wondering what exactly they do again, because they provide little to no semantic cues. Are they filtering? Are they manipulating? On the other hand, of course, you can as always argue that it is "less pythonic" to not use the builtin facility when applicable. But then again: I can chain those functions together and I can do things in-place without reassigning. Those can prove valuable I find. In the end I would say it is a matter of taste and I do value and understand yours. It's just that I finally settled for implementing functions instead of redoing functions that I need for every code base I work on every time (the nested getters and setters for instance are incredibly useful for me right now).
&gt; dict(reversed(item) for item in dct.iteritems()) That does not work on Python3. I want to support both. And, as I said before, I find it provides a lot less semantic cues as to what it does. EDIT: `invert` or `inverse` are probably better names, though. I suck at naming stuff. You are not the first person to get confused with the `reverse` business.
Good resource here. https://jeffknupp.com/blog/2013/08/16/open-sourcing-a-python-project-the-right-way/
@OP please don't ever do 'import *'. not even on the README 
I appreciate your considerate and measured response! &gt; As for me, all of the functions in hawkweed are just things that I needed at one point or another and had to implement myself manually - which is what you do in every one of your examples. There's tremendous value in building tools for you to reuse. One of my two main issues with this, though, is you're subclassing basic datatypes to get functionality that's fairly trivial and largely idiomatic. This makes your code less reusable/portable, when you could just use the idiomatic solutions when you need them. My other issue is the readability. `reduce` statements tend to get messy quickly. They even [removed `reduce` as a built-in function in Python 3](https://docs.python.org/3.0/whatsnew/3.0.html#builtins), saying "99 percent of the time an explicit for loop is more readable." I understand your issue with comprehensions; I found them cumbersome for a long while. Now that I'm accustomed to them, though, I think they're a powerful and highly readable tool. I agree it's a matter of taste. There's certainly thoughtfulness and merit to what you've done. :)
Almost no requests to Reddit hit the application directly. They've built a rather complex infrastructure based on caches and queues. And this is generally what most people forget when they talk about programming languages and performance especially in web applications. With the right infrastructure you can achieve almost anything.
C/C++ is *computationally fast*. So if you need to compress a bunch of video, or calculate pi, or encrypt a bunch of data, Python would be a slower choice. However, most of that isn't what web code is. Web code usually amounts to this: Read request from the user, get some data from backend database, write some data to backend database, render HTML to user. None of that is computationally expensive. *Mostly*, you're waiting for network traffic. Python can wait on a database to return results about as fast as C can. Most web code is really just glue between much more optimized systems, like Nginx, Redis, Memcache, MySQL, and for that matter, the browser. The glue, with business logic mixed in, doesn't need to be incredibly fast. Even if writing a webapp in C resulted in some measurable increase in performance, are you going to triple or quadruple your development time (at least) to save a few pennies? No way. For most companies, even ones that run big websites, hosting is cheap compared to development time and paying a bunch of C/C++ programmers more money to produce inflexible websites much more slowly doesn't make business sense. You can host a really, really big website with the salary of just one programmer, after all.
You're gonna have to give me some context on this.
One possibility is to use Inkscape with Textext. I recently switched to that from using xfig with psfrag.
Me sees 404
How exactly does that work if the code is a part of the 3rd party module's tornado server? 
I access article good.
reduce its really useful for somethings though. Like producing the union of a bunch of sets: reduce(and_, sets, set([])) Or piping a value through several functions: reduce(lambda x, f: f(x), fs, x) But I get why some (most?) people would prefer the explicit loop. 
"reactor" and "proactor" are fairly general design patterns that apply to how I/O is dispatched. But they're design patterns, not precise interfaces. In fact, Twisted's protocol/transport separation and high-level connecting and listening APIs do not directly map to the "reactor" pattern as specified in Douglas Schmidt's original paper. In particular, the "synchronous event demultiplexer" layer described in the paper is not normally used by applications. The object called the "reactor" in Twisted is in many cases an instantiation of this pattern, but its interface is sufficiently abstract (one of the noted benefits of the reactor pattern!) that it actually has proactor-based implementations as well (in particular, on Windows, where the low-level system APIs work like that). The reason it's called the reactor is that it *reacts to things*, which is the same things a proactor does with its external interface. The first and most significant point in favor of "proactor" in that write-up is that "the handler does not care about I/O readiness events, but is instead registers interest in receiving completion events". If you look at Twisted's overall architecture, outside of just the event loop internals, this is exactly what a Deferred is - an abstract representation of a completion event. Whether the internals of the 'reactor' object are based on this pattern or not has more to do with the structure of the underlying operating system APIs they're communicating with; your application talking to high-level Deferred-returning Twisted APIs gets the benefits of the proactor pattern either way.
As a network engineer, I approve!
You can also set a search engine to: https://docs.python.org/3/library/%s.html %s is where your search term will go and index will take you the the contents page of the documentation
You should innately understand this. You only upgraded from 2.7 last week. Well, until you were ready to move, you still wanted updates to 2.7, didn't you? There are still plenty of people on 3.4 who aren't ready to move yet. So we continue to support them. 3.4 and 3.5 are maintained in parallel (on separate "branches" in Mercurial) and get updates in parallel, though sometimes it's really the same update. Anyway, 3.4 is sufficiently old at this point that it's moved into its last stage of support: it doesn't get "bug fixes", only "security fixes". And it doesn't get binary installers. Also, 3.4.5rc1 is "newer" than 3.5.2rc1 by a couple of hours. Does this really matter to you, if you're not using 3.4 anyway?
In C, you have manual memory management to think about, and even strings aren't a native type, so you're passing around char pointers and (hopefully) lengths. C++ abstracts some of the lower level stuff away, but has its own kinds of complexity. They both have compile steps before you can run code, and if your program crashes at runtime, the default error message won't give you much information - whereas Python defaults to giving you a stack trace. Obviously there are all kinds of amazing things written in C/C++, but I don't think it's controversial to say that high level languages like Python let you write code quicker and easier. That's basically why high level languages exist!
So basically you're just looking for opportunities to be a smartass. My fault for falling for it.
See full working code http://pastebin.com/PwwGy6mq
3.5 is a superset of 3.4, generally. Usually there aren't any breaking changes between the minor versions that would render your code unusable, and if there are, they'd be well documented and advertised. For the most part, you can write code in python 3.4 and upgrade to python 3.5 and expect it to just work.
Thanks for writing and sharing. It was helpful. 
If you liked that, you should check out the pycon presentation on property based testing. It was really good. https://www.youtube.com/watch?v=jvwfDdgg93E
Oh, I didn't take your question as an attack. But it did seem to evince a mild degree of confusion. There are loads of differences between 3.4 and 3.5, big and small. If you just want the big ones, read the list of PEPs on the release schedule, here: https://www.python.org/dev/peps/pep-0478/#features-for-3-5 If you want to see every last little thing, you could start reading the changelog for 3.5.2 rc 1, and don't stop until you finish 3.5.0 alpha 1: https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-2-release-candidate-1
404, is there another link?
That was a great read, thanks for posting it.
No, I was just looking and noticed a problem. Looks really cool though!
From the source, it looks like plt.xkcd() is a context manager, so you can call: with plt.xkcd(): # Using xkcd settings plt.plot(x, y) # Previous rc settings restored It seems like there should be an easy way to directly restore default settings too.
Check out the awsome packages that use python prompt toolkit.. they are almost all great (ipython, pgcli etc). https://github.com/jonathanslenders/python-prompt-toolkit
Speed is mostly about aggressive caching...
I work on repl.it and we have an API for that: https://repl.it/api The plans are currently large but I'm planning on introducing a free/cheap plan soon
You should delete the contents of this post and change your password, especially if you reuse it everywhere. You just posted your credentials in a very easily reversible format. Also, why are you using raw sockets for this instead of using something like `requests`?
thank you... dident see that :( 
waste of space submissions like this need to be removed
ID in the html? In the browser go to developer tools. 
Yes, currying will basically just take in arguments until the contract of the function is fulfilled, thus making both curried and uncurried calls work. I know documentation is lacking, which is the achilles heel of most of my projects thus far. I will try to better that soon. But at least every function, class and module has comprehensive docstrings, so I think converting that into neat docs would be fairly easy. The nested indexing part is true, though I find it prettier than chaining `get` calls (and, of course, `get` is only defined in dicts). Also, by subclassing Collection, it will also work in your new fancy collection and it will integrate neatly with the others, which I find pretty useful. Thanks for the input!
there is an ID in the HTML in like &lt;strong id=testid&gt;&lt;/strong&gt; we just need to check for it and make sure that it is blank and there are no numbers between the strongs
You're checking the tag's content: driver.find_element_by_id("testid").text Edit: In the future, post things like this in /r/learnpython
A while back I created a [similar thing, called `zget`](https://github.com/nils-werner/zget). It does not need a central server and negotiates the transfer using ZeroConf. There are preparations done for encrypted file transfers, however it is not using PAKE for password negotiation (might come in the future though).
Can anyone post the correct link? Thanks a lot
begins use the type hints. 
I have hard time to understand what he mean (begining at if we could refactor ...) can someone give examples ?
I also made a twitter bot - http://www.twitter.com/forcehelps
It's an important notice, thanks!
I preferred https://www.youtube.com/watch?v=5kaqOKIqX_4, but there are several talks about Hypothesis on youtube.
Hi guys, thanks for the reply. Nothing is printing. I'm expecting "Status 201 created". I can get response for GET method as "Status 200 OK". But...
Part by part: -Tackling the problem with the pages, the ideal is to create a method that extracts every single page link, and another callback method that will parse the page given each link. -About the multipage, as the movie ID can be extracted, I believe you can make separate requests on the same method and use xpath to grab the information that you need. After that, update the Class and use the pipelines, and you are good to go. If you want to, make a github and show us the progress :)
I use epydoc in [pyparsing](https://pythonhosted.org/pyparsing/).
It's the Reddit app, has issues with # (hashes) in the url
&gt; proper strings are null-terminated, and proper buffers are bounded, so there's no need to pass a length You're mostly right that I don't do much C coding. But I know about null terminated strings, and I also know that strncpy, which takes a length, is [widely recommended](http://stackoverflow.com/a/1258556/434217) instead of strcpy, as a form of defensive coding. &gt; Package / library management for C/C++ is much more well established and straightforward. Hahaha. It's definitely well established, but... &gt; Hint: check out gdb I said that the *default error message* is not helpful. I know there are debugging tools, but that's not the default information you get if your program crashes. If you really want to correct misconceptions, please try to be less smug and condescending about it.
Doxygen?
Yeah, I agree, I asked the author of the blurb to clarify.
I pretty feel that it doesn't fit well with current state of python. The currying thing and the chaining thing are non idiomatic python.
I tried to search for http://pydoc.me/Popen and it did not work
Thanks.
As I said earlier this is a matter of taste. Just saying "it is non-idiomatic" - which it might well be - without elaborating on it is both a non-argument and hinders any discussion on what Python is and should be, because it tells us that the status quo is perfect as-is and rethinking the way we do things as a community is essentially useless.
I use intuitive names and logical code, anything else is a waste of time.
"Just as well" - no. Much better - solr/elasticsearch. If it must be embedded, you can try sqlite+fts
Your concerns are misplaced..no matter what search engine you use it's going to need access to a directory to index data. Even traditional databases keep their structures on disk.
Have you tried virtualenv? With it you won't need admin privileges and it is included in py3 by default
Seconding Elasticsearch. I've used everything, from SOLR, to Whoosh to full text indexing on DB tables, and SOLR is super easy, super fast, and also works as a decent NoSQL table for pre-computing analytics / common queries / everything I could ever have wanted from it. 
Solr really is good, and the interface is great too. Plus, if you're deploying via Heroku, Docker, or AWS, there are packaged ways of bringing up solr instances in those environments that are close to zero hassle. Solr/Hosted Solr/ElasticSearch all the way.
I think OP specifically wants an external documenation, but I agree with you. Good descriptive names, consistence in patterns, and decent docstrings go a long way.
There is one reason-- You're trying to generate cryptographically secure random in early boot on Linux when /de/vurandom may not have been initialized already. Although even then you should use the getrandom(0) syscall instead, but Python doesn't expose that.
 dct = {1:2, 3:4, 5:6} dct2 = {v:k for k,v in dct.items()} # dct2 {2: 1, 4: 3, 6: 5} Simple and elegant.
Pretty much everything can be done using comprehensions and generators. And I fail to see the advantage in wrapping it up in syntatic sugar. &gt; **Overriding the default List, Dict classes is NOT A GOOD THING, IMHO. ** List([1]).append(2).extend([3, None, 4]).remove_empty() # =&gt; List([1, 2, 3, 4]) # remove empty is the only new operation here remove_empty = lambda list_name: [x for x in list_name if x is not None] List(range(10)).take(5) # =&gt; generator from 0 to 4 # I do not think python saves range values into the generator # so this is just a fancy wrapper # a rather 'pythonic' way would be take = lambda list_name, no: list_name[:no] # take(range(10), 5) -&gt; range(0, 5) List(range(10)).drop(5) # =&gt; generator from 5 to 9 drop = lambda list_name, no: list_name[no:] # take(range(10), 5) -&gt; range(5, 10) Dict({1: 2, 3: 4}).reverse() # =&gt; Dict({2: 1, 4: 3}) dct = {1:2, 3:4, 5:6} dct2 = {v:k for k,v in dct.items()} # dct2 {2: 1, 4: 3, 6: 5} 
Or subtly given hints as to how to improve.
True. Ethics is one's personal thing. What I was referring to here was a technological or 'physical' thing - like a gate or a wall - stopping access. Nothing to stop a bot from crawling the site. In the future, OP may very well come across another task that is not impeded by T&amp;C. This may be useful then.
This is the first post in a series by my friend Philip. I'm curious to hear comments!
Hi there! I gave a talk at PyCon a few years ago about Scrapy - you might like it. Let me know! This is the link: https://www.youtube.com/watch?v=-JzH8TcwqxI Scrapy is optimized for big projects, but you don't need to use all of its machinery. The talk explains the core concepts so that you can figure out which parts of the machinery you do need. I've heard from at least one person that it made a huge difference to their understanding!
Also check out: https://github.com/nicodv/pyspark-tutorial
You might look at this tutorial: http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-x-full-text-search Miguel talks about the changes he had to do to make Whoosh work with Py3.
&gt;they would have to start a receive command with the correct nameplate/channel and code But this information is actually being provided by the server (he mentions this feature for autocompletion - see the "list" command) - you just need to be continually asking for the list, and responding to everything before the intended receiver does. Even without that, you could just release the id if you find nothing attached to the other end. Since it's favouring low numbers for memorability, you should be able to just cycle through them all continually till you find a genuine sender who gets in first. You've a pretty large window of time to get in for most usage scenarios after all, so polling each low id even once every 5 seconds should be enough to intercept the senders who get in during your downtime., but before the receivers can get and type in the phrase.
Can you link the article?
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
Your problem is due to PHP's stdout *not* being the same as Python's stdin. Given the code you posted, PHP's stdout is the same as Python's stdout, thus the same file handle that writes stuff to your shell. [`subprocess.call`](https://docs.python.org/2/library/subprocess.html#subprocess.call) does this binding as a default. To bind it to another stream that you can actually use, you have to use the stdin/stdout/stderr arguments. **HOWEVER** note what the documentation says: &gt; **Note** Do not use stdout=PIPE or stderr=PIPE with this function as that can deadlock based on the child process output volume. Use Popen with the communicate() method when you need pipes. So let's use [Popen](https://docs.python.org/2/library/subprocess.html#subprocess.Popen) and its communicate method instead: import subprocess # ... proc = subprocess.Popen(['php', script_name]) stdout_data, stderr_data = proc.communicate() Bam. Now `stdout_data` contains whatever the PHP script wrote to stdout. No streams needed. In case you are sure you need streams for some reason (big files, just in time processing, chunked data, etc), you can do this instead: import subprocess # ... proc = subprocess.Popen(['php', script_name], stdout=subprocess.PIPE) php_stdout = proc.stdout I do not recommend doing this unless you for sure know you need to, because it means you may run into a variety of fun blocking/streaming issues. Hope this helps!
I just use PostgreSQL's fulltext search through SQLAlchemy, works pretty fine with Python 3 and doesn't add an extra dependency: https://www.postgresql.org/docs/current/static/textsearch.html
First I would like to say I'm not overriding anything, I am subclassing. I am not hiding the names of the builtin classes (note the capitalization of my classes names). Second I think syntactic sugar is very useful because it gives semantic cues as to what you want to achieve with your code. Of course I already made that point earlier but I want to reiterate because this is really my main point. Would you really rather reimplement every data manipulation every time you write it? I personally prefer to have a toolbox ready. This library is a part of it, no more, no less.
The blocking is probably at the `read` bit; the /dev/random device will block until it has enough entropy to return 4 bytes worth of bits. You can probably confirm that easily with strace. (Or just put a print before/after the request.) If you really want to read 4 bytes, and are willing to wait on it, but don't want the program to look hanged, do a read with a timeout and print a status about waiting on /dev/random if it times out and try again. That said, though: /u/Rhomboid is right: if this is for more than just playing around reading /dev/random directly is probably a bad idea. 
I'd be totally fine with having any kind of messages being sent to `logging` instead of packed together into a `dict` or printed on `sys.stderr`
Seems relevant. http://wwoods.github.io/2016/06/09/easy-sphinx-documentation-without-the-boilerplate/
Author here- Thanks for the typo correction. Re: raising an exception. This is something I've been vacillating over. I do consider exceptions to be the more Pythonic solution, but my goal for the library is to execute a series of checks and let the application decide whether to live or die. In doing so, I felt that: ok, results = preflyt.check(CHECKS) if ok: pass else: pass Seemed more readable and less disruptive to perceived control flow than: try: results = preflyt.check(CHECKS) except PreflytCheckFailure as pcf: results = pcf.results # do stuff with results As /u/cymrow alludes, the results are important as checkers can return metrics for logging (e.g. a success state could be if your Elasticsearch cluster was 'yellow' or 'green', and the returned message could indicate as such), or other useful information. An alternative, which might provide the best of both worlds, would be to have `check` always raise an exception and rename the current implementation to `check_nice`. All stuff I'm bouncing around in my head - I wanted to get it out the door sooner rather than later to get more eyes on it. Thanks for your feedback!
https://github.com/pallets/flask/tree/master/docs https://github.com/kennethreitz/requests/tree/master/docs see themes folders 
This kind of construct looks like C/Go to me. Resisting the temptation to borrow to much habits from other languages is hard sometimes. A wise man told that if you've got a class with two methods, one of which is `__init__`, it's not a class. IMHO, you could greatly simplify your code base here and there. Less is more.
Definitely agree re: one-method classes (I usually have PyLint yell about it). In this case, I have future plans that necessitate that structure, so I made an exception.
How is the `commands` functionality different from [setuptools `entry_points`](http://python-packaging-user-guide.readthedocs.io/en/latest/distributing/#entry-points)?
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
This is neat, but I feel like functools.singledispatch is probably more robust and handles the exact same use case. 
Thanks
Well you can use less high-level wrappers like the file utilities in `os` which would allow you to open in non-blocking mode or you can use `select` which similarly exposes some OS features for checking if there's anything to read before trying a blocking read.
Did you read the link I provided? They both come from the *exact same CSPRNG*.
To more directly answer your question (although this isn't necessarily the vest approach), you could check out [Anaconda](https://www.continuum.io/downloads), which includes tons of common packages by default.
This looks great. Now I have to either: 1. Operate on the assumption that a file/environment variable present on the system at program start will continue to be present throughout the lifetime of the application. 2. Make all these checks a second time anyway. I think I'll pass on this one.
I highly recommend this resource :)
Idk. I don't care for it personally.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [help with my code](https://np.reddit.com/r/programming/comments/4o4xej/help_with_my_code/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Fair warning: something on that page tried to grab over 1.5gb of ram in a second on Chromium.
ty :D
Nice work! Can't wait to play with this more and see how it compares to RStudio. 
It kills the built in browser for Reddit Is Fun on my 5x after a few seconds of loading. Which is funny coming from the website of an IDE that is light weight.
Twisted.
Same here. Crashed Chrome on my Galaxy S7.
COMMENT YOUR CODE. In a week, or month, going back to look at your code isn't so bad. After a year, you have to look at your code, you'll wonder if you were drunk or just sleeping unless you write in comments to let yourself know why you did things a certain way. 
yea same here... hence why i stay with PyCharm. I love Pycharm 
How are you able to get scripts on/off the device? Are you able to sync with github/bitbucket and push/pull? Or even a Dropbox?
Well that's... interesting. Few things: Never copy and paste large sections of code. It's ugly, makes your code hard to debug, and will cause you a ton of headaches. Instead, use functions. Aside from that, this bit seems unnecessary... Yes = "Y" yes = "y" no = "n" No = "N" It's easier to just have it like this if answer == 'y' or answer == 'Y': There are also better ways to do that, but that's a simple one. You have a double while loop with the same condition here... while int(number) != 11 and number_of_attempts &lt; 5: while int(number) != int(correct_number) and number_of_attempts &lt; 5: A bigger problem is here... elif number_of_attempts == 5: print ("Awww .-. ;-; :( Would you look at that? You're out of tries &gt;:) Pity, I believED in you") again = input ("Wanna try again? Y or N: ") if again == yes or again == Yes: number_of_attempts = 0 answer = yes elif again == no or again == no: number_of_attempts = 6 print ("Fine bitch, I hope you fucking die") else: print ("-") print ('"' + str(answer) + '"', 'is not "y", "Y", "n" or "N"') print ("So you should try agian, unless you're pussy-shit and scared") number_of_attempts = 6 None of this code will ever be executed. Why? Because above it, you have... while int(number) != 11 and number_of_attempts &lt; 5: If the number_of_attempts must be less than 5 in the loop, it will not ever be equal to 5 in that while loop. As a result, if you get it wrong 5 times, the program will end without a message. Another problem: elif number_of_attempts == 5: print ("Awww .-. ;-; :( Would you look at that? You're out of tries &gt;:) Pity, I believED in you") again = input ("Wanna try again? Y or N: ") if again == yes or again == Yes: number_of_attempts = 0 answer = yes All this is doing is assigning the the string "y" to the answer variable. It doesn't actually restart the program (another reason why a function would help massively). So if they type "y" or "Y" here, the program will still terminate (without a message, by the way). But the absolute biggest problem is... number = input ("Guess between 1 and 25: ") if int(number) == int(correct_number): print ("Aye there you got it right!!! In", str(number_of_attempts) + " attempts!") print ("H U G T I M E o(/^_^)/ . . . . . . . \(^-^\)o") while int(number) != 11 and number_of_attempts &lt; 5: **They only input the number once, BEFORE THE WHILE LOOP.** That means the program would evaluate the same number 5 times, without ever asking them to input another number. Keep trying, you'll get there. But you should really do some basic debugging before posting something like this online....
Why are you using `set()` explicitly? Just do this: a = {'a', 'b'} No need of this list-set conversion.
Always write your own code, even if you're taking it from an example. Copy-paste coding seems like a great way to do things, but it helps you learn better and creates fewer headaches if you write it out yourself.
The "pythonista" moniker has existed far before the app came along, fyi.
Apple doesn't allow out-of-the-box support for importing scripts, but you can directly share scripts with all of your other apps (github etc.) immediately. As for getting scripts on the device, there have been many scripts made to download from github, pastebin, etc. easily into the app. It's actually rather smooth. Here's one of many different versions of what Pythonista users (from the Python 2 version) call [Pipista](https://gist.github.com/pudquick/4116558).
Great. Obviously "all" pure python would work. You can also use ctypes and similiar modules in the way you would expect, as well as many other libraries the developer made for the app that give you access to the unique environment (running on iOS). It also ships with quite a few popular libraries that require c-extensions compiled for use in the app. You can see all of these "extra" libraries on this [page](http://omz-software.com/pythonista/docs/ios/index.html), as well as several others (numpy, matplotlib, sympy, and others). Also, I've seen several people post on the [Pythonista forums](https://forum.omz-software.com/category/5/pythonista) who had made stand-alone apps using the Apple Developer license (or you could simply send a script).
I am actually teaching myself through individual online courses. I will study hard but I am not actually technically enrolled in any school. 
The only times that I copy and paste code currently is just to see how it runs and then to I will play around with the variables and continue to run it to see what it changes. I am not writing my own scripts yet and am really just learning the absolute basics. 
Hey, I went through your code and simplified it a whole bunch. Look through this and identify anything you don't understand and use it for the future. [Here is your old code.](https://repl.it/C2Ho) [Here is your improved upon code.](https://repl.it/C2IA) I wish you the best and good luck!
Sphinx, because writing docs *is* manual work. If you just autogenerate something from your source code, the reader gains nothing over just reading the source. Auto-generated "documentation" is completely useless. Documentation is prose, not code.
Nitpicking ? That's actually mature feedback and sound advice ! Thank you very much !
If both the approaches require me to write a line, I'd always go over the default language specific option since I know its behaviour (is the behaviour of the language). No point in added dependencies in that case. This is like the left-pad lib for node (not that silly) but here it makes some sense since it exposes functional operators that are not present in python. But their equivalents are. Python is not meant to be a purely functional language, and hence, purely functional equivalents (as take and drop from haskell) just add overheads. If the same thing can be done in python, in almost similar LOC, it's better to do it the 'python' way. That's just a person opinion, of course. My intention was to point out that most (if not all) of it has a python-only equivalent that's equally (if not better) verbose and efficient.
Why do you expect a difference? -0 and 0 are the same integer; they're indistinguishable. Negative zero is only distinct when dealing with floating point values. 
Because -0 == 0. Negative zero and positive zero function the same in almost every case.
As for the `List` and `Dict` class that you expose, they deviate from the `list` and `dict` offered by python. They're replacements, and they change behaviour. For them to work everywhere (in all code), all lists have to be an implementation of `List`. Or, everything must be wrapped in calls to `List()`. I'd rather have them as a set of functions rather than replacement data structures. See my `lambda` equivalents. They cannot be chained, true. Here's a link to [Moka](https://github.com/phzbox/Moka/blob/master/moka/__init__.py) which is a minimal functional library. Uses a replacement List class as well.
&gt; I've had some passionate sex with boost::asio recently (mostly in the passive role) and I took a look at your code to maybe learn some new positions. Dude, are you sure you don't have a bunch of race conditions? Certain things like the way you have to do a synchronous write are suggestive. The answer is simple: I don't use mutable shared objects. Yes, there are a couple of global server parameters in my code, but they are not supposed to be changed while server runs. The only potentially dangerous object is a Python WSGI application callable, but it is protected by Python GIL. You **must** acquire GIL in a current thread before you even touch Python objects, otherwise a Python interpreter crashes. Don't forget that it's a Python module that is run from a Python interpreter. &gt; Like, in my amateur understanding, the moment you try to use asio with multithreading (on a single io_service), you enter a world of pain because now all your handlers suddenly can execute concurrently by default. Their examples inconspicuously step around that issue because where they use multithreading they don't also use timeouts or any sort of complicated logic. Yes, if `io_service` runs in a thread pool, your callback can be executed both concurrently and in parallel (those are not the same thing). So I chose Asio stackful coroutines because of their promise of "asynchronous code written in a synchronous manner". And while the code is indeed more readable, with multiple threads a stackful coroutine may may resume in a different thread than in was started. This is dangerous for Python environment because a Python interpreter must preserve a global thread state. So in my solution the part that deals with Python objects runs strictly synchronously, and for other parts it does not matter because there are no mutable shared objects. &gt; Also, some minor things that I noticed while skimming the code: why don't you call m_timer.cancel() from Connection::set_timeout first? Because when a `Connection` object is free to perform an I/O, the timer is not primed. A `Connection` object is local to an execution routine so it won't be called while it performs an I/O with its timer primed. &gt; Also, why ` if (!ec) m_timer.cancel();` in Connection::flush()? Like, what happens if there was an error, the callback just sits there after the connection got destroyed? **UPD**: Now I see your point. I guess, here the timer should be cancelled in any case, error or not, but since a non-zero error code results in a connection handler exiting prematurely, the timer will be destroyed as long as the `Connection` object that owns it goes out of scope. &gt; And overall, maybe I'm mistaken, but it doesn't look that you have a consistent strategy for dealing with errors (I'd put if (check_ec(ec)) return; at the beginning of every handler and have that method stop everything and throw an exception unless ec was operation_aborted etc). This part I don't understand. I check all the error codes generated by Asio operations and finish an execution routine prematurely in case of any errors, freeing an `io_service` to perform another callback. There are no other error code sources to check.
Yes, PySDL2 is just a simple ctypes wrapper, so it's much simpler. If I remember correctly CX_Freeze works, as well as Nuitka with some tweaks (I believe you have to specifically tell it NOT to include numpy, which it tries to pull in for an optional dependency but which isn't needed). Edit: Feel free to play around with this game a friend and I hacked on: https://bitbucket.org/treehousegames/pylaxian/wiki/Home The game does include it's own SDL2 libraries, which may or may not work on all systems. Just delete them and use your own if you have trouble. Also, It's MIT Licensed, so have at the code as well :)
THANKS MAN HOLY SHIT :D
This made me really happy cause I can learn alot from this and plus you're a legend!! :)
Thank you /u/ZedOud, your app is a great tool to practice python. Could you make another one for every language I want to learn? ;)
Sad to see it has that vague and unhelpful critique of the src-layout. Maybe change that? Even Twisted is using a src-layout [nowdays](http://atleastfornow.net/blog/twisted-in-2017/). Sorta funny, considering JPC's [antiquated thoughts](http://as.ynchrono.us/2007/12/filesystem-structure-of-python-project_21.html) on the layout problem are completely the other way, and he used Twisted as the prime example.
It's an explicitly opinionated guide. That is an opinion.
You can still have the "api docs" (if you really think they provide much value) using the autodoc feature of sphinx which will do exactly what you want. And then you can easily also add useful, prose documentation on top of that. Maybe a quickstart tutorial or something like that.
It's code-joke. Like this: https://github.com/asweigart/my_first_tic_tac_toe/blob/master/tictactoe.py
Yes, as well as retaining old code or temporarily removing code while debugging #!/usr/bin/env python # commentexample.py # Display the knights that come after Scene 24 print("The Knights Who Say Ni!") # print("I will never see the light of day!") /* This will comment out a block of code or comments. I will usually do something like this to make a section very visible - See the next set of block comments */ /*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-* This is a block of comments that will Jump out from many lines of code With the extra stuff around it *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*/ 
 NameError: name 'normal' is not defined
&gt; but you can directly share scripts with all of your other apps (github etc.) immediately. What do you mean by this? Isn't dropbox "another app" that I could "share scripts" with?
That's a great habit, as it also teaches you the context in your fingers. Getting good at typing it helps reduce the effort from "thinking of solution" to "writing solution"
I cheat and downloaded the python docs and docs of libraries I often use to my laptop, so no need to use the internet! Joking aside, perfectly fine to use the tools available, including the internet. Over time you'll likely use it less for some things as they become natural.
I'm sorry I don't have a link but you can copy/paste a python script that does Dropbox sync for example. You could also write a share sheet extension in Pythonista that grabs a Python file and saves it in the app.
_future plans_, two dangerous words in a programmer's life.
If you use reStructuredText (.rst) instead of Markdown (.md) then you can use Directives .. include:: your-external-file.html or .. raw:: html :file: your-external-file.html see: [including-an-external-document-fragment](http://docutils.sourceforge.net/docs/ref/rst/directives.html#including-an-external-document-fragment) and [raw-data-pass-through](http://docutils.sourceforge.net/docs/ref/rst/directives.html#raw-data-pass-through)
Based off what you are saying it sounds like there is no way to sync with scripts/projects that are not on the device. Is this correct? If so I really feel this limits the usefulness of a 10$ app
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
&gt; I don't use mutable shared objects. The session itself and all that it owns is a mutable shared object. Your deadline\_timer callback can execute concurrently or even truly in parallel with your coroutine. So your code might be calling `socket.shutdown()` from one thread and `asio::write(socket, ...)` in another thread can be in progress or called from your callback, and the documentation says that socket methods are not thread-safe. &gt;&gt; Also, why `if (!ec) m_timer.cancel();` in Connection::flush()? Like, what happens if there was an error, the callback just sits there after the connection got destroyed? &gt; So I need to interrupt the transfer in case of connection errors. No, what I meant was what's the purpose of the `if (!ec)` guard? What happens if there _was_ an error: then the m\_timer is not canceled, so the callback continues to sit in the io\_service's queue and if you're unlucky it gets called from some random thread while your Connection is in the process of being destroyed. That's what I meant by systematic dealing with errors: if some shit happens you want to make sure that you cancel all your pending operations. &gt; So in my solution the part that deals with Python objects runs strictly synchronously, and for other parts it does not matter because there are no mutable shared objects. Wait. So that means that _everything_ after reading the header is blocking, because you provide read and write callbacks to Python code. And that's why you need threads in the first place. Huh, I sort of expected an asio-based server to be more asynchronous, I think. Why can't you manually reacquire the GIL in your threads, at least that would free them for the duration of the blocking operation itself? Or does that not work with your RAII GIL managers because they try to restore a wrong thread state? I guess there are ways around that, but people who want that can use a big gun like twisted or gevent already. &gt; I check all the error codes generated by Asio operations Actually you swallow an error in Connection::read_into_buffer. 
It's supper awesome and I probably wouldn't use PyCharm without it anymore. However, It's a bit clunky and doesn't support plugins or vimscript. My biggest issue is that it doesn't come with hotkey map of sorts, since it makes text editing vim-like when rest of the editor still relies on arrow keys and awkward hotkeys that require two hands at both ends of the keyboard :( 
When exploring data I mostly do the same, usually from a REPL. I suspect it is a usual pattern, actually.
I am a professional Googler, essentially. I understand the fundamentals of programming, but I do not keep all of the syntax, libraries, and idioms in my brain at all times. Especially since I work with multiple languages and systems, and will task-switch between them. Recalling a particular language-specific statement or idiom is what my "external brain" is for. For example, I may Google something like: 'python iterate file system recursively'... "Oh, yeah, os.walk". Now I can do `for _, _, files in os.walk(filepath)` and get on with my day. And I'll probably forget that function name again, and Google it again in the future. Oh, well. Henry Ford is known for saying something to the effect of: "I don't need to know everything. I only need to know who to ask, when I need to know something." I've butchered the quote, but you get the point. The key is, like /u/desmoulinmichel said, to actually *understand* what it is that you've just read and implemented. 
I began writing it after having read a post on Type Checking using the Prolog language. I just thought it would be better for him to do it directly in Python, and it would be a nice training for me to write it. So I did, helping myself from what I remember from a course two years ago. I hope he'll fall on this and might get interested! There are also a lot of things to think on how to do, some kind of magic semantic predicates that would make our life easier. I will continue this project whatever happens, but I would be even more glad if any of you got interested! Sadly, I can't find the post that caused me to write it... EDIT: I've added new examples, some with that type checking idea :)
It's very normal. If you're only looking things up for the documentation, download Zeal and its Python (and other useful ones like Numpy or Django) documentation. Works great offline.
You have a lot of unnecessary code. Golden rule to remember when programming: **never repeat yourself!** Since I myself learnt ALOT from other people's code, I rewrote your program as tidily, readable and easily understandable as possible in Python 2.7 and hope you will learn something from that. The differences between 2.7 and 3+ are marginal on this level. If you have any questions, feel free to ask :) def play_game(): tries = 5 correct_answer = 11 print 'Guess a number between 1-25' while True: if tries is 0: print 'GAME OVER!' break else: print 'You have {} tries left'.format(tries) # input() gets input as an integer, use int(input('Guess:')) in Py3 guess = input('Enter your guess: ') if guess is correct_answer: print 'You won with {} tries left!'.format(tries) break elif guess &gt;= 10 and guess &lt;= 12: print 'So close!' elif guess not in xrange(1, 26): print 'Your guess was not between 1 and 25!' else: print 'Wrong answer!' tries -= 1 while True: # Raw_input gets string input in 2.7 (Python 3+ equivalent is input()) a = raw_input('Want to play? (Y/N)') # Converts the response to lowercase and checks if it is in our 'yes'-list if a.lower() in ('y', 'yes'): play_game() # If input is anything else, just exit the game else: print 'Game exiting...' break
I disagree. POLA says to use exceptions. If the program wants to catch the exception, leave it up to the user.
[From a few days ago](https://wwoods.github.io/2016/06/09/easy-sphinx-documentation-without-the-boilerplate/): Sphinx. Included are comments clearing up everything.
Oh of course. I do that all the time actually. For some reason I didn't get that impression from the post. But even with the REPL situation, I mostly use jupyter notebooks now. This way I have records of what I did and to which exact files that are in a central well organized location. 
`/* */` uuuuuh ? Python multilines comments are done with triple quotes: ` """ Multi-line docstring/comment """ `
Only 50%? You are learning fast padowan :-)
&gt; you'll likely use it less Or more as the things you do become more complex/interesting
I've had to use far too many bits of code in different languages over the years to have a hope of remembering all of it Before the internet I kept a huge library of code snippets and loads of reference books; these days it's much easier to google. 
You're burning through entropy because it's hard for computers to get, and you're consuming *loads* of it. Each read consumes 32 bits of entropy. By default, on a Linux system, the *maximum* entropy a box can store is 4096 bits. Assuming you've got it, each character takes 1/128 of the maximum available entropy on your system, and so you can generate no more than 128 characters before running out. This is totally needless for your program, though. `len(alpha_contents)` is 36, so you don't need more than 8 bits of entropy per character (in fact, 6 would be plenty, but you can't ask for 6 bits of entropy). So why not change it to read one byte from `/dev/random` and then change your struct code to be `struct.unpack('B', randomData())`. That'll mean you consume 1/4 the entropy than you currently are for each character. While we're here: you're biasing your output. This is because you're taking the modulus of your random number. In almost all cases this will bias the output towards the early bytes. In your current code you're slightly biasing towards the digits `0` through `7`: if you make my suggested change you're biasing towards `0` through `2`. The only way taking the modulus is a good idea when working with random numbers is if your modulus evenly divides the maximum random value you can generate. Otherwise, the more correct code is: while True: byte = struct.unpack('B', randomData()) if byte &gt; (255 // len(alpha_contents)) * len(alpha_contents): continue byte = byte % len(alpha_contents) That is, you need to throw away the biasing outputs. This will, of course, cost you more entropy, but that's the price you pay for deciding to use `/dev/random`.
It's free forever. Since it really is just a webapp built with React and Webpack, I'm going to do a server version and a couple other things later that will help justify the product. Also: there are a ton of advantages to being open source, like getting free access to a lot of tools like travis and appveyor, and we can take PRs from the community.
I looked at your code for a few minutes, without running it, and I have no idea what you're trying to do or prove; but somehow I believe it's less clever than you think it is. Correct me if this is wrong, but it looks like a basterized version of multiple inheritance that's near impossible to understand and too clever for its own good. I think a better solution would be to implement a base class (`GetAttr`) that has attribute registration logic instead of trying to infer from the `__call__` method. 
Annihilated Chrome on my Priv. Maybe someone should tell them about it. 
Code blocks are like tenth on my list, behind dragging tabs out of the window to make new windows, and obviously Windows™ support. I'm still trying to figure out the best way to get code blocks to work with [Ace](https://ace.c9.io/). Right now, you can select some code and use Cmd+Enter to run a series of lines. Code blocks would be so much better though.
&gt;Am I the only one in this? No. Unless I am doing similar analyses/projects for more than two weeks I need to google *X.foo documentation* or *apply columnwise pandas DataFrame* ... Especially the later one I already know that i will need to google again the next day. For some reason it just doesn't stick ;). 
Might be handy for some. Just to be sure: **Do not use this script as a production server or anything public reachable.**
My thoughts exactly. Haskell will just flat out refuse to do it any other way.
Brilliant! The second one works perfectly, the first causes an error about an unexpected indent in the included HTML file. Thanks!
The original inspiration was trying to implement Scala style implicit conversions in Python. Given that I ended up thinking that would be impossible outside of serious framehacking, sys.exchook and probably something even more insane. I still wanted to see how far I could push duck typing to the limits as sort of a mad science thing rather than "here's something practical and usable"; I guess this is probably closer to Ruby's "open classes" as far as I can tell. I don't think it's particularly clever other than knowing to wrap the `GetAttr` instance with a function to avoid descriptor madness. Otherwise, the actual instance of the original class disappears and you're only left with the `GetAttr` instance, which is what I didn't want. Outside of that, I think it's all fairly mundane on the surface. Actually using this and attempting to debug that application would be insane as a class is liable to change it's actual form at any point to anything else.
I wasn't familiar with this feature! Thanks for the blog post.
To me googling (as in "using a search engine you actually trust") is as much "acquiring new knowledge" as it is "looking things up that I already kinda knew". It's like a massive indexed hand book. You need it even more when you switch a lot between languages. After a month of JavaScript, Python's standard library feels weird and vice versa. Throw in some Java, PHP, Lua, … and you'll even forget how to get the length of an array (or list…). Was it `count(arr)` or was it `len(arr)` or `arr.length` or maybe something with `size`? (hint: all of these exist in one language or another.) However the more you use a language the faster you get at remembering.
You could also memory map it and then lazy load. Not necessarily great for text files but it can be awesome for binary ones.
I'm wondering if Pandas has something similar to Goal Seek. I use scipy for that, but someone I tried showing it too was overwhelmed.
You need to be careful with copyright and ownership if you're doing this at work for anything that isn't completely trivial.
&gt;What if we want to write tests for it? That would be a problem: the tests would either have to create a file to use as input, and capture the standard output to check the results. I don't see a problem with this. I write tons of tests like this. I can see the point in walling very complex algorithmic code from I/O and testing it by itself, but most of the time I don't see the point.
In my case I use it for data science projects where I load experimental results and play with it using Matplotlib/Seaborn. It is really useful to have the plots in their own part of the interface, instead of popping out in a new window. For "real", big projets, I would not recommend it.
Are there any ways to make IDEs open remote files that isn't a hassle or require me to keep entering SSH credentials every save? One of the biggest reasons I'm using vim all the time is because I'm editing code and files on remote Linux boxes.
That's a really good question. I am not familiar with anything that is as simple as Goal Seek. I'll need to look into that.
Personally I use KDE5 under Linux, which tend to provide excellent sftp support. This would of course require the correct kde packages, but a full DE install usually covers it. Then it is up to how the IDE interfaces with the filesystem. Unless you open the files through dolphin ofc, then it doesnt matter what app is launched, it will just work. Other than that, pretty much any major IDE ive come accross supports remote files one way or another (plugin or not). Also keep in mind youre comparing an editor with IDEs.
This is really cool. Bought it and it's amazing on iPad. 
thanks!
It also has the added benefit of getting to practice debugging when you make a typo.
You'd probably be better off getting python to pull the data in automatically (from internet or files) and then filter and spit out a simple comma or tab delimited file that excel can pull in easily. Once you get that bit working, it might then be worth writing a GUI front-end for it, but be aware the GUI programming is probably more complex than your acquisition and filtering parts.
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Asyncio makes this style of code a hard requirement
Yeah, but asyncio (or more generally, asynchronous programming) is not always the right approach. I think the main argument in this article applies with or without using asyncio.
Yes, I got you point. In extreme cases with really big network latencies there indeed may be a race condition. I guess I need to fix that.
I stupidly uploaded a massive gif to the blog (I work at Yhat), which I think was the issue. Should be fixed now.
I used to copy my own old code on my local machine. Then i copied code from my own blog posts and open source software. Now i just google and copy from there as it has become a lot faster than finding stuff on my own machine. I alway rewrite it to fit the rest of my code base though. So yes, it is completely normal and healthy. "Never bother to remember what you can look up" - Einstein.
Also, i'm very scrub and i've seen it alot, what does it do when you put "def play_game():" at the top? And could you explain this ".lower()" command please? Finally, the way you formatted it will restart the game if they so desire after guessing incorrectly 5 time? Truly I just want to be able to wrap my head around the placement and indentations of your "while" loops But seriously, thanks alot :) 
And doing this accounts for upper and lower case responses? Just using ".lower()"?
And debugging is a whole skillset in and of itself. It takes time to learn to read exceptions, parse out what they mean, and then step through programs to figure out where you went wrong.
You can "sync" in the sense that Github lets you (there's even a dedicated Dropbox library). There are scripts for syncing with all the major repositories. I've seen several really streamlined workflows. You can make a script to sync, set that as a tool shortcut, then activate it whenever you want to do a git push or whatever. Even if people say this-or-that feature is missing or lacking, as it stands, this the best IDE (for any language, let alone Python) on mobile. It's the only *decent* Python environment on iOS, and the best integrated Python environment when compared to any other solution across iOS and Android. (Although I *think* you can use Visual Studio on the newest Windows Phone OS, but that's not going to offer the mobile enhancements that the Pythonista app offers.) TL:DR - Even if something appears to be lacking, as of right now, there are no decent/useful/practical alternatives.
We have a large codebase of API's implemented in Flask. Most of them (the ones that I did not write) have all of their application logic and IO in the views. Maintaining and refactoring this code is awful. Frequent use of the `request` object and the `db` object, provided by the sqlalchemy Flask extension, makes it painful to decouple the code. When I have to touch this code, I try to leave it in a better state than I found it, but it's not fun.
Certainly. You can share scripts to other apps, or you can make a script to upload your project directly from within Pythonista. There's even a few sample file sharing scripts that ship with Pythonista: e.g. a local network file server script. When I say "share" we have to remember the appex system that iOS uses for sharing data between apps, and limitations that Apple policies imposes on importing and executable piece of code to an app. The whole purpose of the policy is so that an uninformed or unskilled user can't be easily lead into executing malicious code. So importing scripts is not restricted, it's just not easy to import it from any other app. Thus: write a script (like pipista for github).
On adding edge cases their use of "master" seems to be a cop-out. It would be better to extract all the words in the executable(s) being tested and test those. It would most likely include "master" but you might discover other fails too.
Sorry, not my app. :) I just love proselytizing for using Python *everywhere*.
There are a lot of apps on the AppStore that can interpret code (almost all the of the popular interpreted languages have an app). You just can't make it easy to import code ( e.g. using app extensions) and you can't include a compiler (I think).
Hey look, a cool new mistake I just realized I was making. Good article.
This is true, I would be using it for sports data/betting analysis.
worked great for me thanks!
From the article: &gt;but bear in mind I’m using it as a vehicle to present some ideas. In real life I would probably just use collections.Counter and the csv module
&gt; &gt; &gt; But... For giggles, as an armchair theoretical exercise - why bother with having to depend on C? Sure, it's effectively impossible. But why bother if you don't have to? If someone has a perfect knowledge of your internal state, then wouldn't it be possible to tell the numbers you're generating, regardless of if you wait for a re-seed? Also, I'm pretty sure you need *root* access or a serious exploit to read internal state. As in, you're completely and utterly fucked at that point, and you have bigger issues, and reading from /dev/random won't help.
First of all you're not installing the wheel here: &gt; ~/does-this-work/bin/pip install ~/path/to/MyApp/dist/MyApp-0.1.15.zip Make sure you clear your `dist` folder in order to avoid confusion. What's the protocol for using your package? Something like this? import MyApp MyApp.function(...) Things in `MyApp.core` and `MyApp.utils` won't be exported to `MyApp` unless you explicitly do so. `core` and `utils` also won't be present as attributes of `MyApp` until they are imported. You could check in `~/does-this-work/lib/pythonX.Y/site-packages/MyApp` to see what was installed. Finally, I figure the name might be made up for this post, but Python package names should not be capitalized.
Yeah, you're quite right. I actually just totally neglected to redirect the imports in my __init__. It actually is working, I just have to import schemagic.core etc. Sigh. Working on the little details and missing the obvious answer. Engineer life.
Thanks for the response, I definitely understand what you mean and agree. Probably the only true blanket statement: "blanket rules always have at least one case where it fails" 😂 [edit]: Also sorry for having to ask about the codebase thing, all too often people come in acting as a subject expert when they're experience has only ever been a small little one-off web app.
Windows version will be out in the next release which is slated for 2 weeks from now.
I use this, very nice for interactive running of scripts: https://github.com/kootenpv/emp
I never used to write tests, but recently started working for a company that tests everything and tends to encourage TDD (without forcing it). I've found that decisions like separating IO from algorithms tend to occur naturally when writing tests. Writing tests that deal with IO and external APIs and things like that tend to be a bitch, so you end up splitting things apart so you can remove those sorts of tests whenever possible.
Looking at the various actions of ".remove_empty" it seems both miss-named and to have functionality in too many disparate areas, (although this last could be another aspect of the miss-naming).
Feels like first you have a lot to learn...
wut. The "M" in MVC *is* the business logic. It's not just some dumb data mapper. IO should be happening at the fringes of your application, not down in the guts.
PyCharm = &lt;3
You can use *eval*, but I would strongly suggest rewriting the code. If you have variables with numbers after them, then it is written wrong. Try using arrays out dictionaries.
I see what you want to say, but wouldn't I have to recreate the business logic when I change data sources this way? When it is done in the controller, my model can provide me with the data I need independently of the source. Not trying to argue, I really don't have much experience in this field.
why? what makes you say that?
Parsing is the conversion of source into an internal representation, usually an AST. It does not imply `l['0']` but rather this : `"l[0]"` which is the entire source code rather than a line in the source code. 
Hey there! This is an attempt (inspired by [this post](http://funnybretzel.com/blog/datamining-the-next-series-to-watch-part-1/)) I made in order to make some simple analysis on Breaking Bad! I hope you like it! Obviously I made everything using python and the source code is on [GitHub](https://github.com/finalfire/analyzing-the-shoulders-of-giants).
numpy has a solver
You're right. Token is the correct term in parse-speak.
To differentiate between `set` and `dict` objects. I instantiate them the same way the `__repr__` displays them for easier tracking.
I don't think so. In the blogs example they going from reading the file line by line vs. losing the whole file into memory. The latter would be impractical for larger files. 
It's a toy example. On one hand, I get it, we should always strive to put our best face on when sharing code -- I've stopped reading articles and blogs because the code quality was subpar at best. But on the other, the difference between `for line in some_file` and `lines = some_file.readlines()` is trivial. I'd actually understand this comment if the post was about best practices of disk I/O in Python. But here? It adds nothing to the conversation.
Well, if Andy Dufresne can do it...
I feel like typing it in activates a different part of your brain, because it forces you to go through the actual motions of writing a correct solution. I find I'm more l likely to notice abnormal things like "Oh that's interesting that it takes X as an argument..." because I'm forced to comprehend each character, and through that correction process I learn more.
I think that's a Shawshank Redemption reference. 
Scipy requires numpy be installed, but unfortunately PyPy, last time I checked, only had limited support for numpy and was actively seeking contributions to finish their support for it. In the mean time, you need to install a special "numpypy" package. See their website for details.
I had the exact same experience as you about a year ago! Luckily, I documented all the steps to deploy on Dreamhost with python 3 in [a blog post](https://brobin.me/blog/2015/03/deploying-django-with-virtualenv-on-dreamhost/). I did it with python 3.4.3, but it should work with any other version.
As a guy who uses Excel and Access on a somewhat frequent basis, I wholeheartedly agree with what you're saying here. I'm very interested in learning python, but I've honestly never had a legitimate reason to learn the language. Without a reason to learn, I struggle to do so... especially when the majority of data transformations/analysis I need to do, can be done fairly easily in Excel.
Sourceforge is still better then softpedia or those 100% virus-laden content sites though. In fact, SF has been my savior.
Awesome!! We've needed this for a long time!!
Yup. I help organize SF Python Meetup and we've been talking about doing a MiniCon for several years. Finally got brave enough to try it :)
I dunno, I would have named the function "load_developer_file".
Yep. And if you're working with code that you know should work, you know the problem should be limited to a typo and it should be an easier way to start to learn what errors mean.
Are you using virtual environments to manage your installation? It seems your windows installation isn't able to find pip or its not in the path.
looks javaish thing and coding for future
Apparently, sleeping can sometimes be of some good. I've just remembered the nickname of the guy, and found his post back: https://www.reddit.com/r/Python/comments/4nchy7/writing_custom_type_systems_for_python_in_50/ It must have been a bit too far for me to reach it by browsing.
If you can, try using python -m pip install plotly 
Can you make an example?
AFAIK excel uses the least-squares method which is numpy.linalg.lstsq
It is a regular install, not running any VMs. I don't know if it helps, but I can see pip in two folders on my hard drive: C:\Python34\Scripts pip pip3.4 pip3 C:\Python27\Scripts pip pip2.7 pip2
Thanks for the suggestion but does not seem to work. If I try with just what you wrote I get a syntax error. If I put a $ in front of the string you wrote, I get an 'Unknown command' error.
You actually probably just want: a = [1, 2, 3] for x in a: print(x) or if you just want to iterate over a range: for i in range(1, 4): print(i) or just to quickly print each value on a newline: map(print, range(1, 4)) or just print each value space-separated (the asterisk/star "unpacks", while print can take multiple inputs): print(*range(1,4))
I wouldn't feel confortable using this kind of design in declarative / oop programming. It would be a reasonable design in funcional programming. what would think of this Jackson or Warnier with their "old" style of designing programs? Cobol style programs?
This - and your whole Pandas series - is great. Thank you.
Raymond Hettinger hype! I try to channel him as much as possible when I sit down in front of any piece of code. And he's a compelling speaker on top of it.
You did not mention which part of PyCharm you are uncomfortable with. Why do you want something other than PyCharm? 
I might use this on a work related issue next week, thank you for this!
When the question is wrong, only idiots answer the question. Now answer this: have you stopped beating your wife?
Ok?
&gt; On one hand, I get it, we should always strive to put our best face on when sharing code I agree with you, and this is really where I was coming from. My comment was not really related to the point of the article. However, I've seen too many cases where someone saw a quick piece of advice like this and copied code verbatim without understanding the differences in *other* parts of the code they are copying. It's thinking along the lines of "This guy knows that separating IO from algorithms is a good idea so he probably follows other good coding practices. I'll just copy this."
Since I've worked in that dept and know all the people I'm not sure how it will go. The main dev that i'd be working with / under direction of is super hard and intimidating. His previous boss, who was mine when I first started, said he's going to ask him to take it easy on me. I'm going to be a nervous wreck until the interview is over.
I have full confidence in discussing all things I've done in IT and how I solved the problems. I've also done a TON of stuff to help our local site grow... * WHen i started they had no corporate wifi, instead ppl would use generic ISP wifi and use VPN to connect in. Now we have site wide wifi AND I'm helping plan out network expansion that will allow us to grow for 2-3 years with ease. * We weren't using SCCM to image computers locally, it was all done via OS disk. This has DRASTICALLY improved things. Also got another site using it * Made a ton of batch scripts for users to run instead of waiting on IT. Most of them are tedious setups. * Improved process for using some old legacy autoclave thing. Previously guy was copying/pasting data to make a nice spreadsheet from an ugly CSV. Now i automated it and created a backup of the originals JUST incase my powershell script messes up. * I rolled over 150 over to a new file server with isolated incidents that were resolved within 5 mins. Did this with powershell by changing all home directory paths that were mapped * made several PDF's of how to do things that were asked repeatedly, thus empowering people and helping educate them on some stuff. When you mention talking abotu solutions you m ade to already solved problems I'm a little unclear? Are you saying I should focus more on the steps I took instead of what the end solution was?
Pretty sure some other mad/sad dude is downvoting everything I Post. I just want to make sure I didn't come across negative or anything. I'm def growing bored right now, this will be a huge learning curve but I'm up for it. We have something called [The Iron Yard](https://www.theironyard.com/) here and I think I may ask if I can do it at some point.
Can anyone tell whether the same problem exists in python 2? It's not clear to me what the corresponding modules to `urllib.error` and `urllib.request` are. For what it's worth, the `requests` module doesn't seem vulnerable. Using `info = requests.get(url).info()` it fails to parse the url: &gt; $ python fetch.py http://127.0.0.1%0d%0aX-injected:%20header%0d%0ax-leftover:%20:12345/foo &gt; Failed to parse: 127.0.0.1%0d%0aX-injected:%20header%0d%0ax-leftover:%20:12345
can you discuss the L2 regularization technique that you used?
[That is a real licence](http://www.wtfpl.net/), just saying...
It's good practice to do this on your own to help understand a topic deeply. I've done this for NN, SVM, trellises, Huffman codes and many Computer Vision algorithms. Using someone's own 'first principles' version, however, seems kinda silly (in lieu of a well tested and optimised ML libraries version). The only other reason something like this would be necessary is to optimize the DNN beyond what the ML libraries allow for and to tailor it your own custom problem.
You may want to have the algorithms work using matrix operations. For example, there's a part that says `for x, y in minibatch:` which is dealing with feature vectors and label singletons. Might be more efficient to deal with all data points in the minibatch at once with matrix operations. 
Sorry, I meant "serious license". A small measure of maturity in other words.
There's a project called StaSH, which gives you a shell inside the app. You can do a lot, including generating an SSH keypair and using git. Check out the Pythonista forums as they're a wealth of information. 
No problem! Feel free to shoot me an email if anything doesn't work or if you have questions about anything.
Woa, thanks! It seems a nice help!
I think this is likely my issue as I know I haven't seen/read anything about needing to do this. I found [this site](http://learnpythonthehardway.org/book/appendix-a-cli/ex5.html) but nothing here seems to work either. When attempting to just use 'pwd' I get back: Traceback (most recent call last): File "&lt;pyshell#2&gt;", line 1, in &lt;module&gt; pwd NameError: name 'pwd' is not defined