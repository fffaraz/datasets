&gt; Last I checked, the purpose of a URL shortener is to shorten URLs, not provide anything resembling security. So long as you can't break the site in funky ways by encoding a zero byte or something you're probably good. You may not want visitors to see *all* URLs simply by iterating an ID, and many URL shorteners randomize their IDs to prevent this sort of crawling.
People hate lots of things. People have different tastes. Just because you and I like Python doesn't mean anybody else will.
Yeah, but people should have a reason for hating Python. If they hate the whitespace thing, that's fine.
IEEE and ACM have some of the best articles I've read _still to this day_. Far better than most of the blogs I've seen posted to Twitter, Reddit, SO, Y Combobulator, and so on. Those sources aside, people have been sharing information about programming for longer than the Internet has existed by quite a bit. In fact, compared to programming the Internet is still a new fad. Regardless, I'm not going to argue the utility of the Internet or the Internet as a source of information. Obviously it's good, but I will argue that not all sources of information on the internet are good, and as all sources some are much worse than others. I'm just pointing out that using sites like Reddit or SO to gauge of the popularity of a programming language is going to be incorrect and heavily skew the results.
I googled "I hate python" and the first result [1] lists lots of reasons this person hates Python. People do have reasons. You don't have to agree with them, just like they don't have to like Python. It's a big world and there's more than enough programming languages out there for everyone. [1] http://wyattbaldwin.com/2014/01/13/what-i-hate-about-python/
Maybe I've been using numpy for too long, but the stepped list slices seem perfectly natural to me.
I always end up leaving :w written all over documents I have to edit with other programs. 
I - A - Are you ok?
It really depends on the field you are working in. Python is "productive" if you have to ship a product quickly, or if you need to get an analysis done quickly, for example. I would say that Python is probably not the language of choice for "professional developers/programmers/coders" who wants to optimize their product. Python vs. Java is like getting it done "at all and in the first place" vs. "having a pretty efficient solution in the long run. That's why people in science love Python so much. They can code up their ideas quickly, they can get the analysis done "conveniently" without bothering much about efficiencies and consumers/users -- keep in mind that most people in "science" (myself included) are not professional programmers. 
Very interesting. Questions: How to start the Worker ? Also may I suggest you put up a demo app. 
If I tried hating MongoDB any harder I might crap my pants. It promised freedom from locked in schemas and the tyranny of rows and columns. Then I discover there is a MongoDB way of doing things that takes a long time to truly master and their clear attitude is that if you don't want to do things their way then you can just bugger off as you clearly don't understand MongoDB. So I have done the exact same thing; I dumped mongo with an uninstall where I nearly broke my enter key hitting that command line so hard. I just use JSON with my relational database and the world is a better place. I do things my way not the way that a bunch of self styled Gurus sitting around a table with their laptops tried to foist upon the world. For a short childish time I was a fan of NoSQL now I am a convert to SomeSQL. 
&gt; the purpose of a URL shortener is to shorten URLs, not provide anything resembling security URL shorteners are used both to shorten *and obscure* URLs. Remember that URLs can contain semi-private details, such as usernames / legal names, physical addresses / GPS coordinates, etc. Would you trust giving this info to a service that would leak it like a sieve? 
Java is a marketing miracle
&gt; I nominate vi as the one true code editor for Python **SILENCE HERETIC**! GNU Emacs is the one true editor!
I didn't look at OP's source. Does it simply direct all shortened URLs to Rick Astley's *Never Gonna Give You Up* video on youtube?
You can also use `itertools.repeat` instead of the multiplied list. from itertools import repeat seq = range(1, 10) zip(*repeat(iter(seq), 3))
Multitaskers on what sense? Anyhow I've been like you for a while but recently got put into a position at work where using PyCharm was my best option that didn't involve wasting a lot of time trying to get IS to install Sublime. Two things that already have me changing my mind are autocompletion of methods from imported classes from another file, and the code checker. For the code checker, it seems to generate a lot of noise (stuff like inconsequential style decisions), but it's hugely valuable as a time-saver to not have to go hunting for missed colons, a function call with a typo in it, etc. I agree it's probably overkill for something like a small standalone script, though, based on my limited experience. 
What are you doing (or how ancient is your computer) that the memory consumption of an IDE matters? 
I am guessing what you are looking for is a solver? http://docs.sympy.org/0.7.1/modules/solvers/solvers.html
I just want to add, for emphasis, that the only reason the Python site lists two current versions (unlike other languages) is because they couldn't make the changes to get us to 3.x without breaking backward compatibility, but people were too heavily invested in 2.x to declare the 2.x branch no longer supported; 2.7.x at this point seems to primarily exist to allow 2.x users to get continued functionality updates (whence import future). 
The fact that Python is used for scientific data analysis should hardly be a surprise. Title makes it sound as if there's something magical about the language that's resulted in extra saved lives. Please don't ever write like that. 
Bingo, it is a solution in search of a problem. 
I used Python a bit in 1999, but didn't even realize it was a serious thing until I tried working with it again in 2006! I thought it was just somebody's casual project.
Page goes into infinite redirect loop for me...
http://www.lfd.uci.edu/~gohlke/pythonlibs/
For IDEs - WingIDE is amazing ... its debugger is better than pycharm (the last I tried both an year ago), and programs run faster in debug mode under wing as compared to pycharm. Its a paid app, but its totally worth it if you are going to seriously program in python. 
/r/learnpython can help you!
I was actually thinking about this not too long ago. Do hashes make better primary keys than integers then? It seems like a no brainer but I want a second opinion.
Python was my first language. I started with Codecademy, which lead to me hitting a lot of road blocks. I found my skills increased largely after reading Learn Python the Hard Way, and I didn't really do the last couple of chapters, I just started building stuff. I would suggest learning Python 2 in your situation because there are more resources, and the differences between 2-3 can be picked up later.
If you are not able to find anything in Python, you might consider rewriting it in Python yourself. The source code of the `gflineq` function in Matlab is located [here](https://lost-contact.mit.edu/afs/cs.stanford.edu/package/matlab-r2009b/matlab/r2009b/toolbox/comm/comm/gflineq.m). I did some (fast) scanning of the function and couldn't find anything that would stop you from doing so!
there are anaconda builds of opencv3 now available
No surprises that the thread intended to reduce the number of "What IDE should I use?" threads ended up turning into a "What IDE should I use?" thread.
Micropython is a good bet as far as embedded scripting. I imagine that in addition to lower footpront it has lower resource and energy usage, but I don't know about the latter. I expect that energy usage will be more relevant to IOT use than speed or memory.
Fantastic pedagogy!
Here's a [video](https://www.youtube.com/watch?v=mxsKP8GhoHw) of the author explaining the hack for those who can't read good.
Hooray, another post about using Tweepy!
Am I only one that feels some mild scroll hijacking? It's subtle enough to think that there is no scroll highjacking, but evident enough to make me annoyed. P.S. Tried on chrome (first was safari). Yup it's scroll hijacking. I hate websites with it.
Should not be hard to implement yourself. I wrote such a code in C back in college, I recall I only need about 100 lines or so. In case you don't want to do that, there is the python package I found after googling for about 30s: https://github.com/Glank/Galois
So salty. Code review is important, and sugar-coating it isn't beneficial. Pointing out flaws in code does not require the critic to implement the same program himself, just like pointing out flaws in a proposed law or budget plan does not require the critic to present a different plan.
Stickies don't often help. Most newbies to `X` will find the subreddit for `X` and go straight to asking a question without reading the sidebar or looking at stickies, or even bothering to search to see if the question has already been asked in the sub.
/r/learnpython
Excellent write up. Thanks for sharing.
Hi, I'm using standard WordPress stuff plus one of the available (free) Themes. I've now disabled the "smooth scrolling" feature and hope it will now work better on your device. May I ask you to test my blog again? Just to be sure it works. If not, well, then I'll try to dig deeper into configs, or maybe replace the theme completely. Kind regards, Harris
Like note that the price of a result pulled from the API has gone up X percent in the last hour.
Many, many thanks! :):) You're right. We all have to be more original on the net. I've tried to do it on my personal homepage www.brakmic.com and this "blog.brakmic.com" thing is something I created just two weeks ago. I wanted to share some stuff on programming with my favorite JavaScript / Python libraries. However, this blog also deserves a better treatment and not just a standard JohnDoe-Design. Thanks again, Harris
As /u/mfitzp suggested, it probably wrote all the files to the same location and name, overwriting each one and just leaving you with the last one. And nothing about the move process assigns a file type, only the extension of the file specifies that, which is only what the OS thinks it is. You can rename a Word document to .pdf and the system will show the PDF icon, but it's still a Word document.
Yeah, in a table. Thanks.
Personally, I like to use *dict comprehensions* for flipping keys-values: d = dict(red=210, green=105, blue=30) flipped = {v: k for k, v in d.items()} `dict(map(reversed, dict.items()))` is also cool!
I was about to gripe about yet another post using twitter from Python, but your client looks great!
Thank-you! I've had an initial look at this and it seems to be exactly what we need. Nice one :-)
I feel like this blog topic is re-written every two weeks.
Hi, Thanks for the hint. The weird hamburger menu was actually a "feature" with a honeypot-link in it to catch the annoying bots. I've removed it now. Regards, Harris
Thanks for the kudos :)
[removed]
Actually they do. If you visit "evil" websites, you get flagged for further monitoring (see e.g. [this article](http://www.theverge.com/2014/7/3/5868159/new-report-says-the-nsa-is-checking-who-visits-tors-website)).
Most IDEs are best with one or only a few programming languages. And even then, they really are designed with programming in mind, so they aren't usually ask that great for other stuff you might do on a computer (writing a book or sending email). The three text editors I mentioned have wide support for many programming languages, but even more they don't focus on programming but rather on entering and manipulating text. In officiate, emacs is often jokingly said to be a pretty good operating system, but only an OK text editor‚Äîeven though it's much lighter weight than any IDE, it comes bundled with games and utilities far beyond what one night expect from a text editor and find have created addons to do just about anything you'd want to do with your computer from writing emails to managing your calendar and on and on. Sublime Text is also highly extensible, though it doesn't have quite the vibrant ecosystem as emacs, and it is much more resource intensive than vi/emacs. The advantage of a great text editor that I was trying to capture by calling then "multitaskers" is that once you've really learned one, you can have most (maybe all) of the advantages of an IDE but within a single, consistent environment for any text you might wish to edit from programming to poetry.
Many text editors can that, too, via plugins.
But if you use plugins to get all the features of an IDE. You have an IDE. Just that it runs in/on a text editor.
If you use Peewee, I put together a little IPython notebook showing how to write the equivalent queries using Peewee: http://nbviewer.ipython.org/gist/coleifer/38eb226123a446a628a8
It doesn't seem to work right now, probably server overload, but great jobüëç.
I'm "self" taught. I put it in quotes because people on the internet helped me so much that they taught me much of it. I used the Python tutor mailing list, Alan Gauld's tutorial, looking at the output of my GUI builder, the wxPython and other libraries' mailing lists, the Python newsgroup, a few books rather cursorily, Effbot's web site, looking at the code of others, Stack Overflow...and mostly just trying to write my own (non-trivial) stuff in Python and banging my head into the desk for a long time.
Here are some additional benchmarks you could run: https://github.com/kenrobbins/python-rapidjson/blob/master/tests/test_benchmark.py Includes more types of tests and more libraries.
One criticism. `zip` is different in Python 3 and Python 2. In Python 2 `zip` is not lazy, upon being called it immediately returns the full list. In Python 2, if you want a lazy yielding version, you can use `izip` from the `itertools` library. In Python 3 many default functions became lazy. The Python 3 versions of `zip`, `map`, ect exist in Python 2 as `izip`, `imap`, ect in `itertools`. I say this because the article opens with docs for the Python 3 version, but the examples are written in Python 2 (`zip` is returning a list in the examples, it would just be a "zip object" in Python 3). Either way, great article, a good introduction to the magic of `zip` and unpacking.
In Python 3 you'll need to use list(). seq = range(1, 10) list(zip(*[iter(seq)]*3)) [(1, 2, 3), (4, 5, 6), (7, 8, 9)]
That's just ain't so. It's like saying, The main difference between men &amp; women is that women generally wear their hair longer. The main difference is: Unicode. In Py3, strings are sequences of Unicode characters; in Py2, they're sequences of bytes. Nothing is lost -- Py3 has byte strings and `bytearray`s -- but Py2 programs that assumed strings are byte sequences can be tough to convert. Several things that in Py2 were list-like, such as `range` objects, in Py3 are generator/iterator-like. Py3 doesn't leak variables from comprehensions. And more goodness. A good quick rundown of the differences can be found [here](http://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html).
https://virtualenv.pypa.io/en/latest/
virtualenvwrapper as well, since it helps with the ease of virtuanenvs.
I think it is back up now! It was a lot of traffic to handle in a short period of time, hope it works for you now!
In case the website is down, the Github link is https://github.com/nikodraca/InStats.
i would prefer to compile it. i am on linux. 
https://github.com/menpo/conda-opencv3
Yup! Good call. Also feel free to fork if you can think of cool stuff.
I agree. Although the ffmpeg backend is about the least stable part of OpenCV these days :-(
If you're fast, but not fast enough, with highly optimised code, your first port of call should probably be PyPy, or C.
I used to use virtualenv and pyenv until conda came around. Here is a recent [blog post](http://bit.ly/1YUQ5KH) I wrote about using conda to do exactly what your asking for and more. Hope it helps.
thank you
thank you
That's a point of view I wasn't aware of, but it's not really surprising. Large-scale charity has a long history of being misguided. I would argue that it has gotten a lot better, and that the Gates Foundation in particular has been more aware of the need to research and understand the problems they're trying to solve. Of course there are going to be negative consequences; it's far too complicated a situation for there not to be. We can only hope they are aware enough to recognize them, and adaptable enough to minimize them as much as possible. There are also a lot of stakeholders involved with complicated agendas, so it can never be as simple as supporting only those that align precisely with your goals, at least if you hope to be effective. And opinions differ considerably as to what is "beneficial". My understanding is that GMO research is saving lives. If you had $40b you wanted to use to help, how would you approach the problem? Maybe you just wouldn't?
If you want to define IDE by the principles of duck typing, then yes, a text editor can be called an IDE. (And so can the command line!) But the functionality and intent of the base program is how I would draw the line. Which puts emacs and vi (focused on editing text of any sort) in a different place than Sublime Text and Atom (text editors targeting a userbase of script writers), and then in a third place go Eclipse and Visual Studio (which are focused first and foremost on software development and only as much text editing as is incidental to software development). 
Have you tried codeanywhere.com's Cloud IDE? I am the founder so would be really interested in your feedback. Cheers
strong goals and aspirations. brb writing a new photoshop
thanks for the advice, i'll give it another shot tonight and see how it goes.
still seems to be down for me. Took a look at the source code, but would love to see it in action!
Going through the source linked there, I think it's about features that are only available to pytest. From the source: &gt; But if you start using features peculiar to one testing framework, then a good deal of rewriting might be necessary in the future if another one of the frameworks develops important new features and you decide to migrate But yeah, I agree that that con should be clearer, I'll edit it to better reflect this.
Compiling numpy with the MKL is a bit tricky. If you are a non-ancient distro, make sure to use the gfortran ABI interface (on MKL 10.X, it is called `mkl_gf_lp64` on 64 bits), and not the MKL one, at least if you build with the gnu compilers. If you have more questions, I would advise you to ask on the numpy ML
[removed]
I'd suggest chewing through the file and doing all your stuff as you act on each line instead of trying to read the whole thing into memory at once.
&gt; Python's interpreted and dynamic nature means that dealing with large codebases can be a nightmare. &gt; The size and maintainability of a code-base has nothing to do with the "type" of language i.e. interpreted, statically typed, functional, ect. The real issue with large code-bases is how it's organized (it's architecture I guess).
 from numpy import genfromtxt data = genfromtxt('file.csv', delimiter=',') Docs: https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html And: https://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html I usually store datasets that big in HDF5 files (using h5py) instead of in memory, then access elements on demand. edit: this might be useful as well: http://stackoverflow.com/a/28554340/1561404
I'd recommend you start with Nodebox. It sounds well suited to what you are trying to do. https://www.nodebox.net/
What about using SQLite? As long as you can fit the entire thing in memory it should be a feasibile option. Then use SQL to do some statistics on it? (may depend on what you want to do) 
Thanks for pointing this out. Actually, for some extent, examples are written in Python 2 intentionally to avoid cluttering code with additional wraps by `list` etc. From the other side it really becomes inconsistent with the references to Python 3 documentation. I will consider this in future writings.
If genfromtxt does not work for you, there is also the underlying function which is [loadtxt](http://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html) it should be faster and use less memory But the data needs to be very tidy and you need to do more (definitions and fixes), so if genfromtxt works it is preffered. Just thought I would put it out there in case you get stuck with gentxt, it took me a while to figure this out myself.
Tightening up the content is definitely the best option. I didn't realize you had more graphs at first because the first graph filled up my whole screen. You have a lot of white space which grants you a minimal feel that looks good, but removing some of it would help the data presentation.
Please specify that the examples are in Python 2.
Be warned that most Anaconda Opencv builds are Linux only. I found this from hard experience.
You may be also interested in the [dask](http://dask.pydata.org/en/latest/) library to do the job. It let's you do memory-constrained parallel computing in a very simple and straightforward way.
MongoDB is fantastic. PostgreSQL is a great SQL database. What most folks who "hate" MongoDB seem to have a hard time understanding is that MongoDB isn't a SQL database. Things are different ... though that doesn't make them bad. If you are capable and take the time to learn this other paradigm ... you quickly learn that MongoDB is actually pretty great. If you compare it to the other available NoSQL databases it really stands out. It provides a uniquely powerful query language in the realm of NoSQL databases ... and the constraints it has are not unique to MongoDB vs. other NoSQL databases. ... and can compete for many of the same use-cases you would normally require a SQL database for. It's not a SQL database though, and thus has many of the strengths normally associated with NoSQL databases. Namely ... sharding and horizontal scalability. If you try and shard in SQL it's terrible in so many ways ... but what you quickly learn is that all of the basic constraints you complain about with MongoDB are also constraints within a sharded SQL system. You cannot perform joins, etc.. Sharding and a clustered configuration is the big selling point for MongoDB ... if you took the time to read even a little of the documentation you might have discovered this. Even a basic production setup of Mongo in the documentation is a 3 server cluster ... 
It seems to fluctuate for some reason! My analytics shows there are no live user right now, so hopefully it will go through now! 
This is the problem ... and the reason most of reddit seems to hate MongoDB. It's not a relational database ... if you take the time to learn just the basics though it's really pretty great. It was a powerful NoSQL option before 3.0 ... but with this latest iteration and the development of multiple backends seems like it's ultimately going to become a defacto backend ... ultimately.
Lots of folks here are going to suggest using Flask. Flask is a whole lot simpler ... which may make for a smaller learning curve. Though the Django is also really easy to do basic stuff with. You can go through the tutorial and over the course of a couple hours it'll teach you all you really need to know to build a basic website. The advantage of Django is also its dis-advantage ... it includes loads of additional tools to help building websites ... and has a standard multi-file layout rather than a single-file or DIY layout with Flask. Flask developers that don't use these features see this stuff as bloat. Though it tends to be quite useful in my experience ... and certainly worth taking the time to learn. The most useful features are the admin, forms, and integrated ORM. If you are building an article submission form for example ... almost all of the work is automated. You create the model with the ORM that represents the database table ... and if you want just the basic ability to modify all of the users submissions in the admin it's a single line of code. Likewise creating the HTML form can be automatically built from the ORM model ... so you don't need to spend any time creating a custom HTML form ... django automatically does that for you. Likewise the article submission view is handled primarily with the automated form code ... including features like security and what-not without any effort on your part. The biggest benefit with Django though is the encouraged organization of the project and individual apps. If you follow the standard they've created you'll quickly be able to understand the layout and structure of any Django project you encounter in the future ... as they're all pretty much structured the same. Flask is minimal allowing you to chose your own ORM, Forms, etc ... when you're just learning the basics the simplicity may be attractive. Though when you're building large websites of a more advanced nature the structure and tight-coupling of Django is a bit more attractive. When I'm hiring developers I know that if they list Django they already know 90% of our in-house code standards ... if I'm working a Flask project though I need to train every hire a bunch and it's a pain in the ass.
I would just steam it in on stdin and use `pv` to provide progress and ETA if needed. 
Don't use IDLE. Use an editor and run python from CMD.
Ah that would make sense. 
I've recently discovered Spacemacs and love it! https://github.com/syl20bnr/spacemacs
Are you looking for feedback from a code quality standpoint? Is writing code your profession or hobby? You definitely have a lot to learn about design and best practice but functionally your app seems to work so I commend you for that.
With reasonable amounts of Ram (8-16GB) this shouldn't be a problem. I recommend archiving to HDF files with compression using Pandas hstore, you'll save a lot of time if you need to reload it. 
That is an excellent point, I haven't really used libraries that need to be compiled but I can definitely see how that would be a pain. 
I was willing to use Bing more often for their rewards, but after I signed in I found out it's US-only so fuck them.
Yes, it's easy. http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking Whether the algorithm can work on chunks at a time or not is another question. If not, and there isn't enough memory, then HDF5 is the solution.
I had good luck with SQLite. I had a 25GB text file download of the ExAC database (basically every single variant found in 60,000 people's genomes) and I needed a way to get info on any arbitrary location in the genome when ExAC's own site often took 2+ minutes to return a single query. With SQLite, after writing a 15 line or so script to read the text file into a database, I can do hundreds of queries in just a second or so. I don't think it needs to be read into memory, since the computer I used only had 8GB of RAM.
Package Control does not come with Sublime for some reason. You have to install it. https://packagecontrol.io/installation
Its the one I'm using in my class
Thanks for sharing this aspect of Django. I'm not surprised really... from what I've read, Django definitely more features than Flask. Flask is minimal by design. However, due in large part to its simplicity, Flask is universally considered easier for beginners to get started with (correct me if I'm wrong). So far I have nothing but love for Flask and no reason to pursue other options, but I know that Django is there if Flask starts to feel inadequate. OP is doing their first web project so I imagine Flask is a better option for OP.
Any personal preferences?
But something like phantom would, right?
Sweet! I just saw the fork, appreciate the help mate!
Thanks a lot man!
Where do you get a $10 software radio?
Depends on what you're using as a key-value store. That being said, it shouldn't matter here. The key shouldn't be directly exposed in the URL. You'd just be rolling the red carpet out for anyone who wants to crawl through your database...
OK, that much makes sense. Thank you.
IMHO, you're in over your head, flailing about. Check out http://blog.udacity.com/2014/01/peter-norvig-teach-yourself-programming.html or, more directly, http://norvig.com/21-days.html: that'll get you started. Vague ideas are cheap. Far be it from me to throw cold water on creativity, but you'll need to get a sense of the state of the art -- whether it's easy, routine, difficult, intractible or impossible to realize any particular idea. +1 on making plugins for existing industry-standard applications which are by now "platforms". If you write a plugin that supplies missing functionality, already you have a leg up on reaching a market of receptive potential users, and you can avail yourself of the plugin API which saves you from reinventing the wheel, the square, the triangle, the shader, etc.
Start writing programs in Python. The best way to learn is to do.
Ok, so I installed SublimeRepl, but I still can't execute a python script from within sublime. I keep watching videos and nothing works. I tried ctlr + B, and it just says it cannot find the file. What is happening?
Django isn't really more difficult to learn ... I guess there's a bit more to learn about ... but it's certainly not more difficult. I think the popularity on reddit and for beginners is the quantity here ... not the difficulty. I guess having multiple files makes it seem more complex ... but it's not ... the file structure offered by django right from the start is perhaps the most important thing you could learn. Flask's worst feature is the fact that it offers a single file app. For production applications where you have a great deal of different pages and features on your website organization is the most important thing. Especially with a team of developers. Working with a standardized structure like django offers makes this simple and easy. Working with Flask that offers no standard structure ... and actually encourages *no* structure ... is a disaster right from the start. Large Flask apps I've seen ... have usually been manually restructured to follow django's standard. The URLs have all been moved into multiple files ... the app.py has been split up into multiple apps and views ... etc.
I also contribute to OpenHatch. People interested in finding a project should introduce themselves on the OpenHatch IRC channel. Open edX, github3py, SymPy, PyDy, and many others are welcoming projects. The DjangoGirls tutorial is also an excellent place to make a contribution. 
A django developer's best friend: django-admin.py shell_plus --notebook
I use Sublime Text.
Think of something simple that you'd like to do/make. What are your interests outside of programming. If you can make something thats interesting to you, you'll learn much faster.
It's just one function to load a csv into a dataframe. You'd have to go in and change the code of pandas to add those print statements. Watching python's memory usage and seeing it increase to 1.5+GB is a better solution here.
This is admitting defeat in the first battle of the war. What if the run is going to take 10 hours? And he gives up after 8? When processing a gigantic dataset, it's best to first get some idea of how long the entire run will take. Start with 10 MB, or 100 MB and time the run. Then extrapolate to 1.5 GB. If it looks to be the case that the run will in fact take hours and hours, it might be worth spending some effort doing two things (either/or/both): 1) optimize the code executing the run. Sometimes one can save 10x the processing time by doing something simple like chunking. 2) make the run fail-proof. Save results as you go, as well as a bookmark to the source data, that you can pick up where you left off in case the run fails. No one wants to do 3 passes through a 10 hr. run.
There are many levels of memory (mis)management that may trip you up here. Caching, paging, etc. will make your life hell, make your code run much much slower. Don't throw everything in memory and call it a day, unless you absolutely have no other way to do processing.
Flask encourages using modular design though. You can use blueprints to split your app into several parts. I dont want to say that its more suited for bigger Tasks than lets say Django. Especially if you work in a bigger team.
I doubt profit is what motivates Bill Gates. He has certainly been shaped by a lifetime as a businessman, and he is quite open about how he uses practices more commonly associated with a corporation than a non-profit. But if Bill Gates wanted to make a profit, I'm sure he could imagine a far less devious and more profitable approach. I doubt they're doing good for the sake of it either, but you would be hard pressed to find any non-profit organization whose leaders and employees operate purely on good will. I suspect it's more about legacy for him, and maybe a bit of atonement. So perhaps that's a bit of ego for you. But, again, you'll find similar motivations almost everywhere. Too much money in the hands of very few is a separate, deeper issue, and I'll agree with you on those points. And yeah, it's likely a lot of that bleeds over when someone like Gates tries to make the transition. If you were to demonstrate to me, for example, that Gates is lying about developing malaria drugs to distribute as cheaply as possible to the world's poorest people, and actually intends to hold people hostage with overpriced drugs they need to live, I might be with you. Otherwise, I don't see much hypocrisy.
Can anyone explain what that monstrosity is supposed to accomplish? My morbid curiosity is getting the better of me.
 &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;
in this case it really isn't an attempt to debug but rather a status indication that is of value. We don't know how many records are involved here but it is very possible that the program was stopped with 98% of the file processed. In other words I don't see this as debugging at all, I see it as keeping the user informed about what is going on. Sort of like what is happening right now on my Mac where a software update is indicating the MB's downloaded so far. If the counter is incrementing I know something is happening as opposed to earlier today when it got hung up. I'm actually surprised a programmer would confuse this with a debugging attempt.
Well, now he can replace that extremely verbose "self" for the simpler and shorter "@instancemethod\n".
What's scroll hijacking?
I only know extremely basic OpenGL. Most of the tutorials I find are just OpenGL with a C language. So, can I just follow those and apply them to python or what do you recommend me doing because I don't know any C language.
What have you tried? Were not going to do your homework for you. Can you make a function to iterate over the vertebrae and return the total? *wrong subreddit (did you even look at any of the posts or sidebar?)
Uh... pretty sure you've got the wrong sub here buddy. This here sub's fer lovers of the great [Python](http://python.org) programming language!
There are only two languages that I would recommend learning OpenGL with: C and JavaScript (via WebGL). Learning OpenGL while also simultaneously figuring out how to use a translation layer for Python would make things unduely difficult. There are a lot of very good WebGL tutorials out there.
Depends.. Is it version 2 or 3? If it is 2 it'll be much older, but still functional, so nothing to worry about.
If you're doing relatively simple processing, take a look at [xsv](https://github.com/BurntSushi/xsv), a CSV toolkit written in Rust. If you happen to be using Arch Linux, I maintain a package in the AUR.
There are JNIs for Java that have the same issue. True of any language.
[No, clearly the next step is to find variable usages and assignments to unbound variables and prefix them with `self.` if their names are members of the class.](http://cdn.meme.am/instances/500x/56306615.jpg) 
If this keeps happening I'll discuss with mods about how best to deal with/remove these. For now, I'm just having a [sensible chuckle](https://i.imgur.com/himZD0M.gifv). /u/CuriouslySeeking11 you probably want /r/snakes. This is a subreddit about a [programming language :)](https://python.org)
Relying on external servers to download dependencies on deployment can be a dangerous game. If you have to deploy new servers for what-ever reason ... even if the whole process is automated ... you want it to take less than 5 minutes form start to finish .. and not rely on any external servers or services that may be down. An hour of downtime may turn into 24 if you can't download your dependencies. Best practice is to pre-compile any C dependencies for your server/python versions (you don't want to have to deal with slow compilations or issues on an emergency deployment) ... and keep all of the dependencies packaged with your application or available locally to deploy with pip (I believe pip can handle custom URLs/etc ...). Personally, I always just include all of my dependencies in the project's git repository (with ~/libs added to my python path) ... and use git submodules. I use submodules to split up large multi-component projects any-how ... especially where there are multiple teams managing different components.
The question I have is why would you do such a thing? Surely it can't be to avoid typing a few characters? Is there some bizzare edge case where it shaves half a nanosecond off the run time?
As mentioned in another comment, you should tell us about your hardware. I load 500MB CSV files with Pandas in less than 10 seconds. I have an i7-4770k and a SSD. 16GB RAM but not relevant to the statement.
Oh my... that's worse than I thought.
I do not think it is so complicated. When I installed both Python 2 and 3 on my Mac, I did not configure a thing. They just both were installed correctly and don't interfere. In a Terminal, Python 2 is called with "python" and Python 3 is called with "python3", as you said. My point is that you will have a hard time screwing it up if you just go about it the easiest way possible, running both pkgs.
Your first port of call should be understanding numpy. PyPy (and usually, self-written C) isn't going to be anywhere close to as fast as calls to an optimized matrix math library that numpy does.
I guess it comes down to scale. Flask is simpler for small one-person projects, Django provides the structure you need for large scale development and teamwork. You are definitely intriguing me into looking further at Django. I went the Flask route very early on because it was faster, easier, less confusing, etc to get started with it. Django seemed more complicated. But maybe now that I've built a few tiny apps and have more clue what's going on with web dev in general, perhaps Django will be easier to dive into. My current project is much more ambitious than my previous ones and perhaps Django would work better for it... And it's true that my website is getting a little scattered by having multiple Flask apps strewn about. HHMMM...
I thought numpy only used that library on intel and if it had been compiled with it?
Ya, which is not too hard to get in most cases. Otherwise, if you get numpy from your linux distribution it will usually come with a very fast OSS matrix library anyway.
Many languages implicitly declare a pointer to the current object, so this emulates that behavior if it's what your used to and really don't want to do it the Python way.
Haters gonna hate. Seriously, people have their preferences. Don't let someone make you think python is bad for their own personal reasons. Lots of people in this field have strong opinions on certain issues. Take what anyone says with a grain of salt and don't be afraid to try something "bad" for yourself.
That is totally understandable. Thanks for your work. I'm planning a python course for my colleagues, and this makes it easier to give it with python 3 :) Now I only hope that numpy 1.10 with the @ operator is released before the course starts as well :)
I have been in the past month learning Go, having been using mostly Python for the past several years. Go is superior in many ways, from it's size (language spec), !object_oriented, native *real* concurrency, being statically typed and compiled, *much* faster, and just working like it says on the tin. All this adds up to productivity and peace of mind. There are tasks that Python is best suited for, but I will definitely be using a lot more Go from now on.
Have you messed with pytables at all (soon to be hdf5 based)? Any thoughts?
I meant concretely? Algorithms? Strings containing private info?
i like to live dangerously
Nah, no parsing is required. You'd have to go through and check if the names aee LOAD_FAST and set, and since the class is already defined check their name in the class, then add a LOAD_FAST 0x00 and a LOAD_ATTR.
1. Don't iterate. Vectorize your code. First ask if you really need to find the distance between A and B or if you need to find the close points. Scipy has a `kdtree` algorithm (actually it has multiple versions), which are far more efficient. If you actually need the N^2 algorithm, there's a whole list of distance formulas 2. If you really have to iterate because you can't figure out how to vectorize your code properly, vectorize the inner loops. http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.pdist.html http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.spatial.KDTree.query.html
That seems to be a font rendering issue. See [Java Runtime Environment fonts](https://wiki.archlinux.org/index.php/Java_Runtime_Environment_fonts). Basically, try launching PyCharm like: _JAVA_OPTIONS='-Dawt.useSystemAAFontSettings=on' pycharm I'm not sure if it is the same on Windows, if you are using that.
yes, pyenv: https://github.com/yyuu/pyenv pyenv installs the python version you want, say 3.5. Py2 is your system default. You enter a new projects where you need python 3. You type: `pyenv local 3.5` and you're done ! .
can you plz throw in some sample code ?
Well, thank you kind sir for pointing me in the right direction, that is hint that it might be Java problem - I didn't do exactly what you described, but swapped OpenJDK with Oracle's Java and it is all looking crystal clear now. Thanks!
 &gt;&gt;&gt; s = "abcd" &gt;&gt;&gt; s[::-1] 'dcba'
Great way to put a damper on his or her enthusiasm.
Programming conference presentations (in person or youtube). Even if the talk isnt about a new library they might have used one you havent heard of to in their examples or to make their slides.
The answer /u/billsil gave is the right one in this case. That being said, there are ways to do this more efficiently in cases where you can't vectorize. For one thing, you don't need to slice and zip. You can iterate over the arrays directly. This will iterate over the outer dimension, which is what you want. You can transpose before iterating to iterate over another dimension. You don't use `j`, so enumerating is useless in that case. If you are using python 2.x, `zip` will be slow in this sort of situation, use `itertools.izip`. Type conversions can be expensive, so make sure `distance` returns a numpy type when given a numpy type. In-place changes are faster than out-of-place changes like you do. So you could simplify it to something like this: for i, x in enumerate(A): weight_distance = 0 for p in enumerate(B): weight_distance += distance(p,x) weight_array[i] = weight_distance As I said, in this specific case, do what /u/billsil suggested. It will be far, far, far faster. But in general, when loops are the only option, these sorts of changes can help.
The reason for the downvotes is that the question should have been posted in /r/learnpython
I would like for it to be assumed that methods worked this way with perhaps the introduction of a "static" keyword to imply that the calling object won't be passed to the method at all. As it stands if you wanted to create a method that wasn't meant to be called with an object argument, you'd have to keep track of that in comments, always invoke it explicitly from the class, and if you ever invoked it the wrong way weird unexpected behavior would be the result. Suppose we assume we always want to call our methods with an object argument. Well then for one it becomes redundant to always have to explicitly state the object parameter and two it leaves room for obfuscation because while I always assume that self is the object and no other variable refers to the object, that doesn't have to be the case.
Only yesterday, two colleagues of mine were working on a script to migrate a sql database table with 800 million rows. The initial version of the script took 6 hours to run on a copy of the production dataset. In the last revision, they got it down to 2 hours. Since the main production system uses a single database server, that means the most central part of the company's system needs to be down while migrating. If we don't run this script, a full month of work will be wasted. Sometimes, surrender really isn't an option.
Python's portability situation is worse than Java's. Python hasn't quite decided where it wants to be. In some parts, it tries to be a Pythonic "this is how you do it" and abstracting the platform, the CPU architecture and memory layouts. In other parts, it's a thin veneer layer over the operatingsystem and breaks down horribly based on small differences. If you `open()` an jsonfile and `readlines()` from it, in python3, what do you get? bleep, trick-question. Depends on the OS/environment that you run in. Even environment variables change things like this. Java on it's side decided early that "fuck the operatingsystem, replace everything". This led to a whole separate environment, millions of lines of duplicated code, infinitely difficult interop problems, and guarantees that the code actually works the same way on all platforms. This then goes into deployment. If you deploy on Linux (Debian vs. CentOS) and different sets, you may need a few minor workarounds. Like replacing all calls to "open" in your code because it breaks when started from `init` vs. started from a terminal. On top of that, the Python deployment ecosystem is _far_ from as well worked out as the Java one is. Even when Python is older, there is very little work done on making a solid deployment that many use. Most cases you end up with a nightmare of different ones. Then again, Java has tomcat and Swing, and really. That's not speaking in it's favour.
Oh, thanks! Removed the post :D
I'd look into using Powershell if you want to configure multiple windows machines. 
Why not file a feature request for Inkscape? Although doing something yourself is nice, sometimes one has to get the help of experts.
A challenge to oneself.
Complementing /u/billsil and /u/TheBlackCat13 answers, if you ever need to iterate over numpy array and there's just no way of vectorizing it, you can considerably speed things up by using either numba or cython. Numba is particularly simple, your code would just need minor adjustments. from numba import jit import numpy as np #with nopython=True you can't use most of python #conveniences, but it's much faster, just be sure #the `distance` function is compatible @jit(nopython=True) def weigh_distance(A, B): weight_distance = np.zeros(A.shape[0]) for i in range(A.shape[0]): for j in range(B.shape[0]): weight_distance[i] += distance(A[i], B[j]) return weight_array 
wait..wow..how exactly does numbah work
I'm no expert, but JIT stands for Just-In-Time compilation. Since the python interpreter can't deduce the types of the function arguments, it can't really compile the code before executing it (like in C, for example). What numba does is compile code as needed (by caching the arguments types every time the function is called). There's also PyPy, which is a completely JITed python interpreter but it does not support all of numpy features. You can take a look in [this post](https://jakevdp.github.io/blog/2015/02/24/optimizing-python-with-numpy-and-numba/) by Jake Vanderplas to get a feel of the optimization possibilites in python.
Your if statement should be if type(foo) != int or type(foo) != float 
Can you share what you dataframe looks like, I had some trouble understanding what each column is about. Also, can't you just subtract the values from one column from the other and plot that? 
Will try a few of them out, thanks 
That does not work. If type is int, then it will fail the test and return false. Also, if a string is entered, the int(float()) calls will fail with ValueError and never get to the condition. This works: odometer_start = input("Enter odometer reading at the start of the rental period in Kilometer: ") try: odometer_int = int(odometer_start) print("Odometer at the start of the rental period is {} km".format(odometer_int)) except ValueError: print("Invalid Input: use only digit numbers 'i.e. 56'") 
Sure ! [Here's](http://paste.awesom.eu/NtYh) the new format. First column is optimization method: simulated annealing (SA) or "improved" simulated annealing (SA+GA). The second is self-explanatory, and disregard the third, it's the solution I found. What you said about plotting that wouldn't work with this particular dataset format. However, that's what I wanted to do with the first format, which looked like [that](http://paste.awesom.eu/Q8Si). Thanks for trying to help me !
What about the changes that happened in the meantime?
Excellent toolchain. The next step you may want to consider is looking at the tutorial videos on pyvideo.org. There are some excellent tutorials there on working with data. Welcome to the community. 
/r/Python is an *obscure* subreddit name for dealing with pythons? :S
Oh fuck. Sorry, it seems like replacing `snb.violinplot(data[['SA distance', 'SA+GA distance']])` by `snb.violinplot(data=data[['SA distance', 'SA+GA distance']])` did the trick. To think I was messing with the x and y parameters to make it work ... I'm quite ashamed of that rookie mistake. Thanks anyway ! 
More specifically, I think this should do what OP is looking for: from scipy.spatial.distance import cdist weight_array = cdist(A, B).sum(axis=1)
Run the script a second time on only those changes? You could take the production server down for the 2nd run which should be much quicker. 
Just put a big VIM &gt; ALL Sticker on the banner and call it a day. 
There is also [relation](https://pypi.python.org/pypi/relatorio) which looks like quite similar to Pod.
I didn't know there was a specific subreddit for snake care, I typed in `/r/snakes ` to check out if it was. \^_\^
Ok. I was going to write more about the scope of self, but I see from your example that I should look at the parameter list as the list of everything the method will use. I'm not sure if I'm going against a PEP if I don't include self if I don't use it (I expect my IDE will tell me once I get off my ass and start programming again). Thanks for the explanation. 
I'm a fan of using the logging module for my console output and giving my programs a --debug option. That way if someone has a problem, I can just ask for debug output to fix it :)
Ah yes, I use it for most things, didn't think about being able to turn on debug through command line though!, love it!
I manage our Jenkins at work, but not much python dev there! I'll have a look at PyTest if it does coverage as well. I must say, I never use virtual env, but I always try and use core functionality over 3rd party libraries, so not sure if it is fully applicable (although I may be fully misunderstanding why I should be using it!)
They're lighter weight than dictionaries and provide indexed access to field as well as named attribute lookup. They sound more like a Python take on `struct` than anything dictionary-related.
By four years, no less.
[click](http://click.pocoo.org/5/) too 
I find the itertools and collections modules getting imported into my code time and again. In addition, you might look at [the boltons library](https://github.com/mahmoud/boltons) which provides a notable enhancement to the python stdlib. I'd suggest [quickly scanning the docs](https://boltons.readthedocs.org/en/latest/index.html) for modules you import. If you see anything familiar, check what boltons adds
Also `enum` (included in 3.4, `pip install enum34` for older versions).
Virtual env is more a way to install python packages and not muddy the system python packages. Plus with virtual env you don't need super user privileges. It will help you manage multiple projects running on the same machine without colliding in terms of dependencies. If you are maintaining libraries that need to be working across Python versions tox will automate running tests and such across different virtual envs. I for one use virtual env to make sure I know exactly what I need to reproduce the Python packages on a production or test machine. 
One of my plans for mypy is to give (optional) warnings if you don't completely specify things - though better inference is also on the TODO list. Also it's `Union[int, float, str]`.
Hey I appreciate that! I started developing about a week ago according to GitHub, and I launched it on Heroku 3 days ago, so 4 days? (Hence all the very rushed code in here)
I have some common code I use to set debug from cli or environment variables. PROGRAMNAME_DEBUG is a common one I use.
I always use `py.test` instead of `unittest` because it leads to more readable tests (just write `assert x == y` instead of `self.assertEqual(x, y)`, does not require wrapping stuff in pointless classes, and supports all sorts of useful features like parameterization and proper fixtures. *** I'm also a huge believer in PEP 484, to the point that I believe *all* code should be fully annotated. I'm maintaining a fork because JukkaL is not very active: https://github.com/o11c/mypy Note that most interesting features are found on branches other than `master` and there are likely to be issues with `./setup.py install` because that is not tested (mypy is definitely in alpha state and there are other problems that are more important right now. But I still definitely recommend using the git version and not the `mypy-lang 0.2` version on pypi).
How do you implement this --debug option ?
I've never used numpy, but the documentation for vectorize says &gt; The vectorize function is provided primarily for convenience, not for performance. The implementation is essentially a for loop. This isn't an area I'm familiar with so it's possible I'm missing something 
But they are not connected to each other. One is a machine in work, then a desktop at home, and netbook.
They do more in a day than you have in your entire life.
That's because most of the people work on more than one project. 
My recommendation is argparse, it's usually more than powerful enough and comes with python. You can even pass in a list of arguments (default of None means that it uses sys.argv) so that you can automate testing of your command line arguments too. 
Depends on the size of the project, but for my site I just did a simple python, pip, virtualenv, and fabric. I have the source for everything here: https://github.com/nficano/nickficano.com
IMMUTABLE 
[nose](https://nose.readthedocs.org/en/latest/) is an incredible testing tool: it discovers tests and run them on the glance!
A trick I like is using repeatable `-v` and `-q` to select the logging level. The logging levels are actually integers with a 10 increment (DEBUG is 10, CRITICAL is 50). It's very easy with argparse's `count` action or click's `count=True`: parser.add_argument('-v', '--verbose', action='count', default=0) parser.add_argument('-q', '--quiet', action='count', default=0) logging_level = logging.WARN + 10*args.quiet - 10*args.verbose # script -vv -&gt; DEBUG # script -v -&gt; INFO # script -&gt; WARNING # script -q -&gt; ERROR # script -qq -&gt; CRITICAL # script -qqq -&gt; no logging at all
They're useful for replacing dictionaries where you always have the same keys, or for basic objects. IMHO they're neater than dictionaries. A nice basic example from [python docs](https://docs.python.org/3/library/collections.html#collections.namedtuple) &gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y']) &gt;&gt;&gt; p = Point(11, y=22) # instantiate with positional or keyword arguments &gt;&gt;&gt; p[0] + p[1] # indexable like the plain tuple (11, 22) 33 &gt;&gt;&gt; x, y = p # unpack like a regular tuple &gt;&gt;&gt; x, y (11, 22) &gt;&gt;&gt; p.x + p.y # fields also accessible by name 33 &gt;&gt;&gt; p # readable __repr__ with a name=value style Point(x=11, y=22) It's shorter than a full class definition, and it's neater than a dictionary, since `point.x` is a lot easier to read and shorter than `point['x']`.
If you like pylint, also check out [flake8](http://flake8.readthedocs.org/en/latest/). It combines the pyflakes code checker and the pep8 style checker. I find it tends to have a lot fewer false positives than pylint, but isn't as thorough. That means hunting through the pylint output for the real problems is a lot harder. So I usually use flake8 first, then once that reports my code is clean I use pylint to get the few remaining issues. I also find flake8 is more useful as live issue reporter for my IDE because of the fewer false positives.
Most Python IDEs these days (and even some text editors) have a live linter like this. So you should just choose the IDE you like.
Vectorization means you don't do things like: y = [] for i, xi in enumerate(x): # lets say x is a list y.append(xi*2. + i + 3.) and instead do: from numpy import arange, array x2 = array(x, dtype='float32') y = arange(x2.shape, dtype='float32') + x2*2. + 3. However, this is not vectorization... from numpy import arange, array x2 = array(x, dtype='float32') for i, xi in enumerate(x2): # lets say x is a list y.append(xi*2. + i + 3.) y = array(y) However, if you really had to do that, this would be better because it avoid issues with list resizing. from numpy import arange, array, zeros x2 = array(x, dtype='float32') y = zeros(x2.shape, dtype=x2.dtype) for i, xi in enumerate(x2): # lets say x is a list y[i] = xi*2. + i + 3. It doesn't mean you use the `vectorize` function, which is something you really shouldn't be using as it supports callbacks. Vectorization means use the numpy/scipy methods with the correct data types and make your flat, rather then nested. However, methods like `map` and `reduce` are not considered vectorization, even though they are flat. Practically, it means you get down into C as much as you can so you can avoid type checks. If you're not defining data types, you're probably doing it wrong (though you can make methods that work on float/int data). If you're using for loops, you're probably doing it wrong.
I usually use [docopt](http://docopt.org/). It is simple and very light(it is 500 lines in total). [This video](https://www.youtube.com/watch?t=286&amp;v=pXhcPJK5cMc) is the reason I started using it.
Thanks for the insight!
For most of my scripts I have used docopt, but click is really promising, I am looking forward to try it in a project; thanks for mentioning.
I love docopt and I use it, but I do wish docopt was still in development. It's got more than a few limitations regarding names of variables.
You get the speed and immutability of a tuple, combined with the lightweight access of field retrievals (`mytuple.foo` instead of `mytuple[2]`). Hashing, equality, iterability, etc., are still there just like for a normal tuple. Dictionaries should be slower to construct and access, though I haven't benchmarked it. They also don't catch errors where you typo the name of the key you're writing to. Named tuples are hard to extend though. See my [simplestruct](https://github.com/brandjon/simplestruct) package if you're looking for a more extensible namedtuple alternative where performance isn't paramount. (Scroll down for a feature comparison table.)
Good call mentioning click. I've really been enjoying it.
I'm not a fan, honestly. As much as I'd like to see changes to argparse's API I feel like docopt's syntax leaves plenty of ambiguity.
Ahh, the good old web :P. Sometimes I like to think the web is great and fun, but then I remember from comments like this it's just a big ball of duct tape. If you don't mind, what were you doing before you started using webdriver + phantomjs?
A good debugger with a Web interface [wdb](http://github.com/Kozea/wdb) (Shameless plug) 
Thats a fantastic little trick *steals*
What issues have you had with virtualenv+pip that docker solved? I've been looking into this for a while and I haven't been able to find a good answer, so I would love to hear your opinion.
&gt; Relying on external servers to download dependencies on deployment can be a dangerous game. Thank you! That is an excellent answer that certainly points out a weak point in my deployment process that I had never really considered. And it is certainly a good point against interpreted vs compiled as well, one I had never really considered before. 
One of my favourite things to recommend is markstory's [lint-review](https://github.com/markstory/lint-review), it's a simple Ci tool for flake8'ing your git diff's when creating pull requests. It also supports javascript, ruby, and a couple other languages. It's more of a CI tool than something each developer can have on their own machine, but it's still awesome. In a similar vein to PyLint there is prospector, OpenStack's [`hacking`](https://pypi.python.org/pypi/hacking), and flake8. Flake8 is simple and quick, so I tend to use that on everything, while hacking is a lot more pedantic, so I only really use it with packages on which I have more input. `tox` is a tool I fell in love with when first working on simultaneous Python 2 and 3 support, which I strongly recommend to people working on any libraries, though it's less useful if you're only working on closed source stuff stuck in Python 2, though it's still useful when considering the move to Python 3.
the best part of this video is how he says POSIX
My usual env set up is using pyenv with pip-tools. pyenv with pyenv-virtualenv allows multiple versions of Python with multiple virtualenvs, and automatically switching to them when entering your project directory. pip-tools gives you easy package freezing and syncing. Probably my most common modules are IPython (IPython notebook is amazing), pytest (test framework), requests (http client), arrow (datetime/tz handling), fabric (basically Makefiles for Python, deploy to remote hosts). Also I recommend PyCharm for your IDE. Handy tools outside of Python's scope: git with git-flow, docker with docker-compose.
Have you looked at photohash? It looks like ImageHash is a fork of that, and it is building for python 3.4: https://github.com/bunchesofdonald/photohash https://travis-ci.org/bunchesofdonald/photohash
&gt; Dictionaries should be slower to construct and access, though I haven't benchmarked it. &gt;&gt;&gt; def tuple_test(): Point = namedtuple('Point', ['x', 'y', 'z']) p = Point(x=0, y=1, z=2) return p.y &gt;&gt;&gt; def dict_test(): p = {'x': 0, 'y': 1, 'z': 2} return p['y'] &gt;&gt;&gt; timeit.timeit(stmt=tuple_test, number=1000) 0.3581736238198661 &gt;&gt;&gt; timeit.timeit(stmt=dict_test, number=1000) 0.00027772752848420623 &gt;&gt;&gt; 
What do you do when the point moves? Namedtuples are immutable, so you can't update the coordinates. You also can't set some keys now and some keys later when their values are available, you have to have all values ready when creating the tuple.
For whatever reason, my phone kept trying to put in parentheses rather than square brackets. I had to correct it for the `List`s, but I missed it for `Union`. Thanks!
Please do not work for free or "little money". You have skill that is in a very high demand, don't sell yourself short like that. Either charge an hourly rate or start working on a personal project.
Ipython notebook. Great reporting tool where you can hide code and show output only with a little JavaScript.
A lot of the time I use it purely so I can use pip freeze to have a record of the requirements.
[removed]
Do you actually looking for something? Drop me a pm
Well many Python libs will rely on some system lib. So when I develop on my system, (archlinux), it is not always easy to get the versions of libs I need in production. Using pip+requirements.txt helps for any pure Python package, but as I said some Python packages need specific system libs. Virtualenv is not relocatable and still relies on the right system libs present when packages are installed. This makes it non-portable in a practical sense with out using requirments.txt. Using docker, I can document my dependencies via the docker file, and use jenkins to build images automatically based on updates to my repositories. This builds a static binary file that I can hand to my sysadmins so they can easily deploy my app on our centos boxes with the only dependency being the docker runtime and a webserver on the host for reverse proxy. 
Are you using the Mesh class in the `stl-numpy` package ? If yes it seems to have a `save` method.
I use virtualenvwrapper. It's an extension of virtualenv and easier to use.
ConfigParser.SafeConfigParser is a really nice configuration parser in the standard library. You can easily write the same boilerplate code every time for loading this up during initialization.
`namedtuple` will be much faster if you move the definition line out of function. Though still slower than dict.
If you do development work for multiple python projects on the same machine that use third party python modules (stuff installed via `pip` or `easy_install`), then you really should be using it (or some equivalent). While pip normally tries to install things to the system-wide python directory, using virtualenv, packages get installed to the local virtualenv python-directories. This can be a huge lifesaver, if you develop on dozens of projects and something you wrote 5 years ago was used some library (say using django=1.0). Now you have a new project and you want to use django=1.8 features, but don't want to break your django=1.0 app (or do the sometimes painful upgrades to get that app working in django=1.8). Virtualenv lets you accomplish this. It also makes keeping track of a project's dependenices easier (if you want to move it to a different machine), if that project has its own virtualenv (run `pip freeze` in the virtualenv) versus needing to install everything in the global python projects directory to move the project to a new machine. It's pretty simple to get started (assuming linux/unix), you just run `virtualenv &lt;name_of_new_venv&gt;` which creates a new virtualenv. I typically go to the place I want my virtualenv (e.g., `cd /production/`) `virtualenv venv_projectname` (where `projectname` is my project). Then in the commandline before doing anything in a project run `. ./venv_project/bin/activate` to start up the virtualenv (changes some commandline variables). Then just use pip/python as normal and it will use that virtualenv's directory as the source of packages. If you add `#!/production/venv_projectname/bin/python` as your hash bang line at the start of python scripts, it will use packages from the virtualenv in it. (And you don't have to enter the virtualenv environment).
Looks great, thanks!
As /u/pstch mentioned, the stl-numpy package has a save method. Otherwise, STL is very easy to write out yourself since it's just a list of vertices and connectivity: https://en.wikipedia.org/wiki/STL_(file_format)
Thanks for the info! Is there much of a disk space difference in using virtualenv compared to docker? 
Namedtuples aren't a complete replacement for dictionaries (otherwise they might as well just replace dictionaries with them), and no, if you need to replace the items inside the namedtuple, it's probably not what you want. The above Point example would be a bad idea if you want mutable points. You'd be better off with a different structure. But what if your points aren't mutable? Let's say I'm collecting voltage samples from a pair of ADCs. (Maybe I'm recording audio or collecting data from a physics experiment.) I could generate two lists of integers, or I could generate one list of tuples. I could remember that element 0 is the left channel and element 1 is the right, or I could say: Datum = namedtuple('Datum', ('left', 'right')) If you're looking for something like namedtuple but mutable, there's a third-party namedlist module. I have no idea how good/stable/up-to-date it is, but it exists.
Agreed. But when you don't need mutability, they're a great tool - as lightweight as tuples but self-documenting.
 &gt;&gt;&gt; from collections import namedtuple; Point = namedtuple('Point', ['x', 'y', 'z']) &gt;&gt;&gt; def tuple_test(): ... p = Point(x=0, y=1, z=2) ... return p.y ... &gt;&gt;&gt; def dict_test(): ... p = {'x': 0, 'y': 1, 'z': 2} ... return p['y'] ... &gt;&gt;&gt; timeit.timeit(stmt=tuple_test, number=1000) 0.0014165400061756372 &gt;&gt;&gt; timeit.timeit(stmt=dict_test, number=1000) 0.00021710898727178574 EDIT: That's in Python 3.4. EDIT 2: They're pretty much the same in pypy: &gt;&gt;&gt;&gt; from collections import namedtuple; Point = namedtuple('Point', ['x', 'y', 'z']) &gt;&gt;&gt;&gt; def tuple_test(): .... p = Point(x=0, y=1, z=2) .... return p.y .... &gt;&gt;&gt;&gt; def dict_test(): .... p = {'x': 0, 'y': 1, 'z': 2} .... return p['y'] .... &gt;&gt;&gt;&gt; import timeit &gt;&gt;&gt;&gt; timeit.timeit(stmt=tuple_test, number=1000) 0.002318143844604492 &gt;&gt;&gt;&gt; timeit.timeit(stmt=dict_test, number=1000) 0.0022079944610595703 
Man, that's really awesome...I may have to borrow that. :)
This is probably one of the biggest advantages. It's a shame that tuple is about the only way, short of writing a C extension, to get truly immutable objects in Python.
One way to make your testing even better is to leverage Gherkin and do behavior driven testing. [Behave](http://pythonhosted.org/behave/) is an excellent way to bring in BDD (which I think unit tests should all be focused on). Also, [assertpy](https://github.com/ActivisionGameScience/assertpy) is a pretty awesome little library for increasing flexibility and readability in your Python tests.
Yes, one of dockers negatives is its huge storage requirments. My simple Python apps clock in at around 700mb per image. For us the image never leaves our network, we have 10 gig E, and we deal with network storage close to half a petabyte so for us the storage issues are simple to work around and do not out weigh the benefits of simplified deployment and isolated dependencies. The sizes though do have me interested in other container technology or deployment methods as possible replacements for docker. 
Without a portfolio, it's extremely hard to hire anyone as a remote developer. From experience, when you work for free, you invariably run into the most entitled, worse clients. They will demand tons of work from you and make you believe working for them is a privilege. Therefore, I recommend building a portfolio. Either with sites you build for yourself or by contributing to open source projects.
I'm currently testing out http://blockhash.io/ unfortunately it's very slow, will take about 3.5 hours to generate a 24 bit hash for all my ~25,000 images, will post results later.
It works and the solution was fairly simple. Now I need to check if all elements in the array match a specific condition, but that's a much easier task. Thank you!
Absolutely. With the kind of freelance work I do I host all my work on Linode so have a 700mb + static assets + database would take up significantly more disk space than I want to pay for. However I can totally see why so many people like docker and why so many Python developers like using it instead of virtualenv. 
I've used click for a couple different projects. It's great.
http://numba.pydata.org/
I've recently been trying out py.test after using nose out of habit for some time. I've been having a hard time finding any justification for py.test at all but I suspect I'm missing something, so I'm keen for the opinion of someone who uses py.test. Can you point me at any examples that showcase the use of parametrization and 'proper' fixtures? Nose supports the `assert` statement just fine, and I find the class-based approach with `setup` and `teardown` methods tends to result in far *less* boilerplate than equivalent py.test tests for anything larger than a trivial application. For those smaller applications you can omit the class definition altogether.
i use pdb a ton in my projects for testing
&gt; redundancy I think it is good to have -h, --help. they are somehow standard in *unix systems apps. &gt; The breakage of standard expectations I didn't get it, probably because I don't know about GNU standards for args, can you please give an example? I probably am a newbie too, I didn't dig too deep into docopt, just enough to suffice my needs. :) for subcommands I use the following structure: &gt; USAGE: &gt; script.py foo ( --new | --uname=&lt;uname_value&gt; --pwd=&lt;pwd_value&gt;) &gt; script.py bar &gt; script.py baz ( --uname=&lt;uname_value | [-] ) then I have a couple of if/elifs. eg. &gt; if arguments['baz']: &gt; if arguments['--uname']: &gt; do_baz() Gist you provided is a better structured one but it makes it a bit complicated IMHO. So I won't use it unless my project is already complex(where it will be messy to have 7 if/elif/elses. Thanks for the link, definitely saving it for later reference. :D
virtualenv tends to be the more highly preferred solution to dependency isolation and version tracking for Python software in the broader Python community. However, if you ever find yourself using the scientific Python packages (numpy, scipy, pandas, etc.), you may want to consider using the conda package manager instead. I'd also like to give a shout out to Twitter's PEX utility, which enables JAR-like bundling of dependencies. I don't think it's mainstream yet, but it does solve many production deployment annoyances (assuming you deploy to Linux).
md5hash? Edit: my bad, you want perceptual hash, not a literal hash
Sublime Text 3 on Mac OS X and Ubuntu.
I've made a few games, and Python has provided the best experience for me so far. Everything the others have mentioned are great tools, but I'd also like to point out [sfml](http://www.sfml-dev.org/) You can find the python version [here](http://www.python-sfml.org/index.html) pygame annoyed me, so I turned to sfml. Give them both a try and use whatever you like best!
What about frozenset, frozendict and the like? Or just custom objects?
Check out the installer that does both at once, just because it's way easier: https://github.com/yyuu/pyenv-installer
Wow, still a factor 7x, didnt expect that.
As far as unit testing goes, I really dislike the built in unittest. I don't love the practices for testing it gives (though I guess testing at all is better than not). You may like `py.test` instead, I find that it has a lot of great features for running tests and making them easier to read and write. Also, there is a multiprocessing runner (I forget the name and am on mobile) that lets you run tests in parallel (separate processes) so it all goes much faster.
Encoding the ID using base-64 or similar is intended to just shorten the key, not obscure it. The issue with an auto-incrementing key is that someone can easily crawl the service to see what URLs are stored. This may or may not be a concern; if it is, randomizing the integer key, instead of using predictable keys, mitigates that problem.
Per http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html &gt; rcond : float, optional Cut-off ratio for small singular values of a. Singular values are set to zero if they are smaller than rcond times the largest singular value of a. Sounds like what you want.
*frozenset* is certainly another way. *frozendict* got rejected per PEP 416, and is available only as an outside library, which only kind of counts. It is the custom objects that bug me about not being able to easily make them immutable. I'd like to have something that behaved more or less like *case class* from Scala.
FWIW, I thought named tuples would be faster also. I also just tried only accessing (in QPython on my tablet, so not exactly the same environment as my workstation) and dictionaries are faster by a factor of 7.9. EDIT: &gt;&gt;&gt; from collections import namedtuple &gt;&gt;&gt; Point = namedtuple('Point', ('x', 'y', 'z')) &gt;&gt;&gt; p = Point(0, 1, 2) &gt;&gt;&gt; def tuple_test(): ... p.y ... &gt;&gt;&gt; q = {'x': 0, 'y': 1, 'z': 2} &gt;&gt;&gt; def dict_test(): ... q['y'] ... &gt;&gt;&gt; import timeit &gt;&gt;&gt; timeit.timeit(stmt=tuple_test, number=1000) 0.0098810195922851562 &gt;&gt;&gt; timeit.timeit(stmt=dict_test, number=1000) 0.0012459754943847656 &gt;&gt;&gt; .988/.125 7.9039999999999999 &gt;&gt;&gt;
...precommit hook to force test running. I feel like a doofus now, bout to go hack this out. 
[nose](https://nose.readthedocs.org/en/latest/) and [coverage](https://coverage.readthedocs.org/en/latest/)
Probably should be *and* and not *or*.
Python 2 or Python 3?
3
Surely hash length is a parameter? If you are generating short hashes you are likely to have collisions.
Don't automatically try to convert the input to float or int. Probably the easiest way to do this in Python 3 would be to get the raw string input, then use a try/except block on the string -&gt; numeric conversion, and catch ValueError if the input wasn't a number. If you're ok with just simple integers, calling isnumeric() on the string may also work. You could also use a regex, but that's probably way overkill.
I am from Ukraine. 
Not gonna lie, these two comments are the reason I decided to watch that video. I was not disappointed.
I like docopt but it either works or you're royally screwed. Their error reporting is nonexistent. I've spent hours bagging my head against the wall, trying to invoke my own script. You will not get `unknown option "--verobse"; did you mean "--verbose"?` from docopt. You won't even get `unknown option "--verobse"`. All you will ever get from docopt is a dump of your help message, which may not be helpful at all.
I recently had a need for functionality like this [and setup a framework](https://github.com/metaperl/image-difference) for comparing approaches. Most failed dismally for me. You can also look into OpenCV and stack overflow for answers.
Yes, search '[--]' in [docopt's home page](http://docopt.org/). &gt; Double dash "--", when not part of an option, is often used by convention to separate options and positional arguments, in order to handle cases when e.g. file names could be mistaken for options. In order to support this convention, add "[--]" into your patterns before positional arguments.
They're also immutable, which is often underrated. (or roughly immutable, anyway -- this is still Python and there are still ways to break that, of course)
Yes, but since they did more bad than good, that's not a great statement.
These huge sites always seem so sophisticated and secure on the surface, but they're built and maintained by humans who make mistakes just like we all do. It's simultaneously relieving and terrifying to me as a developer to know that the Internet can be such a disorganized shitshow. 
&gt; disorganized working as intended
Check this out, it's a pretty neat comparison of argparse, click, and docopt: https://realpython.com/blog/python/comparing-python-command-line-parsing-libraries-argparse-docopt-click/
I use logging for pretty much everything. And at work I write a lot of sever-side apps. Lots of them with CLI output. I've recently ripped out a logging formatter out of one of my projects and packaged it up as a [Python package](http://gouge.readthedocs.org/en/latest/). So I can have the same formatting on each application. And all that with a simple one-liner. It uses `blessings` to put some colour into the log messages. It currently only has one "theme". And I do not intend to change it except fixing bugs (if it has any). If I want to try another "look", I'll add it as a new theme. But that's probably not going to happen any time soon ;)
Oh, that's way cool. Much better defaults than progressbar. Thanks!
That is why there should be a security audit. Leaving the debug mode on is no excuse, this should be first on the list of things the audit needs to check.
They left debug mode on on purpose -- this was a dev/debug server. The problem is that the dev server was publicly accessible...
your right but there isn't an option to specify hash length
And using live data on it too.
No, sorry. There's no "mistake" about "working to mitigate" the debug flag. It's a single fucking flag, set it to false in two seconds, what's taking so long? Their site must be horribly shoddy if changing a flag that gives attackers RCE on your everything takes weeks. 
[clize](https://github.com/epsy/clize) too. The arguments are guessed from the method's arguments !
Since it has been mentioned a few times now that it's too easy to leave this on accidentally the next release of Werkzeug will require a PIN to be entered upon first use of the debugger. The PIN is generated from some (hopefully stable) information from the application and should thus be the same between restarts. It's stored in a cookie for up to 8 hours so should be good for a day of work before it prompts again.
It's afternoon somewhere in the world.
I use numpydoc for this, and vim's pythonmode etc is aware of that too.
It's a step. The fact that Flask's debug mode was so easily left on, combined with the Werkzeug debugger being so "permissive" (a web-based Python REPL), I always felt nervous having it around at all.
That actually implies that you can just have them as seperate functions or even classes (since the only interface involved is a string, and all of those can have a docstring). They can probably even be instances, but I can't think of a sensible use for that that wouldn't already be handled by subclassing. (but no, that file also implies that docopt -doesn't- have subcommand support. The subcommand handling is entirely done by git.py.) 
Check out py-phash, and use a hamming distance for image similarities
Unless people access _protected fields, making an immutable object with properties is easy enough... Mutating a frozenset is possible too, if you're going to violate convention and common sense.
lol
I think by default it is only available from 127.0.0.1 and the Patreon devs changed it to 0.0.0.0.
SciPy or various PyCon events usually have good videos online if youre not up for attending.
[removed]
"have a linter" yes. "Like this" no.
Dressing gown, sandals. You're NOT getting images. Nice try lad.
It was a development server, I'd have no problems with debugging being on. What the heck is the point of a development server you can't easily debug? The largest problems were: * As a development server, it was on the open web. It should have been categorically firewalled / private networked off. * It was running live consumer data.
That's what makes it "not a dev server". Anything with access to production data isn't dev at all. We run our own "dev" servers as well, and they're open to the public, but they have fake data, have debug turned off and are not connected to production in any way.
Damn, /u/Rocka07, I just saw it, now you deleted it. I really doubt people want to help with your homework.
Panda3d is really good and in active development.
&gt; We run our own "dev" servers as well, and they're open to the public, but they have fake data, have debug turned off and are not connected to production in any way. I'd easily call those testing.
I gave this up and alttab between sublime and a commandline. Alt-tab, cursor up, enter. Good luck!
You've made a text post. Text posts do not include the link that a link post does - it's either/or, not both.
Do you even timezone bro?
what features does pillow have above opencv
Thanks for that, as I understand conda is only for Anaconda python installations. I was actually looking for an .exe installer (or whl compiled file).
How, exactly, is it different from what other IDEs can do?
Easier to download one thing rather than 5 things; and definitely easier than the pain of compiling it which as you say takes a long time and in my experience often fails half way requiring additional dependencies to be installed. Also conda seems to be simpler than virtualenv; can be used the same way in python2.7 and 3; and handles non-python components.
I am using with ipython notebook and not had any issues. Also have just started using spyder which is an IDE that embeds ipython and is included in anaconda.
lint-review keeps me honest.
easily installable? 
Are you using ipynb on Linux or Windows?
I think this is a good idea. With venvs, a postactivate hook could be made that sets the envvar based on whatever condition.
Look at what it requires. &gt;Youtubedl It's just doing some clever heuristics shit to figure out what song you want, then downloading the audio from the video of a youtube search.
Huh? Could you clarify what you said?
Too bad it doesn't say what Pillow is/does.
I love using these too, but I often don't because I try to write most things to be python2 and python3 compatible and the syntax is illegal in &lt;3.5 (iirc) :(
I don't have a CS degree at all. I'm constantly surprised and confused by the number of people with CS degrees that have these sorts of gaps in knowledge. I mean when I was learning to program I built linked lists, programmed sort functions, and read extensively about optimization. What I think is even more frustrating is the amount of hand holding that is sometimes required. It's like they've never had to complete a project without explicitly being told how to do it. Some really don't even know how to read documentation or look stuff up without some direction. Also it seems like some of them have never even glanced at a style guide and their code is insanely frustrating to follow. On the other side of this I've met people that had CS degrees that had a much firmer grasp on concepts and how to apply them then myself. That should be the norm in my opinion considering I'm almost completely self taught. I do have a strong math background which helps but I'm by no means an amazing programmer. I honestly think if you devoted four years to a CS degree you should be at least on the same level as my dumbass who learned to program in my spare time. 
&gt; No, sorry. There's no "mistake" about "working to mitigate" the debug flag. Uh, do you know what the word "mistake" means? 
I do these little scripts all the time myself. Actually, you might be interested in hosting [pytrending](https://github.com/kootenpv/pytrending) locally. I was monitoring reddit/github/twitter exactly for the same purpose :) Screenshot: http://screencloud.net/v/30pL
Building proper assessments is difficult, and most CS professors shy away from doing it correctly. It's hard to know and prove in a class of 100 students that some percentage has inadequate knowledge of the scope of CS and should be denied graduation. Not that it's impossible. Techniques like Instructional Design can help with this, but CS Education is only slowly becoming aware of these formal processes.
There's a strange beauty in that level of incompetence
Pillow is a drop in replacement for the python imaging library, PIL
What has really blown my mind is that my programming contributions to science tools while working in labs in college in an entirely unrelated field (which I have MA in) ... seemed to be of greater note than the doctorate dissertations of Stanford educated Phd's ... and certainly left me as a more capable developer.
Yeah, not if you're the one putting out the fires :P
Seriously I just want to learn and you DV me. 
Kivy tends to be good for this kind of thing due to its origins as a multitouch gui framework (PyMT), also aimed at different kinds of input devices. It's probably possible (and potentially easy) to do it in wxpython too, but I don't know any details. Edit: Although disclaimer: I am a Kivy developer.
I disagree on their claim this was a dev/debug server. It was publicly available and had their ENTIRE database. They also claim not much critical data was released, which is bunk: They have a good amount of personal data on file. Either way, it was a terrible mistake and I've had to: 1. Change my password. 2. Cancel a payment card.
Sorry, fixed! ("Pillow is the friendly PIL fork by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors.")
I've been working professionally in software engineering for 16 years. The number of times I've had to know big O notation and linked lists is exactly **0**. However the people interviewing me for various jobs over the years have regularly brought up both -- as if it hand any bearing on the job required. The reality is that quality software engineers are often dismissed by people who decided that if they personally had to burn 4years and a tens of thousands of dollars on a degree, then every candidate they interview must make the same mistake or they won't be hired. All this is fine with me of course. I don't need to be surrounded by a bunch of people who are all so proud of themselves for having a degree that they deploy sites with the debugger on. There's plenty of companies out there that have figured out the true value of a cs degree and they pay my salary just fine.
They both "work" for some value of "work"‚Ä¶ of course and the "main difference" is (if I had to pick one) Pillow is actively maintained while PIL is not.
Of you set `DEBUG` from an environment variable, the chances of this sorry of thing making it into production is limited. Of course in this case the problem was the decision to run a dev server on the open internet. There's really no saving you of your team thinks that this is a good idea.
I have taken a break from my professional career to finish my bachelor's degree in CS. I was so excited to be able to work with doctorates in applied mathematics and computer science. However, after classes began, I cannot describe to you the absolute horror and deep disappointment I experienced when I realized the level of incompetence, laziness, and ignorance being spewed forth at these poor kids by the professors. The faculty are so far removed from modern developments that they have created this twisted chimera of programming standards from 10+ years ago interspersed with the latest "hipster" trends cherrypicked from the "cutting edge blogs" and tossed into the curriculum to make the department seem progressive and modern. I could go on and on about all the things I've witnessed there but I need to stop writing about this for the sake of my blood pressure.
Downloading the audio or doing search heuristics?
which alternatives are they? I only know about argparse, docopt and click.
I can't think of anything that would be in any way secure still or work with multithreaded environments. In any case you can easily disable it by exporting an envvar.
Type annotations in general are available from (IIRC) 3.2. The typing library was introduced in 3.5, but it's available back to 3.2 via PyPI. 3to2 will cleanly remove any annotations, so if you want annotated source code but still need to work in python2 (or, for what I care about, still need to work in pypy), you can use 3to2 to make it happen. In fact, you can specify to 3to2 to only remove annotations, which allows it to run in Python 3.0 and 3.1 as well using the command `3to2 -f annotations -w somefile.py` (careful: this will overwrite somefile.py, but it will leave the original in somefile.py.bak. It's useful for installing or preparing packages, though). An example: lengau@hyperion:~$ cat test.py #!/usr/bin/env python3 from __future__ import print_function def somefun(a: int) -&gt; str: print('Test') somefun(2) lengau@hyperion:~$ 3to2 -w test.py &gt; /dev/null 2&gt;&amp;1 lengau@hyperion:~$ cat test.py #!/usr/bin/env python3 from __future__ import absolute_import from __future__ import print_function def somefun(a): print u'Test' somefun(2) lengau@hyperion:~$ mv test.py.bak test.py lengau@hyperion:~$ 3to2 -f annotations -w test.py &gt; /dev/null 2&gt;&amp;1 lengau@hyperion:~$ cat test.py #!/usr/bin/env python3 from __future__ import print_function def somefun(a): print('Test') somefun(2) As long as the rest of your application works with both Python 2 and 3, this is a convenient way to strip off annotations for when you actually need to run in Python 3.
If you have both IPython and ptpython installed normally, you should be able to run it by executing `ptipython`. No additional packages are necessary.
Completely agree ... I've been at more than one company that has completely imploded trying to implement their concept ... and fumble not with the hard stuff but fumble trying to code some very basic shit. The last place had a 5 second load time for their JS within the first 3 months of development ... basically 10% of the JS code done and a completely un-sellable product. That wasn't even the biggest problem though. The biggest problem was that because the CTO and lead developer didn't disable their caching they were completely un-aware of the problem ... and in complete denial when informed by someone who wasn't a partner.
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: https://www.reddit.com/r/Serendipity/comments/3nhkta/patreon_is_a_flask_app_and_was_hacked_because/
I personally don't believe CS education needs to be modern. It just needs to be solid and comprehensive. If a non modern language like C89 is the language of choice used by the instructor yet the students firmly grasp the basics like pointers, memory allocation, recursion, then I believe the purpose of the CS program has been fulfilled. I feel like if you're teaching CS right, your students will hate on you for being out of date, all the while learning the fundamentals they need to know to keep themselves up to date post graduation.
If I write: assert v == 1 and the condition is false, I get an assertion error. It tells me the assertion fails, not why. So I have to write assert v == 1, 'v does not equal 1' but with an assertion framework like this one (or python's unittest) I can write something like assert_that(v).is_equal_to(1) ## or .assertpy self.assertEquals(v, 1) ## unittest and I get a useful message when the assertion fails, without the extra effort. Granted sometimes I need to add an explicit failure message, but usually not.
This is great, thanks! It usually for coworkers that want my script - so maybe I'll point them here :)
https://wiki.python.org/moin/OrganizationsUsingPython
I literally just ran into a mobile site that would crash an IPhone 6 due to overuse of composite layers. They were trying to sprite image assets to address the performance issues... I always say that writing code that performs functionality is the lowest bar. I don't care. Does it scale with users? Will it scale with a bigger team? If the requirements change just a little bit, will the whole thing have to be rewritten? I blame the current cult of pragmatism that prioritizes "getting shit done" over "getting good done". They're only mutual exclusive relative to skill level. We seem to think the tradeoff is a drastic immutable truth that applies equally to everyone. 
ahh. I remember that cringe and facepalm moment when I understood that our dev server was publicly accessible, with some stupidly easy password set. Gladly we fixed that quickly and will not repeat mistakes of such proportion.
www.google.com
It does have a bearing. You will never implement a linked list yourself, but you need to know when to use one and that requires knowing how to implement one. You should never use the default Python list if you're mutating it a lot, as it is implemented as an array. Same as you should use `null aList` in Haskell instead of `length aList == 0` since Haskell lists are linked lists. This is not micro-optimization. Using the correct data structure can make a program run orders of magnitude faster. Especially with common structures like lists. Again, you'll never implement it yourself but you NEED to pick an implementation and that requires you to understand the tradeoffs of each data structure. People who don't understand data structures often implement their own shitty versions of them because they don't know better. Don't be one of them.
You ever see a story that makes you wonder why you're poor and other people aren't? This is one of those stories for me.
I recall my first job where I was brought on to a Flex app that was running behind. Flex apps had similar pitfalls that SPA have today, mainly dealing with state management and side effects. They were essentially building the app like they would a regular web page and had stale data issues everywhere. I came in and followed a more architected framework (gasp storing the model separately) after deep-diving the flex framework and consuming tons of info from the community. The head of R&amp;D bitched about how my code made no sense except it was the only parts of the app that just worked. I ran into a similar situation my next job where the "architect" derided my solution as being "wrong" but it solved the problem and he couldn't think of a cleaner way. Soon after I replaced him and his counterpart, so I haven't run into that BS since then. But yeah. Due to the talent bubble and the inherent inability of engaged non-technical people's ability to evaluate developers, you get areas of inefficiency where something closer to mysticism and politics are the dominant drivers. People who should've been weeded out are allowed to live whatever narrative they want. The lack of self-awareness is boggling at times. Those same people I mentioned will still claim they were foiled by bad requirements, stupid product managers, incompetent managers, etc. I even saw someone that was let go for incompetence give a presentation on a project that I had taken over and saved. 
Well that is a fair point. Also I thought I did post this there. WELP.
Radon looks really cool, I'd like to use it, but `cc --min` does not work. If I run something like `radon cc -s --min 5 my_file.py` it still shows methods with complexity rating of 1. I figured maybe I was confusing things and tried using `--max 5` and then I get no output. I'm running it on Python 3.4.1 
I second this. Native assert doesn't tell me squat, especially when I'm trying to complex a number of relatively complex things cleanly.
As someone who is learning postgres, that second one hurt me deep.
Quick question: Why Mando instead of something like Click or docopt? Not that Mando looks like a bad idea, I had just never heard of it.
&gt; And i'd wager that performance worry-free jobs are a small subset of the programming jobs out there. Aside from my first, I've never worked at a place where algorithm efficiency wasn't at some point a consideration. In Web development, algorithmic efficiency is often very far down the list of performance concerns. I'd much rather have someone who knows how to write efficient SQL, avoid N+1 problems, use caching effectively (and take advantage of HTTP features for it), deal with replication and/or sharding of databases, make use of task queues, how to dodge common security issues, etc.; knowing whether something is a linked list, or being able to quote the performance characteristics of operations on a linked list, is so rare a thing to need to worry about that it should not be an interview question for anything Web-oriented.
Don't know of any libraries but was pretty simple to write out the code myself when I needed to do it. Give me a little bit and ill upload my code.
Ok thanks, have any stats for your method? 
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." Show them that you've tried to solve your problem and you can get all the help you need. Cheers &amp; best of luck!
Installers for Python 3.5 are not yet available. I believe they will be in PyQt 5.5.1 (at least it seems so juding by the [changelog] (https://www.riverbankcomputing.com/static/Downloads/PyQt5/ChangeLog-5.5.1-snapshot-32c52859b93a)).
Uh, won't this fuck up branch offsets?
Haha, okay, with *write* access to production data. Copies of production data are fine.
You can install Pillow 3.0.0 using conda.
How does this differ from pyenv?
From the google search of your question: http://stackoverflow.com/questions/16086962/how-to-get-a-time-zone-from-a-location-using-latitude-and-longitude-coordinates Asked in 2013.
Pip or GTFO
You stole my wife's name. Freaky. 
I don't get what you mean by create and manipulate. I can do the same in opencv. Resizing, drawing primitives etc. 
&gt; fabric+virtualenv+pip With the option of something like ansible too if you're into that kind of thing.
**Glorious.** At last a way to make Windows/Linux/OS X binaries with one library.
To create something that could run effectively on lower grade machines without a GPU, while still keeping performance.
Wait, I didn't get it. The wrapping your python app part... Does it make an .exe file in a windows environment? So non developer people can run it easily?
4 million records isn't much tbh. You need a database it sounds like. Looked at my personal favorite postgres or also mysql. You can use python with an orm if that's the route you want to take
Can someone please explain to me how this differs from cx_freeze or pyinstaller?
Lol, I know what you mean
Would be nice to add some sort of decorator to native assertion to have nicer error message? Or this would stand against Python philosophy?
How well does this work with CENTOS 6? Fucking hate working with Python 2.6 on centos 6. I end up writing dict comprehensions often and forget I need compatibility with 2.6. centos6 needs to die.
I'm not so sure about that - I got numpy on an Ubuntu 12 distro recently using the sudo apt-get method and, whatever it was linked against was around around 40 times slower than once I got it running with ATLAS. Took me forever to find that difference in runtime for my code on my local machine and our server was in the matrix dot multiple function!
I don't understand the point of this comment. I can download a website with curl and google chrome.
above that you are calling r = praw.Reddit(&lt;something&gt;) right? Is there somethign funny about the input you're passing into praw.Reddit? Is it not ascii, or where do you get that data? Looks like when it tries to login it's using your credentials and for some reason it's not what they expect.
Again, that's not on the user of instantmusic, it's on the uploader of the video.
Sounds like a [bloom filter](https://github.com/jaybaird/python-bloomfilter) might be the best bet, if I understand your problem correctly. Basically, you do check `if imghash in bloomfilter` and you'll know you never ran `bf.add(imghash)` if it's false, with a very low error rate if false positives (0.001 default I think). There IS an error rate inherent with bloomfilters though, but only with false positives. With a bloomfilter, you save memory by sacrificing a perfect false positive rate. If it says an image is in the bloomfilter, it very likely is, to an error rate you set. If it says it's not in there, it DEFINITELY isn't in there, 100%. No false negatives, very low false positive rate. And you set the false positive rate. The lower it is, the more memory the bloomfilter takes up. You can easily work with a billion objects, take a new one and figure out quickly if it was in there. You can also save hte bloomfilter to disk or load it, so you keep the results. You can add things to it, but you can't remove. This data structure is for saving memory while being able to quickly detect duplicates, or if you've seen a particular bit of data before. Does that solve your problem? I've worked with that library to dedupe in the past and had great results. It's very quick too, and can handle a huge dataset. You just have to be okay with it saying something is a dupe very very rarely, when it's not. Edit: Oh... might not be exactly what you want. Just re-read and noticed you want "near-duplicate" detection, not byte for byte duplicate. This might be very interesting for you: http://www.cybercrimetech.com/2012/09/similarity-comparison-with-sdhash-fuzzy.html Using a bloom filter and fuzzy hashing you might be able to quickly and easily detect if you've already seen a similar image. The magic is in the fuzzy hashing though. It needs to be specific to the feature sets you're interested in, and that's a very hard problem, but plenty of resources out there to look into how to fuzzy hash images. But it's very specific to what you care about. Should an inverted image be similar? Should it be similar if it was rotated 90 degrees? Should it be similar if the colors are really close, but one is a red car and one is a red flower? There's no one size fits all algorithm/pythonlib here. The magic is in how you fuzzyhash your dataset.
Every time I read big O, I have to Google it again. I guess there's something wrong with me.
Well, docopt seems a really good idea, but initially I needed something 'programmable' and not declarative. Later on I refactored the code and I could probably make it work, but I don't think it's really a priority. I checked Click too and it had too many things I didn't need. Mando, on the other hand, is minimal and is really good at abstracting away all the argparse madness (read verbosity).
It is better than ActivePython? 
legal status aside, I think this is a nice idea. Sadly it doesn't work very well for me, I tried it with lines from a couple of well known songs and what mostly happened was that it found me some obscure song that hat that exact line as its title. That is perfectly understandable but sort of limits its usefulness. A short list with possible matches (maybe grouped by if they match the title or the lyrics) would be cool.
It literally just finds the first result on YouTube uses YouTube-dl to extract the audio and give you an MP3. 
There's a conceptual flaw with the premise of this library. It assumes holidays can follow rules, which is not true. Some countries for example have election days both at fluid dates and they are public holidays. A library like this can't generally be used to figure out if a day is a holiday or not, it can only be used to say if a day is a holiday or if it's unknown.
I know. I'm not about to waste a lot of my time to install eclipse or some other shit just to figure out the many ways it's broken this year. Suffice it to say, every time I've tried another IDE it's been the same story.
Conda is love. Conda is life
Can you maybe paste more code? Are you possibly setting your password as an integer that should be a string of integers?
PyQt4 wheels for Python 3.5 32-bit and 64-bit are at &lt;http://www.lfd.uci.edu/~gohlke/pythonlibs/#pyqt4&gt;.
Well `argparse` comes with python.
What about contributing to existing game engines?
I think it is off topic
Well I *thought* that was supposed to be an advantage of getting a distro package... Looking at some google / stack overflow answers, it appears you need to also excplicitly install atlas / openblas as well as numpy for it to use it.
I didn't manage to get sublime to run python and it seemed a waste of time to deal with that. I use the commandline to run my python scripts. All it needs is an alt-tab. If the commandline window is too small I use "mode con cols=x lines=y" to extend it. "Cursor-up" brings up the last entered command. Understandable?
`pip install conda; conda init` works everywhere, and is not tied to Anaconda or Miniconda. 
For Win and Mac you find numpy and scipy wheels at PyPI and binstar.org, for Linux this is more problematic, but you can build your own wheels for deployment...
No, it won't work on Windows, because it is written in bash. It just downloads and installs a certain Python version with Anaconda and sets it up so you can run a Python package without manually installing its dependecies. There is no single executable that gets created, but a huge Python installation with thousands of files, in which your program gets executed.
I find it easiest to think about variables in python as names. There is some data somewhere, and you can give that data one or more names. Each piece of data can have any number of names, but a given name can only be used for a single piece of data. So if you later give that name to another piece of data, the original data is still there (if there it has other names), but it no longer has the original name. Python keeps track of how many names a piece of data has, when there are no longer any names Python will throw out that piece of data when it is convenient for it to do so. So when you "assign 42 to the spam variable", what you are really doing is "create a piece of data containing the number `42`, and give that piece of data the name `spam`". When you "copy the value in spam and assign it to the variable cheese", what you are really doing is "get the piece of data named `spam`, and give it the additional name `cheese`". When you "change the value in spam to 100", what you are really doing is "create a piece of data containing the number 100, and give it the name `spam`". So whenever you are using `=`, you are not actually moving data round, instead you are giving a new name to an existing piece of data. This should make it clear why you are not changing `cheese`. The name `cheese` still refers to the original piece of data containing the number `42`. Changing one name does not change any of the other names. This becomes important when using mutable data types. A data type is "mutable" when you can make changes to the actual data. An integer is not mutable, you can change the number an integer refers to. You can change the name so it refers to another piece of data containing another integer, but you can't change the underlying data. This is also the case with floats an strings. However, other data types can be changed. Lists, dictionaries, and sets are common built-in examples. There, if you change the contents a list, you are changing the underlying data. So if you have three variables referring to the same list, and you change the contents of the list using one variable, all the other variables will see the same change. Note that this is only for changes to the contents of a list, using `=` still just changes what a name refers to.
Glad this release supports Python 3 :-) I had been using cx_Freeze for Python3 cross-platform binary distribution, but always glad to have more options (if necessary).
The best think you can do as a maintainer is to say no. I recommend this talk https://www.youtube.com/watch?v=OrpPDkZef5I 
Yea as a dev without the math experience that a CS degree would have afforded I feel it hinders my ability and certainly gives me a ceiling. The importance of the math in low level coding or anything that needs to scale is understated in my opinion.
&gt; The number of times I've had to know big O notation and linked lists is exactly 0. Bravo for saying this. I see that are getting the required amount of hate for attacking the Church of Orthodoxy so blatantly. In fact, the only times these things are needed are in interviews. 
How does this compare to things like CRISP?
I believe this is focused on an application, while pyenv focuses on compiling and/or installing python versions.
Python will put food on your family
Argparse is too complicated, just watch the video, he compares docopt to argparse :)
Thank you. I have fixed it. Do you know if there is a way to write tests for IPython notebooks? Asserting things like output of cell 1 is X would be pretty slick.
I saw a talk on pyvideo which featured an implementation of Brainfuck in Python using only decorators. Can't remember whose talk it was, though.
Wow, the ctypes support sounds amazing! This is incredible news, and looks like it will solve the Python deployment issue for a lot of people. 
The easiest way for Python is to use Conda as another comment mentioned, but there are also the EPEL+IUS repos that allow you to easily install Python 3.4 with yum alongside the stock 2.6 https://iuscommunity.org/pages/Repos.html https://ius.io
Yes, I guess you might say I get spoiled with licenses :) I have worked with both OpenSource and ESRI products, each are suitable for some circumstances but I vastly prefer to work with ArcGIS. 
Yes agreed its not exactly suffering to use the open source tools, there are many well developed ones out there. The main advantage to arc in my mind is the completeness of the suite as you mentioned, the documentation is great and there is a massive user base. That last point is especially important as when working in industry, you will be hard pressed to find larger clients who are okay with open source solutions, in that world the proprietary systems are preferred and having demonstrable experience using the industry standard is a big boost on the resume. 
This is a problem a lot of projects face. KDE dealt with this by creating a spec for defining holidays for most countries in the world. See https://projects.kde.org/projects/kde/pim/kholidays/repository/revisions/master/show/holidays/plan2 There's a file format spec here: https://projects.kde.org/projects/kde/pim/kholidays/repository/revisions/master/entry/holidays/file-format.txt They had a plan to make a larger open source project called 'OpenHoliday' where this data is shared between open source projects (much like mimetype information is), but I think it fizzled out. It'd be an excellent resource to draw from in the spirit of open source.
One of the things I've wanted forever from virtualenv, virtualenvwrapper, pyenv, pythonbrew and everything else that has approached this problem is the ability to upgrade python in a virtualenv. This is especially annoying when using homebrew, which treats 3.4.3, 3.4.3_1, and 3.4.3_2 as individual installations. Cleaning up homebrew breaks the envs that depend on older versions. It looks like `pew restore` is at least partially what I want. Am I correct, or am I doomed to have 8 versions of the same python version on my system?
I've been using their 3.x branch for a while now and it works solid!
How does this compare to https://zestreleaser.readthedocs.org/en/latest/ ?
&gt; --miniconda very good Is there going to be a Powershell version ? 
Not everyone learns and thinks the same way. For me, my experience has taught me the lessons that I take with me from task to task. I don't think of things in terms of algorithms, but rather "the thing that's heavier than that other thing" or "ugh, this code *feels* ugly". While someone with a CS degree might argue that the "ugly" code is ugly because it follows the wrong Big O pattern, I write code that performs because I've seen all the "wrongest" ways to accomplish that task before. To be clear, I don't have anything against people with a CS degree -- indeed some of the finest coders I've worked with went that route. I just object to the idea that the only good coders are the ones who bothered getting a degree. I'm living proof that this isn't the case.
You're putting this all so much better than I ever did. Thank you.
Thanks!
brombaer3000 is right: This doesn't work on Windows. I will probably add that support when I have time; Anaconda does have a Windows version, so this can be done by creating a parallel Windows shell script. I don't know .cmd nearly so well as I know Bash, so that'll be a fun ‚Äî but time-consuming ‚Äî challenge for me. If anyone wants to add that with a pull request, be my guest. :D
Sure. cx_freeze requires you to know your target environment in advance and can be ‚Äî but is not always by any means ‚Äî a more complex process. That, or creating egg/wheel spec files, does not appeal to me personally: it's just yet more meta-work to do, and I feel it takes time away from building your app's core functions. Pythonize differs from that approach by simply ensuring the user has the correct Python and any needed third-party libraries without requiring him to do anything other than just launch your app. That stuff just becomes an implementation detail; users don't have to do anything special. No right or wrong way, here; there's pros and cons to each (pythonize, for example, doesn't work on Windows systems at the moment). This is just how I'm approaching what I view as a serious problem in how Python apps get deployed versus those written in compiled languages. Not everyone has the same itch. :D
how does this compare to twitters pex packager?
I may eventually add a flag to disable that behavior, or a shell function the user can run to easily load/unload it. In the meantime, however, getting back the default system Python can be done in several ways: 1. Comment out the line sourcing ~/.pythonize.startup in your shell startup file (this will usually be ~/.bash_profile or ~/.bashrc). Then, when you specifically want to use the nonsystem Python, you run the command "source ~/.pythonize.startup". 2. Run the command "source activate root &amp;&amp; source deactivate" in the current shell to revert back to the system Python.
I actually haven't tested on CentOS 6 yet; I bit the bullet and switched all my environments to CentOS 7 when it came out. I'll spin up a CentOS 6 machine in a few minutes, test it out, and get back to you. ** Update: Yup, works on CentOS 6.
OK, yeah, my suspicions were correct. To solve this I will probably read the file into an array first. (It's small enough)
What kind of machine today doesn't have a GPU, either integrated or discrete? 
Thanks everyone for helping.Highly appriciated
I can't recommend Anaconda highly enough. The company that makes it -- Continuum -- is based here in Austin, and I've talked (very briefly) with those guys before. They're legit, and their software is awesome. 
Maybe, depending on time and interest. I live in a bit of a *nix silo, so I don't have a strong sense for how useful it would be in the Windows world. But yes, I'd just need to add a .cmd script to the repo that does the same thing as the Bash script.
When will we be able to do all versions with one script? Wouldn't it be nice to create distributions from one environment?
I came here to suggest this too. Why not both?
In theory, what pew does, you should be able to already do manually with virtualenv: The current implementation of `restore` is [literally only 3 lines](https://github.com/berdario/pew/blob/3dafeb5adadf96ed62cae89790d7c45f59cec64f/pew/pew.py#L584-L587) It was thought exactly for your use case, but the kind of breakage brought by system updates is not very frequent (happens only every few months), and thus I haven't tested it very well. The idea is that `restore` will get the path of the python interpreter (which might be broken), and try to resolve the symlink... this should eventually point to something like `python3.4` Then, assuming that you have a `python3.4` executable in your PATH (in theory the next one, that is: the environment was a broken 3.4.2 and the new one is 3.4.3), it'll reinstall the virtualenv on top of it. Let me know if with homebrew the situation is a bit different (like, there won't a be a Python3.4 in your PATH) and, if so, if you have any alternative ideas to recover the latest version of the interpreter 
Thank you. Now I know the general and conceptual difference, but what exactly does this mean? I would love to get some specifics .. we're all programmers here, so if it's not too much to go into, I would love to learn more about why egg/wheel specific files is something one would want to avoid and how exactly one can do so. In the mean time, I'll try to research egg/wheel files as I'd like to gain a better understanding of Python packaging.
Here's some more of the code if that helps. My password is a String, so unless I'm passing that wrong then I'm not sure if that is the problem. import praw import time r = praw.Reddit(user_agent = "Created by Dylan, /u/pigs_have_flown") r.login()
Just use [Chocolatey](http://chocolatey.org/). Like Ninite, but you can use it to keep your packages up to date too, it's kinda like a linux repository system in use.
Is there an option for Windows??
I think I'm being misunderstood here. I in no way object to interviewers making sure that candidates understand the difference between an O(n2) and O(n) process -- I object to the idea that candidates need to know *Big O notation*. No one who's been doing this (well) for any reasonable amount of time should have trouble identifying the differences between the above, but only a CS graduate can answer the question: "Can you tell me the benefits of choosing O(n) over O(n2)?". A much better question is: "can you tell me why you'd choose this function over this one?" The former excludes people with experience, while including people with a degree, wile the second does the opposite.
Liberals arts != non-marketable skill. You can study things like computer science at a liberal arts college (I did; I have a Bachelor of Arts degree in computer science). I don't know why Redditors, as a whole, have this notion that liberal arts == non-marketable skills. Studying at a liberal arts college doesn't necessarily mean taking on a load of debt, either. (A lot of private liberal arts colleges have large endowments and are able to give generous financial aide packages.)
A few different computers all using OS X.10, python 2.7.x and 3.x, Anaconda Distribution 1. prototyping new algorithm, just testing small scripts or bits of code, generating plots from data sets on the fly with speed as the only requirement as opposed to flair/formatting: IPython Notebook and or python interpreter from command line. 2. small-medium scripts that I may want to call repeatedly in the next few days/weeks etc... : Sublime Text 3 w/ anaconda plugin and a few others that have slipped my mind 3. larger multi-file/multi-module projects, GUI work, etc ... : PyCharm Professional (has VCS integration built in) 
&gt;they're considering it but holding off for now due to the fact that most people still use Py2. I cannot stand when people use this argument. If they want more people to start using Python3, then they need to offer it alongside Python2. No one is going to move if you babysit them for over a decade and keep updating their language. They just won't.
Are people recommending moving to python 3 now? When I started learning python it was all about python 2, and I think LPTHW still recommends 2. I quite like python 2, do I need to migrate to python 3?
X-Post referenced from /r/programming by /u/TalkingJellyFish [Making unittests better with Docker](https://www.reddit.com/r/programming/comments/3nmahm/making_unittests_better_with_docker/) ***** ^^I ^^am ^^a ^^bot ^^made ^^for ^^your ^^convenience ^^\(Especially ^^for ^^mobile ^^users). ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher)
I think the best approach for solving this discussion is to finish the prototype and then come back. Once that is done, the code will speak louder than the theory I can talk about optimistically here. I do appreciate the input, but I am going to kindly go on. There are many many ways to obtain perspective based 3D graphics, and not all of them require the use of thousands of parallel GPU cores. As for models not being polygons instead on importation they would be converted to high density point cloud data. So people who enjoy using preexisting programs will not be thrown under the bus. Why am I using Python, or even doing this at all? Well because Python is a great beginner language, and when I started programming I used to browse open source projects for inspiration. In my opinion the great documentation of well cared for open source projects not only teach practical knowledge of the code, but also offer a good lesson in theory. The slower speed of Python is something I am fully committed to work with because of all the wonderful features the language provides, along with its astounding readability. I think the modern graphics pipeline is far to complex for its own good, we have come to a point where the capabilities of a $300 CPU are astounding. I refuse to believe that the only way to create useful 3D graphics is based on the established techniques of brute forcing calculations alone. TL;DR if anyone is interested, and they are a experienced Python developer still send me a message. Thank you for the input, and I will be back with a prototype.
1) Many of the deps which used to not be available for python3 are now 2) Python3 has added many features lately which can help with your python development. There are lots of little things but there are also some fantastic things like: Async await: https://www.python.org/dev/peps/pep-0492/ Enum module: https://www.python.org/dev/peps/pep-0435/ Asyncio: https://docs.python.org/3.4/library/asyncio.html Pathlib: https://docs.python.org/3.4/library/pathlib.html There are lots of small things though, I suggest you read through the python 3 changelogs. Since then many package managers have also started shipping better py3 support along with some even having `python` defaulting to py3. 
Will there ever be an incentive for businessmen to *not* complain about the quality/price of the job pool? 
I am an old guy so I remember when Java came out. At the time, people were mostly miserable and disillusioned with C++/Object Oriented programming's failure to magically fix the miserable state of software development and mass project failures. A lot of this was maybe due to people taking on C++ projects in half-assed ways, not really understanding or trying to understand either C++ or OOP, leading to miserable La Brea tarpits of crap code (that survive to this day). So Java comes along and, if you can believe it, it was this sexy new programming language all the old people wanted to work with. It went through several of its own iterations of severe failure and reinvention, for example when it first came out 'applets in the browser' were what Java was supposed to be used for, but a couple months and a billion browser-bogging crapplets later they 'pivoted', and eventually it found a home on the server side. If you go back even further, Java had been developed at Sun for set-top boxes, but that didn't really work out either. The whole business of throwing crap at the wall until something sticks is a really common theme in the the tech world (recall Zed Shaw calling Ruby on Rails for being a spectacular polished turd in its early days - a truly epic rant of much historical value: http://web.archive.org/web/20080103072111/http://www.zedshaw.com/rants/rails_is_a_ghetto.html.) As for Python, well, it's a pure joy to work with! I'm glad it seems to have really grabbed a hold in the Data Science world, which is infinitely more interesting than today's software development scene, so you younguns may give that one some though, esp. if you are in that early decision making stage of things.
Could you explain the alt-tab thing? I'm a complete beginner. I thought for cmd, you needed to type out the whole directory, which seems tedious
I understand your point of view. But I saw this from a different perspective. Programming is awesome. Since the beginning of technology history, programming has been basically in the hands of the nerds. It's ok, it was a hard thing at that time, one had to learn binary, assembly code, all that stuff. But now, anyone with a computer can write code! And programming is still handled only by hard nerds! No! Lets bring other kinds of people into programming! That's how I understood what was said.
Hmm, I'll have to check it out this week. Thanks!
I'v been hearing lately a lot about [hypothesis](https://hypothesis.readthedocs.org/en/latest/) for Unittesting. I suppose it will be popular in the near future in the next repetition of this question on reddit.
The replacement of things like zip and range with their iterator counterparts is something that doesn't get enough press. Sure, most developers know the difference, but that's not the case for newbies. It's not great to stop in the middle of a lesson and explain why iteritems is better than items.
This exists in Python 3.4. Check out the [asyncio](https://docs.python.org/3/library/asyncio.html#module-asyncio) module. It's a system built around event loops and has a [rich library of back ends](http://asyncio.org/). 
I may be wrong, but I've got the impression that all the "fantastic things" you mentioned could have been incrementally added to Python 2.x without the need of introducing the massive break of backwards compatibility with Python 3.x. Actually I think most of these "fantastic things" already have some sort of 2.7 port, so I fail to see the excitement of switching to 3.x. 
Google "pyinstaller" and "cx_freeze". I have never used them, but from what I've heard, they should provide what you are looking for.
If you really need that level of performance, plain CPython probably won't give it to you. You'd have to use Cython/C extension/PyPy/Numba, which can take advantage of these things. As others have pointed out, CPython itself compiles Python to bytecode and then interprets those instructions; it doesn't apply these sorts of optimizations.
LPTHW was last updated in 2010... 
I use it all the time at work, it's super useful... 
It isn't their job, but that doesn't excuse their use of that argument
I've had good luck over the last few months with cx_Freeze (after finally getting my app's settings figured out) but I'm very excited to try this to compare!
Totally, aspects like that are the ways in which Python3 is a natural progression of the language. I think it's foolish to look for reasons to stick with 2 when it is now the version to be depreciated and the language is moving forward towards 3. Instead you should be looking for reasons why you can't use 3 in your project.
I still don't get why they don't include both, though.
I've been meaning to do some rubik's cube art myself (I wrote a python script to pixelate images into mod 3 size and pick the appropriate cube face color for each pixel) but I can't find a cheap source of cubes, where did you get those cubes if you don't mind me asking? You can PM me if you want. 
Honestly, I'd prefer to have one written in bash or another shell language...mainly because if it's written in Python, we still need to have Python and (possibly) some libraries as a dependency...
The following features are things I use in Python 3 that are not available in Python 2: * Type annotations. (Type hints too where I can now that the typing module exists) (3to2 will remove type annotations just fine though, so I can still write with them and run without them) * asyncio (as far as I know there's no way to get this pre-3.3) Features I use that were added in Python 3 and are available in Python 2 either through PyPI or through __future__ * enum * improved division * The print function * absolute import * unittest.mock * statistics * ipaddress Python 3 features that aren't available in Python 2 that I have used but wouldn't particularly harm my functionality to be without: * `@`, the matrix multiplier operator Quite a few features have been backported, but they're "not native". If I'm targeting Python 2, I have to make sure statistics, ipaddress, and enum are available on the target machine. In addition, whilst pypy is generally faster than either, cpython 2 seems to be generally slower than 3 for most of the CPU-intensive work I do. If you've got an existing Python 2 project, great. Don't port it to Python 3 if you don't want to. But if you have a new project, at this point it's most likely a bad idea to start it in Python 2.
/slightly jealous/ I never want to deal with 2.6 again, or the nightmare of "module load Python 2.7.5" with libraries that need to be compiled... ffs, this is a solved problem! At least I can work in Py3 (with Anaconda, yes) and then backport to the old system.
I like pip better because at the last time I played with conda its biggest advantage was that it had precompiled binaries for numpy etc. But it didn't have access to the rest of pip. At that time it only had a small subset of packages available. I have also never had the issue with numpy taking that long to build. But installing it as a system package will help that. Also the last time I played with conda there was a big push to buy their pro package or whatever it was/is called. The big advantage to conda as I know it is access to GPU enabled numpy and scipy. 
Dropbox? I think github or bitbucket would be significantly better options.
I really wanted to switch to python 3 too years ago, but the libraries I wanted were all for python 2.
Move to 3
Hopefully you'll see this and remember... but what exactly did you change?
There are possibly many things wrong with this approach, depending on your opinion. These two articles have some very interesting points on deploying Python application. Note that they're not specifically about Java, but focus on Python exclusively. [https://hynek.me/articles/python-app-deployment-with-native-packages/](https://hynek.me/articles/python-app-deployment-with-native-packages/) [https://hynek.me/articles/python-deployment-anti-patterns/](https://hynek.me/articles/python-deployment-anti-patterns/) As for "everything can be compiled into a single JAR and copied to a server", that's of course not something that people would do in real life. There are properties files to be modified, JNDI resources to be created, system startup scripts/systemd/supervisor configs to be put in place, monitoring services to be configured around it, and so on. So, no. Even when the runtime itself can be self contained to a certain extent (that "certain extent" is more real than you'd think in the case of Java: recently I spent half a day hunting down a Java problem that ended up having to do with a bug in libnss..), proper *deployment* is never as easy as copying a few files over. &lt;plug&gt; In order to do Python deployments through OS packages (as I think you should), I created a tool called vdist, which lets you create clean OS packages that can then be installed in the same way you install any OS package on your server (ask any operations guy if he would like that, and he'll most likely buy you a beer!). [here's vdist](https://github.com/objectified/vdist) &lt;/plug&gt;
that is exactly what I came here to say. btw, how would this run on..., say travis-ci?
Sure. Just wanted to give you a heads up :P
Maybe try using [`Reddit Oauth`](https://praw.readthedocs.org/en/stable/pages/oauth.html) instead. I know it doesn't directly fix the issue, but it might solve it, since `.login` is gonna be deprecated anyway. What version of Python are you using? I tested with both 2.7 and 3.5 and failed to reproduce the issue.
why is 'host your code on github' not appropriate feedback?
I don't know Lua, but from what I understand it uses tables for its main (only?) data structures, and with that in mind, it looks like you're assigning table()s to variables for what I'm guessing end up being sprites. Is this importing L√∂ve libraries or something? Like other commenters said github or something would be cool, looks interesting. Edit: Also, it's not platform independent?
apt is written in C++.
And the fact that division is no longer mathematically broken.
Why? Github makes it easier for the people reviewing the code. A lot of people will not bother downloading the zip, but would probably read some of it, if it was in github. It also makes it easier to update when OP does. It's about making life easier to the people you are asking feedback (and therefore personal time) from. It's also not that hard to learn how to put something on Github.
I love this idea, just two questions: Will it be as easy as l√∂ve to create .exe or executables ready to distribute? That is the most important thing I love about l√∂ve. And second, do you think it is going to be powerful enough to make games? I've tried many times with python before but it is just too slow. Thanks!
You are right, I've not used pytest, and that is pretty neat. I'll still stick with unittest-like assertions though. Having the test framework rewrite my code isn't something I like the idea of very much. Also, I have a whole bunch of custom assertXXXX methods that are specific to my application.
Would you mind doing Qt5 builds as well? That would be very nice.
Anything for Collision handling? A Rect class? Or does it require Pymunk to handle Physics? That would be a shame to have something as simple and elegant as the L√∂ve syntax, only to couple it with the relative complexity of Pymunk. Wrapping Pymunk with the same level of syntax would be amazing though, but I'm not sure how to do it. 
Which is much easier if it's on bitbucket/github. Why make it hard for people that you want help from. This is a new project it's probably going to move quickly. I don't want to keep up with some some random dropbox folder when I can just pull to update, not to speak of bug reports and contributions. Plus it's an advantage from a marketing perspective. Projects that get some publicity on hackernews/reddit tend to get a decent watched/starred boost. With dropbox you miss out.
Wait, so it's not open source *and* Windows-only? Because it seems the entire framework is contained within that Windows-only binary launcher. What exactly do you want us to check out, when we have neither the code nor any real documentation?
HTTP - http://aiohttp.readthedocs.org/en/stable/ DB layer is trickier, read this article: http://techspot.zzzeek.org/2015/02/15/asynchronous-python-and-databases/ However, there are https://aiopg.readthedocs.org/en/stable/ http://aiomysql.readthedocs.org/en/latest/
So where is the actual code?
If you have [SASShare](https://support.sas.com/software/products/connect_share/index.html) running, you can connect to SAS using an ODBC connector. 
I used to code with PyCharm. I'm back to text editor and I'm using Atom and is fantastic (I've worked with netbeans, eclipse, vim, sublimetext and pycharm)
hey, thanks for the zzzeek link. those benchmarks were very illuminating. It seems to me that despite all the callback hell, nodejs is much much more performant than Python gevent (and so will be much faster than asyncio). And obviously, there is Go. I mean the shift from Python 2.X -&gt; 3.X was already bad enough.. now the shift from gevent to asyncio looks impossible.
Thanks a lot for creating and sharing pew. I have been using it for quite some time now and I like it. And now it gets even better!
Baby don't hurt me...
no. ...because its basically impossible to do that as easily as with lua (or, often, at all) in python.
Did you really have to replicate the horrible API? This is python.
It's not my project. and I stand by my point. Those who criticize where the code is hosted are too lazy to give better feedback (because that's the hard part), and just slap him across the face for not serving the project on the plate they like. 
RHEL 7 ships with 2.7 as the system default and 3.4 is available through a Red Hat supported package (via Software Collections)
I use a table structure just to show how similar the game code is to L√∂ve code. It is not importing any L√∂ve library. Github is coming soon... in a couple of days. Yes, It should be platform independent.
I agree. Github repository is coming.
Could you please tell me if the demo runs in your windows PC?
I was thinking of using virtual boxes. Would there be any caveats?
I think the L√∂ve API is very good for beginners, and to prototype games rapidily. Of course, it could grow to a more Pythonic way. Besides that, I tried to emulate Flask API, so the simplest app should be something like: from amor import Amor love = Amor(__name__) @love.conf def conf(w): pass @love.load def load(): pass @love.draw def draw(): pass @love.update def update(dt): pass if __name__ == '__main__': love.run(debug=True) 
Hey, author here. I am happy for all questions or every kind of feedback.
Short slide deck (PyDataLondon Lightning Talk) on why you should definitely move to Python 3 now: https://twitter.com/gmarkall/status/628651326797410304
[**@gmarkall**](https://twitter.com/gmarkall/) &gt; [2015-08-04 19:38 UTC](https://twitter.com/gmarkall/status/628651326797410304) &gt; Slides for @ianozsvald and my Python 3 talk at \#pydatalondon are at http://gmarkall.github.io/py3lightning/ @pydatalondon ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
That is a difficult question to answer. I can't NOT recommend PyMunk, it's great once you get it working. Setup your space... define gravity... add collision handler... create prerequisite bodies... add the Mass and Inertia variable... and elasticity and friction... setup the shape objects collision type... add the shape object and the body object to your Space instance... register the body with the collision handler... add body velocity for movement... oh and adjust the step argument properly... THEN it works great. But maybe a simple Box2D function would be better, that's what Pygame does and it works pretty well.
We use RHEL 6; it was released in 2011 and is still supported in phase 1. There is nothing to excuse the fact that the core OS software still uses Python 2.6. I know that 3.4 is available through repos, but that's not what I was driving at. At least for my organization, good luck getting then to install it... But that's our problem and nobody else's.
You can deploy your servers using something like Docker images. That way, the passwords are stored in the docker image (which is like a snapshot of your server in time). If you dont want to use docker, then: Do you have to deploy a lot of servers, like in the hundreds? If it's just a few now &amp; then, then manually creating the env variables may work.
We recently started a new project with Python3. So far so good. Had to port an old library but thankfully it was only old-style exceptions and new requirements.
This is basic stuff. You see me confused. You open a commandline and switch to the directoy of your code. I personally use subst to simulate driveletters for this. (look it up). So you have your code in the editor, change something, save using ctrl+s, alttab to the commandline. If the last command typed wasn't the command to execute the script, type it in once. Next time just hit the cursorUp key to retrieve it. Press enter to run the script. Alttab back to sublime. Done. You should get your basics together. :)
The rule of thumb has been "start a new project in py3 unless you have a py2 dependency" for years now... But you basically always had a py2 dependency for a while there. It feels like that's definitely changed now, and it's realistic to start new projects in py3. 
Fellow hokie based on username? Anyway, I'm going to assume you have to use sas and access due to some stupid reason like government or corp environment. Anyway, csv is the simplest route, and you can easily use a script that zips the files for archiving after import into access. Any other options are going to involve Db connectors, and I'm not even sure any work with access. In case you are able to install software in your environment, go with SQLite. Super easy to work with. 
oh wow, you made slate.com? that's amazing. Can you make the site a bit wider? it doesn't render well on my firefox. ALWAYS DESCRIBE YOUR PROJECT SO WE CAN GIVE A SHIT
I might be missing your point, seems you understand why RHEL 6 ships with 2.6. This is exactly why they are offering Red Hat Software Collections. This allows you to run a Red Hat supported version of Python 3.4 on RHEL 6. So you were actually complaining about your organization? You're original post said that "Red Hat insists on using Python 2.6" which is what through me off.
Oh shit! Tangential but reading the numpy release notes I found that [PEP 465](http://legacy.python.org/dev/peps/pep-0465/) added a native matrix multiplication operator from 3.5. That's awesome. edit..... wait..... whoosh?
That is a cool website, but there are many more python modules out there than the top 360 from PyPi and not all of those 360 are compatible. MySQL-python and ansible stick out to me on that list as things I would want that are lacking. I developed a program using simplecv (computer vision) and it is still python 2.7. I teach using PyGame. It uses 2.7 and 3.1. So today I still use both 2.7 and whatever version of three I need to get the job done.
I'm on phone now, let me pitch in with feedback later, alright? 
There is a bit of a conceptual hurdle going from fabric to ansible; fabric is more of a traditional script that executes actions against one or more servers while with ansible you declare the state that the servers should be in and ansible takes care of the steps necessary to bring the servers into that state. Once you get past that, i find it much more useful than fabric and much easier to write the scripts for.
&gt; you're asking strangers to do work for this project for free _I_ am not asking a damn thing. I am not the author. I am just saying that criticising his project for the delivery method is the easy target everyone can spot, but if you ask for feedback on the code and someone tells you something completely unrelated to the code, your motivation and enthusiasm can go to zero in 1.7 seconds. 
It's been available for Linux and Windows since September 14. I remember because all of my CI scripts broke since I didn't specify Python 3.4 specifically, and conda used 3.5 by default :( Haha. I have found these two links to be useful when searching for conda versions of packages for Linux and Windows: * https://repo.continuum.io/pkgs/free/linux-64/ * https://repo.continuum.io/pkgs/free/win-64/ EDIT: Ah, I see that the [full Anaconda package](https://www.continuum.io/downloads) is still on 3.4. I use Miniconda and just use the `conda` CLI to download the specific packages I need.
Does this mean that A * B is elementwise multiplication for all matrices in numpy now? I'm not a big fan of the @-operator. It's not obvious what it does if you don't know already, matrix multiplication should be the standard multiplication type for matrices, and Matlab already have a elementwise operator that I think is more transparent (.*). But I do agree that two operators are necessary, and I guess @ was picked as the best alternative, probably mostly because they didn't want to (or couldn't) use * as matrix multiplication.
No, it means that A \* B does exactly the same thing because they don't want to break existing code. A \* B is elementwise multiplication for numpy arrays and matrix multiplication for numpy matrices, which are essentially depricated at this point. The issue is that most functions return numpy arrays instead of numpy matrices, leading to the whole np.dot issue we had prior to 3.5.
.T and .inv work just fine. And I've always seen ' used for transpose. Plus ' would obviously break any code using it for string syntax.
No Readme?
hmm.. that is interesting. How do you use asyncio primitives from within tornado ? I thought Tornado built its own primitives.
Once you have Python, you can always use pip...
You could just use conda search -f numpy to list available package versions.
The passwords should be stored outside of version control, and can be in the format of your choosing: python, ini, shell, etc. For instance, your `.gitignore` file has a line `secrets.py`. The `secrets.py`file is encrypted on a private AWS S3 bucket, or passed around on a USB drive. It might contain something like this: # secrets.py DATABASE_PASSWORD = 'op3n_s3s4m3!' SECRET_KEY = 'abcdefg123456789' Then, in your fabric file, you would import secrets.
Thanks a ton kinygos!
&gt; I really would like you to test if the executable runs in your PCs. OK. But I have a Linux PC, so it doesn't run natively. It might run in Wine, but I won't run an unknown program without sandboxing it first, and I don't have time for it now.
I did something similar for Indeed, but used [their API](http://www.indeed.com/jsp/apiinfo.jsp) instead of scraping. Made things a lot easier since it was just going to get thrown into a Mongo DB anyway. :)
I have a Python 3/Qt5.4 app that I bundled using PyInstaller (dev 3, a few weeks ago). I got it all working in my native machine, Mac OS. Then I used Parallels VMs to bundle and test for windows and Linux. Key to this was using Dropbox. The source files were on Dropbox, also the distribution folder for the app. So each VM linked to the same Dropbox account. For each of the four platforms (Ubuntu 14, 32- and 64-bit, and Windows 7 32- and 64-bit) I had two VMs. One was the "dev" VM which had PyInstaller, Python, Qt, and all necessary libs installed. The other was the "test" VM for the same platform that did not. Process for each of the platforms: * boot the dev VM. * Wait a bit to make sure its Dropbox is up to date with source file changes. * Run a script that calls PyInstaller, source from Dropbox, output to local disk. * Quick sanity check of resulting app. * Compress the app to a zip file in the Dropbox distribution folder, replacing prior distro for that platform. * Go get coffee while the .zip gets uploaded to the Dropbox cloud. * Suspend dev machine. * Boot "test" machine. * Read reddit while its dropbox updates. * Drag new .zip to test machine desktop, unzip, test. Repeat for 3 more platforms. 
Thanks. I was wondering if I would be wandering down a long hard road to nowhere.
Cdist is vectorized. Also, your space is Euclidean, its just that you have reflections of all of your points to deal with too. You could use cdist to find the distance between each point and the 9 possible positions of each other point, and keep the minimum of those 9 as the "real" distance. Better would be to use numpy and figure out which reflection will be closest before computing distance. Like, if change in y is more than half the total y of one period, you know the one reflected in the y direction will be closer than the one in the same period as the point. This can be done in a vectorised way through numpy with masks.
* I define the function g as given in the task. * Then I create an array (x) that gives values from 0 to four pi. * Then I create an array (y). I apply the function g to every element of x. * Then plot x and y. Which part is not clear?
Use SSH keys.
As usual trying to install with `pip` on Windows failed. Plan B is currently in operation via [Unofficial Windows Binaries for Python Extension Packages](http://www.lfd.uci.edu/~gohlke/pythonlibs/). **Christoph Gohlke** for World President as far as I'm concerned :-)
`id_rsa.pub` !!!!!!!!!!!!!!!!!!!!!!!
This is awesome, I just started coding a small app to scan in my pay stubs to track money in/out I'll definitely check some of these libraries out. Thanks for the post!
To try and push any OS-specific library on any coding subreddit is kind of embarrassing. 
The first rule to presenting anything is to know your audience. If you try and present a video game but it only works on macs, it would be completely natural for the feedback to be unrelated to your code and your motivation and enthusiasm would be 100% justified no matter how good the code. 
Just ported some code last week... From 2.4 to 2.7... T_T
If you don't do the smart thing (ssh keys), you should be able to open the stdin stream and write your password to it (with a newline). See if that works.
&gt; Liberals arts != non-marketable skill. You can study things like computer science at a liberal arts college (I did; I have a Bachelor of Arts degree in computer science). I don't know why Redditors, as a whole, have this notion that liberal arts == non-marketable skills. Because employers look for B.S. &gt; B.A. in almost every field. Of course if you are in the IT/dev world then you don't need a degree at all so this doesn't really matter. This, however, is an extreme exception and it is quite disingenuous to imply that your ability to earn an income has anything to do with your degree. It doesn't, if you are a programmer. &gt; Studying at a liberal arts college doesn't necessarily mean taking on a load of debt, either. (A lot of private liberal arts colleges have large endowments and are able to give generous financial aide packages.) Perfect. This speaks to my point directly. If you can get a liberal arts degree without going into debt, it is perhaps the truest reason to attend college. All the rest of this is pretty much glorified (and way over-inflated) job training. Several colleges in my home state offer a variety of liberal arts degrees at rates in excess of $40,000 per year. This is clearly ridiculous. $160,000 for a piece of paper with no direct job training to help pay it off? Nope, nope, and on top of that, *hell no.* 
For most (if not all) cases I think that good code and good tests got you covered but if for some particular case (mistaken or not) you want to do type checking is good to have something that helps you instead of getting in the way.
Atom describes itself an editor and not an IDE. It lacks ide functionality like integrated debugger or profiler.
.pub, but yes.
I wrote a similar [scraper](https://github.com/pydo/lead_scraper_orm) for indeed.com. I also didn't realize I could use their api at the time I was writing it.
Good question (though the title of your post is a little too vague). IMHO, your Fabric-based deployment strategy makes sense. The passwords shouldn't be stored in the source code, as you said, but you don't need to deploy them on each deployment. They could be put in a central place (/etc, /srv/config, whatever) on each server once and for all. Another point we've learned is that provisioning a server is not the same as deploying an app. Both can be done with Ansible, or with Fabric, but it makes sense to do the provisioning with Ansible and the app deployment with Fabric. 
This looks like a similar issue for 3.5 (though that mentions Win8.1, not 7): https://bugs.python.org/issue25157 The last activity on this is today so unless you have a burning need for 3.5.0 I'd suggest trying 3.4.3 in the meantime until this is fixed.
Now he knows the right way to share, otherwise he would just continue on in ignorance. I think it's kind of asshole-ish to not correct someone and help them out in programming subs. 
I suppose what i was kind of envisioning was more of a credential management, i was thinking something like Zookeeper, and store all my passwords/secrets/everything and anything we share amongst each other now. Have a small client to monitor any changes to the files (sometimes the ip to the db changes, or we the ES master dies and ... shit happens) and we have to redeploy when that happens. But i talked myself out of that because: complexity. I like `freeaddition` suggestion about ansible, and im going to really look into it. Your provisioning the box once with all the information inside it is a close second -- though it still doesnt address where to keep the information should we need to redeploy the boxes. Thanks
Hey so I've run into hardware issues on my laptop. Should be recoverable but will delay me quite substantially.
You can clearly upload new code to this device so even if you can't install modules to the sysroot what is preventing you from adding said modules to your source? It's not a best practice but it'll work until you can upgrade. 
It's really rough around the edges, but a type checking library (actually a program run as a separate step, so as to avoid useless/expensive runtime checks) is already usable. (I'm talking about mypy) Here's an example of [a tiny library that I wrote recently](https://github.com/berdario/resumable-urlretrieve/blob/master/resumable/__init__.py) that is extensively type checked And here's the [tox.ini](https://github.com/berdario/resumable-urlretrieve/blob/master/tox.ini) that [travis CI uses to automatically run the type checks](https://github.com/berdario/resumable-urlretrieve/blob/master/.travis.yml) at every commit
https://www.reddit.com/r/Python/comments/3m767d/python_overtakes_french_as_the_most_popular/ And the article is misleading. 6 out of 10 kids would *rather* learn python than French. Also it's a lot easier to learn a traditional language at a younger age. 
I've staerted experimenting with PyQtGraph recently. If you do any pyqt, it might supersede matplotlib in capability soon.
Thank you, glad you like it :)
&gt; I think it's a shame that * means elementwise multiplication in Python It has its uses, and it's also consistent with how `*` works for other kinds of arrays. That way you can write something like: def cubed(x): return x * x * x and have it work consistently on scalars, vectors, and matrices. In contrast, this function would behave very bizarrely if `*` worked as matrix multiplication. When it comes to designing things in programming, you can't have everything: there's always a trade-off you have to make. I think in this case lifting the operations elementwise was the less confusing of the two.
In numpy, you can broadcast an n length array with an m length array and get an n by m grid. Thus applying a function between every combination between the two sets. Is that like what you're asking?
Heck yeah! 
Essentially, you'd create a private S3 bucket, and only grant access to particular users, or a group, created with IAM. You can further lock down the bucket, by setting a policy that states that the group you created can only read the bucket, and doesn't have write access. It's true that each developer would have to have AWS credentials on their machine, but each user can download the Amazon keys by logging into the console with their username and password. Then, they can `pip install` the `awscli` tools, and the setup stores the keys so they never have to re-enter them again. After that, it's a seamless `aws s3 cp s3://secrets.my-config-bucket/secrets.py ./`. **But**, if you want the *really* simple solution, your developers must have remote server access, since they're deploying with `fabric`. So, why not put the `secrets.py` file on a remote server they have SSH access to ‚Äì assuming they're using SSH public keys, and it's not a publicly accessable folder. You can even encrypt the file, and write a fabric method to pull down the latest version on every deploy.
I cannot do that. I need to be able to enter a password for security reasons. :/
Meh everybody moved to pycharm
[removed]
Hey, next time remember to post questions like this in /r/learnpython instead. Anyway, if you also have to avoid importing modules, you could use `zip` to create pairs of numbers and then `map` with `sum` to actually calculate the result: def pascal(n): if n == 1: row = [1] else: previous_row = pascal(n - 1) pairs = zip(previous_row[:-1], previous_row[1:]) row = [1] + map(sum, pairs) + [1] return row 
 if __name__ == '__main__': # Your code Anything within the `if` statement will run only if that same file is being executed. If the file is being imported somewhere else, the code will never run. [stackoverflow](http://stackoverflow.com/questions/22492162/understanding-the-main-method-of-python) has lots of questions about this. 
Oh cool! I'll have to check out mypy for my python3 work.
I agree whole heartedly. I've lost count of the number of times that they've come to my rescue, and I've never had a problem with them.
This doesn't work, fyi. Openssh checks to see if stdin is a tty, and refuses to take password input if it is not.
The problem in that case would be that person's source of motivation, not the criticism expected of good quality work. 
I came across this pattern the other day which I think is a good way of doing this: import sys def main(*args): ... if __name__ == '__main__': sys.exit(main(*sys.argv)) This also allows testing the `main` method with different arguments, if that's your thing.
That's not totally accurate, otherwise why would there be an [isinstance](https://docs.python.org/3/library/functions.html#isinstance) built-in function?
You're looking for the minimal distance on a torus - that's what your space is. If your space is a*b in size, you want sqrt(min(Œîx, a - Œîx)^2 + min(Œîy, b - Œîy)^2) - it's basically finding the shortest vector and then finding the Euclidean length of that (because locally the torus is Euclidean as long as you don't go around either of the dimensions).
&gt;What are those decorators doing in the backend? A complicated way to get by without subclassing, it seems.
Yeah, this is how pyexpect does it. So looking at the pyexpect source would be a good place to start for those who can't just use pyexpect because Reasons.
I see. I don't understand why. I just want a script that logs into network devices and runs a command. I'm not sure why they would prevent this. As long as I'm inputting a password, either manually into the device, or manually into a script. If it's not possible then I guess it's not possible. Thanks. 
As not washing one's hands is also *a* type of hygiene. :-P
Whoa, love the idea. I rarely make anything that bridges the gap between the virtual and the real and things like this really inspire me. Would you consider writing an update in, say, a month? Im curious to see what kind of issues (if any) popped up.
Waiting for gpu support 
My bad... duh. I really missed the 2d array of rubik's cubes. Thanks. 
All the KDE stuff can be compiled with python 3 support instead, your distro just chose not to.
Yeah, you're right. Whoops.
I will have to try pygame with the latest, but I still need 2.7 for all the old abandoned modules that will never be ported unless there is a big push to convert everthing out there. Simplecv is reason enough to stay at 2.7 for me.
I could have sworn they made some changes for that; not many, but some.
isinstance is actually a complement to duck typing. Both approaches are possible in Python, but when the choice is given, you should use duck typing.
Django source code is notoriously ugle, it's not a good reference : a lot of magic, many non idiomatic python code and PEP8 violations... 
sometimes there is the odd abandoned thing that I need, but those are usually small and rarely are much more than a [`futurize`](http://python-future.org/automatic_conversion.html) away from Py3 compat. generally the pain is much less than having to abandon all the great features that accumulated between 3.2 and 3.5
Really? Can you link me to some doc?
&gt; I think it's a shame that * means elementwise multiplication in Python I disagree completely, I think it is a huge benefit, at least for me. Probably 95% of the time, that is what I want, and the fact that MATLAB doesn't work this way has led to a ton of bugs with people I know.
I think that is outside the scope of ordinary numpy. That sort of thing is better implemented by a dedicated project.
I understand that, but you can still have a README that explains what the project is about and what its overall goals are. I have seen lots of skeleton projects like this that nevertheless tell potential contributors what the project intends to be about.
So matplotlib 2.0 == matplotlib 1.5 + default style changes ?
And one example for this would be .... ?
Is this true? If we had some concrete data we would have a leverage point for our employer to cough up some money for licenses. We are using Eclipse+PyDev combo currently.
Well done. The code looks clean and documented. If you are looking for another challenge, one might be to display the results in a web browser, with a link to the actual job.
I'm not very good at explaining but unless you need remote debug ability or cython/ django support you can try it out yourself with the free community version. 
Oh yeah and when you get install PyCharm, it's 1000x better than using the cmd
Yeah I was thinking about a follow-up as well. Here's some points I want to cover: * Running the tool on my Raspberry Pi * Checking the performance of tesseract on the Pi * The Python USB driver I was talking about * Showing the graphical output on Kibana * The list goes on and on. ;-) As always, time is our biggest enemy. So don't bet on it yet. 
When you need to write scrapper for more then one source. Framework allow you do not repeat you self. 
This is just a silly little thing I came up with while looking for [Python wats](https://github.com/cosmologicon/pywat/blob/master/README.md). Thought I'd share. Let me know if I made any mistakes. Thanks!
What an odd question. If you think you don't need one, then don't use one. That's totally fine. I myself like to keep things as vanilla as *reasonable*. Just move on, and maybe you'll get to a point where you discover that some of them are indeed useful. [Scrapy](http://scrapy.org/) for example doesn't only provide helpers for parsing HTML. It provides a whole infrastructure to run your scrapers/crawlers and to post-process results.
&gt; So when is a framework required? They're never *required*, but they can be useful if you want to get up and running as soon as possible. With most frameworks you can just type simple configuration settings and be up and running within minutes versus having to do everything yourself. If you've ever done web(site) development with Python, let me ask you this: Why *wouldn't* you use a framework? Most of the benefits that apply towards web frameworks apply towards scraping frameworks as well.
Frameworks are just nice to have so you don't have to roll your own solution, they aren't required by any means. They usually offer a time savings so you don't have to roll your own solution and offer more functionality than simply parsing one site. For example: do your methods support xpath? What if the webpage your scraping changes dramatically? Will your code still work? How long would that take?
I work full-time as a data scraper and I use a mix of different techniques, but we mainly use lxml with xpath for most of our findings: num_pages = int(node.xpath('//div[@id="pagn"]/span[@class="pagnDisabled"]')[0].text) i wouldn't even want to imagine finding that with a string find method.
1. Questions on /r/learnpython 2. You could go with the controller model, in which the controller offers certain functionalities and keeps track of certain objects, and the GUI calls those functions. 
I believe using matplotlib or ggplots will help ya
Can you create an overlay on a map using matplotlib or ggplots? Thank you
You are correct, changes would still need to be made but I should have emphasized the potential time savings. Having xpath support would literally mean a copy and paste from the chrome dev tools into my python script. Even if they restructured the entire website the framework would make updating my code take all of 2 minutes including the commit. Nothing wrong with rolling your own solution though. I'm just a lazy programmer. =)
Having same issue, as a workaround you can install Expect on the server, write a script and run it with python. Please post here if you find a good solution. Thanks.
Im not sure what your exact question is, but if you are having issues reading the input file for data, then check out the csv object https://docs.python.org/2/library/csv.html edit: This should also probably be posted in r/learnpython. 
Nice!
I understand storing a password on a script is a security risk. Why couldn't the script place the password into a database and hash it? 
It depends on what you want to plot? Surface fields, 3d vector fields, there are lots of options and tons of variables output from WRF. Matplotlib/basemap are the most used within the Python community, but there are many more options as well. A quick google shows many options: http://lmgtfy.com/?q=WRF+python+scripts+plotting I also believe that vapor has WRF capabilities and you can write python scripts to interact with it as well, but I haven't used it at all. https://www.vapor.ucar.edu/
How does this differ from other plotting libraries?
http://matplotlib.org/basemap/users/examples.html http://sensitivecities.com/so-youd-like-to-make-a-map-using-python-EN.html#.VhUyaqZY5Bc
import csv def get_wages(my_file): for row in csv.reader(open(my_file, 'r')): name = '{0} {1}'.format(row[0], row[1]) hours_worked = float(row[2]) hourly_rate = float(row[3]) overtime = hours_worked - 40 if overtime &gt; 0: hours_worked = hours_worked - overtime else: overtime = 0 print('{0}: {1:.2f}'.format(name, (hours_worked * hourly_rate) + (overtime * (hourly_rate * 1.5)))) get_wages(input('Please enter the file: '))
It's similar to Bokeh and Plotly. At first sight,it seems to be as advanced as Plotly (it can combine line and bar plots which Bokeh can't for example) but as open source as Bokeh (Plotly cost 250$/year if you want to keep your graphs private or local on your PC). So this release could be big in the python plotting world.
You probably don't want to just copy someone else's plotting scripts, because it's trivial to create your own quick plots with the right libraries. There are two building blocks you need to plotting WRF output - 1) A solid netCDF reader. You don't want to waste time at the low-level with the unidata netcdf4 library or NCAR's PyNIO; instead, use either [xray](http://xray.readthedocs.org/en/stable/) (my preferred library of choice for CESM, WRF, and other model analysis), or [iris](http://scitools.org.uk/iris/) (a great package from the Met Office, but a bit more complex and broader). These allow you semantic access to the WRF output dataset you're using, including loading multiple output files. Furthermore, if you're doing heavy-duty analysis, xray seamlessly integrates with [dask](http://dask.pydata.org/en/latest/), and will run computations on multiple cores on your machine/server if you chunk it correctly. 2) A cartographic/mapping library. Basemap was already mentioned, but I don't like it very much. I've had far more success with [cartopy](http://scitools.org.uk/cartopy/), another UK Met Office toolkit. It's very straightforward and easy to use. Plus, it integrates fully with iris (and xray if you use the bleeding edge version). Then, creating a plot is usually as simple as a script with at most 10 lines of code. But these tools will give you the building blocks to quickly and effortlessly create whatever visualization you'd like from the model output.
Thanks! I need something like this!
how do you plan to get the password out? hashing is for verifying passwords, not storing them for use.
Shoulda called it bloompy
I'm still running 2.5, although that was only because I demanded it -- the default installed version was 2.4. Hooray for "enterprise software". But that's not your problem. Assuming this is open-source, free software. If someone needs an older version, tell them to fix it themselves. You have every right to do so, and I think that is a completely fair position to take. On the other hand, if you're trying to sell this, then I guess it depends who exactly your potential customers are. Nobody can answer that for you. But if in doubt, make it as compatible as possible.
I don't feel too strongly about matplotlib one way or another. There is ggplot for python if you wanna go that way. What I'm really excited about are the interactive plots.
It looks like the main difference is that you can inline a plot in a jupyter notebook but still retain controls like pan and zoom. Can anyone confirm? 
do you want a pat on the back? this is such a weird post
this sounds like homework https://www.reddit.com/r/Python/comments/3kestk/post_learning_questions_to_rlearnpython/
Flask + Gunicorn + SQLAlchemy We use it to serve the web frontend for PiPlay which uses several different databases with several tables each. The websites all give lots of documentation plus our source code is open, you can check out the [front-end portion here](https://github.com/ssilverm/pimame-web-frontend). The file app.py does most of the work.
You can do that with matplotlib in an ipython notebook by using %matplotlib nbagg
Ahh, I figured it out. I make two separate connections. Then a single commit at the end is nice in some cases so that if there is a problem in the middle no changes to the database are made.
Looks like there is ajax/websocket requests onsite. You need to discover them with browser debug tools and repeat. My recomendation is to use requests/aiohttp for this instead of urllib directly.
- Pygame - Kivy 
Aren't file pointer locks an operating system's version of a GIL? I understand what you're trying to say, but I think this would require an alternative Python run time...and cython already has this functionality?
Thanks! So then is the main benefit of bqplot that it can render html/javascript? I thought matplotlib could do that too with webAgg, but I haven't actually tried using it to embed a plot in a website.
As an integrated tool for jupyter notebooks, this is a quick, easy library to use on the surface. However, what I find lacking is the ability to export these graphs easily out of the ipython environment with interactivity still in tact (like you can do with Bokeh or Pygal). 
ggplot is great. Sure beats using matplotlib on it's own! I have to say, however, that I've started using Seaborn a lot too. Even more than ggplot these days. Seems even easier to make nice plots in Seaborn for most of my needs. Seaborn plots sure are purty.... :)
The first example is wrong. Here's a value for ??? that makes that snippet work: &gt;&gt;&gt; x = type('Foo', (object,), {'__lt__': lambda self, other: True})() &gt;&gt;&gt; x &lt; x True Edit: ...oops, and then I read further to the "details and scope" section where that approach is prohibited.
Is there a way to export just the code from the notebook and run it independently?
I've been slowly moving python 2.6 out of open source projects for about six months now. Some of those projects are of medium popularity (2000+ stars on GitHub, 10K+ PyPI downloads per month). How many requests have I had to maintain compatibility with 2.6, or pull requests fixing the issue? ZERO. Of course, like /u/cecilkorik/ said, if you have any hopes of selling your code to customers running legacy versions of Python (In my opinion, 2.6 or earlier), you might want to keep running 2.7.
bqplot relies on the ipywidgets framework. There is work in progress to allow using IPython widgets outside of the Jupyter notebook - including bqplot visualizations.
[removed]
The main advantage of bqplot over other solutions is that it is entirely built upon the Jupyter widget machinery, which make it very easy to wire with other IPython widgets (sliders, buttons, dropdowns) and build interactive inline GUIs involving numerical computation in Python.
No. There is nothing like goto in Python. You can do something like it, though. Make use of first-class functions! def a(initial): return initial + 10 def b(aresults): return aresults + 5 def c(bresults): return bresults + 15 if __name__ == '__main__': import sys if len(sys.argv) &gt; 1: if sys.argv[1] == 'a': calls = [a, b, c] elif sys.argv[1] == 'b': calls = [b, c] elif sys.argv[1] == 'c': calls = [c] else: calls = [] else: calls = [a, b, c] val = 0 for call in calls: val = call(val) print(val) Obviously your initial value and functions should be specific to your application. Please note that this is BAD CODE in any language you do it in. It's a fine pattern for certain kinds of problems, but as a basis for application flow control, it's really not a universally good idea. I was merely responding to your question as to whether or not it could be done. You probably don't want to do any of this. Honestly, just copy+paste the lines of code you want to run into a new file. Make lots of files. Run lots of files.
I don't think this is a scraping task. Judging by the source code, starting from line 57 there's a bunch of JavaScript in german (I think), it looks like it changes the DOM element you're trying to grab directly. The one you want is modified by the function @ line 131: function ShowStaatsschuldenInEuro () Looks like just maths functions so it's probably not too hard to translate to english/python. German words are scary though.
/r/learnpython would be a better place for this. you're still at the beginner level and I would suggest focusing on working through some tutorials and guided courses. for now, just focus on getting comfortable with syntax and working in your development environment. programming is a deep topic. there's no need to rush through it. take your time and you'll understand more and more as you continue to practice.
Ok, thanks. I'm guessing there's something I can use to make Python open up another file?
CentOS 6.x is still very popular and has 2.6 as the core distro. I would argue that CentOS 6 is the primary reason to support 2.6. If you can safely ignore CentOS 6 then feel free to ignore python 2.6
To clarify, Bokeh *can* combine a bar and line plot, it just takes a couple extra lines of code. In short, Bokeh provides a "super high level" chart API for making bar plots, histograms, and the like. Then there's a "medium level" glyph interface, which allows you to draw lines, rectangles, circles, etc. It doesn't appear that one can combine a chart with a glyph quite yet. But it is possible to draw a bar plot quite easily using rectangle glyphs, and then draw a line over top. For instance: http://nbviewer.ipython.org/gist/anonymous/d477ef34efb021a0896f
Blumpkyn
Work on any project that you are interested in. You will learn a lot more then any tutorial, when you finish this first project. for me personally, I worked on this web app. https://basic-python-cfd.herokuapp.com/
The example makes no sense to me at all so firmly -1.
maybe OP wants to do something like this? class Borrow: def __init__(self, arg): self.elem = arg def __enter__(self): return self.elem def __exit__(self, type, value, traceback): global elem del elem array = ['foo', 'bar', 'baz'] for i in range(len(array)): with Borrow(array[i]) as elem: print(elem) print('elem' in vars() or 'elem' in globals() or 'elem' in vars(__builtins__)) Borrow in this case is an auto-trashing machine... but if this is the case.. it seems easier to just: for i in range(len(array)): elem = array[i] print(elem) del elem print('elem' in vars() or 'elem' in globals() or 'elem' in vars(__builtins__))
If you are starting a new project today don't bother supporting any version &lt; 3.3. 
Blumpkyn: Tired of not knowing where your blue pumpkins are? Upgrade your 8000 series software license today! --John Deere
PEP8, within reason. I use Py-Flake8-Lint in Sublime Text 3. It's easy enough to learn the format style. Otherwise, use good naming, mind singular and plural items, and be careful with idioms.
I agree with you guys...do a lot of tutorials!! It does matter how many courses you take, once you a mini project and then do bigger ones you learn a lot, especially when you run into problems you have to solve.
I agree, but formatting and naming conventions arent really what im worried about. Im more worried about over designing on an object oriented question, when they expect me to just use some python specific stuff. Lambdas come to mind for what im trying to get at. I guess the analogue of what im talking about is that an inexperienced java dev would try making a 2d array for a key value pairing when really you should do hashmap. Likewise using the '...' operator for gui work
Lookup argparse (stdlib) or click, they empower you to define switches that tell your program whatever you need your user to tell it. While I think a script to launch other scripts is kind of silly, you could at least use it to learn about Python modules and making them importable. If you want to be able to run a module only if directly executed - the community way is to do: if __name__ == '__main__': my_function_call() That allows you to run that script via `python myscript.py`, otherwise you could import it into another script: from .myscript import my_function_call my_function_call() you've now essentially made a library file that also ha runnable utility for its single task and your main file including those starts to be less "script kiddie" and more in route to some legitimate software. You can take this further with packages (grouping of related module files in a directory) instead of modules (single file), but I wouldn't concern yourself with that right now as you seem to just be getting the very basics of programming.
What is the default if you do "/usr/bin/env python" in the terminal?
Honestly any sort of variable would suffice for it. The hope is that I'll simply be able to manipulate it to change to the variable I wish to display. It'll be case studies so changing the region of the map will be irrelevant. So I guess in terms of contours of temperature, humidity, mixing ratio, vertical momentum, pressure. Things of that nature, for different layers of the atmosphere. 
Uninstalled the python tools if it's already installed from the control panel Download "Python Tools for VS 2015" from this link: https://github.com/Microsoft/PTVS/releases/v2.2 Install it Restart VS, the Python template should appear without any problem. Let me know if it worked for you ;) 
Yeah, as far as I know, webAgg will use a Python kernal on the backend, whereas bokeh / bqplot can render html/javascript. [mpld3](http://jakevdp.github.io/blog/2014/01/10/d3-plugins-truly-interactive/) can go from matplotlib -&gt; html/javascript. I haven't really been enchanted by any of these options. It would be great to see them compared side-by-side.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." Show them that you've tried to solve your problem and you can get all the help you need. Cheers &amp; best of luck!
I think you're right, and that it still does. You could use mpld3 to go from matplotlib -&gt; html/javascript (not sure what it's doing under the hood). It looks like bqplot is also using d3 to render.
How is https://github.com/cosmologicon/pywat/blob/master/README.md#converting-to-a-string-and-back a wat? 
So slow to render for how simple some graphs are... and they don't look fantastic either. And they're not interactive, or animated for realtime data. Just simple plotting. Works fine for lots of things, but it's the most common graphing lib for python and it's just not that great for a lot of reasons you'd need graphing. The API doesn't seem that intuitive either, at least not to me. IMO it's the one thing R has on us. R is well known for having great visualization, and it does. I've seen people quickly connect to a mysql db, and you get this web ui for free with all the visualizations you can make with the input source. Personally I think we need that, or a really well put together library that works with all sorts of data sources, launches flask and serves up graphs using d3.js or something.
A few reasons. I'm consultanting on this project and I'd rather not install/maintain an ipython server. The two services I glanced at (Wakari and Authorea) didn't seem like an immediately great fit to run some of the packages we need (including rpy2 with R). Also I think it'd be good to expose people to the filesystem structure of command line development. Even if that's the cd command, typing bin/activate, and then ipython. The online IDE I'm testing has the visual/clickable filesystem along side the command window so it'd could be a good thing for Windows users who almost certainly won't be familiar with the command line. (EDIT) Lastly, I don't know if it works very well for helping people explore code. But I've never really used it that way, so I don't know for sure.
`%matplotlib notebook` Will let you have the normal tools within a matplotlib plot inline in a `jupyter notebook`.
Which R package does this? Sounds great
Check out vispy for better rendering performance. This lib uses OpenGL and has a gl backend for mpl too
I found Trinket (https://trinket.io/ ) works well and can run with matplotlib. See: (hope you can see this) https://trinket.io/library/trinkets/5affb34ebd
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." Show them that you've tried to solve your problem and you can get all the help you need. Cheers &amp; best of luck!
Take a look forma grafana
I'm a library author,sooo... If you support Python 2.7, it's fairly simple to support Python 2.6. That said, if I needed to use it on Python 2.6, I'd make it run on Python 2.6, but since I don't, I won't test it against Python 2.6. Since I'm not testing it against Python 2.6, it's a fair bet, it doesn't work. Pick two versions to test against: Python 2.7 and 3.x (I'm still testing against Python 3.3 just cause) and let other people deal with with compatibility with other versions until you need the other versions. In terms of CentOS, I still support CentOS 5, which ships with Python 2.4. It's annoying. It's often easier to fix packages (e.g. pandas, docopt) to run in 2.4 than rewrite code.
Can you please elaborate?
[Image](http://imgs.xkcd.com/comics/pointers.png) **Title:** Pointers **Title-text:** Every computer, at the unreachable memory address 0x-1, stores a secret. I found it, and it is that all humans ar-- SEGMENTATION FAULT. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/138#Explanation) **Stats:** This comic has been referenced 83 times, representing 0.0979% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cvs58dh)
Are you talking about Example question 2 with the non-commutative addition? That's just an example to explain how the quiz works. I agree it's not a great wat. However, I did have a reason for including it. The speaker in the original JavaScript wat talk says that addition should be commutative, and he treats it like a wat when it's not. (I think his exact words are something like, "this is plus, so I should be able to swap them and get the same thing".) So it's a classic example, even if it's not the best. :)
Agreed, it's simply not going to happen in NumPy. Try Theano or Numba for some NumPy compatible options. 
I'll ask my coworker and get back to you - he showed me a pretty impressive little set up he had going. From what I've read, visualizations is the main strength of R, apart from stats. That's why people have a hard time migrating to Python from R.
so apparently d3 is more like painting whereas ggplot is like taking a Polaroid (not my analogy)
Having done a few similar things myself, I would actually recommended Anaconda. Download the full windows installer to a USB stick ahead of time, then just copy it to each computer and install is at execution speed, not network speed. And you're guaranteed a full set of compatible libraries, which is always nice. 
Made the index-based for loops just to replicate the OP's code.. In fact, the second piece of code would be more Pythonic doing so: for elem in array: print(elem) del elem print('elem' in vars() or 'elem' in globals() or 'elem' in vars(__builtins__)) ;)
Many people confuse the default style with the api and/or the robustness of the underlying computation.
The problem there is that segregating 2.7 ,3 fails miserably on all modules that need the Python headers to build ( Due to non-versioned locations of those, C really sucks there ). Normal python packages separate sort of neatly. scl allows you to do things a bit smoother, and combines neatly with virtualenv. Our deploy script does something like this: scl enable python33 bash virtualenv myenv source myenv/bin/activate/ pip install -U pip pip install requests cd ~/myproject python setup.py install exit And our launchers do something like this: scl enable python33 myenv/bin/myproject Which, while it's a bit cumbersome at times, works perfectly well, and allows you to deploy code that needs the python headers, and also allows you to cross-deploy in the same way between CentOS6 and 7, which is nice. ( Also, our sysadmin will stab you in the gut if you try to install something on a _system_ that doesn't come in an rpm package, for plenty of reason)
There are Python projects that were inspired by R. R has Shiny and Python has [Spyre]( https://github.com/adamhajari/spyre/blob/master/README.md). R has ggplot2 and Python has [ggplot]( http://blog.yhathq.com/posts/ggplot-for-python.html). I didn't like MAPTPLOTLIB initially because I didn't particularly like the MATPLAB-style API and the pylab interface that the official documentation uses a lot. But once I discovered that it has OOP style API and started to make charts that way, it isn't too bad. This ipython [notebook]( http://nbviewer.ipython.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb) gives examples of the 2 different API styles. IMO this notebook should replace the official online tutorial. As far as aesthetics, as others have mentioned, you can change the default style of MATPLOTLIB. Yhat's ggplot is nice and just recently supported Python 3 and [seaborn]( http://stanford.edu/%7Emwaskom/software/seaborn/) is pretty awesome.
This could be interesting http://yasermartinez.com/blog/posts/creating-super-small-docker-images.html But this is really cool too: https://hub.docker.com/r/tatsushid/tinycore-python/
... On Windows?
is ggvis going anywhere or is it still in the developmental fetal state?
I had thought about trying this with either Anaconda or Python(x,y). I'm worried about the fact that there are going to be 15 or 20 people, and if it doesn't work on 1 or 2 people's computers, it'll really drag down the course. The other is that I have some specific packages I'm looking to run that wouldn't come with Anaconda or Python(x,y). Do you think it'd be easy enough to have everyone install some random, Python-only (no compiled code) package in 5 minutes during the course?
If you prepare 32 and 64 bit versions, there's not much that can go wrong - not more than other solutions, anyway. Packages should be easy to install with pip, especially if they're on the pypa. Otherwise a USB stick with a local copy and a script to pip install from a local directory would also work. 
It's cool, but quite buggy I've found - for instance multiple figures created from a single cell don't always work, graphs a sometimes closed, sometimes not and if you don't close them you quickly end up leaking memory
First I wondered, "why would anyone want that? That's totally missing the point of CI!", but then I got it. Lol.
I've heard that Chaco (part of the Enthought Tool suite) is supposed to be good for interactive visualisation, but I haven't tried it myself.
It seems to be Python 2 only, though. This may or may not matter to the OP, but it is something to be aware of.
Hey, it's cool. It would be nice to specify the artist name, because sometimes the song title is too generic and we can't find what we want.
I have recently started using this for real-time plotting and displaying a camera feed. It is pretty easy to work with.
Just copy this to sublime or your editor: http://pastebin.com/dZLJ5zKj Every function calculates one number. Every function has the element name that it updates in it, for example: name="Feld01", is used in function ShowStaatsschuldenInEuro() via window.document.Feld01.Show01.value = "...." You do the maths from there. If you insist on scraping, use Selenium with PhantomJS, but that would not make sense since the data is calculated. 
Haha, nice try troll... PyDev still has many users and is widely used. Unlike you, I have references to back it up, just looking at public sources to make sure it's unbiased: http://marketplace.eclipse.org/content/pydev-python-ide-eclipse shows that around 13k users install it every month -- and https://sourceforge.net/projects/pydev/files/pydev/stats/timeline?dates=2015-01-01+to+2015-10-08 shows 13k users from there too... and neither of those is the official way of installing it! Also, LiClipse (http://www.liclipse.com/) which is its official commercial counterpart is doing quite well itself -- although I don't have a public counter to show it, being its main developer, I can assure you it has enough users to keep me busy ;) Note that I do respect the PyCharm devs though (we do have some common structure, for instance, the debugger is shared by both PyDev and PyCharm -- there are some differences because PyDev is completely open source... for instance the remote debugger is available in the PyDev open source version, while it's only available in the pro version of PyCharm). So, in short, I agree that PyCharm is a great tool, but so is PyDev, and many people use it as their default env for Python coding... to be fair, in the Python land, vi is probably used more than either package though ;) 
matplotlib is a much larger project with a much broader focus. If we used matplotlib as a standard, then practically all software projects would be dead.
Thanks! This is what I was looking to find here - concrete data. Thanks a million.
Yeah, you need Docker for Windows. You can walk them through the install and pull the images ( that you have customized ) from your network. If you're a Linux guy, you can get a really nice, reproducible Docker image that everyone on Windows can run. Then you can deal with the mess of issues that are guaranteed to come up with Anaconda installs on a one to one basis, **after** the training session.
I am using this library from last couple of hours and my productivity has been increased ten fold and my boss just offered me a raise. so it totally works you guys. edit: yay, thanks for the gold /u/The-Compiler
While Matplotlib can be used for high-performance, interactive plots, it's main focus is publication-quality static plots, and achieving the former can be a lot of work. I would recommend PyQtGraph.
Why not compile it from source? \s
&gt;Hack the Derivative! Is this some kind of drug infused h4x0r bullshit that works on magic like the fast inverse square root? EDIT: Well, I'll be damned...
Funny PyCharm download statistic are almost impossible to find.. I'm mentioning it because everyone I know that's somehow involves with python uses or recommends PyCharm. https://www.google.com/trends/explore#q=pycharm%2C%20pydev%2C%20wingide&amp;cmpt=q&amp;tz=Etc%2FGMT-9 That download/install number alone doesn't indicate people actually using PyDev in production. PyDev have been in existence for like since 2003, and back then that's probably the only decent tool available so it won't that much of a surprise that PyDev have a large user base. On the other hand PyCharm that has only released since 2010 and has already overtook PyDev in term of popular awareness and positive word-of-month. So, a programmer new to python that have tried out many different IDE would invariable have choose any of the IDE other then PyDev because honestly it felt like a awkward and clunky eclipse hack instead of python-dedicated environment that could withstand all kinds of abuses. Of course there's nothing wrong with using PyDev when one's already invest so much in PyDev pipeline or for some reason stuck with it. But more and more new programmer would probably going to choose a more intuitive and snappy IDE and PyDev is ultimately going to be left in the dust. Edit: link 
I was super impressed with Bokeh.
Okay, fair enough. I definitely don't think they're all intuitive, only got 5/10. for example [[0]] (which I only just now figured why it works like that), and the one about set comparison (which I had to read about here before I got) I guess someone familiar with set theory might have gotten that one about sets.. personally, I hadn't even considered that you would ever even TRY to use &lt; or &gt; on sets.
I'd imagine it's probably exactly the same. I made it for my own uses, and as an exercise. The existence of a tool that does the same thing doesn't mean that I can't make another one.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." Show them that you've tried to solve your problem and you can get all the help you need. Cheers &amp; best of luck!
How often do stable projects introduce new dependencies? The only reason to support ye-olde python is if you already have a commitment to legacy users.. 
cookiecutter is amazing
robotframework for acceptance testing ! Readily convert any existing python library into keywords. These keywords can test your application / system / or whatever
CUDA is just C, with some extra syntax. So you really need to be proficient in C, because there is some nuance to writing CUDA kernels.
So that Python developers can use it, of course!
Thanks - I'd appreciate that very much. I use both at work atm, and it's mostly the rendering speed of R that keeps me going back. Haven't found an interactive plot setup that I like though; being tied to a browser window has proven too slow with large time series.
Why not docker, this is what it's literally built for. People already have salt containers built: https://hub.docker.com/r/jacksoncage/salt/ Also look at: packer.io Then you could define all your build out except it supports many build options like docker, virtual box, digital ocean, AWS, etc.
you mean like [python server pages](https://en.wikipedia.org/wiki/Python_Server_Pages) or like a [mod_python](https://en.wikipedia.org/wiki/Mod_python)? there is also [mod_wsgi](https://en.wikipedia.org/wiki/Mod_wsgi) for more arbitrary framework integration.
[Jinja2](http://jinja.pocoo.org/docs/dev/) is also powerful enough to embed arbitrary python code into HTML/Document pages, just like PHP.
Came here to say this as well. It's a pretty well put together package, although its documentation isn't as good as matplotlib. 
The idea of ggvis seems great (Shiny + ggplot + vega!), but from looking at the docs and repo, there doesn't seem to be much active development (many outstanding issues, little discussion on them, last commit was in June). I don't think it's totally stale, but I'm guessing it's not on the top of RStudio's list.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [(X-post from /r/Python) OneDrive SDK for Python 1.0.0 released! MIT License, on Github and PyPI](https://np.reddit.com/r/programming/comments/3nzuh5/xpost_from_rpython_onedrive_sdk_for_python_100/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Check out Zope and be ready to have your mind blown.
Jesus Christ, has the inability to use a search engine become a genetic defect of epidemic proportions? A simple query would answer this question...
VW? Dieselgate? Joke? Damn, I feel very /r/OutOfTheLoop. ([But not anymore!](https://www.reddit.com/r/OutOfTheLoop/comments/3lxvfb/what_is_going_on_with_volkswagen/))
I'll take it you were pleasantly surprised?
Personally, I'm amazed at how poorly cookiecutter is implemented. Really hoping someone creates something better soon. Conditional directory creation? No. Ability to expand templates within other files? Nada. Type support for inputs? Try again. Ability to only ask questions if there relevant based on other questions asked? Sorry. The ability to select options from a list for inclusion (say a list of common requirements)? Not even close. To add to the pain, defaults on cookiecutter absolutely suck. You set defaults, and then even if a template doesn't use them, it still asks you to assert them on every template expansion! 
When you say multiple figures, do you mean subplots within a single figure? If not, I would strongly suggest doing one figure per cell.
Impressed, rather. 
Finally! I still don't understand why it was not an option from launch.
It might be because Werner is not big on Python. He threw a couple of jabs at Python during his keynote. Not cool bro! 
Very cool but Python 2.7 only? No 3.4 at all? That's just stupid. I assume it will be added soon then?
What sort of jabs do you mean?
It's a port of the PHP version, just like the javascript version.
I'm paraphrasing but he said something to the effect of "I didn't realize languages with significant whitespace still mattered" and "now you can do more with Lambda... even if it is with Python." 
While I've got you here...are you planning on an official Linux client? Though I'm pretty sure one is gonna be made in a week's time with the Python API.
If you're asking about a sync client similar to the ones available for Windows and Mac, I can't comment on that. You're right though, the API provides enough functionality to build a sync client from scratch. 
I put this in a GH issue, but some operations should not require authentication... For example, I have a couple shared OneDrive folders that are available totally anonymously through the web interface (example: http://1drv.ms/1FWwXAk) that i'd love to access via this SDK....but if I have to authenticate thats a problem.... Thoughts? 
I am loving this new Microsoft. Keep it up guys.
This is currently not supported in the API. We are already working on adding it. Once it's supported in the API we'll add it into the SDK, too. Stay tuned!
yeah that's exactly what i was interested in. I know you said no further comments, but I know Microsoft are trying to break into the open-source scene with their current products and One Drive is definitely one of the standouts from what I've seen.
This doesn't answer your question, but out of curiosity, would it be possible for you to share the code for this visualization? I'd love to see how you were able to get all of the interactivity that you did from matplotlib. 
Wow. Beat Google Drive to linux when they had a 4 year head start! Great job :)
Aha - well the lack of an API would challenge including it in the SDK then :). Thanks for writing this!
My bad, i didnt noticed that
Great question; it does! The [OneDrive for Business API](https://dev.onedrive.com/odb-preview/release-notes.htm) is in pre-release, but anything supported by the API is supported in this SDK.
[Or any documentation period](https://github.com/blha303/bootstrappy/blob/master/bootstrappy/bootstrappy.py)
Why would I do this over using something like Django/Flask/aiohttp?
Yay! I had to roll my own a while ago but I bet the official one has all of the cool stuff I didn't get to implement! Before this there wasn't any good library for it.
We've moved all of ours to 3.5, wasn't so much work as numpy/scipy support it fine. I don't see what most of the huge fuss is tbh, the features in 3.5 are fantastic...
&gt; Those could be deal breakers for scientific computing or anything that needs to call down to C libs. The impression I get is that Lambda really isn't for that kind of application at present. It's for code where you can create a "service" that executes within 60 seconds with a relatively small amount of code - basically, ordinary web application stuff, business and internet infrastructure, that kind of thing. Anything more demanding still needs to live outside Lambda.
thanks, this is exactly what i was looking for
Your title is very misleading, I don't see a question anywhere here.
You're trying to create a key logger? Sounds a bit shady to me.
I think this is awesome. I'm just curious, how did your team answer the "how will we make money from this?" questions? If you can't say, I understand. On the other hand, If efforts like this no longer result in questions like that, how glorious to have this in the DNA of the culture of the company. When everyone just understands that it is worthwhile to build an opensourced onedrive sdk. That is awesome. Congrats.
&gt; program that requires me I'm sure your instructor can help.
Fond thoughts &amp; high opinion of it. It has been excellent, for several years now. Like anything, it has a few quirks, but it's a great editor and environment which continues to improve. I still discover cool &amp; very useful features I didn't know existed. Yes, definitely check out Pycharm "CE".
Wow, you're right! I rarely say the word, but this is actually retarded. 
This should probably go in /r/learnpython One option is to use a [regular expression](https://docs.python.org/3/howto/regex.html#matching-characters). I'll leave you to investigate that approach, and then post to /r/learnpython if you need further help and to show what you've tried.
I don't know about pandas, but I've been publishing an app that relies heavily on numpy, even as far back as PyInstaller 2.1.
I want to like PyCharm. I've used it for different things and found it pretty good. But I often find myself going back to Sublime Text for most stuff. As with any IDE it has a lot of overhead, whereas ST is very lightweight. 
You're not my supervisor.
Don5 know if pyqtgraph is dead but I know that the developer is part of the vispy project: https://m.youtube.com/watch?v=_3YoaeoiIFI
Looks like Fig, but with virtual environments.
May i say that i love you ? Thank you very much, this is exactly what i needed
Maybe someone in /r/pygame knows if or how easy it can be used with Pygame. 
What I mean by full stack installer is a build system that installs and deploys not only your Python application, but all the components required: i.e. databases, key-value stores, caching system, log systems, metrics, MQ's, authentication systems, with their configuration tailored specifically for your application - which may have multiple independent components itself, like microservices - under one common root, that is independent of services installed in the host system. So you can have for example PostgreSQL installed system wide, but also under you application root with the configuration that is independent of system-wide PostgreSQL installation. I know Ansible, SaltStack and Chef. I was looking for omnibus-like software for Python, but using SaltStack - that could ease the deployment and provisioning of application and multiple services. I do not want to use containers like docker, because of specific needs (applications should run on host system without any virtualization). Anyway, thanks for the answer.
I cannot use docker, since it's a container virtualization platform. I am not aiming for application "images". I want a build system that can install all the supporting installation with the use of SCM. Packer.io seems to me almost like docker in a way that it creates a machine images. But I will take a look at it more. Thanks.
yeah, gisted that
*/1 means every minute, just like * does. e.g. */2 would mean every 2 minutes, */5 every 5 minutes, etc.
[removed]
Trollius is meant to be a drop-in replacement for asyncio, meaning the API is supposed to be the same (apart from the differences required by features not compatible with legacy Python versions, such as `yield from`). While Tornado can do similar things, it is primarily intended as a web server/framework and does not have a similarly compatible API to asyncio (although there is a compatibility layer so that you can mix Tornado with libraries based on asyncio).
Kudos for thinking about the usability of important APIs like file handles, but I don't think your module is an improvement. The main feature appears to be a global list of file handles attached to the `Open` class, and convenient ways to close them all. Global state has a bad reputation, because it breaks modularity, the idea that one bit of code doesn't need to worry about what's happening in other bits. For instance, have a look at this code using your API: @filecloser def f1(): a_file = Open(...) # do stuff f2() # do stuff using a_file @filecloser def f2(): # do stuff That would be fine with regular file handling, but with your module, when f2 completes, all the file handles will be closed, including the one that f1 still wants to use. This example is contrived, but in general, closing all open file handles in the middle of a program is going to break things. In general, the only time when you can safely close all open file handles is when the process is about to exit - either because it's done or because it crashed. But at that point, Python and/or the OS will close the file handles anyway as part of the standard cleanup.
It's the best Python IDE currently available. I've switch from Eclipse + PyDev to Pycharm.
This was posted some time ago and the author (/u/Wayneminator) [responded there](https://www.reddit.com/r/Python/comments/3mmzyb/procedural_city_generation_in_python/cvgyssm). Perhaps you should direct questions to him.
&gt; When Python has a file handle open when a script crashes/throws an error, the program is abruptly aborted and the file is not properly closed. The file handle is now useless but will occupy memory and potentially cause variable name clashes. When a script is terminated, both normally or via a crash, all its open files are closed. I can't imagine what could you mean by "variable name clashes". &gt; The program then opens the file in a with statement again when it comes time to actually use the file. &gt; The problem with this approach is that a file can be moved or deleted between when the file was verified and when the file is actually used. Again, just open it at the beginning and keep it open, it would be closed automatically when the script exits. And if you have a more complicated use case when it's not enough to have the file or files closed automatically by the end of the program (because you open a lot of them or can open and close one several times) then your "OK just close all open files now" function wouldn't be of any help, you'll have to actually think about and manage file lifetimes explicitly. I'm afraid that the problem your library solves doesn't actually exist, being entirely a product of your confusion about what happens when a script crashes (and what's the difference between the script crashing and some particular function throwing an exception that is then caught).
It depends on whether the error is the more likely situation or the less likely situation. If you expect your code to encounter the error condition less often the non-error condition, then `try...except` is better. If you expect your code to encounter the error condition more often than the non-error condition, then `if...else` is better.
so do I! &lt;3
Outlaw country! 
I don't get the distinction here. Is there some sort of performance reason? Otherwise it feels arbitrary.
great to hear they‚Äôve come far enough to no longer push this back! another nail in the coffin of Legacy Python ;)
I think the idea is that the `try...except` block is meant for catching exceptions, and by definition, exceptions are "exceptional", and should be "less frequent" than the normal code flow.
Scraper. The word is scraper, not scrapper. 
OK, forget the scientific computing thing. Bad example. My point is that if native libs can't be called from Lambda Python functions, it severely restricts it's usefulness. Want to process a csv or json with pandas. Nope, requires Numpy. Want to process an image? Nope, requires PIL or Pillow. ...and so on. One of the primary benefits of Python is the rich ecosystem of libraries available to it, many of which require calls to native libs. Take that away and you've eliminated the reason why many of us use Python in the first place.
To quote from "Dive Into Python": "Exceptions are everywhere in Python. Virtually every module in the standard Python library uses them, and Python itself will raise them in a lot of different circumstances. " Many times a try...except is the ONLY way to detect an error. So yes, you will need to use try...except in your programs.
Yes i had exactly the same issue but after i installed manually the PTVS from the github link i already mentioned it worked (do not install the python tools package from VS otherwise the python template will disappear again) 
How can I join the team?
In your opinion you mean. My opinion is that the signal to noise ratio is declining and questions *easily* answered with a search engine query are constantly asked here. When is enough.... Enough?
It looks like you can use system libs but I'm still not clear how this works. http://docs.aws.amazon.com/lambda/latest/dg/python-walkthrough-s3-events-adminuser-create-test-function-create-function.html In this example they ssh into an EC2 instance and `yum install` a bunch of dependencies. The virtualenv/pip part is fairly clear but there is no discussion over how those system libs get installed in the Lambda machine. 
Note that, as it says at the top of that post, xkcd-style plots are now [built-in to matplotlib](http://jakevdp.github.io/blog/2013/07/10/XKCD-plots-in-matplotlib/). So you don't need to jump through the hoops shown in that notebook to get them anymore. All you need is to call `plt.xkcd()` before you create your figures.
I mean the Fedora team which works on porting python 2 to python 3 code.
This looks very promising, although the lack of test coverage and CI concerns me. Do you have any plans of getting the project hooked up with CI and a test suite?
I'm not super familiar with python (hour 3 reading/learning). I replace "if line.startswith("line"):" with "if re.match('[0-9]{1,2}/[0-9]{1,2}/20[0-9]{1,2}'):"? Edit. I think I've got it. for n,line in enumerate(data): line = line.strip() if re.match(regexdate, line): data[n] = "\n"+line.rstrip() else: data[n]=line.rstrip() print ('|'.join(data)) Now to just output this data into a results file. thanks
Perhaps your script is outputting errors to STDERR? Your command would not catch that. Try something like this maybe: */1 * * * * /path/to/python /home/user/bin/script.py &gt;&gt; /home/user/output.txt" 2&gt;&amp;1 Notice the "2&gt;&amp;1" at the end, this will output both STDOUT and STDERR to your file.
I've started creating a parser myself. I'm going to have to try this one out. However this looks like its for Django. Mine is for SQLAlchemy.
The project needs some cosmetic work: readme split up to readthedocs, wiki or whatever, and the giant single code file broken into some modules. The scrolling gives me vertigo.
I've never heard of these two ideologies before. Very informative. 
Yeah, that sounds like it would work. I'm curious as to why you wanna do this though. The first guess I have is that you want to try and avoid some type of data cap. 
Thanks everyone for feedback and constructive criticism. A quick note to Synackaon before I make a more general response to prior comments: the pretty file printing involves overwriting str and not repr (I knew overwriting repr would be a bad move). More generally: I did not know about many of these current solutions so thanks everyone for letting me know. When I first thought up PyFHI, a co-worker and I were trying to come up with good solutions to checking user permissions to a file and keeping a hold of the file. Both of us were under the impression (and I seem him better at Python than me so I took his word on it to) that file handles weren't closed cleanly when Python crashes. We thought that the OS would normally do a good job cleaning up but that it's better to do it in a script. As such, I focused my research of the issue on checking user file permissions and keeping files open and other products of PyFHI arose as by-products. This angle/keywords used probably caused me to miss six, opening multiple files with with, etc. The part about variable name clashes, mentioned by xXxDeAThANgEL99xXx, is something I read in a few blogs but perhaps accepted to readily. A note humble way to announce PyFHI would have been to announce a functional release for review, rather then release a "final" version. I had a functional (though, it seems, not very useful) product and wished to distribute it to help others but I was very overconfident/naive in doing so. Thanks Syanckaon for constructive criticism and ideas on ways PyFHI could be made useful (namely the idea about deciding how to beat read a file, which it doesn't currently do). Thanks takluyver for showing me problems with global states. And thanks xXxDeAThANgEL99xXx for the positive callout :D I still feel PyFHI holds value though perhaps not in it's current state. Namely, the use of a class mimicing file object API allows myself and others to quickly extend/implement changes to Python file handling. If PyFHI served nothing more then to give everyone a "class version" of file handles (which I'm sure most of you could write yourself, but hey, it's been done now) then I'll feel happy that I've done something. I think I'll leave PyFHI as is (though change the README and ABSTRACT to better fit reality) and slowly work on a version 2 that is actually useful and implements your guy's suggestions. More feedback is always appreciated. This has been a great learning experience. 
Soon, we are going to introduce *coins* - you can use coins to speed up your upload/download speeds, and get more storage capacity! Only $2.99 for 500 coins! /s Making the SDKs for OneDrive directly reflect our mission to empower every person and every organization on the planet to achieve more. It certainly is awesome that we were able to release it as open source :) 
Python 3.5 was still in beta for much of the time we were working on this. We will support 3.5, but we wanted to get this into your hands right now rather than make you wait until we implemented that, too.
I'm parsing through an access list from a security system, or at least what it considers a report. 08/01/2015 03:10:38 Admitted Doe, John (Card #198977) at MAIN TOWER - DR [In] The data should all be on one line. Unfortunately for some godforsaken reason it puts text on a new line and indents. My goal is to first consolidate all of this data onto a single line so it's closer to uniform. Once that is done I want to delete text between the Time and (Card #198977) so that there are no names. The regex I provided was something super simple to match any date format 01/01/2000 to 99/99/2099 I know that at all times the date format will be a valid date so I don't need to build the regex to be super smart and check to see iv these two digits are between 1 and 12 or 1 and 31. 
&gt; My point is that if native libs can't be called from Lambda Python functions, it severely restricts it's usefulness. My point was that "severely restricted usefulness" may in fact be close to the current intent for Lambda. As I see it, the current sweet spot is for creating highly reliable and scalable, but basically simple, glue services. More serious service development is challenging for a lot of different reasons. I'm sure that'll change in future. But, I wouldn't be surprised if it's possible to deal with the dependencies you're concerned about, one way or another. 
Thank you. Yes I'm working on test suite. 
Well, you've made two mistakes in your example code: using a naked "except:" that catches EVERYTHING, and using "pass" to silently swallow exceptions. You might want to study exception handling a little bit more.
Noted. 
Ubuntu has already done this in the 15 branch. [Here is an older story on it](http://news.softpedia.com/news/Ubuntu-15-10-to-Finally-Drop-Python-2-X-Support-477830.shtml) and having logged onto a 15 system and checked they are using it by default now. 
That was a speed test, not production code
&gt; The Pythonic way would be Oh, absolutely. My point wasn't that this is how one should test for a key existing in a dictionary -- but rather that your example purporting to show the downside of exceptions was (in my mind) really showing that bad things can happen when you don't think carefully about what you put in the `try:` block. I'll agree with you that a beginner who uses the `try: except:` pattern without really understanding how it can unintentionally suppress errors can be a dangerous thing. So perhaps it's fair to say that a beginner should use exceptions cautiously, rather than liberally. But I'd take exception (see what I did there?) with the comment that one shouldn't use exceptions in production code. Rather, I'd just say that a developer should in general try to keep the body of the `try:` block as specific as possible, keeping in mind what exceptions can propagate from the code in the `try:` block. Using `except Exception` should be avoided in general. I tend to define custom exceptions liberally and make each as specific as possible. I've found that this makes my code cleaner and reduces bugs.
 &gt; &gt; That was abnormally rude for a Python community member. &gt; In your opinion you mean. Apparently, this is not obvious... I don't think it was rude. It was more exasperation with constant degradation of the signal/noise ratio. I believe it to be laziness on the part of the questioners. You have stickies that says "Ask your learning questions in /r/learnpython", you have gentle reminders all over the place that search engines know the answer... Yet, the simplest and easiest for the search engine to answer questions are repeated time and again... 
&gt; I started learning code with Python and I love it. I write it for fun. Now I'm having to learn Java for college and it sucks. There's so much more code to write to do simple things and you can make so many silly little syntax mistakes so easily that it makes it harder to actually write functioning code. A lot of people in my class are completely new to programming and they are so lost. I started coding in VBA at work because we use Excel extensively and it's just as bad (Dim x as integer, i as integer... ughhhh) Python was such a breath of fresh air. We don't support Python as well as VBA at work, but support is growing slowly and steadily.
I have been using [pudb](https://pypi.python.org/pypi/pudb) lately and find it quite helpful.
I understand but still I seriously doubt that Ubuntu 15.10 won't contain python2 looking at a bug tracker I posted and the release schedule of 15.10. Yet of course I would be more than happy to be proven wrong (Like really happy, we spent just half a year porting samba's libraries to Python3) :-)
Thanks for the clarification. 
 In [22]: def try_test(): x = {} try: x[0] except: pass ....: In [23]: timeit try_test() 1000000 loops, best of 3: 433 ns per loop In [24]: def try_test(): x = {} try: x[0] except KeyError: pass ....: In [25]: timeit try_test() 1000000 loops, best of 3: 502 ns per loop Blindly catching ALL exceptions is the wrong thing to do. What good is speed if you're doing the wrong thing? 
First off, this is awesome, thank you! Great to see Microsoft contributing to the open-source community. I do have a general question about the RESTful OneDrive API though. I am trying to plugin a OneDrive for business account and I am making the below request as specified [here](https://dev.onedrive.com/README.htm) to list my drives, but the response I get is an empty array GET https://{tenant_name}-my.sharepoint.com/_api/v2.0/drives Authorization bearer: {token} { @odata.context: "https://{tenant_name}-my.sharepoint.com/_api/v2.0/$metadata#drive", value: [ ] } Below does return my default drive however. Any idea why the previous is not working? GET https://{tenant_name}-my.sharepoint.com/_api/v2.0/drive Thanks in advance and keep up the great work! **e:** grammar :(
And you need to have the font installed. [Humor Sans](http://xkcdsucks.blogspot.ca/2009/03/xkcdsucks-is-proud-to-present-humor.html)
As you pointed out, this may be an issue with the API itself. Please go to the [GitHub issues page for the API](https://github.com/onedrive/onedrive-api-docs/issues).
This is likely because most enterprise/large scale software engineering is performed by professionals whereas a the python blogging is done by newer or hobbyist programmers.
Given that I don't know the details of what you're doing, I'd like to heavily encourage you to just go with cpython (or pypy since it sounds like you have it installed already). This way it's more portable and easier to maintain. No reason to jump to cython unless you're doing a lot of number crunching. And I mean a lot. Like the other poster said, if that's a concern, measure your cpu and io usage to see what it's actually bound by. 
No problem, I am really looking forward to a "good" Python blog :). I really think having a defined topic, a special theme, would be an interesting idea and would help you to establish a popular &amp; useful resource! &gt; Would you like to contribute to the blog? Thanks for asking! Unfortunately, life is a little bit too hectic at the moment. I just finished writing a Python-related book a month ago and I thought I would have "free time" again, but there is so much to do that I don't even find the time for my own blog anymore ... But I am looking forward to see and read your blog, please send me a link some time when the first article is out :). Good luck, and most importantly: Have fun :)
~~Do not have a source on this~~, but I think Guido once said that EAFP being preferred is myth. EDIT: From the [PEP 463 rejection](https://mail.python.org/pipermail/python-dev/2014-March/133118.html) &gt; ... I disagree with the position that EAFP is better than LBYL, or "generally recommended" by Python.
Cron job would make terrible WSGI server. 
I'm quite partial to the "robust single linkage" algorithm of Chaudhuri and Dasgupta. It works pretty well on this data set: http://nbviewer.ipython.org/gist/anonymous/92ef4d2a382f41d3ee0b It is an agglomerative algorithm, and so outputs a whole tree of clusters, showing the multiscale structure of the data. It's also a provably-consistent estimator of the density's structure, which is nice from a theoretical standpoint. Also (I'm not sure if you wrote this notebook or not), when you say that a clustering algorithm "shouldn't be wrong", you should clarify what you mean. In other words, you should rigorously define what you mean by a "cluster". In this notebook it seems that you mean to say that a cluster is an "island" of high density. That's quite a good definition of a cluster, but it's by no means the only definition. Furthermore, in practice it can be difficult to recover high density clusters due to the curse of dimensionality. The data you've shown here is pretty "dense" -- if you tried to repeat the same experiments in, say, 10 dimensions, but with the same number of points, you'd find that HDBSCAN and robust single linkage perform quite poorly. In any case, if you define your clusters to be "regions of high probability", then you can precisely study the sense in which an algorithm correctly recovers the clusters. It turns out that robust single linkage correctly recovers the clusters with high probability in the limit of infinite data, as this paper shows: http://arxiv.org/abs/1506.06422
sorry I am new to reddit, something is wrong with this code? It keeps getting erros, anyway to fix it?
I know it's not optimal to suggest something one has not used, but I am currently looking into Bokeh ( bokeh.pydata.org/). 
I have no interest as to whether or not the data should be on one line. I am telling you that you should be extremely careful when iterating around a data structure whilst modifying it. [strptime](https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime) should provide you with what you're looking for.
Not.
I've been doing some research on various aspects of what has been discussed on have found a few points I'd like to further debate: Synackaon &gt;Regarding the issue of dangling file handles, those are subject to the garbage &gt;collector unless used with 'with' or a finally that closes the context. Such is usually &gt;a sign of poor programming (not using with or replacements). In CPython, Jython, etc. this is true but not necessarily for all. When a Python script crashes or exits normally C Lib will clean up resources (including the closing of file handles). CPython, Jython, etc. also has a garbage collector to delete files no longer being referenced. However, not all implementations of Python have a garbage collector and thus not all remove files by reference counting. It is bad practice to not use with statements or try...finally blocks (as you pointed out), the point of PyFHI is to simplify/improve/automate this via the @filecloser wrapper (which as I'll mention below, needs work) while adding additional conveniences (though admittedly unnecessarily). Also to Synackaon: &gt;Regarding the cross Py2/3 divide, Python 2.7 has the next() function like Python 3. As a simple test I ran a Python 3 shell, opened a file, and called next()...it threw an error. Python 2 calls next() and Python 3 calls __next__(). So the cross-compatibility from PyFHI is valid. To takluyver: Thanks for bringing that example up. I will work on ways to fix this issue (I already have several ideas in mind). To both takluyver and Synackaon: Reading over contextmanager, I agree that is a better alternative since it is more versatile and already established. Perhaps I will work on incorporating PyFHI's file handle improvements (like cross-compatibility and printing) into contextmanager (and add more improvements like your great suggestions Synackaon) or perhaps take the contextmanager-like aspect out of PyFHI (aka filecloser) and focus on file handles. Once again thanks everyone for your feedback!
Hey there. Yes, you can use pygame to get the inputs, but the events are tied to the pygame display, which means that you can't use ssh or headless. Tkinter is the same way. so as long as you aren't trying to handle via ssh or headless, you can use this: https://stackoverflow.com/questions/17815686/detect-key-input-in-python
First of all, good on you for accepting criticism gracefully. I know from personal experience how much it sucks to invest a considerable amount of effort (and pride!) into some piece of software only to have people tell you that it solves a wrong problem. &gt; I still feel PyFHI holds value though perhaps not in it's current state. I don't know, man. The `__str__` thing might have some value, true. Not in its current form, trust me, you shouldn't want to see the output with newlines because it would look exceptionally ugly and confusing when you're printing other objects as well. Like, a dictionary containing a file object -- that would be a disaster! And that's, like, 99% of the actual use cases where `str()` is called on a file object implicitly. But maybe you could convince the core devs to accept a patch that adds "closed" and "encoding" attributes to the usual output. That would be an uphill battle because of breaking the code of the people who use `str()` on a file wrong (like, try to parse it for some reason), but I for one am not one of those people and would prefer a Python interpreter that includes those attributes, so yeah. Even if it gets shot down, it would not because it's a bad proposition but because life sucks, and I encourage you to try and propose it (as a patch, not as a PEP, obviously). The `close_all()` functionality on the other hand appears completely useless to me, sorry. As others pointed out [contextlib.ExitStack](https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack) does all that and way more than that and is more versatile and easier to use too. You can use it with `using` statement or you can use it to add files (or anything closeable!) that should be closed to an object that will be used by your caller, if you need that. While the functionality of closing _all_ files is really not useful ever. ------ I'd like to offer some advice on how to avoid spending so much effort on writing and carefully documenting a thing that turns out to be pretty much pointless. First of all, be much, much more strict in describing discrete use cases. Like, "closing files if the script is aborted", "closing files if a function that opened them is aborted", "closing files that a function that was aborted used". What value do you intend to provide in each of them? Instead of that you seemed to go along with some amorphous chimaera of all of them at once, don't do that. Second, make the _minimal_ working prototype that shows some value, then run it through your coworkers and then people here or on other forums to see if the value it shows is the value they need, that it actually solves some actual problems. _Before_ committing to the overall design, implementation, and documentation. "Worse is better" beats "The Right Thing" because it receives feedback from the users, and it's all about the users, it's all about the ways the programmers would want to use your thing, and you can't substitute your own vision or logical arguments for that because you don't have a mathematical model of the users. Designing a good API is a social, not mathematical enterprise. "Social" _not_ in a sense that you'd have to convince people to use your API, but in a sense that you can't deduce what API they _need_ from pure logic.
I can suck too ;)
Thanks for the blog link, adding it in my reader for future reference :)
I'm in the brainstorming stages (that's why I'm asking early so I don't invest too heavily into Cython), but I'm thinking of a process that a person can access directly in the terminal locally or SSH and another process that runs on a port and people can access in their web browser to view details of other processes and manage them.
:3
Sounds like python would work just fine, and probably be a lot easier for you as the person writing the code. Good luck!
&gt; Apparently, this is not obvious... I don't think it was rude. Ahhh, that explains a lot about where our communication went awry, because I couldn't see it as not rude. "Jesus Christ, has the inability to use a search engine become a genetic defect of epidemic proportions?" I ... sort of can see how that could be done not-rudely in in-person communication, but in written form you might want to add a few more smileys or somesuch. :-) Anyway. Have a nice weekend!
Here is what I was thinking at first: 1.) Run setup for the file and detect if PyPy is on the system and also if Cython is installed for the current Python running the setup. 2.) If "has_pypy" then use that but if it doesn't and the user "has_cython" then run the cythonize script on the standard .py files as-is. If neither of those exist, then just make the system Python the default. 3.) I figured a non-optimized straight cythonizing of the standard Python script would have a small performance increase and obviously introducing a .pyx file with more Cython specific code would require extra work. I looked at other projects like the Falcon framework that when installed, check for PyPy and Cython. When Falcon cythonizes the framework, it just does the standard cythonize and doesn't have any static types or cimports: https://github.com/falconry/falcon/blob/master/setup.py 
Tip: if it's not efficient but you want to do something anyway, you can also use this to estimate how many other people would need to use it before you break even. Estimate possible users, compare numbers, decide to do it anyway... And there's an xkcd for this too. 
Thanks very much for this advice. I appreciate it greatly (I honestly feared hostile responses once I saw the first criticism but gaming and programming are different worlds :P). I won't lie that it hurt to be wrong but that's the beauty of open source community; anyone can create and innovate and anyone can criticize and modify so that we all end up with a better product. I'll be going to grad school soon (not in CS) so criticism will be apart of life. I'm glad I got my first real taste here and now. 
Wonderful as it is there's no way you should need to go to that specific site to install pywin32. I'm certain that I've seen this reported so I suggest the OP try google, possibly targetting `site:bugs.python.org`. You never know, you might get lucky :-)
This is a nice post, I am just wondering "why now" since like you said, XKCD in built-in since 1.3 and Jake created this Notebook back in 2012 :)
As a long time user of PortablePython, I would say that [WinPython](https://winpython.github.io/) seems to be the best alternative. It has Spyder IDE and Python 2.7, 3.3, 3.4. I would expect for a 3.5 release in the near future. It also has all the Qt goodies from PortablePython and IPython of course (pandas, other libraries too).
I doubt many people here are going to help you out with that (ethics and all that). If you want to put some work into it, look around on the internet for code examples, and think about what platforms you are going to run this on (I highly doubt there is a way to get keystrokes from all 3 major platforms using the same method). Also, some platforms are secured specifically against this kind of thing.
I could tell you, but I don't want you doing anything illegal with it, so I won't. This is also why few people will answer this type of question. Also, I looked at your account and found that you already asked a similar question on /r/python. You say you are new to reddit, and this is true, so let me explain some things. &gt;__You should go to /r/learnpython and ask a question.__! (from the first question you asked, emphasis mine). Reddit is built around communities, and some more than others apply rules for what content they want in their community. A question of your type is a) suspicious (because of the nefarious possible uses of it), and b) not for /r/python, which is about news, not questions. So essentially: You're using reddit wrong. I want to help you learn Python (and reddit?), but you are making it really hard for me to do that because your doing things wrong (and not taking advice), and our asking a "shady" question. Edit: Having re-read you previous post, I would like to take back some of what I said. You _did_ take advice, and I was wrong. I apologize. But, you still should post questions on /r/learnpython
Is there a way to hand-draw-ify a font? Even Humor Sans is too obviously not hand drawn, each letter instance is identical, spacing is perfect etc. Maybe if each letter was defined by lines, they could be tweaked likewise.
Alright. I'll check it when I get a chance. Unfortunately, Pygame is a necessary part of these courses. Thanks. 
Fucking awesome.
speaking as a university student / hacker: some notes are handy, which I personally do entirely in digital, typed form: comments in my code and Tomboy notes. Not much to Tomboy: it's just a little bit of formatting with simple syncing over SSH / sshfs. Letting a project grow organically will get out of hand relatively quickly. Once you realize you're strapping on stranger and more foreign types of behavior, look for a library that does that stuff and relegate the new behavior to separate modules. `from module import function` is your friend; abstract away complex behaviors and use simplified outputs. I can't think of a specific example off hand, but your file/database interaction, calls to a server, or any code block you've copy/pasted and barely tweaked can probably be abstracted into a simple call into a separate module.
Versions look good and up to date! Again, thanks a ton. Glad to be back on track with learning name mangling and class inheritance the proper way with a working super constructor.
http://www.downforeveryoneorjustme.com/http://www.pythonchallenge.com
Nope. I refactor early and often. I run an open source project that's 200k lines of code. It's works well. The signs of a good code is one that's easy to add new features and one that's easy to read even though there are seriously dusty bits.
I love everything about this except the font. It looks like it's just being rendered too small for it to look good as a hand-written font. Still, a very cool idea, and the implementation really came out well overall.
&gt; A solid example application A Hello World o [To Do List](http://todomvc.com/) would be fine. Good luck
I made a website to do it much simpler! http://xkcdgraphs.com/
I also made this website if you don't feel like programming :) http://xkcdgraphs.com/
And further if I add **if/else 2**, it is twice as fast as **if/else 1**.
*In your opinion* 
These are all terrible ideas. You are needlessly and incorrectly over optomizing waaaay too early in particular, 2) is absolute fucking madness.
At a guess, you've probably opened both projects in the same window - Pycharms default is to dump them into the same repo. Easiest solution in this case is to close them both, and reopen just the new project, then you should be able to commit it to its own new repo. if you've got a different situation, please describe it in more detail.
&gt; If you are about to ask a question, please consider r/learnpython. \- Sidebar
traitlets / traits?
A great QA on StackOverflow on this same matter and the best answer is by a core Python dev. Raymond Hettinger. http://stackoverflow.com/a/16138864/617185 **Quoting:** &gt; In the Python world, using exceptions for flow control is common and normal. &gt; &gt; Even the Python core developers use exceptions for flow-control and that style is heavily baked into the language (i.e. the iterator protocol uses StopIteration to signal loop termination). &gt; &gt; In addition, the try-except-style is used to prevent the race-conditions inherent in some of the "look-before-you-leap" constructs. For example, testing os.path.exists results in information that may be out-of-date by the time you use it. Likewise, Queue.full returns information that may be stale. The try-except-else style will produce more reliable code in these cases. &gt; &gt; &gt; "It my understanding that exceptions are not errors, they should only be used for exceptional conditions" &gt; In some other languages, that rule reflects their cultural norms as reflected in their libraries. The "rule" is also based in-part on performance considerations for those languages. &gt; &gt; The Python cultural norm is somewhat different. In many cases, you must use exceptions for control-flow. Also, the use of exceptions in Python does not slow the surrounding code and calling code as it does in some compiled languages (i.e. CPython already implements code for exception checking at every step, regardless of whether you actually use exceptions or not). &gt; &gt; In other words, your understanding that "exceptions are for the exceptional" is a rule that makes sense in some other languages, but not for Python. &gt; &gt; "However, if it is included in the language itself, there must be a good reason for it, isn't it?" &gt; Besides helping to avoid race-conditions, exceptions are also very useful for pulling error-handling outside loops. This is a necessary optimization in interpreted languages which do not tend to have automatic loop invariant code motion. &gt; &gt; Also, exceptions can simplify code quite a bit in common situations where the ability to handle an issue is far removed from where the issue arose. For example, it is common to have top level user-interface code calling code for business logic which in turn calls low-level routines. Situations arising in the low-level routines (such as duplicate records for unique keys in database accesses) can only be handled in top-level code (such as asking the user for a new key that doesn't conflict with existing keys). The use of exceptions for this kind of control-flow allows the mid-level routines to completely ignore the issue and be nicely decoupled from that aspect of flow-control. &gt; &gt; There is a nice blog post on the indispensibility of exceptions here. &gt; &gt; Also, see this StackOverFlow answer: Are exceptions really for exceptional errors? &gt; &gt; "What is the reason for the try-except-else to exist?" &gt; The else-clause itself is interesting. It runs when there is no exception but before the finally-clause. That is its primary purpose. &gt; &gt; Without the else-clause, the only option to run additional code before finalization would be the clumsy practice of adding the code to the try-clause. That is clumsy because it risks raising exceptions in code that wasn't intended to be protected by the try-block. &gt; &gt; The use-case of running additional unprotected code prior to finalization doesn't arise very often. So, don't expect to see many examples in published code. It is somewhat rare. &gt; &gt; Another use-case for the else-clause is to perform actions that must occur when no exception occurs and that do not occur when exceptions are handled. For example: &gt; &gt; recip = float('Inf') &gt; try: &gt; recip = 1 / f(x) &gt; except ZeroDivisionError: &gt; logging.info('Infinite result') &gt; else: &gt; logging.info('Finite result') &gt; Lastly, the most common use of an else-clause in a try-block is for a bit of beautification (aligning the exceptional outcomes and non-exceptional outcomes at the same level of indentation). This use is always optional and isn't strictly necessary.
Nicely done to track down the source!
[Blinker](http://pythonhosted.org/blinker/) has a much more flexible approach, imo. That said, I've personally only really found signals and slots useful for GUIs.
Written in Haskell, you should say that in the title too :)
Sometimes it is about performance. Sometimes it isn't. `if...else` and `try...except` do completely different things. In general, they are utterly unrelated. One performs a branch depending on a flag. The other sets up an exception handler and a jump on error. But sometimes, you have a choice between writing a test and avoiding a possible error, or catching an error after the event. Which of the two strategies you pick will have performance implications. The classic example is deciding whether to check for a key in a dict before retrieving it, or just try to retrieve it and if it fails catch the exception: # look before you leap if key in thedict: return thedict[key] else: return default # easier to ask forgiveness than permission try: return thedict[key] except KeyError: return default Obviously they perform differently. The LBYL version looks for the key in the dict *twice*. That's usually fast. (Usually.) The EAFP version usually only makes one key lookup (setting up the error handler is very cheap) which is obviously faster than two lookups, but if the key is missing, actually catching the exception is expensive. So which is faster? That depends on how often the key is missing. If the key is often missing, then catching the exception lots of times is very expensive and LBYL is faster. If the key is rarely missing, then two lookups is wasteful when a single lookup will do. The exact break-even point will depend on the version of Python and the kind of objects used as keys, but in my experience, and roughly speaking, the break-even point is about 1 missing key out of 10. So if the key is present more than 90% of the time, using a `try...except` is faster. If the key is missing more than 10% of the time, using `if...else` is faster. YMMV. But remember that fundamentally the two operations do completely different things. In general you can't swap out one for the other. But when you can, there is usually a performance difference.
&gt; No difference between if-else and try-except unless the exception actually gets triggered. Your test is flawed. It's not enough to just check whether the key is in the dict, you still have to retrieve the value: if key in d: value = d[key] # versus try: value = d[key] except KeyError: pass A successful retrieval requires *two* lookups, not one: one to see if the key is in the dict, then a second lookup to actually retrieve the value. When you take that into account, `try...except` will be faster if the key is nearly always present. The difference may be small for a tiny little dict with a single key, as in your example, but in more realistic examples (you have a large dict, with millions of keys with complex key hashes and quite a few collisions) two lookups will be about twice as expensive as one.
&gt; Both of us were under the impression (and I seem him better at Python than me so I took his word on it to) that file handles weren't closed cleanly when Python crashes. Python should never crash. If it segfaults or dumps core, that's a bug in the interpreter that should be fixed. If you mean a regular stack trace following an exception, that's not a crash, that's working as designed, and Python will close files as normal. "As normal", however, is quite complex: - if a reference to the file object still exists, it will stay open so long as that reference exists; - when the last reference to the file object is gone, if you are running CPython, the garbage collector will *immediately* close the file; - but if you are running Jython or IronPython, the garbage collector may not close the file until the interpreter exists; - if the interpreter is killed by an external signal (say, `kill -HUP` on Linux) what happens to the files depends on the OS. I would expect that Linux and OS X will close the files, I have no idea about Windows. 
Looks nice. Keep up the good work!
Any particular reason it uses Python2's `.pyc` convention rather than Python3's `__pycache__` convention?
from stackoverflow community I know that Martijn Pieters has a very high reputation and gives some consultation on https://www.codementor.io/mjpieters from his blog which gives good advices in general I know that Reuven M. Lerner gives some courses on http://lerner.co.il/ 
Does it matter, no. That being said [pelican](http://blog.getpelican.com/) is probably the most prominent of the Python powered ones. By checking out [pelicans github](https://github.com/getpelican/pelican) you can see it has a large community, is very active, and therefore is a safe software adoption decision.
You almost certainly don't need pypy. Stick with CPython until you know you need pypy. Also, realize that pypy doesn't give you performance for free. There's a tradeoff involving memory consumption, so cross that bridge when you get there -- not before.
&gt;I'm in the brainstorming stages Then it's way too early to even consider pypy (unless there are known facts that prove the contrary). Always measure before optimizing. 
Like I was saying, I was purely asking around before committing to any method and developer time. All I did was take a look at other projects that have PyPy/Cython detection and seeing if anyone here had any tests. I mainly asked here because I searched around for Cython vs PyPy for this type of thing and couldn't find any test cases. Oh and your comment is very back handed. I never had a "terrible idea" when I started this thread. Am I not allowed to ask people who have tried this sort of thing in the past before I spend the time and potentially waste effort? If I had spent 100 hours on this, then I would have had a "terrible idea" and was in the wrong. To be honest though, if running a straight up Cythonizing of a .py file without altering the source code and import that, wouldn't I get some benefit even if it was minor?
ITT: A bunch of naysayers naysaying
&gt; Like I was saying, I was purely asking around before committing to any method and developer time. Cool, then you have your answer. &gt;Am I not allowed to ask people who have tried this sort of thing in the past before I spend the time and potentially waste effort? Of course you are. You exposed your ideas, some of which were terrible. It's really not a huge deal. Asking questions isn't a terrible idea, but premature optimization is. You know this. We know that you know this. Please take criticism with grace and maturity. &gt;To be honest though, if running a straight up Cythonizing of a .py file without altering the source code and import that, wouldn't I get some benefit even if it was minor? It seems as though you're missing our collective point: cythonizing a .py file is premature optimization as well, unless there's something you haven't told us. To answer your question more directly, though, this depends on what you mean by "benefit". Decreased CPU time? Almost certainly (in my experience, 25-30% is common). But **this does not come for free**. The complexity of your build just increased, and you almost certainly don't need it. Sure, it works *now*, but you just opened a whole new can of worms... Do yourself a favor and don't optimize until you have an actual problem. There's a reason we're all recoiling with horror at your plan, and it's not because we don't like you; it's because we've all experienced the pain that comes from such an approach. **Edit:** Of course, if the point is to learn how to use Cython, then that's a whole different story...
Just switch to GAE you'll have the same thing
Thats a nice collection out there ripie...
That's not fair. Writing similar functionality in an alien language is an impressive exercise in understanding how the Python language is interpreted. If we were to dial this assholery to 11, Hy would have been met with "So? Just use Common Lisp! What's so special about being able to access Python objects?" 
Advantage is in the eye of the beholder, fool.
So how would the deploy work? You build something and then copy it to your server? Is the server running docker? Does that slow it down?
Why does he do docker exec to run migrations? I'm confused. 
go for ERPNext it has everything that small firm may needs + more with simplicity and mobile friendly..
Kind of confused why he uses compose for Postgres but still runs uWSGI and Nginx in the same container under supervisor. Why not make them their own containers?
ITT: /u/fnork being an arrogant ass.
I'm just gonna put it out there that just because someone wants to build hacking tools, that doesn't mean they want to do illegal things. Many of us that build dangerous shit are doing it for fun, to learn about security, and often in order to understand our adversaries and protect our networks by testing them. Hacking tools in python are a real thing that many criminals use and understanding how they work is important. That being said, if you can't google "keylogger in python" and read the source code for the first thing that comes up, which is definitely a keylogger written in python, I can't help you.
First of all, thanks for your work on [isort](https://github.com/timothycrosley/isort). This is a really useful piece of software that I'm now using daily (as part of my commit hook). Regarding Connectable, and specifically Connectable vs. Blinker, I see at least two major differences: - Connectable (according to the README) is inherintance-based, while Blinker relies on annotations (decorators). I tend to favor the later. - In Connectable, signal names are strings, so I suppose there is a risk of making a typo on an identifier and causing bugs (I tend to do that a lot). In Blinker, signals are object, so one you have defined your signals, you can import them and get the benefits of IDE autocompletion and so on.
I promise you if you wrote an interpreter in another language you'd learn a lot about both of them. If you took the time to learn how this works you'd learn a lot as well. That might not be advantageous for you, it certainly isn't really for me, but it might be for a lot of people. 
Thanks for the info!
Right click, Mark as, Excluded folder https://www.jetbrains.com/phpstorm/help/configuring-folders-within-a-content-root.html
40 upvotes for this rubbish? Seriously? WTF is happening to /r/python?
Thanks!
The reason this was written, I'm guessing, is to be used in concert with [berp](https://github.com/bjpop/berp), a Python 3 *interpreter* written in haskell. I'd say that would have been the more understandable project to make a reddit thread about. I know Bernie Pope (author), he's probably forgotten more about functional language implementation than most of us will learn :)
Rubbish for you might be enlightening to someone else who is much earlier on their journey into python.
Nice and short write-up. My only addition would be that the explicitness of a variable name should depend on its scope. Eg. if the variable is only used within a short loop, a long name isn't always necessary, and can make the code harder to read.
You're spending time trying to solve an imaginary problem that cannot be confirmed to exist because the project in which it may or may not occur in doesn't exist yet by adding massive additional complexity that will make your project more difficult to maintain, restricts your possiblities going further and is definitely going to introduce non-imaginary problems. There are many ways in which this can be objectively described, "not ideal" - unless mentioned passive aggressively - isn't one of them.
Actually, this isn't garbage at all. I've been developing in python for years and,to be honest, you only come across these pieces of knowledge after you have seen a lot of open sourced python projects. However, there are some things I don't really do . The space between every operator, that can be a very subjective reason and not necessarily something everybody finds easy to read. For example, I always do one line per dictionary key in any dictionary I define, which is another subjective option. The other important one, don't be a fucking pep8 nazi. Pep8 wasn't meant to be perfect and followed word by word, there are some cases like the line limit where it doesn't matter. In fact, sometimes cutting the line to 80 characters makes the whole code look ugly. Another thing OP, you might want to add the string formatting. I see a lot of noobs doing this when they start out in python. name = 'John' last_name = 'Smith' #BAD full_name = name+' '+last_name #BETTER full_name = '{} {}'.format(name,last_name) #BEST full_name = '{first} {last}'.format(first=name,last=last_name) However, best is not always the most optimum solution. Sometimes, you really don't need to do a keyword-based formatting, and a simple opening and closing brace without a keyword is faster. 
Very briefly: * Install Python 2.7 from [here](https://www.continuum.io/downloads#_windows) * Save the script somewhere sensible, like C:\Users\Me\Downloads\razer_grabber.py * Open the 'IPython QtConsole' app that was installed. * Change the directory to where you saved the script by typing 'cd C:\Users\Me\Downloads' or whatever in the QtConsole * Run the script by typing `%run razer_grabber.py` (including that % sign at the start), type in the path to your SteamApps folder when it asks.
It really depends, though. If I'm building a simple string (2-3 variables with the string defined right there), sure. But if the string is defined separately from the usage or if it'll be translated, names in braces are a huge requirement.
Anywhere you can use it to solve a problem or have some fun. 
&gt; However, JavaScript frameworks are still preferred for web development (Node/Angular‚Ä¶). Is that a correct understanding? That generally depends on the problem you're solving (or your skill set, or being a JavaScript everywhere fanboy). At least as far as the back end goes, your front end will almost always need some JavaScript (though in my currently project I'm trying to have zero custom JS in my app to see if it can be done). &gt; Do you think Python will ever overtake JavaScript in web development? I'd say for the foreseeable future no, a big part of that is the ubiquitous reality of JavaScript and frankly Python isn't suited for the front end as JS currently is. For the back end probably not either - but again it solves problems differently than node does, generally speaking. JavaScript is also fairly young and has a lot of room to grow and optimize (upcoming web assembly will be interesting). I don't know where Python is going, but so far I'm liking where it's headed. For me it mixes a lot of the perks of dynamic languages (such as JS) and perks of static languages thanks to type hinting, as well as speed and fun of development. But new problems are always arising and different languages provide different applications to the problem (part of why I think there was a more urgent push for asyncio may be due to node), and frankly a big part of that growth in any language comes from those in the community implementing it first. A great think about Python is the community that works pretty well together to do impressive things, and have some friendly competition in route. That certainly exists elsewhere and isn't exclusively Python.
I do a lot of both js and python. I must say that I find python more enjoyable. I just seem to do more interesting things with it. Js wins for async and events though. python only just starting to get there as far as I can see
I'd rather see someone try and fail than not try at all. Best of luck with your version 2. If you do make something useful of it, the way forward is to start a new thread on the Python ideas mailing list. You'd get lots of feedback, some positive, some negative, but at least you'd then know for certain whether or not it was worth writing it up as a PEP.
I understand if other people want to code like that, but personally I don't find much reason to ever go over 80. For long strings, URLs in comments pointing to docs, or regexs, I'll let it go over. Otherwise, I open a parens, square or curly bracket. Otherwise, for something like a long set of if conditions or a larger list comprehension, I'll just split it across multiple lines. like: unique_hosts = { urlparse(x).netloc for x in urls if x.startswith('http') } Much easier to read that way rather than one line anyway. and for longer if conditions: if ( x is not None and x is in dooby_doo and x != 'scooby' ): I like to split my `and`s and `or`s across so I can see each explicit condition. Also makes a pipeline much easier to read if you're using something like pyspark. run = ( ... .filter(lambda x: x[0] in y) .map(lambda x: foo(x) + bar(x)) .map(...) ... )
Yeah. I'll also use shorter names when its explained by the code. Like a comprehension might be [p.name for p in principalities]. Harmless because you know what p is on the same line. But its dangerous when its not obvious: f = self.attributes[0] 
Should I ever use relative imports? I love breaking up code go many files. from . import bar 
I'm following most of those and I agree they do help making sure I don't waste time understanding how a piece of code is organised, but instead, I can focus on the meat itself. &gt; Two blank lines between classes and top-level function definitions, one blank line between class methods. I quite dislike this one though. I find it adds noise. I also believe it is a good thing to properly declare the `__all__` variable in each module.
I feel like microservices and real time async apps are going to grow. Also, IoT is having a lot of tractions. So techs like crossbar.io will probably be useful.
I think that's exactly the mentality that Raymond was trying to promote. A reasonable limit that sort of waves a flag if your lines start to get too long, but doesn't halt your train of thought by forcing you to refactor code because a line happened to go one character above an arbitrary limit.
I would recommend you to take a look at PyQt. It might seem kinda hard at the beginning but once you get your feet wet you'll appreciate it. Design the app with Designer/QtCreator and then convert it using PyQt module (pyuic). Design the UI by hand is a great exercise but a painful one.
Wasn't Django the big thing before node came around? Sounds weird to say JS backend is "still" big. I'd say JS is the most recent backend technology in use. 
Why not just use a second thread for the networking code? That's how GUI apps are usually made. If for some reason you really want a single-threaded app, I think Twisted has a reactor that's compatible with Qt's mainloop.
That's in PEP8
If you are asking about what version of Python is shipped in the upcoming Fedora 23 the answer is Python 3.4.3. When we port the code we prefer to keep compatibility with Python3.3+ and Python2.6+, unless upstream say otherwise.
Rails was... I never heard Django being big (except for us pythonistas)
 [This video] (https://www.youtube.com/watch?v=MCs5OvhV9S4) by David Beazley might provide some insight.
In my latest project i use python for the backend and js for the frontend. The only problem, which i am currently solving is using duplicate templates for the front and backend. Pages being rendered in python on the server and being rerendered dynamically in the browser gives some duplication. I could use js on the server too to avoid that, but it is really not that nice a language on the server, so i will probably make some way to automate the template duplication instead.
I read the API docs thoroughly. I'm asking on python since the API capabilities don't cut it. Perhaps someone has done screen scraping or something similar for things like this. 
I think it'd be easier to do web scraping than reading a screenshot. As long as you can get the URL of the picture, it'd be a cakewalk to do with Beautiful Soup.
 if 1: # change to 0 top else: bottom
there is several possibilities listed in http://stackoverflow.com/q/4591318/128629
Most of these are not really counterintuitive at all. For example I think it's much more sensible that bool(str) returns False on empty strings and True on non-empty strings. Returning False on "False", but True on any other strings would be weird and not very usable or at least prone to mistakes.
As a newcomer to Python quite a few were counterintuitive to me. Particularly the last 4.
Some aren't wats at all: &gt;&gt;&gt; int(2 * 3) 6 &gt;&gt;&gt; int(2 * '3') 33 &gt;&gt;&gt; int('2' * 3) 222 Either you don't know what a string time an integer do and it has no meaning, either you know it and the result is obvious &gt;&gt;&gt; a = [0, 0] &gt;&gt;&gt; (x, y) = a &gt;&gt;&gt; (x, y) == a False Same here: if you have any idea of what you do, the result are obvious. &gt;&gt;&gt; [1,2,3] == sorted([1,2,3]) True &gt;&gt;&gt; (1,2,3) == sorted((1,2,3)) False Again, if you know the return type of sorted there is nothing counterintuitive
A better explanation of #2 would just be 'all builtin types of sequence repeat themselves when multiplied by an integer. In case you don't know -- strings are a type of sequence.'
Now no more docker exec -it &lt;id&gt; bash
I program extensively in both Python and JS and calling ES6 Pythonic is an overstatement. But it may be for the best. The things that make Python really powerful as a dynamic language are also the things that make it really hard to optimize. Javascript weak metaprogramming is a blessing in disguise because it actually makes it highly optimizable.