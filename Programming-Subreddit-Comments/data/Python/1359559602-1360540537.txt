it's like using foreign words though. it only serves to annoy the natives.
No .condarc file in my homedirectory on windows, any idea?
You're right that it is not so simple, but there are benchmarks in the article, and C/C++ based libraries are used for solving real issues, so I don't share your irony. Let me tell you an anecdote. It is not a direct answer to your question, but anyway. I'm developing a pure-python morphology analyzer for Russian language. It uses https://github.com/kmike/DAWG package for data storage (which is a Cython wrapper of the awesome http://code.google.com/p/dawgdic/ C++ library). This works really great (DAWG lookups are almost as fast as builtin dict + 200x less memory required + prefix searches are fast), but it has 2 drawbacks: * the Cython wrapper is painfully slow under pypy; * it requires users to have a working build chain. In order to overcome both drawbacks, I ported the "reader" part of DAWG (which is used by end-users of the analyzer) to Python (https://github.com/kmike/DAWG-Python). DAWG-Python is capable of reading data produced by DAWG package, with the same interface and the same memory requirements. So the users of analyzer can install DAWG-Python instead of DAWG, and use the analyzer without a compiler this way. DAWG-Python is several hundred times slower than DAWG under CPython, but it turns out that DAWG-Python is only about 3-5x slower under PyPy than DAWG under CPython - great job, guys! The analyzer itself is pure-Python, and it does more than just fetching data from DAWG (even though DAWG lookups are in a hot spot). It turns out that under CPython using DAWG the analyzer has exactly the same total speed as under PyPy using DAWG-Python (both about 100k words/sec). CPython + DAWG approach has some advantages: * dawgdic library was already implemented, and there is a *lot* of ready-to-use heavily optimized C/C++ libraries; limiting ourselves to Python mean a rewrite of big (and sometimes obscure) chunks of code, most often with a visible speed decrease; * it is quite clear how to improve speed further if it would be not enough: just port some parts of (currently pure-Python) analyzer itself to Cython. I guess in case of PyPy the way to improve performance is to create a cffi wrapper for dawgdic (I haven't tried this though), but we return to C/C++ land again then. In my experience C/C++ based code is easier to get fast (I can't make DAWG-Python beat DAWG), and it make sense to use C/C++ based code for speedups. On the other hand, writing C/C++ code is not the only way. It is much harder to shoot youself in the foot using PyPy. For example, just wrapping the parse results to namedtuple significally decreases the performance in CPython version of analyzer; under PyPy this is not an issue; the code of analyzer would be better without worrying about CPython. So in my opinion there is no signle best solution to the problem. Does the knowledge of what is available hurt?
Yup - python even guarantees left to right evaluation order, so it actually works fine^[1] - good for trolling newbies with "Python 3 added brace support" [1] for values of "fine" that do not include flow control, assignment, or calling a function returning unhashable types of course.
Learn about: lists, list comprehensions, collections.namedtuple, what the global statement really does, and class vs. instance attributes. What version of Python are you using? 2.7?
So is this sort of like a re-implementation of pip and virtualenv?
What's wrong with the dmg installer SciPy provides? I've been using it with no problems for the last few years.
Yay! I'll private you my contact info. Congrats.
... or make your own URL shortener from scratch including bookmarking, ratings, and thumbnails: [video](http://pyvideo.org/video/714/web2py-ideas-we-stole-and-ideas-we-had), [code](https://github.com/mdipierro/web2py-appliances/tree/master/UrlShortening).
What numpy version does it require? When I first did this the numpy in macports had a bug that broke scipy interpolation, so I needed a different one. The scipy superpack had the bug if that's the same package you mean. Numba also requires a much more recent numpy.
Also, why the fuck is reddit asking for money in a spammy way now? I'm not at wikipedia ...
both [SciPy](http://downloads.sourceforge.net/project/scipy/scipy/0.11.0/scipy-0.11.0-py2.6-python.org-macosx10.3.dmg) and [NumPy](http://downloads.sourceforge.net/project/numpy/NumPy/1.6.2/numpy-1.6.2-py2.6-python.org-macosx10.3.dmg) provide simple dmg installers for OS X. You need to install NumPy first, but you shouldn't have any problems just installing the latest versions of each.
Mountain Lion is 10.8, do the 10.3 installers they publish actually work?
I was just being snappy.
Agreed. Stop it Reddit. Now. Or I'm off.
They aren't. It's clearly a bug that is making the "give gold" window appear at the bottom of listings, with no user filled in for the format string. Occam's razor, bro.
Ah, I see. You use it in place of the `pyvenv` that comes with 3.3. As in: # Make sure Python 3.3 is installed. # Download the pyvenvex.py script and put it into your ~/bin. cd ~/dev mkdir my-proj cd my-proj python3 ~/bin/pyvenvex.py my-env . my-env/bin/activate pip install WhateverYouNeed # work work work deactivate Looks very useful! 
I disagree with catcradle5. You are learning. You have the right to make mistakes. You don't know what you don't know. So, don't trow away your code. Publish it and ask for (non brutal) criticisms. From this exercise, what you should take home is that every time you feel the need to repeat a piece of code with little variations three or more time, you should consider using a list or a for loop. Maybe you don't know yet how to use those concepts. But after you study them, try rewriting this piece of code again. Good luck, and welcome!
That is a lot more clever than what I've been doing: https://gist.github.com/4591655. Are there any benefits to implementing this as a venv.EnvBuilder other than it's implemented in Python?
I wonder who I would owe more money to. I've used wikipedia a ton more for papers and stuff like that, but I spend hours on reddit. 
calm down guys it's just a bug http://www.reddit.com/r/bugs/comments/17kh3t/reddit_gold_comment_gift_prompt_appearing_at/
He wanted criticism, and he got it. I'm not seeing a problem. 
&gt; Are there any benefits to implementing this as a venv.EnvBuilder other than it's implemented in Python? Not especially, though you can use the full power of Python to e.g. provide more sophisticated command-line handling (e.g. help on the command line options). The script performs a useful function, but also demonstrates how you can extend `venv.EnvBuilder` to install other things into venvs as you create them. For example, you might extend it to add additional scripts in the `bin` folder, or supply a `-r` argument which takes a requirements file and invokes pip to install the requirements. Of course, you could do this with a shell script, too. The `venv.EnvBuilder` was meant to be subclassed as needed.
thanks.
2.7.3
Thanks, Vinay. [Updated my blog post from the other day](http://www.unexpected-vortices.com/blog/2013/how-to-spur-python3-adoption.html#update) with this new development. :) Please let me know if you give the script a more permanent home (or differently-named one, anyway). 
that's just the minimum version of os x you can install it on
The script has also been added to the Python documentation for the `venv` package under the section heading "An example of extending `EnvBuilder`". It should appear in the online documentation soon.
To a degree, I guess. They explicitly mention being akin to virtualenvs, so it's not like they're unaware.
Thank you, very interesting. I didn't even know about venv.EnvBuilder. And I'm glad to see someone else discovered the Python 3.3 combo of venv+distribute+pip. I think it's really cool that we no longer need to install anything into the system Python in 3.3; just create a venv and install everything from there.
While I'm sure it's valuable as an example of how to extend EnvBuilder, I think it's even *more* valuable to folks who just want to have a working standard way of installing packages into their Python 3 virtualenvs. Would be excellent to have a note right near the top of both http://www.virtualenv.org/en/latest/ and http://www.pip-installer.org/en/latest/installing.html giving instructions for users of Python 3.3 how to use your script. 
There's a separate 10.6 installer so I'm not too confident in the 10.3 one. There are also separate build instructions for 10.7 vs 10.6, and noe for 10.8. At a minimum they should document this better. Are you running the 10.3 dmg on Mountain Lion? http://sourceforge.net/projects/scipy/files/scipy/0.11.0/ http://www.scipy.org/Installing_SciPy/Mac_OS_X
They are mostly interchangable, but also slightly different. One is not inherently better than the other, despite the implication that the linked answer makes. Anyone is welcome to exclusively use whichever one they prefer, but there's no real grounds for saying that someone else should use one or the other.
Yes, the printf way. Everyone always comes back to the printf way. There was an article on that a while ago, on the evolution of programming languages or something but I can't find it.
yeah, I kind of like the var() method better, because i can just put the variables into my strings without messing with anything outside of the string for formatting. 
Let's hope that the `pip` and `virtualenv` maintainers read your comment :-) I will also post about it to `comp.lang.python`.
This guy thinks he can get off Reddit. Ha-ha!
I have seen it used for inter-process communication. A program runs, maps a file and starts writing counters at given offsets. Then multiple programs map the same file and read the counters and display graphs or send messages etc. [Wikipedia](https://en.wikipedia.org/wiki/Memory-mapped_file) gives a good technical description. 
I've read through your comment history, and you don't seem like a troll, so what exactly is your problem? I'm not generally one to make note of people's karma scores, but you've been here at least ~2 years and your combined score is under 25. That essentially means that not only are you using a free service, but you've literally contributed the absolute bear minimum to it. I mean, this isn't facebook. It's not a pervasive part of society forcing you to participate lest you be forgotten when Becky sends out her xmas party invites. So where exactly do you come up with the stance that you're in any position to dictate anything? This is a genuine question as I find the general level of self entitlement among people on the internet (myself included) fascinating. 
&gt; One is not inherently better than the other the rationale for [PEP 3101](http://www.python.org/dev/peps/pep-3101/) says differently: &gt; The '%' operator is primarily limited by the fact that it is a binary operator, and therefore can take at most two arguments. One of those arguments is already dedicated to the format string, leaving all other variables to be squeezed into the remaining argument. The current practice is to use either a dictionary or a tuple as the second argument, but as many people have commented [3], this lacks flexibility. The "all or nothing" approach (meaning that one must choose between only positional arguments, or only named arguments) is felt to be overly constraining. futhermore, the reasons given in the linked stackoverflow question are also valid, and don’t deserve to be handwaved away by you: 1. you can reuse arguments without using noisy dict syntax: '{0}, {0}'.format('test') is much cleaner than the following with its superfluous arbitrary dict key and more noisy characters: '%(_)s, %(_)s' % {'_': 'test'} 2. it doesn’t bear the risk for TypeErrors: "hi there %s" % name dies if `name` is e.g. `('Guido', 'NeedsNoSurname')`, and the safe variant with one-element-tuples is noisy and ugly again. 3. `map('blah %s'.__mod__, some_iterable)` is less clear in its intention than `map('blah {}'.format, some_iterable)`.
Hey, we're [open source](https://github.com/reddit/reddit) by the way! :)
The annoying thing about Trie implementations for use in Python is that, generally, people *do* want to be able to use arbitrary iterable types as the keys. Now, that pretty much limits you to using C++ for the generics. However, being able to wrap *arbitrary* iterables and then push them into the C++ datastructure becomes quite the annoyance. If anyone's got any ideas for me, that'd be great. However, I'm at a bit of a loss how to implement a generic key based prefix trie in C++ for use within Python. [Here is my implementation in C++](https://github.com/AeroNotix/algostructure/blob/master/CPython/prefixtrie/prefixtrie.cpp). So far it only stores Python strings as `char*` and associates no key with them, though it would be trivial to update it to store a value along with a key (`std::pair&lt;char*, void*&gt;` anyone?). Obviously my implementation is a bit of a toy but I'm looking to improve upon it, for sure.
Look at the grey text next to self posts.
 a="Some text" b="more text" "{a} .... {b}".format_map(vars()) 
I never can remember which format codes (or whatever they're called) to use.
I believe this also works. Haven't checked, though. a="Some text" b="more text" "{a} .... {b}".format(**vars())
we can do that?? That's awesome!
Criticism is only useful if you can act upon it. "It's bad, throw it away" is useless. "These are the things you can get better...", good.
but it's pylons. thats so out of date :-P
I'm not sure the PEP that proposes .format is going to offer an unbiased review of the merits of the two methods. Now that I'm done handwaving that away lets address the other handwavings... Ok, in seriousness, I honestly didn't mean to handwave anything, I'm just trying to say it's a preference. There are advantages to both methods. As someone noted in another answer, format is slower, for one thing. Personally, I don't find dicts noisy or ugly at all. If you choose to use punctuation like "_" as your arbitrary dict key, then it's little surprise that there's too many noise characters in the result. I'm sure you can be a little more pythonic than that. As for dict literals, I use them all over the place, not just for string formatting. Same with one element tuples. Should I feel bad and stop using them, because apparently you and some other people find them ugly? If the consensus is that they are ugly, shouldn't we try to improve how they look in general for all purposes, rather than addressing one specific use-case by making it so you don't have to use a dict literal for that purpose anymore? That seems like a textbook example of a band-aid fix. If dicts are ugly, fix dicts, don't fix string formatting. .format is great. I'm not saying it's bad, and I'm not saying you shouldn't use it. I'm just defending % formatting, because I don't think the appearance of something in many ways superior makes the original automatically obsolete. In my view, for it to be truly obsolete there must be no reason for it to ever be used and while it's easy to *say* "Well, there isn't!" in reality that's a very high bar to pass. Some people still find reasons to use DOS.
Is this available in python 2.x, or only 3.x?
&gt;You're going to need to rewrite it from scratch. That's an actionable item. He may not like that suggestion, but it's a valid suggestion none the less.
&gt; If you choose to use punctuation like "_" as your arbitrary dict key, then it's little surprise that there's too many noise characters in the result. I'm sure you can be a little more pythonic than that. i said it was an arbitrary choice and chose the same “junk” variable that we’re encouraged to use for e.g. unused spots in unpacking. i’m sorry if it came over biased, but what choice would you have preferred? an arbitrary letter? as for dict literals: sure, i also use them all over the place, but not as replacement for a function call. an operator can only take one right-side argument, that’s the whole reason why `__mod__` works on strings like it does. there is simply too much going on in a line with a format string, a dict literal, the modulo operator, and likely more to do something with the formatted string. and for “both are good”: well, the only reason i really prefer .format, is that `%()s` has that awful “s” outside of the punctuation characters, which really messes with my mind in some situations where there isn’t punctuation right after it. so i prefer .format. good thing that it’s also better in the named areas, bad thing it’s slower.
It's in 2.6 and up. Older versions, like 2.4 require just using %()s and vars()
I didn't mean that I preferred one way to any other, just that the assertion that there are two correct ways to do something *in Python* goes against one of the guiding principles of the language's design. 
If being obvious is the goal, the printf syntax wins for many, if not most, users.
I would love to see this article. I do appreciate the power of advanced string formatting and use it when appropriate. But, the printf form works just fine lion's share of my string formatting. It's quicker, easier and usually more readable (ie maintainable).
Oh, and the site's [made in Python](https://bitbucket.org/srmalone/malonecc), too.
Normally when you read data from a file, what happens is that the operating system reads the data into the filesystem cache (which belongs to the operating system, not your process), and then copies it again out of the cache and into the buffer in your process where you requested the data to be put. You can save that extra copy by instead asking the operating system to make that portion of its filesystem cache available in your process' address space. The file appears in memory as if it was read there, but there was no explicit call to `read()` to put it there. And that memory is not part of your regular program memory, i.e. it is not under the care of your language's memory allocator. Each memory extent in your process is a mapping -- the executables and libraries are mapped into your address space, and working memory like the stack and heap are anonymous mappings. In addition to saving a copy, mmaping a file has the advantage of being lazy -- the entire file (or a selected segment, specified by (offset, length)) appears in memory, but only those pages that are accessed need to actually be read or written from disk. Writing can be a little bit dangerous, in that just like the explicit read() step was eliminated, so is write() -- assuming you mapped the file for read/write access. Modifying the buffer in memory is the same as modifying the corresponding bytes on disk, although that will generally happen after some delay that is determined by the operating system's IO scheduler, but that effect is not user-visible, i.e. the change is committed as soon as the write happens, including to other processes that might try to read the file, even if it hasn't yet been physically written out to disk yet. 
The "hi there %s" % name name example isn't very strong, IMO. Most of the time a tuple appears in this situation it's probably just a bug. If a tuple of names is expected, it should be handled. It's no different than if someone did: name_len = len(name) and got the number of elements in the tuple instead of the length of the actual name(s) in the tuple. If you're going to support different types in one "variable", you have to do some work in your code to make sure it works correctly. The language can't do it all. The solution the stackoverflow article gives is indeed ugly as described: "hi there %s" % (name,) A much less ugly, and more intuitive way is: "hi there %s" % str(name) edit: formatting
I control nothing, of course, neither can I dictate the way the service chooses to operate. I do however have an opinion on this, which I have stated. It appears that 'the offering' has gone away now...for me, at least. If you still have it, and you enjoy it, that's fine.
Right. And printf should be that way. Python has always chosen the clear, practical, no-nonsense approach. Furthermore, it has always made no secret about its evolution from the c-style languages, where printf plays a universal role in string formatting. Why learn one arcane syntax for string formatting specific to Python when you can learn a tried and tested arcane string formatting syntax that is universally applied? 
Well isn't that a bit over-commenting? It's says already in the header of the if-statement what the break condition is. If anything it should say "x is the momentum of the particle, once it reaches 7 kg*m/s the 34"%¤#2#2# is executed.
Crushed it. Thanks so much.
Keep in mind Aaron Swartz faced 30 years in jail for scraping content against the rules of the site's user agreement.
"{kwarg}".format(**dict)
Are there are any plans to move to Pyramid?
&gt;i said it was an arbitrary choice and chose the same “junk” variable that we’re encouraged to use for e.g. unused spots in unpacking. i’m sorry if it came over biased, but what choice would you have preferred? an arbitrary letter? In the interests of code clarity and readability, I prefer to use actual descriptive names even in essentially "throwaway" situations. That is rather hard to do with a contrived example, I'll grant you that. `"%(test)s %(test)s" % {'test': 'test'}` is a little silly. &gt;as for dict literals: sure, i also use them all over the place, but not as replacement for a function call. an operator can only take one right-side argument, that’s the whole reason why __mod__ works on strings like it does. there is simply too much going on in a line with a format string, a dict literal, the modulo operator, and likely more to do something with the formatted string. Well there's the rub, you consider formatting to be a function call. And while I completely respect that point of view, I don't share it. I consider it an operation, so in my view, an operator makes perfect idiomatic sense. Too much going on in one line? Maybe. There's also the alternative multiline form of: s = "%(foo)s %(bar)s" s %= {'foo': 'yes', 'bar': 'maybe'} Which I actually prefer in many cases. It tidies things up a little bit. List comprehensions also suffer many of the same problems with too much going on in one line. In many cases they ought to be expanded to a proper for loop. Anyway I understand where you're coming from here. In some ways it does go against Python's philosophy of trying to encourage clean readable code. &gt;and for “both are good”: well, the only reason i really prefer .format, is that %()s has that awful “s” outside of the punctuation characters, which really messes with my mind in some situations where there isn’t punctuation right after it. so i prefer .format. Heh, completely agreed. That is definitely one aspect of it that is totally fair to refer to as "ugly". Sorry if I'm coming across as argumentative by the way. I actually just find this really interesting.
https://www.dropbox.com/s/yogmtrpgd10ywec/particle.zip
So cool
thanks
What company owns reddit?
Not entirely correct though. The gist of it is right. But mmap doesn't really tie into the filesystem, or the buffer cache etc. Instead, it works with the virtual memory (VM) system. The VM system already backs the memory seen by a process with disk (swap space). Memory pages can be swapped to disk, and demand paged back in. It is a way to create an illusion of very large memory, by using disk as a backing store. With mmap, the underlying objects that serve as the backing store, are files.
The answer you linked is quite bad. 1. This behavior of list comprehensions was most likely unintended, and it was completely removed in Python 3 and up. 2. Relying on a fringe feature like that is not good programming style. 3. It is much more difficult to read and understand the code. I can understand if the author was just trying to show an absurd and bizarre feature that could be abused to make ridiculous code, though.
Don't be such a fearmonger. While Aaron's charge was utterly absurd, he was trying to scrape something that was not available to the general public for free. The JSTOR service generally required a person or an organization to pay a subscription. You don't need a subscription to scrape a few pages off the front page of some site, so it is highly unlikely they would notice, let alone care.
I'm just stating the fact patterns of the case. Suing people for violating the TOS will probably become more frequent. To me it seems like a money maker. And you are the low hanging fruit.
Keep in mind it was the federal government that decided to press charges and go forward with prosecution. Both MIT and JSTOR did not want to sue or press charges against him. So obviously I would not recommend even doing a basic scrape of a website related in any way to any government, but something simple like grabbing some pages off Gap.com or the like has a very, very small chance of stirring any trouble.
I tried installing reddit: The following packages have unmet dependencies: python-pylons : Depends: python-webhelpers (&gt;= 0.6.4) but 0.6.4~precise is to be installed c'mon... Really?
Vouching for Chicago market. I get calls and emails every week looking to steal me away for my Python experience
Does freshen handle testing scripts that use raw_input()?
Where are you at? Can *we* steal you away? :)
gave you my upvote so can I say that I pictured you pushing your glasses up before typing this. also I replaced "Not entirely correct though." with "Actually,"
I would love to make my own but our semester project is contributing to other peoples projects, so sadly I can't just start my own. I just threw some ideas out there of what might be a fun project in case someone had some ideas. I am still hunting github but I figured some redditors might be looking for a contributor to their project, so it would be a win for everyone involved.
I was under the impression they are now separate from CN.
this is interesting. I'm a teacher and I have pondered the idea of contributing to an open-source project as an assignment but this does pose a problem. I know that it's hard to contribute to a project you don't use. Are you running linux? I started a project called [caw](http://github.com/milkypostman/caw/) years ago as a taskbar for Linux that hasn't gotten much love cause I now use a Mac and have other obligations. to be a smartass: reddit is on github, anything that pisses you off about it that you'd want to change? 
I tried to convey a general idea of how Python might be translated to native code by following my own project's compilation pipeline. I think the post ended up being a bit cluttered and wordy, and after a while I got tired of writing it, so some parts are probably rough. Any ideas for what I can improve?
Haha, yeah I am a linux/windows user. I will definitely check out the project. I find that the hardest thing is just the finding of an interesting opensource project. It is hard to just browse github. The searches by different terms based on the ideas I had above didn't reveal anything in python. Granted I can use any language I want, I was really excited to finally put my Python skills to use. I have found that github is full of projects based on Django, Flask, Hadoop, various wrappers for APIs, OAuth, and sublime text plugins, I don't see any full applications. As much as I appreciate toolkits/apis/frameworks/libraries I am looking more for a fun product that is easily demonstrated, as part of the grade is a demonstration on what we have implemented. I am wondering if this is a python issue - I know that bitbucket is popular with python programmers as I believe hg is written in python, so maybe they aren't posting to github as much. ( I think I will start poking around over there as well). Anyway, it is definately a challenge and I may just be out of luck with python apart from looking at reddit and your project. It is too bad because I would love to help out my fellow python programmers as I go. I think that the usefulness of the class varies. The main point of the class seems to be to get students to learn how to contribute to code they haven't written and therefore how to work as a team by default. I have been employed as a programmer for the last year so I am fairly used to that aspect of the world as I inherited some 15 years of code. I am taking the class as an opportunity to help out our awesome community. Out of curiosity, how would you implement this if you were teaching it? 
I tried to convey a general idea of how Python might be translated to native code by following my own project's compilation pipeline. I think the post ended up being a bit cluttered and wordy, and after a while I got tired of writing it, so some parts are probably rough. Any ideas for how I can improve it?
likely it would be just a piece of a larger larger, say one assignment that is due by the end of the semester. I have also toyed with having student reply to at least X number of stackoverflow questions with a similar intent; engagement with the community. I think it's hard with Python and it's hard for an assignment like this in general because I do really feel like open source applications are either Linux-based (i.e., open-source) or they're smaller "scaffolding" project; like homebrew, django or the likes. Basically, if someone has a good app idea for the web they're going to publish it themselves because maybe they can make money and so they keep it closed source. If someone has a good idea for a desktop app they're going to either be on Mac or Windows and want to sell the app—keeping the source closed. What about hacking on Python itself? I think if you want to find a project, you're better off finding a project and seeing what that project needs, as opposed to looking at *all* projects and **then** figuring out what you want to do.
May I ask why?
Yeah, I totally agree with that. I had some themes that were interesting to me but I didn't find anything. I was hoping that I would come across something that just sparked my imagination which unfortunately didn't happen. That is an interesting take on opensource, and honestly that makes so much sense. I think I might just look at working on Python, I do have some experience working with actual languages so that might be a lot of fun. Our class is a lot like how you described it, we basically keep track of what we did all semester then at the end present it to the class and do a demonstration and turn it in as a giant chunk of work. If nothing else this project has been eye opening as to just what the landscape of open source is really like, at least in the Python world. Edit: I reread this and I feel it made the class seem a bit unimportant. It is actually really cool to get to find projects and just work on them instead of having to do assignments for once. It is a nice break especially since I can switch projects at any point in the semester. The teacher is the best at our school so I definitely didn't want it to come off as me snubbing the class.
Maybe it is too simple - but I'm not a fairly confident python programmer who is just working on an old application to get it up to date and am open to anyone who is interested in pitching in. https://github.com/bittercode/pyrrhic-ree
not wrong - just different.
If you opened some issues or gave some idea on what you needed done I would love to see if I can be of any help.
If I can get the time I will. I was able to pound a lot of it out during some down time but now life is back to full speed. Basically I just want it to match Kodos for the most part. Then at some point I need to figure out packaging it up.
It's actually much better than most of the articles around =)
I just looked up kodos, that is pretty cool. What is your goal, just learning about regex or do you have some plans to make it better/different than kodos? I could see myself using tools like this. edit: The readme states you want to update the libraries, are you using python3?
The tasteful thickness of it.
Me too. One might think I should have better things to do during exams than discussing syntax, but here we are.
While I agree that this example I'd weak, your solution is, too. I use formatting especially to put stuff of any type into a string. If I'm ok with calling str all the time for this purpose, I'd just do `'prefix' + str(stuff) + 'suffix'`
Source available? I'd like to look.
why can't we pay with [bitcoins](http://reddit.com/r/Bitcoin)?
&gt; Made this with Python What libraries? Why did you make it? Why not like one-page web apps?
&gt; Most of the time a tuple appears in this situation it's probably just a bug. I don't really agree. There are a lot of situations where the intended effect is going to be "show whatever the caller sends me". Eg suppose you write a decorator to trace function calls: def logCalls(func): def wrapper(*args, **kwargs): log("Function %s called with arguments %r, kwargs=%r" % (func.__name__, args, kwargs) result = func(*args, **kwargs) log("Return value was %r" % result) return result return wrapper This'll work perfectly fine until you happen to add it to a function that happens to return a tuple in which case this behaviour results in a bug where your logging code breaks your program. Cases like these aren't that uncommon when debugging and logging - often you don't really *care* what the type is in this particular method, you just want to show a representation of it in your logs. Tuples may be perfectly legitimate sometimes and not others. There's also another flaw not yet mentioned: the older positional syntax can cause more localisation problems. If you have two or more positional format strings in a statement, you're left with a problem when the inserts would naturally go in the reverse order in a different language. Eg printing "Joe ate the pie" using "%s %s the %s" % (person, verb, item) should read more like "the pie was eaten by Joe" in language 2. With the newer syntax, you can just go "The {2} was {1} by {0}" even if the original format string used the "{}" version, but the older syntax requires code changes to fix, and they'll require switching to the more verbose dict syntax everywhere to allow for this. &gt;A much less ugly, and more intuitive way is: It's slightly less ugly, but it's even more verbose, and it actually seems *less* intuitive to me. After all, the whole point of string interpolation is to embed types *without* having to coerce to the appropriate type. It's also more effort to change, and creates a very arbitrary seeming inconsistency. Previously if you want to add a new parameter, it's just a matter of adding ", value" to the tuple. Now you have to delete the str and replace it with the previous way. It seems very *un*intuitive that you would use a different method for embedding one value than you do for more.
bPython, crunchyfrog, devede, gajim, gaupol, monsterz, pioneers, pitivi, rdiff-backup, rubber, screenkey, solfege, zim... You name it...
I'm using python3 and pyqt4 - Kodos is written with Python2 and pyqt3. I tried to contact the author to talk about contributing to Kodos but I was unable to get in touch with him. So I thought it would be just something fun to try and tackle since I'd have a guide to emulate. I'm very much a hobbyist when it comes to Python and programming. I've tried to stick pretty closely to what he did, though I did make a couple small ui changes. Nothing is set in stone and some of my code is probably messy. There are parts I don't even completely understand. I just lifted them from Kodos.
Define the values as floats/doubles like so: &gt;&gt;&gt;7.0 // 3 2 &gt;&gt;&gt;7.0 / 3 2.whatever digits you get
I'd guess because it introduces potential security holes if anything in that string could have come from user input (though this is generally applicable to all vars() approaches, not just the new syntax or just `**vars()`). This isn't an issue a version exactly as written, where a literal string is used directly. However, if someone happens to edit it to a more complex scenario, where multiple additions are made to a string. Eg. suppose you've got a function like this: def logActivity(username, time, event, article, ...): format_string = "Event occurred at time {time}: " if event == Logon: format_string += "Login" elif event == Logoff: format_string += "Logoff" elif event = MakePost: format_string += "Post made to article {article}" # ... various events with different inserts log(format_string.format_map(vars())) Some newbie programmer comes along and realises that we should log username for every event. Rather than enter it as a format string, he happens to change the first line to: format_string = username + ": Event occurred at time {time}: " A bit inconsistent, but other than that it *seems* innocent. Except that combined with that vars() line it opens up a serious security hole. Anyone who can read logs and create users can get access to every variable in scope at this point. Eg. someone creates the username "Bobby {super_secret_private_key}". This is a bit artificial, but there are always cases where someone can make some seemingly innocent change that combines with a dozen other individually harmless changes to make a hole like the above. It's generally better to follow a strategy of defence in depth, and minimal privileges - your should, as much as possible, not rely on no-one else ever doing something stupid but instead save as much as possible even *if* others (or you) do something stupid. As such, since the format machinery doesn't *need* access to all variables, you shouldn't provide them. (One other issue sibsibsib might be referring if they mean specifically the `**vars()` code is performance - this has to construct a dictionary which could potentially contain a decent number of entries. If this is called frequently, that could have a noticable effect on performance.)
// means rounded division. / is simple division and when the arguments are all integers it defaults to rounded division. so... you have 2 options: 1. change the way default division works like this: from __future__ import division 2. cast one of the arguments to a float: float(7)/3 or use a float argument: 7./3 
I use it all the time. For true unittesting I use it and just mock anything external to the core test. 
You are exactly correct. In the end it is truly a matter of taste. I make use of the vanilla assert constantly. Wasn't trying to pick nits with anyone on this; I just wanted a cogent explanation of why py.test &gt; nose. At the end of the day, I'm honestly happy to hear that there are so many people who are actually trying to do good testing.
I completely agree, and I greatly appreciate your input and the submission of the link. I had not seen the pythontesting.net site before, and have now found yet another good resource for my day-to-day work.
I'm actually very interested in the project, mainly because I've always liked regex testers and it would be perfect to have one specifically for python. I am going to forked it to take a closer look to see if it is something I can use for the class.
Thanks you, I am going to take a look at each one of these. I really appreciate the list, the more projects I can find the better for the class, but really I love seeing what python is doing in the real world! 
&gt; Collaborative editing (such as google docs for code) This is one of the goals of the IPython notebook, although there's still some way to go to make it happen. Interested contributors are always welcome!
We had someone turn up recently at IPython, with an assignment to get two patches accepted by open source projects. I love the idea, but it's a tricky way to approach things. How do you find something interesting and approachable? I think most open source contributions are from users 'scratching their own itch'. Maybe the students should first use open source software exclusively for a week, and see what they want to fix. But that might lead them to something extremely difficult, or to UI changes that the projects aren't interested in.
since kingcarter already gave you the link feel free to mess with the code if you want :) especially the "particles" var needs some values bigger than 240. Its a great benchmark to see how many particles your cpu can handle ;)
Very nice! Your code could be a little more object oriented. Generally when you see lots of 'magic numbers' it is a good candidate for a class. A particle class with X, Y coords and speed would be worthwhile. Other than that, I have a suggestion about generating the particle speeds. It looks like you are picking a random value for the x speed and the y speed. This works, but it makes your particles go at different speeds depending on which direction they are heading. Try picking a random angle from 0 to 360 degree (or 0 to 2 Pi if you're working in radians), and calculate the x and y speed from that. You should find that your particles move in a more natural radial fashion. Just a suggestion... 
there's some good discussion here about the issue: http://stackoverflow.com/questions/1550479/python-is-using-vars-locals-a-good-practice Basically it has security and maintenance implications.
Yes Ninja IDE rocks, you could make a plugin or hack the core.
I am the author of [Gate One](https://github.com/liftoff/GateOne/) and am in the middle of adding a terminal collaboration/sharing (and anonymous screencast) feature. This would allow, say, developers to collaboratively work on code with vim. The back-end code for regular user-to-user sharing is complete but I still need to write the code that performs anonymous sharing (via a randomly generated URL) and the client-side code that lets users share their terminals and view shared terminals from other users (that's mostly JavaScript work though). I'd also like to add some audio--and potentially video--conferencing as well. This would require some cutting edge JavaScript and WebSocket (Python) programming on the server side of things. If you're interested just clone the code and start coding! Just let me know you're doing it so I don't duplicate any effort. We could even setup a call where I could walk you through Gate One's architecture (it isn't that complicated) and show you precisely *where* that Python code would need to go. In the process you'll probably learn lots about WebSocket programming, plugin architecture, and potentially lots of stuff about web security :)
I forgot to update this comment, I found that it's generally called the Jenkins Hash, and comes from [this article](http://www.burtleburtle.net/bob/hash/doobs.html).
Better use `'{key}'.format_map(my_dict)`
Awesome! Did not know about that.
Update: python 2.7: AttributeError: 'str' object has no attribute 'format_map' On the other hand, [this](http://www.reddit.com/r/Python/comments/17khxs/python_user_detected_p/c86hh0g) worked, and I'm embarassed it didn't occur to me to try the asterisk trick (whatever it's called... unpacking arguments?).
couldn't get format_map to work but this worked great, thanks!
To clarify a bit, single-slash division was changed in python 3 to do float division (python 2 does integer division if both operands look like integers). Importing __division__ from future enables python 3's behavior in python 2.7 for forward compatibility. 
sounds good. the radial fashion was something I tried to accomplish by adding random values, but there is a limit on the axes. It forms an X but it shouldn't. And yes, you're right. A class would make the code much better. :) I used this particle effect to learn python and some programming techniques step by step, there is a lot of finetuning to be made. thank you.
pretty awesome! could be nice for teachers / classrooms as a clicker / way to get the entire classes answers in order to get an idea of how many people got the right answer etc. one thing noticed: sometimes when i click on a question, and the options pannel expands, it causes the viewport to scroll in a funky way, and the whole question actually ends up outside (above) the viewport...its a bit disorienting. not sure exactly whats happening, and i cant seem to reliably reproduce it in any way...
In Python 2, division between integers is always integer division. Cast one of the numbers to a float to force real division. &gt;&gt;&gt;5 / 2 3 &gt;&gt;&gt;float(5) / 2 2.5 You can also force integer division on floats by using //: &gt;&gt;&gt;8 / 2.5 3.2 &gt;&gt;&gt;8 // 2.5 3 In Python 3, this behavior has been changed so that / is always real division and // is always integer division, regardless of whether the numbers involved are int or float. You can use: from __future__ import division at the beginning of your file to force this behavior in Python 2.6 and up.
Would a game be visual enough? I'm always looking for help on [pyShipCommand](https://bitbucket.org/djkool14/pyshipcommand/). It is an educational game meant to help people learn Python (but also written in python). Check out /r/pyshipcommand for more info.
I used: * [heroku](http://heroku.com) for deployment * [flask](http://flask.pocoo.org/) for the framework * [sqlalchemy](http://www.sqlalchemy.org/) with the declarative model stuff on top of Postgres. This after switching away from mongodb. I liked it but I didn't want to have to write the joins myself when smart people wrote PostgreSQL and can do that for me. I don't use the flask-alchemy because I don't think it's needed. * [CoffeeScript](http://coffeescript.org/) for all things on the frontend * [Backbone.js](http://backbonejs.org/) because I wanted to learn some web frontend framework and because I wanted some structure but didn't want to use a declarative framework like [AngularJS](http://angularjs.org/); I did create an early version in Angular but I felt too abstracted from what was *actually* going on. * [flask-assets](http://elsdoerfer.name/docs/flask-assets/) compiles all my CoffeeScript down to JavaScript on the development server and I just commit the compiled files to my repo and push to heroku. I should put these assets on S3 at some point. * [flask-wtf](http://packages.python.org/Flask-WTF/) for forms only so that I can get CSRF handled for me. * [alembic](https://alembic.readthedocs.org/en/latest/front.html) for migrations and it's pretty nice. In general I try to avoid flask extensions when possible because most of the time they're not needed. This is the first time I've used Flask—normally I use django—and I am loving it way more.
Yeah, I have thought about this a lot but I don't know if I am wanting to open-source it. I've thought about posting a template for the app to give a sense of how all the pieces fit together. Any thoughts on compelling reasons for me to open-source if I instead were to publish the scaffolding?
Sorry. I worded my question poorly. I don't mean the test prompts for input. Rather, I mean the test exercises a part of the code that prompts for input. For example, given a script that, say, has two prompts in sequence, I want my scenario to answer both prompts and confirm some subsequent result. Then another scenario to answer both prompts another way, and confirm that result. If both prompts are y/n, then, for example, answer y to both, then y &amp; n, then n &amp; n, and so on. Can freshen handle that? (I'm doing it with Cucmber Aruba for now, but want to switch to something in Python.)
Well, I'm definitely feeling in this middle-ground. Parts of the app are one-page (like pagination is AJAX) but the add page is not. Also tag pages and things like that. It feels weird because I do use the backbone routing system to do initialization for each page and it seems like it would be easy to migrate one way or the other, but I tend to find a lot of problems with one page apps. like sometimes you hit the backbutton and nothing happens—I have this problem a *lot* with Github—and sometimes you get a full page reload when you go back anyways. Plus I don't feel like the feedback is every the same, I would rather see my browser navigation bar than a spinning wheel; I think I trust my browser more than an AJAX call even though they're the same thing. Plus the loading bar shows progress.
I made the site because I like sites like Quora and Twitter but I wanted a quick way to just get simple yes/no or multiple choice questions without comments and other crap. this site would be way cooler if more people used it though. I want more opinions!
Great description. But there's got to be a catch. What are the disadvantages or dangers?
Take a look at [pypes](http://www.pypes.org/). It's a Python web app inspired by Yahoo Pipes, which is a visual "programming" tool for manipulating web data. I'm not sure how active it is though. The page points to a github project that no longer exists. I (think) I found it on bitbucket though, [here](https://bitbucket.org/diji/pypes/wiki/Home).
Honestly, I don't know. I haven't tried it. But, if I were trying to test how my system handled differing inputs, I would just write a test to pass those inputs directly to the methods/functions in question. At the end of the day, if all you need is y|n, I would just pass y or n as an argument to the function you are trying to test. Otherwise, I would (just a wild guess) look at writing the input to sys.stdin. If you just wanted to use the command line input to a test, you might want to look at expect. That is how I have scripted command-line stuff for years.
In a lot of languages, division between two ints yields another int
It's more complicated. It requires using platform-specific APIs, and everything must be page-aligned. Even though untouched pages don't take up physical memory, they do take up virtual memory. On 32 bit systems, a process only has 4 GB of VM, and usually 1 or 2 GB of that is reserved for kernel space. And the program will have many existing mappings for the executable, libraries, heap, etc. which will further fragment memory. The size of the file that can be mapped is limited by the requirement to have a contiguous and unoccupied area of VM to map it into, and in a pathological case that might severely limit the size. This isn't so much a problem on a 64 bit system, but even on such systems it's common to have only 32 bit versions of apps available (e.g. on Windows, there's only 32 bit versions of Firefox and Visual Studio.) This can be mitigated by doing partial mappings, but usually when you implement a mmap scheme you are banking on all this trouble paying off with the fact that you can simply index into a buffer at any offset to access a file there, and using segmenting/windowing means you have to adjust the mapping if the region you want to access isn't current mapped. Also, there are plenty of times where you want to read the whole file but you don't necessarily want or need it all in memory at once, for example reading a line at a time and analyzing/converting it as you go. Moreover, when you use memory mapping you are at the mercy of the operating system to decide when to let go of pages that haven't been accessed in a while. In theory this should make your life easier -- just write your program to access what it needs when it needs it and let the OS worry about paging out the least recently used data under memory pressure. In reality the OS can't always read your program's mind, and things might not work the way you want; it might sacrifice memory from other processes to keep filling the demands of your mmap process for instance, even if you don't want your mmap'd process to be prioritized like that. There are platform-specific APIs on some systems that let you give the operating system advisory hints about regions you really want to keep in memory and regions you'd like to have paged out, so this can be dealt with to some degree, but you start to end up implementing all the work you would have done anyway if you'd just done the traditional thing with explicit read()s and normal buffer management. And sometimes you want to build up a data structure in memory before committing it to disk. You can still do that by building it in a normal buffer and then copying from there into the mapped file, but again that starts to look like the traditional, portable, simple method of calling write(), with all the added complications of mmap() not paying for themselves. 
Check out Core Python, it worked for me
I've been kind of intrigued by Tornado. What are the advantages/disadvantages of Tornado's event loop versus Gevent's greenlets? What cloud hosting providers actually provide full Tornado support?
How deeply does one need to understand metaclasses if one only want to use them solely for the feature of implementing abstract classes?
If you've got time to watch videos, Asheesh Laroia gave tutorials during a couple PyCons that were recorded and put online. You can find his talks [here](http://pyvideo.org/speaker/97/asheesh-laroia), specifically - [Web scraping: Reliably and efficiently pull data from pages that don't expect it](http://pyvideo.org/video/609/web-scraping-reliably-and-efficiently-pull-data) - [Scrape the Web: Strategies for programming websites that don't expect it](http://pyvideo.org/video/256/pycon-2010--scrape-the-web--strategies-for-progra) I'm pretty sure they're both variations of the same talk, but take a look at both. They are both good primers on the subject.
That presentation is a really great explanation of both Tornado and sockets. I also had no idea [the Python sockets docs](http://docs.python.org/2/howto/sockets.html) were so good.
The biggest difference I think is that Gevent has the ability to make your plain python synchronous code asynchronous with no changes required -- by patching the standard library and making some operations asynchronous in a transparent fashion. For some apps that's not really an advantage, but for instance if you have a web app that uses a pure python sql driver like pymysql, using gevent will boost the number of concurrent requests your app can handle, because every socket operation will be pushed to the eventloop. I had a couple of projects were gevent magically fixed some scaling issues. OTOH, patching the standard library can have some nasty side effects / bugs in some libraries &amp; apps that are not "green" (==compatible with the way gevent patches the code). So it has to be used with extra care. If you do some async work explicitly though, they are roughly equivalent I guess - I think the 2nd biggest difference in that case is the design: Gevent provides synchronization primitives, pools etc, for you to work with greenlets, but you never have to worry (at least I don't) with the event loop - which works under the hood. With Tornado it's the other way around: you work directly with the IOLoop instance and set up some callbacks etc.. I personally prefer Gevent's model because I find it easier to architecture my app with greenlets rather than having to build stuff around a central IOLoop I have no idea how they compare in term of speed, but I suspect the difference is probably ridiculous compared to the time spent in the actual app code. 
I find it hard to remember a sed command like: sed -n 'N;s/\n/ /p' which is easily replaced in pyxshell by: pairwise | join For example, I was recently adding a bunch of submodules in a git repository, from a `.gitmodules` file, which can be done in shell with: $(cat .gitmodules|grep "url\|path "|sed -n 'N;s/\n/ /p'|sed "s/^.*bundle\/\(.*\)\s*url = \(https.*\)/git submodule add \2 .vim\/bundle\/\1/") With pyxshell, you can do it with: cat(".gitmodules") | grep(["url","path "]) | pairwise | join | sed("^.*bundle/(.*)\n\s*url = (https.*)\n","git submodule add \\2 .vim/bundle/\\1") | sh I find this easier to remember. And note also that there is less escaped characters from hell.
You mean like builtin `help`? For usage the module howdoi is pretty neat.
Thanks, I'll check that out.
Hmm...at the same time as pycon?
**Pushing my glasses up** *your glasses.
What you want is pydoc; """ andy47@machine:~$ pydoc pydoc - the Python documentation tool pydoc &lt;name&gt; ... Show text documentation on something. &lt;name&gt; may be the name of a Python keyword, topic, function, module, or package, or a dotted reference to a class or function within a module or module in a package. If &lt;name&gt; contains a '/', it is used as the path to a Python source file to document. If name is 'keywords', 'topics', or 'modules', a listing of these things is displayed. pydoc -k &lt;keyword&gt; Search for a keyword in the synopsis lines of all available modules. pydoc -p &lt;port&gt; Start an HTTP server on the given port on the local machine. pydoc -g Pop up a graphical interface for finding and serving documentation. pydoc -w &lt;name&gt; ... Write out the HTML documentation for a module to a file in the current directory. If &lt;name&gt; contains a '/', it is treated as a filename; if it names a directory, documentation is written for all the contents. """
&gt;&gt; Most of the time a tuple appears in this situation it's probably just a bug. &gt; [snip] &gt; This'll work perfectly fine until you happen to add it to a function that happens to return a tuple in which case this behaviour results in a bug where your logging code breaks your program. Yes, it's a bug. Like I said above: &gt; If you're going to support different types in one "variable", you have to do some work in your code to make sure it works correctly. The language can't do it all. (and then gave an example) &gt; the older positional syntax can cause more localisation problems. That's not what this example is about. Anyway, like I said elsewhere in the thread: &gt; I do appreciate the power of advanced string formatting and use it when appropriate. But, the printf form works just fine lion's share of my string formatting. It's quicker, easier and usually more readable (ie maintainable). . &gt; After all, the whole point of string interpolation is to embed types without having to coerce to the appropriate type. You're way changes types, too - from a tuple to a tuple into a tuple of tuples. Maybe you find tuples of tuples more intuitive than a stringized tuple but not everyone will. 
So instead of downvoting, why not give some advice? Try installing it in a virtualenv.
It's actually far smoother than that, the software I used for recording wasn't very good! Source code and instructions: https://github.com/porost/planetary It's my first program that's not badly screwed up and with which I'm rather happy. edit: UPDATED with Repulsors and separate toggling of gravity for big objects and particles! (so you can now have a stationary collection of attractors and repulsors and throw particles at it) How did I not think of this earlier?! edit: UPDATED again with permanent particle spawners. Just left-click while throwing particles. Right-click to remove. I am having way too much fun with this!
Is Blaze what I think it is?
Admittedly, I clicked on the link expecting cool blinky lights and fiber optics.
You can also use: '{0[abc]} and {0[d]} are worth {1}'.format(somedict, 42) which is why _I_ like the format style: pull multiple values from a dict arg.
I love all the collaborative software that I'm seeing here. I will definitely take a look, I am a vim lover (although sublime text has stolen me for now) I will take a look at what you have going on. This seems like a whole collaboration suite which would be really useful for decentralized companies and teams. This is intriguing, definitely looking into this as well. Thank you for the response, and sorry mine is so delayed. I was sleeping then at school.
I saw this on the forever project subreddit and it seemed pretty interesting there. I have only had a chance to briefly look at it but just so I understand it right, is the interior script python or did you write your own special scrpting langague for the ships? This might be in the manual I just havn't had a chance to read everything. Thank you for the respons and I'm sorry mine was so slow I was sleeping then at school (I said this multiple places but I feel everyone deserves a thank you!)
Sadly when I went to their github https://github.com/diji/pypes it 404d, I am guessing it was moved or has died. Either way that is pretty cool thanks!
Luckily in our class our teacher will be sympathetic to people who can't get their request excepted. Obviously the goal is to actually contribute but it isn't really fair because sometimes you can just be unlucky. I like the idea of using open source software to get a grip on what is out there, the majority of people in our class are linux users so it would probably pretty easy to implement. This class is a fairly high level course so they have been programming for a while before getting in it so I am hoping that projects may be hard but not impossible for the majority. I am actually new to contributions to projects outside of my work so the problem I ran into was just finding something that caught my imagination. I love working on projects but usually it is just my own pet projects, I haven't (sadly) ever taken the time to try to find projects that are out there for me to help with. I am so glad to see all these projects and hopefully become an active member in our community. I really do believe that FOSS is an important ideal and am glad that our class is really pushing us to be a part of it.
Some people are just assholes.
I agree.
Holy shit! How did they not have backups? 
You are extrapolating from one data pont. By visiting reddit you are technically violating all kinds of laws and rules, with your only protection being a non-legally enforceable user agreement. Scraping is more often than not something that the IP holders want you to do. Stop when they tell you to stop. You'll be fine.
And why wasn't https enabled on login!?
What does this mean ? Is my pypi password safe ? 
i don't entirely understand what you're doing here. Mind giving me a working self-contained code snipped?
Thanks r1chard. I use different passwords everywhere. I was just wondering if I needed to change my PyPI password. 
argument 0, somedict, is a dict. the first placeholder grabs that dict's value for key 'abc'. the second one grabs its value for key 'd'. the next placeholder grabs argument 1 and since it's an int, it is coerced to a string.
why would you use string.Template?
While it makes sense to ask, OP does say that this was an attack on wiki through a vulnerability in moinmoin.
yep. this would not of been a problem if they had backups. I guess they have it now after having to manually rescue pages by google cache and archive.org
Nice little project - I like the particle emitters and the vector when you make big objects.
Indeed he does. I was just making certain that PyPI wasn't compromised as well. 
May I print this off for my own use?
If you had data already in that format. It's a fairly common syntax for string substitution. Doesn't offer anything over string.format that I know of.
All else aside, who the hell would do this?
That's the official python documentation (from docs.python.org), not affected by this attack.
Cunts
I'm more concerned, and a little embarrassed, by the fact that a public python.org server has been compromised for over 6 months...and we only found out because the guy decided to finally toss a grenade in for shits and giggles. Not to be a dick or state the obvious, but someone there needs to brush up on their network security practices.
&gt; The VM was rebooted on Jan 7, apparently in an attempt to get things working again. Who rebooted the VM? If it was the attackers, this implies that they had root. But the post only states that they could execute code as the moin user. 
Good one. Fixed. I can't believe I made that typo. 
True, but perhaps we underestimate the sort of resources required for every site we use to have comprehensive network security. I just had to disable my Redmine instance because my provider offers a one-click installer but doesn't offer a one-click upgrade, and with the latest Ruby vulnerabilities, old versions of Redmine aren't safe - but then maybe my Ruby installation isn't either. I have little way of knowing. But if I only ever used software that I fully understand the security implications for, I wouldn't be able to host anything. And I can't afford to pay someone else to know the implications either. I think we're in an age where it's no longer practical to expect every website we use to be secure.
I just wanted to add my name to the list of people who would appreciate it if this were converted into a class. I'd love to use something like this!
PyData partnered with PyCon. Listed under their Events section. PyData will be during Pycon's sprints not the main conference.
The use of the image of a hurricane was distracting
So can I get into PyData if I am going to PyCon?
Thanks! I haven't been involved in the python *scene*, but I've been thinking about starting. Python seems to have a really tight community, and I can see how there might be benefit in getting myself into that. If you're ever thinking about looking at other opportunities, my company is great, and we're working on really exciting stuff.
Can't up-vote you enough; no backups = no sense, not something I expected of the Python community ...
Do your own fucking homework? Here's a google lesson. You asked about integer division in python. I type "integer division python" into google and the third link was this: http://www.linuxtopia.org/online_books/programming_books/python_programming/python_ch05s06.html The fifth link was this: http://www.ferg.org/projects/python_gotchas.html Either would have answered your question. It is OK to ask questions, but you should put thought into it. The second problem you listed just says "I'm stumped on". You didn't say what you did or did not understand or ask any question. It looks like you were looking for someone to give you an answer. Your questions above show you don't take your homework very seriously and it offends me that you say you do. C'mon. By the way, you let a random stranger offend you. You are giving this stranger lots of control over your life. You won't do very well on the internet if you have such a thin skin. 
X
I would assume it is this: http://continuum.io/blog/blaze
Yeah but it gets tricky at high speeds. Things start tunneling through each other. This is easier to prevent for boxes than circles.
Unfortunately not. Separate registration fee.
Correct.
&gt; It is likely that the password information was downloaded from the server in the course of the security breach, so we recommend changing your passwords immediately, if you have used the same password for other services as well. Does anyone know how they stored the passwords? I assume it's some kind of default MoinMoin implementation? I don't have any time to look this up right now.
Cool - I am going to see if I can get work to pay for it...
That's really cool. I'm really interested to see how it works internally. It sounds like an amazing learning tool, is it playable by kids? 
*"it's rare to find applications that accept pickles from untrusted sources."* Except a whole bunch of web applications and frameworks, as we saw a few days ago. http://www.reddit.com/r/Python/comments/17c6to/exploiting_secure_cookies_in_popular_python_web/
Oh good, that's exactly what I thought it is.
It should be playable by anyone with a basic understanding of Python syntax. I think the best part is for players to be able to experiment with different logic in a game setting. I think for many aspiring programmers (and especially kids), it is hard for them to stay interested when all you are learning is how to process files and text. That being said, the game is still in an early phase and lacks a tutorial and many of the attractive graphics that would keep kids interested.
You need access to the **secret key** to attempt this attack. If your key is not secret, you probably already have big problems.
&gt; and you should be ashamed for spreading FUD without any reason Way to be a dick. I constantly hear people concerned that installing two versions of python will mess up their system. Most of the time (*not* always) that's not true but it **is** something that hampers adoption. I know they work side by side, and know how to fix any problems that come up, it sounds like you know this as well. Not knowing that, and assuming that python is like most other programs, is not the same as "spreading FUD".
i have never, ever heard of that and i know for a fact that, as i said, most linux distribution have both packaged both and allow both to be installed. if you install a bunch of software, chances are high that both get pulled as dependency. and i, as a python programmer, have never ever heard that there is even the potential for a conflict. so, please tell me one case how it could possibly go wrong (except people using the wrong shebang)
It would have been the better naming choice, but it's really not that big a deal: pyyaml teaches about both load functions at the same time, and warns not load if you want to safe_load. Nothing like that rails mess.
It is not a big mess like Rails, true. But if you have two functions, a safe one and a dangerous one, why would you name them load and safe_load?
Sorry about that happening, there was a package version conflict in the PPA. I've cleaned out the bad package and my testing shows that installs work now. If you run into any further issues please feel free to PM me or come into #reddit-dev on freenode.
Haskell has some unsafe functions such as unsafePerformIO and unsafeCoerce, indicating their lack of safety in the name.
&gt; PyYAML has a .load() method and a .safe_load() method. Why do serialization implementers do this? Because they used PHP before?
&gt; True, but perhaps we underestimate the sort of resources required for every site we use to have comprehensive network security. I just had to disable my Redmine instance because my provider offers a one-click installer but doesn't offer a one-click upgrade, and with the latest Ruby vulnerabilities, old versions of Redmine aren't safe - but then maybe my Ruby installation isn't either. I have little way of knowing. But if I only ever used software that I fully understand the security implications for, I wouldn't be able to host anything. And I can't afford to pay someone else to know the implications either. Yes, although the PSF can afford to pay someone to handle these things. &gt; I think we're in an age where it's no longer practical to expect every website we use to be secure. I don't think we ever expected every website we used to be secure, though.
Would something start that, suddenly? Because it worked before I install gspread...
Then you'd also need .real_safe_load() 
pretty good for cracking passwords, you mean? It was broken in 2005, the world has moved on. http://en.wikipedia.org/wiki/SHA-1
Yeah, it really bugs me that PyPI will let me sign and put checksums on the Django package when I upload it, but then *does nothing whatsoever* with that information, and neither do any of the tools. PaulM's security talk at PyCon (last year, I think it was?) showing the stuff badly-intentioned people can do when they know you're just relying on the package tools was kinda scary.
Yeah. Although curiously I might even prefer that it does absolutely zero with those checksums (not even simple validation) rather than a giving the impression it does any actual verification of the origin of the code.
&gt;If you lose your secret key to your app you are fucked regardless. Even if the app was not using pickle you would still find other ways to cause havoc (such as signing in as the administrator). wait..how? I've never done any of this in python so I'm not sure how its done, but my experience with sessions(mostly Perl+Catalyst) was that all you use cookies for is storing a single sessionId that ties to your locally saved data. The absolute worst thing they should be able to do is brute force your sessionids. Even that can be very easily protected against -- store some session specific stuff on the server to compare against, so that for example you only accept this sessionid from this ip address, or their useragent if you want to allow roaming(though much less secure). Are people just storing the entire session in the cookie including things like their logged in username and whatever else?
I guess I'm not familiar with frontend work at all but what are you using that makes it so I can see the template tags when I view source? Is that the Backbone talking?
.real_safe_load() is (un)officially deprecated; please use the PVO.serialization module.
That was magnificent! 
Try `idle -n`. &gt;look at the sources, idlelib/PyShell.py in your Python install: -n sets use_subprocess to False, so idle runs the user's code in the same process as the shell and GUI instead of using a subprocess. Eventually, I would suggest to re-install it.
Not just json, the pattern originally comes from pickle: dump, dumps, loads, load. 
&gt; As of 2012, the most efficient attack against SHA-1 is considered to be the one by Marc Stevens with an estimated cost of $2.77M to break a single hash value by renting CPU power from cloud servers tell me if that isn't still "pretty good"
Still, this should be, at the very least, easy to disable. Someone who finds out about my secret key (even if that should never happen) shouldn't be able to open a shell into my system.
He asked me a question I already answered, no need to prove further points &gt; It would have been the better naming choice
Wtf are you arguing about? I said: “It would have been the better naming choice", thus the only answer I could give you is “like I said" aka “i agreed with you from the beginning"
&gt; Are people just storing the entire session in the cookie including things like their logged in username and whatever else? Yes. Most modern web applications store the a signed payload in the cookie. This is done so that you don't need to consult a data store in order to figure out what to do with the cookie.
&gt; Why the fuck would you use the pickle module to read/maintain state? Because JSON does not support a) datetimes, b) markup objects, c) tuples, d) sets. These are all things that people want to have in their session. &gt; De-serialization's #1 rule has ALWAYS been don't de-serialize data from an un-known source (and in this case I'm making an addendum - a terribly guarded source). The source is well guarded unless you leak your secret key. Don't do that.
PyYAML is pretty easy to modify to the point where you want. I think it took me less than 10 lines of code to achieve that.
I knew a guy who wrote a game with bouncing zombies. I also knew a guy who wrote space invaders. The space invader had about ~100 aliens, and the VB IDE almost crashed from all that copy paste. 
Not only this, but the people that have read the docs ages ago and are going by memory, and the people who use their IDE's autocomplete.
Being able to spoof a session is a big problem, but nowhere on the same scale as being able to pretty much own the computer. Given that we've seen that people aren't terribly good at securing their so-called secret keys, it's still important that frameworks don't use pickle on cookie data.
Yes and no. I do it this way so that there isn't a separate call to the sever to retrieve the template. I could make each template its own page but I haven't fully convinced myself single page webapps are the way to go. But the template itself is processed by underscorejs. 
Ah, ok, sorry, I misunderstood.
Helps to read the code.. LOCALHOST = '127.0.0.1' def accept(self): working_sock, address = self.listening_sock.accept() if self.debugging: print("****** Connection request from ", address, file=sys.__stderr__) if address[0] == LOCALHOST: SocketIO.__init__(self, working_sock) else: print("** Invalid host: ", address, file=sys.__stderr__) raise OSError idle is expecting the connection to only come from localhost, yours is coming from 192.168.0.21. This has nothing to do with gspread, looks like you broke your networking configuration somehow. Did you edit `/etc/hostname`, `/etc/hosts`, or `/etc/network/interfaces` ? Did you shutdown the `lo` interface or something? 
thanks. :) stupid downvoting hivemind seems to be a bit slow though, like always.
I strongly disagree: many people will not read the docs, will discover it via IDE autocomplete or other introspection, or will simply read the docs once and fail to remember the distinction the next time they do it in a few months.
That helps, but what if your secret key leaks? For web-apps it's probably better to use a dumb serialization format like JSON.
Quite possibly it's improved, or I didn't try hard enough at the time :) Would love to see the code for future reference. Though my main argument was that this should be an upfront feature, not something you have to reach in and discover. Otherwise (IMHO) mapping class serial identifier -&gt; import path is just as bad as PHP's routing of url -&gt; local file path. 
Can I suggest you x-post this to /r/flask ?
But you can disable it via a setting in php.ini or via a compile time switch. Also, when it's disabled, it actually silently fails.
&gt; It may be very unlikely that the source can modify the data but it's not impossible. It's more than very unlikely as we'd be talking about digital signatures being cracked. Assuming you trust that your secret key as well as digital signatures are not compromised, data that is digitally signed by you *is* sourced by you - it is data that you've bounced outwards and back. This is the entire point of digital signatures - to establish trust. I would agree that it's better to put JSON in there rather than pickled data, but only to the extent that it mitigates the risk of the secret key being compromised. But it is not correct at all to consider equivalent the potential hazards of bouncing digitally signed data back to oneself, which is a simple, transparent and secure process if practiced thoughtfully, versus the Rails' vulnerability, which is basically a wide open hole foisted unconditionally on the entire userbase that was clearly the result of practices that did not take security or trust into account at all. There is no comparison between the two cases. 
&gt; It's more than very unlikely as we'd be talking about digital signatures being cracked. Or shared, or accidentally checked into Github. The fact is that recently it's been shown that lots of people's secret keys are ending up on the internet. http://www.eweek.com/developer/ruby-on-rails-security-flaw-severe-but-not-widespread-researcher/ They consider it a big security flaw; Python people here seem to think it doesn't matter. But mitigating risk is important. Defence in depth. You don't leave your computer passwords and key to your safe just inside your doorway merely because you know the door is locked.
Wow, that's kinda big. 
the question i replied to is &gt; why would you name them load and safe_load? and i replied with &gt; Like I said. the only thing i said about naming choice is &gt; [load_dangerous/load] would have been the better naming choice so the expanded version of “like i said” would have been “like i said, i wouldn’t have named them `load` and `safe_load`, but rather something like `load_dangerous` and `load`, as you proposed.” how is that not obvious?
I always thought this was an obvious but accepted limitation of the current pip/PyPI system. This why at my workplace we use pip only for developers' virtualenvs which we deemed to be n acceptable risk. For deployment we install everything from tarballs coming from our own servers which we do our best to verify.
The IP address of the host is 192.168.0.21, is there some reason why the client-side of Idle would start using the host's IP address instead of localhost?
yes, lots of reasons: Did you edit `/etc/hostname`, `/etc/hosts`, or `/etc/network/interfaces` ? Did you shutdown the `lo` interface?
BINGO!!! What does -n do? I don't see anything about it on the Idle docs. I did try re-installing idle, with no effect.
same discussion over at hacker news, similar arguments being made "well *nothing* is perfectly secure! everything has holes!" and responders pointing out the [fallacy of grey](http://lesswrong.com/lw/mm/the_fallacy_of_gray/) in response. A built in, on by default for everyone with no warning whatsoever remote code exploit is nothing at all like an optional library that requires properly protecting a key documented as critically secret.
This is worrying. I'm not clued about this, average user---sorry, so just asking for some clarification, but I'm not sure what you meant by: &gt;DO NOT install packages using pip (and possibly easy_install) in an untrusted environment! versus &gt;Never, ever download something over an unsecure connection and then execute it, especially not as root. ***What do you mean by "an untrusted environment" here?*** Are you saying that we should ensure that there's an https connection (which I think you say pip does not support), or that we should only ever install into a virtualenv? It's always felt odd to me to conduct most things with a paranoid viewpoint, but then happily allow python or R etc. to download and install from a mirror somewhere. Thanks!
Well this would have been useful to know a week ago.
It's a bit worse than that. Pip does sometimes use SSL to download packages that support it but they don't verify a certificate's authenticity. See here: https://bugs.launchpad.net/ubuntu/+source/python-pip/+bug/1015477 To be verbose, this means that you can MITM the connection with something like Burpsuite that replaces certificates in transit which allows you to do the types of attacks the OP writes, EVEN if the source files used SSL and pip wouldn't care. 
mitmproxy replaces SSL certificates as well... I've just successfully verified that pip happily accepts a fake certificate (you'd just have to add another iptables rule for port 443) For anyone wondering how to avoid making the same mistake: requests verifies SSL certificates http://docs.python-requests.org/en/latest/
Untrusted environment = a public network, for example
I showed the concept to a friend in the class and he was very interested. I have a few projects from here that I'm looking at but this one has definitely captured his interest and mine as well. The concept is really cool. I just forked it, I'm curious as to why you don't want public forks? Is this something I can use in the class or do you want the codebase to be private?
"but it's not really that big of a deal". This is the point you haven't proven. 
I think I do at least...if you mean "do I check the checksum/shaetc or use a manager like pip to do that for me"...then absolutely. But, given Xykr's [response](http://www.reddit.com/r/Python/comments/17rfh7/warning_dont_use_pip_in_an_untrusted_network_a/c88b80v) I now realise that the warning is not about using pip *per se* but about using pip on an untrusted network. That said, my curiousity (sorry if this is well known) makes me ask that even if I were on a trusted network and were checking signatures...whose to say the mirror I'm downloading from and checking sigs against is 'good'?
I'm replying mostly to: &gt; It's always felt odd to me to conduct most things with a paranoid viewpoint, but then happily allow python or R etc. to download and install from a mirror somewhere. Well when you install python through apt, the python package is signed by, for example, the ubuntu archive signing key. And apt verifies this. You can type "apt-key list" to see what keys you are trusting at the moment. You got these keys when you installed ubuntu. So assuming you got ubuntu in a safe way and verified your copy was authentic, you should be reasonably secure in trusting the keys you have. pip doesn't do any of this which is the point of this post.
ok, how to prove that one? it was a guess. it’s not like there are any statistics about it. also you replied with “you didn’t prove your point” to my “like i said”, which meant what i just explained. so you’re off-topic here, as we were discussing about the other part of my post that talked about the naming choice of `load` and `safe_load` being wrong. this has nothing to do with the “it’s no big deal” part, so you’re in the wrong part of the thread here.
The main advantage is being able to serialise all Python classes, not just strings/numbers/lists/dictionaries. Perhaps a whitelist of allowed classes at deserialisation would be optimal. There is no reason to serialise a subprocess.Popen instance, but a small number of custom classes can be helpful.
You don't do so well with communication do you? All conversations have multiple interpretations, lexical ambiguity. You'll notice the large negative numbers next to your comments, these indicate that you're communicating particularly poorly. If you want to get better, go back and re read what has transpired here and ask yourself two questions of each comment: what else could that have meant? Was this comment clear? Best of luck.
Numbers?
What would you suggest that pypi do with the signature information? 
Memory?
Yeah, while this is a rather large problem, it's not exactly a new one- there's been [an issue](https://github.com/pypa/pip/issues/425) open on the github repository for quite some time now. It's still good to make sure people are aware of the possible risks, of course.
As far as I know none of the http libraries in Python's stdlib verify certs. It's not just a pip thing. As the op pointed out in the sibling comment you need to use a lib like requests. 
First off, I find it sad that he (you?) is showing the [Monte Carlo method](http://en.wikipedia.org/wiki/Monte_Carlo_method#Introduction), yet not a single mention of that anywhere in there. Well, it's slightly different since it adds a second formula for the triangle, but that leads me to my second point: does that actually help anything? I'm not sure about this, but to me, it seems like you're already getting all the information you can from one of the two approximations, and the random point can't magically provide more and more information as you decide to use it in different ways. You could even cut your triangle again once more and get a third approximation, and so on. Lastly, I'm not that good of a Python coder, but a lot of stuff in there seemed really off to me. Things like `0 == N_triangle`, or the fact that you're using a while loop and an index variable instead of a for loop. All that aside though, I'm excited to hear how you'll use the integers to improve the approximation. I also enjoyed the few optimizations in there.
not strictly relevant but - would it not be simpler to program a simulation of [Buffon's Needle](http://en.wikipedia.org/wiki/Buffon%27s_Needle)?
Jesus christ, read what I said before you respond. &gt;&gt; I know they work side by side, and know how to fix any problems that come up, it sounds like you know this as well. Not knowing that, and assuming that python is like most other programs, is not the same as "spreading FUD". &gt; i have never, ever heard of that You've never heard of people **thinking** there would be an issue? Because if you'd read what I wrote you wouldn't have thought I was saying there were actual problems. FUD is intentional, you are seeing malice where there is just ignorance. And it's completely justifiable ignorance given how important this is and how poor the communication around this kind of thing is in open source. &gt; except people using the wrong shebang Yeah, the only issue I've had come up, and what I meant by "any problems that come up". And why I made the distinction between people who don't see that as an issue because it's easy to fix and people who don't even know where to start and don't know what a shebang is. 
From [Wikipedia](http://en.wikipedia.org/wiki/Point_cloud) &gt;Point clouds are most often created by 3D scanners. These devices measure in an automatic way a large number of points on the surface of an object, and often output a point cloud as a data file. The point cloud represents the set of points that the device has measured.&gt; So you can manipulate data from a scanned thing rather an an image. 
Very nice. How does one properly install packages for python then through command line?
&gt; Things like 0 == N_triangle Well, you can do this: def pi_formula2(N_darts, N_circle, N_triangle): return N_circle * (2.0 / N_darts + 1.0 / N_triangle) if N_triangle else pi_formula(N_darts, N_circle) (Zero is false, any other number is true.)
Well, I was mostly referring to the order. I'd personally go for `N_triangle == 0`. Seems like a small thing, but it really threw me off. Your solution works too, but I think it's a bit too complicated to be on one line. get_point_in_shapes() could probably use that trick though: def get_points_in_shapes(N_darts): N_circle, N_triangle = 0, 0 for i in range(N_darts): in_circle, in_triangle = get_point_in_shapes() N_circle += in_circle N_triangle += in_triangle return N_circle, N_triangle
It's stylistically preferred to write 0 == x. All languages, not just Python. * If you write 0 == x and miss an equals, you get 0 = x. (invalid) * If you write x == 0 and miss an equals, you get x = 0. (assignment) Really, I'd prefer that calculate_pi() chooses the formula to use. This way both are correct, and the logic of which is appropriate is handled by the calling method. def calculate_pi(N_darts): N_circle, N_triangle = get_points_in_shapes(N_darts) return pi_formula2(N_darts, N_circle, N_triangle) if N_triangle else pi_formula(N_darts, N_circle) def pi_formula2(N_darts, N_circle, N_triangle): return N_circle * (2.0 / N_darts + 1.0 / N_triangle) 
yeah the python is not especially idiomatic. the ternary operators in the in_circle and in_triangle method stand out to me as quite bad. Those are boolean operators anyway. def point_in_circle(x, y): return x**2 + y**x &lt;= 1 def point_in_triangle(x, y): return x + y &lt;= 1 Also, the `while` loop really needs to be rewritten as `for _ in range(N_darts)`.
Python will throw an error in both cases as inline assignment isn't supported. Many languages won't. Inline assignment is often useful, or even part of core patterns. for (i = 0; i &lt; 5; i++) { foo(); } It's a style thing. You don't have to follow PEP8 either. But it's the way many experienced programmers will expect to see it, and consistency is always nice if you're on a team or doing open source.
This is brilliant. I was hoping to get python bindings of PCL soon. And you have used numpy arrays something which OpenCV should have adapted from the start. Any chance of this getting integrated with PCL soon? Once again, kudos to the team!
I would prefer a package repository to do one of two things: 1. Offer no facilities at all for signing/checksums/etc. and let that emphasize the "at your own risk" that comes from tools not doing anything with them, or 2. Offer a real infrastructure for supporting signed packages, integrated into the whole chain of tools and not just displayed on a web page somewhere. The second option is harder to set up, and slightly less convenient for end users, but accepting signed/checksummed packages when nothing in the chain actually does anything with that information is even more inconvenient since it can lead to a false sense of security. This is why, every time we do a Django release, I generate the checksums and put them in a signed file, and we link to that from anywhere we mention that package. Lots of man-in-the-middle attacks could cause you to get a different package. No man-in-the-middle attack can (currently) fake the signed checksum file.
This is precisely my reasoning behind using that syntax =)
These all seem like great tricks for making your code absolutely unreadable.
Ah, I had missed that aspect of those two functions. Those are much more elegant expressions of the logic. I tried using a large Range object in one iteration of the code, and I seem to recall running into memory issues due to allocations for the Range. Is there a more "lazy-evaluative" way to use Ranges so they don't eat up all the system memory?
[x-post from /r/programming](http://www.reddit.com/r/programming/comments/17cw1h/python_shortcuts_for_the_python_beginner/) Thought I mention it because the above has a healthy amount of comments associated with it.
Your version has a nice seem of symmetry running down it. I've heard the 0 == N_triangle style of writing equalities described as "Yoga syntax" (like in [this example](http://jamiethompson.co.uk/web/2010/05/20/yoda-syntax-a-php-design-pattern-for-if-statements/) for PHP).
xrange perhaps (only relevant for python &lt; 3.0 as in 3.0 onwards range is evaluated as needed)
I'm still using 2.7, so I'll give xrange a shot, especially if it helps me write more idiomatic Python.
someone needs to do a demo of this at pycon. *during someone else's presentation*. preferably, inject a script that will print "I could have just owned your machine!" and then let the install continue, so that it's a scare without an interruption. but still. pypi needs to disable non-ssl http for package downloads.
 False = True if False: print "Hello" else: print "World" ...
That's exactly what I was thinking. The code is already nice organized and separated into functions, so it would be fairly easy to benchmark the precision with one equation and two, and see how well it does. In fact, I'll try it right now! P.S. You really want to use pypy for this. It's one of those scripts that really gains from it. running 50M points took 3.7s with pypy and 51 seconds with python on my system. Hmm, so so far, the standard deviation doesn't change when you use both instead of one, which goes to show that using two formula isn't helping, but the sample mean using only the first formula is consistently falling on the left of the true mean. I think it has to do with points being ON the circle, since you use &lt;=. Yup, just changed it to &lt; and it's much better. Not sure if it's now biased on the other side though. So here's the final result: 1 equation, 500 runs of 2 million: Mean = 3.14158337, STD = 0.00121 Mean = 3.14161884, STD = 0.00114 Mean = 3.14145090, STD = 0.00125 2 equations, 500 runs of 2 million: Mean = 3.14158974, STD = 0.00117 Mean = 3.14162728, STD = 0.00116 Mean = 3.14154626, STD = 0.00117 The difference in the std is way too small to be significant, so there's clearly no difference. Not having to do the triangle test would probably speed it up considerably though. 
The fact that Python standard library doesn't verify SSL certs is the most untold embarrassment. Not only that, but it's also hard to fix it for an user because: * it doesn't ship with an updated CA bundle, so users need to come up with a cross platform solution to that * 2.x doesn't ship with a SSL host name matching function, so the user needs to understand SSL certificates and come up with a matching function The only practical solution is to use the requests library. Anything else is at serious security risk.
Ok, a couple of remarks and I want to apologize upfront if I sound rude. I think the documentation of the floating-point precision is poorly written and confusing in the 2.7 version. It is much better for the 3.3 version. Of course python uses 64-bit floating points with a *precision* of 53 bits, but this is just IEEE definition. Second, I think you should look into Pypy or even numpy just for the speed. Doing this in pure python is painful. What I am interested in would be a) the error behaviour of the simulation and b) scaling. For a) you would expect an error of your Monte-Carlo simulation going like ~1/sqrt(N) and I'd like to see this confirmed and the proportional constant to be measured. Therefore, you have to find the actual statistical and systematical error of your simulation and not just the error of the error like Ph0X showed. For b) a plot would be nice with the scaling of execution time with increase of N and the iteration count N necessary to a given precision of \pi. Those would be to interesting plots where you probably can see why this method is not sufficiant for a practical approximation. But anyways, good job and keep going, I am interested in what you will find. 
I'm amazed that that actually works. That shouldn't work. :(
You think? I think very little of it was unreadable and found most of it quite elegant.
That's not as fun though.
i’m sorry if you felt attacked by my choice of the term FUD, i didn’t want to imply malicious intent. and of course people anticipated problems: before the python file tree structure was finalized. that’s why Python was designed from the ground up to support multiple installed versions. about the shebang: the most damage done in this respect was some debian idiot deciding that debian should be the special snowflake without a python2 symlink in a blizzard of distros who have both python2 and python3 symlinks. which makes cross-distro-scripts impossible just. because. of. this. debian. guy.
It was less secure than the demo code assumed.
Is there a good reason to include .pyc in source control? :o
Does it matter what download? I keep finding sources that mention using a specific download, even though they are outdated. 
It is indeed used to verify the integrity of the downloaded files, but it's a bit useless as an attacker can just strip them out to disable the checks. In my example code: if flow.request.get_url().startswith('http://pypi.python.org/simple/'): flow.response.content = re.sub(r'#md5=[a-f0-9]+', '', flow.response.content)
Still pip, but until they implemented certificate checks, you should be a bit careful about it. Alternatively, you could download the packages over SSL yourself and use `pip install &lt;filename&gt;`.
 You need a few packages for this to work: [..] sudo pip install mitmproxy Nice try!
The secure/insecure connection thing is completely misleading. &gt; Never, ever download something over an unsecure connection This is not good advice. There's no such thing as a "secure connection", or at least even if there *was* such a thing, it still wouldn't help you: you need to use real, public key cryptography like GPG. Even if pip checked the SSL certificate properly (it doesn't) /and/ you couldn't strip the MD5 checksums out (you can!), you could still be quite easily MITM'd or someone just needs to compromise the pip servers (Impossible? See the the rubygems issue.) It has its problems, but take a look at how [APT does secure package signing](http://wiki.debian.org/SecureApt) to see how this was solved almost 10 years ago.
Makes me wonder why this is still unfixed.
They know, it's been an open issue for years.
The best (IMHO) solution to this is to do what Apt does: use public key cryptography to sign all of the packages, and check that before decompression. 
The pattern of '(constant)(comparison)(variable)' is arguably the correct way for the safety reasons listed above. Even then, it's a stylistic choice so saying it's the right or wrong way is a tad silly. From the view point of reading/writing code, it comes down to what you are used to and doesn't matter. From the view point of safe code, it's safer and prevents problems that sloppy typing can cause.
Very good post. I have a question: why bother with a "lazy" decorator? Why not simply write a generator function, like this for "zip_merge": def zip_merge(left,right): if left == (): yield right else: (left_head, left_tail) = left (right_head, right_tail) = right if left_head &lt;= right_head: yield (left_head, zip_merge(left_tail, right)) else: yield zip_merge(right,left) Thanks!
Wow.
Ah, I see it now, missed a sentence :) Thanks for the response. Great write-up!
Thanks. I didn't make it clear that it is the preferred implementation. I'll fix edit the post accordingly when I get the time to do it. 
&gt;Even if pip checked the SSL certificate properly (it doesn't) /and/ you couldn't strip the MD5 checksums out (you can!), you could still be quite easily MITM'd or someone just needs to compromise the pip servers (Impossible? See the the rubygems issue.) i really don't understand your logic. it seems like you're saying since HTTPS isn't perfect, it's equivalent to using HTTP. that's simply not true, and if you've gotten even one programmer/admin/user to believe that, that's not making anybody safer. i'm a little bit tired of armchair security experts who think a single realistic-yet-less-likely attack scenario completely invalidates a security measure. security is not absolute. which is more likely to happen - a MiTM against HTTP, or a MiTM against an HTTPS session where the client is using certificate pinning and an attacker has either stolen the server's private key or owned the server. do you really believe these two scenarios are equivalent? guess what? PGP isn't perfect either. if i steal your PGP private key, i can sign trojaned packages all day long to my heart's content. impossible? see the issue where adobe's code signing key was compromised. yikes, i just proved PGP was useless.
This seems like a relevant post to ask this question. Will somebody please explain to me what this part of the Zen of Python means? &gt;Now is better than never. &gt; &gt;Although never is often better than \*right\* now.
This was great and it prompted another question in my mind: is there an analysis tool that you can run against a package to see what security risks that package presents, from the possibly nefarious, eg root shell, to the possibly benign, eg writes files, opens ports, etc? (The obvious one it is called "reading the code", but it is not always the most efficient way)
What's so difficult to follow? Even if you fix all these obvious security holes in pip, you are still vulnerable to the most obvious and demonstrable vulnerability; the pip server being compromised. Fixing *that* fixes everything up the chain, so claiming my argument relies on the probabilities of different MITM attacks (not to mention getting butthurt and accusing me of an armchair security expert, mfw) is completely irrelevant.
I didn't get that one either. I like this guy answer though : http://stackoverflow.com/questions/228181/zen-of-python 
Yeah, I'd always interpreted that as a matter of getting work done, but it can apply to lazy evaluation as well.
I think [this](http://pyvideo.org/video/638/advanced-security-topics) is the presentation (starts at 14:25). 
The following post has the same approach but uses numpy: http://glowingpython.blogspot.it/2012/01/monte-carlo-estimate-for-pi-with-numpy.html
Yes, and I'm sure there are many people who use it who assume that it does verification and so are lulled in to a false sense of security. I believe that PyOpenSSL and M2Crypto can perform certificate validation, but it's a bit more work than simply using urllib or whatever.
Removed. Now you can upvote me :)
reading the code and trying to figure out how an application works is still more efficient than any automated tool... at least for more complex problems which require multiple layers of understanding. There's no tool I'm aware of which can determine how dangerous a certain package is - if a package only uses stdlib modules, it would be fairly easy to tell, but as soon as 3rd party modules come into play it gets increasingly complex. 
Isn't this just arp poisoning, a security issue common to every single local area network?
why the page using a php based wiki?
After a cursory glance, this doesn't guarantee a single evaluation. It will continue to evaluate if the _recipe callable returns None.
It won't answer your question about Eclipse but I am using [Spyder](http://code.google.com/p/spyderlib/) which is a really good alternative to Eclipse. It has an embedded interpreter and tons of other goodies.
Ok it is fixed now. I added a "thank you line" at the end of the post. I hope that's ok with you.
You're exaggerating again with every day stories about someone creating *.gmail.com certificates. They are the exception, not the norm, just like the pypi servers being compromised. Of course, using gpg increases your level of protection, but you have to assume that all users will download the keys used to sign the packages, which they won't, so it's not a perfect solution.
People downvoted you probably because you said that the name change wouldn't be that big of a deal, because the current functions are documented. The whole point is that documentation isn't enough and the default should be sane.
*ouch*.
This attack requires the attacker to sit between the user and PyPi. The easiest way to achieve this is ARP spoofing or having access to the gateway. A well-designed application should not have to rely on the trustworthiness of the network. 
Yep I get this comment a lot. The reason is because MediaWiki is the oldest, more advanced, extendable wiki, with a great community out there. Another reason is that it is nice to have a similar UI as wikipedia. Python is a hell of a language but don't be religious about it! If you can recommend any equivalent python wiki I 'll be happy to migrate.
Considering the recent issue with the Python wiki (attack wasn't noticed until the attacker did an rm -rf /, no backups since July) that might be a good idea. Not a perfect solution though, as you'd have to rely on another secure channel to obtain the correct MD5. A real public key signing infrastructure would be preferable in most cases I'd say. 
That was amazing! Thank you to all speakers and attendees.
MediaWiki really isn't all that advanced, but yeah, good alternatives don't abound. Seems like someone could hack together mercurial with reStructuredText and get something pretty decent. MoinMoin seems to be prevalent in the Python community, but I'm not a fan.
I'm using PyScripter and it has a habit of crashing on me when it's running scripts.
If you use a package manager which verifies signatures (apt for example), an attacker would have to compromise the signing server *and* the main mirror without getting noticed in time. I assume that the signing server is very well secured and that they have specific security strategies, this scenario is way less likely than the simple compromise of a site like PyPi. If the PyPi servers were compromised and malicious code was added to all packages, we wouldn't necessarily notice that right away. Kinda scary. 
Can it use the PyGame interpreter?
so they disagree with me and should discuss with me, not downvote that comment (don’t downvote for opinion!), and more importantly neither all other comments of mine. if you’re right, this subreddit is full of petty malicious idiots, and i really hope it isn’t.
Good stuff. When I was still somewhat new to python I wrote a module called [JSRPC](https://github.com/barneygale/JSRPC) which works in a similar way.
If i run a hotspot and trick you into connecting to it, I can read and modify your clear text traffic (ie: ifintegrity and confidentiality protections aren't being use) Normal wireless the way almost all of the world uses it doesn't protect against a rogue access point masquerading as a known access point. If you start a AP "coffeeshop" with the same password as the coffeeshop AP, you'll get traffic. 
&gt; you could still be quite easily MITM'd or someone just needs to compromise the pip servers I'm not sure if you know what MITM means. It doesn't mean the endpoint is hacked. 
&gt; Was a slight exaggeration for affekt. SSL is pretty good for transport-level security IMHO but truly awful for actually validating that person you are talking to is who you think they are That's not true. It's only as good as the certificate chain, but that is understandable. 
Well, obviously.
Agreed that real signing would be the best. I'm not really convinced there needs to be a secure channel though. If I check the project's homepage, their github page, and PyPI and the md5s all match, I can say with a fairly high degree of certainty that it's right.
That's fair. It does allow you to do pip installs on untrusted networks presuming you've previously gotten md5s in a trusted manner and saved them in your requirements.txt. It's definitely not perfect, but seems like it would be a simple way to take a large step in the right direction.
Eh, that was just a setting I selected soon after making the repo public. I was slightly worried someone would fork the repo and completely go their own direction with it before I had time to put things in place. I just change the repo settings, so you can create a public fork if you want.
Why? Eclipse/PyDev is **very** good, especially if someone's already used to using Eclipse for development. Even if they aren't it still awesome... The only thing about it that I don't like is the slower startup time than most other editors.
Didn't really mean to say it was wrong, just that it threw me off, and I was mostly asking as to why he did it, to which I got my answer. But I still maintain that while such a practice might be useful in other languages, there is no point in using it in Python. I understand that people come from different backgrounds and they have habits, but Python always strives for readability, and I think it's more readable that way.
Thanks for putting it up. I'm having a lot of fun with it. I added abs() to the parts of the denominator for calculating the weighted averaging to prevent division by 0. den = abs(self.mass) + abs(object.mass) I also added a toggle for a grid of swarm objects that generates a set of new minor objects every 100 ticks with the generators 100 units apart in x and y. I got a recursion limit for tick, so I removed line 115 new_object.tick(major_objects) 
MoinMoin tends to be pretty good in my experience i think the upcoming MoinMoin2 could be a pretty good base 
I have never used PyGame but my guess would be that it cant be used like any other library. I don't think that PyGame has any dedicated interpreter(I am not sure though), maybe you mean the Python one.
I don't understand. Just because a site or service is centered around Python doesn't mean it the platform it runs on has to be written in Python. I mean, why is this site using Apache when it could be using a Python-based web server. The reason is because you choose the application more suited for the situation. MediaWiki is a comprehensive and scalable web application with a significant support base.
As an aside this site looks horrible on my phone. The left menu is always visible as well as the right favorite books section yielding no real estate for content.
I would be surprised if that was intended by the author though. Laziness wasn't a big concern back when the Zen was written.
&gt; I don't think that Python frameworks should be acting as amplifiers for people's ignorance by ensuring that a leak of the secret key becomes a machine compromise rather than a session compromise. As I said, I agree with this: &gt; I would agree that it's better to put JSON in there rather than pickled data, but only to the extent that it mitigates the risk of the secret key being compromised. But cookie-based sessions are not turned on by default in Pyramid and the risks are well documented. Noone uses this system without knowledge, and the usage of pickle in Pyramid and Beaker will not cause [thousands of breakins over many years](http://www.kalzumeus.com/2013/01/31/what-the-rails-security-issue-means-for-your-startup/) the way the Rails vulnerability will.
Good stuff but I don't like responses being prefixed with `&gt;&gt;&gt;`. That's the input prompt, not an output prefix. If the beginner types those things into the interpreter, it will look like their (correct) outputs are different from yours and might make them think something is wrong!
That's exactly what [programmers.stackexchange.com](http://programmers.stackexchange.com) is for. There are lots of awesome answers there!
&gt; A MD5 hash is not an assymetric signature. There's no public key you can verify an MD5 against to make sure that is wasn't manipulated. Hence: "the retarded way using a hash, true" I know apt very well. Is there any reason pip didn't duplicate this functionality? RPM managed. NIH?
Why?
That's what I'm asking, why is dogfooding important for a Python pastebin? I understand that there are Python wikis, but questioning the use of a PHP-based wiki just because it's a Python-centric service is irrelevant.
I agree that Yoda syntax reads unnaturally, so if assignment is not allowed within loops then I can train myself away from using it in Python. I write C# during the day, so I inadvertently end up using its paradigms in Python (though I'm working to overcome that practice).
That's a lot more elegant than my method, and produces pretty pictures to boot! I need to read up on pylab.
I can use it on the Python one, and others have used it on Eclipse. I just do not understand where the Pygame interpreter is located. The steps seem to be similar to how you needed to configure the interpreter in PyDev. 
I like your concise expression of testing a large number of random points for membership in the circle. Perhaps it can't do better than 1e-6 because Python only uses 53 bits for floating-point numbers.
PyPy looks like it would improve performance significantly; I'll look into that for my next post. Why would we expect the error to go as 1/sqrt(N)? Is that a general property of Monte Carlo simulations? I'm going to look into the PyLab library for generating plots.
I think you made a good decision here. I find MoinMoin to be really really frustrating (and unintuitive syntax-wise) Cheers :)
Yeah I should probably rethink how negative mass works. I kind of let it happen however it wanted because it's not like there's such thing in real life to draw from. But when I get to doing 2.0 (I have some cool plans) I'll actually do the math and see how it should be.
Python is very nice, there is so much that I dont know. Does this work because everything is a descendant of Object and thus when used in certain situations, a certain method is called from it and LazyObject simply rewrites all of those situations? 
I was just curious as to what I could and couldn't do as I have to let my teacher see the repo so he can actually see that I did work. I wanted to make sure you weren't protecting something that I wasn't allowed to do (sharing a fork). Thanks for making it so I can do it public. I already forked it now I just get to dig through :P
You've still yet to answer my question. I asked why dogfooding is important, because AFAIK unless you're a corporation demonstrating a product there's no reason to do so. So far the only answer I've heard is "because it's important", which is a tautology at best.
Thanks so much for this! Got another project now
That's the issue I'm talking about. I was just on the docs.python.org side of things and noticed they were attacked, I was wondering if they were attacked and shut down their DB for precautionary reasons. 
I suggested a less complicated IDE because OP said that he was new to programming and at the time it wasn't known OP had experience with Eclipse. Eclipse can be over complicated for testing and playing with code, which often happens when learning a new language
You'll want the first Macintosh one. pygame-1.9.1release-python.org-32bit-py2.7-macosx10.3.dmg
A fair point, but, to be equally fair, it doesn't hurt readability. You are used to the idea of "Does variable X equal Y". The statement in question merely reads "Does Y equal variable X". It's no more or less readable. The awkwardness is merely because you are more used to the former than the latter.
Mind me asking what college you're at that's doing this? I had an idea for a course similar to this, and I'd love to hear more about how your school is handling it!
Now if only there was a way to have lazy arguments.
In practice, if your SHA1-hashed password isn't very complex then it will be easy to crack via bruteforce, a dictionary attack, or a rainbow table/lookup table. Other hashing solutions are both immune to any kind of lookup table-based attack and also are much, much harder to launch a dictionary or bruteforce attack against.
Alright, and now this "point to the same Python Installation folder" is it the Eclipse one, or are we talking about 2.7/3.3?
bcrypt (Blowfish), scrypt, and PBKDF2 are 3 hash functions that take a long time to bruteforce/dictionary attack. When developing a new web application in any language, it is pretty much always suggested to hash user passwords with one of those hash functions. SHA1 is better than plaintext, but it is only a tiny bit better than MD5 and is still not much defense.
For various reasons. Extracting data from Text file and use it for later purpose is not useful. Once you get data in standard format(*like JSON*) you can then put it in DB or anywhere else for finding something meaningful.
You seem confused. A few points: 1. I don't feel attacked, You didn't accuse me of spreading FUD, that was someone else. I was calling you out for being a dick in your response the them. 2. If you didn't understand #1 then all the rest of this thread of messages was pointless.
man that's so crap
If you want to get into pygame, here's a couple simple modules that could use some new features: https://github.com/asweigart/pygcurse and https://github.com/asweigart/pyganim Email me if you're interested. al@inventwithpython.com
I think need to add a basic regular expression operations.
While not addressing you, I introduced the term FUD in this thread: &gt; ... arch and ubuntu installs run flawlessly, both being installed? there is no fucking way, and you should be ashamed for spreading FUD without any reason.
I don't know if that applies to the Mac version as I use Windows myself sorry
That's every Linux distro, not just Debian. But Debian is the only stupid distro that has no python2 symlink -.-
It's not from me :) . I'm just stealing the karma of my co-worker because he though it wasn't /r/python-worthy.
It is alright! I am getting there! Thank you. 
Yes I agree with those statements. I was not clear at first, and when I did first learning Eclipse, I was more fresh. It was hard as hell. But, I do know the interface a bit better. Thank you nonetheless. 
https://wheel.readthedocs.org/en/latest/#automatically-sign-wheel-files
 All objects in python (not only those inheriting from object) are pretty a simple wrapper on a dictionary containing your object's property and methods. The wrapper offers a "." syntax to fetch a value associated to the key in the dictionary. The trick is that python gives you the possibility to overload the behavior of this "." syntax when there is no member associated by overloading __getattr__. That's what I used here to trigger the evaluation of the "recipe", and redirect calls everything "." calls to my object. 
How easy/reasonable would it be to patch pip to utilize/depend on requests?
* http://web.archive.org/web/*/http://wiki.python.org * https://github.com/steveyen/moingo * http://python-guide.org
Awesome!
Here are some links which may help in developing a solution to this vulnerability: * https://github.com/pypa/pip/blob/develop/pip/download.py * http://stackoverflow.com/questions/1087227/validate-ssl-certificates-with-python * http://hg.python.org/cpython/file/tip/Lib/ssl.py#l168 * http://pypi.python.org/pypi/backports.ssl_match_hostname/ * https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/packages/ssl_match_hostname/__init__.py * https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/connectionpool.py#L107 It looks like there is an issue filed for this in the project's issue tracker: * https://github.com/pypa/pip/issues/search?q=ssl * https://github.com/pypa/pip/issues/425 (a year ago: "pip should not execute arbitrary code from the internet")
Someone has already posted that link here... thanks for linking to this discussion, maybe it will revive the bug report.
We are performing a sample mean calculation in Monte Carlo integrations (because we just select some samples of the support of our function). Hence, the variance of the sample mean is given as ~1/N and the deviation ~1/sqrt(N). http://en.wikipedia.org/wiki/Sample_mean#Variance_of_the_sample_mean
No, I think you are mixing some stuff up about floating-point representation. Python uses IEEE-compliant floating-point numbers with 64-bit. You just get a precision of 53 bits from this representation. But this is entirely normal and would get you far, far below 10e-6. Although I checked my implementation I guess it is flawed somewhere in a sense that it round a intermediate result to 32-bit screwing up the precision.
Aptana Studio (www.aptana.com) is an Eclipse modified version with PyDev included, and is very nice. It seems to work with PyGame better than the regular Eclipse.
Interesting. I often start instances on Amazon AWS inside my own security group, and I *think* that that means that I'm on a trusted network and so I'm safe from this. Am I being naive?
I did something like that, though my goal was to delay connecting to a database until it's necessary (because it could be unnecessary). Then six months later I looked at the code, got scared/ashamed and rewrote it to just use `get_connection()` function without any magic, because what the fuck, man. Anyway, two notes: first, Real Men would go for an object-oriented solution, delaying `__init__`, and change the `__class__` attribute from your proxy stub to the real one the first time you trap attribute access. This way subsequent accesses are just as efficient as if you used an eagerly-evaluated object, and _more_ efficient than manual lazy evaluation! You also don't have to worry about performance of your trap code so you can generate it programmatically (not with eval but by assigning to every class method that should be trapped a function that remembers which method it was assigned to). Second, actually the textbook approach to finding k largest elements is to use quick-select or median-of-medians algorithms to do it in O(n) instead of O(n\*log(k)). You can even do it online (i.e. as you go): get a buffer for 2\*k elements, when it fills find the median and drop the lower half, repeat. This way you spend O(k) every k elements, therefore get amortized O(1) per element (or in total: O(k) \* n/k = O(n)).
Built an app to allow people to publish their public keys from github for this purpose (github name/email =&gt; ssh key lookups), but never really got anyone interested, so I took it down after the rails exploits earlier this month.
For the first part, I totally agree with you. I would never use this LazyObject code in a project. I put it there to demonstrate the concept of Lazy evaluation and play with it. I believe the generator version is much more simple and straightforward. For the second part, good catch! First I forgot to state k-lowest elements had to be sorted. Then even with this extra constraint, quick-select offers a complexity of O( n + k ln(k) ) which is O(nln(k)). I will fix the article accordingly. 
Sure, but it's a vague statement that could apply to lots of things. I doubt that's a coincidence.
Might be a good modification to have it read what the log format is supposed to be from the Apache config file.
`O( n + k * ln(k))` still is not `O(n * ln(k))` if that's what you meant. Let n = k^(2), for example, then `O( n + k * ln(k)) = O(k^2)`. For small k O(n log k) algorithms might be faster in practice, though I'm not sure, quickselect is fast.
No I mean it is asymptotically bounded above by. We have n + k * ln(k) = O(nln(k)) but we don't have nln(k) = O(n + kln(k). I also used k = sqrt(n) to convince myself :)
JSON is still a text file. You can do the same things with the JSON output that you could with the original file.
The package listing is kinda difficult to read the way it is currently, it would be way better off being a listview, maybe with some filtering and sorting abilities. Also the entire thing locks up if you try `pip install numpy`. Also I don't really understand the purpose of this, you are removing the convenience of the commandline, but still retaining the lack of discoverability that is inherent in it (compared to a GUI), especially considering I cant `pip help`. It doesn't even handle failure for that matter, it just says things are done, which is really misleading. 
So this is using Github in a way similar to a PGP keyserver. Note that to properly trust the key, you *still* need to check the hashes, ideally in person, or over the phone with someone whose voice you personally recognize, just as you would after receiving a pubkey in email or IM. The PGP keyservers are great for distribution, but not trust. Beyond the obvious problem that you might get my exact Github username wrong, or otherwise pull an attacker's key rather than mine, I don't actually trust all the keys I give to Github myself: I often have to push from VPSes that I don't have 100% control over, so I use dedicated "junk" ssh keys that have rights to my Github account and to nothing else. You probably wouldn't want one of those on your server in an authorized_keys file. Also, I had no idea that Github automatically publishes SSH keys. While publishing public keys should generally be ok, there's a potential privacy problem specific to SSH pubkeys because your first three keys are automatically sent to *every* machine you even attempt to SSH into. Generally, this isn't too much of a problem since (a) any privacy issues would only be exposed to people who have access to your keys – i.e. owners of servers you've attempted to log in to, and (b) you probably trust, and have already been identified to anyone whose machine you're logging in to. However, if your SSH pubkey is widely distributed, (a) breaks down, and lets anyone whose machine you even attempt to connect to (since keys are sent prior to login) unmask you. For instance, if you maintain a pseudonymous account on some shell server, it's going to be *very* hard to avoid leaking your primary public keys to the owner of that server. If you've published your keys à la PGP keyservers, or Github, the owner of that machine can now just google your fingerprint and figure out your real name. It's kind of annoying that they don't make that clear. Finally, the `gh:` and `lp:` prefixes make it a bit walled-garden-y. You might look at [PGP PKA DNS records](http://www.gushi.org/make-dns-cert/HOWTO.html), so you can just look people up by their email addresses rather than being tied to a particular service provider. Of course, this kills the utility if you're trying to obtain keys without needing to train the owners in how to find and send them. 
How does SSH send three public keys when unless I tell it different it only has access to a single id_rsa.pub? I have different identities for different servers as required, those are completely different keys in different files.
It sends the first three keys it finds either via filename in ~/.ssh or via directives in ~/.ssh/config; I have lots of keys, so there are always three available unless I've kept things very tidy. Once you have more than three, you have to start binding specific keys to specific machines via `host` blocks in `~/.ssh/config`, since the other keys will never be tried automatically – it would causes too many failed logins as it repeatedly tried keys. `host` specific configs, of course, can fix the problems I mentioned above, but it means your housekeeping has to be perfect all the time. In your case, that single SSH key will be sent to every machine you SSH to, regardless of whether or not you actually have an account on the machine, successfully log in, or use a password rather than an SSH key. To see this, restart your sshd with LogLevel (temporarily – it's a lot of data) set to VERBOSE or higher, and check your logs as you do a password based login (or fail to log in). 
I have some code that I use to scrape content from the legislative tracking service that our municipality has purchased. The code uses mechanize as the web browser and beautifulsoup4 for the parsing. https://github.com/umeboshi2/hubby 
Granted, that single SSH key will be sent, unless I specify an alternate for a specific host. I don't care about my main one leaking, I would care a little bit more about various other keys leaking.
Yeah, that's why I'm a bit annoyed about Github. If I had known they were going to publish all the keys I give them, I'd probably have given them exclusively single-purpose keys, not my default key that is always sent. 
Debian packages are not signed, only the Release files on the apt repository are signed. The developer signs a .changes file that is sent to the server with the rest of the files that are listed and checksummed in the .changes file. This signature is then check before the packages are added to the repository. Once the package is added to the repository, new Package lists are created, the checksums of those lists are included in the Release file. The Release file is then signed by the apt repository key. 
The only thing that was close to unreadable was the golfing of FizzBuzz.
Mozilla tells me when I go to https://pypi.python.org/pypi/ that: &gt; pypi.python.org uses an invalid security certificate. &gt; The certificate is not trusted because no issuer chain was provided. 
how is what you said not effectively signing the package?
This is perfect. Im gonna dissect this.
It does not at this stage (unless you configure pip for this outside of pip for Windows). It could be considered in a later release though.
There's more to it than that. With `apt`, you have a software repository centrally managed by a small group of people, who can vouch for its contents. With PyPI, anyone can register and upload anything - there's no curating of its content.
Of course it does. You said yourself that it's in the Release file.
You have been seriously misunderstanding me, which is why I posted a link to an arbitrary package. All you have to do is post a link to the signature of this package. If this signature exists is the Release file "like I said" (because I actually said that "new Package lists are created, the checksums of those lists are included in the Release file" meaning that the checksum of the file listing the packages is what is listed in the Release file). http://wiki.debian.org/SecureApt This wiki entry does a much better job of explaining the process than I did. The only thing that has really changed in the apt repository for secure apt is the signing of the Release file. The user tools changed a good bit to make good use of secure apt, but the repository itself changed very little. 
Right. A lot of people use Windows - and as far as they can tell, the site can't be verified as being the *real* PyPI. Edit: I just checked on Ubuntu Oneiric 64-bit. I get the same warning there (using Firefox 18.0.1).
X
Our next meetup is this coming Thursday, February 7: http://www.meetup.com/PyLadies-ATX/events/96182362/
All you have to do is actually demonstrate this by producing a signature to the debian package that I posted above so that we can verify who actually signed it.
If you insist, here is your digital signature for the bash package you linked to above: ftp://debian.osuosl.org/debian/dists/stable/main/binary-i386/Packages.gz ftp://debian.osuosl.org/debian/dists/stable/Release ftp://debian.osuosl.org/debian/dists/stable/Release.gpg 
I've updated the installation instructions of pip for windows : it now includes steps to support packages that require a C++ compiler.
Update: ssh-auth-id will be merging with the ssh-import-id project in Launchpad &amp; Ubuntu (https://launchpad.net/ssh-import-id). I'll keep you all updated on the progress of this. Update #2: This project is merged with ssh-import-id. To avoid confusion, I'm going to remove the old name from PyPI. http://blog.dustinkirkland.com/2013/02/ssh-import-id-now-supports-github.html
Don't run pip as root, use virtualenv. That shortcuts a lot of the possible issues with this.
I was not aware of the first three keys being sent to the remote server on connect -- to be clear, the first three in authorized_keys, correct? Can you give more background on this? [Update: nevermind, just saw your comment below] The gh: and lp: prefixes are indeed walled-garden-y. They're shorthand for a service that you and a collaborator mutually trust. If you don't trust these sites, you can add a subcommand for your own prefix -- roll your own keyserver, essentially :) The DNS records are something to consider. PGP signed SSH keys would also be an alternative to "trusted services". For example, if I already trust Alice's public key, it would be cool if I could ask a keyserver for all the public SSH keys she has publicly distributed and signed. It would also be interesting to add some level of confidentiality to the public keys in a keyserver -- perhaps some basic authentication is required to get at some keys, or you must provide proof of membership in some existing WoT to get at some keys. These are extensions I'm considering for my Hockeypuck keyserver project (https://launchpad.net/hockeypuck).
This just reminds me of [my adviser](http://www.cs.sunysb.edu/~algorith/) so old project.
You are confusing integrity with authenticity, which is what the gpg manual is also doing. The gpg manual defies the meaning of "signature" and is really talking about identifying properties of a document using a hash function. This is more like fingerprinting which has nothing to do with signatures. From wikipedia the definition requires authenticity. http://en.wikipedia.org/wiki/Digital_signature#Definition Also from wikipedia... http://en.wikipedia.org/wiki/Signature A signature is used as proof of identity, not integrity. A checksum is used for integrity. An encrypted checksum is used for proof of identity by relying on the checksum's inherent proof of integrity. 
I think what you are missing is that you have a signed checksum. This is what a digital signature usually is, it's just a hash (of the message you want to sign) encrypted with the private key. This is detailed in the links you just provided by the way.
Hmmm... is it the Eclipse plug-in version? Or the standalone one?
That article needs some discussion about how to run many scrapers in parallel. There are TONS of ways to do it, but they all center around threads, multiple processes, or some kind of eventlib approach.
I am still having the same problem :/ The module is not being found. How do I find this bloody module?
Wish I knew this before :( I just started using pip and already did root pip installs of virtualenv, ipython, and a few others. It's my fault for assuming it did at least signing of packages and SSL host verification. Fortunately I was only on trusted networks (workplace LAN and at home, not some random coffee shop). So I'm probably ok. Does pip store the downloaded packages anywhere so I can check MD5? I know it's kind of pointless checking it now, but it would make me feel a little bit better if I could check the MD5s. I'm definitely going to be more careful now. How secure are the pypi servers and mirrors? Has there ever been breaches? I can be careful to only use trusted networks for pip installs, but there isn't much I can do if the actual source servers are compromised. 
This was posted two days ago in this subreddit, and got 84 comments: http://www.reddit.com/r/Python/comments/17rfh7/warning_dont_use_pip_in_an_untrusted_network_a/
&gt;One of the Internet has created a pip_intercept.py script... dem internets
You should *not* be using pip to install system packages. That is what they system package manager is for. If you want a package that isn't in your repo, use a virtualenv.
It's a bit abrupt to dismiss something so readily. Shouldn't "package managers" such as pip protect against MITM attacks? It can't be that hard to implement.
Yeah signed packages and SSL host verification of the repo servers should be basic standard for any packaging system. I'm kind of shocked that pip doesn't have this. Not running as root doesn't somehow make pip ok.
Alright I demand to know what those buttons on the left are for.
So did I... I wouldn't worry too much, though. No, there haven't been any breaches I'm aware of.
I'm "one of the Internet". I feel honoured.
Sorry for the late response, and I've seen your other responses. Still... &gt; It would have been the better naming choice... No. It is still a better naming choice. Citing documentation as a mitigating factor does not mean it isn't a big deal. It isn't a big deal until the moment you get hacked. Security is a huge chain of small decisions. It is a complex problem. Everything we can do to make it easier and less complex (and an example like this is basically a free win) is progress.
Not surprised. IMO, having your own package management system is just asking for trouble, especially when it needs to be cross platform AND run as root. I'd much prefer either a solution based on existing package managers (not difficult, most python libs do this anyway AFAICT) or a solution that stores the extra plugins on a per-user basis, so no administration privileges are needed.
The URLs end with `.txt` and I'm getting HTML back? Screw that. **UPDATE** [Wat?](https://github.com/jdiez17/pastepm/blob/567156745dfd08820918ad9de4361ea4aa338c53/templates/raw.html)
Right, I've done a fix for this, however I'm being a bit dumb and can't get some tests to pass. Could someone possibly take a look at my fork and help me figure out what's going on? Also a file I added isn't being extracted when I setup.py install, despite being in MANIFEST.in Dumb mistakes, I'm sure, but I'm not super experienced at contributing to big projects. https://github.com/radiosilence/pip **TLDR: Pip fork with SSL Validated Certificates!**
&gt; If you don't trust these sites, It's not a trust issue so much as a federated namespace issue: Any "identity" you have via those sites is not a real, personally owned and controlled identity at all – it's just an account on some website. Contrast this with email, which is federated: You can choose to build your online identity around an account with Gmail, while I can choose to own my own domain and manage my own namespace. That's what I meant about losing the seamlessness, though – if you want to propose some service provider agnostic key publication mechanism, you'll lose the keys-are-already-uploaded bit that makes your idea so frictionless. &gt; It would also be interesting to add some level of confidentiality I was thinking about this after seeing your project this morning, and it seems like the solution is basically a social network. To make this cryptographically secure, offline, and federated requires more crypto than I have, though. For instance, even sticking with the "extra layer" of PGP and its WoT, you'd have to devise some kind of expiring trust assertion format for people to sign and vouch for specific pubkey/PGP key bindings. Anything would have to deal with lost keys, revoked keys, and forgotten keys. I think the biggest problem with SSH pubkeys is that managing them, both on the server side and the client side, is a nightmare. They're like passwords that are regularly left lying around in insecure places on the web. Having run a pubkey only server with many non-technical users, it's horrible collecting the keys, then keeping them all straight (and marking which are verified, revoked, superseded, etc). Having installed a key, I'm counting on the user to keep it safe, protect it with a good passphrase, and notify me when it should no longer be trusted. Realistically, none of these can be counted on. On the client side, it's possibly worse – pubkeys get around, and just make connection magic happen. There's no inventory of where your key is installed, and it's a nightmare to know which version (or multiple versions!) are installed on which servers. Pubkeys to ssh.example.com worked last year but not now? Throw on another! Was that because I'm on a new machine with a new ssh key, or because the one that's installed is the one that was on that stolen laptop last year? Realistically, I have *no idea* how many servers around the world my various pubkeys exist on. And this is scary, because only I am in a position to know whether any of the corresponding private keys remain secure (and that only if I'm conscientious, unusually good at maintaining the security of my own computers, and keep pretty good track of my SSH keys and their backups). So perhaps the best improvement would be a better UI for managing and communicating about pubkeys: Maybe something that lets me, the admin, pull all of each of my users' keys automatically from Github, or the company wiki, or wherever, then name the keys, present them to my users to have them confirm which is the right one, then remind me or the users every *x* months that their keys are installed on servers 1, 2, and 3, and could someone please verify that the keys are still valid? 
sure, but it’s the PyYAML team’s decision if they want to introduce backwards incompatibility. and because i guess they won’t, i said “would have”.
But regardless of whether the CA is trusted, it's a configuration error for the site not to supply the complete CA chain to clients when they connect. Otherwise you don't even know who it is that you don't yet trust :-)
&gt; Yeah signed packages and SSL host verification While I agree about SSL host verification, especially now that you can get free certificates for SSL server identification, signed packages can be a bit of a red herring. If I download a signed package, even if I find the signer's public key somewhere and use it to verify the signature, and it verifies without error, how much trust can I have in that public key? AFAIK there isn't anything like a Web of Trust around the keys of people who upload signed packages to PyPI.
That was fairly enjoyable actually, just played with someone else who hands down beat me on diamonds but I got to the girl first. It was actually kind of hilarious when the two players ran into each other and vied for a good jump spot.
what do you mean by *JSON is still a text file*?
JSON is a text based data format. You're still going to have to process the file serially, just like the Apache log file.
internaut? this has to be the coolest word I've ever heard
heh...Python...SSL host cert verification... you're funny...
Yep, I had an XSS there, I hadn't properly escaped the raw view. That's what I get for doing things on the run!
Do you use a different virtualenv for every package? Cause that would be a pain. if you use the same virtualenv as your app, you just compromised the entire app, who needs root if they already have read/write to your entire database, all your access keys for cloud services, etc?
It exists in other languages as well, like "internauta" in Spanish. I'm surprised it doesn't exist in English?
Is there a CVE ID for this issue? * http://cve.mitre.org * http://en.wikipedia.org/wiki/Responsible_disclosure
I understand mathematically what a point cloud is. I just don't really understand when most people would need it. 3D scanners and laser range finders are still rare and expensive equipment, so unless you're working in film or science, it seems unlikely like this will be useful to most people. Also, since it seems mostly geared towards 3D recognition, I'm confused by the project's description as an "open project for 2D/3D image and point cloud processing." When and how would you use a point cloud to process 2D images?
Are you looking for something like this: https://c9.io/? What is software for Chrome OS supposed to be? A web app?
No... should I apply for one? Didn't think this was important enough. 
Ouch. 
True that. Idk why I deleted that haha. How do we install PyGame with Aptana? 
not that cool when overused in serious contexts, but glad u like it :) 
* http://cwe.mitre.org/data/definitions/250.html * http://cwe.mitre.org/data/definitions/306.html * http://cwe.mitre.org/data/definitions/311.html * http://cwe.mitre.org/data/definitions/494.html * http://cwe.mitre.org/data/definitions/807.html * http://cwe.mitre.org/data/definitions/829.html
http://stackoverflow.com/questions/43649/how-to-get-involved-in-an-open-source-project
Errrr... it's a "Doctorific" web site now. Virtual hosts issues ?
www.pythonanywhere.com :)
Our equivalent is "neckbeard". Personally I think the crowning achievement of English internet slang is "slacktivist"/"slacktivism".
Vim.
&gt; So next we will talk about how to recognize `None` properly in the first place. So, the next blog post is gonna be this: If foo is None: # We have successfully recognized None pass 
Netizen is the "best" I've heard
Chromebook is not for content creation. If you insist on using it so, SSH to a server and vim or emacs there.
It seems that we are talking in circles and dancing around what is actually being signed. What I have been trying to stress the whole time is that it is the repository that is being signed, not each package.
You could do the former with `PIL`: + [PIL Handbook: `ImageFile`](http://www.pythonware.com/library/pil/handbook/imagefile.htm) describes reading in a file + [PIL Handbook: `Image`: Attributes](http://www.pythonware.com/library/pil/handbook/image.htm#attributes) mentions the `size` attribute which is a tuple: `(width, height)` For the latter, [Python Docs: `os.path`: `getsize`](http://docs.python.org/library/os.path.html#os.path.getsize) should suffice. 
I think what he was trying to say was that the *actual bytes* of the package file don't have an *actual signature* associated with them. They have a checksum associated with them, and the checksum itself has been signed, but not the actual package. Why he thinks that's a significant point to discuss, I am not quite sure.
You shouldn't use `list` or `len` for your variables names. As they're reserved Python words, doing so can bring you troubles in the future.
Someone play with me?
neat!
[&lt;.&lt;](http://thecodelesscode.com/case/6)
You may be able to check the HTTP status code for "301" or "404" to detect when the image doesn't exist.
The conclusion is misstated. It should be "Not all nothings are equal." "All nothings are not equal" would mean no two are the same.
The solution that I found most usable was having my workstation setup with ipython notebook and connect to it through a browser from my chromebook (or my tablet): http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html I did this because I needed a rather substantial software stack: plenty of libraries (boost with python bindings for one). There are advantages to using your own machine (full customization on infrastructure, no restrictions in code size/result file size etc...). The advantages are obvious: you need to maintain a secure webserver (i.e. buy domain name, dynamic dns) and guard against connectivity loss, power loss etc... The ipython notebook is not as nifty as spyder2 but it works well enough for "most" of what I do.
I am amazed that more websites like this don't exist.
Good site, but I hate TDD personally. Still worth a read. 
Source available?
PyPi / distutils uses GPG so you'd have to find a path in that WOT currently which often easier said than done. Getting a malicious key within signing distance of a target on the WOT is probably not much harder than getting a real key there at this point though. And then how do you know that package X was written by Bob and not Alice to know which package signature to actually accept? Even if PyPi supplied the key id packages of a particular project were meant to be signed by you could use that, but then your security isn't really much better than just downloading the package and checksum over HTTPS anyway.
Hence why I'm trying to [push a patch](https://github.com/pypa/pip/pull/789) fixing this issue ;)
[pretty much](http://blog.startifact.com/posts/none_02_recognizing.html)
Exactly what would have been my answer.
Yes, I was pretty clear I wasn't addressing experienced developers. Though you'd be surprised. I've seen: "if foo" quite a lot, even though None was the only alternative to a value. I discourage that, something I describe in part 2. [a downvote for that? really?] 
Why do you want to install it with Aptana? Do you have to for some reason? 
I'm still not sure exactly what your objection is. Maybe we can clear it up by simplifying the scenario a bit... 1. Do you agree that the Release file is signed? 2. If yes, do you understand that the way this is done is by generating a hash of the Releases file and then encrypting the hash with a private key? 3. Do you agree that the hash of a hash is still a hash? So for example, taking md5sum of the md5sum of bash.deb would still produce a hash of bash.deb? 4. Do you agree that if the contents of Release were simply a hash of bash.deb, that bash.deb would be signed? (You still have a Release.gpg produce in the same manner as before, i.e. by hashing Release and then encrypting the result with a private key) 5. Do you agree that if you have two documents, document A and document B, that you could sign them in the following way: take a hash of document A, take a hash of document B, take a hash of these two hashes. Then encrypt the result with a private key. 6. Do you agree that if you have signed document A and document B as described in the previous question that it is fair to say that "document B has been signed"? I'm trying to figure out where your objection lies exactly. I think we can understand each other better if you could answer the yes/no questions above.
 Python 2.6.6 (r266:84292, Dec 26 2010, 22:31:48) &gt;&gt;&gt; import datetime &gt;&gt;&gt; datetime.date &lt;= None False Added: As pointed out by more than one person, I am comparing the wrong apples to the wrong oranges here. As a result my point ceases to exist...
Keep the good work Marcel !
What is the purpose of comparing `&lt;type 'datetime.date'&gt;` with `None`? And the import is incorrect.
I'll not be argumentative about it, but I'm curious, why do you hate it?
same file, with pep8 fixes and using [requests](http://docs.python-requests.org/en/latest/index.html) library: downloadFile('filehost.zip','d69aa344e84cc3865dc240e31699b21b')
you install pygame(and other libraries) to your OS, not your IDE, that's probably the source of the confusion. 
I do not know what specific behavior you mean, but you compare a type (not an instance of the type) with `None`. Here is the result for 3.3.2: &gt;&gt;&gt; import datetime &gt;&gt;&gt; datetime.date &lt;class 'datetime.date'&gt; &gt;&gt;&gt; datetime.date.today() datetime.date(2013, 2, 5) &gt;&gt;&gt; datetime.date.today() &lt;= None Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; TypeError: unorderable types: datetime.date() &lt;= NoneType() 
It's a Buddhist thing http://en.wikipedia.org/wiki/Koan
nice work! the thing that i appreciate more is that you have used this module to host the file :)
Did little editing, here it is: http://pastebin.com/JJ5wYKvD * Fixed: save path/filename now multiplatform * Fixed: better variable names (some of them) * New: saved files have extension based on mime info from server.
not quite fool-proof since ".pyc$" would match "notapyc"
Here's a [different approach](http://pastebin.com/9NWCQiJ1). * You should definitely look into the requests library. It's way easier to use than urllib2. * Imgur gives you a great way to find a random image with no failures, rather than brute-forcing random URLs. * Your handling of mimetypes might work in this case, but there's a more correct way to do it. * Keep on sharing your code!
Maybe doing the verification on Python &gt;2.5 only? 
A better option for checking for a file not found might be to check the content type: u = urllib2.urlopen(newlink) if u.headers['content-type'] != 'text/html': urlretrieve(newlink, direc + '\\' + newlink[-4 -len:]) numsuccess += 1 else: numfailed += 1 and you can check for an image of size 503 bytes by looking at the content size header so it would look like this u = urllib2.urlopen(newlink) if u.headers['content-type'] != 'text/html' and u.headers.get('content-length') != 503: urlretrieve(newlink, direc + '\\' + newlink[-4 -len:]) numsuccess += 1 else: numfailed += 1 depending on how advanced you want to get, you can avoid having to download each image twice by using a HEAD request to check if it is a valid image before downloading it. Edit: Apparently imgur doesn't always send content-length, so I changed u.headers['content-length'] to u.headers.get('content-length')
+1 for requests. It's really fantastic.
It's cool to have a module called Delorean but pretty weird to have a class called that.
Not sure why you're being down voted, this is the problem I have with TDD. I like to write top-down and you really can't do tests first with that.
If you run python in the console, it shouldn't give any errors. ex: C:\Users\jake&gt;python Python 2.7.2 (default, Jun 12 2011, 15:08:59) [MSC v.1500 32 bit (Intel)] 32 Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; import pygame &gt;&gt;&gt; or do you mean autocompletion in Aptana?
This actually looks a bit dull on the first sight but proves to be a valuable lesson once you are over the first basic koans. I am running it right now and already have learned a bit or two just because you never think about it. Heads up. Extra tip: Run python -m pyinotify -e IN_MODIFY -c 'python contemplate_koans.py' ./koans in a separate shell so you don't manually run the test script by hand.
This is what I've suggested, indeed. 
I had essentially the same reaction. TDD is snake-oil at worst, and a marginal increase in efficiency at best. But this style leads to little atoms of knowledge in a good way.
I agree with you that I can not see a large application being built from the bottom using TDD. But on the other hand, tests are generally good (necessary!) and for certain modules you think first about the design of your code. Therefore, I think there is a certain subset of programs in which TDD makes a lot of sense. I think you should (for your own) take the best from both worlds and make compromises.
If you're going to be passive aggressive, you should at least be correct. :) In Python 2, in which I did my testing, you can't compare a date with None. But only if you write correct code: &gt;&gt;&gt; from datetime import date &gt;&gt;&gt; date(2012, 1, 1) &lt; None Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; TypeError: can't compare datetime.date to NoneType 
Thanks for the Python 3 example. You get a TypeError in Python 2. Which is what I said in the original article. :) 
Easier said than done, but you could also upgrade to Python 3.2+ and not worry about it anymore. http://docs.python.org/dev/whatsnew/3.2.html#pep-3147-pyc-repository-directories
BTW is it supposed to auto-locate a Python installation? Didn't for me. Also it reported a success when every package failed to update due to lack of permission.
glad to be useful :)
It works there, only for 2.7. It does not work for IDLE 3.3 and or Aptana/Eclipse. I type: &gt;&gt;&gt; import pygame Traceback (most recent call last): File "&lt;pyshell#0&gt;", line 1, in &lt;module&gt; import pygame ImportError: No module named 'pygame Where the aptana version just shows me the path of the file. 
Okay, that is where it is confusing. I keep getting "guides" and other things I googled and it keeps saying I need to add the interpreter. Such as follows: http://stackoverflow.com/questions/10305076/how-to-set-up-pygame-with-eclipse I know I need to add the Library as well but I am not sure which way how I am supposed to do?
perfect, thank you! 
Overloaded operators aren't mentioned in the article, so I don't understand where that's coming from. There's a discussion in the comments about it. Concerning a type check, you have a point. But: * "One reaction to a TypeError is to do a type check" - you get a type error, so maybe you should check on type. And I realize it won't work in Python 3. * people may be coming from other statically typed languages that work this way. * I have to discuss the wrong ways somehow. But I could've reversed the order and then contrast with the wrong ways. [edit out passive aggressive reference, that was someone else. my apologies] 
It does not auto-locate python installation, and you can see the log of the installation by clicking View Log. In a future version, I'll show the log after every pip install / update.
Holy cow! http://docs.python.org/2/using/cmdline.html#envvar-PYTHONDONTWRITEBYTECODE That's going to be handy I think.
Not sure if pyc repositories would solve that, what op refers to is pyc conflicts between git branches not python versions.
Yeah I saw the log. My issue was that is that it reported a success when all the packages failed. Might be worth having it verify installations.
Really cool that this exists. Date and time is a pain in the ass in Python. It'd be nice if we could get this stuff in to the main datetime module at some point. Does Python3k suppor this stuff better?
Date and time being a pain is not specific to Python, it's pretty much all programming languages. It's an extremely simple concept, but with a lot of gotchas that have to be paid attention to. Personally, I think the natural language aspect of the methods in Delorean look pretty cool.
http://wiki.python.org/moin/PythonGameLibraries
Thanks. I'll need to check how this can be done reliably. If it is too complex, I'll just show the log to the user.
Does pip have a return value that tells you when there's an error? Might be as simple as checking that. BTW, [here's](http://www.reddit.com/r/Python/comments/1785dd/ipython_notebook_workshop_report_one_of_the/c839fns) that comment I was talking about :)
Thanks. I'm glad you see the need for an all-in-one installer on Windows.
How does top down development work? I've always worked from the bottom layers up. I just can't imagine having a sizable codebase that I can't even run until I code up the lower level dependencies. Or do you mock the lower levels to produce code that runs? If you keep a large un-run and un-tested codebase prior to building the lower levels, how is debugging?
I just let the installer do all that. I do find the pygame folder. Where is it that I am supposed to put it in?
I think you're a bit confused; packages are seldom installed via IDE, while it is possible, it is not often done so since it can be quite buggy (I know it is in PyCharm). So I suggest you just install it with `pip` or `easy_install`. 
for the iPad , Pythonista - https://itunes.apple.com/us/app/pythonista/id528579881?mt=8 
I'm not sure if I understand the difference between .flatten() and .ravel(). Is the example just bad? Because I just stared at it for a minutes and they look exactly the same.
Would love to see some of the more difficult natural language cases handled (ie first/last day of month/year).
then there's first/last weekday of month/quarter/year
Hmmm... how would that be done? I believe I do have easy-install. 
I agree. `delorean.DateTime` or something would have made more sense.
It's worse than that, it is completely devoid of description as to what it does. This appears to be a common issue in Python. I was discussing a topic and used the terminology "pickling", as is done in Python, which immediately confused him. This was rather quickly resolved by using the common terminology of "object serialization". He then made the rather poignant statement that pickling is a one way process. Cucumbers become pickles and cabbage becomes kimchi, but never the other way around. The term "pickling" does nothing to further the idea that this is a standard way of performing object serialization in the language and is rather misleading as a metaphor. But I digress, slightly...
Two more articles with some more information: http://www.pcworld.idg.com.au/article/452907/python_gets_big_data_boost_from_darpa/ http://gigaom.com/2013/02/05/darpa-puts-3m-into-startup-pushing-big-data-in-python/
I remember that BT protocol based live broadcast system introduced during 2004 EuroCup and gained massive popularity in Asia. http://en.wikipedia.org/wiki/P2PTV PPTV aka PPLive was called Synacast at that time. http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=28397370 The PPLive p2p network reached 1M concurrent online users in 2005 for live broadcasting Hunan TV new year show. http://download.pptv.com/en/about.html It's almost 9 years ago ... now I feel *really* old
It probably wont be stdlibs are too slow moving and need to less cut and cry with their assumptions. datetime module intended to be an island of relative sanity - Tim Peters 
I will try to get that in next release thanks for the feedback!
I got the reference, but even in your description it isn't the first sentence. So I have to know that: 1. DeLorean is a car company. 2. The car company makes a model known as the DMC-12. 3. This specific car is in a movie. 4. This specific movie deals with time travel. 5. Therefore the module deals with time travel. Or is it cars? Movies? By golly, that is the definition of **clarity**. 
It's kinda strange that a site about Python is using PHP.
This was excellent! 
How are you displaying the code in your terminal? Vim? Emacs? Look into syntax highlighting for whatever tool you're using.
Something like [bpython](http://bpython-interpreter.org/screenshots/)?
It's not even hinted at in any way in the example, but x.flatten() returns a copy of x flattened to one dimension, and .ravel() returns an object that lets you view x as if it were flattened.
http://ipython.org/ is what I use. Combinted with spyder2 http://code.google.com/p/spyderlib/ I am quite productive.
Can I get a copy of that script you wrote to fall in love? I think it may come in handy
I realize that my words may be taken as a little denigrating. Sorry about that. Thanks for writing module this and making it available. :)
Something like: xcode_project --sources stuff/*.m stuff/*.mm --resources res/*.xib res/*.png -- build/ ProjectName to generate Xcode projects on the command line. 
Okay so I figured it out and I used easy_install... now I get this error: Traceback (most recent call last): File "/Users/Name/Documents/Aptana Studio 3 Workspace/ProgrammingArcadeGame/PyPackage/Test.py", line 7, in &lt;module&gt; import pygame File "/Library/Python/2.6/site-packages/pygame-1.9.1release-py2.6-macosx-10.6-universal.egg/pygame/__init__.py", line 95, in &lt;module&gt; from pygame.base import * ImportError: dlopen(/Library/Python/2.6/site-packages/pygame-1.9.1release-py2.6-macosx-10.6-universal.egg/pygame/base.so, 2): Symbol not found: _SDL_EnableUNICODE Referenced from: /Library/Python/2.6/site-packages/pygame-1.9.1release-py2.6-macosx-10.6-universal.egg/pygame/base.so Expected in: flat namespace in /Library/Python/2.6/site-packages/pygame-1.9.1release-py2.6-macosx-10.6-universal.egg/pygame/base.so 
How do we install pip? This is nothing but headaches. 
Slightly off topic, but thanks for that! Looks puuurretty awesome!
You honestly need to learn to Google stuff. There are over 6 million results for "how to install pip". The installation is as simple as it gets. Also, if this is giving you headaches, consider quitting programming. 
Really? I get the programming stuff. And I have been googling things all weekend long and getting on forums trying to figure out how to install PyGame on this IDE. My bad for not knowing everything! My bad for being such a fucking beginner and trying to teach myself! What you did it all by yourself? My bad for not knowing command line! Who are you to tell me to consider quitting programming?? Damn. Some help you turned out to be. Also, notice how I figured out easy_install. My bad for not understanding everything! Also yeah I did find some results: http://www.pip-installer.org/en/latest/installing.html Like this? Yeah I kind of got I had to use virtualenv or whatever but there are problems there as it is. I got stuck! The terminal doesn't find the virtualenv command even after I stall virtualenv with pip (so I need to install something with something I do not have installed?!). I did not get the expected results. Maybe you should consider quitting giving advice. 
Sorry I tried to help. Have a nice day. 
Sure thing caballero. 
A editor that does syntax highlighting?
You can write a program in Python using `termcolor` which would change the color of `class`, `methods`, etc. You can use Regular Expressions to find that out and use `termcolor` to get colored ouput. Something like this http://jayrambhia.wordpress.com/2012/06/12/ansii-color-formatting-for-output-in-terminal-using-python/
For those not in the know, bpython is _the_ REPL to use unless you need the parallell/notepad tools of IPython. Every virtualenv I create, bpython goes in first.
&gt; Every virtualenv I create, bpython goes in first. Yeah, because the author won't fix [this bug](https://bitbucket.org/bobf/bpython/issue/7/get-aware-of-virtualenv) :/
Why link to slashdot instead of http://www.itworld.com/big-data/340570/python-gets-big-data-boost-darpa ?
1. this was posted by Peter Wang himself either here or over in r/programming not too long ago 2. why would you link to A SCREENSHOT instead of /. itself?
My apologies I should have linked to thw original source...I had not seen the original post 
From page 2: This is paying for the development of Blaze, Numba and Bokeh. Blaze is a replacement Numpy. As far as I can tell, it's part compiler, part data backend (giving you more flexibility with large and / or sparse data sets). (I don't think it's really a compiler, but it lets you process stuff in an IO-efficient way). Bokeh is a new visualization system. Numba is an LLVM compiler. 
I can't figure out if Continuum Analytics will develop Blaze for both Python 2.x and Python 3.x. I tend to use Python 3.x most of the time.
Indeed... is there any reason why this wasn't implemented? It would actually make exploitation of this flaw much more difficult. The attacker would have to hide code in files which will be installed on the system (which means that it could be detected more easily afterwards) and it wouldn't run with the installer rights. 
Oh, I was imagining a different meaning for "network".
Why?
because
There’s only one valid answer to this question: Because.
Because.
its also on the first page of the documentation but the usage page was linked here. 
Because *this is what PyPy is for*. PyPy != Python. Quote Wikipedia: "PyPy aims to provide a common translation and support framework for producing implementations of dynamic languages, emphasizing a clean separation between language specification and implementation aspects." Now, the PyPy project puts that framework to use to provide an implementation of Python. But that's merely the first language implementation PyPy has been used for. Much like how the JVM is being used for languages that aren't Java. The name, unfortunately, adds to the confusion, and it's because the project started out as a Python implementation, and grew into something bigger.
http://tratt.net/laurie/tech_articles/articles/fast_enough_vms_in_fast_enough_time
technically, yes, but very few people seem to know that.
Python is also super popular in DC and at media companies world wide. 
And what are we supposed to discuss?
I said, war, huh Good God, y'all What is it good for.... Funding the development of python, in the areas of large data processing, and visualization apparently.
So... this is a thing now. Let me know when the RPython Javascript interpreter rears its ugly head. *sigh* EDIT: So it appears my comment wasn't very popular. Let me explain my sentiment. I don't necessarily think things like this are bad per-se. I just see this as a rising trend of wasted effort in open source. Another obscure version of a popular language to muddy the water, and rpython has mad it possibly easier to do just that. Look, these guys can spend their time however they want. If they did it as a toy project, then great. Good times. Hope they had fun. I just wish more efforts could be directed at the mainline implementations of these languages. Clearly these guys have some coding chops. Maybe they could have spent the last 10 months helping improve ruby itself. Maybe not. Sometimes the alternative implementations scratch an itch, like jruby or jython, where you want to embed your favorite scripting language into java or .NET. However, in this case, it appears to serve no such purpose. 
I think someone is already working on one. Also, I think rpython has/had a javascript target.
It isn't working for me. After I approve access to github, I'm just redirected back to the index page
The top voted code is wrong &gt;&gt;&gt; contiguous([None]) True
No problem. If you're looking to use JavaScript for front-end development at all, you might also be interested in [her post on "A Baseline for Front-End Developers"](http://rmurphey.com/blog/2012/04/12/a-baseline-for-front-end-developers/). Also, in case you don't already, come on over to /r/dailyprogrammer and practice.
Ok. Nice. But chain() is basically [] + [] and count() is basically range(), or do I understand this the wrong way? I personally wanted to see groupby(), which I find both the most interesting and confusing function. But anyways, nice article. Thank you. PS: There's a small typo 'Relax. The tier in itertools stands for iterable, ...' :)
I submit a solution (for me) in [pyshop](http://pypi.python.org/pypi/pyshop) [here](https://github.com/mardiros/pyshop/commit/ffadb0bcdef1e385884571670210cfd6ba351784#pyshop/helpers/download.py). Let me share what i have done. As I mirror every package requested by pip using requests, I can validate the certificate between the pyshop server and pypi. You have to trust the route to the pyshop, instead of pypi.python.org. Ok, it's not perfect because the real problem is in tools pip, easy_install, but you can easilly trust your infra than the internet. 
And you have to trust the pyshop... but great work!
The itertools module's power comes from creating iterators which allow for lazy evaluation. [] + [] and range require evaluating all entries at once and knowing the termination point. from itertools import * from time import sleep from random import randint, seed def expensive_function(n): memory_hog = range( 10**6 ) print 'Doing expensive work.' #computations sleep(.5) return (n, n % 2, memory_hog) seed('Python Power!') big_iter = chain( xrange(10), count(randint(100, 200)) ) i = 0 for (n, m, h) in imap(expensive_function, big_iter): if m == 1: i += 1 print n if i &gt; 10: break This allows to evaluate a) only what we need and b) only when we need it. 
so ruby can scale
&gt; But chain() is basically [] + [] and count() is basically range(), or do I understand this the wrong way? They are not the same by a long shot. If they were, there would be no need for this module. All of the functions in `itertools` work with *iterables*, where an iterable in Python is defined as something that is capable of returning its items one at a time, i.e. it supports `iter()` and `next()`. Strings, lists, tuples, dicts, sets, and files are all examples of iterables, but they are not the only kind. Nowhere in the definition of an iterable does it require that the entire sequence be in memory or that items are accessible via an index. That's the definition of a *sequence*, which is one type of iterable, but an iterable can also include things that only compute their values as needed (lazily), such as generators. For example, `count(1)` returns an iterable that counts up from 1 to infinity. This is very handy when you want to count things. There is no way to replicate this behavior with `range()`, which requires that you provide a stop value. (And in 2.x, if you gave some large value for the stop value, it would result in a huge waste of memory as the entire list of values would be generated, regardless of how many are needed. `xrange()` in 2.x and `range()` in 3.x lazily evaluate so this waste doesn't happen, but they still require a stop value and can't represent an infinite sequence.) The whole idea of an infinite sequence only makes sense in the context of lazy evaluation. Generators (and in particular, generator expressions) and lazy evaluation open up a whole world of elegant solutions to all kinds of problems.
I'm trying to get into Python and doing so with some computational simulation. Things that matter to me include Unicode source code. I much prefer writing out: λ = c / f Than lambda = c / f or wavelength = c / f Python 3 is pretty much what I want, but most interesting packages make it hard to use Python 3.
Basically anything computer related has been funded by war efforts, usually directly by the military organizations themselves.
RPython is the name for the restricted language the the PyPy translator toolkit can compile to native code.
I had thought functools.partial was going to be unpickleable but it isn't! Thanks, this was the answer I needed
the author himself has said 'for fun', and also to mess with people's heads. Having looked into Pypy a bit myself, it's super cool and I commend dude for his efforts. 
Pretty clever isn't it? The returned objects [implement](http://hg.python.org/cpython/file/bd8afb90ebf2/Modules/_functoolsmodule.c#l203) the [magic methods `__reduce__` and `__setstate__`](http://docs.python.org/3/library/pickle.html#pickling-class-instances) which allow them to be pickled correctly. 
TIL we're all part of the military industrial complex
Fail. Thanks for the report! Should be fixed now.
They gotta sort through all that data they collect on everyone some how.
Honest question: how hard would it be to design RRuby, i.e. a Ruby version of RPython? That is, is the infrastructure bound around the assumption that the implementation language will be RPython, or is it sufficiently decoupled that other implementation languages are possible? I mean, in a sense it doesn't matter as long as RPython is "good enough", but people tend to take a language more seriously when its interpreter and/or compiler is implemented in itself, so this would be a possible downside to porting a language to use PyPy.
Look at http://rubini.us/ for a starting point.
In your code blocks: &gt;&gt;&gt; is rendering as &amp;gt;&amp;gt;&amp;gt;
&gt; -- most big-data analysts will probably not be programmers. If they can learn an easy language, they won't have to rely on an external software development group to complete their analysis -- What kind of data analyst isn't a programmer these days? It seems like being a mathematician or statistics analyst would greatly help in learning languages like R or Haskell due to the common terms and concepts required to yield anything meaningful.
proactively
Time for an update to the Computer History Graphing Project ! http://freecode.com/projects/comp-hist (Didn't we just update this last week? Is there a variation of Moore's Law for the release of programming languages/implementations?)
You are right. As I write it, I trust my code. But if you want to be more safe, maybe users should download it from github.com . I am not going to build a pypi alternative ;)
need a RRuby for that
Its pretty simple actually the way we handle it is each week we turn in a log based on what work we have done, just a simple write up like you might give an employer. We can provide links to things such as pull requests and the like. At the end of the semester the prof reads everything we wrote and gives us a grade based on our contributions and work. He is looking for effort above all. We get 70% on code, and 15% juts for turning the logs in. We also do either write ups on lectures given in class, or we can give lectures ourselves to make up the remaining 15% of points needed to get 100% in the class.
You are exactly correct on what is best. I mostly made this to find people in our immediate community who were working on projects I found interesting (based on the suggestions I listed in my post). Everything you said I agree with 100%, my trouble was finding small projects that need devs can be tricky. I went through hundreds of github projects and couldn't find what I was looking for. this was my solution to finding these projects and also getting a chance to work with some awesome people ^_^
Okay, this is going to be fixed in the next update: https://github.com/pypa/pip/pull/791
Not reasonable currently because a) Pip shouldn't really be pulling in dependencies, and b) requests doesn't support &lt;2.6, which is a requirement currently. Personally I'd like it if requests got included in the stdlib for the next 3.x version. However, https://github.com/pypa/pip/pull/791
yeah, but the 10% is the interesting part (I think it's more than 10, but I'm too lazy to check). PyPy is implemented 100% in RPython (there are a few pieces of C and asm, but they're really for stupid stuff, so 0.01% or so, nothing core)
Ah, okay; I was looking for a remark towards that end on the web site that you linked but I didn't see any such thing. It might have been better for you to link to the Wikipedia page, that does mention it. Nonetheless, 1. Python is 100% implemented in RPython, if I am not mistaken, which is unlike Rubinius which has a C/C++ core. 2. I wasn't so much interested in whether Ruby can be implemented in Ruby then in whether a different front-end language could be used instead of RPython for PyPy in the general case of porting an arbitrary dynamic language to PyPy. 3. It is nice to see whether a language not only can be used to implement self in practice (as obviously in theory all Turing-complete languages can) but additionally whether the best compiler and/or interpreter are written in that language. I have no idea how Rubinius compares to Topaz in that respect.
Thank you for that link. Good read-up although i have to admit I did not grasp all of it. I never figured that PyPy/Rpython is at the moment unique in their approach.
Thanks for your feedback. `dump` and `dumps` methods are appropriate for this idea. I will implement it when I have time.
2 things on groupby.... 1. you need to pass a function that will return a key for each element in the the iterable. 2. YOUR INPUT MUST BE SORTED BY THAT KEY FUNCTION number 2 is what I forget often. So say you had a list of dictionaries {url:...,response_time:...} that you wanted to group by the url def by_url(item): return item["url"] sorted_value = sorted(value, key=by_url) for url, data in itertools.groupby(sorted_value, by_url): print url, [item["response_time"] for item in data]
Whoa, I feel a bit proud to see that my name suggestion (`wdb`) was adopted.
Brofist !
If you like a past OS project, most devs are receptive to a student looking for a project. They usually have tons of ideas and just need someone to do them. So you can usually just email them too. 
Breaking news, last week
Great news! By the way, this issue has been assigned CVE-2013-1629. 
True but it's quite a *familiar* format and devs can convert into JSON and can then save in DB for future process.
Thanks. tumblr has some weird quirks.
You write: &gt; With list comprehensions, that might look something like: [(i / 0.25) + 10 for i in range(100)] &gt;&gt;&gt; [10.0, 10.25, 10.5, 10.75, ...] That is incorrect. Your list comprehension evaluates to `[10.0, 14.0, 18.0, 22.0, ..., 406.0]`. You probably mean: [(i * 0.25) + 10 for i in range(100)] Also, if you just paste `itertools.count(10, 0.25)` you just get a count object whose .next() method returns consecutive values. Another option then the `ifilter`you mention to get a comparable (*) output back as the list comprehension, is to use the following helper function (a trick I learned from [this great article](http://linuxgazette.net/100/pramode.html)): def firstn(g, n): """return the first N values yielded by a generator""" for i in range(n): yield g.next() print list(firstn(itertools.count(10, 0.25), 100)) &gt;&gt;&gt; [10, 10.25, 10.5, 10.75, 11.0, 11.25, ...] (*) Note that the first number is an int and not a float
You are correct -- I'm a dolt. Thank you for pointing that out!
Could anyone compare it with [Werkzeug debugger][1]? [1]: http://werkzeug.pocoo.org/docs/debug/
Personally, I first moved away from Apache because I believed all the hype about NGINX+uWSGI/Gunicorn being ‘n’ times faster than Apache+mod_wsgi. I found out, with a little tweaking and load testing that Apache is just as fast – but it's ‘out of the box’ settings can't compete with NGINX. I've stayed with NGINX+uWSGI for another reason entirely, it's **very** easy to configure and maintain – NGINX's configuration files are an absolute dream compared to Apache's. I can't stand working with Apache any more.
* http://www.python.org/dev/peps/pep-0263/ * http://docs.python.org/2/reference/lexical_analysis.html#keywords * http://docs.python.org/2/reference/expressions.html#lambda * https://github.com/ehamberg/vim-cute-python
The werkzeug debugger does not support stepping.
what kind of neurons are u looking at? any particular disease model or system? do u use brian
Incognito mode humm
Just finished reading it, it's good. I've been using Django for over two years and I actually learned a few cool things.
Woo hoo! If you implement those, I'll definitely start using this :)
Most Big Data analysis today is done using something running on the JVM like Hadoop. I'm curious to see how this pans out and am hoping the results are open sourced.
The most voted feature is by far [web2py support](http://youtrack.jetbrains.com/issues/PY?q=sort+by%3A+Votes) and it is still lacking. They are missing lots of potential clients. Good product but bad management decisions.
How could they get so many? I wasn't able to order one. They're all BO. ...Oh, that's why! 
I don't think the price is particularly expensive unless you're coding solely as a hobby, in which case there are alternatives. But I do agree that their licensing scheme is somewhat annoying. It does makes sense from their perspective, but annoying to me as an end-user. Buy a license for $99 and receive all updates for a year. Renew each year for $59. But if you wait to renew, say until the next big release 18 months after you buy, your renew is backdated to your anniversary. So you're only getting 6 months for that renewal. It makes sense because you've paid the core cost and are paying for a year's worth of updates valued at $59. You can't "skip" updates, so they have to backdate it. This prevents someone from taking advantage of the system by waiting 1 year and 11 months and then renewing, effectively getting two years of updates for the price of one. I'm not really sure what a better system is. 
Just out of curiosity, are there reasons other than achieving speedup that someone would use multiprocessing?
I probably wouldn't have bought it if it was cheaper and/or the licensing scheme was less insane, as I am quite happy with Vim, but I think they're losing money by doing it this way. I think they would earn more money if they sold the personal licenses cheaper.
ITT: someone who'll never buy a product, telling the manufacturer how to price it.
I purchased my first one under a sale and the upgrade costs are worth it to me. I don't find their licensing scheme insane at all really.
I'll use what Guido uses. :)
Glad code analysis has been worked on, I thought it was pretty flaky.
And what is that?
I bought this during the Doomsday sale and, after using it for a bit, I've decided that it's definitely worth the $99 price tag, even though I just write code in my free time. Seriously, you cannot believe how much more enjoyable and accessible programming becomes, especially when you're working with Flask or Django (templates!). **Edit:** The new UI theme, Darcula, looks awesome.
Same here. I almost cried.
google it
emacs
So this is basically an online `pep8` service with tie-in to github?
This shows the amount of non-garbage-collected memory. Clicking the indicator triggers the garbage collection; if you don't click it, the memory will be collected automatically later. Note that the memory meter is hidden by default in PyCharm, because there's absolutely no reason to worry about it.
IMO its just because: 1. distutils2 is a mess 2. it has no documentation whatsoever I tried to port one of my projects to distutils2 and wasted about a day and decided to waste no more time. Alas I'm not saying that distutils2 is bad library, it just is not so intuitive and not documented. 
right-o
It was on sale for $25 over the holidays. Worth it even at full price. 
How do i get the old icons?
Hey Jeff I enjoyed the article. I have a few criticisms: **1.** &gt; A difference of 0.5 * 10^6 seconds shouldn't matter Firstl off, I assume you meant 10^-6 :) I feel highlighting the minuscule times isn't really a faithful way to make your argument. I think this proves that computers do things very quickly rather than exceptions aren't time-expensive. I expect that an exception in *any* language has an order-of-magnitude similar cost. Humans can't make sense of absolute time comparisons at 3GHz clock speeds, but percentages make sense at any scale. Your first data point shows the try/catch block is about 70% slower. That number is so high only because comparatively little else is being done (just an increment), but I still felt you glossed over the large relative difference in figures. **2.** &gt; # Check if the object is printable... if isinstance(some_object, str): print(some_object) (Sorry, it's hard to quote a code block). I agree with the point you're trying to make here, but the look-before-you-leap code example is really contrived. **3.** Not really a big deal, but for a post about writing cleaner Python, it was really mostly about the time cost of exceptions. I wouldn't mind learning a few things about nice/clean/clever ways to use exceptions with common problems ("exception patterns", if you will).
Glad to know I wasn't the only one. 
I just think it might be a better business model to offer the personal licenses for less. Also, I'm not telling them how to price it, I'm just suggesting something which might actually benefit them (or it might not, they have more data than I).
The price of a personal license of PyCharm is already extremely low compared to the prices of almost all other commercial IDEs (check out the price of Visual Studio, for example), and only slightly above the price of commercial text editors (Sublime Text and TextMate back when it was commercial). We don't have any plans of reducing it even further.
There are a TON of data analysts who are not programmers. That's why dashboarding and BI drill-down tools are a massive business. But those tools are not going to hold up to the advanced analytics needs of truly "big data" environments and challenges. 
&gt; Most Big Data analysis today is done using something running on the JVM like Hadoop Not true. If you look at industry journals and reports, it's a very heterogenous/fragmented market right now. Hadoop has the most momentum as a disruptive technology on the ETL/data warehouse side, but the stuff it's disrupting are all handling big data, too. And that's just looking at business data analytics, like clickstreams and customer transactions. If you expand the scope of "Big Data analysis" to include scientific data and GIS data and timeseries data, Hadoop doesn't even show up as a major player. &gt; I'm curious to see how this pans out and am hoping the results are open sourced. An explicit goal of the XDATA program is to produce an open source analytics stack that everyone can use. Continuum's Xdata projects ([Blaze](http://blaze.pydata.org), [Numba](http://numba.pydata.org), [Bokeh](https://github.com/ContinuumIO/bokeh)) are all open-source.
No thanks.
How are the days of the trial counted? Is it 30 days of actual usage (not counting single hours of course) or do the days count even if I don't use it for a week?
Interesting article, good feedback. I have a hard time conceiving of an object that *isn't* printable and gives a TypeError. And I thought 'print' already triggered a str conversion anyway! Exceptions are good, but an example using, say, division by zero or int(), would have been more clear for demonstrating exceptions you want to catch. I also think that in some cases wrapping something in a 'try..except' and handling the except cases is harder to read than the application of a few judicious guard clauses (look before you leap). It depends. 
They count even if you don't use it. After 30 days it'll still work, but every 30 minutes Pycharm will close itself.
I see what you did there.
Too bad, won't even try it then :/
fuuuuuuuuu.... no.......... don't say that. Is it really true?
I took another look at the post and wholeheartedly agree with points 2 and 3. I've updated it with two more useful, if lesser known, exception idioms. Let me know what you think. To your first point, the *comparative* cost of exceptions is quite low in Python. While exceptions across all languages may be within an order of magnitude runtime of each other (although I doubt it), other languages are *many* orders of magnitude faster than Python in exception-less code. Telling me exceptions in C++ add about the same run time overhead as in Python would horrify me if I were asked to add them to my CUDA based matrix calculations. Computers are indeed fast, but fast is a relative term. 
web2py isn't nearly as widely used as the frameworks they implemented first. If you're running a business, of course you'd want to work on the larger userbases first...
Back in the day I used Wing. With it, you had limited activations, you could only run it on one platform (Linux, windows, or mac), you got a crippled "personal" version. I use different a os when at home and work, so that was really shitty. Pycharm's fees and licenses are a dream compared with the possibilities some companies come up with. 
Can you name any other software that counts trial days the way you seem to think they should be counted?
I do code solely as a hobby (and I realize that I may not be the intended audience).
Could you not write the following example that was given in the article def print_object(some_object): # Check if the object is printable... if isinstance(some_object, str): print(some_object) elif isinstance(some_object, dict): print(some_object) elif isinstance(some_object, list): print(some_object) # 97 elifs later... else: print("unprintable object") like this ? def print_object(some_object): # Check if the object is printable... if hasattr(some_object, __str__): print(some_object)
If you're going to click it, do it before you run something, not during. You don't want to trigger gc while doing other things 
Why would you want to run your code in parallel if you don't want speedup?
It's a little light on details. I just rewrote some code to use multiprocessing and even though there weren't many good examples out there it was a pretty simple (well, "simple") two-step process: * Convert the inner part of a `for obj in objs: ...` loop into a function that acts on `obj` and returns a result. Basically, I rewrote that section in a functional style so I could use a list comprehension or `map` to get results out. This function needs to live in its own module (require for multiprocessing) and it can take other arguments besides `obj`, but then you need to bake those into it with `functools.partial` before you pass it into `map`. * Change the `list(map(f, ...))` call into a `po.map(f, ...)` call where `po` is a `multiprocessing.Pool` object. By being able to switch back and forth, you can debug the `f` properly when it fails: when using `multiprocessing`, the location of exceptions isn't obvious. And now my code runs almost twice as fast on my laptop. It's really slick and by using `map` you don't have to learn any new design patterns.
I find it hard to be impressed by a factor of 3 slowdown, especially when it is expressed as an overhead in "seconds" rather than a percentage, almost as if the goal was to spin the overhead as being better than it actually is.
echo's going to be so mad
No, but that doesn't make it any better. And actually I didn't see much software that has a trial period since I use Linux.
how does one "support" flask? it's python isn't it. 
While that was the case in the past, Wingware has much more [liberal license](http://wingware.com/wingide/license) than it used to, right on par with PyCharm. (posted by a happy PyCharm user who knows how long Wingware has supported the Python community)
By far. But it is slow as shit. Running it on a quad-core machine with 16GB of RAM, it still slows down to a crawl. 
Wait, is that a joke? Does it save before closing itself? That sounds like an amazing way to get people swearing at you.
You can't edit. So there's nothing to save. It just allows you to open and navigate projects. 
I run it on my Eee PC and it runs fine, even w/multiple projects open at once (&amp; Chrome). You can also turn on "power save mode". The new Vagrant support reminds me of Id releases: makes me want to get a better, more powerful machine.
I'm having a similar experience with IntelliJ and I'm dreading ever needing to go back to Visual Studio because it's that much better. I don't know what they're feeding their programmers, but I hope they keep it up.
Usually their support takes the form of support in the IDE for cross checking the relevant files for you. Yes, it's all Python, but there's a bunch of other things to keep synchronized in your config files to the code, etc. and their integration support helps keep that all lined up for you. I can't vouch for their flask support, but I've seen IntelliJ do this for Spring. 
Port 1984, hah.
The "Power Save Mode" turns it into pretty much nothing more than a text editor. 
It's not the same thing; some objects may have only \_\_repr\_\_ or \_\_unicode\_\_. A better substitute would be if isinstance(some_object, (str, dict, list, ...)): print(some_object) But really, just use exceptions. Also catch on print statements, because they can fail. For example if writing unicode to a console, or if writing to a file.
Awesome! I've been using the EAP release for some time as I'm too impatient to wait for new stuff. ;) PyCharm is by far the best dev related software purchase I've ever made.
A slow one, at that.
Weird, my laptop is a quadcore with 12G and no SSD. The only thing that could be called slow is "find in files", and that's only as slow as can be expected.
They support Jinja2 markup code completion. You can create a Flask application on the fly (pre-populated config files). There's lots of convenient code-completion and analysis that helps building a Flask app. You could have Googled this yourself and stumbled upon this concise video that showcases some of the main features, though: http://blog.jetbrains.com/pycharm/2012/08/flask-in-pycharm-26-eap/
The interpreter can be on a remote machine just fine. For the files, you'll need a local copy, but PyCharm will take care of synchronizing it with the remote machine automatically.
Actually all of the features of WebStorm are either bundled into PyCharm or (like node.js) can be downloaded as free plugins from the plugin manager.
Yes, of course it does save.
Well, if you're only interested in finding a justification not to try PyCharm, there's no reason to come up with so fancy excuses - you can simply ignore its existence. It's your productivity that is at stake, after all, not someone else's.
Could you please submit us a CPU snapshot? http://devnet.jetbrains.com/docs/DOC-1212
Most of the time, it's ensuring the template language is supported, checking for specific usage patterns e.g. that decorators are correctly set up, routes don't conflict, …, support for automatic generation of framework structures (config files, views, etc...) and dedicated shortcuts (code completion, intentions). To sum up, Flask is Python, but Flask uses Python in a specific way. If the editor understand what that way is and how elements relate to one another in that way, it can provide additional help compared to the core "Python baseline".
Not to mention web2py strays far from a strict and direct python library or framework (see e.g. imports handling and builtins extensions), so its support can't trivially be added on top of normal Python support.
hmm, I have no troubles to run it on both an Apple MBP 13" and a MBA 13" - and the Air only has 2 G of ram and isn't the fastest thing under the sun. Ok, startup takes a moment, but after that it works fine and fluid. But might be more due to both my machines having SSDs than due to the CPU performance or amount of ram. I often found that you can get away with low ram much easier with an SSD to page on.
Same here. Although I don't think there's any bad blood. The web2py community would love to have support and be pycharm costumers. If there really was bad blood that wouldn't be the case.
Last time I did that you guys tagged it immediately as "cannot reproduce"...
I bought a few jetbrain products in their big promotion a bit back. It's pretty nice stuff - but I think it ought to come with a warning for anyone running 64 bit linux that the fonts are going to look horrid. I've given up on a fix for this. (Please don't recommend any of the normal fixes found in a quick google search - I've tried them already.)
PyCharm devs, thanks for the great work, please work on making it faster, the features are all top-notch but there's that milisecond slowness compared to light text editors that drives me insane over time.
That's the thing. PyCharm runs fine on my shitty E-350 laptop. It also ran slightly better than this on my Phenom II X4 desktop that I had before. It seems that with PyCharm, its performance = 1/sysperformance
Well they could have gone to 2.7; but I was pleasantly surprised nonetheless that they did the Right Thing (tm). I think the native integration of unicode is what makes the most sense for LibreOffice - ironically the one thing that traps up other applications. 
I taught high performance python at PyCon last year, the talk + slides + PDF might give you other ideas? I cover profiling, cython, parallelpython and others (at my later EuroSciPy version I also demo IPython cluster &amp; PiCloud): http://ianozsvald.com/2012/03/18/high-performance-python-1-from-pycon-2012-slides-video-src/
Separate threads for user interface and backend (no "hanging"of the application when performing resource-intensive work), separate threads per client on a server, etc etc etc.
The article states that the best way to create a concatenated string from 0 to 19 (e.g. "012..1819")is: print "".join([str(n) for n in range(20)]) Personally I also prefer the functional programming approach print ''.join(map(str, range(20))) although there are [compelling arguments to make both pro and contra this approach](http://stackoverflow.com/a/6407222). 
The first comparison would be that it's free and open source which, while not technical, has a bearing on whether I'm actually able to use it seriously for my work and play. This does mean that I have not properly evaluated PyCharm (though I know they have a free trial). For code completion KDevelop has a complex system called DUChain which allows some very clever code analysis things. For example, in order to evaluate what type a function argument can take, it keeps track of all the times that function is called and look at the types that were passed in. [This is covered here](http://scummos.blogspot.co.uk/2011/09/kdev-python-argument-type-guessing.html). In an upcoming version it will also support Python3's [function type annotations](http://www.python.org/dev/peps/pep-3107/). Also, it does a very nice job of tracking potential types for homo- and heterogeneous lists on an element-by-element basis as [seen here](http://scummos.blogspot.co.uk/2011/06/basic-list-content-type-support-in.html). The majority of the work for Python support is done by one guy (though there's a number of people working on KDevelop itself) and it's amazing the capabilities it has given that. There is of course a lot of overlap between the functionality of the two IDEs and I'm very impressed by the excellent capabilities of PyCharm and there are areas where PyCharm is clearly ahead. Overall however, the free and open source nature of kdev-python (that's the name of the Python plugin for KDevelop) along with it doing everything I need (and fast too) makes it the perfect tool for me. I also use KDevelop for all my C++ work so it's nice to have everything in one place. I'd recommend trying it out and see what you think. Maybe you wont like it but during my searches I couldn't find a comparable free IDE (and paid ones couldn't justify the price IMO).
I'm sure they'll have a fun time with everyone complaining about broken code, but I'm glad they made the switch (for the greater good!), so good luck to 'em.
You can write macros in Python. I assume this means that every python macro written previously is now broken. 
I'm not sure it's the right thing given that it's a language used by third party macros. My company writes macros for clients to interoperate with the rest of our software. This will require keeping our clients below 4.0, rewriting their macros, or providing a plugin for backwards compatibility. This is the python macro equivalent of updating their document format and not including a backward compatibility filter for the old version. I would much rather they offer both side by side (at least for a while), rather than breaking all python macros in one release. 
This statement is based on what data? Have looked at the relative traffic on the two mailing lists?
Link?
&gt; Web2py is written in Python. You still can't read what people write can you? &gt; In fact it uses Python even for templates while Django and Flask do not. The IDEA platform makes it easy to trivial to support new file types (of course the type itself — and related inspections and intentions — have to be defined, but the new file itself is not an issue). And dedicated templating languages is a concern across languages so — again — it has good support in the platform. web2py requires non-standard interpretation of the programming language itself, and even worse requires doing so only in a restricted subset of all files in the language (so it's not possible to have a big toggle "abnormal python mode"). That is the major issue I was describing. That web2py is written in python is irrelevant, cpython is written in C, that doesn't mean C editors are a good fit for python code. The problem is that web2py semantics diverge from standard python semantics.
http://pyvideo.org/video/880/stop-writing-classes
I wish I had snagged IDEA then too. I would use that going forward then. Ah well...
Wing still needs X-11 on OSX to run. No thanks.
There are some seriously complicated macros our there, and I suspect most of the people using python are doing so because what they're doing is significant enough that StarBasic gets cumbersome. I know my company will have trouble if we try to make the switch. 
http://www.reddit.com/r/Python/comments/r0rpy/stop_writing_classes/
When you don't have functionality of some sort that you want, you can make your own using a python macro.
~70% overhead isn't what I would call negligible...
I still would like to try PyCharm but currently I'm not working full-time on Python projects. In fact I would use PyCharm only once or twice a week for less than a few hours. That's not enough time to get used to it and to see whether it's a good tool for me.
Pretty good. I learned my OO off of Java in the late 90's when Object was the father, son and holy ghost. Seems like programming lately has been getting away from the whole "have a beautifully defined hierarchy so in 10 years your code is easy to expand" thing.
I'm not yet clear from the documentation how this functions. Should my working copy be on my Windows dev machine, or the Linux dev machine? Would it be better to just figure out how to run PyCharm on the Linux machine, maybe with an XServer on Windows?
 foo += 'ooo' # This is bad, instead you should do: foo = ''.join([foo, 'ooo']) Really? I disagree. From a readability standpoint, #1 is much clearer, the semantic is perfectly clear. #2 on the other hand, looks obfuscated. #2 might be faster, but readability is also important.
Why should _every_ macro be broken? The changes to Python3 shouldn't be that big that everything just breaks, or do they rely heavy on stuff that changed? (I never built macros in OO)
It depends on your JVM version, flags and whether your index directory is on a NFS drive (and other things). Use Java 7, enable Tiered Compilation, give it 256m permgen and about 2G of RAM, and I haven't seen any problems on the 4 different machines I've used it on.
Your working copy should be on your local machine, PyCharm will sync it with your remote Linux box and allow you to run/debug your code remotely. See http://www.jetbrains.com/pycharm/quickstart/configuring_interpreter.html#remote_ssh for more info.
Your working copy needs to be on your Windows machine; PyCharm can automatically synchronize the changes to your Linux machine. Running PyCharm over a remote X server will definitely give a much worse user experience compared to using PyCharm's deployment support to sync changes from Windows to Linux.
You are doing all these for..... speed up. The "hanging" you are talking about is all about speed up. Seriously, if you don't want speed up, or there is no speed up when you do multi-threading, you shouldn't use multithreading at all. Multiprocessing is not meant for logical or categorical division of codes, it is for speeding up.
That's not the same thing. If you perform backend work in a different thread, then the user interface remains responsive even while the work is being performed. Even if you may loose speed this way, the user will still be more happy because he can continue using the application while the backend work is being performed. This would not be possible without threading. &gt; Multiprocessing is not meant for logical or categorical division of codes, it is for speeding up. This statement is simply untrue. Multiprocessing and threading were around long before multi-processor systems for end users even became a thing.
If someone ported all of R's stats libraries over to Python, would there still be a compelling reason to use R? Personally I find R the language to be ugly and unintuitive, but R the stats libraries are unparalleled for now. edit: I think a good case could be made for R's package management over Python's.
This is quite true, my philosophy on the best way to write maintainable Python is to just build code using primitive python constructs and pure modules as the default namespace. Building enormous class hierarchies with multiple inheritance and "OO for the sake of OO" tends to produce very brittle unmaintainable code in the long run. Although, unfortunately because of the design of CPython sometimes we are forced to use classes since the number, sequence, and iterator protocols are all attached to the object system. 
Granted that web2py does some things differently than other Python frameworks but what you say is wrong. web2py DOES NOT requires non-standard interpretation of code. Your analogy with cpython is incorrect, you make it look like web2py itself is written in python but applications are not. web2py applications are written using python syntax and they are interpreted using the python interpreter we all love. There is no custom interpreter in web2py. It is not true, as you say, that web2py semantics diverge from standard python semantics. Only workflow of execution is different. web2py application files can be introspected fine by IDEA and many existing IDEs for python. It would be easy for pyCharm to add support for web2py is they wanted too. The fact you do not know how to do it, does not mean web2py developers and users do not know.
&gt; Granted that web2py does some things differently than other Python frameworks but what you say is wrong. Oh come on. It doesn't just do things differently than other Python frameworks, it does things differently than pretty much all of the Python corpus. &gt; web2py itself is written in python but applications are not. That's quite a good way to put it. &gt; web2py DOES NOT requires non-standard interpretation of code. Of course it does, web2py application code is exec'd and requires interpretation in web2py's non-standard namespace and context. That's very much a non-standard interpretation of python code. &gt; web2py applications are written using python syntax and they are interpreted using the python interpreter we all love. There is no custom interpreter in web2py. None of this has any relation to my claims. &gt; web2py application files can be introspected fine by IDEA and many existing IDEs for python. That is quite obviously incorrect, or it'd work out of the box in PyCharm which is little more than an IntelliJ IDEA core bundled with support plugin for a specific ecosystem (Python's) &gt; The fact you do not know how to do it, does not mean web2py developers and users do not know. So you're claiming to already know how to add web2py support to PyCharm? In that case, why not do it and instantly fix the issue in question?
wdb uses WebSockets to communicate with your app, and thus when an exception is raised (or you explicitly set a breakpoint using `wdb.set_trace()`) your original request is still live and can be inspected/modified/etc. Because of this, you can use wdb.set_trace(), inspect something, then hit F8 (or ".c" in the console) and continue the rest of the request (which if it was an exception, would show you your 500 error page. wdb also has autocompletion and other improvements over the werkzeug debugger (not to mention it can be used on other non-werkzeug projections like Pyramid since it's at the WSGI layer).
The nice thing is that you can have the same source code work on 2.6 and 3.3 with only a small amount of work (e.g. `from __future__ import unicode_literals`) - so while there might be some initial pain, you shouldn't have to worry about maintaining different macro codebases for LibO 3.x and 4.x. At least, in theory :-)
I was planning on doing the same sort of series converting from MATLAB to python as I solve the assignments for Coursera's scientific computing course. Great work!
Wait. You mean that the equivalent of VBA macros in Office are Python macros in Libreoffice? If that's the case, Libreoffice just jumped in my interest.
Why would you want to catch the error when you print unicode to a console? Instead I'd want the exception to bubble up, see it, and I'd fix the code. 
Incredibly useful, thanks!
What would you fix?
Ya! Go forth and write stuff in... not Visual Basic! 
because multi cores I guess
You have to transfer it back and forth between client and server to do that. If that's faster for you than reading it from the session data (data you will have to read anyway when you get a hit) then you should have another look at you DB setup. The only reason to store anything in a cookie other than a session ID, is incompetence.
Matlab is pretty high quality with amazing performance. R on the other hand just has some nice tools. Everything I have ever needed to do in R, I ended up doing instead in Python. Given Python was how I was massaging and building the data, it is certainly easy not to go across languages.
That is the price of doing business. 2.x has been deprecated a long time. At some point they have to update. They made a very good choice by updating so far.
The problem is that classes have been cargo-culted as part of OOP, which conflates a lot of issues which are actually independent. But there's a useful core to the idea of objects and they are nearly an essential tool in Python. Rather than going into fuzzy stuff about how objects model the world or have 17 required traits or whatever, I like to understand objects by comparison with functions and programs. A nice function has a one entry point, one exit point and a namespace which is private (not externally accessible) and transient (its local state doesn't keep evolving after the line is finished running). An object is much the same, but it has a public namespace which persists, and as many entry and exit points as it has methods. In short: a function contracts to do a job and go away, leaving a result. An object contracts to stick around and do things as asked until it is dismissed. Both are just ways to get state evolving according to rules implementing some contract, with an end goal of getting predictably useful behavior from a programmable device. Compared with functions, objects are more like runs of a program. As a program runs, the associated internal state evolves according to its code, which also mediates its communications with the system outside it (e.g. signals, streams, sockets, exit code, etc.). In the same way, an object's members evolve according to its methods, which also mediate its communications with the outside world. The only real differences between an object and a program are that an object's interaction with its environment is carried out through method calls. Method calls are nicely structured, can pass any number and types of arguments without complexities like IPC, sockets, serialization and deserialization. And between method calls, the object does absolutely nothing. And all methods have access to a common namespace regardless of the context they're called from. So when you write a program by instantiating some objects and interleaving calls to their methods, this is basically building a program out of sub-programs. Transient functions are cleaner because they *fully* encapsulate their internal state. There is no reasonable way to get to this internal state and hopefully no way to twiddle it from the outside. Unlike objects, functions don't have to guarantee behavior over all combinations and timings of method calls. But there are definitely some situations where it's useful to have more flexibility than a function but less awkwardness and overhead than a separate program. Some (but not all) can be equally well handled by functions which return other functions closing over state, or by object-alikes like a dict with some functions in it, a family of functions taking handles to a struct representing shared state, a function internally dispatching to other functions which work in shared state, etc. And objects aren't really different from coroutines either. But in Python, they are one of our best tools for API design when a plain function is not suitable, for a few examples: * many workloads require some expensive setup which should be reused (by repeated invocations of one method OR across methods), enough to make it stupid to just do the setup every time. * closely related - many workloads require a whole bunch of redundant config parameters. It's nicer to let the user provide these once up front than to force them to keep passing in the whole suite for every call. * idiomatic case for objects: state needs to be shared by handlers for several kinds of events which may occur asynchronously, like in many GUI and network apps. I say nothing about classes, prototypes, mechanisms for data hiding, inheritance, interfaces, polymorphism, overloading, subtyping, type checking... because these are separate issues in my mind.
You don't even have to write a script on your own: python -m pyinotify -e IN_MODIFY -c $CMD /$DIR Note the leading / before $DIR.
did you consider flake8.py?
Not really. Instead of integrating pylint/pyflakes, we plan to make sure that PyCharm's built-in inspections can cover all the checks that those tools provide.
I don't think anyone is arguing against using classes and OOP concepts in Python. The point is to not use classes when a function will do the job just as well with fewer lines of code. A year ago I would have thought this topic was pretty odd/silly. But recently I worked w a programmer who was new to Python and made everything a class, just like in the video.
Trying to clarify: if your impulse is to raise an exception by hand from inside an except clause, you can often just let the natural exception raise instead, save some code and still see the issue in the traceback and fix that print statement properly so that it won't happen again unless it's really an exceptional condition. 
Not sure I know what you mean. I love the way python works. :-)
I am curious why you think that list comprehensions are not functional (enough, particularly in this case).
There is a very good general-purpose tool called [watchdog](http://packages.python.org/watchdog/) for this and many similar tasks. edit: also, it's cross-platform
R's package management is really slick for its purposes, but I'm extremely happy with the combination of pip, virtualenv, and tox for managing project dependencies.
You misunderstood me. The idea is to catch a UnicodeDecodeError (iirc) and print a repr instead.
Around every print? I would rather not write print() calls which do that, but it's your code.
Forward porting from 2.6 to 3.3 can be moderately painful (somewhat different story for 2.7 and very different story for backporting from 3.x to 2.7)
Not sure why you were downvoted -- that's exactly my experience. Many Java programs over the last decade have gotten better at not "feeling like Java" -- that is, not having a slow, poorly-rendered UI that has no connection to the way any other programs look or act. I tried PyCharm, and it feels like Java straight out of 1998. 
&gt;But recently I worked w a programmer who was new to Python and made everything a class, just like in the video would you think that maybe the reason for that is because... &gt; classes have been cargo-culted as part of OOP, which conflates a lot of issues which are actually independent. just pointing out that I think you agree with the parent more than you disagree. :) 
Oh I know, I just would rather be proficient with python rather than MATLAB as when I graduate I won't have a student license for MATLAB anymore whereas python will still be free
Noticed this too. The entire interface on Linux looks like Swing straight out of 1998. Who are they expecting to use this? I can't imagine who would willingly use a 32-bit computer for development these days. There are so many tools, including Python, that assume a 64-bit environment so they don't run into a 2 or 4 GB memory cap. 
i use mixins occasionally, but my guess would be there is a danger in that mixins are really multiple inheritance. Multiple inheritance can be complex, almost so complex that you may not understand what it's doing. And not understanding the code you are maintaining is not a good thing. So maybe mixins are a potentially big gun you can shoot yourself in the foot with, and so you should use caution when using them. just speculating.
Sounds like you want to use logging, not print.
I'm sure the numpy/scipy devs would be interested to hear about anything you can do in MATLAB that you can't do in Python or anything in MATLAB that you find more performant.
Well I doubt I'm doing anything interesting enough right now to warrant their interest but its good to know the devs are interested to here others input. 
I think people just started realizing that there's a big difference between beautiful *necessary* code and a shitload of unnecessary/complicated pre-mature framework that will probably have to be changed *anyway* because you can never account for the shit marketing ends up wanting. 
I think being a DSL probably works to R's advantage with respect to packaging. Pip and virtualenv handle most of my needs well, but the amount of effort it can take to get those up and working is going to be an immediate put-off to someone who's going to be using python purely as a statistical computing platform. For R, you download an executable, read in your data and go. For Python, even if you install a distro like Anaconda or EPD, you may have to do some work installing compilers and getting path variables set up right. There are a lot of places where things can get frustrating and cause someone to give up on Python and just go back to R. And I say this as someone who loves Python and doing stats in Python. But I've already sunk the costs into getting my environment set up.
I've made it a goal to get proficient with doing data analysis Matlab, R, and Python over the next couple of years. It seems these are the major tools for people on a budget (not many small companies can afford SAS, etc). You mention a concern about a Matlab license. Are you aware of GNU Octave? It's an open source tool that's very close to Matlab in that the code you write in one will mostly work in the other. It has a nice GUI called QTOctave (for Linux, I'm not sure about Windows). As an example, Andrew Ng's Machine Learning Coursera course uses Octave.
well, there are lots of software that limits how many times the program is opened on trial. close enough.
Real use cases, not just "any" data structure.
That's probably from people who migrated from C# and Java where you pretty much have to do that in order to get any code to run.
I've also used incron / incrontab which is very practical for that. Dunno why it's not standard in distros. 
We're trying to standardise a set of core packages to improve this situation - the Scipy Stack includes numpy, scipy, matplotlib, ipython, pandas, sympy &amp; nose. The idea, which seems to be working, is that all scientific Python distros will include those packages, so users are working from a common base. It doesn't solve all the problems, especially with packaging, but hopefully it gives us a kind of platform to build on.
I totally support his anti-class message, but looking at his OAuth fork, even a quick glance shows a couple of huge red flags. He goes too far in "simplifying," and compromises the goals of the library: First, he's removed all the OAuth 1.0a code. This is dumb. Generally speaking, outside of slightly more sophisticated session handling, OAuth 1.0 is more secure than OAuth 2. There are tons of sites that still use OAuth 1.0, and I don't see that changing – in many ways, it's superior to the "upgrade" (something he seems not to get, given his characterization of this choice as, "Implements a later version of the spec"). Ironically, one of the biggest ways OAuth 1.0 is better is that it's far, far simpler than OAuth 2.0. Next, he exchanged httplib2 for urllib2. The stdlib's HTTP handling does not verify SSL certs, and so it totally inappropriate for use – any use, but in particular, it's a horrible choice for a security library. It would be extremely easy for an unsophisticated user of his library to not understand how they're exposing their data and their users' data (he's supporting three-legged OAuth – so he's risking downstream devs' end users' tokens directly) because of his poor choice. It's particularly galling how proud he is of setting this huge, gaping trap in his README: "Completely removed all non-stdlib dependencies (goodbye httplib2, you won't be missed!)." Simplicity is great, but in this case, he's lessened the security and thus utility of his *security library*. I get the sense from these choices (and his tongue in cheek "F-OAuth" naming) that he sees OAuth not a tool that lets non-experts write secure APIs but instead as just that thing that gets in his way when he tries to hit Twitter's API. 
so... your philosophy is don't ever use classes unless forced to?
I agree: for a casual "end user" or "scripter" who just wants to run analysis scripts (particularly with C dependencies) Python packaging is not that great without a little extra help, and R is especially great. But in general, I think Python's package system and tools are really nice for developers developing. So, probably not a huge deal if you are good enough to write robust R scripts, but a bigger deal for those you are distributing scripts to.
OAuth 1.0a and OAuth 2 are completely unrelated except for the naming.
You're both great speakers and incredibly helpful. 
No. I'll write classes when it's called for, and not when they aren't. I realize that the video doesn't label this as a hard and fast rule, but I'm not a fan of the over-the-top title. I think that it's better to think of classes as namespaces in many cases. Do you need further namespace organization beyond a module inside of a module? Then use a class. There are other occasions that call for a class, but they aren't necessary for everything as in another language that I can think of. Now, can we please stop using these types of absolutist titles for our papers and videos (I'm not blaming you, OP)? Next week: The Python and the Silver Bullet.
Or handle the encoding earlier rather than leaving the operation to fail when it reaches stdout, so that I have to put exception handlers everywhere. My idea (like faasen's, I think) was that exceptions are best for exceptional conditions: so according to me, if your code is doing something which frequently raises MyElbowHurtsError, the first thing to try is to make it stop doing that. So it won't be necessary to catch, unless there is a reason it's bad for the program to just stop. I prefer this because it saves me a lot of ceremony and code vs. wrapping every operation which might possibly throw an exception. I think about each case, but for most of them I'd only reraise to get a traceback anyway, I can just skip the extra step. But I'll happily concede it's a matter of taste: for example, the Go community seems to embrace checking most every result and take quite a dim view of Python exceptions.
I find it's often easier to write complex decorators this way (with actual work in the `__init__` of course).
&gt;can we please stop using these types of absolutist titles It's been tried. The net result is that nobody pays attention to the content.
Thanks for the reply. People with a better grip on security issues than I have are actively discussing potential solutions on the catalog-sig now.
There's only two things that "break" between 2.6 and 3.3, and that's print is now a function not a builtin, and strings are unicode by default. Your macros should not be using print, and the unicode thing is only going to matter if you have code that assumes byte-level access (in which case it was always wrong, because the correct way to handle that in 2.x is still the correct way in 3.x) Have you even tried running your macros? Chances are no changes are even necessary. There's also 2to3 to migrate. 
I don't believe you have any idea about the scientific python ecosystem. The performance is more or less the same.
I almost down voted you because of this: &gt; There are rumors that Microsoft is planning to port Office to linux. What would be even greater is if they supported python out of the box too. Do you know how much cognitive dissonance that set off in my head? I'm still reeling, MS Office on Linux? Sounds almost as real as the prince of Nigeria's plea for help, unfortunately.
I see similarities between the R-Python language debate to the Perl-Python one (I get the sense that Python won over Perl but maybe because I trawl Python threads more often). R's language is terse and there are many ways to do things (though numpy and pandas in Python provide a lot of redundant functionality also). While not easy on the eyes, R's language lets you accomplish a lot of analysis in a few lines while you're taking the same number of lines in Python just importing modules. That's one of the advantages of being a DSL; the context is already established so you don't have to type `math.sqrt` or `numpy.sqrt`, and instead just type `sqrt` (try suggesting importing the name `sqrt` into the global namespace on any Python forum and watch everyone explode over how "explicit is better than implicit"). R does have a sophisticated namespace system, but the convention for explicitness makes it easier to organize a large, heterogeneous collection of libraries and tools in Python, while paying a penalty for shorter scripts. R additionally uses the statistician's idiom, which includes data table operations, 1-indexing, array syntax (which is not as concise as Matlab's, but better than Python's), etc. Overall installation experience and package manager as mentioned elsewhere in these comments is indispensable and for this reason I still recommend R over Python to my experimentalist colleagues. Python installation is a nightmare on OS X for one thing, and yes I know about EPD but when downloading libraries, R can include builds of sqlite, iGraph, and so on, while in Python this has to installed/built outside of it and the appropriate Python version has to be passed as arguments, as well as the location of libraries installed from source or from Macports, and so on. On Linux you don't have these problems, but experimentalists in many scientific fields who still use statistics on large data sets - e.g., geophysics, earth sciences - aren't likely to be working on a Linux system primarily and invoke virtual machines only when held at gunpoint. Python is making a lot of progress with scikits, statsmodels, and the pandas library (which I absolutely adore), making it a useable language when integrating some amount of statistical data analysis into your workflow, but I find that I still end up writing a lot of additional functions to reproduce the functionality of R. Part of it is that R lets you get away with a lot of implicitness; it's effectively weakly typed in the sense that for most functions objects will be converted between data frame and matrix, factor and integer, factor and strings, ad infinitum. Makes for possibly dangerous code, but it's extremely convenient. Functions are also vectorized on a collection of string values rather than characters (despite the designation of "character" data type); the operations are more verbose for manipulating a single sequence of characters but it reflects the use of strings in statistical analysis more often for labeling (hence the `stringsAsFactors=TRUE` convention). Also, the choice of the Python community was to try to overtake the domain of IDL and Matlab initially. With regards to plotting, the matplotlib libraries are loosely structured around Matlab's syntax, which I find much more verbose than R's high-level syntax, even considering just its base graphics. Lattice and ggplot2 are killer features for R at the moment, though I'm keeping an eye out on Python's bokeh. Anyway, that's just like, my opinion, man. I'm grateful for the progress made in Python with regards to pandas and other libraries, but history matters and R/S has 30 years of it. Whether it's a wise use of the open source's community's collective resources to reimpliment something already functional and available (e.g., via Rpy2), but there are numerous cases of such successful efforts - and we can talk about what we should be doing as a community, but the beauty of the open-source contribution model is that individuals who want to see it happen have the freedom to work on it. We can only support or not support the effort. I expect this debate to continue for a while as bits of statistical functionality is gradually added to Python; possibly getting more heated as Python's capabilities grow closer to R over time (numpy aside, pandas was a seriously huge step). [In a separate thread](http://www.reddit.com/r/learnprogramming/comments/106qnd/why_dont_real_programmers_like_coding_in_r/c6b5iyx), I commented on R from a language perspective. There was a deleted thread on Stackoverflow but is captured [here](http://www.stackprinter.com/export?service=stackoverflow&amp;question=1177019&amp;printer=false&amp;linktohome=true). Postscript - the R developers and community have a focus on supporting practitioners; despite its warts they don't break backward compatibility (e.g., Python 3) which is absolutely essential for domains where writing and using code is a means to an end. 
I am somewhat drunkenly happy to report that my 7 lines to do an ensemble kalman filter analysis in numpy are much more elegant than my 9 lines to do the same in R. It's friday evening... Anyways, python has at least one thing over R: interfaces to other scientific packages and great interfaces to fortran and C/C++. This is indispensable for engineers and scientists. Case in point: ADCIRC models use python scripts to do EnKF on weather models. Not R, python. And now time for more beer. 
If order does not matter, then that implies that you should not be using lists/tuples to store these items in the first place, but sets: &gt;&gt;&gt; set(['foo', 42, -1]) == set([-1, 'foo', 42]) True However, this will also treat repeated values as a single value: &gt;&gt;&gt; set(['foo', 42, -1]) == set([-1, 'foo', 42, 42]) True If you need to support that then I suppose you could use a Counter to compare them: &gt;&gt;&gt; from collections import Counter &gt;&gt;&gt; Counter(['foo', 42, -1]) == Counter([-1, 'foo', 42]) True &gt;&gt;&gt; Counter(['foo', 42, -1]) == Counter([-1, 'foo', 42, 42]) False 
I do not think there is any bad blood. Quite the contrary. In the web2py community we like pyCharm and many people use it. We just wish they had better support as we'd be happy to help them if they ask. 
Are there any Django tutorials for python 3?
I feel like giving you a hug just for your detailed comments. Also, time to get unlimited texting and harass friends. 
Awesome work man, I really appreciate it. Any chance you could post an example main() method where you do a sample text? Thanks! Edit: I don't understand what var process is. EDIT2.0: Figured it out, very cool. Now to troll friends haha. 
Can anyone recommend some web hosts that are setup for Django? Seems like 99% of web hosts push php on you but don't offer a django stack, which is retarded because I think PHP is shit language when python is available. Why is php so prevalent over django?!
I had written the reverse function 9 years ago, around the time I got hooked with Python :) http://code.activestate.com/recipes/267662-table-indentation/?in=user-2591466
Ah, so it's 64bit Linux only that sucks? That explains it then. I have tried PyCharm more than once and always give up in frustration from the slow, fugly Swing UI. Given all the raves about it I was wondering if it's something specific on Linux, Ubuntu or my environment that make it so bad.
http://h3manth.com/content/sms-android-using-python
I think there is a page on the wiki about hosting services for django. I'm on my phone so I can't check though. edit : here it is https://code.djangoproject.com/wiki/DjangoFriendlyWebHosts
Crazy. "SMS Gateway" was a totally unknown concept to me before this. Very cool.
some programmers go functional because it scale better on multi cores, while OOP scale with higher frequencies.
&gt; but history matters and R/S has 30 years of it And that's why it has a number of "annoying" features that make writing code when used to other programming languages hell: * Accessing non existing elements in dataframes and lists yields an empty character vector (why?), leading to overcomplication of code to catch all possible corner cases. * Error reporting is *atrocious* in most libraries, where you get completely uninformative errors most of the time and you have to resort to step through execution to understand what's going on . * Debugging isn't easy at all: traceback() is uninformative, debug() and friends are primitive at best. * Performance in native code is terrible: I spent two weeks reimplementing an algorithm from Bioconductor using pandas/scipy/numpy because I was utterly fed up that it would take tens of minutes to do its job (the rewritten part, also using multiprocessing, is 80% faster). That said, I have to use it where there's no alternative (and rpy2 is an excellent piece of software in these conditions), but I'm hoping to get off it as soon as possible.
I'm not sure if [heroku](http://www.heroku.com/) is doing django yet or not, but as others mention virtual servers are always nice as well :)
The [statsmodels](https://github.com/statsmodels/statsmodels) project is a start on that goal (stats with pandas DataFrames).
NumPy contributor here. We're all well aware that NumPy isn't the most performant package under the sun. Part of this is unavoidable due to the way the Python interpreter works: you can't, from a library, redefine the way an expression is evaluated by the interpreter, e.g. in order to obviate the need for 5 separate, cache-unfriendly loops and 4 temporary arrays (plus the result, makes 5) in the evaluation `(a + b) / (c + d) * (e ** f)`. It's also difficult to do much in the way of optimization while maintaining full generality, but the project is certainly open to pull requests in that regard. There are lots of projects that try to get around NumPy's (and Python's) shortcomings in this regard. Perhaps the most exciting as of late is [Numba](https://github.com/numba/numba), which lets you decorate a Python function with metadata about the array arguments and JIT-compiles efficient LLVM code.
As a NumPy user for the past 7 years and a contributor for the past few, I can tell you quite plainly that it isn't.
&gt; I don't know what your basis for this is. The base numerical libraries in Matlab and Numpy all wrap similar C/Fortran functions. This comes up in every thread about this stuff, and there's always someone talking out of their ass. The truth is that NumPy wraps the same functions *for linear algebra*, and even that's not strictly true: unless you've bought Enthought Python Distribution or have a license for the Intel Math Kernel Library and have built Python against it yourself (or you've been very choosy about your BLAS implementation and have compiled and optimized it yourself), you're probably not getting the kind of performance you'd see from Matlab. For elementwise computations (which will form the bulk of your program in many use cases), everything in NumPy is home-rolled, templated code. It works pretty well but it doesn't use any SIMD instructions, nor can it leverage multiple cores. And while it's certainly faster than writing the loops in Python yourself, that isn't saying much. &gt; Matlab's JIT gives it a boost in some cases where code can't be vectorized. No, for loops in Matlab suck about as hard as, or worse than, for loops in Python. The place that Matlab's JIT shines is precisely vectorized expressions. When you say `(a + b) / (c + d) * (f + g)` in Matlab, it can go ahead and analyze that expression to perform the computation in a *single loop* that will make the processor cache very happy, and it will use multiple cores to do it. NumPy will chug along in a single thread computing `(a + b)`, storing it in a temporary array, *then* compute `(c + d)`, store that in a temporary array, then compute their quotient, store that in a temporary array, etc. You can't get around this from NumPy because that's just the way the Python interpreter works and there's no changing it, which is the entire reason that Numba exists.
Wow thanks! Totally forgot about sets. And thanks for the note about counters, didn't know that existed.
Haha. No kidding. The script starts at line 117 and, with a lot of fluff in between, ends at 184.
Any idea how to do this in the UK? Which of the UK SMS gateways will work? Are they free?
I'm not 100% sure that it's just that but I have a 32 bit machine and a 64 bit machine running identical Fedora setups. PyCharm looks good on the 32 bit machine and looks like ass on the 64 bit no matter what I do. The comments I've read from the jetbrain developers are that it's a java problem and they will not/cannot fix it. They also say that the open jdk is even worse - I've never bothered switching away from Oracle's jdk to check. Netbeans has a similar issue so I don't doubt it -but I wont use it. With KDevelop having Python support now I think that's where I'll be working.
From the comments I've seen it seems that the majority of their customers are on Mac or Win and they don't have these problems. I don't like to do my dev work (I use the term work loosely I'm not a paid dev, I do it for fun) on anything but Linux - so I'm just looking at other tools that don't have this issue.
Agreed completely. A VPS provides much more freedom than any web host, honestly. And it's not even difficult to get a free VPS (though don't expect more than 512 MB of RAM). Not quite sure what you mean with "help with the networking side". Networking should work just fine by default. The only thing that requires work on the user's part would be setting up firewall rules or configuring server options and the like.
please use string interpolation ("string % vars", or better, "string.format(vars)") instead of concatenating strings like that
Jimmy, put down that jack Daniels Hard code that into the code, and send text to self every five minutes. Alcoholism cured, user altered. :-) On second thought it would remind the user of jack every five minutes. Back to the drawing board.
Github please :-) 
There's also numerous changes to the standard library, which can be used in macros, and changes to exception handling (forcing 'as' instead of allowing the older comma syntax). I'm not directly involved with the macro development, but I believe our macros use some of our other libraries, which make extensive use of the standard library. Most were written targeting 2.4 (which our clients used as recently as nine months ago thanks to RHEL 5), and some still have to support Jython (which only offers 2.5 syntax), so most of our libraries use comma syntax for exceptions. Currently we still target OpenOffice.org, but I try to keep tabs on on LibreOffice in case there's a compelling business case to switch. I'll talk to our macro devs about keeping this in mind as they move forward to minimize the pain when we do finally make the switch. 
Same in Germany. 0,20€ per message.
I wouldn't bother with any of those sms-gateways... Just snag a Twilio account and call it a day. 1c/SMS, nice and reliable API's. 
The hosting company I used would provide monitoring, firewall hardening, security testing etc for an additional fee. Basically everything but the code side. 
The first line of the description says the main goal of the function is to *alter* the user, which seems a little ambitious.
http://www.webfaction.com/?affiliate=adammckerlie (affiliate) provides 1 click Django installs. http://www.heroku.com/ can run Django apps and is easily scalable. https://gondor.io/ like heroku but with python apps in mind. 
Yes you can set up a django app on heroku or on amazon's elastic beanstalk. So if you want to set your django app up on a PaaS (the easiest way frankly) those are your options.
pretty slick. if I could figure out how to generate the profile from a buildout this would be awesome. 
Better solution: get an old Nokia phone, hook it up with a computer and send AT commands to it.
\#2 is 2 magnitudes slower on my Mac. I agree that #1 is clearer in this case. $ python -mtimeit -s"foo=''" "foo += 'ooo'" 10000000 loops, best of 3: 0.192 usec per loop $ python -mtimeit -s"foo=''" "foo = ''.join([foo, 'ooo'])" 100000 loops, best of 3: 20.6 usec per loop 
1 yes 2 yes 3 no, it calculates the md5sum of the hash, which can differ if the hash is in binary form, or if the ascii hexadecimal representation of the hash is loaded into the subsequent md5sum algorithm. 4 no, the contents of the Release file are not simply a hash of bash.deb. The file contains a listing of files and their checksums, only one of those listed files contains the entry for bash.deb among several other packages. 5 no 6 no 
Looks great and it's still cheap in the UK but it is four times the price. 4c/SMS. I think that's about 2.5p.
I use Django Stack on AWS: http://bitnami.org/stack/django#cloudImage
With IPython you get a nicely formatted traceback pointing you to the error. Also with joblib you can do that even if you use multiprocesses. In R you get the error but without debugging is often impossible to get where and why it occurred.
What is the advantage of this? Normally I use concatenations (eg "a"+"b"+str(0)) because it looks cleaner. 
Why is this a bad thing?
Thanks for sharing. Looking forward to using this for several different scripts I run daily / weekly.
I disagree. People should be writing more classes. :-/
&gt; Probably meant bit. Was a bit **confuses** there :) Probably meant confused. I chucked :)
&gt; Probably meant bit. Was a bit **confuses** there :) Probably meant Confucius. Was a bit virtuous tear :)
Really neat idea and I like, the only thing that I'd be concerned about is that it seems that you are saving (and possibly sending, though I don't know about the implementation of the login command) a password via Plaintext, but as you say, you shouldn't use that account for anything other than this purpose for security reasons. Pretty cool program though. 
Hmm. I'll keep that in mind. Thanks!
While i mostly agree with this post, there is one think i don't agree it: that the array syntax of matlab it better than the syntax of numpy. I'll argue that the numpy is better, mostly because matlab is missing the broadcasting feature, which makes array-code even more compact. Also i think that default operations of * and / are better chosen in numpy. While it is sometimes nice to have short linear operators, i use element-wise operations much more often. Especially i dislike / because its different meanings (solve vs lstsq). Also implementing algorithms, i think 0 bases indices are much less prone to 1-off errors. One big plus as an experimental researcher: i can write my whole stack in one language. 
PHP doesn't usually require shell access or long-running processes to use.
Thank you! This makes it far easier to find providers! 
I'm really new to programming in general and so are the friends I was cleaning this up for. So I just wrote it as simply as I could, with as much in-depth explanation for them as possible, because that's what I would have needed if I was in their shoes. Plus, I love leaving references to literature in parts of my code. 
Aren't you when you program? Google even tries to make sure [goats](http://stackoverflow.com/questions/13375357/proper-use-cases-for-android-usermanager-isuseragoat) aren't using their phones. 
Likewise! This was written as a novice programmer for two other novice programmers. So the style I used was tailored to making sure they understand it, rather than universality. I put it up here for other programmers who are new too, with the goal aimed at education. But thank you, now I have a standard to aim at!
I've done that, and it is better than using this SMTP approach, but still very unpleasant. It was fun to start a minicom terminal though, for retro sake.
Is there, or are there plans for polygon support?
I have succesfully used RoutoMessaging. They give you 10 free texts and have a very easy to use HTTP API. I used it to text me if a space became available in a class I wanted to take.
I'm also in agreement that the `*`,` /` operators in R/numpy are more suited for array transformations in data analysis applications, which is also where the usefulness of array broadcasting comes in. When translating algorithms heavy in linear algebra, Matlab's syntax still shines brightly (as it was also its original intended purpose). Also the 1-indexing is more appropriate here. Not sure where 0-indexing is more useful, if you have a reference I'd appreciate it (I know in the FFT/signal processing community they use it, but usually these libraries are wrapped/hidden from the higher-level interface provided by these scripting languages). The odd behavior of Python's `range` function reflects what's necessary to work with 0-indexed objects; to generate a sequence from 1 to 5, you have to write `range(1,6)` which is not very intuitive. This is one trivial example that comes to mind where I have trouble justifying the logic to new students. By "whole stack" if you mean writing GUIs and interfacing with web servers, these are just not in the interest of experimentalists I work with. Between data analysis and simulation (optimization routines for parameter estimation, ODE/PDE solvers for kinetic batch reactors), I haven't found Python to be up to par with R (for interactive data analysis, the original topic) or Matlab (for numerical solvers). Depending on the student I recommend R if they will be heavy on the analysis side, and Matlab if they might do some light simulation that doesn't require Fortran or C.
Since you're sending an email, you can use [email.MIME.text.MIMEText](http://docs.python.org/2.7/library/email.mime.html?highlight=mimetext#email.mime.text.MIMEText) to construct all the metadata you want. The code looks like [this](https://gist.github.com/rotated8/4746555).
http://www.smsmatrix.com/
processors. CPU are now frequency bound, so only way is to increase transistor count, thus meaning going multi thread.
Fair warning: I've used a method like this before, and had problems with the messages being silently flagged as spam, or arriving at inconsistent times. We were able to resolve the problem by getting a DNS entry, and adding the IP of the sending box in the domain SPF. Other services for smartphones, such as prowl (for iPhones) and Notify my Android work more consistantly, with much less delay. And they're typically only a one-time $5 charge.
Yes... I do. Using a % everywhere looks esoteric. 
well I was hoping you would use .format() since that's nicer 
iPython is absolutely amazing. Specially with the qt and notebook. I almost use it as a terminal replacement at this point. Specially the inline graphs are extremely useful for quick analysis of data.
What does that have anything to do with object oriented programming?
Simple syntax changes like `as` for exceptions can easily be done automatically - just throw 2to3 at it. In most cases, the developer time it takes is mostly clearing up the bytes/unicode distinction.
A rumour about it has been making the rounds of tech news sites recently. It does seem far fetched, but he didn't just make it up.
The sender is gmail. They have their DNS entries figured out and so on to not get treated as spam.
OOP is all about mutable states.
Just a guess, but maybe python isn't passing the path "as-is" to the underlying OS call or some other issue; the simpliest solution would probably be to just map the share to a virtual drive so you can access it via z:/whatever like it was a normal file. If you really can't do this, maybe check out Mark Hammond's python win32 package and use CreateFile &amp; co directly. You can then either just use WriteFile with the handle, or use python's msvcrt.open_osfhandle (http://docs.python.org/2/library/msvcrt.html#msvcrt.open_osfhandle) to create a C runtime fd and pass that to os.fdopen (http://docs.python.org/2/library/os.html#os.fdopen) to get a python file object back for the corresponding handle. Sorry I can't be of much help, since I don't have access to a Windows box to test any of this, but hopefully this info may be of help to you.
Primarily because there is already a learning curve associated with programming. I don't want them to try to run before they can walk. And as I said at multiple points in the comments of the code, and my previous reply, I'm new to this too. I don't know everything, and I appreciate you pointing it out. As I've said, I will now try and get programs to conform to this standard which I've only just been informed of. Beyond that this code is for me, and for them. I shared it with you guys because I thought it was really helpful for my programs which took a large chunk of time. You're more than welcome to take it for yourself and edit it to fit whatever standard you'd like though! 
the lib that you wrote (which of course does not suck). 
I believe that Pillow is scheduled for some sort of update announcement in March for Pycon 2013. Read that somewhere on some forum when looking for a compiled python 3 version. What are you trying to do that you need more than what PIL offers? 
I think he's referring to the fact that it can suck to install. Pillow has made this way way better though. 
In the python community I generally see that people use not enough classes.
If you like the inline graphs of the qt ipython interface you should try Gate One. You can run a regular ipython prompt from anywhere and output the graphs right there in your browser. Soon I'll be adding the ability to display things like graphs in a separate window/dialog so you can keep them visible while continuing to work.
That's neat. How does it compare to ipython's own notebook though?
You'll probably want to learn a bit about Pygame first. This chapter of this free book covers it: http://inventwithpython.com/pygame/chapter2.html
Will look Into it! http://imgur.com/QVQQWXa.jpg 
With gmail, you'll have your own other anti-spam issues if a script goes crazy on notifications. This particular script is hard-coded to use gmail, but it can be used with the local sendmail service on Linux as well, in which case you will need to be careful.
I'm guessing most people don't care, but the lack of true lambdas in python (ie allow statements and possibly multiple of them) - really makes playing with quick analysis functions on the command line much more irritating to me than in R. I can often times in R do a complex statement in one line because of this ability, but when I'm trying to do the same in python i end up having to drop back into a text editor to def out a named function, then call it. Many will claim this is a benefit as it forces me to be cleaner, but when I'm just doing exploration of a new dataset, it's the most irritating part I've found of trying to use python for data analysis - 90% of that code I don't really care about I just want to check some things real quick. Additionally I sometimes wish there were braces to do similar things on one line - editing a single line for loop (with an embedded if else) is easier to me than multi line with indenting 
You may want to make your symbol names more consistent in the future. Just a suggestion!
The problem though is that it's factually false. Would not fly on SO
One way is to install Sage. Granted, it is huge, and on Windows you basically need to run it inside a virtual machine, but it should come with all the goodies preinstalled.
&gt; I don't think I said anything different. And you're right, it of course depends what library you compile against. &gt;&gt; The base numerical libraries in Matlab and Numpy all wrap similar C/Fortran functions. It seems like you're implying that NumPy is wrapping the same sorts of things for the majority of its operation. Plenty of things done by a typical NumPy user will never touch `numpy.linalg`, which is the only part that touches (or can touch) libraries that might have anything in common with Matlab. And as I've said, Matlab can do things like loop fusion on vectorized expressions that NumPy couldn't even if we had limitless developer resources (nevermind that it already utilizes multiple cores, which NumPy could in theory do, but there'd be a lot of refactoring to necessary to multithread without being cockblocked by the $@#%ing GIL). &gt; It was that that sedaak's assertion that Python was not "serious" for performant computing is invalid. Unless you're a) using PyPy and can tolerate losing most of the scientific Python ecosystem with that decision, b) using Numba, numexpr, Theano, Cython, etc. or c) wrapping your own low-level C/C++/Fortran, it's absolutely true. Python is great glue, and the multitude of options for speeding things up is a testament to that, but if you're doing numerical heavy-lifting in pure Python (with or without the help of NumPy, in most cases) then you best not care about performance.
Let me clarify - by fluff in between, I meant double and triple new lines with no code.
Can you send a message appearing to come from a particular number? With skype my text messages appear to come from my phone (and from me), but a cheaper option would be good.
Spoiler alert! Big integer multiplication is a lie.
Very cool article. I clicked on all the banners!
The part describing the packaging system was very helpful. I have been needing to re-organize my first Python project, as it has grown quite a bit. Moving some related modules to a new directory cleaned things up nicely.
Yeah that's what I was after, ah well. I wonder if there's any sms service besides skype that allows you to spoof your own phone number
[sniffer](http://pypi.python.org/pypi/sniffer) is also really useful.
Nooooooo. You're gonna pay for that : Tyrion Lannister is actually a girl.
Thanks! I will definitely take a look, and read the chapter.
Nice post, your example of / % int division is slightly off. You forget to use the mod sign.
Twilio has a python library available as well: https://github.com/twilio/twilio-python Though it's basically just a REST API so you could write your own easily enough if you were so inclined.
RPython is a restricted subset of Python. Meaning it's a like Python but with less instructions. PyPy is written in RPython.
I usually use `ipython -i prepared_script_in_repository.py`, but, I suppose, profiles can be somewhat more convenient. On a remotely relevant note: what can be done when there's a need to introspect and experiment with various objects that are only available within some callbacks (especially from a networked library)? Though sometimes that is possible by embedding ipython (`import IPython; IPython.embed`) (or ipdb: `import ipdb; ipdb.set_trace()`), that is not always very convenient. Recursive object representation (over `o.__dict__` or `dir(o)`) is also useful, but can way too easily go too wrong.
I really like the concept. Another consideration would be contour changes relative to time. 
This would be extremely useful in estimating response times of emergency service personnel, especially volunteers (like myself) to their station or appliances/personal response to an incident or something else. Many thanks for posting your code! Will see what I can do to help improve it! 
Awesome! I have been wanting to look into using python and Google Maps APIs once I've finished my thesis and I've saved this for future reference!
Did you manage to fix it till some extent, if not completely?
If you are talking about how swing renders fonts on 64 bit linux - no.
No, when I really like an article, I don't know how to express my gratitude other than clicking all the banners. But you got a nice way of explaining things, that is very well tuned to the way i understand things. I literally skimmed through and got everything AND (and this is the most important bit) learned something new, so I'm looking forward to reading more. :)
( * o_o * ) Thanks a bunch! I was a bit scared my english sucked too much.
How so?
Yes, there is. This [issue](https://github.com/opentraveldata/geobases/issues/4) will give you details on that part.
This is a little similar to mapumental which is UK based. Cool project, thanks for sharing.
The function descriptions are ok, but I think the whole text before the code actually starts, in this case, should be in readme file instead in .py file.
I am new to this having dome the MIT 6.00x course online and only in the first week. I have completed it early and am just playing with language now, so any help would be great :)
This is really cool. I seriously should start playing with some apis to download data... Are there some resources I might read to get started? Also, upvote for Ubuntu! 
I guess the one winning argument for the web is absolutely zero install effort. If you want to "run" a web application, all you need to go is type the URL into your browser, and it loads more or less instantly. Even python with the perfect app store would require users to first find the application, then download it from the app store, and install, before they could do *anything* with it. Furthermore, web applications update automatically; the user doesn't need to do anything at all, not even confirm. Everyone has a web browser, and every web browser can google. The rest is automatic.
First of all, how many tiles do you have? Are you sure you're not optimizing something that doesn't need to be optimized? Then, why not go for a general case and just sort them back-to-front, like [here](https://github.com/adamlwgriffiths/PyGLy/blob/master/pygly/sort.py#L34) for example? You can even calculate the index array once. Or you can calculate it once explicitly, now that I think it.
That's the whole argument - to make a cython or equivalent environment with a front page that looks somewhat like this. https://chrome.google.com/webstore/category/home?utm_source=chrome-ntp-icon and you would have a set of python apps with a launcher looking like chrome. http://imgur.com/xsfNk47 The environment would maintain a virtual environment for each app - and it would automatically update applications. Web programming is trying to compete with desktop apps (single page apps with ajax calls) - why can't python compete with the browser (tk or pyqt apps that use requests to make it "web" enabled.)
If you need proof, you haven't been paying attention. http://i.imgur.com/XnaFgq1.png Me personally, I think there are much more important things to add that would satisfy more than a tiny percentage of the PyCharm userbase.
P.S.: I starred your repo! Seems useful!
Why not make a second array with the precomputed indexes ? This can improve performance if your tile array size doesn't change and you're iterating many times over it. 
&gt; This example code uses 8 tiles, but I would like this to scale to thousands of tiles. "Thousands" are still not quite where you want to start optimizing arithmetic calculations for iteration over them. Also, `yield`ing them and iterating over them and calling `draw` for each would already take way more time than the math. Function calls are slow, man. So you'd better strive for cleanness and simplicity just now, and start thinking about how to make everything work faster when you have working code and can profile it and see what are the bottlenecks. And then making it faster probably will involve rendering everything in bulk somehow, which would make cute micro-optimizations instantly obsolete. &gt; Just for the sake of clarity, you mean I should create a vector that contains indexes of my map in the order in which I want them to be rendered? That's not my code, that's something I quickly googled, but I'll do my best to explain. The code I linked to creates an index vector for the numpy array, which can then be used to reorder the entire array at once, `sorted_objects = numpy.take( objects, inds )`. As far as I understand, at least, and I'm not sure that it's the best idea in general (why not sort the array directly? However this way if your camera is static you can cache the array of indexes). &gt; Sorts objects based upon their dot product value along the camera direction. Dot-product calculates the projection of one vector onto another, as the product of their lengths and the cosine of the angle between them (which goes from 1 at 0 angle to 0 at straight angle), giving a single number. Luckily for as it can actually be implemented as sum of per-component products, `sum(x[i] * y[i] for i in xrange(len(x)))`, by the way. Anyway, the point is that first you translate everything to make virtual camera sit in the center of the world, then calculate dot products with positions of each object to get distances to the camera. Except it's not a true distance, but distance along the camera direction, which actually works much better. Imagine a plane orthogonal to the camera direction vector. Then dot product between some position and your camera direction is the distance to the point where you should put that plane so that the position belongs to the plane. With your camera position these planes correspond to your diagonals.
Very cool !
Facility Location is a pretty common task. I think your method sounds like the p-median model which attempts to minimize the total distance between the facilities (the school) and the demand nodes (the students). A quick googling came up with this paper: http://www.scirp.org/journal/PaperDownload.aspx?DOI=10.4236/ajor.2012.22030 what kind of data do you have to work with? If you are interested in using a method like this, send me a PM. Maybe I can help or recommend some resources. I'm a grad student in a geography program. My work is more geared towards transportation but this is a pretty common problem in 'location science' and one of the profs here used to do consulting work for exactly your situation.
parsing JSON is easier than a text file. I know it because many users will not be developers.
I find installing Python(x,y) on Windows to be painless and it comes with all the batteries you reasonably need included.
&gt; The goal is to host a php front-end on our intranet. Why PHP? You can have all Python. Look at [Flask](http://flask.pocoo.org/) for example. &gt; I'm able to call Python from PHP and interact with data local to the web server but I need it to access our Windows file server to interact with various project data. What OS runs on the web server? Which protocol is used for file sharing on the Windows server?
Python (or programming in general) is the intelligent poor man's solution to all things like this.
hell yes. My initial thought was to divide all the times on a users time map by their current commute, and plot contour lines of how seriously users commute times (in general) are impacted by the move. The current function just sums all the drive times. A function could also p-meridian line all the drive times. A simplified version of the current function: def sum_data(source = all_data) result=numpy.zeros(shape of grid_file) for f in all_data: r = csv.reader(f,delimiter='\t', quotechar='"') for i,row in enumerate(r): for j,item in enumerate(row): result[i][j] += float(item) return result
thanks, email sent
I like my version, which goes slower and slower, but is nice to demonstrate generators: def fib(): yield 1 yield 1 s1 = fib() s2 = fib() s2.next() while True: yield s1.next() + s2.next() 
I don't understand. The contour lines are travel time contours. 
I think OP means the 'time of day' for the travel times, allowing for variances in traffic at peak times etc. 