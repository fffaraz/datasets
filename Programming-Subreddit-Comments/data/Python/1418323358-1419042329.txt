What are some examples of using compiled python code. I don't understand the need for this.
A typical 3D engine, or a comprehensive 3D library, will do things such as: * Load models and art info (including vertices, materials, textures, shaders) * Manage scene elements (movable models, fixed geometry, terrain, particles, lights, cameras) * Animate models (skeletal animation or morphing) * Decide what to render (based on a scene graph of some sort, maybe with occlusion algorithms, level of detail handling, sorting for transparency, etc) * Provide optimizations such as billboarding, batching, etc * Provide postprocessing via render-to-texture * other stuff 
Sure, Python applied through CPython and C libs will be fine. This is the way I suggest doing things if performance is required and the initial Python implementation is too slow (but always first Python unless we KNOW it's going to be slow). Generally network speed is my bottleneck for almost everything I do, so I can just use gevent and get perfectly fine performance. Still, I don't think performance regarding this is the problem to solve. The hardest problem to solve here is having good C programmers, and all of which goes with that, like memory, freeing pointers and nulling them, code security, etc. If your high performance part hasn't been done by a third party, you need to rely on your skillset in your team and this stuff isn't trivial at all. That means higher skilled devs, which means higher salaries, and also a lot more development time. You lose a lot of the applied benefits of Python, like super-fast development and being able to pull in anyone who is decent with Python and not having to worry about use-after-frees, etc. Python is definitely my favorite language and the one I'm best at, but it's a serious consideration that I feel limited if I rely on having to fall back to C if I need high performance. I love C, I'm just not very confident, and I'll have to really take time to ensure code safety and correctness. Even if I'm just using pre-built C libraries, I still need to worry that I'm using them 100% correctly and not opening up a security issue due to the way they're supposed to be used, or even that the original developers wrote safe code.
I only have a minute but this should do it. ~~You can probably clean up the part where I created 'newdata' and split it so it's all done in your with open block.~~ Cleaned it up. data = [] with open(&lt;filepath&gt;) as f: for line in f: data += line.split() #Create your listbox here. for i in range(len(data)): MyListbox.insert(i+1, data[i]) 
You probably tested against unwarmed pypy JIT therefore test is invalid. PyPy is still faster most of the time if not always.
&gt; Okay, but a lot of that is important in 2D as well. Pyglet will load your models for you (2D models are just images, typically, represented as sprites), it will manage scene elements (it gives you batches and groups for sprites), it has a scene graph of sorts (OrderedGroup, though it's quite low level), provides optimizations (batching, TextureAtlas). It doesn't handle animation or postprocessing (explicitly). But it does provide most of what you need for 2D. Not for 3D. So, you're right, but hopefully you can see the difference.
Great anti-reverse-engineering protection. Seriously. Or how about easy shipping? Or if you want - you can even statically link entire runtime and dependencies into single executable and have your pure python application to be big_big_app.exe.
This doesn't really have anything to do with Python.
You don't need to write it in C. You can use Cython and get like 80% of the speedup[1]. I mean, your Python program begins its life at potato speed as though you were using Perl or even Ruby. If something isn't performing well enough you move the inner loops (almost) verbatim to a `pyx` file and jiggy your `setup.py` and then you get something at about Java performance (or potato quality C code - fast, but not hand crafted shit off a shovel speeds). Then if it's still not fast enough you can get these supposed elite developers to crank out some C to squeeze out even more performance. There are a lot of options to get results based on the amount of work you put in. In a business environment this is sweet since you can time box a lot of the improvements and make actual progress with each sprint. [1] Bullshit made up number. Take it with a grain of salt.
Very interesting stuff. I'm a bit miffed about the scope though. What would you use it for specifically? Can you just compile whatever python app and run the native code instead of the .py?
Right; it's a case where you **don't** get more done with the same hardware, despite having faster code.
Can't wait for Python 4.0!
boom!
Have I wronged you or something? We're **angrily** agreeing with each other.
Certainly not the royals...
It claims to support Windows, but since the major version is zero I would not put my life savings on it.
Assuming not a server, but a script that converts data from files, how do you warm the jit?
I'm using 2 monitors and awesome window manager (http://awesome.naquadah.org/) and i'd like to use 3 monitors :)
I have only tried on small scripts so far, but yes.
So far as I know, the trace info used to decide what to JIT is only maintained during one invocation of the interpreter. If you didn't have a process that looped across something a few thousand times, then PyPy probably didn't JIT anything.
No idea, honestly
&gt; The fact that Go is Google's language And for that reason alone, it's not going to disapear and be forgotten, and will be used in a lor of places. Having said that, I prefer Rust over Go.
You apparently have already decided that python is a "slow as balls" scripting language. However - "scripting language" is not a well-defined term, and is often in a context like this meant as a derogatory description: the local java team arguing that project x shouldn't be done in python because "it's only a scripting language". And fast or slow are so relative that to describe a language like Python as slow is also meaningless: does this mean every application written in it will be slow? does this mean you can't process trillions of transactions in it? does this mean it's merely a toy? While I would like some Python operations to be faster than they are today, I have processed a hundreds of billions of complex transactions using cpython - and performance wasn't on my top 4 list of challenges.
Eventually, yes. But it's not really there yet for all or most cases. The basically sole developer seems really impressively engaged, though, and is working on it all the time and sending out just about weekly updates. I have high hopes for it, but it's a huge undertaking.
It sounded like you were trying to make a counter-point. :) Just a simple misunderstanding.
Much Smaller file size, runs potentially significantly faster, code less easily viewed, distribute as .exe. (Last one is essentially already available with the various Python packagers).
&gt; arbitrage calculations If I understand well what kind of task have to be done there, isn't it faster to use DBMS for all those calculations? I'm an amateur and just curious of effectiveness differences between such solutions. 
Please. Please don't. Global mutable state is the bane of debugging.
Cool, I can never get tired of Markov Chain text generators. 
http://docs.python-guide.org/en/latest/writing/tests/ is a good place to start.
the point doesn't change though.
Me neither, I saw some funny ones the other day and I got inspired! :)
The stuff in the "Other Stuff" Section is kind of depressing though.
Awesome! Could this graph extraction also be accomplished with http://dbpedia.org/sparql and/or http://wiki.dbpedia.org/Downloads2014 ? seeAlso: https://www.reddit.com/r/Python/comments/2og2lq/trying_to_make_an_interactive_visualization_of_a/cmn6grn (cytoscape, sigma.js)
I think pypy simply needs time... I've read somewhere that you should make a pre-run for ~1 second (of course depending on the size of the test) for every test you make in pypy...
Maybe, but the terminology is weird in this case... I see it as different because going backwards works, but forwards breaks.
/r/django Also have a look at the djangogirls tutorial, it's great and should have you up and running fairly quickly.
Not sure what you mean. Can you show an example? I just added a `time.sleep(6)` to one of my script, and it made no difference. I think [gthank](http://www.reddit.com/user/gthank) is right, needs a piece of code that is executed **a lot** in a given run before it can make a difference. 
Donate! :D
For starters, you might find it helpful to watch Ned Batchelder's general, testing talk from PyCon 2014: **https://www.youtube.com/watch?v=FxSsnHeWQBY**
I'm assuming you've tried "inspect element" in your favorite web browser?
So I dislike several things about `namedtuple`, mostly the use of `exec` and needing to supply the type name and fields as strings, so I've implemented something similar that I prefer. * Requires at least Python 3.3 * Uses a whole lot of introspection instead of eval-ing strings * Uses a function declaration to get the type name and fields * Allows default field values * The function supplied is called on `__new__` (but not yet on `_make`, I just realised). You can use it to check if the input is valid. * Use `Thingy._method` and `Thingy._property` to add methods and properties respectively * If the init function has a docstring, it'll be used instead of the default `Thingy(field1, field2)` * Once defined, follows behaviour of original `namedtuple` almost exactly To be clear, this is not a serious proposal to replace `namedtuple` or anything, this is something I hacked together today for fun and that I want to share.
Yep. And I thought I'd found it: &lt;span class="paragraph"&gt; But when I tried that (and different combinations of it), it didn't work. 
Post the error or be more specific about how it "didn't work"
Not getting any errors, but when I try to print out what is supposed to be scraped, I get "None", so I'm assuming nothing is being scraped. Therefore I believe I'm using the wrong HTML tag to try and find the text. I know my code works, because earlier in the program I scrape links out so the user can choose which article to do the analysis on.
But does this reflect real-world usage? You're not going to do that in production code.
Try doing a substring search on the html you get to see if the tag is there (or download it with wget and look at it in a text editor). If the span is added from javascript, then it won't be in there (the javascript has to be executed to create it), but will be in your browser.
Okay, so just pull down the raw HTML code and search it?
Thank you. I forgot about converting to an integer and I'll check that subreddit if I need help in the future.
Okay I'll give that a shot and see if I can get it to work. Still new to scraping if you can't tell haha. 
Nice job man! Each time your run code, a limit is set on the execution time which is probably why your `sleep` did not work. Each bit of code is run on an enclosed env that is created on the fly for each run of code. That env is deleted after it is run. If your code executes longer it just stops it and trashed the env. Because it is an env you can run stuff dangerous stuff. And you can run `while(true)`. 
And while you can execute that python script for now, I'd ask that you kindly don't, just because you are a nice person. Or do it, it should be fine. I am putting things in place to stop stuff like that but the more I have to put in place the slower things will run. 
That's like saying Clang isn't production ready because it doesn't support all GCC extensions. PyPy is extremely compatible against the Python *language*.
So I'm seeing classes such as 'paragraph' and 'im-paragraph', which precede the text I'm trying to scrape, is that what I want?
C code doesn't need "JITsing" as it is already fast.
In a word: py.test in ALL situations. Flask has facilities to help you write tests for REST servers. Read the py test docs and flask testing docs and that has everything you need to know to start writing tests.
...but we're not talking about the python *language* we're talking about python as a viable target for enterprise applications, which means tangibly using 3rd party libraries, that will almost certainly have c plugins.
Precedes, or encompassing?
Here's a screenshot of what I'm looking at when in the inspector: http://imgur.com/2yHNm93
The pattern depends on your Python version. Python 2: https://docs.python.org/2/library/stdtypes.html?highlight=translate#str.translate "For string objects, set the table argument to None for translations that only delete characters" Python 3: Use maketrans first, then call translate. https://docs.python.org/3.4/library/stdtypes.html?highlight=translate#str.maketrans
Works for me. I assume it's the article text you're after? import requests from bs4 import BeautifulSoup soup = BeautifulSoup(requests.get('http://www.motortrend.com/roadtests/sedans/1410_2015_audi_a3_tdi_first_drive/').text) spans = soup.find_all('span', {'class': 'paragraph'}) text = '\n'.join([t.contents[0] for t in spans]) Edit: This is probably better: text = '\n'.join([t.text for t in spans])
Looks good. Don't know what isn't working. You must not be using BS right? Maybe just copy and paste that span into a variable as a string in python to test if you're using BS correctly. Can't help further right now. 
&gt; You'd probably have a daemon process that lives forever and searches for scripts to process, then processes them as they are found. You'd have an optimized file-watcher, but each script would still be cold upon first loading.
So I can compile python and then send it to another machine that doesn't have python, and then run it there? Yes?
Doing this can get a little funky since contents[0] ignores all text after any other tags. Doing `.string` similarly isn't great here. You might be best served by `.text` or `.get_text()` but be careful of nesting.
So that works for that link, but if I use another link, it crashes and gives me the error: TypeError: sequence item 2: expected string or Unicode, tag found That make any sense/mean anything to you?
Got it, the edit to your original code above fixed it. Thanks for saving my ass!!!!
I don't have a machine that doesn't have python so running ldd on the resulting executable: linux-vdso.so.1 =&gt; (0x00007fffac1fe000) libpython2.7.so.1.0 =&gt; /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0 (0x00007f1b7360e000) libstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f1b7330a000) libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f1b730f3000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f1b72d2d000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f1b72b0f000) libz.so.1 =&gt; /lib/x86_64-linux-gnu/libz.so.1 (0x00007f1b728f5000) libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f1b726f1000) libutil.so.1 =&gt; /lib/x86_64-linux-gnu/libutil.so.1 (0x00007f1b724ee000) libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f1b721e7000) /lib64/ld-linux-x86-64.so.2 (0x00007f1b73ba7000) So you do need libpython. Now I'm wondering why... It does mean that you don't have to ship pure python dependencies. Not sure about packages that come with C libs, I suspect you'll still have dependencies on those.
As a guy who works heavily with both Django and Rails I have to say I don't see Django displacing Rails. They both have their place. That said, I prefer to use Flask. Haha.
Looks like LibreOffice Draw to me.
I actually run 8 monitors
There are a good number of ways you could accomplish this. What specifications do you have in finding a solution? Python has plenty of web frameworks like the ones you've mentioned, but you should find one that most accurately suits your specific use cases. PHP is going to cause issues for you if you need to continually poll for changes to output. If you insist on a Python based solution, look into Tornado(http://www.tornadoweb.org/en/stable/). Rather than a I/O being blocked Tornado will help in making your app asynchronous. Node.js deserves to be mentioned, but only if you have a good bit of time to research. The easiest/quickest to implement would be to use ShellInABox or AjaxTerm. These essentially provide an web based command line.
Beautifulsoup is great but its slower. I'd recommend lxml, its faster, less verbose, and once you get the hang of it, very convenient. import lxml # get your raw_html using requests or urllib lxml.html.fromstring(raw_html).xpath('//span[@class="paragraph"]/text()') Gets the paragraphs you need from the link you posted.
I use three most of the time. Would like to add a fourth for comms (email, chat, etc)
&gt; Can you just compile whatever python app and run the native code instead of the .py? Python doesn't get compiled to native machine code, it gets compiled to an optimized bytecode. Launching a "compiled" python app still means firing up the interpreter, opening the .pyc bytecode files, and running the python environment. There are cases where you will see better performance running native machine code, like you would do with a compiled C program.
time.sleep(6) wouldn't have any effect on the warm-up, it needs to be running the actual code. A pre-run of ~1 second means running the actual code for 1 second before you start timing.
Real-world code is going to be running long enough that the warm-up time is largely irrelevant.
Assuming you use the redistributable option, it's just baking the python runtime (and any other dependencies, c or otherwise) into the binary.
Calling compiled machine code great obfuscation might be a bit of an overstatement...
I don't get "much smaller file size"; .pyc/.pyo files are pretty compact. "Distribute as .exe" -- I'm dubious. Typical apps depend on lots of modules, numpy, tk/wx/pyqt, etc, and if all the dependencies are not also Nuitka'd into machine code (and I don't see how they could *all* be), you're still looking at using py2exe or similar for bundling. Just more .so modules in place of some of the .pyo's.
Someone still has to tell the library what the encoding is. I run a library that tries to support unicode. Everyone wants it to just know the encoding. Sorry, I can't do that. Text editors like Notepad++ can't do it either.
&gt;That's the good thing about freezegun: these tests will not fail due to problems like DST, since you are freezing the date to a specific day. That is exactly the point. If your code is wrong and breaks around DST changes then mocking datetime.now to point to a certain day hides that for longer.
Are you talking about nuitka? Which appears to be compiled native binary from python, not optimized bytecode?
I heard lots about asyncio, but still don't know what is it useful for. could someone explain in what real life scenario/problems ansycio is better than threading or multiprocessing modules? I'm confused which one should be used for what. thx
**tldr;** So its actually a Python to C++ translator that then compiles C++ to native code.
I misread the question as 'Can't you just'... Leaving my response as is, with this note
That's some cool stuff. I haven't seen that before. There is definitely some learning curve to writing Cython code, but it's still a very neat trick without having to code raw C. I see your point. I still wish we had a faster reference interpreter than CPython though.
A scripting language is also a controlling language. It is that language embedded in larger applications that makes those tools subject to your will Embed them in an office suite or a CAD program and suddenly you can do the repetitive, effortlessly. Give then access to libraries, and deep science, cloud computing, or GPU's become accessible. There is greatness in scripting languages!
It's for coroutines. You skip the overhead of spawning and managing a thread or process and get an async event loop that integrates tightly (i.e. using yield). The summary has the major features: https://docs.python.org/3/library/asyncio.html
Which one is in the repos?
So it turns out there is a `--standalone` option just for this. Sadly, it took a long time, and wasn't able to produce a working executable. Still, I think this is very promising.
For many people, the best way to learn is to do. So find a project and get to work. Enter a competition on kaggle. Or scrape reviews from a website and write a sentiment analyser. Pull down financial data off Yahoo and use it to determine a trading portfolio where you only change positions each week (or month). Write a spam filter. 
Learn Machine Learning, not Data Science. Data Science is basically an application of ML. https://www.coursera.org/course/ml https://www.edx.org/course/learning-data-caltechx-cs1156x#.VIqibTGUcms
While my official title is not "Data Scientist" (I'm a post doc at a US DOE national lab), about 75% of my day-to-day involves what I would consider data science using numpy,scipy,scikit-image, some pandas, matplotlib, etc... I would suggest finding something you are interested in and doing some "data science" on it. My personal opinion (which is worth what you have paid for it) is that it is best to learn by doing, rather than just reading. The reading and courses will help, but that is only a tiny fraction of it. Things you may be able to do: * Analyze stock tick data * Find out some information about sports players and their statistics * Look at currency market data (there is a lot of historical data for bitcoin readily available for various exchanges) * Analyze ebook data (for common words, sentence length, ...) * Analyze twitter feeds/trends (similar stuff to ebooks, and you can throw in some info about geospatial location) * Look at price data of a product/s as a function of time on something like amazon or newegg (you can learn some simple url scraping with this too) * Learn something about your local region with weather data. I'm sure there are more options that others can think of too. Good Luck! 
http://datasciencemasters.org/ seems to be a good collection of resources and books.
Even a dollar is something. Shows appreciation and support.
Not really a data scientist, but I believe Data Science is more than machine learning, it encompasses more techniques from statistics (descriptive statistics, testing...), data vizualization, big databases problems, and so on. 
You don't say much about what your background is. Do you already know some Python, stats and a bit about scientific method? If not, you might be better off doing more focussed courses on those first. If you've got the background then I would complete (including the exercises and projects) any of the courses you listed above and then grab something easy off Kaggle or come up with your own managably small project and see it through to completion. TLDR- learn *something*, figure out what you still don't know, iterate
Haha
Indeed. A tracing JIT (like PyPy) should blow static compilation out of the water for a dynamic language like python. This is assuming the JIT got hot.
Hmm, interesting feature. I'm still mainly using python 2, but I do hope that the python community doesn't start moving too much towards static typing, etc... Some of pythons best features comes from its spirit of flexibility, and typing (while awesome in other languages), I don't think would work for python. For example, argument packing and unpacking could never work if lists were typed more rigidly. 
Right on time, as I was thinking on playing around with text adventure games myself. Thanks!
I watched it. Awesome.
Basically what /u/cwillu said, what I meant was: Looping 10 K times may not be enough, depending on the content of the loop! If it's &lt;1s, it's probably not! Also loop __before__ starting the timer!
A good package to do this is python-readability
https://wrdrd.github.io/docs/consulting/data-science.html /r/pystats /r/datasets /r/machinelearning /r/datascience 
Not necessarily. When the paper becomes published, read "Dynamically Composing Languages in a Modular Way: Supporting C Extensions for Dynamic Languages" by Grimmer et al. Here the authors compose Ruby and C VMs which JIT using the truffle/graal stack. The authors report that Ruby programs that use C extensions can execute faster than the conventional IRB+C approach.
Yes it'd be quite interesting to have a performance comparison with the original namedtuple. I'll try to that when I get him.
Thanks for the help, but seems like that way of doing it requires linux and just a whole lot of things that are out of my league. Is there no way to generate a full users list just with some python module?
Just because the *examples* use `curl` doesn't mean you have to use it. The examples simply show requests and expected responses. Just replace `curl` with your preferred HTTP client (e.g. Python `requests`).
IMO, you have that backwards.. ML is really just one application of Data Science. How are you going to understand when to use a linear vs non-linear kernel in an SVM, or even troubleshoot error messages, without a basic understanding of Linear Algebra? What if you've got results from multiple ML algorithms and want to compare them for correlation or statistical significance against other data? You should have some statistical training and familiarity with a stats package (R, SASS, SPSS, etc..) What about visualizations and turning those results into a simple infographic for C level execs.. Experience with Tableau or other packages would be very helpful. If you're asked to extract your own dataset based on existing data from a DB or warehouse, you need some basic knowledge with SQL and relational databases.. or if you're given a really messy dataset and need to clean it up, you want to have some knowledge around awk/sed/python, etc.. To me, all those skills are core skills for data science, and you wouldn't be very successful in a lot of ML tasks without them.
I mean, the absolute best course of action would be to get a masters in CS, math or statistics with a focus on applied methods and machine learning. The coursera machine learning course is really good and is what got me started down the path. I took the innaugural course a few years ago, studied a bunch on my own, went to grad school, and now I'm working as a data scientist. It can be done.
[Harvard's CS109](http://cm.dce.harvard.edu/2014/01/14328/publicationListing.shtml) is one of the most comprehensive data science courses out there, IMO. It's rigorous, but you'll learn a lot from it if you follow it till the very end.
 If you are into MOOCs Coursera's Machine learning with Andrew Ng and edX's Machine Learning course from Caltech (just ended...) go into greater depth. Udacity released an intro machine learning course last month that uses scikit-learn; it doesn't go into much mathematical depth but it covers a lot of different topics and uses python. 
For some more ideas, check out the curriculum linked below and some of the comments on it from Hacker News. I don't have anything to do with it, but I just remembered it looked really well put together when I saw it a few months ago. * https://www.mysliderule.com/learning-paths/data-analysis/learn/ * https://news.ycombinator.com/item?id=7815906
I have a weird, and maybe bad habit/philosophy. I don't believe that objects should be aware of the design patterns they participate in. For example- an observer pattern is a lot of stupid boilerplate code. So why not build a decorator that injects the boilerplate in? Unlike Ruby's mixins, it can do it in a way that avoids collisions. Edit: I forgot, I posted a [gist](https://gist.github.com/RemyPorter/784cab94a29508bcec5c) of what I'm talking about.
No seriously. I by no means say its sufficient alone because its not. However compiled code logic gets blurred really good because of all the behind-scenes stuff going on at the lower level. Besides you no longer can recover bytecode that can be decompiled to python code. It really is great. A comparable thing is compiling IronPython code to CLR dll. It also produces a messy output but essentially its python code running behind the scenes. And again original src cant be recovered. Its good stuff ;)
This is exactly how I started. Then I started posting my results publicly. Sometimes people would give feedback, I'd go learn about what they suggested, fix the problems, and post it again. Then the cycle began anew. Check out the problems on Kaggle if you can't think of anything at first. You don't even have to submit an entry -- just find something cool, play with it, and learn. --- That said, just aimlessly playing around isn't going to make you an expert data scientist. Find some highly rated books on machine learning, data mining, statistics, etc. and work through them. Make up a cool project that applies what you learn in every chapter. You're not going to really learn something until you apply it. 
Depends on the kind of thing you want to do! If you're more interested in the statistics aspect, one way to start would be to get a copy of an old edition of a textbook (can usually be bought for under $20 used on Amazon, for example) for something like Econometrics (the Wooldridge book is highly recommended) and working through all of the computer examples. This would be a nice way to start if you're interested in modeling and statistics.
There is a fair amount of overhead involved in using C extensions with PyPy. If you're only using C for speed (as opposed to wanting to bind to pre-existing functionality), it is usually a bad idea to use it with PyPy: if it is truly a hotspot, then the PyPy JIT will most likely generate code that is just as optimized, if not more (because the tracing JIT has access to **far** more data than a static compiler, and can issue machine code that is optimized for the actual data you're receiving instead of data you *might* receive based on the information in the weak—in C, at least—type system).
The JIT can't do its thing until it's seen a given piece of code repeatedly (probably 10,000+ times, though I'm not sure what the actual number is these days). What people are talking about when they say "non-JIT PyPy" is PyPy before the JIT has had a chance to profile code and start generating optimized machine code.
I found this a couple weeks ago: [**How to become a data scientist**](http://imgur.com/R9cotrv)
The author should refine his or her writing. It has some grammatical mistakes that make some of the writing very difficult to read.
What Decency said, but also this could be a cute way to do what you want: from collections import Counter def two_or_more(numbers): counts = Counter(numbers) return [number for number, count in counts.most_common() if count &gt;= 2] 
Most web apps aren't usually very computationally demanding, and there are plenty of other bottle necks(e.g., database structuring/connections/queries, caching, #/order of requests) that can be optimized to improve performance than the speed of the language alone, as such websites shouldn't be used for a measure of overall speed.
No need for a Counter here, list already has .count() built in and you can use a set to avoid returning a list with duplicates. def two_or_more(number_list): return list(set(number for number in number_list if number_list.count(number) &gt; 1))
are you related to drunk economist?
who said django is displacing rails? edit: as I re-read your entire comment, what the fuck are you talking about?
Sorry for him hijacking your post but just wanting to add to your list. I've found games to also be a great source for gathering data and doing data science. Plus it can be really helpful to both other users and devs. 
&gt; I don't get "much smaller file size"; .pyc/.pyo files are pretty compact. There are some numbers on [this page](http://nuitka.net/pages/performance.html), but they are not super meaningful to me from a quick glance. It appears that those are the size of the binary produced when compiling PyStone. I'm guessing this is one of those things that *may be better in some cases*, but it depends on the use case. &gt; "Distribute as .exe" -- I'm dubious. Typical apps depend on lots of modules, numpy, tk/wx/pyqt, etc, and if all the dependencies are not also Nuitka'd into machine code (and I don't see how they could all be), you're still looking at using py2exe or similar for bundling. Just more .so modules in place of some of the .pyo's. This is a pretty good point, but it appears that it does that step itself. From skimming the documentation, it looks like it has two primary modes of operation: - compile just the one .py file you specify into an executable binary - compile just the one .py file you specify into an extension module and there is also an option to resolve the imports and embed those into the produced binary. I'm not entirely clear on whether or not this compiles those where possible as well, or simply embeds the bytecode. I can at least say with reasonable certainty that after compilation it uses libpython to handle importing anything that was not compiled by Nuitka, whether embedded or not. So, you wouldn't need to use py2exe in addition to Nuitka, at least.
weird. my philosophy is the exact opposite. But as I said in another post, scalability in web development is more affected by other infrastructure decisions than the code alone.
Only if it will run python2
Haha nothing fancy, actually somebody did it on their phone: https://play.google.com/store/apps/details?id=com.drawexpress.lite&amp;hl=en I don't have enough experience to recommend it over anything else. The process of making a visually pleasing diagram is always slow and laborious. None of the diagrams were made specifically for the blog post, but were re-used from various internal presentations. Edit: I will double check the exact program used. 
You may have already found this and its not really a structured course, but youtuber SentDex does a ton of hands-on videos involving several types of big data analysis. 
I still find set literals unnecessary and they seem forced into the language, so I mostly don't use them. The empty set literal also doesn't exist, which is annoying. In time maybe they'll seem intuitive to me.
Here's one example: &gt; $ &gt; $ python &gt; Python 2.7.6 (default, Mar 22 2014, 22:59:56) &gt; [GCC 4.8.2] on linux2 &gt; Type "help", "copyright", "credits" or "license" for more information. &gt; &gt;&gt;&gt; print 'foo' &gt; foo &gt; &gt;&gt;&gt; exit() &gt; $ &gt; $ python3 &gt; Python 3.4.0 (default, Apr 11 2014, 13:05:11) &gt; [GCC 4.8.2] on linux &gt; Type "help", "copyright", "credits" or "license" for more information. &gt; &gt;&gt;&gt; print 'foo' &gt; File "&lt;stdin&gt;", line 1 &gt; print 'foo' &gt; ^ &gt; SyntaxError: invalid syntax &gt; &gt;&gt;&gt; exit() 
This will be reasonably speedy for pure python. Note that join builds a list so switching to a generator comprehension is not faster. s = ['\n','\n','\n','\n','\n','\n','Sticky','"','toffee','"','pudding','\n','\n','\n','\n','\n'] ' '.join([i for i in s if i not in ('\n', '"')]) 'Sticky toffee pudding'
Did you do some profiling to see why? PyPy typically generates insanely fast numeric code, so I'd expect CRC to be a sweet spot. That said, CRC libs in C are also extremely likely to be optimized to within an inch of their life (probably with lots of fun vectorization, and memory optimizations to wring every last bit of performance out of cache lines), so beating the C lib would be a tall order for PyPy. I'm still a bit surprised there was a difference as big as what you seem to be describing.
Fantastic.
Someone else has answered your question sufficiently, but I feel like we could maybe help you more if we took a step back and asked: "How did we get here?" It seems like you split a string by some value to get these multiple strings (many of which are not desirable). Maybe if we see the code that executes before this, we can see if there is a better way to pre-process the string?
I'm a big fan of the "click" library by Armin Ronacher for accomplishing similar tasks (or in this case creating a terminal-based game)
Looks like no python3 support! :(
Most of the packages I'd want to use with this are 2.x anyway
 #!/usr/bin/env python import requests url = 'https://api.soundcloud.com/users/{user}/followers?client_id={clientid}' user = 'veelabeats' clientid = 'YOUR CLIENT ID' if __name__ == "__main__": r = requests.get(url.format(user=user, clientid=clientid)) for user in r.json(): print "User: %s, URL: %s" % (user['username'], user['permalink_url'])
So, it kind of can. Keep in mind java and c support the ... syntax for a list of arguments.
I think the hard part of that approach is choosing the tools. Depending on that decision you may find one day that you have to start nearly from scratch again to continue with something new. It's nice to have a starting point, but it's important to have a roadmap. IMHO.
It's not a web framework, but more like a web server that follows the traditional CGI approach: If the URL points to a python script file in the file system, it runs it and serves the standard output. Just like good old PHP, CGI or mod_python. I personally don't like this model and prefer web-applications over web-scripting. 
What do you want to learn?
Same here. I don't really see the advantage over CGI. If there is, it is not easy to find it in the docs.
Yeah these kind of mini projects I've been doing every now and then really help me try new things out (usually I'll try a few approaches and see which I like best) This is what inspired me: http://thedoomthatcametopuppet.tumblr.com/?
Have a think about something you could do with having, and then do it. 
Looks like a step backwards; what's the advantage supposed to be?
My two cents: You do not need to keep terminal tab open (IPython nbserver) and can use the browser; or run it under screen/tmux. In the windows world too, you can have a shortcut. Adding and removing cells is customizable (defaults are vi keys, which I am familiar with -- though I use emacs more). Exporting -- you can have it as a menu item with correct flags (falls under css/html/js tweaks). Matplotlib defaults look ugly: use prettyplotlib or seaborn Native app is going to be too much of an effort -- why would someone use it when PyCharm has some reasonable support already; emacs has support. You may be better off with the css/js/html tweaks and develop an IPython extension or some patch. For native app like experience, you can probably develop an extension to an IDE or a stripped-down browser. 
I've written a very similar framework and used the method described above. I highly recommend it.
The main issue with keyboard shortcuts is the discoverability. They certainly don’t show up when I hover over menus. I think a native app more as a webview with some controls and system menus. Thant is not a big codebase to maintain. Thanks for feedback. 
The string before split into letters is scraped off of a website. Once it is taken from the website there are '\n's and '"'s in it.
Did you learn Python just to learn it, or was there some purpose to it?
Some ideas [here](https://github.com/karan/Projects/blob/master/README.md )
in the grand scheme of "is this practice enough to tear down an existing system," the CGI approach is very low on the list. most major applications have semi-abandoned it anyways with routing conventions. what does this run through? i thought all the major apache-&gt;CGI plugins for python were no longer actively supported.
Try /r/dailyprogrammer
Welp, looks like my weekend just got a lot more interesting. Thanks for the heads up!
You need to have your proxy switching middleware handle `process_exception` by returning the request again. [Here](https://gist.github.com/therg/c80e24454ee353d3c123) is a proxy middleware I use with Scrapy. `PROXY_LIST` is a path to a text file of proxies. 
The first real thing I ever made with python was a bot to scrape rss feeds and post articles to Twitter. 
that's a really shit rule of thumb.
Oh to have more time. I'd actually like to see the Python code and run it through a disassembler, just to see where it's spending its time, but I'm already backlogged. I need like a Bat Signal for Python gurus that like to blog.
I already had been using a set comprehension to consume the generator. By set literal I meant using braces instead of the built in set() function call, which really just seems superfluous.
You just saved me spending like 100 hours figuring this out, thanks dude.
Thing is, dictionaries are unordered, so foo might not end up being the value of "foo" in the dict. You can already do `foo, bar = mydict.values()` or, you could use setattr to make foo and bar be variable names equal to the values of those dict keys, but it would be unclear and dangerous to do. I have a hard time seeing why you would want this feature, as dictionaries are often for variable length and unordered data. Why would one use a dict in this case in the first place instead of a list or tuple? edit: OP uses variable names that are the same as the keys being accessed in order to hand wave away the problem of matching variables on the left side with the dictionary being unpacked on the right. A serious issue with this is that OP is thinking about a particular case where the key is a string, but dictionaries are so much more versatile than that. I think OP wants to use an [ordered dictionary](https://docs.python.org/2/library/collections.html#ordereddict-examples-and-recipes), and then unpacking values() would work in a consistent way regardless of keys, and no changes to python syntax are necessary.
You are missing a closing ')' on line 56. Common issue with debugging syntax errors -- look above where the problem was reported.
I've run into use cases for this. It would be very helpful when iterating over a nested dictionary, for example: d = {datetime(2014, 12, 10): {'a': 1, 'b': 2, 'c': 7}, datetime(2014, 12, 9): {'a': 3, 'b': 4}} for date in d.keys(): if not d[date].get('c'): d[date]['c'] = d[date]['a'] + d[date]['b'] for date in d.keys(): a = d[date]['a'] b = d[date]['b'] c = d[date]['c'] quotient_dict[date] = a / (b + c) This is a simple example of a problem I run into a lot where you're looking at timeseries data, and for some dates a, b, and c are provided, but in others c is not provided yet can be derived from a and b. In the example you can deal with not having c in the second loop but that's not always practical. OP's idea saves two lines in my example (and a lot of characters if I named them apple, banana and carrot) and is something I've looked into before. 
I could almost see the use of tuple multi-indexing: foo, bar = my_dict[("foo", "bar")] But it'd interfere with cases where tuples are already single keys...
That was my first thought too. "Oh great, Python as PHP." (not to disparage this project though, it was just my initial gut reaction)
Makes more sense. Funny how that negative completely changes the entire meaning/tone of that question. I guess that's always the case, but still, neat.
woah that's actually really cool. I've not done a lot of research into using python with the interwebs, but it sounds intriguing. It seems though, wouldn't python not be the best language to work solely with sites on the web? 
This looks... really bad.
It's a class at my school; I'm in hs. I realized though, that nobody gives a fuck about it, so I decided to learn more of it. I find coding pretty satisfying.
You should (1) give your code and (2) run with full_output=1 as suggested. But from what I see (this is going to be very amateurish and approximative !), I would guess that you use scipy.odeint (or possibly scipy.optimize) at some point. These methods have a maximum number of steps in their optimization. If they don't converge before reaching that maximum of steps they may throw an error. What you can do is (1) check that all your functions are indeed correct, for 99.9% of differential equations you shouldn't have this problem, (2) increase mxstep in odeint or optimize, (3) play with other parameters like hmax in odeint.
What u talkin about? Besides the built in functions, there are no other functions that can be used without &lt;module&gt;.&lt;function&gt; if it lives in another function, unless you do something stupid like "from &lt;module&gt; import *".
 In [1]: from django.utils.timezone import now In [2]: import inspect In [3]: inspect.getmodule(now).__name__ Out[3]: 'django.utils.timezone' In [4]: now.__name__ Out[4]: 'now' As a function: import inspect def get_function_path(f): module_name = inspect.getmodule(f).__name__ function_name = f.__name__ return module_name + '.' + function_name But what do you need this for anyways?
Maybe - just maybe - because "python is **not** as PHP" its not catching on. Just saying, dont throw stones at me. There surely is a reason why people prefer php over python even though python is by far superior in any way. 
Python is a great language for web dev! It has libraries for [scraping](http://www.crummy.com/software/BeautifulSoup/) [sites](http://scrapy.org/), [serving](https://www.djangoproject.com/) [content](http://flask.pocoo.org/), [http requests](http://docs.python-requests.org/en/latest/), and more. Fun Fact: This very site you are using runs on Python. 
Explain.
Thats great! Few advices: * [PEP8](https://www.python.org/dev/peps/pep-0008/) it. If you would like to skip reading lengthy and boring guidelines then [PyCharm](https://www.jetbrains.com/pycharm/) has PEP8 support and highlights code that does not match style guidelines. There is free community version of this awesome IDE. * Always use 4 spaces for indentation - will save LOTS of headaches in the future (probably covered by PEP8). * String concatenation is bad. String sprintf formatting is better ('some %d number' % 1), python string formatting is the best ('{my_local_variable}'.format(**locals())) * [argparse](https://docs.python.org/3/library/argparse.html) is terrific argument parsing library. For more complex set of arguments check it out. * Python2.7 is on its way out. No reason to start new projects in Python2.7 if all libs you need support Python3. So thats that :)
There's a fairly easy way to get this functionality without much work. Basically just do: class MyDict(dict): def __init__(self, *args, **kwargs): super(MyDict, self).__init__(*args, **kwargs) # or super().__init__ in python3 self.__dict__ = self You've now got pretty much the same thing (indeed, it's more efficient, since it doesn't have entries in 2 places for everything): &gt;&gt;&gt; d= MyDict() &gt;&gt;&gt; d.foo = 42 &gt;&gt;&gt; d['foo'] # Prints 42 42 &gt;&gt;&gt; d {'foo': 42} Admittedly, this doesn't do the "Also convert dicts which are attached as values to MyDicts", but to be honest, that seems very dangerous to me, because it's going to result in unexpected behaviour for many (suddenly, adding keys to a referenced dictionary will stop changing your referenced object, instead updating a **copy**), and given the issues with this approach in general (see below), I think you'd want to be in firm control of the scope of its effects if you do it at all. You also don't seem to be handling all the `dict.__init__` syntax - indeed, you ignore everything except the first arg, and even then, only if it's a dict. Worse, you silently continue without error when this happens. IE. Dict(foo=42, bar=12) or dict((k,v) for (k,v) in something_generating_data()) will raise no error, but initialise an empty dict. You should either handle it all (eg. by calling `dict.__init__` and then fixing things up afterwards from the keys, or else change the method signature to only take a single arg, and raise an exception if it's not a dict. However, there are issues with doing this at all. What happens when you do stuff like `d['keys'] = 11`. Suddenly basic operations that take a dictionary may stop working because you've overridden methods that are part of the dict interface, and if this dict is general purpose, you don't want to have to mark arbitrary keys off limits.
Here's what I'd recommend. # GETTING STARTED WITH DATA SCIENCE If you're interested in [learning data science](http://www.sharpsightlabs.com) I'd suggest the following: &amp;nbsp; **Tools** 1. I’d recommend learning R before Python (although Python is an exceptional tool). Here are a few reasons. 1. Many of the hot tech companies in SF, the Valley, and NYC like Google, Apple, FB, LinkedIn, and Twitter are using R for much of their data science (not all of it, but a lot). 2. R is the most common programming language among data scientists. O’Reilly Media just released their [2014 Data Science Salary Survey](http://radar.oreilly.com/2014/12/2014-data-science-salary-survey.html). I’ll caveat though, that Python came in at a close second. Which leads me to the third reason: 3. R has 2 packages that dramatically streamline the DS workflow: - [dplyr](http://www.sharpsightlabs.com/dplyr-intro-data-manipulation-with-r/) for data manipulation - ggplot2 for data visualization Learning these has several benefits: they streamline your workflow. They speed up your learning process, since they are very easy to use. And perhaps most importantly, they really *teach you how to think* about analyzing data. GGplot2 has a deep underlying structure to the syntax, based on the Grammar of Graphics theoretical framework. I won’t go into that too much, but suffice it to say, when you learn the ggplot2 syntax, you’re actually learning how to think about data visualization in a very deep way. You’ll eventually understand how to create complex visualizations without much effort. &amp;nbsp; **Skill Areas** My recommendations are: 1. Learn basic data visualizations first. Start with the essential plots: - the [scatter plot](http://www.sharpsightlabs.com/how-to-create-a-scatterplot-in-r/) - the [bar chart](http://www.sharpsightlabs.com/r-bar-chart-basic/) - the [line chart](http://www.sharpsightlabs.com/line-chart-in-r-basic/) (But, again I recommend learning these in R’s ggplot2.) The reason I recommend these is 1. The are, hands down, the most common plots. For entry level jobs, you’ll use these every day. 2. They are “foundational” in the sense that when you learn about the underlying structure of these plots, it begins to open up the world of complex data visualizations. As with any discipline, you need to learn the foundations first; this will dramatically speed your progress in the intermediate to advanced stages. 3. You’ll need these plots as “data exploration” tools. Whether you’re finding insights for your business partners or investigating the results of a sophisticated ML algorithm, you’ll likely be exploring your data *visually*. 4. These plots are your best “data communication” tools. As noted elsewhere in this thread, C-level execs need you to translate your data-driven insights into simple language that can be understood in a 1-hour meeting. Communicating visually with the basic plots will be your best method for communicating to a non-technical audience. Communicating to non-technical audiences is a critical (and rare) auxiliary skill, so if you can learn to do this you will be very highly valued by management. I usually suggest learning these with dummy data (for simplicity) but if you have a simple .csv file, that should work to. 2. Learn data management second (AKA, data wrangling, data munging) After you learn data visualization, I suggest that you “back into” data management. For this, you should find a dataset and learn to reshape it. The core data management skills: - subsetting (filtering out rows) - selecting columns - sorting - adding variables - aggregating - joining You can start learning these [here](http://www.sharpsightlabs.com/dplyr-intro-data-manipulation-with-r/). Again, I recommend learning these in R’s dplyr because dplyr makes these tasks very straight forward. It also teaches you how to think about data wrangling in terms of workflow: the “chaining operator” in dplyr helps you wire these commands together in a way that really matches the analytics workflow. dplyr makes it seamless. 3. Learn machine learning last. ML is sort of like the “data science 301” course vs. the 102 and 103 levels of the data-vis and data manipulation stuff I outlined above. Here, I’ll just give book recos: - [An Introduction to Statistical Learning](http://amzn.com/B00DM0VX60/). This is a highly regarded introduction - [Machine Learning with R](http://amzn.com/B00G9581JM/) - I’ve also heard that there is some foundational ML information in [R in Action](http://amzn.com/1935182390/), though I haven’t read it myself. After you get these foundations, then you can move on to specialize in a particular area. &amp;nbsp; # OTHER RESOURCES: **Data Visualization** 1. Nathan Yao of [Flowing Data](http://www.flowingdata.com) is great. His blog shows excellent data visualization examples. Also, I highly recommend his books. In particular, [Data Points](http://amzn.com/111846219X/). Data Points will help you learn *how to think* about visualization. 2. The book [ggplot2](http://amzn.com/0387981403/) by Hadley Wickham. This is a great resource (though a little outdated, as Hadley has updated the ggplot package). 3. I also really like [Randal Olson’s work](http://www.randalolson.com/blog/) (AKA, [/u/rhiever](http://www.reddit.com/user/rhiever)). He creates some great data visualizations that can serve as inspiration as you start learning. &amp;nbsp; # TL;DR **I'd recommend learning R for data science before Python. Learn data visualization first (with R's ggplot2), using simple data or dummy data. Then find a more complicated dataset. Learn data manipulation second (with R's dplyr), and practice data manipulation on your more complex data. Learn machine learning last.**
Then its not new, and mostly crap
Exactly. Machine learning is a sub-discipline within data science. The 3 core skill areas are data wrangling, data visualization, and machine learning. These all work together in data science. &amp;nbsp; Here's an analogy I like to use: You can think of data science like baseball. 'Baseball' basically breaks down to a few core skills: hitting, catching, throwing. Different people on a baseball team have different mixes of these skills. And some (like pitchers) are highly specialized. Machine learning specialists are sort of like the pitchers of the data science world: highly technical, specialized, and in many cases, the players creating the most value. But, there are also many fewer pitchers on a baseball team (i.e., fewer pitching jobs on a baseball team). &amp;nbsp; Similarly, there are vastly more data science jobs that require a broad mix of data skills instead of pure ML. Companies can't find people that know the full mix of data science skills. 
If you haven't already, read through [pep8](https://www.python.org/dev/peps/pep-0008/) (python enhancement proposal 8) I'm not sure about os.popen, but when handling things outside of the module or script I think it is good to use a [with](http://effbot.org/zone/python-with-statement.htm) statement. With statements do some really nice stuff with scoping that usually make your code cleaner. Also, I recommend the use of the [logging](https://docs.python.org/2/howto/logging.html) module instead of print statements. For a small bit of code like this it's probably not necessary, but if you ever plan on using this code as a module, use the logging utility. It makes it very easy to create debug statements in your code that are filterable and portable. Highly recommend. This may not be as relevant as logging, but [argparse](https://docs.python.org/2/howto/argparse.html) is also a very good utility for running your code from a shell. **EDIT**(new): Here's [a very simple example of argparse](https://github.com/cparker1/CellularAutomata/blob/master/CAMain.py) in a Cellular Automata program A comment on the data collection (the os.popen("cmd text") lines): I have no idea what this stuff is doing. I don't know what the program you call does, or what its output looks like. So the code that manipulates that output is equally confusing and meaningless to me. It's ok to have that kind of code, but I recommend wrapping it in a function that describes what it's doing and has a docstring or something. That way in your main statement (by the way, use ```if __name__ == "__main__":```) you can make a series of function calls that describe what this code is doing. That makes it easier to read and understand the code and help modularize the code as well, making maintenance and improvement simpler. I also recommend you try [learning python the hard way](http://learnpythonthehardway.org/). It looks like you're already going down that path, though. :) Just remember. **EDIT**(phrasing): Python is all about simplicity and efficiency. It has a very good set of built in features that do a huge amount of tasks for you if you know how. It's *totally worth it* to spend time on learning and understanding the rules and nuances of the language. Hopefully these links are helpful. I'm a just coming up to speed with python myself. The resources I linked you to were some of the things that made me go, "damn, this language is so awesome!" over and over again as I learned how useful they were. 
lol, didn't see you post this. good point on switching to 3.x There really is no point to sticking with 2.7 if the project isn't going to touch a big legacy codebase
Why is the GPLv3 license an issue for you? Here's an article by a lawyer regarding the advantages of GPLv3 over GPLv2 : http://lawandlifesiliconvalley.blogspot.fr/2007/07/general-public-license-version-3-legal.html Of course, if you have very particular requirements, GPLv3 might be an issue for you, but it's probably worth investigating whether or not this is the case.
Well, in case you feel like a distraction, here's the Python implementation (short and sweet). Takes a `bytes` or a `bytearray` and returns an `int`: _crc24_init = 0x0B704CE _crc24_poly = 0x1864CFB _crc24_mask = 0x0FFFFFF def test_crc24_iter(data): crc = _crc24_init for b in iter(data): crc ^= b &lt;&lt; 16 for i in range(8): crc &lt;&lt;= 1 if crc &amp; 0x1000000: crc ^= _crc24_poly return crc &amp; _crc24_mask 
Thanks for your input man! Some stuff I hadn't thought of. Appreciate it! EDIT: A bunch of these concerns have been fixed. The one still standing is the fact that dicts are being turned into addicts. I'm going over this to see if that's really necessary. Again, thanks :-) 
thanks all! my pycharm as an interpreter named 3.2mu. is this a good one?
Can you explain what's going on? Super at #3 initializes the standard dictionary object, but how does #4 makes it possible to treat keys as attributes?
`__dict__` is the dictionary that python uses to perform the lookup of attributes on an instance. What that line is doing is replacing that dictionary with the instance itself. This effectively results that when you ask `instance.foo` it will resolve into `instance.__dict__["foo"]`, which due to that replacement is `instance["foo"]`. Additionally, you can subclass from defaultdict instead, and have it return something no matter what attribute you look up. This is probably evil in most cases though.
Actually, it looks like `__missing__` doesn't really work on its own, since it seems python *checks* for the existence of the item in the `__dict__` first (ie. equivalent to hasattr()), so this fails before creating anything when used via attribute access. I think you'd need to provide `__getattr__` as well, though that leads to its own problems (now any random hasattr() call will create new items in the dict, and potentially confuse code using those attributes). (Though it looks like OP already uses this, so will have the same issue too). 
Essentially, the way instance variables work in python is that each object has its own dictionary that contains them. This dictionary is attached to a special name, `__dict__`. Thus if you do something like: `self.foo = 42`, this is the equivalent of `self.__dict__['foo']=42`. The line `self.__dict__ = self` basically makes the object (which inherits from dict) it's own `__dict__`. All attribute lookups / sets thus look first in this dict directly.
&gt; Typography and whitespace It's a web interface. Just use stylish (https://addons.mozilla.org/en-US/firefox/addon/stylish/) or submit a patch to their css. &gt; Adding and removing cells is a pain Actually it's just a keyboard shortcut. Shift + Enter creates a new cell. &gt; It is constrained to browser window "Constrained" is a funny word considering the power of browsers these days. And that you can detect a tab. An rename it. &gt; I have to keep the terminal tab open Cou can also run iPython in a terminal. Or in a QtConsole (which will include plots) : http://ipython.org/ipython-doc/dev/interactive/qtconsole.html &gt; Hard to export ipython nbconvert --to latex --post PDF Good point, but harly a reason to create a new project. Submit a patch. &gt; Opening existing document is a pain I agree. Again, patch, patch, patch. It's open source mate. Don't scratch your itch with a fork, contribute. &gt; Matplotlib defaults are ugly There is a module for that : http://blog.olgabotvinnik.com/prettyplotlib/ I don't mean you are wrong. It's good to improve things. You can help them integrate them. Or make a tool to easily configure iPython to use them. But a whole new project ? That's overkill, and would waste competent people time and energy that could be put into improving the real thing. That being said, I like the idea of having a native app for iPython. But it's so much work man, you can't imagine, the work... 
Thanks. Apart from overwriting the builtin attributes such as 'keys', are there any other ways this can be harmful? Pandas has a similar functionality in its dataframe object, how do they handle this problem? 
I hadn't before. This: https://gist.github.com/gvx/272ef41bdd57dc1f2758 suggests that my implementation is 3x faster than `collections.namedtuple`. I haven't looked at memory consumption yet. EDIT: memory consumption seems to be 22% higher for the regular namedtuple, compared to mine.
Have a look at Bunch: https://pypi.python.org/pypi/bunch/1.0.1 They had some ideas like speed up the thing using C extension. https://github.com/dsc/bunch/pull/5
Unless you added a handler to your program that knows what to do with full_output=1 it probably is an environment variable and needs to go BEFORE Python on your command line
Use `dir()` on the module (or class). &gt;&gt;&gt; from datetime import datetime &gt;&gt;&gt; dir(datetime) ['__add__', '__class__', '__delattr__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rsub__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', 'astimezone', 'combine', 'ctime', 'date', 'day', 'dst', 'fromordinal', 'fromtimestamp', 'hour', 'isocalendar', 'isoformat', 'isoweekday', 'max', 'microsecond', 'min', 'minute', 'month', 'now', 'replace', 'resolution', 'second', 'strftime', 'strptime', 'time', 'timetuple', 'timetz', 'today', 'toordinal', 'tzinfo', 'tzname', 'utcfromtimestamp', 'utcnow', 'utcoffset', 'utctimetuple', 'weekday', 'year'] The list starts with all the double-underscore methods and ends with the functions contained in the datetime.datetime module. Is this what you were looking for? 
If the dict. was nested fairly deep, then the long line would be much different then before. You'd then have one long, chained line. Good work though.
One of the big bugs that this might introduce is accidentally overwrite methods or attributes of your interface. For instance from addict import Dict # This will work &gt;&gt;&gt; dicts = Dict() &gt;&gt;&gt; dicts.all.the.way.down = "sausage" &gt;&gt;&gt; dicts {'all': {'the': {'way': {'down': 'sausage'}}}} # This will not &gt;&gt;&gt; my_dict = Dict() &gt;&gt;&gt; my_dict.has.some.items = "bag" &gt;&gt;&gt; my_dict.has.some.items.and.then = "more items" File "&lt;stdin&gt;", line 1 my_dict.has.some.items.and.then = "more items" ^ SyntaxError: invalid syntax 
[Pretty easy](https://github.com/TomOnTime/timetravelpdb)
I agree on the python string formatting, but I would recommend explicitly stating the variable, rather than using `locals()`. '{my_var}'.format(my_var=my_local_variable) That way, you can still do search for `my_local_variable` to see if it is used anywhere in the current function.
Good catch, hard to work around maybe? Could you add an issue?
To be fair, when kitware released cmake, the broad community didnt like it, and abhored the syntax. ...and its now the go to build tool for c projects: because it solved some very specific problems, that, it turns out, were 1) very big problems on windows machines and 2) actually pretty generic. (...except for the diehards who still prefer automake, despite how bad it is) These guys arent dumb, they're really very good at what they do. Theres a lot of interesting stuff in here, if you dig into it (@restful for example)
Thanks, it's getting better every hour since all the attention is generating lots of issues that are being fixed!
&gt; it probably is an environment variable How do you know? Can't full_output be a parameter which mcr.py can be invoked with?
I've done this when the computer I was debugging on wasn't connected to the internet :)
I'm not a big fan of having the constructor be the same name as the normal constructor with just a capitalization change. The human mind is not accustomed to looking for changes in capitalization changing semantics.
The 'and' keyword syntax problem not withstanding, I would argue that once you reach an attribute whose type isn't for storing an indefinite chain of attributes (e.g., a string, a number, a list, anything not dict-like,) you're not really supposed to expect it to continue chaining attributes inside dicts. In your example, that attribute is now set to a string, and needs to behave like a string. You can add all the attributes you want, but at the end of the day, it's still a string. I would think that's expected behavior, and I wouldn't see anybody intentionally expecting it to magically change into a dict. You could still continue nesting using a different attribute name, e.g.: my_dict = Dict() my_dict.has.some.items = "bag" my_dict.has.some.other.items = "sack" my_dict.has.some.other.stuff = ["foo", "bar", "baz"] **edit**: Or am I just not understanding the problem?
I made it like this so it would be easy to swap in! But I see your point :-)
Yep. This is the way to do it.
There's this weird thing that's happening: &gt;&gt;&gt; from addict import Dict &gt;&gt;&gt; a = Dict() &gt;&gt;&gt; a.x.y.z = 5 &gt;&gt;&gt; a {'x': {'y': {'z': 5}}} &gt;&gt;&gt; a.values() [{'y': {'z': 5}}] &gt;&gt;&gt; dir(a) ['__class__', '__cmp__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_delete', '_list_reduce', '_prune', '_prune_list', 'clear', 'copy', 'fromkeys', 'get', 'has_key', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'pop', 'popitem', 'prune', 'setdefault', 'update', 'values', 'viewitems', 'viewkeys', 'viewvalues'] &gt;&gt;&gt; a.values() [{'y': {'z': 5}}, {}, {}] &gt;&gt;&gt; a {'x': {'y': {'z': 5}}, '__members__': {}, '__methods__': {}} If you call `dir` on it, it adds 2 empty dicts (Python 2.7). What I was trying to do is see what happens if one of your keys are actually a method name, i.e. `mydict.values.x = 5`. It won't work, so you'll have to do `mydict['values'].x = 5`. It's a nice idea though, there are so many caveats though.
One other issue comes in with the autovivification in the original version (ie. overriding `__getattr__` to create a subdictionary so you can do stuff like `a.b.c=42` without `a.b` existing). This can create problems when you pass it to code that does `hasattr` checks on various objects to determine what to do. Eg. IPython allows you to define a function on any object called `_ipython_display_`, which it can use to control how to render it (this is how stuff like displaying function results as graphs etc works). Use them in ipython, and all your objects will now acquire this `_ipython_display_` attribute/dict key if you look at them. For similar code that hasn't been coded defensively to also check it's callable, this'll cause an exception when it tries to call a dict. Even where this is prevented, code like this adding extra keys could cause subtle problems down the line. 
Oh wow, have't seen that before! Would you mind filing an issue? Thanks! EDIT: Regarding the dir that is. The method overriding is a known issue I'm trying to adress.
I guess the cool thing about addict is it does this recursively :-)
This exact problem is one of the worst problems with the object model in javascript. Also Dict('a space') is hard to address as an object attribute. You would need to use getattr(), and then a dict is easier.
GPLv3 is often a poison pill for companies, sometimes their lawyers forbid it from being used for fear of accidentally forcing some proprietary doodadd into the public. See also, Minecraft + Craftbukkit
Oh, sure. You can add that with the following line to ADict or BDict def __missing__(self, key): return type(self)()
It's because you're overriding the default .items method from a callable to a string. This can happen to any attribute, you have to avoid them in your code.
You should tell us a bit about this site, as one can't really get what is your website about, which makes suggestions harder to give.
/r/learnpython
 &gt;&gt;&gt; b = BDict() &gt;&gt;&gt; b.a.c = 2 &gt;&gt;&gt; 'a' in b False &gt;&gt;&gt; b.a {} This doesn't work properly though?
Its simple enough you write / rewrite your own
I asked the exact same question at StackOverflow where they decided the question couldn't be understood and delete it, of course, despite getting good answers here at Reddit and also getting the following good answer which I didn't want lost. Stupid StackOverflow. This answer was from someone at SO called "abarnert" If you dir a function or method object, you can see all of its attributes. To understand what they mean, see Data Model, and scroll down to (or search for) "User-defined functions". There are two different ways that functions can be "dotted". First, if you're looking to include the class, or nested function, that a function was defined in, if any, that's __qualname__. (For a plain-old top-level function, __qualname__ and __name__ are the same thing.) This is only available in Python 3.3 and later.* If you're looking to include the module that a function was defined in, you have to put it together yourself from __module__ and __name__ (or, if you want both, __qualname__). mypkg/mymodule.py: class C(object): class D(object): def f(self): pass main.py: from mypkg import mymodule fun = mymodule.C.D.f print('{}.{}'.format(fun.__module__, fun.__qualname__)) Now: $ python3 main.py mypkg.mymodule.C.D.f * For Python 2.6-2.7, PEP 3155, which introduced __qualname__ for 3.3, shows how to indirectly get the same information. For 3.0-3.2, as the PEP implies, it's basically not available without very ugly hackery.
Okay, so first idea on top of my head, the base page layout should stay between all page, to avoid confusion. For example between [this page](http://tomta20014.pythonanywhere.com/user/user_profile/#) and [this one](http://tomta20014.pythonanywhere.com/user/user_thai_study/), the layout changes for no reason. I just also realized that your post may be more welcomed at /r/learnpython, so you might want to try posting there.
I don't. Here's why I thought that: Numerically analyzing magnetically coupled rotors isn't a general problem. So I assume OP wrote it -- and it would be really weird to write code for solving a certain numerical problem only to have it to ask on stdout for command line options you didn't add. But it seems pretty likely that [we're both wrong](https://github.com/scipy/scipy/blob/master/scipy/integrate/odepack.py#L151). SciPy doesn't check the environment and this option that needs to be passed directly to the ode solver. Anyway, this is probably the solver reporting some kind of numerical stability issue. If he wants further analysis he needs to post or point us to the code. We won't be able to debug it as a black box -- it's highly dependent on whatever system he's analyzing.
`a, b = b, a` is not used because it's some clever optimization, that type of nonsense is pointless. It's used because it more clearly expresses the intent to swap the values, and thus is more readable.
I've added a fix to this. You can not set attributes that are native to a dict anymore. I think that lessens the confusion and dangers.
Definately. The implementation has changed now though. Everything are keys in the dict, check it out. It's much leaner and smoother!
The whole point of a stack machine is that the stack is used for calculations. In fact, in CPython you'll find that for the most part it doesn't matter how much work a bytecode does as most of the cost is in modifying objects and interpreter overhead. Adding a third variable inside Python would require going through Python's slow scopes and would have more instructions (so more interpreter overhead). But you can hardly call it a great speed optimization either, when you don't need to `ROT_TWO` at all. PyPy doesn't: 4 0 LOAD_FAST 0 (b) 3 LOAD_FAST 1 (a) 6 STORE_FAST 0 (b) 9 STORE_FAST 1 (a) 12 LOAD_CONST 0 (None) 15 RETURN_VALUE except when the left hand side assignments could have side effects (eg. `a.a, b = b, a.a`).
Small thing, but with [PEP 479](http://legacy.python.org/dev/peps/pep-0479/) coming up, I think you should `return` instead of `StopIteration` [here](https://github.com/jimmycallin/plainstream/blob/bade29ee8cf26f5d1581102599d5f55bff258356/plainstream/plainstream.py#L27)
This looks cool but you could explain more what this is for in the README, e.g. by saying from the beginning that "Brython's goal is to replace Javascript with Python, as the scripting language for web browsers." And maybe providing an example/screenshot. In the link you give the text is rendered as monospace, that must be because there is one or two rst syntax errors in the readme, you should try and fix it, we can't click the links right now.
awesome thank you very much! :-) anything else??
How about doing: from addict import Dict as UseWithCautionDict
I use something like this in many of my larger projects. I love this sort of data structure.
Good idea, didn't know about that myself. Thanks for the heads up, fixed!
I see you've already done that :) I'll make sure to check back in a while
yeah theres many python / programming learning subs I didnt know where to post. But ill take your suggestion and re post it there. thanks.
Oops. def __missing__(self, key): return self.setdefault(key, type(self)()) Hmm... starting to get a bit long. This is another possibility, not sure which is more readable: def __missing__(self, key): self[key] = type(self)() return self[key]
I'd probably just use an underscore as an attribute. The only way that'd be a problem is if you expected to have both `user_name` and `user name` as keys, but that's unlikely.
https://github.com/akheron/cpython/blob/a509d5f7b7676d268d3685277558430b25a6aa3a/Python/ceval.c#L1398-1404 TARGET(ROT_TWO) { PyObject *top = TOP(); PyObject *second = SECOND(); SET_TOP(second); SET_SECOND(top); FAST_DISPATCH(); }
Thank you so much for helping out! Ended up getting everything fixed and a 100% on the assignment.
Thank you so much for helping out! Ended up getting everything fixed and a 100% on the assignment.
Thank you so much for helping out! Ended up getting everything fixed and a 100% on the assignment.
Yeah...I read this explained another way elsewhere in the thread and that definitely made sense. I had just overlooked that "items" is a dict method.
Ha! [write-good](https://github.com/btford/write-good) isn't my program, but it's a reference to a quote from a movie. It's described as: &gt; Naive linter for English prose for developers who can't write good and wanna do other stuff good too. Quote from [Zoolander](http://en.wikiquote.org/wiki/Zoolander): &gt; At the Derek Zoolander Center For Children Who Can't Read Good And Wanna Learn To Do Other Stuff Good Too, we teach you that there's more to life than being really, really ridiculously good-looking.
Yes Please.
Glad you're interested! Leave a comment in the stickied thread, and edit the wiki so your username is next to the game you want to make. Then you're ready to start programming!
This looks very similar to TreeDict: http://www.stat.washington.edu/~hoytak/code/treedict/
&gt; The answer is pretty much what he guessed - in some implementations it is faster. Veedrac has a good explanation. I assume PyPy does not require this kind of thing since it is JIT. I think you misunderstood /u/Veedrac. It's not about speed, it's about correctness. `a, b = b, a` could easily be implemented as "push a, push b, set a, set b" or "push b, push a, set b, set a" (and apparently PyPy at least does that), _except_. Assignment can have side-effects in Python. IIRC not when assigning simple names, but attribute assignment? Item assignment? Absolutely. And Python has a defined order of operations, IIRC like this: `3, 4 = 1, 2`. So the byte code would need to be `push 1, push 2, set 3, set 4`, except it's the wrong way around (thanks to the way stacks work), so you need to do a swap: `push 1, push 2, swap, set 3, set 4` and there you have ROT_TWO.
This is something I could get behind. My older brother is moving into programming and i've suggested him to look into this project. I don't have a lot of free time but i'd like to contribute to this project in some capacity. Interestingly i've thought about starting something very similar to extend my ember.js chops but i'd rather work on a project like this. Kudos to you for making this.
Some minor spelling, but I doubt you mind much
Two other, perhaps simpler, suggestions: 1. Some type of folder system for managing the notebook files. 2. Templates (e.g., Jinja2) integration allowing python environment variables to be easily rendered within markdown cells.
It is constrained to browser window This. Im sick of zooming. Plus a vim like syntax to add multiple indentation would be neat.
But that was your job, you are forcing all clients to make a fix that should have been done by you.
Yes, i'm interested
I think your `_ipython_display_(self)` method has some unexpected behavior. Looks like using tab completion will ovverwrite your base `__dict__` items. http://i.imgur.com/x02ffbG.png 
If you do: print (t.friends.ids(screen_name=TWITTER_HANDLE)['next_cursor']) and it is a non-zero value, that means that there is more than one chunk of 5000 you need to handle like in the linked thread. The same applies to t.followers. To get the additional chunks you'll need to do multiple requests and pass a cursor value in the requests. I found [this link](https://www.safaribooksonline.com/library/view/mining-the-social/9781449394752/ch04s02.html) that has some code samples where a cursor value is being used that may help you.
I suggest you don't read " N Weird Python Tricks Guido van Possum Doesn't Want You to Know " type articles for your programming advice ;-) The main point is that all the calculations on the right are completed before assignment to the names on the left. If names or their values appear on both sides of the assignment then it "does the right thing" when updating the x,y position of a point based on its last position. Their being less chance of a mixture of old and new x being used to calculate y for example compared to if x is calculated then y calculated in separate expressions.
I clicked the password reset link and I got an error. ViewDoesNotExist at /user/user_profile_update/password_reset/ Could not import user_picture_album.views.ProfileRequest. View does not exist in module user_picture_album.views. Request Method: GET Request URL: http://tomta20014.pythonanywhere.com/user/user_profile_update/password_reset/ Django Version: 1.6.6 Exception Type: ViewDoesNotExist Exception Value: Could not import user_picture_album.views.ProfileRequest. View does not exist in module user_picture_album.views. Exception Location: /usr/local/lib/python3.3/dist-packages/django/core/urlresolvers.py in get_callable, line 118 Python Executable: /usr/local/bin/uwsgi Python Version: 3.3.6 Python Path: ['/var/www', '.', '', '/home/tomta20014/.local/lib/python3.3/site-packages', '/usr/local/lib/python3.3/dist-packages/setuptools-7.0-py3.3.egg', '/usr/local/lib/python3.3/dist-packages/matplotlib-1.3.1-py3.3-linux-x86_64.egg', '/usr/local/lib/python3.3/dist-packages/certifi-14.05.14-py3.3.egg', '/var/www', '/usr/lib/python3.3', '/usr/lib/python3.3/plat-x86_64-linux-gnu', '/usr/lib/python3.3/lib-dynload', '/usr/local/lib/python3.3/dist-packages', '/usr/lib/python3/dist-packages', '/home/tomta20014/mysite'] Server time: Sun, 14 Dec 2014 06:15:07 +0000
So rpy2 is excellent for interfacing python and R. What I want to know is what you are attempting to do from python that is being done in R (the essence of why you need rpy2). If you have python for data collection, then great. Have a script of python that runs and dumps your data somewhere (db, csv, txt, etc) and then have an R script that utilizes the dumped data. In your case, and based on the limited information, it seems like you don't necessarily need the two to communicate.
Here ya go: https://docs.python.org/3.3/c-api/ 
Can you provide a link to the vendor's website? That may make it easier to give you concrete advice. I do wrappers in one of two ways: * [Cython](http://cython.org) * [ctypes](https://docs.python.org/2/library/ctypes.html) If you just want to get access to functions in your .so file, `ctypes` might be the easiest way, but IMHO Cython is the way to go if you want to make a more robust and complete Python interface. 
opps sry, yeah i was working on it and broke it 0.o i just fixed it
/r/learnpython would like this
Also see AttrDict, https://github.com/bcj/AttrDict
There is also: http://www.swig.org/Doc1.3/Python.html 
its good for few variables yes, but when there are many it feels awkward to write say action=action, message=message and so on. Besides when using locals() you can find your local variable used in string format immediately where when assigning named variable will drop you to format() call where it is not immediately visible where exactly its used. Its used in this string formatting yes, but not clear in which part. For short string formats it does not matter but for big ones i surely helps a lot.
[cffi](https://cffi.readthedocs.org/) is a great option for C. Unfortunately the equivalent for C++, [cppyy](http://pypy.readthedocs.org/en/latest/cppyy.html) wasn't nearly so straightforward the last time I looked into it.
As already mentioned, ctypes is great for wrapping, although in order to make it work with C++, function declarations must be within an `extern "C"` block. I have done this frequently in the past. It Just Works, and is nice since ctypes is part of the standard library. Another option is [Boost.Python](http://www.boost.org/doc/libs/1_57_0/libs/python/doc/). I have not used this yet, but have considered it for the next time I need to interface C++ with Python.
Dilbert
CFFI is the cool new kid on the block (it also works with PyPy)
Note that the console and QT interfaces to ipython are not notebooks, but rather enhanced REPLs. So if you want ipython notebook, you are "constrained to the browser".
Pointing out that this isn't REST *is* constructive criticism. The person now knows that there is a problem and where to start looking. Plus there's a horrible tendency for beginners in this profession to learn by examples rather than by reading documentation. Calling this "REST" is harmful to beginners and pointing out that it isn't protects them from that harm. In addition to what granger already pointed out, REST APIs don't define a bunch of URI patterns, they define media types. If your documentation lists a bunch of URI patterns, your API is doing the opposite of REST. [Further reading](http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven). 
But isn't it great that Python has the possibility to utilize pure C as plugins? Isn't that a feature of the Python language? Writing everything in pure C would no doubt be faster to execute, but horribly more slow and difficult to program. Python makes programming fast and when using C routines executing quite fast.
Obviously video games make no sense in pure Python, especially modern 3D games. Some may use Python in AI or scripting. I don't think engines are written in Java or .NET either, but I'm not sure about that though. AFAIK the multiprocessing module allows true concurrency if it is really required. Anyway, I still would't accuse Python being a "slow" language, since 95% of the use cases it's quite fast enough (so fast that the user would'n notice anything) and for the last 5% there are ways to bypass Python bytecode in the hard parts, and still be able to utilize the language's cool features.
Just curious why you cant use R to collect and manipulate the data instead of python? Have you heard of [pandas]( http://pandas.pydata.org/ ) library? It allows you to manipulate data in a similar fashion as R. Check out its [documentation]( http://pandas.pydata.org/pandas-docs/stable/) and panda's IO [tools]( http://pandas.pydata.org/pandas-docs/stable/io.html ) to get an idea how you can get data in various formats. If you are trying to retrieve data from a url, you should check out the [request]( http://docs.python-requests.org/en/latest/ ) library. Also you may be surprised there's a Python version of Shiny called [Spyre]( https://github.com/adamhajari/spyre/blob/master/README.md)
&gt; But isn't it great that Python has the possibility to utilize pure C as plugins? Yeah sure. I'm certainly not arguing cpython is unusably slow. It's totally usable. What I'm saying is that *practically* to be fast you have to write c code and writing c plugins in python is a pain in the ass: you end up almost inevitably trading the expressive quick safe nature of python, for a clunky, hard to maintain crash prone piece of software like pygame. There are exceptions; numpy for example, is an excellent piece of software. ...but I can count on one hand the number of really good 3rd party cpython plugins I've used. Much more common: The python api is poorly implemented and crashes (Spotify... :P) or written in pure python and therefore ends up being painfully slow. *shrug* Practically from an ecosystem point of view it means python apps run slowly. Look at calibre. It doesn't have to be slow, but oh man, it's painful to use (compared to say, atom, which is implemented in javascript, which for all the rubbishness of the language, has a fantastically optimized runtime).
I think you meant ctypes. This C API looks like it's for porting Python things to C/C++. OP wants the other way around.
I am just going to pick out on some points. &gt; This should be left up to the user. What looks good is subjective to begin with This is not true. There are many acclaimed books about topic of information design ([The Visual Display of Quantitative Information](http://www.amazon.com/The-Visual-Display-Quantitative-Information/dp/0961392142) by Edward Tufte for example) that set up many rules about quality information design that matplotlib does *not* follow. [Majority](http://www.uie.com/brainsparks/2011/09/14/do-users-change-their-settings/) of users tend to not fiddle with setting, as they do not have time for it. Sane defaults go a long way. &gt; I’d rather you didn't fork!!! The project as it is could use more help. Have you talked to any of the current developers about this. I totally agree with you, my plan is to develop in parallel and push changes back into iPython. I think significant change like this needs more breathing space.
I am planning to use a single web frame (tabbed) to do exactly that. Just a simple app that has all the existing icons and actions in the native toolbar and menu bar. 
I am in same position as OP - Followup Question: Could anyone with experience in several of these options describe some key differences, and pros/cons? For example, I've heard from a friend who uses swig now that boost.python is a living hell to manage headers/wrappers for large projects.
In my experience, Python(x,y) is the most "native" feeling scientific Python distribution for Windows. Glad to see it's still an active project.
&gt; If you just want to get access to functions in your .so file, `ctypes` might be the easiest way I agree. Assuming you’ve already got a driver available for loading as a .so file, you don’t need much more than this: import ctypes driver = ctypes.CDLL("driver.so") result = driver.driver_API_function(arg1, arg2) Watch out for the default assumptions that `ctypes` makes about argument and result types. If they aren’t what you need for some API function, you can specify the correct types, for example: driver.driver_API_function.restype = ctypes.c_int driver.driver_API_function.argtypes = [ctypes.c_long, ctypes.c_char_p] The [documentation](https://docs.python.org/2/library/ctypes.html) for `ctypes` is helpful if you need to work with more tricky types, “out” parameters, and so on. Edit: One other thing to watch out for, since the OP mentioned C++, is that `ctypes` does assume a C calling convention (aka `cdecl`). If the API you’re working with relies on C++ tools like exceptions or polymorphism, probably `ctypes` isn’t the best tool for the job.
I used go use python(x,y) a few years ago. Then I discovered WinPython, and never looked back. Then I discovered Anaconda, and never looked back. Even if you don't use Anaconda, check out it's package manager `conda`, which can install binary packages without compiling, and works in any Python on Windows, OSX, and Linux.
Don't do this. This leaks memory. Your attrdicts will never be freed.
I've tried them all actually. I like WinPython, but it won't register on my system for some reason. Anaconda is great, and I use it on Linux, but on Windows there are some quirks that bug me, like window icons not being set correctly. I also can't seem to get PyQwt working with Anaconda, which is a deal breaker for me on Windows, for now anyway. EDIT: I just tried again and got PyQwt working with Anaconda. I had to install PyQt 4.11 then install PyQwt using the [unofficial Gohkle binary](http://www.lfd.uci.edu/~gohlke/pythonlibs/).
Ah, didn't know that was a sub! Thanks.
I didn't, although ctypes does look like the better choice. The CPython C-API is for "...extension modules for specific purposes; these are C modules that extend the Python interpreter..." I was aware of it because I saw it used to [wrap Hunspell](https://code.google.com/p/pyhunspell/) for use from Python. OP wanted to wrap some piece of code for use from Python, so... Edit: the difference seems to be, that with the C-API you write a wrapper *in C* that puts a Python-compatible face on an existing dll, representing its values and methods as Python objects. With ctypes/cffi/swig(?) you write code *in Python* to access methods and values of an existing dll. 
Thing I would appreciate: rewriting the README without the Swedish Chef text and without mixing in Chef, which I've never used and which seems completely irrelevant here.
Forewarning: Anyone using the PyVISA library that comes with Python(x,y) will notice that it now has updates after a dead period of several years. Maintenance has shifted to a new developer and he's made some significant updates, some of which might not be backwards compatible. The new version is certainly better, particularly concerning asynchronous callbacks, but it might break your code if you're not careful.
I really liked Anaconda when I tried it, but it seemed to have a few issues getting certain packages. In particular, there was some Qt-related package I was trying to install with conda which I couldn't, and I also couldn't use pip for it. I'll probably give it another shot in the near future. /u/mooktank's comment below seems to indicate it is possible.
They still haven't fixed their spelling error of "Torando" instead of Tornado.
Why not? They'll be freed by the gc when there are no references to the object. I'm guessing you mean because they set up a cycle, but python has been able to break those since 2.0. The only issue would be if you define a destructor.
Cycle detection only works when there are more than one object involved in the cycle. This is a cycle of an object onto itself. This will not be detected.
You pasted twice the link, rendering it 404
Working link: http://www.developingandstuff.com/2013/12/replay-network-requests-on-you-tests.html
I went through level 2, I think? I liked the first few problems then totally forgot about it. I never thought I would get contacted by Google.
I read somewhere that they review your code if you do enough of them. I'm pretty sure it's a recruiting thing, or I don't know why it would be invitation only. Although, I also don't expect to be contacted by Google. I just wondered if anyone has been.
I didn't know it was invitation only. I read about it in one of the comments here: someone linked it. I figured they would have thousands of people clicking on it after that. I had to google all the Turing stuff.
How did you get invited?
I searched for "python list comprehension" and a message popped up over the search page that said "You're speaking our language. Would you like to try a challenge?" or something like that. Of course, I had been searching for Python stuff all evening long, because I was working on my first ever Python project and trying to figure out how to do things the right way. But the invitation popped up after I searched for list comprehension.
The interpreter translates a program written in Mochi to Python3's AST / bytecode.
Yeah, I just clicked on a link in a reddit comment somewhere, found the crossword puzzle, googled the Turing stuff and then started doing puzzles. I did it for a few days. I was disappointed that the bunny animation didn't change for each level.
You may need to add `302` to the `RETRY_HTTP_CODES` setting. 
Thanks for the feedback. I love mochi, too! I hope that Mochi language is easy to handle as mochi.
I didn't see a crossword puzzle or anything about Turing. Just an invitation challenges that, so far, ask me to define one method based on a ridiculous backstory about bunnies.
This is very interesting. I see a lot of good ideas from other languages, e.g. pattern matching, pipeline operators, etc. The pipeline operator would be especially useful in data processing applications (if it works like R's %&gt;% operator in magrittr). Pattern matching on a dynamically typed language is interesting -- I haven't looked in the source to see how it's done. 
Glad you're interested! Leave a comment in the stickied thread, and edit the wiki so your username is next to the game you want to make. Then you're ready to start programming! *^\(copied ^from ^below ^comment)*
3 or 2? edit: I've had this exact case a while back (Python 2.7). App used gigs of mem and culprit was my attrdict implementation of the exact same implementation as yours. I didn't bother to check for the root cause back then, but your test suggests there may be more to it than just the cycle. Detected it with the (wonderful) objgraph module.
Thank you. That's right.
Any idea when documentation will be posted?
I'm reading and I'm really loving what I see. The only question that come to my mind (now) is... how much is the performance affected?
Wow, I am not the OP but I appreciate the mention of all these. Very instructive! I have slight personal experience with the C-API and none with the others, but here is what I have gleaned from the docs about these. Please feel free to amplify and correct. (1) Python C-API: Part of [Python 2](https://docs.python.org/2.7/c-api/) and [Python 3](https://docs.python.org/3.4/c-api/) although there are small incompatibilities between 2 and 3, requiring minor recoding and maintenance of two versions of a wrapper if support is needed for both. Using the C-API one writes *in C* a wrapper that represents some DLL's methods and values as Python objects. The compiled and linked wrapper can be imported as a Python module. (2) Python Ctypes: part of [Python 2](https://docs.python.org/2.7/library/ctypes.html) and [Python 3](https://docs.python.org/3.4/library/ctypes.html) (don't know if there are 2:3 incompatibilites). Writing *in Python* one defines the name of an external DLL and its argument and result types. Then one can call its methods as if they were members of a Python object e.g. ctypes.cdll.lib_name.method_name(args). There appear to be some platform differences so one might have to write conditional code testing sys.platform or os.uname()? (3) [CFFI (C Foreign Function Interface)](https://cffi.readthedocs.org/en/release-0.8/). Said to work with both Python 2 and 3, except that some results need to be explicitly coded as Byte type for P3. The doc for this one rather confuses me (I would say it is not well-organized as a technical doc, seems to mix levels and leave concepts undefined) so this summary may mis-represent it. It seems one defines the wrapper for the external lib in C code, but puts this within triple-quotes in a Python function call. At execution time the C compiler is called to compile a wrapper dynamically. The stated advantage is that you don't have to translate C concepts into a Python meta-declaration as with Ctypes. To me, a disadvantage is that a C compiler is needed at run-time (the doc gives [a way](https://cffi.readthedocs.org/en/release-0.8/#distributing-modules-using-cffi) to use setuptools to pre-compile the package for distribution). (4) [Cython](http://cython.org/#about) is a compiler for a restricted version of Python. One writes code in the Cython subset of Python and then it compiles to machine language, creating a module that can be imported and used like any other Python module. As a natural byproduct you can write a wrapper in Python(-like) syntax to [call any C library](http://docs.cython.org/src/tutorial/external.html). Also can [call into C++](http://docs.cython.org/src/userguide/wrapping_CPlusPlus.html). Compatible with Python 2 and 3. (5) [SWIG](http://www.swig.org/Doc1.3/Contents.html#Contents) is a generic wrapper-builder for C and C++ code, allowing you to interface practically any C/++ code to practically any scripting language, TCL, Ruby, Python etc. You write a meta-definition of the DLL's methods and types in a SWIG syntax, from which you can generate a C/++ wrapper (basically this translates the meta-declaration into the Python C-API) which you compile and can then import to Python. Supports both Python 2 and 3, and knows about the C-API differences between them. (6) Boost.Python was mentioned here but looking at its [Known Working Platforms](http://www.boost.org/doc/libs/1_57_0/libs/python/doc/v2/platforms.html) page, it was last tested against Python 2.2(!) and its [regression log](http://boost.sourceforge.net/regression-logs/) was last updated in 2006. So probably not supported 8 years on, certainly out of the question for Python3. 
I remember looking at benchmarks that someone did. But I guess that alone isn't credible. For what it's worth, CFFI docs say the same thing: https://cffi.readthedocs.org/en/release-0.8/ 
Well, the change in implementation solved the leak, so I'd say it was garbage. I can't be sure though that there wasn't something else going on.
How does python class call a Mochi class (and visa versa), unless this question is somehow nonsensical.
What's Actor style programming? Is it like object oriented?
I saw the link to an IP address on hackernews a couple weeks ago. That took me to the crossword puzzle and once I did that the foobar challenges opened up. Did it a few times. Fun challenges! Professor Boolean has some crazy plans. I went back to the IP address the other day and it says "the code has been cracked" and the crossword is no longer there. It just says "search on..." so I'm guessing that searching and getting the invite is the only way in now? I could be wrong.
Short answer - no. Longer answer - maybe, but it is very unlike what you probably consider object-orientated code. It's isolated components talking to each other via message passing, rather than shared state. In Erlang, you work with tiny independent processes that can crash, be garbage collected and/or live on different computers than the rest of your program and everything works great.
It's a concurrency abstraction that involves message passing between processes.
When I finished Level 3, a message popped up and said would you like to send your information to a google recruiter? It asked for a name, phone, number and gave an optional entry for URLs. Then you can continue requesting challenges and it says your progress will be reviewed. Something along those lines.
That's true, but I wouldn't necessarily prefer the code without it. The fact that dict iteration returns keys only is sort of arbitrary and non-obvious, so the explicit form does add something to readability.
I made a compile-to-Python language with similar features before ([here](http://breuleux.net/ug/tour.html) and [here](https://github.com/breuleux/ug)), though I've abandoned it since. I have thought about pattern matching a lot in subsequent ventures, though, (specifically [this](http://breuleux.github.io/earl-grey/repl/?eval=help.patterns)) and one feature I ended up finding pretty useful is allowing the match keyword in argument lists and in patterns. For instance, your factorial example could be rewritten like this: def factorial(match, acc = 1): 0: acc n: factorial(n - 1, acc * n) The advantage is that it lets you pick which argument to match on. Alternatively, with nested matching you could write: def factorial: n: factorial(n, 1) match, acc: 0: acc n: factorial(n - 1, acc * n) Just throwing that out there in case you find it useful :) 
&gt; python list comprehension Keep refreshing a search for this and it will eventually slide down :^) 
It depends on the situation. But, for example, instead of dothis() dothat() onemore() Do mylist=dothis(mylist) mylist=dothat(mylist) mylist=onemore(mylist) The later is more clear. Or, wrap your variable in a class, so when you call methods on it, it's clear that the methods are operating on the internals of the object.
Hope I can snag an invite. Thanks for the share, OP.
Since you have done both, I'm curious to hear why you say your applications were a mistake to put into Matlab. I use Matlab at work and am trying to advocate for python when possible. It would be good to hear your experience. I've built an extensive GUI application in MATLAB and quickly found out how limiting it is. For python, I've been putting my efforts into HTML since it can be used for many purposes and platforms.
This is really interesting! I find the project really interesting and will continue to follow its progress.
is the Python interop two-way? i see the Flask example, how about importing Mochi things into Python?
&gt; https://www.youtube.com/user/PyDataTV/videos http://pydata.org/ -&gt; http://numfocus.org/take-action/donate.html Thanks!
&gt; Pattern matching on a dynamically typed language is interesting -- I haven't looked in the source to see how it's done. This is a toy language, but see [line 409](http://jsfiddle.net/f1ycatas/3/) (runnable example on the right). It's an interpreter, which simplifies things a bit, but it's not super complicated to generate specialized code for each pattern. A lot of dynamic languages in the Lisp family have pattern matching, e.g. Racket and Clojure. Personally, I think *all* languages should have this feature. 
I'm really only a hobby developer, but i've found over the past few months. Using workspaces, and a single monitor seems to be a bit more productive for me. Mainly because with the 3 monitors that i have on my desktop i can have so much crap open at once, and i will get easily sidetracked on other crap. I've really been wanting to set up a space somewhere that has only one of my 1440p monitors, and is in a different room from my main PC. That way when i want to work on various projects, I'm in a ready-to-work setting. I've also bee transitioning to vim in the last couple of weeks :D
Shouldn't be much. The PythonVM is still executing the code, so it shouldnt be anything different from the expanded Python equivalent code. You might see some parsing time when you first run a script.
By AST. Python is nothing more then some text. It gets parsed into AST which is then read by the python compiler/vm. You can basically swap out this top layer with other things like Hy (https://github.com/hylang/hy) and now Mochi. The interesting property here is that since everything is AST, you can import Hy into Mochi without any problems. But Mochi can't yet as it needs a MetaImporter.
From what i have seen, it's missing a metaimporter. Hy got this and it's not really hard to write. You basically just let your MetaImporter find the .mochi files and stuff it back into sys.modules as AST for the PythonVM to use.
yeah, i also wrote one, [impala](https://github.com/roman-neuhauser/py-impala).
I love you to
Persistent data types and actors can be implemented in python without too much trouble, e.g. tuples or overridden \_\_setattr__ methods can be used for immutable data structures, which can be used for persistent data types: from collections import namedtuple class Cons(namedtuple('Cons', ('head', 'tail'))): def __radd__(self, x): return Cons(x, self) nil = Cons(None, None) &gt;&gt;&gt; 1+(2+(3+nil)) Cons(head=1, tail=Cons(head=2, tail=Cons(head=3, tail=Cons(head=None, tail=None)))) and actors can be implemented with generators: def actor(): while True: x = yield print("printing " + x + "!") &gt;&gt;&gt; gen = actor() &gt;&gt;&gt; next(gen) # prime the generator &gt;&gt;&gt; gen.send("hello") printing hello! &gt;&gt;&gt; gen.send("world") printing world! Also, a limited form of pattern matching is available with Python's [singledispatch decorator](https://docs.python.org/3/library/functools.html#functools.singledispatch) and it's not hard to make a slightly more powerful version of it using decorators and python 3's annotations, though full pattern matching is inherently difficult to implement efficiently at runtime with Python: class PatternMatch: def __init__(self, fn): self.fns = [] self.register(fn) def register(self, fn): types = [fn.__annotations__.get(name, object) for name in fn.__code__.co_varnames] self.fns.append((types, fn)) return self def __call__(self, *args): for types, fn in self.fns: if len(args) != len(types): continue if all(isinstance(arg, cls) for arg, cls in zip(args, types)): return fn(*args) raise TypeError("Unsupported argument types") @PatternMatch def foo(x:int): return x*2 @foo.register def foo(x:int, y:int): return x + y @foo.register def foo(x:(list, tuple)): return x[0] @foo.register def foo(x): return x &gt;&gt;&gt; foo('asdf') 'asdf' &gt;&gt;&gt; foo(8) 16 &gt;&gt;&gt; foo(7, 2) 9 &gt;&gt;&gt; foo([1, 2]) 1 &gt;&gt;&gt; foo((3, 4)) 3 
All databases will return an error when an attempt to create a connection fails. You could write your code to allocate as many as are allowed and use connections you have established when you hit the error. However every database is different. SQL, NoSQL... in general you want to find the best balance for the operations vs connections. Many databases are faster if you use one connection to insert 1000 rows vs 1000 connections to insert 1 row each. Experiment with varying the number connections each inserting a some number rows to find the best balance between connections and inserts. In SQL databases creating a connection has a lot of overhead, so connections are reused for thousands of rows. If your database connection supports bulk operations they will be much faster than single row operations.
Using pip with a requirements.txt file is far more convenient. The problem for web applications is that some Python web hosting providers do not support running pip for you when a requirements.txt file exists. Instead they require a setup.py file and they will do 'python setup.py install' for you instead. Reading in the requirements.txt file into the setup.py and adding the dependencies there is sometimes therefore done to support installation of dependencies in the absence of pip support. In general though, people these days would just use pip and not otherwise bother with a setup.py for an application. Dependencies in a setup makes more sense though if uploading a package to PyPi.
thanks, we used this one and it worked great
Question. Won't multiprocessing in this case flood the database server with connections? 
Roger that. So multiprocessing would create multiple connections to the database. This is the post I am using as a reference. http://www.reddit.com/r/Python/comments/2naz7x/found_this_interesting_multiprocessing_in_python/
PyCharm. They have a free community edition, or a licensed professional edition. https://www.jetbrains.com/pycharm/features/editions_comparison_matrix.html If you are using it for an open source project, you can apply to use it for free. I use it daily, and it is awesome.
That's a strange thing to need. Are you a mathematician?
First, delete this thread. Second, take a stab at how you would do it, even if just describing the process in words. Third, post your question in /r/learnpython in a way that isn't asking "can you do my assignment for me?"
`:^)`
JetBrains also changed their Student licensing to give pro versions of most of their software at no cost. 
Couldn't you shell out to get pip if it doesn't exist and run it?
I could be mistaken, but to me that was the point. I.e., access to other python tools with minimal modifications.
It's more about the pre-processing of the data than the actual database insertion. edit: I usually use multiple processes to do the heavy lifting pre-processing, and then queue that result back to the master process, which is the single db handler. This also avoids concurrency issues if you're doing something like checking if a record exists before insertion.
I agree with EdditLomax, PyCharm is probably the best IDE I've come across, and it's very good looking and easy to use. Although the only other IDE I've attempted to use was Eclipse, I definitely like the feel of PyCharm and that it has a built in window for viewing your code that you just ran.
I recommend PyCharm as well for Python on Windows. I teach a statistics class and introduced Python as our tool for doing numerical analysis. I had the students use the educational version of PyCharm and it worked really well for them.
Sorry the only thing I can suggest is look at [rPython](http://www.r-bloggers.com/calling-python-from-r-with-rpython/) since I am assuming you want to stay within R and run Python code from within R environment. Which is why I think it is odd you are asking Python people. I would suggest asking R people if you haven't already. If you are looking to run R code from within Python, you can do so using IPython notebook's R magic extension. Here is an [example](http://nbviewer.ipython.org/gist/yoavram/5280132) of that. But not sure how to control Shiny within Python though, so I would just stick with staying within R.
(Shameless Plug) I would love to hear your experience, should you choose to use it, of using LearnDataScience learnds.com It's a collection of IPython Notebooks for self directed learning in data science. Starts slowly with Linear Regression. Leads up to Random Forests on Android motion sensor data. Purely pragmatic. Reusable code in notebook and accompanying helper code. BSD licensed. 
Crazy idea that would take way too much cpu: place every pixel in RGB 3d space and simulate gravity to allow them to collapse into a few colors.
In fact that is what Alan Kay describes as object oriented programming 😋
That's sort of like k-means, except for the complex physics simulation.
Everybody seems to want this, but it's still a bad idea. (What will happen when you have keys named 'get' or 'items'?)
wtf?
That is a biased example because you unnecessarily complicated the non global version. The comparable version would be: mirror = "http://example.com/goodies" downloads = r"d:\stuff\new" def get_stuff(new_hotness, mirror, downloads): fetch(new_hotness, mirror, downloads) get_stuff("python_v9.9", mirror, downloads) Or def get_stuff(stuff, source=mirror, out_dir= downloads): ... get_stuff("pythonv9") Those imaginary functions seem more reusable and clear to me than using a global. But really, its mutable globals that should be avoided, and teaching new users to use immutable globals sets them up to reach for mutable globals in the future unless they understand the difference. 
1. Is this a question or what is this? 2. Indent code 4 spaces for reddit formatting to know it's code.
Try it :-) You can have them as keys, but you can't access them through attributes as those are reserved for the native methods. 
I will recommend the Cloud9 IDE (http://c9.io). It's free for public workspaces, and you can have the premium edition for private stuff. It works really well for Python, except for debugging. The good thing is that you run it in the browser, so it works on all your machines. You get your own VM where you can install all you need. For debugging I use PyCharm or Visual Studio (pytools).
Heh, yeah, it's what OOP was supposed to be. 
CodeAcademy is great, however it really depends on your background. If you already know programming then you would be better off just reading "Dive into Python 3" or something like that (see sidebar). All that being said, it's still a great introduction if you are new to programming, one of my starting resources was codeacademy so I do recommend it personally.
Visual Studio Community 2013 is free (http://www.visualstudio.com/products/visual-studio-community-vs) and works really well with Python (pytools), which I think is included. Very good intellisense and debugging capabilities.
the question on all our minds
Yes
If you are totally new (haven't even touched matlab or the command line) then no. It will introduce you to a small amount of python code and then leave you not knowing what to do next. It's best used used for introducing you to new languages, not programming. Use Learn Python the Hard Way (LPTHW) (it's free), it will get you set up with a python install, teach you how to really use it and leave you capable to continue learning more advanced topics else where. I used Codeacademy first, then LPTHW. Codeacademy is easy but, beyond learning some basic syntax it's useless. Just skip codeacademy and go straight to LPTHW. Once you are ready to learn another language, codeacademy might be an option. There are some other great resources but something that really makes LPTHW standout is that it doesn't simply spoon feed you, it attempts to teach you how to fend for yourself.
No
Yes. this one reason why this sort of thing is a bad idea.
A guide will teach you the syntax, how to solve certain exercises and such. It helps to learn the language, but learning to use it is a different matter. Many have a hard time breaking down their task into the simple steps a computer can understand. I prefer to use what little i can and set myself a goal, like "make a tetris clone" or "read and modify a image file(BMP) from scratch". Then i use google and documentation to work my way forward.
Very cool
I have a small feature request for something I miss a lot in python code at work, which seems like it would also make mochi a bit nicer: foo(=a, =b, =c) {=a, =b, =c} as shorthand for: foo(a=a, b=b, c=c) {'a':a, 'b':b, 'c':c} Or something, the syntax isn't so important. I hear OCaml has this feature with ~ used so foo(~a, ~b, ~c). Obviously that's not feasible here because ~ is already used though. 
I'm going to go against the grain here and give a qualified yes. I have just started learning programming as of two months ago and Codecademy is in my opinion fantastic. While switching between LPTHW and Codecademy, I was initially drawn more to LPTHW for the simple reason that you are actually writing real scripts in real .py files and actually running them, not just putting text in the browser. However, the one (major) downside of LPTHW in my opinion is that more or less, every exercise is a monkey-see monkey-do type of exercise where you literally copy the text that the author gives you. I found it to be extremely wrote and hardly 'learning' at all. In contrast, Codecademy tests you a ton and makes you think hard. In every module, they will give you an example and try to get you to think to produce some similar code that produces the desired effect. As such you have to actually think 'what is the syntax required for this?' and 'what is the logical way I would tackle this question?'. Hence, I feel that there Codecademy, despite being an artificial environment, is very good at forcing you to problem solve (and not just copy some text). Having said all of that, two caveats: (1) If you are the type of person who will read and diligently perform the study drills in LPTHW, the above may not apply. Then you are somewhat self-directed enough 'to test yourself' and don't need to be tested. I found the drills so mundane that I couldn't do them all; so many are about repeating some task 10 times with small variations which isn't really going to make me remember that I need a colon at the end of an if statement when I'm just copying my code from above (cf Codecademy where you will quickly learn correct syntax every time you fail an exercise). (2) I think you should do LPTHW anyway to see what 'real coding' is like. Realistically, what I think is optimum having just completed both is to switch back and forth between Codecademy and LPTHW. Do LPTHW to start, learn how it all works, and if you get a bit bored and want to be 'quizzed' or 'tested', do some Codecademy. Maybe you get up to the dreaded LPTHW exercise 40-41 (Classes) which are explained pretty poorly - at that point switch to Codecademy which explains classes fantastically (holds your hand which many need for OOP), and then back into LPTHW. Rinse and repeat until you finish both! Hope that is helpful :-)
Could you shortly explain how this compares (pros, cons) to e.g. using a wiki system?
I don't agree. It's been very helpful in my usecases :-)
Ok don't listen to Pinewold. This is how it works: There are different databases and different database drivers and they all handle connections very differently. Some give you connection pools, some manage connection pools internally, some use "channels" (lightweight connections within actual TCP connections), some manage transactions differently based on whether queries are sent on the same connection, etc. You have to consider your system design thoroughly. For example, Cassandra uses "channels" which are lighter than creating multiple TCP connections for each "connection" you want. These channels are spread over a small number of TCP connections and are very efficient. You can use one channel per process/thread and it'll work out great. The Cassandra driver manages all this for you. For PostgreSQL (psycopg2), you can create connection pools if necessary. PostgreSQL uses one process per connection on the server side, so consider the overhead of that. It's a very efficient database, but as always, you have to be responsible and first exactly understand the characteristics of it behaviour before selecting a design. If you choose to use a single, or small number of connections accessed by multiple threads, note that if transactions are enabled, everything done by other threads in the middle of a transaction get counted towards that transaction since transactions are *connection* scoped. Also, if you're aiming for efficiency, creating a small number of processes isn't going to help you solve your I/O bound problem. The vast majority of the CPU time for an application such as this is going to be spent waiting for I/O, not doing calculations. Use an asynchronous event loop or something (here's mine if you're interested: https://github.com/veegee/guv). The overall design is much more important for performance than micro optimizations, and this is more true than ever for something like this.
Please do not make generalizing statements.
Looks really nice, though it would be good if you split the code into multiple files. I wanted to check the code but a single, giant file is really hard to read through. Edit: Also, consider moving those css and templates out of the code and into their own files. Using Jinja for HTML templates would be a good idea as well.
But besides that, I agree with you about parsing **non trivial** HTML with regex.. TONY THE PONY and all... ;) http://stackoverflow.com/a/1732454
Your mobile layout needs work, your title, subtitle and login fields are on top of each other. 
doing colors = tuple(c for c in colors if sum(c) &gt; len(c)*50)[:10] at line #5 might speed things up a bit. 
The code is in this [notebook](https://github.com/nens/python-subgrid/blob/master/notebooks/particlemovie.ipynb), which you can paste into the [notebook viewer](http://nbviewer.ipython.org), once it becomes available again. 
One option is to move from PIL to OpenCV. An image in OpenCV is represented as a NumPy array, which is highly optimized for all sorts numerical operations (like finding averages). Basically, all the matrix traversal-summing-averaging you are doing here - it does it for you, fast.
I don't think they care what phone you use. If you are working on Android you'll be given a dev phone to work on. If anything having a Windows Phone would mean you might notice a nice feature that Windows Phone has that Android doesn't and you can either suggest a change or make it yourself. 
Take a look at the algorithm (MMCQ linked therein) used by [these folks](https://github.com/Dannvix/ColorTunes). [Demo](http://owo.tw/ColorTunes/)
That sounds interesting. Do you use data from your own games or are there sources that you can point me to?
I thought about this a while ago and nerver persued it. Totaly great! 
Looks like water current over time, with brightly colored dots leaving the island to demonstrate where the raft might have been pulled. If you didn't know, the escapees left by rafts made of rain ponchos (iirc) and were never found... presumed dead.
i found Python Tools to be incredibly confusing to use due to you sitting in an IDE with 99% of its buttons now doing nothing for you as you code in Python. The run/build buttons and debug buttons do jack now, etc. I really like VS2012/3 but Pycharm is for Python.
PyCharm is really really good.
Coding wars!
A setup.py has some more advantages : - it's only one command to install and install dependancies. - it works with many tools expexting a setup.py - you can use it to install your lib offline, providing it doesn't have online dependancies But for me, the most important feature is : ``` python setup.py develop ``` This allow you to have your package installed AND keep working on it. If edit the code, anything using your package will have the modified version while importing it. Usually, I'll have a setup.py for my standalone libraries and projects, and requirements.txt for non reusable projects such as complete web projects.
When was the last time you tried PTVS? I don't have any of those problems. run/build dont exist. I have run without debugging, send to interpreter, etc. Also, I figured out how to use the IDE fairly quickly. I'm quite happy with it so far. What I like is integration with GitHub, Visual Studio Online is actually kind of neat, Integration with IPython, and the Dark Theme. For what It's worth, I'm using it with the Anaconda distro. On the down side, I don't think anyone would call VS "snappy" or light weight. Still better than eclips IMO. The latest version is actually full on VS. MS recently released it, free as in beer, for non-commercial/small team use. I'm using the Visual Studio Shell + Python tools version. So far my projects have all been web-mining stuff done with Python Scripts written in VS then analysis later done in IPython Notebook.. I'm a beginner so maybe I'm missing something but PTVS suits my use case nicely. 
Last time I used it was like 3 months ago, think 2 new versions came out and that new free VS since I tried. I'll have to peek at it again if I open VS for C (actually been using netbeans for C recently, was perfect for porting java to C) sometime. But currently I'm totally satisfied with Pycharm haha. 
And slow.
I run it just fine, no hickups on a mobile tablet with a dual core i3 in it. It has a bit of a bootup process checking for updates, building indexs yada but after the first 30 seconds there is nothing slow about Pycharm. 
When i checked python tools it was just a basic syntax highlighting with very basic autocompletion. And it could run and debug file. Just notch above doing it in notepad++.
Please provide some history/background/timeline of project.
This is a major point for me. I've moved exclusively from Windows to Mac because everything I was working on was getting published to linux anyway. So all my VM's are running Linux, and it just makes life easier having it all the same. Cygwin isn't really a good replacement for bash and after now 4 months on Mac OS, I'm not moving back.
Are you the creator of Komodo?
That's pretty clever. Plogbook is more simple in the sense that it's purpose is for simple short log/message writing via console with ability to later review those logs in a clear, easy-to-read manner. The cons and pros I can think of for Plogbook vs keeping logs in cms (like mediawiki). Pros: console input, fast, easy and simplistic. Plogbook.py is one file that will work (at least should) on anything and pretty much any python version (2 and 3) so no setup of anything is required and no external modules are being used (except if you want markdown to html conversion) so it's highly portable. Cons: less features and less content management than cms, there's no search or any other fancy html features usual cms has. Plogbook is a static webpage while cms's have databases and all that fancy stuff if you want to something more advanced. Making a page in cms is probably more user friendly experience as well since you have these formatters for you but you could potentially write plog using some html editor using --editor argument. I'll definitely keep working on Plogbook so let me know if you have some suggestions!
How doesn't it? You only have 1 file to keep track of no templates, configs or anything of the sort, just a .py file that you use and that has everything you need in it. It is similar how a lot of windows portable apps work, just one .exe file that has everything in it.
I think you're conflating code organization and binary distribution. A single .exe file is almost always built from multiple source files. The correct approach is to organize your code in a way that optimizes readability by programmers, and then use these source files to build a single distributable binary if desired. I'm guessing Python has lots of options for distribution tooling, but here's the first doc I found by Googling: https://docs.python.org/2/distutils/builtdist.html Even a single distributable file doesn't guarantee the file will run in multiple environments, so I don't think keeping everything in one source file is helping with portability in any case.
Cool stuff, I just saw you on the news :)
You could use the [Colorific](http://99designs.com/tech-blog/blog/2012/05/11/color-analysis/) library (or implement it yourself using their algorithm as a base).
I'm interested! Bookmarked!
In my case I had to do that because I wanted to extract the documentation without having to run the module (and install/mock all of the dependencies, if they're not present). 
Among other things creating huge nested json-like dict structures for querying elasticsearch!
I think you should test on someone you know who doesn't know how to program and see what questions they raise. It is hard to tell how people who actually need to learn this stuff will react if you already know the concepts.
Thanks for your feedback. I'm going to test them out on my girlfriend. So far she said she likes them because they start from the very basics. But an educated opinion would also be worthy to me. Cheers.
Ok. I think that the tutorials are very nice and not too tedious for beginners. I like the way that you have broken them up into very specific sections- this could also be used as a quick reference when they need to use a specific concept in their program. I think you should add some sort of more in depth explanation in the description, along with some examples of how &lt;insert concept here&gt; is used. I think this would be good as, I know when I learnt to program, I had trouble seeing how I could incorporate list slicing in my programs. Also, I think that you should give short exercises in the description so that people can actually understand how to code, not just know concepts (but if you do make the examples accessible as no beginner will be writing something too complex when they are just starting out). Apart from that, great work! I wish I'd had these videos when I was learning :)
As for the one-file vs multiple argument, why not take advantage of python's ability to execute zip files? The code would be organized, and you'd still only have one file to carry around.
This change also broke the Google AppEngine Python SDK. Making breaking changes in minor versions is a shitty thing to do.
It is not unnoticed, the cost is prohibitively expensive.
The Seafile web client, known as "seahub", uses Django, which follows the model-view-controller (MVC) programming pattern. To find the root of the application, we must first find the handler for the root route. https://github.com/haiwen/seahub/blob/master/seahub/urls.py defines all the routes that the Seafile web client uses. The line (r'^$', myhome), means that the path that starts and ends with the empty string is served by the function named `myhome`. Searching the repo for `def myhome`, I find it in https://github.com/haiwen/seahub/blob/master/seahub/views/__init__.py At the end of that function is a call to `render_to_response('myhome.html', ...`, so I deduce there should be a template called `myhome.html` that is being used to render the homepage. Full path to that template is https://github.com/haiwen/seahub/blob/master/seahub/templates/myhome.html
I'm sorry, the link seems broken, don't know if it's just me though 
You just need to come up with a project and execute it. Something relatively simple but still interesting/useful. Expect to ask a lot of questions and do a lot of googling. I do this for a living and I still mostly google my way through every project that comes my way. Hell, I've copy pasted thousands of lines of VBA into excel and I still don't understand the difference between a function/subroutine, or why some assignments use the Set keyword and others don't.
It appears to be just you.
Pick one, but learn where the differences are between the two.. I suspect most places won't care which you know now, as long as you know the differences, you should be able to pick up the one you don';t know pretty quickly. 
We actually introduced personal pricing a while ago.
Indeed, one of the tips even talks about how python might not be suited for data sets with "tens of gigabytes of data". It gave me a good chuckle. I've worked with terabytes of data in Python. That's not to say it wasn't a useful article and good to read... I just wouldn't call any of the advice here "Big Data". To me, "Big Data" refers to data sets so large that even simple operations like line counts require careful treatment.
When I first heard how about **ipython notebook** I had the same thoughts. Why would you do a suboptimal application by doing it in the browser? Having used ipython notebook a lot, I have to say I was wrong. It is great and really useful. Yes there are things to improve, but I do not think it deserves the negatives comments. And some issues are just out of scope, like the matplotlib defaults. I completely agree with you about it but this is another issue (And you should take a look at [this blog post](http://www.huyng.com/posts/sane-color-scheme-for-matplotlib/), [mpltools](http://tonysyu.github.io/mpltools/index.html) and [prettyplotlib](http://blog.olgabotvinnik.com/prettyplotlib/))
It's not worth worrying about in advance, most everything is transferrable and employers won't consider them two different skill sets.
You can get images as arrays in PIL
Seriously, I think k means would be great for this. You'd get the mean color for each of the major color groups so you'd get several major different colors to work with.
Yeah, you use a distance metric (which with rgb would be simply r+g+b) and set a number of nodes and then iteratively move the cluster nodes so the nodes minimize the distance within each cluster. [Example](https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/) [Fast python implementation]( http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) Feel free to ask for more help. I think this would be cool.
Sounds like a bad deal - Julia's too immature and missing too much functionality for that to be a good deal. By the time you've validated your input record's JSON format - you've added a lot of book-keeping - regardless of what language you choose to use. Let alone start adding transformations, defaults, and logging to the mix. Personally, I'd rather use a language that allows me to start with almost no knowledge of the data, then I can iterate with while incrementally adding validations &amp; transformations as their requirements become more clear. As opposed to, say a type-oriented language that assumes I know all that up-front. Which we almost never do.
 &gt;If I'm understanding k-means correctly, it's the process of finding k clusters of data, where each cluster contains a close grouping. Is that right? Yes &gt;Should each cluster contain the same number of data points? No &gt; Anyway, if that's right, how would it work with something like RGB? Would k-means work on a 3D space? That would mean I could plot each pixel of color with r, g, and b being the three axes. Yes, kmeans works on n-dimensional data (I've used k-modes on 500 dimensions). You dont have to actually plot anything (impossible with 500 dimensions). R, G and B are each one dimension, so (255,0,0) is a point in 3D spaces, and it is closer to (234,10,10) than (100,100,100). I misspoke about distance being r+g+b, it would really be the euclidian distance: (r1-r2)**2+(g1-g2)**2+(b1-b2)**2 where r1 is your pixel's red and r2 is your node's red.
You could try: https://www.youtube.com/playlist?list=PL60jbhF5T10Fkk3Aq5ujhKLZXACBhMU1s - its by another python redditor.
Burroughs is probably my favorite author of the twentieth century but in some ways I lament the cut-up method because while in Naked Lunch it seemed quite appropriate as the context was that of a junkie storyteller coming in and out of consciousness it later became a justification for unnecessarily obfuscated storytelling. One of the biggest issues in art is the blurring of the lines between art and fashion. This is aggravated by the effects of money and markets. Everybody wants to be trendy in order to court the favor of the fashion conscious public. That leads an interesting technique like cut-up to legitimize all kinds of useless plot techniques that really just make stories hard to read so the author can claim --"Well I'm only interested in readers who want to be challenged." I see way too much of this justification in academic literature that just points back to other writers who used it as though that makes it appropriate for their work. There is the stinging rebuttal to this criticism which I've received from some authors whose work I was tasked with editing that cut-up is analogous to scratching in electronic music. Yes, there is an analogy there but for me personally it doesn't strike a chord. There are very distinct differences between music and prose. I can see the analogy carrying through to poetry to a certain extent but it often gets carried over to prose and there I feel it all too often becomes an excuse for obfuscation. Who is going to wade through that stuff? 
Cool. I'd recomend trying with different numbers of nodes on each image, maybe twice or three times per value of n (because of local optima) and keep the one with the tightest clusters. Use the three or so biggest clusters in the best one. The scikit version is implemented in c++ so it should be fast enough.
`submission.title.replace("Big ", "")`
Have you tried [`pefile`](https://pypi.python.org/pypi/pefile/1.2.10-139)? --- In a more general sense, I find that the `struct` module is sufficient that converting data between binary formats and Python objects is relatively painless. I'm curious what you think is "silly" about it. Ruby and Perl both provide similar utilities for packing/unpacking binary data, so there's nothing unusual about it, and it doesn't seem hacky in Python either. EDIT: I would also like to point out that perhaps experience is what makes this task easier in some languages than others. If you have been writing C++ for years and Python a relatively short period of time, it will probably be easier for you to write a PE parser in C++ regardless of how Python is designed.
As someone who's deeply interested computational creative writing, I really appreciate your perspective. The script I wrote was an exercise for a class in my graduate program where we were asked to respond to some readings, one of them being Burroughs' explanation of the cut-up method. It's definitely not favorite method for remixing text. With respect to your scratching analogy, I believe it's apt, but I would note that [this project from one of my classmates](http://www.phasesofsputnik.com/blog/scratched-poems-poetic-collaboration-with-a-mind-machine/) might be a more exact analog.
I like the fact that you do very, very short tutorials. I like the fact that you use something that's readily available without having the user install anything: anyone can try on their own very easily. I don't like the fact that you use Python 2.7 (and the default integer division). If you used http://pyschool.net/ (look at the shell tab on the right hand side) you could use Python 3. You say that variables "hold something": that gives an incorrect mental image of the way Python operates. Variables (on the left-hand side of the equal sign) are synonyms (or labels ... which you did mention) for the object that is evaluated on the right hand side. You introduced multiplication without explaining first what the * symbol meant. For beginners, you are going too quickly there. Overall, I think that this is very promising. I would ignore the suggestion made by someone else to number your tutorials. I predict that, after you have made quite a few, and start getting more feedback, you will likely want to go back, redo a few, add a few more in between existing ones, etc. Numbering your videos would only make it more difficult for you to do that. In spite of my negative comments above, I do hope you are going to continue. You have an overall great idea for introducing Python with these short videos. It's easy to criticize; it is much harder to actually do something (like you are doing).
No, don't do that. from PIL import Image from numpy import array img = Image.open("input.png") arr = array(img) http://code.activestate.com/recipes/577591-conversion-of-pil-image-and-numpy-array/ Opencv may be better if that is still slow.
Don't use for loops with numpy. Theres almost always another way.
I've found every attempt to do serious work on Windows or OSX to be painful. Each can be coerced to mostly work but why bother with the hassle when Virtualbox is free? Mount the "remote" VM filesystem on the host OS and you get the best of both worlds.
I would mostly agree (except for the OSX part ... brew+pyenv+virtualenv and your set). Thanks for your input!
&gt;Sorry, but I spend most of my time parsing binary formats. Python does not work well here, and it's frustrating because it really should. It seems to me like you're trying to use Python even though it's not the best tool for your job. Is there a particular reason you have to use Python? I don't use Python for everything; there are many instances in which a better tool exists for the specific task I'm trying to accomplish.
I'm not well versed in `ctypes` so I can't give you specific advice, sorry. However, my general observation is that Python is designed for references to C data to be abstracted by libraries such as `pefile` or direct C bindings to the language. I don't think the language is designed to make it easy to work with C data directly from Python.
Yes, and it makes me sad :( Cause it's soooo close. 
Yes, I would prefer Python instructional materials were compatible with Windows, which I use.
Quick tip - In the pure python CSV example, instead of a `header_skipped` check on every iteration, a simple `next(f)`, after opening the file, is a much simpler, cleaner way to skip a CSV header.
Same I've done HPC with python, ~3k cores, 2 TB of ram....
I'd definitely be interested in following along as you're putting this together. It's a resource I've looked for in the past, without much success. I'd vote for an *NIX focus.
well, that's confusing. how does it help with creating dicts?
I code on Linux and Mac OS X... So I've never seen VS. I've used PyCharm and love it, I've also used IntelliJ IDEA with the Python plugin and it's very similar. A little less integrated but IDEA can also do Java and other languages. (The IDEA Python plugin is the same codebase as PyCharm)
Not really better, but my strategy is to do something like this: for i, line in enumerate(lines): if i == 0: continue pass I just dislike calling next for some reason. Plus, I get a handy counter I can use if I ever want to add logging.
Good to hear, Thanks!
Thanks for the input.
Yeah any IDE will be fine. Notebook++ is also fine. But I have to recommend the IPython Notebook! It's a really great platform for learning how to code in Python. http://ipython.org/notebook.html
How about the flexibility of python for everything and speed, ease of julia for numerics with numba?: http://numba.pydata.org/ and blaze http://blaze.pydata.org/docs/latest/index.html 
I was just working on some ctypes wrapping today and the same thought occurred to me. It didn't take long to google a solution: http://stackoverflow.com/a/1827666.
&gt; I don't like the fact that you use Python 2.7 I came here to say the same thing. I can not imagine why you would want to begin teaching someone programming on Python 2.7.
Python is cross platform, it doesn't make sense to me to write a book about packaging and leave out Windows.
If you don't know what you want, you probably want PyCharm. It's free: https://www.jetbrains.com/pycharm/download/
Most of the book would be platform agnostic. It's really the parts about environment setup and isolation that are unique to the os. Thanks for your input.
These are fun and will get you thinking about efficiency which is really interesting to some programmers: https://projecteuler.net/problems I also found this to be really useful when I learned Python: http://codingbat.com/python These are good for getting the basics down, but they'll teach you the language and problem solving, not how to be a good software engineer. To do that you need some sort of a project that grows over time so you can learn as it grows. Really, just think of someone you do that's monotonous (and thus usually automatable): checking your 10 best friend's twitter accounts, checking your grades on some website, looking up pokemon stats on a wiki, whatever. Then try to write a program that can do it. Or if you want to, come up with some rules for a game and then start to build that game. And if you get stuck, ask for help: /r/learnpython 
Yeah, me too. To me it feels like everything to do with getting through the file is being dealt with in the same block.
Does anyone really "spend a large chunk of time writing custom CSV loading functions" ?
A few options: * Take a look at https://code.google.com/p/dpkt/ it is used for parsing network packets. But you can build your struct to do parse any binary blob. * Use struct, especially the Struct class and maybe throw in there some pack_into() calls for efficience. * See how dpkt does the magic field packing/unpacking and write your own on top of struct * Keep partying with ctypes, maybe something like this: (struct_type.from_buffer(buf)).value do the "casting" magic you are looking for
Thanks, ill be sure to check it out. 
Text editor. I prefer Sublime Text. Unix shell. Simple scripts can just be called directly from the command line. More complex apps you'll probably create some kind of build system for, but don't worry about that yet, you're still learning. You can use an IDE if you prefer but I usually don't recommend using them. I find that its incredibly valuable to learn about the environment that your code runs in and learn how to interact with and control it. If you ever want to do something practical you won't be able to rely on the "build" button on your IDE. You'll have to learn how to interact with the Operating System sooner or later. Don't wait. Just dive in.
 import ctypes class Point(ctypes.LittleEndianStructure): _fields_ = [ ("x", ctypes.c_int32), ("y", ctypes.c_int32) ] buffer = "\x01\x00\x00\x00\x02\x00\x00\x00" point = Point.from_buffer_copy(buffer) assert point.x == 1 assert point.y == 2 buffer = bytearray(ctypes.sizeof(Point)) assert buffer == bytearray("\x00\x00\x00\x00\x00\x00\x00\x00") point = Point.from_buffer(buffer) point.x = 1 point.y = 2 assert buffer == bytearray("\x01\x00\x00\x00\x02\x00\x00\x00") point = Point(1, 2) buffer = ctypes.string_at(ctypes.addressof(point), ctypes.sizeof(point)) assert buffer == "\x01\x00\x00\x00\x02\x00\x00\x00" Edit: import io # BytesIO as file buffer = io.BytesIO("\x01\x00\x00\x00\x02\x00\x00\x00") point = Point() buffer.readinto(point) assert point.x == 1 assert point.y == 2 point = Point(1, 2) buffer = io.BytesIO() buffer.write(point) buffer.seek(0) assert buffer.read() == "\x01\x00\x00\x00\x02\x00\x00\x00"
I run an open source project that reads an excessively complicated Fortran-style binary file. I've done a lot of binary IO. Parsing a binary file is not that hard, even when you don't really know the format. You should know the general kind of data (e.g. points, element connectivity). Then you just make a print method that prints out the next say 100 bytes as ints/floats/strings and look at the data. Then add a goto method to jump to a specific byte or skip ahead/behind x number of bytes. Finally, check early and often that you're on the byte number that you think you're on with f.tell(). It should be really obvious what data type you have. Assuming you have enough sample files, you can prove you understand the format properly even when the binary spec is wrong, which is more common than you think. &gt; Manually struct.unpack seems silly, Use s = Struct.Struct('4i1000f') s.unpack(data) and it will be faster if you can avoid remaking the class object. Numpy has a method too, which depending on your data format could be faster, but generally will be the exact same speed and even slightly slower, even when you end up with numpy arrays at the end. I've done the tests and it's incredibly disappointing. &gt; ctypes looks like it would make sense, but I can't seem to apply it here. The biggest issue I've seen that I'd like to avoid is casting a float32 as a float64 and then recasting it as float32. I thought I'd be able to get around that with numpy's binary IO, but it was slower. Ctypes might work, but I haven't experimented. Since all my float data is float32s, its' worth a test, but you might not have that issue. &gt; This is easy in C, no-brainer in C++, but a nightmare in Python (where is should be easy!). What's hard? Certainly, struct.unpack isn't any harder. Write a few helper methods.
It would be good to include information about windows. I started out on windows, but once I actually became serious about programing I switched to a Linux based os. I'm sure that most readers of your book will eventually switch from windows, but it's worth putting info on windows in. You might recommend conda, from continuum, it's really simple to use. It's both a package and environment manager (which is really great). It also works on both windows and *NIX, using the same api on all --which makes teaching it through a book easier. I'm actually in your target audience, I'm in the middle of figuring out all of this packaging business. Is there a way to keep up with the development of the book? 
[Dito](http://guidesmedia.ign.com/guides/059687/images/blackwhite/pokemans_132.gif) everything [metaphorm](https://www.reddit.com/r/Python/comments/2pfmrt/new_to_python_need_advice/cmwavcl) said. In Addition, assuming your only experience is codeacademy, checkout ["LearnPythonTheHardWay"](http://bit.ly/lpthw-link) (LPTHW). It will teach you the basics of actually using python and set you up to go out and learn more.
I'm pretty sure this is a promotion for The Imitation Game since the IP address in question was in the trailer for that movie
IPython Notebook is a really great tool to experiment/play around with code, but it's not really a replacement for an IDE, or text editor. That being said, I do have mad love for IPython and it's notebook. 
You'd be amazed...
https://github.com/audreyr/cookiecutter
I agree with all of this. I bounced between a number of different sources. The great thing about python is the large number of resources available to learn it from. If you only use one resource you will miss out. LPTHW is at the very least a decent checklist for things you should know to get started using python. Ultimately, no matter the resource, the most important thing you need to do is write code, lots, and lots of code. Until you actually start to use a language to solve problems or do things of your own design, you will never truly understand the language. 
You can export ipynb files to py files. It really does a lot of things an IDE does. It is good for learning, as I said. 
The `if i == 0` condition still evaluates at each iteration.
[Spyder](http://code.google.com/p/spyderlib/) ?
First take a look at http://learnpythonthehardway.org, than http://www.diveintopython.net and https://docs.python.org/2/faq/. These links are best crush course i know.
Definitely needs work on mobile, hard to use although half the site I can see, background layering etc looks good, would just be handy if I could login/register! Keep at it :-)
Quick look at code, and I guess you're not aware that these two lines of code are equivalent: print('{}'.format('spam'.center(24))) print('{:^24}'.format('spam')) which means, that your (for example) `PlogCategory.__str__` method might look like: def __str__(self, pretty=False): template = u'{:^30}|{:^15}|{:^30}' if pretty else u'{};{};{}' return template.format(self.name, self.plog_count, self.creation_date) You use this center method a lot while formatting anyways, so I just wanted to show you that construct. ;-), so for instance your line 294 might look like: print('|{:^78}|'.format('Saving plog &lt;{}.html&gt;'.format(title))) Second look, and I'm not quite sure why you first define function `DEFAULT_PLOG_FORMAT` (similar: `DEFAULT__CAT_FORMAT`, `DEFAULT_CAT_ITEM_FORMAT`, `DEFAULT_MAIN_FORMAT`, `DEFAULT_MAIN_ITEM_FORMAT`), to call it in: log = DEFAULT_PLOG_FORMAT(template=self.template_plog, msg=msg, cat=cat, date=date, title=title) and not just use: log = self.template_plog.format(msg=msg, cat=cat, date=date, title=title) (similar for other four cases)? I find it redundant. :-)
Your less complicated examples are still using globals though. ... Light dawns. It's not use of /globals/ themselves being argued against, it's /changing/ globals (invisibly, inside a function)! Now I get it. :)
Great question! Some tools, and some techniques: * Data aggregates were stored in a sqlite database. Python dicts for this particular application took roughly 8x the space of the same sqlite database. sqlite's memory-resident mode allowed the data to remain very fast to access, and some kinds of calculations were considerably faster than they would be in python. The server running this analysis was a 32 core server with 128 GB of RAM, and I used python to make the application as parallel as possible. * Individual file sizes were about 2 GB, and there were several thousand of them. The way the data were laid out, I accessed 32 files in parallel at a time, loading them all directly in to memory with `.readall()`. While it's true that using an iterator would save on memory, this method was **orders of magnitude slower** than just reading the entire file at once. In theory, memory-mapped IO could have made this better, but in practice I found that `.readall()` won in every practical use case... and did so with just one single function call instead of sometimes very ugly cacheing code. * Extreme care must be taken to avoid algorithmic asymptotic divergence. Put it another way - shaving a few microseconds off of a small operation can lead to minutes or hours of saved computing time if your algorithm nests five or six loops deep. `map()` and `reduce()` (and the parallel cousins found in `futures` and `multiprocessing`) will help you design code which beats algorithmic complexity. This is of course an active area of research, so look elsewhere for tips on writing good high-speed code... "macro optimization", I call it. * Be very skeptical of third party libraries. A lot of libraries have implicit size limitations after which the libraries crash or - worse - silently lose data. I did a lot of work with Bloom Filters before realizing that all of the third party libraries available at the time were not designed well for the application I was working on, giving very poor performance. Other than that, all I can say is that I used almost entirely `stdlib` tools. Profiling was a big help. Test every line of code to see if it's doing what you expect. Big data requires a lot of care... tools like hadoop/map+reduce are cool but python can do it itself, too.
I don't quite grasp what it does exactly. It's a template to create a basic project layout, but, it also runs at each commit? It wasn't very clear.
Funny no tips on chunking data to load rather than load everything to memory.
Including windows in a book for a serious audience is, in my opinion, noise. Plenty of instructions exist for the subset who are technical enough to think about packaging but for some reason still use Windows. More, with RaspPi and friends sweeping programming clubs and schools, even the kids are using Linux these days (finally!).
Yes I was absolutely amazed at how performant sqlite was when in memory-resident mode. I personally accessed it directly through `sqlite` but you can also use the very solid `SQLAlchemy` package to get a richer interface. Just don't use the ORM, it will not be suited to big data analysis. The main drawback of sqlite for big data is that it is very much single-threaded - if you are doing purely read-only queries then you might be able to get something out of it with the multiprocessing flags, but I spent a good few weeks banging on sqlite to get parallelism out of it and did not yield much fruit. Instead, I redesigned the problem so that different parts of the calculation were parallelized. In a different application, if that wasn't possible, I probably would have switched to hadoop/hbase or even PostgreSQL.
Don't know about thesubdb but when I was working on my subtitle downloader that used opensubtitles I found the hash to be reliable only about 70-80% of the time, so you might wanna add some additional episode name/number checking. 
The other thing I'll say as to the quality of the `stdlib` (which is very high) is that a lot of the 'slowdown' that python exhibits compared to C or C++ is really only noticeable in three areas: * RAM useage, where Python tends to require 2 to 8 times as much memory for the same amount of actual information. This really sucks. * Garbage collection - Python's GC is pretty good but clearly has a big performance impact when doing scientific computing * "warm up" time for compilation and such, which does not have ANY noticeable impact on scientific or big data computing. So basically, just watch your RAM and you'll be fine.
Sure. I will look into it.
What does "viewable workspace" mean? In Eclipse "workspace" means all projects and files that they consist of. And in that respect you have a "viewable workspace" in both Eclipse (with PyDev) and in PyCharm. But perhaps you have a different definition of "workspace". I haven't tried R or Matlab so I don't know what "workspace" means there
I have historically used `struct`, while knowing that it is awkward, error-prone, and slow.
I don't know what area of software you work in where you can afford to switch to a different language as soon as you need to do any I/O.
&gt; how can I convert a byte buffer into a ctypes structure? You can go from structure to buffer, but I can't find the other direction. I'm trying to avoid reinventing something I feel probably exists :) &gt; &gt; Also, I listed PE and ELF formats as an example, same could go for any binary format, including network protocols. This is missing a key distinction: whereas the binary representation (and thus, a C struct) is unambiguous (endianness aside), the on-disk representation is not, unless you know for a fact that it is a linear dump of memory - which most binary formats and network protocols are not. For your solution, /u/cymrow's approach looks like it would work. (It is strange that it does not form an official part of ctypes, but there you go.) An alternative would be to create an array from your bytes and use `ctype.cast` to make it into the object you need. For arbitrary on-disk formats, I have previously ended up rolling my own parsers (using struct, sadly) and pulling them in field-by-field. Trying to read a whole struct at a time places too many restrictions on the content (eg. expecting everything to be fixed length) so I don't do that.
&gt; build the RST like normal You have a different idea of normal to me, because I've never done that before. But thanks!
It is same language with a few quirks between the two versions. If you can program in 2 you can program in 3.
You can also try www.checkio.org, it has set of cool tasks along with web-based IDE with syntax highlighting. I'm using it atm and i'm pretty happy with it. It also has great community, which is kinda important when you are learning something! Good luck, yo!
Oh, that must have felt like a day we'll spent :-s Yes. Changing the default behaviour of a library is not a bug fix.
I think it's simply because people aren't aware of this construct. I've been programming in Python for a decade and have only started to use it recently. I find it is particularly nice if I'm emulating a switch-statement, where I look up a callable in a dictionary. In such a case I want to catch the KeyError from the lookup but not from the call to the callable. So given, switch = { 1: case_one, 2: case_two, 3: case_three, } instead of, try: switch[n]() except KeyError: case_default() I would do, try: case = switch(n) except KeyError: case_default() else: case() edit: fixed typo
Nice usage :)
I am aware of else clause, but I see no point in using it. Code without it is clear to read and works as expected, so why bother?
None, because rewriting things is a huge waste of time and only really needs to be done in a few, very specific cases, or if you have a lot of money to spare for very little gain. Don't get me wrong, go is a very nice language. But I run an app development company, and I've never actually come across a case where I've had to use go over python for anything. Some of our app backends target 100k users. There's no problem a solid event loop with greenlets doesn't solve. Python is much more productive to write in than go.
Everyone's talking about Big Data being gigabytes or terrabytes of data. Nonsense! Big Data is a term describing big data sets. Big data can be a 1Gb CRM database coupled with a hundred MB geographical database, spiced with a couple of sheets from a simple marketing tool. That is what Big Data is about. Forget about what software and hardware vendors are telling you. Big data is all about creating (big) collections of data, that's what the *future* is about. Linking data in new (sometimes not so new) ways and doing new stuff with it. The reason people think Big data is about size is that new startups such as Facebook, Twitter and others have pioneered the concept of Big Data and they just happen to deal with giantic amounts of data (which does make a better base for Big Data). But the concept/idea of Big Data doesn't really deal with the size of datasets. Just my .2 cents.
indeed! I did not use it for years when try/except/else statements does not exist. For me, it can be useful limit the scope of the try/except block but its adoption is very limited. I do not bother :), just asking people why they are using this statement ... or not
Geany? 
Companies with the need of disqus are very few. Most people will never work, in their entire life time, with similar constraints. Hence, swtichting tech, from Python to x or x to y, has to be motivated by productivity, not performances. And Python, is VERY productive.
Eclipse with PyDev - it can connect the interpreter window to a debug perspective and you can view variables, set watches, breakpoints etc. Also does automatic reload of changed modules.
Now, start with Python 3. If you find limited with it, you can always switch back to Python 2. The contrary is harder. Plus Python 3 is more fun.
Posts like this, and the comments that come with them, make me realize how little about practical programming I have been learning at University. I keep reading these posts and trying to agregate something, but oh boy it is hard, I guess it comes with time and more real world experiece. Sorry for the rant... Thanks for the post, found it very helpful.
When I do scientific computing, I usually go for 80% of the code in Python and 80% of the run time in C++ (using boost::python). My rule of thumb is that, if the algorithm is complicated enough to have to be written down on paper before implementation, it goes into the C++ part. 
Like others have stated: you need to define what you want more. Right now it sounds like "I want a notebook, but I don't want a notebook" and that can't be what you mean :P
Attempting to parse HTML with a regex is the ultimate generalizing statement: regular expressions are only valid against regular grammars, of which SGML derivatives are not.
If you are on Windows, IMO Anaconda http://continuum.io/downloads is the best and easiest method to install Python and all important general-purpose libraries. Spyder and IPython are also included in it. On Linux, you can just install everything with your package manager.
Thanks for the feedback! I wasn't aware of that format functionality, which is really cool and definitely saves a lot of typing, so I'll definitely migrate everything to your approach. Regarding why I defined _FORMAT lambdas is because I wanted an ability to easily change the formatting, so if you change up the template you wouldn't need to dig through the code and change every instance where the template is being formatted, you just change the lambda ones. Though it might not be as useful as I thought it could be.
Use an IDE. 90% of the Python community uses either Pycharm or Eclipse with the PyDev plugin Edit: It seems I need some facts to show what I base the above statement on :) Here it is: I teach Python Programming courses (and have been doing so for about three years now). When I don't work for a big IT company which has Python as primary language. So I run across a pretty big user base in my work. Pretty much all my students use IDEs in their companies as do all my co-workers.
Still I would use a regex to grab all urls from a HTML page, or extract the title or similar **simple** tasks. I get it.. the &lt;div&gt; example was rather unlucky but that wasn't my point. 
Wasn't aware of zip execution, that is very cool and might be the best solution here. Thanks!
You can do whatever you like if you're making informed decision. I firmly believe in libertarian programming.
But that check for the first line each time, feels like an anti-pattern... You could just add a comment, explaining the call to `next`.
I'm sorry you're so mad.
I would use cffi instead of ctypes. Especially since it sounds like your familiar with C. See [Struct/Array Example](https://cffi.readthedocs.org/en/release-0.8/#struct-array-example) in the cffi documentation.
It would be far easier and cleaner to do something like: func = switch[case] if case in switch else default_case func() 
Actually, to improve on my own code substantially, even better would be simply: switch.get(n, default_case)() which is neither EAFP (my original) or LBYL (yours) style, but has what you might call 'functional elegance'.
Have you previous experience in programming? A simple text editor with minimal features will go a long way. I'd advise something simple like Notepad++ on Windows, or gedit on Linux machines. I'd listen to Zed Shaw's advice in the section [Warnings for Beginners](http://learnpythonthehardway.org/book/ex0.html) in his Learn Python the Hard Way book. Skipping simple editors and graduating straight to a full-featured IDE or a complex piece of software like Vim, may cause a lot of unneeded stress. Simply put, when you need to graduate to an IDE, you will know why.
You want to make it clear where you expect an exception to be raised and prevent an exception from being catched unintentionally. The latter becomes especially important, if you're calling into libraries that may change in the future. The code being self-documenting and not making debugging a pita is more than enough to bother.
And who, **including this guy**, hasn't managed to find the `csv` module in Python's Standard Library??? If it's in the standard library, it's vanilla.
This _IS_ a cookiecutter template, so I'm not sure what you are suggesting here...
Well, I was in a bad mood this morning. It does feel good to say it though.
(Not a downvoter, just curious): Could you cite your stat? I am honestly curious about that claim, since my experience hasn't suggested this at all (though I don't doubt that many professional django devs use PyCharm, given the features it brings along)
Lines 39-45 are really strange to me. Instead of attempting to replace all of the extensions with blank and then doing a check, just use `in`: if fileName.split(":")[-1] in videoExtensions: Also don't forget to remove the periods from each video extension in the list. videoExtensions = ["avi","mp4","mkv","mpg","mpeg","mov","rm","vob","wmv","flv","3gp"] EDIT: In your get_hash function you set a block size for reading instead of just using f.read() which gets all the data at once.
There are two major fields of thoughts for a newbie: 1) Use an IDE because it can help you visually see errors and give fairly instant feedback as you are working. It also helps you know the params and other intel on functions that you may not be super familiar with, giving you quick insite into what you're doing as you're doing it. If you side here, then I'd say that PyCharm is far and away the best Python IDE I have seen/tried. 2) Don't use an IDE at all, because it makes you rely too much on the helpers. If you subscribe to this, the typical perks are you learn how to read the docs more often and you more likely commit things to memory as you work since you have to keep jumping back and forth. Here you can pretty much use any text editor, vim, sublime, emacs, notepad++, etc. You really cannot go wrong either way, there are certain benefits to either approach. Personally, I think as a new programmer you have enough to worry about - so work with whichever is more comfortable. Try both your favorite text editor and pycharm out and see which helps you feel better. Once you learn the language more, then learn your environment inside and out, it'll help you be far more productive in the long run. Since I am technically employed as a 'full stack' developer, I use the Intellij family of products because they are functionally the same no matter which language I am working in. (They make PyCharm, IntelliJ, RubyMine, WebStorm, etc.)
I think my point is this: it's incredibly awkward and redundant to do these operations in Python, where it's easier and simpler in C/C++/assembly. Yes, I understand these are different languages for different purposes, but binary processing is the overlap area where I really could use a nice high level language like Python (for my purposes, I'm not concerned about speed... if I was, I wouldn't be using Python! :) ). 
Thanks! The readinto() method doesn't seem to be available in things like mmap, but I'll keep looking. The from_buffer() and from_buffer_copy() looks like what I was wanting though, thanks!
Yeah, I was looking at a stack overflow article suggesting namedtuples. You're right.. not pretty. 
Now, that's a new one. Nice, takes C structure formats (?!). Oh my. I will have to read further into this, thanks for the reference! 
A project created with this template comes with a Makefile that lets you execute all the same commands locally that the CI server will run after you push. Essentially, running `make ci` before every commit ensures the build will pass (barring any later integration issues).
I think I will have to spend sometime understanding what this all means but thanks for the feedback :)
[citation needed] I've been developing in Python professionally for 5 years and in that time I've known exactly 1 developer who used an IDE for Python development.
What about static analysis? Do you install your project into a virtualenv to test installation against a particular set of dependency versions? The `Makefile` is also very Windows-friendly, if that's your thing. ;-) 
Your definition of Big Data means that you can only do primitive analyses with your "Big Data".
&gt; even the kids are using Linux these days music to my ears
I've used Thecus brand NAS and run python scripts directly on them. Just simple ones to age files, archive things to S3, etc. Python standard lib is python standard lib. Just make sure it isn't a custom crafted python install, missing tons of components.
Not mine but it might be handy at work.
With a typical project, you would perform the following sequence of steps to set up your test environment: $ virtualenv env $ source env/bin/activate $ python setup.py develop $ pip install py.test $ py.test tests/ The `Makefile` handles the setup automatically so all you have to do is run: $ make test If your `requirements.txt` file changes or `setup.py` changes, and you run `make test` again, the changes are automatically detected and the `pip install` process will be run again to ensure that your requirements changes are applied to the virtualenv. Having a "wrapper" around all your common development tasks makes it super simple to get everyone on the team (including [Travis-CI](http://www.travis-ci.org)) to be able to replicate your setup.
Nice straw man. I did not assert that Python is never useful for any I/O. In fact, I think text processing is one of the areas where Python shines in terms of elegance when compared with other languages. Also note that in my other comments, I acknowledge that from personal experience, working with binary formats in Python is not as bad as OP makes it out to be. I have worked with binary file formats and binary network protocols in Python without issue. The point of my comment was that if this is a recurring problem that OP ran into for their specific situation, and they did not feel that the available solutions were adequate, then perhaps it's the wrong tool for the job. I did not claim that it is definitely the wrong tool or that it is the wrong tool for anyone else to do binary I/O in their specific situations.
Author of article here: Yes - the worst offender was somebody who used pandas to load the csv (upon us suggesting it) and then proceeded to meticulously extract lists of dictionaries from the pandas dataframe to work with them. 
&gt; Big Data -------- &gt; Doing the task in vanilla Python does have the advantage of not needing to load the whole file in memory uh...Lol?
I have learned a ton the past several years running (but not necessarily reacting to) PyLint on my personal projects.
Is it better than Python (x,y)? I've been using it and it served me well.
Well, in his example, `function_that_should_not_be_called_if_an_exception_is_raised` might raise its own `Exception`, which would be caught by the try block. This is probably not what is wanted.
PyCharm does that out-of-the-box, although it's not very obvious. The Python Console has a 'Show Variables' button, which sounds like the thing you're looking for.
I'm not saying these are bad practices, just that the page that was linked was failing to show me clearly how those scripts are better. It was confusing. 
I use it too though it can be way to verbose and its output needs to be filtered.
In a normal scenario with no block at all if there is an exception doesnt the script halt? how is function_that_should_not_be_called_if_an_exception_is_raised() going to get run if there was an error? I think this is not needed
Sure, it was a straw man, but I/O is just the edge of a system. If a language is the wrong tool just based on that part, then I'd say that's a problem with the language.
Can we get the answer to this in a FAQ or a sticky? Every one of these threads devolves into ill-informed nonsense.
I have worked on hundreds of terabytes of data on a system that has about 60,000 steps looking for insurance fraud. We basically had a 6 hour timeframe to create reportable data from the raw data. The python we used was primarily designed to orchestrate steps within the data. Unfortunately, python's limitations in both performance and GIL meant that we would then dole out tasks to other processes in other languages. The orchestration was nice, but polyglot has it's own set of development time sinks. TLDR; python can be used in conjunction with other languages as an orchestration rather than a full on data manipulation for big data.
Tens of gigabytes is not 'big data' in any sense.
I'd rather use a `list` if you're just using indexes as keys. I recognize that it isn't the point, though.
There are 2 types of big data tips for python: * how to break data up incrementally or in parallel * just some tips about working with data in Python. 
Anyone who's wrestled with data with a lot of control characters and integration with other tools has.
#mediumdata
&gt;* Individual file sizes were about 2 GB, and there were several thousand of them. The way the data were laid out, I accessed 32 files in parallel at a time, loading them all directly in to memory with `.readall()`. While it's true that using an iterator would save on memory, this method was **orders of magnitude slower** than just reading the entire file at once. In theory, memory-mapped IO could have made this better, but in practice I found that `.readall()` won in every practical use case... This kinda makes sense if you are doing parallel reads to a HDD. Reading the entire file at once would give you much better locality om the reads than using an iterator. Reading 24 files line by line would have a disk head seeking like crazy. This would probably be true with a raid array, but less sure.
That's why it's better to catch specific exceptions instead of the base exception. You can get the exceptions you expect, pass the ones you don't up the stack, and anyone who understands what your code calls are doing can infer which errors you're catching and see why in the catch code.
Why does struct.unpack seem silly? That's what I use and I'm happy enough with it. What is it about using it that you find awkward and silly? The ability to introspect easily into the binary data with just prints and slices is a great improvement over c++ as well and python gives that to you for free.
I think most people would disagree with me but I actually like the old site better.
Why?
The new site looks prettier; the old site was easy to navigate.
Hey, can you elaborate a bit more on what you find to look different in practice than what you've already been learning? If you feel you're not up to speed with the real world, you can try contributing to a well-maintained open source project. Then you get a taste of the real world for free. ;)
My two cents... Whether to use try/except or try/except/else depends on the nature of the exception. When the exception is part of the normal flow (as getting a KeyError for example, instead of checking if the key is in the dictionary), then try / except / else makes sense. If the exception is truly exceptional (as in something unexpected happened), then try/except makes mores sense. Some say that having exceptions be part of your logic flow is a bad practice, but that is a different discussion.
I decided against mentioning compression in the article, because my experience has been negative. We're using avro compression in our ecosystem, but unfortunately the performance so far in python (with the avro and the fastavro modules) is not good enough. And yes, on your third point, some data processing problems are embarassingly parallel - processing fragments on different cores is a good idea, even better when you have a scheduler (like luigi) to launch and keep track of workers and tasks. 
I don't want to nitpick, but I really think the sidebar should be a bit wider.
Too much whitespace. In the old template, you could see things at a glance. Now scroll, scroll, scroll. Even more fun if you're on a laptop.
Big is relative: if your data set is on 100 GB, but you're running on small servers, running many concurrent resource-intensive queries with demanding performance requirements - then you have a Big Data problem. And in fact, it may be more of a BDP than the guy swaggering about his 1 PB hadoop cluster.
Huh.... it looks a bit like a Wordpress default template now.
Because if case() raises a KeyError your code then calls case_default(), which is not what we want. Your code is exactly equivalent to my first example.
Well, different people will either find the article relevant to their big data problems, or they won't. I don't think a size threshold is the only thing that determines this. FWIW I've used python to orchestrate pipelines several orders of magnitudes larger than 10 GBs. 
You're right, it isn't the point. However, if you *did* use a list in this case you would need to specially handle the zeroth item or perform some confusing arithmetic by looking up n - 1.
Kinda nothing. Spyder attempts to, but is no Matlab. PyCharm and WingIDE have decent variable explorers, but only during debug mode. However, they don't work on Matlab's save everything approach. iPython gets close in that it "saves everything", but doesn't have a variable explorer. It's most similar to Maple and Mathmatica and I strongly recommend it. Here's a screenshot. http://en.wikipedia.org/wiki/IPython#mediaviewer/File:IPython-notebook.png
Nuitka doesn't compile to a single file, which is nice for end users
&gt; And something about BitBucket just rubs me the wrong way Why is that? I am using it for a couple of private repos and don't have any complaints. 
I have a plan and enjoy it. I usually use it for things I want to collaborate on others with, though. I also store other repos in Dropbox, like [this blogger](http://jetheis.com/blog/2013/02/17/using-dropbox-as-a-private-github/). It's handy, I have them sync'd across my computers, and yet can still work with the standard git tools. $ cd ~/Dropbox/Git $ git init --bare mytestrepo.git $ cd ~ $ mkdir testrepo $ cd testrepo $ git init $ git remote add origin ~/Dropbox/Git/mytestrepo.git $ git push -u origin master
I dunno, something about it being so GitHub-ish while not being GitHub. I read an article once about how Atlassian basically copied GitHub's UI. Unfounded? Maybe, but that's it.
As a long term solution that deals with types and offers performance, I'm mostly looking at Scala+Spark. However, Julia looks quite interesting. Gadfly especially. Anyone notice how visually it looks so similar to bokeh?
Thats possible, but very narrow use case. I very rarely do anything with exceptions beside logging or wrapping and reraising them, so I did not see a need to be so specific yet. 
How do you get stock data? 
Nope, not alone by any means. The old version never felt dated because it was such a solid design.
I do use GitHub private repos. I believe I'm paying for 10. I've been considering switching to bitbucket because I don't actually use all of those repos and I don't love paying, but I'm super happy with GitHub so I've stuck around. I have both a personal account and an account for my organization. I don't have too many "one-off" django sites, but I would use my personal account for that.
I just checked it out and unfortunately you're only limited to the Windows platform.
For here and there things, it's fine, but it's not suitable for something with a large number of structures, e.g.: elf/pe file parsing. What happens is you end up writing wrapper classes for each structure to hide the nastiness of struct.unpack things with 15+ named fields. For example, the main elf header's format would be something like '16BHHLLLLLHHHHHH', with as many separate field names. Also, the names and formats are separated, and lends to difficult to read and easy-to-screw-up code. With ctypes, at least the types and names are maintained together. My solution (thanks to this thread) was using ctypes structures and the from_buffer() member. A better solution (arguably?) may have been to use cffi (since C structures are easily available from references), but I'm not familiar enough with it.
neat
I found that careful use of multiprocessing allowed us to get around the GIL. Care must be taken though, as multiprocessing has a setup + pickling cost, and obviously can only return values in a specific way.
Well I think it looks nice. Maybe a bit too much (sometimes low-contrast) green everywhere, but in general it looks nice.
Is it really? How much is it worth?
I just run my own git server from under my couch for private repos. Given a vanilla debian server, all it takes is: me@localhost&gt; ssh githost me@githost&gt; sudo adduser git me@githost&gt; sudo -ugit -i git@githost&gt; git init --bare superduper.git ^D ^D # Now set up stuff so you can ssh as git@githost using key-based auth, which is usually just a matter of: user@localhost&gt; ssh-copy-id git@githost # And now we can clone out the repo we made earlier: user@localhost&gt; git clone git@githost:superduper.git 'Course then you'll need to take care of backups and all, but if you don't mind giving a third party service access to your private repos (which, considering how you are willing to pay for github, I assume you don't), mounting a dropbox folder onto git's home shouldn't be a huge issue.
This solution ignores microseconds if your input has them (which the `datetime.isoformat()` method includes by default). And what's with the `*` and the indexing at the end?
You mentioned 5 minutes, but ask for 5 deliverables. Any well put together step by step guide is going to take at least an hour for quality work. In addition, this person will need to familiarise themselves with this module so that they don't give you unreliable information, which will take a couple of hours. I would say you are looking at six hours here, for quality work. A software developer values his or her time at probably about 20-60 dollars per hour, so you are looking at probably 120 dollars here, minimum... For good work. Some non-English-speaking dude from India might do it for 10-20 bucks though. YMMV.
I use GitHub (free) for public repos, and BitBucket (free) for private repos. 'Cause what I need primarily is web-hosted repos, not the fluffy features GitHub/Bitbucket offer. As for GitHub, 10$ a month for private-repo is too much for an entry-level plan (then again my repos are small is size and never abused). I would gladly switch completely if they proposed lower-level plans. Something like 2$/month or 20$/year sound reasonable and I could live with volume constraints. As stated in the comments already, nowadays you can get virtual private servers plan for less than 5$/month (including backups and reasonable performance), and use it for hosting Git repos as well as your project's ecosystem. 
In our case, much of what we spent time on was moving data in and out of Postgres rather than manipulating it within python. We had to run a full report on several petabytes of data, but the raw differential data was only a few hundred terabytes. 
Well for a start I'm a python enthusiast and I'm always lurking around here. I've been through all the basics of data structures and also AI algorithms (think of genetic or neural network stuff). I have some basics in Java Web and would probably say a decent knolowedge of Computer Vision with OpenCV. Yet, when it comes to complex applications, I would say I lack the experience (not sure if that will make sense haha). Even though I implemented some basic AI and even some OpenCV descriptor in Python, it does not go beyond that. I am usually trying to use Python, as it is my favorite language to work with, to solve my problems. The last script I'm working on is a group of digital filters for an simulated ECG signal, instead of using Matlab or Excel. (I'm majoring as an engineer so yeah there is that.) For an example, this tips to work with Data/BigData, they were all pretty new to me, and some of them I can't still grasp the full significance. The only real world experience I have is comes from programming some automated test scripts with Sikuli in Java, which was boring as hell, although I've learned some interesting stuff. I read some of the top posts on this sub and people seem to have a really good grasp of the whole language, what tools to use in which situation and how to progress in their work enviroment with Python, be it coming with new solutions or making some random task easier. Or else, how to integrate Python with other real world applications, or properly make a deployable script so other workers can use it. Contributing to a open-source project always looked as a nice idea, but I'm not sure how to start, it is kinda scary, so to say. Thanks for listening to my rant haha.
The easiest approach would be to use the .NET interop APIs (IronPython?) or the native COM interfaces with pywin32. The interface for accessing OST/PST files is the Store Interface: http://msdn.microsoft.com/en-us/library/office/ff862128(v=office.14).aspx .NET Version: http://msdn.microsoft.com/en-us/library/office/bb652780(v=office.14).aspx If you really must parse it yourself, the format is documented here: http://msdn.microsoft.com/en-us/library/office/gg615595(v=office.14).aspx
codementor.io - they have both short and long term help
I think yahoo has a web api for getting historical data. It is open, close, high, and low data for the day. You can get more detailed data, but it is usually not free.
I couldn't agree more with your sentiment of "submit a patch!" over forking the whole dang project. Honestly, I don't usually use notebook over the QT console. It seems most of the OP's suggestions are focused just on the notebook functionality. 
I have done this exact thing in both Windows and Linux (converting complex, Multi-module python scripts into compiled PE and ELF binaries, respectively. Most notably, mechanize and BeautifulSoup are handled by Nuitka well) many times and has made Nuitka one of my most cherished Python tools for distributing applications to users &amp; customers. It does work, and beautifully so.
I agree with the previous commenter that in this day and age there are tons of resources besides books. For example, one that I don't see posted a lot here are conference talks. Search PyCon, PyData, SciPy, etc. on Youtube and Vimeo. There are interesting talks but also tutorial sessions with hands-on code examples. Usually you can find all the code online too and follow along as if you were there. Also, if you're going to read *5* Python books, you need to read the Python docs too. I would put them a head of any book as far as what to read to get started with Python. I started by reading the docs straight through and I feel like that vaulted me straight to the intermediate level somehow. Seriously, they're quite good.
ITT: my data is bigger than yours. *flex* Mutter mutter up hills both way in the snow with only a hex editor.
I don't usually use this pattern becuase, often, FTSNBCIAEIR can *also* raise an exception, creating the following try: function_that_might_raise_an_exception() except Exception as e: do_something(e) else: try: function_that_should_not_be_called_if_an_exception_is_raised() except SomeExceptionMoreSpecificThanException: handle_second_error() and it just ends up being much cleaner to use try: function_that_might_raise_an_exception() except Exception as e: do_something(e) try: function_that_should_not_be_called_if_an_exception_is_raised() except SomeExceptionMoreSpecificThanException: handle_second_error() In addition, depending on the code, you can end up with a situation where one is indented several levels for what is essentially a single likely codepath, making it hard to read.
I thought the old site looked like the old version of Metafilter. 
They shouldn't use green for the buttons and the the icons. Personally I thought the round green icons on the left were links. Also too much green.
If you miss old Docs UI, here it is on ReadTheDocs http://django.readthedocs.org/en/latest/ :)
Late to the game, but you do have templates, configs, etc.., they're just stuffed into the same text file. I guess it increases portability in the sense that I might have to add the -r flag to cp, or drag and select multiple files. To your credit, I've noticed that this is incredibly important when sending scripts to some coworkers (who don't do much programming), because a single file makes it clear what they should run. However, at a certain point it becomes difficult for people to read your code, because everything lives in one giant scope. Also, when you make commits, it's not clear from the files that were changed, what aspect of the program (templates, engine, etc..) were modified. Another complication is that if multiple people are making changes (say, one to the CSS, one to the python code), then they will have a merge conflict when it comes time to push. After looking at the code, though, I felt kind of 50/50 about the whole issue. I prefer dividing the code into separate components, but the entire codebase isn't thaatt long, yet.
I find bitbucket both clunky and "try hard"/"copy cat"ish. On the surface it looks good, but it feels like they are trying too hard to *not* be github while at the same time trying to copy github, it feels forced. Not that github is perfect, or that I support a monopoly on web based git frontends.
Bingo. In many cases you can entirely eliminate globals, but that doesn't make them wholly useless, you just have to know *why*. Its like implicitly modifying a variable vs. returning a modified one.
For me, it's [Core Python Applications Programming](http://www.amazon.com/Python-Applications-Programming-Edition-Series/dp/0132678209) by Wesley Chun. Just check out the table of contents. He gives you insight into seeing the power of Python and allows you to explore what else you want to do. I think docs are cool and all, but project based learning is what I like best and he goes line by line through working code samples. For example, in chapter 7 or 8 (I forget) he codes a functioning web crawler. I've been sticking print statements everywhere to see what works and why and he explains things really well. I'm skipping the first part of the book and diving directly into the web development portion, but the stuff he has in there about GUI programming with Tkinter, Multithreading, and Network Programming all seem really cool.
All this rote memorization is so much fun. . . I'm still a novice and've gone over the basics of CSS, HTML, Javascript, OOP, and now Python, and everything so far seems to be reviewing the same concepts, but hey! Here's how to build a loop IN . . . PYTHON!!! Isn't that neat!?!? (Computer Science writers keep reminding me how exciting every little thing should be.) Do you have any suggestions for what to try to build to take it to the next level? 
Thanks for the information. The second question was based on questionable information found in a blog that said entry-level Ruby programmers could expect to do little more than set-up the IDEs for the junior and senior programmers and write code snippets here and there for APIs or routine functions. 
The new one is usable and I could def get used to it, but the contrast is much worse. The old website had great contrast. If they fix that, I am 100% fine with it.
Make a script deleting all saved password on one OS : aims for browsers but also more and more apps, using a plugin architecture. Bonus if you can make it multi plateform. It's something achievable, but you'll get a sense of usefullness.
GitHub is good. My only beefs are - The issue/task manager is very limited - No personal messages. but they are minor complains. Overall I am really satisfied. Rock solid product, good interface, free. 
Thank you. Sounds difficult. I'm on it. 
Premature optimization. Just ran a quick test In [1]: NUM_ITRS = 1000000 In [2]: %%timeit for i, c in enumerate(range(NUM_ITRS)): if i == 0: continue pass 10 loops, best of 3: 95.1 ms per loop In [3]: %%timeit for i, c in enumerate(range(NUM_ITRS)): pass 10 loops, best of 3: 82.8 ms per loop For a million loops of doing nothing, the difference is 13 ms. There are almost certainly going to be bigger fish to fry once you actually do something in the loop.
I appreciate that, but my personal feeling is that once I open the consumable, I don't want extra work happening -it breaks my expectations that everything is happening in the loop construct. This is all personal style, so do what feels best. The performance hit of the conditional is incredibly tiny, so I defer to what I find most readable.
It hurts my eyes a bit. Don't know, the white background with the green letters is no way comfortable for extended reading.
Get vimium and it's just j j j j ... H
I'd guess 3 makes it a bit clearer at least with their split of strings into b'' bytebufers, and then pure unicode strings... I generally get annoyed at anything involving byte level operations as I've yet to find a way to byte additions , shl, shr, etc, without then promptly &amp; 0xFF masking of the result. Nothing worse than forgetting to mask, and having all your code go sideways.
The old one's color schema make the document looks clean! I miss that!
** Python for Informatics it’s available in here: http://www.pythonlearn.com/book.php You can view all of the “classes” here: https://www.youtube.com/watch?v=G721cooZXgs The above books give you two ways to go. It’s designed to teach the anyone to Python. Beyond this, I find most programming books to be horrible reads. ** Python Practice Book: You might also want to look at this “book.” It’s a book with practice exercises. It’s a “beginners” book, but can be good for refreshing your knowledge. http://anandology.com/python-practice-book/index.html ** Think Stats For statistics (advanced topics; it’s O’Reilly, but not bad): http://greenteapress.com/thinkstats/html/index.html ** Problem Solving with Algorithms and Data Structures A skills builder book (skip the basics; “Computer scientists learn by experience.” &lt;- Amen!): http://interactivepython.org/runestone/static/pythonds/index.html All of these are free. If you read all of them; you be ahead of 90% of “programmers” out there.
It sounds a bit like you're expecting to hit the ground running when you haven't yet learned how to walk. 
I'm not sure where you're getting the 2 sentences thing from. I count 6 sentences + 1 headline. Not that it's a horrible thing, still...just at least be accurate. Below are my screenshots (also 1440 x 900, same page): [Old Documentation](https://dl.dropboxusercontent.com/u/11285872/Screenshot%202014-12-16%2021.37.56.png) [New Documentation](https://dl.dropboxusercontent.com/u/11285872/Screenshot%202014-12-16%2021.37.54.png)
Cool, I'm planning learning some more in depth python (I know only the bare bone basics for programming/scripting) and specifically Django as my Xmas holiday project. So it's nice to see a big change just as I'm looking into it all. Now if I can find a nice html5 boilerplate tutorial I should be good to go.
Or just the spacebar in every browser since the mid-90s ;)
Try splinter for that sort of stuff. It is more for form automation but may work for CivClicker. Beautifulsoup may help you but i dont think it is quite what you need. I once saw a python library that could sample screen graphics in a nice way but I dont recall it.
A crude way of doing it is running apscheduler/cron with a scraper - otherwise you will need to monitor and decode HTTP requests
The trick is not to just use namedtuples, but to actually use the same technique as the code in the stdlib (Lib/collections/\_\_init\_\_.py). You might notice that the way namedtuples are constructed is horrifying. A big string is created, formatted with the appropriate fields, and eval'ed. You can use the same general technique to implement a struct-based system that has similar properties otherwise to a namedtuple, but accepts format strings in addition to the field names. The end result would let you do something like: TCPHheader = typedtuple('TCPHeader', 'Hsrcport Hdstport Isequence Iack Hcontrol Hwindow Hchecksum Hurgent') TCPHeader.read_from(buffer) It's not the most elegant definition, but it was pretty effective (except for the "control" portion, which isn't nicely representable using the struct module, but to be fair also sucks in C).
I just move/rename the directory.
What would you use private messages for? I personally think no private messages is fantastic. I can imagine people getting bombarded with questions etc. 
Make a chrome extension (or whatever browser you use) to do this in the browser would probably be the best way. If you can't do that, see if you can have two different browsers looking at the same civ data at the same time. If you can do this, then use urllib2 or whatever to refresh/scrape the data in a never ending while loop which every 10 seconds or so? It shouldn't be hard to make sure you are getting what data you expect because i'm assuming the layout of the page is pretty static. Then you just need to set your alerts up.
That makes it easy. I like it. I just created this function in my .bash_profile to create and link a git repo once a project is gitworthy, based on your method. # project dir should have at least one file in it. Run in project dir. function gitbox() { git init --bare ~/Dropbox/git/"${PWD##*/}".git &amp;&amp; git init &amp;&amp; git remote add origin ~/Dropbox/git/"${PWD##*/}".git &amp;&amp; git add . &amp;&amp; git commit -m"initial commit" &amp;&amp; git push --set-upstream origin master; }
Here is one comparison - Old: http://i.imgur.com/6OoM5sM.jpg New: http://i.imgur.com/JtFEeSh.jpg And I'm sorry I don't have an Apple-class laptop. I'd also assume a large number of people don't. 
Yeah you are right, I learned a lot from the feedback here and I'll be splitting everything up and use executable zip for portability. I'll probably do it this weekend with few other tiny fixes and features. Thanks for the feedback! 
Here's my lib: https://github.com/boxed/iso8601 It supports most of the ISO8601 spec. The example you have in that post is one out of MANY valid ISO8601 formats.
I host my own using gitlab software. Love it.
if function_that_might_raise_an_exception() actually raises an exception then a jump to the exception handling block will occur. So function_that_should_not_be_called_if_an_exception_is_raised() will never be called. Which means that either of your two examples can be used. 
do you work primarily in Windows?
Great feedback (IPython creator here)! We know we have a lot of work to do, and UI/UX is an area where the project has increasingly complex needs. Don't hesitate to ping the list or our repos with PRs on this front. And I do agree that it may make sense for some of these things to work out for a little while in separate repos until you have a feel for what works well. But I also encourage you not to wait for *too* long: it's easy to end up with a divergent fork that can't be merged. And I promise you, maintaining a full-fledged fork of all of IPython is way more work than I suspect you want to deal with (it takes multiple full-time developers as it is).
Hi, Synology NAS owner here. What would you like to do exactly with Python on your NAS? I bought a Synolog DS212j a while back, thinking like you to use it just as an online hard drive, but it comes with a huge list of applications and packages, so I quickly turned it into my music library, photo library, video storage space and private "Dropbox". It comes with two packages for Python: one for [Python 2](https://www.synology.com/en-global/dsm/app_packages/Python), one for [Python 3](https://www.synology.com/en-global/dsm/app_packages/py3k). However, I've never tried any of those (yet).
I'm actively using the else clause of try, mostly because I feel it aids readability and makes the purpose of the code easier to understand. Also, it avoids catching the same exception if raised from the code that should have been in the else block. It answers the questions in order: - what code raises the exception we want to catch? - what code should be executed if the exception is raised? - what code should be executed if the exception is not raised? - (finally) what code should be executed no matter what? I think it's more readable for the same reason that if condition: return x else: return y is more readable than if condition: return x return y and if condition: x = 4 else: x = 2 is more readable than x = 2 if condition: x = 4 In both cases, the either-or-ness of the code is emphasized. It's easy to see that *all* cases are covered, and that the two branches have the same "level". 
I find this new bicycle shed colour agreeable.
If you're just starting out, you will want to read [Learn Python the Hard Way](http://learnpythonthehardway.org/) If you want to learn to do thing the "pythonic" way, I've found that [Idiomatic Python](https://www.jeffknupp.com/writing-idiomatic-python-ebook/) is a very good book. If you already know Python and you want to learn about a wide area of subjects that can be dealt with in Python, I recommend the [Python Cookbook](http://www.amazon.com/Python-Cookbook-David-Beazley/dp/1449340377). While some cookbooks are somewhat shallow, this book is very different. It provides extensive and very practical information even on complex topics such as multithreading (locking mechanisms, event handling, and so on). It's really worth it. Also, don't forget to simply read and embrace the [pep8](https://www.python.org/dev/peps/pep-0008/) guidelines. They really help you produce good, maintainable Python code.
Thanks for sharing this useful information. DataBAGG offers online cloud storage services that helps to store your files on cloud. https://www.databagg.com/
Never use urllib2 for anything, requests is better in every way.
How is this any better than just having a git repo inside your project folder (which is on Dropbox)? You can still commit, but there's no pushing.
I have an Apple-class laptop, and I still prefer the older website to the newer one, but you could say I'm biased, since that's my first reaction to almost every website redesign.
I'm... not quite sure yet. I already have a webhost for anything that's "live" in production, but I always thought it'd be cool to have python on an always-on server at home. Not sure what I'd do with it though, probably run some home automation or monitoring stuff?
It's a challenge to find the passwords location and how to give rid of them, but the programming itself can be simple And you can make it more and more complex with time (adding a plugin system, make it non blocking, etc). Plus it will exercice several areas of programming, it's really a good way to progress. Take your time. Months is ok.
I think I can relate to this to some extent. If I had read my own article two years ago I would probably be like "Hmm, are timezones really that important? Is keeping track of your data transformations that important?". Joining a startup was beneficial for me in the sense that we all started from this place of not grasping the significance of good practices, but then real life hit us in the face and we realized why we needed to change what we were doing. Joining a big company where the systems are already in place doesn't give you that, this knowledge of exactly why some things are significant (as opposed to just bureauocracy). Regarding Open Source projects: As an example, https://github.com/OpenBazaar/OpenBazaar/ is in a very good shape code-wise and we're always looking for people to write some python tests. 
I couldn't figure out what felt "wrong", but this is it. It seems they also increased the font size, which makes it more aerial, but I don't like it so much. For blog articles, it makes sense to have super airy text, because you're reading through the whole thing. For documentation, you want to quickly identify the paragraph that's related to what you're doing, and I'm not sure the font size / font spacing are ideal for that purpose.
I was browsing the docs yesterday morning and practically watched the update happen live. So that was cool. Glad to see work's being done. I think it's a bit too big currently, but I'll just zoom it out to compensate.
Easier, or more familiar?
The Indian/Paki/Pinoy programmer is your only hope. 
Some people online have designed path to learn data science online for free. I am collecting them here: https://skim.it/u/ThomasV/data-science-learning-path 
The question is really what percentage of people reading the Django docs mind scrolling: there's a considerable amount of discussion in the web UX community suggesting that if there ever was truth to the belief that people won't scroll, it ended somewhere around the Netscape 4 era: http://uxmyths.com/post/654047943/myth-people-dont-scroll It's not just an Apple-class laptop – it's any device with a mouse wheel (very common in the Windows laptop world), touch screen, or a user willing to hit the spacebar or page-down. I'm not saying that there isn't anyone affected by this but that you have to balance a modest amount of extra scrolling – none of these pages are anywhere near fitting entirely on-screen – against the people who find the new design easier to read because of the extra white–space and support for other screen sizes. I would bet that most of the people complaining about it now won't even notice within a week of regular usage.
Thanks! I found the -out flag a few hours after posting this...
[`aniso8601`](https://bitbucket.org/nielsenb/aniso8601) is a good 8601 parsing library. Supports datetimes/intervals/durations in multiple formats. 
Thanks for posting this.
Ah, if you are working solo, that (can be) fine. However if you use this method, you can share a git repo with a small team using Dropbox, treating it like you would a normal 'origin'.
Demo of a match with a Javascript framework: http://benqus.github.io/strategypy-ui/
It's a good question. I'm guessing there are multiple reasons, but one of the biggest is probably that try/except/else isn't a common feature of other languages. By extension, when python isn't your first language, you're not as familiar with it. Even if it's your first language and/or you take a university class in python, it's enough of an "advanced" construct that little time is likely spent on it. That said, I like it. It's good practice for code readability and maintenance to catch exceptions as nearly to where they're anticipated as possible. 
Also the differences in if/else, try/except/else, and for/else can be a bit confusing. * if/else: execute else block when given a negative condition * try/except/else: execute else block when try block is ok (the negative condition is that there is no except block execution) * for/else: execute else block when "negative condition" of for block early exit is encountered
Nope and nope! Haha, to elaborate - I use games that I play that have random or graphical aspects to them: Tf2, minecraft, borderlands, bitcoin billionaire (idle clicker on ios), etc.
I don't have experience with this library so I probably won't be much help. I was curious about this and took a look at the rss2email page. The setup instructions and examples seemed like the rss2email people thought them through and that they want those steps to work for you. If you hit a snag, you should be able to reach out to them. (maybe you've already tried that.) Their email archive is at http://dir.gmane.org/gmane.mail.rss2email and there are instructions to join the email list at bottom of the main github page. One thing I noticed is that you want to be able to "add and subtract any EMAIL email address". I don't think that is supported. From what I saw (quick scan only) is that rss2email supports ONE email account. The example statements that add feeds don't specify an email address and tracking past updates for more than one account is harder than tracking for just one. Here's how I interpreted the install steps. In ubuntu, you don't need to do the manual steps. From terminal (like Putty), you can sudo apt-get install rss2email When that's done, you'll need to run a few commands to set up your email recipient and rss feeds: r2e new thatbird47@gmail.com r2e add http://www.ksl.com/xml/1070.rss r2e run --no-send (create a new item in your feed) r2e run I haven't actually run these steps so i don't want to go overboard on details. You may need to provide details in the config file for how to send mail or mail format. Those are described at https://pypi.python.org/pypi/rss2email/. I'm not fishing for your 'coin and I know my notes don't address all of your bullet points. If you get it working, consider donating to the project. Good luck.
I've never made a chrome extension, but I'm willing to learn. Is there a specific topic/tutorial I should look for to be able to do what I want to do? Thanks!
Because there isn't any choice but to get used to it. So yeah, will have to.
These two things bothered me: 0. `standardUp()` and `largeUp()` are essentially the same function with different `paperLength` and `paperWidth`. They should be replaced with a single function with 2 parameters: `up(paperLength, paperWidth)`. Then you can call that function twice with different parameters `up(20, 14)` and `up(19, 12.5)` instead of calling two different functions. That function (`up`) can also be moved outside the `main()` function (not necessary, but I would have done it that way). 0. Instead of calling `main()` recursively, the main block should be converted to an infinite loop with `while True`. See below for the second point: main(): while True: ...user input (and exit) ...do stuff up(19, 12.5) up(20, 14) As you mentioned, user input can also be simplified like: `if (x == 'e') or (x=='E'):` to cover both cases. I didn't have a deep look, so probably some of the other calculations can also be consolidated. But one step at a time... And: this sort of questions are better suited to /r/learnpython **Edit:** It would be better to simplify the user nput this way: `if (x.lower() == 'e'):`.
I disagree. I think its good to avoid patterns that introduce a 14% greater execution time. It would be a premature optimization if you wrote it one way and came back later to try and speed things up, but it is still best to write code that is not doing things inefficiently in the first place. if 1000000 needless conditional checks are okay here, then what other wasted operations is the rest of the code performing?
Ah, I missed that you meant it to be for multiple people :)
I think everything happening in the `with` block satisfies that aesthetic/practical consideration for me. Doing something different with the first item in an iterable is so common, I don't have the expectation that a loop has to handle the initial item and all the others too. A line or few of setup before a loop makes perfect sense to me, and I find it confusing to handle a one time initial step within a loop.
Formatting: not sure if its the dpaste.org site or not, but you're using 8 spaces for a tab. 4 is better because it allows you to nest more in less space. Also spaces are preferable over tabs in python code. [PEP 8](http://legacy.python.org/dev/peps/pep-0008/#tabs-or-spaces) You repeated code for exiting the program when receiving raw input. You also don't need to check for both 'e' and 'E'. def exit_program(): print '' sys.exit() if x.lower() == 'e': exit_program() You can just do this: print '\n************* Enter E to exit **************\n' instead of this: print '' print '************* Enter E to exit **************' print '' Alternatively you could leave the extra prints in, but in that case you just need print Instead of: print '\n\nFor a %s x %s sized project:' % (projectLength, projectWidth) you can do this: print '\n\nFor a {} x {} sized project:'.format(projectLength, projectWidth) `format` method handles converting to string More code smells because of essentially exactly the same code for if and else: if layout1WithBleed &gt; layout2WithBleed: print '{} up WITH bleed'.format(layout1WithBleed) else: print '{} up WITH bleed'.format(layout2WithBleed) if layout1NoBleed &gt; layout2NoBleed: print '{} up NO bleed'.format(layout1NoBleed) else: print '{} up NO bleed'.format(layout2NoBleed) You *could* fix that like so: print '{} up WITH bleed'.format(layout1WithBleed if layout1WithBleed &gt; layout2WithBleed else layout2WithBleed) print '{} up NO bleed'.format(layout1NoBleed if layout1NoBleed &gt; layout2NoBleed else layout2WithBleed) Some python programmers argue that that solution isn't elegant/beautiful/easy to read though. A version with less repetition + above improvements: [gist](https://gist.github.com/bitpirat/37b8d16e7ba8327c129a)
I've been using Django for several years now and so this could just be my familiarity with the old UI speaking, but I think the new site looks terribad. At least make the body text dark gray -- the green is too much.
http://www.reddit.com/r/Python/comments/2p9b3h/pythonxy_2790_available_for_download/
I've heard some good things about brython for this sort of stuff, but I've never used it myself. Personally I use a Greasemonkey script, but it hangs a lot.
* Pep8: Something that bothered me was a lack of apparent pep8. https://github.com/hhatto/autopep8 I highly recommend [Sublime Text](http://www.sublimetext.com/) with [Package Manager](https://sublime.wbond.net/) and [PyLinter](https://sublime.wbond.net/packages/Pylinter). The (relatively cheap) license lets you buy once and install on many. In addition, there's a try before you buy policy. * readability: Picky or not, something I've learned over the past nearly 20 years of writing code... it's really critical for me to write code that I can quickly scan and read to understand at a later date. In addition, if everyone agrees to the same style, then that means that everyone can also quickly read and understand the code and give pointers. To go along with that, your variable names could be improved. When I read a line that uses 'aWithBleed', I have to go back to the assignment to find out what the differences are between 'a', 'b', 'c', and 'd'. Not everyone really follows this, but I think one exit and one entry point per function is a really good practice. In other words, flow within a function should be: initialize local variables and test input parameters, do logic, finalize exit values and return. As opposed to a number of exit points within a function. Finally, and I know you've already identified them as crufty, but it really looks like you could refactor large up and standard up to be a single function with input parameters. * string.format vs string % tuple: The new style of print is a little more 'pythonic' than the old style. Assuming you're using Python2.7, you should consider using the new string.format method. * docstrings: I love that you use them. Sometimes, extra information on return types and parameter lists are helpful. 
Dare I ask why `getch` is needed? Whats wrong with `sys.stdin.read(1)`?
*sys.stdin.read(1)* waits until the return key is pressed before recording the character. *getch* takes a single keystroke, records it, and continues with the program flow.
Absolutely agree with that!
it reminds me the old Facebook Career challenge game: dinosaur island!
| Some say that having exceptions be part of your logic flow is a bad practice, but that is a different discussion. True and Python by design is a language that raises a lots of exception (the classic exemple is the end of iteration on generator that yields an exception). Thus, it can be hard to write a full program without some logic flow in an except block. The extensive usage of exceptions in Python is the thing I like the less in Python.
Some of my thoughts: Line 11: use string formatting "{}{}{}".format(datenow.year, datenow.month, datenow.day) Line 13: Why is vols a tuple and not a list....a list would be notably more efficient, because its a natively mutable data structure. Line 27, 37, etc....what happens if the snap fails? There's no error checking here, and so your entire script will bail out if something bad happens. This is a great moment for try/catch. Line 58: When you see the 'for x in y do z' structure its a great chance to use a list comprehension. Line 62: Again - this would be better as a list. 
windows not much different If you are targeting 2.7 install Microsoft Visual C++ Compiler for Python 2.7 (http://www.microsoft.com/en-us/download/details.aspx?id=44266) first, and you can still install / compile dependencies through pip. 
Same here. I'd appreciate a book focused on Linux much more.
There's a lot of Python related plugins in ST, but one I found really helpful is Anaconda. Helps a lot with the style and also does linting directly for you.
It's often highly recommended to write 'a {x} x {y} sized'.format(x=projectLength, y=projectWidth)' over 'a {} x {} sized.format(projectLength, projectWidth) Not that it matters in a case like this, just saying. Makes it **a lot** easier for others to read the code
[This one is great](http://learnpythonthehardway.org)
And significantly easier to alter the format later if you add more items or want to rearrange them. Named &gt; {}
It's 14% when nothing is happening. If you add even trivial work, the performance differences will shrink. Look, I don't mean to argue that mine is the holy way -if you really care about performance, definitely make the call to next(). My use case almost always boils down to IO, so I don't care about a few wasted cycles. If performance really mattered to me, I wouldn't write in Python. In [1]: NUM_ITRS = 1000000 In [2]: SAMPLE_LINE = "a,b,c,d,e,f,g,h,i,j,k" In [3]: %%timeit for i, c in enumerate(range(NUM_ITRS)): if i == 0: continue SAMPLE_LINE.split(",") 1 loops, best of 3: 548 ms per loop In [4]: %%timeit for i, c in enumerate(range(NUM_ITRS)): SAMPLE_LINE.split(",") 1 loops, best of 3: 521 ms per loop
Damn. I made quite a disaster. Sorry for that. I meant: how would I implement a Python Shell into a HTML? I had a python file that [runs fine in Python's IDLE program.](https://i.imgur.com/2DuOi7L.png) The link I had to Zork has something of that like a terminal, similar of what I'm trying to recreate. Utilizing the source code of Zork's site, [I have obtained an input field](https://i.imgur.com/hoiz2MQ.png), but not the display field with the results. What I meant by redoing my python file is by writing it entirely into a different language to be compatible with HTML. For instance, Java. Skulpt, which I found my looking up help as to implement a Python Shell into an HTML, [does not work with Python versions higher than 2.6](https://i.imgur.com/lPxV5xK.png), as said so on their [site](http://www.skulpt.org/) (at the very top of the "interactive" terminal.) My python file is written in Python 3.2. Lastly, I managed to convert it successfully into an .exe file, but has a few [spare files](https://i.imgur.com/b7JXTSP.png) that has to go along with the file to run properly. I had this ready in the case that there is no way that I can actually accomplish a Python Shell in HTML, in which I'll just have a download link to the .exe instead. But I also thought at the time that perhaps HTML needed a .exe file, not a .py file, in which case I would ask if all the files needed to be bunched up in a single executable file instead of being scattered around. Hope this clears it up.
What you could (should) do is write a simple webapplication with Flask for example. The website will send the user input to your webapp via ajax calls. Then in your webapp you can feed the input to your game and send the feedback back to the website. http://flask.pocoo.org/
For everyone trying to learn programming, this is one of the best kind of projects you can get into. If you know how to play the game, you intuitively know what you have to do -- from there you just have to figure out the specifics: how to I click this specific region of the screen, how to drag the mouse, how to store and retrieve data on how to perform specific procedures (in the case of the OP, for example, the recipes for each dish). I wrote two of these, about two years agne for this same game Sushi Go Round [\(code\)](https://github.com/roddds/sushibot) and another for Diamond Dash [\(code\)](https://github.com/roddds/DiamondDashBot) [\(video\)](https://www.youtube.com/watch?v=AQv-z2LBu6w). The Sushi Go Round was one of the first instances where I got to try my hand with some features of object-oriented programming, and how everything worked together from then on just sorta *clicked*. Thank you for doing this Al, I'm a big fan of your work. :)
Correct me if I'm wrong, but typically all function definitions would go above the main block, and main would simply execute the order of things. For example, all the 'up' functions would go above main in their own blocks. This makes it easier for a reader to see the modular chunks of code that constitute functions (black box abstractions) you later reference on higher order in the main(). That way when someone goes and read's main() they see roughly the high view of what's being executed, and roughly what the business logic is, and the individual methods contain the guts of how to get there. What you're doing is called refactoring, and it's critical to good software engineering. I typically write something in a fury, and then go back and refine and make things more efficient / less redundant. gurdulilfo touched on this and he/she(s) spot on.
Depends on what you want to get out of it and how much work you want to put in, as well as what your learning style is. I prefer to read a chapter, start working on exercises, and then go back and reread the chapter. Two books came to mind: http://www.amazon.com/Python-Programming-Introduction-Computer-Science/dp/1590282418 http://it-ebooks.info/book/2467/ The first is a much softer introduction if you don't have prior experience with programming, the second is if you want to get into the guts and details of how what you write executes and how the inherent properties of different things interact with each other. I'd recommend the first and then the second as a follow up. There's some redundancy but reinforcement of the same concepts from different perspectives makes you well rounded. __Monty__'s suggestion is good too. What's the binary method for __Monty__? Can I operator overload? Lol 
[https://www.youtube.com/watch?v=8BiOPBsXh0g](https://www.youtube.com/watch?v=8BiOPBsXh0g)
I guess I could see that. I have been working in Python for over two years now and took a course originally. At my job, I've written several small scripts and used Django to normalize client data to research a CRM system. If there's an intermediate step I've missed or perhaps there's some beginner tasks that might be helpful, please let me know. I'm always willing to take on new challenges. Also, I got Python built from source, cPython, used the developers guide. Totally forgot I had an Ubuntu VM lying around for trying out new things! I'm checking out the Pyladies guide to open source contributions before getting any further. Don't wanna accidentally make a bad commit!
Nice, I have a similar programming game for Python where people write bots that play Zombie Dice: https://github.com/asweigart/zombiedice I'll take a look at this project.
Well, well, well. Seems like we've got another player in the cut throat sushi-automation scene. Seems like the only logical thing to do now is see whose bot can get the highest score! 
I've seen this video before, but he doesn't explain in depth what everything does.. And that's what I'm here for.
I've got a ton of Python books. A few I've classified as intermediate, but if you actually read them, they just cover the same stuff as beginner books with a little more detail. So here's some books for certain topics. Python for Unix - covers networking, SNMP, os stuff like pyinotify which monitors the fs for changes. Also concurrency, guis. Each chapter isn't very long though - maybe 30 pages, so there's a limit to how deep books like that can get you. There's the Python Bible, at 700+ pages It covers: * Part 1: Python lang * Part 2: Files, Data Storage, OS services * Part 3: Networking, Internet * Part 4: User Interfaces, Multimedia * Part 5: Advanced Python * Part 6: App deployment * A Primer on Scientific Programming - almost 1000 pages, covers calculus, discrete math, differential equations etc later in the book. * Beginning Game Development with Python and Pygame * Bioinformatics Programming Using Python * Natural Language Processing with Python * Programming Computer Vision with Python * Mining the Social Web * Violent Python - forensics, penetration testing, security, packet sniffing, keylogging
OK, well if you think of anything just let me know, I can try it for you :) I guess you can also ask on Synology forums.
I got about to 38k. :) Though the top human players break 100k. (Or at least, I assume they are human players.)
I really like the Python Cookbook. It has some interesting and useful tips.
I'd recommend doing a smaller problem by hand.
Thanks for the reply! I have played with pygame a bit, and it seems pretty fun. The Python Bible seems really comprehensive, but after a tiny bit of research it seems to be for python 2 only. Is there an updated version, or are the differences minor enough that it should not be a problem? Mining the Social Web looks really cool as well, thanks!
Why is that something we care about?
It is now. Thanks!
Here are some: * [The Hacker Guide to Python](https://julien.danjou.info/books/the-hacker-guide-to-python) * [Pro Python: Expert's Voice in Open Source](http://amzn.to/1w1rXnj) * [Black Hat Python: Python Programming for Hackers and Pentesters] (http://amzn.to/1wklEjT) update: including "[Core Python Programming](http://www.amazon.com/Core-Python-Programming-2nd-Edition/dp/0132269937)
It's best if you ask here: /r/learnpython 
Python 5? Do you mean 3.4? If so, you have the correct version. If you mean python 2.7, then I believe you need the older version. -edit for clarity- There are two versions of qpython available, one for python 2.x and one for python 3.x 
Packt Publishing have some great python books [Building Machine Learning Systems](https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python) is a great and practical book about ML [Mastering OOP](https://www.packtpub.com/application-development/mastering-object-oriented-python) Is a great, in depth look into OOP [NLTK Cookbook](https://www.packtpub.com/application-development/python-3-text-processing-nltk-3-cookbook) more on ML and sentiment analysis specifically. These are the ones I've actually read, but there's a bunch more. Also, Packt do a ridiculous sale over xmas; absolutely everything is currently $5. Can highly recommend all the above.
Try r/learnpython ?
You should use functions, it's bad practice to put the whole code at the top level. Do at least : def main(): # your code here if __name__ == "__main__": main() But it's really better to use functions for every feature you implement. That will make your code testable, importable and much more readable. Another good practice is to set your API keys in the environement and get them using `environ["aws_access_key_id"]`. That way you won't be scared to share your code ever again. 
Intresting, although i prefer to just use xlrd, xlrt and xlutil for this
Do you have any project running on python ? You mention Django and Flask, have you really made a project from scratch to production using these frameworks ? You don't get better by reading books or learning another library. You get better by building something. Because that's during the building process that you have to solve the real problems. Python is simply an elegant way not an end. It just empowers you build what you want. And guess what, when you follow the Python path, you get better at it. TLDR; Don't learn to learn, learn by building things that matter to you.
Ah
Thanks! I don't know that there's subreddit for that. 
yup, they do the trick whenever in need. 
I like this reply. Learning is only half the battle; start some projects, and maybe post them to github. Go around on github and see if someone wants help with their project. Also, you can do projects with NLTK and scikit-learn if you are interested; those are some other popular modules within (my subset of) the Python community.
I've used the def main() before and forgot about it for this script, will go back an add it now. Good info on the environment keys. I read through that thing line by line like 20 times before pasting to pastebin (and again in pastebin before hitting submit) terrified that I had missed something and it would be stuck on the interwebs forever. Thanks!
So much for readability. 
Sure, it makes sense to go that route. I wanted to point people into a little more straightforward approach if they didn't already have something in place. As the data munging gets more complex, you will probably have to drop into using those tools. I haven't looked at xlutils before. It does have some interesting features. Maybe I'll do a write-up on that at some point. Thanks for the input.
First time I heard of it. Gonna become a heavy user I guess.
I would check out Python Projects, a brand new book by Alan Gauld and and Laura Cassell. You can check out the contents and chapter 1 online for free. Sounds like you have more experience than the audience this book is aimed at, but you never know, you might learn a few things. 
Python Projects, a brand new book that's has been awesome so far! It's aimed at bridging the gap between beginner to intermediate-level. I can't recommend it enough, and you sound like you are in the same position I am.
Did you download the source code for the book? Or type it in? Or copy/paste from the PDF? If you copy/paste from the PDF, you may have an indentation error. Ditto, if you typed it in. Otherwise, see if there is an errata page for the book, and/or source code. Is your problem from the book? If not, try the code with one or more examples from the book to see if you get the same result as published.
So, ultrajson is much faster because it is much worse? I suppose there are some applications for that, but it doesn't make it the best. Also, no love for jsonpickle? (I'm one of the maintainers).
Maybe some pandas topics? If they are familiar with matlab and other environments, it might be cool to show them how to read in some data and run some quick analysis on it.
Thanks, I thought it's almost the same, where hash table is more general concept, but as I'm searching the Web now, I can see there are no easy shortcuts in terminology. Meaningful differences exist.
&gt; In that case, keep in mind that ultrajson only works with well defined collections and will not work for un-serializable collections.
Damn, you are right! But I am still a little confused. I opened Word and I checked with right ALT-o which should give me "ó". It does in fact print this character. Later I pressed right ALT-CTRL-o and Word opened a pop-up window related to Fax Service. So it would seem that it distinguishes between right ALT and right ALT-CTRL. **However**, when I try to create my own keyboard shortcut and press right ALT-o, Word detects... right ALT-CTRL-o (?!!!). I "fixed" my problem by removing all conflicting shortcuts (ALT-CTRL-a/c/e/l/n/o/s/z/x) from PyCharm but I am not satisfied with this. Plus, I would like to understand what is going on (either in PyCharm, my PC or Windows) just for the sake of learning.
MatPlotLib, here is a link I like with examples http://cs.smith.edu/dftwiki/index.php/MatPlotLib_Tutorial_1 Also, Pandas as others mentioned.
Good Idea, we have alot of specific proprietary formats but numpy is for sure on the list. To be honest I'd like to delve more into object oriented design than how to crunch numbers (thats easy for theses guys)
I am becoming a big fan of your posts. I didn't know about the categories feature in pandas either. Good to know. 
Start with a simple script. Then upgrade it to have extensive command line options. Then upgrade it so it can be used a stand alone lib that your script happens to use it. Then upgrade it to include a GUI using the lib. Baby steps, baby steps. The most important thing is to code something that works. You can improve it after.
This isn't for a school. I'm doing this class in my own company that has a mix of various types of applied engineering. I'm expecting a mixed background of experience but in general some people are pretty sophisticated. I should have mentioned that before. I like your suggested cirriculum. I'll def add some stuff to my plan. Thanks :)
I also teach Python to engineering types who come mostly from Matlab. I've found that 99% of the barrier is just getting them to feel comfortable with "How do I install all of this? Where does my code go? How do I run it? How do I share it?". Pick some good development tools (A good IDE, a single-point installer like Anaconda/Python(X,Y), etc) beyond IDLE, and that will help a lot.
Good Point but you could have written this like ;) 'Data Structures ' * 25
Are you part of [Software Carpentry](http://software-carpentry.org)? If not, I would start from their lessons. I am a mechanical engineer, and to be honest I think the skill most sorely lacking among us is version control (Git)--but maybe this is out of scope. Next would be getting set up to even start writing Python (plain text editors and command line, Spyder, IPython shell and Notebook). Many are used to Matlab, where everything is in one place. Next up is the syntax. Assuming they know programming concepts, these three things will get them on the path towards viewing Python as a useful tool.
Ah, I see. Sounds very nice. Thanks!
Even easier, you can poll them for their most done daily tasks. Just don't try too hard on abstract programming (how to make a decorator, advance oo, the wonders of metaprogramming, etc) These are good for programmers, but they will mostly want to get things done right now.
Because potentially, that one LF could mean the difference between tying and winning a competition (if input, including line feeds is counted). Personally, I like it better. 
Pick some topics in this answer http://www.reddit.com/r/Python/comments/2982uc/python_interview_advice/ciic9rd
You don't need an LF to make stdin work- that's just when you're physically typing at a terminal. You can hit C-d to indicate "end of input", which will flush stdin to the program, then exit. Alternatively, `echo -n 'Some Text'` will write "Some Text" (no quotes) to stdout, without an LF, so you can write it `echo -n 'Some Text' | jagli.py -c &lt;program&gt;`
Best approach - [run your own benchmarks](https://gist.github.com/lightcatcher/1136415) 
How do you deal with the uppercase D and trailing space on the last repetition?
SciPy is a lot more flexible than most engineers realize, and it's a hell of a lot more flexible (and cheaper) than MatLab. 
I think this should fix the issue. ... msg['Subject'] = "Email Subject Here" attachment = MIMEBase('application', "octet-stream") attachment.set_payload(open("SOMEFILE.TXT", "rb").read()) Encoders.encode_base64(part) attachment .add_header('Content-Disposition', 'attachment; filename="SOMEFILE.TXT"') msg.attach(attachment ) sendMail(msg) ... 
Interesting results. Is the JSON module in PyPy really _that_ slow?
Who exactly is lucky enough to be in the situation that JSON parsing is the bottleneck in their software?
I just tried it out using the flask example, then noticed that it doesn't support python 3 yet. :(
Or who is unlucky enough to work with large blobs of JSON data.
Looking at the commit history for simplejson: https://github.com/simplejson/simplejson/commits/master It looks like most of the commits are documentation related, bug fixes, or version compatibility. Not a whole lot to do with actually parsing json.
Thanks for the response. This is probably something simple I am missing but, I have to define msg before msg['Subject'] = so I threw this in above: msg = MIMEMultipart('alternative') I assume this is the problem since that's what you changed too. How else could I define msg? EDIT: Sorry, you had it right! It was the msg = MIMEMultipart('alternative') that was giving me the issue. This is the final code: outer = MIMEMultipart() outer['Subject'] = "Subject" fp = open(filepath) msg = MIMEText(fp.read()) fp.close() msg.add_header('Content-Disposition', 'attachment', filename= "filename.txt") outer.attach(msg) composed = outer.as_string() sendMail(composed)
I would focus on a single/few examples where you show them a problem they care about and can solve. Don't spend time on syntax. A few hours is just enough to whet their appetite.
Python 3 support is on the way. Thanks to Emmanuel Leblond: https://github.com/peterhudec/authomatic/issues/50 https://github.com/touilleMan/authomatic But I first want to have all the providers covered wit functional tests: https://github.com/peterhudec/authomatic/milestones
If it's just a few hours once, then you should teach them object-oriented concepts using something like the ipython notebook. If it's just a few hours once a week or something, I'd use the first lesson to familiarize them with the standard python environment(virtualenv, pip, etc) while showing them some data processing goodness(to keep their interest). Next time show them some useful libraries, and show them how to properly convert their formulas to run in Python. Third lesson would revolve around developing their own classes/modules/etc.
Considering that is a list with 25 elements, there is no trailing space anywhere... But just for funsies, a sloppy kludge: Topics = ' '.join(['data structures' for x in range(25)]) Topics = Topics[0].upper() + Topics[1:] 
I would start out with getting a good setup: installing Python, setting up pip and a virtual environment. Give them a quick syntax overview, then move on to using imports to get useful libraries to use, and have them complete an exercise where they have to seek out a useful library for something straightforward, install it, and write a simple function based on it. I would explain what classes are and how they work but not go much further than that. You can do 90% of what you want to do in Python just using functions and no classes, but they should be able to use them from other libraries. I would skip OO stuff altogether,honestly. Truthfully you can even uses classes without being strongly versed in OO. It's just as useful to think of them as widgets you can do things with, and eventually create customized widgets (by subclassing but you don't have to term it that way). I think generators are actually a pretty weird concept to wrap your head around so I'm not sure if they belong in an intro course. List comprehensions, tuple assignment, yes.
Or how much it matters on the deserialization side - say consuming high-volume events off Kafka.
Go online and find yourself a Python cheat sheet that you like. Base a part of your lesson -- I would suggest a decent amount of time -- right from what's printed on the cheat sheet. Tie in how to go from the cheat sheet to the Python documentation, to fill in the details as necessary.
Why the downvote? Is this against regulations?
I agree -- also, I like the simplicity of the task/issue reporting system. If they started expanding it (like adding assignees), we'd end up with a project management system like Jira, which is not what github is for. They do have some good APIs for connecting issues to a project management system, however, to extend this functionality if desired.
Me. We're at the point where we're using the dis lib to find wins - any win is a good win :( 
Yea this is an important first step! I personally like WinPython at work because its portable and comes with all the goodies. Spyder doesn't work really well with large dev work but its good just to get your hands dirty.
Nice Idea. Thanks!
This was two years ago so I can't give you any deeper insights about that choice except for the fact that I was toying around with classes (and that `Ingred.shrimp` looks slightly nicer than `ingred["shrimp"]`).
cancelled. i'm curious about permissions, too.
That's a great point. The biggest source of negativity is the unfamiliar environment messing up.
In this vein, you could also shadow one for a bit if you have time.
One thing that all json libraries for python don't have is STREAMING output and input. The yajl wrappers for python do exist but there are no 'humanized' functions to make it so you aren't creating the damn document manually like you have to do in C.
Unicode bugs? Json by spec has to be utf8, if its not, then its not valid json.
`Topics = ' '.join('data structures' for _ in range(25)).capitalize()`
I had started it because I couldn't find a Python module that was cross platform and did gui automation with a simple API and worked on Python 3. Somehow, I missed AutoPy, which does exactly what I want. :P I think I might have skipped over it because it lacks Python 3 support. Knowing about it now, I would have just contributed to that project to make it Python 3 compatible.
As for an ide, I've become a big fan of pycharm. It's open source though so I dunno if corps will like it
Here's what I'd want to see: * Installation and tool setup * Basic syntax, data structures, and flow control * Reading/parsing and writing text files. * Writing example code and using code in real life can be different. You need to go over real life code usage focused on what they will be doing - whether it's scripting, interactive analysis, or software development. * Get some examples from their Matlab/Fortran work and show them how to do the equivalent in Python. 
Anyone have any good references for using JSON and Python together?
Yeah. 10 users: $10 one time fee. 11 users: $1800 one time fee. Better not to hook into Atlassian too much when your team is around 10 people.
That's the community edition. Us corporate types use the real one :-) P.S. pycharm is amazing.
I use `simplejson` because the error handling is the best. We get records for 100's of different sources, and sometimes the JSON is weird -- `simplejson` is the happy medium for performance and fewer errors decoding.
Thank you. I appreciate it. It has been fun writing them - it's forced me to learn some new things.
 Topics = ' '.join(['data structures'] * 25).capitalize() I had forgotten about .capitalize()
Daaahhhhhhh, I glanced right over capitalize() I knew it had to exist, but two skims through the docs and I missed it both times. I kept trying to shoehorn some absurdity using title() but we won't discuss that any further...
Whoa whoa whoa.... you can * a list to replicate it!? And it returns a single list.... Fffff. My confidence is getting demolished today. 
Nice idea in theory but completely unrealistic and counter to how most Python code works. How much code raises ValueError alone for all sorts of different reasons? Python culture simply doesn't encourage raising sufficiently specific exceptions for that approach to work and at this point the battle is lost.
pyautogui looks great. I've been looking for an alternative to Autohotkey. Have you worked with AHK before? Do you know of other automation frameworks? Specifically targeting Windows, to be exact. I love autohotkey but the syntax could use a lot of improvement, and forget about writing actual program structures.
Eventbrite added also to the live demo: https://authomatic-example.appspot.com/
I believe the categorical support is relatively recent.
TIL thanks!
If they use Nastran, show them https://code.google.com/p/pynastran/ They'll eat that up. It takes an hour to cover everything. Other than that, they probably won't like pandas (I can't stand it), but they need to know numpy and HDF5 for availability. Do everything in iPython.
Don't understand the "much worse" statement here, can you expand?
Komodo by Active State is great. Open source and they are HUGE Python contributors and event sponsors. 
Hi. I'm a senior ME student who just finished a python class this fall (with an A :). It was a graduate/undergraduate elective. I'm also a TA for the MATLAB class at my university. The class wasn't terribly rigorous, but I got two very important things out of it. 1) How to make my code more efficient. 2) How to use other data structures. The first one is a bit too deep to go into and really teach in a short amount of time because we mechanical engineers just love iteration and loop variables and arrays. You're not going to change our minds in a few hours. The second one, you can teach. So teach them about sets and dictionaries, especially the ordered dictionaries. The speed increase is just phenomenal from arrays. We're smart, we'll figure out that lists aren't completely the same as arrays in matlab on our own. Also what the top comment says is right on. I always ask computer scientists questions like, "So where do I write C#? Where does the output happen? Where's the language program/interpreter?" And they look at me like I've got five heads. And I still don't know. So explaining the mechanics of writing the code and installing the libraries is crucial. Honestly I still don't know how to use Python effectively on Windows since we used linux for the class and I just did all my work in the linux computer labs.
Saved
My background's in mechanical engineering but I use Python every day now that I've switched to software/systems engineering. In that past life, the single most useful tool to me would have been the combination of IPython+Sympy - there's so few (affordable) symbolic algebra packages that I was using my TI-89 about as often as Pro/E. Old-school engineering software tends to be monolithic and horrible rather than useful and interoperable, but being able to seamlessly move from symbolic algebra to equation solving systems to a spreadsheet or graphing program opens so many doors. Things like Numpy, IPython notebooks, Pandas, and Sympy will definitely get peoples' attention. FreeCAD exposes a lot of functionality via Python but it's definitely no replacement for CATIA. 
It should be used very carefully, the result is a list of references to the same list.
It's a Python module with an accelerator. Pypy actually bundles its own rpython accelerator there: https://bitbucket.org/pypy/pypy/src/default/pypy/module/_pypyjson/?at=default The link's data is more than 3 years old though, uses Pypy 1.5.0 alpha (current is 2.4.0). Here's the data I get on my machine (sans yalj as it doesn't build and with the iteration count increased by an order of magnitude so pypy's jit can stretch its legs a bit): JSON Benchmark 2.7.9 (default, Dec 13 2014, 21:12:33) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.56)] ----------------------------- ENCODING cjson: 7.054388s json: 12.14997s jsonlib: 6.844709s ujson: 3.639264s simplejson: 4.808569s DECODING cjson: 6.640028s json: 6.501783s jsonlib: 4.422544s ujson: 2.771387s simplejson: 7.717226s JSON Benchmark 2.7.8 (f5dcc2477b97386c11e4b67f08a2d00fbd2fce5d, Sep 22 2014, 12:12:48) [PyPy 2.4.0 with GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.40)] ----------------------------- ENCODING json: 2.939507s DECODING json: 4.296559s 
You will probably need to resample the data so that they are both in the same increments - http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.resample.html 
&gt; FreeCAD whoa I have never heard of this before. I am excited to check it out. 
Better than being unlucky enough to work with large blobs of XML.
Might be better if the "own benchmarks" didn't include 3-years old data though. Gives people the wrong idea.
It may have been that slow back in 2011, which is how old the bench is.
I haven't used anaconda, but at work we use Spyder and Python(X,Y). Speaking as someone who is extremely familiar with Matlab, and still uses it, Spyder made the transition a lot easier due to its familiar layout. 
Because Python doesn't know how to make a new copy of an object properly, you have to give it a function that makes one. 
The argument to `defaultdict` should be a callable that returns the default. In your case, `list` and the `lambda` are callable, but `defaultdict(list)` is not: &gt;&gt;&gt; from collections import defaultdict &gt;&gt;&gt; list() [] &gt;&gt;&gt; f = lambda: defaultdict(list) &gt;&gt;&gt; f() defaultdict(&lt;class 'list'&gt;, {}) &gt;&gt;&gt; defaultdict(list)() Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; TypeError: 'collections.defaultdict' object is not callable
The point isn't to use the results of the benchmark - it's to perform the benchmark on your own json structures and see how it runs - using the appropriate python environment.
Careful with the * with empty lists, you might try something like m = [[]] * 4 But that inner element is actually the same element, and modifying say m[1] will modify all the sublists in m. 
I have an I really love how simple AHK is. The main things that I'd want to improve about is 1) make it run on platforms other than Windows and 2) instead of inventing an entire new language, just implement it as a Python module. Most of the features that I want for PyAutoGUI are inspired from AHK. In fact, the next set of features I want for PyAutoGUI are functions to find and manipulate windows (find title strings, maximize/minimize, move/resize windows, maybe some limited menu-manipulation). I'm implementing it in my PyGetWindow project here as a several module first: https://github.com/asweigart/pygetwindow I could definitely use help with it!
Alternatively you can use: x not in [1, 2] Which some would consider more pythonic. At the minimum its faster to type.
Found [repl.it](http://repl.it/languages) the other day. It lets you skip all of the installation and just focus on coding.
It might be the part where Python's built-in json will *write* a string with an unpaired surrogate in it to JSON, and then be unable to *read* it again. Sure, that probably shouldn't be considered valid JSON, because Python doesn't consider surrogates to be valid UTF-8. But it should probably catch it before writing. That said, I just keep that edge case in mind and the built-in json works fine for me.
&gt;"How do I install all of this? Where does my code go? How do I run it? How do I share it?". I have spent a lot of time learning Python, but I have never learned these things well. I wish I knew of some Python courses or books that focus on this as well as how to actually program in the language. 
The signature of `re.sub` is sub(pattern, repl, string, count=0, flags=0) You're using `re.I|re.S` (which evaluates to 18) as the `count` argument. What you meant is re.sub("this is the first line.*[\n]{2}", "", text2, flags=re.I | re.S)
Fair enough. However I can think of quite a few places I use JSON right now that just don't need that sort of protection - the input to dumps is pretty specified. Seems to me, that there is a pretty good optimization chance here - get the code working with a "good" json implementation, including some sanity checks on unit tests, then swap in "import json" for "import ujson".
Thank you. That fixed it. Thanks for taking the time to answer a stupid question! :) 
I strongly agree. The 'in' operator is very powerful.
I'm in the same boat. Anaconda is just an installer that includes spyder and most of the science packages.
Obviously some skills to solve http://www.pythonchallenge.com/ Ensures they will keep learning after your sessions given how addicting it is!
3 is the present and future of the language. A very small and continually decreasing number of libraries don't have support for 3 *yet*, but you're unlikely to come across them as a beginner. 3 is a better language - better performance, better API (why else would they have made those changes?) but most syntax, particularly for a beginner, is basically the same. 
The only one I've come across is openCV, which is meant to be getting its python 3 bindings before 2015 (there's already a mainly-functional beta available). And, stupidly, a recent project called pyopenworm which is still under development. 
&gt; 3 is a better language - better performance, Last time I saw, Python 3.3 was slower and used more RAM. Unless that has changed... Python 3 is what you should learn. It's harder than Python 2, but going from Python 3-&gt;2 is easy. Going from 2-&gt;3 is much harder. If you need library x, then go use Python 2, but until then...
VTK, wxPython are the big ones for me
Can you use .fillna with the 'bfill' method on the merged data?
https://python3wos.appspot.com/ A lot of important libraries there, some of which have substitutes. I wonder if the wos could recommend substitutes or forks.
this might help you out [learn python 2 and 3 side by side](http://www.reddit.com/r/learnpython/comments/2po4tr/udemy_learn_python_2_and_3_side_by_side_free/) EDIT: added link
I'd argue its more pythonic. 
+1 the remote debugging in pro is real swell.
Is that a Python 2 issue?
Wow. You are truly awesome man and thank you for taking the time to provide such a thoughtful reply! If I remember correctly at the time, I realized that the two functions were almost identical, but I didn't think about just creating one function and passing in the 2 parameters that differed. If you don't mind my asking are you a formally trained programmer? Or someone like me who has learned over time and writing programs for which you may recognize a need? This is the next level of thinking I need to utilize to transition eventually beyond the beginner I consider myself to be. That's one of the things that truly amazes me about python and I guess programming in general; it's when I see how things can be written more concisely and elegantly! Cheers. 
&gt;Hashing passwords is supposed to be slow -- never cache this bit. Just for clarity: yes hashing is supposed to be slow, but you shouldn't not cache it due to a requirement to keep it slow. You shouldn't cache it because to cache it, you would need to map plaintext passwords to their hashed values, and if you're storing plaintext passwords (even in memory) then that defeats the entire point of hashing.
Wow, this is actually very cool. I have a [translation tool](https://pypi.python.org/pypi/py-translate/) on the cheeseshop, and I was considering integrating translation of website content as a feature, but could never find a library for extracting the content. I think I'll play around with your module and see how that goes. Thanks for the contribution
I'm not sure, and possibly ignore me, because I can't reproduce it on current versions of Python 2 or 3.
Awesome contribution!
Well, you are spamming yellowpages' website, my guess would be that they stop answering you after a while cuz they don't like it. If you run it until it stop working, kill it, and start again, does it work right away or does it keep on not working? If not, then the problem is probably what i said.
I would personally stay away from OOP if you only have a few hours. I think it will be more useful for them to learn how to use different APIs, toolkits and frameworks as suggested by many of these comments instead of delving into the wormhole of a paradigm known as OOP.
IMO: Map is good if you have 4 coordinates, and want to change them all from degrees to radians, with one function. Zip is good if you want to iterate through an array of coordinates x, and y together in a for loop. 
/r/learnpython in the future please.
I thought I responded to this.. Weird. Anyways, thanks for checking out the project! Please let me know how the integration works out, and if I can do anything to help out. P.S. Test driving your module now. And very cool .gif demo!
TIL. Good article, clear, direct to the point, and explains a somewhat obscure topic (at least for me).
Testing out your code now; I'm pretty bad with decoding/encoding issues, but a few swings at it should hopefully do the trick. 
So basically readability?
It works on your [demo](http://web-tier-load-balancer-1502628209.us-west-2.elb.amazonaws.com/filter?url=https://parahumans.wordpress.com/2013/11/16/teneral-e-5/), which is what is really puzzling.
This seems to fix the error: with open( 'article.txt', 'w' ) as f: f.write( eatiht.extract(url).encode('utf-8', errors='ignore') ) 
Works perfectly! Is there a place to submit requests?
I have yet to test readability, but yes in terms of functionality, they do seem to do identical things. What might distinguish my implementation to anything else is the "simplicity." Check out /u/tweninger's [comment](http://www.reddit.com/r/compsci/comments/2ppyot/just_made_what_i_consider_my_first_algorithm_it/cmzai6s) (he's one of the authors of a paper I cite in the README as having researched and implemented another text-extraction algorithm)
much simpler readability, which is rule-based i think
Sure! Probably the surest place to submit a request is the [github's issue page](https://github.com/im-rodrigo/eatiht/issues). But feel free to email me at rodrigopala91@gmail.com or give a shout out/msg me on [twitter](https://twitter.com/mi_ogirdor) (I need to start catching up with the rest of society w/ regards to social media lol)
Nitpick, but you should move your imports out of your "get_sentence_xpath_tuples" function and to the top of the page. [PEP 8 on imports](https://www.python.org/dev/peps/pep-0008/#imports).
Go or C. Possibly D or Rust. C++ is such a monster that unless you need to learn it for money, save your time. 
No, you're not nitpicking. This is the reason why I posted this on reddit :) I need these sorts of tips. Thank you.
&gt; 99% of the barrier is just getting them to feel comfortable with "How do I install all of this? Where does my code go? How do I run it? How do I share it?". Set up a local wiki and have your students use it to build a FAQ. Every question that gets asked - goes there together with the answer. Nothing beats official documentation, except a personalized FAQ.
OO in Python is actually simple. You don't have the explicit confusing public/private stuff from other languages.
I might be mistaken, but I'm pretty sure "algorithm" refers to some kind of complex number crunching to produce a result, for example a hashing algorithm uses a series of math problems to produce a unique value for every input. The word you're looking for is probably "class" or "function" (with "function"/"method" being the block of logic which performs the extraction, and "class" being the greater object which encapsulates other functions and variables etc which are related to your readability function.). Anyways, its a cool project. I've always loved how simple it is to do things in python, its a great language to start out in. 
With my Master in Computer Science I can tell you that you are wrong.
That might be the theoretical point, but if you post bench results people are going to use the results as-is as /u/badsector demonstrates and you've effectively fed 3 years old data to readers. I don't think the 75LOC test harness with no explanation helps either.
It doesn't explain why it wasn't implemented using NotImplementedError in the interpreter.
Returning a value has less overhead than using exceptions for normal control flow. Furthermore I imagine it's named such exactly because it's not yet an error. It would only error out when neither type supports the operation.
Well, maybe you can check me on this: an algorithm is a formal set of steps generally guaranteed to reach a particular result (generally because there are some probabilistic algorithms); a heuristic is more of a fuzzy concept of steps which may lead to a correct result and are more efficient than a guaranteed correct result (or a guaranteed correct result is not possible because the general problem (here, full semantic parsing) is not yet solved). Did any of what I said make sense, and if so, would it be reasonable to term this a heuristic rather than an algorithm?
Stack overflow (source below) agrees with me, calling algorithms "abstract" and "totally not the right word to describe a class which uses some regular expressions to parse a web page". Okay, I made that last one up, but while "algorithm" is a pretty generic and closely defined word and could conceivably be applied to one of his functions, he did in fact write a collection of functions encapsulated in a class and bundled as a python library. If I were him I'd stick with calling it a class or a library, because I'd think most people would find it a bit misleading if you went around talking about your new algorithm and it turned out to be a regex. Its still a nice bit of code and highly useful. http://stackoverflow.com/questions/3391475/what-is-the-difference-between-an-algorithm-and-a-function
That would mean the interpreter would be responsible for catching the exception and making decisions based on that. Python rarely (if ever) hides exceptions from the user.
Very interesting! Could have used an alternative like this back when I was writing my master's thesis. There is a library called [Goose](https://github.com/grangier/python-goose) which pretty much does the same thing - maybe you should check it out :)
(((_____ &lt;&lt; ____) + _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; __) + _)) + (((___ &lt;&lt; ___) - _) &lt;&lt; ((_______ &lt;&lt; __))) - (((_______ &lt;&lt; ___) + _) &lt;&lt; ((_____ &lt;&lt; __))) - (((((_ &lt;&lt; ___) + _))) &lt;&lt; ((___ &lt;&lt; __))) - (___ &lt;&lt; ______) + ___
Hey ender89! This sort of question, while a good one, is one where you'll get a lot of debate/reactionary comments. But I believe everyone in this thread is technically correct. Here's my justification as to why I called this an algorithm (I'll be referencing wikipedia's take on [algorithms](http://en.wikipedia.org/wiki/Algorithm): &gt; An algorithm is an effective method expressed as a finite list of well-defined instructions I believe eatiht's functions and the set-building instructions I use within those functions would make up a "finite list" of instructions &gt; Starting from an initial state and initial input (perhaps empty), This point is satisfied by the condition that it requires some list of structured data (specifically HTML) &gt; the instructions describe a computation that, when executed, proceeds through a finite[5] number of well-defined successive states, Here's a crappy state representation/finite list of instructions within the project: url[,xpath] -&gt; xpaths of text-nodes-&gt; frequency distribution -&gt; argmax( freq. dist. ) = ... &gt; eventually producing "output" and terminating at a final ending state. ... = likely xpath leading to article's content &gt; The transition from one state to the next is not necessarily deterministic; This condition, arguably, is not satisfied in, what I hope we all consider now as, the eatiht algorithm. &gt; some algorithms, known as randomized algorithms, incorporate random input Again, *proving* that the input is randomized is, I believe, rather difficult. Consider this: I'm formally defining my algorithm, and I start with something along the lines of: "For all possible sets of symbols in the Language X..." That, I think, would be enough to formally define *non-random* input to my algorithm. 
As a non-Python related side note: that regex isn't very robust. There are loads of abbreviations people could use, like Capt. off the top of my head.
Aha, maybe you can update/republish your thesis but use eatiht this time? Haha, jk, and congrats on achieving that feat, that's something else!
&gt; that regex isn't very robust. I agree! Please refer to this [issue](https://github.com/im-rodrigo/eatiht/issues/2) for discussion on the un-robustness of the regex approach. I would like as much input as possible to improve this package. Thanks!
&gt;As expected, a1 is equal to a2 and the __eq__() in class A took care of this comparison. Comparing b2 with itself will also yield a similar result: This paragraph needs to be re-written, as there isn't an a2 or b2, rather than you are comparing a1 and b1 to themselves. Either that, or a2 and b2 aren't explicitly created
&gt; One thing that all json libraries for python don't have is STREAMING output and input. Not sure if I understood you correctly, but [json.JSONDecoder.raw_decode ](https://docs.python.org/2/library/json.html#json.JSONDecoder.raw_decode) can be used to parse streams.
The OP's point is that `zip(a, b, ...)` is precisely equivalent to `map(lambda *x: tuple(x), a, b, ...)`. So when you're doing something with that tuple next with a map, don't use zip, do what you want to do directly in a multi-sequence map.
It does: &gt; You might be asking yourself, "But I thought I should raise a NotImpementedError when an operation is not implemented!”. We’ll see with some examples why that shouldn’t be the case when implementing binary special methods. &gt; [...] `b1.__eq__(a1)` method returning `NotImplemented` caused `A`’s `__eq__()` method to be called and since a comparison between `A` and `B` was defined in `A`’s `__eq__()` then we got the correct result (`True`). &gt; [...] Note that raising a `NotImpementedError` when `b1.__eq__(a1)` fails would break out of the code unless caught, whereas `NotImplemented` doesn’t get raised and can result in/be used for further tests. So there's the difference: `NotImplementedError` means "this operation is not defined for this type", while `NotImplemented` means "this operation for this type is not defined in this place, go look somewhere else". I really think `NotImplemented` should have been named something else, both to prevent the confusion and to signify that it may very well be implemented, just not in this function definition.
Hey no_game_player, I'll give you my two cents on whether this solution should be considered an algorithm or a heuristic. I've come to believe that heuristics are a sort of "helper function." When looking at the problem at hand, and in a very "big picture" sort of way, a heuristic (function), I believe, can be thought of as a function that serves to aid in finding a solution faster/more-optimally than solving the problem **without** heuristics. For example, if your goal is to beat some chess-playing humanoid, instead of your AI going for the quickest kill, you might have some heuristic function which can calculate some score that suggests the AI *not* make that easy kill, because such a move may either: 1. increase the chances that the human opponent checkmates your AI (it's commonly said that his is very unlikely for modern chess AI's) or 2. more likely that there is some other move, with a higher score (outputted from a heuristic function), that tells the AI, "actually, don't kill this nearby pawn, because if you do, you'll be increasing the odds that you'll checkmate this human in **4** moves, instead of the **3** moves - that is if you not kill this pawn right up her') So with regards to my solution to the content-extraction problem, I don't think I make use of any heuristic function that helps *improve* its output. But you see, now I'm starting to reinterpret what I mean by "heuristic." If you consider the "text finding" xpath I defined in the project as a problem-solving helper, that is, it aids in finding a solution (getting the main text) faster, then I guess, yes, one can argue that it is a heuristic. Damnit, I confused. Anyways, all-in-all, no, I don't think my solution would be better named a "heuristic" than an "algorithm." Edit: the above is my interpretation of a "heuristic function" that is described in Russel &amp; Norvig's *Artificial Intelligence - A Modern Approach*. See their section named "Informed (Heuristic) Search Strategies," chapter 3, section 5
Thanks for that. It's fixed now. In the original draft I had `a2` and `b2` as well but I thought it was confusing to introduce more names; obviously forgot to change the text. Thanks again!
If your second Python package is far more complicated, and more clearly in agreement with everyones sense here of what an algorithm is, will you feel embarissed that you labelled this package (a bunch or regexes) as algorithmic? 
You're not really overwriting NotImplemented but shadowing a built-in. 
You should try to get this put in the next edition of CLR. 
It took a while for me to realize that you didn't mean .NET's CLR. But now I'm stuck at whether or not your serious lol. I've been having sporadic "Can't tell if..." moments since I've posted this package. Either way, thanks!
Make them [watch this](https://www.youtube.com/watch?v=OSGv2VnC0go) for homework.
I think this is true of any language/setting.
It was a serious-ish question. I mistakenly described the first shitty robot I built as having an intelligent controller. Then I did a PhD in a robotics related field and felt embarissed with my past self. To a first approximation, everything simple has been done before.
 (?&lt;! Mr\. ) # Don't end sentence on "Mr." (?&lt;! Mrs\. ) # Don't end sentence on "Mrs." (?&lt;! Jr\. ) # Don't end sentence on "Jr." (?&lt;! Dr\. ) # Don't end sentence on "Dr." (?&lt;! Prof\. ) # Don't end sentence on "Prof." (?&lt;! Sr\. ) # Don't end sentence on "Sr." What about other common abbreviations, e.g. St., Rd., e.g.?
Ugh, the PEP8 linter in PyCharm doesn't catch this particular convention by default. I believe I have a few modules lying around that import in the function. I'll look for them and correct as I find them.
It goes nicely with the `operator` module (eg. `all(map(eq, xs, ys))`).
&gt; To a first approximation, everything simple has been done before. No debating that. &gt; I mistakenly described one of the first robots I built as having an intelligent controller. Don't be embarrassed about that. I mean, gawddamn dude, you freakin' built a robot!
Just added the changes! Thanks for the headsup. 
Great Job! The project looks great and professional, everything is well documented, you have LICENSE, good README and the package is published. You may think "but this is just common stuff", no, it's not that common to first publish something with everything in place. If you are accepting suggestions, besides the tests that really goes out into the wild of internet and get the article content, I would also have local tests with a folder like `tests/assets` where I put some regular use cases and edge cases, for example, an article that has only short sentences "10 things you didn't know about foo" all in their separate div.
Actually there's a setattr() method and a __setattr__ 'magic' method which can be used inside classes (see http://www.rafekettler.com/magicmethods.html for some details). Is it what you are searching?
To be honest I was helping someone else out (my Python is decent, but I didn't know a quality answer to this one for him, but it seemed like a good question). I'll inquiry with the original-asker. Thank you for your input.
 &gt;&gt;&gt; (((_____ &lt;&lt; ____) + _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; __) + _)) + (((___ &lt;&lt; ___) - _) &lt;&lt; ((_______ &lt;&lt; __))) - (((_______ &lt;&lt; ___) + _) &lt;&lt; ((_____ &lt;&lt; __))) - (((((_ &lt;&lt; ___) + _))) &lt;&lt; ((___ &lt;&lt; __))) - (___ &lt;&lt; ______) + ___ Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; NameError: name '_____' is not defined Edit: Nevermind, I see that this is a replacement in the code. "Cool!"
I've got it (mostly) figured out! Thanks for the tip.
Not when I made that comment.
This is the highlight of my day so far. Fantastic!
I have mixed feelings about understanding that.
I would push the data frames each into a database table, then query them with a full outer join.
"A monad is a monoid in the category of endofunctors, what's so hard about that"
That's true, I forgot that – but using exceptions in this case fits well as it is about control flow, and something that is mostly used internally unless you're making custom iterators. Let me rephrase: Python won't hide exceptions from the user unless it has a very good design-based reason to do so. In the article's case, and in most other cases, it does not.
[My reaction whenever I discovered a module involved Haskell](https://www.youtube.com/watch?v=Bcy8bvhewtU).
https://www.youtube.com/watch?v=61R2P20Ca2U
Which other ones in the "plethora" are you referring to?
The most appropriate description of this is what happens to Kirby when you are tripping balls.
Is there a reason to allow shadowing for some constants but not other?
This is my favorite post to this sub in a very long time. Well done!
&gt; Let me rephrase: Python won't hide exceptions from the user unless it has a very good design-based reason to do so. Well, yeah, the important part is why there's no design-based reason to do this, why we shouldn't want to have custom comparison operators throw NotImplemented and the core comparison implementation catch them similar to how custom iterators throw StopIteration and the for loop implementation catches it. And that's because NotImplementedException already has a particular meaning and overloading it with this new meaning could lead to all kinds of problems, like a completely unrelated method deep down the call stack from your comparison operator throwing it because it's actually not implemented. By the way, I'd say that StopIteration stuff was a mistake too, because I've seen similar problems (though not as bad) with them. Every naked (not wrapped in try/except) `next()` call in your program that you wrote because it looked basically like an implicit assert is a rake in the grass that can exit some loop instead of terminating the program if it's executed at the right time. And the risk is greatly increased if you write a custom iterator/combinator without actively trying to minimize the number of statements in the try block. And any line in your custom iterator/combinator that uses a naked `next()` can be a bug if you didn't actually expect it to throw.
BoF's, birds of a feather can be fairly interesting. Otherwise once the schedule comes out, rate every talk you are curious about in something like a 1-10, go for the highest rated ones and for the ones that lost you can always watch the videos later. There's a very good chance two or more talks are going to overlap. Last bit, google for the names of presenters. Some people are better than others in talking in front of a crowd and then others just weren't able to put together a good presentation. I don't think Zed Shaw does talks anymore but so far, one of my favorite presentation was by him on ZMQ. He raced through code examples, explained why ZMQ was cool, and he was done; fairly efficient. Flip side was Alex Mortelli who's done several talks at PyCon's over the years and he presents high level topics fairly well. Last bit, even if you're not looking for a job, the job fair is a fantastic place to get a lot of cool stuff as well as be introduced to python friendly technologies. On this part, I'd recommend making a one off email address because you will get spammed like no ones business after PyCon. Actually rather hateful towards a few companies that still email me about stuff thats not relevant.
Yeah... It seems you've reached one of the less attractive portions of the code. While I really have no issue with adding all possible end-on-period abbreviations, I'm wondering how useful that may be to the project in the long run. Someone suggested a pretty good approach at tackling this problem, read about it [here](https://github.com/im-rodrigo/eatiht/issues/2). I have yet to give it a shot, but if anyone implements a fix, send a pull request, and I'll add it in. Thanks! 
For some reason, it's not reindexing when I run this. I still get 15 minute intervals. I indexed the timestamp column from each csv file when I read them in.
I love this kind of cleverness.
My apologies. I took note of null_vector's comment and added the "(shadowed)". Thanks.
What exactly is the fundamental difference between a bunch of regexes and a bunch of bit shifting, arithmetic operations, list manipulations, etc.?
code -&gt; http://discoverflask.com
While what you say is accurate, OP was more precisely saying that `map(lambda tup: f(*tup), zip(a, b, ...))` could be more compactly and efficiently written as `map(f, a, b, ...)`.
The suggestion for googling presenters is a good one. No idea if it this is true at PyCon, but at other conventions sometimes this isn't the first time the presenter has given that specific talk, even, and you can find the talk already on YouTube.
Especially true for consultantsn where presentations is part their business. 
Depending in what yours and their interests are, you could teach the subset of Python necessary for pyopenscad, which is as Python frontend to the programmatic mCAD tool openscad. It might be a quick way to get them interested by leading with a practical example of Python versus just doing language constructs.
In Pp's "App" directory is a "python.exe"; use that instead of the executable in the root PortablePython directory. (the PP executable just calls that anyway)
I'm using win 8.1. I know pip, but I can't get it to work. 
A strong +1 to this. The material there is designed specifically for [teaching scientists and other non-developers](http://software-carpentry.org/faq.html#why-does-swc-exist) how to use Python effectively. It is very well done, and [meticulously](http://arxiv.org/abs/1210.0530) [thought out](http://arxiv.org/abs/1307.5448).
Even trying to use that, it still won't start in Powershell.
So? You asked for a reason and I gave you an answer.
Even better, ditch the unnecessary parens. if x.lower() == 'e': # stuff
Looking around quickly, [gnupg](https://pypi.python.org/pypi/gnupg) looks like the better deal. It's got public source hosting ([github](https://github.com/isislovecruft/python-gnupg)), a [bug tracker](https://github.com/isislovecruft/python-gnupg/issues) and the PyPI page has Python 3 packages. [Doc is here](http://pythonhosted.org/gnupg/). Enjoy.
Thanks! These are useful metrics. 
when you run "pip install tweepy", what happens?
I feel like this is what the hitchhikers guide was supposed to solve. It isn't quite there yet unfortunately. Please contribute, experts of python. http://docs.python-guide.org/en/latest/
I used the name `python-gnupg` for my package because I didn't want it confused with the actual GnuPG program which I wrap. The `gnupg` project was forked from mine and there is an [acknowledgement of this in their docs](http://pythonhosted.org/gnupg/gnupg.html#about-this-fork). Note that their apparent reason for forking (use of `subprocess` with `shell=True`) is no longer valid (`shell=False` is used throughout, though it wasn't in early versions).
As the maintainer of `python-gnupg`, I would like to point out that there is [public hosting](https://bitbucket.org/vinay.sajip/python-gnupg/) and a [bug tracker](https://bitbucket.org/vinay.sajip/python-gnupg/issues) for it, too, and Python 2.x and 3.x are supported in a single code base. So I'm not sure what makes the project mentioned a better deal. I would say, look at both APIs and pick whichever one is better for your needs. The `gnupg` project (GPLv3-licensed) was forked from `python-gnupg` (BSD-licensed) a while ago. 
I have figured it out now. The problem was I didn't have the correct paths. Why is it so complicated?
I have figured it out now. The problem was I didn't have the correct paths. Why is it so complicated?
yep. I usually ask people to delete questions from /r/python so that others don't see them and follow that example. I see /r/python go back and forth between being helpful and not helpful to beginner questions based on how annoyed folks are at the time.
I agree. I think your choice of name was decent. Then, the creator of the `gnupg` project could have chosen a better project name (rather than simply `gnupg`), and definitely a better github repo name than what he chose: `python-gnupg`. These can still be remedied though, if he's open to it!
`True` and `False` are keywords. `print` was a keyword. `NotImplemented` isn't and never has been a keyword. For a more in depth discussion, check out [Guido's explanation](http://python-history.blogspot.com/2013/11/story-of-none-true-false.html).
Welp, I tried. I never used Powershell on the rare occasions I was forced to use Win/Mac so I'm flying blind here. I know it'll work from the standard CMD prompt, at least.
Just reading about data science for the first time and, as a gamer, this is interesting. Could you give one simple example of something you could analyze in a game? And how are you extracting or pulling the data from the game? 
Why should I use it? It doesn't seem to really make it that much easier to create easy-to-understand or easy-to-parse log messages to justify using another non-standard module.
Speed comparison! Comparing regular print("Hello World") to the obfuscated version. [me@linux ~]$ time python HelloWorld.py Hello World real 0m0.012s user 0m0.007s sys 0m0.005s [me@linux ~]$ time python ObfuscatedHelloWorld.py Hello world! real 0m0.014s user 0m0.008s sys 0m0.007s 
Motivate me you smart motherfuckers!! 
I implemented something like this using a [LoggerAdaptor](https://docs.python.org/2/library/logging.html#loggeradapter-objects) to keep track of shared context in the same way this uses bind. Since then, I've moved to Sentry and not really bothered with manually binding anything that I can see in their context.
I think the benefit is not repeating the same context, which can get significant if you are using `extra` to pass context instead of just making long messages. If you are just using log.bind(thing=data) log.info("operation happened") ... log.info("operation happened") ... log.info("operation happened") It isn't bad.
There is also the `abc` module that provides the `abstractmethod` decorator, another way of indicating something isn't implemented https://docs.python.org/3.4/library/abc.html
context: http://arstechnica.com/tech-policy/2013/03/how-dongle-jokes-got-two-people-fired-and-led-to-ddos-attacks/
is pycon that good? should I go?
There are at least 3: Shedskin, Cython and Numba. Shedskin and Cython are ahead of time compilers. Numba is a jit. 
Python is probably the easiest language to learn. Maybe you should flip a burger...
RemindMe! 3 weeks "dropping acid again"
Messaging you on [**2015-01-10 02:20:55 UTC**](http://www.wolframalpha.com/input/?i=2015-01-10 02:20:55 UTC To Local Time) to remind you of [**this comment.**](http://www.reddit.com/r/Python/comments/2prv14/obfuscating_hello_world_in_python/cn05hu4) [**CLICK THIS LINK**](http://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[http://www.reddit.com/r/Python/comments/2prv14/obfuscating_hello_world_in_python/cn05hu4]%0A%0ARemindMe! 3 weeks ) to send a PM to also be reminded and to reduce spam. _____ [^([FAQs])](http://www.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/) ^| [^([Custom Reminder])](http://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!) ^| [^([Feedback])](http://www.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback) ^| [^([Code])](https://github.com/SIlver--/remindmebot-reddit)
man I can't even get a whale, I'm too retarded lol 