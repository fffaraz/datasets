Embrace the Lisp arts, learn the ancient TECO magicks reborn in Emacs and you will know the way!
Depends on whether or not you want the IDE experience. It's a cop-out, but I do all my development in a VirtualBox VM running headless Ubuntu. I just do everything over ssh+editor.
I think you mean Monty Pythonic
If you're planning on using vim then beware that the out-of-the-box version doesn't come with the python interpreter builtin (needed for the pyflakes plugin and code completion). [Here](http://brilliantcorners.org/2011/02/building-vim-on-osx-snow-leopard/) is a how to on building it on osx. After that I'd take a look at pathogen. [Here](https://github.com/sontek/dotfiles) are very nice dotfiles to have a look at.
Well, first off a few comments on that code piece: - he didn't use strict - he used camel case variable and function names (which reduces readability a lot in Perl) - he doesn't unpack function parameters and instead accesses them directly via `$_[1]`. This is how it should be done: `my ( $prompt, $width, $default ) = @_;` - he uses completely pointless prototypes There's more, but really, i can't be arsed to point everything out. To improve your own understanding of Perl and ways to write it better and more readable at the same time, read [Modern Perl](http://onyxneon.com/books/modern_perl/modern_perl_a4.pdf) and get a copy of Perl Best Practices.
I prefer Netbeans over Eclipse. Netbeans has a run command that works across all project files, instead of setting up individual environments like Pydev. 
not so much import re for line in open('file'): print re.sub('this', 'that', line) 
Confound these ports
FYI. You can use sitecustomize.py to import frequency used modules automatically.
global rulz
On the other hand, it's not reasonable to expect every user of Python to know the entire documentation inside-out. The behaviour he's seen isn't explained on the page for sets, nor in the Python glossary definition of hashable, and the latter implies that any object you make is hashable by default based on its id, so I'd argue it's a documentation 'bug' at the very least.
Even though you mentioned vim I still upboated. Brew is awesome and so easy to contribute too (my git pull request to update a project took a day max) and ipython is great.
On a mac you need a linux style package manager for development IMO, so whether you choose homebrew or something else you still probably need to choose something.
Although I personally find Twisted a bit overengineered and frustrating in places, it does actually do a fair bit more than ZeroMQ does in terms of supporting HTTP, remote procedure calls, etc. ZeroMQ is mostly just a transport layer, albeit an incredibly effective one.
&gt; On a mac you need a linux style package manager for development IMO I seem to get by just fine without one. Between ./configure --prefix=/foo/bar &amp;&amp; make &amp;&amp; sudo make install and python setup.py [config|build|install] I want for nothing. And even using the former is pretty rare.
Fair enough, although I think in both cases you mentioned, there are better ways forward.
1. install linux
Yes, that is basically what brew does, except it also will pull in dependencies and compile them as needed. Don't get me wrong, I also compile my own stuff when needed but it is nice to be able to check for updates and install a new version without having to browse to the projects website all the time. And because brew is hosted on git I can easily add in my own projects. (granted I've never tried to submit a project to yum, apt or macports but brew was super easy.)
If your not, vim + Pylint + pydiction + ctags.
my thoughts exactly. (I'm a mac user) 
Nice. Will it cleanup / overwrite MacPorts installs? Edit - OK, I RTFM, and it looks like if you run into problems they recommend uninstalling macports, like so: http://guide.macports.org/chunked/installing.macports.uninstalling.html
Just recently I started using Aptana which is based on Eclipse and includes the Pydev plugin as well as a few others.
Macport port search 'any package you want to install' sudo port install 'package name you wanted to install' sudo port install 'python_select' # this is useful to select which python you wanted to use Lastly, use macvim, don't ever touch aquamacs. ;) But my suggestion is, OS X can never be the puuurfect python dev machine, sometimes compiling package just failed or there's no package for OS X (It tooks me years to configure how to compile previous version of pyside and numpy(fucking atlas error)( If you wanted a puuurfect python dev machine, Linux is the best. 
Or with 2 simple steps you can install python from macports. Works perfectly for me for cli and web app development.
Ah; I'd been hoping it was getting better. I have a couple of python apps I've written with wxpython, and the toolkit was kind of starting to grow on me. I guess we'll find out what Apple's opinion on the matter is when Lion's released..
Try [#python](http://pound-python.org) on [freenode IRC](http://webchat.freenode.net).
No. The Django ORM is way underpowered relative to SQL; SQLAlchemy provides the full spectrum between total concision and total control.
Fair enough, god knows I too have seen Python code too horrible to even share here. Maybe my head just doesn't work the Perl way.
we're going to start a mailing list to get more feedback like this. check out http://groups.google.com/group/colony-users 
Is that three layers of package management?
Cool, thanks for the advice, I'll check it out :)
Actually the development version (2.9) works fine under snow leopard and is cocoa.
Nope.
Does homebrew do a framework build yet? I was under the impression it did not.
It gets even better if you do it on the commandline. $ perl -pi -e 's/this/that/' file1 file2 Do that in Python.
My favourite application to write code in is '[Coda](http://www.panic.com/coda/)' It supports plenty of languages but my favourite feature being that it can use sftp/ssh to manage files remotely. This means I can quickly open a file and save it and it's ready to go. It also looks pretty nice. It did take me a few bits of tweaking to get it to act as I wanted - but once setup I love it. I hope one day to save enough dolla to buy it rather than re-installing the trial every 15 days.
Can you let us select a map *size* with CLI arguments? Such as -s -m -l?
That is extremely cool. I'm going to have to remember and use that in the future!
&gt; You misunderstand what ORM does for you. No, I don't. I have been doing this stuff for a long time. You misunderstand that trading syntactic sugar for an entire abstraction layer with significant tradeoffs in the categories of flexibility, performance, lock-in to a specific API, not to mention creating a generation of developers who know fuck-all about how SQL and databases work (I know; I've worked with these people) because they've been abstracted away is bad judgment.
fyi: there's now * a google group (http://groups.google.com/group/colony-users) as well * as an IRC channel: `#colonyframework` on `irc.freenode.net` 
 % brew options python python --framework Do a 'Framework' build instead of a UNIX-style build. --universal Build for both 32 &amp; 64 bit Intel. --static Build static libraries. --no-poll Remove HAVE_POLL.* options from build. Yes.
There isn't enough emphasis on the your dotfiles link. Sontek did an great job, and should be commended. He took vim and made it awesome++!
Maybe this is me not quite understanding the problem, but it seems like there may be a simpler solution. As I understand your question, you have three levels of things running. Main program -&gt; Luck tester -&gt; Nethack instance. The main program spawns a luck tester. The luck tester spawns an instance of nethack, inputs random keystrokes, then checks whether the desired random result happens. The main program then queries each luck tester to see whether the particular random keystrokes used resulted in the desired action. Why not have the luck tester simply kill its own nethack instance, then print the result of the test and the random keystrokes used to stdout, then quit? Your main program then reads from the stdout of the luck tester to determine which of the random keystroke combinations were a success.
First off, I hadn't considered that. It's simplistic as hell and I love it. However, I tinkered with a similar idea, but the flaw is that nethack takes an exorbitant amount of time to put things to stdout. I've been hacking the nethack source so that the stdout is null. All we care about is the message buffer for these checks. My main problem is that if I put it through stdout without recoding nethacks entire message system entirely instead of simply redirecting the message buffer, it slows it down well over 20 fold. That being said, your idea is better than any of my implementations thus far, and I thank you for it! Me and my stupid overengineering.
Ah. And so if you allow nethack to use stdout, then it takes forever to wait for it. Question: I have limited experience with running subprocesses, and so I am not entirely sure how stdout works with multiple processes. If your luck tester is querying nethack in some fashion, and nethack is not writing to stdout, then couldn't you have your luck tester still writing to stdout? The messages from your luck tester would then be the only things in the stdout of that subprocess, if I understand correctly what is being done.
If you add a map to maps/, you can specify the map size in that file. For example, if you look at maps/macro.py, you'll see that the MAP_SIZE is 500. You can copy macro.py to a new file, say "my_map.py" and use -m maps/my_map.py to load the settings for that map. Are you asking if the flags -s = small, -m = medium, -l = large can be added? My internal mapping is: small = default map, medium = maps/village.py and large = maps/macro.py
There'd certainly be no problem having my luck testers using stdout, no need for a buffer there. The problem is, how do I manage 256 stdouts with a single supervisor thread? I feel there's a more elegant way to do all of this than relying on stdouts or pipes, but I just can't figure any way to do it. 
Numpy devs highly recommend not to use pip / easy_install or any of their brethren to install numpy or scipy. It's not really supported and can cause all sorts of problems. 
Thank-you very much. Seriously. I pick up and put down Perl as I need it in my career- I know I'm not only rusty, but the foundation was likely not steel-based in the first place. I'm definitely going to read up on this. The old camel book itself is no longer sufficient for this droid :) Thanks!
Way ahead of you there.
Download requires registration, and PDF appears corrupted. Nice reference though.
[Here](http://pastebin.com/2ZKw4Jxt) is how I went about throwing together a quick example. The subprocess in this case is [this script](http://pastebin.com/tcL2h7rU), which just prints a random string. It spawns 256 threads, each of which spawns its own child process. Each thread reads the stdout from its process, then adds that stdout to a list of all results. The main thread then waits for all of them to finish, and prints the list of 256 output strings. I apologize for the lack of comments in the code, but it should be easy enough to understand. There is another thing that I would suggest for your application. Here, each new thread is started immediately, runs, then returns. Since you have a random chance of an output, you will need to loop over the threading part, trying again and again until you find a luck manipulation combo that works. I would suggest only having as many threads open as your computer can run nethack instances at full speed. Otherwise, you wait slowly for all 256 instances to complete, all at a slow speed, rather than having a few running at full speed, checked, and then quitting as soon as a proper combination is found. That also scales better with respect to probability, as something with a 10% chance won't need to be repeated as many times as something with a 1% chance. Now that I think of it, it probably would be better to have a threading.Event to watch for the creation of a good combination. One moment, and I'll try to come up with an example.
Thanks for taking the time in your response. I try not to be dogmatic about it, and play with the latest ORM tools about once a year. But maybe I'm just an old dog, and it's a new trick. And all that said, some of our tools at work actually use SQLAlchemy. But mostly for the engine abstraction, and query building. SQLAlchemy is by far the best python ORM I've used. But I'm still more comfortable writing straight SQL than I am having a tool build it for me. It's most dangerous with the junior devs, who don't know SQL in the first place. And the guys that already know SQL enough to effectively get the "wins", would generally just stick to SQL. Maybe it's an audience problem? Best of luck!
Cool!
pyflakes is for python level, django-lint is for the Django level.
I'd look at the multiprocessing or threading library, I imagine that python isn't a bottleneck so either would probably work. You can use subprocess to spawn the nethacks. You can push data into a queue which the master thread is waiting on with a blocking get(). You could probably even use a threadpool or something to limit the number of workers, and have the job create and run the nethack instance. Can you reset nethack, or do you run it from scratch each time?
Use the [(Plone collective's) Python buildout](http://svn.plone.org/svn/collective/buildout/python/) (especially handy if you need to build all versions of Python)
Upvote for Coda. Panic makes some of the most beautiful software I've ever used. When I first started playing around with Python that's what I used. BUT, as I started to do more dev work I switched to Eclipse just because I wanted some of the debugging and more of an IDE experience. As far as simple code editors go, yea, Coda is great, but it will fall short for bigger projects.
Okay, [here](http://pastebin.com/z31QBJA5) is a quick example of having multiple threads, each with a random completion criterion. It spawns a specified number of threads, each of which attempts to pass the completion criterion, then stops all of them when the criterion is found.
Thanks for the heads up. I've never ran into any issues, but now I know what to blame if I do :)
[Here's the source](http://www.slideshare.net/dabeaz/python-generator-hacking) that joejoe500 ripped off of. http://www.dabeaz.com/generators/ is another of the author's great generator presentations.
There is no PERL. It is "Perl" for the language and "perl" for the interpreter.
Well I am ssh'd into the remote server anyway, so I can just run python on there and have an interactive session. I have IDLE on here too, but I just don't like it.
I'm learning Python/Django as well via freebie books. We might be studying the same ones. Wanna team up and learn together? Help each other out of jams if we can? Things like that? PM me. I have GTalk + Skype.
Thanks changing it to self.actiongo() fixed the error I was having. And I will use pastebin next time to post my code. Thanks for your reply!
Yah with SQLAlchemy as well as with Mako templates I haven't yet solved the problem of novices lulled into poor practices because the tools are open ended, yet can't critique their code. But I write my tools for people that either know how to code and will appreciate the functionality, or are looking to become coders. Cut and pasters won't have such a great time.
you shouldn't deprive a craftsman of good tools just because novices are going to misuse and lean too heavily on those tools. Besides, the SQL no nothings are all using mongoDB now anyway, they've moved on.
Thanks for the suggestions! My current structure is exactly that, parentRoom subclasses into firstroom and secondroom. Seeing that both my subclasses are almost identical I was trying to fit as much as I could into the parent class as possible, but I wasn't sure exactly how to do it. I can see how making everything a global variable could cause problems in larger scripts. I still am trying to wrap my head around how python handles variables and deals with scope. I was wondering about using 'input' as a variable. It makes sense not to use a default function as a variable. I will be sure to use something else from now on. I will have a look at your example and see if I can make mine simpler and more efficient (and add more rooms!). Thanks again for your help!
Ok i think I understand. I will try to implement that.
&gt; Plus wx is an awful GUI toolkit from the programmer's perspective when compared with Qt4 + pyside. could you elaborate a little please? i have been using wx a bunch recently but i'm always open to looking for better solutions. after looking at a number of the common gui tks for python i landed on wx, how does qt4 + pyside excel?
I'm really surprised this statement of fact has garnered so many downvotes, 18 to be exact. Why is that? And when seeking to understanding with this question, the followup question earned 16 down votes. I suspect the reason is that Perl users feel threatened by the truth. Note, I concisely answered the question the OP posed. Back-story: I've written a lot more Perl code than Python. I said nothing about why Perl is better than Python, and it has a great deal of merit, particularly for sophisticated coders. I just answered the question, and find myself barraged by those that would seek to censor the truth. Strange, I've made lots of posts in the decades I've been online, but seldom have encountered this degree of negativity for voicing a simple, [generally accepted](http://www.google.com/search?q=python+vs+perl) statement [backed by experience](http://www.linuxjournal.com/article/3882).
We use Python as a replacement where I work. We found SciPy, NumPy, and matplotlib are what was needed to fully replace it (for our needs). We looked at spyderlib, but since we never really used the matlab GUI, we didn't need it. Also, IPython is useful. To make it easier to plot, you can use ipython -pylab.
if you're not using virtualenv and virtualenvwrapper you're insane.
I am aware of that. I was saying that if one would build a django wrapper for pyflakes it would not have to be GPL licensed. I was under the impression that django-lint was GPL licensed because of pylint.
It depends on if you need to use any of MATLAB's crazy add-ons. If not, you are all-set. We have migrated a large chunk of our MATLAB work to Python and have been very happy.
Perhaps even: while (&lt;&gt;) { say if /\D\d+\s\w\s\d\.$/ and -d; }
I recommend Python + NumPy + SciPy to anyone who even mentions MATLAB to me :) Seriously, Python is just better language, which is important if you're doing anything that isn't trivial. The main advantages of MATLAB are that it's faster for certain things (and more forgiving of poorly written code), and it has more scientific toolkits than Python. You'd be surprised at how little those matter though.
I would argue if you're making your own data types you should be familiar with the data model documentation, or at least have taken a look at the documentation for the particular functions you're overriding.
Well, I am an Electrical engineering student so I might do some signal analysis and so forth. do you think python is capable of that?
IMO the real advantage of MATLAB is the documentation. I work with MATLAB since my workplace didn't let me use Python and I very rarely have to consult the Internet when I don't know how to do stuff. That being said, MATLAB's data types to me really are a pain.
Have you considered Octave? I'm assuming this is a cost issue rather than a language issue, right?
Yes: http://docs.scipy.org/doc/scipy/reference/signal.html
Looks like people do [similar](http://narnia.cs.ttu.edu/drupal/node/59).
Python is *perfect* for that, and quite widely used.
Matlab! Honestly though, I've used both (a bit) and I guess it's my background that does it, I like python as a language for the occasions I need to script something; but as an engineer MATLAB is way more quick and down to buisness for me, especially since I work a lot with control design, signal processing and SIMULINK (Though I haven't explored alternatives that much). Still, if I were ever to see MATLAB as a "language" it would be pretty bad. But I can get a lot of things done awfully fast in MATLAB, partially because of familiarity and partially because of some really neat functionality. And I disagree good toolkits matter, especially if you aren't working with something that is gonna go into production. If you use it as an advanced calculator, plotting, doing algebra Python + ... (sorry I'm lazy) is prolly the neater alternative. But for me it's a matter of availability of MATLAB in work and, well I'm used to it in other applications. And as someone else said, it might be messy but there is some useful resources. Edit: Looked some more at SciPy and it's signal analysis part, looked quite interested I shall play around a bit with it. Still I feel that a lot of the functionality is very MATLAB-esque (in what functions that are implemented etc.. for better or worse! :-) )
The good thing is that a lot of Python libraries are based on their MATLAB equivalents, so you can still benefit from their documentation :)
I heard that PyIMSL is better than NumPy, but I've never used it. Spyder IDE is really good (and nice).
Other people have had reasonably good experiences using either the python that comes w/ Snow Leopard, or the MacPorts or HomeBrew python builds. Good for them. :) For me, however, I've had nothing but pain using the system python or any one provided by a packaging system. It works well at first, but then I get to a point where I need to use C extensions and everything goes pear shaped. Major pain. Never again. Instead, I'd strongly recommend using your own Python build. There are 3 options. One is to just get the python source code and build it yourself, from scratch. Another is to use the python.org binaries for MacOS. These are both fine, they'll get you a working python installation, but then you'll have to install virtualenv, pip, and etc. to get the good developer toolkit. My preference (also already mentioned here) is to use the following buildout: http://tinyurl.com/2gyrsmp Instructions for using it are here: http://tinyurl.com/2vhv5jp After you run the buildout, you'll have several separate python versions, and you'll also have several different "virtualenv" binaries (e.g. "virtualenv-2.6", "virtualenv-2.7") all set up so you can immediately bootstrap sandbox environments using the python of your choice. This is the first thing I do w/ new Macs, and I've not had any headaches or C extension problems since I've done so. I've also rescued several other folks lost in yak shaving by telling them to throw out the system or homebrew python they were using and to start over w/ the buildout. In every case it worked and the problems they were experiencing went away. Note that I'm not slagging packaging systems in general. I love homebrew, and I use it to install all sorts of dependencies (database engines, underlying C libraries, etc.). I just don't use it for python itself.
Google pyImsl, wrappers for the IMSL numerical libraries. Much better than SciPy imo. Pretty sure it's free for non-commercial use.
Yes, you should do this. Those are the packages to use plus matplotlib for graphing. I originally got into python because I needed a matlab substitute before I found it could be used for other things like web apps (I had been using perl for that).
I was a MATLAB user myself, and also rather hesitant to switch. Give Python another try, I've found it can be just as "quick and down to business" and **way** more flexible.
How much did you rely on Matlab previously? Did you use it for basic plotting and data management? or did you really use the linear algebra and control system toolboxes? I'm genuinely curious as I use Matlab a metric ton here at work, but would really like to use Python at home to emulate some features and serve as my own engineering solution.
yeah, this guy is constantly posting these examville links to presentations hosted elsewhere, with no attribution that I can see. -1.
http://www.sagemath.org/
Python + Numpy + matplotlib fully replaces basic plotting and data management. If you rely on the toolboxes a lot you might have problems, but the basic Matlab functions are almost all duplicated. I use it to make publication-quality plots all the time, like [this one.](http://clouds.eos.ubc.ca/~jdawe/FIGURES/paper2/png/cloud_base_schematic.png)
This setup has the added advantage that it allows you to develop against the same environment you will deploy to. It's also good practice should you ever need to use a server for some intense computational work that requires oodles of RAM... Many IDEs support the SSH + editor type setup as well, making this less of a pain than you might think.
Any experience with matrix stuff?
SQL Alchemy has a much better model and base that it works on. It would be good to rebase the Django ORM on top of SQL Alchemy so that the Django ORM has the same interface, but better internals. I would love to do this myself, but for time and money.
Yes, python is better than matlab. The key libraries for me are: numpy, scipy, pytables, matplotlib, VTK, traits (+TraitsGUI), Cython and (obviously) python's std-lib itself. Additional tools like IPython, Mercurial and Eclipse+pydev make up my working environment. 
Yeah - definitely. Filter design, analysis, etc. No problem. If you need to use the neural net toolbox or the aviation toolbox or whatever - get MATLAB. 
As someone who switched from perl to python, idiomatic perl code is definitely harder to read even when it's my own code. The main problem is the $/@ clutter and the bias toward implicit variables. Python has some ugly and confusing parts, but you generally use them to write low level frameworks that make it possible to write simple and elegant code at the high level. I'm sure it's possible to write elegant code in perl just as it's possible to write a functional program in C or an OO program in Lisp, but that's not the default.
I wish that could be said of matplotlib, especially it's 3d plotting
You seem to be missing the crucial part: a dead process which has not yet been polled (so called "zombie") is extremely lightweight, basically a placeholder for a single integer -- the return code. So don't worry about them consuming critical system resources, they don't (except for PIDs, that is). Then, about sending back the results: well, you can give each child an output pipe where it would put the answer on success (and would block until the answer is read by the parent, but that's OK since there are very few of them), or just die -- what's the problem? Or you can make your parent decide which keystrokes to send, remember that decision and correlate that with pids of the children who called exit(0) as opposed to exit(1).
How fast do you want to go? http://www.scipy.org/PerformancePython
The matrix math stuff in numpy is great. Its based on LAPACK and BLAS which are also used in some form by matlab. It is a little more verbose then matlab or R and you have to be more aware of typing then some either languages or python will either yell at you or give you undesired results (ie 1/2 = 0 but 1./2.=.5 and "1"/2 = error). For me the big plus side is that you get far superior string handling and core python constructs like the Dictionary, Iterators, Generators and list comprehensions that make building non matrix math algorithms a lot easier. And if you ctype everything you can compile it with cython for big speed gains on the parts you can't vectorize. 
Damn straight. In either sense, actually. EDIT: totally blew it. Reply should have been, "You know me."
I use vagrant and a headless Ubuntu. The code is nfs mounted onto the Ubuntu server so I can use my mac editors: textmate or vim in ssh. To start work in the morning I just `vagrant ssh` to get onto the box and then installing packages I can use apt-get, much easier than homebrew, fink fun.. Becoming more advanced you can set up chef rules to ensure your environment has the correct packages installed etc.. If I screw the environment up I can scrap it and start a new base box easily, without a total system reinstall :)
You and MereInterest are both my heroes. I would have never thought of a simple solution like that; I tend to overengineer and over semicolon. Thank you so much, this is the perfect launching pad for me.
No need to thank me. Thank you for asking! :)
Fair point!
I don't feel I asserted an inability, he said, trying not to take the tone of the reply personally ;) But, yes. "There's more than one way to do it." is a mantra in perl. I'd been a fairly heavy perl user for 6 or 7 years, and I would *still* encounter idioms (and syntactic conventions!) that were completely new to me, sprinkled throughout Other Peoples Perl. Having several, completely divergent, syntaxes for effectively equivalent operations is a design flaw, not a feature, IMHO. "Magic" or implicit variables as default parameters to and return values from functions is a scoping *nightmare*, IMHO. Keeping everything straight in your head while reading perl is *doable*, I'm just saying that in python it's *easier*. Easier to such a degree, and in such a way, that I think it can be said to be objectively easier for humans to read and understand. Although, I'm willing to hear arguments to the contrary. And, of course, if one choses to use a baroque, inefficient, or obtuse language out of personal taste, I say go for it. I love to learn languages just for practice, and for fun. But for keeping things working well, for keeping the [REDACTED] online and the [REDACTED] flowing through the [REDACTED], python is, IMHO, the better tool.
We have to run it through KVM each time. I've been looking into CryoPID2, but I'm not certain it would work. I think it might, but the other two fellows on the project are pretty resistant to changing the base platform we're running it on given that they're already having the fastest run of all time by a long shot.
Just write them to disk? sys.stdout = open("afile","w") print "blah" sys.stdout.close() (Though it is really hard to get stdout back in a process after you do this and I'm not sure how portable it is.) ON linux you can even put the files in "/dev/shm" and have it in shared memory that acts like part of the filesystem... or put it on a network mount from your vm. 
They did re-add 3D support fairly recently, though it's still far from MATLAB quality. I believe there are a bunch of other 3D plotting libraries out there that are much better, but I haven't tried them.
FYI there are a bunch of Python neural network libraries out there.
I haven't done the most python threading but won't the Global Interpreter Lock make this run as slowly as just doing a loop in one thread unless you use processes instead of threads?
Is Traits that popular? I've glanced through the tutorial once or twice, but never felt too compelled to actually use it.
Do they do that at the Apple store now?
Put "from \_\_future__ import division" at the top of each file and you'll never have to worry about integer division not doing what you expect. 1/2 = .5 is normal behavior in Python 3 anyway.
http://www.cryptomuseum.com/crypto/enigma/ * Enigma Simulator for Mac OS X (Terry Long) * Enigma Simulator for Windows (Dirk Rijmenants) * Crypto Simulation Group (Windows) * Enigma Simulator for RISC OS * JavaScript Simulator for Enigma I, M3, M4 and Abwehr (works with all browsers) * Enigma Java Applet (Andy Carlson) * Java Applet of the Enigma-G (Anton Haddeman) * Simple Java Applet of 3-wheel Enigma (James Brunskill) * Enigma Simulator in Macromedia Flash * Enigma Simulator for Palm Pilot (Michael Rövenich) * Build your own electronic Enigma: Enigma-E ?? 
[pdb](http://docs.python.org/library/pdb.html). It doesn't do exactly what you want, but step through your program and check your memory usage (top on \*nix, Task Manager on Windows). 
A quick run thru the forests of Google into the land of Overflowing Stacks leads me to suggest you have a look at: [Heapy](http://guppy-pe.sourceforge.net/#Heapy) 
SIMULINK? Really? 
Talk to me, I do dsp.
Scipy is pretty great. It's not as feature-complete as matlab yet, but I find having the code available is a big plus. And of course python is a much better language than matlab script.
My knowledge of python threading is also limited, but here is how I understand it. In this example, there are no speed increases from the multithreading. However, in NonVotingFelon's actual program, there should be speed increases, because it will be done using subprocesses. In nethack, there are many random things that happen randomly based on an initial random seed and the player actions. I assume that the virtualization mentioned is so that a particular random seed is chosen for each test, as I think that nethack's initial seed is solely time based. If, for example, you want to ensure that a particular outcome occurs, say, the genie that came out of a lamp gives a wish, you can test it by spawning a new instance of nethack with the same random seed, running it through to your current position, then rubbing the lamp. If the result is not the one that you want, you spawn a new instance, run it up to the current position, add in a few junk movements to eat through the next number in the RNG, then rub the lamp. Repeat until you get the response that you want. In a speed run in nethack, this can be very useful, as you could drink from the first few fountains that you find, be guaranteed a wish-granting water demon, then proceed much quicker than would normally be possible. Each luck tester, as I understand it, would be managed by a thread in the main program. Each luck tester spawns an instance of nethack, tests a particular set of moves, then returns the set of moves and the result in stdout. The thread checks if the result is the desired result, and signals for the other threads to stop if the desired result occurs. Since most of the time in the main program is just the thread waiting for the luck tester to finish, there should be a speed increase by using multiple threads. This could be done in a loop, maintaining a list of currently running subprocesses and checking each one as it finishes, but I think that using threads is a bit cleaner.
Yeah - I know. That was just an example. Also, I don't think that any of them are as hand-holdy as the stuff in the MATLAB toolbox.
I tried Octave - I mean really tried it. I couldn't get MATLAB to install on linux a while back and needed to write a simulation for a grad class I was taking. At the time I was a very heavy MATLAB user at work. Octave mostly works but its plotting is horrible and debugging is horrible. I would take the code that I wrote at home and bring it to work periodically to debug it to see where I had screwed up. I want to be able to recommend it but at the end of the day Python is way better.
i would highly recommend taking a look at IPython, especially the upcoming version (due out soon). if you need to have Matlab compatibility, then check out Octave. if youre on Windows, check out http://pytools.codeplex.com which has IPython integration as well. yet another (which also incorporates IPython) is Sage. your other major computer algebra options are: Maple, Mathematica if u dont want to go the Python route.
so your only "medium" map is also one with extra bases? nice going
Seems to be missing the link, which I assume should be http://www.stealthcopter.com/blog/2011/05/recreating-the-enigma-in-python/
Thank you for your reply. My experience is the opposite of yours. Not every app written in Python is well designed, well factored and easy to maintain. But is that the fault of the language? No, of course not. But then, if someone writes a crappy app in Perl, it's the language's fault? I really don't understand that. I've worked on a bunch of large, well-factored, readable, maintainable Perl code bases. If I see a crappy app in Perl, I think "crappy code," but others seem to think, "Perl is to blame for that." That is such a peculiar phenomenon to me.
Do you have any recommendations? Especially for embedding in GUIs, e.g. wxPython
Sorry, I haven't played with them. Try looking up Mayavi2.
well, I can give you that it isn't pretty ;-)
Answered my [own question](http://wiki.python.org/moin/NumericAndScientific/Plotting) :)
* Perl 5 is a better shellscript language * Python is a middleground between Perl and C♯/Java et al * Perl 6 will be a better Perl 5, Python …
I do use Python/SciPy from time to time, but it seems that if I want something done I usually revert back to MATLAB, it's not that I dislike the former but I guess it's a bit of a comfort zone thing - I love Pythong as a language though, it really get things done.
However, NumPy is implemented in C, so even if the rest of Python is doing something sensible, dividing NumPy integers still takes the floor. But most matrices are made of floats anyway.
Sage is more a replacement for symbolic systems like Mathematica than numeric systems like Matlab. Last I knew, if you wanted to do numeric things with arrays, Sage's documentation said "ignore Sage and use NumPy, and maybe convert it later".
Install ActivePython. It includes easy_install, pip, and virtualenv, so you don't have to install them yourself. Furthermore, it comes with with a binary package manager ([apt-get of pip](http://code.activestate.com/pypm/)).
Of course. But the fact that if you redefine equality you probably have to redefine also hashing is quite common in OO languages…
Could I have simultaneous Java and Python projects from the same codebase? Also... does Netbeans support the features of Pydev? I could not live without the pylint error detection features.
I'm using NumPy in Python 3.2, so I get 'true division' automatically, but even on Python 2.7, I don't see this happening: &gt;&gt;&gt; import numpy &gt;&gt;&gt; a = numpy.int32(1) &gt;&gt;&gt; b = numpy.int32(2) &gt;&gt;&gt; a/b 0 &gt;&gt;&gt; from __future__ import division &gt;&gt;&gt; a/b 0.5 
Why don't you post your code? It's probably something simple.
Nice snippet, thank you. And thanks for the prompt to upgrade to Sublime 2. 
I'm in grad school studying computational neural models. Almost all of my analyses are written in python, mostly using numpy, a small bit of scipy, and a healthy dose of scikits.learn. Python is definitely the language to go for this stuff, although depending on what you're doing, legacy matlab code might bog you down a bit... If you do use numpy on a reasonably modern desktop, I would recommend getting it to work with Intel MKL -- the multiprocessing for certain operations (matrix mulitplication and SVD mostly) does speed things up.
Those are NumPy scalars, but it applies to arrays as well. In [1]: from __future__ import division In [2]: a = arange(5) In [3]: b = arange(5) + 3 In [4]: a.dtype Out[4]: dtype('int64') In [5]: b.dtype Out[5]: dtype('int64') In [6]: a / b Out[6]: array([ 0. , 0.25 , 0.4 , 0.5 , 0.57142857]) In [7]: (a / b).dtype Out[7]: dtype('float64') 
You only need a single Python process. Keep a pool of nethacks, starting with a single process. Fork your nethack population to make it more numerous, send each a random keystroke, pick a few that seem to perform well, kill the rest. Rinse, repeat. I'll assume you're not on windows and have cheap forking; linux is particularly good at that. Since you can't ask the kernel to fork an external process on your behalf, you'll have to patch nethack, or maybe use an LD_PRELOAD hack to fork before read().
By the way, check out [/r/scipy](/r/scipy). Not just about SciPy the package, but about the entire Python for scientific computing ecosystem.
That's it. It really comes down to what you're trying to specifically do. There's a huge overlap between the capabilities of the two languages, but there are some packages or imports that are only on matlab, or only on python. So if your specific thing you're trying to do can be done in python, then go for it! But there are certain things matlab has packages for that you'd have to basically start from scratch in python to do.
[here it is](http://freetexthost.com/ijosvhxml0)
Well, I will only be starting next semester in courses where I need to do signal processing so I am not sure what to expect. How is it in matlab compared to python. This is from the course description &gt;Discrete and continuous signals. Transfer functions and convolution. Difference equations and differential equations. Fourier series and Fourier transforms. Fourier analysis of discrete signals. The Laplace transform in analysis of continuous signals. The Z-transform in analysis of discrete signals. Homework and Matlab computer assignments. Think I will be able to do this in Python? 
Oh boy, I tend to be a bit of a perfectionist at times. I was considering, and I don't quite like the idea of having a separate script just so that there can be a separate python process. Looking into the multiprocessing module, it looks easy to have a function from within the current script as a separate process. This also makes it the pipes between processes be unnecessary, as multiprocessing can pass information in different ways. [Here](http://pastebin.com/i3GgwPQx) is an example of how to use multiprocessing to run multiple instances of a new process without needing to use multiple .py files.
What is Simulink used for? I always see it advertised in IEEE spectrum.
When I see the term "Pythonic" I read it as "I like it".
I have a free license from my school for Matlab so that is not a problem. I just know a little more Python and really like it. I also like that Python is open source and in the long run I think it has more potentiality than Matlab. I have read a little bit about Octave and I don't think that would be the best alternative. If I was already good at Octave I might stick with it but I think that I would rather invest my time learning an all around good programming language than a rather limited one.
You can use [this](http://blog.ufsoft.org/2009/1/29/ipython-and-virtualenv) to make python system's ipython know about virtualenv.
Simulink is for people who *refuse* to learn how to program. 
Grab SAGE. You'll be able to do all this just fine. However, you probably won't have much support from your professor or fellow students if you're the only one using Python.
Fuuu, I have a AMD machine :(
I am a decent coder compared to the rest of my class so I don't worry to much. I also have /r/python, /r/scipy and google :)
&gt; TL;DR: Ubuntu 12.04 LTS will contain only Python 2.7 and 3.2, while Ubuntu 11.10 will contain Python 3.2, 2.7 and possibly 2.6, but possibly not.
How is Haskoli Islands?
Oh dear. Your indentation is entirely lost. Can you please paste it here: https://gist.github.com/
Just fine, but on the smaller scale. They are trying really hard at the moment to become a more research oriented university and performing overall better on the international stage, but the recession hit hard :/
Sorry about that, here is [try#2](https://gist.github.com/982198)
import this
It's easy to reproduce the memory error: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; from scipy import ndimage &gt;&gt;&gt; a = np.ones((200,200)) &gt;&gt;&gt; b = np.ones((200,200)) &gt;&gt;&gt; c = ndimage.convolve(a, b) .... MemoryError Try using the signal processing package instead of ndimage: &gt;&gt;&gt; from scipy import signal &gt;&gt;&gt; c = signal.fftconvolve(a, b) Note that the 'mode' options are different , so you'll have to experiment to get the results you expect. You'll have to do your own reflection pre-processing since the default 'reflect' mode of ndimage.convolve has no corollary in FFT convolution. I usually use zero padding, anyway. 
Thank you, I will read up on the signal package tomorrow and try it. However, do you know why convolving 2 arrays with ndimage should be so demanding? It shouldn't even use 1/1000 of the RAM I have, let alone have no memory to run. Does python limit ram allocation? If so, is there a way to turn off the limit?
I seriously hope this doesn't conflict with my teaching schedule. Last years conference was very cool.
Mayavi is supposed to be good for this.
Octave is a nightmare, it is only great for running some basic Matlab scripts on a computer which does not have matlab. Stick with Python. I was able to manage to convert all my systems biology work very nicely to Python. Python is also great for organizing large pieces of data to integrate things like data mining and things like ODE models.
Or [termcolor](http://pypi.python.org/pypi/termcolor).
 colorgrn = "\033[22;32m{0}\033[00m" print colorgrn.format("this text is in green") This one prints the text in green then sets the color back to default.
Blah blah blah. Say less next time.
sounds like those are descriptions of classes. i love python, but in all honesty, if the course is designed for matlab, it will likely be a significant amount of extra work to do it in python. you'll have to look up the python equivalent of everything you learn in class. you won't be able to use any example code and functions provided to you, you'll have to rewrite it all. your professors/TAs might not know python and might not be able to help you with it. unless you are very confident in your abilities so you think you'll have plenty of time to mess around with the programming rather than the engineering side of things, i think you should stick with matlab for now.
You should try [Sho](http://msdn.microsoft.com/en-us/devlabs/gg585581), which integrates IronPython (.NET) with visualizations and numerical libraries. [Enthought](http://www.enthought.com/repo/.iron/) is working on NumPy and SciPy integration as well. For electrical engineering and control systems though MATLAB is standard, don't fix what ain't broke. You're a student, the software is free, spend your time learning the theories not debating with yourself over which programming language to use...
Finally, but with their latest release I don't know if I'll go back to Ubuntu or not. Too many other problems. But I'm happy for python, especially 3.0+ being adopted so soon by a large project.
Either elegant or readable. Which occasionally mean the same thing. To be more specific, it is pythonic to mimic native python objects. 
I would recommend against eval or exec for security reasons, especially when dealing with input from end-users. The way that I would do it would be to have the program read from a file and generate the first generation from the contents of this file. That way, you don't have the security concerns of executing arbitrary code. I can think of a few different ways that this could be done, depending on the type of program you are writing. If there is a small population, then you could have each individual specified as follows. AABb aaBb AaBB You then parse the text file to determine the makeup of each individual in the first generation. If you have a lot of different traits, then you could have a csv file or something of the like with a list of the different traits. Alternatively, your population might be large enough that you are dealing solely with distributions rather than with individuals. Perhaps there is a uniform distribution of alleles throughout the population. You could then specify a trait distribution as follows. Trait: Height A: 40% B: 60% The key would be to think about the data that you have, and what you want, then make a format that suits that data. Do any of those sound reasonable, or get you started on the right track?
As simple as possible, but no simpler.
python has some synchronized stuff in the standard lib like [Queue](http://docs.python.org/library/queue.html). You could use these to simplify things a bit (no futzing around with locks, for example)
Based on looking at many Python projects on github and other sites, I've concluded that every Python project description is required to include the term "Pythonic". It seems like new Python programmers are dying to use the term, and as a result it has very little meaning. A bit like how the word "meme" has been so overused that its meaning has been diluted.
It's probably a bug. Here's the most recent commit of the `NI_Correlate` function in [nfilters.c](https://github.com/scipy/scipy/blob/master/scipy/ndimage/src/ni_filters.c#L152) and also `NI_InitFilterOffsets` in [ni_support.c](https://github.com/scipy/scipy/blob/master/scipy/ndimage/src/ni_support.c#L483). Either of these could be triggering the call to `PyErr_NoMemory`. However, I'm not set up to debug this and don't have any connection with SciPy development. Maybe you should file a [ticket](http://projects.scipy.org/scipy/report). 
We will not be studying matlab in class. We are expected to know it. There are two courses that teach introduction into Computer science. In one they teach Matlab and in the other Java. EE and CE learn Java and are expected to learn it Matlab by our selves :/
That was exactly what I was thinking, the thing is that I know python but I am a total newbie in Matlab. But guess I will one day or another be forced to know something in Matlab :/
Be warned Apple removed support for the PPC architecture in XCode 4. This can lead to complications when installing some modules. See this [bug](http://bugs.python.org/issue11623) for more information. Best advice is to stick with Xcode 3 for now.
Well the main advantage I was trying to gain with letting them supply a function would be for gender and whatever else would effect the mate-ability of a person (a girl with the bitch gene is less likely to mate so there is a smaller chance of her being included in the first generation, etc). But I was also trying to do this for... well, I didn't know what to call them, so I decided on calling them "sliders".... just floating-point descriptions of phenotypes that I figure are far too analog for a computer to be doing multiple punnet squares over - height, weight, whatever (so an African is likely to have their nostril width be like 80% or so, an Asian would have a height around 40%, a female would have a lower height, etc.). And I'm not really worried about security (it's their own computer, WTF do I care?), it's just that eval() can't do code of any decent length and exec() always returns None. I don't know, could I let them fill in a dictionary with a reference to a function (assuming this is called from another script)?
To me its implementing the program using the Python language, as opposed to implementing a program that happens to require the Python interpreter to execute. So the program will be using language features such as generators &amp; comprehensions, the methods and classes provided as builtins or in the standard library, and object oriented code will inherit from builtin/stdlib classes instead of reinventing (possibly triangular) wheels. Imagine a representation of storing a list of people's names. class People: def __init__(self): self.names = [] def add_name(self, name): self.names.append(name) Its not inheriting from list (or collections.UserList), nor is it inheriting the basic operations that lets other code use it as a list, plus its defining its own interface for adding data to a list. Will it work? Possibly - its syntactically correct at least. Is it Pythonic? no. 
I've found that doing things the right way almost always results in longer and more uglier code - particularly when using an OO paradigm. E.g. when you inherit from an existing abstract base class you first need to define your class that way: class People(collections.UserList): def __init__(self, stuff_for_me): collections.UserList.__init__(self) self.stuff_for_me = stuff_for_me as opposed to the 'mimic a native python object' approach: class People: def __init__(self, stuff_for_me): self.names = [] self.stuff_for_me = stuff_for_me Its the second that ends up being not Pythonic because when eventually using the People object, I can use it as a list with all the normal iterable operators, methods, etc. and you can't. http://docs.python.org/py3k/library/collections.html#module-collections 
I was not away of this. Thank you. Unfortunately, for this particular use, I'm not sure how well it would work. I can see Queues being very useful when there is a set number of inputs, all of which need to be operated on. However, here the goal is to find a random sequence that satisfies particular conditions, and so each thread is to create and test one such random sequence, then kill all such threads and continue if a working sequence is found. Is there a similar module that would manage this? I didn't find anything that seemed like it would do the trick after a quick look, which is why I was trying to figure it out using events and locks.
Can you explain this a little bit more? I'm still not great with Python.
While exec will return None, the code will be executed in the local scope (unless you give it a different one), so after the exec, whatever function was defined will now be accessible with its normal name. code = """def foo(bar): return (1,2,3,4)""" exec(code) foo(42) # foo is now a function accessible in this scope
but even so, i assume your textbook will use matlab, your lectures will use matlab, your examples will use matlab, functions provided by your professor will probably be in matlab, your homework assignments may be required to be in matlab, you might even need to do your exams in matlab. i had engineering classes like that.
py2exe
Shoot me a private message.
Can someone tell me if there's a compelling reason for both 2.6 and 2.7? 2.7 should be completely compatible with 2.6. Most of your packages would be available in apt-get (so prebuilt for the 2.x release in the distro). A modest number of users would have extensions that would need to be rebuilt but that shouldn't be that a big deal - and if it is a big deal, they probably don't upgrade with each new release. More likely they would just upgrade when an LTS (long term support) release like 12.04
BTW, my preferred way of working is Python for ease of development, but C/C++ when you hit performance bottlenecks. SWIG is useful for wrapping libraries into Python modules. Although I'm not sure how that works in the Win32 world.
Use multiple inheritance in the first case. class People(list, collections.UserList): def __init__(self, stuff_for_me): super(People, self).__init__() self.stuff_for_me = stuff_for_me Note that the order of the base classes is important. EDIT: Now that I've read the official docs for UserList, I think inheriting from those abstract classes is completely unnecessary. If you really want People to be used as an iterable, just expose self.names as a property (using the descriptor protocol).
Regarding the code you posted, NumPy is usually imported into the np namespace via `import numpy as np`. That's the usage style in the docs. Also, it's unusual to import scipy. Everything you need is in numpy: &gt;&gt;&gt; import numpy, scipy &gt;&gt;&gt; print('\n'.join(x for x in dir(scipy) if x not in dir(numpy))) SCIPY_IMPORT_VERBOSE __numpy_version__ fftpack ifft integrate interpolate logn majver minver misc ndimage optimize rand randn show_numpy_config signal sparse spatial special The functions logn (numpy.lib.scimath), rand/randn (numpy.random.rand), and ifft (numpy.fft) can be hacked into the np namespace if needed: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; _temp = __import__('numpy.fft', fromlist=['ifft']) &gt;&gt;&gt; np.ifft = _temp.ifft 
And /usr/bin/python2?
If you know python I would suggest using R through Rpy interface. R has extensive list of packages for scientific computing **EDIT**: R/SPlus is another python interface to R
It really depends on your requirements. I see below that you are a student and want a software package primarily for your subjects, but also future benefit. Python + Numpy + Matplotlib (+ iPython) is great as a pure Matlab replacement for many purposes. After spending the last 15 years in Matlab-land, I have recently (5 years) moved to Python and it is perfect for most of the stuff I do (vehicle modelling, simulation, operations research &amp; analysis). However, I still use Matlab for some things that depend heavily on Simulink, such as control system design. Unfortunately, for graphical model-based block level design, there is still no comparable equivalent in the Open Source/Python world! As someone else pointed out, you want to spend your time learning the subject matter, not any supporting technology. In that respect, if you anticipate building any systems graphically, you will find it much easier in Simulink than trying to cobble together some Python code to achieve the same result. If on the other hand, most of your work is going to involve coding, with some graphical output &amp; analysis, then you could probably use Python, etc. Bear in mind that you are not going to be writing any software for deployment, or even dissemination; it's more just a means to an end. I also note that you know some Python but not Matlab. Well I wouldn't worry about that too much: Matlab is very easy to pick up. In fact, one of the original aims for Numpy's design was that it have a similar syntax to Matlab. [Here](http://mathesaurus.sourceforge.net/matlab-numpy.html)'s a great quick reference guide which illustrates the similarities between them. Its object-oriented capabilities/design are nowhere as elegant as Python's, but you will have no problem learning the language - it would have to be one of the highest-level languages out there. Cost is not a consideration, and your assignments are going to be in Matlab, so I would just stick with that for the time-being. I could be wrong, but if you are going to end up working in the signal processing field, you are probably going to want to know Matlab/Simulink anyway. Good luck!
having read through more of this thread, I think I understand the problem a bit better -- queue is probably not a good fit. 
Between not liking unity ( i know you can replace it fairly easily) and far worse video performance with the same drivers I've given up on it. Currently I've got mint installed on my laptop and its working well. If they are going to be supporting unity as their most common desktop it only makes sense to switch to something that isnt. 
Okay, well I'm taking point against "mimic native python objects" as opposed to "using native python objects". Should have clarified that, especially as I may be interpreting fatalfred wrong. While it may be syntactically correct to roll your own list-like object, its more Pythonic IMO to inherit from the perfectly good list object Python provides. I picked on the collections module because IMO it does the right thing, has a PEP, links to more documentation on abstract base classes, etc. so I think it represents "Pythonic" well. When it comes to using these example People objects (I didn't try to make them complete implementations btw), the pythonic approach can do: p = People(("Sir Robin","Sir Galahad","Sir Lancelot")) print(i) for i in p while the non-pythonic approach will likely end up more like: p = People() p.add_person("Sir Robin") p.add_person("Sir Galahad") p.add_person("Sir Lancelot") len_p = p.get_length() for i in range(len_p): print(p.get_person(i)) However the code to implement a People class in my experience ends up looking uglier and possibly longer because you're dealing with pre-defined methods that look like def __set__(self, *kargs, **kwargs) def __del__(self) etc. and if you're doing it right you'll have all the right input validation, etc. which people who have started out the wrong way generally aren't thinking about. Its the use of the instance of such an object that ends up looking elegant and much more readable, as opposed to the entire code. Edit: formatting
Is this what you want? import sys, imp code = """ CONSTANT = 3 def spam(x, y): return(CONSTANT + x + y) def main(x, y): return spam(x, y) """ spam_trait = imp.new_module('spam_trait') exec(code, spam_trait.__dict__) sys.modules['spam_trait'] = spam_trait &gt;&gt;&gt; spam_trait.main(1, 2) 6 
:(
tanks :)
Idiomatic Python.
I think your safest bet is to use a domain specific language.
Could I do something like this: prepend = "def sirspamalot(whatever):" append = "Traits[s] = sirspamalot\ndel sirspamalot" code = "whatever they want" #Indent all the lines of code, too tired to think of a way to do this exec(prepend+"\n"+code+"\n"+append) Would that work (assuming that Traits is a dictionary and s is a string gleaned from a for statement in the current scope)? Hey, I'd only have to do that once (ooh! performance gain!), and I could just call it like Traits[stuff](whatever).
Traits is not nearly as widely used as it deserves to be. As the Enthought people say, traits is a Bid Deal. It changes the way you program and provides the fastest way to build a GUI app I've come across.
Is the source for CPython itself Pythonic?
Python is a scripting language, don't bother turning it into an exe when it's more than sufficient to run as a script.
Well, it "should be" completely compatible, but there are some small bugs that differ. If making 2.6 available is cheap, why not just do that?
Personally, the major blocker is the complete failure of the nvidia drivers. Proprietary or not, it seems they are just broken (at least they are for me and [a few others](https://bugs.launchpad.net/ubuntu/+source/nvidia-graphics-drivers-173/+bug/772207)). In my case drivers give me black screen, or they sometimes don't recognise the resolution of my screen, but sometimes they do. It's just utterly random. This makes the desktop totally unstable and frankly unusable. I've switched to Debian Squeeze with slightly older kernel and drivers and it works just fine so that's okay by me.
I read a great comparison between Perl and Python today and this was Perl's major pro. It's con was readability, Python is much easier to learn and maintain. From between the two, I'd suggest it would come down to this: if there's multiple people working on the project/system, Python's syntax readability and mandatory use of white space can go a long way to negating the problems inherent in a large team. If you're the sole IT guy and already know Perl, I see no reason to switch especially when presented with CPAN.
Seriously? Which IDEs? I'm going to have to investigate this. My big thing with this setup was like you said about working in the environment I deploy to, with all the same versions of our toolset. It sounds like I can have the best of both worlds if I can use a decent IDE over ssh.
Yes, also, making use of Python's built-in operators for example: http://www.markus-gattol.name/ws/python.html#pythonic
(disclaimer: numpy/scipy developer here) I have never used pyIMSL, but their package includes numpy/scipy code (it is even advertised in their white paper). I would be the first to recognize scipy is not as polished as Matlab (nor even as numpy), and that things in scipy.signal for example need more work. But I think some stuff are also starting to appear outside numpy/scipy proper, which is good IMO. For example, scikits.learn starts to have a nice following.
Using recent ATLAS with multithreaded support is pretty good too if you are on linux (or any non-windows OS), but is a PITA to build and install.
&gt; especially 3.0+ being adopted so soon by a large project. Simply being including in a distribution doesn't mean anyone is writing code for it (hint: they aren't).
Because Python doesn't make me want to kill myself and others.
Netbeans supports pylint as a plugin. Netbeans also has pretty smart code completion. Netbeans has a different environment paradigm to Eclipse. Netbeans is similar to MSVC or Xcode with project files instead of workspaces. Each project has a language, though write different languages in the same project. launching different applications through the single project might be tricky however. 
The first version certainly is not pythonic, but rather an abomination. Never use inheritance like this !
look dude, UserList is deprecated since eons (python 2.2 at least !) and inheriting from builtins is incredibly rarely a good move ... are People a kind of list ? no ! applying common sense, OTOH, is pythonic :)
not always
I think it's a rather large step. People can now count on python 3+ being available on every up to date ubuntu station they work with / develop for.
Don't make me guess.
It is also for people who has to work with it, because of existing models and previous decisions, not entirely made on your own.
Aside Arch, openSUSE has already been shipping a parallel-installable version of Python 3.
Given that it's in C, surely not.
errrr. How about for convenient distribution or another of the dozen reasons you might need a single executable?
Bleh. Doesn't answer the question, provides unfounded claims and FUD, factually wrong. What else could you ask from an answer?
elegant yet obvious. Don't play syntactical tricks.
Convenient distribution? If I update my python I don't want all my scripts to run on old versions, taking up disk space as well. I have yet to see a system without python installed, and as long as you specify the correct version (2 or 3) in your shebang distributing a python built into a binary is no more convenient than distributing the script itself.
Perl's con is not readability. Perl has many cons, yes, but readability isn't one of them. What people are complaining about is Perl's *flexibility*. Python and Java and Ruby have (generally) one way to do a certain thing. So, when ten experienced people need to solve Problem X, nine of them are going to do it exactly the same way. And, when they read someone else's code, they're going to see it solved that way almost every time. Since it's how they would have done it, they can read it. Perl's *motto* is TMTOWTDI (There's More Than One Way To Do It). So, when ten experienced people need to solve Problem X in Perl, it's very likely that there will be nine different solutions. This means that, since it's not how *I* would have solved the problem, I have to think through your solution. Think about it like this - Shakespeare can be harder to read for a lay person. I certainly found it so, when I first read it in high school. Yet, after the second play, it wasn't any harder than the scifi novels I read for fun. Same with the dialogue in Spartacus (if you ever watched the TV series). Or when you're talking to someone with a strong accent. You just have to flip a switch in your brain. Don't confuse that with Befunge, which is truly unreadable. Or, with Java which is so verbose as to be nearly unreadable (at least to my eyes, used to Perl's compactness). And, I still have trouble reading Lisp because I don't have the necessary 60 weird-to-me function names in muscle-memory so that it makes sense in a quick scan. Does that make sense?
Python + Pyside (Qt) + py2exe / py2app == Cross platform App Battery Included! 
if windows had a sane package manager that wouldn't be an issue ... windows is the only platform that needs such abominations for "convenience"
Have you by chance asked stackoverflow.com?
Tropospheric/PBL research via (doppler?) LIDAR by any chance?
I don't support legacy operating systems.
Yes, well, that's nice for you. I, on the other hand, work in a small repair shop where I see ~50+ never-before-seen machines per week. The are almost all windows boxes, and almost none of them have python installed. Many of them I will never see again. All of my custom tools are written in python and stored as single file executables for ease of use on these machines as a matter of necessity, let alone convenience.
I concur!
Not to mention cython (http://cython.org/). This lets you add a few type hints and then compile to native modules for big speed boosts, and also gives you an easy way to wrap your only native c code.
One solution for distributing a simple Python app is to package all of the scripts in a zip file (or an egg). The Python interpreter will run `__main__.py` in the zip. For example: #myapp.py print("spam, spam, spam") #__main__.py import myapp Zip these two files up as myapp.pyz and run it with `python myapp.pyz`. Also, if your app is a GUI, use the extension '.pyw' for the zip file. This runs the interpreter without a console window. For example: #myapp.py import tkinter root = tkinter.Tk() root.geometry("150x150+100+100") w = tkinter.Label(root, text="spam, spam, spam", height=75) w.pack() root.mainloop() Zip this up with `__main__.py` and call it `myapp.pyw`. The only thing it's missing is a unique icon for the file. You can use the GUI toolkit to change the icon while it's running.
If you can't install the interpreter on these machines, you could use Portable Python rather than build executable files. 
building an executable is basically combining your script with a portable python...
Yes, but you'd only need the one copy of the interpreter and installed modules, which would be simple to upgrade. Also, you wouldn't have the hassle of building exe files.
You can also use it to make the exe's into windows services... very handy stuff! 
What? Why? Just don't clear globals. Easy.
 &gt; (just for fun)
http://twitter.com/#!/nathanmarz/status/68800805855178752 What part of "just for fun" don't you understand? :-P
Good for you. You are not the only person in the world, though.
I think the fact that the core Python language is so small is one of its best features. Tim Berners-Lee called Python "a language you can learn on one laptop battery". Edit: URL to back that up: http://www.w3.org/2000/10/swap/#L88
py2exe is pretty easy, at least as easy as Portable Python.
have you ever used http://www.pyinstaller.org/ ? Is it any good?
In those examples, saying you had to append each name to your collection anyways wouldn't they just be the same? For example if I had to connect to a DB and .append each name to the list, wouldn't it be no more complicated than the example you've just given amongst both implementations? Although I guess you could just take an existing list and pass it, holding off initialization of People() while passing in the entire list in that case?
I &lt;3 PySide. 
Solid
Works for me. Can create a single-file executable.
Works for me, too. You can deploy to Linux, Windows and MacOS X with very little trouble. This is why I prefer it to the alternatives.
Cython is awesome, but ShedSkin is awesomer. I do think that RPython is the future, though.
&gt; *I will probably run into cases where c/c++ or some other language will work better* Unlikely. I moved to Python as my primary development language about 8 yrs ago. Only once have I had to use C++ since, and that was because I had to link against a C++ DLL. 
If the python interpreter is bugfree, you can't restore them all as you are running in a restricted frame.
You're right. I think they package scipy, and their wrappers make use of numpy. I wasn't trying to say that it's a replacement for scipy, but in my experience when an algorithm is present in both packages, it is more feature-complete on the pyIMSL side. Both packages contain some things the other doesn't... just another set of tools in my Python toolbox. :)
Because while 2.7 is mostly similar to 2.6, 3.2 is more closely related to 2.7 than 2.6. This way, you can adjust your programs in a linear fashion, from 2.6 to 2.7, then to 3.2. There are some major changes required by coding in 3.x, and 2.7 is at least peripherally aware of these requirements, sometimes giving DeprecationWarnings or other exceptions notifying developers that their code may not work in future versions of python.
There used to be a module called 'rotor' that worked similar to the enigma. It came with python back in the 1.4-1.5 days...
You all may want to take a look at http://pyschools.com as well. 
So nobody ported the quick and dirty prototype to an actual programming language?
And now with [ctypes](http://docs.python.org/library/ctypes.html) you probably wouldn't even need to use it for that :)
I've used it as well. Note, newer versions of python (2.6 and up I think) on windows are compiled using MS VS C 2008, requiring either the MSVCRT 2008 installed on the end user's machine, or you to do some crappy SxS stuff with your end executable. Either option is not that bad, just something to be aware of.
I do a fair amount of Android development, which is (mostly) in Java. The SDK's support for Python is nonexistent, AFAIK.
 __builtins__ = [x for x in (1).__class__.__base__.__subclasses__() if x.__name__ == 'catch_warnings'][0]()._module.__builtins__ There is a reason why rexec is deprecated.
A winner is you!
&gt; Can someone tell me if there's a compelling reason for both 2.6 and 2.7? When you have a [large software product](https://github.com/reddit/reddit) it can be difficult to test the entire codebase for compatibility, so you may want to upgrade your OS without having Python switch out from under you. For example, when reddit moved from Python 2.5 to 2.6, it was supposed to be 100% compatible, but it turned out that [there was a slight difference in the way C filters were handled](http://code.reddit.com/changeset/9813d94741cfff8c16d2326d9c7110ce61f820b7/r2/r2/lib/c/filters.c). That wasn't easy to track down.
I like the idiom of replacing [x for x in something if whatever][0] with next(x for x in something if whatever)
Except that you don't have `next` until you recover `__builtins__`.
Octave is like MATLAB, but without the toolkits and neat whizbang stuff that make putting up with MATLAB's programming aspects tolerable. I used it for a while and then gave up; it was usually easier to port whatever I was doing in MATLAB to R than to try to make it work in Octave.
This recent development is going to make you happy.... http://www.linuxjournal.com/article/10940 http://hameedullah.com/develop-your-first-android-application-in-python.html 
As an addendum: for quick-and-down-to-business work, I find that R is very often a suitable replacement for MATLAB and porting requires very little effort.
I tried. Honestly I tried. But the C++ DLL was both multithreaded and made use of callbacks in an odd way. I did try to wrap it in python, but it quickly became more effort than just doing it in C++ (What I did finally end up with was abstracting the whole library API into a JSON message passing thingie....so now it works with *any* language and I never have to touch the C++ again. Yay) 
While I was in your situation few months ago, the more I use the more I am unsure at this point. I am having a hard time with python's lack of encapsulation for objects (a class can modify another class by inserting members and functions which makes it very hard to figure out what you are getting without runtime debugging). Not so bad if only few people work on a project, but very tedious for a larger group. I still think it is the best scripting language out there, but I am having doubts about it being a language that I will develop desktop or web applications in the future.
You're not allowed to distribute a dll needed to run those exe's unless you have the relevant license (although the users are allowed to download it themselves). Small snag in distribution...
Why do you prefer ShedSkin?
Similar version, but works in Python 3.1: lookup = lambda n: [x for x in (1).__class__.__base__.__subclasses__() if x.__name__ == n][0] try: lookup('Codec')().decode('') except lookup('BaseException') as e: del lookup __builtins__ = e.__traceback__.tb_next.tb_frame.f_globals['__builtins__'] Okay, of course I'm well aware of the fact that there is no such thing like a restricted environment in Python 3.
You guys are aware of a lower level of Python I do not understand. Can someone clarify what "restricted frame" means or "rexec"? 
Dope
I work on a fairly large Python codebase shared among multiple developers so maybe I can comment. While nothing is stopping you from doing stuff like this, you just learn to stay away from it. There are no private or protected attributes in Python, and once you realize how loose everything really is that's kind of scary. But with some exceptions (such as reflection) we mostly program as if it had the constraints of a language like Java. I would say the biggest annoyance with Python when working on a larger project with multiple people is the lack of clear return types and unclear function parameters which may involve awkward or hard to determine behavior (passing None around, default arguments in different places, args*, kwargs**, etc). I believe that a good IDE is important when working on a large project (100k+ loc), and the nature of Python often makes it more difficult for the editor to help you than it could in a more structured language. There are some dirty implementation details and gotchas to Python, and I wish they had taken a more thoughtful approach to some of the language features, but overall it is a decent language to work with even in a large project. You do need to be careful of who gets into the codebase without supervision though, as you can do some crazy shit in Python that could never pull in Java. If I was doing it from scratch I might look at Ruby, seems like they took a more robust view to their object orientation.
Your approach is similar in concept to what we security folks call "fuzzing": throwing random input at programs in an attempt to crash them and thus find their vulnerabilities. For us, "success" is a crash. For you, it's not dying in the game. Maybe you can use the process-spawning, random input generation, and process monitoring features of an existing fuzzing framework. Some Python frameworks are available. On a related note, be sure to read about [a (fixed?) bug in Nethack re: it's PRNG](http://taeb-blog.sartak.org/2009/03/predicting-and-controlling-nethacks.html) if you haven't yet - neat stuff!
&gt; Having several, completely divergent, syntaxes for effectively equivalent operations is a design flaw, not a feature, IMHO. I hear this so often, but i have honestly never seen an example of this being to a programmer's detriment. And by example i mean actual code. Care to share your examples? Also, as for the matter itself: Having the ability to have different syntaxes can be a really good thing in finding out which one is best. Keep in mind that "There is more than one way to do it." has a corollary: "But not all ways are equal." Most perl developers are aware that in fact many ways to do things are crap. But you know what? Without the ability to go or MAKE different ways, people would be stuck with that one crap way. Which is why Perl says "hey, let's keep the old way around for a bit, for compatibility, and slowly deprecate it away" while adding a new better way. TMTOWTDI is a major factor in keeping perl an **evolving** language, just like any natural language. Just look at the way Perl 5 is converging towards Perl 6 with the creation of things like Moose.
Imagine that you want to run a untrusted code and still don't want the code to mess things up (e.g. writing arbitrary files, opening network connections). Therefore you naturally want a sandboxed environment. If the environment is properly designed, a untrusted code will either work as expected unless a potential attack is detected, in that case it will cause a proper error (not a crash or so). The problem is that Python is a highly reflective language, which means you can inspect the inner details (list of base classes and subclasses, bytecode for the functions etc.) of almost everything. Even if you erase (say) `file` type from the environment, you can summon it by searching `object.__subclasses__()`. An untrusted code may crash the entire interpreter; there is a direct interface to generate a function from a raw bytecode, and if the bytecode is incorrect, CPython will crash or more bad things will happen. In Python 2.x, there used to be a feature that restricts the access to the internal attributes somehow. This was called a "restricted environment" and enabled when it seems that the caller intentionally removed the access to the built-in module (i.e. `__builtins__`). `Bastion` and `rexec` module from the standard library relied on this feature to provide a sandbox. Unfortunately this was not sufficient (as I and sanxiyn demonstrated), and eventually got removed from Python 3.
Now the real question is, who's implementation is correct: PyPy's or CPython's.
Close, LES modeling.
Enjoyed hearing your experience. It reminded me a lot of the excellent article [Why Python?](http://www.linuxjournal.com/article/3882) by Eric Raymond, which helped me realize exactly what it was about python that made it so enjoyable for programming.
So how do you provide a sandbox in Python 3?
That's fine, as long as I could have a way to manage a bunch of source code in which there are C, Java, and Python files.
Welcome new convert! Now that you are one of the initiated, we will email you the antigravity module.
I think there is no way to support a sandbox with CPython only. The typical workaround is to use the sandbox mode of PyPy (I never tried it though) or OS-level sandboxing like chroot or jail.
i would echo what keypusher wrote; it is up to a team of software engineers to avoid certain issues that python makes possibilities. it's definitely possible to write very bad, non-scaleable, hard-to-comprehend-and-harder-to-extend code in python. it is also very easy to write code that is a pleasure to maintain &amp; extend with a group of developers. following best practices as a team ensures your application is the latter &amp; not the former. 
I am using Eclipse PyDev as my IDE and it helps a lot, but finding references has not been as accurate as I hoped. We use django which has the database ForeignKey stuff that inserts functions into other classes which is very confusing for new people and often confusing in general unless you know the DB structure. Also there is heavy use of fixup classes that you derive from and they modify the behavior of members/functions of the existing class (adding to the overall confusion when debugging). I didn't even touch on the *args and **kwargs issue, it can be very useful and it can cause mass confusion. What IDE do you use?
I completely agree, I have written code in python 10 years ago that is clear and easy to support, yet the codebase I am working on now is 1 year old and akin to a root canal every time I have to fix bugs introduced by other people. Let me clarify: If given a choice of language for a large project, I (personally) would not pick python as the main language, but rather a language for auxiliary tasks (build, lint, script). This has to do mainly with a wide variety of developers available on large projects. I have noticed that more people know of python the more they tend to abuse the language constructs. The most experienced python people on the current project are the ones that write the obfuscated code that mid to junior developers misuse and create a mess that needs constant refactoring. It may be a multi-layered problem, but as with any language, discipline is very important and often hardest to find and maintain...it seems.
Please enlighten me why you prefer ShedSkin. For my treebank parser I now use Cython because shedskin required more work, but if it's worth it I should look into it more. The limitation of Cython that I run into is that you can't tell it that a list or a dictionary only contains items of a certain type, which I suspect would make a big performance difference.
We also work on a fairly large Python codebase. We've gotten around the return types/parameters issue with having a strict docstring policy. You must document every function's parameters and return types in a format that's compatible with epydoc. Give it a shot!
FUD? You're reading into it way much. I'm not even sure what side you think I'm taking in this supposed FUD-spreading.
http://i.imgur.com/zW5GT.jpg
Just don't turn it on in an environment without boundaries! Like Python itself, if you use it in an unrestricted environment... well, you're screwed.
When you need some form of static linking (which this isn't, I realize that - but it serves the same purpose - call it static packaging instead if you will), you need it. I personally am a little tired of virtually everyone saying we don't need static packaging. It has it's purpose, even if it is the exception. You shouldn't have to fight variations of the DLL hell battle in essentially unknown or hostile environments. 
The shooting yourself in the foot part. (The PG thing doesn't appear to be relevant here.)
I think that my favorite examples of divergent syntaxes would be: Quoting shortcut macros: qw/bla bla/ as a substitute for ("bla", "bla") And, in fact, the entire family of "q*" shortcuts: I'm not a big fan of minimalist, easily confused functions or macros. And: autovivification, particularly in the case of hashes. Combine this with the "optional" quoting of hash keys, and you can get into some *very* hard to track down errors. At least, they caused me enough trouble that the weren't worth the typing that they saved me. I totally agree with your "right tool for the job" position. Python itself is evolving, I just find it more coherent than perl. Perhaps the lack of a free-form syntax makes python to constricting for some folks. I find that, to borrow a quote, stinginess with privileges is kindness in disguise. 
I agree: you can write awful python. But you do almost have to work at it. ;) Awful perl, however, is pretty much the default in my experience. You can avoid it, but again, you have to work at it. I will go on record as saying that I've seen some very well written, maintainable, nicely indented, well commented, well designed perl. 3, maybe 4 times. ;) Perhaps we are merely the products of having been exposed to different kinds of nasty.
That is what our standard is as well.
You can use typed numpy arrays with Cython but I've been using [go](http://golang.org/) to rewrite some web services that were prototyped in python and would recommend it...once you figure out interfaces it feels like a statically typed compiled language with a python like philosophy instead of static types tacked onto python. I'm not sure how the python go interface is since i've only used it for self contained webservices 
Welcome to the club! :)
Do you know if it's possible to use py2exe to access Windows UAC, I started reading up on it and it looks like UAC looks for a bit field in the executable file header... just curious if py2exe covers that.
[Annotations](http://www.python.org/dev/peps/pep-3107/) should be helpful for that once people start moving to Python3.
Because, in my tests, it gave comparable (if not faster) performance with exactly zero modifications (my code was highly algorithmic, though).
It depends on what you were doing, if you were making something very dynamic I can see how you would prefer Cython (and it's great, because Cython is awesome). My algorithmic code (machine learning training, simulations, etc) could get a 50x speed boost with ShedSkin with no modifications at all (I think maybe I changed one line? I'm not sure). I've detailed it here, if you like: http://www.korokithakis.net/posts/speeding-up-python-code-with-shedskin/
Awesome, good to know. I've been using Cython but will try out ShedSkin.
Actually, we have a 24-core Opteron system in our cluster which we also use MKL on. Although it doesn't quite benefit as much as the Intel from multiprocessing support, it does improve overall throughput. More likely, I think MKL speedup for mostly parallel operations (such as matrix multiplication) is not strictly linear. It remains linear up to about 6 threads then stops gaining an advantage.
Generally: * Follows the [Zen of Python](http://www.python.org/dev/peps/pep-0020/) Specifically: * Reads like a "book" * "Looks" like Python code (not C code, not Perl, not PHP). As an example, I've dealt with code from new-ish Python programmers that had a long history of C, in which Python functions used return codes instead of exceptions to relay errors.
import soul
google.... http://stackoverflow.com/questions/195109/running-compiled-python-py2exe-as-administrator-in-vista
&gt; Quoting shortcut macros: qw/bla bla/ as a substitute for ("bla", "bla") And, in fact, the entire family of "q*" shortcuts: I'm not a big fan of minimalist, easily confused functions or macros. See, i didn't ask for things that are disliked. I asked for things that are demonstrably adverse to the user, along with explanations of how they were bad. As for qw(): It really serves to increase readability. Often i'll have to do things like this: $flight-&gt;{$_} ||= 0 for qw( provision taxadt servicecharge taxchd preis main_nr restplaetze ); Compare with: $flight-&gt;{$_} ||= 0 for ( "provision", "taxadt", "servicecharge", "taxchd", "preis", "main_nr", "restplaetze" ); Not only is the qw variant easier to read, it's also a lot easier to type. :) As for the other q* things, keep in mind that sometimes it's really hard to use the punctuation variants. To wit: ; in #win32 on irc.perl.org (Ranguard) how can I do: perl -e 'print "hi\n"' ? - I get Can't find string terminator error (Mithaldu) Ranguard: dos quoting rules are stupid as hell (Mithaldu) for a simple case like that though, this would suffice: (Mithaldu) perl -e "print qq{hi\n}" Without that little shortcut he'd have had to grapple with DOS' quote-quoting rules which are sadly not as simple as putting a slash in front. &gt; Combine this with the "optional" quoting of hash keys, and you can get into some very hard to track down errors. Would you mind elaborating? I've been doing perl for a long time now and i cannot recall those ever causing errors. :o &gt; I totally agree with your "right tool for the job" position. ... I find that, to borrow a quote, stinginess with privileges is kindness in disguise. I tend to think about it more in a "right tool for the right brain" way, as both languages are really well suited for almost any job you'd want to solve with them. :) And yes, for the right brain python's philosophy is kindness, and for the wrong one a straight-jacket. (I really cannot stand how it lacks full lambdas. ;) )
That would work, I believe. For the tabs, I think that the following would work. ''.join('\t' + line for line in f.readlines()) I rather like using comprehensions, as I find them to be more readable than loops. Again, I recommend against using exec for security reasons unless you are the only one using this code, since something could throw "shutil.rmtree('/')" in there and severely mess things up, but if it is just for personal use (nothing on a server), go right ahead. I would suggest using ast.literal_eval() instead, if you can deal with having a bit less flexibility. It works without the security implications of exec, but only interprets strings, numbers, tuples, lists, dicts, booleans, and None. from ast import literal_eval Traits[s] = literal_eval(stringThatEvaluatesToSomething)
Thanks for this, great read!
 python -c 'import re; print "".join(re.sub("this", "that", line) for line in open("file"))' not as short but python has its one-liners as well
Ok. Let's dance with the devil. :) find dir -name '*foo*' -type f -exec perl -pi -e 's/this/that/' dir1/file1.\{\} \; Pipelining is your friend.
Welcome to the fold, my son.
You can always use mailinator to register to these things
[Yoda says it best](http://www.python.org/doc/humor/#python-vs-perl-according-to-yoda): EXTERIOR: DAGOBAH -- DAY With Yoda strapped to his back, Luke climbs up one of the many thick vines that grow in the swamp until he reaches the Dagobah statistics lab. Panting heavily, he continues his exercises -- grepping, installing new packages, logging in as root, and writing replacements for two-year-old shell scripts in Python. YODA: Code! Yes. A programmer's strength flows from code maintainability. But beware of Perl. Terse syntax... more than one way to do it... default variables. The dark side of code maintainability are they. Easily they flow, quick to join you when code you write. If once you start down the dark path, forever will it dominate your destiny, consume you it will. LUKE: Is Perl better than Python? YODA: No... no... no. Quicker, easier, more seductive. LUKE: But how will I know why Python is better than Perl? YODA: You will know. When your code you try to read six months from now.
I was referring to the case of working in a shop and having a toolbox of scripts to perform maintenance/analysis on Windows computers. The simplest solution, IMO, is Portable Python on a thumb drive.
Is py2app mature and good? What about dependencies? Will the end result be a huge download? I've been interested in Python for a long time but I've been a bit put off by deployment issues for OS X since 2009 when [this article](http://arstechnica.com/open-source/guides/2009/03/how-to-deploying-pyqt-applications-on-windows-and-mac-os-x.ars) was written. But maybe things have improved since then. I'm talking mostly OS X here but that's in part because that my primary dev computer at home, and part because I assume the Windows tools are more mature.
This whole thread has been great. &lt;3 Reddit. I knew neither about PySide or ShedSkin before clicking on that Comments link 5 minutes ago. :)
SL4A has been out for a year and its somewhat lackluster. It's great for personal scripts or one-offs, but its not really something that you would use if you expected anyone else to use something you wrote, and I don't think you can even deploy SL4A scripts to the android store (though there might be some workaround where you make a Java app that imports SL4A and then executes python scripts that you include in your apk. I'm not exactly sure...) Last time I checked, SL4A supported only a tiny portion of the Android API, and you couldn't even make real UIs using it, you could only use a few things like alerts and prompts. That also may have changed, or it's possible that I managed to misunderstand when I was looking at it, but that was my experience.
it's solid - most of the exe makers embed the python interpreter and whatever minimum libs they need for the code, so the final download usually isn't outrageous
Sorry, I meant to say that "I perceive multiple shorthand macros (q*, for example) as saving typing at the expense of readability." Obviously, readability is a variable. ;) Pointing out that the feature helps get around poorly designed terminals on dos/win machines doesn't carry any weight with me, but I have to imagine that some people find that a big win. But on the autovivification issue: The statement $a_hash{ABCDEFG} creates the hash %a_hash and the key "ABCDEFG", with a null value. No errors, no muss, no fuss. Except in those cases where the hash in question *wasn't* originally called "a_hash" when initialized earlier in the code, but is called "ahash" or "a_has", because of a typo, for example. Or the case where 'ABCDEFG' is a constant defined elsewhere, and you've typo'd (or had a variable name confusion, etc) 'ABCDFEG'. In these cases (and in others), the thing you have after the code is executed is not what you thought you'd get, and the only muss and fuss are yours, not the interpreters. No part of the system will help you catch the non-existent hash, or the non existent key, without turning off autovivification. Granted, these problems are programmer problems rather than language problems. But, they're programmer problems that are easier to have in perl than in python. Which is my only real point. That being said, it looks like I'm leaning closer to Java than I am to python. But I'm not. I'm all about the strong typing, but not the static typing: I assert that for the things that perl is *really* good at (the class of problems to which it is generally applied, the flexible, deals with the real world kind of problems), python is going to wind up giving you many of the benefits of better type and edge case checking than perl will, while staying much more "nimble" than Java could. And, I find consistently indented code and explicit namespaces *much* easier to follow, and I'm pretty sure that's objective. I mean, take a bit of *very* well indented perl code. Give someone about 15 malicious minutes with it. Then try to read it. It will probably *run* the same, but it won't read the same. I totally agree for the "right tool for the brain" bit. I know folks who swim the depths of perl, and feel perfectly comfortable there. And they're very productive. I know full time windows developers, also. Some of them are even happy. On the topic of lambdas, I agree: the minimal support for functional programming is frustrating at times.
Sure, you're welcome! tweet/blog about it if you like it as it will spark responses which can only lead to improvements.
Eclipse + PyDev.
Where does it say that? According to the list of [Sage component packages](http://www.sagemath.org/links-components.html), Sage includes NumPy and several other numerical packages, and contains the whole of scipy. 
Hi Fred Fred, Perhaps. And I appreciate your thoughtful reply. But I disagree that "Python is good by default" and "Perl is bad by default" regardless of experiences (not that you're really saying exactly that). Good design is a function of a good programmer. Hell, even _recognizing_ good from bad takes a decent programmer. The difference is simply that Python has less syntax (and more verbosity) vs. Perl having more syntax (and more conciseness). And off to one side are still other people who think both languages are crap compared to Lisp! One thing I can say about the Perl hackers I've worked with is that they can (and do) write excellent code in lots of different languages. They are language geeks, and many of them hack a lot of Python too :) It's been a long time since I could hack on only one language. These days, it's about four different languages, sometimes using one to output another :) So language-specific debates are kind of retro. We're stuck with all of them :) Have a good weekend, my friend!
Yes, it includes NumPy and uses it, but Sage has its own matrix class that's not interchangeable with NumPy. Sage's matrices are designed for doing high-level linear algebra and group theory operations, and aren't particularly designed for number crunching. Sage's matrices contain Sage-style numbers (or even field elements that you wouldn't normally call "numbers"). NumPy matrices use C/Fortran-style numbers, like Matlab does. It's a matter of the right tool for the job, and the subject line is "Matlab replacement". You *can* use Sage for number crunching because you can poke at the NumPy innards. Just like you *can* use Matlab for symbolic math by bolting on toolkits. That doesn't mean you want to.
The extra spaces in your sentence caused me to parse it like a ternary operation. :P
The pydocs say that it can only handle expressions, which is the same problem I have with eval(): ast.literal_eval("def lazy():\n if None is None:\n return \"Yellow\"\nprint(lazy())") Traceback (most recent call last): File "&lt;pyshell#5&gt;", line 1, in &lt;module&gt; ast.literal_eval("def lazy():\n if None is None:\n return \"Yellow\"\nprint(lazy())") File "C:\Python32\lib\ast.py", line 48, in literal_eval node_or_string = parse(node_or_string, mode='eval') File "C:\Python32\lib\ast.py", line 36, in parse return compile(source, filename, mode, PyCF_ONLY_AST) File "&lt;unknown&gt;", line 1 def lazy(): ^ SyntaxError: invalid syntax You can't define a function in it (whereas exec() does that just fine). Thanks for the indenter, though.
It is relevant. Obviously doing this in a production setting is a terrible idea. No one would ever do it. The point is obviously to understand how Python works in greater detail. If you're not curious about this question, I think you're not very interested in how Python works, hence Blub syndrome.
Simple and clear without verbosity. Of course, these are sometimes contradictory hence people disagree about them. For example, `map` is frequently considered unpythonic because there should only be one way to do it. Personally I consider map(foo, bars) much clearer/simpler than [foo(bar) for bar in bars] The first is shorter, but requires a different kind of syntax than the second. So it's unclear which is simpler. Personally, I'd like to do away with the term, since it mainly seems to mean "the subset of Python that I like", which is pretty meaningless.
I'm also looking to start learning - maybe we can get a small group going?
&gt; I will probably run into cases where c/c++ or some other language will work better You can also mix the best of both worlds. When a module of your Python app needs to be written in C/C++ (for efficiency, interfacing with an external DLL, etc) and then "plugged into" Python using its [C Extension API](http://docs.python.org/extending/).
via: http://groups.google.com/group/sqlalchemy/browse_thread/thread/7864341d8d9d2469/2994eb464a8d598c?show_docid=2994eb464a8d598c&amp;pli=1
&gt;...in 5 hours. wow
 find dir -name '*foo*' -type f -exec python -c 'import re, sys; print "".join("".join(re.sub("this", "that", l) for l in open(f)) for f in sys.argv[1:])' dir1/file1.\{\} \; this and my previous answer will buffer everything in ram though find dir -name '*foo*' -type f -exec python -c 'import re, sys; [[sys.stdout.write(re.sub("this", "that", l)) for l in open(f)] for f in sys.argv[1:]]' dir1/file1.\{\} \; will stream but still winds up with many long lists of Nones building up in ram find dir -name '*foo*' -type f -exec python -c 'import re, sys; p = lambda a, b: None; reduce(p, (reduce(p, (sys.stdout.write(re.sub("this", "that", l)) for l in open(f)), None) for f in sys.argv[1:]), None)' dir1/file1.\{\} \; OK yeah you have to get really crazy to do true streaming pass-through in a one-liner :)
Very true. It won't do functions at all, but if the particular type of initialization does not require a function, then it is a much safer way of inputting data. If, for example, your first generation consisted of 100 individuals, each with a specified genotype, then you could have a list of the form ['AaBb','aaBb','aabb','AABB'] that has the genotype of each individual. ast.literal_eval() can then convert this string as read directly from a file into the actual list.
Not really, but frenchtoaster has already explained why, so I'm just going to leave it at that. Thanks anyway, though. =)
Without examining in detail your case by definition standart (slow) convolution algorithms requires N^2 operations for N outputs. So (200*200)^2 = 1,600,000,000 which explains why you run out of memory. The fasts convolution fftconvolve in the signal package has O(N log N) complexity and that would explain why it works. 
Thank you for providing exactly the explanation I was looking for!!! :)
Completely -- thanks for taking the time to write it out. I read down the thread and saw similar, if less complete explanations.
I don't know why I just love(yes the irrational kind of love) SQLAlchemy to the point where I want to write a wrapper for it to use it in C++/Qt applications. I know there is a option where I can program everything in python using PySide(last time I checked it was not fully there yet) and SQLAlchemy but I also love(irrationally) Qt Creator and that is why I want a wrapper.
new features http://www.sqlalchemy.org/trac/wiki/07Migration
I have been wondering this and hopefully someone here can help: is there any sign that Python 3.x is being adopted by anyone in industry, or is it still relegated to the level of intellectual hobby. It was my understanding that Python 2.6 and 2.7 still dominate, and that neither (at least as of a year ago) showed little signs of being replaced by 3.x.
If you create a People class for this use case the code is definitely not pythonic in the first place, inheriting from built-in classes is a very bad idea as well.
Has anyone read this one?
No idea...3 is the future. So it is only a matter of time.
Never tried it, but looks quite interesting. When I switch to Python 3 I'll definitely check it out.
As someone just getting into Python, what is the real advantage of SQLAlchemy over doing it myself with the plain ol' mysql connector? I've been reading the docs and so far I'm not seeing a reason to use it.
* Perl 5 is a better shellscript language -- FUD * Perl 6 will be a better Python -- FUD
I'm curious what level this book starts off at and how you make Python web apps without a framework...
&gt; Never tried it, but looks quite interesting. **Why** I switch to Python 3 I'll definitely check it out. Freudian "why"?
Ha, maybe!
Things which come to mind immediately: * Automatic type conversion (MySQL only gives you back strings of data). * Ease of programmatically building complex queries (calling more methods is typically less error-prone than concatenating strings). * And potentially cutting down the code needed to do the above--abstracting building a WHERE clause and then JOINing required tables to a point where it's highly reusable isn't easy. * Portability between different databases for free (MySQL, SQLite, SQL Server, Oracle, etc, etc). * Potentially easier to use secondary caches (memcached, etc), particularly in a transparent manner. * Potentially less load on your database server through the "Unit Of Work" mechanism. * Automatic key propagation (no need to insert a row, get it's primary key, and then insert the dependent rows with that primary key). More generally (and perhaps most importantly?), SQLAlchemy is an _ORM_ (object relational mapper) that enables you to "bundle" logic with your SQL data making implementation of the [domain model design pattern](http://www.martinfowler.com/eaaCatalog/domainModel.html) trival.
&gt;how you make Python web apps without a framework... probably straight [cgi](http://docs.python.org/release/3.2/library/cgi.html) which would suck. :) 
Okay, I guess that makes some sense. Suspect I need to fiddle with it to be sure. I've not done anything specifically with ORMs before but guess I ought to dig into that too :)
I had issues with PyQt, Sip, with py2exe. I have had zero issues with pyinstaller, no hoops to jump through to get it working. My current project, ~10k lines and several different imports is also only about 12mb when compiled to an exe with pyinstaller.
looks nice. How does this compare to [plac](http://pypi.python.org/pypi/plac)? I'm guessing it's Python 3 only?
nice idea :O any suggestions on what we could use to communicate?
Could you explain this to me? If a particular structure, list for example, has most of the functions that I want, but I need just a few other functions to go along with it, wouldn't it make sense to inherit from list and then add the particular functions that I want? I ask because I am doing something similar in a program of mine, where I subclass from gtk.TreeStore to make it simpler in interfacing with a gtk GUI. I then use this subclass for storing and processing data, and since it is subclassed from gtk.TreeStore, it is simple to throw it into a gtk.TreeView to display it.
I don't use Python 3, and regardless, argparse seems to do the trick. I think argparse supports a decorator-like syntax.
&gt; also aren't the docstrings well integrated to tools like Sphinx and whatnot? AFAIK, they are completely separate. You view docstrings using pydoc (the `help()` function in the Python REPL -- that is, pydoc), and you process static text file docs into html using things like Sphinx and Pandoc. 
Did you even scan the article? It took me about 20 seconds to see he was using CherryPy.
.. and I take it you've never heard of WSGI.
+ "(with CherryPy) extract from a new book"
I hope I don't have to explain why that isn't python's strong suit. :)
Just witnessed my first code-off. That made my weekend. Thanks! :) Little off topic here; just a general question: I've been fiddling around with $bash scripting and it's starting to feel like a gateway drug to perl. Am I far off? I'm wondering because I'm curious about both Python in web dev and Perl, which seems to be more for automating tasks for OS system management? Or am I over-generalizing?
PyInstaller tales care of all the SxS mess for you and produce single-files that run on vanilla Windows 2000, XP, Vista, 7. No need to even be aware of this nonsense.
well you could probably discern from my post history that answer. :) since the wsgi spec was only updated for python 3 mere months ago, I would be surprised to see a book on it already. does that answer your question?
Why pyopt over [optparse](http://docs.python.org/library/optparse.html) or [argparse](http://docs.python.org/library/argparse.html)? Edit: also, as a suggestion, if it doesn't already, it would be cool to support the Sphinx doc string syntax for parameter documentation: http://packages.python.org/an_example_pypi_project/sphinx.html#function-definitions 
private google group with some skype action from time to time?
At work I use 3.2. Admittedly very limited projects but you are crazy to invest to much time in the old series if yo are learning. As to adoption I think things will move relatively fast now? Why because with 3.2 most issues that where holding things up have been resolved. Plus a large number of libraries are being ported or just recently released. There never was an expectation of instant porting, as there are significant changes in the 3.x series. The transition is however taking place and is accelerating. As to domination, honestly what do you expect, there are years of 2.x legacy code out there. That code won't go away, in fact it might not ever get updated. 
&gt; Just witnessed my first code-off. That made my weekend. Thanks! :) you bet! &gt; Or am I over-generalizing? over-specializing I think. both are totally general-purpose scripting languages. you'll find people doing system administration with python (though not generally in one-liners for the obvious reason) and you'll find people doing web dev with perl. &gt; Am I far off? In my experience, yeah. I'm pretty comfortable with both bash and python and don't know a lot of perl. I find if it isn't simple enough to be done with bash, it's probably complicated enough that python's readability and maintainability are going to be important (the code-off example above **is** simple enough to be done with bash, and the python version would never belong in a one-liner).
nope. just having a bit of fun. python's strong suit is obviousness and maintainability. learning python felt like learning a few powerful concepts that go a long way. trying to learn perl feels more like learning dozens of tiny rules. tell me this is hard to follow: import os import re r = re.compile("this") for dirpath, dirnames, filenames in os.walk("dir"): for filename in filenames: if 'foo' in filename: for line in open(os.path.join(dirpath, filename)): print r.sub("that", line, 1)[:-1]
You should read [PEP 8](http://www.python.org/dev/peps/pep-0008/) and follow it. Good code is self-consistent code. You omit a lot of context in favor of prose. Why is a 'meta' function like this an instance method? What class does it belong to? What's organism() and why is it a global? Why does the method fail silently when rand_func isn't given? Why 'for k in my_dict.keys()' instead of 'for k in my_dict'? &gt; (I realize I can do this with zip, if expressions and a lambda, but for blogging purposes I decided to do this in more than one line) 'Exercise for the reader' excuses like this do not make for interesting reading. If you realize you can do this, why not actually give the example?
only updated three months ago, but it's been around long enough that every web framework under the sun is built around it. Ian Bicking's blog is a good read.
At the time I made a decorator syntax branch for argparse with the help of Steven Bethard, but it never got integrated to the trunk because of a few obstacles. Pyopt really is a different way of doing the command line thing.
I'll add a comparison in the docs soon. It's a different way of going about parsing options/arguments at the command line. Pyopt - you have a function in python syntax and docstrings, you just magically expose that function to the command line. Argparse/pyopt - you define a parser, its options, help strings, etc, and after you get whatever parameters you wanted figured out - you call whichever function you wanted to run.
I haven't used argparse so I can't comment on it but I have used optparse extensively. From about day three, I was pretty disappointed at it being the option parser distributed with Python. The first 2 days, I was ecstatic that Python now had a built-in CL parser. But I got disappointed quickly. In earlier versions of Python (2.2?), the Python optparse docs included a diatribe by the author about why "required options" aren't supported. Basically he said, "they're called 'options', damn it, how can you have required options?". That kinda sums up optparse for me. CL parsers are about the most generic thing in apps. If you're writing a CL parser for a general-purpose language like Python that could be used by millions of people, you have to check your ego at the door. There will be a lot of valid use cases that you don't agree with or don't realize. (In this case, it's a [human] language problem. "Options" is just a poor word choice (noting that they are also referred to as "switches" and "parameters"). Apart from that, the module was awkward to use and more so to extend and there were a number of questionable design decisions (beyond just being "different than I would have done it"). Don't get me wrong, it's a decent module. I ~really~ like the callbacks. They let me factor away some insanely complex code that add some awesome power to my platform. However, a nice command line can make or break an apps usability. I coded around the optparse limitations but not all of the millions of Python programmers out there are willing or able to do so - or have the time. Python needed a better CL parser. I haven't used pyopt (still using 2.6) but have looked through the docs. It looks like a step in the right direction at the very least.
They said at Google IO 2011 it would take 10 years. Personally, I just use Python 3 for all new projects, and kept my old stuff with 2. I could always convert it with 2to3, but meh. =)
I see more and more library developers support python 3.2+ as well as python 2.6+. Tools and methodologies to do so where extensively covered during the last pycon edition. I think the developer community is starting to do the transition. Example major packages that support python 3: numpy and scipy. More here: http://py3ksupport.appspot.com/
Hardly any migration? Numpy and Scipy are just now converting. These are libraries with huge followings. With ipython and Matplotlib following in upcoming releases, practically the whole scientific python community will soon be ready for the switch.
I do tend to say "good" and "bad" when I mean "better" and "worse" in this particular context. I don't mean to imply that I find perl to be "bad", even though I do find myself having used those specific words. I meant to express that, I can make a strong argument for python being a better choice in the collaborative engineering space (specifically in the area of explicit readability and maintainability), and that it is therefore "better" than perl as a tool in this space. Subject to a heated debate during which my argument undergoes scrutiny, of course. I do have some *very* intense personal, aesthetic, and anecdotal reasons to want to never touch perl again, but I understand that those are only arguments for *me* to avoid perl, and I shouldn't try to convince anyone else that way. Those reasons tend to cause my rants to get a little rantier than they strictly need to. For which I apologize of my tone ever gets pushy.
You won't see large frameworks like Django and Pyramid switch to Py3k until the dependencies are all on py3k. This is starting to happen by the way. Virtualenv, Pip, Mako, Jinja2, psycopg2, Numpy are some of the popular packages that are now supporting python3.
You note that conversion to Py3k is no where near the point where 50% of packages have converted. It seems to me that the process is likely to begin slowly (as we are seeing) but as more and more packages are converted, we will see an exponential increase in conversion rate.
or just not packaged. pyqt has supported python3 for a long time but hasn't been packaged by ubuntu / debian. https://bugs.launchpad.net/ubuntu/+source/python-qt4/+bug/400826 http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=558389
And of course SQLAlchemy.
Pyramid has 2 students in GSOC working on porting Pyramid and its dependencies to Py3k. It takes time for higher profile code to transition because there are so many low-level frameworks that people don't necessarily use directly, but that are used in libraries that we have to wait to get ported... and that's happening more and more.
There **is** a [3to2](http://pypi.python.org/pypi/3to2/1.0).
I think the correct answer is if after 5 years we don't see the wall of shame go over 50% that's when we have a problem.
Django doesn't have dependencies
For complex projects with tons of dependencies it's almost impossible to go with a migration plan, without creating a fork for python 3.0. I'm surprised that there are not many projects taking the fork path.
The way you use *vars* is discouraged, since modifying the returned dictionary [leads to undefined behavior](http://docs.python.org/library/functions.html#vars). Why not declare a proper dictionary inside the class and use that? Or even better, why not make your class a subclass of *dict*?
The truth is that for many people the incompatible changes in Python 3 are not good enough (worth) for migrating the old Python 2.x code. What Guido needs to do to make people migrate is a Python version with a Hotspot compiler and without the stupid GIL. Then we can think about migrating. Before that meh. 
They do, they just incorporated it into their source. Example: json serializer.
I'd be worried about bugs concerning mutable values, given the shallow copy. Shouldn't the parent dicts be deep copied with copy.deepcopy? The copy could be optimized for efficiency with a custom [`__deepcopy__`](http://docs.python.org/library/copy.html#copy.deepcopy) method. 
I've started using [matplotlib-1.1.0.dev (Windows binary)](http://www.lfd.uci.edu/~gohlke/pythonlibs) in 3.2. So far so good, but I only do simple plotting.
IPv6, 10 years in the making, hardly any adoption either. Migration will come when there is an problem, there is a ton of legacy software and no 'really good' reason to migrate, google still uses 2.5 iirc.
The whole move over was slated to take 5 years at least. No worries here.
If it was deprecated as you say, then it would be a) marked as such, throwing a DeprecationWarning on use b) marked as such in the documentation c) and, if it was deprecated in Python 2.2, then it would have been removed in Python 2.4 as per normal Python deprecation rules (see PEP 4) But none of those has happened, so "look dude" you're wrong. I think you're confusing the UserList class exported by the collections module with the state of Python before 2.2 when builtin objects couldn't be inherited therefore there was a separate iterator type that could be. 
You don't need multiple inheritance, collections.UserList is an Iterable and includes (and exports access) to an instance of the builtin list type. If you want to use a class as an iterable, it has to do more than expose a builtin list type member - it needs the accessor methods of the iterable type itself, which is why you inherit from one. This then lets you use your custom iterable in list comprehensions etc. which expect an iterable operand. 
I think in your case the Pythonic method would be to use a generator to pop a person from the DB then perform whatever action using them, that way you don't have to first duplicate your database of names into memory in an iterable type. With past programming experience in other languages, I learned some bad habits which unfortunately are hard to shake off; for example the procedural way of thinking is "I'll get all my data, then I'll do stuff with it, then ..." and I've had to learn the hard way (operating on data sets that consume all ram &amp; swap then die with a SIGBUS if you take such an approach) to use the features of Python to do what I want better. So now I'm passing this experience on. 
"every web framework" you say? are you too fucking dumb to realize that wsgi is only for python based web frameworks, and until january it was only for python2.x web frameworks. big bang theory is an entertaining show.
If all of the values in the dict's are in lists or (preferably sets) you can change just one line. return [k for k, v in dic.iteritems() if val in v] 
You're thinking linearly; you should be thinking exponentially. All of a sudden things will seem to pick up and everything will be ok. 
The reason your current code doesn't work is because: 'man' == ['man', 'woman'] will return False. To get this working, you need to determine what type the value is and act accordingly. If it's a string, use ==, if it's a list, use 'in'. Example: http://codepad.org/iQfz13ZE **edit**: personally, I'd probably take an approach more like what fatalfred suggested, where you ensure all your values have the same type (it really makes things a lot cleaner). ---- *sidenote*: never use 'dict' as a variable name. In your sample code, you've overwritten the built-in dict() function. 
You're never guaranteed to get what you want (or need) back. If you're really interested in doing this, you're going to have to drop the one-liner as you'll simply need more logic. Maybe what you're really looking for is a better idea?
good to know about dict. thanks.
Edit: The semantics of `in` are different for strings vs lists, tuples, and sets: &gt;&gt;&gt; "woman" in "woman" True &gt;&gt;&gt; list("woman") ['w', 'o', 'm', 'a', 'n'] &gt;&gt;&gt; list("woman") in list("woman") False &gt;&gt;&gt; "man" in "woman" True With a string `in` acts like a substring test -- hence the need to use a separate equality test for string values. Thanks, sibsibsib. +1
If this can help: 'pythonic' is the contrary of 'java-esque' An example of java-esque code is Zope.
yes, but &gt;&gt;&gt; 'man' in 'woman' True
No worries at all, amigo. I totally got what you were saying. Who knows, in five years you might wake up and say, "Screw everything, I need a new language for this project." And hey, it might happen that the weird, powerful way that Perl continues to evolve might be a fit for that particular moment. Sometimes a bullet _really does_ hit another bullet :)
&gt; Many perl people are even still using perl 5 Must be on account Perl 5 being, you know, the stable release and Perl 6 not being anywhere near production-ready. The Perl and Python situations are not comparable.
Bad example. We're out of IPv4 addresses, and we're still not switching to IPv6.
This is exactly what I needed thanks. In the program i'm using it with all the values are lists.
Yes! Working multiprocessor support, either by dropping the GIL or by figuring out how to do process based MP in a sane and fast way. That would make the transition worth the pain. Or if they made Python not dog slow. Py3k drops support for a bunch of silly legacy cruft and needlessly changes a number of things that where already fine, like string formating and the print builtin. The only _big_ fix in Py3k is unicode strings vs. byte arrays. Unless you're doing web development, that's just not enough.
I can use it against Postgres without using psycopg2 or some other postgres driver? Cool. 
ok, thanks for the clarification
Do you find that you are significantly more productive in Python3?
That would be my thought too.
The shell is designed for command interfaces and using it for scripting is so-so. My point was that Perl 5 is more a "real" programming language while keeping many of the quick-dirty-hacks of shellscripts that are useful in that context. I don't actually use Perl 5 myself and don't like it much personally. Perl 6 unlike 5 is actually designed as a sensible language for generic programming and the expressive power that would make it neat for shell-like scripting is more a side-effect of the overall design. Today it is a toy but eventually it can be a thing of awe. As it stands Python is my go-to language at the moment. Your accusation of spreading of FUD is FUD itself.
According to [this guy](http://www.drbrett.ca/about) (some core Python developer or something), [Python 3 was right on schedule](http://sayspy.blogspot.com/2011/01/my-semi-regular-reminder-that-python-3.html) as of January.
Dunno about you, but I now only write Python 3 code and maintain 2 code. All the essential libraries I use are already ported, the only thing I'd really like is a decent crypto library.
As someone who's doing the exact same thing... no, python 3 is not _that_ much better than python 2. But it does have some really nice things that make you more productive in a few places, with statements, dictionary comprehensions, etc. EDIT: yeah, somee people have pointed out these have been backported to 2.6/2.7, but you still don't get real unicode support.
Use Arch. Python 3 is the default python on Arch now.
Uh huh... and the thousands of existing projects didn't need a way to upgrade I suppose.
The problem is: I have those in 2.7 as well: PythonWin 2.7.1 (r271:86832, Nov 27 2010, 18:30:46) [MSC v.1500 32 bit (Intel)] on win32. Portions Copyright 1994-2008 Mark Hammond - see 'Help/About PythonWin' for further copyright information. &gt;&gt;&gt; from threading import Lock &gt;&gt;&gt; with Lock() as l: ... x = {i : "%s"%i for i in range(10)} ... &gt;&gt;&gt; x {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'} &gt;&gt;&gt; 
those are just for backwards compatibility, the json in the stdlib is used when available. the problem is django wants to support redhat's long term enterprise distro, among others... which just recently put out a version with 2.6 as the supported python, which will last 5 years... there are still some distro's shipping 2.4 apparently?
I wish, but the GIL is inherent in the way Python is implemented with regards to external functions. It's not going away soon. But we can dream. And import multiprocessing.
Here's a slight change up that doesn't confine the value to string and list sequences. def find_key(dic, val): match = lambda v: ((val == v) or (not isinstance(v, basestring) and hasattr(v, "__iter__") and val in v)) return [k for k, v in dic.iteritems() if match(v)] 
As usual, downvoted. [Here's the non-scraped version uploaded by the author.](http://www.slideshare.net/ffunction/fabric-cuisine-and-watchdog-for-server-administration-in-python)
I think that makes it a good example. Even if it is a horrible scenario.
&lt;troll&gt; But Rakudo Star! And Parrot! &lt;/troll&gt;
you can have 2 and 3 at the same time? If I install both 2 and 3, then everything is going to be OK and no conflict?
You mean the new concurrent.futures API of 3.2 that handles both threading and multiprocessing with the same high level abstractions? http://docs.python.org/dev/library/concurrent.futures.html#module-concurrent.futures
nice addition. was too tired to think of that :)
I _love_ with statements, but they are available in Python 2.6. dict comprehensions are nice, but they only offer a tiny amount of syntax sugar - you can just use dict([clever list comprehension]). Or, if you actually like functional programming (I do), you can use dict(map(whatever)). 
&gt;Many perl people are even still using perl 5 And I will continue to do so even when Perl 6 comes out. The only way I would really move to Perl 6 is if Perl 5 stopped being improved upon. Even then I might move to something else besides Perl 6.
The context of this discussion makes "every web framework" to be "every Python web framework" by default. He shouldn't have to specify Python there. However, your second remark is right. They just nailed down the spec for WSGI under P3.
Why should I spend weeks if not months migrating my projects? What does that buy me, besides having to build and distribute Python alongside the product as most linux distros are not based on Python 3.x, spending time debugging and re- thousands of lines of existing reliable code ? Most libraries I use are still on 2.X, my code is tested and proven to work with 2.x. Yes it is a chicken and egg problem but I am the shipping an end product so there isn't even anyone expecting to use my code with 3 and me holding them up. So there is all this extra time and work I would have to invest, what do I get back, can anyone tell me? (I am not being facetious, serious question)
That looks like Twisted's deferreds just renamed to 'futures'. I would rather just use Twisted.
You can connect to a network socket from Python can't you ;-) 
well the most deployed version of rhel (5) is on 2.4 and people running rhel are not exactly the ones who are willing to jump to the latest and greatest, they want stability, so 2.4 is still here to stay for a while ( i am not happy about it).
&gt; I could always convert it with 2to3, but meh. It would be nice if they offered a 3to2 tool as well. I mean, if you have a library (or even a tool) that you want to work under both branches during the transition period, having only `2to3.py` kinda sends a wrong message and puts you into the wrong mindset. It means that your primary version remains 2.x and you don't feel any pressure to upgrade, while if you could switch to 3.x and constantly backport your code to 2.x, each time you wanted to use some 3.x feature but couldn't, you would think about dropping 2.x support.
No, twisted is a callback based, sequential library for efficient concurrency management of network resources. Concurrency is managed by using non-blocking async call to kernel resources for networking or filesystem IO. It does not do multi-threading or multi-processing.
&gt; No, twisted is a callback based, sequential library for efficient concurrency management of network resources. It still concurrency though -- you get an object that acts like a promise of a future result, it doesn't execute in parallel on the CPU but the concurrency is still present. And you can have multiple such objects hanging around at any point in time. And in twisted you can just launch internet.utils.getProcesValue() and get a deferred back that you can use to get the result of the process later, or you can do deferToThread() and get a deferred back. So still don't see how futures are not just renamed deferreds in principle. You are just saying how they are implemented but they are a concurrency abstraction, how they are implemented doesn't matter at that level.
Uh, how do you expect that to work out? You will definitely have conflicts. Arch Linux switched to 3 as default and now everyone has to use python2 or make a symlink. There's no way there'll just magically be no conflict.
What wrong message does it send...?
He means a conflict that would prohibit you from having both installed. You can certainly have both installed, you just need to know which one you use by default. 
If you have to use unicode, then yes, py3k is worth it. 
Which OS? On Fedora, RHEL and Ubuntu, I have no issue. They install in a different directory, I just call a different binary depending what version I write it for.
That you should stay with 2.x until everyone else switches to 3.x! Obviously, since it sends this message to almost everyone (except for the people who can switch to 3.x because they are the only users of their code, and the people who can afford maintaining two branches themselves), it poses a bit of a conundrum!
I did. &gt;...you would normally have to learn how to work with a framework as well. &gt;Have the freedom to make your site your own without having to learn another framework &gt;Python 3 Web Development Beginner's Guide shows you how to independently build your own web application that is easy to use, performs smoothly, and is themed to your taste – all without having to learn another web framework. &gt;A practical guide to building and customizing your own Python web application, without the restriction of a pre-defined framework &gt;Design your own framework that will make developing more of your own custom applications easy 
Indeed, this is the normal model for conversions. 
Could you elaborate? It looks like at least 2 to me (?)
For all the things mentioned, 2.7 does the same.
&gt; The only big fix in Py3k is unicode strings vs. byte arrays. And it seems rather botched on GNU libc platforms, where one character suddenly occupies four bytes.
But isn't it the case that Django doesn't even use WSGI?
If your dependencies haven't migrated yet, you can't migrate anyway. What you _can_ do, however, is to make sure your code can be migrated easily. Follow best practices and you'll be fine when the day comes.
But that still hasn't answered the question as to why should I migrate. What incentive to I have to make that investment of time and money. 3.X is not terribly faster than 2.X, it has some nice syntax improvements but otherwise it is just not worth. Not to say that 3.X is a bad release in general, it is just that 2.X is really good.
http://pypi.python.org/pypi/3to2
Thanks! Have you used it yourself? Why is it not included in the default distribution (or is it, for newer versions of 3.x)?
How do you know what's what he meant? He could have meant anything by "conflict", frankly...
I would have thought that having a 3to2 tool would have sent the wrong message; it encourages people to keep using 2.x, because even code written for 3.x can be converted back.
Because 2.x won't see any further progress. There's nothing after 2.7 -- it's a dead end. In the long run, 2.x will be limited to legacy software. There's nothing wrong with not migrating old internal code to new versions, of course. If it works and won't need changing, let it run. If you can afford to migrate, however, or have published the code as open source, it's a good idea to keep up with where the major development is shifting towards -- it will make future updates much easier.
Not all that code, only the code that was converted with 2to3, with some simple changes applied! It is impossible to write a 3to2 converter that can convert any 3.x code, it would require backporting the entire 3.x. So that, as I said, you could take your 2.x utility or library, convert it to 3.x and continue supporting only that -- as long as you don't use any of the 3.x features or standard library modules that can't be backported. And every time you'd like to use one, you'd be tempted to drop 2.x support.
No Djangi uses WSGI, it is the recommended deployment method and has been for awhile now.
Does it? What program are you using where a significant amount of machine memory is taken up by text strings? I've written a bunch of Python web apps, some of them rather complicated, often using heavyweight frameworks like Django, and I've never encountered a situation where plain old text in the program layer, regardless of encoding, was anything other than a tiny blip in the memory usage profile. If we where talking about Postgres, Squid or memcache, it would obviously be a different story, but we're not. In fact, my experience is the opposite; I've written my own command line shell, and it uses wchar_t strings internally, and the entire shell still only uses a few hundred kB at most. You do realize that wide character strings has the major advantage of significantly simpler code, resulting in fewer bugs. Not to mention that stuff like locale aware string comparisons become quicker. I honestly don't think that multibyte strings make sense for any other reason than backward compatibility. Just like I've never been convinced that the compressed pointers used on most 64-bit Java VMs are worth the programming effort.
the generic ":id" form of parameter is via the `text()` construct: e.execute(text('select * from table where id &lt; :id'), {'id': 2}) Session.execute(), being part of the ORM, applies text() to strings automatically, whereas engine.execute() does not (high level vs. as close to DBAPI as possible). Really the main issue is that the DBAPI spec itself thought it wise to have six varieties of bound parameter styles. IMHO there should be :named and ? only, and all DBAPIs should support both.
I just started with python, and I chose 3.0 but after running in to multiple problems not being able to find packages and dependencies to do what I want (as well as most code for python being written in 2.7 style) I downloaded 2.7 and have been using it ever sense. 3.0 just sits there...
Shalom, Pedro.
This is similar to asking why some people still use languages other than Python. Everyone has a choice. Python 3 already has rich support, but due to age Python 2.x just plain has more stuff. Is there a specific tool you are waiting on?
Put another way, why should my employer pay me to spend weeks if not months migrating my projects? Right now they have a product that they know works. It would cost them tens of thousands of dollars, and they would risk introducing regressions as we make changes in code that works just fine. The fact is I'm still targeting Python 2.4 because our product is deployed on RHEL5 and our clients don't want to use versions of Python that aren't officially supported. Right now the developers are pushing to switch to 2.7, and I'll count myself lucky if we get that. So far, the best answer you've gotten to your question is "2.X won't see any further development after 2.7," but that doesn't seem like a huge selling point to people who are hesitant even to move beyond 2.4.
Is the WSGI situation still dire?
You mean, like, 3to2? http://pypi.python.org/pypi/3to2/
It's the other way round. Having a 2to3 tool allows you to support Python 3 whilst doing your main development in Python 2 (and generating the Python 3 version from the Python 2 source). 3to2 allows you to switch your main development to Python 3, whilst still supporting Python 2 by automatic backporting. As many developers are going to want to support both Python 2 and 3 during this transitional phase having 3to2 is very valuable. Not that I've used it. :-) (For my own libraries I tend to support both Python 2 and 3 from a single source code base.)
Hmmm... at PyCon and around the time of the release of Python 3, people (Guido included IIRC) were saying about five years. That still only puts us half way through.
i know my second remark is correct. that's why I said it. :)
It's two 'layers'. easy_install and pip essentially provide the same functionality. I may be pulling this out my ass, but I think pip is the successor of easy_install. Easy_install just comes with the version of python provided by homebrew
Yes. I do everything I want to do in Python 2.x (2.6 currently) and I simply don't have a single reason to go the 3.x route. That said, I'm effectively an end-user of the programming language and have nothing to do with any amount of development of shared code, so all the more reason I'm going to sit tight, at least until the modules I rely on stop developing in 2.x and use 3.x exclusively.
I would argue that this shouldn't be a lambda, though. It doesn't exactly make it any more readable, plus you're assigning it to a variable, anyway: def match(v): if val == v: return True return (not isinstance(v, basestring) and hasattr(v, '__iter__') and val in v) This could probably be improved, but it makes the two distinct conditions clearer (either an exact match or a successful equivalence check). Of course the names are a bit too generic anyway.
dict can also take a generator, such as `dict((x, x**2) for x in xrange(1 &lt;&lt; 22))`. The generator results in lower memory overhead, even if you run `gc.collect()` afterward. 
And this, kids, is why we salt before hashing.
http://docs.python.org/dev/whatsnew/2.7.html#pep-372-adding-an-ordered-dictionary-to-collections
:o
there was a time when you had to write software for 16bit Visual Basic, even though 32bit Visual Basic was the new hotness. It was a very very dark time, and many people lost their lives needlessly. 
See my comment on the /r/programming article for the PyPy performance numbers: http://www.reddit.com/r/programming/comments/hh8uj/a_beginners_guide_to_using_python_for_performance/c1vgqdo
that's lovely... will def use for some of my own tools... should be a part of argparse in standard lib
Good idea. Now let's turn it up to crazy: def find_key(dic, val): def match(v): def match_iter(v): return (not isinstance(v, basestring) and hasattr(v, '__iter__') and val in v) return (val == v) or match_iter(v) return [k for k, v in dic.iteritems() if match(v)] 
I prefer to bitter my passwords first.
Thank you so much for posting this, I am about to get into SQLAlchemy to write a few things I need that needs to run as a service, and this tutorial really helped.
You could implement a rigid solution with the `find`, `split`, and `replace` methods of strings, or you could use a [regular expression](http://docs.python.org/library/re.html). 
That's a very quick start article which covers 0.001% of SQLAlchemy. &amp;#3232;\_&amp;#3232; 
I have found most of the stuff i use or for my kids(Pygame) is on 2.6. I am a heavy user of numpy and scipy. 
Cool! See also [this video](http://blip.tv/pycon-us-videos-2009-2010-2011/pycon-2011-packaging-from-distutils-to-distutils2-4898961) from PyCon 2011: "Packaging, from Distutils to Distutils2"
But I don't see anything new coming for Python 3... Yes, it does have one or two "syntactic niceties" but nothing more. Python 3 doesn't really provide any incentive to switch. As far as I'm concerned, I'll use whatever PyPy provides (and they seem to be sticking to 2.x for now). That brings a lot of speed to python, so I do gain by using it.
In the case that the passwords are using salts, how long does it take to compute the hash databases? You could organize this into a python package and possibly submit it to pypi. You only support hashlib. You check into py-bcrypt and crypt.
In my (very limited) experience, this seems right in the wheelhouse of regular expressions. That would certainly be the cleanest way to run through and pick out those values of different lengths, particularly if the format in which they're presented is always the same. I learned from the [Google Python Class](http://www.youtube.com/watch?v=kWyoYtvJpe4) 
I'm a fan of the `pystring` style myself (I suspect that's why we ended up with 6 in the first place...) :)
Sarcasm fail? I didn't introduce any new names, I just extracted one of the conditions into an `if`/`return` shortcut and turned the named lambda into a proper function. The point is that if your function deserves having a name (and isn't so trivial it can be replaced by something from the `operators` module or such), it's pointless to use a lambda instead as you'll still have exactly the same overhead sans readability. Lambdas are just anonymous functions that consist of a single expression. In this case the function wasn't intended to be anonymous and the expression was a compound containing a complex condition. Moving the named lambdas into proper functions just makes the overhead more visible, it doesn't add any. You can play crazy with lambdas too: match_iter = lambda v, val: (not isinstance(v, basestring) and hasattr(v, '__iter__') and val in v) match = lambda v, val: (val == v) or match_iter(v, val) find_key = lambda dic, val: [k for k, v in dic.iteritems() if match(v, val)] This should be functionally equivalent, except the two lambdas pollute the global namespace because there's no way to define a name inside a lambda. So here's the clean version: find_key = lambda dic, val: [ k for k, v in dic.iteritems() if (val == v) or (not isinstance(v, basestring) and hasattr(v, '__iter__') and val in v)] Pure bliss, isn't it?
Thanks to further fragment the python distribution tools. Seriously. What the hell is going on here? Could you please explain why we have we have distutils and setuptools at the moment and what will happen to them? What about easy_install and pip? Same here. To quote the Zen of Python: "There should be one-- and preferably only one --obvious way to do it."
Sounds like ```packaging``` will soon be the only one that matters.
Haha awesome - love the presenters sense of humor!
I said it was a good idea. I'm sorry, but I thought it clear that I was joking and that my alteration was "crazy". I never insult people, never play politics or games, never make back-handed comments, almost always give people my honest opinion when asked, and rarely use sarcasm, especially online. Before getting offended, please ask for clarification. I wouldn't hurt a flea. 
Thanks for the help guys, if anyone is curious here's the regex I finally got to work: define\('(?P&lt;name&gt;[A-Za-z_0-9]+)',\s?'(?P&lt;value&gt;[^\']+)'\);
Thanks for the help guys, if anyone is curious here's the regex I finally got to work: define\('(?P&lt;name&gt;[A-Za-z_0-9]+)',\s?'(?P&lt;value&gt;[^\']+)'\);
It's python 3 only. I think plac has more abilities but also a more condensed and complicated syntax. It seems less magical. For example it uses annotations for a lot of things, your function signature can easily become overwhelmed with settings. e.g... in plac: # plac example8_.py def main(dsn, command: ("SQL query", 'option')='select * from table'): print('executing %r on %s' % (command, dsn)) if __name__ == '__main__': import plac; plac.call(main) -------------------------- in pyopt: import pyopt expose = pyopt.Exposer() @expose.mixed def main(dsn, command='select * from table'): '''command - sql query''' print('executing %r on %s' % (command, dsn)) if __name__ == '__main__': expose.run() 
&gt; What program are you using where a significant amount of machine memory is taken up by text strings? ETL-like work. If the input data can contain anomalies, you might end up with individual lines (or other segments) that are quite large. Streaming within lines is theoretically possible, but significantly reduces performance for the standard, small case. It's also difficult to use modules such as `re` when streaming. &gt; Just like I've never been convinced that the compressed pointers used on most 64-bit Java VMs are worth the programming effort. They are visible in benchmarks.
There's the Python Cookbook [recipe set 2](http://code.activestate.com/recipes/sets/2-python-cookbook-edition-2), or all [3317 recipes](http://code.activestate.com/recipes/langs/python/popular) on ActiveState's site.
It should be obvious by now that distutils and setuptools are not the way to do it. The Zen of Python doesn't say we have to get it right on the first try.
The stuff from activestate seems out of date. For instance the top item uses a module that's deprecated.
I clicked on [ruby.py](https://github.com/danielfm/pyruby/blob/master/src/ruby.py) with my [fingers crossed](http://www.youtube.com/watch?v=jyaLZHiJJnE). Thankfully the universe didn't end. 
&gt;◦for end-users Python gains a pysetup script you can use to install, uninstall projects, browse installed projects, browse PyPI, and many other things. Oh hell yes
The Python Cookbook, 2e was published in 2005, so the recipes are at least 6 years old. On the other hand, the general listing sorted by popularity has a lot of recent recipes.
I have a different suggestion: Print [PEP 8](http://www.python.org/dev/peps/pep-0008/). Print [PEP 20](http://www.python.org/dev/peps/pep-0020/). Read them SLOWLY then read them AGAIN. Read PEP 20 and meditate on each line asking it why does it exists. Next, write some code and 1. Assume it sucks. 2. Figure out why it sucks. (read the PEPs again if you need some reminding). 3 Make it suck less. The end product will show you the end product but it will do very little to teach you how to get to the end product. Just like cooking. You can look and taste things produced by great cooks but any attempt to imitate the outer final object is a recipe in frustration. Make peace in your heart and listen to the Will of the Code. It will teach you the discipline you need. 
What do you mean "even if you run gc.collect()"? Comprehended list would consume memory only during dictionary construction, and will be freed immediately afterwards (by means of refcounting, even without gc.collect). I'm not denying that generator instead of list will take less memory in the process. And also there is a minor difference in that list comprehension "leaks" loop variable to the outer scope.
I was curious about the black magic required to implement something like this in pure python... till I saw the source code. Well done sir!
Read and understand the standard Python modules. They are tried and tested Python code. Write a walk through for some simple modules.
*slowclap
I concur. This guy is a pretty decent poster 'round here.
You are right, but it's really hard for a rookie to feal the Force :) Seriously, you understand more those pep with practice and reading good code is a good opion to progress. Just read what you like. You have to read Web frameworks if you want to use them in an advanced way correctly, not only the doc. It's true for every project, and if you understand sqlalchemy, you are a jedi :))
I'm sure kingkilr will write a PyPy impl next week just to show off to the rest of us.
Did you try it? The leaked variable here is just an int from xrange, but the generator version ended up using about half as much memory. This was on 32-bit Windows Python 2.7.1. I ran `gc.collect` just to make sure there wasn't anything pending collection. Obviously something is being left around -- maybe as a cache of some kind. I've had a similar issue with NumPy's fft internally caching results. 
Setuptools was built on a top of Distutils, with its own standard for metadata, that was partially backported in distutils. That's how the fragmentation started. The fragmentation of distribution tools is something we all suffer from, and the only way to fix it is to have all tools rely on the same standards. That why we wrote PEPs and we provided reference implementation in packaging. Our hope is that eventually all third-party tools will use the unified standard. Distribute will and Pip should follow. Last, I don't see the problem in having numerous tools in this area, as long as they are inter-operable. See PEP 386, 345, 376 as the "wsgi standard for packaging"
I know is hard to feel the Force and that is precisely the way it should be. Doing it in spite of being hard trains you. You cannot build muscle lifting twigs. Also, by all means, we all should read good code, I'm not saying that doesn't help. I'm merely pointing to another option. 
I always found going through the standard library pretty interesting. Haha, the [BeautifulSoup 3.x code](http://www.crummy.com/software/BeautifulSoup/) made me laugh — came away feeling Leonard really hated parsing HTML afterwards. Also, someone else pointed out that you should check out Github. It's definitely a great place to read through interesting projects and code.
1 upvote and 4 downvotes. Wobsta raises a very legitimate concern imo... What's up[?](http://www.reddit.com/help/reddiquette)
Actually, last Friday in Pygrunn there was a talk about embedding python into ruby and vice versa (and this time, for real): http://pygrunn-paylogic.blogspot.com/2011/04/added-speaker-emil-loer-embeddng-python.html It was about creating a python extension in C that would link with the ruby libraries and call the ruby code (and vice versa). It would be scary to put that code in production environments, but it worked and it was extremely cool. I haven't found the code online, but if you're interested you can probably contact the guy.
[Peter Norvig's](http://norvig.com/) code is very nice.
I don’t like this joke. First, it is not funny. Second, it wastes a good PyPI name that can probably be chosen by a serious Ruby implementation written in Python which is not joke in the future. I think [goto][] can be an exemplary joke — it is funny (probably because it is useless) but still works. Sorry, I am a boring man. [goto]: http://entrian.com/goto/
Anyone looked at the code for IDLE? How is that?
Oh, thank you, I'll try to put this in. 
It indeed is, but still I've got few responses thanking for it - SQLAlchemy's own tutorial provides long and extensive introduction and if you just want to jump in, it can be a bit tiresome to follow.
I just tried, and I get somewhat surprising results. Final memory usage for both statements is the same (although list-based version peaks to about twice that memory during execution, as I expected). But! When I delete this dictionary, only fraction of this memory is freed with refcounting, and the remaining awaits for gc call. I thought dictionary without cyclic references is fully amenable to refcounting gc. Apparently, I was wrong. statement mem usage after that statement &gt;&gt;&gt; N = 10**7 2.8MB &gt;&gt;&gt; x = dict([(i, i) for i in xrange(N)]) 329MB &gt;&gt;&gt; gc.collect() 329MB 0 &gt;&gt;&gt; del x 132MB &gt;&gt;&gt; gc.collect() 7MB 0 &gt;&gt;&gt; x = dict((i, i) for i in xrange(N)) 322MB &gt;&gt;&gt; gc.collect() 322MB 0 &gt;&gt;&gt; del x 125MB &gt;&gt;&gt; gc.collect() 7MB 0 Python 2.7.1 (r271:86832, Nov 27 2010, 18:30:46) [MSC v.1500 32 bit (Intel)] on win32 
Don't for get that there are 2 independent implementations of setuptools as well just to further complicate matters. I'm not saying that it's bad there's so many packaging systems for Python, it's just confusing and it would be great if this whole process was being documented better. What are people meant to actually use these days? What's the transition path between different systems? What is supported by what distro?
By soon I guess you mean in 2-3 years while we wait for distros to re-do their entire Python module packaging toolchain again.
I have similar results for the generator case, but my list results are very different: Python 2.7.1 (r271:86832, Nov 27 2010, 18:30:46) [MSC v.1500 32 bit (Intel)] on win32 &gt;&gt;&gt; import gc &gt;&gt;&gt; N = 10**7 &gt;&gt;&gt; x = dict([(i, i) for i in xrange(N)]) &gt;&gt;&gt; #Working Set: 428,452 KiB, Peak Working Set: 867,408 KiB &gt;&gt;&gt; gc.collect() &gt;&gt;&gt; #No change &gt;&gt;&gt; del x &gt;&gt;&gt; #Working Set: 231,496 KiB &gt;&gt;&gt; gc.collect() &gt;&gt;&gt; #Working Set: 59,520 KiB &gt;&gt;&gt; #Restart &gt;&gt;&gt; import gc &gt;&gt;&gt; N = 10**7 &gt;&gt;&gt; x = dict((i, i) for i in xrange(N)) &gt;&gt;&gt; #Working Set: 325,428 KiB, Peak Working Set: 371,332 KiB &gt;&gt;&gt; gc.collect() &gt;&gt;&gt; #No change &gt;&gt;&gt; del x &gt;&gt;&gt; #Working Set: 128,512 KiB &gt;&gt;&gt; gc.collect() &gt;&gt;&gt; #Working Set: 18,232 KiB 
We'd like to focus on documenting things better next. That's: consolidating all the existing doc and provide migrations paths. Note that packaging already provide wizards to migrate your distutils or setuptools-based projects into packaging projects.
Python packaging noob here. Can someone please explain/correct me on what these various things do? * distutils - build packages from python source code, included in stdlib * setuptools - enhancements on distutils (especially easy_install), available from PyPI * pip - package installer/maintenance tool, available from PyPI * virtualenv - somewhat orthogonal to these tools, helps to maintain separate python environments for development/build stability * distutils2 (now renamed to packaging) - enhancements to setuptools, including standardisation, and clear documentation of internals, to be included in Python 3.3 stdlib
I doubt distros will use packaging like they didn't use distutils. They use their own toolchains.
So what would you suggest instead? The *reason* Python packaging is so fragmented is that distutils was so poor. packaging is the standard library (i.e. Python) fix for that. The intention is to *end* the fragmentation by providing a good solution in the standard library.
everything is right except: * pip: it's a easy_install replacement with extra features, like uninstallation (setuptools does not provides this) * distutils2 does not enhance setuptools but distutils. It's originated from the distutils code base and does not use setuptools code, except maybe in the pypi crawler in some places, but that's minor. Although, some of this enhancements are clearly inspired from setuptools. In particular project dependencies. 
well, you could do that by setting your own engoding and writing # -*- coding: pyruby -*- but that's hard, i guess)
this link http://www.markus-gattol.name/ws/python.html#package_distribute_install has a few infographics that might help you understand things quicker/better
Last present in [2.3](http://docs.python.org/release/2.3.5/lib/module-rotor.html).
Whelp, you _did_ turn it up to crazy. Sorry if it wasn't intended as a _reductio ad absurdum_.
true, and part of the plan is to make their lives easier. For instance, packaging allows a fine-grained description of data files with the possibility of relocating them without breaking the code
Oh, I completely agree. When I write out pseudocode for large Perl projects, it looks like a mixture of Python and Javascript. That said, pseudocode is meant to be expressive. Perl is meant to be succinct.
Agreed. My favourite: http://norvig.com/spell-correct.html
The downvotes are probably ok. There are more important things than reddit votes to me. Actually I don't care at all and I didn't wrote a polite comment either to begin with. But as you wrote, my concern might be quite valid. Why do we need another new system. Couldn't we try to evolve one of the current solutions to become better? Couldn't we merge at least one of the existing solutions right from the beginning. We do not even have a distutils2 in the end, but a new, third solution. What a mess. It is probably right to get rid of the old stuff, but why couldn't it start from an existing solution without breaking everything. Probably we don't want setup.py anymore. I don't know. Maybe. But why start something completely separate. (At least form the end users point this looks like something completely separate now. Similar separate than diskutils and setuptools at least.)
Well, and that is my point. Why couldn't we start to improve an existing solution instead of getting something completely separate. This was a failure for setuptools with respect to the existing distutils to my eyes. Even if people like to have their one project and don't want to work together (or whatever reason it was, that we got separate solutions), for something that crucial like a packaging, a joint solution with a BDFL setting the direction would have been great from the very beginning. Maybe we'll have a final solution in a few year (maybe based on packaging). Ok. Still, I don't like the fragmentation in the first.
I remember reading about enhancements proposals to pip, but I think they were rejected - is that why we have pysetup, and does it effectively replace pip?
[pypants](http://pypants.org/) have some samples.
I can't answer for the rejected stuff, but: There's some overlap, so we work closely with the pip team to make sure everything progress smoothly. But in any case, pip will still be relevant as it provides advance feature pysetup won't (like freeze reqs etc). At the end, it's all about inter-operability: what pysetup installs, pip must see, and vice-versa 
1 Simple, clear, elegant. 2 Uses OO and Functional programming as needed, picking the right tool for the job every time. 3 Uses Python for what it is best at: If you needed to optimize for speed, you would have written it in Python, looked at the resulting C code, and optimized that instead.
We tried to make distutils evolve but that turned out to be impossible without breaking stuff. I worked 2 years in distutils, and eventually reverted all changes to continue in a new namespace. This project is impossible to make evolve smoothly, trust me. It's patched in setuptools for instance, in some internal parts. This is historical: 5 years with almost no maintenance has rotten distutils to the bones. and having it in the sdtlib has made any change very very painful because all APIs are public and used out there, 
&gt; You cannot build muscle lifting twigs. Actually, as long as you continue lifting until you feel tired in your muscles, it will be just as effective as lifting heavier weights. &lt;/ot&gt;
I've always loved the overall design of [CherryPy](http://www.cherrypy.org/).
why are you talking like a Jedi ??? oh i see....
if you want to learn about wxpython....
Could you go into more detail; comparison to plac is the most important thing to me. Also, maybe comparison to baker and clint?
In weight training it is hard to feel that you're doing things right until you've built up some muscle. Until then it's best to rely on a spotter or trainer for guidance.
I'm humbled every time I read his essays. 
https://github.com/facebook/tornado/ Clear, concise, and well-documented.
I can thoroughly recommend this - it is very very good code. Also in terms of web frameworks you should check out Flask
&gt;Note that packaging already provide wizards to migrate your distutils or setuptools-based projects into packaging projects. That is a good feature. :)
Are you just trying to figure out beautiful soup or are you really trying to get information from the opencongress.org site? They seem to have a pretty open policy on code and data: http://www.opencongress.org/api If you are just trying to figure out beautifulsoup, maybe this will help: ... for cronie in person('li',{'class' : 'cronie'}): print (cronie).string ... Also, repr() and help() are your friends if you are trying to figure out how to access things if you are unsure what they are. Just wrap the variable in question with help() or repr() and run your code again. repr() displays a string representation of the variable and help() will display information about the methods or data stored in the variable. You may want to bring up a python interpretor interactively to figure out more about BeautifulSoup. For example: knappogue:~$ python Python 2.6.6 (r266:84292, Apr 20 2011, 09:34:38) [GCC 4.5.2] on linux2 Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; import BeautifulSoup &gt;&gt;&gt; help(BeautifulSoup.Tag) Hope this helps! *edited for my bad engrish.
just in case: done
I would just like to thank you tarek for stepping up and figurheading this. It's been a long process and you've taken a lot of flak for it but things are finally looking up for python packaging in general.
Thanks I appreciate this comment. What makes me very happy is that we gained a fair amount of contributors this year, so I can eventually do other stuff than packaging at some point ;)
There is also a good write-up of the code here http://golubenco.org/?p=16
[Code Like a Pythonista: Idiomatic Python](http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html)
As well as .string, take a look at .renderContents(). Wholeheartedly echo DaGoodBoy on using an interactive session (would recommend ipython) to work out what is going wrong. I think your problem is that some of the reps do not have a crony class entry, Eni Faleomavaega for example, which may be the cause of the NoneType error. Wrap it up in a conditional (as below, no crony bro) and you should be fine. Personally, I like to use what BS calls a soupstrainer to just select the chunk of html I need to work on further, so I'd do something like this: from BeautifulSoup import BeautifulSoup import httplib2 http = httplib2.Http() response, content = http.request('http://www.opencongress.org/people/representatives/') soup = BeautifulSoup(content) persons = soup.findAll(attrs={"class" : "person"}) for person in persons: name = person.find(attrs={"class" : "name"}) name = name.find(text=True).strip() # loses all the \t and \n cronyism = person.find(attrs={"class": "cronie"}) if cronyism: crony_record = cronyism.renderContents() else: crony_record = 'No Crony Bro' print name, crony_record This outputs Rep. Donald Young [R, AK-0] Votes 89% with Party Rep. Robert Aderholt [R, AL-4] Votes 96% with Party Rep. Spencer Bachus [R, AL-6] Votes 95% with Party Rep. Jo Bonner [R, AL-1] Votes 94% with Party etc. To get your exact final desired format, just use some re on the strings returned above, e.g. crony_pattern = re.compile('Votes (?P&lt;crony_percent&gt;\d{2}%) with Party') 
EDIT: on looking at the output of the above, it seems that all the folks entitled Rep. or Sen. always have a crony record, but the folks entitled Del. or Res. Comm. never do. Do you need the Del. or Res. Comm. folks in your result? If not you could use: from BeautifulSoup import BeautifulSoup import httplib2 import re http = httplib2.Http() response, content = http.request('http://www.opencongress.org/people/representatives/') soup = BeautifulSoup(content) crony_pattern = re.compile('Votes (?P&lt;crony_percent&gt;\d{2,3}%) with Party') persons = soup.findAll(attrs={"class" : "person"}) for person in persons: name = person.find(attrs={"class" : "name"}) name = name.find(text=True).strip() # loses all the \t and \n if name.startswith('Rep. ') or name.startswith('Sen. '): cronyism = person.find(attrs={"class": "cronie"}).renderContents() match = crony_pattern.search(cronyism) crony_percent = match.group('crony_percent') print name, crony_percent Note, the slight change to the regex \d{2,3}...turns out there was at least one represenative with a 100% voting record, meaning the \d had to extend to 3 possible chars. The above gives output like Rep. Donald Young [R, AK-0] 89% Rep. Robert Aderholt [R, AL-4] 96% Rep. Spencer Bachus [R, AL-6] 95% Rep. Jo Bonner [R, AL-1] 94% Rep. Mo Brooks [R, AL-5] 96% etc.
BTW... this is not a new feature. web2py had this feature since 2009. Just the example script was missing but the code was in the book. Actual new features in trunk, instead, include: digitally signed ajax calls, every app includes a a CAS consumer and a CAS provider. This will make building distributed applications even easier. Will post as they become stable in one week or so. 
This idea stood out for me: "Professionals but not professional programmers" I see this everyday in my working with Python. A group of non-programmers but capable users throw a "prototype" together in python and get it up and, barely, running. Then they hand it over to real programmers who are able to easily go back, refactor, stablize, document, and clean it up so that it becomes a real product. To me, this seems easier in Python. It fits right in that magic spot where it is just easy enough for technically minded people to rough out what they want but structured just enough that it can be easily "fixed up" by "professional programmers."
I haven't used plac, clint or baker, so I can only tell you by what I see from a very short glance. * clint is completely different, it helps you do all sorts of things to the command line like indenting, coloring and storing configuration. * baker is similar to pyopt but it's for python 2.6 so it doesn't have the annotations goodness, though it does support adding the help for a parameter in annotations for python 3. I think I like the pyopt API better, but I'm not objective.
[My reaction.](http://i.imgur.com/gYDpv.png)
Good, that still works. 
I built a very large multi-tenant security data warehouse at a fortune 100 company - in which we pulled in ids, fw, vscan, av, and other data and then supported a wide variety of reports, scoring and analytics. The initial front-end was PHP, the database was DB2 and the ETL (or back-end data integration) was all Python. Except for myself, the entire initial team was composed of security analysts that had to learn PHP, Python and DB2. The solution proved itself, and then got an infusion of cash and full-time developers that took the code and tightened it up. The backend processes about 200 million rows a day with a lot of very hefty transformations and is all python.
Hey, I'm the one that "created" PyRuby. It's obviously not the kind of joke everybody enjoys but, regarding the PyPI name, if anyone needs it for serious purposes, I'd be more than happy to let it go. :-)
https://github.com/dag/attest is a unit testing library, the code is extremely elegant.
People still cared what ESR said in 2000?
This is what got me interested in Python.
Yes indeed. Part of my experiment was to see how easy it would become to attack hashes that have no salt. But you are right I probably should have mentioned this in the text. Any sane password hash storage won't store the hashes without salt, like I am doing in the code.
The 600,000 hashes without salt are generated in under 15 seconds on my computer. Adding a salt will dramatically increase this time and storage requirements: a 8-bit salt would make it take about an hour and a 16-bit salt about 273 hours. And the database file would not fit on disk anymore. My code is naive in the sense that it 'assumes' the hashes are stored without salt, something that should never occur in real code... (on the other hand, it does show what happens when a system is foolish enough not to use salts...) Thanks for the headsup about bcrypt, I have glanced at it and will study it further. As this code was really an experiment, I hadn't planned on making it a proper module, but maybe I can polish it a bit and publish it on Pypi indeed. Thanks for your comments!
You realise that methods are called by name, right? So if you want the framework to call your method, you have give it the name that the framework is expecting. If you really want, you can write: def move_mouse(self, event): print "Mouse moved", event.x(), event.y() ... moveMouseEvent = move_mouse but it would be much easier to just write in the style of the framework you're using. If you really, really want you can override `event()` to call your `move_mouse()` method instead of `moveMouseEvent()` but if the style of the framework offends you that much - pick a different framework.
&gt; You realise that methods are called by name, right? No, I didn't know that was the only way they could be called. I've actually just started using graphical toolkits last week so I'm not too experienced. &gt; if the style of the framework offends you that much - pick a different framework. It's not that i offends me, I was just wondering if there was a way to redefine the names. Either way I would've left my code the way it is, since whatever alternative I find would be unnecessary and harder to read.
+1. If the framework needs it to be named a certain way, then use that name. If you want your own alias to the same function, then make an alias (as in `moveMouseEvent = move_mouse`)
Frankly that is also missing the fact that modern Perl is quite different now, in 2011, than when that was written. I love Python and Perl, but ever since the "fork" in Python from 2-&gt;3 and the mixed library support I've shied away from it. Once Python 3 is the defacto standard and primary environment for the large majority of libraries I'll switch to it, but other languages currently offer similar features without worrying about whether a library I'll need will be available. (Disclaimer: I do some pretty oddjob stuff with the code I write, so I do require many libraries, many of which are not exactly the most frequently updated).
Oh, you're acting like a little bitch. Python has not "forked". Python-3 has a five year window. We are actually doing rather well in transitioning. And, you can still use all the libraries you used to in Python 2.
what the fuck? I'm sorry, in this context I thought it was safe to assume I was talking about the myriad of Python web frameworks and not those written in any other language. Thanks for your well-written, considered and reasonable response. You really help keep the tone up around here.
Hrmf, the parts I scanned used cherrypy for the example code. Interesting dilemma.
=D thanks 
why is it not safe to assume that I don't know what wsgi is and then give your fucking smart ass response? i'm not responsible for the quality around here. I heard that was you, and quite frankly you suck at your job. 
No, its good. Python is always seen as a 'slow' language (although that isn't always true). The fact that python is so good at doing this means that salts really are important.
I'm glad it's well known on reddit that Peter Norvig writes really nice Python.
Really cool experiment, thanks for that.
I will fully admit that I feel I've been "broken", in that I now compare all other languages to python (made myself not capitalize it!), rather than weighing them on their own merits. I made the same mistake learning esperanto. Now every other language I try to learn is so painfully *organic* as opposed to "engineered to be easy". Every bit of idiomatic nonsense, every irregular verb, makes me face palm. But I would not want to live in a world without people speaking Russian in it, for example.
It was only after 9-11-01 that he exposed himself as a bigot/fascist hater.
Sure, but before that he was still a talentless shrill hack with a gargantuan inflated ego.
I hope he writes better Python than Perl. The only Perl code I've seen that was written by esr looked like he was trying to program in C. 
:-)
&gt;...since whatever alternative I find would be unnecessary and harder to read. Indeed. If someone is reading your code, they're going to expect the standard function names, and they'll curse your name if they find out you've remapped all the defaults. Not that the standard functions won't still be there, but that would make for some nasty, nasty code.
Short, but I think it's pretty good: [Software Carpentry: Using a Debugger](http://software-carpentry.org/4_0/python/debugger/).
I recommend you to read [Werkzeug][]. (Read [PEP 333][] first if you don’t know WSGI.) It consists of several independent modules (I think it makes it easily understandable), and there are well-written documentation also. [Werkzeug]: http://werkzeug.pocoo.org/] [PEP 333]: http://www.python.org/dev/peps/pep-0333/
I didn't downvote you, but I wanted to explain why (IMO) you are getting downvoted. Your question is phrased in a way that is needlessly hostile, and when we get prominent people from the Python community to come in and answer questions, people want to make sure we come across as a reasonable and amicable bunch, so that we continue to be the kind of place that invites casual participation from interest{ing|ed} people from across the interwebs. That's not to say that you should lobotomize yourself and not be critical, but you'll actually be more persuasive and stand a better chance of having your questions answered if you package (heh) your comments in a slightly more generous way.
So let me piggyback a slightly off-topic request on the occasion of you bringing up contributors. Would you do an AMA or write something short to give some tips for developers who want to get involved? It's a bit overwhelming sometimes, so it's useful to hear personal anecdotes about how you got involved and what things made it easier to contribute.
I enjoy reading his blog just because his ego is so inflated. A recent favourite is when he [pulls social rank](http://esr.ibiblio.org/?p=3163) at a fantasy convention. This guy is so cool he can join the vikings because he is esr.
\&gt;&gt;&gt; from BeautifulSoup import RobustInsanelyWackAssHTMLParser
The `convolve` function has a mode parameter that can be set to grab the valid range of the convolution, i.e. `[N-1: len(data)]`: import numpy as np N = 10 w = np.ones(N) / N data = [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5] data_ma = np.convolve(data, w, mode='valid') Instead of using the `convolve` function, you can use a generator to sum over the sliding window (first pad the data with zeros to implement linear convolution instead of circular convolution): pdata = np.hstack([data, np.zeros(N-1)]) data_ma = np.fromiter(iter=(sum(pdata[np.arange(i - N, i)]) for i in xrange(1, len(pdata) + 1)), dtype=np.float64) / N If you have a large real-valued data set, use rfft to speed things along. Again, you have to pad with zeros if you don't want circular convolution: from numpy.fft import rfft, irfft L = len(data) + N - 1 M = 1 &lt;&lt; int(np.log2(L).round()) data_ma = irfft(rfft(data, M) * rfft(w, M)) data_ma_valid = data_ma[N-1: len(data)] 
I made a post about it [here](https://pythonadventures.wordpress.com/2011/04/24/debugging-in-python/), you'll find there some useful links. It's text, not a video though.
Use Python 2.7 till then?
Can there truly be a "Perl way" with TMTOWTDI? Isn't part of the point of Perl that you can program any problem in the way you prefer to think about it? IMO, that's part of the problem with it too, but I digress.
I'm not going to defend his flaws, but I do wish folks would just acknowledge his contributions and move on. We all have our issues and have to play the hand we're dealt, and it wouldn't hurt to just take the higher ground and move on. 
This may be true to an extent, but there *is* (as with all languages) a distinction between clean code and ugly code. I work with perl often enough to say that I know some people who write absolutely beautiful perl. Perl just makes it very easy to write very ugly code, and even in the best of cases you'll probably still need to run through a reference to understand it if you've never seen code like it before (I can't impartially say that the same isn't true with Python; *I* feel like it looks obvious to anyone on first glance, but I also learned Python long before I started reading other people's Python). So there's more than one way to do it, but the simplest way is still probably the best. And for the love of all that is good in the world, if you ever write perl code, use strict and use warnings. Also, ESR is the reason I started programming with Python, in that I learned Python as my first language because I read ESR's famous How To Be A Hacker article which recommended that. No regrets here :)
I wasn't aware... Any source/link?
Not sure if anyone can answer this question, but are there any API services for stock ticker data? More specifically, I need PINK and OTC stock dat - I want to build my own stock screener.
Indeed. I read itthrough just to see whether it was the same article. It was.
[related](http://www.thc.org/root/phun/unmaintain.html)
What happened after 9/11? I've shied away from keeping up with what ESR was doing a long time before that.
&gt; I work with perl often enough to say that I know some people who write absolutely beautiful perl... Perl just makes it very easy to write very ugly code I don't think anyone disputes those ideas; it's just the infrequency of the latter vs. the former that keep people like me away from it.
Google's finance API allows you to create portfolios that can be managed and queried with the gdata Python client. But it's a bit much if all you want is a random stock quote. Here's an alternative Google API that doesn't require portfolios or a Google account: import urllib from xml.etree.ElementTree import parse STOCK_URL = "http://www.google.com/ig/api?stock={0}" def get_quote(symbol): url = STOCK_URL.format(symbol) xml = urllib.urlopen(url) tree = parse(xml).getroot().find("finance") stock_data = {} for field in tree.iter(): data = field.get('data') if data: stock_data[field.tag] = data xml.close() return stock_data print(get_quote("IBM")) 
Well holy shit.
http://armedndangerous.blogspot.com/2002_10_13_armedndangerous_archive.html
http://armedndangerous.blogspot.com/2002_10_13_armedndangerous_archive.html
&gt; acknowledge his contributions What, exactly?
I don't want to rely on debugging tools. I want to know how to use it when I need it.
that's a good idea, I'll think about it. In the meantime you can check the python insider blog http://blog.python.org/ it has such content
ESR is an old-school hacker from way back. Check out http://www.catb.org/~esr/software.html 
Whats it written in? Browsing the source code looks like Pascal (Delphi?). Any reason why its not written in Python?
Delphi...my guess is...they like Delphi.
because it makes use of SynEdit, an advanced multi-line edit control that is written in Pascal.
Hahaha. Python IDE that only works on one platform. Yeah right. Good luck with that. 
foolish question, what are salts ? do you have any link ?
Anyone try this yet? Is it more geared towards beginners, or intermediate programmers?
Sure, there are lots of ways to do things in Perl, so that someone can come from another language and get stuff done. It may not be pretty, but it'll work. But after a while you stop writing things like for (my $i = 0; $i &lt; 6; $i++) { # do stuff } and write for my $i (0 .. 5) { # do stuff } From perlstyle: "Perl is designed to give you several ways to do anything, so consider picking the most readable one." But then, "readable" depends on level of familiarity, so something that looks cryptic to a beginner (e.g.; a Schwartzian transform) may be a familiar idiom to a more experienced programmer. And yes, that can be a problem. If you're determined to shoot yourself in the foot, Perl will cheerfully allow you to do so. :-) 
looks for beginners like me
O my god... wow...
&gt;WE THEREFORE ASSERT the following convictions as the basis of the anti-idiotarian position: &gt;.... &gt; THAT Saddam Hussein poses a particularly clear and present danger in combination with them, a danger demonstrated by his known efforts to develop nuclear weapons, his use of chemical weapons even on his own population, his demonstrated willingness to commit aggression against peaceful neighbors, and his known links to the Islamic terror network in Palestine and elsewhere. Good Lord. Did anybody tell him there was no WMD in Iraq?
Good article. Anyone care to elaborate on which approach (numpy alone, weave_numpy, pyrex) is best to use in general or under certain usage cases. They all look useful, but hard to decide which one to start with. Edit: This helped a little: [link](http://books.google.ca/books?id=S0l1YFpRFVAC&amp;lpg=PA370&amp;ots=VCT0y1qRpW&amp;dq=python%20performance%20weave%20pyrex&amp;pg=PA370#v=onepage&amp;q&amp;f=false)
This isn't all python but the book 'Beautiful Code' is pretty awesome, http://oreilly.com/catalog/9780596510046. 
&gt;"If you're like most programmers, and you're using a basic editor like Notepad" &amp;#3232;\_&amp;#3232; Anyways, I doubt that tutorial was what he was looking for. It covers how to use the Wing IDE debugger, when Python actually does have a builtin debugger ([pdb](http://docs.python.org/library/pdb.html)).
PyScripter is light and very responsive compared to other Python IDEs for Windows (based on "external library" like PyQt for example).
I've never used Wing, but Spyder IDE for example uses pdb for [debugging](http://code.google.com/p/spyderlib/wiki/Features). Graphical debugger is simple to understand how it works.
http://en.wikipedia.org/wiki/Eric_S_Raymond
Better an over-reliance on debuggers than "It's not working. I'll just change this and see what happens. Nope. Maybe the other way around. Nope. What about this? Nope ... (some time later) ... Ah! Fixed it! Next!" (Also known as the "Off By What?" approach). At least a debugger-reliant programmer will have a chance of understanding what his or her code is actually doing. Which is not to say that it is in any way an ideal approach.
Generally I wouldn't develop remotely if I had the choice. I would develop locally then deploy to the remote server. If you haven't already seen it, Flask has a debug mode that gives you a python shell within the web page when there's a traceback: http://flask.pocoo.org/docs/quickstart/#debug-mode Obviously heed the large warning there about not enabling it for production.
I don't see much there that really warrants the position he once held (and still does?!) in the minds of some. The only moderately popular thing he "made" (iirc it was just a fork/abandoned he took over?) was fetchmail, which is pretty horrible.
web2py has a web based IDE and web based a ticketing system (all errors are recorded and grouped by traceback and frequency). [Here is a working demo](http://web2py.com/demo_admin) (try click on demo_app1 edit button).
&gt; If you're determined to shoot yourself in the foot, Perl will cheerfully allow you to do so. :-) Not only that, but it will steady your hand while you aim at your foot, wipe your brow of sweat to make sure you don't miss your foot, and whisper encouragement to you to make sure you know that blowing away your foot is doing the right thing: *"Go on, you know they'll be in awe of your skills and omg, who would ever expect this to be the right solution?! Now be a good precious and pull the trigger.."*
Think it could be adapted to work with Flask/Werkzeug? Does it do more than just be a texteditor?
It's actually more like "Which variable is off by one and which way?" but that doesn't have the same ring.
Flasks debug mode allows you to execute arbitrary code in the event of an error. Check out the documentation. Otherwise you can try this dirty python hack: try: code here except: while True: eval(raw_input()) Assuming you have your server up and running in a terminal it will give you the raw_input prompt so you can mess with the program. There are much better ways of doing this, but this is super simple and easy to do.
Hey, if I can't develop remotely then I can't modify live production code!!! As root! In all seriousness though, when developing in python for a linux server from a windows machine, there are reasons not to choose the local OS for development. 
Unfortunately, the code quality of modules in the standard library isn't uniform, so in the general sense, I wouldn't recommend just poking at modules randomly. If you have specific modules to recommend, do so.
If you're committed to using a set of method names/conventions that aren't default to your framework your best bet is probably to write some sort of [adapter](http://en.wikipedia.org/wiki/Adapter_pattern) to provide a centralized location for translating your naming conventions to the framework's. That being said, you almost certainly do **not** want to do that. Follow framework naming conventions while inside a framework. You'll thank yourself in 6 months 
I don't disagree that IDE graphical debuggers aren't particularly complicated. What I mean to say is that I believe the OP was looking for a tutorial on how to use pdb from the command line (i.e. the python interpreter). If that is true, the video above won't help him.
A salt is some sequence of bits that is added to the data (usually a password) that is passed into the hash function. The most important benefit is that simple dictionary based attacks (like the one from my experiment) become infeasible, because it is no longer practical to compute and store a complete lookup table for all possible salt+password combinations. More info as always on Wikipedia: http://en.wikipedia.org/wiki/Salt_%28cryptography%29
You have to define the method the framework is calling. It's going to try to call a method with a particular name, and that method is either going to execute or not be found. Assuming you don't want to patch the source of the framework to change the names. You don't have to define a second method like above though, you can just assign a member variable like in cibyr's post: moveMouseEvent = move_mouse Alternately, you can implement \_\_getattr\_\_ on your class and map missing method names to the name you want to call them by. That would be kind of pointless though.
Yeah, I would like to do it from the command line. Once I have that knowledge it goes with me to graphical ones (just the nuances change).
Unless you want to turn your dynamic language into a de facto statically typed one with asserts, you're going to need a debugger to figure out why your map doesn't contain what you were expecting it to or why your arrays are the wrong dimensions.
It includes two text editors (editarea and Amy) which you can use with Flask, but the IDE does more and very much tied to the web2py app structure (web2py forces one, Flask does not) and therefore it would not work without major rewrite. For example web2py has its own distribution and packaging mechanism and its own internationalization libraries which the IDE provide an interface to. The web based IDE also has an app wizard, a google app-engine deployment interface and an a mercurial interface. All but the mercurial interface are web2py specific. EDIT: This is very different from what zope used to do.
According to whom? Are you suggesting that using print statements is the way to go? 
Ah, fair enough. I do really like Flask but web2py sounds interesting.
&gt; Think it could be adapted to work with Flask/Werkzeug? Developing right from within the application I am not a fan of. ZOPE used to do that a long time ago and there are good reasons why it was abandoned. If you really want a web IDE then it should run independently of the application, but not in the same python interpreter for too many reasons.
I use ssh(kitty)/screen/vim and just work on the server directly. [Screen](http://www.mattcutts.com/blog/a-quick-tutorial-on-screen/) especially is nice (fire up the devserver in one console, ipython in another, can directly import and test your flask functions from ipython and interact with devserver using exception catching as sedaak [mentioned](http://www.reddit.com/r/Python/comments/hizdd/ides_for_wsgi_development/c1vriut).
Generally people that do WSGI development do work on local machines and not on remote ones. The simple reason is that it works for everybody and does not open yourself up for security problems due to misconfiguration. There are two things that are commonly used: Werkzeug's in browser debugger (as provided by Flask). Just type "1/0" in a python file and you drop in a debugger at that point. Alternatively pdb is commonly used (import pdb; pdb.set_trace()). Generally I recommend not depending on debuggers for Python development. I use them constantly with C++ and C#, but in Python my whole workflow works differently. When I have the situation where I feel like extensive debugging is needed I write a simple test file I can run from the command line that imports the broken function and execute it from there and debug with prints. Why am I doing that? Because it works so much better than a debugger, even in C++. The reason you don't do it in C++ however is because compiling takes for ages and you can't often easily call single functions without having to start up a whole framework or something similar. That's usually not the case in Python and a more agile approach to debugging is very helpful.
Aye, remotely I use byobu, which is like Screen but also has some other nice things (check it out), but I like clicking around and stuff, so I've never gotten on with vim as well as I have with GUI editors.
Thanks!
A hdd costs $30 new or less if you use craigslist, save the pain and use an environment that is conducive to python development. Sublime Text is even available for linux and it has great font rendering.
Aye I will once I get paid perhaps, but I'm moving soon and need to save as much money as possible.
The new web2py in trunk uses a thread local object like Flask does. We copy good ideas when we see one ;-) We will release it by next week.
Another scammy link to examville. [Here's the original slides](http://www.slideshare.net/shawnrider/django-forms-best-practices-tips-tricks)
I also read in another debugger talk elsewhere that using a debugger to do a code review of someones code is a good way to go as well. That got me to thinking that I have no real knowledge of how to use one. :)
A new version will shortly be released that fixes issues with the debugger component of PyStudio on MacOS and UNIX.
Doesn't run on linux: Not interested.
It is geared towards beginners, gauging from the problems I've done in section 2 so far. I like the concept, and I wish this had been around when I started learning python.
Beginners, but I tried it for the fun and got stuck here: http://www.pyschools.com/quiz/view_question/s1-q13 (sigh) (I've found '%#x' gets more what they want, but it still fails with '10'.) Edit: they wanted "%#.2x" so I guess I learned something....
Seems pretty unstable under Windows too :(
You are missing my point. I am not bashing or whining or complaining or whatever. Stop acting so insulted. There are simply many good options out there and I do not use python for major projects at the moment. In a year or two, who knows? I'm not arguing that py3k was a bad idea. edit: by fork I meant in features, not in the usual sense, hence the quotes
That is a viable option. Just not one that I choose. People act like the language you use is a religious choice. I know many many programming languages, and currently another language (a couple, actually) suits my specific needs better than python due to the limited time I have for programming. 
Given that 'Windows' is in the subject line, why even bother opening this link to come and post that?
As long as distutils is in the standard library, it's not obvious. Similarly it's hardly obvious that setuptools is the wrong way to do it, when it can successfully install a Windows binary extension when alternatives like pip cannot. And judging by this comment on the link provided - *"Compilers, and especially Windows, are the worst part of our code."* - `packaging` is not on course for fixing that, meaning half the world will still prefer setuptools/easy_install.
I'll just leave [this](http://frogstylebiscuit.com/images/content/06-aug/eric_s_raymond.jpg) here
Nice, but how long did this take on how much hardware? I've done the exact same thing with python, but without a parallel framework like hadoop. My solution maxed out at about 1500 lookups/second/core. So, theoretically 43 million lookups per hour on an eight-core machine if I was processing files back-to-back.
Given that you think I posted an extraneous comment, why bother replying? 
I'm not insulted. But I guess I misunderstood what you were saying: So what you meant to say was: "I don't use python right now, but might later. There are a lot of good tools other than Python." And all that stuff about forking and Python-3 was really just filler.
For your ASCII control chars, see [Wikipedia](http://en.wikipedia.org/wiki/ASCII#ASCII_control_characters). For the string format chars, see [the Python docs](http://docs.python.org/library/stdtypes.html#string-formatting-operations).
It wasn't exactly filler. I feel the points I made are legitimate (particularly the main point that this topic's author made his argument on an extremely outdated version of perl), but perhaps not well explained. I do not like the idea of 2to3 being the 'official' method of maintaining multi-version compatibility when it comes time for 2.x's end of life, or whatever they are calling it. A better summary would be: I don't like worrying about library compatibilities and searching around/hacking fixes to get them working, so at the moment, considering the vast number of options available in the modern scripting language realm and my limited time for coding, Python just isn't a fit for me right now.
This is entirely more reasonable. Although I would suggest that Python is still well worth the effort, and that the effort you speak of is really quite limited. In fact, I would say barely ever have to worry about these issues, and when I do it's as simple a fix as saying: pip install &lt;module&gt;==&lt;some version I know works&gt;
Thanks a lot.
Under operators it's missing the bitwise operators: ~ __invert__ &lt;&lt; __lshift__ &gt;&gt; __rshift__ &amp; __and__ | __or__ ^ __xor__ And the corresponding augmented assignments: &lt;&lt;= __ilshift__ &gt;&gt;= __irshift__ &amp;= __iand__ |= __ior__ ^= __ixor__ For example, taking the 2s complement of an int (i.e. computing its negative): &gt;&gt;&gt; ~10 + 1 -10 &gt;&gt;&gt; (10).__invert__().__add__(1) -10 
The performance is in the range of your implementation. Hadoop does not magically make execution of processes faster, the big advantage is that is automatically takes care of distributing the work units among multiple nodes. On top of that it takes care of managing failure of individual tasks or even nodes. If you can get by with a single machine to get the job done, split and the multiprocessing module will do just fine. If, however, you grow beyond that, the ability to code directly to your business problem without having to worry about the operational mechanics of task splitting, distribution and failure management is invaluable. In my use case we process multiple billions of rows once a week and on top of geocoding perform statistical calculations on the data. 
Thanks for the response - I'm definitely going to keep an eye on what you've done. I used to do about 20 million lookups a day in the system that I described. But I'm going to need to reimplement later this summer to handle about 300 million rows every day. I've considered hadoop, but a few things have held me back: * I need an extremely low-latency solution - when my data files appear (in million row chunks every 10 minutes) I only have about 1 minute to process them. * I already have a solution I can redeploy * My per-host support cost is pretty high in our corporate data center - so banks of cheap commodity servers aren't practical * I have access to cheap, used, very fast 16-core servers that I think will handle my 300 million lookup a day load But there's a chance I'll need to up that to 20 billion lookups a day if another project moves forward. And that definitely sounds like hadoop territory. 
In case no-one else has explained it to you, that's the handler that will print to your screen - the text "console" that you're interacting with like cmd.exe, or an xterm or gnome terminal or terminal on the mac.
throw my 2 bits in for eclipse + pydev, the remote debugging is pretty solid import pydevd;pydevd.settrace('183.155.12.11') you can step through, into, out of, execute expressions, inspect objects, etc, all from 1000 miles away. also, use virtualbox
Any modern .NET or Java application is equally susceptible to the same process (the applications being distributed as bytecode). The typical workaround is obsfucation. 
Your post is asking for proof that would have no meaning to anyone who has any clue about programming. Those same people also shouldn't be worrying about things like ease of decompilation. Anything can be decompiled.
well, python bytecode is pretty easy to reverse engineer, but there are lots of other languages with similar issues. 
Decompiled machine code is typically harder to interpret than decompiled bytecode, so compiling your top-secret algorithms with [Cython](http://cython.org) would help to provide some measure of IP protection and security through obscurity. That said, someone determined enough could still reverse engineer your algorithm. For true protection you need a client-server model (i.e. keep your secrets hidden behind a server) and the vigilance to stay on top of security problems -- and above all trusted collaborators and partners.
&gt; Those same people also shouldn't be worrying about things like ease of decompilation. One of the commercial industry people I talked to are afraid of their competitors stealing the source code, changing stuff and selling it as their own. I pointed out that if their competitors have the know-how to decompile, they might as well make a clone from scratch. They were convinced and we built the software in Python. Granted, this is a small software (standard CRUD app) but I'm afraid my argument won't work if someone is planning a large project and the Eve decompilation is brought up. &gt; Your post is asking for proof that would have no meaning to anyone who has any clue about programming I'm sorry if I wasn't clear. I'm just covering my lack of ideas on what to say if the Eve decompilation is brought up. But you got a point that I should specify if I'm going to discuss with programmers or non-programmers. Editing post now. Edit: While thinking up how to edit my post, I realized that I want to hear both technical and non-technical arguments. So I won't be asking to limit the discussion to just one of the two. Sorry about that.
Most modern encryption (devices) relies on the difficulty of the math (P!=NP) and not on hiding the algorithm from you. Same should apply to software development. For EVE, their security concern is malicious use of the service (the clients capabilities for counter-measures are revelead?). They can deploy counter-measures server side and updates to the client, it's a constant war for every MMO I think. For software that only runs locally... meh... but all software is vulnerable to some kind of reverse engineering. If you're relying on your software being closed source to give security to your customers you're not doing it right IMO. TL;DR nothing is safe.
+1
You can just obfuscate your code before you compile and release it. It's not too much work to write a script to automatically copy your source code, obfuscate it, and then compile it to an exe.
+1, because obfuscating or shipping binaries is NOT the way to go if you want to keep things a secret * http://www.markus-gattol.name/ws/python.html#protect_code
Then with a $500 debugger you decompile the exe into source code, and everybody who knows anything about security laughs at your feeble attempt. http://en.wikipedia.org/wiki/Security_through_obscurity 
Security through obscurity is no security at all.
I'm going to assume he posted in anticipation of a reply so he could downvote you twice.
Yep, you could decompile the source code, but I've obfuscated the source. If I don't obfuscate my source code, then you'd be able to retrieve the documentation, class names, method names, variable names, and argument names in addition to the program structure. I can remove the documentation and make all of those names completely random before releasing my program, so you have to guess at what would be appropriate names for classes while being unable to figure out what they do. Tell you what, I'll show you what some source code from an actual program I wrote would look like after I obfuscate it and you decompile it. Now, you tell me what it is and what it does: import VED from YQS import AFT class GVC (AFT): def __init__ (self, LUK, ERT, AID, UNB, LIJ = None, AXV = None, YYT = None, PLO = None): AFT.__init__ (self, LUK, ERT, AID, UNB, LIJ, AXV, YYT, PLO) def NBV (self): self.DMN = VED.OGJ.WCF (self.IMJ+"\\idle.png") self.DMN = self.HOR (self.DMN) def ISU (self): self.CRV = VED.OGJ.WCF (self.IMJ+"\\active.png") self.CRV = self.HOR (self.CRV) def QAF (self): self.NJR = VED.OGJ.WCF (self.IMJ+"\\pressed.png") self.NJR = self.HOR (self.NJR) def HOR (self, SQN): if not self.ZEF and not self.IFL: return SQN MRY, DNJ = SQN.OFU() FER = float (MRY)/DNJ if self.ZEF: MRY = min (MRY, self.ZEF) if self.IFL: DNJ = min (DNJ, self.IFL) PGQ = float (MRY)/DNJ if FER &lt; PGQ: MRY = FER*DNJ elif FER &gt; PGQ: MRY = FER/MRY MRY, DNJ = int (MRY), int (DNJ) return VED.GVB.LIM (SQN, (MRY, DNJ)) Any idea?
Let's add to that that you can use VirtualBox or VMWare to set up a virtual environment to better match your production server (and virtualenv to create a Python environment with exactly the right modules and versions). I use PyCharm, which comes with its own debugger; but I've found that when running apps in the framework's debug mode, I rarely need "proper" debugging. Most of the problems can be explored in `bpython`/`ipython` (or just `python` if you're old-school) and the rest usually boil down to state (which should be relatively easy to recognize unless your app is ridiculously complex). The few cases that would benefit from a debugger can often be solved with a few prints as you say. It's obvious why you would enjoy Werkzeug's debugger (\*cough\*blatant self-promotion ;)\*cough\*), but most major frameworks seem to come with at least _some_ level of detail in the debug mode error pages (IIRC Django's template error messages are a particular bad example, though). I enjoyed Pylon's error-handling the most as it gave you an interactive shell right in the browser that let you investigate the exact state of the program at a given point.
&gt; IIRC Django's template error messages are a particular bad example, though Django can be used with Werkzeug's error debugger too as well as the pylons one. &gt; Pylon's error-handling the most Mostly the same as the Werkzeug debugger or the other way round, depends on how you look at it.
Those reasons seem to be a thing of the past. My development notebook runs Windows 7. All my actual coding happens within a VirtualBox instance running the desktop version of the Ubuntu release running on the production server. pip/virtualenv lets me freeze the exact releases of all packages I use in both environments. This has the added benefit that if anything I do messes with the OS, only the Linux guest is affected and I can easily replace it without having to worry about any important data (the entire code is in a repository; the virtual environment is trivial to reconstruct -- it could even be recovered from a backup).
This. I would argue that all development should happen in a local environment. It still makes sense to have a staging server to deploy to before deploying to production. This not only allows you not to have to worry about your development environment not being an exact copy of the production environment (e.g. running a desktop edition of your Linux distro rather than the server edition), it also means you can mirror the production environment much more closely (e.g. RAM, latency) without having to worry about doing so on every developer's machine.
The problem wasn't with the error handling in general. Just that errors in templates would be particularly unhelpful because they wouldn't directly provide any information directly relevant to what needed to be changed to fix the problems (i.e. wrong level of abstraction). I'm not sure how Werkzeug can help if the library lacks the sufficient level of introspection. I'm not sure this is still a problem, though. Django is pretty actively maintained. That said, Werkzeug (and Flask) _is_ pretty awesome indeed.
It looks like it shows the states for a button press! idle.png - mouse/cursor is not over button active.png - button is selected pressed.png - button is being activated
I am just going to leave it here http://depython.net/
&gt; Just that errors in templates would be particularly unhelpful That's a problem of the django template engine and completely irrelevant to the choice of a debugger.
Regularly I see that you are the top poster around here. I've orangered you in my feed so I can see the good advice you dish out. Can you link me to your favourite Python book that you read when learning? Or some amazingly insightful abstraction that made something 'click' for you?
Have you tried PyQt or PySide? I'm looking at using PySide for a new project - do you think its unresponsive? Just interested in your opinion if you have used it.
I didn't say it was due to the debugger. I just said that other frameworks usually come with some form of debug information already (if not a full-fledged debugger) rather than just dumping a traceback in the browser; Django's (?) template errors being on the more unhelpful side. I'm sorry if I didn't make that clearer.
He put the Hacker Koans into one place. I appreciate having easier access to them.
A very reasonable answer, upvote for you! We've left plenty of questions open, however. What exactly does this code do? What exactly should we call this class? What should we name the methods? What should we name the imports? And, is this really a class related to buttons? What if it's a class for units, or cursors? What in God's name does HOR do and why is it called every time those three other methods are called? If I hadn't obfuscated it, you'd already have all the answers once you decompiled it.
&gt; that if their competitors have the know-how to decompile, they might as well make a clone from scratch. Which is also arguably less work in almost all conceivable cases.
Distribute the software together with source code. Sue anyone who "steals" it.
I love how all these companies always assume that all other companies are fine building their business practices on completely illegal actions that would instantly result in bankruptcy if they ever became public. If it's so easy and fail-safe to just rip off another company's software, why aren't _you_ doing it? Oh, right, you're concerned that you would open yourself to a lethal lawsuit that would not only remove the foundation of your business (i.e. your software products) but also the entire income you earned from it (by paying damages and legal costs). Yes, most companies won't shy away from _unethical_ practices if they can get away with it, but _illegal_ ones are a whole different matter. If they are small enough that they have nothing to lose, they probably won't pose a threat. If they're big enough to afford the lawsuits, they are more likely to develop a clone in-house, license your software, buy your company or just ignore you altogether. All your competitors can legally steal is your ideas. And for the most part, there's no way to prevent that. In all likelihood, your software is not complex enough to be particularly innovative in how it does certain things. And when it is, the disassembled source code still doesn't contain all your knowledge and research, i.e. _why_ it was implemented that way.
pocoo.org projects are generally elegant and well documented, with some eventual ninja or magic touches.
The only 100% proof way to save your source code from being stolen by the competition, however, is writing atrocious source code in the first place. Eschew best practices! Use Hungarian notation (and don't change the names during refactoring)! Unroll loops! Use lambdas for everything! Nobody will want to steal your code when it's ugly enough! (with apologies to Independence Day)
&gt;What in God's name does HOR do and why is it called every time those three other methods are called? Well, it obviously services the Johns. It's called with every other method because the pimp has a small stable.
I really wish that I had this during my Physics undergrad, instead of Excel and Maple.
instead of eval/raw_input use [code.interact](http://docs.python.org/library/code.html#code.interact) to create a read-eval-print loop and pass the locals &amp; globals
I am kinda amazed that the client is written in python and not C/C++ etc. But I haven't looked at the code, I just assume it was written in python. Someone clear this up? I work in the IT industry and I hear a lot from managers about security and that sort of thing. They want to be protected from as much as possible and allow no access to the internal systems. I tell them, ok disconnect the hardlines to the internet, and I get funny looks. I tell them, dont get to hung up on it, google/DOD gets hacked, do you not think that your system will stop these people? You are protecting yourself from 90% of the attacks by using best practice, otherwise your going to make most of your time about this security system, and all it takes is a hump and your presidential car gets stuck. Getting back on track. Yes, protecting your IP is important, but any language can be decompiled. Any executable can be seen. Making it hard is just a challenge to the hacker. The best bet is to use a client/server model and expect the client to be read. If you can't do this, develop some sort of polymorphism, self aware code among other methods.
Read up on some theory behind them. E.g. read about feed-forward networks, back-propagation training, activation functions and error functions. Then implement it! It's what I did more or less. I can suggest a first toy problem to solve too. The XOR-problem. Dataset: in[(0,0), (0,1), (1,0), (1,1)] out[1, 0, 0, 1]. 3..2..1.. GO!
&gt; Someone clear this up? They used a combination of python and C/C++ as extensions. This comment from the DMCA takedown should help: http://www.reddit.com/r/programming/comments/hj8aj/eve_online_submits_dmca_takedown_to_github/c1vva1b
This had 73 upvotes and only one comment, so I decided to peruse it before upvoting it myself. Conclusion: Good stuff. 
there is a book:[Machine Learning: An Algorithmic Perspective](http://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html) this machine learning book starts with NN, and there are Python (and NumPy) code examples in the book.
thanks !
XXX ? XXX. XXX XXX.
That is if you enjoy working in a VM. While I've met a lot of people that develop how you say, for some reason I hate it. I think it is because I like setting up a test environment on the remote server where the App is designed to run in order to get a sense of the bandwidth and performance that I should reasonably expect. At that point it is personal preference I suppose. Since I use Vim mostly it doesn't matter if I'm SSH or not.
What is the advantage? Does it prevent you from doing anything? It seems to just be less code. So what I wrote + print statements becomes import code code.interact() ?
I was just being a hard-ass; your concerns are very real. I have a reasonable amount of experience dealing with the vast differences in understanding between coders and salespeople/management. In a few years if you still have these concerns I'd be pushing you toward Go(golang). I'm having amazing results in keeping maybe 70% development speed of python with nice deployable binaries. A lot of the database connectors are experimental. Also, I don't think it has much GUI support. Makes me think of Python in the late 90s, but advancing more rapidly.
dont think there is any advantage, just mentioning a builtin alternative.
We're talking about decompiling object code with professional toolkits, not reading over the result of running a short sed script over a snippet from a .py file. Python bytecode is known to be fairly straightforward to reverse engineer, which is made even easier by the fact the interpreter supports introspection. Nobody seriously attempting to decompile your program is going to be going "gee, he renamed tokenA tokenB, I'm totally screwed now!!". They're not even going to have a copy of that junk you just posted, unless you're trying to make their life easier by separately supplying it &amp; thereby removing all doubt about the algorithms you're implementing. You've walked right down the path of security by obscurity, blindly assuming that because you call the function "printf" and I call it "print" that I'm wrong and therefore I can't rip off your code. Its totally irrelevant to the task at hand. 
&gt; Python needed a better CL parser. Hence argparse, which got included in the stdlib in 2.7/3.2
If you're just interested in machine learning in general then check out Support Vector Machines they typically perform much better than neural nets out of the box. scikit-learn has a great implementation. But if you're interested in neural nets check this [SO post](http://stackoverflow.com/questions/2276933/good-open-source-neural-network-python-library#answer-3143318) and the PyBrain library.
http://www.willamette.edu/~gorr/classes/cs449/intro.html College class material on the subject. Very thorough!
Spyder IDE, Eric IDE (for example) uses PyQt and works fine, though (under Windows) needs more time to run and uses more memory (compared to PyScripter), but I don't think that they are unresponsive.
No problem. The logging docs are getting better but still fail to explain simple things like this for people who don't have the background already. Maybe the OP can address that.
&gt; Which is also arguably less work in almost all conceivable cases. I tell this story often, but I still think it is funny. We had a configuration management system at one company I worked at, written in C atop Solaris. Somehow the guy who wrote it managed to write in such that moving it to another version of Solaris or another C compiler broke the hashing code (yes, they used a bespoke hashing algorithm) or another machine type (it seemed to be happiest on an e250 or the like; it used assembly to generate hashes "really fast!"). Anyway, no one was allowed to see the *source* of this system, since "what if someone leaked the source or changed something?" (no version control) Who would want this? It wasn't even a product we had, it was just a piece of old infrastructure that some how survived all the purges, and I couldn't imagine someone saying "you know, I'll get a whole bunch of old shitty Sun machines, an old, insecure version of Solaris, just so I can run this thing that should have been replaced by cfengine or the like 20 years ago!" I left in 2009, but as far as I know, they're still using it.
packaging is the final solution! (But seriously - fixing distutils was tried and it just broke *way too much stuff* that was already using it, because of the way it is designed. A clean break was the only realistic possibility.)
I'm starting Python and like it, tho I still don't get what all this double underscore business is all about...
Use the following coupon code to get the book for free: G06fOg63rfqs It's also available in online form here: http://korokithakis.net/tutorials/python
yeah, I like it. thanks
but the point is python when decompiled gives you pretty much exactly the code you started with, function and local variable names, etc. If you obfuscate those, then whoever decompiles your code doesn't have that and has to work harder to understand your code. With a sufficiently large codebase this can be a huge task. It's not designed to give total protection, just make it harder. More advanced obfuscation can seriously screw with efforts to reverse engineer it, but of course it's impossible to completely prevent it.
Answer: if the code is on the client, it is easy. The question really should be for the developer: what code are you going to put into a hostile environment? Dropbox uses a Python client AFAIK, so something similar could be done with their code. I don't know what, if any, measures they took to obfuscate the source before compiling an .EXE for distribution to customers.
[Link to wiki with workbook materials and helpful links](http://ipython.scipy.org/moin/Py4Science) Alternatively &gt; svn co https://matplotlib.svn.sourceforge.net/svnroot/matplotlib/trunk/py4science edit: will I ever format correctly on the first try?
minor correction - dataset out should be [0, 0, 0, 1] 
The code in the online version looks terrible when read with noscript on. Maybe put some pre tags around the snippets for non-script surfers?
"A class can implement certain operations that are invoked by special syntax (such as arithmetic operations or subscripting and slicing) by defining methods with special names. This is Python’s approach to operator overloading, allowing classes to define their own behavior with respect to language operators. " [from the language reference document.](http://docs.python.org/reference/datamodel.html#special-method-names)
&gt;I don't even have an application in mind, neural networks just seem so fascinating. I was once in the same boat. What do you find fascinating about neural networks? If you are interested in them from machine learning perspective then you should understand that an artificial neural network is literally a linear classifier, i.e. take inputs, multiply each input by some real number and add them together and that is your output. There are better machine learning techniques to understand.
Mercurial (http://mercurial.selenic.com) is very nice, many pieces are nicely engineered (commands.py with decorators for example).
I &lt;3 Jesse
Oh, I hadn't considered that, thanks. I'll do it now. EDIT: Bleh, turns out the CSS makes it look terrible with pre... I'll try to fix it later.
Thanks!
&gt; For true protection you need a client-server model (i.e. keep your secrets hidden behind a server) and the vigilance to stay on top of security problems Protection from what? If you are talking about server security you are right. But what about bots? They operate within (technically) allowed boundaries, so it's not about security. But they are undesired by CCP, I suppose. And leaked source (or decompiled sourse which preserves enough of the original structure) simplifies greatly their creation.
catchy, but false. Security isn't a binary 100% or 0%; obscurity can give you some level of protection from amateur attackers. Not 100% security, but some security nonetheless.
It would be nice to have an explanation of what their Private Test Cases are... I am on time24hr and it apparently wont let me use 'hr' anywhere in the program.
\*shrug\* I prefer using an IDE and having access to a full desktop when working on code, so VMs are a godsend. Latency and bandwidth only become relevant at later stages and are hard to judge as a single user, so I'd lump that in with most optimisation (i.e. post-staging). As you say, it's mostly a matter of taste at that point. The important thing is you can't interfere with other developers' development environments.
There is a problem, what if the competitor is Russian or Chinese? Then you can probably not reach it anymore. That said, it is that much harder for the competitor to steal your customers too.
Exactly. Russians and Chinese can steal your code all they want, they're not competitors because they don't target your audience. Also, the customers would still be using illegal software, even if the software isn't illegal in its country of origin. If your customers are fine buying knock-off software, you have bigger problems.
Thanks! I just installed Python 3.2 and pygame yesterday, i'll let you know how it goes!
That's... Highly [TDWTF](http://thedailywtf.com/Articles/The-Brains-of-the-Operation.aspx)-worthy, quite apart from being terrifying. You should definitely repost it there.
Thanks, my summer plans include learning python so this is perfect!
If you have to stick to python, you will probably have to come up with all of this stuff on your own. I'm not aware of anything for Python, aside from implementations; and the one I think I remember was not a parser, but an analyzer. Also, the good analyzers I am aware of also do not use Python, but [HFST](http://www.ling.helsinki.fi/kieliteknologia/tutkimus/hfst/). By parsing, do you mean something that would hyphenate words into separate morphemes? Why are you choosing Russian, if English is morphologically poor? Yes, Russian is more morphologically complex, but because it is fusional, it may be quite difficult. You might want to try something like Quechua or Inuktitut instead, which are really quite polysynthetic and often have very clear morpheme boundaries.
If you want a non-boring and, in fact, mind-blowing intro to this sort of thing, check out http://www.amazon.com/What-Thought-Bradford-Books-Eric/dp/0262025485. There is a critical analysis of neural networks in the first half of the book.
Cool, when will it be in the amazon store? I would love to pay for it there. I don't want to plug in my kindle right now to transfer it over.
Link is dead?
url.rstrip(".") http://www.amazon.com/What-Thought-Bradford-Books-Eric/dp/0262025485
is this for version 2 or 3? I noticed that in the file I/O chapter it uses the function file() to open a file instead of open(). 
Based on how the print function is used I'm guessing this was written for version 2. As to why he uses file() instead of open(), I have no idea.
I think exclusive-or should be [0, 1, 1, 0] contrasted to or which is [0, 1, 1, 1]. That dataset out corresponds to an and of the inputs.
Thanks! Your name looked familiar, and I recalled you from Historious - had a similar idea. How's that going these days?
I just wanted to thank you for the coupon and awesome book.
I bet if you posted the imported classes and I had any idea what the overall application was doing I could tell you the answers to the questions you've posed. I've taken apart bigger obfuscated systems and reproduced source as good as or better than the original. Sure it takes more time than reading the eve dump, but it is really only a small barrier if the reader is motivated.
But he does not derive from **object** at class definitions - idiomatic mistake or Python3?
I don't have an answer to your question, but I'd love to help out with the actual programming. I'm competent in both languages -- let me know if you'd like a collaborator!
How about os and sys? They are tried and tested, cant go wrong. 
Completely agree, I went to grad school to learn about NNs. I learned that they kinda suck compared to other algorithms like SVMs.
For one, I found scapy insanely useful. We use scapy for monitoring packet flows in my telescopes.
Yes, it's catchy, and it would be better for people to understand that you can only be more or less secure, not absolutely secure. However, the level at which most non-technical, and technical people will engage with security is at the level of obscurity. I have no evidence for this. It's just a gut feeling. I've emailed numerous websites whenever they send me my password in plain text, or displayed it on the screen even, and it takes an enormous amount of effort to get beyond them saying 'don't leave your machine with it visible'. I think the saying 'no security at all' is an untrue adage but a very good rule of thumb.
You can protect from all of that (bots), even if your protocol is open, with TPM chip and DRM. I know this is very unpopular, and I'm far from an advocate myself, but this is, as far as technical solutions go, the only real one. Truth be told, DRM for all of its evil, if used responsibly, could enable some very interesting things to be created. 
Hmm, that's a good idea, I guess I can self-publish it right now, thanks! EDIT: Thank you for the suggestion, I've published it on Amazon (it costs $3 there, due to some of their options), and it should appear within a few days!
Please do, I'm curious about Pygame!
0.2 has been released. Can you see if you still have instability issues? I've been using this plugin in my office for a while now with few problems. If you do still have instability issues or find other problems, please can you submit a bug report here: http://code.google.com/p/editra-plugins/issues/list 
This *was* written for 2, back in the day when file() wasn't deprecated yet. I'll change it, thank you.
It's going well, it's not growing very fast but all its users seem to like it! I use it for bookmarking everything, so I'm dogfooding it pretty well.
No problem!
The protocol methods (the double underscore ones) are ones that you typically don't call directly, but the interpreter calls *for you*. For example if your custom class provides the \_\_nonzero__ method then bool(YourObject()) will call this method. This is how Python does things like operator overloading - your classes can implement the numeric protocol methods to behave like numbers.
Hehe, man crushes are the best. :-)
You are of course absolutely correct. Thank you
That would be an and function, not an xor, wouldn't it?
see also: http://www.h1tman.com/2010/08/libcloud-python-example-kick-py/ libcloud Python example – kick.py Posted by Tim Galyean on Aug 24, 2010 and: http://libcloud.apache.org/getting-started.html 
os and sys are mainly thin platform-specific wrappers, I doubt you'll learn much software design from them
Poor pydanny is having a bout of histrionics today. "This much code" and a "debugging" nightmare? Really? Yes, it's more code, 7 lines versus 1. But those seven lines are tightly coupled and will occur together and should probably be bundled into a subroutine, so the practical upshot is they are basically the same. There's no flow control statements, just a series of API calls that define parameters. If that is a "nightmare" to debug, please step away from the computer because programming is not for you. Not to take anything away from the Requests library, which I'm sure is a very nice library. I just take offense at the breathless chicken-little tone. And, you know, *some* of us like/need to know the nitty-gritty details of how the https connection is being setup and glossing over it with a convenient library may not be an ideal solution. 
I found a fiver today, so I'm paying some of it forward. Thanks for the book!
I won't disagree with your point that sometimes you need to go down to the "nitty-gritty details". Yet, for simple tasks, it would make sense if urllib just had a convenient method for making the default case a one-liner. Batteries included should not induce that I have to assemble them first. Edit: And while we are at it, yes it is too much code if you just need something simple where this level of control is not needed - beginners easily stumble over such overcomplicated APIs, where a more convenient wrapper for the default case would be totally sufficient. The best code is code I don't have to write.
It's not just a LOC comparison. The urllib2 code is ugly and verbose and I have to look up the docs every time I do it. import requests requests.get() Is something I'll just remember.
&gt; *The best code is code I don't have to write.* Indeed. And yes, a convenient set of defaults is always nice. As for overcomplicated API, I think that's a matter of perspective. The setup (PasswordManager -&gt; AuthHandler -&gt; Opener) is no more complicated than setting up the logging module (Formatter -&gt; Handler -&gt; Logger) and clearly describes the parts that are involved in establishing a HTTPS connection. Unfamiliarity with an API doesn't necessarily mean it's overly complicated. And in this case I would argue it's not. We should compare urllib2 and Request in a situation where we need something different than "HTTPBasicAuthHandler". From my use and understanding of the Requests package, it's much the same amount of overhead to register AuthObjects. Note: I'm not saying urllib2 isn't a bit of a mess or that Requests isn't a spiffy little library. I'm just saying that this issue isn't a very good reason to choose one over the other. 
httplib2
A few other options for http clients: [httplib2](https://code.google.com/p/httplib2/) has a pretty good api and implements the spec well (including caching and authentication; though it lacks a cookie jar). In the async world [Pendrell](http://pendrell.olix0r.net/) is pretty comprehensive; it implements a full http agent, with a cookie jar (cookielib-compatible), multiple connections per host, pipelining, and support for various proxies. 
I think a good API can and should provide both - an easy start to accomplish the (edit) simple (/edit) task (aka requests.get) - but also let's you reach deeper when you need more or different (aka PasswordManager-&gt;..). Especially when you can then go ahead an use the code inside the convenient method as a template for your needs.
You make an excellent point. And sadly urllib2 wasn't designed, it evolved. Haphazardly. I was just irked at Danny's reaction. As warts go, this is really, really minor. A trivial annoyance. Something that takes a couple of minutes to check the docs. Not to mention its not code you have to touch all the time - you do it once and never see it again. To me, it seemed like, "OMFG When the power goes out, I have to reset my bedside alarm clock!!!! What a horrible API!! I have to look up which button to push! Look at this other clock that has batteries, I don't have to do that." Maybe I'm just sensitive today. Perhaps I'll go have a lie down. 
I agree with bushel. Using urllib2 never irked me one bit. encapsulate the functionality you need into a method and be done with it. If you could avoid it, why would you not use the python standard lib?
I absolutely agree, it is a minor wart and Danny sometimes has a tendency for the dramatic :) I would probably just post: "Not happy with urllib2? Try requests or httplib2, they are on pypi" Though different people have different angles on such things, so let Danny ramble a bit so people on reddit can learn about alternatives in the comments, as excellently executed by [obtu](http://www.reddit.com/r/Python/comments/hkm8f/python_http_requests_for_humans/c1w4wvf).
Dunno if I'd want to submit it to TDWTF; for one, everyone who works at that company would know instantly what system is being discussed and for another, I've always been under the impression that TDWTF was embellished by the editors to make it funnier. Plus, mine isn't too long; there was a shitty system setup atop old Sun boxes and an old Solaris install with some (amazing | terrifying) C abilities. Definitely my favorite "wtf were you thinking" moment from my mere decade of work experience.
Upvote for you and an opportunity for him to downvote me again. :-)
That view is kind of out of date now that better methods exist for training deep networks. Geoff Hinton's groups has come up with some great stuff in the last few years.
Try this: http://pybrain.org/
Thanks; nothing against sunqiang's response, but your explanation clicked in my head a bit better then the technical version. 
&gt; When your code you try to read six months from now. Reading this in (internal) Yoda voice makes this post a win.
I'd like to drop my pycurl code and use this instead. But... I'll need the following options first. * SSL_VERIFYPEER: If true, verify peer's certificate. * SSL_VERIFYHOST: 1 = check the existence of a common name in the SSL peer certificate. 2 = check the existence of a common name and also verify that it matches the hostname provided. * CAINFO: The name of a file holding one or more certificates to verify the peer with. Or CAPATH: a certificate directory.
 import pygame import math import random pygame.init() screen = pygame.display.set_mode(( 800, 600)) selected_Particle = True class Box(pygame.sprite.Sprite): def __init__(self, posx = 600, posy = 200): pygame.sprite.Sprite.__init__(self) size = random.randint(19, 20) ##self.image = pygame.Surface((50,50)) self.image = pygame.image.load("images.jpg") self.rect = self.image.get_rect() self.rect.centerx = posx self.rect.centery = posy self.x = self.rect.centerx self.y = self.rect.centery self.size = size self.move = False self.dx = 600 self.dy = 10 self.dir = 0 self.rect.centerx = posx self.rect.centery = posy "pizza" def update(self): if self.move: self.rect.center = pygame.mouse.get_pos() (mouseX, mouseY) = pygame.mouse.get_pos() self.x = mouseX self.y = mouseY def findParticle(self, (x, y)): if math.hypot(self.x-x, self.y-y) &lt;= self.size: print "Bouncer Selected" self.move = True else: self.move = False class Pluto(pygame.sprite.Sprite): def __init__(self): pygame.sprite.Sprite.__init__(self) self.image = pygame.Surface((25,25)) self.image = pygame.image.load("images3.jpg") self.rect = self.image.get_rect() self.rect.centery = 00 self.rect.centerx = 10 self.pos = [200.10,0.0] self.pos[0] = self.pos[0] self.pos[1] = self.pos[1] ##TEST FOR GRAVITY self.gravity = True # fragments fall down self. FORCE_OF_GRAVITY = 1 self.dx = 1 self.dy = 2 self.offscreen = False self.seconds = .03 self.r = self.rect.right self.l = self.rect.left def update(self): #... if self.offscreen == False: if self.gravity: self.pos[0] += self.dx * self.seconds self.pos[1] += self.dy * self.seconds self.dy += self.FORCE_OF_GRAVITY # gravity suck fragments down else: self.pos[0] -= self.dx * self.seconds self.pos[1] -= self.dy * self.seconds self.dy -= self.FORCE_OF_GRAVITY # gravity suck fragments down print "in gravity" self.rect.centerx = round(self.pos[0],0) self.rect.centery = round(self.pos[1],0) if self.pos[1] &gt; 1000: self.offscreen = True def bounce(self, listOfObjects, numOfObjects): i = 0 while (i &lt; numOfObjects): if (self.rect.right &gt; listOfObjects[i].rect.left and self.rect.centerx &lt; listOfObjects[i].rect.centerx and self.rect.bottom &gt;listOfObjects[i].rect.top and self.rect.top &lt; listOfObjects[i].rect.bottom ): print " WASD" if (self.rect.centerx &gt; listOfObjects[i].rect.centerx and self.rect.left &lt; listOfObjects[i].rect.right and self.rect.bottom &gt;listOfObjects[i].rect.top and self.rect.top &lt; listOfObjects[i].rect.bottom ): self.bounceRight() print "cool guy" else: self.bounceLeft() print "your mother" i += 1 def bounceLeft(self): self.dx = 10 self.gravity = not(self.gravity) print "horse" def bounceRight(self): self.dx = -10 self.gravity = not(self.gravity) print "donkey" def main(): pygame.display.set_caption("Catch it!!") background = pygame.Surface(screen.get_size()) background = background.convert() background.fill((55, 55, 13)) screen.blit(background, (0,0)) box3 = Box() box2 = Box() box1 = Box() box = Box() p = Pluto() listOfObjects = [] listOfObjects.append(box) listOfObjects.append(box1) listOfObjects.append(box2) listOfObjects.append(box3) allSprites = pygame.sprite.Group(p) Boxes = pygame.sprite.Group(box,box1,box2,box3) clock = pygame.time.Clock() keepGoing = True while keepGoing: clock.tick(30) for event in pygame.event.get(): if event.type == pygame.QUIT: keepGoing = False if event.type == pygame.MOUSEBUTTONDOWN: (mouseX, mouseY) = pygame.mouse.get_pos() print mouseX, mouseY selected_particle = box.findParticle(pygame.mouse.get_pos()) elif event.type == pygame.MOUSEBUTTONUP: box.move = False if event.type == pygame.MOUSEBUTTONDOWN: (mouseX, mouseY) = pygame.mouse.get_pos() selected_particle = box2.findParticle(pygame.mouse.get_pos()) print mouseX, mouseY elif event.type == pygame.MOUSEBUTTONUP: box2.move = False p.bounce(listOfObjects, 4) allSprites.clear(screen, background) Boxes.clear(screen, background) ##Gravity Force allSprites.update() Boxes.update() allSprites.draw(screen) Boxes.draw(screen) pygame.display.flip() if __name__ == "__main__": main() 
He didn't ask for software design, he asked for good python code to read.
passwords.
The problem happens because the `Pluto.bounce` function is called twice for every update step and negates `Pluto.gravity` twice. This happens because the `Pluto` object is being hit by two `Box` objects simultaneously (the mouse is dragging two boxes at the same time). But why are you trying to reverse the direction of gravity anyway when you can just set `self.dy` to `-10` or something?
You might want to look at time.gmtime() :)
An example please. How would you create a midnight for arbitrary date?
 unixtime = time.time() # or datetime_object.strftime('%s') midnight = unixtime - unixtime % 86400 # 60 seconds * 60 minutes * 24 hours midnight_dt = datetime.datetime.from_timestamp(midnight, your_fav_timezone) or midnight_gmt = gmtime(midnight) midnight_local_tz = localtime(midnight) (untested, been coding Perl all day, but something like this should do it :))
if you don't mind dependencies.... """ example of wrapping webob stuff to provide similar functionality as what you would get from requests (https://github.com/kennethreitz/requests) """ from webob import Request from paste.proxy import TransparentProxy import base64 REQUEST_METHODS = ("GET", "POST", "PUT", "DELETE", "HEAD") proxy = TransparentProxy() def make_get_response(method): def _f(url, params={}, headers={}, cookies={}, auth=(), timeout=None): if method.upper() not in REQUEST_METHODS: raise ValueError("method %s is not in methods %s" % (method, REQUEST_METHODS)) else: if cookies: headers["Cookie"] = ";".join(["%s=%s" %\ (k, v) for k, v in cookies.items()]) if auth: user, password = auth encoded_creds = base64.b64encode("%s:%s" % (user, password)) headers["Authorization"] = "Basic %s" % (encoded_creds) request = Request.blank(url, headers=headers, POST=params, \ method=method.upper()) return request.get_response(proxy) return _f locals().update([(m, make_get_response(m)) for m in REQUEST_METHODS]) if __name__ == '__main__': print str(GET("http://google.com")) 
I don't understand from your description what you're trying to achieve but the solution would be: from pytz import utc, timezone from datetime import datetime from time import mktime input_date = datetime(year=2011, month=1, day=15) and now either mktime(utc.localize(input_date).utctimetuple()) or mktime(timezone('US/Eastern').localize(input_date).utctimetuple()) 
Problem is that pytz is not part of standard library. Preferred way in my case is not to add additional dependencies.
don't forget datetime.datetime.utcfromtimestamp(midnight)
Any particular reason? PyPI is usually a good way to avoid reïnventing the wheel. `pytz` isn't exactly huge either.
OK, this works for current time. I still don't see how you would get UTC midnight say for January 15th, 2011. Also how would midnight = unixtime - unixtime % 86400 # 60 seconds * 60 minutes * 24 hours behave when date happens to be the day where EST changes to EDT? 
That's a good point, but there's a place for doing simple stuff too. Sometimes you just need to make a quick, simple, throwaway thing, and it's great if the language makes that easy. I think the best language would be one that lets you easily do something simple with just a couple lines then smoothly transition into a big, complicated, robust project while still working during every step along the way. Python isn't perfect on this score (you can imagine something working better at the massive project level) but it's pretty good.
How can I post XML or JSON content? It looks like it's based on an idea of POST from HTML4 forms, and doesn't offer what REST applications need (or what XForms or XHR provide)... 
I guess some more redundancy in the python world can't hurt.
You don't need backslash new-line continuations inside parantheses.
You can pass a byte-string in instead of a dictionary, and it will be sent up to the server.
If you are too new to programming to get Jython working, why not just use Python? It's already installed. Type "python" in Terminal.app.
I've been using httplib2 the past few days just to create and check on some redirects. It's simple and works well. 
This problem has been solved before, and more elegantly than what he is suggesting. Look into Pycurl, Twill, Selenium bindings, httplib2, or one of the other many solutions. There is a healthy middle ground that already exists between urllib and his solution.
I've found that httplib2 is full of bugs. I ran into multiple even doing simple things (like... a HEAD request.)
Makes sense for Python 3, but there's really nothing super about the Python 2 syntax of super(). The main thing is, although super() protects against changing your superclass or its name, in Python 2 you have to give it both "self" and the name of the class you're currently in as arguments. Which means, in addition to all the extra typing, that if you change the name of *this* class, all the supers will break. In that case, it seems much more understandable and more idiomatic to just call the appropriate method on your superclass by its name.
Are you sure you're requesting HEAD properly?
How about pastebin since this is ... text. A screenshot of text... yargh.
&gt; The main thing is, although super() protects against changing your superclass or its name, in Python 2 you have to give it both "self" and the name of the class you're currently in as arguments. Which means, in addition to all the extra typing, that if you change the name of this class, all the supers will break. "all the extra typing" -- Having to type ``Bar, self`` inside parentheses is really an issue to people? It's not as nice as the Python 3 way, but it's hardly a real problem.
TIL: `super()` is much less retarded in Python 3.
The code that you have in the class simple_math after the function definitions is not run whenever you instantiate simple_math, but when you are defining the function. You can test this to confirm for yourself as follows. class test(): print 'hi' print 'there' test() This will print 'hi' when the class test is defined, then 'there'. When the class is instantiated, nothing is printed. I think what you are looking for is a function, rather than a class. By changing 'class simple_math():' to 'def simple_math():', you will have the function run each time you call simple_math(), rather than only when defining the class simple_math.
When I set self.dy to -10, when it reaches the top o the peak of the top of the parabola, it drops straight down versus following the parabola. Buut thank you so much for your help, I'll post my progress later.
The problem is that you aren't using classes correctly. You are basically using them like a function. Here is how you fix this: Move all of the code out of the simple_math class. You don't need a class here. the code that is in the class but isn't a function definition (everything starting at `num_1 = firstnum()` all the way to your print statement, create a function named simple_math and put all of that code in there. Now just call the function. Other things: use `input` instead of `raw_input`, input only accepts numbers so you won't have to do that testing for whether it is a number or not. You also won't have to cast it as an integer afterwards. This can be drastically simplified and if you post the actual code I'll be happy to a provide more information regarding how this could be better written. This is how I would have written this: def get_number(prompt, min, max): while 1: try: x = input(prompt) except: continue if min &gt; x or max &lt; x: print "Entry outside of bounds (%d-%d)" % (min, max) continue return x def simple_math(): x = get_number("Choose a number for x(between 1 and 10)", 1, 10) y = get_number("Choose a number for y(between 1 and 20)", 1, 20) h = get_number("Choose a number for h(between 1 and 10)", 1, 10) nth = float(get_number("Nth root(between 1 and 5)", 1, 5)) solution = (x ** y) / pow(h, nth) print u"(%d^%d)/(%d\u221A%d) = %d" % (x, y, h, nth, solution) if __name__=="__main__": while 1: simple_math() print x = raw_input("Again? (y/n): ") if x == "n" or x == "N": break edit: accidentally did 2 spaces in front of the code instead of 4
I use Python as well, it's just that in my computer science course next year we'll be learning Java, so I thought it might be neat to check out Jython. I can get Jython working just fine if I install it in a different directory and navigate there, but I don't see why I can't place it in the PATH environment. If the Python installer (2.6, not the preinstalled 2.5) can do that just fine, why can't Jython?
Unixtime is always UTC. Only when you use localtime() it gets a time zone (the "current" one according to your environment, or the one set using tzset)
You probably need to chown /usr/lib/jython or chmod it correctly to be writable by your user or group.
You should have super user privilege to write data into /usr/lib so run the installer with: sudo java -jar jython_installer-2.5.2.jar After installation completed, don't forget to create a symbol link in your path: sudo ln -s /usr/lib/jython/bin/jython /usr/bin/ 
Thanks, I found a way.
Looks like you were commenting right as I was editing my post. Thanks! I tried your symbol link method (after un-doing what I had done) and that works too.
Hmm. I don't really understand what you mean, there's no reason why it should do that. Here's how I'd write `Pluto.bounce`: def bounce(self, listOfObjects): for other in listOfObjects: if self.rect.colliderect(other.rect): if self.rect.centerx &gt; other.rect.centerx: self.dx = 10 else: self.dx = -10 self.dy = -100
super() in Python 3 is the single most awful idea that made it into Py3000, IMHO. It makes *self* implicit for only special case, the use of it generates some magic bytecode and thus super can't be aliased.
Indeed, why no `object` built-in method `super`? (`type` can have it too)
You can still use the old syntax. I still apply the old syntax for example, because of cross-Python compatibility.
To quote the Zen of Python &gt;Special cases aren't special enough to break the rules. &gt;Although *practicality beats purity.* It's a simple solution, even if it doesn't perfectly adhere to explicit self. As for aliasing super, why is that useful? It sounds horrid for readability.
[Late reply but...] When I have a large software project, I try to stick with a platform for a while. With Ubuntu, I'd probably upgrade with each LTS or maybe every other LTS. If you have a commercial project that requires more frequent upgrades for hardware support or the like, then you'll have to be prepared for upgrades. In that case, hope that your unit tests cover the bases well enough to give you some confidence in the upgrade. I've seen dependencies break lots of code: glibc, Boost, mysql, gcc, .... even minor revs sometimes. Pick a lib and you can find some project that was broken by an upgrade. I don't think anyone would say it would be a good idea to keep multiple versions of core dependencies. 
game ran without any errors but you could use better graphics for the tiles.
There's probably no real use case for aliasing. It does, however, show how complex and unusual the implementation is. Whether or not a certain name(!) of a global function is used should never have an impact on the bytecode generated. Also, if you don't need *self* in this case, what's the point of having it in the first place?
`super(type, obj)` still works. The chief difference is now there's a closure added whenever a method references `__class__`or `super`: class Test: def with_class(self): print(__class__) def with_super(self): print(super()) def with_super_args(self): print(super(Test, self)) def no_class(self): pass &gt;&gt;&gt; Test.with_class.__closure__[0].cell_contents &lt;class '__main__.Test'&gt; &gt;&gt;&gt; Test.with_super.__closure__[0].cell_contents &lt;class '__main__.Test'&gt; &gt;&gt;&gt; Test.with_super_args.__closure__[0].cell_contents &lt;class '__main__.Test'&gt; &gt;&gt;&gt; Test.no_class.__closure__ is None True As a closure bound to the function, `__class__` references the class in which the function was defined rather than the instance class (i.e. `self.__class__`): class A: def __init__(self, **kwargs): print(__class__) super(A, self).__init__(**kwargs) class B: def __init__(self, **kwargs): print(__class__) super(B, self).__init__(**kwargs) class C(A, B): def __init__(self, **kwargs): print(__class__) super(C, self).__init__(**kwargs) class D(C): def __init__(self, **kwargs): print(__class__) super(D, self).__init__(**kwargs) &gt;&gt;&gt; d = D() &lt;class '__main__.D'&gt; &lt;class '__main__.C'&gt; &lt;class '__main__.A'&gt; &lt;class '__main__.B'&gt; Edit: In the comment section of the article someone mentioned doing the following: super(type(self), self) `super` needs the method resolution order from the object, plus a type from the mro. The above can cause an infinite loop, leading to a max recursion depth error. The type of `self` isn't going to morph in context. In my example above it would always be `&lt;class '__main__.D'&gt;`, so C's `__init__` would call itself recursively.
I assume you reported them. 
I use jython on OSX quite regularly, ie all the time and I have never installed into into /usr/lib. I install it into ~/bin/jython and add the install location in my .bashrc
Pythons OO, in general, is horrible. In specific, it is icky. The code is so verbose and full of boilerplate.
It's inelegant. Rename the class or copy it to tweak it (for whatever reason) and now there's a second spot where you have to change the name of the class. Also, add to that the confusion for newbies. Even trying shortcuts like super(type(self), self) This *seems* to work but of course it doesn't.
You have never used the python debugger. Nor do you actually know how to use gdb to extract a call stack out of a core file.
Is it? I don't have that feeling. Could you give an example?
Yes.
They were already reported if I recall.
Private tests are used to prevent users from hardcoding their answers. You can email your code, and the administrator will point out the problem in your answer. 
Well, if you know vim and want an IDE with vim-like key bindings... vim would be the first thought that comes to mind. Seriously -- if you've started down the road with vim, *stick with it*. Vim can integrate with iPython, do word completion, etc. What more do you need? You shouldn't search for an IDE for every programming language you want to use. Make a further investment in learning your editor of choice. See this old thread for more thoughts: http://www.reddit.com/r/Python/comments/6zzrt/ask_reddit_any_good_existing_vim_and_python/ With that said, I'd suggest that you junk vim completely and move to Emacs ;-)
I don't really "have to" stick to python, as I discovered yesterday from my adviser. In fact, it would be a bit more preferable to have something that can be made available to the cloud. I just have some experience with Python and Java (and prefer the former), so I thought it would be easier to do the initial coding in it, even if I end up porting it to something else. By parsing I mean splitting words into morphemes. When kids acquire a language, noone (except perhaps a language teacher) explicitly sits down with them and says: the root of "dogs" is "dog", and the suffix is "s"... they somehow figure this out on their own through exposure. Our goal is to model this behavior. As for the choice of language, my adviser has already been working on morphology in Russian, I am joining somewhat mid-process. I'm also a native speaker of Russian, thus I expect it to be a bit easier for me to come up with more complex cases once the algorithm gets sophisticated enough.
Thanks for the offer, dude! I'm afraid my adviser might not appreciate that, though. He's against volunteering and the funding I'm getting is not really much if split :(
I agree, when I learned of super() I was pumped to clean up some code. Then I realised that Py2.6 has the most half-assed implementation. I don't understand why it wasn't coded the same way as Py3k. I mean it just has to access the class's `__mro__` if I'm not mistaken. If I'm missing something epic, please let me know :P
thank you for share this! what days are you meeting in irc?
Deep learning is based on neural networks , and according to [1] surpasses other learning methods for a lot of tasks. it also learns features automatically so it much simpler( a lot of the work in learning algorithms for audio , video and text is around creating and optimizing features). [1] http://forum.stanford.edu/events/2011slides/plenary/2011plenaryNg.pdf
Yeah, vim can be made as powerful as an IDE, you can choose how you want it to do syntax highlighting for python, pygments or lint I think there was, then you can add code-completion, just google it its not hard to find. Most of it is just settings for vimrc and the rest is plugins. Because Python is interpreted you can just use it in a prompt as you code in vim, or with a more poweful one such as iPython. Now what more do you need? If you want a nice file-browser and a terminal to go with, use kate (and KDE4 becaue it kicks ass) and use vim as editor in kate. 
I once ran into a plugin that would allow you to run your code from vim with a keypress, but I don't recall it. I've used a debugger for vim too, but I wasn't impressed. Probably better to just use the pdb module. I would love to run into a new way to run script directly from inside vim, though. Currently, I just use tmux to jump between vim, ipython for help (it'd be nice to have this inside vim too, I'm not fond of pydoc), and a shell to run my scripts.
I don't know about the scientific part of your question, but I've been writing code in Komodo with Vim key-bindings for almost 5 years and it has been great.
Thanks! Looks good.
Before anyone suggests it: this is a dupe, but only kind of. The first time this was posted on http://blog.python.org, it got wiped out the next day by the big Blogger meltdown. Victor just re-published the post yesterday.
[Pida](http://pida.co.uk/) is worth a look.
Might I also recommend [weka](http://www.cs.waikato.ac.nz/ml/weka/) for a pretty nice little package which implements several learning algorithms. Very good for beginning to play around with [datasets](http://archive.ics.uci.edu/ml/)
There are usually a few people in the IRC at all times.
Couldn't you just use utcnow() and replace the relevant parts with zeros? from datetime import datetime as dt d = dt.utcnow().replace(hour=0, minute=0, second=0, microsecond=0) 
At best I want to stick with vim on a terminal emulator. However, for scientific computing, I need to be able to evaluate expressions in a terminal, without launching the whole script. It would be great to have vim in a console emulator, and an iPython console directly attached to it.
PyCharm. Best vim keybindings I've seen in an IDE, and it's a solid IDE besides. Not free, but worth it, imo. 
The reason I asked is because there are several terms that can be a bit ambiguous across traditions and also across groups within computational linguistics and language technology people. I wonder if taking a look at [Morfessor](http://www.cis.hut.fi/projects/morpho/) would be any use to you. It uses a statistical means to produce a morphological breakdown of words. I imagine statistics may be a part of child language acquisition, in that children can't acquire some things without having enough data. Of course, I'm not saying it's all statistics, but I'm sure that plays a role. Morfessor of course, is different from rule-based morphological analysis, which in a way relies on a smaller set of data to produce these results... In the last day I was looking for something unrelated, and found a Quechua morphological analyzer that uses Python, but I haven't yet delved into the source to see what it does exactly. Documentation [here](http://www.cs.indiana.edu/~gasser/L3/l3morpho_guide.pdf) though. Looks like it doesn't break words up into individual morphemes at identifiable morpheme boundaries, so much as provide an analysis of the morphosyntactic function of the words. The project you're working on sounds really interesting though. If you'd like to keep in touch, feel free to send me a DM. :)
&gt;However, for scientific computing, I need to be able to evaluate expressions in a terminal, without launching the whole script. In Emacs with python-mode, I can hit C-c C-r to evaluate a selected expression/region, C-M-x to evaluate a function, etc. I often keep a scratch buffer in python-mode that I can toss snippets in &amp; evaluate in the inferior-python mode, which shares state with my main Python file. And Emacs integrates directly with pdb! I say this because vim -- though definitely not my favorite! -- is an *extremely* powerful editor. And if Emacs can do those things I just mentioned, I'd bet my house that vim can as well. Don't immediately look for a new editor, or else you'll end up like so many programmers I know: using Eclipse for Java, Textpad for PHP, X for Y, and hating all of them. Instead, try to learn one editor well.
Another vote for PyCharm. PyCharm+IdeaVim is great, if you're really looking for a full-fledged IDE. Granted, nothing is better at being Vim than Vim, but I find that it implements enough commands to be very usable.
Not really. When you do replace the returned datetime is in EDT.
Get the [gdata-python-client](http://code.google.com/p/gdata-python-client/downloads/list). Here are the [installation instructions](http://code.google.com/apis/gdata/articles/python_client_lib.html). And here's the Python developer's guide for the [Calendar Data API](http://code.google.com/apis/calendar/data/2.0/developers_guide_python.html). And finally, here is the info for [OAuth authentication](http://code.google.com/apis/gdata/docs/auth/oauth.html), including Python sample code.
The db library will handle quoting/escaping/etc. for you in the second example. You're not going to be inserting a string literal into the query, it's going to be a variable that won't always be as safe and sanitized as 'Fred'.
http://en.wikipedia.org/wiki/Sql_injection
Online python tutorial requires silverlight. Nice troll.
Oh you're right. My first example won't even work because it won't have quotes around Fred. But if Fred was a variable in the first example, I don't see anything worse happening than ending up with invalid sql syntax that'll raise an exception. Is that the only issue to be worried about?
If the variable is coming from user input, it could be malicious. Read about SQL injection.
SilverPython ;-)
Each sprite you want to check for collisions should have a rect attribute, usually derived from the image attribute. Something along these lines: self.rect = self.image.get_rect(center=(self.px, self.py)) (And remember to update your rects as the sprite moves!) Then check out the docs for pygame.sprite.spritecollide -- this will pretty well cover it.
That explains it. Thanks.
Question answered. Thanks.
From vim you can run the current file through python with: :!python % When I develop I keep vim on one monitor and next to it the python interpreter. I'm not really sure how much more you could hope for exactly. When you're doing work with the interpreter you're basically entering in one line at a time, so you're actually spending most of your time in what would be insert mode anyway, I'm not sure how being modal would greatly improve things.
[Obligatory XKCD cartoon](http://xkcd.com/327/)
'cos it's a lot of f'n work, and you shouldn't have to care what your config mgmt system is written in. That said, I'm designing/prototyping one right now. :)
I'm confused about this. &gt;paramstyle &gt; &gt; String constant stating the type of parameter marker formatting expected by the interface. Set to 'format' = ANSI C printf format codes, e.g. '...WHERE name=%s'. If a mapping object is used for conn.execute(), then the interface actually uses 'pyformat' = Python extended format codes, e.g. '...WHERE name=%(name)s'. However, the API does not presently allow the specification of more than one style in paramstyle. &gt; Note that any literal percent signs in the query string passed to execute() must be escaped, i.e. %%. &gt; Parameter placeholders can only be used to insert column values. They can not be used for other parts of SQL, such as table names, statements, etc. Also the examples in the docs use the % syntaxes.
I worked on one at my company (available on github called enki_cloud) that worked as a config database for deploying nodes using xen. Not sure if it fits the bill, but it's also in ruby/rails. I was working on one in python for a personal project, but m0j0 is right. They're a lot of f'n work... :\ 
Puppet is Python, I believe. 
try [buildout](http://buildout.org). there should be configuration recipes on pypi
&gt; the ease of decompilation is always brought up. why is this a big deal ?
It's ruby.
no, pycurl
Upvote for username alone.
Any time you "build code" (whether it be HTML, SQL, etc.) you must quote and escape any inserted data. The probability that you will get that wrong is very high, so you will be best protected by using library functions.
Wouldn't this work in python2: super(self.__class__.__name__, self) (I know its ugly... But it does the desired decoupling...) Edit: must be super(self.__class__, self) As pointed out by spoolio...
yeah, would have appreciated a warning. i need to go shower now. 
Actually, I was being imprecise. You just give it the class. So the invocation that works is super(self.\_\_class\_\_, self). Which is hopefully the same as super(type(self), self) which turns out to be what the linked article suggests. I'll still just call the superclass method by name, because Python code is supposed to be readable.
Cuisine looks really promising http://www.slideshare.net/ffunction/fabric-cuisine-and-watchdog-for-server-administration-in-python (starts at slide 42) [github link](https://github.com/sebastien/cuisine)
If you find yourself working on some perl code, you'd probably be happier running it through perltidy first.
Puppet and Chef are both great tools. They're easy to use, (fairly) easy to get started with, and play nice with apps written in Python (for example, there's an official Opscode recipe for Django). What would being written in Python give you that's not already there? Put another way: I think Python people tend to be fairly undogmatic when it comes to tools. We're happy to use the right tool for the job.
We use bcfg2: www.bcfg2.org Not quite as popular or widely used as the others, but it does the job pretty. Probably not the best choice if you are XML-averse though. 
If it's in a language you already know and use all the tome, it's easier to hack on and script. 
there is [kokki](http://samuelks.com/kokki/) - I've looked into it briefly and it seems fairly similar to Chef. Not sure how robust it is though (haven't tested it out yet).
Aside from the security point of view that others already mentioned, using parameterized queries is also faster, or at least it makes it easier for your database to be faster. Many database servers work internally by compiling your query (after all, SQL is just a language) to an execution plan, which then acts on the database with the given input parameters. If you resend the same query, but with different parameters, the query will only have to be compiled once, and can then be cached, whereas if you construct and send a new query each time, the database server will have to parse, recompile et cetera the query each time (Well, it might be smart about it and cache it anyway to some degree, but it's not that easy). So using parameterized queries will give you great speedups especially if you have frequent small queries, queries in a loop, et cetera. Also, if you insert parameters using string substitution mechanisms, you might end up doing a lot of unnecessary replacing (like escaping characters) so you'll end up having tons of extra characters in your database, which then again has to possibly be un-escaped when you retrieve it. More data to send, more data to process, more data to store, more inefficiency.
You should crosspost this to [/r/programming](http://www.reddit.com/r/programming/), since this is more of a general methodology question rather than a Python-specific one. You'll get a bigger audience and therefore people who know what this mathematical witchcraft is all about. Your concept looks sound, though. It's a big enough undertaking (with expandable potential) to make it object oriented rather than a quick functional program. &gt; I want to make a "map" to determine which section a recorded sound falls into You could probably get away with just appending to a list, if all you need to do later is check if it's in some section. If there aren't too many sections, you can make a function that has an elif party that eventually returns the relevant vowel and performs the necessary appending. Otherwise, you have to do more clever things with Python.
Bcfg2 is Python.
I get the SQL injection problem, but this point about quoting and escaping still isn't clear to me. I know string parameters inserted into SQL will need quotes but that isn't hard to accommodate. Numbers can be dropped into sql without problems. But I'm not sure what's meant by escaping. Another issue I'm having is that you can't parameterize a table name, so if the tables used in a query are determined on the fly, you have no choice but to build the sql using string functions.
Didn't know that, thanks.
http://en.wikipedia.org/wiki/Comparison_of_open_source_configuration_management_software 
Hey, is Django still OK for this?
HAHAHA... good joke. 
[r/DSP](http://www.reddit.com/r/dsp) might be able to help.
/r/programming doesn't allow self-posts, but he might have some luck with /r/learnprogramming.
"Escaping" is the name for how you pass special characters to your DB. Think about it. Let's say you wanted to store a " or a ; or a * or something like that in your DB. How would you do it, since those things have meaning in SQL language itself? The answer is by "escaping" the string, so that those characters don't interfere with the query. Basically, you precede the illegal characters with another character that tells the DB, "Hey, don't treat this next character as a command, just as a string." Escape characters can vary wildly from system to system. Even within Python "\n" means "one newline character" but "\ \n" means "a slash then an en" and if you do % string interpolation %% means "just an ordinary percentage not a string interpolation." SQL has its own standards for escapes. However, there are a lot of things that go into properly escaping a string and if you're not careful, you'll do it wrong, so just use the library function and have them do it for you.
I've spent the last 18 months working with speech/audio processing and Python. It is incredibly difficult (the speech/audio - not the Python). You'll likely want to do more than just an FFT - in particular various forms of filtering, noise reduction etc. Numpy has that functionality as far as I can tell but I haven't used it. Some other engineers were matlab folks so I would run a copy of their code in GNU Octave which output the processing result and then do the final algorithmic matching in Python. (Octave and Matlab are fast at vector operations but eye wateringly slow at normal programming constructs like for loops.) I captured the processing results getting them to dump the array output as regular ascii, captured via the subprocess module and parsed in a few lines of Python. My advice would be to optimise for debugging and analytics. Your code is not going to work so you will need to visualize what it has done, tweak and try again. Get that process working well and you'll get somewhere. In any event having lots of nice pretty colour visualizations will get you better grades.
Just wanted to say thanks for the book. Been dabbling with python, and look forward to reading it. Keep up the good work. 
This. And you should spend sometime with a machine learning text book. GL 