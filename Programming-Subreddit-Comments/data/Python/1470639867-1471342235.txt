At worst it's a minor think you have to think about for one second. I've never saw any dev having problem with this, I met, however, plenty of them having trouble with i18n, relative date calculations and bad APIs.
You're right, that's better. Otherwise you have to again later check for the capitalization.
This is going to be a weird analogy, but the standard library is like everything before iron and diamond tools in minecraft. Not the best but serves a purpose..led to other things. A means to an end. I'm no Python expert but that's certainly what it seems like to me.
Doesn't requests make the same assumptions? What happens if you get a 404 or no JSON back?
I actually got frustrated enough to make a stab at doing something about this: https://github.com/unixpackage/unixpackage I have more faith in people helping out with the maintenance work on a project like this than I have of the pip team deciding that it's actually a problem with their software.
I'd suggest trying some practical ML challenges via Kaggle (start with the Titanic competition) to practice coding, data munging and ML. Also look at PyData/SciPy/EuroSciPy videos, there's a lot of free and very useful content e.g. here: https://www.youtube.com/user/PyDataTV Try to find a local meetup. I'm guessing you're in the US, in the UK we have http://www.meetup.com/PyData-London-Meetup/ (I'm the co-chair) and we meet every month with 200 data scientists. Related events run in the US and throughout Europe. Finding like-minded folk to talk to really helps. Good luck!
Yes, it's worse. It doesn't tell you what kind of request you just made, it's not easy to change that (do a POST, etc.) or start adding params.
I wish they will upload the event videos to YouTube. I really want to watch Armin's talk.
Better solution, don't use factory pattern because it's useless.
I don't think the (abstract) factory pattern is generally useless in Python. But I think it works quite differently to what you might be used to from Java/C++. In general you don't need more than a couple of dictionaries holding the classes you would otherwise put in you abstract factory class. Another option is to choose which set of classes to use at import time and populate a module with them. You're then bound to what you've chosen once, but that might work if you have to choose some kind of backend only once during your application life time and you're not going to change it afterwards.
Great idea! Will try to split my lib onto two. But what is way to pass events back to «io worker»? Should this have some standard? Something like tuple event, extra-data-dict. Any suggestions?
I can totally see `requests-woopdeedo` landing in PyPI in the next few days.
Just my $0.02 based on my previous work. Lots of answers here are some variation of "it depends". In general, if you're starting a project from scratch, you don't need to focus on many of those details unless you find you they fit your usecase. One of the more important things though is the ability to configure where a log goes based on the command line flags. &gt; what level should I log to by default (INFO, WARNING, ERROR or CRITICAL) WARNING is for messages that signal the potential precense of a bug/error/exceptional condition. For example, a network timeout was triggered so the program is retry-ing. ERROR is for messages that signal a definite, but not fatal, bug/error/exceptional condition. For example, program can't find a file so it can't proceed. CRITICAL is for messages that signal a bug/error/exceptional condition that will cause the program to die right now. For example, KeyError's or something. There's a bit of a grey area between ERROR and CRITICAL if the response to the ERROR is that the program would quit. Generally though, ERROR is for things that could potentially be recovered from. So not finding a file is potentially recoverable if you use default data (which you might do in the future), but KeyError may not be. &gt; should I log different levels to different locations I don't because its too much work usually. Many people do and say it helps. &gt; should I log to /var/log or a custom relative path directory The answer depends. If you were making a unix daemon you might but if you were making a statistical script you probably shouldn't. &gt; should I use syslog Same answer as previous. &gt; what about interleaving program output (aka print) When opperating in console mode, that might be fine but ideally you need an option to control putting log output and stdout in different files. &gt; should I use a rotating file, or should I start a new file each run Depends on what you're making. If its important to you that you always have a record of what happened when you ran your program then you should not use a rotating file. This can happen if you're making software for a scientific software where reproducability is important, then you really need the logs to be able to trust the output. You may want to log things like the version of the software and the full date and time. If you're making a daemon that will always be on and generates large logs, then using a rotating file would probably be better so you don't start using tons and tons of disk space. If you're making a small console script that will be run many times and your end-users aren't expected to fiddle around with logs everytime they run it, then maybe a rotating log is good. If you're making a library, then you should leave most of these things up to the caller unless you have a good reason not to. EDIT: Don't print anything but ERROR's or above in unittests though, that shit is annoying. 
Probably. Despite using Django at work, we actually don't use the ORM so I don't have a lot of experience with it. I do know that Django uses the Active Record pattern, so you might be running into that if you're just doing `model.save()`, especially if you're inserting relational data. Each save is a request to the database (not necessarily a connection). Apparently there's a `bulk_insert` method. The Active Record pattern is actually one of my least favorite things about Django's ORM and I feel SQLAlchemy got it right by going the unit of work path. [Here's a SO question that could help you.](http://stackoverflow.com/questions/4294088/accelerate-bulk-insert-using-djangos-orm) 
When I think of scheduling in Python, I pretty much always think Celery Beat. It does scheduling for you, basically allowing you to just write your function and let it be called via celery.
First of all, I really like the idea of splitting up parsing and IO. That seems to me to be a really good idea. The upside of better testing by itself makes it worthwhile. What I'm not so sure about is that it's good and/or worthwhile to try to come up "the-single-HTTP2-parsing-thingy-to-rule-them-all". What people are liking about Requests, vs. lower-level libs, is the API. Right? In the linked video, Cory is using the "API" a lot. My interpretation of his way of using it is that it's the interface between Requests and e.g. Django or between Requests and "the user". While he does acknowlege that there is an interface between the HTTP-parsing-thingy and Requests as well, he kind of glosses over that that interface also needs to have a good API. The author of Requests is "the user" w.r.t. the HTTP-parsing-thingy. It took a pretty long time and a pretty good designer to come up with the beloved API of Requests. I don't really get why Cory seems to think that the HTTP-parsing-thingy is going to get it's API as good on the first try. The question /u/pohmelie asks seems by it's nature to hint at this problem too. Personally, I think that while, again, the separation of parsing and IO seems great, the community would still benefit of having a couple different designs on e.g. HTTP-parsning-thingies and their APIs. Over time that plurality has the potential to produce the Requests of HTTP-parsing. 
Requests only gives them that if the data happens to be in JSON. A well-designed library would work equally well with any data format.
That's because JSON (apart from being the overwhelmed fan-favorite, which in itself is usually ground for specific syntactic sugar) can be mapped into straightforward `dict` objects with zero controversy. What other http-serializable format can do that? Any XML dialect is off the table, for example.
And /u/nerdwaller was talking about scheduling posts on _anything_, not just Instagram. 
Hi, I'm Cory. =) Good thought! &gt; Personally, I think that while, again, the separation of parsing and IO seems great, the community would still benefit of having a couple different designs on e.g. HTTP-parsning-thingies and their APIs. Over time that plurality has the potential to produce the Requests of HTTP-parsing. Sure, it would, but I don't think it needs to. Requests and things like it get good value out of having great APIs because they are used by huge numbers of programmers, many of whom are novices, or who don't understand the problem domain in any depth, or who are fundamentally not interested in solving the problem that the library solves (e.g. HTTP). Those programmers get a great deal of value out of APIs that are expressive and flexible, allowing them to write lots of code very simply and without getting in their way or requiring them to think too hard. The parsing layer suffers from this much less. Frankly, most programmers should never have to even *see* the parsing layer. hyper-h2 right now is up to about 30k downloads a month, but most of the people downloading that library have no idea that they're using it. This is very much by design. I don't want the average user consuming hyper-h2, because by itself it doesn't *do* anything. It moves some bytes around in memory and consumes some CPU cycles: that's it. It needs an I/O layer to do anything. And given that I've made someone else write an I/O layer, it doesn't seem unreasonable to make someone else write a great API *either*. More importantly, anyone writing the I/O integration to the parsing library kinda has to understand the problem domain. If you don't understand HTTP/2, at least at a high level, then having a parsing library isn't going to help you that much. You still need to work out how the parsed information translates into the *semantics* of HTTP: how seeing a content-length header works, what to do when a PRIORITY frame is emitted, how to handle stream termination. These are all decisions that are beyond the scope of your basic parsing library. With this in mind then, I think the priorities for libraries of this nature are different. For the high-level libraries that novices and non-experts interact with, API is king: having a great API allows you to get away with a huge number of sins. But for low-level parsing libraries, the API is less important than feature support, correctness, and performance. Certainly I don't object to having multiple implementations of the parsing libraries. However, I think that unlike with the higher-level ecosystem where we can support multiple libraries that do the same job with different interfaces (requests, aiohttp, etc. etc.), with the parsing layer there is really only room for one great implementation of each protocol. Any time a better implementation comes along, it will rapidly eat the lunch of the lesser implementation except in small corner cases.
So there is certainly an interesting question around exactly how these should be structured. For h2 and h11 at some point we want to try to commonalise the events to make code handling both protocols a bit simpler, but for now it's not a big deal for each library to carry their own around.
Maybe it depends on how it's done, but I don't think anyone would be forced to acknowledge the possibility of failure no matter what. The return type is not part of the call signature, so it still comes down to documentation. Python just isn't Haskell, and doesn't enforce solid programming like acknowledging errors. Don't get me wrong, I think Haskell is great and python isn't perfect. I don't see passing an I/O error through the return value really ever being a good idea in Python. But I'm about to go look up monads in python. Edit: for example, I found this [maybe monad in python](https://gist.github.com/senko/4b09ad285340c8ec2661), which could just as well be and Either monad for exception handling. In Python, one would have to check the documentation to know what errors it might pass and how, and once one knew a function returned Right(Dict) and they could treat it like a dict without anything special, then they could just ignore the possibility of a Left(Error) if they were so inclined. This doesn't seem any better to me, and because it is unfamiliar it seems more prone to misuse.
Programs like py2exe, py2app, cxFreeze, etc aren't perfect. They work by taking your code, scanning through it to see what libraries are needed, and then they attempt to bundle your code alongside a standalone python interpreter + all the system libraries required for it to function. However, often they miss certain files. In this case your program isn't able to find python35.dll which is required for your interpreter to function properly. Try doing a search for python35.dll in your OS, when you have located the file make a copy of it and move one copy to the folder with your code that you're trying to package into an exe. That will generally solve the issue. You'll need to re-run the packaging process, and you may need to put the .dll into a subfolder within the directory of your code; I don't recall which one.
Pickle is a terrible, unsafe protocol so "uncontroversial" that the format changed quite consistently across Python versions. If you send pickle over http, you are a bad person and you should feel bad. The others are equally controversial, as proven by the fact that there is no parsing lib for them in stdlib -- and none of them is remotely as popular as json. They are perfectly served by .text or .raw coupled with whichever parser you choose. Honestly, you are clutching at straws. Requests is popular because it makes easy tasks trivial (getting json or text over http) and difficult tasks possible (any other exotic format); as such, it's well-designed -- certainly compared to anything previously available in stdlib, or it wouldn't have gained such massive popularity. If you disagree, feel free to build a superior lib :)
[Celery Beats](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html) is a module. It will work with Python.
This may or may not help, but have you looked into splinter? https://splinter.readthedocs.io/en/latest/ It simplifies selenium deployment. There may be something in there that's better presented that may help with what you're doing. Good luck!
&gt;Pickle is a terrible, unsafe protocol so "uncontroversial" that the format changed quite consistently across Python versions. If you send pickle over http, you are a bad person and you should feel bad. In general yes, but that's not the point. &gt;The others are equally controversial, as proven by the fact that there is no parsing lib for them in stdlib Nonsense. Quality and popularity are pretty much independent (see: Javascript). &gt; -- and none of them is remotely as popular as json. They are perfectly served by .text or .raw coupled with whichever parser you choose. Sure, but so is JSON. &gt; Requests is popular because it makes easy tasks trivial (getting json or text over http) Getting JSON over HTTP *is* trivial. All Requests does is letting you put the deserialization call at the end of the line. &gt; as such, it's well-designed -- certainly compared to anything previously available in stdlib, or it wouldn't have gained such massive popularity. Again: Popularity is not a sign of quality.
I'm a Unix systems guy, and I've got plenty of opinions here too ;-) ## INFO Used when your app is running normally, and everything is dandy. It's the default, should be the default, and shouldn't drown your logfiles. - Should be low-traffic ( &lt; 100 messages per hour or so ) - Should always contain "I have started succesfully" - Should always contain "I'm shutting down on user request" - Should mention it's configuration file ( if several are possible ) - Should mention if it succeeds with one-off tasks ( Signed into database ) ## DEBUG - Your sysadmin is currently debugging something. Give him all the info he needs - Your system runs slowly, give _timing_ data on how long things took - Don't log credentials (passwords) but do log that you've logged in, who logged in, and so on - A debug notice for every thread that starts up is perfectly fine ## NOTICE (Warning) Should be tooling important things. Imagine things that a sysadmin wants an alert about. Someone shut down the service (that's supposed to be up?) networking issues, bad permissions, etc. - Important exeternal events, "DB connection lost", "connection reset, reconnecting" - Signals from the OS/users ("Shutting down on user request") - Always log _starting_ and _stopping_ commands &amp; reason ## ERROR Exceptions, proper debug stack of why everything is exploding, and so on. This is where you throw the stacktrace and fuck off. This is where you berate the user for having world-read permissions on their TLS key. ## Logging: File versus Syslog. Syslog should be the default. Big systems have syslog daemons that will log to remote servers and run nice analytics on them. Small systems have syslog daemons that keep messages in a sorted memory buffer and toss them out. Syslog is the standard logger ( Unless you use systemd, when journald is syslog, on steroids. Remember what we've said about doping and steroid damage?) File log: File logging is for when you can't log to syslog. When you have file logging, you should enable a signal to rotate your file (for logwatch ) or manually doing it. I generally dislike it when software logs to it's own logfiles, and in a modern system, it should either be to STDOUT/STDERR, or to syslog. Logging to a file can be useful for some, but most of the time, it's not. Logging to file brings interesting risks as well ( overwrite, permission related attacks ) if you aren't careful with how/Where it's logged. 
Oh cool, that's good to know!
Thanks! I actually just came across a meetup link this morning. I'm in NYC so I think there are lots of options.
The difference is that shell=True means that is executed with something like sh -c "/the/executable parameter parameter" This launches 2 processes for every call and would be a security leak if you used any user input for the call. Also if my memory is correct then the specific shell that is called is dependend on the user that you run your script as so that can intoduce subtle bugs when the shell is something else than you expect. In your case you use the str.split command to seperate the pieces in your command. It is safer to build your commands as a list in the first place. This will make sure that everything you push to the shell is properly escaped. command = ['ls', '-lah', 'some file to delete'] This will actually remove the 'some file to delete' file instead of 4 seperate files 'some' 'file' 'to' 'delete' https://github.com/pcote/AptPackageShow/blob/master/roles/aps/files/model.py#L38 is an example of a command that can go horribly wrong with your current code.
You might be able to get some help at /r/computervision Python specific I know scikit image should be able to do this, but I haven't used it in a while. http://scikit-image.org/docs/dev/auto_examples/ 
Yeah but why not pip? Or is pip only for packages on PyPI?
&gt; Quality and popularity are pretty much independent (see: Javascript). Javascript is a monopoly, it has nothing to do with this conversation. Popularity is independent of quality when other forces are at play (like marketing from vendors or de-facto monopolies). That's not the case for libraries in PyPI or stdlib, which are all on an equal foot. `Requests` is popular because it's *good*, something that other http libraries clearly are *not* in equal measure. JSON is popular enough to have a stdlib module because it's *good* (or at least is *good enough*) more than the other formats. The Python community excels because it favours *actual quality* over architecture-astronaut "quality". In any case, as I said, feel free to write your superior version, I'm sure everyone will use it. &gt; All Requests does is letting you put the deserialization call at the end of the line. No, `requests` lets you see in a heartbeat if that's a GET or POST, as well as pretty much ignore if it's an http or https call and so on and so forth. *On top of that*, it makes it trivial to get a dict when calling a json resource. The equivalent effort in other libraries would be extremely verbose. *That* is why it's good. End of story.
You don't need a shell for environment variables. You have some other problem. The first fix most of the time is adding a 'home' setting to wsgi (I use mod_wsgi in apache, not uwsgi). By default the script is running with / as the working directory instead of the script directory. Also see the flask wsgi troubleshooting tips: http://flask.pocoo.org/docs/0.11/deploying/mod_wsgi/#troubleshooting
I can add that specific protocols may involve their specific notations of "events", for example, AMQP uses "frames".
Pip's main use is for installing packages already built and put on PyPI. However you can use Pip to install a package from source (via setup.py) or from a git repository URL where setup.py is in the root of the repository. The setup.py script is much more than just installing it. It allows you to build packages for various environments (source tarball, python egg, python binary wheels, RHEL RPMs, etc). You can also use setup.py to build C/C++ python extensions in your package, but this can be a complex topic. There is also a feature provided by setuptools to install your package in development mode (`python setup.py develop`) which allows you to modify code while the package is still "installed" in your python environment. I usually consider setup.py to be on the developer side of python packaging and pip is on the user side of python packaging. Try cloning a git repository for a python package you would like to use and running `python setup.py --help` and go from there. The typical use case when installing a package from source is: python setup.py install For creating your own you'll need to research.
 In[1]: import requests In[2]: requests.get('http://google.com/404').json() ... JSONDecodeError: Expecting value: line 1 column 1 (char 0) In[3]: import urllib.request as request In[4]: import json In[5]: json.load(request.urlopen('http://google.com/404')) ... HTTPError: HTTP Error 404: Not Found `requests` ignores the 404 and attempts to parse the page as JSON anyways, `urllib` raises an exception at the 404.
Are you using the most current version of Spyder? I'm using it on a 4K display and the scaling seems fine. What elements aren't scaling properly? (icons? Text? Toolbar?) 
Is the tool's usage intended to be selecting (or iterating through) possible combinations of values and trying to match a known hash value? 
See the new docs with the latest release at https://pythonhosted.org/pyparsing/
Yes! [kite](https://kite.com). Looks like fun.
Not that I want to discourage you but I guess there are already such tools: http://unix.stackexchange.com/questions/19008/automatically-run-commands-over-ssh-on-many-servers From looking at the screenshots I would say it would be nice to be able to create groups of hosts (web servers, dns servers, backend servers etc.) and apply ssh commands to these. Your list of hosts looks like it cannot be extended and filling in hostnames and IP addresses every time is cumbersome.
If you can upgrade to PyQt5, that should solve a lot of the high dpi display issues. Qt didn't officially support dpi scalling screens until Qt 5.5.
Thank you! you're a godsend. Im attaching an imgur album of images of spyder and my qt program on the 4k monitor and what they looked like before http://imgur.com/a/EQvlF
I've tried downloading the most recent version of winpython and running spyder straight from that, as it has spyder3, qt5, and pyqt5. Still no luck
Another tool for quick and simple REST API: https://github.com/timothycrosley/hug
I am seeing that you are using QtDesigner to develop the UI. First things to check is the QMAINWINDOW font size. Decrease the size of the font and tested again. I had the same issue when developing my program. I develop the user interface without QtDesigner and have better control on the UI. Provide some feedback after the change. Are you using sizers in the UI?
/r/titlegore
Yes! Thanks so much. Too bad it's not compatible with ipython notebook.
That's pretty easy and useful but I think you can do the with C++ too, I just don't know how. 
&gt;&gt; Quality and popularity are pretty much independent (see: Javascript). &gt;Javascript is a monopoly, it has nothing to do with this conversation. Popularity is independent of quality when other forces are at play (like marketing from vendors or de-facto monopolies). That's not the case for libraries in PyPI or stdlib, which are all on an equal foot. `Requests` is popular because it's *good*, something that other http libraries clearly are *not* in equal measure. &gt; So the logic goes like this: 1. Requests is popular 2. Therefore it must be good 3. Therefore people use it, and because people use it.. 4. Go to 1 Popularity may not be statistically independent is quality, but as an argument it's still useless. &gt;JSON is popular enough to have a stdlib module because it's *good* (or at least is *good enough*) more than the other formats. JSON is in the standard library purely because it's popular. Or do you think anyone even considered the merits of the gregorian calendar when `datetime` was added? Not that there was anything wrong with JSON. But it's not the only possible serialisation format and it's not particularly special, so it doesn't need to be special-cased. &gt;The Python community excels because it favours *actual quality* over architecture-astronaut "quality". If you think separation of concerns and loose coupling have nothing to do with "actual" quality, I don't want anything to do with the code you write – or the APIs you design. &gt;&gt; All Requests does is letting you put the deserialization call at the end of the line. &gt;No, `requests` lets you see in a heartbeat if that's a GET or POST, as well as pretty much ignore if it's an http or https call and so on and so forth. That's cool I guess. Honestly, once I saw the `.json` method I went back in disgust. &gt; *On top of that*, it makes it trivial to get a dict when calling a json resource. Getting a dict out of JSON *is* trivial, even without hacks. &gt; The equivalent effort in other libraries would be extremely verbose. Multiple people have posted the equivalent in other libraries. It's slightly longer, yes, but not "extremely verbose". 
I've never used spyder so I'm not sure. If it is a PyQt or Qt app and open source you could always add the line yourself and see if it works. 
They can always ignore the error itself, but that becomes them actively deciding to ignore the error rather than passively accepting success in the normal case. Ignore proper monads for a second and just take a tuple example: Result = namedtuple('Result', ['success', 'result']) def function(x): if x: return Result(True, 4) return Result(False, None) In order to use this function and get a successful value out of it, you need to do "`_, x = function(4)`" at the very least. 1. Makes it part of the function's signature, in that its returning an object where a portion of the return value is the success or failure status, rather than just the value 2. means that to get the successful value, they need to unpack the tuple. At that point if they ignore the success or failure its a client decision, and you deserve what you get. For happy comingling when using exceptions, you need cooperation from the function writer (they need to (and they should) know what exceptions their code can call, and they need to document it (which I find often is not the case)), and from the function's user (obviously through catching the exception). If either party doesn't do their part, you have problems. Doing something like my above example *forces* both parties to do the right thing
The setting has to be done in QtDesigner or in code. You also need to check the font size of QMAINWINDOW in QtDesigner. 
I generally agree with the other 2 comments, but wanted to add 1 point: convention/consistency. What is/are the convention(s) for logging in your workspace? For example, all of our servers log to their own file in the same log folder with &lt;their name&gt;.log. Convention is often one of the most significant considerations... until you're trying to change it of course!
I don't see me using your program as I'm not on Windows. Linux has enough of it's own ways to get my things done. But in the end, it's not so much about if anyone will use your script, I guess. The important part is what you learned from it while writing it. Anyway, my feedback from what I could see from screenshots: * Add the ability to use a ssh key to authenticate instead of a password. * Why not make it open source so people can take a look at what you are doing before entering their credentials to opening ssh connections to their servers. * Instead of having a hardcoded list of 10 servers, make these rows a dynamic list that can be extended. Start with one server and a button to add a second row. * Save once entered data to a local database and reload it on the next start. 
&gt; So best practice and advice on the internet suggests that we should use the standard library's logging module I prefer [Logbook](https://github.com/getlogbook/logbook). &gt; how to you or your work collect output from the logging module [Flume](http://flume.apache.org/) -- but I understand this is overkill for most of usecases.
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
It doesn't really change much...I think I may have stupidly set the font size for each individual text piece and that is over riding the QMAINWINDOW font? I also am not even sure what "sizers in the UI" means, sorry
Twisted (https://twistedmatrix.com/trac/) has been doing this for years..
No it hasn't. =) Every time you see `transport.write` somewhere, Twisted has shoved I/O into its parser. That call encodes so many I/O assumptions that it's not generically portable, not least is that the I/O won't block. As an example of how *not true* that is, I encourage you to rewrite a Twisted reactor using purely blocking I/O and see how that goes. On top of this, if you see any reference to the reactor or to Deferreds, that has once again encoded certain assumptions about control flow into the protocol that are *entirely* unneeded.
&gt; In your case you use the str.split command to seperate the pieces in your command. It is safer to build your commands as a list in the first place. This will make sure that everything you push to the shell is properly escaped. Agreed and updated. https://github.com/pcote/AptPackageShow/commit/a701c448f07a6bb86a1ffbf107b2abafcd8f78aa 
I guess you're right when you get down to it... I always did everything through deferreds, and I vaguely recall that I had sync'ed and async'ed data sources, and it all just worked for me (this was about 10 years ago).. I dont think I had transport.write() in my protocol stuff.. Maybe I misremembering... 
Thank you for the awesome feedback! * I wrote this program to be able to be compiled for any OS but I haven't done that yet. * I should make it open-source. I think that would help make the product better. I think this is a great idea. * I definitely need to make a dynamic list that can be extended. * I do have the option to save entered data into a file to be reloaded at start. Thank you very much, this is great feedback! :)
an incredibly fast answer to a very specific question, thanks a lot
I'd say that urllib is doing it the right way. If I get a 404, I want to know its a 404, not have it get swallowed up as a JSON error.
I would personally never use this. 
Thanks, i've just added the x64 to it but now it tells me Fatal error! failed to execute script. I've checked my program severally and it works fine.
Right, so a scheduler schedules a function to happen. What's that function do? Posts on Instagram. You need to break programs into logical units: e.g. Write something that posts to Instagram. Then make that post be dynamic content based on your criteria, then schedule that thing happening. When you become a more senior developer you can better align those all at once verses chunking them up, the the thought process throughout is the same.
That sounds like a lot of overhead. What's the benefit?
The thing with PyQt is that you have to set the font size before installing the components on the UI. Otherwise you have to set it for each component if changed after the fact. You can still redo the UI, set the font size then put the different components last from my own experience. 
Make sure your logs are machine parsable.
As someone who who has to maintain/develop on a few Plone sites - yes, unfortunately Zope is still a thing.
Python is still one of great languages for beginners to start out and for machine learning.
Pretty sure most developers have switched to better alternatives by now. Everything about Zope feels outdated and clunky.
print ("192.168.2.0") Done... :)
thank you :)
Managing developers.
Your examples just use integers and not actual box names. Where are the box names coming from? How are passwords handled? What about encryption? I guess bigger picture, what is your code even trying to do? There are no external calls, so it seems like a lot of code to check to see what machines are available. What I'd assume you have: box_names = ['box1', 'box2', 'beast_box', 'amazon'] # not all names follow the same pattern encrypted_passwords = ['cat', 'dog', 'cat', 'cat'] user = ['joe', 'joe', 'joe', 'bob'] With that you can query if a box is down or not as well as if the box is doing somethign. Then just pick the first available one to run your job.
From just scanning your code, here are the things I'd change: - You should adhere more closely to PEP8, particularly with regard to whitespace around operators and commas. - `parse_hostname` and `next_server_number` are private methods, and generally private methods start with an underscore: `_next_server_number` - It's nice that you have tests, but the tests would run every time your code was imported. If you are going to put tests in the same file as the main code, accept an argument from the CLI. For example, `python tracker.py run_tests` - They provided some example interaction, and your code doesn't work the way they expect it to. There's no `new` method in your Tracker class, and `allocate` doesn't print the newly created server name.
From a performance standpoint, the code could be improved in a couple of places: 1. `parse_hostname` could make use of regex to pull the digits off the end of the name rather than iterating through all the characters. 1. `next_server_number` could be improved and simplified by using a while loop with an index starting at 1. On each loop, check if the index is in the provided list. If not, you know that's the lowest index that can be allocated. It will short-circuit to the right value as soon as possible.
Yeah Pyramid is awesome for making Rest API's, not sure why it "sucks" as I have been creating APIs with Pyramid for a while and love it. I don't like the Cornice + Colander combination so much though, but Pyramid itself is great for REST, also Marshmallow for schemas instead of Colander.
I don't know why someone down voted you - this is a perfectly reasonable option for any nix based system.
If I install a package globally, such as Django, will I then have issues when I have a project that I do not want to use it on?
Everyone loves a top 10 list, but what are the criteria by which these articles are chosen? Reading the page it seems to say that "Mybridge AI" chooses the articles, whatever that is and whatever criteria it uses. The line &gt; 1,100 -&gt; 10. Only 0.9% chance to be included in the list. on the page is really meaningless without further details.
I forgot to mention. I am using Ubuntu 16.04
This is very interesting. So instead of using ENVS for each specific project, I could create ENVs that use specific packages? So I could have a ENV for Django, one for Scrapy, and so on?
&gt; This sentence is nonsense, and if you believe it than I guarantee that either you'll have an insecure configuration or that you'll get broken by your distribution one day. When I have enabled only the security repo, for example in Debian, the only fixes I get are security-related ones. They are carefully crafted in order to not introduce breakage. Of course, it is possible to find some fancy examples of breaking security fixes (like you did with RC4), but they are extremely exceptional. With `pip install --upgrade` I get **every** upgrade, including breaking non-security-related changes. No one takes care to not introduce breakage, as pip simply follows the edge. So, my app can become broken after literally each pip package upgrade. There is no way to prevent that (for example to opt only for security fixes from pip). Therefore, no admin will add `pip install --upgrade` to everyday maintenance script on his servers.
Very good point. I feel like there is so many great packages / modules that I almost need to slow down and concentrate n the bigger picture. I have only been really going hard on learning this for the past 3 months. I am currently trying to build scrapers/spiders to help automate mundane tasks to make my jobs easier to concentrate on important tasks. I finally feel like I have a good grasp on the syntax for python. If you don't mind, what would you suggest as the best combo for learning and creating spiders and scrapers?
I'd say, zope components are still alive, as well as ZODB. Zope as a web framework probably morphed into Pyramid (which is alive!). Also Plone still uses Zope2, which is painful :-)
Hello, thank you for reading and giving feedback- I really appreciate this. Your comment is awesome- I will update my post with your solution- thanks again.
Could you elaborate more on this? Thanks for the comment.
Using a lambda with `map()` is kind of clunky; you can do better using a generator expression: return ':'.join('{:02x}'.format(x) for x in mac) or return ':'.join(format(x, '02x') for x in mac) I personally prefer the new-style formatting and don't use the old style, but that's mostly personal preference. Don't use `os.system()`. Use `subprocess` for everything. And avoid the shell entirely. You can also avoid the need to source the file by just having Python read the password from the file: def set_ether(): subprocess.call(['ifconfig', 'en1', 'ether']) def set_wifi(state): subprocess.call(['networksetup', '-setairportpower', 'en1', state]) def random_mac(): mac = [0x00, 0x16, 0x3e, random.randint(0x00, 0x7f), random.randint(0x00, 0xff), random.randint(0x00, 0xff)] return ':'.join('{:02x}'.format(x) for x in mac) def set_mac(mac): with open(os.path.expanduser('~/.mysecretfile')) as infile: password = infile.readline().strip() p = subprocess.Popen(['sudo', '-kS', 'ifconfig', 'en1', 'ether', randommac()], stdin=subprocess.PIPE) p.stdin.write(password + '\n') p.wait() set_ether() set_wifi('on') set_mac(random_mac()) set_wifi('off') set_wifi('on') set_ether() But that's still not ideal from a security standpoint as it's not how `sudo` is meant to be used. What you're supposed to do is add a configuration entry in the `sudoers` config file that says that running `python your_script.py` is allowed without a password, so then you remove all the `sudo` stuff from the script and instead run `sudo python your_script.py` and it doesn't prompt. Or, use one of the `sudo` graphical frontends to prompt for the password. 
The point of an abstract factory pattern is to to "Provide an interface for creating families of related or dependent objects without specifying their concrete classes." (Quote from GoF). You still need that in Python if e.g. you would like to program a GUI application which is supposed to work with different GUI frameworks (QT and GTK) or a machine learning application which should work with Tensorflow and Theano. In Java you would have some kind of Factory class with methods such as makeButton, makeTextField etc.. One such class for any widget library you want to use. In Python, since classes are objects as any other, you can just have a dictionary where the keys can be strings such as "Button", "TextField" and the values are the respective classes (not instances of them). This is the classic GoF example. A probably more pythonic example is the keras machine learning library (check keras.io) it works with a Theano and a TensorFlow backend. The library has a module called "backend" which looks something like if selected_backend == "theano": from .theano import * else: from .tensorflow import * This is conceptually pretty similar to a factory pattern. A difference is that you cannot select the other backend any more (without hackery) once you've done the import.
I propose what I believe to be a fairly elegant solution. Though, I think Kenneth Reitz would tell me to fuck off. JSON: requests.get('http://someurl.com/').content.parse(format='json') YAML: requests.get('http://someurl.com/').content.parse(format='yaml') Plain ol' bytes: requests.get('http://someurl.com/').content Want content as a str? str(requests.get('http://someurl.com/').content, encoding='utf8') Headers: requests.get('http://someurl.com/').headers # returns dict of headers Sure, it's more characters, but it's more explicit.
I think what /u/toyg was trying to say is that json is a pretty much 1:1 representation of a python dict.
Most importantly, you ignored the explicit sample code. Where's your `Tracker.new()` method? Personally, I agree that `Tracker.new()` is unpythonic and your way of doing it will be better. But they probably wanted to see how you respond to crappy customer requirements. Try something like this: class Tracker: def __init__(self): ... def new(self): # Required by specification. # FIXME talk to project manager, can we lose this? return type(self) Or better still, *talk to the interviewer*. Ask them to clarify what they wanted. Suggest that a new() method isn't standard Python style, but you'll write one if required to. The specifications require that `next_server_number` be a function. You made it a method. What happens if they need to call `next_server_number` from something else that *doesn't* have a tracker instance available? Again, you should clarify: should all trackers share the same pool of server numbers? If not, then you might be justified in making it a method of Tracker. Or possibly keeping some hidden state somewhere. (Do they have coding standards that prohibit global variables?) But if all the trackers are intended to share the same pool of numbers, then I think your code is broken. At the very least, again you completely ignored the written specification. Lastly, did you ask what the expected data would be like? If the number of servers is never more than, oh, 1000 or so (at a guess), then sorting as you do will probably be faster than any cleverer algorithm implemented in Python. But if you've got (say) a billion servers, then its likely that this will be too slow. At the very least, you should acknowledge (in code, and verbally if possible) that this is intentionally "the simplest thing that can work", not optimised. Chances are that they wanted to see *how* you work, whether you will ask questions, how you deal with under-specified requirements, etc. rather than just what code you write. A few other little things: - `max_num = l[len(l) - 1]` is better written as `max_num = l[-1]` - `l` is an awful variable name - your test code runs unconditionally when the module is imported the first time, it is probably better to put that under a `if __name__ == '__main__'` block 
The examples don't print the server name either, not unless quotes are part of the name. I interpreted the examples as being a faked session in the interactive interpreter. They use `&gt;&gt;` instead of `&gt;&gt;&gt;` as the prompt, but that's not important. Given that, the examples show the `allocate` method being called, which returns the server name, and the REPL then prints the repr() of the result.
Advanced Python Scheduler is nice and easy to setup: https://apscheduler.readthedocs.io/en/latest/
/r/learnpython
You'll get better help at /r/learnpython. Did you add the script to your path? Their installation instructions explain how to do that. 
The point of the original post/link is that it often isn't exactly written well for professionals, either.
Either install it in a [virtualenv](http://docs.python-guide.org/en/latest/dev/virtualenvs/) or use: pip install --user git+https://github.com/tejado/pgoapi.git
ty, isxek, I appreciate that. I posted this announcement to the Python mailing list as well, and someone reminded me that I should mention this is a volunteer effort being released under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
Thanks for sharing. I have been learning Python for few weeks, and after transferring all my code to it, still I enjoy learning more on Python. I have some good starting points from those articles.
The word you're looking for is likely "telemetry". It should be possible for you to just run a server somewhere, and have your program send HTTP requests to it with user input, which are then recorded to a server-side database. Also keep in mind that if you're logging user input like that, it's ethical to inform the user before recording anything, and possibly allow them to opt out. Also, this question may be better suited for /r/learnpython.
For roughly the same reason that Twisted probably shouldn't have every protocol it supports in its core: it forces you to download and use a 16MB dependency when you need 1kB of code. A HTTP library doesn't need or want a FTP parser, so it shouldn't be pulling one down as part of its dependency trees. That goes double for all the *extra* stuff Scapy includes, such as an I/O layer for packet filtering and injection. On the other hand, Scapy could use these separate libraries *itself*, rather than maintaining its own protocol implementations in-tree. I'm not suggesting this for HTTP at this time (there's no point, Scapy already wrote theirs), but for things like HTTP/2 they could save a lot of effort by using tools that already exist.
Thanks!
Many, many, many ways. If it's a really simple console application, using a standard file IO system and writing inputs to a text file is the most basic way to do this. If you're looking for a more technical program, then perhaps a database is the way to go. This is not really a python question, more of a software design question. Once you've decided what you need to do, and how you want to do it, Python is very likely to be able to help you put it in motion. From the sounds of your question, what you need to do first is consider: * What your program will be doing with the inputs * What format the inputs will be * What you want to know about the inputs a user gives you * What you want to do with the inputs after the user has entered them * How you want to store the inputs (e.g. system memory, text file, database....) * What the end goal of your whole system will be. Python is just a tool which can help you perform all these steps, but like any programming language you need to have a firm idea about what you want to do before you start, or you'll end up with some real conceptual problems in your code. If you're just looking for someone to do your homework for you, then I think you're in the wrong place! If you're looking for some genuine design support, have a think about what you want and I'd be happy to point you in the right direction. edit: formatting
.... at coming up with a better title?
Thanks for such a comprehensive reply! Lots of useful things here :D Can I ask what kind of systems you deal with?
Thanks for such a comprehensive reply! Its also good to know some of the previous decisions made before you ended up with your current system. Interesting that you say that DEBUG is your default but disabled in productions opposed to INFO being default with DEBUG enabled in dev.
&gt; syslog always seemed less convenient for me. My application's messages will be interleaved with unrelated system messages, which can get rotated out before I get a chance to see them. I disagree here. Syslog as default configured may, but almost all modern syslogs (syslog-ng, rsyslog, journald) will separate your logs into application-specific logs. If you handle logfiles, you _need_ to have an rotation command in case you fill the system, and to prevent clobbering, permissions and rights. I'd rather see your application set log output to stdout + stderr than to open it's own logfile these days. I do agree with you on module-level naming, and prints though. Hook stdout +stderr to the logging module, don't mess there. Also, logging formatters can be awesome.
&gt; I'd rather see your application set log output to stdout + stderr than to open it's own logfile these days. So does that mean you rely on what ever is calling your program to redirect its output to syslog?
Nice job! Some input to help you learn: --- Instead of doing 8: while player_choice != "1" and player_choice != "2" ... look into the functionality of the `in` statement. You could for example do this: alternatives = ["1", "2", "3", "h", "H"] then 8: while player_choice not in alternatives: would do the same thing. --- Instead of doing 10: if player_choice == 'h' or player_choice == 'H': You could do 10: if player_choice.lower() == 'h' --- Also, you don't seem to be using your `n`-argument whatsoever in `computerChosenMove()` and `playerChosenMove()` --- You should refrain from ever using `global` variables, as it's a very bad habit to keep. I know, the alternative is a lot more complicated, but it'll pay off in the end. You still seem kind of new though, so it's not that big of a deal :)
Thank you very much for your suggestion, I will improve on my next tutorial!
If forced to implement `new`, it should be a classmethod: @classmethod def new(cls): return cls() Also, the server numbers are shown to be per-name by the examples (it returns `apibox1` followed by `sitebox1`).
See if you can implement your own version of ipcalc, that will teach you a lot about IP addresses and how to work with them.
I haven't really been using a package manager. I tried installing Anaconda before but it was doing nothing but making things harder so I stopped using it. That was a while back. Think now that I have a better grasp on things I should try it again. Only one I had really heard anything about was Anaconda until this pt.
Probably they are emphasizing it filters out the ugly content. Honestly so many poorly written articles around, especially for beginners. This list is actually good.
But it's not the only one.
You would get better response on /r/learnpython. Personally I think 10060 is indicative of a general connectivty problem that will fix itself, and not anything specific. Unless you really are running a broken proxy, but you would surely have turned that off to check before posting ...
Questions such as these are better suited to /r/learnpython. * Your code invokes a shell. That's probably not necessary if you just want to run a command. * Your code after exec_command tries to read from stdin (which is your /input/ to the ssh command) instead of from stdout or stderr. 
Spaces. 
Because it's the default, **DEBUG** has the potential to to be overused and slow down your application with file I/O. Having it off in production is typically a performance optimization. If I could, I'd have it on all the time.
Author of library here. It's a good question, and one I've been asked before. A couple of (honest) answers: - I wanted a library that would allow for multiple methods of inference on time series models, not just frequentist methods. In fact, some of the models in the library require non-frequentist methods of inference - e.g. non-Gaussian state space models. I think the library is unique in this sense from a time series point of view - most other TS libraries (not just Python, but in R etc) can be restrictive in imposing a given method of inference for a particular model. - I needed to implement ARIMA/Gaussian state space models as building blocks for the more complicated models that are included, e.g. GAS models and Non-Gaussian state space models. - I had a (selfish?) interest in wanting to build a library afresh for my own personal development as a programmer. And worth noting that I don't see these two libraries as competing but complementary. I would, in fact, be very for porting some of the models over to the statsmodels library, but I am simply short of time. Thanks! - Ross
I really should switch to tabs as I always get mixed spaces/indentation errors from my text editor tabbing for me. I do, however, prefer spaces.
do what everyone else do. standards and expectations &gt; tabs vs spaces &gt; didn't get terribly mangled once opened in any different environment than the one it was written in. doing it wrong etc tabs for hard indent, spaces for soft indent def test(): → print('abc') → print('def', → ······'ghi') which is also a bit why its not fun using tabs, because then you need to deal with mixed indent to achieve the best results, and the mixing is invisible
in this case it isn't, because it doesn't consider the last line as indented &gt;&gt;&gt; def test(): ... return ('a', ... 'b') also works fine for example
Thanks, it looks like they changed the article's slug,
&gt; An integer isn't a valid server name. Good thing they aren't using an integer as the server name then. Just keep digging man, just keep digging. Next I suppose you'll tell me that "apibox1" is a valid integer in base 36...
Did you find a way to do this? I've set up a virtualenv with the scientific stack and jupyter installed, and even though the server starts on localhost and is accessible through a browser, the kernel isn't working for me. 
It might actually be the only one with that property. But I don't think it's a particularly important one.
Maybe not from a usage standpoint. However, from a parsing standpoint it definitely is. I would say a function that parses a Python dict to json is most definitely going to be more performant than a function that parses a Python dict to yaml. I work with generating json responses a lot. The ease of parsing is a huge factor.
I wrote this for determining the probability of generating a collision with an 8-digit, 6-bit ID (48 bits total) across one million items: $ python birthday_probability.py 1000000 2**48 Probability is 0.001774780051374103, or about 1 in 563
I reccomend PyCharm - it has all the stuff you mentioned about the Python plug-in for visual studio, it's free and has great autocomplete.
Wow, that's awesome. PyPy3 languishing in a "mostly works" 3.2 version, I think, is actually one of the largest things slowing down the continued "cultural shift" to Python 3 nowadays. There are now very very few actively developed, widely used libraries that haven't got 3 compatibility yet, so alternate implementations of Python are one of the next big things need to get a move on. I believe it could even be hamstringing PyPy's own popularity to a small extent, because stuff that wants to be the brightest and shiniest of Python (i.e: only running on Python 3) like all the asyncio stuff, is basically locked out from getting to run on PyPy atm.
Parsing would be JSON -&gt; Python dict. What you are talking about is serializing. &gt; I would say a function that parses a Python dict to json is most definitely going to be more performant than a function that parses a Python dict to yaml Why should it?
This is seriously awesome! I am totally making that a thing in my Jupyter.
When I had my bike stolen, I wrote a bot that checks local classifieds websites every morning and send me reports. It looks for keywords such as the bike brands, the tyre size (was very peculiar) and so on. So far, I have not caught the thieves :(
Thanks for the questions! Let me do my best to answer them: 1. Did you mean Linux on Windows? The book covers installation for Linux and Mac, but doesn't cover installing VirtualBox for use on Windows. At this time, I don't think that the Bash-on-Windows built-in feature is capable of providing full support for TensorFlow. Hopefully soon, though! 2. Whatever is most comfortable for you- using an IDE for Python is nice, but doesn't offer the same benefits as it does with a strongly typed language like Java or C++. Pycharm works, Sublime Text, vim, etc. We show the user how to install [Jupyter Notebook](http://jupyter.org/) as well, which provides a nice environment for interactive scripting. 3. The book is officially targeted at Python 3.3+, but our examples should work with 2.7 without much headache. We show how to install TensorFlow for both 2.7 and 3.3+ 4. Obviously, it depends on your style, but here are a few resources I've found useful (aside from Stack Overflow): [Hitchhiker's Guide to Python](http://docs.python-guide.org/en/latest/intro/learning/), [Learn Python the Hard Way](http://learnpythonthehardway.org/book/), and the [Lynda.com Python 3 Course](https://www.lynda.com/Python-3-tutorials/essential-training/62226-2.html) Hopefully that helps!
I hate when tutorials like a Django tutorial start with an introduction to Python. If you haven't programmed Python before, you should probably not dive into Django after just learning the basics of the language. 
According to the man page: ~/.pythonrc.py User-specific initialization file loaded by the user module; not used by default or by most applications. This indicates that if the `user` module is imported, then it will execute the content of `~/.pythonrc.py`. This would suggest that perhaps `ipdb` is loading, directly or indirectly, the `user` module.
`from functools import partial`
I always found the Python docs great and way better than C++ docs from the authors example. I even read the Python docs without trying to solve a particular problem when starting out. It was just a joy to read a little about everything in the standard library. Not sure if it's personal preference, maybe some people are used to getting a typesignature + short example and be done with reading. 
&gt; like all the asyncio stuff I was looking at alternative runtimes today (my first foray into this) and obviously pypy came up. Closed the browser tab as soon as I got to the homepage and saw the "3.2" written there. 
I never found that to be true, would you (or someone else) mind elaborating? 
Default browser behavior on Linux when clicking address bar is to not select the text. Click bar, ctrl-a to select text, missed control, assume you missed bar with mouse, address has extra a in it, click, ctrl-a, ctrl-c. I've done it a few times until I got used to it. Firefox lets you change the behavior, not sure about Chrome. I did change it under Firefox to behave like Windows.
Use this on pretty much every script I write. Jupyter and pandas integration is an absolute godsend.
&gt; Last I checked, we're in the 21st century, where storage is measured in TB and memory in dozens of GB. A few MB of dormant code isn't going to do any harm That argument has two key flaws. Firstly, it depends on your use-case. If you are, for example, deploying your code via Docker Container image to your data center, those few MB can rapidly increase to gigabytes of bandwidth. That goes doubly if you're distributing it that code yourself: each extra MB of code you distribute from a popular package source becomes gigabytes of bandwidth consumed for popular services (e.g. the docker hub). But that's not the main problem. The main problem is that that idea doesn't scale. If every library becomes monolithic, then complex applications end up depending on 10, 15, or maybe 20 gigantic packages that each distribute many tens of MB of code that isn't needed. &gt; Think a few steps ahead. Don't worry about the scapy authors' efforts, worry about your own. Got a program that uses Scapy and want to swap in a different protocol? That's much less effort than swapping in an entirely different library. I am worrying about my own efforts. I have a HTTP/2 protocol stack and three-and-counting HTTP/2 implementations built on it. The Scapy developers have no HTTP/2 implementation. I should note as well that simply writing the data out is only half the battle. Scapy's core purpose is to understand framing, not semantics, and it does really well at that. A useful HTTP/2 implementation wants to encode as much semantics as possible, to reduce the amount of work others need to do.
It looks like [bpython](http://www.bpython-interpreter.org/)
Oh, and I should mention: if you want the freedom of using Scapy to swap between protocols, that's fine. [Here](https://github.com/python-hyper/hyperframe) is the code that knows how to parse HTTP/2 frames. [Here](https://github.com/python-hyper/hpack) is the code that knows how to encode/decode HTTP/2 header blocks. Both fully spec compliant, both encoding no semantic knowledge, both requiring no non-stdlib dependencies. Scapy is welcome to depend on that code, or vendor it (it's MIT licensed, go nuts) for their own implementation. That's the advantage of this no-IO approach: it allows us to *also* have everything in Scapy. We can have both.
Python 2 isn't getting any more feature updates. If the PyPy project is going to devote *any* resources to Python 2, it should be the absolute minimum maintenance. As long as they don't give a focus to Python 3, they're harming the progression of Python as a whole.
Actually I'm pretty sure there's no way to securely erase memory in Python without resorting to ctypes or something else low level. 
[magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) for python, so cool!
It may not be polite to advertise in other's project posts.
A version for C++ is in it's very very early stages of development. I'm CrazyPython on GitHub. [link](https://github.com/tqdm/tqdm.cpp)
Thanks for the hint. Not certain, but I believe ipython loads the rc file by itself, and ipdb is hooking into that. The stackoverflow question now has a stack trace, if you're curious.
You can take a class without attending the conference - and who doesn't want to learn about descriptors, metaclasses, decorators, descriptors etc from Raymond Hettinger? I teach Python classes - but I've always learned something when I hear Raymond talk. We've also got Wesley Chun offering a client/server network programming class. If you're not doing anything Friday the 19th and you're close to SF come take a tutorial session...
Which is a benefit.
I'm sorry to tell you, but your chances aren't the best. Learning it all on your own can work out, but I could imagine that a lot of employers don't recognise that or at least treat you badly for it *(getting paid much less, only being there to correct other people's mistakes, and so on)*. You can get to know a lot of stuff by learning it yourself of course, but you might lack the in-depth details to know what's behind everything and how to do it properly, and the like. On top of that, python isn't enough most of the times. Especially in web-development it's highly recommended, if not demanded, that you know at least basic HTML, CSS, PHP, and javascript.
I don't think that this library is being maintained anymore (at least the last updates are fairly old). If you can't work around it you might have to roll up the old sleeves and scrape the data yourself from the http or JSON directly.
I reposted because the other post had a bad link. Reddit didn't say not to post, because the URL was different, again, because it was a bad link.
As a new user, I must say I am glad to have this. It will be a great aid in formatting while I learn more about formatting! Edit: I seem to have found a bug(?) with this. After calling the text color, it actually makes my terminal stay the color of the last color called before sys.exit.
Thank you for the information! I will definitely apply to nearby stores when I am 100% confident in myself. After researching online a little I realized I should also build a portfolio, so I as soon I build a couple different sites I will look for a job. Again, thank you.
Good for you, feel free to PM me questions you have down the line if you like.
Thanks for the feedback!
It's definitely possible, but it's very hard. Best of luck.
Probably your best shot is to go to local small companies and ask if they would be interested in having a site developed. Make sure you have examples of past sites. I really doubt you'll get hired at an actually legit company as a programmer. But I'm sure there are people who need somewhat basic sites who would be ok hiring a teenager. Even most prodigy programmers don't get paid jobs. They'll get like an unpaid internship in high school at google or apple or whatever.
I'm just doing some moderately large scaled image processing and my issue is I load the pickles (about 6gb) then reconstruct the data to the format I need it in (now at about 8 gb) then when I start my actual work on the data and as that progresses I eventually hit my ram limit and soon after the kernel dies or I can't progress because the modules know there isn't enough ram left. 
Question for everyone -- do comments get deleted if they go below a certain threshold?
I had my first programming job when I was 14. So yes. Totally possible. 
They mostly work on the underlying engine (JIT), which basically speeds up both versions. But the "front end" (translating python 3 code -&gt; JIT) isn't there yet.
Oh, OK, in that case, just releasing the last reference will do. I'd still rethink the design though, as you seem to keep a lot of data in RAM when you might be able to process things in a more streaming fashion - is the 8 GB of RAM you're using really just one image? If not, consider processing images one by one.
Thanks for the advice!
Thanks for the advice!
I'm going to come out and say that you are better off creating a few projects you can show as proof-of-concept. Make your name openly-visible. Show them, and if they ask about your process, go into an in-depth about everything. I'm talking deep enough for anyone that doesn't have a grasp of Python to drop their jaw. You can woo them over by simply sounding intelligent in your craft. That's pretty much exactly how I got my last job.
Huh, cool. (I'll still stick to spaces though)
I modified my description. Re-read it!
When I was 15 I managed a translation companys site (freelance). And trained their staff in how to edit and maintain their site. I had 3 staff students daily for 2 hours (on site). It's definitely possible. I'd recommend finding local businesses, those with the horrible crappie websites. Pick one, redesign / rebuild the site. Contact that business and ask them for a meeting so that you can present your work. Present, sell, go home, rinse repeat. You may fail, don't give up, move on to the next prospective client. Also im self taught, like many professionals out there, and I have a belief that passionate self taught coders will always outperform the vast majority of office drones. Best of luck! 
There are about 30,000 images I'm working with 
This is awesome!
Thanks, that got me further. It seems to have installed without error, but getPokeStats.py is nowhere to be found on my machine.
I really like the explained process, as I said before, I will do this once I feel comfortable making secure sites. I'm also very glad to hear a response from you as I'm pretty sure I have seen some pretty helpful posts from you, thanks for the input!
Thank you! I guess I am fortunate that I live in between 3 massive malls.. I will do what you suggested. Also, I really don't mean to pry, but would you mind giving me a rough estimate on how much that translation company paid you?
In the end your code just ends up being less readable and pythonic. As /u/lightshadow noted, there's already partial function support in the standard library, and instead of the piping example: filter = ellipsis_partial(filter) to_list = ellipsis_partial(list) range(1, 10) @ filter(lambda x: x % 3 == 0, ...) @ to_list(...) we have a much simpler filter: list(filter(lambda x: x % 3 == 0, range(10))) or an even simpler and more pythonic list comprehension: [x for x in range(10) if x % 3 == 0] The final examples look more like Haskell or OCaml than Python. I'm not sure why it would be nice to have since we already *have* these features courtesy of functools and list comprehensions.
There's almost certainly no reason to keep all your images in memory. Why not work in batches? I found HDF5 was a convenient file format when I was experimenting with pyCaffe. 
&gt; XMLTag does not provide escaping content by default. That seems like a bad default.
That sounds amazing! Sadly though I believe my state requires me to disclose my age.. Also, what kind of program was it? I tried looking up local programs to no avail, as they all require me to to be a college grad and show up in person.
isn't spyder IDE serving the exact same purpose?
How does Mozilla have money? I always hear about them handing out grants, not sure where their money comes from though. Donations?
I read somewhere about 2factor, that it would be better to use an app that provided a code like Google authenticator rather than a SMS in case your phone has been stolen. 
Yep, but it's ugly in pipeline like: range(1, 10) @ partial(map, lambda x: x + 4) \ @ partial(filter, lambda x: x % 3 == 0) \ @ partial(map, str) 
One question: are you a software engineer?
Switched to this from Progress Bar for Jupyter work and have been using it ever since. It's light, fast and does what it says on the tin.
Haha no, I meant to ask why you mentioned the times! I basically said “someone else posted it earlier” and the times support it, right?
https://en.wikipedia.org/wiki/Mozilla_Corporation?wprov=sfla1 They used to get many millions from Google for having Google be the default search; presumably Yahoo now pays them more.
PyCharm has integrated support for notebooks so it might give you a small boost. Emacs is really only good for people who have to work across an ssh connection (and even then, PyCharm does ssh uploading so that's debatable) or who have already invested 10 years in using it. 
This may not mean much coming from a random on Reddit; however, I would definitely hire you. Here's why: 1) You're humble and ask questions. It's a great quality to have as a developer. I can't tell you how many developers I've fired because they thought they were irreplaceable. Ok, also for very reasonable other reasons, but the moral is - stay hungry, stay humble. 2) You have drive and ambition. I'm an alumnus of a top 3 business school and run an analytics shop for a very large, global company. I've seen VERY smart people come and go, but the people that have that inner fire and can execute on their vision and plan ALWAYS come out ahead. 3) You have a reason to work hard. They say that necessity is the mother of invention. I'm the oldest of a large family of children whose parents could not afford to send me to college, buy me a car or any of the other "normal" purchases I saw my friends' parents make for them (not complaining, just elaborating). This fueled my work ethic and I created my own path to achieve my goals. You reaching out like this is the beginning of your path...well, really the preparation you've done prior to this was the beginning. Remember: Luck = preparation + opportunity. You have to prepare (and you are) and you have to, sometimes, create your own opportunities. Please do not get discouraged. Youth is the best thing you have going for you. I'm a little more than twice your age and I wish I had picked up programming when I was your age. I have a few pet project ideas I could throw your way if you want to give it a shot. This might help you expand on your skill set while developing a portfolio of work that you can market to potential employers. What's your experience with scraping websites? Also, a little database knowledge would go a long way. My thoughts, if you can learn Python then you could definitely get your head wrapped around SQL. Also, look at fiverr. You can do small projects for a few bucks and as you get more work and reviews you can charge more, etc. Ok, this turned in to a novel. Feel free to ping me with any questions, etc. Best of luck, young friend! tl;dr Stay motivated, invest in yourself (knowledge), don't be bashful and keep putting yourself out there. 
Which the better to New programmer beginning 2.7 or 3.5 ?
Then I would definitely look into ways of processing them one by one.
I chuckled audibly +1
Highly depends. For example - for most corporate HR depts you will be a "no degree" type, which translates to a lower probability of employment. Also, being underage, you fall into a different laws in labor law. But we have worked with web dev companies that had very young graphic designers, devs etc. Go for it. Do your best to get a job. If nothing else, you'll get experience dealing with hiring managers and HR people. And this counts as something.
Could it be [this article](https://techcrunch.com/2016/07/25/nist-declares-the-age-of-sms-based-2-factor-authentication-over/) where NIST says that sms based 2 factor auth is not to be considered secure anymore? I also wonder what the scenario would look like if the author did not know a person over at github? I assume it would be slower, but by how much?
No, depending on the user's setting they just show hidden/collapsed if they go below a certain threshold. Ignore it though, anyone who has ever done something worthwhile on this planet has had to deal with pointless passing comments like that.
Isn't it F6?
What you know by the name Python is actually CPython, a Python implementation in C language. Jython is Python implementation in Java. So as you can use C extentions in CPython, you can use jar in Jython, but the interpreter is different. I'd recommend this blog post that dives a little into different Python implementations: https://www.toptal.com/python/why-are-there-so-many-pythons
Scope of work is what is agreed upon between you and the client. When either party adds unplanned work (additional changes, more work etc, additional features, etc) that's called scope creep as in the scope is expanding. You can also aquire work from freelance sites such as freelancer.com. 
Did I say otherwise?
Definitely use Python 3, most major libraries have ported to 3 already.
&gt; PyPy is close to supporting all of Python 3.3 now; but the list of what is new in Python 3.4 and 3.5 is far, far longer than anyone imagines. Hopefully PyPy can release a solid 3.5 implementation; that would be a huge step forward. A lot of projects would switch to PyPy either as their default platform, or as an alternative build to CPython, given a PyPy implementation of a current version of the Python language, assuming that PyPy continues to lead in performance. Still, it seems quite likely that CPython releases will arrive months or years before corresponding PyPy releases for the foreseeable future, if only because implementation of new features is easier in the relatively well known and straightforward CPython codebase. Convincing core Python contributors to implement in PyPy first or even concurrently with CPython doesn't seem likely.
ferranpeg's answer is the easiest way to go about this: just have a list of tuples and iterate over it. To add a bit though: if you have your lists in the format you have initially for some other reason. Ie: lb = ["th", "he", "llo", "an", "text"] dc = ["1", "2", "3", "4", "5"] You can convert this pretty easily to the "paired" form with the `zip` function. for s, rep in zip(lb, dc): X = X.replace(s, rep) However, I'd go with the "paired from the start" approach ferranpeg advocates unless you've already got it in that format for some other reason - with two lists you can get out of sync if you accidentally miss one element from one of the lists, and end up messing up *everything*. One final thing I'd mention is that this is not a very *performant* way to do this though - which probably doesn't matter for short strings, but could be relevant if there are a lot of text (or a lot of patterns to replace). The reason is that each replace does a new search on the string, and ends up allocating a brand new string and disposing of the old one (your original version would be even worse here, because it keeps hold of all those intermediate strings, so will cost you a lot of memory if they are very large). Fortunately, one advantage of this data-driven approach is that it's easy to change the algorithm later without having to change the list of replacements if we find we need more speed. (Eg. you could switch to one using regular expressions, or dictionaries and translate as you walk the string, all driven from the same list). But no need to do so unless you find its too slow for your purposes. Finally, just to mention why your original doesn't work: You're writing: com = X.replace(lb[c], dc[c]) But the fact that you're using `X` means you're always starting from the original form each time. You should instead set `X=com` before the loop, and then do: `X=X.replace(...` inside the loop. Also, while I definitel recommend using ferranpeg's approach, if you **do** find yourself needing to use indexes, there's a handy function called `enumerate` that will pair up the index with the value. Eg. you could have written your loop as: com = X for idx, pattern in lb: com = com.replace(pattern, dc[idx]) (But zip is a nicer way to do this - it's generally good to avoid using indexes if you can just iterate over the thing itself directly)
Thanks for the report, i fixed it. It should be working good now.
Heh. Needed to go to GitHub to see that it's for R. Fail. 
God damn that shit is scary. I followed the links to @N's and @mat's stories which I had not previously read. While I'm not surprised that there may be *some* cases where calling tech support is a viable attack, I was not expecting it to be so common and trivial even amongst big tech companies that should really know better. Running Linux I feel moderately safe because at least no badly guarded cloud service is able to wipe my hard drive (plus I don't own a Twitter account with a short name). Still, I'll take this as an opportunity to revise some of my online accounts and how they are interlinked.
Worth noting - in Jupyter if you use `tqdm` with the defaults you get a nice progress bar which `nbconvert` cannot process (it chokes on the unicode characters somewhere in latex). Instead add `ascii=True` and you'll get slightly less pretty rendering which can be exported as e.g. pdf just fine.
Check out the toolz module: from toolz import pipe, curry @curry def add(x, y): return x + y add5 = add(5) add6 = add(6) pipe(2, add5, add6) # 13 
Python: You don't have to remember those #(*@ semi-colons. C++: You don't have to worry about your white space. While they are both computer languages one is on par with Esperanto and the other Chinese. Do you want to knock out a few short scripts to automate something or just do something simple and straight forward? Python. Do you want to write the next Photoshop or some low level portable app? C++.
Are you wondering why you aren't getting any "B"s in your output? Here are your first two replace statements: com = X.replace("th", "A") com1= com.replace("the", "B") Since you replace all the "th"s with "A", any place that originally read "the" would be changed to "Ae", and so not matching the "the"-&gt;"B" step. This also affects the later "tha"-&gt;"N" step. Putting all the pairs as tuples in a list named `repls`, here is a little validator snippet for this setup, to detect these cases where one replacement masks a later one: test_string = ' '.join(r[0] for r in repls) for from_,to_ in repls: test_string = test_string.replace(from_, to_) print(test_string) assert test_string == ' '.join(r[1] for r in repls), "one or more replacements collide" Using your original set, this prints: A Ae C D E F G Cr I Ge K L M Aa O nA Q R S eA U V X dA &amp; Z W # $ % ^ * 6 7 8 9 0 ! @ Traceback (most recent call last): File "repls.py", line 47, in &lt;module&gt; assert not any(c.islower() for c in test_string), "one or more replacements collide" AssertionError: one or more replacements collide I wrote this script to identify which replacements mask later ones: seen = [] for from_,to_, in repls: for p in seen: if p in from_: print("{!r}/{!r} is masked by {!r}".format(from_,to_,p)) seen.append(from_) Which gives: 'the'/'B' is masked by 'th' 'her'/'H' is masked by 'he' 'her'/'H' is masked by 'er' 'ere'/'J' is masked by 'er' 'tha'/'N' is masked by 'th' 'nth'/'P' is masked by 'th' 'eth'/'T' is masked by 'th' 'dth'/'Y' is masked by 'th' The simplest way to resolve this is to sort the list of repls longest-first. repls.sort(key=lambda repl: len(repl[0]), reverse=True) Now rerunning the above test code gives this output, and no raised AssertionError: @ ! B D F H J L N P R T V Y A C E G I K M O Q S U X &amp; Z W # $ % ^ * 6 7 8 9 0 The sorted list of replacements now looks like: [('false', '@'), ('true', '!'), ('the', 'B'), ('ing', 'D'), ('and', 'F'), ('her', 'H'), ('ere', 'J'), ('ent', 'L'), ('tha', 'N'), ('nth', 'P'), ('was', 'R'), ('eth', 'T'), ('for', 'V'), ('dth', 'Y'), ('th', 'A'), ('he', 'C'), ('in', 'E'), ('er', 'G'), ('an', 'I'), ('re', 'K'), ('ed', 'M'), ('on', 'O'), ('es', 'Q'), ('st', 'S'), ('en', 'U'), ('at', 'X'), ('to', '&amp;'), ('nt', 'Z'), ('ha', 'W'), ('nd', '#'), ('ou', '$'), ('ea', '%'), ('ng', '^'), ('as', '*'), ('or', '6'), ('ti', '7'), ('is', '8'), ('et', '9'), ('it', '0')] But for maintenance purposes, I would keep the list in the A-Z order that you have now, and just sort it at runtime, in a `repls.sort` statement right after the list is defined. 
&gt; Do you want to write the next Photoshop or some low level portable app? C++. &gt; &gt; So you're saying I can't write Photoshop with Python? 
Python: pretty, easy to read, super-fast to learn the basics to get going. Not really suitable when performance matters, unless you know what you're doing. And even then there're a lot of limitations (e.g. GIL). C++: ugly (to my taste), not safe (unless you know what you're doing), but pretty much as fast as you can get, and has no limitations whatsoever.
You can. And as a glueing language it will work just fine. But the image processing part, GUI library and some other performance-critical parts will have to be written in some lower level languages. Unless you hate your users and want to make them wait forever after each action.
Cherry vs Potato To develop it further, a cherry can be quite sweet if you like sweet stuff, however potatoes taste better when fried. Of course you can fry a cherry, but i don't know about the practicality of that. A potato can be also used in soups. A cherry can be used in cookies. But you can also use potatoes in cookies and cherries in soups with less satisfying results to my taste. In the end you can do whatever you want, you're an adult after all. That kind of sums it up I think.
I'm not referring to the implementation. I'm talking about the abstraction and the interface it presents, and the value it loses in not distinguishing between attributes and contained values.
In my mind, this no different than asking: C++ vs. Assembly. Python and C++ are for different developers with different end-goals. Python is quick and simple where C++ is deep and extensive just the same as how Assembly will be used for raw and tedious computing. You know what you want to develop, so decide your niche community based on the limits of the language.
It might be better if they used PyPy instead of CPython, but it's a start! The International Collegiate Programming Contest (ICPC) is one of the most prestigious Algorithms &amp; Data Structures contest for College Students, and it might give Python more popularity.
So this is an open source software I made during my internship. I made it to simplify the process of controlling robot arms with computer vision. I used OpenCV for all of my comuter vision, and tried to make it super easy to add objects, track them, and pick them up/use them for robot movements. You can, for example, pick up an object by simple dragging a "pick up object" command and selecting a "vision object" that you created. All of it through an interface, no special gimicks! Python made this project incredibly fun, because it translates so well for making other programming languages within it. I even made it very easy to add custom python scripts within the script you build, and have all kinds of cool crazy things you can do with it. Download Link (Windows 64 bit only currently): [Google Drive](https://drive.google.com/file/d/0B8WHbiufjfVCaTZlM3EwdklvQ2M/view?usp=sharing) [Github Mirror](https://github.com/apockill/uArmCreatorStudio/releases/tag/v1.0-beta) Source Code (Multiplatform): https://github.com/apockill/uArmCreatorStudio The robot (uArm Metal only currently): https://www.ufactory.cc/en/uarm_metal/ Contact me at Alex.D.Thiel@gmail.com
Tkinter ships with Python, unless you aren't using an offical distribution. You also could try PySide/PyQt, which each wrap the Qt Gui library Or you could use Kivy, which is a graphics toolkit that is written in Python. Python 3 is absolutely compatible with tkinter, if you're having issues with it, please post a question on /r/learnpython.
Of course! I was genuinely surprised when I was greeted with a red command line.
The simplest way to do that would be to use an external library, as getting unicode to work on Windows console properly on your own is rather labor intensive. The excellent [click library](http://click.pocoo.org/)^1 comes with the `echo` and `prompt` functions that should cover most (all?) use cases. Alternatively you can take a look at the [win_unicode_console](https://pypi.python.org/pypi/win_unicode_console) module. ^1 - The page shows docs for the previous major version by default for some reason. Paging /u/mitsuhiko.
This is one of those little gems that seems amazing at first, then just becomes natural. Another cool one, is that many, many languages have compilers written in their own language.
Yep. Click's echo and prompt functions work on Windows console in both 2.x and 3.x
The first thing I could think of is hosting a Flask API.
Thank you! It's been a wild ride. I started this project a bit before the internship, for fun, then was hired and got to work on it full time, now it's being released and marketed. Craziness!
He had this big mental health breakdown that he wrote pretty extensively about elsewhere in his blog
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Tkinter definitely works with Python 3 as Tkinter actually comes with Python. As already mentioned, you can also use PyQt and Kivy. wxPython also has a port called Phoenix that works with Python 3 and is finally getting some work out into it so it can be officially released.
Finally!
If you are on ubuntu (or any of its flavours, in fact using ubuntu repos) you have to download tkinter separately, it is not in the default python3 (not python2 either I believe) package, the packet name is something like python-tk.
use python3 instead of v2, and your fixed and *working* code is: def main(): print("Здравствуй, мир!") print(input("Message: ")) if __name__ == "__main__": main() 
You rock! Awesome tutorial as usual!! I wonder if it would be possible to use this to find and track my cat inside the house using a webcam? So... for instance if I wanted to track her travels?
Indeed, he found out he was undiagnosed with bipolar disorder, because of a temporary bout of manic psychosis. He's doing better than ever now :) 
There are vast differences between the flexible and compat environments. The normal app engine and all its APIs are mostly absent from flexible. 
Is your camera fixed and non-moving? If so, then background subtraction would be easier to get up and running. You can then apply a classifier at "motion regions" to prune false-positives to discern between your cat and say, a person.
:) Thanks for sharing your story on it. Your willingness to talk about it was one of the things that made me more comfortable with talking about my own struggles with my mental health.
This solution will work for console windows with appropriate codepage set, but will throw an unicode error everywhere else.
Probably yes. The issue is that a lot of modules are implemented in C using the CPython API. Those modules are not compatible with PyPy, Jython and IronPython. I see Cython in use pretty often, usually paired with a pure Python implementation to fallback to ([example](https://github.com/aio-libs/multidict/tree/master/multidict)). Cython was not developped to be an alternative to CPython. It is tightly bound to CPython. It's a way of writing modules for CPython with C-like performance.
Hi Homercles337, I'm co-founder of Continuum and co-creator of Anaconda. I disagree strongly with several of the assertions you make in this comment, and I'm disheartened that you would publicly slander the hard work of myself and countless others here at Continuum as well as in the broader community, some of whom (like my co-founder) that have been contributing to open source for decades. I'm interested in specifically the "shitty practices" you refer to, and if there are material improvements to our business practices that you can suggest there, I'm happy to hear them. My co-founder and I started this company so that we *could* sustainably invest in the open source community around Python, Scipy, and PyData. We count amongst our friends many key contributors and core developers in those ecosystems, and haven't heard negative feedback from them about how our business operates or what we do, so I'd be very keen to hear from you about that. Additionally, if you have particular criticisms of Anaconda itself, or conda, and what they do, I'm very eager to see if we can improve those things. The large nature of the distribution is something which joshadel already addressed with his answers about miniconda. Finally, I do want to point out that you are factually incorrect that conda is just an installer. It can also resolve dependencies between packages, create packages, and uninstall packages. I'm not sure what else is needed, before something qualifies as a package manager vs. being merely "an installer".
You probably want r/learnpython. But while you're here, let's take a look at [a project of mine](https://github.com/xiongchiamiov/github-pr-stats). There's a bit of general project stuff, but once you get down to it there are only two files that matter. There's [an executable](https://github.com/xiongchiamiov/github-pr-stats/blob/master/bin/github-pr-stats) that handles argument parsing, and it calls [the library](https://github.com/xiongchiamiov/github-pr-stats/blob/master/github_pr_stats/github_pr_stats.py) that handles all the actual work. This allows another program to similarly import the library without having to hack around cli args.
C++ has had smart pointers in the standard for half a decade now. Over a decade if you count TR1 which only has shared pointers which is the only one available to you in python anyway. 
Awesome work! On an unrelated note, how do I find an internship like this? :D
Hmm. One the one hand this is starting to look pretty cluttered, and I can see a return of people writing "Java in Python". On the other I do love type checking! 
wicked! will have to check it out later as I am just leaving work! I won't setup an Ark installation until my laptop comes and I get a distro on it, but at least I can research it for now!
If you want the latest advanced stuff in OpenCV you'll have to implement it yourself. Has a pretty good amount of fundamental computer vision methods implemented though.
Haha well, if you see an opportunity, recognize it, and take it! I did about 3 projects with this robot arm that the company saw, and by the time I asked to work with them it was almost a formality because we'd been in contact for a while. China's been incredible :)
This is ridiculous.
If you want to share a variable between functions you will need to declare it outside a function and if you want to modify it in a function, as a global there. A better solution for you is just to return it and pipe it between functions. Also these questions would be better served in /r/learnpython #global style x = 2 def increment(): global x x = x + 1 def double(): global x x = x *2 increment() print x &gt;&gt;&gt; 3 double() print x &gt;&gt;&gt; 6 #piping style x = 2 def increment(x): return x + 1 def double(x): return 2 * x print double(increment(x)) &gt;&gt;&gt; 6 
Let me see if I am understanding you correctly. Let's say your project has two pieces, A and B. Each functions as its own standalone CLI application and has some functions inside that do stuff. You want to be able to easily create C, which imports functions from A and B to do something new. However, you would like to also import the argument parsing from each of those since they are pretty self-contained. I am not familiar with any library that lets you do this. The closest thing I can think of is having a set_up_args function within each module takes an argument parser and adds the required arguments. --- **A.py** import argparse def set_up_args(parser): parser.add_argument('-a', type=int) ... if __name__ == "__main__": parser = argparse.ArgumentParser("A") set_up_args(parser) ... --- **B.py** import argparse def set_up_args(parser): parser.add_argument('-b', type=int) ... if __name__ == "__main__": parser = argparse.ArgumentParser("B") set_up_args(parser) ... --- **C.py** import argparse import A import B if __name__ == "__main__": parser = argparse.ArgumentParser("C") A.set_up_args(parser) B.set_up_args(parser) ... --- Note: I haven't tested the above code. set_up_args may have to return parser to the parent after it modifies it.
Thanks for pointing that out! I just realized there are about 5 functions in Vision that are never used in the project that are leftover from experimentation. I commented them out for now, but later on I'll take a closer look to separate the useful from the bad. You're totally right though, I could definitely use that function! I think I was using an older version of openCV- or more likely I just didn't do my research on that function properly ;) That's the beauty of open source though, it's nice to have a community to fall back on! Feel free to throw me any questions. Edit: Words
Transforming function AST in decorator looks interesting.
Yes, PyPy uses different machine level data structuring and representation than CPython's, so if you want to transfer data between the two virtual machines that each runs (CPython's bytecode interpreter and PyPy's JIT) then you always need a function, with non trivial overhead, to do a conversion.
Actually I'm still in Shenzhen! Today's my last day at the office, tomorrow I go to the FAB12 conference, then the day after I'm flying back home. Goodies: - I bought 4 $30 projects and want to do some projection mapping with them for fun - I've saved enough for a computer and a Vive, so I'll start developing robotics/vive applications (!!!) - A new (unreleased) uFactory robot arm - A robot from Kamigami Robots, they gave it to me to experiment with - Some misc nick-nacks from Huaqiangbei I made it to Hong Kong twice! It was awesome both times! I'll be at the NY maker fair this year, will you by chance be there?
China, huh?
This doesn't really add type checking, just changes the proposed syntax.
Sure, but the main motivator is `mypy` - the optional type-checking module.
The syntax looks clunky at best and the same goes for PEP 484. I'm all for type annotations and even static typing, just as long as it remains optional! I see the point of using static typing to speed up execution or avoid nasty type-related bugs, but we love Python specifically for it's flexibility and most of the times Python code is clear enough as it is, clear enough to avoid this sort of problem. I looks like they are forcing themselves to make use of the old `__annotations__` dict in some way and I hate it, because it feels rushed and not well though through.
This is a question for /r/learnpython.
Still sad to see us probably get this before a `switch` or `do while`.
BSTs are defined recursively. They consist of a bunch of `Node` objects where one Node is the root. Each `Node` object has a `left` and `right` property that point to the left child Node and right child Node respectively. So `self.left` is the left Node of whatever Node you're currently looking at and same goes for `self.right`
You sound very much like me. Perhaps you can assist me? If you happen to use Linux or Windows, that would be a perfect complement, but even if you're a Mac user like myself, your feedback on the manuscript would be very useful.
&gt; Overloading comments to mean something else inherently makes the job for text editors harder than necessary. Not more than necessary, no. &gt; What happened to do one thing and do it well? I don't think that that is a principle Python promises to adhere to at all times. &gt; Sometimes you want to declare a variable without defining it's value. It's impossible with the current syntax. Give me an example where you can't assign it None &gt; I find the muddling of variable definition with value assignment one of the biggest weaknesses of Python Fair enough, though I don't agree. Either way, the proposed syntax is very ugly.
&gt; The "switch" to Python 3.x might be starting to happen. I am really growing tired of this head-in-the-sand attitude of many legacy Python adherents. That said, this is great news. The lack of Python 3 support in PyPy has been the major thing stopping me from using it (especially now that their numpy support is pretty good).
If you only want to modify arguments, use os.execv(). This allows your program to do its argument fiddling and then be replaced by the process you really wanted to run.
In `Wages` you need `return totalWages` and `Saving` have to be def Savings(total_wages): save = total_wages *.15 return save or shorter def Savings(total_wages): return total_wages *.15 and then you can result = Wages(...your values...) another_result = Savings(result) or another_result = Savings( Wages(...your values...) ) 
Thanks so much! Just some follow up questions: how does self.left know to point to the left of the root node though? 
I use a dictionary for switching.. options = { 'one': fn_one, 'two': fn_two } options.get(choice, fn_default)(*args)
&gt; So you're saying I can't write Photoshop with Python? Just like he said you can't use C++ to knock out a few short scripts to automate something or do something simple and straight forward.
1. We profile. You'll find plenty of articles on how to use profiler. 2. You are comparing apples and oranges - both mentioned applications do something very different from what you are doing, so there is no point in comparing them. 3. However both of them are written in c++, not python. That alone could account for the performance difference. I'd say that increasing cpu idle time from 98 to 99.9 does not look like a goal worth the effort. 
Sorry let me try and explain. When you set self.left to traverse the left side of the tree by saying if the current node is less than the root, how does it know to look left? Hope that makes sense 
I don't like it. IMO this will just scatter Python projects. Most people won't use it(and function annotations too), and when they will hit code that uses it, they would ignore it. Similar to how C++ has an handful of features to the extent some places ban them.
Now that we have this, we also need a best practices guide of when to declare and when not to. I personally have never included type comments in my code. In fact, most of my comments are about why I do a certain thing, rather than what I do. Anybody here use type comments? Why do you do it?
externally typed files seem a lot better e.g. https://github.com/python/typeshed/blob/master/stdlib/3/ast.pyi#L30-L39 It'd be nice if they came with syntax coloring on github though
Should be interesting... But not enough online examples and documentation. Why should I clone/install it just to see what it looks like?
That's interesting, I thought the [documentation](http://mulholland.xyz/docs/syntex/) was pretty comprehensive. Do you mean you'd like to see examples of full documents written in syntex?
Oh, I see: menu links in a head of documentation page looks awful in mobile browser. It is hard to recognize that it is not just dumb square picture.
Ah, I see what you mean. Thanks for the feedback. I was actually thinking of redesigning that icon so it had the word 'Menu' beside it - I'll definitely do that now!
Its pretty good to help with structuring things note: not actual code or actual type defs (atm), but something I have written for my own benefit at the top of a file # memory: # user_queues = Dict[discord.Member, Set[Song]] # default_playlist = Iterable[Song] It allowed me to document what I needed from those variables (while making edits later in the document, I could go back and change those comments to something more practical), and in turn, it tells me what I need to actually give to my code when I come around to implementing it "but that could be done with any format!" but yeah whatever, I did it with that format cause its a nice format! and I can just throw it into a pyi file before I start programming so everything I type out matches up with what I expect to need in a testable way too, easi
This looks pretty cool - and i am sure you put in a ton of hard work into this. Congratulations. A couple of suggestions: 1) Check out blockly, an open source framework for creating web-based visual programming environments. App Inventor from MIT uses this for their visual android programming environment. [Reference](https://github.com/google/blockly) 2) During the youtube presentation, there were times when you lookied into your laptop and continued talking. It might be more effective to look into the camera during times when you want to talk, and shift focus to the programming environment for demo purposes alone. Just my 2 cents. Great work though.
This would fit better at r/learnpython
Python 3 support just arrived and it's 3.4, not 3.5 :( This inflexibility is exact reason why I would prefer Heroku over GAE.
This pep is describing a new syntax because the previous type declaration through comments *is weird.* Moving the type declarations a first class citizen in the language in a good thing. 
this sucks. all the pseudo-static typing stuff seems ugly and poorly thought out, it will lead to a big divide wrt to what is considered "good python code" in the future. build something like TypeScript instead of messing with Python like this, please
Can’t stress that point enough: profile! Trying to guess where the performance hits occur seldomly works.
What does "listen to a source continuously" actually entail?
same opinion. i also expressed that on the mailing list, but their argument was this (paraphrased): &gt; the RFC is just for typing via parameter annotations and therefore contains no syntax addition. cramming that in there would make the RFC touch too many parts and be too complex. so we just work with what’s there syntax-wise and leave it to a later RFC to get in typed variable declarations.
How are you doing the image tracking. kalman filter/particle filter with feature matching?
You can use ```help("modules")``` in the interpreter for modules included with Python. For items installed with pip, you can use ```pip freeze```
What have you done? Just read? What are you focusing on the language? Trying starting out with logic the stepping stone of all languages. You can focus on logic through using Python. If you want examples there is also https://automatetheboringstuff.com
If you look through similar topics you will see this is the most common answer: build your own project. It may be something really small at first, but it is important that you build something real. That way you will encounter real problems and will be forced to face them. If you have no idea what should you build, here is subreddit that can help: /r/beginnerprojects/top/ Also, for begginer type questions you should use /r/learnpython
What you are asking is entirely doable. The question is why you would want to.
Sending data to all peers won't scale. As a general rule, a full-mesh structure won't scale. P2P networks typically have a routing mechanism, by which you can send a message to some neighbor and it will eventually end up in the target. So you also need a lookup mechanism. If you are interested in P2P I recommend you read the Kademlia whitepaper, and make and implementation to fully grasp it. Then you can read on into more complex P2P networks, or try your own ideas :-)
If you're running Flask under uWSGI, then there's stacks of things you can use without including external dependencies. Look up the spooler, mule or hell even the plain http routing stuff.
Why not start with writing some simple scripts? Automate some problems that you face as a computer user. - Extract some information from logfiles - Use Python to fetch data from websites and parse the information and display it in one place (prices for computer components, cars, the current lunch dish of your favorite restaurants) - Organize your MP3 collection or your photos (extract ID3 tags / EXIF metadata) Do something practical. That is the reason you learn Python in the first place, right?
Normally, yes typing via comments would be weird. But here the types aren't actually ever processed by the interpreter. So it makes more sense that they are comments.
Internship? They better have hired you!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Run, do not walk, and get yourself a copy of docker. 
And JupyterLab soon!
Isn't there a website around that has like 500 simple project idea.. I'm trying to find it for you. That's my biggest problem.. I can never come up with ideas.
One thing I dislike about this approach variable type hints is the lack of introspection options. Correct me if I'm wrong, but there is no way to determine the variable type given a variable at runtime? This possibility exists for function signatures. I am not certain when this would be useful however.
&gt; no way to determine the variable type given a variable at runtime? Can't you do `type(var)` or are you referring to something else?
If you do: `a: int` you probably wouldn't be able to do `type(a)` and get `int` back. If this PEP was to be accepted I really think there should be a way to get `int` back.
I'm pretty new to this too. Beautiful Soup 4 seems to be a fairly well used tool for scraping data into a structured format. I tried watching a few tutorial videos to get me started, but [this video](https://www.youtube.com/watch?v=3xQTJi2tqgk) was by far the easiest for me to follow as a complete beginner.
Honestly i havne't really picked anything out yet. I've got recommendations on books and everything. I'm comfortable with SAS/R/etc. and wanted to get more into scraping websites to then analyze their data. Preferably things biostats related
I was going to say this was a very under-developed PEP (how does the grammar change? when and how is it being introduced? what are the rejected syntax alternatives?) but then I looked at the [commit](https://github.com/python/peps/commit/82fbafb1186456f126de503f5514ad8e6a3638b8) and GvR says, "This PEP is *not* ready for review! This commit is just to claim the PEP number." So this isn't a real PEP yet, and will probably be fleshed out a lot more before it's ready.
I would definitely be down. I sense a community forming!
I'm definitely interested in learning more Python as well as more stats! I just finished a coding boot camp but it's focus was on JavaScript. I'm currently running through a natural language processing class on coursera. https://www.coursera.org/learn/natural-language-processing The homework is in Python and a lot of interesting (and relatively complicated code) is given as a starting point and could be used as a reference for some well written Python. 
Try [codingame](https://www.codingame.com/games/training/). The easy puzzles tend to focus on one area each. Lots of different concepts are explored. Makes it simple to figure out what to code next. Note that it's not a set development path. One puzzle doesn't necessarily connect to any other and you might have to learn things 'out of order' of what would be taught in a programming class. But as long as you're ok with searching online to find the answers, it's pretty good for re-inforcing concepts.
Or possibly a streaming request. 
That's a bummer!
I can second this. Codecademy is a great resource for anyone learning Python.
Kind of... but the point of PEP526 is to introduce a nice syntax for type hints, and it omits optional specifiers. I'm not really sure if the original PEP484 is only supposed to apply to function declarations (which is what all the examples do) or is it's also supposed to apply to variables.
Hey, I would also like to contribute ;) I'm also quite a beginner but worked trough web scraping in "automate the boring stuff", so I know the basics of scraping. A discord would be nice?
Code Academy has really helped me retain some things. I did one course and nothing stuck so I did code academy next and things began falling into place. I've also been going through and trying to rewrite ally bash scripts as Python. Its great because I know exactly what my bash script does, so I'm more focused on translating then starting from scratch. Once its done I try to find a better way to so it that only Python can do. 
PEP 484 applies to type comments too, and I assume the syntax for variable type hints will be the same - there'd be no point making it different. PEP 526 is only a draft, and was only created to reserve the number (source: somewhere in the comments on this post), so I wouldn't make judgements based on its current state.
Most websites are built with JavaScript these days, so BS4, scrapy, and LXML are great for parsing HTML but won't do much good if you're trying to capture a rendered element of a modern webpage. Check out selenium, used in conjunction with Google Chrome, headless chrome or phantom JS. That's what I've used for quite some time to scrape complex Javascript laiden webpages.
depending on what youre tryign to learn and your background in statistics already, but i'd recommend discovering R by andy field, its a great work
yeah i plan on doing that :)
i already have a discord for gaming but could turn it into a programming one too haha, send me a PM if you wanna talk
kk will do thanks
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
It helps that Jupyter Notebooks doesn't do chained completions. It forces me to assign an intermediate variable. (I usually do the alphabet, a, b, c). Then once it works go back and give good names to the variables and done.
&gt; I'd have to agree with Raymond on this one, seems like fanfare. Why don't tau proponents just write &gt; &gt; &gt;&gt;&gt; tau = 2*math.pi Because this way, tau would always stay a second class citizen. That would negate its purpose to be a more reasonable, simple choice for a circle constant. Always redefining it relatively to pi would increase mathematical and code complexity (which is exactly the opposite of what tau aims for). &gt; This is especially true since tau is used for so many other constants which predate its proposed use here. I hear this very often but I don't get it at all. pi (yes, written as "π") is also re-used in mathematics for many different things, e.g. as a name for projections or permutations. Re-use of variable/constant names is unavoidable and pi is definitely not an exception. The context in which you use it should always make it clear.
Beat you by 7 seconds :) https://www.reddit.com/r/Python/comments/4x92rf/pep_628_got_accepted_introducing_the_mathtau/
For the same reason tabs v spaces triggers people: programming languages are more than just syntax, and this is one of the reasons Python is so successful. I'm only here because I saw `import antigravity` and then googled a Python course. (I was a Node.js victim before). Addition: Disclaimer - I do use tau.
Hey! I'd love to do this. I'm actually going to be making a Reddit-scraper soon using PRAW, but any sort of data mining / scraping would be interesting to me. PM if you have an idea on something.
In case it helps with examples, I've done a bunch of that and written about it here: https://bigishdata.com. Definitely fun stuff.
I think they actually have a project to do that emulation. However, if you use CFFI, you can support both very easily.
Hey I can help in your learning processes. I have used bs4, requests, and scrapy and recently released a search website based on live scraping data. Tell me if you guys need me and where you need me to help 
I too am interested in machine learning and nlp as a hobby. I set myself a goal to write a somewhat reliable classifier for news topics. To get a corpus together, I used a tool called Scrapy to crawl english news websites along with another tool called newspaper3k to extract a plain text article from each url. As well, I targetted news websites where there was some sort of metadata available in the html that said what topic(s) the article fell under. you can check it out here https://github.com/psiopic2/psicrawler - I hope it helps. with that data btw, I used scikit-learn to create my classifier. 64% precision. Not great :( I did this in the past using solely wikinews and NLTK and got over 80%. Probably my feature extraction is not good in my second attempt (this second attempt is not on github currently)
You're awesome for making those docs. They're even linked from the official docs! Great work
They may miss that step, but it is documented in a very straightforward way: http://docs.python-requests.org/en/master/user/quickstart/#json-response-content But if you don't read the docs you can't really say that a behaviour is unexpected. Most modern api will return a json answer always, with errors and whatnot so raising an exception on 404 makes little sense, specially when reading json.
~~The only thing I don't like about tau is that it makes Euler's Identity ugly. And that's a cardinal sin. =)~~ I done goofed. See comments below.
you can do error recovery based on the viewtype if they are registered as such or based on accept headers and you can also do retries in requests itself. Meaninless noise in sentry might be exactly what you need to discover a fault in some other service that is not reporting to the same sentry instance. I haven't had need for circuitbreakers inside python but I'm pretty sure you can probably have one inside transports of requests as well. You can also do 503 responses for some kind of exceptions and that will make your code much cleaner and is the whole point of exceptions, making you centralise error checking in one place.
Any time!
I thought I deleted this message but I must have not hit confirm. I was thinking of something entirely different or my brain was offline because yeah, it's FAR more beautiful in my opinion using tau. *shrugs*!
Anytime! I also suggest Real Python (eBooks) and Enterprise Python at Paypal (videos) to further your knowledge and take it to the next step. 
[Automate the Boring Stuff](https://automatetheboringstuff.com/) with Python- Chapter 11: web scraping. Only started it yesterday but already making progress. Simple examples to follow along with. Highly recommend. 
We do it at work more and more. The reason is pretty simple really: with PyCharm you get the benefit of static typing in that you get typing errors pointed out to you. And it's as-you-type You want to add more of this the more stable the code is and the lower it is in the stack. Lower libs should be heavily typed while top level code like django views don't need any. 
I agree that that is a good use but I think you have some problems with your naming convention if you need the type comments to make that clear. 
thanks man ill def take a look at that!
The data would be in \_\_annotations\_\_ I believe. For classes at least, not for locals.
Thanks. Nice list for the basics.
Just a suggestion, but while you're scraping your data with Python, you might also analyze the data with Python, using various libraries (e.g., Pandas, numpy, scipy and matplotlib). If you're familiar with the data, you could write a script to do the scraping, cleaning, and analyzing all in one go.
&gt; Because this way, tau would always stay a second class citizen Maybe it should be? All I could find about this "constant" is just a manifesto page and some BBC news. It doesn't sound like it's an actual constant. Moreover, /u/DadAtH_me [points out](https://www.reddit.com/r/Python/comments/4x92rf/pep_628_got_accepted_introducing_the_mathtau/d6dmr2f) that it looks like Tau is used in different places (not in this sense though) so it looks like it creates more problem than it solves if it solves anything.
I don't get what is the level of math literacy these decision takers have. Nathaniel Smith fought hard to the bone to get the most basic and the crucial thing, matrix multiplication operator but tau goes in by the BDFL. That really doesn't make any sense. And other than OCD nothing makes tau useful other than contaminating namespace. 
Yeah, cause that approach makes perfect sense. Instead of having 1 unified codebase let's have 2 that are totally incompatible and each with half the resources...
Thanks cap.
I wonder how many of those **np.zeros** could have been **np.empty**...
I understand people having doubts about including any form of static typing in python. I have my own share of doubts. But this PEP is about syntax for variable declaration, there is not point weeping for the inclusion of type annotations here, what's done is done. And the proposed syntax is pretty standard. 
It's https requests.
ATBS is a good place to start. There are even a series of YT videos of the chapters you can follow that are well articulated. It's been my goto source for high school students who come to me with a coding interest. I send them there, and they usually come back over the weekend with a working program/text game. Edit: here's the video link Automate the Boring Stuff with Python: http://www.youtube.com/playlist?list=PL0-84-yl1fUnRuXGFe_F7qSH1LEnn9LkW
Clickbait titles seem to be the norm now. Top 5 organs a doctor should know about. You'll never guess number 3!
I'm not 100% sure what you mean, could you clarify?
This is more of a subnetting question than a Python networking question to be honest. If you'd know that a /28 for a class C ip range was 255.255.240.0 you'd have been able to surmise your way through it with some simple list comprehensions. You likely were expected to understand the networking concept more thoroughly than knowing about the library called ipaddress in Python 3. I would start there. When someone says Python network programming I tend to think of communicating over sockets, setting up a Python web server, knowing twisted and such.
Thanks for the help! What I am trying to achieve is something along the lines of *one peer sends data and all other peers receive the data*, so I thought that a mesh network was a good idea. Besides asyncio, do you have any other guidelines for me? P2P internals are kinda new for me. Thanks again.
Guido got the link to the ViHart video included too! Masterful!! This puts Python at the vanguard of righting a wrong. Import this! 
I love this series, thanks for keeping it going!
Not a huge `math.tau` fan here, but let's be honest : *do we really care?* I mean, sure, if you do `from math import *`, this will probably introduce some unexpected errors in your code, but maybe that's because it is awful to do that? I regularly use `PI` in my code for "photo-ionisation rate" when I don't want to use `Gamma`, and no big deal. It will probably be used as much as never, but if people find that it is fun, maybe it is ok enough to include it…
You're not a mathematician, right? They love to come up with more elegant ways to do things. If the new version doesn't resemble the original anymore, that doesn't matter.
Not giving it a chance will obviously prevent it from going into textbooks. And since it's the more natural constant, that's where people want it to go.
https://automatetheboringstuff.com/chapter11/
Combine that with this https://automatetheboringstuff.com/chapter16/
Hint: Use glob to get list of word file. Iterate over each, opening using with and search each line you read for your string then output that as csv (",".join). 
Look at the author: &gt;James Le &gt;Aspiring Product Manager. K
Migra! Migra! GTFO! 
Yeah, why don't arithmetic mean proponents just write: sum(list) / len(list) instead of: mean(list) /s
Thank you, I put a ton of my hard work and energy into this. Check out my YouTube channel, it pretty much lays out what I did to get here. My first robotics (and vision) project was automatically finding and stacking jenga blocks into a tower. I already knew how to program, but that I learned over years of playing around. I started when I was ten, using YoYo game maker's visual programming features, which taught me about basic logic and variables, then I went from there.
awesome man, hit me up with a PM
The nerve, using [title case](https://en.wikipedia.org/wiki/Letter_case#Case_styles) in a title!
There is no better resource for web scraping than Ryan Mitchell's "Web Scraping with Python". Not for beginners, but if you know your way around Python generally it is very accessable. I run a small business that scrapes websites for marketing leads and this book is an invaluable resource when you inevitably hit a wall in your code. Only seen one website that I couldn't scrape with the tools and tricks in this book.
&gt; It doesn't sound like it's an actual constant. All constants started out that way, it's not less valid just because it's not in vogue yet.
Has debugging been orders of magnitude slower for anyone else? I'm working on a QGIS plugin, and debugging through PyCharm is essentially unusable now. Something also changed where if I accidentally edit the plugin file instead of the project file, it no longer pops up and warns me. Seriously thinking of rolling back until 2016.3 comes out.
Yes... However I have to ask: If this is your own website of patients, you should be able to ask (whoever runs your database) to give you that information. If this is not your own website, wtf are you doing scraping someone else's patient info, and wtf is someone else's patient info doing exposed to you?
Honestly if the goal is to learn, by all means go bs4 but if the end goal is all you care about(scraping) I would use scrapy. Much more flexible in terms of handling requests and ill-formed responses and will save you a lot of boilerplate you'll need to hook up to your bs4 code every single time... What I mean is, using soup to write a scraper is like using jinja2 to write a webapp instead of Django. Scrapy like Django does the boilerplate for you. 
This may be useful: https://automatetheboringstuff.com/chapter11/
Hey its great to see so much enthusiasm in this post. I am new to reddit and am surprised to see how positive the reddit Python community is! I have done some serious scraping (at the terabyte scale) with Python and presently trying to learn the machine learning side of things, so exactly the opposite of you I guess. Wanted to wish you the best of luck with your study/work group!
I thing the `six` package should introduce some kind of intermediate syntax like `# coding: six`
Locals don't say "US border patrol" they say "immigration". Or in the case of Mexicans, they say "la migra!". 
&gt; then the implementors of code should follow that convention when they implement formulas and algorithms. So you're advocating for 1-based indices? If you're never tried it you'd think 1-based indices are great or at worst, the same. 1-based indicies screw up most of your loops.
&gt;Things are sometimes confusing &gt;We should make things more confusing!
You could try looking up mechanize for pyhon. It's used for interacting with user-input on webpages (i.e inputting a name in a text box and clicking the print patient info button). 
Wait, what? Really? To convert existing patient info to an e-chart (which I presume is some sort of electronic chart) you are : - Querying an exising electronic database using some sort of proprietary web frontend - Printing that information out individually for each patient - Faxing those printouts to a third party company - Third party company will then re-digitize those faxes and/or perform data entry for each patient? Am I getting this right? Because if so, this has to be the most astoundingly inefficient way to do this imaginable. Yes, python can do what you described above fairly easily (there are several packages). Google python browser automation. But parent is right, just get the dba for the existing database to run the correct query and send the results (digitally) to the third party company compiling your e-charts. This should save a TON of work on both sides.
Average drops to 20 moves with three players and 15 for six. (tweaked to support multiple players) https://gist.github.com/paul-schwendenman/92f1d530028260ce71acd77329310093
Bs4 is good for learning about web scraping or doing small tasks. However, if scraping will not be your main task (e.g. I need it to just get my data for doing machine learning) I would suggest to learn the Scrapy framework.
Is there a reason you can't use pdb?
It would be, but if they start doing that... I feel like then it's re-implementing Cython annotations. Which now that I think of it; if Cython could simply take Python 3 type annotations and spit out C modules, that'd be fun.
Hey, I have tons of experience scraping things, I think I've dealt with 50 + different websites. JSON, XML, CSV, XLS, html tables...you name it I've been there. Hop on https://devchat.devolio.net/ and sign up for our slack team, ask on #python or msg me (sigma).
If you're running it in a deep learning world, 5.2 gigs is nothing, and the initial load time is inconsequential
https://github.com/python/typing/issues/258
https://github.com/python/typing/issues/258 for the actual PEP as it evolves.
In /r/python because /r/python readers might find it interesting? 45 upticks and counting.
The point is it doesn't have to be 5.2 gigs. Pickle is the wrong file format to save the data in. Especially when you're adding up bandwidth costs to share the data, time to download the data, etc.
τ isn't a joke, it's actually useful. I prefer I over π in everyday work. 
We funded a startup (now defunct) which sought to add a geospatial layer to news articles. I helped them to write their code to identify places using [nltk](https://nltk.org) as a [web service](https://www.webpy.org). If you would like assistance, please let me know.
Doesn't everyone love to use τ/24?
Only for multi-server settings. On single server, you can also use [the IPC backend](http://channels.readthedocs.io/en/latest/backends.html#ipc), using POSIX shared memory.
Nice! I have luckily almost completely managed to avoid roll-and-move-games with my kids (oldest is 8) so far. There are many nice little kids-friendly modern games that we can all enjoy instead of having to endure "the classics" (note in this country Snake &amp; Ladders is not known at all though, I only know it because it is frequently mentioned in game-design books etc, but there are other similar horrors to avoid). Also a number of free pnp-games that have the added benefit that the kids love to help out constructing the games, sometimes including coloring cards and similar. Search bgg (in particular last year's pnp childrens game design contest... looking forward to the new contest later this year).
Eli, nice post explaining the basics. What are you using to render math formulas? I see they are images, not mathjax but yet not codecogs or similar.
Cool :) Would you not want to know if range = xrange fails? Also, why not just write in Py 3.5?
Interesting. I like your use of .get() on this line: squares[player] = SNAKES_LADDERS.get(squares[player], squares[player]) Ive never used .get() before. Its basically saying: Look up the players new position in the dict. If that number is in the dict: set the players position to the value else set the players position to what it already way See here: http://stackoverflow.com/questions/11041405/why-dict-getkey-instead-of-dictkey 
Don't have a tutorial but I have had write an assignment to do a similar kind of thing. If you give me a bit I'll write something up for you which should give you some pretty good starting points. -------------------------------------------------- ##Assumptions To start off with I'm going to make a few assumptions for the sake of keeping things simple enough that you can work through the problem and not have to worry about these more advanced issues. 1. You do not need to worry about the security of your transmission. 2. You do not need audio streams. ## Plan So this guide is going to be a game of "fill in the blanks". So lets start by putting down what we know: &gt; Webcam video stream &gt; ??? &gt; "network activities" &gt; ??? &gt; Video stream displayed ###Step 1: Get the video stream from the webcam I'm sure there are lots of ways to do this. The one I have used is the python bindings for OpenCV; if you happen to be using Ubuntu these should be installed by default as OpenCV is used by Unity. From memory when reading from the webcam I believe OpenCV will read in a RGB bitmap format, in other words, a 3 dimensional array of X*Y*3 in which X and Y are the width and height in pixels of your webcam. ###Step 2: Convert the image into a format for transmission You now need to decide how you would like to transmit the image. Are you going to send chunks of pixels or are you going to send an entire frame at a time. You could potentially send multiple frames at a time if you wanted to. Lets assume you want to send an entire frame at a time. We now need to decide on how we are going to represent each frame into bytes. We have to do this because TCP/UDP work on bytes, not objects. This process is called encoding. Our process now looks a bit like this: &gt; Webcam video stream :: i.e. a series of frames &gt; Encode frame into bytes &gt; "network activities" &gt; ??? &gt; Video stream displayed With your background in C you should have a good idea on how this encoding process is going to work. Essentially we need to take each of our python objects and turn them into bytes in a predictable way. The predictable is important here as we will need to do the opposite of this later (decoding). Luckily we know that in RGB format that the max value for each item in the RGB is 255 and thus we are working with 8bit integers. What we can now do is represent our frame as a series of bytes where each 1st byte is the Red value, each 2nd byte is the Green value and each 3rd byte is the Blue value. `RGBRGBRGBRGB....RGBRGBRGB` Our next problem is we now have a line of RGB values, what information do we need to provide to know how to reconstruct this line back into a frame? The X and Y of course. So now we have 3 bits of information that we need to transmit per a frame: * `X` (probably a 32bit int) * `Y` (same size as X) * RGB values (X * Y * 3 8bit ints) To turn python objects into these kind of raw data types you will want to use the [standard library's struct module](https://docs.python.org/3.5/library/struct.html) or you might want to learn the [construct module](http://construct.readthedocs.io/en/latest/basics.html) As an example, we might want to encode the following image 5*4 (X*Y) (where each letter represents a pixel (which we know is actually 3 8bit ints) AAAAA BBBBB CCCCC DDDDD We might encode this as: XYAAAAABBBBBCCCCCDDDDD ###Step 3: Transmit the encoded data So here is where we need to start considering the type of data we are transmitting and if the delivery needs to be reliable or not. For many streaming applications, the speed of delivery is more important than ensuring every bit of data gets transmitted. Thus many streaming applications/protocols (e.g. Skype, VOIP, online games) will use UDP because the loss of a small number of packets is insignificant to the overall transmission. If you are interested in reading more about why you would want to use one over the other in real time situations I suggest you take a read of [this link](http://gafferongames.com/networking-for-game-programmers/udp-vs-tcp/). Alternatively you may be more concerned with the reliable delivery of your webcam at the expense of either stuttering or buffering. Buffering being a technique to avoid stuttering. Applications like youtube and netflix will use TCP + buffering. At the end of they day, because UDP and TCP are both socket abstractions you can easily swap one for the other if you wish to change your mind. ###Step 4: Decode the data back into a frame Looking at our plan: &gt; Read Webcam Video Stream &gt; Encode Frames &gt; UDP or TCP connection &gt; Decode Frames &lt;&lt;&lt; we are here &gt; Display Webcam Video Stream Coming back to our example: XYAAAAABBBBBCCCCCDDDDD We now need to reverse the encoding process we did before, that is, we need to decode the raw bytes back into an object that makes sense. The first thing we will want to do is read the two 32bit ints from the socket. This will be our `X` and `Y`. Given these we now can work out how many pixels we need to read from the socket i.e. `X*Y` (remembering that a pixel will actually be 3 8bit ints). With all this data now read, we should be able to turn it back into the python object representing a frame. ###Step 5: Display the frame For this you might want to use something as simple as GTK or even OpenCV which should be able to display RGB bitmap images. ## Conclusion Hopefully that's enough to get started. There are other considerations that you might need to take into account when deciding on your network protocol (such as including a frame number so that you can ignore old frames in situations when they arrive in a weird order (1 2 5 4 3 6 7 8)) If you have any other questions happy to help out :)
I put pickles on everything. McClure spicy sweet are the best 
I'm not sure you understand the point of both of those arguments... &gt;The idea is to get τ into textbooks and have one of the next generations of mathematicians use it. And this statement holds no weight with me, since I've never seen any good / convincing reason to actually switch the convention to using tau. In my experience looking at the topic, every argument given to do so has just as strong if not stronger counter-arguments, on top of all the other very good reasons not to switch that usually aren't addressed by the tau movement.
Yeah, I figured that, and I meant to file an issue (note to self: do it) after I didn't get very far fixing it myself ;).
This works, it absolutely works and will get the job done. But the overheads in comparison to hunting the source of the data out are quite large; you're talking about using an entire browser (headless or not) to render a page in memory and then plucking the bits you need out of it. All browsers will download everything on the page - all images, scripts, styles: 95% of which you do not *need*. The majority of websites out there that require javascript to render the page will be retrieving their data from a private undocumented API. It is much easier to go straight to the source of the data than to render the entire page; it's also a lot less likely to change based on the design of the page and therefore coding for it is a better long term investment of time. These APIs usually come in the form of parseable data (JSON, XML, HTML) so dealing with them programmatically is usually fairly straightforward too. All you have to do to find the source is to have a good understanding of web technology and a comprehensive knowledge of the Network tab in Chrome Devtools.
I think they're trying to add ptpython features to ipython, and in the meanwhile there is ptipython.
you think he doesn't trust himself? it's your problem if you don't want his pickle.. oh and btw, loading a pickle is no more unsafe than doing `pip install` for a random PyPI package, but most people don't hesitate to do that.
you're not wrong, you're just an asshole. actually, you're wrong also. probably for a beginner tutorial they didn't feel like adding two more dependencies (libhdf5 + h5py). besides, you can do just as good as hdf5 with pickle. just using `protocol=pickle.HIGHEST_PROTOCOL` drops it to 1.7GB - and it saves/loads in only a couple seconds Using `gzip.open` to wrap the pickle you can get to 308M - better than any of your hdf5 options - although now it takes longer to save/load. btw for deep learning the loading time is by far the most important parameter, you've gotta keep that GPU saturated.
I didn't know about the Vi Hart video. Just watched it, it's brilliant! Now I wish I had learned it that way back in the day!
Thank to all of you for the advices and the help. Finally I've managed to make my script work applying the following changes: 1) Change the Command Prompt codification to `855`. chcp 855 2) Then change the Python script codification to `855` too. # -*- coding: 855 -*- 3) Decode the input following the codification. print(raw_input("Message: ").decode("855"))
That's in OP's version as well.
REALLY
Thanks for the kind words, everyone :-) (Replying to one comment so I don't spam the thread)
&gt;It is not the place of python developers who are implementing things to make these political decisions. It just makes for hard to read code. Could you be any more dramatic? It's not going to hurt anyone, is likely to get more people interested and hardly makes any code 'harder to read'. &gt;The tau proponents need to win the battle in the academic field not try and sneak a victory by slipping the value into computer code. Seriously? Did a 'tau proponent' punch you in an alleyway? Grow up.
Using [Markov Chains](https://en.wikipedia.org/wiki/Absorbing_Markov_chain) and NumPy in case you want to know the exact value (around 36.193070219): http://pastebin.com/F85yvDGM
Interesting that you both assumed what I thought was a "house rule" allowing a player to win even when "over-shooting" the final square. When I was a kid, you had to land on it exactly. We wrote nearly exactly the same [code](https://possiblywrong.wordpress.com/2015/02/20/analysis-of-chutes-and-ladders/). (In the version requiring landing exactly on the final square, the expected number of turns is about 39.5984.) You can extend this to multiple players... or at least, feasibly, to two players, as described [here](https://possiblywrong.wordpress.com/2015/03/11/update-chutes-and-ladders-is-long-but-not-that-long/). In that case, the exact number of turns is about 52.5188/2=26.2594 (again, requiring landing exactly on the final square). 
Literally did the same 3 weeks ago. I explained to my son the since the player doesn't make any decisions, we can just let the computer run the game. On vacation now, will post my code Sunday.
At the minimum they could output to PDF instead of printing and send the PDF's or if the company is really arsebackwards and insists on Faxes use a softmodem.
Elsewhere you said &gt; the idea is to get Tau into textbooks so that the next generation will use it. That I see as sneaking it in. If mathematicians aren't using tau and don't see any reason to use tau, then why change our education system? Why change our code? Just leave the convention as it is. They need to win that argument first, and convince people that tau is so much better on the merits to actually get published papers to want to switch. Not to try and cultivate a generation who expects a different convention. It's like the apple/mac. You can give them away to schools all you want, but people still have to learn windows when they grow up and enter the business world. Alternatively you can have a product like Linux which has compelling reasons business want to use it and then you get students actually wanting to learn it. A good standard will pull people to it, it doesn't require being pushed along. Let people define tau at the top of their code if they need it, and if it ever becomes remotely popular then put it in math.
Oddly appropriate that this script for **Snakes** and Ladders would be written in Python. ^^^^...get ^^^^it?
Not all papers used 1 based indexes, but it is more annoying to have to switch back and forth within a single language. It would be nicer if there were one standard that everyone followed, instead there is a standoff where the paper declares what kind of indexing it will use and the programmer adjusts that to match his preferred languages approach.
It is NOT a more natural constant. It is just there. pi also shows up in numerous places without the 2. Search for tau in github you'll see why this is a very very romantic and ill informed practice 
&gt; Pickle shouldn't be used General advice, Pickle probably *isn't* what you want, in general. Remember, Pickle carries a lot of overhead because it needs to contain *type* information so that classes themselves can be de-pickled. It's also, as a result, very slow. It's not guaranteed to work across Python versions.
The part about the fundamental matrix is very interesting, I find the wikipedia article a bit terse. I'll have to read up on that..
Yes, newbie here and wanna get in to the train.
It's still important to point out that you shouldn't just open some random dude’s pickle. You don't have to even know what pip is to have a dirty pickle trash your entire machine.
likely some code not written in Python ;) if (weAreHiringDevelopers = true) { makeDevRedditsTrendy(); } else { showShockinKardashianPics(); }
Why my comment has -9 points here, and the same comment on /r/programming has +9 points? https://www.reddit.com/r/programming/comments/4x02fm/attempted_github_breach_on_kenneth_reitz_of/d6bqr66 Readers of these both subreddits are pretty much the same, so sentiment towards my comment should be similar in both subreddits. Therefore, I suspect some kind of brigading here, on /r/Python...
Yes, please! I've been struggling with this for a month. Python doesn't make *everything* easy, it turns out. The more I learn about machine learning the more I don't know where to start. Edit: I guess I'm in the opposite boat. Strong in Python but weak in stats.
Agreed, completely. Most of the database view-type sites I want to scrape are all javascript. The first time I scraped them with BS4 I was *so* confused.
I saw on Twitter that Kenneth Reitz was claiming more traffic to his Hitchhiker's Guide to Python page from an article on hacking Pokemon Go: [https://twitter.com/kennethreitz/status/763578391924658178](https://twitter.com/kennethreitz/status/763578391924658178)
I've got a question : Have any of you ever tried [Hickle](https://github.com/telegraphic/hickle) ? How does it compare here ?
Yeah, that's why I'm catching NameError -- on Python 3, referencing `xrange` raises a NameError, so it just uses the Python 3 generator version of `range` directly. But on Python 2 no error is raised, so "range" is set to xrange (the generator, Python 3-style version). A lot of folks do it the other way around and name it "xrange": try: xrange except NameError: xrange = range But I prefer writing future-facing code that looks like Python 3.x (range), rather than backward-looking code that looks like Python 2.x (xrange).
Excellent, thanks! When I finished my code, I thought "drat, this isn't correct for 2 or more players ... but it'd take too long to fix it". But your tweak/fix doesn't change the code much at all -- nice work!
This has little to do with the specific dataset in question, it was just a low hanging fruit I found to use as an example. If there are any 'data scientists' reading they should know not to use pickle to store data. HDF5 was literally designed for the task. Ignore where the sample dataset came from.
&gt; probably for a beginner tutorial they didn't feel like adding two more dependencies For a beginner tutorial on neural networks where the end user has already had to get TensorFlow correctly installed? &gt; drops it to 1.7GB So ~4 times larger than the hdf? There's a reason HDF5 is used by NASA and the likes. It's a data format designed for storing a lot of data. Pickle is not.
I have not. It looks like a compromise. 
&gt; pi also shows up in numerous places without the 2. you mean like euler’s identity? but *e*^(*iπ*) = −1 is a worse formula than *e*^(*iτ*) = 1 &gt; Search for tau in github you'll see why this is a very very romantic and ill informed practice pi is used in programming because it’s defined in a constant in many languages. it is defined because mathematicians are used to it. they are used to it because one or two guys started using it in their influential papers centuries ago.
"=="
Reddit is written in Python, though! make_devreddits_trendy() if hiring else show_kardashian_pics()
So convince a generation it is worth doing. In particular convince the current generation it is worth doing and get them to change their behavior. Don't just push the responsibility for doing this on a future generation. The attitude you are describing is precisely the kind of attitude that leads to serious bugs in code: "I could code around this case, but its not likely to come up for me in my usage, so I'll just let the next guy deal with it." If the tauists are unwilling to dogfood their own notation then nobody should take them serious, and if they are dogfooding it, then they need to keep dogfooding it until they actually get some traction.
That's just shit code, Python doesn't help in that regard.
Ah makes sense! I was looking at it backwards 
&gt; it's your problem if you don't want his pickle.. At that point how useful is the tutorial?
Why not? It's easy to learn, easy on the eyes, and found in many programs -- either for user scripting or as the code base.
Do you know if that would be able to print something? I read that things get difficult when trying to print because you have to interact with windows? 
Can we get a quick ELI5 of pickle and HDF5?
&gt; pi also shows up in numerous places without the 2. pick any engineering book &gt; pi is used in programming because it’s defined in a constant in many languages. &gt; it is defined because mathematicians are used to it. &gt; they are used to it because one or two guys started using it in their influential papers centuries ago. Exactly and that is **centuries** ago. They can't force people to stop using Python 2 and switch to 3 and now they are trying to convince to drop pi. That is just funny
Ah, I see. That's a pretty good reason then.
Cool, I wasn't aware of the 'Optional' type hinting. Thanks for demonstrating that.
they’re not saying that they have a good chance at succeeding. they‘re just saying that they have the better mathematical (unlike “inertia of human habits”) arguments
they probably do, yeah
No it isn't. It's idiotic. It's also up there among "Pi Day" and Neil deGrasse Tyson memes on the list of things loved by the "I Fucking Love Science" crowd, which is why there's so much pushback on it.
* Pickle: Python-specific serialization format. * HDF5: extremely high performance, self-describing file format frequently used for storing scientific data; can be thought of (to some degree) as a file system within a file.
To be fair, Pandas does lend itself nicely to this. For example, try calculating how many NaN entries are in a dataframe overall... total_num_of_nans = sum(in_data.isnull().sum(axis = 1).tolist())
Yay! No more node and miscellaneous javascript errors trending.
&gt; but it STILL reads/writes faster than HDF5, even on magnetic disk. Um, not according to my tests. Pickle took twice as long. &gt; and the gzipped pickle is SMALLER than your "optimized" HDF5. # Pickle is the wrong tool for the job. Pickle is fine for hacking together some data passing. HDF5 is literally designed for it. https://www.hdfgroup.org/HDF5/whatishdf5.html Additionally hdf5 has MATLAB, C, Java, et al support. So if you have any other tools in your tool chain (as most of us at work do) it's much simpler to deal with one file format. What next, suggesting they use CSV files?
It was in jest, that's one of the main reasons I use pandas. Also in your example you can just call sum twice if all you want is the total: in_data.isnull().sum().sum() 
I just mentored 2 interns on their project building an internal web app for our team. We pushed them towards using Python because of Django. It is kept well up to date and there is no shortage of documentation and tutorials for building even quite complex web applications. In my mind it is perfect for simple REST-ish projects that won't face too much traffic but that you want to be able to modify easily. I've found that it's best deployed as a WSGI app behind gunicorn/nginx and containerized if possible. You're right that "web development" is a very vague term. It even overlaps significantly with "web design" if you interpret design as including Javascript interactions. I like to believe that "web development" can be used for frontend and backend development. EDIT: that is not to say that Python is slow. I just find that it's more forgiving during development.
Seems like you already have found a lot, I'd join if your group has not become unmanageable yet ;) 
nice! you should add the `exceptions` arg to the readme, though
You should add async support, since asyncio coroutines sometimes need retries. Also, there is some exceptions, which should not be handled (StopIteration, StopAsyncIteration and [future ones!](https://www.python.org/dev/peps/pep-0525)).
ronpaulitshappening.gif
[ronpaulitshappening.gif](http://i.imgur.com/Hf1yqgr.gif) --- ^(*Feedback welcome at /r/image_linker_bot* | )[^(Disable)](https://www.reddit.com/message/compose/?to=image_linker_bot&amp;subject=Ignore%20request&amp;message=ignore%20me)^( with "ignore me" via reply or PM) 
Unfortunately, game engines and frameworks for Python are all outdated or immature. You can use them, but usually have to do a lot more yourself. I recommend the [Godot](https://godotengine.org/) engine, which has a scripting language that is similar to Python, lots of great features and is completely free and open source. There's a tutorial series on [gamefromscratch.com](http://www.gamefromscratch.com/page/Godot-Game-Engine-tutorial-series.aspx).
If you are suicidal or know anybody struggeling with depression you should seek help [here](https://www.suicide.org/international-suicide-hotlines.html). Bleep Boop! I am a bot. If you found any problems message me [here](https://www.reddit.com/message/compose/?to=Depression_help_bot)
If you are suicidal or know anybody struggeling with depression you should seek help [here](https://www.suicide.org/international-suicide-hotlines.html). Bleep Boop! I am a bot. If you found any problems message me [here](https://www.reddit.com/message/compose/?to=Depression_help_bot)
"left" and "right" are just monikers. You could call it "red" and "black" or "top" and "bottom" or "charm" and "strange". The point is that each and every node has pointers with the exact same names and, furthermore, that when data is inserted or deleted, the rules of the BST are followed. It is the rules, I think, that you're really asking about. As long as data is inserted properly, smaller keys inserted into the left tree and larger or equal keys on the right, you will be able to find the data in subsequent searches. Make sense?
Been thinking about learning Python. Saw trending subreddit and subscribed. I guess this the beginning of a new journey eh?
Its Stallman sending you a signal to start developing
I join you in thinking Vi Hart makes great recreational maths videos. My son and I had a great time playing with [Hexaflexagons](https://www.youtube.com/watch?v=VIVIegSt81k). I wonder if Vi uses Python? 
What's a snake? 
The power of tau?
Interested in fantasy football? I'm trying to teach myself Monte Carlo simulations to show playoff percentages for every week. This includes scraping the ESPN website for any given league.
There's no way /r/pokemongodev is not in some part responsible for the increased popularity. ;) Most of the programs written in that sub are in Python 2.7.x, and it has caused lots of code-naive (not meant in a pejorative way) users to flock there and inquire about how to set up the programs (with various motives) since many of them have no GUI, require use of the terminal/command line, require installing dependencies with pip, etc.
Can't stop the signal!
the most important thing is motivation beyond just "it would be neat to learn". Try to think of things you want to automate, programs you wish existed, reddit bots (jk pls DONT)
Is there any thing special that needs to be done to have async support? 
That is fascinating! I know nothing about Python and I really got curious about the language. Any videos/articles/book recommendations?
Hey, I'm a bit late here. I have been slowly starting a project that is in line with what you're wanting to do. My project is: scrape Ferry data (schedule, remaining capacity, etc.) and make present it in a more accessible way. Beyond that, I'd like to do some analytics of the data. So far, I've been working with BeautifulSoup and Postgres. My biggest challenge is making sense of the website structure (various chunks of data are on a assortment of pages). The project interests me because the data is relevant to me, however I understand that you may not be interested as it's likely not relevant to you. FYI, the site in question is: bcferries.com
Blog Post: "DAE think JS is a garbage language!" Blog Post: "Why you are a horrible dev if you aren't using &lt;flavor of the month&gt;" Code Pen: "Look at ridiculous contortions you can go through to do things in only CSS" 
There doesn't seem to be anything specific which triggers page to appear as trending. The Rick &amp; Morty sub was trending the other day but we were asking the same thing as there wasn't anything special going on in that sub either.
pastebin.com
Post the damn source code to pastebin
What about using gzip instead of zip for the pickle?
Ah yeah, I'm familiar with redis, but what I'm looking for is something similar to the python sqlite library, in that it's really lightweight and doesn't require an external installation beyond a `pip install ...`. That's why I compare it to SQLite. I can just do `pip install sqlite` and I can create databases easily from within python.
Thank you so mutch it works now!
Yes
unqlite is nice. Gets the job done and doesn't really have any dependencies outside of unqlite itself.
Good finds - I think I'll play with those. Honest question: I'm curious about your use-case, whereby you'd really desire a lightweight NoSQL, versus just saving records into a local SQL store. What's your reasoning, and what problem are you trying to solve, if you don't mind me asking?
If you don't want/need anything too fancy, the [dbm](https://docs.python.org/3.5/library/dbm.html#module-dbm) module from the standard library might serve. Simple, easy, and nothing to install.
Should simply be if (weAreHiringDevelopers) { ... 
Semester is starting, I assume it's college students that need to finish IS related courses Which is why I'm here 
Because there are 4 posts in that sub
If you don't mind working with networking, you should talk to me. I'm trying to get some peer-to-peer utilities off the ground.
The underscvores are for reddit ___ example
&gt; What about using gzip instead of zip for the pickle? Then it makes the read a 2 step process and doesn't solve the underlying problems with Pickle. When you are sharing data files on the internet to help other people learn you should avoid something that has a big red warning on it's documentation page: &gt; ## Warning &gt; The pickle module is not secure against erroneous or maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source. What if medium was hacked? What if the author slipped something in there I didn't know about? At least with the source code I can read it, get help on it, etc before I run it. ## HDF5 HDF5 is where everyone is migrating to in terms of numerical data storage. [Look at who uses it.](https://www.hdfgroup.org/users.html). If you are collecting data you want it in HDF5. Case in point, it's what NASA uses to share their data: https://earthdata.nasa.gov/standards/hdf5 And I know others were laughing at the data size saying "It's *only* 5 GB". The NASA dataset *consists of 7,850 HDF-EOS5 files covering the period from 1 July 1987 to 31 December 2008. Each file is around 16 MB, bringing the total to about 120 GB.* How big do you think that would be pickled? The HDF5 group has blogged about it: [NASA DATA: Putting some Spark into HDF-EOS](https://hdfgroup.org/wp/tag/nasa-data/) They even extended it into [HDF-EOS5](https://earthdata.nasa.gov/standards/hdf-eos5), &gt; *HDF is the Hierarchical Data Format developed by the National Center for Supercomputing Applications. Specific data structures which are containers for science data are: Grid, Point, Zonal Average and Swath. These data structures are constructed from standard HDF data objects, using EOS conventions, through the use of a software library. A key feature of HDF-EOS is a standard prescription for associating geolocation data with science data through internal structural metadata. The relationship between geolocation and science data is transparent to the end-user. Instrument and data typeindependent services, such as subsetting by geolocation, can be applied to files across a wide variety of data products through the same library interface* Furthermore, [HDF5 is something that is starting to show up on resumes.](http://www.indeed.com/jobs?q=hdf5&amp;l=&amp;radius=100). It's a technology worth learning. tl;dr: If you're dealing with numerical data, save it in a hdf5.
I am not 100% proficient with namespaces but can you explain exactly how "math.tau" would affect a module's own definition of tau, unless you did something stupid like 'from math import *'?
&gt;now they are trying to convince to drop pi. They are? Did they state that they are removing pi? Or are they simply adding another constant for those who prefer it? 
Nick Berry has a blog post on this from Nov. 2011: http://datagenetics.com/blog/november12011/index.html That's a cool blog, btw. Check it out.
Well, I found [this](https://unqlite.org/) but I think you'd need to write some python to interface with the C. This appears to try to fill the same niche as sqlite, minus the sql.
I think he meant this: https://pypi.python.org/pypi/redislite
That's...not how it is. And the flow of charge should be shown the opposite of what it is currently. It's a shame that simple inertia is the reason why it hasn't changed.
&gt;That I see as sneaking it in. Please explain how having it posted in a public setting with a commit history and discussion is 'sneaking it in'. 
&gt;So convince a generation it is worth doing. It's funny that you are saying 'so convince a generation it is worth doing', yet any attempts at switching to it or making it more popular you are vehemently against. In other words, you are attempting to block the very thing that would make people more convinced it's worth doing.
It doesn't. I would think that most programming languages don't because when you change something low level, you can break everything. The change from Python 2 to 3 was apparently a major change, but as far as I can tell, the really big change was in how unicode and strings were handled. So no, for someone in your boat at least, I wouldn't think it's outdated.
I was going to complain about the same thing. I clicked the link early in the post to the `attr` docs, and was immediately turned off by it because I had no idea what was going on by the brevity. Continuing with glyph's post, it makes more sense. While I see how this *can* be nice, I also don't find it that useful for most of the classes I define.
I work with some scientists (the kind that does simulations and research). It's not going to work. Flat out, I'm saying your approach and desire is just not going to work. It's not you, it's them (that is, how they see it and how their concerns are architected). Why do they do a pickle? Usually because they want to insert one-two lines and it comes built in. Quite a few of them feel that any pressure on them to try something different will end up preventing them from getting some critical results ("more science"). The best occasion I had was showing one of them who had a simple entry point for an embarassingly parallel program was: from multiprocessing.pool import Pool # ... several lines later to the entry point for particle in particles: pool.apply_async(func, particle, arg1, arg2, callback=handle_result) It took five tries before she added that code. And it sped up her code almost linearly for each core. You'd think that'd be one hell of a carrot -- yet it took quite a bit of effort just to convince someone that a 32x speed up really was worth it.
The main thing is that wrapper should be a coroutine. And also take care about CancelledError exception.
&gt; I can just do pip install sqlite and I can create databases easily from within python. Or you can just use the standard library `sqlite3` module and not `pip install` anything.
&gt; you are going to run his code, after all. I can read his code first. I can confirm my packages came from other than him. I could compile them myself. Python code also doesn't have a big red warning label.
TinyDB (which you link to) is fairly popular. I have not used it myself (I generally use SQL unless I have a really good reason not to), but it seems pretty solid.
&gt; /r/pokemongodev Disappointingly isn't about poking Mongo devs.
Give [Vedis](https://github.com/coleifer/vedis-python) a try.
I wonder how many people came here expecting Monty Python.
&gt; For a delete, a not found is considered a success. I beg to disagree. If you want to perform an action on a resource, the given resource must exist in the first place. So, if you're deleting (or accessing in another way) an already deleted resource, the proper response should be "gone" (410 in HTTP response codes), and not "not found" (404).
GUI desktop applications are almost certainly C++, Objective-C, or a higher-level language binding to one of those. Server daemons are quite often written in C, though Java is also quite prevalent. Mobile apps are mostly Objective-C/Swift (iOS), Java (Android), Python (Kivy and friends), or JavaScript (ReactNative, WebViews). Embedded stuff and OS is almost certainly C. I humbly submit to you that there are a whole lot more web apps and mobile apps than operating systems. I suspect that embedded is where a lot of professional C programmers make their living. So again, I think what's most common is wildly dependent on your problem domain.
https://gitlab.com/mailman/mailman http://www.list.org/devs.html
Then you just screwed up the Idempotence if your REST API. 2 deletes to the same resource should give back the same result, at least in terms of success or fail. The status code can change....
75% of all coding gets significantly easier with Python while 25% of all coding gets more frustrating. Based on that math, you still have a net positive of 50% so it's a nice ride. 
I hardly needed to scroll to see that &gt;(If you don’t like the playful attr.s and attr.ib, you can also use their no-nonsense aliases attr.attributes and attr.attr). 
No indentations?
I'm on iPhone, so indentations are quite difficult.
CPython, i.e. the thing you download from python.org, is what the overwhelming majority of Python stuff runs on. The only competition here is PyPy, but that's still not nearly as prominent. Both CPython and PyPy have separate branches for 2 and 3 compatibility. The internal structure of CPython is easier to understand, but PyPy has more optimization potential, and it's more likely your optimizations will make their way into upstream. If you just want to do generic compiler work though, consider Rust - it's a new language, so there's still a lot of optimization possible in the compiler, and it's amazingly well documented.
fix the bugs def encode(pswd): return stringify(do_encode(pswd)) def decode(pswd: return stringify(do_encode(pswd)) add some docs the functions Don't do this... separator = '-' def stringify(old): new = '' for char in old: ascii = str(ord(char)) new = new + seperator + ascii return new You're using a global variable for the separator. Just put the value in so it's more readable. Don't code because you might need it someday. You can add it someday. You should also only have one set of stringify/unstringify functions. You have a forceReadablePassword, use it in the function (and not as a global variable). This is not so relevant for the exact topic, but very relevant in terms of ideas https://www.youtube.com/watch?v=o9pEzgHorH0
welcome!
Depends on task of course, but generally JavaScript with some database experience is more versatile. I know too many Python devs that create code mass smell because they try to do it all in a script. Other languages are more efficient for concurrency and parallelism when that matters.
&gt;As Mark mentions, math.e is useless (compared to exp()), yet where is the backlash for that still being in? This kind of reasoning is one of the many reasons why there is so much backlash. You just basically advocated adding tau because there is supposedly some other useless bit of the `math` library, so why not add your useless bit? &gt;Pretty sure if Tau could cure cancer I'm more inclined to think that tau is the cancer. Cropping up where no reasonable person wants it or needs and annoying everyone to death.
Hold on, how do you define greater than for a class that you have *no idea* how it's meant to be compared? That's like saying "Is fish greater than or equal to potato?". You have no clue. For example, if I say I have 2 complex numbers, a = 2 + 30i, b = 3 + 1i. Which one is "greater"? If you order by real, then by imaginary, it's a &lt; b. If you order by the magnitude, it's b &lt; a. Which one's right? The only thing you *can* (reasonably) easily define is equality. Most sane way of doing that is seeing if all of the attributes are equal, and if they are, then say they're equal. If they're not, they're *probably* not meant to be equal, so return false.
I also highly recommend the Code Academy Python course. It's a good jumpstart. Knowing the mechanics of Github will also help you tremendously.
Well, first of all, you've obviously never executed this code, as indicated by the missing parenthesis that /u/billsil pointed out, and various misspellings: "sep**a**rator" vs. "sep**e**rator", "for char **is** pswd:", etc. My normal thought would be, "Don't waste my time with code you've never even tried to execute." But, hey, the sun is shining, the birds are singing, and I'm in a generous mood. So .... (1) It's tough to figure out how code might be better organized, or whether it does the right thing, if we don't know what the pieces are for. The code needs to explain itself. When you write a new function, you're intending it to perform some task. Indicate what that task is, in a docstring (or your favorite form of documentation, if you run across something better than docstrings). def stringify(old): """Given blah, blah, returns blah, blah.""" new = '' ... Of course, "blah, blah" should be replaced with the function's actual parameter(s) and return value(s). This docstring doesn't have to be long &amp; involved. Just make a quick note -- for yourself or anyone else who reads the code -- about what the function is supposed to do. Perhaps later you'll rewrite the docstring in a more detailed form. You can also add example usage to a docstring. def stringify(old): """Given blah, blah, returns blah, blah. &gt;&gt;&gt; stringify('abc') '-65-66-67' """ new = '' ... The above example usage can actually be executed as a test, using the doctest module. So a good way to go about creating a new function is to work as follows. * You declare a function, indicating its name, what it is supposed to do, and how to use it. * Then you write the function body: code to perform the required task. * And then you can run the doctest, and see immediately whether the function you have written actually does what it is supposed to. Comments can be helpful for many variables, too. new = '' # The string we will return But even better are variable names that make it obvious what the variable is for. string_to_return = '' or maybe stringified_version = '' You can also put comments on imports, to indicate *why* you are doing the import. import sys # for .stderr And lastly, comment anything you write that is odd in some way, difficult to justify or understand, or was difficult to figure out how to write correctly. For example, why does the following line appear in the code? pswd = pswd[2:-2] I have no idea; you need to tell me. Similarly, I have no clue what the following is about. pswd = 'vs' + pswd + '48' Those two seem to be related -- maybe (?). Explain! (2) You have statements at global scope interspersed with functions. It's difficult to follow. And, as /u/billsil points out, you have a number of global variables scattered around. I would put your initialization code into a single function, perhaps called `initialize`. Then you can have a single call to that function at global scope. You can also have your file do different things, depending on whether it is directly executed, or imported by another module. Perhaps you want to call `initialize` in one of these cases, but not the other. if __name__ == "__main__": # Stuff here only runs if file is executed directly I would also suggest putting all your configuration options in a common place. They could be variables local to the `initialize` function. They could be in some kind of `dict`, or members of some object. There are lots of ways to handle this. I'm not sure of the best way for you to deal with the above issues, since I don't know what your code is for. Think about it. (3) `sys.stderr.write` is a bit low level. Use `print`. Instead of this sys.stderr.write("Yo!\n") Say this print("Yo!", file=sys.stderr) The above assumes you're using Python 3. You *are* using Python 3, right? (4) If you find yourself writing something like `x = x + ...`, then you're doing it wrong. Use `+=`: new += separator + ascii (5) And, by the way, the use of `ascii` suggests that you're not ready for modern character sets. The days of ASCII-only are gone. In Python 3 all strings are Unicode. And the `ord` function returns a character's *code point*. It is true that, for characters that lie in the ASCII set, the code point is the same as the ASCII value. But there are lots of characters that do not lie in the ASCII set -- over 100,000 of them. (6) Your key (`'pymailcgi3'`) is found in the code twice. Consider what happens if you change one of these, but forget to change the other. Problems like this are the reason for the [DRY Principle](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) \[Don't Repeat Yourself\]. (You also might consider the problems inherent in having your key hardcoded in the source.) (7) This line is strange: return str(res) In that context, the variable `res` is aways going to be a string. So the above will be no different from return res (8) The output to stderr might not end up being as helpful as you might think. Perhaps some years down the road this file will evolve into a wonderfully documented and packaged module. It will be imported by someone's security &amp; identity management suite. And that will get used in the latest hot app that helps 20-somethings find new places to play croquet (a game that's sure to be a huge hit in the mid 2020s). So, some obsessed croquet enthusiast fires up SooperDooperCroquetApp on his iPhone 37, and he sees a little window pop up, with a message in it: using rotor Consider that there might be more appropriate ways to indicate how your package is configured. Okay, maybe you don't know what those ways might be. Maybe I don't, either. So, for now, you write a function to handle configuration messages: def send_config_message(msg): """Given a string holding a message about module configuration, send it to some appropriate place.""" print(msg, file=sys.stderr) # TODO. Ick! Do something better here! Now call `send_config_message` whenever you want to output a configuration message. And in the future, when you actually figure out how those messages are best handled, you only have to change the code in one place (there's that DRY Principle again).
Excuse me? No language had an equivalent to import * 40 years ago. (either that or they didn't have namespaces, and there was much wailing and gnashing of teeth) Additionally import * is a _very_ easy way to send your python code to programming hell, because it abolishes the namespace separating code you made from code somebody else made. It also makes it incredibly difficult to tell if you're using a sinh function from math or from numpy. This could introduce some subtle problems into your code that you might not notice initially but find later when the damage is done and you've permanently married two namespaces together that shouldn't be in the same namespace. And if you're going to be writing some new functions anyway shouldn't you go the verbose route anyway? The point of the functions is to reduce the amount of code you'll have at the top level anyway.
Thanks for your comment! I'll think about that class, I'm still confused about using class or function. About `except:`, can you point out which method? I use `except` at last to catch any unexpected exception and return code 500. I don't want users to see the full traceback when errors occurred.
Very late to party but I started something similar just for craigslist data scrapping. I am interested in joining like minded people. I am still new to python in general . [Here is my craigslist scrapper](https://github.com/Pushkr/Python/tree/master/CraigsListToExcel) work is still in progress.. 
As far as I know, Python doesn't have a GUI of any sort, and as a programming language, it is unlikely to get one. Likely, the book is asking you to open a new window in your IDE.
oh wow lol thanks, I thought I was in the IDLE but I wasn't 
The thing is that most people aren't writing web servers. They are writing code that runs on web servers. Is C a good language to know? Sure. But there are far fewer developers going to work every day and writing C than those writing Javascript, C#, Python, PHP, etc, and most of them work in embedded systems.
Interesting, I'll have to check that out. Thanks! I doubt we'll target Rust, just due to the fact that the kinds of applications I'm interested in targeting wouldn't be written in Rust. Too low level.
Its too low level for the scientists who just want to write their equations and process data. Rust certainly is interesting, this interest with python and my interest in Rust are different.
&gt; asm.js In layman's terms, what is this thing can be useful for? Any good examples?
That said, maybe some work could be done on getting scientists interested in Rust, but currently its a lot of R, Python, and Matlab. Theres also the obvious hard core masochists who use C and Fortran, but thats well covered.
Pokemon Go.
Two of the complaints about namedtuples aren't really valid in my opinion. The one about the weird internal name can be avoided by just using the same name for both the namedtuple and the class name like so: class Point3D(namedtuple('Point3DBase', 'x y z'.split()])): pass Secondly, the "issues" surrounding inheritance don't apply to this case as it's just using inheritance as a kind of syntactic sugar. None of the dangers or pitfalls are exposed when using it in such a simple manner like this.
C++ is actually a lot more dominant in desktop apps. Basically some low level server tools (Nginx and Apache like you listed.. also Redis is another example) and GTK desktop apps are the only mainstream C programs you see nowadays. By mainstream I mean programs running on a traditional computer. There's lots of embedded "C" although this is usually some weird, hardware specific subset of C. Windows supports C++ first class as does Android, and iOS supports it but forces you to call out to Objective-C for UI calls (via Objective-C++). In my experience it's much more common. Is C worth learning? Yes. But 90% of software jobs won't have you writing C.
I'm pretty interested in this project but I'm assuming you already have the group of people you're interested in working with. So I was wondering if you had any advice on what to study for the stats side of this. If you're willing to share that is.
Great! Numpy/Scipy are pretty heavily used by our target scientists. The Nuitka tip is great too! A compiled/jit-ed/interpreted python comparison will be important.
If you downloaded Python 2 &gt;=2.7.9 or Python 3 &gt;=3.4 from the official python site then pip is already installed, all you have to do is set the PATH for it. Go to cmd and type in setx path "%PATH%;C:\Python27\Scripts" You might have to replace the "Python27" part (meaning Python 2.7.x) with whatever python version you have (it might be Python34 or Python35) Then, into cmd, type pip install praw 
What's a python
He means that you should group functionality into smaller classes. Using attrs speak, instead of: @attr.s class C: ... _coord_x = attr.ib() _coord_y = attr.ib() _coord_z = attr.ib() ... and having a bunch of methods working on it, you should put them into a separate class ([SRP](https://en.wikipedia.org/wiki/Single_responsibility_principle) and everything): @attr.s class Coordinate3D: x = attr.ib() y = attr.ib() z = attr.ib() ... @attr.s class C: _coordinates = attr.ib() ... This is a much nicer design but if proper classes are painful to write, people tend to avoid it.
Any questions just ask* * don't ask questions
You seem to have a wrong impression of attrs’ scope. It’s not a magic meta-system to replace classes. It’s a way for you to declare how you want your classes to look like and attrs in turn generates a bunch of dunder methods that a slapped on your class and never touched again. It’s regular Python classes that behave like Python classes except that you don’t have to write the same mindless boilerplate over and over again.
`contents or []` is a bug. If you pass a variable intending to share state: my_list = [] an_bag = Bag(my_list) "..." my_list.append("secret squirrel") stuff = an_bag.get() `stuff` doesn't have an item in it. If you want to use an expression, what you *actually* need here is self._contents = contents if contents is not None else [] but one of the benefits of using a library like attrs is not having to keep baroque subtlety like this in mind all the time.
No, I don't, I still don't see the problem since it's still not good practice to use 'from math import *' which as far as I know is the only scenario that would cause an issue.
&gt; If it is better notation they should use it in their published work and try to get others to use it. The problem is that there is a huge psychological inertia from people growing up using Pi all the time. That, more than anything, is why people are fighting so hard against Tau. Therefore, it is extremely difficult to get people to use it *even if it is the better option*. Otherwise, they are forced to use Pi even when they would prefer Tau simply because they still want people to read their paper. Regardless, this discussion is still ridiculous. Having both should not be so upsetting to some people.
Do you know what the D in BDFL stands for? It's not just there for show and it's pretty clear he's earned it. Perhaps Python wouldn't be nearly as successful if people like you were allowed the final say.
No it won't. They are keyword arguments, not variable names. And the dict constructor takes arbitrary keyword arguments. &gt;&gt;&gt; dict(foo='bar', bar=42) {'bar': 42, 'foo': 'bar'} 
You have a very different bug. Try: bag1 = Bag() bag2 = Bag() bag1.add("foo") bag2.get() The bug glyph mentioned is in the `contents or []`part that is supposed to work around *your* bug.
great, np. :)
... but make sure to not overuse it. Quite often what you need is merely a dict, not a class.
&gt; Where is the bug? In the behaviour that is not worth taking into consideration each time. Basically, it might not be a bug itself, but it's very prone to causing hard-to-debug bugs.
Yeah, I figured it out, it's just in the original article words 'bug' and 'python' are right next to each other so I subconsciously assumed that I'm supposed to find a python bug, i.e. something that would break the code and raise an error.
well, if you do it by accident in production it *does* fuck everything up. :) I daresay it’s one of the most common confusions for beginners/intermediates. Only challenged by closure late binding.
yeah, but *if they have an easy wrapper/api then yes
I have added an explanation to the README to calm the confusion. :)
Have your own datastore. Could be an SQL database, could just be a file server. On the internal network, firewalled off from the internet at large.
That's not Euler's identity. Euler's identity expresses a 180 degree turn around the unit circle centered at the origin in the complex plane. That is a 360 degree turn.
Nothing to do with python, but does anyone know which font this blog uses ?
You don't sound very convinced, and "do all this extra work that doesn't sound like it's related to the task you're trying to accomplish" probably isn't convincing either. So, if nothing else, strip out any personally identifying information. No names, employee IDs, locations, etc. Assign each person a random number and only refer to them by that.
There is a difference between using tau in your own paper or code and putting it in the standard library everyone uses or the textbook everybody's children use. It's a bit like these new math arithmetic exercises that offend so many parents. They see their kid drawing dots and lines all over the place to do basic multiplication and think "why the fuck are they not doing long multiplication?!" They aren't convinced the new approach is worthwhile. This also comes up in other areas of python. The recent discussion about pathlib comes to mind. I have no intention of ever using this library because it makes me jump through hoops to convert strings to paths. Because of the fact that I think this library is written wrong, I don't want it in the standard library, and that is despite the fact that it's presence in the standard library won't really affect me. It's pretty normal to think that the standard library should be standard agreed upon by everyone and not just a dumping ground for everyone's pet projects. 
Sold. I basically never write those magic methods because they're a PITA to write. Furthermore, you can write custom ones should you desire. I'm sure there are situations where there are downsides, but there aren't any in the applications I'm writing these days.
Hmmm, I can see where that might cause problems, but in 7 years of programming in Python it's never happened to me once. These days I mainly guard against it by not doing silly things like that. I do my best to not modify values after passing them into a function, although I will start avoiding this bug to keep my less experienced coworkers from shooting themselves in the foot. 
It's read attrs/attrib. There's aliases called attributes/attr if you don't like that (although it's very readable and scannable once you get used to it).
It's ok now, I used dropbox instead.
great article!
Dear god I hate jump cuts. It's the youtube equivalent of the radio "cash box" they used to use to cut out every bit of dead air.
Heh, I work in a software company already. I'm stuck using Delphi for work, but I was planning on picking up Python for my own projects.
&gt; low hanging fruit . &gt; pickles Cucumbers really aren't a hanging fruit at all 
Aye, I also mentally pronounced . so attri.b =&gt; attri dot bee. Appreciate the correction.
That doesn't really make much sense for ordering, though, at least to my mind. Automatically creating `__eq__` and `__ne__` makes sense, but the others feel a bit like guessing in the face of ambiguity. You can use `@attr.s(cmp=True)` to tell it not to create those methods, but then you lose `==` and `!=` as well.
It's likely talking about IDLE, which is a program that allows you to work with Python more conveniently. As to compatibility, any version of Python starting with the same number is compatible to all versions starting with that number - for example, programs written for 2.5 will work with 2.6 and 2.7 (but programs written for 2.7 aren't *guaranteed* to work for 2.5), and programs written for 3.1 will work with 3.2 and 3.3 and 3.4 et cetera, but not with 2.x.
You'll have to phrase your question a bit more concrete if you want a helpful response. An attrs-generated `__init__` is no different from a self-written one.
Caching, counting, and naming... ;)
Say you wanted to validate that `self.x &gt; self.y` but wanted the rest of the `__init__` functionality (accepting args, accepting kwargs, doing the rest of the validation etc). There isn't a way to do that.
For high performance you should probably stick with Cython (C/C++) or Numba (LLVM) unless you have specific reason to invent or contribute to something else. PyPy is not used at all for high performance numerical computing due to its half-baked support for C-API (combination of cpyext, cffi, rffi). Some domains have specialized compilers, but Cython is quite general approach. Numba is mostly numpy-specific.
There are issues with inheriting from namedtuple though. * It's iterable by default. * You're bound to the getitem for tuple,which means it must be subscriptible even if you don't want it to be. * It has a length, even if that makes no sense for your type. namedtuples are okay record types, but let's not fool ourselves into thinking they're perfect. 
tip: visualcppbuildtools_full.exe /layout [download_diectory] for offline installation. Still 3GB though!
Awesome tutorial. D3, DC and python are a data scientist's best friends for amazing charting.
Couldn't the if...else sequence at the beginning be replaced with something else, like maybe a list of tuples and something from itertools?
100% agree. I've lost that "battle" a few times before I really understood why most research types don't care. You can always find time to pickup more cores somewhere before you make time to change your code, test, verify results and then be happy. 
This article speaks to my heart. Definitely going to use that. 
So you're just super new to http? Anyway, just use requests. 
Don't waste your time implementing algorithms and data structures. Spend time learning WHY you would use the different algorithms and data structures. 
https://en.wikipedia.org/wiki/Recurring_segments_on_The_Colbert_Report#Tip_of_the_Hat.2C_Wag_of_the_Finger
Wot! No Gale/Shapley algorithm? :-) 
Windows, linux or osx? If windows, uninstall python and reinstall and make sure you've checked the box for python path. Osx and linux should've installed it from the get go.
`pip install numpy, pandas`
The (very interesting) wikipedia article you linked says the exact value is around 39.2 https://en.wikipedia.org/wiki/Absorbing_Markov_chain#Games_of_chance I think you could improve at least that part of the article. Thanks for the snippet, well done!
There were several responses and it was a lively debate. But hey, thanks for ending it forcibly, making me feel awful, and reinforcing my belief that people that manage online coding resources are usually snobs.
Windows, i'll try to reinstall. 
Lots of people seemingly use this, and it could use some help, so I thought I'd share the link.
Luckily, I have a little bit better with the engineers I work with. But I still encounter the "Why don't you just do it in Excel and brute-force it?" with my simulations/libraries
"I couldn't download something like Anaconda." "So here's something like Anaconda that's way worse." Idiot. Or WinPython / Pyzo employee. Edit: I apologize OP for calling you an idiot. I just was confused about how you couldn't download something like Anaconda for work and then downloaded something like Anaconda for work. 
True enough. Here's [a good article](http://www.visualcinnamon.com/2015/11/learnings-from-a-d3-js-addict-on-starting-with-canvas.html) that talks about that.
Play nice. Ideas that may be 'distasteful' are best explained as to why rather than resorting to personal insults. No need for a Linus clone within the python community.
Replace `if message.content.startswith('@Anteater Generator#2550')` with `if message.content.startswith(bot.user.mention)`
If you mean PTVS, then I'm not sure if you can install it on top of VC++ 2015 Build Tools. Steve Dower from MS may be able to answer this. /u/zooba
While I agree with you completely in one sense a post like this which provides zero evidence of motivation, zero evidence of sufficient research and zero evidence of providing useful input into the community requires, I feel, an equally zero sum reply. Starting the conversation out with "Could we discuss the merits and demerits of Anaconda vs alternatives" is the way more healthy for what we're trying to achieve here. I suppose calling the person an Idiot is taking it a little far but I have no sympathy for those who come to a new community and respect none of their rules or their purpose here. 
I'm curious, did a bunch of people from PyCon AU jump in after Damien's keynote about MicroPython yesterday, or was this release already planned for at the moment?
VC++ 2015 Build Tools does not include anything Python-specific, it is just C/C++ compiler for various architectures on Windows. But it is essential toolchain for Python extensions, Cython, and other languages.
This is great news for support on Windows! It should be easier build C based modules.
I'm self employed and use python for everything. I design, code, and deploy smartphone and desktop apps for many different companies in Mexico. I use kivy for mobile and desktop apps and use Google Cloud Platform for servers and databases. I don't have a typical work day for the nature of being a full stack developer, some days I might design uis, other times code them. Other days I do code servers using flask and GAE. I also go to meetings with my clients and turn in work. Worst part of my job I would say is deploying apps to the app store. I work from home wearing my pajamas without a fixed schedule which gives me a lot of flexibility.
Not as easy as gcc, mingw, tcc considering 1.7GB installer size and 3GB in installation. But this provides much better support on Windows and compatibility with other Python C-extensions. Particularly anything related to OpenMP. 
Engineer, about the same. Lots of one liners that save *a lot* of processing time. Mainly spend my time automating dSpace: https://www.dspace.com/en/inc/home.cfm 
I work on a daemon server in Twisted. A typical day starts with catching up on email and code reviews, maybe re-reading some code to remember whatever I was doing the day before, than some coding until lunch. After lunch we have a team meeting, about half an hour, but we're trying to make it even shorter. After that it's either more coding, or trying to address stuff brought up in the meeting. Mixed with the coding there's also a lot of helping coworkers with stuff, like debugging, or maybe pairing/brainstorming the design for some new feature, things like that. Our app has two parts, the one in twisted, that I work on, and a server in Java, so, some days I spend a lot of time either helping the Java people debug something on the python side, or helping the other guys that work with python, because I'm a little bit more experienced. My project also shares the space with a lot of other projects in my department (yeah, open plan office, sometimes it kinda sucks, but, what can I do), so there's some around and talking bullshit sometimes too =P
I work for a bond investor. I create prepayment and default models for residential mortgages. I usually will write code for automating some process, whether it is data collection, parsing, exploration, or aggregation. The end goal is to turn a bunch of data from disparate sources, usually in poor shape, into something presentable from which I can draw conclusions. The most common libraries I use are Pandas (200%), Numpy, Scikit-learn, and Statsmodels. 
Been reading this book for an hour now, it's great! thank you.
would you like to help out on a DSP project involving eeg signals? We're trying to build a classifier and could really use some domain knowledge regarding morlet wavelets and fourier transforms.
can you just use the quandl api? https://www.quandl.com/data/YAHOO
Why do you need an IDE for webscraping?
Would be a lot nicer if you could use pythons own type descriptors. 
First of all, one should understand that there is a clear distinction between errors and exceptions. Error occurs at compile time and exceptions are raised at runtime. And Python has a no. of built-in exception types. So they should be more than sufficient to handle any kind of exception. But it allows the creation of custom or user-defined exceptions that you can use if you want to be a little personal. However, if you don't want to add multiple &lt;try except&gt; blocks in your code, then add a generic exception to catch all types of errors. And invoke a global method from there. In this method, you can add the required decision-making steps. And return a useful message relevant to the type of the exception.
See https://github.com/dsc/bunch
Yes! This cannot be overstated. Don't let the extra overhead of non-"happy path" thinking scare you, though. One of the advantages of TDD (at least in my experience) is that it tends to *minimize* how much error handling goes on in your code. To make tests short and efficient, TDD code tends toward shorter functions and methods, which minimizes the possible error conditions they can have. That's more for unit testing, though, rather than functional/end-to-end testing where you consider the different end-user behaviors. 
I think you are looking for [webassets](http://webassets.readthedocs.io/en/latest/).
Pro python dev? My experience of developing in «pro» context is most managers are confusing functional and unit testing, and just want test to have tests. So I would not ask pro dev, but dev that have a well used pypy module. Which is not my case I am a modest average pypy package maintainer. My approach is : If I have a class that has a well defined behaviour I do unit test. And in the setup of my pypy packages I do call the unit test and abort install if test do not pass. If I have a bug, or a difficulty I do a test. At least to know when I resolved the difficulty or solved the bug. I add it to the setup test. If I have something asynchrone I do functional test, creating scenarios of action and more than reasonable timeline of action / reaction that can be checked against a list of expectation. If it is CLI driven I use expect, if it is web driven I use requests.... Since computers may not be always connected (FW) I do not force this tests at install time. Basically, I use test as a Perl coder : to prevent a broken package to install. Sometimes slight platform specific problem may be the problem, not your python code. I strongly regret the absence of a pypy policy on test before install, and therefore of a failure collector per CPU/OS that would help improve the reliability of our packages. I also regret we do not have a globally accessible ticketing system for every python packages helping us see which packages are actively fixed. 
Would love to have revdb in cpython and pycharm. 
Would love to be able to use PyPy for everyday work!
I did it yesterday. It took me about an hour (more like 45 minutes, including the survey). It was a good estimation on their behalf.
How about explaining what "some reason" is?
Can you have virtualenv to create symlink for global ipython?
Yeah, I got that mail yesterday as well. How do they get people's e-mail adresses? Do they just scrape github for mails?
Look up py.test. I think you will find it more in line with how you're working now (in a good way!). 
That's not a survey, it's an interview. 
In this approach, I still run into issues with non-Python dependencies. Recently, I needed to install [tables](https://pypi.python.org/pypi/tables/3.2.3.1), but my system's HDF5 libraries were an unsupported version. [Conda](https://www.continuum.io/) handled that better for me. I guess any language-specific packaging tool is going to have this issue, and a system-level packaging tool will be necessary, but I'm not sure where the line between them should be drawn.
Which almost never happens if you are using virtual environments.
Playing the devil's advocate every once in a while may be a refreshing attitude, but I feel Lutz is increasingly becoming negative about python evolution. He dedicates a considerable amount of space to negative critique in an already voluminous book neutrally named "Learning python" which isn't the right place for such a libel. While I partially agree with some of his points I find the overall exposition of downsides way too hyperbolic.
Thanks for the feedback. You have a point here. From a user perspective it would be much nicer to pass a Python type name instead of a string. My rationale here for using strings is that I want to have an API that also works in other programming languages. And when you registers functions you define a contract for JSON-RPC and not for Python. But I will think about it. With Python types the code would definitely look better in a text editor.
looks like you are running on windows. Good luck
I also did it. It was quite interesting, and I sent them an email asking for the results when the study is done.
I dunno, what could be the problem… let's do an `ls -lR node_modules` and see…
Someone recently told me that if I can't figure out how to create a PyPi package, maybe I'm just not a good programmer. Maybe I'm not, but it's unbelievably confusing to try to go from a single python file to a PyPi package. I found golang to be much more intuitive with a directory structure that makes sense and a simple `go get` to the needed repositories. I would much rather write python however.
NPM2 was the last time I used it. But multiple versions of any dependency still makes it hard for me to know what my dependencies are and how they're being used. 
Isn't that exactly what wheel was made for?
IMO the problem isn't necessarily with npm itself, it's with the community that uses it. Most of them feel that every little thing should have their own installable module. Sometimes even going as far as having a module for every function they can think of (see the left-pad debacle). Although in theory modularity is good, this does create the problem of incredibly large dependency graphs which are just a pain in the ass to work with, because in a lot of situations the packaging tool can't figure out what to do, so you have to figure it out yourself. Or even worse, it figures it should do something, which breaks everything and sents you on a hours long debugging session just because you wanted to upgrade a package. NPM2 was even worse, and NPM3 mitigated a lot of the problems that NPM2 did have, but the whole ecosystem is still far from perfect. Dependency graphs in python are often way smaller, which makes dependency handling way easier.
tldr: python packaging isn't as bad as it was
Correct.
Do you happen to temember what the problem was? Should bpython accept a wider variety of version number for six?
I might be wrong, but I don't think so. My understanding is that wheels are intended to contain a fully-compiled package, but not extra dependencies. Maybe you could also make a wheel of HDF5, but it has no direct relation to Python, so its developers wouldn't. Still, I guess that's why it's available through conda: someone decided to package it.
You need to install GCC it sound like. 
I know! There's no clear dividing line, and someone is eventually going to have to do a bunch of work to integrate packages (whether its packagers or end users). Packagers are volunteers who can't package everything in the world, and end users just want to get their work done. So I don't know how this should be addressed.
That's not what I meant, although it's a good point in itself :P I meant you could use the type annotation features: https://www.python.org/dev/peps/pep-0484/
thanks so much!!!
Don't be afraid. We've all been there. It sure is confusing. But since you managed to figure out programming, you will figure this one out, too. Or just ask. Python people are an unusually friendly bunch! 
Just as I was thinking then. Using exaggeration to get page hits. I shall carry on dabbling 😀
This is the internet, we're writing Python, and you've never heard the word weeaboo?
The really nice thing about conda and manylinux is that they make great effort to build on very old platforms with newer compilers, which confers backwards and forwards compatibility. This makes the task much more feasible. Presently, conda's ability to ship library packages that can be shared among many packages is a major advantage over pip. There's some effort under way by Nathaniel Smith and others to fix that (sorry, name of project escapes me right now), but for now, conda is much better in situations where a shared library might be employed by more than one python package. As for particular hardware - where there's a will, there's a way. The hard part is not really building things out (that's just a matter of time), it is providing the distribution channels and standardized build tooling for each bit of hardware. I think both pip/pypi and conda provide some ways to accomodate this hardware platform separation, but I think both of them are currently somewhat hard-coded. Both would benefit from modularizing this. If you do things right, it should be possible to require a lot of machine time, but very little human time.
Ring23. All you need to do is to automate the insertion into a db, then use an ORM to read from it and return JSON. Flask has templates specifically to parse JSON and return tables.
I do it this way when creating dicts that will be used to call a function with kwargs. It just looks similar which I like. 
This has never been a problem
Biggest issues with conda is side channels not being built in the same environment as conda itself. So many glibc issues from building channel packs in Ubuntu when the core is compiled in centos 5.
Hopefully this is something that conda-forge can help to alleviate by providing a central consistent channel for extra conda packages. Not everything is ever going to be on conda-forge, but the more things that are the closer you get to a consistent ecosystem.
At least on linux this is a non-issue since the `manylinux` architecture is available: https://www.python.org/dev/peps/pep-0513/ Wheels built with this policy rely only on a very small subset of shared libraries that should be available on every system, everything else is linked statically.
The re-write resolves a host of problems with the previous version. Since pyhooked is based on `ctypes`, it runs on most all Python implementations and versions (save Jython for now).
Mark has a very good point about that question. You can absolutely ignore any language feature that's bloated or redundant - right up until you have to work with code that uses it. While Python is still a great language and in many respects 3 is a much more consistent and logical language than 2 (especially w/r/t objects and generators), it's also definitely true that "there should be one (and preferable only one) obvious way to do it" is being violated quite a bit by the core developers. Right after extensively overhauling generator-based coroutines, we have the incredibly abstruse async/await-based coroutines. We have a genuinely preposterous number of string formatting methods. We have several weird-optional-ish type declaration hints that are totally ignored by the interpreter itself, but that risk becoming standard. Lots of other stuff too in some of the more arcane metaclass/super() parts of the language as well, which Mark talks about in the book itself. Don't get me wrong, Python is a very strong language and imho the 2 to 3 jump was a good one if a bit painful. But there's little question that 3.4-5-6 have introduced some very un-Pythonic redundancies. You can mostly ignore them, but some people won't, and to the extent you have to use their code it could be quite a headache.
In your editor, open two windows or tabs: one for the module you are writing, and one for your unit tests. Then open a terminal window. Every time you think "I need a function (or method) to do foo", I start by writing documentation **and a test** as a docstring. def spam(num): """Return lots of spammy goodness. *num* is the amount of spam to return. &gt;&gt;&gt; spam(5) 'spam spam spam spam spam' """ return "spam" I always write the documentation first. Otherwise, if I haven't written down what the function does, how do I know what it needs to do, what arguments it needs to accept, and what it needs to return? *Document first, then code.* **Always.** Notice the part of the docstring that looks like a snippet from the interactive interpreter? That's called a [doctest](https://docs.python.org/2/library/doctest.html). Doctests are *executable documentation*. Notice that the function isn't correct yet. Doesn't matter -- don't even try to write the entire function all in one go, not unless it is simple enough for your granny to write it. (If your granny is a professional Python programmer, try asking your granddad instead.) Save the incomplete function, and try to run the doctest. In your terminal window, run: python -m doctest path/to/your/module.py It will print out a failed test. Now you know that the doctest is working! (Always ensure your test fails at least once. Before I learned this rule, I wrote my functions to pass from step 1. Sure enough, they passed. It took me weeks to discover that the reason there was no output was because the doctests weren't being run at all! So always make sure they fail the first time, so you can see that the doctest module is actually auto-detecting them and running them.) Now you have a failing doctest. Time for a failing unit test. Swap to the second tab in your editor, and write a [unit test](https://docs.python.org/2/library/unittest.html): import unittest import module # your module that you are writing class TestSpam(unittest.TestCase): def test_spam(self): self.assertEqual(module.spam(2), 'spam spam') def test_spam_bad_arguments(self): self.assertRaises(TypeError, module.spam, None) Back to your terminal window, and run: python -m unittest path/to/your/module.py Again, you'll get some output. At least one test will fail. Now go back to your module, and start editing the function so that it works. Don't feel that you have to do everything in one go. Make a few incremental improvements, run the tests again, when they pass add some more tests, run them and watch them fail, edit the code, run the tests again, and so forth. The important things are: - Documentation first. Doctests are primarily documentation, so write them first. - Unit tests second. - Run the tests. If you don't run them, they might as well not exist. - Fix the code so that the tests pass. Then write more tests. - Your users found a bug? That's not a bug, that's a missing test! First write a test for that bug, watch it fail. Now fix the code so that the test passes. Keep that test forever, to prevent the bug from returning. - Unit tests can easily end up using ten times as many lines of code as the code you are actually testing. But don't be intimidated: any amount of tests is better than nothing. Even if you test only *one thing* in your entire code base, that's better than testing nothing at all. Tomorrow you can aim to test a second thing. Next week test a third and a fourth. You can gradually build up to testing as much as possible. Eventually, you'll start to design your functions and classes to be easy to test. Easy to test means easy to understand and easy to maintain, so you win *four ways*: you have documentation, you have tests, you avoid bug regressions, and your code will be better. Writing tests will not only give you better code, but it will make you a better programmer because you learn how to write simple, understandable, maintainable code. 
To be honest, if I wanted to guard against every possible value that could be passed in I would use Haskell instead. 99% of the time this sort of shortcut is going to save me time and effort, while guarding against every possible input is just going to clutter my code and be very unpythonic.
Another fun fact, when `npm install` runs it will make liberal use of temp files that it will intentionally not delete. Because everything is small you end up not using a lot of disk space but on Linux systems you can run out of inodes if you never shut down the machine (e.g., a server). Problem is made worse by the hypermodularity style of javascript packages encouraging more files and overhead like `package.json` for each. This behavior may have changed in npm 3 however.
That may have been my problem too. It's been a while. It was my first attempt at getting into npm/node etc. I ran npm once said "If this is how something is designed by those that know what they're doing... nope". I still don't know why .deb isn't just a standard of some sorts. I know guys, lets re-invent the wheel!
You haven't understood the nature of the bug. Your "shortcut" *introduces* a bug. You're not guarding against anything: you're taking the caller's argument, and if it is falsey, replacing it with an empty list **whether or not it is None**. That's the opposite of a guard -- it's more like an anti-guard, replacing things that shouldn't be replaced. Thinking about Haskell is a complete red herring. Your shortcut is somewhat equivalent to: if content is None or contents == [] or contents == {} or contents == set(): # among many others # Oh, you wanted to use a dict? A set? A deque? Well screw you buddy, you're getting a list, # whether you want a list or not! self._contents = [] else: # Non-empty dicts, sets, deques, etc are perfectly acceptable. self._contents = contents 
I'll check out cookiecutter, thanks!
I get the desire to avoid OS-level packaging. If you decide to provide OS-level packages, you have to put out a package for each OS: Debian, RHEL, OS X, etc. However with a functioning language packager, an implementation of the packager is available for each platform so I can put out one platform-independent package. It's a reasonable goal to make the lives of package maintainers easier. However experience has shown me that it doesn't take long in the lifecycle of my projects for language packaging to show that it is not up to the task of fully specifying my project's dependencies, and I'm cobbling together build scripts to do it all. 
Python packing is OK at this point - package consumption is still completely garbage, especially when it comes to continuous integration or developer tools. If I write a tool for my developers, they have to know how to setup a venv, how to use the reqs file to install the packages, and how to use it. That's a fucking terrible experience if you're, say, writing tools for C++ developers in Python. So, you have to go way out of your way to create some venv automatic creation script and put the packages in it for them, which is what I've ended up doing. It works, but it sucks.
Flask-Assets? Regarding js minification, `slimit` does an excellent job.
Several steps. * Learn about how http requests and responses work. [This](https://www.httpwatch.com/httpgallery/introduction/) is a good starting point. * Learn to make http requests using [Python Requests](http://docs.python-requests.org/en/master/). * Learn about various schemes of http authentication, focusing on the one that the website in question uses. There are many different kinds of authentication like [basic authentication](https://en.wikipedia.org/wiki/Basic_access_authentication), [token authentication](https://scotch.io/tutorials/the-ins-and-outs-of-token-based-authentication) etc. * Implement the authentication using Python requests. [This page](http://docs.python-requests.org/en/master/user/authentication/) is a good starting point. * Once you are authenticated, you can now perform the action. Find out how to do the action that you want to do. In particular, what is the endpoint (URL), what is the HTTP verb (GET/POST etc), what are the parameters and values (key value pairs for this action)? If the website has a public facing API, then this information is well documented in the API docs. If not, then you have to find this out yourself. You can log in from the browser and use tools like Firebug to find out what requests were made in the background. * Once you have all the information, implement the HTTP request using Python Requests. The reason I am suggesting requests (and not urllib etc) is because requests has a gentle learning curve and takes care of many things automatically. Wish you all the best with your effort. 
Great article. 
Like others here, I advise test-driven development. I use it myself for my daily work. 1. Write a minimal failing test for some component of your design. 2. Write the minimum of code needed to make it pass 3. Refactor: eliminate duplication, and restructure your code to match your problem domain, while keeping all the tests passing. This is also known as the red-green-refactor cadence. If you want to see what it looks like to work like that, watch Gary Bernhardt's [String Calculator Kata](http://blog.extracheese.org/2010/01/string-calculator-kata-in-python.html). 5 minutes, wonderful music. If you want to read about what it looks like to work like that, have a read of the chapter "Test Names Should Influence Object’s API" of Corey Haines' [4 Rules of Simple Design](https://leanpub.com/4rulesofsimpledesign). Just hit the Free Sample button in the bottom left -- it's pages 7-10 (PDF pages 12-15). If you want a good library for writing tests, I recommend [pytest](http://doc.pytest.org/en/latest/getting-started.html#getstarted). There are examples of unit tests in pytest on that page. &gt; But I often find as I making progress of writing up the module, I need to change A LOT of code in the `if __name__ == '__main__'` blocks to test added module code as well. That's good! It means you are conscientious about updating your tests! All you need to do is chop up your code into small functions with single responsibilities, and that statement becomes: "When I add or change one of the many small functions in my module, I write or edit the unit tests for that function. Because the function is small, the number of tests is manageable." That's a very livable way of working: I work this way myself. Good luck, and have fun!
I don't know which dependency versions don't work with my app. What should I put in my install_requires rather than reading from requirements.txt?
I think it's a good practice to follow [semantic versioning](http://semver.org/). Under the assumption that other packages do the same, the version I list in my `setup.py` file is `&gt;=` to minimum version of the package I depend on and `&lt;` the next major version. If I find a package that breaks backwards compatibility in any release, or is pre version 1.0 release, then I fix it to the patch version.
I'm very surprised to read this post by glyph, if only because he gave a talk at this year's pycon about how distributing Python packages is a nightmare that no one's really figured out. This read like a pretty big turn around
I bet the person you were talking to was the same kind of asshole who loves to argue about why Python is better than Ruby, or why EMACS is better than VIM. That kind of person is toxic to the community and you shouldn't tolerate their shit. Let them wallow in their own filthy small minded world. No developer is better than the developer who is willing to ask questions and admit when they don't know something then take action to learn it.
Yeah this is fine as most JS deps are built with this in mind, using functional immutable style or replacing exceptions
Do you have a github for this? 
I didn't get this. :(
No, Python packaging still isn't good enough. On my phone, so I'm just writing this now so I remember to reply to it later. But no, stopping the bleeding != thriving. 
[pipsi](https://github.com/mitsuhiko/pipsi) is great for doing this.
Several years ago, I saw a preview of IPython notebook, and immediately fell in love with it (although it was alpha) and started using it for my daily work. Seeing this was a repeat of that feeling. I have a feeling this will be an important part of my workflow in years to come. 
That's literally everyone on IRC.
For these hotkeys to be registered, would the user have to have to be typing into the terminal window?
Aww yes. Jupyter Notebooks is a staple in *all* my development.
&gt; requires, I feel, an equally zero sum reply No, you are wrong. You have no idea what situation other people are in. The way you replied makes a lot of other people reading *your* reply afraid to participate in case their lack of knowledge or expertise in a particular area results in getting "idiot" back. Please try to assume the goodness in other people first, even if it's unclear.
You just had very little exposure to a similar technology. All packaging systems work similarly in that you have to tell them some essential metadata (package name, author name, license, version, ...), some optional metadata, and the files to include/exclude. You do this either in a declarative data file (package.json, Cargo.toml, ...) or a programmatic one (setup.py, Rakefile). Then you use a web or CLI interface to publish your package. And once you understood one of them, you know the important parts of the others. Python's way isn't harder or easier than others, it's just the general concepts you have to learn.
The more times I've been around the block as a programmer, the less I care about the particular cool syntax of any language and the more I care about the availability and ease of use of libraries that I need. Python does very well on that score in my opinion, because the day-to-day experience is not too much more complicated than `pip install lib-i-googled`. There's usually a library available in any problem domain, and one pip install and I'm good to go. However the discussion here did touch on some of the big caveats. To use python seriously is to use virtualenvs, which is fine but has a bit of a learning curve. And of course I agree that packaging things up is weirder than it needs to be. I've gotten by cutting and pasting `setup.py` more than I should have.
That's probably true. But do other packaging systems also need an __init__.py file in every subdirectory? As mentioned, golang's system felt the most intuitive to me.
Some modules document when particular functionality was added; even when it's not in the API docs you can often see a lot from reading the changelog. Or just specify `&gt;=` the current major version, if you can't work it out - semver isn't perfect, but people should get the idea - and *might* let you know if they discover a more specific issue. If you have tests, you could experiment with `pip install -e .` and virtualenv to install development versions, and iterate down through versions of your dependencies until something breaks. One the one hand this sounds like a generally useful shell script; on the other it's a fair bit of work to write it. Or finally, the correct thing to do if you don't know is just to not specify versions!
Yes please!
That's not a PyPI thing. That's how [Python knows to treat a directory as a package](https://docs.python.org/3/tutorial/modules.html). I agree that making your first Python package can be annoying and frustrating, but this is just a basic core language thing, not a packaging issue.
Nope! This is entirely global. No terminal needed. You can catch keys typed from any open application, as long as they don't steal the key presses first (which the program shouldn't).
Without wanting to be condescending, it *is* pretty easy to make a PyPI package. Just read a guide, it's like 5 lines of a setup.py. No need for grouchy programmer elitism though...
Yeah, those people really are small minded idiots! I mean, Emacs better than vim!?!
Obligatory mention of conda. It makes these things so easy. So much more sensible to have packages and environments controlled by the same tool, not to mention the cross platform and cross language way its going.
It's not really even about programming, its just memorising a few things. 
But only on a local machine? I assume use this with a Django website.
I'd consider myself a pretty good python dev, but writing a working setup.py on the first try is still impossible for me, so I gave up and just copy paste one known to work and adjust it. Python packaging for users may be good now, but for developers it's still horrible. 
Come on man, the posting was a fairly comprehensive machine learning tutorial which probably took an absorbitant amount of time. Does the data format really take away from the post? Like I said, contact the author if you're so inclined. As for the NASA comment, there's plenty of hdf5 documentation on how to use the API with various languages, except the very sparse C++ support, especially for parallel IO operations. I've looked around quite a bit, but there isn't a ton of "showcase" documentation. The reason people are using it at NASA, national labs, academic shops, etc is because those places have relationships with the HDF group, OR have the right staff to determine that it's the best fit, and modify their code (and do data transformations) accordingly.
I do *really* appreciate where pip has gotten (vs setuptools pre-merge). That said, doing docker + python has been fairly painful, especially with certain libraries that depend on OS-level packages. Takes a lot of tweaking to figure it out, and a lot of time-consuming building.
Golang's approach doesn't scale though. Its simple at first and has really low barrier to entry, but the fact that everything is just a git repo has side effects: -Dependencies aren't forced to use semver. As a result a lot of dependencies aren't curated but are just a random revision off the master branch. Yes you can tag a git rev, but in practice most library developers don't bother and app developers don't pay attention. -There's no distinction between libraries, build tools, and executable programs (Python has this issue too). This makes dependencies harder to manage than they need to be IMO and also prevents some nice features, such as the ability to only pull required dependencies -Since you could be pulling dependencies from any git repo, developers and any automated build systems potentially need to authenticate against many different repositories instead of a central public or private one. I just went through this pain setting up a Go build server to pull from a gerret git server and it was a PITA
&gt; Does the data format really take away from the post? No, which is why I stated already that it wasn't about the post. It was about storing data, I just found the large pickle dataset I needed in it. &gt; The reason people are using it at NASA, national labs, academic shops, etc is because those places have relationships with the HDF group. They have a relationship with the group because the group gets the input of NASA and the like. 
Well, I fixed it: uncomment Port 22 in the /etc/ssh/ssh_config file.
Sounds interesting. More info pls
Not related, but a feature I would love to see would be in imbd, when I select an actress's name, the option to search Google for name + nude
No you're confusing things. People coming for advice is half of what this sub is about, irrespective of their situation or level of expertise. I have nothing against a person who knows zero about Python coming here and asking questions. I have a huge problem with someone coming here showing ZERO evidence of effort. I don't care what their personal situation is, if they come here and lower the standards without even trying they'll get the kind of reply they deserve. 
It is with evil mode 😈
Well, that someone was me ([original post](https://www.reddit.com/r/learnprogramming/comments/4wsmfi/how_to_upload_a_single_file_to_pip_for_others_to/d69mz30)). There actually were some reasons why I said this. 1. you posted on /r/learnprogramming and most people there are complete beginners. Just look at the number of "what language should I learn for X?" posts. 2. you didn't include any code nor did you explain what you tried. 3. I said "your code may not be ready", not "you are a bad programmer" 4. you didn't react to anything in that thread. That just reinforced my impression that you were a beginner. How could I have known that you don't belong to the majority of beginners there? And yeah, I still stand with my stance that most beginner code shouldn't be published on PyPi. I'm quite shocked that you took it that way, since I participate there quite frequently to help people and I think I actually do most of the time. If it doesn't, it would be very helpful, if you could just SAY it and not make fun of me in an entirely different thread...
But you have not tested it with versions that are not out yet... Not very intuitive as a package author.
What's your point? You conveniently left out the part I mentioned about having the right staff to even begin considering not only a data format change, but also software development required to leverage the data format and APIs. Do you work at NASA, the HDF Group or something? I was willing to give you the benefit of the doubt, but as others said, and now I'll say, your approach *SUCKS*. 
I looked at your linked post, it was a fair assumption to make and you definitely don't fall into the category of person I was talking about in my post.
I remember having a hard time getting that to work right a couple of years ago, but just last week I went back - and it's extremely simple now. 
Thanks a ton! I remember looking for a package like this and being unable to find one.
Fair enough, and I probably should have mentioned this in the article (will update soon), but I still don't think it takes away the main point from the article which was finding motivation for making early savings when it can seem like an awfully long-term investment. What could have been clearer was how it could be the difference between catching up to the inflation, or actually getting ahead of it. Thanks for your input.
Wait.... So you published a book and want us to buy your book to give you feedback?
I definitely agree.. I've learned this in school a number of times and everyone really tries to push this point of early investment. Theoretically of course it makes way more sense to and I won't argue that, but the idea that 8% return is sustainable over 30 years is where I'm skeptical. I've watched my parents 401K over 15 years pretty much grow linearly if that, as the recent market has been just garbage. Obviously factor in the inflation and it becomes worse and worse. Apparently, there is a movement from generation X that pretty much believe 401ks are a scam/not worth the time. Actually calculating your NPV and then the annuity you can withdraw during retirement paints a pretty bad picture. That being said.. python is great. You crushed it OP.
Quite welcome! I originally was looking for something like this myslef, and then decided to write this library.
My 8% year over year return was based on the fact that Stockholm index has had an average of 11% over the past 30 years (that includes two financial crises). Based on this, 8% felt like a nice conservative number. But yes, it's obviously very unclear if it's a viable prognosis.
Perhaps post a subsection of the manuscript. Really all I can think of 
&gt; Super() in multiple inheritance — Understadning the behaviour of super is straight forward in case of single inheritance. Since python is one of those languages that support multiple inheritance, the world of Super() gets a bit murky. The murkyness of "Super" made me realise how multiple inheritance was something I wanted to avoid entirely. I can't reason sanely when using it. I believe readability and code simplicity is a premium. 
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, this post has been removed as it is not directly related to the Python programming language. It might be more topical on /r/programming, /r/coding, or /r/technology. Cheers, /r/Python mods
So, everything new is bad. Got it. /s
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
I'm currently using autohotkey extensively in work, do I understand correctly that this something quite similar with the exception that it comes with all of python's features? Can I also send keystrokes/mouseclicks similar to ahk's SendEvent command?
"The right staff"? Seriously? You need to rehire staff to train them to use the *right* tool for a job? I'm a freaking Mechanical Engineer, this stuff isn't rocket surgery. &gt; Do you work at NASA Close enough. &gt; but also software development required to leverage the data format and APIs import h5py What else do you need? &gt; I was willing to give you the benefit of the doubt, but as others said, and now I'll say, your approach SUCKS. Sorry the advice for how to use the right tool for the job didn't come with bedside manner. 
That's a legacy Python thing. PyPI doesn't need that, older Python versions do.
That's how Python *used to know* that a directory is a package. It doesn't need that anymore.
Also, check out using py.test with tox. It creates virtualenvs automatically and tests with a number of python versions you set, even pypy. You can use it with both py.test and nosetest. Great tool. I maintain a stupid client API at work and customers have complained about it breaking in both 2.6 and 3.x... Such a pain in the ass to support 2.6, 2.7 and 3.x but it does and tox makes it easy to test.
To add to unit-testing and TDD, great tools in python for this are tox and either pytest or nosetest and tox can use either. Tox is great because you can test in a number of different python versions and it'll automatically set up the virtualenvs for you and run everything, and alert which tests/versions failed. Really makes it easy when you support a customer facing API or something popular that you intend on supporting for both 2.x and 3.x.
Sounds like a question for /r/learnpython. In the meantime, read up on https://docs.python.org/3/library/socket.html (and/or https://docs.python.org/3/library/asyncio.html).
Well, that's an odd suggestion (are PEPs suggestions or something that will be implemented in CPython after discussion?) 1. Would break backwards compatibility in the language 2. As the PEP says, PEP 484 introduced type comments (which I've never heard of, but might start using now that I have), which seem sufficient for the goal of noting what type a variable should have. The issue with parsing comments in IDEs is a matter for the IDEs to implement on their own. Similarly, a system for parsing comments could be added to the interpreter, but that would be another breaking change, albeit significantly less intrusive. edit: After discussion and thought, it turns out this syntax was already in the language and is very unintrusive: def foo(bar: float): pass and it doesn't force you to use that type (but it's an option in other interpreters; cpython doesn't force the type), it's just for documentation. So this pep is the next step. And it's not like python 3 has never introduced backwards-incompatible changes before. So I'm now for it.
I've seen this language. It's called C. There's a reason I use python for what I use it for.
&gt; are PEPs suggestions PEP - Python Extension Proposal
So if they are going to make this syntax a part of the language are they going to make the semantics part of the language too? I.e. if i say: x: int = 3 x = "three" it should be an error (preferably at compile time)
you are indirectly saying something favorable about a java tool. that's a no-no in this sub. ;) FWIW, I've gotten tangled up in dependency hell in clojure too and it was my java experience from way back when that helped me solve it. I couldn't imagine anyone troubleshooting those kinds of things without getting into java/maven or miraculously landing on just the right stackoverflow answer or github issue. 
manylinux fully solves the problem of having a optimised c version distributed with the library, there's still an issue if you depend on outside libraries which have some build time configuration. HDF5 is an example, where you can either build with MPI support or not (there are other changes you can make to the build, but let's ignore those). There is no way to generate a h5py wheel (manylinux or otherwise) which supports both. There is no way to make two h5py wheels with different build HDF5 builds and have them on the same index server (or equivalent). You're going to need another package manager (apt/yum/etc.) which can deal with the multiple build configurations.
Should we also declare variables using "var"? You know... for clarity. /s
Optional language features become mandatory when you have to work with the code of someone who uses them. I'm not necessary opposed to type hints, but there's definitely some real danger in the blurring of Python's highly duck-typed lines. 
I don't think you can do this: monthly_rate = 1 + yearly_rate/12 This way you get more than the wanted interrest over a year because you get interests interest through the year. Edit: (1+(0,08/12))^12 = 1,083
&gt; are PEPs suggestions or something that will be implemented in CPython after discussion? It depends. In this case Guido seems to feel strongly in favour, so it's all but guaranteed that it will eventually be added to the language. &gt; Would break backwards compatibility in the language It's optional and doesn't conflict with existing syntax, so I don't think there are any serious backwards-compatibility issues. &gt; As the PEP says, PEP 484 introduced type comments (which I've never heard of, but might start using now that I have) FYI the standard python interpreter doesn't do anything with the type hints, they are only useful if you use an IDE or some kind of static analysis tool (like mypy) or alternative interpreter that makes use of them. I think they're intending that the new syntax will cause the type hints to be stored in a dict at runtime though, so it will be possible to use them for your own purposes if you want. &gt; After discussion and thought, it turns out this syntax was already in the language and is very unintrusive That syntax is specifically for annotating function arguments: you can't use it anywhere else.
More helpful for /u/suudo (since it could be that only accepted proposals get a number) is the Status line (Draft / Active / Inactive / Final) and the categorization given in [PEP 0](https://www.python.org/dev/peps/) (Being Considered / In Progress / Finished / Historical). PEP 526 Variable Declaration Syntax is listed under *Open PEPs (under consideration)* and has a Status of Draft. 
&gt; another breaking change No more than `with`, `yield from`, `async` and `await` where. Personally, I don't like it so I won't use it. Hopefully, most people that do use this (in libraries at least) will use the type annotation files that 484 also introduced, at least until 3.5 is EOL'd.
They make working on large pieces of software much easier if you know how to use them, but as every single language feature they should be used within reason. And your program will never* break because of having/not having type hints. \* unless someone decides to check the annotations at runtime, but if they do they are asking for it.
AFAIK it's just going to work like function annotations. The interpreter will check that the type hint is a valid expression, evaluate it, and store the result in a dict, but otherwise won't do anything with it (this allows you to use the syntax for some completely different purpose if you want). If you want to ensure types are enforced you can use a static analysis tool like Mypy.
This was entirely my thought when I saw this... If PEP526 is implemented, it's going to become 'good style' to use type hinting. Noobs will be expected to learn it, it will be integrated by people who give talks to be more correct and user-friendly, and anyone who publishes publicly will see PEP8-like scrutiny. 
Why are they asking for it?
Which is why the two PEPs feel like misguided efforts to me. 
typing hinting syntax is already in python, to give you an idea of how popular the pre-existing feature is. Its not popular. Also it is intentional for it to be not popular
Why? The only problem I have is that variable annotations weren't part of the initial pep.
Because it's either a small project that you develop alone and you're adding unnecessary overhead (because you can check the types with linters like mypy anyway) or you risk breaking someone else's code for no good reason. The only time I would find it useful to check types in runtime would be in an integration instance, but even then I would consider it too risky since it would add extra complexity to the code.
Sure. Not saying it's perfect, but compare: lein to: virtualenv env source env/bin/activate pip install -r requirements.txt I know which I think is vastly superior from a usability standpoint. And thinking about it now I don't see why. It's not like it's hard to write a python script to do those three lines. It's just that it needs to go in the standard library probably...
I'm really not a fan of this or the type signatures. 1. The language was not designed with types in, so types are just parsed documentation. 2. Code which is polymorphic through duck typing can't be expressed. My understanding is ABCs try to formalise interfaces, and so could be a path to follow, but I've never seen code that uses them. 3. No type aliases that I've seen, so complex types are hard to express. 3. The syntax itself makes already working code more difficult to read. Type information is sprinkled through statements whose primary function is defining functions or assigning variables. 4. The MyPy tool seems to have been a college project which was blessed far too early. It had potential, but it wasn't ready for prime time. Yet it dictated the path on type annotations. 3. Python's a dynamically typed language. Types should be inferred, not declared. Embrace it and write better tools to infer types if you want docs and static checks. It all just smells of people getting excited by type systems, probably because of haskell, but not really appreciating what they've got (a dynamic language), nor how you design a type system. Disclaimer: I also write haskell and see the power of its type system. Often I'll choose python, dynamic types and all. 
If the variable are annotated, you can check the types without any "extra complexity". traitlets does this already. Also, there are many reasons for runtime type-checks. For example, if you're accepting a callback method in a registration method `r`, then it's much more useful to check if it's callable in `r` and raise while you have a useful stack trace rather than raising later when the callback is called.
Yes, callable isn't perfect, but the case of someone overriding `__getattribute__` to provide callability is much much rarer than someone passing in a callable and not getting a good error message. I also think it's weird to implement magic methods through `__getattribute__`. Also, this isn't an example of asking for permission rather than forgiveness since we're asking for permission much earlier than we would be asking for forgiveness.
This sounds fucking stupid. Here's a better solution anyone else would have come up with: def some_function(): # my_var type: Logger ... I have absolutely no fucking idea why the fucking hell one would think a documenting code that has absolutely no effect on the program flow would be anywhere else BESIDES A GODDAMN COMMENT. BUT WAIT, WHY IS THIS PREVIOUS PEP A COMMENT TO BEGIN WITH??? I THOUGHT WE HAD VARIABLE DOCSTRINGS FOR THIS? my_var = 3 """type: int""" Stop javanizing my precious python!
What Django version it covers?
For my own code being lazy can be nice. But if you compare using an IDE in Python vs Scala.. it is a world of differences. Does a method take an int or a string? 
I posted in learnprogramming because I figured someone might have some good tutorials other than the official one. After your post it basically confirmed for me that the official tut is the best and if I find it needlessly confusing I should just port my code to a different language. You may have said that "the code" isn't ready, but the implication is that the coder isn't ready or experienced enough. Code doesn't write itself after all. Not making fun, I just thought it was an illustrative example for this thread.
&gt;anywhere else BESIDES A GODDAMN COMMENT. Did you read the PEP? It's so that other scripts can parse the information out using `ast`, because it's part of the syntax tree even if it has no effect by default. Also, you could make interpreters that enforce the types. CPython doesn't, but you could. &gt; I THOUGHT WE HAD VARIABLE DOCSTRINGS FOR THIS? That isn't a "variable docstring". The triple-quotes have no special meaning (they just allow strings to wrap lines), and in your example `my_var` doesn't get a `__doc__` attribute (the way that a function would with a docstring. It "works" in the sense that you create a string and then do nothing with it so it gets thrown away again; but it's not the same thing. &gt;Stop javanizing my precious python! It's nothing of the sort. For one, you don't have to use it.
Yeah, that's probably true. I think my reasoning was something like "more complicated packaging -&gt; fewer submitted packages -&gt; less moderating effort for PyPi". Admittedly not a great argument.
Checking it before running will catch it much earlier and it you won't risk introducing bugs or introduce overhead into your application.
"Compile-time" checking is better than runtime checking, but you can't always do compile-time checking. In your example, you were suggesting a function that you would have to call to know it was callable. That cannot be checked at compile time. The "overhead" of type-checking is in libraries. When we program with Python we usually don't worry about small constant runtime factors. Also I don't see why runtime type-checking using annotations is any more bug-prone than compile-time type-checking.
Oh, I see. I was just a bit flustered by the hostile atmosphere here. Just in case you still need a 3rd party tutorial, [this](http://peterdowns.com/posts/first-time-with-pypi.html) is the top result on google for "PyPi tutorial" and in my opinion it's quite simple. 
&gt; Code which is polymorphic through duck typing can't be expressed Exactly. This is by far the biggest issue. They've added a type system to python which hobbles its ability to express highly polymorphic code -- and that's 70% of what makes it powerful. 
Here's something I wrote on [Reddit](https://www.reddit.com/r/Python/comments/4wl029/requests_vs_urllib_what_problem_does_it_solve/d682975): It's complicated and not all the factors are pips fault, although all of them are collectively pip's problem to solve (or some replacement project). In no particular order: - setup.py is a disaster. It's a metadata format that's mixed together with live Python code. That makes it unparseable without execution. It's executable because people have various installation special needs, but a good metadata format could just say "when you run install, go to location X and use this subclass of the standard installation class called Y instead of the default installer." - In spite of setup.py not being a metadata standard, there are a couple of inscrutable, poorly documented metadata files you need for Python app installation, like MANIFEST.in and requirements.txt (see below for more complaints on requirements.txt). - Python relies on C very heavily, and compiling C code is a nightmare. Every installation problem I've had with Python at work has traced back to some C library that you need to have installed before some other Python library will work. This is not Python's fault per se, but it is a job that needs to be solved before you can say that Python installation doesn't suck. - The command line UI for pip is crap. The commands for things like "just download stuff here" and "use my cached downloads instead of connecting to the web" are non-obvious. There is no command at all for things like "add a new dependency to my app's requirements" because there's no metadata standard, see above. - Conceptually, an installation system should have two metadata files: one for loose requirements (Django &gt; 1.4) and one for strict requirements (Django==1.9.3). The first lets others use your libraries, and the second lets app distributors have reproducible builds. Pip kinda sorta halfway has this between setup.py and requirements.txt but it is extremely half-assed and not at all thought through. - When you start using Python, all you need is the standard library, and it's great. Then you get a little further, and you install a couple of libraries, and things are still okay. Then you get a little further and realize that you need separate libraries for separate apps and then everything breaks down. If you think about it, there are three possible ways you might want to install something: globally, per user, or in a particular project location. Python was designed to install everything globally, and while it has been retrofitted to support the other two use cases, it's extremely kludgey. A "virtualenv" is just a case where because Python is so geared around global installation, the easiest way to do a project based installation is to make a "project global" by reinstalling Python in a second location. This is super-hacky, and extremely confusing to non-Python people who try to get into Python (e.g. at work when I need to explain to frontend devs how to install our web app). - Pip does not handle and does not try to handle the case of trying to distribute apps to non-Python users, the way that py2exe or pex or Conda or other projects do, but when you think about "packaging" as a whole, there's no reason why a Python packaging tool shouldn't do those things too. Basically, pip doesn't try to tackle that problem because it's too busy doing a bad job solving other problems, so it doesn't have any resources left over to try to solve this use case for people who want to provide GUI apps or command line tools to non-Python users. So pip sucks. I would say compared to bundler and npm, it's mostly worse except it never did the npm nested dependencies thing (which I've heard they've stopped doing). Compared to the platonic ideal of good package manager, it's not even close. ---- And on my blog, [I linked](https://blog.carlmjohnson.net/post/2016-05-06-python-packaging/) to [Ionel Mărieș](https://blog.ionelmc.ro/2015/02/24/the-problem-with-packaging-in-python/) who said: &gt; In a way, packaging in Python is a victim of bad habits — complex conventions, and feature bloat. It’s hard to simplify things, because all the historical baggage people want to carry around. […] Concretely what I want is […] one way to do it. Not one _clear_ way, cause we document the hell out of it, but _one, and only one, way to do it_. ---- Briefly summarized, Python packaging is a nightmare for software distributors, who have to learn a ton of complicated crap to package up their software for others, and it's unpleasant for software consumers, who often run into problems with pip's UI and weird C-compilation errors. Yes, wheels help, but no it's not fixed yet.
&gt; No type aliases that I've seen, so complex types are hard to express. ‘Types’ are just values used in an annotation, so making a type alias is just a matter of assigning: multidict = typing.Dict[int, typing.List[str]] 
Post your questions (specific ones) in /r/learnpython - I'm sure people will help you.
Take a look at https://www.reddit.com/r/beginnerprojects/ and look at the sidebar it has resources for online exercises for python.
Mark is wrong about e never appearing except as a base of an exponent, although correct about its misuse. Formulas involving e without an exponent appear very frequently in certain optimization problems. That said you certainly do want `e**x` to be computed with `exp` and you probably don't want to reserve the one character symbol `e` in the library. So it might be better to rename the value as `eulers_constant` and allow users to`import as e` when they need it. That said i don't see how it has much of anything to do with tau.
I'd suggest you use selenium and praw (both are python modules) for that. If you want to dive in, here's a good [video](https://www.youtube.com/watch?v=bhYulVzYRng) about selenium. PRAW is a wrapper to make your life easier when making reddit bots.
Thanks man! ill check that out
Guido loves type systems now
You can also check out this simple reddit bot that I've made. [Github](https://github.com/rufinium/CustomHS-Scraper/blob/master/main.py). Aaand oh, this [site](http://www.jsoneditoronline.org/) will help you as well! I have lots of resources for making reddit bots because it was my hobby back then. EDIT: still searching my bookmarks with stuff that might help you. EDIT2: [Found it!](https://www.youtube.com/watch?v=Uvxu2efXuiY)
It isn't wasted effort - the comment version is fully backwards compatible, so it has value even if it's less ergonomic.
thats great man! thank you very much. if i could ask for one more favor. Could you look at this script and tell me how it looks? [github](https://github.com/phito/PlaystvBot)
That's the whole point - programming is the art of expression. Memorizing a few inscrutable incantations is not programming, it's *witchcraft*. My inner programmer heart despises magical incantations.
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
&gt; Optional language features become mandatory when you have to work with the code of someone who uses them. Using the fibbonachi series for indents and writing variable names and comments in latin become mandatory when you work with the code of someone who uses them. There are *plenty* of stupid style guides you can follow, and this feature can be actually useful.
PEP's often start out as suggestions. One big difference with this PEP vs many others is that Guido is a co-author. This automatically gives the PEP quite a bit of weight and pushes it forward far faster than if JoeSchmoe wrote the PEP. That said, I think this is still a suggestion. More specifically, the PEP was entered on 09 Aug and the original draft was very sparse. Today's has a little bit more detail but it's still quite cumbersome for Python's dynamic type system.
Oh ok, It's not wrong but it's not easy to start with either. If you go with selenium and praw combo you can do it with much lesser and more readable code. The only drawback is that selenium opens up a browser (but that's not a problem for me). Or you can just purely go with selenium since it allows you to automate the whole process of browsing. *For youtube just checkout the API if selenium wont work there (but i think it can).
The monthly compounded interest over 12 months is not expected to be the same as the yearly compounded interest over 1 year, since you also include the return upon the previous months. See [here](https://en.wikipedia.org/wiki/Compound_interest#Calculation_of_compound_interest). EDIT: So why the _monthly_ compounded interest? Well, it eased the calculations a bit. But it's nothing more than an approximation, really. 
okay i will check that out then. But is it reasonable to even try to make it? because i never worked with python before so i dont know how hard it is to make a bot that checks comments on playstv bots that automaticly uploads it to youtube and then comments with the mirror. Just a comment seems fairly easy to me but the whole combination of scripts seems pretty hard. Do you suggest to do it, or is it more advanced coding. Really appreciate the help man! youre a god
&gt;It isn't wasted effort - the comment version is fully backwards compatible, so it has value even if it's less ergonomic. You missed the main point. &gt;so it has value even if it's less ergonomic. A wrong value comment, is not valuable and could possibly be harmful. 
This is an obvious second step in a steady march towards making Python an explicitly typed language. Guido has spoke many times how companies have been pressuring him to add explicit types and type checking to the language. Guido has his ear to the ground and has worked in many corporate environments, he knows that if companies think maintaining Python code bases is harder than something like Java, then Python will die a slow death. Type hints are an obvious bone thrown the way of large code base maintainers and IDE devs, and this is the logical continuation. I would be willing to bet that any of you $50 that if this is accepted, within two years mypy or something with a similar feature set will be merged into the default interpreter accessible via CLI switch. This change is not necessarily a bad thing, but it will change Python 3 into something new, something that's not quite Python. btw to those of you downvoting anything negitive in this thread, the downvote button is not a "I disagree" button
I learned python in 1 month just because I wanted to make a bot. It's a fun tool! It's not too complicated if broken down in parts. But for an overview here's what i think: Easy: *Searching comments to reply to. *Replying to comments *using Selenium Hard: *Uploading stuff in youtube. (depends on the API, but if selenium works there, you're in luck!) *Getting stuff from plays.tv. (again, depends on the API, but if selenium works there, you're in luck 2.0!) I keep on saying selenium because it's just so awesome for me. My webscrapping work just got easier after I've decided to learn it.
Please no. Stop trying to make Python something that it isn't.
ah okay, i will defenitly try out selenium then. Really thanks for the help!
Because most of us think that the standard library documentation is pretty great. You're the outlier here. There are no function signatures because, up until recently, Python did not have anything like a "function signature". PEP 484 introduced something like this. Much of the documentation hasn't been updated to reflect this. Pydoc probably doesn't even inspect the type hinting signatures. I've never found the examples to be overly complicated. I've found them to be comprehensive. Python is *not* PHP. There are many APIs that aren't as simple as "what does this function return". If you're trying to learn Python by reading the standard library reference, you're going to have a bad time. Maybe a book would be better?
&gt; Your suggestion is to add static typing to the language **optional** static typing. &gt; Python will remain a dynamically typed language python will overall remain dynamically typed if you need to use special syntax to make *individual variables* statically typed. &gt; and the authors have no desire to ever make type hints mandatory, even by convention idk why you highlighted that, nobody proposed to make them mandatory
I thought Yahtzee was Chinese?
No prob. Just saving you from the hassle lol. 
yeah, but I agree with @the_hoser. You need to understand how python works, where you can apply it, and when you should use python language. I suggest you to read some PEPs that define what is the Python Language. and don't forget to read the zen of python (pep 20). 
&gt; Because most of us think that the standard library documentation is pretty great. Amen.
The term 'sequence' has [a defined meaning](https://docs.python.org/3/glossary.html#term-sequence). A dict is not a sequence. A list or tuple is. In this case the documentation was copied from [PEP-249](https://www.python.org/dev/peps/pep-0249/#fetchone) which describes the Python DB interface standard. A DB driver can choose to return any type from `fetchone()` as long as it's a sequence. It will probably return a tuple, but it could also return some other type. The point is that you can only count on it having the properties of a sequence, nothing more. 
True and false :). Mypy for example will display the full type and not the name of the alias. So we need a proper typedef function for that.
try also on codereview.stackexchange.com
It will be an error if you use mypy.
We are not special :(
I responded to your issue you opened, I have added proper attribution, it seemed to me that his work was based on my original implementation (which would make his work, LGPL), but I added attribution in the readme, and the BSD header as you requested. I had no intention of making this look my work. I assure you it was more of a different understanding of the situation than any malicious action.
This is what I'm looking for! But see, I think it is misleading to call [this](https://docs.python.org/2/library/sqlite3.html#module-sqlite3) documentation. I find much more useful the PEP rather than the sqlite3 docs. Plus, if you google "python sqlite3" you don't get that PEP in the first page (at least I don't) and this is not reasonable. I understand that the PEP 249 is just a generic specification on how a sql module should be made, but this is, more or less, what is needed to understand how the module works. The point is, I don't want a tutorial on sqlite3 for python, I want a list of the functions of the module, a brief description for each of them, and quick and clear examples.
This library cannot. If you want something more like AutoHotkey, for the full suite of tools I highly recommend that you try [pywinauto](https://github.com/pywinauto/pywinauto/)
Yes. Also, when two different python libraries both rely on different C libraries, that then have a shared underlying dependency. This happens in the math, openGL, audio, video, etc. world much, much more often than you think. Simply "bundling" up the direct, first-level C library dependency into a wheel doesn't solve this problem, because they'll each try to load up a DLL of the underlying dependency. This is not allowed by the OS, and one of them will end up with a shared library loading exception, which in the best case will be reflected into Python as an `ImportError`. I say this is the best case, because the worst case is when they look to be compatible, but due to differing compilation options, the actual underlying machine code is making incompatible assumptions about the size of data structures or whatnot. This will then lead to a segfault at runtime. The core Python packaging folks mostly don't want to think about this problem, because it is a really really major pain in the butt. If you decide to go even a little bit down this rabbit-hole, you end up, like Alice, falling for a long time until you hit a Wonderland of obscure shared library idiosyncrasies, on all sorts of platforms. Fighting through this nightmare of decades of OS-level quirks is not why any of us got involved with Python in the first place. But if we are to make it work well, someone *has* to be thinking about this. We've made really great progress in solving this problem with conda and in the Scientific / PyData community. But I have to admit that it's frustrating when others in the Python community pretend that these problems are really easy or just go away magically if they ignore them, and what are all these scientific python weirdos doing with their weird conda packages, and why can't they all just get on board with pip. etc. etc. (I've seen all these arguments - and worse - being flung at scipy packaging folks for over a decade.) I bear no ill will towards folks in the PyPA because they're trying their hardest to make the best of a messy situation; but I do wish that there wasn't always this subtle undercurrent of shade being thrown at "scientific and data science python folks doing non-mainstream packaging".
By saying that it returns a sequence and not a concrete type, it's telling you that the only things that you can assume about the type is that it's a sequence, i.e. it has numeric indices and a length. Anything beyond that cannot be counted on. Therefore it doesn't matter whether it's a list or tuple, because relying on any aspect of its behavior that would differentiate between a list or tuple would mean you've gone outside of the contract of the API. This is done to allow flexibility on the part of the DB driver. It's not being vague just to be vague. 
&gt; This is what I'm looking for! But see, I think it is misleading to call this documentation. I find much more useful the PEP rather than the sqlite3 docs. Plus, if you google "python sqlite3" you don't get that PEP in the first page (at least I don't) and this is not reasonable. Well, I agree, and so did the author of the documentation for the sqlite3 module. That's why there was a link to the PEP-249 documentation in the very second paragraph of the page, right after the intro text. If you'd actually read the docs, instead of scrolling down to find a function, you'd know this. &gt; I understand that the PEP 249 is just a generic specification on how a sql module should be made, but this is, more or less, what is needed to understand how the module works. Absolutely, which is why it's right at the top of the documentation. &gt; The point is, I don't want a tutorial on sqlite3 for python, I want a list of the functions of the module, a brief description for each of them, and quick and clear examples. The point of the sqlite3 module documentation is to be a comprehensive documentation of the module, not to be a listing of its functions. The documentation is great. I don't think the problem is that the module maintainers are bad at writing documentation. I think the problem is that you're bad at *reading* documentation. It's not your fault. I gather that you learned this nasty habit with PHP and its ADHD documentation. The only advice I can give you is this: Stop and read. Don't google for the module, skim down to find something that seems relevant, and throw a tantrum when it doesn't make sense. Start reading from the top. You'll gain a deeper understanding of the module you're using. A little time investment now adds up to a massive amount of time saving later.
It doesn't matter. Just assume that the value you get is iterable, and indexable, and be done with it! You're getting a sequence, which means that the object must conform to the sequence api specification. This is object-oriented programming at its finest!
I would like to point out that since this hasn't even been accepted, it will be at least a year before this ships in Python, much likely longer. So you'll have plenty of time to learn the syntax. By then there will be plenty of articles. And anyway, most intros to Python don't cover new syntax anyway. The same argument could have been made for the new string formating syntax, which would be terrible. I started programming Python when 2.x was used by like 80-90% of users. I had to learn a lot more than just type hints to move to Python 3.4.
You're missing the point entirely. If you have some super polymorphic code that can accept every type under the sun then good for you. But most functions only accept a very limited number of types. &gt; It seems like the typing proposals are by people who don't really understand -- in typing terms -- what makes python attractive in the first place. Conversely it seems that the hate against typing proposals are by people who don't really understand -- in real terms -- what makes Python fucking horrible to work with at scale.
I learned programming from scratch using edx a few years ago. I think this was the first course I did - https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-8 The advantage I found with this over doing something like codeacademy alone was that I learned a bit about data structures and efficiency that I would have missed out on otherwise. I skipped a good few rookie mistakes that way. Currently finishing up Msc in CS now and I'd still say that edx course was a decent starting point for a complete beginner. Udacity is also a good place to look. I've taken a few general machine learning and deep learning courses there which were pretty good (all in python too)
*Sigh*. It's that time of the month again: time to bash Python type hinting for no good reason. Python needs to evolve people. Languages shouldn't stagnate, and it's **very very very** clear to anyone who has worked with a large Python codebase that the lack of type hinting is a pain. Pythons dynamic nature is awesome and I'll be the first to defend that, but in large codebases it's not that great. Most functions you write really only accept a single type, admit it. Sure if you have a function that accepts two integers and does something with them it's nice to be able to pass a `MyFakeInteger` with an `__add__` dunder that actually divides the operands, or launches a pigeon out the fucking window or some shit. But thats the 0.0001% case, not the 99.9999% case. Adding explicit type hinting optimizes for the 99% case as it should. And at the end of the day if you don't agree with the type hints *then just ignore them*. Nobody is taking your dynamic typing away, nobody is turning Python into a compiled, static language. It's just so editors and refactoring tools work better in huge projects, the ones that large companies create and the ones that help Python be as popular as it is today.
But I'm dutch so thats why it's in Dutch
1. The game, score and menu are split in different defs. 2. I never used classes before, could you help me with that? 3. I don't understand what you mean with this :$
&gt; Sure if you have a function that accepts two integers and does something with them it's nice to be able to pass a MyFakeInteger with an __add__ dunder that actually divides the operands, or launches a pigeon out the fucking window or some shit. Honestly, type hints really shine in this case, because one of the big issues with duck typing is not knowing which subset of a class' interface you need to support (and what properties each method needs) to make a duck quack. This is the perfect use case for an abc, or even without one it's simple enough to just inherit from int, but in either case you almost certainly benefit from being forced to define the interface.
Even a small one can be a pain if it's difficult to test. I wrote a small package which monitors network health (large, enterprise network). Literally the only way to test that code was to push it to the live network. Simulation / scripting / mocking was out of the question (mostly because of timing and trying to simulate network failure modes). I can't tell you how often I'd push code only to find out that it's failing because of some dumb typing error.
Yes, sorry for the confusion. It has been a while since I had to throw around terms like that :-)
Join programming server in discord. There's like 300+ active members all the time, so i'm pretty sure you'll find real time help there. Good luck!
You don't have to declare types: even without types in your programs, you still can get some of benefits from type declarations in your libraries as long as your tools support that.
That's not really a compelling reason in and of itself
Where are you hosting your code though? 
I think this is answering the opposite of the question asked. Also `s/=/: /`
Numpy and scipy do not follow semantic versioning. Shoot, Python does not. There are minor things that break every release. When you have a large enough package, you will find things that are mind numbing. I develop open source software. I will not test every combination of versions that I use. I will specify versions that I know work. I do not trust future versions of packages to not break my code. When you do everything inside of the little box Python is good at, yes, there are no issues and I won't even specify a version requirement at all. When you push the boundaries, you find problems and I will be very specific.
Interesting, I ddin't know about that
I know. It's not like it will keep me sleepless at night. Still think it is a bad idea.
Yes. And a great deal of python code is polymorphic. Only totured java programmers are retyping heavily interface-driven OO code in python.
Incorrect.
Why would using libraries (importing stuff) require you to use type hints?
No it's called Java.
&gt; Now, as much as I like this helping prevent issues with code I also feel that it would make Python bulkier. Its probably intentional to a degree They don't want no one using it They don't want everyone using it for everything They want some people to use it only when its really useful. Sort of similar to how `lambda` is the most silliest thing to have a full keyword for (too long for most uses, short enough for some uses) - - - also zen of py has been mentioned in like every other pep thread in existance, including other threads on this topic, aaaaaaaaaaaaaaaaaaaaaaa
It *is* optional. Wait until the PEP is finished before you start judging it?
They lost me at `ClassAttr[bool]`, vs just `bool`.
If you were comfortable going through the [Django tutorial ](https://docs.djangoproject.com/en/1.10/intro/tutorial01/), this book should be easy enough for you to follow.
what's your point?
Choo choo all aboard the hype train!
That's more to do with the hell of 'undefined'.
It's not that git is baroque but that it's too generalist and reductionist to first principles. Its generality makes it able to cover lots of use cases and to implement lots of specific tools (commands and subcommands). Now, people learn git from a problem solving perspective and find it a mess of unassorted facts, the same than people looked at physical reality before Newton. A bottom up approach is a convenient complement to stackoverflow quick searches when it comes to git, but most users won't bother themselves with it.
You may just create a shell function. It's not the goal of a general purpose packaging framework to provide the most efficient cli but some reasonably orthogonal set of tools. If the cli is also concise, well, that's better. But it's not an important dimension to compare.
We are not snowflakes :( Senpai will never notice us or pat our heads :(
ITT - too many people with no clue of interfaces (examples litter the collections module), isinstance, and getattr. Doing protocol oriented programming in Python is trivial if you use interface types to indicate membership of complex types. And your docstrings should be crystal clear on **what** they accept. This whole line-by-line level "type checking" is just compensating for code smells: - giant functions (the larger it gets, the chances of type violations increase) - not documenting critical or confusing functions unambiguously - not having unit tests written for said critical/confusing functions This is DropBox twisting the language unto itself because apparently they're the first company to discover that a large Python code base requires discipline. And my coworkers who frankly care nothing for advanced Python, just their PyCharm IDEs and fancy docstrings, are able to continually deliver on quality code despite not being experts in the language. I end up writing test cases for them to demonstrate capabilities or features of the libraries I work on, which they often use an example for how to get things done. It's highly effective. I see normal people handling complex Python quite well - I've mostly answered difficult questions about library versioning, not on what this PEP proposes. -1 for this PEP. Doesn't address a significant pain point but clutters up the code. Can't we compromise and have an adjacent file with type declarations, even for inside a function? 
There is nothing quite like it, but my favorite is Schematics 
If package A is a wheel and contains a dependency B. If I install the wheel for A do I need to install B to use B? Can I install a different version of B without breaking A? 
Even if other ecosystems suck. That doesn't mean Python needs to suck.
Because javascript is *crying out for type safety*. Python doesn't have that problem nearly as badly (it has far stricter runtime type checking).
&gt;Python needs to evolve people. If it's going to evolve into anything statically typed, I'd rather it was in the direction of F# rather than Java. &gt;it's very very very clear to anyone who has worked with a large Python codebase that the lack of type hinting is a pain. I've worked on several and I couldn't disagree more. I want *stricter runtime type checking* (e.g. if x: should fail if x isn't bool/implementing __nonzero__), not half hearted attempts at static typing. If you have a real problem with types in large code bases that's a hint that you probably have a minimal level of test coverage - which is the real underlying problem.
&gt; why can't they all just get on board with pip. etc. etc. (I've seen all these arguments - and worse - being flung at scipy packaging folks for over a decade.) And people tend to forget the origin of our independent packaging efforts, when GvR spoke at the first PyData meetup in 2012: Travis O. asked him for suggestions on our packaging difficulties, and GvR basically said that core Python is not going to solve them, and that our community should probably develop our own solution. I wish I had that Q&amp;A on tape.
Don't blame your lack of tests on lack of static typing.
The one part that kinda caught my eye and bugged me was b: str alone on a line, default to `None` Right now in python, I can put a variable on an empty line. It'll do nothing, but I can do that. Now, if I add a type next to it, it's suddenly a variable declaration? That just feels off to me. Type declaration should be completely removable without changing the program. It's the `= None` that makes that statement a declaration, not the type.
If you're using Firefox, you can open the `Inspector` with `ctrl`+`shift`+`c` (idk the keys for OS X) or `Menu` &gt; `Developer` &gt; `Inspector`. Then click the `Fonts` tab on the right.
The point is, static typing would have caught a whole class of errors that I otherwise would have to catch manually.
He's not in a minority though. And even if he was that's still a bad argument! His experience is valid and of the people who agree a big chunk are beginners. The very people that will grow the language. They are THE most important people. Because in 60 years or whatever they will be the only ones even alive. We need to take their experience many times more seriously than every old hat. Python documentation is thorough and complete. But it's bad at two things: 1. Google:able. The docs predate Google so that makes sense. But there is no good reason why it still hasn't adapted to how the world worked a decade ago. 2. Quick access. I like to get in and get out fast. I don't read docs like a book, I need to solve a problem NOW and don't have the time or inclination to read a book. Because that's what the Python document is structured like: a good reference manual book. No one uses those anymore though! That's why we even have computers. You aren't committing some sin or being disloyal by admitting the flaws of your favorite language. In fact, it's the exact opposite. 
It's bad because it's old. It's bad because it's volunteer work. It's bad because of Stockholm syndrome. 
I feel really skeptical about making comments a part of the programming syntax. It's one thing to use it for generating documentation, but another to actually affect how stuff works. Annotations (as in Java and PHP) are already kind of iffy, but to use them for type declarations just seems too much. I'm all for this change. (The exact syntax is debatable, mind. I'd probably prefer a separate statement. Maybe a double colon.)
we've got our None, it's not as bad but it doesn't make it good.
in short: ugly but necessary.
&gt; Then what's the point? C# example (statically typed except when using the `dynamic` initializer): parsing JSON into a dictionary
&gt;In a weakly typed language, 2 + "1" yields 3. In a strongly typed language, it yields a type error. ...or "21"
It's not bad. &gt; How come nobody came up with the idea to put a simple function signature for every built-in python function (like C, Java)? What would be the documentation for `math.sqrt`? Takes a square root, returns a float? It's self documenting (though I bet that's exactly what it says). What are specific functions you have a problem with? Are they functions that you shouldn't even be using? Next question is how do you find this documentation? There are examples for everything on the Python docs page with lots of pretty colors. You're not supposed to be going to the source in the first place. &gt; Why do I have to read the whole function description to understand (if I'm lucky) what the function returns? So you want more documentation without having more documentation? I can't help you.
Declaring variables would actually be *awesome*, though I'm not a fan of `var`. Why play "spot the typo" when I could type 4 extra letters the first time I use a name?
&gt; He's not in a minority though. And even if he was that's still a bad argument! His experience is valid and of the people who agree a big chunk are beginners. The very people that will grow the language. They are THE most important people. Because in 60 years or whatever they will be the only ones even alive. We need to take their experience many times more seriously than every old hat. Absolutely! And it's a good thing that we have other documentation for people in these situations. Perhaps there should be a disclaimer at the top of the reference documentation that they really ought to seek out a good tutorial, first? The reference documentation needs to make assumptions about the knowledge of the reader. It is not the place of every module's documentation to teach you the basic Python concepts you need so you can do something with that module. Might make a great premise for a book, though. &gt; Google:able. The docs predate Google so that makes sense. But there is no good reason why it still hasn't adapted to how the world worked a decade ago. I... have no trouble finding documentation for Python modules, standard library or otherwise, using Google. I can't agree with you on this one. &gt; Quick access. I like to get in and get out fast. I don't read docs like a book, I need to solve a problem NOW and don't have the time or inclination to read a book. Because that's what the Python document is structured like: a good reference manual book. No one uses those anymore though! That's why we even have computers. No. Full stop. No. If you need this, then just quit programming. You don't want to learn. You just want the results. If that's the case, hire a programmer and move on. You *need to understand how to understand*. This idea of "just show me how to do it" that's plaguing the software development community is responsible for some of the most mind-numbingly bad software developers the industry has ever seen. I'm not asking you to read 40 books before you start writing code. I'm not asking you to go to college and be formally instructed. I *am* asking you to read the module documentation from the top of the page in order to develop a semi-complete understanding of what it is the module does. I *am* asking you to understand (and have used!) some of the basic principles of the language before complaining about the reference documentation. &gt; You aren't committing some sin or being disloyal by admitting the flaws of your favorite language. In fact, it's the exact opposite. I'm certainly aware of this, but if the fact that Python's documentation doesn't come in ADHD format is your criticism, then I simply cannot get on board with that.
I would take any perceived hostility as constructive input. It's not directed at you, but rather a strawman version of yourself and the community in general. I'd be more concerned if there wasn't passionate discourse on the topic.
I agree that MyPy is a bad path to follow, and much prefer Obiwan for the same use cases. And I think adding this Variable Declaration Syntax is going way overboard; adding type hinting to method signatures seems like plenty. But if this is going to happen, this PEP (really, all Python type hinting projects) should take a page from Obiwan -- which handles duck typing [nicely](https://github.com/williame/obiwan#if-it-quacks-like-a-duck).
True. I saw dict() and thought {}. :-P
You're right, all dependencies in the tree need to have wheels. As noted, you can build those yourself using pip wheel, which will build wheels for the packages and all their dependencies (and their dependencies, ...) Whether you can install a different version of B without breaking A depends on the packages. The current ecosystem assumes there is always one version of a given package installed. There is another thread here discussing the pros and cons of that ( https://www.reddit.com/r/Python/comments/4xnip4/python_packaging_is_good_now/d6gyssh )
&gt; MyPy had potential, but it wasn't ready for prime time. That was known at the time. That's why a PEP was developed and the idea of MyPy was extended. MyPy has since been updated to match the PEP. There is continued work on type declaration that is coming in Python 3.6. I don't understand why people expect things to be perfect the first time around. `1/2` in Python 1.0 produced `0` and did until Python 2.7. It now produces `0.5`. String handling no longer works as it used to. AsyncIO has undergone multiple major changes and continues to. The choices that were made at the time were reasonable, but in some sense, you're feeling them out and as new features and use cases come along, opinions on how things work changes. Python is an evolving language. Things get added and things get removed. I don't see a problem with that.
Just one example: Javascript:: "2"+1 &gt;&gt; 21 Python:: &gt;&gt; "2"+1 TypeError: cannot concatenate 'str' and 'int' objects Javascript gets even *crazier* than that though: https://www.destroyallsoftware.com/talks/wat
Whenever the idea of being more strict with types in Python comes up, there's always a sizable camp of people who say that due to the duck typing of the language it's against the language's philosophy, but the simple fact is that virtually every project out there uses fairly strict type constraints in their APIs. Formalizing an optional syntax for type constraints would serve to clear up a lot of the strange `isinstance()` clutter and I think would be a huge boon to the language. Duck typing can be extremely powerful in many situations, but it can also be a massive pain when designing and working with API's (see: Javascript). Having one doesn't exclude having the other!
1. Your code violates PEP 8. PEP 8 says you should use `names_with_underscores` for all but classes. Don't use `camelCase`. 2. GitHub gists is more suitable for this kind of thing. Get an editor like PyCharm that can automatically warn you of PEP 8 errors and rename stuff for you. It's awesome, try it. `while correct == False:` - PEP 8 says you should use `while correct is False`. it's more pythonic More annotations in the github diff, there should be a blue dot over the notifications symbol.
C# also has a typing system incapable of expressing the actual (structural) types of dynamic code, that's why the dynamic fudge exists which is *untyped*. If C# was a language where you constantly had to use dynamic you'd say it's type system was broken: and thus so is pythons. Since ordinary looking python code is polymorphic in ways the type system cannot express. 
But that's not what it looks like in their example. If I type `b` in an empty line, one of two things happen. 1. If I had already declared b, nothing happens 2. If I hadn't declared b, I get `NameError: name 'b' is not defined` But now, if I put `b: str` on an empty line, according to [their example](https://www.python.org/dev/peps/pep-0526/#id2), it will: 1. Replace whatever variable I had defined with `b = None` 2. Add a new variable `b = None` to the namespace So no, it is NOT TRUE that types are completely ignored and have no effect. Having type information there makes the code **different** than if it wasn't there.
You can do puzzles on https://www.codingame.com
It's not going to evolve into a statically typed language. Python has very strict runtime type checking. If __nonzero__ isn't implemented then attempting to call it will explode. Same for any method that doesn't have a default implementation. That's a lazy answer. Tests cover so much and we do have a high percentage, but not 100%. Type hints help as an added part of your testing process (are you passing None to something that can't handle it?) and as part of your development, helping your editor give you decent type hints rather than nothing. Not to mention the documentation angle.
Great point, I worded this poorly. But, this proposal doesn't implement Python enforcing types. Unfortunately, I don't see how it address the the problem.
I think the assumption is that if you use these types you're integrating a static analysis tool alongside it. So for a very large codebase you can catch bugs before releasing.
It's not *just* that - it's an example of extremely weak (implicit) typing. Most of those examples should just throw exceptions, which is what python does. Python is not weakly typed - except in a few specific circumstances.
&gt;Python has very strict runtime type checking. if x: do_something() Not strict enough. This should explode if x is a string, datetime, int, float or indeed anything other than a boolean (or acting like a bool via duck typing). Instead, it decides that it should avoid doing something if x is None, 0, an empty string, 0.0. Or, in python &lt; 2.6, fucking *midnight*. I've lost count of the number of times I had to change the above to this to fix some obscure bug:: if x is not None: do_something() *That's* what's broken about python. Not the lack of pseudo-static typing. &gt;That's a lazy answer. No, it's the real answer. Once you have an acceptable level of automated test coverage, type errors (of the kind caught by static typing) in production become much rarer, making the added overhead of additional characters littering every variable in your code not worth it.
&gt; BTW: How do you make your code clean, without many x, y and many other variables that act as placeholders, because they make me hate myself when debugging my code. By not using variable names without semantic meaning. You can improve your code immensely by using variable names with meaning whenever possible (and '_' when you have to assign a variable you don't need). For example, rather than: for x in possibilities: if string_contains_string(letters, x): output.append(x) You can write: for possibility in possibilities: if string_contains_string(letters, possibility): output.append(possibility) 
Ad hominem.
No, it clearly tests if a variable is truthy, and you literally just included the reason it doesn't explode in your first sentence. Because it's acting like a book via duck typing. Yes, a string can act like a bool. And it's part of the awesome duck typing that makes Python great. Excluding the midnight bug, which is fixed and is in an irrelevant unsupported version of Python so I'm not sure why you're bringing that up. If your checking if it's None then you use x is not None, else you just use if not x. Simple, and hardly a broken feature. You're right about test coverage, it reduces production errors. But even 100% test coverage won't catch all bugs. Type hinting adds another tool to catch bugs, maybe even the ones you described above. Also stop strawmaning me. You don't have to have the huge insurmountable overhead of a few extra characters 'littering' every variable you code. That's preposterous. You add it as and when you need it (and if you don't need it then don't add it!) In parts of the code that are problem areas for you. Much better than having the overhead of having your "if x:" statements explode everywhere for no reason.
&gt;Yes, a string can act like a bool. Yes, and it *shouldn't*. Nor should it act like an iterator. It should act like a string unless directed otherwise. &gt;And it's part of the awesome duck typing that makes Python great. It's part of the reason why I have to fix an awful lot of bullshit. Non-obvious implicit type conversions are evil. I'm not against __ nonzero __ / __ bool __, I'm against it being implemented on the string/int/float class. That's not awesome duck typing it's *stupid*. &gt;Excluding the midnight bug No, that is simply the worst manifestation of this bullshit. EXACTLY the same problem - after all they clearly did it to be consistent with all the other types with magical implicit type conversions. It's just with that one it caused bugs whose apparent cause is just too stupid for words. The others cause more common-o-garden style bugs that seem more justifiable but are, in reality, pretty much just as bad. &gt;But even 100% test coverage won't catch all bugs. Duh. Reasonable test coverage (not even 100%) renders *static typing* almost completely pointless though. Logic errors, bugs in the test, bugs due to the unrealistic nature of the tests - these will of course slip through. Naive type errors like "oh I forgot to pass an int and passed a string instead" not so much. &gt;You don't have to have the huge insurmountable overhead of a few extra characters 'littering' every variable you code. Oh, it'll happen. This is PEP8 all over again.
Pep 8? I don't think Python is for you mate.
so you probably want to do this using functions in each script or compile them but I guess this is the basic idea of modules in a way so say you have 3 files with python scripts: f1.py, f2.py and f3.py each with one you your functions in them (x(), y(), z()) so firstly we want to import the functions and then execute them. For ease its best to have everything in the same directory, there is a work around if this is not the case. So in a separate script: #import the 'scripts' (modules) with the functions import f1 import f2 import f3 if case_1 == True: f1.x(foo) if case_2 == True: f2.y(bar) if case_3 == True: f2.y(baz) does this help ? I'm terrible at explaining stuff ... also just to say you will probably get a better response over at r/learnpython. I love those guys
yeah pretty much. I mean if you have them all as functions you can chuck it all into one file and then you only have to import one thing. ^^ just check they are as proper python functions otherwise it wont work
This is a very interesting project. Some difficulties you might encounter: * Are you accounting for fixed-column layouts as well as character-delimited layouts? * What if a log entry spans multiple lines? I think in any case you'll require some level of human verification of your output, at least to establish a baseline for a particular log type. After that point, you can compare logs to your known log types to see if they are a good fit. Off the top of my head, some metrics you might use... * In the case of fixed-width columns, you might check for a header line to indicate column widths * In the case of delimited logs with an equal number of columns, you might check for characters that appear the same number of times in each row Delimited logs with an uneven number of columns are more difficult, and I'm not sure off the top of my head of a good solution. I'm sure there's some statistical method, given enough content.
After the 20th or 30th time running into this class of bug it'll probably dawn on you why this language misfeature wasn't such a great idea.
&gt;They don't want no one using it &gt;They don't want everyone using it for everything &gt;They want some people to use it only when its really useful. And I'm sure everybody will be sensible about it just like they were for PEP8.
In my line of work, I have a lot of tasks walking through line-of-business applications to do boring things that any computer could do. Laziness being the mother of invention, I decided to script what I could. I found SikuliX to be a tremendously valuable tool for the job, but its Java dependencies and limited Python coupling posed problems in several cases. So, I decided to implement my own graphical automation library in pure Python.
yeah I mean the basic idea is def func(var1, var2, vary3 ...): total = var1 + var2+ var 3 return total #then just to demo it a = func(1,2,3) print a &gt;&gt;&gt; 6
Try /r/learnpython
I'm sorry, my previous comment was a little facetious. But I maintain and work with several very large Python codebase and thats literally not even on the radar. It's not an issue. Just treat None as null. In a lot of cases an empty string can be equated to False, so it's a little too verbose to do "if x is not None and if x != ''" all the time. And for what gain? If you're messing the truthy/falsey values of things up then there are much deeper issues.
Tango w/ Django is real solid. Looks like they are currently updating it for 1.9 http://www.tangowithdjango.com/
&gt;Most functions you write really only accept a single type, admit it. Ok I don't think I understand why type hinting is done if it doesn't do type *checking* at runtime, but especially in your case, when a function really only ever takes one type, why would you need to make that even more explicit or obvious than it already is? Can't you just assert type(input)==yourtype if you need it to be that much more explicit and safe?
2nd schematics. It's the gold standard for validation libs in python 
Meanwhile, pyvideo.com looks very shady... EDIT: props to the site creators.
Python packaging has had some dark days indeed. Notably the introduction of the 'requires' keyword, which was supposed to list the name of the module(s) that the package imported (and not the name of the package itself!). And PJ Eby arguing on the distutils mailing list at the time that such a field was totally nonsensical and being shot down and 'requires' was PEP'ed and added to Distultils (and subsequently never) used while Setuptools forked a 'install_requires' field that was actually useful and become the standard. As far as improvements, it's 2016 and the Python Standard Library is still a blackhole of packaging metadata, which is totally absurd. I think if someone could use a time machine, then going back to the mid-90's and preventing the creation of site.py and site-packages directory entirely would been the biggest fix to packaging overall. site-packages only resulted in people running "sudo setup.py install" because site-packages was owned by root and the user just wanted to install a library for their own local use. Globally installed libraries done poorly are just a mess, if they had at least been confined to a /lib/pythonX.X/ for linux distributions it would have been a lot cleaner. 
&gt;It's not going to evolve into a statically typed language. That's the goal. Guido once introduced a PEP for optional static typing that got withdrawn because of widespread outrage. This is his tripartite way of sneaking it back in. First it was with optional type specifications in Python 3 function definitions, then the mypy gibberish, now that's not enough so he just wants to put it right in the language. Then we get optional static typing, then something, something TPaintFactoryFactoryFactory. 
Static typing catches a small number of errors and causes a crippling lack of functionality and exploding verbosity to languages. 
THANK YOU. This was my theory exactly; he's been trying to sneak this in via the "backdoor" ever since he explicitly suggested optional static typing in a PEP and people freaked out. 
On the value level Rust and C++ are both Turing complete and are on par with the amount of access to hardware. On the type level Rust is ~~significantly~~ less powerful than C++ as C++ allows arbitrary type-level constant-expressions ~~and unbounded recursion (it is Turing complete)~~. (Of course, this is compensated by Rust's rather powerful macro system, compared to the simplistic CPP.)
I used to use Delphi, and this looks like one "Var" section away from becoming Pascal to me. :-( I don't *want* to begin every function definition with Var x : integer y : str z: dict 
We do tell it to Guido van Rossum and the 91 contributors, but they were content to rush into this tomfoolery. Annotation was supposed to never be defined for any particular purpose so we could use it for whatever we wanted. Many people did so, coming up with things far better, and then Guido broke that promise. 
Congratulations; now you're programming in Pascal, minus the semicolons and Begin...Ends. We're half a step away from having mandatory "Type" and "Var" sections; maybe we'll add constants with a "Const" section too. :-(
I largely agree with your criticisms, and want to throw in one more of my own: this is continuing to take Python down the road of both ossifying specific patterns which haven't been tested in the real world, and closing off alternate future paths. The original annotation spec in PEP 3107 gave Python a formally-blessed way to store and retrieve *arbitrary metadata* on functions and methods. Of course type hints were a big potential use case, but so were other things (like attaching documentation to arguments, something that's always required awkward ad-hoc patterns to be invented in Python). But PEP 484 locked that down to only one use case (type hints) and tied it to the use of a single specific standard-library module (`typing`) and a single specific third-party package (`mypy`, which isn't anywhere near ready for widespread production use). That was all very premature given how little time there was for people to iterate on real-world use cases or bring the tooling up to maturity, but at least it still concerned an optional documentation-only feature. Now it's starting to spread out into executable statements, further locking in a specific, still-not-widely-tested-enough implementation. This does not bode well.
In this case, it's no true Dutchman, and Guido seems to becoming less Dutch by the moment. 
Then leave it in comments for the static analysis tool to process.
No, it's not ridiculous. It's like complaining that Java won't run on your embedded chip with 64KB of memory. You're using the wrong tool for the job. Haskell isn't designed for object orientation; Python isn't designed for static typing (or speed); C isn't designed for type safety, C++ isn't designed for rapid application development. 
I love Marshmallow, hate Colander and wouldn't recommend it. Why do I hate Colander so much, it's been painful to do validation across multiple fields in our project where we did use Colander, also it serializes everything into strings including booleans and ints which doesn't make nice JSON. In order to "fix" Colander, we practically had to subclass every field type they provide. Interestingly enough I have never heard of Schematics until today, but it seems similar to Marshmallow, I wonder how it "stacks up" to Marshmallow.
&gt; Some did just that, coming up with things like PyContracts. That does not break anything. &gt; . Then that promise was broken, the type annotation became for MyPy, and it was a half-finished implementation. Now we're stuck with it. How was the promise broken? CPython doesn't use the type declaration for anything. CPython made the promise, not PyPy (which also doesn't use it). MyPy can require it (it doesn't), which is external to CPython, anybody else (e.g. an IDE) can require it (also none does), but that doesn't break the contract specified by CPython. Also, we're not just stuck with it. As I said before,it's changing in Python 3.6. It was never meant to be finished. It was a good first start; kinda like how unicode was implemented in Python 3.0. It changed in 3.1, 3.2, and 3.3. &gt; Now it seems like they're experimenting with things within the language itself. Like lambdas, list comprehensions, and async? So what? Don't use that feature or pick a version (e.g. Python 1.0) and use that.
Let's consult what SemVer states, because trying to handle arbitrary version criteria is so incredibly full of non-generalizable edge cases that pinning a version is a better idea: Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards-compatible manner, and PATCH version when you make backwards-compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. I don't see your statement prohibiting ``&lt;`` makes any sense, as the SemVer specification should give us a reasonable expectation that the major version increment is broken. If a package doesn't respect that, then we're no better off than using ``==`` and hoping for the best. ``&lt;`` is a last defense against a common pattern of "I rewrote the entire thing and incremented the major -- what are you whining about?"
Very cool! Why didn't auto it wrappers meet your needs, not cross platform?
This is great. I think Python is finally evolving in a good direction, like Java. 
I quite agree. Also, this seems like a step towards a crappy, java-style type system that doesn't actually capture the real types in the program (most trivially, java allows every object-typed variable to be null, whether or not that is expect). If we're to have type checking, let's have type checking that actually captures the types in the program: declarations of protocols, sum types, parametric and inferred types, and other goodies. 
Could you please give me an example scenario? ELI5? Sorry I'm a noob but love to see what I could eventually do. 
Some of my work involves setting up access to a particular module for new users in our third-party EHR system. Because of the way it's set up, that involves several steps in different menu paths, and it's easy to miss a step. I wrote a simple script to take the basic information (user, access level, financial access codes) and automatically open the EHR, open the correct menus, and save the information in the right places. In another case, we had spreadsheets of data that had to be manually entered one record at a time. I wrote a script to automatically loop through the menus and fill in the data. Of course, sometimes the automated processes will have bad data or something like that, so you can also check for familiar popup error messages (like "Do you want to overwrite this file?") and either handle them or abort to let the user manually resolve the error. That's business automation. This can also be used to create automated tests for GUI applications, where you want to verify that your changes didn't break any interfaces and the user experience is still the same. Hope that helps. =) Edit: I should also add - I'm targeting screen elements based on screenshots of the particular button or field I want to click on. The script finds a matching area, meaning it can work with regular Windows apps, web pages, or complex UIs.
Looks like a good tutorial, though a bit of the benefit of Rethink is lost on flask (auto subscribing to changes). Not to say it can't be helpful, but the big selling point is "real time" stuff. Secondarily, I think loading the schemas manually is pretty dirty, may be worth utilizing jsonschema or marshmallow. I have a [sample](https://github.com/mrasband/elo/blob/master/elo/schemas.py) that loads the request based on a decorator - I think it cleans up the code samples a bit (though I'm glad in those samples they made it clear where the data is from) Kudos to the author for showing the imports and such, it's always hard when an article doesn't show that and you have to guess!
Hey OP, I know a lot of time passed but I'm facing a problem while using this with Python 3. For some reason it's creating the images all skewed (or overly indented) but cannot figure out where the problem lies. [An example.](http://imgur.com/7nP0NIf) The last lines are not indented at all in the original code. Any ideas? I'm looking around the tab replace but nothing yet. Thanks!
I think the best example of this is what short shrift `pycontracts` has been given in relation to `mypy`. `pycontracts` is much more pythonic and has a DSL that actually helps with cases such as, e.g., bounded integers or enums while `mypy` is just... ugh. Between the `Any` type and the `[]` notation and all the other little abominations, ugh. https://andreacensi.github.io/contracts/
[Link](https://medium.com/@nntaled/the-most-intolerant-wins-the-dictatorship-of-the-small-minority-3f1f83ce4e15#.v9ssh53zj) to that article.
Great idea. I actually have some autoit scripts wrote by a friend to help me with some boring tasks. I'll definitely try your package to implement those autoit to python! 
Your best bet is to Google the title and edition with filters for PDF, Google Docs, or Scribd. That usually works in a pinch. Try and find the ISBN on WorldCat or Amazon and then you could try more sketchy ebook sharing sites. Good luck!
Is distributing opencv for a module installable via pip really as easy as just putting package_data = {'':cv2.pyd} in your setup.py? It there anything more to it? I've always found it difficult to have opencv as a dependency simply because its not installable via pip.
Here's some interesting code using pyaudio. Looks like it is close to what you're trying to do! http://www.ugw.name/?page_id=157
No it's not. opencv can actually be a challenge due to the way they insist on packaging it. But if you are running ubuntu 14 or newer there's a side channel in anaconda for it. Won't work in centos 6 however.
I fucking love Brandon Rhodes. Watched one of his videos on Pandas and bookmarked all his other videos because he's actually entertaining to watch.
Yeah I know about using conda - my point was I was surprised to see this project available via pip install lackey while simultaneously needing opencv as a dependency and thus wondered how they'd gotten that to work so easily.
Sikuli is great! Did you manage to improve performance over it any?
Oh, yeah! Sikuli is impressive, but I was always wondering, why the hell it was written in Java/Jython? Always seemed like a poor decision too me. I really hope Lackey will get more recognition.
eh, PEP8 is more similar to PEP20, where shoehorning it in takes less effort than actually making an argument for something Its deffers way more like lambdas, in that they are intentionally bad
Makes sense, thanks!
Does this support all OS? Does this use OCR to get strings of things the user could read?
Hi all, this is a simulation for gossip protocols that I thought is interesting and wanted to get some feedback on the design/code. I have very little/no experience writing "numerical" code so here is my third attempt at it. Currently the code generates [JSON lines](http://jsonlines.org/) as output for consumption by other programs (WIP is a PDF generation script) via piping, but if anyone has a better output suggestion I'd be more than willing to consider. Also I am optimising (of course, with profiling and `top`-ing) for memory usage and speed on PyPy. Any help on that front will be appreciated! The next step forward would be to write the core in Go and perform regression tests (probably using root mean squared so I need any stats guys to help me out here) against a slow but absolutely correct Python version.
It would be nice if the contents listed in your README file were hyperlinked to the actual notebooks, so that people don't have to search for the link in the repo listing above and then click down to find the notebook. Some explanatory text in the notebooks, surrounding the code, and some comments in the code itself would be helpful. Most people aren't going to find raw code without docs or explanations very useful.
It takes you to a huge page with the entire module, not a page with that thing. That means you have to search in-browser too. That's not the case for any other language afaik. Or people just go to stackoverflow which makes the argument on if the docs are good moot because they could not exist and still be as good :P
You write 500 words yet fail to define EHR?
Yeah, I haven't delved into that large of a dataset, but with my current data visualization project at work, I first tried building it out with d3 with moderate performance, but definitely nowhere near 60fps and that meant it wasn't going to scale nicely at all. I then tried webGL and yeah. No. That wasn't happening. After getting used to 2d canvas, it's amazing. It's more challenging than working with typical DOM elements... But if you play things the right way, keep yourself in check, and get your hands dirty with basic geometry/trigonometry/linear algebra, you'll be able to run 2k resolution data visualization with 2000+ data points at 60 fps easily. And the best part is that it's entirely possible that people won't believe that it's *in a web browser*.
That's great to hear, thanks for the notice.
Rust type system is also Turing complete, it's 'just' much harder to use in the C++ way, and sometimes not possible at all, which is what you meant I assume.
Yea it is ridiculous, mainly because it makes no sense. As does yours, who's adding static typing to Python? 
Look into Selenium, which is a browser automation tool. Its primary goal is for automated testing of websites, but it might also be ideal for what you're after. http://www.seleniumhq.org
I think you would be far better off using selenium for something like this as it is more specifically designed to target web applications. And I believe you can do everything you mentioned with selenium. I'd add a link to the python bindings but I'm on mobile. Shouldn't be hard to find.
&gt; And it's a good thing that we have other documentation for people in these situations We do? Where? And if we do, that's worthless if we don't redirect the newbies to these resources. &gt; It is not the place of every module's documentation to teach you the basic Python concepts you need so you can do something with that module. Strawman: That's not what OP wanted or what I'm saying. &gt; No. Full stop. No. If you need this, then just quit programming. You don't want to learn. You just want the results. If that's the case, hire a programmer and move on. Eh. Look, I don't want to "learn" what the 7:th parameter of some python standard library function does no. If you do, you either have amazing memory or YOU should stop programming because you're wasting the time/money of your employer. OF COURSE I "just want the results"! Why in gods name would you do programming for in the first place if not for the results? What does that even mean? &gt; You need to understand how to understand And hiding the explanation of the parameters of functions in a huge block of text instead of just listing them is helping how? &gt; This idea of "just show me how to do it" that's plaguing the software development community is responsible for some of the most mind-numbingly bad software developers the industry has ever seen. Ok, I think I see your confusion. You're talking about a "full stackoverflow developer". Someone who basically copy pastes from stackoverflow/examples every single thing. I understand that I might have come off like that now that I reread it. Sorry, that's not what I meant. I meant more like it should be easy and fast to skim to the part of the docs you are actually interested in right now. Not that there should be an example just for me or whatever. &gt; I'm not asking you to go to college and be formally instructed. Ok, now you kind of went off the rails a bit. I did go to college but dropped out because I was already self taught to the level where I could instantly spot that almost all professors were less competent than me :P You don't need a formal education, but you DO need to know your basics. There I agree. I just don't believe most universities teach them :P &gt; I am asking you to read the module documentation from the top of the page in order to develop a semi-complete understanding of what it is the module does Sure. But the docs should have a page for that, and reference pages for the classes/functions. There is no conflict here. You don't need to just shove it all in one gigantic page.
OK, thanks. Detailed analysis has been shown pyhooked-0.7 and hooks.py are only 40% similar while the code structure is different and pyhooked-0.8 is &gt;=81% similar to hooks.py with almost the same code structure. Let's discuss this confusion further in [GitHub issue #15](https://github.com/ethanhs/pyhooked/issues/15).
&gt; Because most of us think that the standard library documentation is pretty great. You're the outlier here. So untrue. https://www.reddit.com/r/learnpython/comments/4sdw83/is_it_me_or_pythons_documentation_very_poorly
Intrinsically, nothing. The problem is that junior developers treat it as a template for what it means to have 'clean pythonic code', which it absolutely isn't. They try to push it on code bases because they think that's what they're supposed to do. Guido even complained after putting out the PEP8 that people would submit pull requests to the python code base that did nothing except PEP8'ify code, which he rejected because it was risky (it had the potential to introduce bugs) and was kind of pointless. (this is a story the downvoters probably haven't heard...) I've had similar experiences with other code bases. What Guido complained about rang true. I see this as a negative unintended consequence of PEP8. I suspect a similar thing will happen here.
&gt;eh, PEP8 is more similar to PEP20, where shoehorning it in takes less effort than actually making an argument for something Precisely. Same thing will happen here. 526 is going to be shoehorned in where it's not necessary. With the crucial difference that PEP8 didn't increase the character count.
If statements:: if not empty_string or None or Zero or 0.0 or midnight: do_something() Also, string is treated as an iterable - which is why if you feed a string into a function that expect a list of strings you get bizarre outputs like ['w', 'h', 'a', 't', 'i', 's', 'g', 'o', 'i', 'n', 'g', 'o', 'n'] rather than a more obvious failure.
Is there a reason there is so much bytecode included in the repo? 
It's really a pity CPython hasn't done what pypy did: just make all dicts ordered dicts. In pypy they claim it even makes them take less memory and are faster so seems like a no brainer.
&gt;I maintain and work with several very large Python codebase and thats literally not even on the radar. Just last month I was told by a junior what you just told me ('if x' is the pythonic way of writing the if statement) and he subsequently ran face first into *exactly* the class of bug I just described. If I hadn't said "this is a risk" beforehand he probably wouldn't even have realized that he was setting himself up. &gt;And for what gain? So you don't accidentally treat having no price at all the same as having a price of zero. So you don't accidentally treat the user *not entering a phone number* the same as the user not having been asked for a phone number. So you don't have a rare scientific measure of 0.0 cause your system to throw a wobbly because it thought that the measurement wasn't taken. Subtle edge case bugs like this are the worst because they take you by surprise in production. That's why I want to see a stricter type system fail *early* when it sees ambiguity. Junior developers rarely see benefits of stricter typing up front, and repeated exposure to the same class of bug only sometimes causes them to see why it helps.
Hey, thanks for the bug report. I din't test with Python 3, but I think it's line 249, which in python 3 performs float division instead of integer division. I'm gonna fix it ASAP. If you want to try fix it yourself, replace "/" with "//" on line 249. EDIT: I pushed the fix (but still only tested with python 2.7).
you have a bad attitude mate
That is a good way, but I don't like large variable names. Well then I will have to get used to it Thank You.
mmmmm, I really didn't love lambdas, probably because i didn't really try to learn them seriously, but they seem to be very useful. I didn't know about filter so I'll have to check it out. Thank you and I'm pretty sure your cleaned code will help me.
Not yet, but both of those are on the TODO list.