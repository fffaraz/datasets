Not purely selfish though.. keyword argument code is a lot less brittle, so having "it's faster" as a motivation for positional will hurt python programmers in the long run.
What kind of job? Python mostly sees use in data analytics and IT automation/DevOps from my experience. Web or application development with Python is getting more rare by the day. What kind of projects do you want to involve yourself with?
&gt; If you're keeping track of how popular a global lookup is and optimizing based on that, isn't that basically JIT behaviour? No. JITs take some representation of the source language and emit natively executable code at runtime. Your likely confusion is that execution engines (notably most production JavaScript ones and the old HotSpot JVM) profile code execution in the way you describe in order determine which code should be JIT-ed. JIT-ing code implies a one-time compile overhead with the hope of execution-time gain. If a bit of code only gets run once, the JIT overhead may outweigh any execution time advantage. Particularly sophisticated execution engines may start interpreting byte code, JIT some more commonly called code with a simple, fast-to-compile JIT and then spend some quality time re-JIT-ing particularly commonly called code with a slower-to-compile JIT. See, e.g. https://en.wikipedia.org/wiki/Just-in-time_compilation#Startup_delay_and_optimizations
What's the error message you get when it crashes? I haven't heard of that before. You could try filing an [issue on Anaconda](https://github.com/ContinuumIO/anaconda-issues/). Though I wonder if there's something wrong with your computer. 
I suggest you post when you've completed your tutorial.
if you need to deal with CAD checkout [pythonocc](https://github.com/tpaviot/pythonocc-core/)
With enough thrust a brick will fly ;). Agreed though the performance of js for a language that is hard to optimise *is* incredible.
Make a PEP8 tutorial! So many coders would benefit immensely. 
No problem! Feel free to make requests!
Great idea! I will look into it! 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/numba] [\[xpost r\/Python\] Numba applied to to high intensity computations: A casual look at its performance against optimized Fortran code](https://np.reddit.com/r/numba/comments/4332eu/xpost_rpython_numba_applied_to_to_high_intensity/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I've used [Mayavi](http://code.enthought.com/projects/mayavi/) for 3d visualization. It builds on VTK and can be used with pyqt. 
Your "artifact" would be a miniconda package list. Your install file would just install miniconda, then pass conda the package list. 
...with an Elasticsearch backend.
I doubt that
Skipping to SQLachemy without learning SQL for abstraction is a nightmare and promotes harmful habits
There is no excuse why CPython implementation couldn't use similar highly optimizing JIT, instead of rather naive bytecode interpreter. JS is also a dynamically typed so they fit in the same category. But the intention was comparing progress of both implementations, performance of former/earlier versions and recent ones. Remember how sluggish was JVM or different JSEs before V8 released.
I don't believe IPython supports syntax highlighting in the terminal. Perhaps you're thinking of [bpython](http://bpython-interpreter.org/screenshots.html)?
for years I've recommended going through [SQL for Smarties](http://www.amazon.com/Joe-Celkos-Smarties-Fourth-Edition/dp/0123820227) which in the years since I had it seems to have grown to be three times its size, but nevertheless describes relational databases and SQL querying from the classical DBA perspective. He also has [SQL Puzzle books](http://www.amazon.com/Puzzles-Answers-Edition-Kaufmann-Management/dp/0123735963) that are also great. If you go through Celko's books you'll really have a sense for how a relational database was really meant to be used, and when you see someone touting MongoDB queries you'll just laugh your ass off.
since when required registration implies "free"?
Please post this to the numba github. I am certain the devs would love to read this.
Not sure if it's still in print but the best book for learning about SQL is [Teach Yourself SQL in 10 Minutes](http://www.amazon.com/Minutes-Sams-Teach-Yourself-Edition/dp/0672336073) (really, just a bunch of very very short chapters that each take around 10 minutes of time to learn the topic). Really fast read, comprehensive, and, last time I read it, completely free of mistakes.
To be totally honest I don't care enough to dig up all the resources I read through that originally put Mongo et. al. into the 'not even a consideration' bucket. They were convincing at the time though for whatever that is worth. I don't know the current state of affairs for 3.0 because it isn't relevant enough to my work for me to bother keeping up with. [Here is a fairly recent Quora post on the topic from what appears to be a reasonably credible poster.](https://www.quora.com/How-much-credibility-does-the-post-Dont-use-MongoDB-have) Super mega caveat: I only skimmed the first few sections of the post and checked out the authors credentials. It could be a shitty article and it could even disagree with my statements that Mongo is terrible. Don't know, don't care. &gt; Lots and lots of people seem incredibly frustrated with Mongo, This should be a HUGE warning flag. The mere fact that there are dozens and dozens of lengthy blog rambles bitching about the platform/concept would be enough to sour the water for me again though. NoSQL is a decent concept and it does have use cases (just like graph databases, CSV files, JSON files, and on and on), they just aren't use cases I care about in my current work.
SQL for Smarties is not in any sense a book for a beginner in SQL. Furthermore, his focus on using correct-but-nonetheless-complex terminology (when he starts talking about sets I very much wish I had a stronger CS background) which makes it more opaque to the self-educated programmer. I feel like a lot of his stuff is highly theoretical too. Like, some of the solutions use SQL statements that I'm not even sure are supported by many engines (or at least, weren't when I read the book 10-15 years ago). I'd only recommend the books for someone whose primary focus is databases or for people who just really love solving complex problems with SQL. I think *most* programming this day is focused on CRUD with a few reports in the mix. For example, you're more likely to put things like business logic or data aggregation in the code rather than in the SQL query. And by the way I think this is partially why these days often NoSQL data storage works just fine — especially for very large scale websites, the overhead of a relational database just isn't worth it, and you're putting a lot of what used to happen in the database in the application code instead.
I've definitely heard people say Postgres is a stronger database and is recommended for use with Django. I've always used MySQL as my database of choice and never had problems. I can definitely recommend *against* SQL Server if you can avoid it — I can't due to tons of legacy data — you have to rely on pyodbc which is sometimes buggy.
That said, its performance is remarkable. It can handle a *lot* of requests before you notice a slowdown.
Now that's a great &amp; honest suggestion. I'd also recommend dividing up time between CRUD/transactional queries vs reporting queries. CRUD is what the vast majority of people think about when they think about SQL. It's also kind of simple, repetitive and boring. And it's also what ORM's can later help you with and what most NoSQL offer. Reporting queries however, are another beast entirely. They're generally not handled nearly so well by ORMs. NoSQL solutions really struggle to support them. And they can be complex, fun and fascinating (mostly like in a puzzle kind of way). Interesting example I saw this week: [traveling salesman problem solved in sql](https://www.periscopedata.com/blog/postgres-recursive-cte.html)
Even though this is not a Python library, it does sound useful. Do you know if it accepts JPEG images as input instead of videos?
Hm, sorry, I am not 100% sure if this will work then. I would suggest to just try it out
&gt; It can be a pain if you're looking up a SQL query on Stack Yeah, that's a great point. Any dialect of SQL is similar to the others, but snippets for one don't always work in the next. Even a seemingly simple SELECT can be booby trapped with non-standard clauses that simply don't work on the next. The beginner must stick with product and version specific tutorials in order to avoid confusion of this sort. 
You sparked my interest on how to get a random sample from a generator. It seems for that you could use the [Reservoir Sampling](https://en.wikipedia.org/wiki/Reservoir_sampling) algorithm. Here is a very good explanation of the algorithm: https://stackoverflow.com/questions/12732982/design-a-storage-algorithm/12733515#12733515 A python implementation: https://code.activestate.com/recipes/426332-picking-random-items-from-an-iterator/#c2
Check out flowingdata.com. 
I wonder if these books are machine-generated from a template or a copy paste workshop in the third world?
Can you think of any advantage of using VTK as opposed to something like PythonOcc? I'll be rendering stuff in 3D sometimes and 2D other times/most of the time. Most of the user interaction will be in the 2D realm... 3D is more for checking/viewing the geometry. The 2D will need to be highly interactive.
My username approves.
You can actually give Numba a hint and tell it what types to use which you may find speeds it up slightly. Edit: The doc I linked to was out of date, here's a newer one... http://numba.pydata.org/numba-doc/dev/user/jit.html
Initially, I'm trying to keep things mostly working as they already do to ease the transition (it's quite a big change!). It already gives us syntax highlighting and proper multiline editing, though! Once we've made the switch, I want to look into what else we can do with prompt_toolkit. But for now, my focus is on integrating it and simplifying things without breaking more than I can help.
Interesting! I'm putting together a Cython version for comparison. Would your guess be it's sufficient to optimize the inner loop, `shift_ecalc`? Also does this output look right for the initial config? Initial Energy: -132.182667634 Simulation Start! ('Culmaltive Energy: ', -1.0918605873481917) ('Final Energy: ', -1.0918605873458798) I imagine I edit the "dimension" in your config generation script to run longer simulations?
My reason is the boss, who won't waste man-hours needed to to convert / rewrite whole project for no reason or benefit.
very cool! assuming that a C version would be not as fast as Fortran but pretty close?
Would this mostly be focused on out-of-the-box python or the larger ecosystem?
The dimension parameter controls the size of the crystal lattice that is created. Increasing the dimension creates a larger lattice which means more particles and thus longer simulation time. You can do a "wc -l configuration.dat" or the equivalent on your OS to see how many particles there are. The shift_ecalc is the dominant part of the simulation in terms of time. So optimizing that usually gives you the best results. That and perhaps the trial move generation (the creation of the disp vector) are the parts that are done thousands of times. And yes the -132.18266763384776 should be the starting energy for 64 particles. The final energy can vary because of the nature of the calculation since it is based on a random number generator. 
&gt; And by the way I think this is partially why these days often NoSQL data storage works just fine — especially for very large scale websites, the overhead of a relational database just isn't worth it, and you're putting a lot of what used to happen in the database in the application code instead. Putting things in application code that should be done by the database is exactly the mistake people make when they fail to understand the essence of SQL, that it is set-based. If someone can't get through Celko on this (edit: not saying you didn't, just that yes, it's a lot to take in but the effort just to understand a *little* better is very much worth it) then I'd be concerned they need to try harder before they get access to the database. The classic design mistake that remains ubiquitous today is known as [row-by-agnosizing-row](https://www.simple-talk.com/sql/t-sql-programming/rbar--row-by-agonizing-row/) programming, and it is the direct result of the disdain and impatience so many have for SQL and its appropriate use.
&gt;This should be a HUGE warning flag. It is and that is why I'm trying to know what to expect since replacing our db is not a short term option. Thanks for the link! 
Is there any easy way to know whether the calculation was accurate? I want to check that I haven't introduced bugs.
No trace of python on my PC. This is anacondas pythonw.exe that's crashing when the installer tries to run it
I've been meaning to create a logo and some cheeky text to play off of that
/r/learnpython man. read the sticky
I would suggest MATLAB - the psy toolkit there is better than the python tools here (Ive used both)
You could mention this is only for OS/X.
This is a cool idea, I think we need more of this. One thing I have a hard time finding in books/tutorials is bridging that gap between knowing how to write code and coding like a professional. I'm talking about organizing thoughts, designing the architecture that will efficiently solve the problem, designing good tests, working with version control with other people, contributing to projects properly. A guide with some better insight into a professional programmer's inner workings would be great, like how they actually approach problems and utilize tools in a real working environment.
Mentioned here: https://github.com/yask123/redditwallpapers#requirements Also, I am working on it to make it compatible with Linux and Windows too.
https://automatetheboringstuff.com/ 
yeah I'm aware of this, it's on my list.
I would hope OP isn't planning on focusing on 2 in any way more than transitioning from it to 3. Otherwise, this book may have a short expected lifespan.
Thought so, was also just generally curious - nearest neighbor calculations pop up everywhere :) thanks for the cool post. 
anybody?
&gt; Python 3.5 definitely doesn't give much of a performance boost from Python 2.7 I thought Python 3.5 was slower than Python 2.7...
That probably is because ints and longs were unified.
Sounds like a job for Jython.
&gt;Hofstadter's Law: &gt;It always takes longer than you expect, even when you take into account Hofstadter's Law. &gt;— Douglas Hofstadter I like that, thanks.
I can probably give you a pull request for Gnome3 support this weekend.
I do agree. As I written in response to other comments - I do intend to write on this also, but I'm trying to start from more practical parts of the required skill set.
Ha. I'm fully aware of that, but eventually they will need to move forward if for no other reason than all of the talent will be using Python 3+. 
Yeah there are. Compatibility is the main one (a proper JIT keeping C extension compatibility is *hard*; [see what JRuby+Truffle needed to do to get it to work](http://chrisseaton.com/rubytruffle/cext/)), but there's also * Most core contributors don't really care; CPython "shouldn't" be running performance bottlenecked code so there's not as much of a need to meet that use-case. * PyPy exists and works really well. * It's a lot of work, and most of the people contributing to CPython aren't JIT compiler experts. * CPython is meant to be really simple; a heavily optimizing compiler makes that hard. * CPython code is typically written such that JIT compilation of idiomatic code is hard.
Thank you! I must have missed this class. 
You might wish to post in /r/learnpython - this post will likely be removed by a moderator as homework style questions are not supposed to be submitted here.
You said since the release of Python 2. That was many years ago. Many performance tweaks from the time of Python 2.x starting.
Well, I didn't get pi_fm_rds had already implemented stdin ! So, in this case I think the #3 option would be easier, like : sox playlist.m3u -t wav - | sudo pi_fm_rds -audio - where playlist.m3u is the random playlist you have generated before.
Thank you. I am very new at this so I was hoping you could provide a quick step by step as to how this would work? For example do I need to save the website page and then parse it or can I parse it directly from online? I'm using bs4 on pycharm.
Fortunately for now in this industry, there are enough options that talent generally goes where they will be happiest working, money is pretty much everywhere 
Yes, any feature can be abused. But there are many good use cases for it. You got: a, *b, c = get_data() And: [*foo, *bar, "yeah", "cool"] And of course: do(*foo, *bar) Not to mention that all of these work with any iterables. So this: x, *y, z = open('file') Works. Also: x, y, *trash = get_data() Is pretty useful if get_data() return a non sliceable iterable. You don't have to create an intermediary list. It plays very well with the itertools module. It's also useful if you expect your iterable to may have only one item: &gt;&gt;&gt; x, y = range(1)[:2] Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; ValueError: not enough values to unpack (expected 2, got 1) not enough values to unpack (expected 2, got 1) &gt;&gt;&gt; x, *y = range(1) &gt;&gt;&gt; y [] 
/r/learnprogramming is also a good place to go. When you get there, highlight your code and click the button that looks like this &lt;&gt; . That will format it so that it's readable. 
Thanks for all of the info! Very helpful stuff... We technically use our own file format so I'll have to create a package around it no matter what I do. Its mainly a manipulation tool for a specific geometry that is parametrically prescribed (B-splines, ellipses, etc)... I guess having the ability to loft would be a bonus that im guessing PythonOcc would be better for. Again, thanks for your information!
Cython results: $ python mc_main.py -t 8 1000 Initial Energy: -3319.13281342 Simulation Start! ('Culmaltive Energy: ', -621.4429293261129) ('Final Energy: ', -621.4429293285303) Total Time: 622.355845213 Also on an i7, vs ~1000 seconds from your Fortran code. However, I cheated :p. The result above uses multi-threading. It's a dumb cheat too, since I've just stuck a `prange` on the inner loop. I couldn't be bothered doing random numbers in C or C++ to release the GIL on the other loop. I'm not sure what the multi-threading story is with numba. I'm waiting for the single-thread numbers. In the meantime, here's the code: https://gist.github.com/honnibal/8432f362a008901e732c It's not so nice --- I think your Fortran code is nicer. Actually I probably prefer your Fortran implementation to the Python one...And I imagine it was much quicker to write. Edit: Single-thread performance: $ python mc_main.py -t 1 (1000, 3) 1000 Initial Energy: -3319.13281342 Simulation Start! ('Culmaltive Energy: ', -621.4429293261177) ('Final Energy: ', -621.4429293285303) Total Time: 1704.56570697 So, 20 or 30% slower than numba, I think? I haven't benched vs. numba myself yet. There's ample room for tweaks that could close the gap, especially the compiler settings.
&gt; [*foo, *bar, "yeah", "cool"] That's not allowed because it's ambiguous. &gt; do(*foo, *bar) Wasn't that already allowed? &gt; It plays very well with the itertools module. I can see that. I hadn't thought of that. &gt; It's also useful if you expect your iterable to may have only one item: That makes sense.
Python has pbkdf2 built in, which is about as good &gt;&gt;&gt; import hashlib, binascii &gt;&gt;&gt; dk = hashlib.pbkdf2_hmac('sha256', b'password', b'salt', 100000) &gt;&gt;&gt; binascii.hexlify(dk) b'0394a2ede332c9a13eb82e9b24631604c31df978b4e2f0fbd2c549944f9d79a5'
Tell that to all the companies using FORTRAN 77 
My bad, thabks
Yes indentation matters in Python. It is considered a 'significant whitespace' language as opposed to C or Java where indentation doesn't matter. Put four spaces before `self.song.scrub_by(flipValue)` so it lines up with the `if` and `elif` lines.
I had forgotten about that repo. Needs to do some house cleanup. Here is some of the latest prototype (https://github.com/web2py/web3py). But we are still designing. Not everybody agree on what should be in web3py. Anyway, there is some good stuff in there. 
Of course it is allowed. Since 3.5 :) &gt;&gt;&gt; [*range(10), *range(10), "yeah", "cool"] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'yeah', 'cool'] As for do(\*foo, \*bar), do(\*foo) was allowed, but do(\*foo, \*bar) is 3.5 too. 
I realize this somewhat defeats the purpose of this library, but it would be interesting to see this optimized with numba, in comparison with numpy.
If working with virtualenv directly, I'll generally just create a .venv dir directly in the root of my project. That will /always/ give you the context you're after. I've also been a relatively heavy user of virtualenvwrapper, which allows you to name your venvs (I generally just use the project name). 
Have you tried running some of these functions with numba? Would be interesting to see if could be a suitable replacement.
You can get that right now at a few major US investment banks (who are on python 2.7)
With virtualenvwrapper, assuming you haven't installed environments in multiple paths, you should be able to list them with lsvirtualenv, like Unix ls but for your envs. You could use verbose names to remind yourself what they're for. Now, annotations or a javadoc-like thing for envs would be interesting.
All abstractions are tradeoffs. Orm frameworks can be great but don't scale well for things like data warehousing and complex ETL logic. I think understanding the tradeoffs and deciding what's best for the use case is the important part. I've both loved and hated SqlAlchemy but hats off to building and maintaining it! You guys also have really good documentation.
While django has you structure your program itself, I've found whenever I want to do something semi complicate I end up fighting it - the internals are not put together well. With Python and django I find increasingly when I look at the implementation it is like something I might have written, when I want something better.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Does this still put pandas dataframes into memory on one computer, or does it somehow do it in a distributed way?
Thanks. Commenting to save for later. After learning the basics, do you think a tutorial like this to develop a website would be the next best step or just trying to write a lot of basic programs? Looking into Python for the purpose of BI analytics
The problem with pushing bcrypt is that the message people hear is not "bcrypt is a good default choice if you don't know much and just want something that works, but other functions work well too". The message people hear -- regardless of how it's presented -- is "oh, even if you use PBKDF2 with salts and 100,000 iterations it's just as bad as using unsalted MD5, because that dude on the internet said bcrypt was the only safe hash ever". Plus there's competition in the memory-hardness space now too, which may end up making all the "just use bcrypt" posts disastrously obsolete in a few years.
SHA was designed for speed. Specialized hardware can calculate an obscene number of SHA hashes per second. Ideal password hashing schemes are designed to be slow and to be difficult to implement in hardware. 
http://codahale.com/how-to-safely-store-a-password/
Just a question, I can edit the CSS using firefox Web developer, but how do you render the page with new css code ?
"Pushing" bcrypt, e.g. writing a blog explaining how to use it
I really have noticed there's a growing trend of "the idiots were't using bcrypt" in comment threads under hacking stories. We're not looking to promote brand loyalty here. Good password security is always a rolling mark at the best of times. Beyond don't use an unsalted, one-way hash etc etc people should understand that what was fine in 2000 doesn't stand up in 2016 and you've got to keep improving. The idea of not being married to one hashing library for all time *ought* to be absolutely second nature to anyone using bcrypt.
Exactly. The idea is only attackers need to run the algorithm on billions of possible passwords so choosing one designed to be efficient, parallelisable and fast just benefits attackers.
There are tons of blogposts about it already, so yeah, they are pushing it.
I haven't so long as the tables were defined correctly in the first place.
Thanks buddy, I did something similar, I used @classmethod. Works beautifully now.
No, it's a confusion of the background and foreground issue. The font is very thin (raleway isn't really meant for small document text, but rather for headers / large text), but the main problem is that your background image is very complicated and obscures the lines of the text. On top of that, you've reduced the contrast of the foreground and background by making them both dark. My change makes the background white, meaning the contrast between the elements is heightened. But it retains a transparency which allows the background to bleed through.
&gt; The idea of not being married to one hashing library for all time ought to be absolutely second nature to anyone using bcrypt. And yet... the people who are still using unsalted MD5 are doing it because once upon a time they read an article and came away with a message against storing passwords in plaintext, and simply believed that whatever was recommended in that article must be "safe".
Thanks! I'll look into writing a similar article for this algorithm too. 
Why would you make a static website with Flask? Teaching people to use an inappropriate tool isn't very helpful, it just gives beginners a screwy mental model. This is a tidy tutorial, but at it's conclusion the student will have built an overcomplicated system that they don't understand.
Thanks I'll take a look. My goal is deference to established standards in this area to avoid becoming a guinea pig, while also not being complacent in the face of changes over time. 
You need to learn the basics, yes, such as variables, datatypes, functions, loops, and conditionals. Write some trivial short programs to get familiar with the basics and then you should be very comfortable with getting started with Flask.
It's also worth mentioning that `bcrypt()` actually has a [maximum input length of 72 characters](https://en.wikipedia.org/wiki/Bcrypt#User_input), and most implementations will silently truncate anything past that. Alright for passwords (mostly), but not good in some situations. If you want something that has no such limits I recommend using `bcrypt(sha256(input))`, as that will never truncate. For all you Django users out there check out the [BCryptSHA256PasswordHasher ](https://docs.djangoproject.com/en/1.9/topics/auth/passwords/#s-using-bcrypt-with-django)
I really like the concept of exploiting x86 architecture to get an edge over GPUs and FPGAs. I'll have to keep an eye on argon2.
I really like scrypt's design, and it can be optimized for a specific x86 deployment. argon2 is also neat as it has a good premise for gaining an edge over FGPAs and GPUs.
Correction. Pbkdf2 + sha256
It's a pretty trivial task, there is no "best" language, but there definitely is a worst language, and that is C
What? So... def func(): return [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'yeah', 'cool'] *a, *b, yeah, cool = func() print(a) &gt;&gt;&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] that works? I have no problem with: call_other_func(*a, *b, yeah, cool) that's fairly boring and seemingly obvious. I need to get on the Python 3 train...I've solved the unicode problem (when I test my code in Python 3). Now I just need to solve the nobody I work with wants to use unicode problem.
Update: Added support for Windows and Linux too :) Please test.
Ok testing. But if you crash my laptop, you owe me a beer.
redditwallpapers -sr EarthPorn -sort get_hot -count 40 -t 10 EarthPorn get_hot 10 40 0 Traceback (most recent call last): File "/usr/local/bin/redditwallpapers", line 102, in &lt;module&gt; main() File "/usr/local/bin/redditwallpapers", line 80, in main with Image.open(imagePath) as im: File "/usr/lib/python2.7/dist-packages/PIL/Image.py", line 528, in __getattr__ raise AttributeError(name) AttributeError: __exit__ 
I'm not sure. The json importer made me throwup a little. This just scares me. Both are so beautifully and elegantly evil.
I would only use that hashing scheme if there is a requirement to use NIST approved algorithms. Keep in mind, 3DES and SHA-1 are still NIST approved algorithms scrypt, bcrypt, and argon2 defeat FPGA and GPU attacks by being memory-hard. pbkdf2+sha256 can still be heavily parallelized in hardware, and this is the main reason why pbkdf2 + sha256 doesn't hold up in cracking competitions.
&gt; The mode. There are two valid modes: "localhost" and "public". &gt; They are aptly named --- "localhost" means run the server on &gt; 127.0.0.1. Therefore, the server is only visible on the machine &gt; on which it is running. "public" means run the server on 0.0.0.0 &gt; --- make it visible to anything that can see the machine on which &gt; it is running. weird. now I don't trust the author
Well, talking about languages with more than 1% adoption rate, hahah. Assembly is another valid answer
On it, can you please create an issue here: https://github.com/yask123/RedditWallpapers/issues Thanks for testing and reporting error :)
Nice theoretical idea. For now, too much "Restrictions on functions" to be used in practice.
What is Parse and why do I want an alternative to it?
just use autoenv
I am still getting the same errors. 
So, which restrictions would you like to see lifted? 
This book is going to be self published. On being an editor - all editors will have access to every chapter as it is being written, but ideally each editor wouldn't be tasked with working on more than 1 chapter. On honorarium - I don't intend to pay you in money, however, every editor will have full access to every material related to the book before and after it's being published. Also, there's a pool of consulting hours I'm willing to give in return for help.
Simple problem: write a regexp to parse out emojis from unicode string on every platform :)
Frankly, I have a pool of consulting hours I'm willing to give up in exchange, but that depends on how serious the contribution is going to be. Also, there's a lot of books for beginners and quite a few niche books. In other words it's like taking CS101 and Dark Magic of C classes, but never really having anything in between. I constantly get to see people who are beginner level programmers really but believe themselves to be masters of a certain niche, e.g. scraping. A well rounded developer is a rare sight.
If I can make 50% of Mike's magic happen I'll be happy.
Good to hear that :)
Oh man, this tempts me to add some chapters on real world tools for certain problem areas :)
Tokens can have a time [Expiration](https://tools.ietf.org/html/rfc7519#section-4.1.4). Revoking a token is [also possible](https://auth0.com/blog/2015/03/10/blacklist-json-web-token-api-keys/) if the infrastructure is setup for it. I guess that could require some communication between the authentication server and the web servers (although they would be the same server for traditional non-microservice architectures). It would also be a good idea to invalidate the token when you 'logoff' and provide the users with a way to revoke them. Basically it boils down to roll your own system but you have all the pieces.
How about [brainfuck, interpreted by Python, interpreted by C](https://github.com/Kopachris/pybrainfuck)? It's even extensible! :D
According to the release notes, Pip version 8 dropped support for Python 3.2. Why not use a more recent Python version?
Pyglet, or Vispy. Vispy is made for scientific visualizations, but has very nice OpenGL bindings.
I updated Python and it is working now. Thank you!
Hi, author here. To be honest, I don't know much about networking. The reason I wrote this is because I've found myself in need of a non-blocking TCP server on multiple different occasions, and I can't find another one that's easy to just run out of the box. I really love Tornado, but its implementation of a TCP server is very hard to just "run" if you don't want to learn a bunch of Tornado-specific dogma. Anyways, my point is, I don't know much about networking, so my project reflects that. I read as much as I could about 0.0.0.0 on [python.org](https://docs.python.org/2/howto/sockets.html) and [Wikipedia](https://en.wikipedia.org/wiki/0.0.0.0). Both of those pages seem to point to this idea: &gt;In the context of routing, 0.0.0.0 usually means the default route, i.e. the route which leads to "the rest of" the internet instead of somewhere on the local network. If you're not complaining about the 0.0.0.0 thing, then I guess this comment was a huge waste of time :( In terms of only allowing you to bind to localhost or 0.0.0.0: this is entirely changeable. I only finished the rudimentary version of this yesterday. There's already a [GitHub issue](https://github.com/gragas/simpletcp/issues/1) about this and it will probably be fixed/implemented soon.
Hi, I'm the author. In terms of only allowing you to bind to localhost or 0.0.0.0: this is entirely changeable. If you read [my other comment](https://www.reddit.com/r/Python/comments/4359j2/a_minimal_nonblocking_tcp_server_written_in_pure/czg9819), I explain why I chose this implementation. I realize now that there's little reason why this shouldn't be fixed. There's already a [GitHub issue](https://github.com/gragas/simpletcp/issues/1) about this and it will probably be fixed/implemented soon.
Okay. I will consider this. Since the project is open source under the Apache License, you can consider this, too :-). I'll try either do what you just mentioned or at least provide support for binding to any IP address by the end of today. Thank you for the feedback. It's greatly appreciated.
From what I've gathered, a free storage service that's rather easy to use. Was bought by facebook, who announced they'll shut it down in a year. They open-source the code, but that would still leave you at having to run the server.
The code is beautiful and well documented, thanks for sharing
Thanks &gt; Since the project is open source under the Apache License, you &gt; can consider this, too :-) I know that answer, I have given that answer thousands of times. I hate that answer. I maintain several large open-source projects and I'm not going to contribute to another one. Think hard before giving this answer.
Non that doesn't. But you can unpack INTO a list, tuple or dict [*foo, *bar] or (*foo, *bar) or {**foo, **bar}. It's very handy because it works with any iterable/mapping and mix well with positional args.
Hello fjarri ! You really should get in touch with Victor Stinner and Yury Selivanoc on the Python mailing lists. They are currently working on python optimizations which are currently creating a big debate among the core devs. And your work is actually in direct corelation with what's currently going on. Please step in ! Mailing lists: https://www.python.org/community/lists/ (actually a bit spread among various topics between the python-dev and python-idea lists so you got some catchup to do) Main topics : - FAT Python - Explicit variable capture list - PEP 509, 510 and 511 
I have created a distributed physical computing platform called Xideco https://goo.gl/OWRfVo . Feel free to borrow any applicable ideas.
*op==author* It's currently at version 0.9.3, and I'd really like some feedback before I bump it to 1.0.0 and pin myself to a stable API. 
In the same sprit, here is a script to alert you when songs are deleted / blocked / become private in your Youtube playlists: https://github.com/Lucas-C/youtube_playlist_watcher
Thanks, I had a brief look at these PEPs. 509 and 510 will certainly simplify mutation detection, and 511 is indeed very close to what I am doing. Talk about synchronicity :) I'll scan the maillist tomorrow. *Edit:* in fact, [fatoptimizer](https://fatoptimizer.readthedocs.org/en/latest/) seems to be doing the very same thing as `peval`. From the first glance, it seems that I should freeze my project and contribute to that instead. Oh well...
Arguably about as good, but it's [probably a better choice than bcrypt](http://www.unlimitednovelty.com/2012/03/dont-use-bcrypt.html), due to it having passed a much larger amount of scrutiny.
My favorite part of that feature is that you can prioritize the hashing functions, and Django will automatically upgrade a user's hash when they log in.
You wouldn't. Be it's a very simple code, nice to read, many comments, and a cool API. It's great to learn.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Yes, i'd love to play around with the combination, especially once numba supports precompilation.
Working on my first decently sized Python project now and I would love a book that did a good job of showing best-practices with regards to unittests. 
&gt; In a typical REST architecture the server does not keep any client state. The stateless approach of REST makes session cookies inappropriate from the security standpoint.
Honestly Perl is far more focussed on text manipulation than python. And there's a reason that PCRE (perl-compatible regular expressions) are the standard. Another alternative is AWK if all you need is text record processing and nothing else. Python does give you everything you need, but it doesn't have nearly as many shortcuts to deal with text and regexes as perl does. Saying all that parsing arbitrary format user input is a impossible task in any language.
This is fascinating to read, but I got the feeling that there was nothing most than cutting and pasting templates going on. It needs more hook to get someone to invest their time in doing it.
&gt; on every platform Does it change on every platform?
[removed]
Yes they did. You can install your own parse server. 
It is true that at least I couldn't find anything that suited me. That's why I started this project. 
I couldn't agree more!
Yes there are. Main one being they don't have a lot of money (http://pyfound.blogspot.fr/2012/01/psf-grants-over-37000-to-python.html) and a lot of contributors are just doing it pro bono, second one being that other JVM are improved over several years by big players hiring all the specialists. With money.
oh no, I have a perfectly functioning application environment that has been using Parse for over 3 years (long before Kinto coming along) and now I have to rewrite everything if I want it to continue to function? As an app developer with a small workforce having to change backend solutions is a huge pain in the ass, so yes, I'm very bummed about parse closing down. They provided a very useful tool and it's a shame facebook is closing them up. 
Well, depending on which python built you run the regexp will be different. Python with wide unicode support will work mostly as expected. Python with narrow unicode support will crash with the obvious solution.
I've added the fabric module which conveniently encapsulates the HTML5 canvas, and added a Pong example to www.transcrypt.org
I [learned](http://stackoverflow.com/a/13095555/564755) something new today. Thanks!
thanks /u/melizeche do you have any clue how much do they charge on an hourly basis for development?
so now you'll have to pay a person rather than parse, or rather, facebook?
Will do! Thanks!
Google (and Stack Overflow, of course. Sometimes just Google with site:stackoverflow.com) is still my friend, even after 5 years of development. Then again, *most* of what I look for is documentation on X. It's not the how to do what I want to do, it's the incantations I need.
ELI12 Jupyter please. I'm just some stupid Python newbie who's happy to use IntelliJ IDEA for his Python work. 
Or, if IPFS catches on as a content addressable global CDN, the virtual machine would just be shared between all sites which use it, and the browser would cache it effectively. 
If you just want to do what you descibed, then maybe its easier to go with Excel, if that is what you use for sheets. I am on tablet but search for PowerQuery for excel, it lets you login to websites and copy tables directly in excel, of course it updates data when you work in sheet. The route you want in python, use only if website you are going to use has some api backend, I would not go for parsing html, every time they update site you will have to update your code.
Could someone clarify what they mean by, "A side-effect of designing a JIT interface for CPython is it also allows for designing a JIT framework for Python." I don't exactly follow.
So to the best of my knowledge, CoreCLR is the engine behind Microsoft's .NET framework. Essentially its what handles things like compiling code (C# or other languages) into machine code, garbage collection, etc. There are many different python 'implementations' you can use. When most people talk about using python they mean CPython. Which is the standard python interpreter that you get via python.org, or built into your OS by default, or with Anaconda. However, there are other python implementations (PyPy, Jython, Brython, etc...) that act as alternate interpreters for the python language (or sometimes some sub or superset of the language) This appears to be an implementation created by Microsoft which uses the .NET engine in the background to handle the code compilation (interpretation) to machine code. They are seeking to add JIT functionality to python which stands for Just In Time. This means, for example, that rather than the entire source code file being read by the interpreter and compiled to byte code all at once, sections of the code can be flagged to only compile when they are needed (ie. at execution time) I'm sure I've missed a few of the finer points, however.
[emcee](http://dan.iel.fm/emcee/current/): the MCMC hammer. 
I'm the one who started this at PyCon last year. The title and your description are a little bit off because this isn't actually a new Python runtime. If you want JITed Python on .NET that's what IronPython is for. Instead this is a JIT for CPython 3.x. There's some minor changes to CPython to enable JITs to plug in and compile byte code to native code. This JIT translates the Python bytecode to .NET's IL and then has the JIT compile that down to native code. The CLR's JIT is compiled as a standalone JIT which we statically link into our Python JIT library. So this is completely standalone and doesn't call into any .NET implementation. It also doesn't use any of the .NET VMs other facilities - e.g. there's no .NET string, object, or dictionary implementations floating around. 
Yes. A fantastic language imho, but I wouldn't say it's *better* at string processing.
I read about this earlier today, do you think this is a viable option to Cython, in regard to speed? I'm working on a small encryption programme and really need it to speed it up a notch. Anyone have any experince using etc.? I'm not that good with C/C++ (it's on my todolist okay) so i don't really know what to look for.
Yep, although not yet nearly as good, and we don't have to develop any actual native code generators on our own. Edit: it's also similar to Unladen Swallow in what it's trying to do
Look at that, that is interesting. I noticed you used the \_\_new\_\_ constructor to instantiate the canvas object in the Game class. Why?
Pandas dataframes only run in a single machine. Spark Dataframes can be distributed.
Reddit hug of death site is foobar
True, but as a platform it was perfect for what I needed. A) Free, B) Not having to spend a ton of hours setting up (and purchasing) a personal server, and C) not risking having outages. My statement still stands, it makes me sad. But no, it isn't the end of the world. 
So in terms of compatibility - would (for example) a wheel with the PEP425 tag 'cp35-cp35m-win64' work with this interpreter?
Yeah, trying to write anything substantial gets convoluted really quick. I write the program in pycharm, and then import it into the jupyter for interactive visualization.
This is really cool. I have a javascript map plotting app that I dread maintaining. I'll look into porting it to transcrypt and maybe contribute to this project if it is needed.
if you are doing webdev or games, it is completely useless. If you are doing scientific programming, it is quite handy.
You can use the [subprocess](https://docs.python.org/2/library/subprocess.html) module to run any commands you can run from the Windows shell. You can then use the output of those commands however you like.
Like "pigeon". Dino wanted a name that had Py and JIT in it and that's what he came up with.
Arethere any benchmarks available? Can it run any if the ones from PyPy https://bitbucket.org/pypy/benchmarks?
Can it be used to create programs that run in Node? It would be really cool to use the clean syntax of Python to run Async program in node. 
As someone suggested try it out. It is a little different that regulate IDE programming. Some compare it to MatLab and similar apps I the way code and output is handled. If you are new to programming, realize that this is a conceptually a different way to develop code. However it has great potential for a number of disciplines from research to education. 
African or European? 
Never paid parse, always used the free plan. 
I always thought that Spyder was the closest thing to MATLAB 
I did use VENV, but did not know about freeze. I'll look into that. Thanks!
I'm a bit confused. Is this another Python implementation like IronPython?
Is that a good thing or a bad thing?
From what I gather PySide is kinda fiddly with Nuitka. If you have the option of using PyQt then that should be a lot easier.
Great!
No problem, thanks for sharing your work!
Thanks for clarifying! 
No. IronPython was a complete re-implementation of Python. Pyjion is trying to add a JIT to CPython.
OK i'll do that...how do I uninstall my solr server now though lol. I'm on mac and sorta new to this stuff
The documentation is a good source of info: http://django-haystack.readthedocs.org/en/latest/installing_search_engines.html.
I personally think it's hilarious.
You can do async in native python if that's the only reason to want to do that.
&gt; It appears that Scrapy fulfills the same intent as Livescrape, without looking too closely at either. Scrapy is Python 2 only &gt;:(
Very nice!
Could you perhaps paste in the stack trace? Also, you shouldn't have to put the folder in the Python system folder. Once you add Python to your system path that should take care of things.
Ahh ok, no that makes sense. Thanks!
Upon entering hello.py into cmd after opening python, I get Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; NameError: name 'hello' is not defined I hope this is what you meant by the stack trace.
Probably hard-won experience with other languages :) Guido's in particular. The standard library functions seem to have just the right functionality, and neither too much nor too little of it. There are even some overlooked gems (e.g. the `translate` function). It's not just the library, though, but also the way strings interact with the rest of the language. You can slice them, iterate over their characters, `join` sequences of them, repeat them, etc. in very natural ways that are both concise and readable. Concision (expressive power) and readability are usually tradeoffs, and it's easy to cite languages that have one but not the other, so the combination is exceptionally cool.
Standard FORTRAN surely is worse, it hardly knows about strings :)
[Pywin32](http://sourceforge.net/projects/pywin32/) might be what you want. I've used it and my experience was that it's not very user-friendly but very full-featured. The win32api module in pywin32 is a thin wrapper around (all of?) Microsoft's Win32 C++ API. [This](http://docs.activestate.com/activepython/3.4/pywin32/win32api__GetDiskFreeSpace_meth.html) looks useful.
Thanks for the comment. I already removed this requirement.
/r/learnpython, please
Not for long, there will most likely be support for Python 3 in the 1.1 release which is scheduled for the end of February. Source - https://github.com/scrapy/scrapy/issues/263
Wow, it's finally happening. Exciting!! I know they had a lot of tangled dependencies, I have to salute them on their work.
That's been my experience as well. Very similar feel between Spyder and MATLAB 
There are many uses of the Jupyter notebook. The three that come to mind are: 1. **Demonstrating code:** Notebooks are a great way to mix explanation of code (through markdown cells) with the code itself. It makes it easy to walk the reader through the your own thinking. For example, you can define helper function and explain what they do, then show the ultimate result using those functions, and finally explain the significance of the results. This is especially useful for education or for scientific computing. In both cases, it's important that the reader fully understand the code and its purpose. 2. **Development:** Many python programmers have a workflow that makes heavy use of the interactive prompt. For example, when writing a function, you might open up the prompt, load example arguments as global variables, and then try things until you get the result you want. The problems with this strategy are that it lacks organization and that you end up typing the same thing again and again. The notebook gives you essentially an interactive prompt, but also allows you to save blocks of code that you might want to execute again. 3. **Plotting:** The main way I use notebooks... I often create a pandas DataFrame with non-notebook code and pickle it for future use. Then I open up the notebook and can inspect the DataFrame, alter it as I see fit, and plot it inline. That is, I can write a plotting line like `seaboarn.factorplot('weight', 'diabetes', hue='economic_class') and see the results right there, tweaking the plot as I desire. Later, I can then completely log out of my computer and easily return to my previous state by loading and running the notebook. It's not a replacement for an IDE or a text editor. I view it as a glorified interactive prompt, and a communication tool.
I think the best you can do to use some version control system like git and create a repository on github or bitbucket.
You're most welcome. Especially on the front of encapsulating good JS libraries there's quite some work to be done!
Any plans to integrate with the untappd api? 
Certainly
I do not yet have any Node experience yet, so I couldn't tell. If Node itself has async capabilities it would be a matter of encapsulating them. Maybe they even can be used as is. However, running on a server CPython (or e.g. Stackless) would probably to a better job.
Better yet, post it as a gist so that people con comment on individual lines. 
&amp;gt; When they're done, they're done and all data given to them is gone. This isn't really accurate. The data given to them is still referenced by the caller, unless constants are used. Even when constants are used, if any caching (i.e. memoization) is done, the values still aren't "gone" after the function is executed. &amp;gt; Functions are born when they are called. For the most part, functions are born when the file they reside in is evaluated (i.e. at import time), not when they are called. &amp;gt; Generators return multiple values in a specific order. They keep the data you give them until they return the last thing you wanted of them, then they die and take all that data with them. Generators are only born when you specifically demand it. Generators implement the generator protocol (https://docs.python.org/2/reference/expressions.html#generator-iterator-methods). I'm not sure what you mean by "order" as the order is entirely dependent on the implementation. Remember, there are both generator expressions as well as generator objects. So, your definition here is likely a little /too/ simple :)
Pymcu
Does it make source maps? EDIT: &gt; * No operator overloading. May be added for [] and (), but low priority Really? Thats like one of the nicest things with python. &gt; * No iterator, generator, xrange stuff. Maybe in the future if JS becomes better at that kind of things Another pillar of python. Not sure if i would want to use python without two core features whose absence make python so much less useful.
[removed]
I'm looking to choose a programming language for processing basically text from a text field that could basically be a city state and zip code and area I want to be able to take either numbers or words and then pull up regional data based off of those numbers or words with a zip codes States area codes etc I want users to be able to put in basically any type of basic geographical information and whether that's a ZIP code or state or city I don't care I just want to be able to process the text strings so I can pull up the relevant geographical data that will correspond to either the zip code the area code the state or their city I have been looking at other programming languages and I really like how python is the easiest to read and so it seems like it will be easier for programmers and myself to maintain the code I guess python would be great for this
Maybe try entering `python hello.py` Post any further questions to r/learnpython, it's very active and helpful. This sub is more for programmers who work with Python to discuss Python stuff. Also, use Python 3 goddammit. If Google is telling you to use 2 they can get bent.
There's a ongoing Kickstarter for porting micropython to esp8266. Currently micropython works on pyboard perfectly but esp8266 port requires unpythonic code and works half the performance it should be. Kickstarter says 6 months later port will finish and opensource the code and if you Kickstart them, you can try beta releases. 
You sir deserve a medal of helpfulness. 
Ugh. Talks about COM
Interesting, thanks for sharing. As this problem screams GPU at the top of its voice I would be really interested in seeing this comparison with Theano (an algebra-aware CPU/GPU compiler for python) and CUDA :-)
Holy shit Microsoft. Looks at project's license and back at Microsoft. What happened? When did they start releasing stuff as free software? I thought Microsoft was supposed to be evil.
Have thought about it for pulling down beer ratings. Having a way to auto check in would be cool too. 
&gt; deployment strategies This cannot be understated. I'm increasingly inclined to believe that ease of deployment is the path to a successful company, not by itself but by being an enabler for a bunch of other important practices: short cycles of user feedback, fast response to bugs, being able to experiment with features, etc. Having worked both for a company that only released code twice a year and for a company that releases every other week, there's no way I'd want to go back to the former.
You can always Ctrl+Shift+C select element and edit its CSS rules on the side, it will reload automatically.
Ok. hello world is text, and you need it to be in quotes print "Hello, world" 
micropython
Would you consider Raspberry Pi a microcontroller? I went to a talk recently that was promoting a new IO library being developed called GPIO Zero which was nice. 
I suppose it depends on how you are using MATLAB, but in general the Jupyter Notebook is more similar to the popular notebook interface to MATLAB. I prefer Jupyter, though; MATLAB does some weird things.
Github Pages sounds like a good option. It's my go to service for static sites.
We are using Pelican which has a plugin for ipython notebook, it's pretty seamless, for example: this article https://blog.wearewizards.io/the-hacker-news-effect-examined is mainly a notebook that i embed in my article by doing `{% notebook ga-stats.ipynb %}` (see https://github.com/WeAreWizards/blog/blob/master/content/articles/hacker-news-effect-examined.md for actual markdown file)
Nice work! Bootstrap is great. You might want to try splitting your common header/footer code into a separate template and use %include in each view instead of repeating your header, navbar etc.
Still, Node is more performatic regarding async code. Everything is async by default, whereas Python is synchronous by default. So, it would be desirable to write node programs whereas taking advantage of all the npm packages.
I can't imagine transpiled code benefiting from that performance in practice, though. In any case, neither node nor python are appropriate for performance critical applications, just "performance decent" ones. 
MicroPython on the ESP8266 https://www.kickstarter.com/projects/214379695/micropython-on-the-esp8266-beautifully-easy-iot
answered
Thank-you for the heads up.
Ok, so CoreCLR is the portable, open-source .NET code, so this could be run off-Windows. And it's CPython 3.x, so any Linux-based Python 3.x code would generally run unaltered in this? (With exceptions for C-based libraries.) I think this is really cool, but I'm not quite sure. /u/dinov , for someone who uses Python on Linux and has only poked at IronPython a time or two, what does Pyjion do for me? I'm thinking I can more easily run my "Linux" Python code under Windows now, right? (And, from MS' point of view I can run Pyjion &amp; CoreCLR on non-Windows platforms.)
As I was adding new pages I was wondering if there was a way to do that. Was getting annoying. 
&gt; I tried Wordpress. Wordpress is terrible and bloated, no reason to install something that heavy for some static HTML. [Nikola](https://getnikola.com/) is a static site generator and can publish anywhere that can just host static sites (Including GitHub Pages). It does require a bit of setup, but is the most full featured. Github itself will automatically convert ipynb pages for viewing, but I've found it a bit slow. https://github.com/DadAtH-me/CTP/blob/gh-pages/01.%20Introduction%20-%20System%20Modeling.ipynb nbviewer on jupyter's website will also render any ipynb that you can throw at it: http://nbviewer.jupyter.org/github/DadAtH-me/CTP/blob/gh-pages/01.%20Introduction%20-%20System%20Modeling.ipynb
I love synapse snap modules. https://www.sparkfun.com/tutorials/367
Well normally we do this on our local computer cluster which has 20 processors per node. :) These problems parallelize quite nicely. 
pyboard, you can program the chip directly
&gt; CPython code is typically written such that JIT compilation of idiomatic code is hard. Would it be possible for you to expand on this or give me a few links ? I fail to see why the idiomatic aspect of the code makes it hard to JIT-compile ?
Signatures encrypt identity, not payload. Technically, this is still encryption.
Recently I saw something about this. Link: https://automatetheboringstuff.com/chapter11/
Putting aside the discussion about using regex to parse HTML (see robertrocky's reply), the fix for your regex pattern is to use "MTG_DAYTIME\$\d" rather than your "MTG_DAYTIME$1" . Notice the '\' before the '$' (which is a regex special character) and the '\d' to catch any number rather than just the number '1'.
Urllib2 may help here.
Yes, the idea is that it'll be cross platform and we're only targetting 3.x, and that's one of the reasons to use CoreCLR's JIT for doing the compilation. Our hope is that Pyjion will basically just make your code run faster and you won't see any other differences. It won't change anything about making your code cross platform - you'll have all the same libraries available (both normal Python libraries and C extension libraries), but pure Python code should be faster. But otherwise your Python programs should be exactly the same.
Hmm not sure there is a legit python certificate but I'd definitely be interested in getting one. I recently got one in Linux and it definitely helped with the learning process!
IMHO, the best certification (and one that would stand out more to me personally when reviewing candidates) is being a contributor to the Python standard library, or one of the more popular libraries (i.e. Django, requests, Open Stack etc etc). No need to be a committer, but a contributor over time whos contributions are easily searched would stand head and shoulders taller than taller than someone who had an online certification that I'd likely never heard of.
I think I can give this a shot. Thanks. If I run into trouble I'll probably respond to you again with more info.
Because every simplification is wrong. Every. Single. One. If you don't understand that, and why it isn't that big a deal, then you should never ever ever try and teach. Ever.
No your github profile is your cert 
I wonder what's your view going to narrow on in the future
Why do we use anything other than carrier pigeons with USB sticks?!?
I just wanted to thank you for your website. It provides a very lucid account of the basics that seem overwhelming to beginners. I only wish it existed 15 years ago so that I wouldn't have been overwhelmed then.
Thank you, I really appreciate the positive feedback. It's comments like this that keep me updating the site every day and figuring out what I can create to make it even better. What topics would you like to see in the future?
SQLite is awesome for development, although is not well suited for production applications with more than a single user. I've just started a page for that one with some initial resources. More to come on this one soon: https://www.fullstackpython.com/sqlite.html
You did not learn a single thing in school that wasnt what you now insist is misinformation.
I really like Spyder. It is kind of ugly, but, it is great for development as it allows you to test small sections of code without running your entire script. Great for data analysis and science
MySQL is the PHP of database world: http://grimoire.ca/mysql/choose-something-else
If you're having the same issue on two OSs, it makes me think its a connectivity issue of some sort. Did you happen to import settings from one OS version of PyCharm to the other OS version of PyCharm? There's a HTTP Proxy setting too in PyCharm. Make sure there's not incorrect information entered there. Can you use pip to install packages outside of PyCharm on either OS? 
Could you explain the meaning of `.notice()`?
Also might want to try posting in /r/compsci, /r/computerscience, /r/askcomputerscience, and /r/programming.
&gt; Why do you say so ? Kind of proves my point. &gt; No, I don't think, because even if it can benificate them in the sort term, it will hinder their usage in the long term, and it introduces unnecessary fog in a world where learners often feel lost and without visibility. The word is "benefit" and the thing about simplified explanations is that they are routinely superseded in the natural progression of the learning process. Look at the first response and your own for examples of *fog* that hinder learning. If someone has to google a term you used in your explanation, then you have missed your target. Memory semantics? Is that a general term or does it refer to something specific? Memoization? You guys think these are helpful for the people the OP is aimed at?
Manual entry on Add Beer screen. Initial setup sucks but after just small updates. 
Can you clarify what you mean by "good ones"? Some certifications such as RHCA or MCSA may open you doors for certain job offerings, and in case you haven't had prior industry experience you'll be at least a 2nd choice instead of no choice. There is no such thing in the Python world AFAIK. If you just want a consistent learning effort which is committed through a certificate, Coursera or EdX may do fine, but I have no idea if they have any relevance to the industry. Since they are rather cheap in price it may be worth checking them out.
The irony of your comment is making me smile. 
Forget about cython and try Numba. Its generally faster and easier than cython. Since it specializes in compiling numerical code only (as of now), it will also be faster than nuitka...around fortran fast in face. 
Hey! Django is definitely appropriate for this task. Especially if you're considering on doing credential checks to see if a particular file belongs to a user etc. Django has some nice authentication and login functionality out of the box. It's session framework is really rock solid too. I currently use it for all of my projects where there is a mobile front end. I simply pass JSON back and forth. Good luck :)
I don't think it is a good service done to people bullshitting them with idealistic beliefs about their high visibility through github repos or other artistic œuvres. There might be some companies hiring on that base but then that's mostly a self advertisement of those companies who like to be in the media. 
Hey man, I'm going to reply here discretely: &gt; &gt; "PostgreSQL is often viewed as more feature robust and stable when compared to MySQL, SQLServer and Oracle." Yes, for MySQL, but this statement is so obviously untrue (mssql and oracle) to a DBA that it sort of taints the rest of the post. I'd at least soften that statement a bit ... 
Django is certainly capable of something like this, but for what you're doing, something like Flask might be more appropriate.
and your /r/python comment history
Thanks for your help! Edit: ended up going with Flask. Will get to Django at some point.
Can you elaborate on why flask would be better?
Whether or not you use this package, [Pep 257](http://www.python.org/dev/peps/pep-0257/) is worth a read/refresher. 
It's far simpler. No need to learn a huge platform just to accept and handle GET requests. Django is awesome for large, content-focused websites. If you want a large, sophisticated web application, Django is pretty spectacular. If you want simple stuff, Flask is a great, simple tool that just gets out of your way.
Does it have to be a virus for your plot? If not, a very real thing you might be able to use is the P versus NP problem: https://en.wikipedia.org/wiki/P_versus_NP_problem Long story short; if true, then good bye encryption!
There is a quantum computer in my story :) Although it's in my [first book] (http://mybook.to/thefallofman). In the first book went way out there in terms of speculation, but in the sequel I'm trying to bring it back to a more real-world scenario (although still jumping the shark in some spots). The thing is that one of my characters, who will end up being able to log in to any computer, will have no knowledge of the super computer, he'll just stumble upon the exploit (which will be there because of the quantum computer). I think the easiest way to do this will be to implant a backdoor in the semiconductor chips. It's basically undetectable since its at machine code level. 
I'd recommend using BeautifulSoup, Here is a function that grabs a certain number of words from a given soup https://github.com/sdobz/avalarky.report/blob/61aa629e85af1fbc0912bf51528a2dd6dd5f9ac2/modules/everdown/parser.py#L89 soup.get_text(separator=u' ').strip() will probably do what you want
When I was learning python my current boss advised me to stay away from SQLite. It is really nice for early development since it just works out of the box with next to no setup time, but can be a bit of a hassle moving forward. I find that if you are doing something that will move to production its usually a good idea to go with your production database from the start.
&gt;That is what is meant by GET in an http request. No, what is meant by GET in an HTTP request is the fact that the browser/client sends over the characters GET over the TCP connection. (Other request types include POST, PUT, DELETE...)
So numba helped by what... a factor of 100? It's hard to even read your graph at 64 particles because it ran so fast.
Is it really hard to accept that people just want to show some basic skills instead of diving deep into the community and help out? Suppose you learn Hadoop, and now you can configure a cluster, send Hive QL commands, push files into HDFS, work with Mappers and Reducers and other basic things. This qualifies you to do some grunt work in that area for about 50$ per hour and that's what a certificate is for. The indirect benefit for the community is that the number of users grows, with the number of users, the lines of code and with the number of LOCs the systemic inertia. The system is less likely to erode quickly and getting replaced by the next wave of technologies but budget is spent for maintenance. Python is still in risk stumbling across its 2 vs 3 divide. This is not over yet. So people demanding visibility as professional Python 3 programmers shouldn't be dismissed just because they don't fix bugs in some arcane and not frequently used part of the CPython machinery.
&gt;&gt; Python is still in risk stumbling across its 2 vs 3 divide. This is not over yet. So people demanding visibility as professional Python 3 programmers shouldn't be dismissed just because they don't fix bugs in some arcane and not frequently used part of the CPython machinery. No, by all means they shouldn't be if there was such a cert for Python. However, what I'm saying is that, while having a cert demonstrates some basic knowledge of a programming language (which isn't something I would hire for anyways, it's more abstract problem solving), it /doesn't/ provide you with all of the side benefits of working on an OS project like Python (any flavor). What that demonstrates to me (and, of course, this is only my opinion), is: * A passion for the language, going above and beyond what most would consider doing. * The ability to get over a learning curve and stick with it. * The ability to work with an entirely distributed team. This will generally demonstrate one's ability to get out and just talk to people rather than staying in their own shell. As the scale of systems grows, this is an essential quality. You can't just learn /everything/ in a vaccuum when dealing with multiple distributed systems (of course, this all depends on where it is that you work) * The ability to take constructive criticism and apply it to their work. * The quality of their work is sufficient for the committers of a project to accept. The above are all qualities that I love in people I work with. While a cert tells you that an individual has abilities in a given field (to what degree varies wildly), it misses on all of the above. /That/ is why I say that a candidate who has contributed consistently to any large OS project over time stands head and shoulders above others. I /did/ also say contributing to other projects as well (and there are a /good/ number of heavily used libraries with large communities), not just Python itself.
Array math is one part, but the other part is the Fortran syntax is a little cleaner than C. C was designed with hardware programming in mind and so a lot of the design choices was targeted toward that. Fortran was designed for mathematical operations and scientific computations. So it's syntax was designed to simplify those operations. In the end you can get the same performance from both if you understand the languages really well so it is more user preference on which to use. 
I did something very similar and was able to pull it off. What you need is beautiful soup and ust the get_text() function to clean the text. On the other hand there is also html2text by Aaron Schwartz 
This program is too simple to be made into an OO program in python (without adding some extra complexity) some notes: Upper CamelCase like you're using is (typically) only used for class names in python. Everything else uses snake_case (no upper case at all) or ALL_UPPER_CASE for constants. You can use `.sample` to simplify your code *a lot* Also you're iterating a lot with an index variable 80% of the time this is wrong (and 19% of the time you should be using `enumerate` or `itertools.count` or something) `map` is somewhat advanced, and you could do something besides map, but definitely use a list comprehension or something. `list.extend` can be used to append multiple things to a list at once. #Define case for words if options[0][1] == 0: wordlist = list(map(str.lower, wordlist) if options[0][1] == 1: wordlist[i] = list(map(str.title, wordlist)) if options[0][1] == 2: wordlist[i] = list(map(str.upper, wordlist)) just use `random.sample` (well random.SystemRandom().sample for better security) #Select Words words = random.SystemRandom().sample(wordlist, options[3][1]) #Select Numbers for i in range(options[1][1]): words.append(str(random.SystemRandom().randint(0, 9))) here's a version using `list.extend` and a generator expression which is *too clever*, but more consistent maybe? #Select Numbers words.extend(str(random.SystemRandom().randint(0, 9)) for i in range(options[1][1])) `random.sample` and `list.extend` to the rescue! (also UPPER_CASE consts) #Select Specialchars CONVERT_CHARS = ['!', '@', '#', '$', '%', '^', '&amp;', '*', '(', ')'] words.extend(random.SystemRandom().sample(CONVERT_CHARS, options[2][1]) for some reason you switched back to the less-secure random variant. random.SystemRandom().shuffle(words) print(''.join(words)) It's mostly good, your style looks like a win32 C or PERL programmer not used native pythonisms yet. If it was me I would use [`click`](http://click.pocoo.org/5/) or [`docopt`](https://github.com/docopt/docopt) to handle the command line arguments.
dbrecht, just pick a random job offer and scan it for requested skills. The only thing that is added to the label of tools and techniques is "proven industry experience" ( which means you need to stick with what you've done in the past which can be deadly if the industry configuration changes to your disadvantage - it makes you fragile ) and occasionally the provision of a certfication, it is not "general problem solving", love of the language and various other soft skills everyone finds personally desirable. If you and many others perceive this as a major market inefficiency in the distribution of labor commodity to the industry, chances are that things will change and we will experience a gamification and exploitation of the open source domain with an abundance of junk projects and circle jerking, simulating the academic reputation system. However, in some sense a formal education is already some kind of simulation. It may also embody some of the practices you mention - in particular peer reviewed code - but it is clear cut, unemotional, non-parasitic and objective as good as it can be.
Modules you're looking for: requests lxml beautifulsoup4 newspaper 
SQLite is well suited for production applications that have quite a few users, since one user rarely needs the database lock all the time. The point where SQLite stops being good is when your single-server app gets crowded and you need a second server to grow.
There's [aiopg](https://github.com/aio-libs/aiopg) for PostgreSQL.
Open a command window, run your program using "python calculator.py" (or whatever your program is called), try the same thing, and see what it says.
When I ran in in the command it gave me this error. UnboundLocalError:local variable 'ans_string' referenced before assignment
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/machinelearning] [Project Template for Data Science\/Analysis : Python](https://np.reddit.com/r/MachineLearning/comments/43imbv/project_template_for_data_scienceanalysis_python/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I think perhaps 5 seconds is not a very long time to let it run for? I can't see why `gun` would be any better than the rest. they all have the same chances. 
Was thinking about doing something similar, thanks. 
Thank you so much for your comment, it makes so much more sense to do it this way. I didn't realize I could have used the message dictionary all along. I am very grateful for the reply. 
Glad I could help.
I've only gotten started so I can't say yet. Right now, I'm going through the Django tutorials and working on my own computer to run tests. I'm ahead of myself a bit but I'd like to be able to share my work with people as soon as possible. Perhaps an article on how to get python, django, mod_wsgi, sql, and apache all working together on a remote server?
"tolineEdit" doesn't seem to be a valid method, did you mean something else there? Although, that shouldn't cause the entire program to crash.
It line 14 print (ans_string) http://pastebin.com/psq77x8j 
&gt; UnboundLocalError:local variable 'ans_string' referenced before assignment See comments here http://pastebin.com/8G5J2EeN 
Thank-you for reading! I'm glad I could be of service :-)
Even with your suggestion it did the same thing.
That would be ok if OP doesn't need 2.7 and can upgrade to 3.x. If not, then this would be a problem ::Edit:: oh completely misread that, sorry woops
In your site-packages, check and see if you find any folder named something like "urllib"
First suggestion is to use the right tools. Upload as a github gist.
install requests this should solve ur problem.
Inner pages cannot be favorited or directly linked.
&gt; list.extend can be used to append multiple things to a list at once. Also, `+=`
 - In Python we usually put the "scripting" code (not the functions and constants) inside the conditional `if __name__ == '__main__':`. This ensures that it's executed only if you run the script via the terminal. Otherwise, if one day you want to import the source file as a Python module, typically to profit from the functions you've written, the whole file would be executed at the moment the file is imported, which is probably not what you would want. - I would suggest to use UPPER_CASE to name the constants such as `wordfile`. Also, I would consider `convertchars` to be a constant so I'd move it to the top of the file. This makes it easy to find it and edit it for future development. However, `wordlist` and `words` aren't constants, so you should probably move them just at the place you need them. - There are several argument parsing modules that will make you life easier. The one in the Python standard library is *argparse*. [This blog post](https://mkaz.github.io/2014/07/26/python-argparse-cookbook/) is a nice introduction. - Anyway, in `ParseArgs`, you can use a dictionary instead of a list of lists to store the arguments and their values: options = { '--case': 1, '--number': 0, '--special': 0, '--word': 3 } This way, you could access (and set) the arguments value using `options['--number']` instead of the less readable `options[1][1]`. - Nothing to do with Python but with complexity, I wouldn't iterate over the whole wordlist to convert all the elements to upper/lower/title case. I would make the conversion of only the elements I have chosen from the list.
So pip has support for urllib3/requests?
What do you mean "support for"? It uses both internally. https://github.com/pypa/pip/tree/develop/pip/_vendor
Eh, it [pep 257](https://www.python.org/dev/peps/pep-0257/) from 2001 and although getting the basics right, is a little light on the details of technical documentation for code objects.
If you're interested in continuing to learn, you could work on making it multi-tenant with user auth. Looks good, though! Keep it up!
Have ypu considered reviewing code snippets as a career....nicely done
First step is to register, and then I will grant you permission on that branch. 
I'm not sure what the current practise is for generating secure passwords: several cryptographers say don't use dictionary words, XKCD says it's OK, and the advice changes frequently as new techniques are developed. (The XKCD was published several years ago in 2011). I think the idea of passwords is obsolete and where possible you should combine them with 2 factor authentication.
&gt; Hi everybody. &gt; I'm working on a project to perform k-means clustering on a given set of data. I've been able to successfully do so on a local machine and the next step involves performing the same on different connected systems. Since I'm new to Python, I don't really know how to go about this. Is there any package that I can use ? &gt; Any help is greatly appreciated. Sorry! Didn't know /r/learnpython existed. I'll submit it there. 
I always run code through this and `pep8` and `pylint`.
I'm sure the "minitwit" and this microblog tutorial are similar, but Miguel Grinberg's mega tutorial is great. Still have it in my bookmarks bar! http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world 
Please replace import random by: from Crypto.Random import random There's a difference between pseudorandom and sufficiently random for cryptography. Last I checked Crypto.Random is terribly slow on MS Windows, but probably because it has to be.
If you want to analyze words, why not remove everything that isn't a letter? import re rawtext2 = re.sub(r'[^a-zA-Z0-9 ]', '', rawtext)
Crypto isn't in the standard library. Except for the shuffle at the end, the code is fine. `random.SystemRandom()` uses `os.urandom()` to produce random values which are safe for cryptographic use. This generator is provided by the OS for this purpose. `random.Random()`, and the module-level functions (which use a hidden `random.Random()` instance) are pseudorandom and unsafe.
I think the most plausible scenario would be something like what's happening with the recent Juniper backdoor, but extended to chips or boot loaders. You could realistically claim that all computer have settled on one boot loader for security reasons and that happens to be flawed somehow.
Also check out r/netsec and r/asknetsec
For future readers: [I uploaded the code to GitHub](https://github.com/FilipVdBergh/RaspberryPi-Pihole-Display).
It might be that what you want can be achieved merely with a webserver (e.g. nginx) - their config files will allow you to serve files with authentication, have specific files sent in case of error, send the content-types headers in a configurable way, etc., etc. They won't have as many options for uploads, but if you're just serving content then it might not require a programming-language.
&gt; The word is "benefit" and the thing about simplified explanations is that they are routinely superseded in the natural progression of the learning process. I agree that simplified explanations are required and that they will be supersed in the learning process, but it does not mean that any wrong statement can be justified for the sake of simplification. &gt; Memory semantics? Is that a general term or does it refer to something specific? I was referring to "They keep the data you give them until they return the last thing you wanted of them, then they die and take all that data with them.". This is very inaccurate, and I can't see its value as a simplification. It's actually very misleading, generators don't "return the last thing you wanted of them". I understand you want to provide clean and simple descriptions of these concepts, but I really think it can be made less misleading and more precise. Assuming the notion of object and for-loop are already understood, I think the following (over-) simplification provides a better understanding of a generator's utility. "A generator is a function which returns an object that produces multiple values, and can be used, for example, in a for loop." The next step would be describing how such generator functions are defined (the 'yield' keyword). Maybe you have your own opinions on learning/teaching Python, and we just can't agree what is the best way (not that it is a problem). EDIT: reformat paragraphs
Isn't that a escaped character sequence? `\xe2` should be just a single character, unless you did something to it.
Why is using that better than a for I in range loop? Genuinely curious.
You're making a few leaps in your example, so I'm not sure you entirely understand what's happening...but close enough. I agree with the other commenter that suggests flask. You'll be doing more coding because it provides less magic behind the curtain, but Django isn't the best platform to build something like a RESTful API on top of. You can, but it's more trouble than if you had used flask. Yes, I know there are REST plugins/modules for Django. But still. Django is heavy if most of your logic is going to be in a mobile app.
It doesn't matter here. random is used to select a word from dictionary and then a random selection of special chars is added to the mix. The generated passwords are extremely weak and open to dictionary attacks (you are choosing valid English words out of a dictionary after all :D). 
There was a good video of a talk that someone (I think a core developer) gave at pycon I think where he discusses having made a bunch of things from itertools fast. I've looked and can't find it, but if someone else knows what I'm talking about I'd love to see it again :) Edit: got it, but it doesn't go into very much depth like I thought it did. It's still a great video though. He talks about enumerate at roughly 6:55 [video](https://youtu.be/OSGv2VnC0go)
I briefly considered picking it up at FOSDEM, but didn't. Now I wish I had seen your review sooner. 
I prefer [docopt](http://docopt.org/).
 with open(wordfile, 'r') as f: for line in f.readlines(): ins = line.strip() if len(ins) &lt; 6: wordlist.append(ins) Why are you only using words with less than six characters?
I did, thanks!
Exactly - you can precisely quantify the entropy in OP's passwords by looking at the program, dictionary-length, and options. If that's enough entropy for you then who cares that the output is in the same format as weakly-chosen passwords.
Your estimates are way off. For some kinds of password hash, ordinary desktop computers can test over a hundred million passwords per second using password cracking tools running on a general purpose CPU and billions of passwords per second using GPU-based password cracking tools. https://en.wikipedia.org/wiki/Password_cracking Years ago, when I was looking into this, there was cracking software (using GPU) that could crack passwords formed from mixed letter casing of a longish sentence with special chars in it within a day or two. Passwords of the form ILMd3$! (for I Love My dog 3$!), but much longer random sentence.
Right, but that discounts a significant factor. You can't just test a password infinitely on most systems. For a database server password, I'm not gonna use dictionary words and will probably use at least 20 characters, because of the local brute force risk. For my gmail password, there's no way to attempt passwords that rapidly.
TL;DR: crossbar is a tech allowing transparent, clean and easy pub/sub and RPC (http://www.slideshare.net/slideshow/embed_code/43021672). This release let you specify which event you want to keep track of, so if a new client (say a JS chat) want the last sent messages after it connectioned, it can request them. It also now let clients authenticate with TLS certificates and public/private key pairs, so your micro-services on your servers can recognize each others in a secure manner.
At that point, they're not brute forcing the password, they're brute forcing the encryption key used to encrypt the password. I'll admit crypto is not my strong suit, but in symmetrical encryption with openSSL at least, you will get a nonzero exit status if you try to use the wrong encryption key to unencrypt a string. You don't actually need to know what the string is to know whether you succeeded in decrypting it or not.
TL;DW: if you used twisted, it's defer.inlineCallbacks on steroids. 
I have a folder named urllib3 in site-packages\pip\_vendor\requests\packages 
Interesting that DefaultSelector could double the throughput on a simple accept &amp; send benchmark compared to the asyncio event loop, despite for instance the docs [here](https://docs.python.org/3/library/asyncio-dev.html#debug-mode-of-asyncio) claiming "The implementation of asyncio has been written for performance." I wonder what explanation the core members that have worked on it have for why that can happen.
Is that a very truncated traceback?
I'm sorry, what do you mean by that? That's the full message it gives me when I try get-pip.py. When I type Pip into Powershell, the error message is similar but a lot longer. Edit: Here is what it says in Powershell when I just type pip: &gt; pip Traceback (most recent call last): File "c:\python27\lib\runpy.py", line 162, in _run_module_as_main "__main__", fname, loader, pkg_name) File "c:\python27\lib\runpy.py", line 72, in _run_code exec code in run_globals File "C:\Python27\Scripts\pip.exe\__main__.py", line 5, in &lt;module&gt; File "c:\python27\lib\site-packages\pip\__init__.py", line 15, in &lt;module&gt; from pip.vcs import git, mercurial, subversion, bazaar # noqa File "c:\python27\lib\site-packages\pip\vcs\mercurial.py", line 9, in &lt;module&gt; from pip.download import path_to_url File "c:\python27\lib\site-packages\pip\download.py", line 38, in &lt;module&gt; from pip._vendor import requests, six File "c:\python27\lib\site-packages\pip\_vendor\requests\__init__.py", line 58, in &lt;module&gt; from . import utils File "c:\python27\lib\site-packages\pip\_vendor\requests\utils.py", line 26, in &lt;module&gt; from .compat import parse_http_list as _parse_list_header File "c:\python27\lib\site-packages\pip\_vendor\requests\compat.py", line 7, in &lt;module&gt; from .packages import chardet File "c:\python27\lib\site-packages\pip\_vendor\requests\packages\__init__.py", line 29, in &lt;module&gt; import urllib3 ImportError: No module named urllib3
Was going to suggest Flask so that you have a finer grain of control over your app. But looks like you're already going for it! 
No question about cost and especially true for Oracle (I work for Oracle) ... but, consider for a moment that companies shell out huge amounts for Oracle and they do so for a reason. I'm not saying that PG doesn't have advantages over Oracle, but when compared on the whole they're in different leagues. DB performance is a very tricky thing with all databases ... I can make an Oracle instance perform like shit, but it's an amazing product when managed well. It's sometimes the only DB that'll handle a given load. 
Essentially you need to make a call to begin_fill() before you start drawing, and when you end your drawing you need to make an end_fill() call. Also on your drawSquare function you don't need the last 90 degree rotation. You can find the documentation here: https://docs.python.org/2.7/library/turtle.html#turtle.fill The fill() method works the same as begin_fill() and end_fill(), except it takes a boolean argument, you call fill(True) to being, and call fill(False) to end. If you ever start experimenting with python3, take note that the fill() method was removed, and there is a new one called filling() which returns True if filling is currently active and False otherwise.
A Kickstarter by Damien George, the originator of MicroPython, was started on Thursday to fund working on an official port for the ESP8266. Personally, once this is done (for some definition of "done" — this is Software, after all) The Sky's The Limit. Espies have rather anemic specks, so newer WiFi/IoT modules/chips with more leg room should be even easier. This is a welcome event, and it excites me not only for $6 Python-programmable MCU's of NOW, but for many other current and near-future WiFi/IoT boards/chips/modules that are/will be available soon. Not only do I prefer to use Python daily, I think it's about the best language for anyone to learn to program. (I know how biased I am, but for cause! ;-) The Maker/Physical Computing thing has been wonderful, especially to help/encourage kids (of all ages ;-) to get into, well, *making* and *building* and *coding*. Anywhere Python can be where this happens I think only adds to the mix and helps/encourages more. Tell your friends, and/or back yourself if you feel so inclined. I can't wait, myself. An official port that Just Works! :-)
&gt; but it does not mean that any wrong statement can be justified for the sake of simplification. Then I would think that the task is to explain how the statement is harmful. What specific harm is caused? &gt; "A generator is a function which returns an object that produces multiple values, and can be used, for example, in a for loop." Normal functions are functions. Normal functions are objects and return objects. Normal functions can return and produce multiple values. Normal functions can return objects that produce multiple values. Normal functions can be used in a for loop. What exactly are you trying to teach people here? What do you mean "produces", why is that important? Put another way, why does it matter to a novice user, whether a generator returns an object that does something, or that the generator does that thing itself? How is it going to change their use of the thing? &gt; The next step would be describing how such generator functions are defined (the 'yield' keyword). Wrong. The next step is stating that a generator only creates the next item to return when it is requested, meaning that a generator that gives you a very large number of items, one at a time, uses far less memory and is quicker than a very large list containing those items. The term "yield" provides absolutely none of the information a novice user needs to recognise the value a generator gives them.
Why not just from yourapp import models you can put that into an ipython profile and have your models available. Or you use an ipython profile and use [this part of the flask documentation](http://flask.pocoo.org/docs/dev/cli/) to build your own ipython shell. 
They can clone a github https repo
Yeah, that's pretty much it.
https://caremad.io/2013/07/packaging-signing-not-holy-grail/
I'm not sure if you quite understand the kind of Python we talk about here.
So what have you done so far? Also /r/learnpython might be a better bet for basic programming questions. 
My sympathies. Your professor must think wasting time with and cursing at Notepad builds character, or something. I'd say that's not true. [PyCharm (Community Edition)](https://www.jetbrains.com/pycharm/download/) is free and excellent.
Thank you, downloading it now and will give it a try.
Hmm. I've been looking for excuses to delve into asyncio but maybe I just should. 
~~You said it was free but it says that it is $199, is there a free student version?~~ Never mind, I found the free student version. The only reason why I'm in this class is because I have to take a programming class for my degree. This class is about the same as taking a semester of a foreign language and expecting to be able to use it for the rest of your life. 
I've also posted there, so far I have.. not a lot. 
Yes, aiohttp also has a websocket client. 
This may be orthogonal to the decision you're trying to make, but the consensus I've seen has been that redis is preferable to memcached, and there are also good Python bindings for it. Probably not a deal breaking difference, though, and either seems like a good match for your actual need.
TIL.
&gt;I could load all the data in a database but it seems like all the queries to the DB would hamper performance. Relational databases are pretty fast. If you can get all your data in one well constructed query, it'll be way faster than doing the same in pure python. With sqlite, you can mount your database in ram too.
Learning to program is a useful skill these days that can pay dividends - you can find plenty of opportunities to program throughout your life if you choose to.
Actually, I found rapydscript through your site and that seems pretty spot on. I personally don't use deep or nested inheritance much, so that goal of yours is unimportant to me. Either way, ecma 6 will bring generators to javascript, which will make all these projects way better.
I thought the idea of asyncio was that it would be the "one and only" event loop running in your program which would increase sharing and reusability of everyone else's code. I thought that the twisted problem that Guido was trying to solve was that once you went twisted you had to use twisted versions of everything. I don't know if what Dave has done here solves that problem. He's saying just promote the async/await API not the implementation but without the implementation don't you just end up with everyone writing their own event loops ? How do they play with each other, do you get back to the twisted problem ?
&gt; nodemcu The language used for [NodeMCU](http://nodemcu.com/index_en.html#fr_5475f7667976d8501100000f) is [Lua](http://www.lua.org/). You might check [/r/lua](https://www.reddit.com/r/lua/), or [/r/esp8266](https://www.reddit.com/r/esp8266); they both seem rather active. [Adafruit's HUZZAH](https://learn.adafruit.com/adafruit-huzzah-esp8266-breakout/using-nodemcu-lua) NodeMCU Tutorial. I have one (generic) on order, so can't speak/help directly. [Adafruit also has a Tutorial](https://learn.adafruit.com/building-and-running-micropython-on-the-esp8266/overview) about the current (*Proof of Concept*) MicroPython you might check out. If you read the [Description](https://www.kickstarter.com/projects/214379695/micropython-on-the-esp8266-beautifully-easy-iot/description) of the Kickstarter for the Official Port of μPy: &gt; So why this Kickstarter? There is already a proof of concept port of MicroPython to the ESP8266 but it uses the execution model provided by Espressif. This model requires you to set up callbacks to process Wi-Fi requests and leads to very cumbersome code. Python is built around standard Berkeley sockets and what we want to do is develop such “proper” sockets for MicroPython on the ESP8266. We aim to provide a true Python socket API to make development a pure joy. Check out the *What is the Berkeley socket API and why is it so important?* section in the desc especially. There's a demo vid (one so far, but another showing using [REPL](https://micropython.org/doc/tut-repl) via WiFi promised shortly) which also Damien describes a few concepts/goals. Personally, I truly **can't wait**! Yes, I think THIS project will change the way you, me, and most people interact and leverage Espies! Watch the vid and read the desc and tell me it doesn't sound like a breath of fresh air! :D
This looks awesome, is there a list of the datasets that are included?
It uses [Rdatasets'](https://github.com/vincentarelbundock/Rdatasets) collection. [List.](http://vincentarelbundock.github.com/Rdatasets/datasets.html)
It's too late for me to watch the entire thing but, 17 minutes in.... Damn. Him properly coding while speaking at the same time! I can't even talk and type at the same time, let alone when the typing that I'm doing while talking has to run! I guess I will now be watching the rest of this tomorrow! Nice. Very nice.
This subreddit is for things related to the programming language Python, not for things related to snakes. Your comment has been removed, but if it's cute enough, you might try posting to /r/aww
Does the cache need to be accessed by multiple instances? If not, you could simply use lru_cache, which is part of the standard library. 
If anyone has any feedback, let me know. Please tell me if I'm doing things totally off based in my script or in my use of Pillow and Tkinter. I'm also open to UI suggestions. I'm going to be honest, I focused more on functionality than than the UI. Improvements can definitely be made to usability and UI. And of course, please let me know of any crashes or suggested features. I'll get on those.
The idea would be that each developer keeps their key locally with a strong password. I imagine its rather difficult to steal a key that way.
(serious question) Why is dependency resolution not a high priority feature? Is everybody okay with the status quo? I'm talking about https://github.com/pypa/pip/issues/988
I have a web app that I use for expense tracking, but I have to input data manually. It would be so much easier if my bank sent me an email, but they only do messages (SMS). That's a shame. I think banks of the near future should learn to provide a secure API for expense tracking apps. Parsing all that different receipt slips, SMS, emails is just tiresome.
Well, you have check to make things thread safe, and then you have the handling of the default thread executor which is activated by default.
"This project also serves as a case study for why you should get a decent artist and UI designer when working on projects." - Great!
Unidecode (via stosh00) and ftfy are 2 good options. I've written lots of notes on this sort of stuff in the past, I spoke on it at PyDataParis: http://ianozsvald.com/2015/04/03/pydataparis-2015-and-cleaning-confused-collections-of-characters/ and I've written up notes on general data science (including lots on data cleaning) here: https://github.com/ianozsvald/data_science_delivered/
Incase you're unaware, HTTPS includes integrity (which is what GPG signatures are providing) as well as confidentiality. With GPG the package repository itself can't (generally) attack you, but anyone with access to the build server can. The way a distro uses GPG is a bit similar to how pip uses HTTPS, the main differences being that the GPG signatures "travel" better from from the main repository to mirrors and the like whereas HTTPS you have to trust the actual thing serving you files (instead of the build machine). In all likelihood we're going to get some method of package signing into Python packaging, the current best candidate ending up being a bit more comprehensive than what you'll find in most linux package managers, but it's also a bit harder for us because we're solving a fundamentally harder problem. We're not going to do it because HTTPS is insecure, but rather as an additional layer that has the nice traveling property of signatures that aren't based on the transport and which will ideally allow for cryptographic verification back to the person who uploaded to PyPI. Now, when you add in another plan we have though, that of a build farm for PyPI that authors can use, we start to revert back to the "well you have to trust PyPI" situation anyways (much like how you have to trust the build machines running Debian builds or what have you). Hash checking is a bit different than that though. This allows you to, assuming you've safely acquired a requirement.txt, completely eliminate any ability for anyone to serve you a different file (barring sha256 being completely busted). This is a stronger guarantee than you can get with signatures since they can only prove that a file was signed by someone possessing that key, not that it was the exact file you expected, but it requires more manual effort to actually vet the file yourself and handle securely getting the requirements.txt.
It's sort of only used for integrity, but sort of not. In the sense that everything that comes from PyPI is pulled down using HTTPS, including both the expected hashes (not the ones in the new hash checking feature in pip 8, just the general hash) and the packages and if you're capable of attacking the HTTPS that is downloading the packages you're capable of attacking the HTTPS that is downloading the checksums so it's largely not adding any security, yea it's just for integrity checks. The flipside of that, is that the files (currently) stored in Amazon S3 and the hash is proving that the underlying storage of those files (S3 in this case, but could be anything) hasn't been compromised. That being said, MD5 is not currently *actually* broken in the way that matters for what we're using it for. The property we largely care about is preimage resistance which as of yet, the best attack that I am aware of on the preimage resistance of MD5 is a theoretical attack published in 2009 but no practical attacks. Of course, relying on something that has a bit of a sense of, "one foot in the grave, the other on a banana peel" is not a great place to be since it typically means you end up scurrying after it's been completely broken to try and patch things to fix it. There have been efforts for years to prepare things for switching away from MD5, pip 1.2 included the ability to use a variety of sha-2 hashes in addition to MD5 as does setuptools since 0.9 (pip started verifying TLS in 1.3 and setuptools in 0.7). Just recently we removed the ability for PyPI to link to files not hosted on PyPI, removing the only source of MD5 hashes that we don't directly control. On top of that we've also just recently backfilled sha256 hashes for all of the files currently hosted on PyPI (a little over half a million) and started recording the sha256 hash for all future files and Warehouse (PyPI 2.0) no longer uses or displays the MD5 hash, instead it uses sha256. So once we cut over to the new codebase, MD5 will be, practically speaking, dead on PyPI.
It's funny because you are tracking bottles using bottles
Not really related to the topic at hand, but largely because there's been either lower hanging fruit that people have chosen to work on, or people had some other kind of problem they cared about more. Basically, the same reason any particular issue isn't worked on in any OSS project.
Isn't it just the reference implementation that is slow? the reason is that the interpreter needs to be understandable, and that means belaying certain optimisations.
RapydScript strikes a very sensible balance. It's what I would have used otherwise.
Then watch it :) ... seriously
[removed]
That's precisely what [Yodlee](https://www.yodlee.com) and [Plaid](https://www.plaid.com) purport to do, but their stuff is often behind (Yodlee, in fact, [is documented to be 19 hours behind](https://developer.yodlee.com/Aggregation_API/Platform_Overview/Refresh_Policy)); while it would be nice for the financial world to get a swift kick in its backside and provide the data to you for free, I'm not holding my breath for this to happen.
I'm sort of surprised how rarely is used although given the price tag I guess it makes sense its only at larger corporations
In answering the first question what you describe is very important but it's not what I intended with the question. You describe verifying the accuracy of the data collected. What I meant to ask was determining if the correct data is being collected in the first place. So for example if I want to answer a specific question, what data do I really need? If I think I need data set 'X', is that the right data to answer the question? Of course once I have data set 'X' what you describe is perfectly correct in that you need to validate that you really do have data set X.
Ye you are right. But its a beast of a machine when used correctly. And cheaper then the competition. Before i left my previus company i open sourced some of the python helpers i developed: https://github.com/dusans/netezza-helpers 
This is incredibly similar to my doctoral research project, "CORGIS" (Collection of Real-time Giant Interesting Datasets), meant to make datasets available to students at the introductory level. Instead of a single library, I've been going with distinct libraries for each dataset. The pitch there is that the datasets end up being big, language bindings for Java and Racket are also available, and I have a few weird innovations that make them more convenient for students (e.g., disk-based access for more efficient sampling). I've also got a stronger focus on the social context provided by the datasets, too. I'll be very interested in how your platform develops. In particular, what do you see your major use case to be?
Do you have a website or some other location where we can keep track of this, and know when it's complete (apart from Reddit, of course)?
Designing your test methods using a simple structure such as given-when-then will help you: - Communicate the purpose of your test more clearly - Focus your thinking while writing the test - Make test writing faster - Make it easier to re-use parts of your test - Highlight the assumptions you are making about the test preconditions - Highlight what outcomes you are expecting and testing against.
I appreciate the effort, it looks very sexy and I'll probably use it as soon as it's in beta, but jeeez, framesets, really?
Check the dependencies 
wouldn't it be great if we could just use python as is in the browser instead of this mess? 
I think every python dev has thought of that at some point
This is a cool project, but how many posts are we going to have about it? Are these two, that _you submitted in the last 8 days_, not enough? * https://www.reddit.com/r/Python/comments/42bl46/transcrypt_new_python_to_javascript_transpiler/ * https://www.reddit.com/r/Python/comments/43a2r1/transcrypt_lightweight_python_to_javascript/
Both of the previous links earned over 100 points, the most recent one only two days ago. I think the "herd" has been "told to come". It would be one thing if each one added significantly to the previous posts, but I don't think they do. IMO this is bordering on spam.
Oh, I don't know ... after 15 years here at Oracle Consulting I think I've learned enough to make a few statements. I'll stick to my guns here. Our license prevents you from benchmarking Oracle for the exact reasons that I talked about above: I can make Oracle perform like shit even on the best hardware. We don't allow people to make our products look bad, either intentionally or not, as a means of proving "we're faster than Oracle" in some specific case ... benchmarks are usually bullshit in any case, but leave us out of it because the benchmarker, ourselves included, always has an agenda. PG is a great DB for some use cases, but it's not Oracle (or mssql which has improved quite a bit over the last decade or so). 
Miguel also has a nice tutorial on [authenticating an restful api](http://blog.miguelgrinberg.com/post/restful-authentication-with-flask) which might relevant to your goal. 
That's a very generic question. Well what problem are you trying to solve? I'm an aerospace engineer 10 years out of school and do R&amp;D for places like NASA, the USAF, Navy, Army, etc. I program to solve engineering problems. My coworkers/customers sit down and hash out what the high level API functions, inputs, and outputs to the code will be. Then I go sit down and code it. Then I find out I need something else, so I pass it in because I can't solve it. So just add more data. If you're trying to solve a generic problem and you don't understand what you need and you don't figure things out as you go, you're not going to solve the problem correctly. You need to go learn more about your problem. What information do you need to design an airplane?
DEFINITELY put those images in the readme.
It's hard to provide a "deep link" to a page, and can't be bookmarked for the same reason.
How does it handle `yield`?
That's only step one. There would still be an ecosystem of libraries we'd need to get anything useful done.
As I said, I was learning about ORM's and I thought to make something with it. So here it is :)
Yes it is generic and I intentionally kept it at a high level because most of what I've found to-date has been too specific. People describe how to solve a very specific problem. There's not much discussion on "how" to solve a problem. With this post I was hoping to spur discussion in this area.
You can not connect juypter notebook to a running kernel. But you can connect qtcpnsole or a regular ipython console to a running kernel. Spyder the ide will also connect to a running kernel. Embed a kernel instance and then connect to it with one of the above I mentioned. 
You making sure you run the function with an x that is at least 8? If x &lt; 8 then it will always return 0.
Yes, no matter what I enter it's always returning 0.
Thanks for the feedback. I haven't tried any mods yet, first I want to finish the game "cleanly". I wanted to keep the script simple. It works well for me and thought I would share it.
It's an abbreviated form of addition and assignment: a = a + 1 is the same as a += 1 Same with `-=`: c -= 2*c + 4 is equivalent to c = c - 2*c + 4 
[Relevant StackOverflow page.](http://stackoverflow.com/questions/4841436/what-exactly-does-do-in-python) For future reference, try searching "plus equals python" rather than "+=" or else you get irrelevant results.
def sum_up_to_div8or13(x): i = 0 total= 0 while i &lt; x: if (i % 13 == 0) or (i % 8 == 0): total += i i+=1 return total 
Yeah, when you transpile just look for an anonymous function in JS and replace it with the word *foo* and then two lines up in the code put *var foo =* and then the code you replaced with *foo*. **Edit** I cannot into Facebook markup
You can't make your content scale well for mobile if it's split down the middle in two panes.
I figured it out, my indentation on one thing was wrong. Thanks for the help!
with a stern wag of the finger
Seriously /r/learnpython
Good question. Competition is good, but there are so many projects like this (none of which is really mature) that it'd probably be better if the community rallied behind a single one.
I don't know, this is the first time I've seen it. Not all of us F5 the new queue all day.
If you are only modifying text in a file then it would just be easier to use notepad++'s find and replace functionality, they have an option to make a new line for every time it sees a "[" I understand that you may just want to do it in python but problem solving shouldn't just be limited to code.
Neat. An area where Python3 is speedier than Python2. Thanks for pointing that out.
Thanks to all of the great questions we got, we have now written an FAQ for the project: https://github.com/Microsoft/Pyjion#faq
I enjoyed this article, fun read. It's also a pretty good way to advertise some capabilities of twilio, I may check it out sometime 
Any benchmarks?
I'm wasn't talking about a benchmark. I'm talking about a real world problem. I regularly see 10%+ performance loss on my problems.
No, it's not. += and -= do augmented assignment, it's not syntax sugar or equivalent to regular assignment. They do different things and while they can be equivalent they don't have to. On lists + and += do different things for example. 
Saw it before, but I don't feel bothered by seeing two posts on eight days for a young project. EDIT: replied to wrong post
I was on reddit basically all weekend and I didn't see either so idk people can miss it
You're right, forgot about the Lua aspect of things. I love the idea of a minimal interpreted language on an Espie, but the node/lua thing just seems... fragile. Plus I run out of memory *very* quickly. I like the python language itself. I'd love to program ESP's in python, even if it's a subset. I actually do like callback (or even better, promise) based networking... especially since berkeley sockets + async is a little tricky (selects, non-blocking sockets, etc). But I'd rather have a predictable known API than a weird crazy one with weird gotchas. So yeah, can't wait for that. Maybe I'll actually get some of my ESP projects off the ground when this lands.
Ah! Thanks. :)
I made a very project called HDD-INDEXER https://github.com/coolharsh55/hdd-indexer that indexes movie files and stores their metadata such as director, ratings, etc. in a local database and uses django under the hood to provide a guide via the browser. You should check it out, it's quite similar and related. And if you're going to continue the project, then maybe we should have a chat.
A simple tutorial on what metclasses are in plain english and an example of how to use them. Basically python classes are nothing more then fancy dictionaries and using metaclass you can modify them. Not sure if it useful for anyone. Basically I just want to keep notes so I remember how to do things. 
Is multiple inheritance the only thing it doesn't have that you want? Have you checked to see if they'd be open to a pull request? There's lots else they have to do that I'm sure you could be helpful in.
How does this to compare to [RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html), as implemented in sklearn?
Yeah right but the event loop will be defined in the implementation. .. and if no-one is sharing the definition of the even loop then isn't that a problem ? I'm not an expert here, just trying to remember the justification that Guido gave for implementing asyncio.
I think you mean the right thing, but to clarify: You don't compute the performance on the individual feature but on each feature subset. E.g., if you have 4 features, "a b c d", in SBS, you'd fit model on abc, abd, and bcd, and eliminate the one feature where to get the best performance based on the subset after elimination. So, let's add some numbers to make it more clear (the higher the better) abc=88, abd=77, bcd=99 Since bcd is your best performing subset now, you eliminate feature a and go to the next round and so forth ...
This is a good observation. i actuay had to write a script to replace multiple line breaks with one break shortly after.
That's a good point! I wasn't aware the function was available. I did get to bone up on python a bit though.
How's that compare speed-wise to RFE?
Haven't done any speed comparison's against RFE; here, it's more about the predictive performance, plus the SFSs are more "universal" I'd say. RFE only really makes sense for linear models. However, you are welcome to do some benchmarks, I'd be interested as well :). PS: There's a [paper](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=02CB16CB1C28EA6CB57E212861CFB180?doi=10.1.1.24.4369&amp;rep=rep1&amp;type=pdf) that benchmarked the predictive performance against genetic algos and the optimal solution. SFFS comes pretty close to the optimal one. Anyway, the SFSs are more about offering a trade-off between the exhaustive search (optimal) and computational restraints. I'll add it so scikit-learn at some point when times are less busy, there was already an "issue" thread on github discussing this
The "most powerful" imgur downloader crashes and burns on python3
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Is it your gauge? The EASIEST and most reliable thing to do is to paint a green dot in the center, red dot at the tip of the needle, and then add some fiducials for scale with blue dots. The details will depend on the gauge readout. For instance, you can mark 0 and 60 psi on your example gauge. Then the angle green (center) to blue left (0) is numerical 0, the angle green(center) to blue right (60) is numerical 60... and the angle green(center) to red(needle) is the actual readout value interpolated over the calibration you found above (above). Since you are only using two points, you can compute this just by dividing two numbers! Once you fiducialize the gauge, the entire algorithm reduces to &lt; 10 lines, requires no computer vision algorithms, and is self-calibrating (i.e. if you move the camera a little, it'll retain its accuracy as long as the gauge face is still adequately represented with simple 2D geometric transforms)
Not 100% stable but ridiculously easy esp8266basic.com I'm still hanging out for python implementation, but for a really quick hack this basic is great (but slow). Has a built in IDE via wifi so easy to edit /program from a distance.
You could do something with scraping new stories on companies and compare stock market changes (sentiment analysis). 
There's also [jrnl](https://maebert.github.io/jrnl/).
You can run it with debug mode, but to be honest I just debug with print statements. I don't think an IDE will make you a better programmer though; maybe just a faster programmer.
Thanks...:)
I'm halfway through and still kind of confused as to why one would want to do this via SMS?
Even better is `foo(**settings, **overrides)` which previously required either combining the dictionaries first or using something like itertools.ChainMap to read from multiple dictionaries on the fly. Like with plotting I might store a dict of line properties, them a dict of marker properties, then a dict of other settings like a label, then `**` them all together into the plot function.
I can understand texting to control the game, like twitch plays pokemon, but why texting to manipulate memory?
Yes I cleared it with my boss. But the more interesting stuff I wasn't allowed to publish :P
Thanks for the heads up, even with the 19-hour lag this is quite interesting.
I think JavaScript is the only language which is sure to die along with the world.
Because why not?
You could install documentation, if it isn't already, but probably that won't be allowed.
Yeah I agree with this. This isn't really a tutorial at all.
Hey I know where you're coming from. It doesn't really have a "practical" purpose other than the fact that I thought it would be fun to do and would make for a good tutorial. It's actually funny that you mentioned the Twitch Plays Pokemon type thing. Last week I did a live coding stream on Twitch where I pretty much rebuilt the code from this post, and towards the end I added button input to the things you could text in. I didn't have enough time to make it work perfectly, but the basic stuff was there if you want to check it out. http://www.twitch.tv/sagnewshreds Edit: I'm not streaming right now, but I will be live coding some more Python on Thursday this week if anybody is interested.
What if a dictionary combination is used? Picking [\w\d\s]+ for 12 characters is something approximating 70^(12) combinations. Assuming 100K words, I find 100000^(4) to have fewer combinations. What am I missing?
I don't know anything similar but coding something like this in python would be easy. You just need to create a web interface using Flask you something like that, and create a cron job to delete the expired text.
&gt;calling systems programming unnecessary because you can emulate a system at half speed in a browser written using systems programming This is why I don't like js developers
I just use vim.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Apparently "for humans", in addition to sounding a little worn out, is also insulting to bots... I didn't expect this to be the most interesting part of the submission. lol
Looks like spam to me. Shouldn't have to purchase anything like that for this type of tutorial.
One type of label, for one type of plot
read the sticky. we don't solve your homework see /r/learnpython
I previously submitted this with the link title copied from the GitHub group's description, but the wording of that description seemed to distract from the actual content. (Specifically, the "for humans" part.) Since you can't edit a post's title I decided to resubmit with a less distracting title. Hope that's OK.
I've always felt the introduction of asyncio would lead to a lot of needless library rewrites, with no significant benefit over other available options (in my case, gevent). I don't know how involved OP is with these projects, but perhaps someone who is could chime in. Would you say this was a major project, or no big deal? How do you feel about asyncio after having worked with it for a while? How does it compare to other options? *edit: /u/lukasa's comment [below](https://www.reddit.com/r/Python/comments/43vvsn/aiolibs_libraries_for_mysql_redis_elasticsearch/czm9sub) demonstrates what I consider to be a much better approach towards implementing future network protocols/stacks than that represented by aio-libs.
I worked with OpenERP (now Odoo) for years and now I have migrated to Tryton. I'm really happy with the change. Upgrade between versions is covered and document. This will save you many headaches in the future. You can use flask-tryton, galatea or nereid to create beautiful UIs (if you have in mind django the way is similiar but with a better business backend)
Great find! I love the gamification behind this. They also seem to have a lot of R courses as well.
It will probably be like that for a while until the concepts involved begin to click. One benefit to asyncio is that it provides visual clues to help remind you what's going on ("yield from", or "await" in Py 3.5). Personally, I feel that once you are more comfortable with what's going on, the extra syntax gets in the way of readability. What you missed in my comment are the "other options". People have been doing asynchronous IO long before the asyncio library came along. Twisted, Eventlet, Gevent, Tornado, to name just a few. Gevent and Eventlet, in particular, are capable of working with existing, blocking, libraries without requiring any changes to them (in most cases).
The title is a little silly and misleading. However, this would make a decent 'cheat sheet' 
I'm not involved with any of these projects (I only started using asyncio recently), but it looks like at least one of the main authors, Andrew Svetlov [0], is a Python core committer and a major contributor to asyncio itself [1]. [0] https://github.com/asvetlov [1] https://github.com/python/asyncio/graphs/contributors
There is reasons why gevent and others is not that popular: 1) monkey patching 2) no visible context switch 3) hachish internals, "stack slicing"
I started to work with asyncio shortly after first release and it feels great! Very easy to work with and debug, especially after experience with Twisted/Tornado. 
also he is author of aiohttp https://github.com/KeepSafe/aiohttp
Thank you for all of that great software. I linked to it from http://blogory.org/python-coroutines/ Python Blogory is a hand-crafted directory of blog postings, YouTube videos, software packages and other web assets. I invite you to link to your blog postings. 
Have a look at https://pypi.python.org/pypi/tldextract
&gt; If you are about to ask a question, please consider r/learnpython. Homework-style questions will be removed, and you'll be encouraged to post there instead.
Take out the float conversions, and change print(onethird(13)) to print(onethird(13.0))
Monkey patching is what allows gevent/eventlet to reuse existing libraries even when they are written for blocking IO. That means I can write a single library, and use it directly with blocking IO (great for simple projects or testing/debugging), with threads, or with something like gevent. Regardless of the requirements of my current project, I can use just the one library, which means I don't have to re-implement all the hard to catch corner-cases that are common in network protocols. There is already a library out there for almost every protocol you might come across. A lot of them are having to be updated for Python 3. Now they're being completely rewritten for asyncio, and are going to run into all the same corner-cases and bugs. That seems to me like a significant amount of work. But I honestly don't know, which is why I asked in my original post.
&gt; My teacher runs his version and gets 0.3727810 It gives 0.0364132... for me. Might want to verify you have the same code. Then stop by /r/learnpython if it's still incorrect.
Wasn't ubuntu called linux for humans at some point?
Who are the idiots who are voting this down. IPython/Juypter rocks. Basically it allows python to be used like lisp/scheme where you have an evaluation environment. That means you can change things at runtime. This is what makes lisp special, most of the probes to outerspace ran lisp, they where completely reprogrammed without rebooting them. One of the amazing features of lisp is most variants could generate machine code for the function at runtime. Python is getting a lot of these features. Pretty much you can do anything in it. Most of the time I don't make shell scripts I just use IPython or if I need to plumbum. With Juypter things have even gotten more amazing. There are language kernels fpr nodejs etc. So you can run node just like python in the notebook. You can connect to a kernel remotely, with something like qtconsole etc. This is really handy to see what is happening on a server. I then couple juypter notebooks with docker. I then run the notebook from within the docker container. So I have different docker files for different nodejs/python versions. In the past I have wasted too much time using vitural environments like nvm for node or what ever the python one is called. 
I've been working with these for a little bit now and find that overall they're pretty good. I can't speak to whether or not the rewrite was totally necessary yet. If you're used to SQLAlchemy's orm you'll be a little disappointed though. One nice thing is if you do have a blocking call to make that hasn't been updated to also support async calls - you can just use [`loop.run_in_executor`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.BaseEventLoop.run_in_executor). It's a little heavier but nice if you're needing to do something like an LDAP call or whatever. It's probably wrong for me to do this, but I almost favor the executor call so I can use the libraries that I know are both good and have a good record of maintenance. I contribute where I can, but many libs go defunct too quickly. Edit: I should point out that the default executor is a ThreadPoolExecutor, depending on what you do a ProcessPoolExecutor may be better. You can either pass an instance in as the first param or use [`loop.set_default_executor`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.BaseEventLoop.set_default_executor)
I have not done much gevent programming but have experience with writing libraries for asyncio. For protocol handling part you are right, it is same corner cases, but IO approach could be completely different (for instance protocol piplining). In perfect world protocol parsing should be separated from IO so each async and sync way could implement only IO specifics. 
The first line of the Calc method is trying to print the value of a variable that you haven't defined yet. Note that ans_string isn't defined until several lines after this point.
How kind of you to leave debug mode on and expose your full tracebacks to the internet, [like this one](http://units.d8u.us/seo/).
Beautifully written, as you can tell I am still going over the Python documentation to see what I can and can't do but this really pushes me in the right direction. Thank you!
The argument to the function shouldn't matter (for this size, at least), it's all integer math until the last line. However I think the source of his problem may be using the bitwise XOR operator instead of exponentiation.
Are you actually trying to use bitwise XOR on count on line 5, or did you mean to square it? &gt;&gt;&gt;10 ^ 2 8 &gt;&gt;&gt;10 ** 2 100
It's useful to know a bit of some other programming languages - but if you're just starting, don't worry about that yet: get to a point where you feel comfortable with one language first. C is quite interesting because there's a *lot* of C code out there, and it's kind of the base layer for a lot of computing systems (the Linux kernel is written in C, for instance). But you can certainly be a serious programmer without knowing C - there are many kinds of programming, involving many kinds of languages. As for sharing your program with your friends: have a look at [Pynsist](http://pynsist.readthedocs.org/en/latest/), a tool I wrote which makes installers, or [PyInstaller](http://www.pyinstaller.org/), which despite the name makes exe files rather than installers.
Gevent has had support for Python 3 since June of last year. They just released 1.1rc3 in January. It has very active development but a drawn out release schedule, which is a part of why it has always been so stable. I've heard the interoperation argument often and I think that's fine in principal. I've seen exploratory projects to integrate asyncio into Gevent and Tornado but I don't think they've gotten much traction. Probably because there are already ways to integrate with other event loops and it's not something that's commonly required. Anyway OP's post, and my concerns, don't have anything to do with that. *edit: 1.1rc3
Yeah, I thought of that too, but I wasn't sure of what they were trying to do.
You have gotten some good advice here. But I would recommend leaving the script as you originally had it, rather than making edits for style or convention just yet. Simply typing out what was provided here is about as useful to learning style and convention, as blindly accepting a pull request. Read the suggestions. Understand them. Then return to your script in a few days (or week) and make the changes you THINK are necessary or worthwhile. If you do this WITHOUT immediate reference to the text here, you will force yourself to grab these changes from your own memory. From what you have already internalised. After that, reread the suggestions, see the ones your remembered and, more importantly, see the ones you are yet to internalise. This will better enable you to OWN the style conventions.
weather would be a cool addition.
If you're OK with a really simple GUI you could use the [EasyGUI](https://pypi.python.org/pypi/easygui) package.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
not saying it isn't, unstated biases just rub me the wrong way
I like the Idea as I live in the CLI but don't know how results are given..... and too lazy to look them up... Maybe I should google it... Dang it... 
OK I glanced at the code it uses launch... If I use this I demand Duck Duck Go!!!! 
Yeah I have tried it but not very good results.
What a mess! Async specific methods should be added to original packages. There is no need for separate package clones just to support async.
My entire company had infuriating bugs due to the hackishness of gevent until we moved off of it. It was also one of the things that conspired to ruin the backend of a large event called the MIT Mystery Hunt in 2013. Let me be clear that everything I'm going to say is about gevent with monkey-patching. I'm not quite sure what gevent would be without "optional" monkey-patching. Something where you have to yield explicitly but don't get any of the benefits of a library designed for explicit yields? And let me also be clear that I haven't used gevent since 2013, for hopefully obvious reasons. When you monkey-patch, you'd like to believe that everything automatically works the same way except asynchronous. But there's so much code that just doesn't expect to be asynchronous, or depends on details of the standard library that gevent doesn't emulate 100% the same way. You need to understand what every line of every dependency of yours is doing, and if you don't: bugs. You can even get heisenbugs that change behavior when you add a line to log them, because the extra yield changes the order of things, in library code that assumed it was running in a deterministic order. You can't just replace an entire programming language's standard library safely. It's kind of amazing that gevent monkey-patching gets as far as it does, but notably, it can't even run correctly inside ipython. Don't use gevent in production.
You don't need to know C. **There is Cython**. 1) Install Cython and MinGW: https://github.com/nuncjo/cython-installation-windows 2) Compile http://docs.cython.org/src/reference/compilation.html 3) Install PyInstaller to bundle all to .exe
Yes, that is common in most top IDEs, like Komodo, Eclipse, to name a few I have used. However sublime just works best for me. Not much clutter, elegant and plugins for anything that's missing.
Yeah, I guess there is no way to specify extra data files outside the spec file. It's usually best not to distribute executables in the repo itself. Use git tags and github's Releases feature for that. Also add `build` and `dist` to the repo's `.gitignore`.
Using an executor is absolutely a valid option. It's especially nice if you're adapting existing code or have a blocking call that lags just enough to really justify an async call. The Tornado wiki has some [good advice](https://github.com/tornadoweb/tornado/wiki/Threading-and-concurrency) on how to approach async vs. blocking calls.
Just curious, when did you last use Tornado? It has used a coroutine approach similar to asyncio since version 3.0.
Not that easy, method does not become magically async if you add `async def` before it. And packages are not clones, IO part completely different in most of the time.
I wonder why you mentioned MySQL and not postgres n the title, considering there are many people warning you due to their experience not to use the former, because of insane defaults, choices, and quirks.
The main problem here is that people write libs for techs, instead of writting neutral protocol handlers, then adapters for one tech or another.
The main selling point about await/async is the common interface for any asynchronous work. Being it aio, or threads, or multiprocessing, or just plain regular callback or the upcomming (in 3.6) multi interpretters or calling shell cmd with multiprocessing. You can mix all these, and they will do their things on their side, but in the end, when they are done, they go back to the await keyword. And that's fantastic. Another selling point is that you know what's going on. You can identify clearly what are the things doing an async job, which makes debugging much, much easier. You can see the data flow, and where it breaks. Yet another selling point it that it's not monkey patching, which can have unexpected propagated side effect and will break everytime the initial implementation changes too much. await/async will work on CPython even when they change deeply the intervals (like the plan on using nodejs' libuv). It will work on Pypy. It will work on nuikta. It will work with pyjion. And so on. And last, but not least, it's embeded in the stdlib, which make it a de facto standard available all the time. One major problem with the current async trend in Python is that they write libs for one implementation. It's a waste of ressource. You should make a neutral lib, handling the protocol and the workflow, and you provide hooks. Then you add on top an adapter for your favorite implementation. Then when a contributor come, he/she can add an adapter for twisted/tornado/gevent/whatever. async / await will not encourage people to do things cleanly, but at least it gives them a common interface, so that if they don't, it still easy to reuse.
Not having to rely on gevent's monkey patching is a significant benefit in my opinion.
Are you afraid of losing your job because you don't want to learn new things?
thank you for your answer. could you maybe elaborate little bit how exactly will C be helpful in future? i have basic understanding of C, assembly and comp architecture but i am curious is that enough or should i be devoting time to learn those because if i dont i will miss out on something
https://github.com/TommyHanusa/AmericanPolitic *Let me know how I should go about attribution for your work* I also worked off of what /u/anossov had, I'm gonna message him about attribution as well. The rar has a stand alone. the .py is messy and in 3.x; I'm sorry for the messy code, I'll fix it if its a real problem. Edit: Thanks for the gold!
Personally * people that have never learnt C tend to be architecture astronauts * little appreciation for performance tuning * cannot debug crashes in native code (unfamiliarity with debugging tools like GDB etc) * generally worse at performance optimisation than those with C knowledge * cannot port parts of application to C/C++ if needed
This is great, exactly how I imagined! Attribution is okay as it is, I mainly wrote the stuff I did for a few laughs. Thanks again. 
OP is a newbie, so /u/pygrown's approach is good. If you are going to go the websocket route, just use autobahn/wamp. But it might be overkill for OP's project and he needs to be able to know twisted/asyncio. A wamp router is extremely fast. On a small digitalocean instance I had a wamp router sitting in front of a mongo. I can't remember the exact numbers but I complete something like 2000 rpc calls in 10 seconds. This included fetching from the db and it was from another digital ocean instance in another country. 
Yup It says "This could mean that PyPI is down for maintenance, or is having an outage."
I read about it somewhere that it will be down for a long time and just wanted to confirm if that is true.
That's fair. I think `gen.Task` is mostly still around for legacy reasons and is not particularly used much these days. Debugging may still be difficult, but at least for me, that hasn't been a problem since I tend to do debugging more with `print` than with an actual debugger. I never quite understood why more libraries didn't develop around Tornado compared to what we quickly got with asyncio. But as you mention, the interoperability between the two means this is not so much of a problem anymore. I much prefer Tornado as a web framework over aiohttp (which is in my view less of a web framework and more of a... framework for a framework?), but am glad I can make use of the good work being done on the asyncio front.
Yes Dude I understand that it is running in static mode and this is not the first time. I just came to know about it so wanted to know if the event is true or not. Anyway thank you for your help
https://learnxinyminutes.com/docs/python3/ seems to have this easily eclipsed
i kinda get most of these and kinda grasp how they work. i dont now i might come back to C right now i am really, really insterested in Python, Java, Php, JS/html/css (py and java are part of my CS curriculum) and when i get free time i will learn php,js... i will tell you my plan that is synthesis of professor advices, internet research. i will read, learn, practice and do projects in this order feel free to give your perspective -(assambly(prof assembly - bloom, c - k&amp;r, comp -architecture -henessy ,html/css basics- w3school (project basic internet presentation)) -intro to CS - zelle * done -project - info system for running furniture store(selling,adding,removing,analyzing sales) -Clean Code - in progress -Skiena algorithm design 2nd edition - in progress plan on doing -Pyhton 3 oop - dusty philips -projects from https://github.com/karan/Projects -OOP and Java (part of semestar that starts in few weeks) -Info systems and soft. development (collage) during summer break -php,js,html,css (want to make my own site, where i can post projects,blogs and all that fun stuff) -ios/android dev course (got 100% discount https://www.udemy.com/ios9-swift/learn/#/) -try to get internship or get involved in student projects using python/java hope you read it and share your honest opinion. Cheers mate &lt;3
Dude, twisted is a very active, professional and clean project. There's *great* software being written with Twisted and it powers a lot of backends of fortune 500 companies. You really have no idea what you're talking about.
&gt; Only if you are installing something that that developer makes. It depends. Not true for any of the major Linux distributions.
For me it isnt working. I download your remi repo, then: python setup.py install cd editor python editor.py I get your GUI, but when i try and make a widget, it never appears. There is no error in the console Screenshot: http://imgur.com/EpAeaaZ OSX 10.11.4 Chrome 
import re re.reload(editedfile)
I am just an old hacker who manages to scrap out a living literally 😊 And I believe in using the best tool for the job. So for somethings python is better then node JS, but node JS is better for web development in my opinion. For websites, look into rest API, reactjs or angular. That way you can generate basically as static website and then implement widgets which call rest api. That way you need a lot less sever resource. You then can use wamp router like http://crossbar.io/ the database or what ever resource you want. What is cool is you can do bidirectional rpc calls. So you could do a rpc call from user A web browser to user B web browser. It goes through the wamp router. Also the rpc calls work with python, JavaScript, node, lua, C etc. So a web browser can call an application made in python lets say. It is extremely fast. I have a distributed webscraper the old one was based on pyrpc, but I am replacing pyrpc with wamp. The reason being it is quicker and I can make the scrapers is either node or python. About all I can say is everything you learn in school will be obsolete in 5 years. The exception being algorithms and maths. So when I went to school, I learned Novell networks, how to set them up on DOS etc. Things like visual basic, Delphi etc where the in thing. Python was a little known language that wierdos used back then. In school I was making stupid little programs that where web scrapers. Several of my teachers told me that I should stop playing games, stupid little programs like that won't make you money 😊 Web scraping is now called data extraction and publically admitting that you scape no longer means you will get death threats. I guess I should start calling myself a data extraction engineer 😊 So the only advice I can give is learn how to learn new things fast. Things like algorithms and maths will never become obsolete. So the url with the problems about algos is good. And teachers are usually too conservative that is why they become teachers. Find something you like to do, and figure out how to make money doing it 😊 Learn how to solve problems. 
I'm on win7 64 bit was using python 3.x and i think qt5. that being said I THINK this is what I read and used to get it to work https://www.smallsurething.com/a-really-simple-guide-to-packaging-your-pyqt-application-with-cx_freeze/
I made a little change in the editor_app layout. Maybe you can try now?
PyPI monthly download counts: tornado: 565,460 twisted: 285,275 gevent: 279,146 gevent doesn't looks a leader :)
More info: https://status.python.org/incidents/nmjnchhjddjb
`Viz` mines data directly from the [GitHub API](https://developer.github.com/v3/). It uses the following: * [`github3.py`](https://github.com/sigmavirus24/github3.py) to access the GitHub API through Python. * [`pandas`](https://github.com/pydata/pandas) in the following [IPython Notebook](https://github.com/donnemartin/viz/blob/master/githubstats/data_wrangling.ipynb) for data wrangling. * [Google Maps API](https://developers.google.com/maps/?hl=en) through [`geocoder`](https://github.com/DenisCarriere/geocoder) for location data. * [Tableau Public](https://public.tableau.com/s/) for visualizations. I might try to do some of the visualizations with Python in the near future.
Lots of packages have C optional acceleration modules. Gevent requires C to function.
Don't listen to him. He's idiot.
Nice rebuttal fuckface
For anyone of a certain age from England, Viz is not a good name. Or it may be an excellent name, depending on your point of view. https://41.media.tumblr.com/012619dfbcd56b2d5594f7964b793ea2/tumblr_mgy6k1BiuC1rbnl13o1_400.jpg
Well, there is a certain amount of time someone needs to spend at something to be good at it. Somethings take more time and while others take less time. It all depends on the complexity of the something and how fast a person can learn. As an average Joe, how long does it take to become pretty good at Python? I don't see why people find this such a hard question. If you are someone that has became really good at Python, you would say "well, I spent approximately this many hours(years) working on python to become the person I am today". It's really not that hard to answer. I am not asking you people to be fucking precise to the decimal point. Share your experience. 
The thing with gevent is that you don't have to have to write a gevent compatible version *at all*. You just write your library in the traditional blocking manner, and the user can decide if they want to use it with threads, gevent, or whatever.
Twisted is and remains a serious software project. It didn't go away, it's the basis of a huge amount of work. The fact that Twisted has taken a while to be ported to Python 3 is commonly cited as one of the problems institutions have migrating to Python 3. It hasn't gone away. Also, to be clear, *no-one* is reinventing Twisted, even if they wish they were. Modern Twisted can be thought of more as a collection of network protocols that share a programming style and *just happen* to include a reactor, rather than a reactor that just happens to ship some network protocols. Essentially, you can think of Twisted as a car and gevent as an engine: yeah, the engine is pretty damn important to the car and getting it right matters a lot, but there's a whole lot else going on too. Interestingly, I agree with you that there's a problem with rewriting every library that touches I/O. I just disagree that the problem is that we have too many I/O paradigms. I/O is *hard*, and there are lots of ways to want to do it, so it's reasonable to have lots of approaches. The problem, IMO, is that too many libraries do I/O. How many HTTP/1.1 stacks are there in Python-land? Off the top of my head there's httplib, httplib2, hyper, the mutilated freaky version of httplib that requests/urllib3 uses, Twisted's stack, Tornado's stack, aiohttp's stack, and gunicorn's stack, and I'm sure there are far more that don't come to mind right now. What possessed all these tools to write their own HTTP/1.1 stack? Well, there wasn't one available that didn't already have its own opinions about how to do I/O. This is a really dangerous pattern, because it *forces* us to rewrite the stack each time we move to a new concurrency paradigm. Gevent doesn't fix this by the way, it also has this problem, and if you don't believe me you should dump httplib2 into gevent and see how long it takes before it crashes in a heap (not long). What we as a community need to be doing is to write synchronous stacks that *don't do I/O*, and then write thin wrapper libraries around them that work well in the specific concurrency and I/O model being used. And to prove I'm putting my money where my mouth is, [that's exactly what I'm doing with HTTP/2](http://python-hyper.org/h2/en/stable/). You'll note [in the examples](http://python-hyper.org/h2/en/stable/examples.html) that you can use the same protocol stack in whatever concurrency model you want by hooking it into the I/O that the concurrency model likes. This is not a new idea: plenty of C libraries do exactly this. Protocol stacks are simple and have a good definition of 'done', so it really is a waste of effort to keep rewriting them. However, writing helpers for a specific style of concurrency? That's not a waste.
&gt; You should make a neutral lib, handling the protocol and the workflow, and you provide hooks. Yup. It baffles me that we haven't been doing this in Python already. This is exactly what I'm doing with [HTTP/2 in Python](http://python-hyper.org/h2/en/stable/), and I'm really optimistic to see it getting into lots of different projects.
Yesss lol
Almost the best thing about asyncio is the event loop is also a defined interface; so you can write other loop implementations under it, wrap them in the API, and then give that loop to all of your other asyncio code. E.g, [quamash](https://github.com/harvimt/quamash) provides this for Qt's event loop.
&gt; C is important mostly because all of the other languages you will use are themselves implemented in C This is not true. I'm not even sure what you mean by "implemented in C". In the case of CPython the PVM (Python Virtual Machine) is implemented in C. In the case of other languages: The VM or compiler can be implemented in any compiled language.
I don't understand your comment about different use cases. All of those libraries depend on network IO. asyncio is supposed to be the new, easy to understand standard for working with network IO. An executor sidesteps integration of an IO library with the asyncio event loop. If I need to use the CPU for some task, OK, an executor makes sense. If I'm doing IO, I would expect to be able to do all my IO through my event loop. There's no reason I should have to treat it as a blocking operation.
Thank you so much for the feedback and useful info! I'm working on it a lot, and for sure, the usage experience of other developers is really important. This evening I will fix the issues you noticed. ;-) In order to exit the app, just stop it by the console and the browser tab.
They approach network IO in different ways though, at least Django, Flask, Bottle (they're mostly using the pre-fork model by default). But as far as the network IO in the loop or in an executor, I picture this the same as the Python2/Python3 migration - not everything will be supported right away and in many of the cases you list the framework authors need to do work to make them compatible. It's really unreasonable to just expect that almost 30 years of code written with only the synchronous concern in mind would just work asynchronously. As you know, if one bit is async the rest needs to be written that way. One bad call in any lib and you're now blocking the whole loop. Hopefully we will find ways to better implement asyncio, but if people aren't trying it by writing libs like the OP link or finding annoyances with the event loop - we won't make that progress.
This is bad ass. Looking forward to learning a ton from the source code. 
Not python related.
That makes a lot more sense. I work only in the realm of experimental control and data analysis for physical science and very rarely on actual production software. I work with python and with many colleagues who use python for data analysis and number crunching. There's an extensive bit of optimization involved, performance profiling, algorithmic design, etc, but at the end of the day this tends to remain as internal scripts and Jupyter notebooks rather than working into a full fledged software package or web service. So I understand how I wouldn't be an ideal candidate for your line of work.
I'm pretty sure there's a fancier way, but can't you just calculate a hash of the file and if it changes you just use "reload(foo)"?
I also work in science, but I write commercial analysis and experimental tools. I used to do scientific programming / experimental work / data analysis / postdoc shit, but now because of where I work in the ecosystem, I lie in a different place on the repeatability &lt;-&gt; reliability and exploration &lt;-&gt; throughput data analysis axes 
Way to move the goalposts
So quick question here. I know the python driver for cassandra, already supports libraries like, gevent, eventlet, and libev. It also has future support, pooling and what not. (provided you bypass cqlengine, and use straight up cql). This is essentially an example of how to get those benefits when using the object mapper(cqlengine) right?, or am I missing something.
If you are about to ask a question, please consider r/learnpython.
Thanks!
Thanks! I'll go look into it.
Exactly. There aren't any async calls for cqlengine yet (which I co-wrote). The native driver works just fine with async calls otherwise. If you go this route, you still get the benefits of the driver's pooling.
&gt; Unless a library is very poorly designed and is hiding IO deep within function calls that should have nothing to do with IO, I don't think it's that difficult to determine where IO is going to happen. This is ridiculous. Logging can happen deep within function calls, and it does not make a library "poorly designed". You're blaming completely reasonable Python code for gevent's failures. If changing the log level can change the behavior of a function because the logging I/O is monkeypatched, the conclusion to draw from that is "monkeypatching is bad", not "logging is bad".
&gt; I'm not quite sure what gevent would be without "optional" monkey-patching. Something where you have to yield explicitly but don't get any of the benefits of a library designed for explicit yields? gevent's entire point is to remove the need for explicit async yields, and this has nothing to do with the monkeypatch system. You just use its [API](http://www.gevent.org/gevent.socket.html) directly. The calls are almost entirely identical to the stdlib versions and it's a joy to program in. This is pretty basic stuff to understand before a project should make any major investment in gevent. &gt; When you monkey-patch, you'd like to believe that everything automatically works the same way except asynchronous. But there's so much code that just doesn't expect to be asynchronous, or depends on details of the standard library that gevent doesn't emulate 100% the same way. you have to be careful with native C libraries but other than that I'm not aware of any such standard library issues; a major portion of Openstack, which is an immense ecosystem of code, is built on eventlet monkeypatching. The only real problem it has is the incompatibility with C libraries.
Any minuscule change to the file will cause the hash function to return a vastly different number. Another option would to check the date of the last modification. 
There is no avoiding the responsibility of knowing where IO is done. asyncio forces you to pay attention, which is not a bad thing, but also not something that's all that difficult to do. If I add a network IO to my logging pipeline in a Gevent-based project, I can and should find everywhere that logging is used to make sure it's not going to cause any problems. Now look at that same situation with asyncio. I start logging to stdout, then I decide I want to push them out over TCP. I now have to modify my entire codebase to add yields/awaits to every single logging call. That's sounds like a nightmare to me. What if I then disable TCP? Do I then have to edit all that out again?
Thanks for the link that; a well written clarification of the benefits and dangers of monkeypatching.
how do i use pyinstaller? i installed it using pip when i type pyinstaller mycode.py it says "failed to create process" help?
Still annoying, but you can just type "reload(foo)" That IPython extension that the other poster mentioned sounds cool though 
My company is going with azure I think. I never heard of netezza. Should I intervene?
My company is in search of a database and might go with Microsoft azure. I was learning postgres and sqlalchemy to load data from pandas to a database just last month. Any idea if a large company can still use postgres? We have many clients who all send us data and then we generate internal data too. Over a thousand sources all in different formats etc..
Not sure about that one. This one is definitely pretty easy and short coz its an intro. edX I would imagine would have a longer course more like a uni course rather than an interactive tutorial.. 
For anyone not of a certain age from England, what is the connection of that comic to the word "Viz"?
Viz was the name of the magazine.
Such great advice for other people (like me) who are learning. Thank you for sharing! 
Hi, I'm working on this GUI library that renders in the browser: https://github.com/dddomodossola/remi Furthermore it is equipped with a WYSIWYG designer https://github.com/dddomodossola/remi/tree/master/editor Another alternative is https://github.com/zoofIO/flexx. Or maybe django, flask ... 
By default you don't get async code, you get sync code that can execute at the same time other sync code does. None of the libraries can take full advantage of streaming and parallel evaluation until they are aware of the underlying executor and event loop model.
 import sys sum = 0 if len(sys.argv) &gt; 1: for fn in sys.argv[1:]: with open(fn) as f: for line in f: try: sum += float(line) except ValueError: pass else: for line in sys.stdin: try: sum += float(line) except ValueError: pass print sum
I remembered more perl magic.
Feel free to keep updating :). This is fantastic. Thank you so much
`aiopg` uses `psycopg2` on backend and just provide convenient asyncio API. You use same driver in both async and sync cases.
Tried it again. Thing is. Neither will work. I have created exes with pyinstaller and cx freeze. But neither worked. They do not run. Nothing ever opens. The exe is made. That is not the problem. The problem is it does not run. 
No, for your use case it sounds like an interactive notebook works perfectly. But if you ever want to create something with lots of classes, each with tens of methods, you really should put each class in a separate file, and the best way to organize and debug all of that is a traditional IDE. For me the magic of jupyter/pandas is I don't have to implement my own plotting methods and wrangle with matplotlib directly. I just store data as pandas dataframes, import my module into jupyter, and then do object.data.plot() to get beautiful inline plots. 
are you able to make an exe of something that does NOT use pyqt, just a simple python script that could be ran from command prompt? I used pyinstaller for tha tpreoviusly and had no issues. If you can't do an exe w/o QT then throwing in QT will be a nightmare
When learning for example C, you catch some knowledge about memory allocation and other hardware stuff for using in C. Those things can be practical to be aware of when using other programming languages. But you really don't have to learn C to know those things. I suggest you just go on learning Python and then pick up some hardware knowledge on the side.
Are you saying I can save that to a file in the editor and run it and it'll work, or do you mean that if I type this each time in the console, it'll work? Edit: Sorry if this is a stupid question. I thought "print('hello world')" wouldn't work because **[the hello world page on qpython.org](http://wiki.qpython.org/doc/hello-world/)** has just the more complicated version.
Like dirtyfreddie said, you need the configure method e.g. from Tkinter import * import random def update(): uptime = "Tempo acceso: " + str(random.randint(0,10)) a.configure(text = uptime) a.pack() root.after(1000, update) root = Tk() uptime = "Start..." a = Label(root, text = uptime) root.after(0, update) root.mainloop() 
Nothing really compares to "best looking" like Qt. Gtk3 isn't too bad either, but I'm not sure how cross platform that is compared to Qt. It's difficult to pass along a Qt application though. If you use PyQt you need to have other people also have PyQt installed, which requires Qt, and PyQt, and there is probably some compiling involved. At the very least there are some dependencies that you won't get from Pip. In my recent opinion, even though I think web apps are kind of silly, at least they're pretty cross platform. It's much easier to require somebody to install Flask or something through Pip. Then find some small light weight grid css framework, something like boot strap, and everything kind of works. The only problem is you lose out on the way things like "signals" work in a GUI framework, and you fix that with javascript. It's not terribly involved, but it's kind of the easiest solution to just getting something up and started that interacts with Python. At least you get the added benefit of being able to throw a live demo on a server. For what it's worth, that's essentially how Ipython/Jupyter works with the notebooks. It creates a local tornado server and then renders objects.
Great! It worked like a charme! But.. a question...i did't really understand why a "root.after in the function and one .after of 0sec otside the function, can u explain please?
The part on Bayes' is really confusing. In it, are we assuming that we *dont* know that 40% of the women like green? I've re read it a few times, still can't grasp it. What does "extend the problem" mean? Are we picking a sample from the exact same population? And is everything we know and worked out about the population still a given? 
No, we still know that 40% of women like green, as that what given in the problem. By extend the problem, I mean that we are picking up where we left off in the first problem --- all of the information we started with and the information we found out still exist.
I guessed from your time/sleep/while code you want some kind of repeating task. Tkinter provides alarms and alarm handlers for this task. Just read more in Tkinter docs/tutorials. root.after(0, update) Sets the first/initial alarm after 0 ms with alarm handler update(). root.after(1000, update) Sets next alarm after 1000 ms with alarm handler update(). Meanwhile root.mainloop() loops forever and checks whether a alarm point is reached and if it is, then the alarm handler is called 
What are you talking about? Gevent is a complete hack birthed from the fact that the python core guys screwed up and never put in a proper 'blessed' asynchronous event loop into the stdlib a long time ago (they could have done it in 2005 when they introduced coroutines). Of course, GvR himself states that he didn't understand the issue (or Twisted for that matter) until he had to do a lot of async programming at Google. I only wish it could have been added pre-3.0
can you save a python / .py file and then try to run that from command prompt by... cd'ing into the dir where its at typing.. python ./name_of_file.py if that works maybe try using pip to install cx_freeze and pyinstaller to see what happens?
What did you use in its stead?
[removed]
Does one of you keys contain any weird characters? Also have you verified that awscli is working with other params/tried on another bucket?
&gt; failed to create process It's common problem when You rename any folder in path to Python. I recommend to start with fresh Python install or restore proper path.
Have a look at https://docs.python.org/2/library/fileinput.html that is the closest Python substitute for perl &lt;&gt;
I suppose I should have clarified this, but I have run the command "chcp 65001" and seen it successfully apply but the issue persists. I also verified I was using a console font within the command prompt. I've tried running these commands through the normal Windows command prompt as well as CMDer (which is what I generally run PS/batch scripts through,) to no avail.
thanks for the contribution!
there's weather option (more of an alias) to google engine now.
It doesn't mention monkey-patching at all. It's more of an argument for explicit yielding. It's well written but I didn't ultimately find it convincing.
It is, look...he has pycharm installed!
I seem to have fixed it by simply updating the AWS CLI (just run the MSI installer over your existing installation.) That was like, the first thing I tried, but I just did it again and now it's working on the problematic buckets. Leaving this post up for Google posterity.
I added more "User Type" filters, thanks for the suggestion!
Thanks!
Thanks! I'll be adding in the 1-, 3-, and 6-month rolling stats to the source soon.
Looks like Viz is a keeper.
Actually it does. That's why you have many libs like : https://github.com/tmc/gevent-zeromq. It's just stuff using the stdlib that don't need to be adapted. But it just mean that they will work without, not that they will be faster. Indeed, a lib written in a blocking fashion is never written in a way that maximize parallel calls.
* Check out r/learnpython * Post your code
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
'line4' never gets run in the loop, but manually it works. specials = ',' for line in data: # data is a set of lists print line line4 = [] line2 = [] line2 = [value.replace(specials,' ') for value in line] line4 = (" ".join(str(x) for x in line2).split(" "))
looks like your indentation is all out of whack - `print line` should be indented with four spaces to be in line with `line4 = []` and the other grouped lines.
Correct, I know this. In my editor that was OK. Still the problem exists!
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
No way? Javascript front end making ajax calls over localhost would not be unresponsive. Sure, there's a little overhead in serializing, but it wouldn't be noticeable. Not that javascript is a *good* way to imbed a repl (it would be a lot of unnecessary work) but I'm just speaking to the responsiveness claim.
That's a generic message. I think a more basic question like this (essentially, "What's a good library for X?") is more suited over there. Alternatively, did you try [searching](https://www.reddit.com/r/Python/search?q=sms&amp;sort=relevance&amp;restrict_sr=on&amp;t=all)
It's not in python 2.7, I said that in my post.
Thanks. And yes, I tried searching for a bit and didn't get anything python 3 or anything that actually used sms instead of a sms gateway that costs money. 
Seems like this would cause more problems and confusion than it attempts to save.
Great editing from walterl at https://github.com/alexwaters/python_2.x_snippets/pull/1 These were my key takeaways from his awesome feedback: * Design a functional module by starting with the key algorithm and building outwards. This reminds me very much of laying tile; start with the big pieces. * List comprehensions and string formatting make things so much more readable. * Print statements should at least be separated out into their own container. It may be even better to create logging functionality and pass arguments to manually force debug statements and feedback. * The file is far more modular/usable if it works as both an import and standalone script file. Wrap top-level statements in a is __ main __. * Error handling should either be done in the implementation or potentially in a separate file wrapper to isolate logic from quality assurance. * For a small amount of data, overwriting the file with the 'w' flag - and passing a single string with items separated by newline tags is ideal. For larger amounts of data, serialization in newlines and appending to the file may be more optimal. * Block """ """ comments for docstrings, # for commentary makes it more readable * I need to not forget the python shebang and encoding
Thank you, I will try it out
&gt; Sadly, development and support for Stackless Python has slowed down in the last few years. It however astonishes me that the core idea of stacklessness hasn’t been embraced by C Python even yet. Is it possible that some of this is due to lack of understanding? Apparently lots of people don't know what "stackless" means -- otherwise there would be no reason for this post. Also, a significant amount of the development work is aimed at performance these days. How does Stackless compare, performance-wise?
It's "CPython", not "C Python". Anyway, yes, upstream lost a chance to improve the architecture by ignoring the Stackless fork.
I use pandas for all merging, aggregation etc. Then I use sqlalchemy to write my dataframe to a postgres database which is connected to tableau. Is that a decent work flow? I don't know sqlalchemy or postgres so I literally just store data and do everything in pandas. I don't think tableau connects to sqlite though or I would use that. 
I know it may sound totally out of question for some devs but depending on the complexity of the app he wants to build, tkinter may be a suitable option, one can relatively easily make it it work on both py2 and py3 and it's in the standard lib anyway. I must admit it's not pretty at first but with a bit of work you can make it look good and most of all, it gets the job done. Example of a nice-looking app : http://web.mit.edu/jabbott/www/gallery/GUI.png
I totally misunderstood stackless this whole time. &gt; the C stack has been decoupled from the python stack To me that means there are now two stacks instead of one. It should be called extra stackful.
Link is not working for me. edit: it's back up
You could do this without sk-learn just by using the cor function within pandas. DataFrame.cor() that will give you a new dataframe that shows you the relationship between all of the variables. 
Might try that tommorow then.
apparantly it got killed off by Greenlets and PyPy http://stackless.readthedocs.org/en/latest/stackless-python.html#history &gt; How does Stackless compare, performance-wise? I'd imagine that it'd be around the same performance scaling as asyncio (but faster) as they are fairly similar concepts (hop around between tasks, but can't run two tasks at once. Stackless can save and restore the python stack, while CPython saves it a bit more weirdly) I might be completely wrong however, I don't read the internals of either interpreter that much
Check this out, ocr.space
this is incredible
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
well in that sense a heap is mega-stackful
Doesn't a stack have order and a heap does not?
[Seems also available on PyPI.][1] [1]: https://pypi.python.org/pypi/python-html-assert
Woah, it really is a hassle to use PyQt huh. I'll do more research on Qt and other GUI toolkits.
Nice looking GUI there. Is that PyQt? Uhm, how about Tkinter?
What is this?
Thank you :)
you said &gt; Gevent continues to be popular and used and it didn't need to be reincarnated a bunch of times and forced into the standard library, just let it go. then people pointed out that Gevent isn’t the oldest of the bunch, and not the most popular → if anything Gevent is the less popular reincarnation someone pointed that out and instead of admitting you were wrong you moved the goalposts: &gt; Gevent needs to be compiled, a lot of downloads aren't going to be through PyPI.
What font is that?
never heard someone say that they only go through PyPI if there’s no compilation involved. either people do everything with pip and virtualenvs or they use system packages for everything.
Actually, after reading the code some more, this is the worst thing I have ever seen.
of all the "learn python via X" and "learn python in X minutes" claims, this one seems the most sane. As such, I've sent the picture to all my friends who were planning on learning programming, so they can learn as quick as possible via really tall lines with some text around it
Do you have a blog that people can follow by any chance?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [Implementation of IGT's secure enhanced validation algorithm (x-post \/r\/python)](https://np.reddit.com/r/programming/comments/444pbx/implementation_of_igts_secure_enhanced_validation/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
have you looked at opencv tutorials http://docs.opencv.org/2.4/doc/tutorials/features2d/table_of_content_features2d/table_of_content_features2d.html#table-of-content-feature2d 
I'm glad to see your interest in remi. Do not hesitate to contact me in case of help. I'm really interested in your usage experience in order to improve this library. And don't forget to try the graphical editor, it could help. The editor is a work in progress, but without beta testers, I cannot improve it. ;-)
It's a deal. I'll get back to you then.
True. Still the worst thing.
Monaco
Hmm I'm not so sure about that. But maybe you can find the answer here on this blog post: blogory.com/.......
The satnog web site appears to be down at the moment? Is this correct?
a heap has many orders of 1.
Maybe the idea would work to show how much more complicated C++ or whatever language is, but on its own the mind map is incomprehensible.
I've more or less given up on native desktop GUI applications. Instead I create web based apps using jquery/jquery UI and cherrypy.
This is solving a problem which doesn't exist.
This is actually great. 
Pretty cool, how did you make the graph thing (spider chart or mind map?)?
Took long enough!
Yes it is an argument for explicit yields; but the thing he is contrasting against is implicit yielding which is what gevent's monkeypatching causes. &gt; implicit coroutines: Java’s “green threads”, Twisted’s Corotwine, eventlet, gevent, where any function may switch the entire stack of the current thread of control by calling a function which suspends it. and &gt; A system that presents “implicit coroutines” – those which may transfer control to another concurrent task at any layer of the stack without any syntactic indication that this may happen – are simply the dubious optimization by itself. His point is that if any line of code could be yielding (e.g. the log message that is now networked; something inside a function you call) then you are back to the same cognitive load as trying to reason about pre-emptive threading. I had only recently started using gevent and I'd had a niggling "this is too good to be true" feeling. I'll probably continue using it in the near term, but now that vague uncertainty is clearer: I better know the beast I'm dealing with.
if you don't need a really pythonic way, you can use subprocess to call tesseract as you would do normally in terminal
I had no problem accessing it. 
You might like toga and flexx: https://github.com/vinta/awesome-python#gui btw, I personally find that web UIs involve a lot of overhead and complications that I'd rather ditch if I could (ie: javascript, build tools, js libs, testing, browser debugging,…).
Even for a synchronous library, it would be very unusual to do a() b() c() in a single function where each did not rely upon the result of the previous output. For a web crawler, what I'm writing is: pool.spawn(http.get, *args) pool.spawn(http.get, *args) pool.join() Unless my http library is poorly designed, I can also have it just grab headers, then spawn new greenlets to load the content if I need more precision. Could you get more performance by specifically designing for async? Sure. But it's rarely necessary.
It's not self-referential. As someone else said, "|" means "given" or "conditioned on the knowledge of". So P(A | B) means "the probability that event A happened given that we know that B happened". It's a statement of what knowledge you're bringing in to the probability. So when you see P(A | B) = .. P(B | A) ... It's not referencing the same things. In the first case it's A, conditioned on knowledge of B and B conditioned on knowledge of A alone. In other words "A | B" is its own term. If you replaced "A | B" with x and "B | A" with y, you'd get: P(x) = P(y) P(A) / P(B)
Is learning this a pre-cursor for machine learning? Specifically the monte carlo simulations with a tree like structure with the leaves being probabilities of different outcomes?
If you're interested in code optimization, [this is a great article to read](https://www.python.org/doc/essays/list2str/). It'll give you some direction as to what's going on under the hood of python. You can do your own testing to answer some of your questions, though. Look into the `timeit` command, for example. It's simple enough to write different functions that do the same thing and test them to see which is faster. You might also check out CheckIO.org, which has lots of coding challenges. Those will provide some good examples and motivation to learn how to write more efficient code (and learn some classic and useful algorithms). 
&gt;What is "easier" for python to do, sum two numbers or multiply two numbers? There's going to be almost no difference, unless perhaps you're using really really huge numbers, when multiplication *might* be noticably slower. &gt;And how do I find those answers? The most obvious way is to try it out and time the solution. Python has a module called `timeit` designed for this purpose, so if you want to compare the "multiply 5 numbers vs search a list of 10 numbers", you can do: &gt;&gt;&gt; import timeit &gt;&gt;&gt; timeit.timeit("52*324*12*55*67") 0.025991704023908824 &gt;&gt;&gt; timeit.timeit("42 in [1,2,3,4,5,6,7,8,9,10]") 0.2828988250112161 Timeit will repeat each one a number of times (a million by default), so this tells you that you can do a 5 million multiplications in under 3 hundredths of a second. Conversely, finding a number in a list takes ~10 times as long (if it's not found - it's going to depend on how early it finds the number). If you use ipython, there's also a handy convenience magic command for this: %timeit. In Ipython, you can write: %timeit 52*324*12*55*67 100000000 loops, best of 3: 18.5 ns per loop You should also note that this level of optimisation is not going to matter much unless this is in a **really* tight loop. Computers are fast: a million multiplications is done in an eyeblink, even in a languages as high level as python. More important is generally *algorithmic* improvements. Eg. in your prime number example, you're doing trial division of every number in sequence, but this could be radically improved by using something like the sieve of Erastothenes, and this will make a **far** bigger impact than any microoptimisations. (And there are even more advanced algorithms if you want to go deeper into the maths, but the sieve is a good starting place, since it's fairly easily understandable).
If you want to know if an algorithm is the most efficient, that's a bit tricky. For well known problems like finding primes you can at least know that an algorithm is the most efficient KNOWN algorithm. You find that out by looking at what the math theory says.
Me too. Their site works perfectly fine for me
I sense conflict within you. Perhaps your flare or your comment need adjusting :-)
is it selenium? Cause I hate selenium!
Oh boy. Class numero uno, and you're already swinging for the fences. Trying to punch way outside your weight class. Please don't interpret that as me responding negatively, I think it's great that you're thinking about programming this way. It's just that there is a lot of baseline knowledge needed to really understand the answer to the question you asked. To start though, there are 2 very, very important things to keep in mind: **Premature optimization is the root of all evil**. Don't start trying to solve problems you don't have. It makes it a nightmare to debug, makes it hard to determine if your output is even correct, and is difficult to modify if you need to. After you write the code, you might find that it already runs fast enough for everything you need it for. This brings us to the other thing to keep in mind. **There is no speed boost bigger than when your program first goes from not working to working.** It doesn't matter how optimized your algorithms are, how perfect your coding techniques, all the little time saving tricks you used if the code doesn't run. Getting it working is more important than being the best. With that said, it's still good to learn about optimization, it's just common for new programmers to go way overboard. You were close when you used the word complexity, a big concept for optimization is [time complexity](https://en.wikipedia.org/wiki/Time_complexity) of an algorithm. This focuses on how much longer algorithms take when they need to compute a bigger *n*. Using your prime number example, a very simple program just divides *n* by every integer smaller than it. If you try and find out if 101 is a prime, it will take a certain amount of time. If you try and find if 101,001 is prime, you need to divide by 1,000 more numbers, it's going to take 1,000 times longer to computer. Well, you actually don't need to divide by any even number greater than 2, so instead of dividing by every number, just divide by every other number, so now it will only take 500 times longer to compute. We don't need to divide all the way up to 101,000, either. Once we pass the square root of 101,001, we know since their is no smaller number that is a factor of 101,001, there's no larger factor, because any 2 numbers above the square root of 101,001 multiplied together would be bigger. Now it's only going to take about 30 times longer to compute for that increase. Trust me when I say this, focusing on reducing the number of times your algorithm has to loop is almost always the easiest, and best, way to make your code run faster. [Use a profiler](https://docs.python.org/2/library/profile.html) to figure out what is actually taking up time, and then focus on just making that code run less. You use the example of multiplying like 5 numbers together, or checking if a number is in a list of 10. If those 2 methods are different ways of accomplishing the same exact thing, the correct answer is almost always just figuring out how to run the code less. The fastest code is the code that never runs. But okay, you're trying to dig deeper, and this rabbit hole goes deep. Let's talk about Python actually doing work for a given line of code. Well, python is an interpreted language. The code you write gets turned into bytecode, which the python runtime executes. Each little thing you write might get turned into a varying number of lines of bytecode. Even if 2 lines of code are different (even a bunch of lines), if they are functionally the same, they might even get turned into the same exact bytecode. Each line of bytecode takes a certain amount of work to complete. The Python runtime takes the bytecode, and turns it into instructions the computer can understand. Each line of bytecode can turn into a lot of lines of instructions. Each of those instructions takes a certain number of clock cycles, and different instructions take different numbers of clock cycles. Try and focus on just the algorithms though, there are a lot more focused, higher classes on this, and it's better to get the baseline of Python before digging too deep. Oh, and for the last question of &gt; How do I write the most efficient program and how do I know it's the most efficient? A lot of math and proof writing. Primality tests are a very active area of research right now, no one has proven an existing algorithm is the most efficient. It requires proving that a given method works, and requires proving a more efficient method could not mathematically exist, which is very difficult.
I always thought xlrd was one of the better readers. Maybe check out PyExcelerate.
Some people are remarking that it took so long for this to happen. The following comment by one of the core Scrapy developers provides some of the justification for it. https://github.com/scrapy/scrapy/issues/263#issuecomment-83108351
Yes pls
I had saved this post to read it later. Now I'm going to read it and it just results in a blank page :( Looks like it's still readable if you View Source, for whatever reason. Or if you view Google's Cached version in "Text Only" mode.
How pytest, unittest, and nose deal with assertions. The job of the test framework to tell developers how and why their tests failed is a difficult job. In this episode I talk about assert helper functions and the 3 methods pytest uses to get around having users need to use assert helper functions.
Currently it doesn't.
Class based OO and especially bound member functions play a bigger role in Python. They're not anonymous indeed. E.g. callbacks / event handlers can be done like this: HTML ==== &lt;script src="__javascript__/hello.js"&gt;&lt;/script&gt; &lt;h2&gt;Hello demo&lt;/h2&gt; &lt;p&gt; &lt;div id = "greet"&gt;...&lt;/div&gt; &lt;button onclick="hello.solarSystem.greet ()"&gt;Click me repeatedly!&lt;/button&gt; &lt;p&gt; &lt;div id = "explain"&gt;...&lt;/div&gt; &lt;button onclick="hello.solarSystem.explain ()"&gt;And click me repeatedly too!&lt;/button&gt; PYTHON ====== from itertools import chain class SolarSystem: planets = [chain (planet, (index + 1,)) for index, planet in enumerate (( ('Mercury', 'hot', 2240), ('Venus', 'sulphurous', 6052), ('Earth', 'fertile', 6378), ('Mars', 'reddish', 3397), ('Jupiter', 'stormy', 71492), ('Saturn', 'ringed', 60268), ('Uranus', 'cold', 25559), ('Neptune', 'very cold', 24766) ))] lines = ( '{} is a {} planet', 'The radius of {} is {} km', '{} is planet nr. {} counting from the sun' ) def __init__ (self): self.lineIndex = 0 def greet (self): self.planet = self.planets [int (Math.random () * len (self.planets))] document.getElementById ('greet').innerHTML = 'Hello {}'.format (self.planet [0]) self.explain () def explain (self): document.getElementById ('explain').innerHTML = ( self.lines [self.lineIndex] .format (self.planet [0], self.planet [self.lineIndex + 1]) ) self.lineIndex = (self.lineIndex + 1) % 3 solarSystem = SolarSystem () 
For the first release: - One for beta - One for the actual release Seems reasonable to me.
They also follow the standard Python packaging structure. Importantly, you can execute `python` from the root and do, e.g., `import flask.app`, etc. If you have the `.py` files in the root, you actually have to go *up* a directory when you run `python` in order to import items from the package. Can you come up with a justification as to why they're "ugly"? And a justification as to why "beauty" is a good enough reason to break packaging guidelines?
Some would say it is ugly to have to prefix every command with `insideout` just to have `.py` files in the root directory, you know. I already specified the technical restrictions: `pip` doesn't work. Here's another: You can't execute `python` from the root directory and import the package; instead, you have to go *up* a directory.
Not yet. But it'll have to be done to convince people. And I'm afraid I am the one that has to do it. Somehow I hope that people will just discover themselves. But of course, that's not how it works.
I am not wrong or right. This is just a possible alternative. I just asked for help and educated opinions about `insideout`. Until now, in all your malevolent comments, you don't describe a single technical aspect or problem that you see in the project. &gt; If everyone... Who is this "everyone" you are referring to? Is this you? Are you actually saying that you find that huge list of configuration files in `pypa/pip` something beautiful to show up front to other github users? I will not respond to any impolite comments.
They're not really the same thing though, are they? I've been using splinter (a wrapper around selenium I think) to interact with websites that don't provide an API, and it hasn't been so bad.
Hi, the link don't work.
Remi looks awesome! Definitely going to try to include it in my projects I'm working on. :)
&gt; What is "easier" for python to do, sum two numbers or multiply two numbers? It hands off to the CPU. Addition is a little faster, but we're talking nanoseconds here. &gt; Is there a difference for python to check wether a variable is equal to some value or smaller than some value? Equality is easier because it's comparing bit patterns. Again, we're talking nanoseconds here. In C/C++, there's a mild difference between for(int i=0; i != 10; ++i) { and for(int i=0; i &lt; 10; ++i) { but this is absolutely never the cause of any bottleneck in your program. If it is, you're doing something inside the for loop that's literally less work than counting. &gt; How much easier? A few clock cycles. You know how your CPU runs at 2-4GHz? That's 2-4 billion clock cycles. If you can save 1 cycle, then you've improved by 0.25-0.5 ns. Completely pointless to worry about. &gt; Is it "better" to multiply like 5 numbers together or to check whether a number is equal to one of ten different values in a list? I can't imagine how multiplying 5 numbers together is the same as checking whether a number appears in a list of ten. This is sort of like asking is it prettier to pet a cat or to drink herbal tea. &gt; We all had to write a little program in class and looking at my fellow students I noticed that people had vastly different things that all basically did the same thing. At that moment I became somewhat interested in optimization. Our program was meant to look for the nth prime number. All programs basically checked every number and divided it by every number smaller than it and if it was prime, a counter variable was increased. If this counter variable ever reached n, it would print the number that was being checked. OK, here's the deal. The way you optimize that algorithm is by knowledge of mathematics. For example, if we're asked to write a function `IsPrime(n)` then we know the first line will be: def IsPrime(n): Now there are a few approaches one may have to start with. For example, we could start just by checking the first few: def IsPrime(n): if n == 2: return True elif n == 3: return True elif n == 5: return True elif n == 7: return True And so on. Let's look at what this does. It compares `n` to `2` - a cheap operation. If it is `2` then it returns `True` and the function stops right there. So for `n = 2`, it's a single comparison. If it wasn't `2` then it compares to `3` and does the same thing. For `n = 3`, it's two comparisons. So on, so for `7` it's 4 comparisons and then after that we may proceed with some algorithm. Comparing that to another way to write the same thing: def IsPrime(n): if n in [2,3,5,7]: return True Does the same thing. How do we determine if `n in [2,3,5,7]`? Well, we make a temporary `list` holding `[2,3,5,7]` then we loop over it and compare the iterator to `n`. If we find that it is `n`, then we stop the iteration and determine that the `in` statement yielded `True`, so the conditional code is performed -- `return True`. You'll notice that this *sounds* like it does much more: build a list, store a list, create an iterator, compare the iterator, and increment the iterator if the comparison was false. Feel free to use the `timeit` module to see whether this is faster, slower, or if it just doesn't matter. But on average, neither of these is a good approach. Instead, the first line in our `IsPrime` function should be able to turn 50% of `n` values into a quick exit. Let me show you: def IsPrime(n): if n &gt; 2 and n%2 == 0: return False Bam. Now every even number, starting at `4`, is immediately not prime. This would make the function much faster if you were running over a bunch of numbers like this: for n in range(1000): print("{}: {}".format(n, "Prime" if IsPrime(n) else "Not Prime")) Since we'd be looping through all numbers in 0 to 999, half are even so there's no point in trying to figure out if they're prime or not -- even numbers are always divisible by 2 so they aren't prime. This is what I mean by being familiar with mathematics. You can try to rewrite algorithms and stuff and worry about comparisons versus list construction and iteration, but it will not help you turn an O(n^2) method into an O(n) method nor turn an O(n) method into an O(log n) method. *That* is what you need to do in order to write a good algorithm/method, and *that* is where mathematics comes in. For example, with the primes, you could try this: def IsPrime(n): if n == 2: return True if n &lt; 2 or n%2 == 0: return False # Now we've handled 0, 1, and 2, and all evens; only odds &gt;= 3 remain for x in range(n): # Consider all numbers smaller than n if not IsPrime(x): continue if n % x == 0: return False return True Let's run through `n = 3`, we compare to `2` it's not so we continue. It's not `&lt; 2` but I have an `or` here now so we then try `n%2` which is `3%2` or `1` which is not `0` so we move on. Then we build up `for x in range(n)` which will run through 0,1, and 2. It will only see that `2` is prime, so for `0` and `1` we hit the `continue` then for `2` we have the `if n%x == 0` line actually executed (this is the only time in `IsPrime(3)` that that occurs. We again see that `3%2` is `1` which is not `0` so we go back to the loop, but we're at the end so we go down to just after the loop which is a `return True`. Which is correct. Now you may think it's silly to ask `IsPrime(0)` and `IsPrime(1)` each time so you change the loop to `for x in range(2,n)`. Brilliant, now you reduce two iterations. But this is not particularly great as it's a constant time saver. A time saver, yes, but only constant. Its value is like 1/t, pretty big when the time is short but meaningless on long operations. Instead, if we turn to math we can look at composite numbers. If we look at 24, that's 2\*12 or 3\*8 or 4\*6 or 6\*4 or 8\*3 or 12\*2. Notice the symmetry here. Notice how it flips at 4\*6 to 6\*4, and how the mid point between 4 and 6 is 5. Notice how 5^2 is 25, which is greater than 24. Notice then, that the factors of 24 can be found by considering numbers up to sqrt(24) because if you found a factor greater than sqrt(24), the other factor would be less than sqrt(24). So, we can instead have this as our loop: for x in range(int(math.sqrt(n))+1): Note that `int(x)` rounds down, and here `sqrt(24)` would be 4-something so `int(sqrt(24))` would be `4`, however `range` doesn't include the last number and we want to include the last number therefore we have the `+1` to handle that. It ends up being `range(5)`. Now we won't run through as many numbers when we do encounter a prime -- for composite numbers this will have found 2\*12 right off the bat. If we think about 15, it would find 3\*5 before 5\*3 so we don't need to worry about it. It goes until `x = 3` then hits the `return False`. But... we've already handled whether it has a factor of `2` so we actually can just start at `3`. Then we may consider for x in range(3, int(math.sqrt(n)) + 1): That's a constant time saver, but still useful. However the range for, say, `100` would be `3` through `10`, meaning 3, then 4, then 5, then 6, etc. We know that since the even numbers are all multiples of 2, that if the number is a multiple of, eg, 4 then it also a multiple of 2. This means we don't need to consider any even numbers. With that in mind, we can actually write this: for x in range(3, int(math.sqrt(n)) +1, 2): Now let's run through `IsPrime(113)`. The square root is ~10.63, so the range constructed is `range(3, 11, 2)` which is `[3, 5, 7, 9]`. This should make sense since the next number should be `11`, but 11^2 is 121 &gt; 113. Remember composite numbers decompose into a product of primes. So if `11` were a factor of `113`, then its other factor must be a prime that is smaller than `11` -- one of `[3,5,7,9]` so we can just check those. This in turn will cause `IsPrime(3)` to be called, that's a True so we check `113%3 == 0`. It's not, so we call `IsPrime(5)` and that's a True so we check `113%5==0` and that's not, so we call `IsPrime(7)` and that's a True so we check `113%7 == 0` and it's not so we check `IsPrime(9)` and that's not so we stop and just `return True` from the end of the function. So comparing to the naive version with `range(n)` we would've initiated `113` calls to `IsPrime` from our call to `IsPrime(113)`, but *by using math (number theory)*, we ended up with just `4` calls. Now, each of those calls had more calls so the actual numbers are rather scary. For example, with the final version I ended up with IsPrime(113) calling IsPrime(3), making no calls, IsPrime(5) making no calls (`range(3, sqrt(5))` has nothing), IsPrime(7) making no calls (`range(3, sqrt(7))` is empty), and IsPrime(9) making 1 call to IsPrime(3) which makes no calls. So, IsPrime(113) in total made 5 calls. Now with the naive method, IsPrime(113) would make 113 calls, each of which makes `n` calls. This means IsPrime(0) makes 0 calls, IsPrime(1) makes 1 call, IsPrime(2) makes 2 calls, etc. The total number of calls would be the sum of i from 0 to 113, which is 6441. This means the final version I outlined is 1600x faster on a number like `113`. The reason is that the naive method has O(n^2 ) behavior. How so? Well, the sum of n=0 to N of n is 0.5\*N\*(N+1) meaning it's 0.5N^2 + 0.5N, so the leading behavior is N^2 . My method is, I think, related to the harmonic number of N which at large N has logN behavior. So naively, one has O(N^2 ) and with a bit of math you can get O(log N) out of it. Comparing a few large numbers you can see a 1 million times improvement with log N compared to N^2 with numbers as small as 10,000.
The choice made with Transcrypt is to accept JS's supremacy in the field. So whatever Pythonesque runs on the browser it HAS to cooperate flawlessly with JS. A perfect Python implementation in the browser that doesn't offer such cooperation has a problem, in my view. JavaScript rules, ok, accepted. I want to do my own programming in Python because it makes me more productive. People are diverse, so tools should be diverse.
Thanks bardoforlais. I don't have much experience with SQLAlchemy but given your comment and little more research I did today I may have to define a MetaData object explicitly with a schema that somehow specifies the primary key for all tables in the db. I wonder if that is how you did it. Thanks!
Sorry to hear that !
it's okay. not your fault i am full of hate.
&gt; Do you have any insight to the pros/cons of using this over VHS.py? I recently had to choose between the two and the features looked very similar. I ultimately went with VHS.py because it allows you to record http calls made from other libraries without using requests.session. ftfy.
No, it isn't the only thing. Recursive tuple assignment a, (b, c), d = 1, (2, 3), 4, properties, assignment of bound functions, multidimensional list comprehensions with ifs, to name a few ones that I really missed. Now RapydScript doesn't claim to be a complete Python implementation like Brython. That's the power of and that's what makes it a good project. But to my judgment the things I wanted weren't so easy to graft on. Moreover I wanted Transcrypt to stand with two legs in the Python world, using Pythons own parser and written in Python rather than JS. The philosophy of Transcrypt and RapydScript is the same: Taking the JS world seriously and providing a good interface to it. This is quite different from e.g. Brython, which seems to have a world of its own.
This is awesome!
now make this with the star wars intro font
You've looked well!! Transcrypt uses the CPython parser. Syntactically it cannot deal with new as is done in JavaScript. Another way to go is to encapsulate every constructor in a Transcrypt version of it, so you could just say canvas = Canvas (). But that would mean encapsulation of a lot of functions. While from some intensively used JS libraries that would be a perfect choice, I'd like to keep the barrier to the JS world as low as possible. So I came up with \_\_new\_\_ (), which function syntax can be parsed into a Python ast. The consequence is that any JS library can be used as is, without any encapsulation. But, to be sure, encapsulating JS constructors into a Transcrypt constructor that does the \_\_new\_\_ for you, is perfectly possible and in fact one line of code per constructor.
&gt; splinter
Having trouble understanding this code: def solve(n): summ=0 while n: summ+= n%10 n/=10 return summ I know what it does, it just doesn't make sense to me.
If you told so, I find it more efficient not to duplicate efforts here and there. 
"How bad is it" ? Your program is deadlocking on IO and your question is extremely vague.
Well, I was interested in the technicality behind it...BTW I saved that problem already, still would be interested in how to go about this, though.
Beautifulsoup?
From all I've heard/read/seen over the years, the Lua interpreter is quite minimalistic. And a decent language - I just personally abhor extra characters/keywords when I'm indenting *anyway*... thus one of the many reasons I'm admittedly prejudiced toward Python. What Damien &amp; Co. have managed with μPy is awesome! YES, it's not _completely_ the same as The Real/Big Brother, but it's so close, and it has all I care about for *microcontrollers* - things without GB of RAM and Mondo Screens and such. It's all the Python I (think I ;-) *need* on a MCU. AND they've made it so, and continue to make it even more so small, efficient and light weight. Incredible Times I live in... Incredible Times... ;-)
SO what is SatNOG? What does it do?
It is stable yes, but it seems to be slow .xlsx files with heavy calculations.
Pycharm is the most recommended IDE around here, although I prefer using a plain text editor.
Just grab a decent text editor. It's best to learn Python without an IDE. The hints the IDE may or may not give you may give you false impressions about how the language works. Also, the REPL is indispensable. Once you're comfortable with the language, I recommend PyCharm.
http://vignette2.wikia.nocookie.net/matrix/images/b/b1/I_Know_Kung_Fu.png/revision/latest?cb=20130130061639
No VIM is better 
What do you use for Java? Eclipse has PyDev which is okay. If you use IntelliJ, then PyCharm is a no brainier, it's excellent. Personally I don't really feel the need for an IDE with Python, I use Emacs with Elpy.
Is your machine a Dell by any chance? I get that too sometimes. You could monitor your screen for a black pixel with [pyrobot](https://github.com/chriskiehl/pyrobot) (a Python equivalent to Java's Robot class).
How are you looking to collect and use this information? "hosts in the LAN" might be pretty easy, depending on the environment you're in you could probably get away with just pinging a subnet every once in a while (bonus points if use ARP tables to decide who's 'live' or not, rather than actual ping responses since those can often be filtered out by host or network firewalls). The connection data itself... maybe you can collect that through some kind of WMI call to the windows box, though I'm not sure. One option might be using WMI, etc, to run 'netstat' commands and such (or their more modern powershell equivalents?) remotely. And if there *ARE* powershell solutions to be had, it might be worth writing some or all of this in powershell itself. Perhaps a powershell client that collect the data and submits it to a REST backend you setup with python?
I'd prefer nearly any other possible word to that... pythonist pythonicist pythonicator pythornicator whatever... I get it, we've got a BDFL that's old and he's allowed to have his own taste in vocabulary, but I groan every time. 
This is exactly what I am looking for, lot's of netstat commands being monitored with python. That sounds like a lot more effort than it's worth though.
PyCharm is [open source](https://github.com/JetBrains/intellij-community/tree/master/python) and it does full [type inference](https://en.wikipedia.org/wiki/Type_inference) which allows for correct refactorings like extract method, rename and move. These refactorings operate on the AST, they aren't a simple textual substitution. You can ⌘-click into any package and ⌘-[ to navigate back out, reading the source is often the best documentation. Try some other editors, but give PyCharm at least a week, even though one can be productive in about 20 minutes. I can't think of a compelling argument to _not_ use PyCharm. It runs everywhere, is free for the community edition and offers a multitude of framework specific tools in the paid version. I use the paid version, but rarely use paid features. The base model is that good.
It makes tuples look like they are homogeneous. They aren't *just* immutable lists.
I like sublime text 2/3
Any IDE is going to be confusing to beginner programmers I think, with the exception of IDLE. PyCharm isn't perfect (particularly when working with PyQt/PySide programs), but of all the IDEs I've tried it's the only one I felt compelled to stick with.
If you do find the time and you find bugs, things that really should be in or unclarity in the docs, please let me know. Thanks!
Teach yourself /r/emacs and you'll never have to ask this question again for any language.
Love it!
This sounds like more of a Homebrew issue. You need to run 'brew doctor' and then fix any issues that it tells you about. For this one, you need to grant yourself ownership of /usr/local/opt/ to be able to write files there. 
Is there any demo of it?
Idle's a piece of crap you use because it comes with Python installs and is the first thing you try without knowing any better.
Pythoneer?
If I recall properly the Ubuntu version does'nt include it, there is a separate python-tk package. The windows version always include it but doesnt include curses. Concerning pip, starting from 2.7 and 3.x (3.2?) they all include pip. I mainly use tkinter because "it's there" (less dependencies) and what I build are more tools than end-user apps so to hell with "beauty". 
Technically you are correct. These are different things. The issue is managing complexity. You really do not want to be doing callbacks, it makes Twisted too complex. You want one place to keep state for each connection. CoRoutines provide that. Stackless provides that. While you would not want to write Eve with coroutines, you could write many networked applications in Stackless. That is the sense in which they are competitors. Thanks for the question. 
The thing is that Bayes' Theorem is not the definition of conditional probability. It applies the concept, which itself is defined elsewhere: P(A|B) = P(A,B)/P(B) Edit: Why are you downvoting? 
Which python module do you use for Bayesian modeling? 
Here are some problems that needs to be fixed: - if the old code is the actual used code don't make all the github pages look like it's not. - the linked page says there are crashes. So where are the crash logs? Maybe set up sentry or something. - no bike shedding. There is a ticket for changing all string formatting which is 100% unnecessary and won't solve any problems (it will make the code not crash but be broken which is much worse than a crash)
Neat, but my mind wasn't boggled. You start with the same number of black and red cards. You remove the same number of black and red cards into the top 2 piles. Of course you are left with the same number of black and red cards in the remaining piles. 4 red and 9 black in one bottom pile will always mean 4 black and 9 red in the other. Doesn't matter how much you shuffle them. *EDIT very slightly wrong, the top two piles could be of uneven sizes, but the bottom two piles will always mirror this uneveness, resulting in the same cross distribution of colours in their piles.
Also see [the annoucement](http://pytest.org/latest/announce/sprint2016.html) for some more info. If you are close and want to learn more about pytest or start your first contribution, please consider coming as well!
Coming from the Java world, you might be influenced into thinking that an IDE is necessary for any programming language. No it is not. Since you are learning python, I would suggest that you pick up an editor that suits you and use it write python code. That way you will learn much better. Pressing . and waiting for the auto-complete dropdown may not be the best way to learn. Later once you are comfortable with python, you can configure your editor to get some IDE-like features as a convenience. I use emacs with elpy.
&gt; Not this endless reinventing as with aio-libs. Not sure about the other libraries in aio-libs, but apparently [aiopg mostly just wraps psycopg2](https://www.reddit.com/r/Python/comments/43vvsn/aiolibs_libraries_for_mysql_redis_elasticsearch/czmo7h0) and offers an asynchronous interface. Perhaps that was explicitly enabled by psycopg2 by separating protocol from I/O as Lukasa described?
Thanks that was a good article.
thanks for this! It dropped my run time from 142 secs to -&gt; 30 secs.. really smooth! much more easier to read in the code as well.
Well, sort of, but yes using P(A,B)/P(B) instead of P(A|B) in Bayes Theorem is much more intuitive, but it's rarely done.
Haha, this link is a webpage that has an iframe stating &gt; Here is a heavy user of SQL Alchamy presenting data against using async with databases. And in the iframe is a blog post by the author of SQLAlchemy. Silly geese. [This is the original post](http://techspot.zzzeek.org/2015/02/15/asynchronous-python-and-databases)
That's pretty much the example in the documentation for py3 concurrent.futures ThreadPoolExecutor. I have used this with Selenium Webdriver / grid to run tests in different browsers simultaneously. 
Thanks, glad you like it!
There is no best tool for the job. There is how ever the right tool for you. You need to try out a couple of tools and see which one is you you.
Ah, the disagreement seems to be about whether computation should go in the "synchronous semantics" box.
OK, I completely agree with that. However that doesn't work with all protocols (and not efficiently with others), since most synchronous protocol implementations you expect to get a complete data packet (for example, HTTP header parsing), whereas in some cases you can start processing the data even though you haven't received an entire protocol 'frame'.
This is exactly how Twisted works (which is why you can run most protocols over any transport). Unfortunately history has shown that most people don't understand this and think it's too complicated when all they want to do is scrape a webpage.
Pycharm automatically checks against pep8 while you type. I found [this](https://gist.github.com/jsmits/9033655) gist for running flake8. You might take a look at MyPy.
The fun things is figuring out why it will always match. Getting a predictable result from a random starting point is an interesting concept, and one that digs very deeply into security. The best way to do this is to start with an extreme outlier. So we start with 26r and 26b cards, shuffled, they'll be split into 4 stacks, a face up r pile, a face up b pile, two face down random piles, one of each equal in size to one of each of the face up piles. First, the swap is a misdirection. You're swapping random for random at that point. Even if you could see the X number of cards your swapping between piles, you come to the same result. So if we work with a 24r/2b split, then we know that face down piles are 24 and 2, and we have 2r and 24b in those two piles. We can only swap 1 card. The face down has 3 possible end combinations. 2r, 2b, or 1r/1b. If there are 2r, there are 0b in the small pile (under the 2b face up pile), and 0r in the large pile (under the 24r face up pile). Same with 2b in the black pile, then there must be 2r in the red pile. So let's look at it mathematically. X_r1_ + X_b1_ = S_r_ X_r2_ + X_b2_ = S_b_ S_r_ + S_b_ = 26 X_r1_ + X_r2_ = 26 - S_r_ X_r1_ + X_r2_ = S_r_ + S_b_ - S_r_ X_r1_ + X_r2_ = S_b_ X_r1_ + X_r2_ = X_r2_ + X_b2_ X_r1_ = X_b2_ So explanation here: The number of reds and blacks face down below the red stack is the same number as the red stack. Same is true for the number of reds and blacks being the same as the number of cards in the black stack. Both the black face up stack and red face up stack added together will be 26. The total number of reds in both face down stacks will be 26 minus the number of cards in the red stack. From there, it's just substitutions and cancellations all the way down, arriving at the number of reds in the red stack equalling the number of blacks in the black stack..
The protocol API for HTTP would be much more precise, so that each method would process only the amount of data required to determine what to do next. So, parseStatusLine, parseHeaders, parseBodyChunk, etc... You then feed each of those functions with input that you get however you get it async/sync/threads/whatever. It's not as simple as SimpleHTTPServer, but at least you're not writing all the parsing code.
There's a great video discussing Clean Architecture which I think overlaps a lot with what you are saying : https://www.youtube.com/watch?v=DJtef410XaM Basically the idea is that you put IO at the top level of your application (instead of burying it in libraries) so you can easily swap implementations etc - which would include swapping synchronous with async versions as needed
fuck your blogory.org
I mostly started automating annoying tasks to me. Before switching careers I was a financial advisor, so some simple stuff was anything people use excel for a lot (calculations, projections, retrieving stock data, filling out documents). From there it grew into finding some side projects (web apps and scientific computing stuff).
I'm on mobile, so navigation is rather tricky, but it looks awesome and interesting idea. To all naysayers : it might not be the best learning material, but it does have its uses! 
Cool, Which plugin in sublime allows in ide debugging ?
TL;DR: Because of Github stats and job trends...
That's ideally how it would work, as long as the protocol parsing was designed in a way to take a chunk of input, parse out the part it cares about, and then return how much it consumed (or alternatively the remaining unparsed data). But no one does this unless they were designing for async from the beginning. The reason for this is that each data chunk you get from the transport isn't guaranteed to be split along boundaries that are logical for your protocol. 
What you are proposing (I like it), sounds a lot like the functional programming ethos. Write code that has no side effects wherever possible. And, if there must be side effects, make that as close to the last 'layer', near the end user as possible.
Don't over complicate it bro. Just have one program that does both, read the serial line, and serve the web requests. Keep the data in memory in some circular buffer and just display it when rendering the page, or if you want to keep all values, throw them in a sqlite db as the arrive, and then query them out when you build the page. Once you have some basic demo working we can talk about how best to complicate your life to turn your skoda into a ferarri.
FWIW have an upvote for the work. It seems to be reasonably well put together :)
&gt; You cannot overlay asynchronous semantics on top of synchronous ones You very easily can. Check out any pure functional language, they've been doing this for ages. Note that you're not putting asynchronous semantics on top of synchronous **I/O**.
This is a pretty interesting idea. I take it everyone is basically hosting their own textfile and then tailing the textfile of those people they're following.
These just show that Python is popular. The why piece is in the Opinions section.
I usually use 13-lock from dmenu as well but I wanted to make it transparent and so my python script actually takes a screenshot then i3-lock uses it to give a sort of psuedo-transparency if you will. But yes, it's actually surprisingly easy to add your own applets to the sys-tray!
so how many rows in that sheet ?
&gt; If neither encoding nor errors is given, str(object) returns object.\_\_str\_\_(), which is the “informal” or nicely printable string representation of object. For string objects, this is the string itself. If object does not have a \_\_str\_\_() method, then str() falls back to returning repr(object). [§4.7](https://docs.python.org/3/library/stdtypes.html#str) Since the bytes class does not define its own \_\_str\_\_ method (you can't just make a string from an array of bytes without knowing the encoding), you get the repr instead. **Edit**: I do have some sympathy for the idea of making str() just fail in those cases and forcing the programmer to write repr(), but that would break a lot of code, including perfectly reasonable debug logging.
Man, that's annoying. I guess I'm decoding my packets as cp1252, then.
The row count wasnt really the one slowing it down. Its just heavy with formula.
No, I think this is a prime example of how functional programming can greatly improve code. You seem to be making the same mistake the OP of this comment chain made. This discussion and this library is 100% irrelevant to async I/O, because it's irrelevant to *any* I/O. It's a rare model of the FP philosophy in the Python library ecosystem. Functional referential transparency + purity + idempotence (slightly overlapping use of terms there, but whatever) can make handling I/O way easier, whether you overlay a synchronous or asynchronous model, or just a mock, atop it. Lukasa's Hyper library is an example of an attempt to make somewhat functionally pure code with respect to I/O. Obviously it's not truly pure like it would be in a pure functional language, but it's adopting functional philosophies. The primary goal of referential transparency is not for compiler optimizations, as you describe, though that is a beneficial side effect (pardon the... anti-pun?). It's for better and more maintainable code; taking separation of concerns to the extreme. Treating I/O as a blackbox and writing I/O-agnostic code is the goal here. The intent is not to aid async I/O, it's to make a layer that *any* "outside-world-interfacing" model can plug into, including async I/O. This is why languages like Haskell have no actual way of directly performing I/O (unless you break the glass and call `unsafePerformIO`, which is strongly discouraged), and instead require use of the `IO` monad.
Let's make python great again. 
Sorry for being pedantic, but I believe you mean decode it. e.g you have a string, and you want to get it in binary form to be able to send it over network. What you do is you encode the string using UTF-8 representation and you get bytes as a result.
*Decode* is bytes to unicode. *Encode* is unicode to bytes. A byte string representing text does have some encoding - your code might not know what that encoding is, but it has it. For instance, if the bytes C3 B8 are used to represent ø, it's UTF-8 encoded. If the byte F8 is used for that character, the encoding may be Latin 1 or cp1252.
Seriously, my suggestion isn't to look for something to do &amp;mdash; and this goes for anyone working with any programming language &amp;mdash; instead, try to be a little more aware of the next time you go to do something, and find it arduous or not as streamlined as it could be. Take that as an opportunity.
async is not just about I/O, nor is it about simple functions. Its about anything that could cause a delay. A large complex computation could be done in a synchronous or asynchronous fashion, and ultimately there really is no difference between waiting on disk and waiting for a large computation to be finished. To give a silly example. I might want a function that returns a list with the running total of the sum of integers up to N. So if N=4 it would return [0,1,3,6]. This could be done with some kind of side effect introducing I/O. Perhaps one stores the integers in some file and reads the file, or perhaps fetches a sequence from a database server or any other idiotic way to do this. That would obviously make it synchronous. This could also be done without side-effects with a simple one-liner. def stupid(N): return [sum(n) for n in range N] But that doesn't make it asynchronous. Its still synchronous. I am still waiting while my CPU cranks on a bunch of additions at the end of the list before I see anything come out the front of the list. The correct answer is obviously: def smart(N): for n in range(N): yield n*(n+1)/2 async is about how you process and return data, not about having "simple" functions. And once you go down the road of returning things differently you have to consider how that will impact program complexity. I cannot process a generator twice without either buffering the result, or rerunning the computation. Which is the better course or action is unclear in a side-effect free computation. Side-effects simplify things because they dictate that I *MUST* buffer. ---------------- To go back to this silly HTTP/HTML parser example at the beginning. Suppose I am downloading a large table and rendering it on the screen. As the first N rows come in I can perform partial parses, and partial layout, and perhaps even put a full screens worth of data up on the display. Then somewhere at the bottom of the page I suddenly get a field that is a bit wider than the previous fields in that column (and where I am not allowed to wrap or truncate the field value). I need to redo my layout and stretch the columns to match the widest column entry. Now I could make my rendering synchronous and make it depend on my downloading the entire page before it puts anything up on screen, or I could have some relatively complex logic which causes a new layout pass to trigger... and lots of buffering to make this happen, and yadda yadda. Definitely not simple to do this because it touches all layers of the stack.
Actually that raises another problem: HTTP is in no way tied to parsing HTML or XML. What if the body is a 6 GB file? You sure don't want to buffer that all into memory to begin parsing the first line. What if the body is gzipped for transport? You'd want a way to chain that HTTP protocol handler with a gzip protocol handler. But you don't know you need a gzip handler until you've parsed all of the headers etc.
Exactly. Static site generators are ten-a-penny because they're fun to build. They're also an ideal size for a language-learning project - big enough to be interesting, small enough to be manageable.
 chmod +x env/bin/uwsgi And try /r/learnpython in the future.
&gt; Eclipse Try Eclipse and PyDev. It works pretty well for me.
No. But cool snake.
Thanks, but that was not relevant to the question. I probably have not expressed my self well. I remember that it was apriori an executable and ask why isn't it now.
&gt; Does "foo" get printed BEFORE Popen is done? No, but you are likely misunderstanding what `Popen` does. It only starts the child process (fork+exec in POSIX lingo). It does not wait for the child process to terminate. So `foo` may be printed before or after output from the child, or even in between. That is undefined since the two are running in parallel. There is another aspect to this: the child's stdout is usually closed by the process terminating. So reading EOF on the child's stdout is likely caused by the process ending. Since you are outputting stdout to a pipe and using the data, it would be useful to see some actual code. None of the code you've shown so far uses the stdout data. 
No worries, it's reddit, I can afford a few fake Internet points for the answer that I wouldn't have otherwise seen. 
The point is that the original quote is confused. The semantics of async cannot be put on top of a synchronous stack based system. The reason LISP works in an async fashion is because in LISP there is no distinction between call and yield, because there is no distinction between code and data, so everything is a closure, everything is lazy, everything is asynchronous, and everything is a partial computation. The only time a LISP interpreter would ever "wait" on an entire computation is when it actually has to evaluate the entire computation to generate the proper output. Stack based procedural languages are not really able to do that. They approximate it by returning closures, or promises, or generators, etc... And those have different semantics than the rest of the procedural languages because they exist outside the stack.
I've used vim for well over ten years, but I recently switched to PyCharm (for Python projects). I still use vim heavily for everything else, but for Python projects, and especially Python-based web projects, I am finding that PyCharm makes it significantly easier to focus on the project and not my editor. The single biggest reasons for switching were project management features for managing and attaching virtualenvs (and conda envs!) to a project, while retaining so-called "code intelligence" features. In PyCharm, the code completion engine obviously automatically switches to the correct Python for the project. As the author of [vim-conda](https://github.com/cjrh/vim-conda), I tried really hard to make it all work in a convenient way, at least for conda, but it's really difficult to point the jedi completion engine to a different Python path on the fly. Once you begin working on, and switching frequently between multiple projects that use different versions of Python, vim starts to creak, at least for me. Neovim makes it a little easier, but as far as I could work out, you do have to exit and restart nvim with the correct Python configured on the current path, after installing the required neovim package into that Python instance. There are other things. The paid version of PyCharm includes *WebStorm* features that provide code completion features inside web templates and JavaScript code are also extremely convenient. I obviously have had *tern* configured in my vim (here's my [vimrc](https://github.com/cjrh/vimfiles/blob/master/vimrc)) and a bunch of other tools, but what I have found is that you have to put a lot of time into configuring vim plugins, particularly if there are collisions in keys and/or other problems like python versions, or lua dependencies and so on. I have put an embarrassingly large amount of personal time into vim configuration over the years, and this is not something to be proud of. There are things I use in vim that I have not yet found how to do in PyCharm, such as the amazing [colorv](https://github.com/Rykka/colorv.vim) plugin. I'd love to know if there is something similar, preferably invokable from the vim emulation mode. I do still use vim keybindings in PyCharm, obviously. Beyond the obvious like regex search/replace, it even has support for splits (CTRL-W, v) which I use a great deal.
thanks, good point!
One thing that makes notebooks useful is notes. There are no notes here. Aren't even any comments in your code. How is someone else going to find this useful?
I agree with your explanation. I also think it's important to understand that * and ** both do two different things, depending upon what side of the function they're on. \* when used on an argument means "break out this list of arguments into positional parameters". But if you write *foo in a function definition, it means "pack all unused positional arguments into tuple foo". So it's packing or unpacking depending on the context. It's two sides of the same coin. And the same thing happens with **kwargs, except with a dictionary and named parameters. This two-sides pattern is useful because the most common usage of *args and **kwargs is passing unused arguments to another function. This works very easily, like in OPs post. I think worth mentioning, the other time I use *args and **kwargs is when building up to a function. kwargs = {'foo': 1} if bar: kwargs['optional_parameter'] = True func{**kwargs} In this case I could just cast bar as a bool, but I think you get the point. Sometimes this helps a lot with readability.
The latest in a series of trivial articles with grade school examples.
Good call. Reminds me of [this Brandon Rhodes talk](https://www.youtube.com/watch?v=PBQN62oUnN80). /u/Lusaka's admonition has a similar logic, I think, but on the ecyosystem scale rather than the scale of a single project.
This likely only works in scenarios where the authors of the non-I/O portions are also authoring the various I/O models to work with them. The problem is that in order to provide an API for the non I/O portions of your code, it helps tremendously to know what the possible I/O or concurrency models are going to be! 80% of the code ends up being the I/O portion anyway, and will be opinionated and distinct from its sync/async counterpart. The result is that you don't end up benefitting as much as you might expect from reusing non-IO modules. What does make sense about this is the idea that the module authors should be authoring the sync/async interfaces simultaneously.
I'm hijacking the top comment because I think there is a lot of confusion as to exactly what is being proposed here. I suspect several commenters only read the title and not the original comment. The comment belongs to /u/lukasa, so I apologize if I am putting words in his mouth. First of all, in the context of the original post that the quote came from, the use of the words synchronous and asynchronous are only in reference to I/O. The intent was only to differentiate between concurrency models specific to Python, such as Twisted, Gevent, threading, and sequential socket calls. Second, the proposal deals only with I/O, or more specifically, network protocols. Not file formats, markup languages, compression algorithms, or anything else that, yes, can be processed with asynchronous semantics. The proposal is really rather simple. Implement the parsing components for the protocol in a model (concurrency model) agnostic manner. This means an implementation of the individual components that any given protocol (HTTP, FTP, IMAP, SSH, etc...) requires to enter into different states. Additional libraries would have to be developed to simplify the API for various concurrency models, but at least the core protocol implementation would be solid. That point is the key, and it's why we're interested in this dialogue. An example implementation for some protocol might pull in a buffer of data using the concurrency model best dictated by the situation. It would then repeatedly feed that data into the model agnostic protocol implementation. It would fire events whenever the current state of it's state machine changes. For HTTP that could mean: status_line_parsed(str), headers_parsed(dict), content_started(BytesIO). The developer for the e.g. Twisted version of that library would have to snap all the pieces together correctly. But the key is that if there is a bug in e.g. the cookie parser for the protocol, the fix becomes available to all of the dependent model-specific libraries. Personally, I would love to see these sort of model agnostic protocol implementations in the stdlib. They weren't necessarily necessary when threading was the only real option available. Honestly, I was satisfied with Gevent, which is able to... monkeyback? on all that work. But with the advent of asyncio, I'm left wondering if all that effort couldn't have been much more wisely spent in agnostic protocol implementations.
*args and **kwargs are not concrete things in themselves. *args means "expand a sequence-like thing and apply them as positional arguments", and **kwargs means "expand a mapping-like thing and apply them as keyword arguments. Given: args = [1,2,3] You can print(*args) because print() takes an arbitrary number of positional arguments, so its the same as print(1,2,3) So, print(*args) is identical to if you had done manually print(args[0], args[1], args[2]) You can't print **kwargs because print doesn't take arbitrary keyword arguments. These functions are called "unpacking", they basically explode an iterable/sequence or dictionary into arguments, unless used in a function definition, at which point they mean, "this function takes an arbitrary number of extra parameters" or "this function takes an arbitrary number of extra keyword parameters". def do(*args, **kwargs): ... In this case, inside 'do', "args" is a regular sequence, and "kwargs" a regular dictionary. You don't talk to them with * or ** in front, unless you're passing them to something else and applying them. Note, once upon a time there was no unpacking operators, and there was an 'apply' function which took a function, a sequence and an optional dictionary and if called the function with those data types exploded into arguments. In your example, you don't *have* to ever use **. You could have done: use_attributes(a=1, b=2, c=3) You don't have to construct mydict first. You CAN, but you don't have to. Note, PEP448 in Python 3.5 extended the ways you can use in more situations. Check the PEP. 
This is syntax for packing/unpacking a callable's arguments as you have mentioned. Your case involves doing both and, without seeing the rest, seems like a bad example. In Python 3 `print` became a function. It accepts `*args` for printing and `**kwargs` for settings: &gt;&gt;&gt; print("foo", "bar", "bat", sep=", ", end="!") foo, bar, bat! &gt;&gt;&gt; printables = list(range(3)) &gt;&gt;&gt; printables [0, 1, 2] &gt;&gt;&gt; settings = {"sep": "-", "end": "@"} &gt;&gt;&gt; print(*printables, **settings) 0-1-2@ &gt;&gt;&gt; settings["end"] = "%%%" &gt;&gt;&gt; print(*printables, **settings) 0-1-2%%% Now you can make changes to `printables` and `settings` and keep feeding the `print` function cleanly. Its all about reducing redundant function calls.
Thank you very much! Is there any downside to just using the existing C libraries in Python? Would Python modules be faster or easier to use or something?
Mind to elaborate on how good the vim keybindings are on PyCharm? Thanks!
The var stands for variadic. so variadic arguments, but you probably won't see that outside of a textbook.
You can't apply the term '(a)synchronous' to programming languages.
For people who are interested, I'll be presenting a talk on exactly this idea at PyCon SK in March and then at PyCon US later in the year.
I don't believe I'm confused. =) Hyper-h2's API is incremental. You feed it data, it parses it and returns whatever events it can and buffers the rest. This means that execution time is strictly bounded: you could compute a theoretical upper bound of how long the computation would take for a given amount of data. This means it is *synchronous* from the strict perspective of thread flow control: your thread will block until the parsing of that data is complete. However, it's not synchronous over the data stream: you pass whatever data you have and get whatever you can, and then go about your day until more data is received. That's how you can slot this into both synchronous and asynchronous protocols: this can be done with HTTP/1.1 as well, with some care, though HTTP/2's "stream of length-delimited frames" makes it particularly easy.
ah my bad
 USER="TheBrain" echo "...try and take over the world!" &gt; /home/${USER}/.plan
Jekyll is built with Ruby
Any chance of supporting non-YAML formats for front matter? TOML, JSON, etc?
Nice work. You should check out aalib.
You could try /r/snek_irl.
Speaking of Selenium. Is there a Chrome driver yet?
Thank you. I think there is no way to debug python in visual code?
You should search for a good [tutorial](https://www.reddit.com/r/learnpython/wiki/index). user_input = input('Enter something: ') # Python 3 user_input = raw_input('Enter something: ') # Python 2 Edit: [Program Arcade Games](http://programarcadegames.com/index.php?lang=tr) is available in Turkish.
With the `finger` protocol you hold a text file in your home directory called `.plan` that contains what you are currently planning to do (in truth it can contain any text string, usually a joke). When another user issues the `finger &lt;username&gt;` command the contents of your plan appear on their console stdout. I think /u/fdemmer was being a little flippant in making the comparison but at a basic level it holds. Much like saying the `talk` protocol is the same as `hipchat` or `slack`.
I wrote a user directory for this if anyone is interested: http://twtxt.reednj.com
I had problems wrapping my head around them to start. Basically think of all functions as taking only two arguments. An array and a dict. So the array is called * arg And the dict is called ** kwargs So def foo(a,b,c) would become foo([a,b,c]) def bar(a,b,c = 2,d=3) would become bar([a,b],{c : 2,d : 3}) So now you can describe all functions as taking only an array and a dict. Once inside the fucntion it magically unpacks the array and dict and assigns it to the variables. Just accept it. So you can make a dict by doing dict(a = 3, b = 4) Try it. It works because dict is a function. Once you figure out how it works is really useful. 
This could reveal a public email address and people could use email for DMs. The @ stuff is a client-side thing.
Ah, 3.5-specific stuff.
1) If You develop something complex You need real full featured debugger. 2) The best You can get for debugging is PyCharm it has most advanced debugger and even remote debugger that works with Vagrant. It works even for multithreaded applications. 3) Pdb is usefull to debug quickly in production when something go wrong. Used in local environment it's waste of time.
It's from a standard Linux tool, and I would disagree with that statement anyway, I really like the Python flags for file modes. 
If you have a (small) upper bound on each functions complexity then you are basically building a real time system. You could certainly call what you build off of that async. So in that context sure. What you are describing is a generator object instead of a generator function. Instead of yielding partial computations and keeping state in the closure you yield events and keep state in the object. But as it is presented here. No you cannot build async off of generic non blocking functions. You need some kind of guarantee, and you need to change rather fundamentally how you approach the problems. 
No, the `subprocess.run`/`subprocess.CompletedProcess` API is a 3.5 thing.
I don't see this as a programming issue. Motivation is the hardest part of being self-employed (which you count as, since you don't have a boss telling you what to build). I'd recommend looking though books aimed at the self-employed. Perhaps the local library? You may also want to look on the web for various "Why I founded XYZ Corp" talks.
It attempts to receive a maximum number of bytes (the bufsize argument) from a socket. Sockets are used in low-level networking, creating endpoints for communicating over a network allowing one process to `send` some data to be `recv`'d by another. Generally you want to avoid programming at the low level like that and would rather use higher level libraries such as Twisted or asyncio (and there are plenty of resources online for both of those). If however you do want to learn more about the low level aspects of this in python, a good starting place would be the python docs (assuming Python3) at - https://docs.python.org/3/library/ipc.html - https://docs.python.org/3/library/socket.html 
It's a bit more than that, as anyone who tried to build a twitter client knows. @ should include a reference to the actual message it's replying to, otherwise things get jumbled up pretty fast and you cannot follow conversations.
Thanks, I am going to use it, I've added this to my browser's bookmark
Personally, no. I agree that it's possible to accumulate quite a lot of clutter in the root of a repository, but I think the cure is worse than the disease: - Using a completely different layout is confusing for anyone else looking at your project on Github. - If you use the `insideout git` commands, that means that anyone who wants to clone and work on your repo needs to install insideout first. And it's presumably not compatible with any GUI interfaces to git. - I expect there will be quite a few exceptions that need to stay in the root of the repo. You've already made an exception for README, and config files for tools like Travis and Readthedocs also have to be in the root directory. It's a clever solution to a real problem, so don't take this as a slight. I just think that the problems it creates are much more important than the one it solves! ;-)
I am just a student not a pro. PyCharm look professional, I installed it. But I really like Visual Code theme etc. I couldn't change Pycharm font to Monaco font which VS code's font. Also, Visual Studio theme is not quite same.
It reads a number of bytes from a network socket. The extra gotcha bit that isn't always clearly mentioned is that the actual number of bytes returned may not always be the amount requested. It may vary anywhere between 0 and the requested size. If your network protocol uses sized frames, you'll need to compensate in your application. Stream oriented protocols are a bit easier and you can just keep appending recv'd to your input. Happy coding! 
Hey, I wrote something very similar in 2013, though my engine has a completely different syntax style for writing games. Nice work. https://github.com/freshollie/DisplayDriver
Yeah, it is out there for at least 4 years.
Since you're using Python 3, you can use [iterable unpacking](https://www.python.org/dev/peps/pep-3132/) and change lines = p.stdout.decode("utf8").splitlines() headers = lines[1].split() Route = namedtuple("Route",headers) to _, headers, *lines = p.stdout.decode("utf8").splitlines() Route = namedtuple("Route",headers) You don't need to split the headers. namedtuple will do that for you.
Well, to be clear, that was never the claim I made. The claim I made was that you can build libraries like hyper-h2 that are *agnostic* to the I/O and concurrency paradigm that you use (modulo some constraints, e.g. is the state machine thread safe, is it reentrance safe etc), by applying some appropriate design principles: namely, don't touch any I/O primitives (I/O takes too long) and don't require all the data to arrive before letting the caller do some work, free the caller up as soon as you can. This design pattern, when used, allows us to avoid duplicating work on complex but tightly scoped problems like network protocol state machines. It is obviously not applicable to all problems (for example, it's hard to see how you'd get much traction parsing something like HTML this way), but it is applicable to this specific class of problems. Yet the Python community seems to have abandoned this principle, long known and understood before us, to instead tightly couple to our I/O.
In my opinion Twitter and similar services are just not meant to have full fledged conversations. There are better ways to do that. So for me, personally, not having references is not a problem. And simple @-mentions are indeed just client-side sugar, which I might add in the next version. 
Nope, it is still a cure to a problem that doesn't exist. I want to see that a project has a LICENSE immediately &gt; A file on the root directory is ugly if it is not absolutely needed &gt; &gt; when making use of the package. LICENSE is absolutely needed immediately
Thank you for your observations. Well, I hope a simpler root directory makes it easier to understand the code first, and then afterwards understand how to test and contribute. The `insideout git` approach is optional. RTD config file is normally on `docs/conf.py`. Travis is the only problem remaining to solve. If the config file is mandatory to be placed on the root directory, then I believe that is wrong. They should allow customization
Is it interpreted? Because I don't think Trump likes to be interpreted...
For readthedocs, I mean the [new, optional readthedocs.yml file](http://docs.readthedocs.org/en/latest/yaml-config.html). Besides this and Travis, I also know of Appveyor and CircleCI which look for config files in the root directory. And I'm sure there are others I don't know about.
To the 'abandoned this principle' point: do you have a good example of a Python network protocol stack that does not have its I/O composed directly into its state machine?
FYI on mac os x, the command is /usr/sbin/netstat -nrl -f inet and it has a couple of extra header lines to discard. Edit: also it has an 8th column "Expire" which is blank in most lines, and absence of data at that position will cause an error "builtins.TypeError: \_\_new\_\_() missing 1 required positional argument: 'Expire'", so you have to preprocess the lines to add a null item or else trim "Expire" from the headers. Bleagh.
If by "professional" you mean "good" then yea. I don't see why being a student should mean you should work with bad tools.
Doesn't happen with other python entry scripts. Haven't ever tried uwsgi though. What if you pip install autopep8 (auto-formatter) and then run "autopep8". Is it executable? 
Did you declare the player_hp value to be a certain value before using it?
Care to elaborate more?
read the sticky man, /r/learnpython
 Could you post the whole conde?
Kivy only SOMETIMES uses Pygame under the hood. It is not very safe to assume you have access to pygame features with your kivy application. In addition, we are moving away from pygame entirely. This is because Pygame is a wrapper around SDL1.2, and there is now the much better direct SDL2 provider for Kivy. Loading times for mobile devices is heavily improved using the non-pygame backend, and many other lingering issues related to pygame being stuck on old libs are also solved. 
No, could you give me more info why you recommend it? Why you suggest Pyramid over Cherrypy and the others? From first glance Pyramid seems quite messy in similar way like the Flask. But I might be wrong :) I have to admit I have never heard of Pyramid before.
Yeah, fair. Probably would have been better to just say "easily" or something to that effect. 
I mean it just seems complex to me. 
No Windows support :( But as /u/delicious_brains pointed out, it was based on [a project with windows support](https://gist.github.com/MichaelPote/92fa6e65eacf26219022)
I am just an old hacker. But either use vim or emacs as your editor. Personally I like emacs but some people like vim. Both are light weight, they have been around forever, they run on any platform and they can be extended. Initially you will need to invest time learning the emacs/vim way of doing stuff. But it saves time in the long run. Basically you want an editor that is fast and can be extended. These fancy new editors like Atom and Eclipse (Pycharm is based on Eclipse) can be extended. But I find them slow and sluggy my emacs can do the exact same thing much quicker. The key with emacs is learning how to install extension to make it do what you want. Learn VIM. I don't like VIM but every Linux distro has vim on it. So you need to change one line is a ngix config file, do it in a terminal with VIM. Learn a few of VIM's keys. 
Nono, what I was saying is that there already exists a Windows version if you want it. His version was to fill in for Linux.
Calling [`collections.namedtuple()`](https://docs.python.org/3/library/collections.html#collections.namedtuple) creates a new class which is a subclass of `tuple`. So it has all the usual properties of a `tuple`, such as being immutable. For the implementation, see https://hg.python.org/cpython/file/tip/Lib/collections/__init__.py#l301
&gt;&gt; Not sure if the "correct" way to fork a discussion is by making a new submission as I've done here. Sounds like the answer is "No."
thanks for advice 
 print ('Lütfen 3 bilinmeyenli 2. dereceden denklemin köklerini giriniz.') girilena=input('a değerini giriniz:') girilenb=input('b değerini giriniz:') girilenc=input('c değerini giriniz:') girilena=float(girilena) girilenb=float(girilenb) girilenc=float(girilenc) def delta (girilena,girilenb,girilenc): fark= (girilenb**2)-(4*girilena*girilenc) print ('Deltanın değeri:',fark) if (fark&gt;0): x1=(-girilenb+(fark**(1/2)))/(2*girilena) x2=(-girilenb-(fark**(1/2)))/(2*girilena) print ('x parametresinin alabileceği değerler:',x1,x2) elif (fark==0): print ('Delta değeri sıfıra eşittir.') x0=(-girilenb)/(2*girilena) print ('x parametresinin alabileceği değer:',x0) elif (fark&lt;0): print ('Delta sıfırdan küçük olduğundan reel kök bulamıyorum.') OP's program formatted. The problem is that you're never actually calling the function, you only defined the function. At the end of the code you should add `delta(girilena,girilenb,girilenc)`. That would actually execute the function.
Sympy is not really an "alternative" to Wolfram|Alpha, except in that - with more effort on your part - you can convince a sympy console to do some of the same things Wolfram|Alpha can. You could with more complicated scripts convince Sympy to do some things Wolfram|Alpha can't. Wolfram|Alpha's major strength is it's wonderful natural language processing engine. The fact that it so often knows what you want, without you having to consult documentation - or write a script - is invaluable for students. It can figure out what someone is asking, and show a step-by-step solution. Similarly, Sympy on its own cannot do most of the things Mathematica can; Together with the rest of the available tools, however, hollistically Python makes a good altenative.
Oh I agree, Sympy is great. I used it for nearly half the day today. I don't even have Mathematica installed on any of my computers (except on the dept. server); I haven't needed to pass a call to it in many years. But you need a lot more than SymPy for that level of coverage; a more complete/familiar alternative might be Sage. Sage is a odd beast, but it is very good at lots of things, and can be more intuitive than SymPy+NumPy stack. Sage is a lot better at interoperability, too; between its modules, and even working with other programs like Matlab, Octave, Mathematica, or Maple. And Sage is better at working with more general rings than the integers/reals. It is better at working with symbolic vector spaces, too. So, depending on what you want you can take a look at Sage!
It really depends on what your goals are for this/these projects. If you want to learn how large frameworks are built of out smaller components, I think this approach is nice. If, on the other hand, you're more interested in getting working projects out the door, but are skeptical about the frameworks like Django/Flask/Pyramid, I would suggest you reconsider. You didn't specify any kinds of projects other than a blog, so I'll make a small case study from it. Here are some of the things you may need for building a blog website in Python (although you could just use Pelican, as /u/jpj_shadowbanned said): - A template formatter, like Mako - A web router, like CherryPy - If you're using a database, an ORM like SQLAlchemy can be handy (but isn't strictly necessary) - If you're using an ORM, you're obviously using a database This sounds easy enough if you're not doing anything involving accepting content from people on the web. The second you do, you introduce a whole new set of complexities: - Security about logging in and storing passwords (Authentication) - Security about making sure that people who log in are allowed to do specific things (Authorization) - Checking to make sure that things like cookies, passwords, etc don't leak, people can't intercept requests, fake requests, etc. Although you can technically piece together a website from components as you've described, the reason for using a complex framework like Django or even smaller frameworks like Pyramid or Flask is that they've figured out solutions for these immense problems already. If there's a security flaw in your ad-hoc framework (and make no mistake, it would be an ad-hoc framework if you roll your own), you need to fix it yourself. If a new security flaw is discovered in an existing framework, you can be assured that someone else will fix it.
In the article you have a link to the SymPy Online shell, but [SymPy Gamma](http://www.sympygamma.com/) is the one you want if you're looking for alternatives to Wolfram Alpha and Mathematica.
You found me out. I am an uneducated old hacker. I heard educated people using words like parser, syntax, grammar etc. So I decided to use them to appear educated 😊 Probably you are theoretically correct. Either way it is a method to get data from a line into a structure you can use. 
lol wtf are those things!
The buggy one? The one that doesn't work?
I wasn't trying to start some editor war. I was just saying that pdb is great. I personally use it with vim. But I couldn't think of a better debugger than pdb. You can start a breakpoint, jump ahead, go back, whatever. What more could you ask for? Well maybe a debugger that debugged your code for you would be neat. But thats just crazy talk.
$50 an hour
Yes, this is a dataset I am trying to tie into a research project. I should have mentioned that I contacted them, and they could not figure how to get that information. So I would also share it with them afterward. Since users are requested to signup their own location information for others to find, I guess they don't have a spreadsheet on the backend. Its a list of food banks, soup kitchens, pantries, and religious groups that offer food to the needy. I sent them a formal request so they could show management that there is a demand for this type of data, but nothing will be done within my timeframe from their side. I also got written consent to access and use the data as I needed but they couldn't run support. 
Thank you, I really dont want to mess with complicated stuff just Django. But your other suggestions are appreciated.
Thank you, never heard of them before. Well, it is rather complicated my choice :)
Yes, I know. I might have security issues. On the other side, this is why I have chosen SqlAlchemy and Mako. In case Cherrypy is not enough I just try my luck with (let say Pyramid) using already known libraries. Right know, I dont intend to do anything big.
Network programming has a lot of things that can go wrong. One thing is that the connection between your client and the server can fail. If you don't check that the connection is good when you ask for data, then you're telling the program to use a connection it doesn't have. That's the short version - at some level, that connection has failed (hence "connection reset by peer"). You haven't checked for that, you've asked for data on a connection that doesn't exist, so the program has quit. The fix is to check that the connection is ok every time you ask (look into "try"). If the connection is bad, wait a while and try again.
Aha thank you. I'm familiar with "try" with simple uses, such as user input validation, but what exactly would I be testing to see if the connection exists? 
it defines the *name* for a parts of the string. Sentence(noun=apple, verb=eat) 
If you have LaTeX installed on your system, the IPython Qtconsole will sense it, and will render the sympy output in proper math font. This means real above/below fractions, integrals, derivatives, sub/superscripts, proper matrices, etc.. I usually start it with from sympy import init_session init_session() And you're off and running . . ---------------------------------------------------- Edit: The notebook does this as well - I just prefer the Qtconsole when trying to get stuff done.
Tricky question! I don't date 10-year-olds.
nevermind I figured it out. It had to do with how I was calling the function. Super confusing but I figured it out lol. Thank you guys anyway!!
The `Twitter` constructor doesn't take a username and password, it takes an authentication object, for example: twitter = Twitter(auth=OAuth(token, token_secret, consumer_key, consumer_secret)) See some examples at their GitHub page: https://github.com/sixohsix/twitter#the-twitter-class
Hmm how do we make this article aesthetically pleasing for kids? COMIC SANS!
If you're using persistent connections (i.e. Connection: Keep-Alive), could be that the client is expecting an open connection that has already been closed by the server. Like previously mentioned though, it really could be also due to a wide variety of other issues as well.
I'll take a look at it and see if there's anything I could do. A small tip: instead of having a "backups" folder with "checkpoints", use git tags.
https://www.jwz.org/blog/2003/02/ph33r-m4d-ski11z/
Google "automate the boring stuff"
you might want to consider splitting the classes into different files edit: also try to avoid globals
Cool. What os does this work on?
This reddit title is a good example of why commas matter.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
and it was even the fifth one
Notice me SymPy.
A comma is the difference between helping your uncle, Jack, off a horse, and...
What's wrong with globals. Everything is in a function. And the variables have to be able to be changed by multiple functions. There isn't that much returning happening. Must functions just change some things.
Yeah I'll stop using that. I just did it to feel legit lol
Also, I've never had multiple source files. I guess I just have to import the other source files, but I don't understand the benefit. Could you please explain why I should?
OAuth isn't in the twitter.api module, try this: from twitter import Twitter, OAuth
It's not really that classes should be different files, but you should try to group related logic together in modules/packages. It's makes it a little easier and natural to find things, like "where can I find inventory functions??? Probably in an inventory module.. etc"... And also so you don't end up with a million lines of code in one file that'll be shared and merged with later. Check out http://docs.python-guide.org/en/latest/writing/structure/
Intel optimized the linear algebra library to optimize loops and minimize cache misses if I understand right. You could always get it for free, but compiling numpy/scipy to use it was a bit tricky, especially on windows machines. Anything that uses numpy might benefit. If you have an Intel chip, it can be much faster in some applications. In one language modeling application that I wrote, the matrix math (done in numpy, mostly dot products and straight matrix multiplication) ran about 40% faster out of the box. Not a terribly scientific benchmark, but it definitely makes a difference. Giving it for free is awesome.
Start by getting rid of your git scripts. What use is a bunch of commits with the message "Standard commit"? If you're that lazy, just use SourceTree or something. Lots of room here for OOP and inheritance, especially for your `Rooms` and `Items` classes. Extract all of your new `Rooms` and `Items` classes into new Python modules, to reduce some of the lines in the main file. Really try to avoid globals. Pass the variables you want to use around. Create objects that group the properties you are using. You have a good start with using classes. Keep doing that, and wrap up all of your game functions into class. Avoid using module level functions unless necessary (e.g. it's a function that doesn't really belong to any other logical unit). Lastly use an IDE, like Pycharm. Use it to inspect and format your code. An IDE will make finding and avoiding bugs much easier. 
Truly. I'd hate to run across code where the author had replaced standard Python idioms with their own functions.
Thanks for all the tips. I'll try separating my classes from my main source file. I use geany for my local workspace, but cloud 9 is really useful when I go to school, as I can code anywhere. I still don't understand why I shouldn't use globals. Is it somehow inefficient? Or is it just standard procedure?
I would also like to see you're RPG game. Could you please link me to your github?
Some time ago I recompiled my R with MKL; my numerical experiments went about 10% faster compared to OpenBLAS. I assume the difference will be similar with Python and Anaconda.
That is really cool. I use R for most of my mathematical programming but will try this out. 
This is a perfect example of where you would benefit have making a class. Your globals would be class attributes and the functions class methods. Globals are bad. Do everything you can to avoid having to use a global.
Hmm? Just write code and don't use all the features. Mostly you want the as-you-type-fixes. Plus the search-everywhere (files, classes, symbols, commands) feature you get with double-tap on shift is awesome. You don't need to learn any more really.
Hi, check this out: http://python-sounddevice.readthedocs.org/en/0.3.1/ https://wiki.python.org/moin/PythonInMusic
I did some benchmarking, I'm seeing no difference on OS/X. Should be very handy on Windows, though. Haven't benchmarked yet.
99.85% of programmers have opened files before using them, it's what we expect to see. Your code should, as frequently as possible, do what is expected so others can instantly read and understand it. First rule of coding: you're writing this for the maintenance coder. And he's an axe murderer. The second rule of coding should be "don't be cute." If there is a standard way to perform a task, use it. Only suggest a new way of it makes the process significantly faster, safer, or introduces a useful feature.
You should check to make sure that the MKL is actually being used (i.e., `np.show_config()`). I see a 20% speedup in simple matrix multiplication on OS X..
Of course. The matrices I used were 5000 x 5000 and drawn from a Gaussian. But I mainly wanted to dispute the idea that the MKL somehow doesn't result in a speedup over OpenBLAS or similar on OS X or Linux. It certainly does.
After upgrading from Anaconda 2.4, mine gives the following message: &gt;&gt;&gt; import numpy Vendor: Continuum Analytics, Inc. Package: mkl Message: trial mode expires in 30 days The blog entry does say it's free, however. Still investigating.
How did you get that message? Importing numpy (or most other packages) usually doesn't spit out any messages. I just upgraded to 2.5.0. Here's what I got: &gt;&gt;&gt; import numpy &gt;&gt;&gt; 
Mah anaconda don't, mah anaconda don't.
Hey Ffuenga, I'm quite new to coding myself, and I've lurked around these types of threads for a while. In short--these things happen. I've seen it before where a newer developer like us finds something in the language or development process that feels like it could be made more intuitive, and the community seems to flatly disagree because of a greater context that the developer is unaware of. It's important that you be honest with yourself, and listen to the feedback you are receiving. Based on the others are saying, this solution creates problems. Computer Science uses the scientific method, which is basically an organized process of trying, failing, learning, revising, and trying again. It doesn't matter that it's useless. You undoubtedly learned a great deal while putting together your project, and the desire to improve the development process is admirable. Just take the experience, learn from it, and try again in the future. In many respects, it's great work. You found something that you wanted to change, implemented the changes successfully, packaged it up as a deployable solution, and gave a nice presentation of the finished product. Take that great motivation , move on to another project, and count this one a finished success. 
Maybe it's an anomaly on my system. I upgraded from 2.4 on OS X.
[Here are the official benchmarks](https://github.com/ContinuumIO/mkl-optimizations-benchmarks/blob/master/README.md)
I am working on fixing the globals (as there seems to be a witch hunt against them) and am thinking about the multiple source files idea. But I'm not sure. Having to refer to multiple files seems confusing, especially when calling functions and creates objects from classes defined outside the main .py file. 
I admit that `act_on_conf(with_read("myconf"), 42)` looks quite nice, but opening the file _is_ the important part. You have hidden it and made it look like there aren't any side-effects when there are. I would go so far as to say that wanting something like this is an indication that you might be doing your I/O in a bad way.
It certainly can. I'd be cautious drawing conclusions from such small matrices though.
I hate to break this to you, but I wouldn't call this powerful. Handy, sure. Simple, sure. But, it's also, at least the [8001st](http://pythonhosted.org/pg8000/) attempt to make a better driver. ...everybody always ends up crawling back to the DB API 2.0 compliant drivers sooner or later. Psycopg2 or pg8000 are the best, IMO.
I had the same feeling switching from Python to Go a while back, after having used Python for so many years and then coming to Go which has completely different formatting conventions again, initially I didn't really like it much. The difference with Go, is that every time I saved a .go file in my editor (Atom) with a Go plugin it will run a tool to *automatically* format the code on each save, so there is "no way out" really, while pep8 is more a guideline I think. Over time though, I have come to appreciate it.
Id love to hear from someone who does use trello for the purpose 
Alright, thank you for the thoughtful response. I'm just a college kid trying to improve my skills with a fun little program. I've been bashed on before for using global variables, but it's the most simplest way to get the job done. Yeah the simplest might not be the best, but I've tried breaking it down into multiple functions and it was just a mess and couldn't get it to work. &amp;nbsp; You're right about me trying to get fresh data quickly. This program activates a goal light and horn when my team scores, so I'm trying to reduce the delay of when that happens. Right now my delay time ranges anywhere from 3-40 seconds. If I refresh every 60 seconds as the API does, those times sky rocket. I guess I'll just have to bite the bullet since I'm coding for fun with a free API. You mention &gt; you'll write an algorithm that determines when the data &gt;changes and keep your update time at api spec. isn't that what I'm doing already? If I'm understanding you correctly, I don't see how it's possible to detect change in the data without loading it in like I am. That's like trying to read a book that you don't have. 
There's also https://postgres-py.readthedocs.org, which Kenneth looks like he [gave his blessing](https://twitter.com/kennethreitz/status/398852371952173056) to use the "for humans" tag a while back
/r/titlegore
Is there a way to upgrade my Anaconda 2.3.0 distribution? Or should I download and install the new distro?
Can you explain what the point would be?
So i don't need to renew all my expiring accelerate licenses?