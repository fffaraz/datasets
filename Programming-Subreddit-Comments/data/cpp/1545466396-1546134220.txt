Why should you write it everytime? Put it into a small utility library (I bet you have that anyway) and that's it.
I will always upvote a comment that speaks against the proliferation of header only libraries for code that really has no business being header only. 
I didn‚Äôt downvote your comment but the burden is on the library author (who presumably is the domain expert) to provide context on why existing solutions were deemed insufficient at the time of authorship. 
In all seriousness: Why does it have to be header only? What's so difficult about adding a cpp file to your build?
My apologies, I should have exercised a bit more restraint in my reply. The question however is very general and difficult to answer without additional information on his use case. But let me take another shot at it: I do not believe that Conan, CMake &amp; Ninja can offer the same functionality as bake, nor does bake offer the same functionality as Conan, Cmake &amp; Ninja. So without further information on what you're trying to do, I can't really argue for one or the other.
Ah no need to apologize to me, I just wanted to mention it since newer library authors tend to assume the reader will evaluate/compare/contrast the library against existing options. In practice, this just never works and even if it's not something you've researched thoroughly, it's definitely a useful way to spend the time, if not just to inform future design decisions in your own library.
well not all header-only libraries are slow to compile... have a look at [doctest](https://github.com/onqtam/doctest) - costs around 10ms to include the header ([benchmarks](https://github.com/onqtam/doctest/blob/master/doc/markdown/benchmarks.md#cost-of-including-the-header)). In effect it is just a bundled ```.cpp``` at the end of a ```.h``` (compiled conditionally in just one translation unit when an identifier is defined before including) and [many other libraries](https://github.com/nothings/stb) are like that. This type of libraries is indeed easier to use.
\&gt; I wanted to build a tool that, with a single command, could build a whole tree of projects, in the right order, based on the project dependencies. I've worked in environments that had hundreds of projects, with lots of dependencies, and maintaining a separate makefile that specified the order in which everything had to be built was pretty annoying. &amp;#x200B; This objective seems similar to the objective of colcon ( [https://colcon.readthedocs.io](https://colcon.readthedocs.io) , see [http://design.ros2.org/articles/build\_tool.html](http://design.ros2.org/articles/build_tool.html) for a related discussion ) and to a lesser extend CMake-based superbuild [http://robotology.github.io/ycm/gh-pages/git-master/index.html#superbuild](http://robotology.github.io/ycm/gh-pages/git-master/index.html#superbuild) .
&gt; # compile a program which uses C++11 features &gt; g++ -std=c++11 hello.cpp -o hello ./hello You should really consider getting a new compiler (newer versions of g++ use c++14 by default)
I still don't see the advantage of such hybrid source/header files. What is so difficult about adding a cpp file to your project?
Hmm, googling this does seem to reveal a few people offering C++ certification. I must say though, that I've worked with C++ programmers for a long time now and I've never come across anyone who had such a certificate or seen any hiring process where it was a factor.. Have others found this to be a thing? I know it is in other sectors, like network engineers etc..
Let me explain the background of the question. I work in the finance industry and even from pure Front Office side it seems that be able to code is a very important skill and this is growing. Many people like myself did some programming in engineering school. This was a long time ago however everyone still puts it on his resume and my goal is to differentiate myself a bit.
Open source projects are more valuable and trust-able than certifications. 
Certifications are maybe relevant for IT administrators or cybersecurity experts, but I heard of no such certifications for C++. Even if there is, that would be an "easy way out" and does not proove that you could solve real-life problems with the use of C++. A GitHub portfolio with side projects made with C++ worth much more. 
Certification in a programming language won't differentiate you. Just look at all the industry knowledge in a CppCon video. Ain't no way any certification can attest to the proficiency of knowledge in just a single aspect of the language.
Well I get it... but I won‚Äôt be able to build a GitHub portfolio on the back of my old knowledge from uni üòè, I need to find something in the middle
What's wrong with index &lt; vec.size()
What about `std::vector&lt;T&gt;::reference_type std::vector&lt;T&gt;::at(size_t index) const;`?
Checking `index &lt; v.size()` is sufficient for bounds checking, so I haven't seen such errors. No comparison against zero is needed because of promotion to `unsigned long long` and wrap-around. That said, this would be a good method for all types implementing a subscript operator, including `map`.
You should apply the rule of 5 rather than the rule of 3. This essentially means adding support for move construction/assignment as well as copy. A vector-like class is a perfect example of a class that can benefit from move operations as it can steal the contents of the soon to be destroyed source. You can also use the initializer list on your copy constructor rather than assignment in your copy constructor as op suggested for your constructor.
&gt;more And if you are worried that your index type might not match vector's size\_type (and possibly result in signed to unsigned conversion), then you should still correct your code and do the checking above the right way rather than adding a useless function to the already cluttered interface that vector represents.
throwing an exception is not the same as returning a bool. also, why deal with a reference to something if you are not at all interested in any specific element. at() can do bounds checking for you, but it would be less than elegant to do it this way.
It's a lousy idea. If employers start asking for some bogus certification, we'll need to start paying hundreds (if not thousands) per year for a worthless piece of paper if we want to find a job in the future. It won't actually guarantee skill in any way, but it will act as an expensive barrier to entry.
While the unique\_ptr solution with a custom deleter is usually appropriate as others have suggested, or otherwise a specific RAII wrapper, I have found an abstraction that is sometimes useful. The idea is that you create a local variable encapsulating a lambda that will do the cleanup in its destructor, unless the cleanup is specifically inhibited. So that if an exception is thrown before you call .inhibit(), the cleanup lambda is called. Here's a quick implementation and test case: [http://coliru.stacked-crooked.com/a/510b81273dfe73e3](http://coliru.stacked-crooked.com/a/510b81273dfe73e3). This is practical when you have a function that does some state changes but you need to make sure that if there is an exception in the middle, what was done is reverted. Like this: void function(int x) { insert x to data structure 1; auto cleanup1 = MakeDestructorWrapper([&amp;] { remove x from data structure 1; }); // The next line may fail (e.g. out of memory if for no other reason). // In that case, cleanup1 reverts insertion to data structure 1; insert x to data structure 2; auto cleanup2 = MakeDestructorWrapper([&amp;] { remove x from data structure 2; }); // More things that can fail. In that case, cleanup2 // runs first then cleanup1. something_that_may_throw(); // All done, inhibit cleanup. cleanup1.inhibit(); cleanup2.inhibit(); } &amp;#x200B;
is_valid_index is more readable. And it is easy to make the mistake: if (index &lt;= vec.size())
Though I have seen many times a 'int' being used instead of a 'sizet_t', so when the int is &lt; 0, you see the picture...
Start with something easy and work your way up. Even implementing the algorithms you know from university and implementing them really well is worth something. Good documentation, plots displaying their runtime, etc. Then just put this on github, as well as some mini-project like a trivial game that you put together making heavy use of libraries (there are tutorials for this, it's very easy). In the end most employers will already be happy if you fulfill this list of basic questions: 1. Can this guy write easy to understand and well documented Code? Does he structure his code well through functions? Are his variable names well chosen? 2. Does he know about asymptotic complexity of algorithms and data structures? 3. Does he make good use of libraries? 4. Does this guy know how to use git? 5. Does he make use of unittests? How complex the code is will not even be that important, the important thing is that you code **looks** like you care about readability, correctness (tests), asymptotic runtime.
Is there something build system for c++ like python, if you change one line of the source before running the executable then it automatically rebuild the needed sources?
I used your json library in one of my Unreal Engine 4 projects. Thanks Mr. Lohmann! Great work!
Nobody familiar with zero-based indexing is going to make that mistake. C++ isn't BASIC.
Yes, nobody makes bugs ;)
That's not a "bug", it shows a fundamental misunderstanding of how the language (family) works. C++ is not a language for beginners. You should be very familiar with basic concepts like zero-based indexing before you start using it for anything serious.
None. I have never seen one on a resume. Nor would seeing one influence whether I would interview that person. Its a waste of money.
This is a good idea with poor execution
I'm going to add namespaces and smart pointers in the near future. How else do you think it can be improved? I'm open to suggestions.
Think you replied to the wrong comment, mate.
C++20 introduces `contains`
Brainbench has several C++ tests: * C++ [https://www.brainbench.com/testcenter/taketest/C++/54](https://www.brainbench.com/testcenter/taketest/C++/54) * C++11[https://www.brainbench.com/testcenter/taketest/C++11/2994](https://www.brainbench.com/testcenter/taketest/C++11/2994) * C++ Fundamentals [https://www.brainbench.com/testcenter/taketest/C++-Fundamentals/1166](https://www.brainbench.com/testcenter/taketest/C++-Fundamentals/1166)
which has nothing to do with bounds checking.
Err what? With ¬¥contains¬¥ you cam check if a key is present in a map. Thats exactly what the top level comment said in the last sentence.
If you really need that service (*), having it as a free function would be much flexible: it could work with std::vectors, std::dequeue, std::span, std::string_view, std::string, std::array, C static array... Then, why restrict it to one single radom-access container/range? (*) personally, I hardly need to check the bounds
Think of `contains` as ensuring that the value is a valid key for a mapping. `vector&lt;T&gt;` has `T &amp;operator[](size_type)` (etc.), and can be seen as a mapping of `vector&lt;T&gt;::size_type` to `T`. Given the way the STL interfaces work, of course a vector and a map aren't really much like one another. But suppose you decided that it would be a good idea to have a function that would let you know whether an index was valid? - why, `contains`, by analogy to `map`, would be as good as any, and pithier than `is_valid_index`.
&gt;I think there was even a standard proposal for something similar flying around but I can't remember how it's called. [p0052r9](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0052r9.pdf) \- Generic Scope Guard and RAII Wrapper for the Standard Library
I've been a professional C++ programmer for 30 years and I've never seen anybody with such a certification, nor would I take one seriously if I had to hire someone. 
Thank you. Seems there is a consensus on which way C++ skills will be assessed, away from any certification. I suppose as well that depending on your field of work some are more useful than others. Still a bit scary because C++ programming is so unlimited that a man‚Äôs life is probably not enough. Hence the quest to find a consensual and safe way to get a good base.
thank you, I‚Äôm going to take a look
Why are you focused on C++? If you're looking to differentiate yourself as having some programming experience that would be useful from time to time, I wonder if something like Python would be a better fit...? C++ is powerful and I enjoy working in it but it's got a pretty steep learning curve for someone looking to differentiate themselves by learning additional skills. 
Point definitely made üòä
I am learning Python right now but I learned C++ at basic level a long time ago. I work for a bank as a trader and I would like to use programming to implement some algorithmic trading, and also to use bloomberg Api to use their live prices and calculate theoretical prices for equity derivatives (varswaps, vix, v2x etc). Most of he bloomberg doc is in C++. I also know C++ being compiled will run faster which can be an edge for trading strategies. In fact I see a growing demand from hedge funds for C++ coders despite the very good reputation enjoyed by Python in Finance.
&gt; No comparison against zero is needed because of promotion to unsigned long long and wrap-around. Not only is that undefined behavior, it gives a compiler warning https://godbolt.org/z/vh_Rx7
My concern with `vector&lt;T&gt;::contains(val)` is that it sounds like it should check whether a value is present, not whether an index is valid. (A `vector` contains values, not indices.)
JSON for configuration? Oh, noooo. Please use at least YAML.
unfortunately too slow in many cases compared to rapidjson
Nothing you develop by yourself, using Bloomberg's API, will be fast enough for the difference between c++ and python to matter. If you're not trying to transition to pure software engineering, focus on python
Let me amend that: There is an advantage in cases where the library resides in some sort of common package repository (e.g. system package manager): With the merged/ header only approach, every consuming project effectively gets the source code and compiles it with its specific project settings instead of having to hope that the flags with which the library in the repository was compiled are compatible with the ones used in the project. With all the conditional compilation going on in c++ this is a valid concern, but unless al libraries are header only, that problem has to be solved anyway.
do you have numbers? is the difference that big?
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/a8jjfh/what_is_best_online_certification_for_c/ecbw5zc/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; Not only is that undefined behavior [citation needed] &gt; it gives a compiler warning So don't use `signed char` for local variables?
&gt; You cannot just claim that O(n) is O(1) when N is finite. In this case, "N" is the number of template parameters, not the number of elements in some collection. What OP is claiming is that in general, most template parameter packs are sufficiently short that even a linear search over them is essentially O(1), and I think that's very reasonable. I mean, how many types are you likely to have in your `std::variant`, anyway? And there's a hard upper bound to the number template parameters, anyway: &gt; A search over the type space is allowable so long as it is bounded by a finite number of steps, which the compilers already "helpfully" provide by having limits on the number of template parameters that can participate in a pack. 
I don't have numbers handy but in my experience the difference is quite substantial (on the order of 5x or more slower). It quickly became the bottleneck in my server application. Too bad, because the API is quite nice!
Looks like I was wrong, it is defined. But it still does give the warning. Are you really suggesting to never use signed variables? That seems ridiculous
&gt; In this case, "N" is the number of template parameters, not the number of elements in some collection. *N* **is** the size of the variant's collection of template parameters. &gt; What OP is claiming is that in general, most template parameter packs are sufficiently short that even a linear search over them is essentially O(1), and I think that's very reasonable. That's irrelevant and not how Big-O works. If the number of template parameters is limited by the compiler, then indeed one could claim that any way to decide the right code to execute is in O(1). But then, why even include it in the standard? 
Vinnie - I just checked out this example of usage of dOxygen for generating a reference page for a templated C++ datatype. It illustrates one of the main beefs I have with dOxygen for this purpose. This example documents a function named async read which looks like this template&lt; class AsyncReadStream, class DynamicBuffer, bool isRequest, class Derived, class ReadHandler &gt; DEDUCED async_read( AsyncReadStream&amp; stream, DynamicBuffer&amp; buffer, basic_parser&lt; isRequest, Derived &gt; &amp; parser, ReadHandler&amp;&amp; handler ); If I want to use this function, I need to supply 5 parameters. With one exception, there is no information which tells me exactly which types can be substituted for these parameters. For example, what operations should Dynamic Buffer support? Does it need to return a "begin()", does it have to be random access? What is it a dynamic buffer of? char, wchar, strings? or .... . These questions only occur to me because I can attribute some meaning to name DynamicBuffer. This attribution on my part might or might not have any value. One cannot use this function without more information as to how to call it. Same goes for the other parameters. The one parameter with sufficient information is of type AsyncReadStream. It points to another part of the documentation which suggests it will list the operations that this type is required to support. I'm just presuming this of course as I can't see it here. At the least, dOxygen should be able to give you a hot link to the type. I don't know if dOxygen can be used to create and maintain correct reference documentation. I don't remember seeing any. My main complaint is that it gives one the illusion that he's creating documentation when in fact he isn't. 
I'm the instigator of the Boost Library Incubator. I'm very disappointed that it hasn't had the impact I originally hoped. There are a number of aspects to this which I'm going to list here. Feel free to take what you will from this. 1. Though it hasn't been the huge success I've hoped for, a number of libraries have passed though incubator on their way to be accepted as boost libraries. So it's not a total failure either. 2. I made it in part to promote my ideas how one should create a boost quality library. I see that it has about 400 hits a day so I'm inclined to continue to make it available. To this end I also gave a talk at CppCon 2014 on the subject - [https://www.youtube.com/watch?v=ACeNgqBKL7E&amp;t=1s](https://www.youtube.com/watch?v=ACeNgqBKL7E&amp;t=1s) 3. I learned a lot about creating a set of dynamic web pages using popular tools such as wordpress, php, etc. This satisfied my curiosity about what I've been missing and convinced me that I want to stay in the C++ world. 4. A few libraries have made their way into boost from the incubator. But more have not. This is to be expected. It often occurs that I have really good ideas but that not many others see the merit in. One can't let that discourage you. 5. Dropping a library into the incubator is useful. It advertises that you've made a serious effort and that your submission meets minimal requirements: it has a repo, it has documentation, etc... But it's not enough. Even the best idea needs to be promoted to gain traction. Be prepared to promote on the Boost mailing list, responding comments here on reddit, etc. Explaining your idea at C++ meetups and other conferences is also valuable in interesting. One good/bad thing is that you can get some feedback this way and often one will be disappointed that not everyone is convinced. You can improve your pitch, improve your code, improve your docs and try some more, or move on to something else hopefully having learned something from the disappointment. 6. Submitting a library to an organization like boost or the C++ standard is much, much, much more work that one would think. You are forwarned
It was about tooling for C++, so it‚Äôs kind of related. 
This question has come up a few times, so I've added it to the FAQ: [https://github.com/SanderMertens/bake/blob/master/README.md#why-use-json](https://github.com/SanderMertens/bake/blob/master/README.md#why-use-json)
Yeah, I just saw `signed char` but actually it's the same for `int`. Signed types are the better default, of course. Yes, I agree GCC `-Wall` behavior is a fair argument in favor of the proposal.
&gt; Consistent comparison (operator&lt;=&gt;) Is it really *that* difficult to implement this? Aside from the addition of the operator to the stdlib... it should just be adding a new operator.
I'd like to hear some advantages to CMake. CMake is far from perfect, but provides of functionality, runs on all platforms and has more or less become the standard. At least most projects run now with CMake, and I get annoyed, if a library I want to use doesn't provide a CMakeLists.txt ;-) For packaging I think the race is still open between Conan and vcpkg. 
Maybe he did not notice that himself does not want to do it. A patch is a minor version without API addenda. If there are never addenda between major release, there are no need to differentiate patches without addenda and patches with addenda. So the extra 0 is just noise.
CMake is more mature and feature rich than bake, but that's ok. I did not intend to write a CMake replacement or clone. Bake is a different kind of tool. I'll try to explain with an example. First, I create a new project with this command: bake init my_lib This creates a new directory `my_lib` with a `src` and `include` directory, a basic `project.json` file that contains build configuration, and runs `git init` to initialize a new git repository. I then change the `project.json` file to add `foo.bar` as a dependency: { "id": "my_lib", "type": "package", "value": { "use": ["foo.bar"] } } If I now run `bake`, it will: * discover projects in current directory * if the `foo.bar` project is discovered, build it before `my_lib` * generate a header file that automatically includes `foo.bar`, from `$HOME/bake/include` * build source files in `src` and stores object files in `.bake_cache` * link with `$HOME/bake/lib/libfoo_bar.so` and builds `bin/x64-darwin-debug/libmy_lib.so` * copy `libmy_lib.so` to `$HOME/bake/lib` so it can be accessed by other projects * copy the include files to `$HOME/bake/include` so they can be found by other projects * copy `project.json` of `my_lib` to `$HOME/bake/meta` so the package can be discovered If I now want to publish a version `1.0.0` of my project, I do: bake publish major This will: * add a `version` member to the `project.json` file with value `1.0.0` * git commit the change to `project.json` * create a new git tag with name `V1.0.0` If I then want to share this project with somebody else (who uses bake) I'll ask him/her to do: bake clone https://github.com/SanderMertens/my_lib This will: * clone the git repository * try to clone [https://github.com/SanderMertens/foo-bar](https://github.com/SanderMertens/foo-bar), or find it locally * build the two projects You can see how bake does a lot of things implicitly, to make sure that things "just work" with minimal configuration. If you want full control over your build, these things might actually feel like a disadvantage. Personally, I like the amount of time it saves me, and the uniformity it imposes on my projects.
Ok, think about this from a compiler writer's perspective: This would be *HELL* to implement, and even more confusing for readers of the program you're making. Please no.
Telling people how to program in C++ isn't going to work when you aren't sure yourself. What you can do, since you are motivated, is break down topics and ask experts, then organize what they tell you. Also take a look at cppreference.com since that is that is the standard.
Well as far as implementation. I don't think it would be difficult at all. We already have `auto`. So for variables, when you write: auto&lt;Type&gt; x = 5; it would be as if you wrote: auto x = 5; using Type = decltype(x); along with a few extra rules that disallowed conflicting deductions being given the same name. For parameters, it is similarly simple. when you write: void foo(auto&lt;T&gt; x); it would be as if you write: template &lt;class T&gt; void foo(T x); Why would either of these situations be harder to implement than what we currently have? Am I missing something?
The whole "In the program you can declare more random types as if it were a using statement" means we now have at least 3 syntaxes to declare random types, most people don't like that.
We already have a lot of ways to declare types or similar... // the obvious using T = int; typedef int T; // this is as if you have a limited scope where T is decltype(arg) when writing "f(arg);" template &lt;class T&gt; void f(T x); // this is kinda like if you have a one time use typedef to declare x decltype(arg) x = arg; 
In your case, the auto part is completely redundant. Why not just write void foo(T foo); The answer is, back when Bjarne designed the template system, he used this syntax, and it could work. It's the committee didn't have consensus on this because it makes it hard to distinguish from real types, and hence the template&lt;&gt; part was added. My point: There is a reason behind each syntax decision, practical or technical.
To everyone, this was not left here talk down about this project or burn your eyes out, so I am sorry if you were offended by this.
You can't do that, because T could legitimately be a real type. If we were allowed to use other symbols that are currently forbidden, I would have suggested this though: void foo($T foo);
As for technical syntax decisions, currently `auto&lt;T&gt;` is a syntax error, so we have the flexibility to make it mean something without breaking backward compatibility and we have the added bonus that it doesn't depend on new symbols being used.
You can as long as you don't have a real type T. You can check the concept-lite proposal in which they reused this syntax again, and now it's in C++20.
Indeed performance *is* an issue we need to address now that the API remained stable for quite some time.
Sure, if you determine that there is no type `T`, you can just assume that it must mean a template. But such code is terribly brittle in that if you include a header that happens to (and unbeknownst to you) defines a type T, then suddenly your code has an entirely different meaning. Based on that, I think it is fairly necessary to require at least **some** special syntax to say "this is a generic type" of some sort.
&gt; If I want to use this function, I need to supply 5 parameters You really do need to get out more. Every single one of those "5 parameters" is deduced, which should be obvious from the synopsis. Yes, I assume that readers of the documentation have a firm grasp of C++. None of these parameters are ever specified manually. A typical call site looks like this: http::async_read(socket, buffer, parser, handler); Note the absence of template parameters. Okay, now that we have dispelled the first part of your straw man, lets look at what is left. Two of those template parameters, `isRequest` and `Derived` are template arguments of a class template parameter. You would expect the documentation for those parameters to be on the class template (`http::basic_parser` in this case). And that's precisely where they are: `isRequest` true if this parser parses requests, false for responses. https://www.boost.org/doc/libs/1_69_0/libs/beast/doc/html/beast/ref/boost__beast__http__basic_parser.html `Derived` is a bit more complicated, but it is explained in the description that the interface to the parser uses the Curiously Recurring Template Pattern. This is all academic anyway, because unless you are an advanced user (in which case you already understand these things) you will not be using a `basic_parser` (like how no one uses `basic_string`). Instead you will just use `http::parser`. And what do you know, it is documented: https://www.boost.org/doc/libs/1_69_0/libs/beast/doc/html/beast/ref/boost__beast__http__parser.html And thus another element of fiction is dispelled. There are only 3 template parameters left, which are all deduced: AsyncReadStream, DynamicBuffer, and ReadHandler. And every single one of these is a hyperlink to the relevant concept: https://www.boost.org/doc/libs/1_69_0/libs/beast/doc/html/beast/ref/boost__beast__http__async_read/overload1.html Every parameter to the function is documented in the table under "Parameters": https://www.boost.org/doc/libs/1_69_0/libs/beast/doc/html/beast/ref/boost__beast__http__async_read/overload1.html#beast.ref.boost__beast__http__async_read.overload1.parameters The beauty of this workflow is that the reference documentation comes from specially formatted comments in the declaration. These comments look like this: https://github.com/boostorg/beast/blob/develop/include/boost/beast/http/read.hpp#L525 The best time to document a function is the precise moment that you write it, because the more time that passes when you don't document it, the more you forget and the worse that documentation will be. I like the Javadoc system for reference documentation because it eliminates the temporal separation and mental context switch required when this reference material comes from another tool used on another file. A reference is essential but it shouldn't be the only form of documentation (better than nothing though). And here Beast delivers as well, entire sections with example code and example programs explaining how to use these functions: https://www.boost.org/doc/libs/1_69_0/libs/beast/doc/html/beast/using_http/parser_stream_operations.html https://www.boost.org/doc/libs/1_69_0/libs/beast/doc/html/beast/using_http/parser_stream_operations/incremental_read.html Really, what more do you want?? At this point I am starting to wonder if you are really arguing in good faith because these objections quite frankly are invalid. Or maybe I am just not seeing it, does anyone else think that this generated Beast function reference is lacking? If so, what can be improved? (Please speak up even if you don't think it is lacking, so that I know you think it isn't lacking). 
Well, maybe the committee felt the same way as you does now which led to the current syntax. But again as I mentioned, the syntax was Bjarne's original design and it could work. And don't forget, in C++20 you can write T bar = foo(); and T can be a concept. 
That doesn't make it O.K. to add 5 more.
LOL, I agree with that, but... C++ is a language that provides many tools and often (recently) provides new tools to improve upon the ones we already have. I believe it is "OK" to add new features that not everyone will use because not all users are required to use them! There are plenty of features already in C++ that most users simply choose not to use. The question is, can we provide a new feature, even if technically redundant that results in simpler code which performs the same task? Personally, I do find: template &lt;typename X, typename Y, typename Z&gt; void foo(X x, Y y, Z z); to be more complex than: void foo(auto&lt;X&gt; x, auto&lt;Y&gt; y, auto&lt;Z&gt; z); at the very least, it's about half as much typing to get the same meaning.
Did you try any kind of optimization? \`basic\_json seems to use default containers. I guess that with a combination of proper allocation and replacements could do better? Not sure though. For sure something thought from the ground-up can be faster, but the API is very easy to use.
I don't understand your point. It seems to me that the line you just wrote is very different from declaring a parameter of automatically deduced type. Or maybe I misunderstood your example?
&gt; I believe it is "OK" to add new features that not everyone will use because not all users are required to use them! I am not required to write them, but I am required to read them. The more esoteric features there are, the harder it will be to understand other peoples code. Everyone should read [Remember the Vasa](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0977r0.pdf) that Bjarne Stroustrup wrote earlier this year. 
In both cases types are deduced, and I was using the OP's other proposed syntax as an example. Also, in C++20 this one is known to work, but to be honest I forgot whether the function parameter in-place concept syntax got accepted or not.
You should really check out the many template introducer syntax proposals. The last one was from Herb I think.
&gt; I am pretty sure there is a huge number of C++ code that does this kind of bound checking Why would there be? Where is your index coming from that it could possibly be invalid? The only source of invalid indices is bugs ‚Äì this doesn't help with _that_ in any way, and in fact it makes things worse.
Maybe you have already heard about defensive programming? In the same way as you may check pointers against nullptr, sometimes you may check for valid indexes. If the standard has created std::out_of_range, this is not without reason. If you don't want to use exceptions, then "is_valid_index()" might be useful. 
If you've already got a header-only library or are offering one, you don't want to depend on any non-header-only libraries. The users appreciate that. They also appreciate the simplicity of just cloning a repo, adding an include path and being able to do `#include "mylib/mylib.hpp"`. C++ build systems and package managers have come some way in the last few years (CMake, conan, vcpkg), but we're not at the stage of simplicity of `pip install mylib` for **all platforms** yet.
According to the cppreference page it doesn't seem so. Thank you for clarifying.
&gt; Maybe you have already heard about defensive programming? I've certainly heard people cite it as the go-to excuse when they don't understand basic separation of concerns and that input should be sanitized _well before_ it gets this close to touching your real data structures. &gt; In the same way as you may check pointers against nullptr Again, this occurs somewhere well away from my actual data structures, in an entirely different tier of the application/library. &gt; If the standard has created std::out_of_range, this is not without reason. The reason is that David Abrahams didn't teach his lesson that preconditions should not use exceptions early enough on.
The syntax with `auto` you're describing is similar to what Herb proposed in [P0745](https://wg21.link/p0745). The other syntax is similar to what the originally proposed template syntax was: void foo&lt;class T&gt;(T var); That is, no `template` keyword. That keyword is technically unnecessary on all declarations... the leading `&lt;` is enough to disambiguate from the compiler perspective: &lt;class T&gt; void foo(T var); That could probably be made to work. Is it worth it to? 
https://prometheus.io/ https://opentracing.io/ https://www.jaegertracing.io/ 
Massive syntactic reductions to existing, widely-used features do not constitute "esoteric features".
&gt; You cannot just claim that O(n) is O(1) when N is finite. But that's what big Oh analysis means. It's a statement about the behavior of the complexity function where you can find values a and c such that aO(n) + c &gt;= f(n) as n approaches infinity; it completely breaks down in the presence of N that never gets over a few hundred and never close to infinity. Given that library complexity requirements in the standard are generally specified in terms of the number of user visible operations (e.g. applications of the less than function), and there are no user visible operations here, one could argue that the entire complexity requirement is vacuous. If the intent were really to say that the time taken couldn't depend in any way on the input then it would be an unimplementable requirement, as even the jump table solution will result in time dependent on the input in the face of branch predictors and caches on real hardware.
&gt; &gt; In this case, "N" is the number of template parameters, not the number of elements in some collection. &gt; N is the size of the variant's collection of template parameters. For library requirements N is usually some number of user visible operations. For example, some operations in deque are listed as constant time, but that's a constant number of manipulations of the elements in the container, not a constant amount of time overall, as the index map often has linear in the size of the container operations. Similarly, the complexity requirements on something like sort are specified in terms of the number of less-than invocations, not overall algorithm running time. If you've got a sorting algorithm that takes quadradic time but only does n lg n user visible less than comparisons, technically the standard allows that. The reason this doesn't matter in practice is because standard library implementers aren't idiots trying to make the most hostile implementation possible. The spec is clear enough on what the intent is, and we do that. We are only going to break out the "well technically the spec allows this" in cases like this variant example, where a jump table based solution results in substantially worse code than the comparison based solution. (And in any case, we only do the comparison based thing for N&lt;128, so even if it were possible for N to approach infinity it wouldn't matter)
Thanks! 
I feel like a simplified template syntax does the contrary of what everyone says: it makes reading code more difficult. It adds no new features, but rather another thing for people to learn that is functionally identical to existing constructs. We don't need n different ways to declare a template, variable or function, we only need one. Will those few characters you save really be worth making your code that much more difficult to read?
this kind of attitude can only serve to delay or prevent improvements to the language simply because there's already other ways to achieve something.
You seem to be making two claims here. One I believe, the other I don't. I believe that it is offering an alternative way to do an existing thing. I agree it is not adding a new feature. I don't agree that it is harder to read in any way. Any new feature has a minor amount of overhead in learning what it means, but once you know it, you know it. It's not like it's ambiguous, in what way is it "more difficult to read" other than you'd have to learn something new. I'd argue that we've gotten recently lots of features that don't add something new beyond syntax. ex. if(T x = foo(), t == 5) { ... } // offers nothing new but better syntax or how about lambdas? I can do the **same exact** thing by writing a class with function call operators. It doesn't offer a new capability, but sure adds new syntax... and it uses a syntax that **many** would find confusing until they learn it. I mean, really, did we **need** to make the following valid code? [&amp;](){}(); you show that to a non-c++ developer or one who hasn't seen c++ in about 8 years, and they'll have literally no idea what it does at all...
That is probably true, but depends on adoption. We don't know until afterwards if a lot of people will adopt it or if it is one of those things you rarely see. But even if it gets widely adopted, it adds to the burden of things you need to understand. New people need to learn both this and the old style. I kind of like this new suggested syntax but it is not enough of an improvement over the existing syntax to be worth it.
I love this: void foo&lt;class T&gt;(T var); Or adopt a more Pascal-styled, strictly left-to-right definition syntax and move away from the hard-to-parse &lt;&gt; to \[\] then we have: foo[T:type](var:t):void;
So this seems optimal: struct A{int x;}; struct B{ A a; }; struct C{ A a; }; using foo=std::variant&lt;B,C&gt;; A* get( foo &amp; f ){ if (f.index()==-1) __builtin_unreachable(); if (f.index()==0) return &amp;(std::get_if&lt;0&gt;(&amp;f)-&gt;a); if(f.index()==1) return &amp;(std::get_if&lt;1&gt;(&amp;f)-&gt;a); } Result with -O2 gcc is: get(std::variant&lt;B, C&gt;&amp;): mov rax, rdi ret 
Yes, but as Bjarne points out: Even if each individual change is an improvement in the local scope, it can lead to a disaster in the global scope. I'm not against change. I even think we should sacrifice backwards compatibility in some cases, to make improvements to the language. But I am against thoughtlessly adding new features without thinking about the big picture.
I'm talking from a users perspective: What is the advantage for me as a user when a library I use is header only. Adding one or a few cpp files to my project is absolutely trivial.
"Absolute C++" for theory and exercises, geeks4geeks for beginner practice.
You'll probably get a better response at /r/learnprogramming/
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/a8q6uk/new_to_c/eccsk4n/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Thank you, will check it out :D
Makes sense, thank you! Sorry for wrong post 
Declaring generic types like `void foo&lt;T&gt;(T x, T y)` would be a lot clearer and have a better design overall. C++20's lambda templates are already closer to this syntax: `[]&lt;typename T&gt;(T x, T y) {}`.
We should fix the standard, not "creatively" read problematic requirements into meaninglessness. Especially if the "creative" reading also makes every other complexity requirement in the standard meaningless (because in the end everything runs into implementation limits).
I think it is an example of what herb means by "Making c++ code simpler by adding features to the language". The reduced signal to noise ratio makes the code objectively easier to read. Wether it will be a net win will of course only become apparent afterwards, but in general, I'm pretty much in favor of any feature that makes c++ code less verbose.
Google and Facebook address the programmatic vs. declarative approach using Starlark to maintain some programmatic functionality without all the bells and whistles of a full programming language.
&gt;You really do need to get out more. Every single one of those "5 parameters" is deduced, which should be obvious from the synopsis. I'm not disputing that the parameters are deduced. The question is what types are permitted. For example, can I write?: char buffer\[42\]; async\_read(socket, buffer, ... I'm guessing not. But it's just guess because the documentation doesn't tell me what requirements of the second parameter. I see no way to discern this. What I expect to see somewhere is something like: Dynamic Buffer Notation: db - an instance of a dynamic buffer n - size of a dynamic buffer in characters valid expressions: db(n) - constructor db.begin() - return start of buffer db.end() - return end of buffer \*db - return current character \++db - increment pointer to next character... Now, given an instance of some type, I can know a priori whether or not it is allowed to use it as the second parameter. This information maybe somewhere else in the documentation but it's not readily available here. This method of defining sets of permissible types in terms of the valid operations on those types seems somewhat awkward but if one thinks about it, it's the only way that it can be done. The C++ standard says as much: **15.4 Method of description (Informative)** ... **15.4.1.3 Requirements** ... "Requirements are stated in terms of well-defined expressions that define valid terms of the types that satisfy the requirements. For every set of well-defined expression requirements there is either a named concept or a table that specifies an initial set of the valid expressions and their semantics. Any generic algorithm (Clause 23) that uses the well-defined expression requirements is described in terms of the valid expressions for its template type parameters." I've tried to illustrate my point for the second parameter. The same considerations would apply to the other parameters but the examples would be more complicated so I'll stop here for now. One more thing: You've used names for dummy parameters to inject meaning into them - presumable the name of some set of type requirements (aka concept) which instances of the type are required to fulfill. In itself, this isn't too bad, I believe that concepts lite uses a similar idea but with a different syntax. Overloading the name of the dummy parameter could be confusing in some cases. In other cases it might suggest that something has been specified when in fact it hasn't. So I'm not convinced it's a great idea for now. Finally if the type requirements for "Dynamic Buffer" are actually defined somewhere else, you don't need the specify all the type requirements above every time. You can just refer to the place where they are defined (hopefully with a handy link) Notation: buffer - instance of a type meeting requirements of DynamicBuffer. Finally, you don't need to convince me that async\_read ... is documented. I see you've got a lot of information all over the place. The problem is that it's not available in the place where you need it. This makes it a lot harder to use and a lot harder to write.
&gt; the documentation doesn't tell me what requirements of the second parameter. I see no way to discern this. What I expect to see somewhere is You mean like this? https://www.boost.org/doc/libs/1_69_0/libs/beast/doc/html/beast/concepts/DynamicBuffer.html Come on man...the template parameter is a hyperlink like I have said a few times now!! Hopefully this image will clear it up for you: https://i.imgur.com/wTJKkER.png 
I don't think so. The word template may be unnecessary, but it helps comprehension.
You know what they say... "Out-of-Process, out of mind!"
Oh wow. I've thought about letting users build drivers with Lua (because it's easy to embed), but this looks interesting. I'll look into it.
There's no versioning on artifacts pushed to the bake_cache? I may need to work on a few different versions of a library at the same time, or applications which themselves require different versions of dependencies.
No.
LOL - touche - I didn't occur to me that that was a link. I didn't hear you say that. Of course it's obvious to me now. Sorry about that. So you've convinced me that it IS possible to make Doxygen create useable reference documentation for templated C++ types and functions. I think it would take more work to make things as transparently understand able as the could/should be. &amp;#x200B; &amp;#x200B;
Well, you can say the same about "auto" or even lambdas (as they may be implemented as structs with operator() implemented).
&gt; So you've convinced me that it IS possible to make Doxygen create useable reference documentation for templated C++ types and functions. Yes, it is possible. But you need to have a good program that translates the XML produced by Doxygen into the format you want, with the style you want. That's what this is for: https://github.com/vinniefalco/docca/blob/master/include/docca/doxygen.xsl Granted, it can be improved - there a number of bugs: https://github.com/vinniefalco/docca/issues I've been working on it... 
No problem
It's in Apple's clang compiler as a part of XCode 10, which I believe needs to run on Mojave. 
You're the reason we can't have nice things.
Another feature I'd like is to be able to limit template types to just a list. for example, you're operating on strings, you only want your string functions to operate on utf-8, utf-16, or utf-32 types.
Call me crazy but I like the ever-so-slightly more verbose version. Such things make parsers easier to write. Introducing a new syntax will just bloat the language more at this point as well.
You're crazy!
cppreference has the current syntax which doesn't have the shortform. IIRC there is a proposal for short form but has not been voted in the standard yet, though it is highly likely in the next meeting.
How about void foo(&lt;T&gt; foo);
&gt; `foo[T:type](var:t):void;` I'm not fond of this.
`void foo(&lt;T&gt; foo)` `void foo&lt;T&gt;(T foo)` 
Templates can be implemented with macros.
It means, "is this pointer not null". The if statement is entered if the expression evaluates to not zero, so if the pointer is not null, the condition is true. It is effectively equivalent to if (nodePtr-&gt;right != nullptr). As a note, in your example, you have a single equals character, which means "assign" and not "test for equality". Testing for equality requires a double equals.
There is an implicit conversion of pointer type to bool. NULL value is converted to false while any other value is converted to true. 
Hi there. First of all: if (nodePtr-&gt;left) if (nodePtr-&gt;right) NULL equates to 0, so these are just checking to see if left and right have an address assigned to them. Then, this: if (root=NULL) This is typically poor code. A single equal sign is assignment. A double equal sign is comparison. The above statement assigns NULL to root and then checks to see if root is NULL (which it is because it was just assigned to instead of compared against). What you really want is: if (root == NULL) Or: if (!root) As an aside, when you put code on reddit, use 4 spaces before each line of code. It formats it appropriately and makes it easier for the viewer to read the code.
In C/C++, any none 0 values will pass if test. NULL is just a macro defined in some header files, which value is 0.
I don't disagree to it being easier to read. I just don't think it is better enough to be worth it. We can't include every minor improvement someone can think of.
Follow up question: I see people using NULL all over instead of nullptr. Is there a good reason for it or is it just that people are using whatever they're used to? I personally prefer nullptr because it gets nicely syntax highlighted, instead of looking like a generic define as with NULL.
Well, I don't agree, mainly because the fact that it is "just" syntactic sugar also means ot is not difficult to understand/learn. But I don't have any real experience with the feature yet, so I might overestimate its worth.
Found the type theorist!
&gt; lambdas /.../ It doesn't offer a new capability But it does. It gives you the capability to put the code where you want to read it. This is a huge benefit for readability. But I totally agree the syntax is confusing. If there were no legacy it would for sure not look like that.
The only reason is when writing code for non C++11 compliant compiler.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/a8tauy/what_does_it_mean_if_nodeptrright_in_binary/ecdkkdw/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Yes. You also benefit from the powerful variables implicit capture ability which is not an option if you don't use lambdas.
Consider scope guard from boost as well if you can use boost.
See https://cor3ntin.github.io/posts/template_syntax/#down-with-template This article was written before San Diego and never published. (though it ended up on Reddit anyway because I messed up) I think it's worth exploring further :)
As /u/TomSwirly correctly claims, n, in this case, is the number of template parameters. It is not a constant, it grows as you add types to the variant. Can it grow big enough to be a problem? Possibly not, but that doesn't make it O(1). It is still O(n), except most people won't care. O-notation is property of algorithms. It doesn't say anything about specific data sets, it makes no claims about actual time used, it never 'breaks down', and it certainly has nothing to do with branch predictors or caches (I'm assuming you had a few beers when you wrote that). As for how many types someone is likely to have in his variant: if I rewrite my formula engine to use variant instead of its current OO, inheritance-based design, I'll end up with 93 types. Is that enough to start caring about O(n) lookup, do you suppose? 
Looks interesting, but why yet another build tool? 
Old addins didn't work for new versions of visual studio anyway without some recompilation/packaging. Most of those addins could just be recompiled for x64 without any significant code change.
 template&lt;class S&gt; requires UTF8String&lt;S&gt; || UTF16String&lt;S&gt; || UTF32String&lt;S&gt; void f(S); seems good enough to me. It might save a few keystrokes if this is allowed: void f(UTF8String || UTF16String || UTF32String s); ... but the benefit might not worth the trouble.
Java's generic methods look like that void &lt;T&gt; foo(T v) void &lt;T extends E&gt; bar(T v) 
What? Really? After years of reluctance since "contains easily allows to write slow code" this finally arrives? Does it return an optional iterator or similar to still allow for fast code? Is there more info on that?
You know: I do actually believe that it is a cost issue too, because a fucked up code base with insufficient test coverage so that they are afraid to touch it is the only reasonable explanation I can find for shying away from 64 bit. What they are apparently trying to do is to move individual chunks out of process which are small enough that they can port them to x64 with reasonably confidence. 
I'm the wrong person to shoot your sarcasm at. All i did was look on cppreference.
But if my library needs yaml-cpp (not header-only) as a dependency, then you have to either install yaml-cpp from a package manager - which means on Windows you may need vcpkg, if it's in there, compile it by yourself, on Linux you can probably try the system package manager, but good luck to have the versions match (my library may depend on the latest version, and the one in Ubuntu 16.04 would be ancient - just hypothetically, not yaml-cpp specific), or you have to build yaml-cpp as part of the build script of my library. Which means you need to buy in to my build-system or at least know how to use it. Or you need to add my libary's .cpp files to your build, _plus_ the yaml-cpp .cpp files. And that for each dependency.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/a8ugs7/cpp_newbie/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
 void my_c_callback(void* array, int n) { std::vector&lt;int&gt;&amp; v = ...; if(n &lt; v.size()) { // boom } }
&gt; The word template may be unnecessary, but it helps comprehension. no it does not. People writing ML-like (caml, haskell, etc) code have exactly zero problems doing so even if types aren't written at all most of the time. 
See the [rationale](http://design.ros2.org/articles/build_tool.html) here. I guess the tl;dr is that the use case it supports is building *multiple* packages together - i.e. multiple CMake and distutils projects that may or may not depend on each other. CMake alone doesn't solve that problem in a scalable way, so they added extra infrastructure and tooling around existing build tools.
This looks kinda like buildstream for poor windows users. I like it!!!
ruke of 0. template&lt;auto x&gt; using k=std::integral_constant&lt;std::decay_t&lt;decltype(x)&gt;, x&gt;; struct ClassA { ClassA(): member1(liba_create_objA()) { // some code that might throw member2 = delctype(member2)(liba_create_objB()); } ~ClassA() = default; std::unique_ptr&lt;AType, k&lt;liba_free_objA&gt;&gt; member1; std::unique_ptr&lt;TypeB, k&lt;liba_free_objB&gt;&gt; member2; }; c++17, but 11/14 just requires more verbose `k`. }
Well one could use CMake ExternalProject, though is not so comfortable, to do exactly that if each project has it's own repo. Or develop in a monolithic way where every project is a sub directory. Both are not easy and have their own cons.
If the yaml library is header-only you also have to get it somehow and you are suceptible to the same version problems. That is nothing unique to compiled libraries. &gt; Or you need to add my libary's .cpp files to your build, plus the yaml-cpp .cpp files. And that is what I keep asking about: Why is that a problem? Just because there are libraries out there that are freaking complicated to build doesn't mean building libraries has to be difficult in general. If you have any nontrivial c++ program you are anyway using some kind of build system that compiles cpp files. Adding a few more cpp files should not be aproblem, even if that library uses a different build system than you. 
Those are *totally* different languages, and their syntax basically can't carry over. There's no point introducing yet another redundancy in C++ because someone wants to be cute. There are much bigger fish to fry than an occasional explanatory keyword.
Good bot
&gt; Well one could use CMake ExternalProject, though is not so comfortable, to do exactly that if each project has it's own repo. Right, that's what I was getting at when I said CMake doesn't solve the problem in a scalable way. &gt; Or develop in a monolithic way where every project is a sub directory. The ROS ecosystem is too big and too distributed for a single repo model to scale. However, colcon and its predecessors are basically trying to enable this workflow without a single code repository. You can build/develop multiple projects in their own subdirectories as though it was one large project, but they don't all have to come from the same vcs repo.
You should check out FunctionalPlus; it has a quite good API https://github.com/Dobiasd/FunctionalPlus/blob/master/README.md It also provides an API search site √† la Hoogle http://www.editgym.com/fplus-api-search/ The author also hosts a very interesting course on functional programming in C++ on Udemy 
!removehelp
Conan is the most promising one for C++ by far
It does because we're used to it. And things just plain look weird without it. But on some level, it does add zero information on top of what the angle brackets indicate: template&lt;class T&gt; void foo(T); &lt;class T&gt; void foo(T); Unlike the various terse concept syntax conversations, the bottom row really does have _all_ the information that the top row has. And indeed, such a change would get you most of the way to terse syntax too? void foo(Concept auto); &lt;Concept T&gt; void foo(T); But I'm personally happy with where we are, the `template` keyword never bothered me as much as it does other people.
There we go with the discussion ;) I will do the same for conan as already done for vcpkg
&gt; n, in this case, is the number of template parameters There‚Äôs nothing that says that. N is a compile time constant. The requirement is that the running time is constant, and it is. That‚Äôs why the requirement is vacuous. &gt; it never 'breaks down' Of course it does. It‚Äôs reason for existence is analysis when N is huge. It isn‚Äôt the right tool for when N is not huge. &gt; it certainly has nothing to do with branch predictors or caches (I'm assuming you had a few beers when you wrote that) No beers :). It may assume the RAM model of computation, but nobody has made a machine that even comes close to approximating that in 40 years. Implementers aren‚Äôt targeting a hypothetical RAM machine, we target real machines. &gt; if I rewrite my formula engine to use variant instead of its current OO, inheritance-based design, I'll end up with 93 types Doesn‚Äôt sound like variant is the right tool for that. It doesn‚Äôt matter anyway; customers‚Äô use of variant doesn‚Äôt often do that, so implementers aren‚Äôt going to optimize for it. And honestly, I‚Äôm not sure the 93 cmps would lose. Unpredictable indirect branches are fantastically expensive. And if it‚Äôs a predictable indirect branch then the direct branches we emit will be predictable too.
Bryce Lelbach comes to mind. Particularly his talk about having a good statistical approach to performance analysis. A lot of the time, scientific computing means high performance/parallel computing. A few talks that come to mind include: Mark acton (2014) on data-driven design Chandler carruth (2014) on algorithms/data structure performance Chandler Carruth (2015) on profiling performance (micro benchmarking, rather than Bryce‚Äôs ‚Äúmacro‚Äù benchmarking talk) I‚Äôve also learned a lot from Pablo Halpern‚Äôs talks on parallel computing. 
I‚Äôm sorta ignorant, can you explain to me why a 64 bit VS would be superior to a 32 bit? Is it because addressable space is limited to 4 gigabytes in a 32 bit program? Or am I way off?
Not cppcon, but I recommend &amp;#x200B; [Scott Meyer: CPU Caches and Why You Care](https://www.youtube.com/watch?v=WDIkqP4JbkE)
Don't know about speakers, but I recently saw these two talks. I thought they were good. https://youtu.be/CPPX4kwqh80 - Multidimensional arrays https://youtu.be/KHa-OSrZPGo - Not exactly scientific computing, but I think knowing how CUDA works would be useful
I'm pretty sure it has come up before, but the number of if checks is a compile-time constant, so it can still be seen as fulfilling the O(1) requirement.
So you know std sort? Everyone sane does quicksort except on small lists where it does an O(n^2) implementation. In short, this is what std implementors do.
Could you please explain why and how is Conan better as vcpkg?
A default copy constructor and copy assignment operator are generated for you. You can disable it by declaring them to = delete;
This is not assignment: Type name = value; that is construction. This is assignment: name = value; 
Your constructor is not tagged with 'explicit', therefore it takes part in an implicit conversion from T*. Try putting explicit in front of the constructor and see what happens then. 
As another commenter mentioned, `T t = T(...);` is not an assignment; it's a form of initialization. Assignment is changing the value of an object that already exists. However, as a different other commenter also pointed out, na√Øve copy and move assignment operators are generated for you. Note, though, that the auto-generated operators in your case will be wrong! They will just copy the value of the pointer, whereas you need for the copy operator to be deleted or allocate a copy of the object or some other reasonable thing and for the move operator to invalidate the pointer in the moved-from object!
they won't be generated if you define a custom constructor.
how is smartpointer&lt;T&gt; convertible to T *?
Like I said in other places, I think that the keyword helps simplify parsing, if nothing else. That may seem like a minor benefit in such a large language as C++, but introducing another syntax at this point would be unnecessary bloat and yet another thing that could be abused in ways we can't yet imagine.
This pointer isn't actually very smart. 
The custom constructor only prevents the default constructor from being generated, it does not accept the copy operations.
His talks are extremely good, thanks!
just make RAII wrapper classes, one for objA, one for objB. The wrapper class can be as simple as a typedef to a unique\_ptr with a custom deleter. RAII all the way makes things easy. RAII nearly all the way is a world of pain.
Wonderful talks, thank you!
Using a jump table if N &gt; 64 and a binary search if N &lt;= 64 would still count as O(1). 
Great stuff, thank you!
Very nice indeed, thanks!
It seems like the bug here is that an unsigned integer is being compared to a signed integer without checking for wrap-around (due to the signed integer being treated as unsigned for the sake of the comparison operation). One would need to handle the negative signed integer case (probably casting the unsigned integer to signed, with possibly more bits). [https://gcc.godbolt.org/z/s4B1Aj](https://gcc.godbolt.org/z/s4B1Aj) &amp;#x200B;
I was playing with finite sets, and generate switchs to match the number of indexes possible seems to lead to the better code : [https://godbolt.org/z/R3LtSl](https://godbolt.org/z/R3LtSl) (scroll to the end for the part where the variant is visited, among other things)
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/a8ydga/text_based_graphic_games/ecf0829/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
nope.
The main argument for conan (to me, at least) is that it supports proper versioning which vcpkg still lacks. So you can say you need not simply Boost, but Boost 1.65, stable branch, from your own providers e.g. It is also a distributed system and it's very easy to setup even a simple server for your team packages, with possibly extending it to Artifactory later. You can also host your own forks of packages on your servers. Another one, though I am not sure how vcpkg jandles it: you can either download prebuilt binaries for your OS, compiler, stdlib and arch, but also build it from sources if none are found. I also believe conan was cross-platform from the start, while vcpkg expanded to Linux comparatively recently.
Hardly the same thing. "small lists" for `std::sort` is not even close to the largest input size that most users will ever use, or for that matter what any user could possibly use (for the argument invoking implementation limits). If a complexity guarantee is only realized/useful for a tiny fraction of users, while also requiring implementers to engage in "creative" interpretative dances to stay technically conforming, then we should simply remove it altogether.
Why? What makes you think you are qualified to do this? How does this improve on intro books like "A Tour of C++" by Stroustrup himself?
Why? Because it can be helpful for beginners. Also "A Tour of C++" is not a "quick reference" which you can use for quickly (in a couple of seconds) looking up things like how to create a vector of 10 ints each initialized with 1. 
Your godbolt link has a mountain of code. If you're interested in this I'd suggest going through the issue I opened on mpark variant, the godbolt links, and the linked PR (with more godbolt links) on various approaches: https://github.com/mpark/variant/issues/51. We're able to get quite optimal assembly using get_if (this is just for explanatory purposes; actual PR changes the visit implementation internally). I can assure you when done right, that get_if is sufficient to get optimal code gen. You need to anyway check the index and switch on it, so as long as the get_if function gets inlined you won't have any redundant checks. In my experience it does get inlined. 
So, the solution that I'm working on with Michael Park right now (though it's been on a bit of a backburner most recently), is recursive switch case. I think this is the best all around solution from everything I've seen; totally general, very good assembly (much better than chained if-else for larger variants). It does technically violate O(1) as well (you eventually have a # of recursive calls that scales linearly with the variant size), but I think the plan is probably just to ignore that. I think the O(1) wording was a big mistake, given that a) the counted operations are not very clear (unlike with say containers), b) you basically always care about quite small N; N&lt;10 is the vast majority of cases and N&gt;100 is nearly non-existent (at least, I hope so).
I have not used vcpkg, but I have used Conan pretty extensively. Just based on reading the documentation, here are my thoughts: 1. For context, we deal with 5 platforms (Windows, Linux [RHEL], AIX, Solaris, and HP-UX) and our development/build environment does not have direct internet access. * **Conan**: All existing community packages are useless * **vcpkg**: All existing community packages are useless 2. We depend on proprietary third-party libraries (e.g. Oracle) and not all internal source code is available to all developers so being able to use pre-built libraries easily is a must. * **Conan**: this use case is pretty much central to their whole packaging model and absolutely trivial to do. Package recipes are identified by `name/version@user/channel`, package recipe gets downloaded and executed to generate package id (which can use absolutely any information: the build platform, the target platform, compiler, compiler options, dependency versions). Given the computed package id the client tries to download it from the server or can build the package if desired. In our usage we never build from source to prevent inadvertently using incompatible sets of dependencies, and ensure that the full version of all dependencies is used when determining the package id. * **vcpkg**: the tool does nothing to facilitate this use case. You are expected to write your own implementation in CMake per [here](https://github.com/Microsoft/vcpkg/blob/master/docs/about/faq.md#can-i-use-a-prebuilt-private-library-with-this-tool). 3. Different projects have different release cycles and risk profiles, both of which are decided by individual project and development teams. As a result we want to easily maintain dependency versions on a per-project basis. * **Conan**: When packages are downloaded locally these packages are organized into the local user-specific repository under `package_name/version/user/channel/packages/package_id` folders and then any combination can be consumed by any projects by specifying the desired reference `name/version@user/channel` (package_id is generated dynamically as mentioned above). * **vcpkg**: (if my understanding is correct) assuming we had an internal vcpkg repository that had the recommended set of dependencies specified at the recommended versions then having to use a different set of versions would require a branch/tag for the repo that had the state I wanted. If such a state didn't exist then I would need to fork and push back to a particular branch for my project. The governance of that project seems like it would be complex, e.g. who has approval rights on which branches. 4. Given we have to deal with binaries, we need a binary repository manager. * **Conan**: we use Artifactory to manage our generated artifacts (and CI build logs) which integrates well with CI (e.g. web hooks on packages, build status API, `conan upload`) and is good for package discovery via the web UI or from the command-line (`conan search`). * **vcpkg**: it seems like creating binary packages is OK but the consumption side needs work per [here](https://github.com/Microsoft/vcpkg/blob/master/docs/about/faq.md#will-vcpkg-support-downloading-compiled-binaries-from-a-public-or-private-server). We would have to come up with the binary repository layout, HA setup, authentication, integration with CI, on our own. 5. We don't want to burden developers too much with details of the dependency management. * **Conan**: We wrap Conan in a CLI so typical developers need to know no Python and just provide a yaml file in their project root which gets converted to a CMake toolchain file with all dependency paths. Once Python was compiled this was relatively painless to develop. * **vcpkg**: Given a developer needing a new version of a package would need to create a fork of the vcpkg repo or get consensus to update some common branch, I don't see how we could hide these details as well as we have while also satisfying the above-mentioned concerns. Where "better" means "less effort for satisfactory results" I don't think one is universally better than the other. Conan is more flexible and our specific requirements and company culture made getting it to meet our requirements more approachable than changing the requirements.
Scientific computing like comp sci data structures, or like stats, or like data science, or something else? There are a few kinds of scientific computing. Super computers? GPU programming? ...
&gt; "One is that the standard library implementation isn't strictly speaking O(1)." Oh yes it is. *Formally* speaking, it is absolutely O(1). If you think it is not because it is faster for very small sizes, you absolutely don't understand the O notation.
Does conan also differentiate between different builds in c++11/14/17 for libraries that support multiple standards? The lack of proper support for that can sometimes be an annoyance in vcpkg.
I've only been doing C++ for a few months, but to get around that particular issue I like to use a char instead of an int. int size = 3; cin.getline(input, size) if(isdigit(input\[0\]) &amp;&amp; input\[1\] == '/0') //if input is a digit and is only one digit long if(input &lt; 1 || input &gt; 2) etc etc. 
This is not the subreddit for asking such questions. Read the sidebar. Please post your question to /r/cpp_questions and format your question according to the stickied post. 
Oh sorry, I never knew, I'll delete post.
Technically since there is an upper limit on n everything can be considered O(1)
If you felt needed, why not just make a free (template) funciton?
Have you tried other lib like [NTL](https://www.shoup.net/ntl/), [boost.ublas](https://www.boost.org/doc/libs/1_69_0/libs/numeric/ublas/doc/matrix_sparse.html) or [more](https://en.wikipedia.org/wiki/List_of_numerical_libraries#C++) before making new wheel?
tldr; Speaking from the trenches\*, there really isn't much that meaningfully distinguishes scientific software from other performance-oriented domains these days, aside for (perhaps) MPI and related distributed computed computing models. We're interested in throughput-oriented computing; on today's architectures, that means * accessing memory efficiently * cache-friendly access patterns * prefetcher-friendly access patterns * maximizing compute per memory access * parallelizing effectively * minimizing coordination * hiding latency Folks have already mentioned Scott Meyer's cache's and Mike Acton's data-oriented design talks. Their must watch videos. In addition, towards the first major bullet, I highly recommend the [Eric Niebler Range-V3 CPPCon 2015 keynote](https://youtu.be/mFUXNMfaciE). The lazy range operations provided by the library provide a sort of automatic kernel fusion for the sorts of range based operations we frequently see in scientific computing. Towards this end, see [Christopher Di Bella "Introducing Parallelism to the Ranges TS"](https://youtu.be/0-n7fyQYsVk) how this approach can be extended to GPU based programming. For linear algebra specifically, there's a decent [talk on Blaze](https://youtu.be/w-Y22KrMgFE) by Klaus Iglberger, which discusses the techniques he (and other libraries like it) use to do kernel fusion using more tradition metaprogramming techniques. Towards the second major bullet, towards distributed parallelism, there are a [number](https://youtu.be/8M2X7mGVdqU) [of](https://youtu.be/js-e8xAMd1s) [talks](https://www.youtube.com/watch?v=uM4oRWsZJKc) by Harmut Kaiser and Bryce Lelbach regarding HPX and (more importantly) the philosophy behind it. &lt;rant&gt; &gt; there really isn't much that meaningfully distinguishes scientific software from other performance-oriented domains If your coming from an engineering or physical science education background, many of your professors may have insisted that there is. If your currently working in the industry, your managers likely believe there is. They're not (entirely) mad, just a bit out of touch. It's important to recognize that for a very long time, in the era of Moore's law, the sorts of performance concerns that are the hallmark of scientific HPC were considered rather niche. This lead to considerable isolation between the field and the rest of the software industry, as a result, the today's community is pretty inbred. These folks were trained to work with Fortran and likely used it for most of their career. It's had an impact on how they think. A subset of those people are in a position to direct effort and research. It's why the performance portability libraries that have been come out of the labs, such as Raja and Kokkos, feel so incredibly regressive. &lt;/rant&gt; \*I write simulation software for predictive science and engineering for a major national laboratory.
Why libclang, not clangIndex?
Yes, Conan can do it. Personally I would model this using an `option` with a name like `cxxstd`. Inside the conanfile.py for your library, you would need: 1. configure the option in your class, see [here](https://docs.conan.io/en/latest/reference/conanfile/attributes.html#options). 2. use the `cxxstd` option to set the corresponding variable for your build system in the [`build()`](https://docs.conan.io/en/latest/reference/conanfile/methods.html#build) method. With the CMake helper you can do something like def build(self): cmake = CMake(self) defs = { 'CMAKE_CXX_STANDARD': self.options.cxxstd } cmake.configure(defs=defs) or if you're setting it on a target-specific basis then pass some cache variable that you use in your target property declarations. 3. take the option into account in the [`package_id()`](https://docs.conan.io/en/latest/reference/conanfile/methods.html#package-id) method. By default all options are already considered for the package id calculation, but if you have transitive dependencies that also have the same/similar option you may want to ensure that users do not mix and match a c++11 version of library A with a c++14 version of library B that it depends on. See more information [here](https://docs.conan.io/en/latest/creating_packages/define_abi_compatibility.html), specifically the information about `full_package_mode` for dependencies. Then a library consumer may set the default `stdcxx` options for you as a dependency in [`default_options`](https://docs.conan.io/en/latest/reference/conanfile/attributes.html#default-options) for their builds (or `configure()` if done dynamically) and an application consumer could set the desired `stdcxx` option in the [`[options]`](https://docs.conan.io/en/latest/reference/conanfile_txt.html#options) section of their conanfile.txt.
This question hurts me. 
Again, good work on your PR. The code generation is much better compared to `std::visit`. But it is far from ideal. See [godbolt.org/z/i8frl9](https://godbolt.org/z/i8frl9) (I removed the custom variant and added `mpark::variant`.) With `__builtin_unreachable()` GCC is generating perfect code. Clang seems stuck in the jump table. Visual Studio is unwilling to optimize the condition chain. Unfortunately your PR also seems stuck or abandoned since November.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/a909el/attaching_to_another_process_in_c/ecfna1d/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/a8wk0y/help_please/ecfnawl/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
That incantation requires being a moderator :-)
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/a8xtyr/how_is_assignment_operator_exactly_works/ecfncs2/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I could listen to Chandler Carruth talk about anything for hours, no matter what. Always interesting, always a great presentation.
Bryce was the first person that came to mind for me as well.
Actually, I did that in the beginning and, of course, it was working fine. But then I thought: *"It shouldn't be hard!" ...* and ended up here :D
yeah , it's obvious from looking at repository https://github.com/bloomberg the code is C++03 if not C++98 I hope the new projects at BGG are at least in C++14 ? For newest C++ you should apply at facebook their folly library looks modern https://github.com/facebook/folly Google is probably using modern C++ and for sure Microsoft compiler team in Redmond WA 
I might be wrong, but I think they've introduced some sort of setting responsible for standard ABI differences, but I am not sure if it covers all 98 11 14 17 20 standards, likely just 98 / 11
You are clearly drunk.
I bet that's not the intent of [\[variant.visit\]/5](http://eel.is/c++draft/variant.visit#5).
Maybe I‚Äôm just naive but it seems to me that adding more and more IPC would end up costing more then the x64 port. But I‚Äôm just some desk chair quarterback who has an opinion. 
I just started at Google and C++11 is the current standard here.
Yes there is a compiler.libcxx setting but that applies for the stdlib and is like you say for 98/11. That is important because as far as I know you cannot have more than one version of the standard library in your process at once. My assumption was that /u/kalmoc had a compiled library with interface differences depending on the cxx version, so you could only link against the 14 version if compiling with c++14 for example.
Which perfectly demonstrates the problem with the focus on algorithmic complexity in the standard.
[fmt](https://github.com/fmtlib/fmt)
[I've been a fan of {fmt} for a while](https://github.com/fmtlib/fmt) which has most of this (unfortunately haven't had time to use it outside of small test projects ;_;)
`-Wformat` deals with the static checking. It's possible to write a template wrapper using `constexpr` to autoconvert other argument types. Optimal code is not going to happen because i18n is more important.
Yes. But actually it‚Äôs only 2 GB unless the program is marked as large address aware (which VS is not).
Wasn't one of the purposes of `allocator_traits` to deprecate inheriting from `std::allocator`?
Will it be in c++20?
If you are already using `boost` you could look into [`boost.format`](https://www.boost.org/doc/libs/1_69_0/libs/format/doc/format.html).
&gt; using variadic templates and other features to check the format string is matching the arguments at compile-time printf syntax must die. Why specify the types manually and check them if with variadic templates all of them are known anyway? It's enough to only provide placeholders (and optionally formatting options) and this is what [fmt](http://fmtlib.net/latest/syntax.html) does.
Partially I think. IIRC no I/O.
I love printf syntax :) it's so concise ...I have 3,5 kloc [custom extension](https://github.com/tringi/emphasize/blob/master/Windows/Windows_Print.hpp) written above it. Although that fmt library indeed looks very cool.
This might be interesting if your work involves lots of numerical computations: [https://herbie.uwplse.org/](https://herbie.uwplse.org/) [https://www.microsoft.com/en-us/research/video/numerical-tools-for-non-experts/](https://www.microsoft.com/en-us/research/video/numerical-tools-for-non-experts/)
I do like boost.format but I don't believe it supports static checking of parameters. https://svn.boost.org/trac10/ticket/6815
*Mike* Acton's talk was on data-*oriented*-design.
Great introduction to allocators [Taming dynamic memory - An introduction to custom allocators in C++ - Andreas Weis - code::dive 2018](https://www.youtube.com/watch?v=FcpmMmyNNv8).
Thanks already forgot about that one.
&gt; You can't do that, because T could legitimately be a real type. No: It starts with a capital letter and can does not be a type as that would be inconsistent with C++-naming-conventions¬π. Yes, some people are idiots and ignore them, but that doesn't make it **legitimate**. [1] The stdlib-naming-conventions of a language are by definition the only correct ones and everyone who disagrees is simply acting unprofessional
As a supporter of the utf8-everywhere manifesto I don't see a point to ever support anything but utf-8, making this a bad example. ;-)
&gt; I love printf syntax :) it's so concise `"N = %d"` is no more concise than `"N = {}"`. And the latter won't break if the type suddenly changes from `int` to `double` or even just `long long`. Try properly printing std typedefs, it's a PITA.
how the fuck do i get downvoted so much. am i missing some pertinent piece of information? the size of all variants is a compile time constant and there is always an upper limit to the time that is needed to visit it. the whole point here is that you have to compare 2 variants of different length and only if their performance is the same, can you say that it is O(1). if you are using one branch per type in the variant, a longer variant will of course be slower than a shorter variant. Thus it is NOT O(1).
Agreed. I wrote a custom printf for my game which just uses %1, %2, %3 (I could easily just make it replace the same one each time, but this is better for two reasons: * I can reuse arguments (`print("Error %1: %2 (search online for code %1)", code, msg);`) * I can quickly see which argument is missing in the output because it didn‚Äôt get replaced ). It doesn‚Äôt do static checking because my game requires dynamic casts as well, but so far it works pretty well.
Allocator is a class that controls allocation of dynamic memory(which is obvious from its name). It is used by dynamic containers like std::vector, std::string, etc. Most of the time the default allocator works fine, because it is general purposed. But sometimes it is wise(debatable) use a custom allocator instead of the default one. Custom allocators are not a very good practice anyway, so don't do it unless you really have to.
As I've shown in my example, it does make inheriting from \`std::allocator\` a huge pain, such that it compiles, but doesn't work. But I'm not sure it was a purpose, probably more of a side effect. See how many "*if present, otherwise*"-s are there. \`std::allocator\_traits\` tries really hard to make it easy for us to write an allocator from scratch, and the effort goes on in its static functions. As a result, \`construct\` and \`destroy\` functions are now deprecated and will be removed in C++20, with \`std::allocator\_traits\` taking care of them.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/a94u10/trying_to_learn_how_asio_works/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I really enjoyed using Conan, was sometimes a little bit of a pain, but pushing our whole stack (about 20 3rd parties and 15 1st party libs) with a mixture of C++11 and 17, static and shared and passing other bits of data between them (use this version of this lib etc) as well as upgrading our libs from 11 to 17, and putting the whole lot on a build server took just over 2 months. It allowed us to just switch what toolchain and prebuilts using the simplest python script (no python knowledge needed to use it, but could override things if you knew they were in the lib that meant you could do whatever you wanted) that also meant all the data was in artifactory. Basically loads of freedom, loads of control but still powerful as hell and to our end devs, things just happened like magic (we also wrote a plugin for visual studio that updated things in the background).
Well, the standard states that it should be in "constant time" which might be interpreted as stronger than O(1). Still, I agree it is probably misguided to specify that given it is very unclear what it means considering the usual compiler and CPU implementations. The reference to "time" makes it even worse (although "constant time" is usually meant as an expression); typically you analyze / specify the complexity on a number of operations rather than "time", because the time taken by a particular operation can dynamically vary enormously (for example because of a cache hit vs. cache miss, or a branch prediction success vs. failure) So yes given it is rare to have variant of hundreds of types, and it is well known how to do efficient dispatch, it should have been left as a quality of implementation issue, especially since the number of types to consider is static to begin with.
There is [Stringify](!https://github.com/robhz786/stringify), but it is still in the initial stage of development and not ready for production use yet.
I could find std::map::contains, but not a std::contains.
Thats what i was referring to, thought it's obvious from context.
One of the best use cases I've found is in CUDA. I use a custom allocator so I can use pinned memory with std::vector. There's a sample one in the thrust library of anyone is interested. 
`boost::interprocess::allocator` is a good reason to use a custom allocator
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/a95hd5/know_c_want_to_learn_cpp_what_book_do_i_go_with/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Unfortunately, it still doesn't have an equivalent `CMAKE_PREFIX_PATH` setting to tell meson where the dependencies are installed.
so implementations are free to do something else for smaller variants. they are not required to use a jump table. size of the variant is compile time constant. no problem. even if it was somehow a runtime dynamic value, an algorithm that branches to choose between different implementations during runtime can still be O(1). 
To be honest, I am even on the fence regarding formatting options. I can see the appeal of specifying everything into a single string, through a custom embedded language, as it can allow switching format strings on the fly which is useful for i18n... yet at the same time it seems unnecessary to change formatting in most i18n cases, and I am not clear that formatting is sufficient to cover all needs of i18n. An alternative would be to *wrap* the arguments, compositing functionality through decoration: print("The {} jumps over the lazy {}", bold("fox"), italic(center("dog", 20))); Compositing has one inherent advantage: it's fully extensible. You won't find `bold` or `italic` formatters in `printf`. You won't find `center-left-tiebreaker` or `center-right-tiebreaker`. The extensibility also makes it possible to write *semantically named* formats, enhancing reusability: auto animal(std::string_view name) { return italic(center(name, 20)); } It can be argued it's more verbose, though I'd assume commonly used options would come with shortcuts: `b("fox")`.
As others pointed out, its full of basic mistakes and therefore not that helpful to beginners. Learni g is hard and teaching is harder, unless you are an expert yo7 probably shouldn't be do8ng something like this. 
Editing/deleting a downvoted post, and then repeating it with a bonus whine, seems pretty rude.
Or you can do it with a \`settings.cppstd\` to preserve the same version across the whole dependency tree instead. 
Open source is about building stuff together. When you see a bug, you open an issue or make a pull request. You don't tell the author(s) to "not do it". I fixed the mistakes that others pointed out, and added new sections (namespaces, smart pointers etc.) upon suggestions. If you still see some mistakes or have any suggestions, let me know and I'll try to apply them. Or you can just ignore it if you don't like to be a part of it.
Loicense?
what are you talking about? no editing, no deleting. i can whine as much as i want. if the comment i replied to is correct, then why are we even here? if if/else is allowed, OPs point becomes meaningless, because the performance guarantee does not hurt performance.
`8yy`
Wait... there's fmt-style formatting in C++20? Do you have a link for further information on this?
Thanks, I didn't know about that option. [Link](https://docs.conan.io/en/latest/howtos/manage_cpp_standard.html) for others.
[Last I heard](https://www.reddit.com/r/cpp/comments/9vwvbz/2018_san_diego_iso_c_committee_trip_report_ranges/), [P0645](https://wg21.link/P0645) was on track for C++20.
(cmake&amp;cpp noob here) ahh so that's how it knows where to find installed dependencies. I was wondering how it could possibly work on Windows where there's not really a place to install shared / static libraries
If you deliver I will run away from doxygen. Right now I use antora + doxygen for my main project.
`std::allocator::construct` and `std::allocator::destroy` are deprecated and removed because they add nothing over the default. Custom allocators can still provide their own `construct` and `destroy`; that's not deprecated.
Whoops, thanks
Of course, Standard can not deprecate our own functions :) Having this 2 functions in your allocator is not mandatory, as it wasn't before.
How are custom allocators bad practice? They are basically mandatory if you want good performance 
* smart resource handles (e.g. smart pointers) are your friend * Also you can bind exception handling to the ctor block: struct foo { foo() try { // ctor code } catch(...) { // ...clean up... } };
I have been using CMake for years and I did not even know. I used to add the whatev_ROOT env variable for each lib to point to the install location.
There is no silver bullet but you can easily integrate https://matplotlib.org/ or https://pandas.pydata.org/ to your workflow or CI pipeline which are the most commonly used in academia.
It's honestly really incredible the amount of work /u/jpakkane has put into meson, as well as the general adoption by so many Linux projects as they abandon autotools. If you go back far enough on this subreddit, you'll see a lot of doubt and disbelief that he could write a tool better than CMake, and yet here we are. Thus far, my only issues with meson are the inability to have the 'wrap' tool read from the local filesystem. The current workaround is to do something like `python -m http.server` in the directory you've stored all your wraps.
Honestly meson looks awesome but until it has globing support I just can‚Äôt use it. I know the arguments against it but it‚Äôs too much of a hassle for me to maintain the list of files for my build system.
&gt; the standard states that it should be in "constant time" which might be interpreted as stronger than O(1) Just because you could interpret the words with lay definitions to be different doesn't mean it means that.
It's not so much that there was any doubt that a better build configuration tool than CMake could be created - that much was obvious. The doubt is more if any other build system would be _so much_ better that it'd make sense to move away from CMake (remember, this isn't just about one project, this is also about the ability to use existing projects).
Bazel does this, you use ‚Äúbazel run //path/to:target‚Äù and it will do any building needed the run the target. You pay for that with a ~250ms no build overhead, but it works great for limiting what you need to build.
Actually I'd argue that constant-time is actually being used as something stronger in the security community when discussing algorithms that are designed to eliminate side channels. Algorithms that are O(1) but can take 1000x different times to run based on input can be used in a timing side channel attack despite being constant-time from a computation complexity standpoint.
For the syntax, I think it'd be important to use a syntax that already exists and is accepted, like TeX, and that can be extended relatively easily. 
If it has a `requires` clause, then it's a template, even if there isn't an explicit `template&lt;...&gt;`.
As a result of [P1190](https://wg21.link/p1190) and [P1185](https://wg21.link/p1185), `&lt;=&gt;` no longer "generates" `==` (more accurately, a call to `==` will no longer implicitly call `&lt;=&gt;`). Additionally, class types as non-type template parameters will be based on defaulted `==`, not defaulted `&lt;=&gt;`. See also [Improvements to `&lt;=&gt;`](https://brevzin.github.io/c++/2018/11/12/improve-spaceship/)
that has nothing to do with algorithmic complexity. Words have meanings in contexts. Saying that they have different meanings in different contexts doesn't change anything. It's just being incorrectly pedantic.
Maybe it‚Äôs because I‚Äôm on mobile, but I don‚Äôt see where I can read this... 
[Meta] In future, please link to the actual article (https://inversepalindrome.com/blog/2018/12/23/the-spaceship-operator-and-its-importance-for-class-types-in-non-type-template-parameters) for a more permanent link, instead of the main blog page that will push the article out of view at some point.
That is because the link was not initialized properly.
No download link (desktop). **Looks like a bait and switch.**
P0645 was accepted by the Library Evolution Working Group in San Diego and forwarded to the Library Working Group for wording review (already had one iteration) targeting C++20.
looks like you have to sign up for the mailing list 
yes, you do.
I do not really think it is a big deal in practice.
Optimal code still can happen with \`constexpr\` handling of format strings. For example, [https://github.com/fmtlib/fmt/pull/949](https://github.com/fmtlib/fmt/pull/949) implements compilation of format strings although making it \`constexpr\` requires a bit more work.
The problem with such composition is that it quickly becomes unwieldy. Even specifying full floating-point formatting this way is a pain, and if you consider something like chrono and add styles on top of that, it becomes virtually infeasible. That's why {fmt} ditched this approach in version 5.0.
I think you're missing my point. You seem to be talking about (in-language) compile-time parsing of the format string. I'm talking about something that makes: template&lt;class... A&gt; void my_printf(const char *fmt, A&amp;&amp;... a); my_printf("%s", std::string()); preserve the `const char *` argument, and convert the `std::string` instance using `.c_str()`, in such a way that the compiler's `-Wformat` option can detect type mismatches. While still allowing `gettext` on the format string.
*cough* ^^^https://gallery.mailchimp.com/9f01739d94fc2e23b2f359cb0/files/a52ced77-8d6e-4289-b1d4-5d1d1d0d3a8e/Cpp_smart_pointers_ebook.zip *cough*
Over wordy... I like the books makes me 'Eureka!!' with the code. 
If anyone is interested about said work, there is an LCA presentation about it called [4+ years of work to become an overnight success](https://www.youtube.com/watch?v=gHdTzdXkhRY). &gt; Thus far, my only issues with meson are the inability to have the 'wrap' tool read from the local filesystem. I think we changed it so that if you put all of your dependencies' files (source tars, wrap files etc) in the `packagecache` directory, we will use them directly from there. Thus you can do a fully offline build if you want.
I think `{fmt}` is state-of-the-art in the C++ world. Thank you very much for your work! I must have missed the ability to provide one's own formatters, though, could you please point me toward them?
If only performance... [It doesn't work at all in GCC if headers contain BOM](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=56549) :(
I don't understand why you need this. In every project I've been working with that used globbing, there has been at some point problems which led to having to revert to listing all the source files.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/a9ej6q/im_currently_13_through_bjarnes_ppp_book_and_i/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Which usually stops working as soon as the function is wrapped. And not sure if msvc has a -Wformat by now.
Good point! I'm indeed more interested in optimal codegen, i.e. it takes the format string apart at compile time.
Or just learn to use cmake‚Äôs FetchContent module properly (or manage a submodule with your vcs of choice). It‚Äôs really not that hard. 
I see. I think you'll need `my_printf` to be a macro for this to work.
[http://fmtlib.net/latest/api.html#formatting-user-defined-types](http://fmtlib.net/latest/api.html#formatting-user-defined-types)
You‚Äôre not adding to an established project, however. This isn‚Äôt a program trying to solve a problem. This is you putting out a ‚Äúquick reference‚Äù, when it doesn‚Äôt seem like you are qualified to decide what a quick reference should contain or what points to zoom in on, requiring a large amount of help to make it correct from the community. It‚Äôs essentially someone asking the community to do their homework. 
[https://github.com/eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee/eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee](https://github.com/eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee/eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee) Have a nice day.
Lua is definitely much easier to deal with for you. Just a question about the better build language for large projects.
Just tried Python Tutor,it's good! QtCreator is very cool. I really like! And it is useful not only for beginners. But I'm more used to working with Visual Studio and debuggers like *Deleaker* and the *built-in debugger*. It is my choice due to my needs)
Do you know what the deal is with them getting rid of their static buffer or whatever they called it? I was a fun of using it on embedded systems where I have no dynamic memory allocation, which when fmt is used with the static buffer satisfied, but they seem to have taken it out recently?
I remember there were macros *somewhere* in the process. I think the format string had to be declared as a constexpr member of a local *class*, which was then passed to a template to do all the conversions ..
Avoiding that "usually" is your job as a programmer.
Depends on what to do, analyzing a dumpfile windbg otherwise visual studio debugger. Should make myself comfortable with gdb but didnt have the time yet. &amp;#x200B; Thats strictly debugger. For looking at assembly: x86/x64dbg
I can advise [Valgrind](http://www.valgrind.org/) for Linux. And [Deleaker](https://www.deleaker.com/) for Windows. Thelast one is paid, but it is really cool. Its free counterpart may be the [Visual Leak Detector](https://github.com/KindDragon/vld/wiki/Using-Visual-Leak-Detector), but it is less informative.
Well, visual studio has nice debugger and disassembler. What else do you need?
It's not an element-wise multiplication. It's a matrix multiplication. In order two do this, you need two matrices with sizes MxN for one and NxM for the other. The errors thrown are a little ambiguous but the comparisons and implementation are fine 
None &gt;.&gt;
gdb. Do valgrind and strace count as debuggers?
IBM Debugger for work... GDB with CLion for personal projects.
I'm purely command line at this point for C/C++, so I only use GDB.
`std::cout` for most part.
lldb
I noticed that the C++ runtime has a design goal of: "Negligible cold-start overhead (single digit millisecond)". How far off from the goal is it atm? How does it compare to Python, Node, or Go?
WinDbg.
I‚Äôm not ashamed to admit that my one true debug aid that has never defied me by being absent when duty calls is the humble printf.
Clion with lldb. For some pathological cases, Eclipse or just `arm-none-eabi-gdb -tui` . I think a lot of folks don't know and appreciate the power of TUI 
&gt;First of all why is C++/WinRT an add on? Shouldn't that be something the compiler handles? No. The whole point is to use *standard* C++ instead of non-standard compiler extensions like C++/CX. Read: * [https://moderncpp.com/](https://moderncpp.com/) * [https://github.com/Microsoft/cppwinrt](https://github.com/Microsoft/cppwinrt) * [https://en.wikipedia.org/wiki/C%2B%2B/WinRT](https://en.wikipedia.org/wiki/C%2B%2B/WinRT) * [https://docs.microsoft.com/en-us/windows/uwp/cpp-and-winrt-apis/intro-to-using-cpp-with-winrt](https://docs.microsoft.com/en-us/windows/uwp/cpp-and-winrt-apis/intro-to-using-cpp-with-winrt) etc. etc. etc. Project templates and the kind and MSBuild files come in extensions. There's really no other way. (Except being built-in, but that's technically the same thing. Just like PTVS is now part of the installer rather than a separate addon, but technically it uses the same interfaces etc.) [https://marketplace.visualstudio.com/items?itemName=CppWinRTTeam.cppwinrt101804264](https://marketplace.visualstudio.com/items?itemName=CppWinRTTeam.cppwinrt101804264) &gt;Lastly, do you really think Microsoft, one of the most valuable companies in the world, doesn't have the resources to rewrite three fucking addons in x64? I don't know and I don't give a fuck, smartass. I replied to the other smartass who said "*no one* cares about VS addins". At no point did I say that it's hard or even necessary to make any changes to these addon to work on a hypothetical 64-bit version of Visual Studio.
This is a completely different use and a childish response. 
I fully agree. If Meson added proper globing there is a good chance that I'd switch all my personal projects too it very soon. IMHO having to list all project files manually is about as pointless as manually listing all dependencies in plain make, yet for some reason people heavily defend the first yet complain about the second, when they really are birds of a feather.
&gt; It is also discouraged in CMake after all. Yet the CMake-guys understand that their users are adults that may well know better and provide sufficient support for it so that it's easier to use that then creating a fork or a completely new system. &gt; Globbing comes with its own set of problems. I've never seen somebody mentioning one that I found convincing in any way. And no, saving 0.1 seconds in a 30 second build is not convincing.
There are objective reasons why globbing can mess up things. For example: forgetting to delete an old file (not adding) a file and getting an ODR violation. I also think that when doing team collaboration, globbing can be potentially more misleading that maintaining a reviewable list of files. 
gdb, only if cout doesn't cut it.
Why? Do you enjoy needing to compile your codebase to accommodate a new print statement? I don‚Äôt understand why people like bragging about using printf exclusively for debugging. It just seems like either you don‚Äôt understand how to use better tools, or you work in a relatively small codebase. We‚Äôre engineers and should always make the best use of the tools available, debuggers included where appropriate. 
Those are great reasons not to use the feature but not good reasons not to implement it. If your use case works better with listing the files then go for it, but for cases when globbing can save work and time I‚Äôd like to use it.
LLDB and Valgrind I‚Äôm surprised that there aren‚Äôt more people using LLDB ITT. I‚Äôve always thought that their diagnostics were better 
Can't speak for that guy, but my personal reasons include: 1) The process I'm debugging is part of a bigger system (read: has watchdogs) so pausing it in a debugger has side effects. 2) Inspecting the value of a variable involves attaching GDB, setting a breakpoint, resuming execution, and typing the GDB commands when the breakpoint is hit. 3) Typing "printf()" and rebuilding actually takes less time than #2. &gt; We‚Äôre engineers and should always make the best use of the tools available, debuggers included where appropriate. Amen to that, but I think printf should be a part of that toolbox. Sometimes firing up GDB is a little... well, overkill. Where GDB shines IMHO, are situations like when I don't have the debug symbols in a core dump and need to look at the assembly, when I'm looking into use-after-free/memory corruption issues, or when trying to reproduce race conditions. For quick little "I wonder what the value of this variable is", given some circumstances (like the annoying multi-threaded system with watchdogs I mentioned above), I think printf can be a "good enough" tool.
I add perf, hotspot and heaptrack to the list.
Totalview
Growing up in kernels will do that to you. 
This is one of the most common screening questions I've seen in games. It's usually something like make your own allocator without std, makes sure allocations are always continguous, allows resizing. It's always funny seeing how they try to word it without using the word allocator to give away what exactly you're doing.
When I was using visual studio 2017 it had some kind of extension for debugging process to trace memory leaks. I do not remember it's name though :(. It was outputting all memory leaks after program completed execution. 
Yea I‚Äôm aware of plenty of reasons ‚Äúprintf‚Äù (more suitably called a serialized log perhaps, for environments without even a terminal or constrained memory), and you list several. Another one where I personally use it is for reasoning about mulithreaded code. I was more speaking against the Luddite argument for *exclusive* use of printf, which to me is a silly thing to be proud about. It feels like some cargo cult thing passed down by the previous generation of software architects. 
gdb when I use an actual debugger, but I tend to use printf debugging a lot too.
What's wrong with Visual Studio debugger?
Probably VLD.
Yes. VLD is a must with VS when doing C++ imo.
Visual Studio
Just to give a concrete example of where cout debugging is a reasonable choice I think: after the hundredth time using a regular debugger only to see ‚Äúoptimized out‚Äù for the variable you want, usually I just stop trying. Not everyone has the luxury of debugging debug builds :D
Visual Studio debugger for Windows, lldb for Linux/Darwin, barely-usable versions of GDB and DBX when I am forced to work on old-school Unix platforms with inadequate C++ compilers, WinDBG when I want to hate everyone and everything in the universe... or am forced to because I'm dealing with the Windows Kernel. Mostly, though, I use static analysis, tests, and strict coding habits to avoid having to use the debugger at all.
WinDbg is more powerful than the VS debugger -- e.g. you can run code when hitting a breakpoint, and there are tons of extensions -- but the syntax is atrocious and weighed down by a ton of ancient legacy. (Kind of like Lisp has CAR and CDR because those were the names of two CPU registers on a machine in the 50s!)
&gt;I'm interested in debuggers for Visual Studio 2017 + Windows 10
Nope, wrong. I‚Äôm actually thinking of a multi-million line codebase, embedded systems (video games, not a my first raspberry pi project), I was lead online developer (p2p online multiplayer) cross-platform code. I actually find it kind of cute you think I wouldn‚Äôt even know better tools. Aww. Nice guy, just trying to educate me on ‚Äúproper‚Äù engineering. Thanks! Actually enjoyment was far removed from my experience, and a debugger is fine for debugging code where it‚Äôs ok to stop the world, but when you can‚Äôt connect to every dev-kit simultaneously, and the software runs too slow in a debug configuration, and you have nothing but tty output, a core dump, and a symbol map, and even pausing a single host in the sim would send everyone out of sync, I‚Äôm sure that one day you too will learn to love (or hate) the humble printf.
Printf
There is a subscription box at the end of page. It isn't as quick as clicking a download link but I don't expect it from free stuffs. Fluent C++ is worth reading too.
It seems the best choice for non-Windows systems :)
He should add "on macOS and Linux" on the subject since this question is almost meaningless on Windows :)
It doesn't work on macOS/Linux :D
How do you debug core dumps with printf?
Meh, kernel dev on Windows has WinDBG, another excellent debugger (well, excellent to some...).
Hypothetically speaking, one could use [AWE](https://docs.microsoft.com/en-us/windows/desktop/memory/address-windowing-extensions) to access more than 4GB or physical memory in a 32-bit application. The problem is that is more complicated than just recompiling as 64-bit at best and requires major redesign of the software at worst. You could also use AWE only in certain components, like the debugger that loads huge PDBs, but then there's not real benefit over simply taking the debugger to a separate 64-bit process, only drawbacks.
I used the debugger in Visual Studio. It works great and has an easy UI.
lol did I click the wrong button and somehow got back into r/pcj?
It's just two sentences. Is it really that hard to fight the reddit syndrome and read it past the title? &gt; I'm interested in debuggers for Visual Studio 2017 + Windows 10 
I count everything that leads to less bugs by executing the executable as debuggers.
I count everything that leads to less bugs by executing the executable as debuggers.
I'm not making a recommendation to OP, I'm simply answering **your** question.
`%d` is indeed pointless; but "pad with leading 0s", "use scientific notation if shorter", "leave space for `-` even on positive numbers", "align decimal space at 5 characters in", "5 digits after decimal place", "treat tabs as 2 space" and many other formatting options are type specific. You'd want to check that those types match. 
VS supports Linux targets with remote debugging.
You should try remote debugging. I used QtCreator as the UI but TUI also works. Put `gdb-server` on the embedded board and connect via socket it's a charm.
Lldb in CLion
`gdb` combined with a GUI is quite powerful though.
Or you do your job properly and control your timestep so you can debug things correctly. You‚Äôre not the only one who deals with durango and orbis devkits (an odd flex). The debugger is plenty useful even for optimized builds, and the disassembly is just a step away. Ring buffer logs and other in memory logs are also inspectable and can be made fast enough for those scenarios where debug mode isn‚Äôt quick enough. That said I‚Äôm sorry that whatever situation was so bad you didn‚Äôt have the time to write better tools for yourself. You just may be experiencing stockholm syndrome, especially if you‚Äôre still convinced printf is the best way all the time. 
Depends on what I'm debugging. At work we use a lot of structured logging with [DLT](https://github.com/GENIVI/dlt-daemon) because issues are found in the field and we need to understand what happened prior to the bug to understand the bug. If an issue is reproducible locally then gdb.
vs debugger is fantastic though?
Done it a long time, even back on uclinux 2.4 era kernels. Doesn't apply for MMU-less RTOS-running targets
`std::cout`
Sure. Debugging RT systems must be hell :/.
Game dev here. I'm sure this is just lazy, but I'll wrap my code to only be compiled to editors and filter the outputs when needed. I use a debugger when things are funky. But most of the time I get pretty werid issues. Especially when I need execution time of a multiple million line codebase to be under 3ms(11ms with graphics). Debugging Multithreading and async asset loads gets really funky and tedious. Where just hot reloading the code and watching the output is not that time intensive... sometimes. But I'm pretty green and still learning quite a lot. Honestly I feel pretty low on the knowledge base here.
So... My whiteboard?
gdb and assert. But I'm only a humble hobbyist.
Well. Maybe you are right. But you could, in some way, that you want nested subprojects or meson_options.txt be called in another way or include files. I think at that time you are losing the simplicity that Meson gives you (and that CMake, as a witness, made difficult for me to find many of those things sometimes). My personal opinion is, anyway, that it saves a tiny part of time. If what u need is a project that already has many files, a script or a bash one-liner can generate the list for you for the first time. I did it myself. I really do not think adding and removing a file at a time in a build system takes a lot of time. 
This check is present at summation and difference. Check operators `+` and `-`. There, you need matrices of the same dimensions (both rows and cols). So, you check cols against cols and rows against rows. And at multiplication, you only need to check that the cols of the left hand side matrix is equal to the rows of the right hand side matrix. You don't have to check the others. 
This is a better explanation than mine 
It's not better... It's correct. The second matrix can be NxP... Not necessary NxM. 
Your intend is really good and I personally appreciate it, but your problem in the wrong language choice for quick reference. The standard of C++ contains more than 1000 pages, thus almost every simple thing in the language has its thin points, that aren't obvious. The simple example is object instantiation, that looking almost the same in code may be done with: 1. Constructor 2. Copy constructor 3. Move constructor 4. Copy assignment 5. Move assignment And this quite hard to explain without explaining the language details. Moreover, there are important not obvious points that significantly affect the code execution because of Undefined Behavior. I write production C++ code for last 5 year full time and still there are things that confuse me in this language. But if you want to continue working on this reference, please do the following: 1. Get rid of C past (watch this for explanation why https://youtu.be/YnWhqhNdYyk) 2. For quick reference examples try avoid everything that doesn't work as it looks 3. Each thing that you explain quickly should have a reference to details 4. Be very careful with any tiny thing that you explain in order to avoid of leading newbies into the wrong place
This is embarrassing, apparently I dont remember basic linear algebra. Upon a quick review your original comment is a correct critique and my comments are wrong. My bad
You should give [codewars.com](https://codewars.com) a try. There are tons of interesting problems ranging from very easy to very difficult.
When I switched I did not look back. Well, I hesitated a bit at first, but, overall, I do not regret at all. Out of the box: &amp;#x200B; \- sanitizers \- coverage \- sane options and cache handling. Options are much more structured \- sanitizers by default \- cleaner code \- ccache autodetection by default (I think this was added to CMake) \- no accidental empty ${SUBSTITUTIONS}, though I know nowadays there are CMake::Targets. \- cross-compiling: there is no contest in this area, at least since I last tried CMake seriously. \- in 0.49, besides cmake target detection there is native files (similar to cross-compile) to default your tools to non-standard. \- easy system-to-subprojects compilation fallback. &amp;#x200B; The obvious thing at which Meson is weaker is at XCode project generation. I do not know Visual Studio project generation at the moment, I just did not use if for a while, but I think it is in a decent shape already. &amp;#x200B; I encourage you to give it an experimental try. The result of the build files talks by itself.
gdb with cgdb as front-end (or the TUI if I need to use raw gdb).
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/a9msrd/a_begginnerintermediate_looking_for_help/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Ditto
WinDbg has better low-level capabilities and is not bloated. If you are already into console debuggers on other platforms it may be less overwhelming than VS.
I like having a graphical shell, so I use cgdb.
And what is wrong with Visual Studio? One of the best IDE on Windows and debugger is available even in its free version.
They probably never had to debug a problem that is only reproduced in a release build of a huge project.
&gt;Typing "printf()" and rebuilding actually takes less time than #2 For some projects you will have more than enough time to learn how to use GDB while it is being rebuilt.
Quick question: what is the relationship (if any) between Parallel STL and Intel TBB/Microsoft PPL?
Bake allows you to work on different versions of the same package at the same time, but you'll have to use different bake environments. A bake environment is essentially a collection of libraries where the user (you) have full control over which packages and which versions are installed. While this means that you'll have to do a bit more work to make sure that packages are compatible, the behavior of bake is easier to understand. By contrast, tools that attempt to do versioning automatically (like NPM) will sometimes have to make seemingly arbitrary decisions on which packages are upgraded to which version. Bake is aware of package versions, but at the moment it doesn't do anything with that information. Eventually I'd like to add a feature that checks whether the environment is consistent, based on versioning constraints specified in the dependency configuration of projects. I've added this as an item in the FAQ: [https://github.com/SanderMertens/bake#how-to-use-different-versions-of-the-same-package](https://github.com/SanderMertens/bake#how-to-use-different-versions-of-the-same-package)
&gt; At Intel, we have developed an implementation of C++17 execution policies for algorithms (often referred to as Parallel STL). We hope to contribute it to libstdc++/GCC, so would like to ask the community for comments on this. https://gcc.gnu.org/ml/libstdc++/2017-11/msg00112.html
lldb
There's http://sourceware.org/gdb/onlinedocs/gdb/Non_002dStop-Mode.html though. Not sure if VS or an extension for it provides that too.
Didn't they already donate it to GCC? Whatever happened to that?
This looks cool. Any thoughts on adding explicit input-output binding for the tasks? 
Work in progress. The ParallelSTL is not using `__ugly` and `_Ugly` identifiers and depends on Intel's TBB. Both need to be solved before you see it in libstc++. It should be ready for GCC 9.
Why get downvoted? 
How are you printing the value? Can you post the actual code? 
`fixed` just specifies to use a fixed precision rather than scientific notation. It still rounds to the appropriate number of decimal places. If you want truncation (or rounding towards zero) you'll have to do that yourself. 
I'll definitely check out the talk on YouTube and I especially like the idea of providing references to details. I'll do that as soon as I have time. BTW thank you for being one of the very few people in this thread with a positive attitude.
&gt;It's just two sentences. Is it really that hard to fight the reddit syndrome and read it past the title? "reddit syndrome"? Do you mean people do not read the question, but answer the question title? I recently on Reddit, but also noticed this trend. People see the title and begin to write the answer, hundreds of answers. Probably, it is necessary to make more accurate titles? 
If I'm correctly inferring what you are actually trying to do, this is just standard mathematical rounding, which you could read about it a bunch of places. Ignoring any finite precision effects from the fact that these are floats, the rounding done when you set the precision looks at the digits beyond those you've asked for to decide where to round up or down. If the next digit is 5 or over the final digit is rounded up one, if it is 4 or lower it is preserved. This is the mathematical heuristic for how this works, the actual rounding algorithm will be quite different. In your case you've asked `0.666666666666666` to be rounded to 6 places in fixed precision. It initially rounds to `0.666666` and then looks at the next digit which is a `6`. As `6` is more than `5`, that means that `0.666667` is closer to the true value than `0.666666` so it round up the final digit to `0.666667`.
As an addendum, what I've described is the default rounding mode. You can change the rounding mode to something else that may match what you desire using [std::fesetround](https://en.cppreference.com/w/cpp/numeric/fenv/feround).
If no one writes bug reports this will never be fixed.
Because you didn't read the rules. "Questions and help posts are off-topic for this subreddit. Use r/cpp_questions or StackOverflow instead."
There are many deaggers, and you need to try different options to choose what you need. I use **Deleaker** for Windows and **Valgrind** for Linux.
Can someone tell me how TBB compares with [Cpp-Task-Flow] (https://cpp-taskflow.github.io/cpp-taskflow-documentation.github.io/) ?
If you can process assembly on your whiteboard, then yes
Slight inaccuracy in the last paragraph. If the next digit is 5 or more it rounds up. Next digit 0-4 truncates, next digit 5-9 adds 1 to the last digit. 
&gt; "reddit syndrome"? Do you mean people do not read the question, but answer the question title? Yeah, pretty much. Or don't read the linked article but start writing comments =) Sadly, not always it can be solved by adjusting the title.
Yes, and type-safe `{}` guarantees that. See: http://fmtlib.net/latest/syntax.html#format-examples for example.
My question was to the OP and in the context of OP's question. You've answered something that no one asked.
Interesting that there is a significant difference. Could you maybe also try auto a = 7; as initializer? I'm wondering what the compile time overhead for auto is. 
I know dude, I just couldn't miss the opportunity to shit on visual studio
`{ int a = 7; c += a; }` is 11 tokens. `{ int a {7}; c += a; }` is 12 tokens. That is 9% more. Is it a coincidence?
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
It looks to me like it uses an explicit dependency graph, which in my opinion is a poor design because its fallible to human error. A better design is to express the graph through data dependencies, which is essentially what futures and other modern parallel constructs do. 
I happily take the slight compilation time slowdown if it means that there is no ambiguity between calling a constructor and defining a function anymore.
Yes I relaised it as well
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/a9pcuy/using_c_primer_to_learn/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I'm curious. What is __ugly?
Reserved identifier names.
Let‚Äôs someone does: #define pretty 0 #include &lt;algorithm&gt; What about all the templates in `&lt;algorithm&gt;` that do things like: while (pretty &gt; 0) { ... } So the standard say you can't define macros that start with __ or _[A-Z] and the standard use those __ugly identifiers for anything not part of the API. 
If you are talking about \`fixed\_buffer\`, then it was removed as redundant because you can use \`std::array\` or a C-style array with the iterator-based API: [http://fmtlib.net/latest/api.html#output-iterator-support](http://fmtlib.net/latest/api.html#output-iterator-support)
Yes, and now you just get the ambiguity that comes from wondering/remembering if the type you're constructing accepts an `std::initializer_list`. At least ‚Äùthe most vexing parse‚Äù generally (in my experience) resulted in a compilation error. The greediness of initializer list constructors will cause many things to compile and lead to unexpected (or even undefined) behavior. The most commonly example of this probably being `std::vector{10, 20}` giving you a 2 element vector you immediately overrun, instead of 10 element you expect if you think of braces as a direct replacement for parenthesis when it comes to constructors,
Yeah, something tells me that the preprocessor is probably dominating the build time.
Is "it" TBB or cpp taskflow?
If you read the article you would know this isn‚Äôt the case. 
I did read the article, and I didn't see what you mean. Can you explain?
In the final note above the takeaways, the author says that they also ran tests where the preprocessing was done beforehand. 
There's a whole subculture of programmers who are committed to "minimalism", and have a disdain for using tools to make their lives easier if they're not strictly necessary. Tends to be a lot of overlap with the FLOSS crowd. I really don't get it, personally. Of course, printf debugging is highly useful. I kind of wish that a debugger existed with close compiler integration, so you could compile in printf statements without actually modifying the code. A bit like VS's breakpoint actions, but actually compiled into the instruction stream, not done via a slow interrupt mechanism.
Ah, yes, I missed that one.
Ya, sorry, I meant taskflow. I assumed knowledge of TBB. 
I was told the canonical way to initialise an integer in C++11 is: auto a = ([]()-&gt; int {return 0;})(); However, the fight is still on where to put the enclosing parentheses. auto b = ([]()-&gt; int {return 0;}());
As does `std::tuple`.
This also puts an end to the Always Auto vs explicit type debate. This is a great middle ground because you get to do both.
A disadvantage of coroutines is that they can't (yet?) be constexpr, so that version is the only one you can use at compile time.
Came here to say this. The only correct way to write a comparator for a record type is `std::tie(lhs.x, lhs.y, lhs.z) &lt; std::tie(rhs.x, rhs.y, rhs.z)`.
Or: auto tied = [](auto const&amp; e) { return std::tie(e.x, e.y, e.z); }; return tied(lhs) &lt; tied(rhs); To avoid repetition and weird bugs that could happen if you accidentally list the members in different orders.
What does the compiler produce from this? Does it just create a left-hand tuple and a right-hand tuple, and call the appropriate comparison function on those tuples?
something something [godbolt](https://godbolt.org/)
&gt; And even without understanding iota, you can still read this and abstractly understand the algorithm. That's not really true, though. It's essential to understand what iota does here.
What would a constexpr coroutine be useful for? 
Any coroutine that do not await but yields is interesting to use at compile time. For example the ones that generate an infinite sequence of numbers following a specific algorithm. Not all coroutine are related to asynchronous computation.
Seems to produce the same code, only ordered differently. https://godbolt.org/z/wjG5Pa
I found this one quite difficult, since some of the behaviours were were up to the whims of the compiler. Clang is intent on ruining one's attempts to avoid short-circuiting, but GCC gives very different results. May the smarter man always win, though, because you can force it with !((!xs[i]) + (!xs[i+1]) + (!xs[i+2]) + (!xs[i+3])) 
This is death by a thousand paper cuts. Doesn't matter for smaller projects, but for big ones at some point your productivity may tank due to compilation times. Good example is Chromium - 40 minutes to build 6.7mln loc on i9 vs Linux kernel 39 seconds to build 25 mln loc on the same CPU.
[Here's what I came up with](https://godbolt.org/z/jrArUs). I don't really know what I'm doing (novice), but I did notice that the `std::tie` approach explicitly calls 35 call std::tuple&lt;double&amp;, double&amp;&gt; std::tie&lt;double, double&gt;(double&amp;, double&amp;) 42 call std::tuple&lt;double&amp;, double&amp;&gt; std::tie&lt;double, double&gt;(double&amp;, double&amp;) 47 call bool std::operator&lt; &lt;double&amp;, double&amp;, double&amp;, double&amp;&gt;(std::tuple&lt;double&amp;, double&amp;&gt; const&amp;, std::tuple&lt;double&amp;, double&amp;&gt; const&amp;) which I think is creating new tuple objects and invoking the appropriate comparison function? However, [if I pass `-O1` to gcc](https://godbolt.org/z/3LBIxi) I don't seem to find any assembly that corresponds to the `std::tie` comparison function: only the manually written one. Has gcc's optimiser determined that the two functions are equivalent?
So basically C++2a becomes Python with zero-overhead performance and a tad uglier syntax. I love it!
Yes, that's true. It's all very compiler-dependent. But that's a part of the context and the context matters.
Do we practically use std::variant specialization where number of types gets any performance on O(N)? Like... handle 1k types in a std::variant?
Worth remembering that C++ coroutines are _also_ the hypothetical approach we'd use for generators, not just async/await or generalized coroutines. A constexpr generator could be *very* useful for constexpr metaprogramming. *Guaranteed* zero-allocation generators (even across TU boundaries and in non-optimized builds) are also very useful for latency-sensitive use of ranges and generic programming.
Wait, what!? Does cpp do parameter type inference in lambdas?
It's not type inference - it's deduction. Generic lambdas (also called polymorphic lambdas) have a call operator template instead of just a cal operator, were introduced in C++14. 
`&gt;` is only a problem in instantiation, and not usually in declarations. ``` void foo&lt;T&gt;(T t); foo&lt;SomeType&lt;EvenMoreSpecialized&gt;&gt;( new &lt;SomeType&lt;EvenMoreSpecialized&gt;&gt;(ctor_param)); ``` D has solved this problem with the bang ``` void foo(T)(T t); foo!(SomeType!EvenMoreSpecialized)( new SomeType!EvenMoreSpecialized(ctor_param)); ```
this is a joke right or am i and idiot. i thought the uniform way of initializing vars was int a {a}; in c++ there was even a cppcon talk about it.
TS Coroutines and Core Coroutines are both naturally sync. They return control deterministically to their owners. You need to add something to make them async. 
Ouch. Is the performance difference between C and C++ really on the order of 250x!? Does anybody know why it is so bad?
&gt; Now, to understand this code, you do need to understand the ‚Äúiota‚Äù function. This dependency is fine! What a fucking idiot. Is so disappointing that shit got so many upvotes. 
It can be used as a repl, but it isn't a very good one and has so many limitations when used as one that I can't really imagine anyone finding it very useful.
&gt; How long does it take you to check that line 13 is correct? Should it be ++count or count++? Should it check for == or !=? So much bullshit. If anyone has problems understanding that line he will just vomit when reading the generator syntax crap. 
A "tad" uglier is a bit of an understatement.
We all agree the syntax is fugly, however I am not sure about the performance. To be honest performance would be the only reason to use a feature so ugly it hurts the eyes. 
`__` can be anywhere in the identifier and is still considered as reserved.
I asked myself what iota even stand for? I see [cppreference](https://en.cppreference.com/w/cpp/algorithm/iota) say: &gt;The function is named after the integer function ‚ç≥ from the programming language APL and googling further I see a [stackoverflow](https://stackoverflow.com/questions/9244879/what-does-iota-of-stdiota-stand-for) post that explains more. But were there no easier to understand names for this to use? &amp;#x200B;
AFAIK the difference between C and C++ compilation times of the same code is very small. I'm no build engineer like Aras, so I didn't ever track small differences, but most outliers which I have seen were files with lots of template instantiation, and solution was either to remove templates or use pragma optimize off.
Check out Effective Modern C++ items 26 &amp; 27 (correct me if I'm wrong). And yes, forwarding/universal reference takes precedence, even over copy assignment. You may need to use enable_if to disable cases where T==whitespace, or in future use Concepts in C++20.
I haven‚Äôt been following ranges too closely. Is there a breakdown somewhere of what‚Äôs going on under the hood in terms of overhead or allocations, if any?
And yet, once you google `iota`, you know exactly what it does, and it's easy enough to remember once you saw it the first time. That makes it a pretty great name, no? And at this point, we already have the algorithm `std::iota()` so calling the generator `std::ranges::iota()` is practically the only reasonable choice. Python calls this `range()`, which is a very overloaded term in C++ and would make for a truly terrible name for us. 
I prefer `int a = ([]()-&gt; auto {return 0;})();`
Those conditions are not mutually exclusive.
Agreed, iota is a terrible name. I'd prefer "range", as Python and C# call it, but even that is ambiguous. In my opinion, it should be called something like "number_range".
I'm not sure what you mean by "scaling project." I have an offer to help someone on their project if we use my software as part of project. PM me for more info.
This is a great post. One annoying thing that's orthogonal to the question of range/coroutine preference is how you actually implement `take()`. The post proposes (and I'll slightly improve) this signature: template &lt;Range R&gt; generator&lt;iter_reference_t&lt;iterator_t&lt;R&gt;&gt; take(R&amp;&amp; range, int count); But this won't actually let you implement the rest of the example. You can't write `triples() | take(10)` because you can't write `take(10)`. So you have to write all this other mess too: struct take_fn { int i; }; template &lt;Range R&gt; auto operator|(R&amp;&amp; range, take_fn f) { return take(std::forward&lt;R&gt;(range), f.i); } take_fn take(int count) { return {count}; } That's... I mean, it's not the worst thing in the world but it's pretty annoying that this is how we have to do partial application right? And it's not like we ever want to _store_ a `take_fn` anywhere. We really just want `x | f(y)` to just evaluate directly as `f(x, y)` in this case. 
I prefer #define let int let a = 3; #undef let
What about "sequence"? Bash has `seq`.
You should check out Arlo Belshee's articles on variable naming: http://arlobelshee.com/good-naming-is-a-process-not-a-single-step/ IMO `iota` might be the worst variable name in the entire `std` namespace. It's up there with `car` and `cdr` in that it encodes no actual information, just useless historical context to be memorized. Anyways; not invested enough to debate it, just wanted to offer you a different perspective to consider.
Second here &amp;#x200B; R has seq() as well
So for msvc it looks like `equality` and `less` for empty tuple didn't get inlined, a bit strange. 
&gt;auto a = (\[\]()-&gt; int {return 0;})(); I'm obviously missing something, since it looks like the canonical way to make the language unreadable to me. Though, that may be the ultimate end for any language that has been around this long. The never ending desire to keep adding stuff to the language continues until everyone is just pretending like they understand it. I'm not even much pretending anymore.
On, and one quick followup. Something I can see that one might do in the old style enums, and it might end up killing you... as an example, maybe you come up with an enum you are going to use as the return for all relative magnitude methods. enum (less = -1, equal = 0, greater = 1 }; Then end up with a lot of code like this: if (eCompareFoos(foo1, foo2)) { } and the same taking functors or creating mixing interfaces that things can implement of that type for magnitude testing and so on, assuming that anything not 0 means unequal, and vice versa. That would work since the enum is implicitly converted. I could see doing something like that and ending up with no way to deal with that. I don't think you can play any magic to make that return value magically turn into a boolean result? So I'm guessing you'd end up having to do them all manually.
If you compile with GCC or Clang, you can use `__attribute__ ((format, (printf, n, n)))`: http://gcc.gnu.org/onlinedocs/gcc-3.2/gcc/Function-Attributes.html
What a dreadful example. What has lexicographical order got to do with int comparisons?
Wow, it's really just a range... That's really quite a terrible name. I figured it did something like select a small portion out of an enumerable type
Isn't converting `int` (for old kind of enum) to boolean unsafe while you are intending to make safer code by converting the old enum to enum class? So don't try to "I don't think you can play any magic to make that return value magically turn into a boolean result", because you had said it's magic. Just add another inline function bool eWhatEver(f1, f2) { return eCompareFoos(f1, f2) != equal; } 
I agree. Imagine if everyone or a majority of people working on C++ decided to start over with everything they've learned and create a new language that didn't have to maintain compatibility and aesthetics with C++. That I would use.
Isn't that D? 
I've yet to convert an enum to an enum class and not catch at least one thing that was "smelly". Another thing you can try depending on what the code base is doing: make your copy (and maybe move) constructors explicit. Every time I do that it finds a load of places people did things they didn't mean to do. Specifically: ```for (auto x : y)``` instead of ```for (auto&amp; x : y)```
`std::vector{10, 20}` gives me exactly what I expect. The whole purpose of the c++11 changes was to allow identical initialization of array like types as language level c-arrays and for pod like types like actual pods. I would be much more disturbed if `my_array{10,20}` would give me anything but an array with two elements. 
At the end he presents an implementation of iota. I don't understand how this works: sure, if there is just one call to iota (in a loop somewhere), each successive call returns a new value. But how does it work when there are two calls in two different loops? Wouldn't they start returning values from the same sequence? Surely the state of the coroutine must be tracked somewhere, allowing the coroutine to be used in multiple, independent locations? 
&gt; `int a {a};` You are now initialising `a` with itself which leaves the value undefined. If instead you hard written `auto a = ([]()-&gt; int {return a;})();` you would have gotten a compilation error. Better safe than sorry!
Yeah, it is a bit much to type. Thus in C++20 with the namespaced macro proposal it will [simplify](http://benalman.com/news/2010/11/immediately-invoked-function-expression/) to `auto a = std::iife&lt;int&gt;(0);`.
iota does not return value from sequence on each call, it return sequences. iota coroutine returns a `generator`. This generator stores and manages coroutine state.
Ah, I see. That makes sense. Thanks for the explanation!
Of course it should be auto c = std::invoke([]() -&gt; int {return 0;});
The range-v3 library has `ints` if the things you want to generate is a sequence of ... `int`s.
&gt; IMO `iota` might be the worst variable name in the entire `std` namespace. 1. It's not a variable. 2. `std::clog` is obviously worse.
I know but I suspect the question I answered is from someone who only saw examples of coroutines awaiting asynchronously. So I clarified that it's not always related.
That ambiguity doesn't exist for the case tested here. 
Should have no or minimum overhead compared to writing out the code yourself. Should have no implicit (heap) allocations. At least, that's what I would expect. No idea if it's true in practice. 
my wrong i meant to type int a {10}; ill change it.
&gt; Yes, and now you just get the ambiguity that comes from wondering/remembering if the type you're constructing accepts an std::initializer_list. Nbd, I've given up on initializer_list already.
I do f√ºr think those people would agree on exactly what parts of c++ should be keep and which not, but D and rust are two possibilities how such a language could look like.
I hoope: TEArray&lt;tCIDLib::TCh*, tCIDLib::ESpecialPaths, tCIDLib::ESpecialPaths::Count&gt; comes with a default 3rd argument. ;) 
"Safe" is relative. &amp;#x200B; enum class color { red, green, blue }; void do\_something(color const c) { } void test() { do\_something(static\_cast&lt;color&gt;(5)); } &amp;#x200B; Good luck with that!
This will probably miss GCC 9.. no activity on the gcc / libstdc++ mailing list..
I have a couple of questions, as I'm pretty fascinated with this type of process - mostly from a logistics perspective. It's not particularly trivial or simple to consider refactoring hundreds or maybe even thousands of classes, methods and functions. Further, I imagine there is a large team who also works on the code - so logistically speaking, I imagine it is difficult. Arguably to the point, where I imagine a number of developers would simply say some version of: "screw it, it's too big. It worked yesterday, do we really need these "new" enums... We'll probably just introduce more bugs than we fix." 1. Overall how long did this process take you, a few days, few weeks? 1.a. Were you working alone on the conversion? 1.b. How did you convince your boss this was a good idea? 2. Any thoughts you can share about the strategy you used to approach this? 2.a. Were you able to do it in parts, or one component at a time, or you had to do it all at once? 3. Given the size, I'll assume it has a significant amount of testing built around it, did those tests recognize any bugs you created while converting? 4. Aside from the technical details you shared, any insights you can share regarding "must-haves" or best practices or advice you feel others would find helpful? For example, without unit-tests don't even think about it - or you had to add a bunch of tests before starting... 5. Last one, any advice on gotchas you would recommend people keep an eye out when refactoring a large code base like this? Again, it's really interesting, thanks for your post.
I thought it converted an integer to a string.
So, ranges are great, assuming we have coroutines which may be available in the Future. I'll stick with plain old unobfuscated for-loops, thanks.
Can't agree more. I like range, but Eric's tutorial is very very hard to understand. I think the tutorial must be easy to understand even for a beginner of STL 
That‚Äôs on you if you want to write out the purposefully verbose cast stating to the compiler you don‚Äôt want its assistance with this conversion anymore. It‚Äôs also up to you to write code that handles enums outside of the range you‚Äôve defined if you ever cast something to the enum type.
This is a compiler warning (which I turn to error) for Clang, conveniently
Optimizations are off by default so not passing flags means your code is more or less a 1:1 translation to assembly. If you define non-inline functions without a main you don't get all the bloat and having to find your functions. Functions defined in the class body are inline. [Link](https://godbolt.org/z/iHHuK3)
`std::ranges::view::iota(n)` generates an infinite range starting from `n`. That's different from `seq` in both languages AFAIK.
"C++2a is going to be the best version of C++ yet" When I first see this, I doubt it. After I read the whole article, especially the section Ranges and Coroutines, that is really cool!!! I am looking forward c++2a now!!! 
As a rule of thumb, I only ever use default values to disable an optional feature on a complex method or subprogram. I find that default values are generally dangerous otherwise.
Really great post. I absolutely love how the result is much cleaner than the raw-loop style.
Isn't concepts "thing-able" a bad concept in general? I'm not yet familiar with concept, but I think it's what I saw in multiple conferences.
Do you have a link which I can look at? I would be interested. Our project is also in academia.
I find [counting_range](https://www.boost.org/doc/libs/1_68_0/libs/range/doc/html/range/reference/ranges/counting_range.html) intuitive.
How std::seq(0, std::numeric\_limits&lt;int&gt;::max()) would be different from std::ranges::view::iota(n) ? &amp;#x200B; You could even make it explicit &amp;#x200B; template &lt;typename T&gt; T seq(T start, T stop = std::numeric\_limits&lt;T&gt;::max()) { ... }
&gt; or even &gt; `void lock(const Lockable&amp; in_val);` Is this form current? I thought that the short form required `const Lockable auto&amp; in_val`, so that functions could be clearly distinguished from templates.
Came here from r/haskell haven't you :)
Unless you've given up on the standard library too, and meticulously check the constructors provided by every object in third party code you use, you still have to be aware of how `initializer_list` plays with overload resolution if you're going to do brace initialization. The whole risk is naive and blanket applications of ‚Äúuniform initialization‚Äù can accidentally invoke an `initializer_list` ctor when it's not what you wanted.
&gt; Note the ‚Äòtype‚Äô of concept ‚Äì bool. In fact, all Concepts are of type bool, so could have been omitted; the bool was left in since all definitions in C++ must have a type! Indeed it _was_ omitted. The correct way to declare a C++20 concept is: template &lt;typename T&gt; concept Bufferable = ...; Likewise the terse function template syntax is missing `auto`: void lock(Lockable const auto&amp;); And the rules presented for concept subsumption (the post says "overloading") are incorrect. `Value_comparable` **does not subsume** `Equality_comparable`. See [this q/a](https://stackoverflow.com/q/52062386/2069064).
I'm certainly sympathetic to that position. But of course with software it's seldom black and white. Readability is ultimately an issue. If a method has five parameters and three are very seldom used and quite wordy, and that method is called thousands of times, there's a good argument that readability would be much increased by defaulting the seldom used three. And there's also flexibility arguments. If you get those seldom used arguments embedded into the code in thousands of places, changing one becomes a huge slog, whereas it's trivial to change any non-explicit use of one of them throughout the whole code base in seconds (plus rebuild of course) if it's defaulted. OTOH, any spooky actions at a distance are potentially dangerous because it's difficult to really foresee the ways that the evil compiler might make use of them. So you have to walk a line, and it's hard to know where the line is. 
Bjarne mentioned this in his cppcon 2018 keynote. His concern was that concepts with the ‚Äúable‚Äù suffix tend to describe things at the syntactic level rather than semantic which he says is usually wrong.
I wrote all of this code, so it's me doing the conversions. I've been doing it piecemeal over the course I guess of about a month now? It's relative to whenever that previous post I linked to above was made. This is both good and bad. It's bad because it's just me having to do that work while still doing all the other work required. OTOH, it's good because I know every inch of this code base; and, where there is stupidity, it's always my own stupidity and likely to be consistently stupid in the same way everywhere. I updated my IDL compiler to allow enums to be marked individually as safe style, so that I could do IDL based ones as required, and not have to do an entire DLL/Exe at once. And there are good number of fundamental ones that are defined below the level of the IDL compiler (because they are referenced by the virtual kernel) so those are done by hand and could be converted as required. &amp;#x200B; This particular change is amenable to careful search and replace across the whole code base. As long as I checked my search/replace criteria carefully, I could in most cases do one enum across the whole code base in a couple minutes once I had gotten the process well figured out. The places where it took effort was when it was used as an index, particuarly down in the low level stuff, so I had to convert from raw arrays to the new TEArray template. And to handle the conversion to/from numbers down in the low level code. But, once up out of there into the higher level stuff, mostly it was search and replace, and updating an array or template member to use the new index type stuff. &amp;#x200B; Of course the above means that I'm my boss and so, multiple personality disorder not withstanding, it wasn't too difficult to convince my boss to do this. Actually, being sort of obsessive compulsive (shocking in a software developer, I know) this kind of thing is overly appealing. It's like picking at lint or something. So I can spend too much time doing it. &amp;#x200B; Given the nature of this change, tests didn't really come into it much. This is all about increasing compile time safety. So, where there was a problem, it showed up in compilation, because suddenly an enum that had silently (and incorrectly) gotten converted to a number suddenly couldn't be anymore and caused a compiler error. So, in that sense, this is a lot easier than some other magic overhauls might be, because it was all about increasing compile time safety. It meant that even mistakes I might make in the conversion would almost always create compiler errors as well. So there wasn't too much danger of introducing silent errors. If I did a bad search and replace, that would immediately get caught because the types are no longer convertable. &amp;#x200B;
I'm not sure I understand? In this case, there would be no way to default the third argument, I don't think. I think that would require preprocessor text replacement type stuff. And, these are only used in the very lowest level code anyway, and are explicitly only for use with enum indexed arrays, so they would always be using enums and would always be exactly the size of the enumeration. &amp;#x200B;
Like this: template&lt;class Data, class Key, Key size=Key::Count&gt; struct enum_array; you may have to get fancy with how you evaluate `Key::Count` to avoid too aggressive SFINAE, but maybe not; regardless, nothing fundamental. You control the enums so you can force them to all, or almost all, have `Count`. 
The link to part 1 is broken.
I have never used auto. As I said, my general position is that we need better code, not more code. So I don't consider using something like auto just to avoid explicitly indicating the type to be something that would contribute to the former scenario. Obviously there might be some special cases where it's warranted for some reason, but on the whole I don't have a problem with actually saying what the type is. My position is that, I don't want to have to remember if I was sane at the time I wrote that code when I look at it later, and don't want someone else who might support it later to have to assume what my intent was. So I try to be very explicit in my code. I try not even depend on things like operator precedence, since I don't want to have to figure out later if I really was correctly using precedence when I look at that code (or for someone else to.) If I explicitly indicate groupings, then there is zero ambiguity that that is exactly what I meant. I don't do any automatic conversion operators. And I do use explicit on single parameter constructors. Though, that's also something that came in long after I started this stuff, so I have just been adding it in classes that I happen to be messing around in and where it would be most important. Overall, with the exception of default method parameters, I try to avoid as much auto-magical stuff as I can. It does make the code wordier, but on the whole I don't consider that a problem relatively speaking. Better the devil you know that the potential devils you don't suspect.
Naw, auto a = +[a=+[]{return 0;}]{return a();}(); has more `+`s. 
Oh, it would be, and didn't do that. I was just saying that, if you ended up with a situation where you were going to have to make many thousands of fixes of that sort, you would have to actually make those changes in every place, so that it would be a particularly bad scenario. There's no good way to magic yourself out of it, even if only to temporarily move forward. You could create another inline function, but you would still have to change every instance to use that. It's difficult to search and replace that type of change, because the calls could come in a number of variations. &amp;#x200B;
Yeh, they all do. My IDL compiler generated enums always have them, and I add them to hand created enums as well. For non-bitmapped enums both types always also have ::Min and ::Max values, which are used for enum based loops. My IDL compiler can generate Inc/Dec operators for enums, and I have a macro that implements some of that stuff for the low level enums that can't use the IDL compiler. So I can fully use them as indices in a natural way. &amp;#x200B;
Great, that behavior is what I would expect too, but that's probably because you and I have learned to pay attention to nuance and detail in this regard. `std::vector` is also a baby example chosen for it's familiarity. You simply cannot rely on every object that can be constructed from an `initializer_list` actually being at all similar to an array. Conventions are great, but the actual language rules are what matter, because you're always going to consume code written by people that (for good reasons or bad) push the boundaries of the language. Regardless, no matter how you skin things, it doesn't change the fact that brace initialization has been sold to many as ‚Äúuniform initialization‚Äù, and that it's suitable for a replacement of using parenthesis for construction everywhere. The whole idea that it solves ‚Äúthe most vexing parse‚Äù, as the person I responded to suggested, relies on that idea. But despite all that, it's a hard fact that there are constructors you *cannot* invoke using brace initialization, because `initializer_list` ctors are greedy and can end up shadowing a lot of alternatives. So either you learn to use braces when they actually make sense (which it seems like you probably do) and maybe still deal with an annoying vexing parse on occasion, or you try to use braces everywhere as a rule and occasionally get surprised (and maybe catastrophically) when the code does something unexpected.
Depends on who you ask. Java interfaces tend to end in -able; Rust's traits do not. I've seen good arguments both ways.
I use native debugger and [Deleaker](https://www.deleaker.com/).
Oh, and in terms of general thoughts... I guess one practical one, if you are not in a situation like I am, i.e. not a single developer but in a sizeable team, doing something like this may be a big issue from a version control stand point. Suddenly large swaths of files are being changed every day, swamping the actual functional modifications and bug fixes. And, when a new release is ready, almost every file in the code base might have been touched. And just the coordination required to do code base straddling changes like this in a team environment would be bad. For me, it's not much of an issue. I know when it's OK to do these things. I have though been making GIT choke fairly often recently because of the huge numbers of changes being checked in. So it's gone into its long housekeeping slash optimization phase a few times. &amp;#x200B; In the end, my obsessive/compulsive nature has actually helped. I can grab a cup of coffee, throw on something like Mellon Collie and the Infinite Sadness, and just go deep for many hours at a time. Yesterday I realized it was 8PM and I'd eaten nothing but a bran muffin all day, because I was so heavily into this process. I was seriously wobbly by then. But that kind of deep immersion really is a benefit for this type of work. In a team environment, where you might have to do it really piecemeal to avoid stepping on toes and constantly stop to deal with other things, I could imagine it would be a lot easier to make mistakes due to distraction. &amp;#x200B; Obviously, if you want to make such a change, you could have a team sit-down, explain to them the issues, and make it clear that they should be avoiding certain problematic things, basically any dependence on automatic conversion of enums, and make that a thumbs down criteria in code reviews and such, so that you aren't making things worse in the meantime. And, where the use of enums are internal to a file or module, allow for those to be taken care of by the maintainers of that code as time allows. &amp;#x200B; Clearly one problem with doing it piecemeal across the code base is that you end up for some time with a hodgepodge of new and old style enums, never being quite sure which syntax is correct now for a given enumeration. That's one reason I've been pushing pretty hard to get through this process. It could be really annoying if it's stretched out over a long period of time and constantly changing the whole time. &amp;#x200B;
Agreed. I look at some of this stuff and scratch my head. If you need a degree to figure out how to do a loop, because it's become so byzantine at this point, I don't get it. I keep coming back to the mantra: we don't need more software, we need better software. Explicitness, in my opinion, is more likely to create better software. Incomprehensible and auto-magical mixed together, to me, seems the absolute opposite of what is likely to create better software. Is the goal maybe to ultimately turn C++ into Javascript? 
Yeah, that's an unfortunate limitation of C++. Other languages use extension methods to do this sort of thing, which allows for a much cleaner (and more discoverable!) API.
&gt;Indeed it *was* omitted Thanks for clarifying. I thought they went back on that decision for a second there.
Unfortunately, the current implementations in clang and MSVC don't do a great job of optimizing coroutines. You can see what the assembly output looks like here: https://gcc.godbolt.org/z/009O3w Some notes: - There are no allocations, which makes sense because the stack size of the coroutine is known at compile-time. You can think of it as putting all the local variables from the coroutine into a struct, and then the coroutine runs as a member function on that struct. - It *is* able to inline and optimize the uses of view::iota and view::take, since those aren't actually implemented with coroutines. In general, use of the ranges library is optimized as well as "hand-made loops" are, if not better. 
&gt; As I said, my general position is that we need better code, not more code. And that's why you should use auto? Or you think that this: for(std::unordered_map&lt;std::string, double&gt;::iterator it = foo.begin(); it &lt;= foo.end(): ++it) { // ... } Is better (and not more) code than for(auto &amp;pr : foo) { // ... } ? 
I don't even use the standard libraries. I have my own. So I never went down that 'everything is an iterator' path to begin with, therefore my for loops aren't long complicated statements like that. Generally they are simple for index statements. Where things are iterators, such as collection cursors, the syntax is simple and straightforward while or do loops. 
https://blog.feabhas.com/2018/12/a-brief-introduction-to-concepts-part-1/
I can ignore it at work since there's an internal container library and it's pretty unlikely we'll add a library that uses them. Fair point though.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/aa1upg/book_to_learn_c_with_previous_knowledge_of_java/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; I don't even use the standard libraries. I have my own. Another thing that usually (though not always) conflicts with &gt;As I said, my general position is that we need better code, not more code. If you don't ever use any collections that can't be indexed by an `int`, don't care about interacting with "outside" world/stdlib and roll everything you own accommodating your usage patterns then indeed you may not find auto that useful (though you'd still need to use it for lamda's, unless you don't support that too...). But yeah, it's in no way an argument against `auto`.
I've been implementing move constructors in key classes where they would be most likely to provide performance benefits. Though, from what I've seen, the compiler is so good at eliding copies that I'm not sure how often they really get invoked if they are not forced. I do use collections that can't be numerically indexed, such as hash tables and hash maps. Those are iterated via cursors, which have a simple while/do loop type syntax, as mentioned. I've not used auto in any of that implementation of move constructors though. Anyway, I said there could obviously be some specific circumstances that require it. But, in those cases where it's just a way to avoid typing the actual type, I don't consider that a benefit to long term robust code. If I indicate the type explicitly, and what I get isn't that type, I'll know immediately. It won't auto-magically get used and potentially do the wrong thing. 
I think it has too many shortcomings to be able to actually earn that name. For example, you can't define new functions at the GDB "repl".
And why do you need this whole k scaffolding?
I avoid auto too. I use scoped for, though. I'd probably write that as: for (Foo::value_type &amp;pair: foo) &amp;#x200B;
A small issue is that it makes `size()` UB for negative `start` (by current design `size()` is disabled for infinite range). A bigger issue is it only works with numeric types (where range-v3 already offers more descriptive names: `ints` and `indices`).
lol, thank you for saying this. constant time has a very well defined meaning in terms of algorithmic running time and I was kind of miffed at the point being made.
Oh, and another gotcha I forgot to talk about... When using enums as bitmapped values, how often are you going to end up with something like this: if (eMyValue &amp; eSomeMask) { } Is eSomeMask multi-bit and all the bits should be on, or can any bits be on? Obviously as written any bits will work, but is that the intention? It's not at all obvious from the code, and probably most of the time you really want all bits to be on (if there's more than one in the mask.) But I bet we'd all end up with plenty of those. I created a set of templatized helpers like bAllBitsOn(), bAnyBitsOn(), eOREnumBits(), eANDEnumBits(), eMaskEnumBits() and so forth, to make dealing with these types of scenarios a lot easier and more type safe. I can't mix bitmapped enum types by accident either now, which is an important check. The OR and AND ones come with two, three and four value variations for convenience. And even if you do the right thing: if ((eMyValue &amp; eSomeMask) == eExpectedBits) { } That still has to be replaced since those types of things don't work anymore with type safe enums. So the above helpers would have to be used to replace those, and it pretty much needs to be done by hand, because it would be tricky to search and replace all those possible variations. Luckily it's not something that tends to be done enormously often, at least not in my code base. But it's one of the most common by hand fixups I have to do, and sometimes I have to stop and carefully check whether I correctly am checking for any/all bits appropriately to do the conversion. &amp;#x200B;
Care to share the warning flag?
I like it!
That looks really great! I hope it gains traction
No
How is that better? You don't know what `Foo::value_type` is anyway. You just have to write more.
&gt; my code base is very high quality Based upon? There is plenty of low quality code being used every day by thousands or millions of customers. You've described C with some C++ features stuck in.
Based on the fact that I've been developing it since around 1992, and that I've been an extremely hard core developer for 30 years now. I suck at a lot of things, but coding isn't one of them. Unless you are going to put in the time to evaluate what I'm capable of, don't try to denigrate it. I posted a link to one of my other posts, which has links some others, where I talk about some of the technology I've developed. The standard libraries work I've done is really baby stuff compared to a lot of the other stuff I've done as part of this project. And of course I never said they were BETTER. Someone else said, or implied, that they were LOWER quality because I did them. I was just responding to that, indicating that the hugely complex and very broad automation system that is built on top of my general purpose code base is very well regarded as highly robust and stable. It's network distributed, multi-user, highly multi-threaded, enterprise level stuff and has to run for very long periods of time without failures or it just wouldn't be acceptable. So, anyway, feel free to look at the stuff I posted if you want. Otherwise, it's a waste of time to me just say this or that. Other people will take every attempt I make to back up my claim as some sort of challenge to their manhood and it will descend into a huge circle jerk. Been there, done that, don't want any more t-shirts. &amp;#x200B; Oh, and BTW, it's as far from C with some C++ features as possible. How you got that I dunno. Keep in mind that the standard library is NOT the language. They are separate things. You can write very C++'ish code without using the standard libraries.
I find `std::integral_constsnt&lt;std::decay_t&lt;decltype(foo)&gt;, foo&gt;` very verbose at point of use? And I want stateless destroyers, so want the function to be stored in the type.
I agree, only a twisted mind can really believe the code looks "cleaner" with them. They are so ugly they belong in the same sack as goto: Allowed only if you convince all the senior developers the alternatives are worse. 
To be honest I think that there are other things that greatly improve the compilation time a whole lot more than which kind of initialization you use between a C-project and a C++-project, such as an abundance of potential template-instantiations that are checked.
Correct, because this is a contrived benchmark. It could exist in production code, or when you have a library exposing functions and/or preprocessor-macros where you do not know what kind of types will be supplied by the library-user.
Not to hijack OP‚Äôs topic, but you seem like a good person to ask: could you recommend any multiarray libraries for C++? I need it for some PDE code, something that can handle at least 4d.
Well, there's a [r1](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1030r1.pdf) with feedback from LEWG at Rapperswil. (You can always find the most recent at http://wg21.link/p1030 .) I don't think it was discussed at San Diego.
It is a micro benchmark checking two specific different types of initialization for integral values. Obviously, any micro benchmark is a contrived example, but for other types in your "real world code", where you have the ambiguity usually don't support the syntax `MyType I = 5;` in the first place and the benchmark has nothing to do with them, so why bring it up here?
And of course the passive-aggressives have now shown up, as they always do, pounding away on the down-vote button in brave anonymity, lest something that challenges their world view be read by others.
For those interested in functional programming using C++, look at this link from one of the demo notebooks: [https://code-ballads.net/generated-notebooks/cpp/repl\_cling/notebooks/2\_Functional\_REPL.slides.html#/](https://code-ballads.net/generated-notebooks/cpp/repl_cling/notebooks/2_Functional_REPL.slides.html#/) (press space to go from slide to slide)
I don't like that it has dependency on path and string so every time I include path_view I include also filesystem and string headers.
Do you think the extra 10 copies of 3-tuples of ints is really what makes or breaks this example?
Agree.
&gt; (And also probably ‚ÄúDon‚Äôt use Ranges,‚Äù at least not on Godbolt Compiler Explorer. If you do, you‚Äôll need to know about the ‚ÄúClear cache &amp; recompile‚Äù button in the lower left corner of the compiler pane. I had to click it about a dozen times before Eric‚Äôs toy program managed to sneak in under CE‚Äôs eight-second limit on compilation time.) lovely
"To prove my point that it always works, here's another case where it works." ... That's not how this works.
You know what else avoids the unnecessary copies and also does *not* hide information from the reader of the source code? Good old `const auto&amp;`. The advice of using `auto&amp;&amp;` all the time is terrible, as it hides information. **Use it when it's needed.**
lmao this guy and his "guidelines"
&gt; A better design is to express the graph through data dependencies, which is essentially what futures and other modern parallel constructs do. well, what would you do if you really wanted a graph ? e.g. if you had to implement a software where you put nodes in a GUI, connect them together, and have the computation run ? 
`auto&amp;&amp;` doesn't convey the semantics of when the object _should be_ and _is_ `const`. You should do const by default and write const whenever possible. That's another rule. When I see `auto&amp;&amp;`, I don't know whether the object is supposed to be const or not and maybe it's modifying something by accident?
I said this before, but that's a total disaster. Are we actually expected to use this in real code bases of hundreds of thousands of lines of code? 
It might be possible to come up with use cases for an explicit graph, but I've yet to see a good one. Take your example; in this case I would still use data dependencies based on the input and output of each node to generate the graph implicitly at runtime. 
If it always works then why don't compilers treat the behaviour of auto as though it was written as auto&amp;&amp;? 
Let's wait until C++20 compiler are mature and we have libstdc++ and libc++ implementation of ranges leveraging C++20 features. Things might be a lot better by then.
Oh cool, someone else using this as well. I thought I was being paranoid for factoring out the common `std::tie` into a lamda :) You can make the lambda `constexpr` as well (C++17).
&gt; disallowing templates Man I love having to resort to external code generation tools or use void pointers everywhere
To be fair it is a clever way to compare version numbers with major and minor components (and revisions if you use a tuple). Also perhaps comparing analog clock hands, but that's a long reach. Personally while it looks cleaner it is *not* clearer. The first example needs no comment while the second would have me opening a browser tab or two.
Are those pumpkins?
This looks really good. Io addition to godbolt, we start to have nice tool to explore C++
I wouldn't expect my changelists to pass code review if I try to use it, so ü§∑‚Äç‚ôÄÔ∏è
Note that explicit copy/move ctors are forbidden by the STL - such types don't meet the CopyConstructible/MoveConstructible requirements. You can play around with such types as a temporary investigation, and use them with the Core Language all you want, just don't give them to our library templates :-)
&gt; The standard libraries are the way they are due to arbitrary choices made by the folks who created them Strongly disagree. The Standard Library has been carefully designed by experts. It's not perfect but it's far from "arbitrary".
Please stop. [Reddiquette](https://www.reddit.com/wiki/reddiquette): "Please don't [...] Complain about the votes you do or do not receive, especially by making a submission voicing your complaint. You may have just gotten unlucky. Try submitting later or seek out other communities to submit to. Millions of people use reddit; every story and comment gets at least a few up/downvotes. Some up/downvotes are by reddit to fuzz the votes in order to confuse spammers and cheaters. This also includes messaging moderators or admins complaining about the votes you did or did not receive, except when you suspect you've been targeted by vote cheating by being massively up/downvoted." HN has the [same philosophy](https://news.ycombinator.com/newsguidelines.html): "Please don't comment about the voting on comments. It never does any good, and it makes boring reading."
I made a mistake one too many times (i.e. once): return std::tie(lhs.a, lhs.b, lhs.c) &lt; std::tie(lhs.b, lhs.b, lhs.c); Never again!
References on container requirements: https://en.cppreference.com/w/cpp/named_req
It wasn't about me, it was someone else who was getting abused. I don't agree with that person's point, but down-voting him into oblivion wasn't warranted. &amp;#x200B;
By arbitrary I didn't mean bad. I meant they made choices that were not forced on them by the language. They chose to do it that way. Though, to be fair, arbitrary does imply no underlying systematic reasoning for the decisions, which I didn't mean to imply. The point was just that the library is not the language. They library isn't the way it is because the language requires it to be so, and you can create something completely different without being any less C++'ish. It is the way it is because they just wanted to do it that way. Mine is the way it is because I wanted to do it that way. 
Can you provide examples?
There's a lot of gorp that's been thrown into the language and the accompanying standard libraries over recent years, making it way too complex and convoluted (IMO.) Depending on whether the examples in your book are using this new stuff or not could explain the difference. You can write very straight-forward C++ and not use any of this new stuff. Or you can go full out and write stuff that barely even looks like C++ anymore. The folks driving the C++ language these days seem to have Javascript Envy or something. Though of course as mentioned throw out some examples and that will help. &amp;#x200B;
[https://github.com/RyanSwann1/C-Developments/blob/master/include/Animations/Animation.h](https://github.com/RyanSwann1/C-Developments/blob/master/include/Animations/Animation.h) Above is a link fo somebody's project I was browsing. Obviously as a beginner I am not at the stage where I could create a similarly complex project, but I would at least like to be able to understand his code. Yet none of the syntax is even remotely familiar. Is my textbook too limited, and if so can you recommend some other sources to learn more advanced syntax/functions? 
Why does everything have to be so fancy? 
Looks like a pretty simply class that holds data and has accessor methods. What syntax are you not understanding?
C++ is a huge language that‚Äôs acquired a lot of cruft from backwards compatibility and implementing new ideas and paradigms. A main reason it doesn‚Äôt get ‚Äútaught‚Äù to beginners is that a big portion of the time can go to learning intricacies of the language. Those intricacies are more difficult to understand without a larger picture of the current state of the art or without context of the past. One part that may be especially confusing is in templates since client code normally doesn‚Äôt require the programmer to write complex templates. But you can still write perfectly performant and clear C++ without ever knowing that syntax. It‚Äôs normal to see examples that you don‚Äôt understand. Just be open to reading a lot of background on, e.g. cppreference.com, and asking questions, and you‚Äôll be fine. I wouldn‚Äôt expect to know all the ins and outs and syntactical features unless I was the author of a large library. 
Has the text book introduced classes and references yet?
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/aa6asa/feel_like_i_dont_know_certain_syntax/ecphfe7/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
We already have that, it's called `decltype(auto)`.
Agreed, and I'm the author of P1030! But baby steps first. The committee currently doesn't see the point of this proposal wrt existing standard library facilities. They feel `filesystem::path` sufficient. That said, lots of feedback on R0 came in which resulted in R1, which has addressed all the major technical objections. So now it's really just one of "is there a sufficient value add?"
If the committee approve the proposal in principle, you can rest very assured I intend to purge the path and string dependencies. **Very** assured.
But it already works in current compilers, it doesn't actually require any new technology. Are you saying there is still so much left to be gained that we can afford this type of design? Given how much effort is poured into compilers already, I'm doubtful that there is enough to be gained to make this worthwhile... &amp;#x200B;
It was not discussed at SD as I was unable to attend SD due to work contractual requirements. I'm currently unemployed, so until I find a new employer, I cannot say when I can attend another meeting. I am very sure I can't attend Kona due to my wife forbidding it. So Cologne is the next most likely. If I do attend Cologne, you have some chance it will be discussed by at least LEWG-I, if not even LEWG as I did submit the paper before the incubators were established.
The godbolt example uses range-v3 and its SFINAE-based concept emulation; that's known to be slower than real concepts - even for the prototype implementations we have today. Including everything and the kitchen sink doesn't help either.
We cannot ban stupidities from code. Now try the same kind of things with unsafe enums. Ah, and good luck grepping the problem with old enums!
I am curious and maybe someone here knows the answer, how did this new ambiguity make it into the language? Was it a conscious decision to allow it or an unexpected side effect?
Why do we avoid goto statement and raw pointer? Because they always work.
&gt; I guess one practical one, if you are not in a situation like I am, i.e. not a single developer but in a sizeable team I have nightmares of ever seeing your codebase. Every post of yours mentions a single developer working for decades on a massive codebase, with every layer written yourself. Do you have other developers on your team? Who assesses the quality of your work? Do you do pull requests? 
You know, I don‚Äôt know why, but this never clicked until now. I use auto, especially when iterators are involved, but I never put together that any change in the type of `Container::iterator` would have the same effect whether you‚Äôre using auto or not.
Why don‚Äôt you have something like: (and vice versa) template &lt;typename Enum&gt; constexpr auto GetEnumValue(Enum enum_case) { return static_cast&lt;std::underlying_type_v&lt;Enum&gt;&gt;(enum_case); } I assume you have your own implementation of `&lt;type_traits&gt;`.
I don't know if and by how much compile times for standard ranges will be better than v2, but ranges-v3 has to jump through a lot of hoops to work around missing concepts and I guess other language features like generalized constexpr and constexpr if should simplify the metaprogramming aspects of the library. Sure, if range-v3 compiletimes turn out to be represerepresentative anyway, I'm pretty sure I'll stay clear of the views and actions too. 
Huh, that‚Äôs an interesting requirement. Isn‚Äôt depending on the non-explicitness of a copy / move constructor just a normal implicit constructor call at that point? (As opposed to really being a copy / move)
It sounds like you‚Äôve created iterators for your enums, and used those as indices, instead of using the iterator concepts in the STL.
&gt; `int delay` Oh come on. You're using `chrono`!
https://github.com/OpenRCT2/OpenRCT2/labels/good%20first%20issue
Since you‚Äôre working with MPark on this, I thought I‚Äôd ask, what bearing does wording like the constant time wording have on actual implementations, especially something as high profile as variant performance? I would imagine that implementations generally treat the standard as something that needs an excellent reason to violate / bend, but something that *can* be violated.
Some quick notes: this presentation was done in July, right after the Rapperswil meeting. Some parts of the presentation might be out of date, but I think still very much relevant. I've read recent blog posts and conference talks on Concepts, and there is some information or context I feel is missing from those sources that is provided here. Of course, there are many things that those sources have which I don't, so you can see this presentation as a sort of complement to the new articles that are popping up.
What do you mean by `extension methods`?
I don‚Äôt think it‚Äôs fair to categorize ranges as just ‚Äúdoing a loop‚Äù. auto result = input | view::transform(DoFoo) | view::filter(IsBuzzed) | view::unique; Is non-trivial to do *lazily* without ranges. I don‚Äôt think your mantra adds any value here.
The author has advised this in this [tweet](https://twitter.com/andrew_koenig/status/1075796048851992576): "There is not currently a second edition in the works. AT&amp;T holds the copyright and so far the publisher has not been able to obtain permission from them for a revision. I don't know why the shipping time is that long; the book is very much still in print."
&gt; Regardless, no matter how you skin things, it doesn't change the fact that brace initialization has been sold to many as ‚Äúuniform initialization‚Äù, and that it's suitable for a replacement of using parenthesis for construction everywhere. Imo that was indeed a mistake. The notion that `Foo bar{...}` ist just "a better spelling" of `Foo bar(...)` has never been true and I find it sad that it has been sold as such. I like to point people to the following SO answer, which is pretty much aligned with my own usage : https://stackoverflow.com/questions/18222926/why-is-list-initialization-using-curly-braces-better-than-the-alternatives/37228443#37228443 The thing I don't understand with the whole discussion though: I don't see why initialization with curly braces introduces a new problem in the first place: I never calle `Foo bar(a,b)` unless I've looked up the documentation of `Foo` an checked what that constructor actually does anyway. Why should I now call `Foo bar{a,b}` without first checking, what it does? You have to either check the documentation of a class or try to deduce the meaning by relying on convention for any function call (including constructors) anyway. 
Delfine *container adapter*.
If I understand correctly, the article is a code review of a [small, random library](https://github.com/shalithasuranga/timercpp/blob/master/timercpp.h) for a timer. That library: - uses C++20 concepts just to get `auto` for parameters, but says it's a C++14 library; - has an `int delay` for milliseconds; - has undefined behaviour with a non-atomic `bool` being used in multiple threads; - includes `&lt;iostream&gt;` for no reason; - uses detached threads; - is not exception safe; and - will most likely crash when destroyed. All that in 40 lines. The article says that "it should maybe not be used in production", which is quite the understatement.
We are always looking for developers: https://github.com/x64dbg/x64dbg
I didn't expect to say so, but that is actually looking quite interesting. At a first glance the only issue I saw was that you didn't use standard-naming-conventions: Classes in C++ are written in `lower_snake_case` as the standard-library clearly demonstrates.
Shouldn't `clear` variable be `std::atomic_flag` instead of bool? And using `sleep` for timer is not very good - you will wait for timer period even after stopping it (and it could be a very long time). I guess it's better to use waiting on `condition_variable` with timeout. Another issue is that detaching thread is quite dangerous - if it will not die before end of `main` and nobody calls `stop` function bad things may happen.
Great article. Really insightful.
Thread per timer? Are you kidding? I don't understand why such toy implementations as mentioned [timercpp](https://github.com/shalithasuranga/timercpp) are discussed at all. There are several well defined timer mechanisms those allow to effectively handle millions of timers on just one thread: timer\_wheel, timer\_heap or timer\_list just to name a few. Very good implementations of timers (with selection of underlying timer mechanism) can be found in [ACE](https://www.dre.vanderbilt.edu/~schmidt/ACE-overview.html) library. But ACE is a rather big library (and it seems to be not very popular today). So it is not a convenient to use it just to get timers in your application. Because of that in 2014 I wrote [a small header-only library for C++11](https://sourceforge.net/p/sobjectizer/wiki/timertt%201.2/) that implements such mechanisms as timer\_wheel, timer\_heap and timer\_list and has some points of customization for your need. This library hadn't attract any attention, but it just works and can handle hundred of thousands timers. There is just a simple example of how work with timers can looks like: timertt::default_timer_wheel_thread tt; tt.start(); // Adding single-shot timer. tt.activate( std::chrono::milliseconds( 500 ), []() { std::cout &lt;&lt; "Hello, World" &lt;&lt; std::endl; } ); // Adding periodic timer. tt.activate( // Pause before the first timer activation. std::chrono::milliseconds( 250 ), // Period of timer activations. std::chrono::milliseconds( 500 ), []() { std::cout &lt;&lt; "Hello, World. Again" &lt;&lt; std::endl; } )
Though I'd agree with you usually, and personally follow that style. It's incorrect to say that so generally. Unlike Java where it is a compilation error if they're not in UpperCamelCase (though that may have changed since I last used it), it's very much a personal preference and the style guide you are following. For example, Google's style guide for C++ uses UpperCamelCase rather than lower\_snake\_case It would have been fairer to state: *I saw that you didn't use the naming convention of the standard library; I personally would have preferred if, at least the public API, was in lower\_camel\_case.*
Really nice project, makes me wanna write some console app and use it ;) A small improvement possibility I've noticed is in the [clear\_line()](https://gitlab.com/miguelraggi/tqdm-cpp/blob/master/tqdm.hpp#L158) method. Instead of the for loop and accessing the os 80 times you could use this: `*os &lt;&lt; std::string(80, ' ');` &amp;#x200B;
Conan or vcpkg would be great
I usually define operator&amp;() so that code would continue working. I'm happy with bitwise operators that return a new value of the enum. I do use a named function for converting to bool; so far I've only needed bAnyBitsOn(). I'd probably use == if I needed something different.
&gt; I don't understand why such toy implementations as mentioned [timercpp](https://github.com/shalithasuranga/timercpp) are discussed at all. I have no problem with discussing it, it's a great example of what _not_ to do, and those are always worth reviewing. That's how people learn. My problem with the article is that it's very tentative. It has to be made clear that this is a completely broken implementation which shouldn't be used anywhere.
Something like stack in the standard library. 
&gt;it's a great example of what &gt; &gt;not &gt; &gt; to do Ah, I didn't see this point in the original article. Sorry.
So concepts will allow using auto for function parameter types?
It's better because it avoids `auto`. In that example, `auto` buys you very little over the code that doesn't use `auto`. It has the additional advantage of suggesting that `pair` is not a named type, because if I would not have used `value_type` if it was. I would use `int` or `Customer` or whatever.
&gt; Ah, I didn't see this point in the original article. Sorry. No problem. It's at the top, third paragraph down: &gt; If it is not production-ready, why talk about it then? For two reasons: &gt; - its implementation is instructive to learn about C++ standard library‚Äôs basic usages of threads, &gt; - the reasons why it should maybe not be used in production are also instructive. 
It's part of what's called ["abbreviated function templates"](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1141r1.html) and was voted in C++20 just last November if I'm not mistaken.
\&gt; There are no allocations, which makes sense because the stack size of the coroutine is known at compile-time. You can think of it as putting all the local variables from the coroutine into a struct, and then the coroutine runs as a member function on that struct. &amp;#x200B; I¬¥m not that deep in the topic, but is this argument really correct? A coroutine size should always be known at compile time, right? As I understood it from the [llvm](http://llvm.org/docs/Coroutines.html#avoiding-heap-allocations) reference, heap optimizations can/will be avoided as long as the coroutine is created, used and destroyed within the same function. 
No bad things can happen if you taskkill it. *Insert meme*
What is wrong with detached threads? 
I've had something to say about this at CppCon: [https://www.youtube.com/watch?v=YxmdCxX9dMk](https://www.youtube.com/watch?v=YxmdCxX9dMk)
you might find this interesting: [http://blincubator.com](http://blincubator.com)
Cool thing. Will have a look at this 
There are several common naming conventions in C++, and it isn't particularly wrong to use any of them. To be perfectly honest, I rarely see snake case used outside of boost and the standard library. Most of the libraries I run across use variations on camel case.
There is one absolute rule for naming: The standard-library of a language is by definition correct, everything else is by definition wrong! And yes, I am aware that google loves to demonstrate in their coding-standards that they have no clue about how to properly write C++ (like when they banned lambdas!) and that they prefer messy code with lots of varying naming-conventions over consistency, but that doesn't mean that it is not an unprofessional approach to programming. I really don't care much what naming-conventions a language use, but I do care about them being applied consistently and will seriously try to avoid libraries that turn the codebase into a pure mess. 
There are three things that I generally look for in documentation * Every function on the public API should have doxygen (or similar) documentation. * There should be an overview document that explains the concepts that span the entire library. * There should be a suite of examples small examples showing how to use the library for various tasks. I think that the examples (which should be well commented and use modern best practices) are one of the parts that are most often lacking. Most projects have function level documentation, but example code is incredibly helpful for getting started with the library. For an example of a project that does a very good job: vtk is amazing about this. It has tons of examples, each with descriptions of what they are showing, and the examples tend to demonstrate concepts clearly.
stared it nice work; will read your implementation a little later. got to love a sexy progress bar.
in general a `CMakeList.txt` with a library target in a namespace *and* alias; as is convention. That way the project can be included several different ways and still use the same target identifier `tqdm-cpp::tqdm` would probably be the most conventional; though anything like `tqdm::tqdm` or `tq::tqdm` or something.
You could make the `set_xxx()` functions return `*this` so you can chain the calls: std::vector&lt;int&gt; A = {1,2,3,4,5,6}; for (int a : tq::tqdm(A) .set_prefix("Iterating over A") .set_ostream( std::cout ) ) { //... } 
This happens in many forums. I think it is inevitable.
Not a critique of this paper, but the fact that we find ourselves needing to write `_view` classes so often, duplicating interface and writing boilerplate, when references are precisely supposed to be a built in language mechanism to solve this problem, is kind of disturbing.
Well, its buffered, so it shouldn't be that bad... Either way, probably better off with something like: (*os_)&lt;&lt;'[' &lt;&lt;std::setfill('#')&lt;&lt;std::setw( num_filled)&lt;&lt;'#' &lt;&lt;std::setfill(' ')&lt;&lt;std::setw(size-num_filled)&lt;&lt;' ' &lt;&lt;']';
I wholeheartedly agree with you, good examples are hard to come by, and a rare sight in libraries. The main way I learn an API is by looking at examples and seeing how components interact. Sure, having a nice reference is nice, especially once you've learned to use the API, at the start though, it's invaluable with the small examples you're describing. Also, the example can't be too big, or they'll just confuse. They should just be long enough to demonstrate the main usage of it. Having some longer examples is fine, but I tend to prefer shorter examples. The one library that is a big culprit of having large examples is [OpenSceneGraph](http://www.openscenegraph.org/index.php/documentation), which is also one of the libraries I still have no idea if I'm using properly or not. A lot of the examples and documentation are also outdated and don't work. An example of a library which I think has excellent documentation (it's C though) is [GLFW](https://www.glfw.org/docs/latest/). And part of it is because they have short, to-the-point, examples of how to use parts of the library. Also, IMO, it does the whole Doxygen documentation bits in a way that makes sense, and actually provides valuable information. Lots of Doxygen in many projects are completely useless, and add nothing to the docs that are already available. 
Stroustrup's own preferred style is Upper_snake_case in order to *differentiate* from standard library types. 
There different types of documentation: https://www.divio.com/blog/documentation/ What are your goals?
The goal is that new colleagues have an easier starting point for collaboration. Espacially for some of our students, so that they don't have to ask every 5mins a supervisor 
As far as function/class level documentation I generally point to Qt as being a great example. I have never ben unable to find the info I want about any class or been confused about how something works after reading the documentation.
That's essentially what I have, though I want explicit conversions not generic ones. I want it clear from reading the code what types are involved.
It‚Äôs a single header include, I‚Äôll probably vendor it into my projects. Full on package management for something as simple as this feels like overkill!
[removed]
I love how people just assume that a single person cannot write an incredibly high quality code base. And also just ignore all the the other types of horrendous problems that occur when large teams have to create large code bases over long periods of time. Both schemes can create great code and both schemes can create crap code. I promise you, it's incredibly high quality, incredibly clean code. And, as I've mentioned elsewhere, there's a very powerful, very broad, and very robust automation system built on top of the general purpose layer that is out there in the field running 24x7 in very demanding circumstances. My quality of life and my reputation both depend on this code being of the highest quality. It's never been compromised in any way. Unlike large code in team based environments, I've never had to hack something in. If something needs a fundamental change, I can take the time to do it cleanly and without evolutionary baggage. I'm under no pressure from management to just get something working, so I never have to hack. I understand every part of the code so I always know what is the right thing to do to avoid problems and never have any redundancy. &amp;#x200B;
If you are already using a package manager for your project it becomes an easy way to track third-party stuff, even if it doesn't simply the build at all.
For those things that are indexed by an enum, then yeh, of course I iterate them directly via the enum. It would be crazy to create an iterator to iterate an enumeration that it's right there to be used in a simple and direct and obvious way, right? That gains nothing and just obfuscates. My collections do support cursors of course, which are equivalent to STL iterators. But for a private collection that's not being accessed by anyone but the class that contains it, there's no need for the overhead and obfuscation of using a cursor or iterator for something that's clearly and visibly index based. Again, it gains nothing and just obfuscates. I use cursors where the collection I get is via the base collection class so it's not known what actual type it is. I can ask it generically for a polymorphic iterator to use. Or it's not an indexed collection, obviously. Or where the collection is not directly accessible, i.e. a class hands out cursors for a collection it keeps internally instead of exposing the collection itself. That sort of thing &amp;#x200B;
If you work with oss in a large coorporate environment, including headers into the source three is problematic due to clearing... everything should have a cmakelists with exported targets, license, readme, changelof etc
I like that SO link, I'll probably share it myself in the future. I can't speak for more than myself, but I think that the problem is `initializer_list` constructors are *way* too greedy, and their interaction with brace initialization had unintended consequences. Even if you are diligent in looking at what constructors are present and how overload resolution would play out, I'd be willing to bet that the large majority of (reasonably experienced) c++ developers either don't know or don't actively appreciate that during brace initialization, `std::initializer`ctors are considered separately and before anything else in overload resolution. An initializer list ctor that requires conversions will be selected before a different ctor that doesn't require any conversions at all! That's totally surprising if you've not encountered it before. So if you're in the "always use braces camp" odds are high you'll end up surprised at some point. #include &lt;initializer_list&gt; #include &lt;iostream&gt; struct Foo { Foo(std::initializer_list&lt;double&gt;) { std::cout &lt;&lt; "initializer list\n"; } Foo(float f, int i, char c) { std::cout &lt;&lt; "normal ctor\n"; } }; int main() { // Expected non-initalizer list ctor Foo foo1(12.0f, 3, 'a'); // Expected initializer list ctor Foo foo2({12.0f, 3, 'a'}); // Surprise?! This still calls initializer ctor despite being perfect match if using () Foo foo3{12.0f, 3, 'a'}; return 0; }
My personal preferences on documentation go like this: I want to understand what happens when I put something into a function and what I get out, does the function mutate anything? Do I have to worry about anything in particular with that function? Next step: An example of the function. Functions are pretty useless to me if you describe them and I have no idea what context I need to set them up. Generally a good example can be a great stand-in for actual documentation, but should never be a replacement. Best setup: Docs come with examples, shows use, and example outputs for example inputs. Shows the big three, expected inputs/outputs, examples on usage, and an example program that you can use to mash together it all.
The *setInterval* function won't fire every X milliseconds. It will fire every (X + time to execute function) milliseconds. 
Thanks! I didn't think of that! I'll implement it later.
Thanks! I'll correct it when I get to a computer. 
I didn't know cerr didn't buffer. I'll fix it, many thanks.
Sure, when I feel it's ready, I'll look into it.
Thanks. I think you are right: I'm mixing camel case and lower snake. I'll fix it. 
This mainly exposes how badly C++ needs a standard event loop. Networking TS can't come soon enough; not for the networking (that's the easy part), but for a standard vocabulary to use for event-driven libraries.
Thanks. I've never understood the advantage of header guards over pragma once. They don't express intent and feel like hacks to me. Do they have any other advantage? 
On slide number oh-wait-you-dont-use-numbers you say something about "p&gt;q" being bad but that clause does not occur in the code.
&gt; I‚Äôll guide you through the design of the app and explain which places can be parallelised. In the end, we‚Äôll discuss what worked, and what were the issues and possible improvements. But not here, in the book, right? Or at least I don't see it. Seems to me the most reasonable point to parallelize on is on files - process multiple files in parallel each one in a sequential manner. Sure you could load a file entirely into memory, divide it into even subjects, then adjust the boundaries to land on line endings, then parallelize on the processing of each chunk... but that seems both more error-prone and also quite possibly less of a win in performance.
My biggest issue with people who complain about the direction of modern C++ is the implication that they'll somehow be judged less of a developer if they don't use the new features. No one is breaking existing C++ or fining you for not using the latest features. Also, complaining about the debug performance and compile time of brand new features? Seriously, give the compiler makers a couple of years to work on improvements. 
Thought #1: C++ is starting to look like Fortran in its desire to be ever more modern while retaining compatibility with archaic features..... Thought #2: I think I'm no beginner anymore (I've written objects that you can iterate over) but some of the code in your posting is indeed horrendous. People who understand that stuff are scary to me.
Complain about compile times while this is in the code: ```#include &lt;range/v3/all.hpp&gt;``` Seriously, include what you need, and report back. 
This. Because of the painstaking way that the standards committees have worked to keep backwards compatibility, it's still feasible to work the way you feel is best, which I think is a strength of the language rather than a hindrance.
I‚Äôm in a similar line of work as the author, and I must say the game dev twittersphere is all up in arms about this massive groupthink about all sorts of nonsense. I would feel better about this sort of blog post if the Unity product wasn‚Äôt absolutely terrible with about the least data oriented design available (Unreal your object graph is no better). The problem with all these rants is I don‚Äôt think they are particularly effective. I‚Äôm sure if I thought for a bit I would be able to come up with plenty of cases where ranges offer better composability in a less contrived example. Is it always the first thing I would reach for? No, but I‚Äôm glad it‚Äôs going to be there for when it makes sense. Now the part about compile times which is also the subject of another recent post from the same author... This is absolutely not the right conclusion to draw. Compile times are important, yes, but writing worse code to improve compile times seems incredibly backwards. We have things like the pimpl pattern and such for a reason. Furthermore, for those cases where we operate in massive codebases, we have other tricks up our sleeves, like defining good module boundaries and library sandboxes for testing or iterating on individual components. If the Tim Sweeneys, John Carmacks, etc read this, please stop fueling this unproductive and heavily biased ranty holy war. I guarantee if you weighed in earlier to the committee in literally any capacity, at least a more productive dialogue may have ensued. In my case, I‚Äôve voiced concerns to the committee before over a number of things, but within this industry I‚Äôm just one person. As for the people on the committee, I think you are better served by making the process and yourselves more approachable. Understand that for us in the trenches, it‚Äôs really tough to stomach the rigamarole needed to provide feedback on things. I remember being pretty discouraged when trying to dissuade the committee‚Äôs efforts to pursue a 2D drawing api, and I shudder to think of my industry‚Äôs backlash had it come to fruition. The C++ community is ill and there is a large degree of separation between some of its hardcore users and its creators. The way things are, it‚Äôs tough to see things improving much, and I wish I could be more optimistic but in the worst case scenario, in the eyes of the games industry, we get an unusable form of reflection without user defined attributes, we get a networking api that isn‚Äôt performant enough to work with, a bunch of features we don‚Äôt care about because the STL is still abysmally slow (containers and IO mainly), and get alienated even further. This results in everyone being even less inclined to be cooperative on the next cycle. I know with better direction, we could be C++‚Äôs staunchest allies and supporters; I was in this camp as despite it‚Äôs flaws, it was the workhorse that could get the job done. But increasingly, my support of C++ has become more of an apologia and I am definitely looking past C++ more opportunistically.
Excellent blog post. The issue of compile times has been on my mind recently, and as someone actively working to learn more "modern" C++ structures and concepts, the issue of increasing complexity as well. Something occurred to me, though. As stated in the article, many larger companies work on "roll-your-own" implementations of various parts of the STL. Since C++ is so widely used, and consequently the members of the standards committees so diverse, it seems to me that the current STL standard is for the "general case", which due to the above factors has grown broader and broader over the many years that the language has been in use. This almost places a need for non-trivial codebases to "specialize" parts of the STL to suit them. I don't think this is a good thing, necessarily, but a consequence of the general use-case of C++. It's certainly a way for user of the language to become more familiar with the concepts underlying the STL data structures, but on a sufficiently large-scale codebase it may become unmanageable to rewrite every single container or algorithm needed for your specialist case. Though, on the other hand, that is exactly what things like EASTL and I'm sure parts of Unity have already.
&gt;My biggest issue with people who complain about the direction of modern C++ is the implication that they'll somehow be judged less of a developer if they don't use the new features. No, it's not about being "judged less of a developer". It's more about junior programmers instilling "modern" features and bringing about death by a thousand paper cuts. We the "backwards devs" would be completely fine with the new features if they could be explicitly and completely disabled, but that's a politically undesirable option to have, as it would slow down adoption. &gt;Seriously, give the compiler makers a couple of years to work on improvements. Seriously, we can't. We have software to ship. :)
well, From what I recall, it was MSVC that added that pragma, and GCC and Clang have added support. https://en.wikipedia.org/wiki/Pragma_once notes that compilation is usually faster, as when scene it can just skip the file; whereas header guards, the whole file still needs to be read. but there are a few down sides to the pragma `#pragma` directives by definition are non-standard. There is *no* guarantee for them to exist, and compiler venders are free to do whatever the fancy; they can even have `once` be something completely different. Even if they are implement to do what you think, they may not protect against multiple definition errors; header guards do, always. header guards, will work on all compilers; you just have to ensure you do not have a name clash; and do not start with any underscores (though common, those are reserved for the compiler). I would really like to see pragma once standardised, but I guess that's the point of modules; which I am very much looking forward to seeing in the language.
&gt;Doesn't matter for smaller projects, but for big ones at some point your productivity may tank due to compilation times. Sure, and the solution is to not use certain constructs if you are in the tiny minority of people who work on such large projects. Or use incremental builds. Better to make more/most people happy than make everyone unhappy..
STL is 20 years old and still debug performance and compile times are bad. Maybe there is some truth in the post that there should be more 1st class features, instead massive amount of templated code in headers.
Clarified my comment, plz no downboat
While it can be argued that the author could‚Äôve included less, the problem of massive compile times cannot simply be dismissed like this. Include a `vector` here and an `unordered_map`, maybe throw in some `functional` and `iostream`, mix evenly with a couple handfuls of translation units, and leave to bake at 350C for 30 minutes. The author doesn‚Äôt describe a Ranges problem, nor a C 20 problem, not a compiler problem; this entire issue is intrinsic to the model of the language itself, and you can‚Äôt just brush it aside by saying ‚Äúoh he included too many things‚Äù
Excellent post. I stopped using c++ after trying to do gamedev in UE4. Compile times alone killed any productivity. I'm glad you included the C# version of the code, because it shows you can have readability, fast compile times, and fast run-time performance. I'm using Xenko now, which is a full c# game engine. Writing script code is a breeze; compiles take about 5 seconds and I can run the game and test changes. It's so funny how awful c++ dev is in UE4. You have to use these weird ass macros everywhere as well as a bunch of header includes. Compile times skyrocket, and if you miss a header somewhere, the compiler takes a big shit, of which you need to dive into to figure out what is missing.
While I was really anxious to finally move to C++11 when I was stuck on 98. I am now more and more anxious that I will have to move to C++17 and C++20 eventually. 
I beg to differ. I've spent a not insignificant amount of time optimising compile times and it almost always come down to one or more of: - Including more than one needs (and cousins such as gigantic cpp files that need too much, lack of forward decl, etc) - User error such as instantiating a quadratic number of templates - Genuine compiler or library bugs, or lack of optimisations, which are fixed eventually. Also, for what it's worth, this problem is almost aways due to application level code, not widely used library code. At least 3 times I've worked in a code base that had bad compile times that we're drastically reduced when we were able to replace their home grown template library with the standard c++ library. Ironically, one argument for rolling their own was always compile times, which may have been valid at one point, but as the internal library grew, and std was optimised, the problem reversed itself. 
thanks for the feedback! the code is available under this link: https://www.cppindetail.com/data/cpp17indetail.zip The description of the project is, however, "hidden" in the book. the chapter uses a simpler approach: process all files serial and try to parallelise the "computation/parsing" steps. I've decided that such approach is good to explain issues with parallel algorithms. There's also a discussion about possible improvements.
I think a big motivation for all the new C++ features is staying competitive among beginners. I.e. have the STL loaded with enough to get people started on interesting projects, so that C++ is more approachable. That way people begin with C++ earlier, and have a longer time to ramp up. Teachability and approachability are important components of any language. &amp;#x200B; Being all things to all people is definitely the issue at hand.
That's related to the values of p and q, so if the pointer value in p is larger than the pointer value in q, bad things happen, since the loop will never terminate (OK, it might, if p overflows). It's just pointing out that the code there isn't something you should really write in a real-world scenario, since it doesn't do any error handling. I should re-write that though, you're right, it can be interpreted that way. Thanks for catching the problem with the slide numbers! I thought I had those enabled, but I must have disabled them by mistake. I'll fix that!
UE4 is particularly bad though. 
I really feel OP, expecially the part where he claim that one of the biggest issue is that features should be added to the language. Libraries are not the correct way to evolve a programming language environment IMHO. 
Well, I did the test and.....it didn't make a huge difference. ``` #include &lt;time.h&gt; #include &lt;stdio.h&gt; #if INCLUDE_ALL # include &lt;range/v3/all.hpp&gt; #else # include &lt;range/v3/range_for.hpp&gt; # include &lt;range/v3/view/for_each.hpp&gt; # include &lt;range/v3/view/take.hpp&gt; # include &lt;range/v3/view/iota.hpp&gt; #endif using namespace ranges; int main() { clock_t t0 = clock(); auto triples = view::for_each(view::ints(1), [](int z) { return view::for_each(view::ints(1, z + 1), [=](int x) { return view::for_each(view::ints(x, z + 1), [=](int y) { return yield_if(x * x + y * y == z * z, std::make_tuple(x, y, z)); }); }); }); RANGES_FOR(auto triple, triples | view::take(100)) { printf("(%i,%i,%i)\n", std::get&lt;0&gt;(triple), std::get&lt;1&gt;(triple), std::get&lt;2&gt;(triple)); } clock_t t1 = clock(); printf("%ims\n", (int)(t1-t0)*1000/CLOCKS_PER_SEC); return 0; } ``` On my machine, 6 seconds to 4.5/
Wholeheartedly agree: * it's painful to see how readable the C# code is. I mean, even replacing `iota` by `enumerate` would be a considerable win. I've even used `std::iota` in the past but when reading it I still need to go one mental stack level deeper to keep in mind what it does. Most of my colleagues never saw `std::iota` so they would need to google it. * compile times are a BIG issue. Our code-base takes 30 mins to an hour to compile. This has a big impact on everyday development, not in the least because we're doing cross-platform development and it takes hours before a dev gets feedback from the CI system that they broke the build on a different OS. 
[removed]
Anxious means two things. I would indeed be anxious to move to 11 (or 14) because it's so much better than 98, but I'd get anxious about moving to 20 because it means I'd have to remember library calls, rather than acquiring skills.
Like you, I‚Äôve spent what I can only describe as too much time trying to cut down my compile times. In my case there‚Äôs limited opportunities to forward declare things, and I have already reduced the includes, as far as I can tell, to the minimum. What are your suggestions in these sorts of cases?
Thanks for the explanation. Of course that loop is bad in another sense: you should never compare floating point numbers, what with computer arithmetic and such.
&gt; It's more about junior programmers instilling "modern" features and bringing about death by a thousand paper cuts It's perfectly reasonable and easy to mandate a particular standard level of C++ in a given programming shop. Shit, I'm sure there are places out there that are still refusing to use any STL and have their own bullshit version of string and vector. &gt; Seriously, we can't. We have software to ship. :) I meant "wait for the improvements before you decide to adopt new features", not "wait to write your software". 
Which is the [point](https://clang.llvm.org/docs/Modules.html#problems-with-the-current-model) of modules, right? 
Well, it's almost always some sort of trade off. Either through indirection, or code maintenance. For example, at the extreme end you might use pimpl and the split the cpp file into multiple smaller ones, which is both. It's also possible to put template implementations into separate files. For example, a class might declare several functions that drag in dependencies that aren't needed for other bits of the class, so you put those functions into _impl.hpp files and have the caller include them only when necessary. This puts some onus on the user to include things properly, but lets them optimise their compile times. Lot's of techniques, but it's all work. 
Meh, once you encounter `iota` for the first time, you just look it up and it sticks in your memory forever, kinda like what happens when you see a variable named `gjkjsdklfjlkg`.
C++11 was an amazing update. 14 and 17 were minor releases, but 20 is shaping up to be pretty amazing. 
Everyone wants faster compile times, no matter how fast the compile times are, up to the point where you can be working on a 1000 file project and be able to hit run at any point and not notice the compile time. I basically put no stock in complaints about compile times for a number of reasons... * You should be making your code more modular. If you're tweaking and compiling and tweaking and compiling over and over again and you're annoyed by compile times, then you need to modify the problem so that you're compiling less shit in order to accomplish your current task. I work on a large project myself and have to work with both Android builds and Windows builds. If I'm trying to debug some obscure problem with one small component (UI, audio, etc), you can bet your ass I'm not going to do it as part of the whole project, but rather as a smaller custom test project that includes the minimum I need to reproduce said problem. * Architecture is (and has been for a while) going to more cores rather than faster cores. Every time the number of cores in my dev box doubles, my compile time drops by a similar factor. * Background compiling &amp; network build load sharing. Build systems are smarter and I only expect that trend to continue. With tools like incredibuild and build environments that know that if a file has changed and I'm not actively editing it, that it should probably be eagerly compiled, the actual delay between saving something and hitting run should only ever by the cost of compiling the last thing I touched. 
Libraries could be an excellent way to evolve a programming language, if only you weren't compiling the entire library in every translation unit again and again. The problem here is not the new library facilities, it is the ridiculous compilation model. 
So... interestingly, growing the language is not without issues either: - Stroustrup: [Remember the VASA (PDF)](http://www.stroustrup.com/P0977-remember-the-vasa.pdf). - Hoare (Rust original author): [Rust 2019 and beyond: limits to (some) growth](https://graydon2.dreamwidth.org/263429.html). Both Stroupstrup and Hoare point the same thing, though I prefer Graydon's emphasis on the issue of limiting growth to control it in this case. I'll quote the TL;DR, but encourage you to read the whole: &gt; **TL;DR:** &gt; &gt; I'm writing to make a recommendation about paying attention to -- acknowledging, planning, making explicit mechanisms and policy around the limits of -- the growth of two things in the project: &gt; &gt; 1. Necessarily-shared technical artifacts, specifically the language definition itself. &gt; &gt; 2. The strains on people participating in conversations about those artifacts. &gt; &gt; In particular, I want to draw attention (as others have done, with other words) to the fact that unlimited growth in both of these things is neither possible nor desirable, that there are natural limits, and that as with all natural systems reaching their growth limits, it will be better for everyone if the encounter with the limits is made in a controlled, planned fashion. The problem is that any language feature interacts with *all other language features*, so that any added feature will forever (or until excised) add effort overhead to adding any other feature. In the past, we've seen collisions already. The most famous being the interaction of Uniform Initialization Syntax and Initializer Lists, which results in Uniform Initialization not being applicable to all cases... and in adding a constructor with a single `initializer_list` argument being a breaking change. --- So, I agree, I'd really favor `std::variant` being obsolete with proper support for sum types as visitation is just too limited (and code generation suffers) for example; however, there are very good arguments for trying to keep the core language as lightweight as possible given the added burden each feature adds *for perpetuity*. In the end, it's all a big balancing act; and it's to foresee which is best.
What new features do you see as appealing to beginners? 
I'll link to my thoughts on language vs library [which are in another comment](https://www.reddit.com/r/cpp/comments/aabthn/modern_c_lamentations/ecqznod). Another issue of layers upon layers of generic code, beyond compile times, is debugging. Each time I try to step into a function call taking a dereferenced `unique_ptr` as argument, I despair. It'll take a dozen steps for dereferencing that `unique_ptr` as it goes inside `unique_ptr` internals, which somehow also go into `tuple` internals (used for the EBO for the deleter, I guess). **A dozen steps to get into a function call!** Now, we could argue the debugger should be smarter, however there's a fine line to tread there. If the debugger hides too much, such as skipping `std` code, then it doesn't make life easier. For example, it's natural to want to step into `std::remove_if` when the result is not to taste. Hopefully, for this particular case, the upcoming attribute to allow 0-size data member should allow cleaning up the clunky implementation of `unique_ptr`; unfortunately, it won't help much with all the range (or `std::function`) machinery.
Well, the motivation came from JavaScript, not real-time systems :) 
Is that a time-based debounce / redraw limiter I see? _Thank you._ I've seen too many console progress bars that ignore the IO overhead or the fact that beyond 30-60 redraws a second the human [won't be able to perceive the difference anyways](https://paulbakaus.com/tutorials/performance/the-illusion-of-motion/). Npm was [notorious](https://github.com/npm/npm/issues/11283) for having a lacking such a simple feature. And drawing on a loop modulo like `% 100` or `% 1000` is too naive when an iteration might take 15 picoseconds or 2 minutes. So again, thank you. 
People have been complaining about this stuff for a long time though. Complaints about boost and compile times are nothing new, and the existence of things like EASTL and such have been out there for a long time. Unity builds and other hacky workarounds run rampant. It's not that surprising that people feel like the wrong things are being focused on, is it?
There is actually an ansi escape sequence to clear a line: Esc[K [from here](http://ascii-table.com/ansi-escape-sequences.php)
Maybe since I learned about `std::iota` from the beginning when learning about the std algorithms and such but I'm finding the arguments against it a bit silly? Idk. It's a one time thing to know and understand about unlike member function pointer syntax which took many.
Filesystem and networking TS's, concepts, type inference (though debatable), and the declarative syntax of ranges. In general, higher abstractions and less verbosity. The example in Eric Niebler's original ranges post is purposefully more in depth than a beginner would need, but something like view::filter is going to be more to-the-point than manually looping through iterators and mutating a container defined outside the scope of the loop, then iterating through that container. Of course, there's always the conflicting viewpoints between teaching beginners the "fundamentals" vs getting them started with something interesting.
The application I work on takes about 10 minutes to compile fresh right now, and that seems too long to me. It is using cmake + parallel builds and we've cleaned up a lot of header issues but it is still a real PITA when iterating on a piece of code.
And re-linking every single symbol in your entire project.
No. Modules don't affect template instantiation.
I don‚Äôt disagree entirely, as I have plenty of gripes with the status quo, and it‚Äôs been this way for me since 2002 when I first learned the language. If I had my own way, I think the biggest things that need to change are the process by which features get both phased in and phased out of the language. I would also firmly intertwine the interests of build tools with the language itself, which is currently just a language standard in the purest sense. With respect to the STL, I personally ignore most of it‚Äôs developments except as a study in API design, and I worry the only reason it gets so much attention is because core language improvements are just too hard. All that said, there hasn‚Äôt been zero effort to improve things either. I just don‚Äôt know that it‚Äôs nearly sufficient. 
Wait, so you need high quality implementations of the new features before you can ship, but it's only the juniors who use those features? And you're completely certain there's no way to prevent this situation?
5 minute link times are 'fun'.
I dunno, 12.5% difference seems pretty significant
`reserve` of `std::unordered_map` doesn't allocate contiguous memory?
It's not really doing that though, both p and q are pointers, therefore while(p != q) is actually just checking if p (a pointer) has reached the last element: q (also a pointer). And yes, this example is supposed to be a bit cryptic, so its OK to be a bit confused :-). This is based on an example from Andrew Sutton's talk, which was equally as cryptic too. The idea here is to present an example that forces you to think about the requirements of the function by themselves, and so you don't get any preconceptions about what it could do it needs to be a bit cryptic.
Thanks for the link! Yeah, apparently I was incorrect about how LLVM is implementing the coroutines. I'm curious why it allocates the coroutine frame on the heap rather than the stack. In fact, I'm surprised it's implemented on the LLVM IR level at all! Perhaps there's something I'm missing, but all non-recursive coroutines should be able to be compiled through existing semantics similar to [this](https://gcc.godbolt.org/z/gQf4sZ). Note that it optimizes the entire thing away.
It does not really matter what language you use, that is not the behavior you expect from a repeating timer.
Hmmm I usually just let exceptions unwind to the top and terminate since I don't work with high availability. I always assumed smart pointers would handle exceptions nicely for me. So is the moral of the story to always ensure destructors of smart pointers pointees don't throw? (Or at least the deleter passed to the smart pointer constructor).
The first problem is solved with underrated MSVC's "step into specific". Personally I almost never use normal "Step into" anymore since it also gives great perspective as to what actually happens in debugged line. Microsoft solved 'std::function' problem too, however their solution seems to affect performance in debug build a bit yet. Generally I strongly believe that debuggers could and should get better.
https://godbolt.org/z/EiGVwr Switching to pass by value seemed to improve things for both versions, and I also simplified the manual version.
I've written a toy engine at home - maybe a couple of man weeks of work total, and my compile times are already measured in minutes. This is code that I'd regard as pretty standard, perhaps even light on STL and templates. My point being that it's incredibly easy to ruin your compile times, even as a solo dev that knows which bits of the STL are necessary.
It's still ridiculous, are you kidding? It's 4.5 seconds for 10 lines of work. That's completely out-of-whack.
There's a difference, and you know it. None of this stuff is a problem in modern languages like C#, Rust or even D. Compile times are a uniquely C++ problem. Even C requires some pretty beastly codebases to even come close to matching this 10-line example.
_Instantiating_ a template is a far cry from _parsing_ 100k lines of header code in order to get the template definition in the first place. 
vector and string are poor examples. There's a lot of optimizations you can make with custom vectors or strings that aren't possible with the STL versions.
Yes, and, distinct from ranges and other such features, C++ is working on the problem of massive amounts of parsing work with the concept of modules to replace the rather archaic include mechanism.
Totally agree on everything. - compilation time is a plague that has been ignored in 20 years now - STL is unfriendly and over complicated. I stopped using it after finding the "imbue" function name ... - template syntax is a crazy and unreadable mess - we wrap almost every stl class/function with usable and user friendly wrappers to be used - if one of the developer ever writes code like that is getting rejected as unreadable and unmaintable Instead of adding new things please solve these issues! At this point I don't even mind of source compatibility, just fix it, the cost would be less than keeping the actual limitations ... 
At the cost of having to maintain your own version, and the cost of not being able to hire someone from outside your organization and have them start working with your custom classes and already known the ins and outs of them. People always go to "optimization" like runtime performance is the only metric that's important. In the long run, computers will always get faster, but those maintenance costs never go away. 
Aras and myself work in games. Runtime performance is everything to us, because our platforms don't get faster every year. And seriously, if you don't care about runtime, why are you using C++? It's the only thing it has going for it.
Yeah, and I get that what feels obvious may not be obvious to the people on the committee. In the end, hopefully some of the grumbling helps prevent the worst case scenario you described in your first post.
Kinda useless gamer rant, range-v3 is slow, there's concept emulation and what not, using it for comparison is a waste of time, it's like comparing pre-C++11 stuff living in Boost to what got into standard and compilers, speed will vary widely thereafter.
On the LINQ stuff: http://scottchamberlin.tumblr.com/post/55152416452/linqinterview
Yeah, throwing from destructors is bad in general. C++11 makes destructors implicitly noexcept too, which effectively means you terminate on throw. On a similar note, functions like close or free should never fail, because how can you clean up from a failed clean up (for example, fclose can return an error, but always results in a closed stream).
It does for the buckets in the hash but due to the chaining nature of the hash table individual elements are allocated using a node structure. There are some very fast alternatives like Robin hood hashtables but they can't be used because they would have a different API or different iterator invalidation rules, which is unfortunate because unordered_map is usually overkill.
Slightly off topic but might be worth watching [this](https://www.youtube.com/watch?v=fX2W3nNjJIo&amp;vl=en) cppcon video where Bjarne presents some of his thoughts on how to effectively teach C++.
I always wondering why c++ has to take the heavy burden of back compatible. The old library can use the old compiler, isn't it? 
Rust actually had pretty terrible compile times last time I checked it out.
It's very clear to me that the users of such simple libraries will be perfectly satisfied with the (X + time to execute + random OS syscall overhead + other resource contention overhead + etc) behavior. YMMV!
The Timer API itself is quite useful for some tasks. Something similar is part of the Java standard library as well. Just because the library is simple does not mean it won't be used for advanced tasks. Of course there will be a lot of use cases where the timing is not that important, but definitely not all use cases where a Timer class would be a neat solution.
Because that's one of its selling points: your investment in code does not become worthless because you will never get stuck with an end of the line compiler on an obsolete platform. 
I wish C++ had chosen not to do that, but ok, too late now I suppose. For the record, you can suppress this yourself if you don't like it using \`extern template ...\`. It's a pain to maintain though... 
Can we please stop the tired meme that people who don't give a flying fuck about saving a nanosecond here or there are somehow not entitled to use the language? We still get to enjoy good performance even for less performance-sensitive parts of our software, as well as use the excellent tools, the rich ecosystem, and the joy of not having to deal with two implementation languages (one for performance-sensitive parts and one for parts with less stringent performance needs).
A `for` loop over a temporary probably isn't going to work out as well as you'd hoped. ;-]
Oh right, you were comparing the pointers, not the pointed values.
I've long argued that we ought to be able to just call sort on a vector without needing to use vec.begin() and end(). I'm excited to see this finally happen. I think that 14 and 17 didn't add much for new programmers, but 20 looks like a really nice boost for them. 
The issue here is that modules STILL don't solve this if your header (or in this case "header hidden as a module") needs to be different based on user-defines, clang made a good example with assert.h, or any of the platform dependent modules, or the numerous treat the file different in other files to keep code organized model clang uses (good example [here](https://github.com/llvm-mirror/clang/blob/master/lib/Basic/Builtins.cpp#L21)), modules don't solve this with the way users think it might, another good example of this is windows.h, where everyone pretty much does: #define NOMINMAX #define WIN32_LEAN_AND_MEAN (or any one of: NOGDICAPMASKS NOVIRTUALKEYCODES NOWINMESSAGES NOWINSTYLES NOSYSMETRICS NOMENUS NOICONS NOKEYSTATES NOSYSCOMMANDS NORASTEROPS NOSHOWWINDOW OEMRESOURCE NOATOM NOCLIPBOARD NOCOLOR NOCTLMGR NODRAWTEXT NOGDI NOKERNEL NOUSER NONLS NOMB NOMEMMGR NOMETAFILE NOMINMAX NOMSG NOOPENFILE NOSCROLL NOSERVICE NOSOUND NOTEXTMETRIC NOWH NOWINOFFSETS NOCOMM NOKANJI NOHELP NOPROFILER NODEFERWINDOWPOS NOMCX ) just to be complete about this list (grabbed directly from my windows.h) This problem does not get solved from modules, it only gets worse. (Though arguing with windows.h is bad, yes, assert.h is clang's poster boy for while the thought-process doesn't always work).
Also I can't believe I forgot about modules! Although, like I said I expect programmers to understand #include from C when they start C++, but it certainly doesn't hurt. Maybe modules combined with concepts will make the ecosystem a lot friendlier.
It‚Äôs getting better with incremental compilation and lots of work on the LLVM side, but itis still not great indeed.
I agree. Its time to deprecate things. Im cool with an incompatible split. And if we can get rid of,the preprocessor, even,betterer. 
Writing networking code in C++ is far from being beginner friendly
I'm not kidding - 25% is a significant difference. I said nothing about whether the remaining time was acceptable. Are you kidding? You could make your point even stronger by removing 9 lines - omg 4.5 seconds for 1 line of work. Or remove all the lines, omg 4.5 seconds to compile nothing but some includes, ranges must be teh suck.
I cry every time I go to include a library in a project and see that their CMakeLists.txt is using `include_directories` and `link_libraries` instead of `target_include_directories` and `target_link_libraries`. And don't stick your include directories in a variable that I then have to manually add to my projects includes! Do it the proper way and CMake will handle it all automagically! It would be fantastic if there were better resources available to try and show people how to write a GOOD CMakeLists.txt and avoid all the pitfalls. So many libraries seem to kind of wing it, and end up being a pain to use in a project.
You still have to deserialize the module file. Don't expect more than an order of magnitude speedup over re-parsing.
C++098
After we have modules. I agree that all that text manipulation stuff with the cpp is evil, but handling header files is useful and can not be done in another way.
Honestly, I'm not really sure, we encountered this issue and talked about it but I didn't get any clarification on the standardese, nor have we completed the implementation and moved towards getting it into libc++. Once that happens I'll learn more.
&gt; but writing worse code to improve compile times seems incredibly backwards By what metric is the "simple" code worse?
I read Aras‚Äô blog post and I wholeheartedly agree. For me, the only priority I see is a standardized build system, modularity and package management. ABI platform stability would also be nice to add. I‚Äôm even surprised that these are not even a priority to the committee! These are actual pain points in the ecosystem. Not every functionality needs to be in the standard.
Not exactly the argument I am trying to make. The example in question is really contrived, and one can always come up with a poor enough example that makes the costs seem clearly not worth the cost. My argument is that if compile times are important, we should be attacking the problem directly (eg modules) and consider this as an orthogonal space. 
But const auto&amp; results in errors when you try to pass it to functions requiring mutable references! Just use the thing that always works (tm) and never use const anywhere.
What do you consider to be unusable or non user friendly about the standard library that you had to wrap? Can you give me some concrete examples?
I realised last week that I don't take real coffee breaks any more, I just make it while I'm linking....
I was browsing through [P1362R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1362r0.pdf) in which Gor Nishanov strongly advocates merging coroutines support into C++20. I really do hope that this will succeed! But I guess we still have to wait two months for the resolution from the meeting in Kona! 
&gt;...had people in the room who saw the abstraction who said ‚Äúthis is exactly useful to things we have done in the past, I want to see this‚Äù. Cherish those beautiful moments. It doesn't always happen that you can put an abstract idea in front of people and they see the idea, not only for what it can do for their specific field of study, but also for others. &amp;#x200B; There are a few items that dig at me when it comes to standardization. 1) Is it incremental? Or put another way, is the change being made/proposed building up the standard? Or is this a means-to-an-end for some paid-participant's sponsor? 2) Too often I see "bike shedding" going on. I see a lot of API's that ignore what is available in C++ standard, the accumulation of countless person-hours of effort. And the API's I've seen are not about to go back to re-write code to use the latest C++ standard. Case in point, look at the libraries that you use, how many implement their own string construct? 3) Adoption Rate. Testing out distributions this past week and I went to try out a bit of C++17 filesystem code only to get an error message back saying the filesystem header couldn't be found. It was then I discovered that LLVM and GCC compilers had not been updated. A rolling linux release punches out latest but the distributions I was looking at were still stuck on LLVM 6 and GCC 7. Hell! Unless you use LTS, we get a new linux kernel just about every couple of months. And distributions are hesitant to pull in an updated compiler? &amp;#x200B; This a good article about "behind the scenes" interactions. I recognize a couple of things that I personally have tripped over. \\o/ well done! &amp;#x200B;
The C# example is unfair. 1. System.Linq is extremely slow, and causes many unnecessary memory allocations. For example, \`Enumerable.Reverse\`, which allocate an internal buffer, copies the entire sequence into the buffer, yield return them; 2. You could not call \`Enumerable.Sum\` on arbitrary types, because CLR, or Roslyn, doesn't support advanced compile time constraint. Concepts, which is called Shapes in the C# world, has not been voted into C#. Also, I believe as compilers support concepts &amp; modules, it would be better.
Optimize away the double negative lol I had to read 'not insignificant' twice before I was sure I properly understood. Just a joke don't hit me!!
 std::vector v = {1, 2, 3, 4}; std::thread([&amp;]{ f(v); }).detach(); When can I safely destroy `v`? Since I can't join the thread, the only way to know is to rely on some external flag that is set by `f()` when it's finished, or at least when it's done playing with `v`. I then need a polling loop to wait on it. That's in fact exactly what `join()` does, but more efficiently and less error-prone. If the thread doesn't have any sort of signal that it's done, then it's impossible to destroy `v` safely. Detached threads typically cannot reliably have inputs, which makes them useless except for the most basic uses. In the timer library we're discussing, the thread uses a member variable, but the lifetime of the object is not tied to the thread in any way: void Timer::setInterval(auto function, int interval) { this-&gt;clear = false; std::thread t([=] { while(true) { if (this-&gt;clear) return; std::this_thread::sleep_for( std::chrono::milliseconds(interval)); if (this-&gt;clear) return; function(); } }); t.detach(); } This object is impossible to destroy safely. You can set `clear`, but you don't know when the thread is going to read it. If you destroy the object, the thread may be accessing a dead object later on. This can only kinda work with a global `Timer` object and rely on the system to cleanup after you, which probably the intended use.
Have you tried split dwarf/debug fission? My project had pretty large link times for Debug builds due to massive amounts of debug info. Switching on split dwarf reduced our link times by ~88%. It does come with its own downsides too, so weigh the pros and cons.
Hey! Random question on a old post, but I was curious: How many developers are working on MSVC's STL?
The spam filter caught your post (no idea why, you aren't shadowbanned and there were no URL shorteners that annoy it). I've manually approved it.
Spam filter caught your post (don't know why - you aren't shadowbanned and there is nothing weird about that URL). Manually approved.
C++ was specifically designed to allow libraries to extend the language as no other language does. I believe this is one of the reasons for it's long lifespan. Libraries such as boost.spirit, though maybe a bit esoteric, are examples of bending the language into a shape not possible with other languages. The cost of that is the compilation model. I do hope that some of that is mediated with modules, but I suspect we'll get a 2x speedup at most, which is not nothing, but not 10x either. 
This whole post reads like a Trump speech parody.
Could you elaborate on why you feel using `/* */` and `//` is not adequate? I want to emphasize that the bake project JSON files typically do not get very complicated- comparable with an NPM package.json file. The reason for this is that much of the "hard stuff", like rules to convert .cpp into .o, are implemented in a driver (a build plugin), and not in a project file. Drivers are implemented in C or C++. &amp;#x200B;
The number varies over time as the MSVC libraries team works on many projects. It's always at least 1 and often 2-3. That's not the total number of libs devs who have ever contributed STL features, so the "all hands on deck" number would be higher, but the STL has almost never needed such burst capacity and our other projects are important too. 
 template &lt;typename Container&gt; void Sort ( Container&amp; container ) { std::sort ( container.begin(), container.end() ); } This doesn't need to be in the standard library.
I hope to see source_location part of C++20. It will reduce the usage of macros even more. 
But it‚Äôs not necessarily the correct underlying type.
&gt; I promise you, it's incredibly high quality, incredibly clean code. But that‚Äôs my question, the point of having others review your work is so you have a check on your perception of the quality of your code.
I‚Äôm still confused why you‚Äôre using enums here? And what obsfucation from iterators? std::array&lt;int, 4&gt; arr = {1,2,3,4}; for (auto element : arr) { ... }
Because you're using a non-standard extension to a standardized file format. If folks want to parse your files then they have to support that same non-standard extensions.
This blog post is nearly as huge and impractical as the c++ standard.
&gt; C++ was specifically designed to allow libraries to extend the language as no other language does. *You suddenly hear the thundering of an angry stampede of neckbeards coming from the direction of /r/lisp*
&gt; And if we can get rid of,the preprocessor, even,betterer. No, the pre-processor is useful, do YOU like importing a million things in windows.h? Do YOU need platform specific code (linux, windows, macOS)? Because both of these things require the pre-processor, and there's no way you can get rid of that without making C++ platform-independent, which is not happening.
I think it might have been his appspot link, might have a filter on that, as the other link is definitely not spam-filtered.
This is all a matter of scope and life time, not a problem with detached threads. I'm not sure where you this idea that detached threads were bad just because you couldn't fire them off with a reference to something created with the same life time. &gt; Detached threads typically cannot reliably have inputs, which makes them useless except for the most basic uses. This is ridiculous. Communicating through lock free data structures that have lifetimes longer than the thread is just one obvious use. I'm not sure how who drove this idea in to your head, but you should have thought about it more. 
I'd be glad to discuss this with you, but _ad hominem_ isn't the way to go.
That's not what ad hominem means.
`decltype(auto)` sometimes acts as `auto`, for example, `for (decltype(auto) b : my_nonconst_vector_of_bool)`. But `auto&amp;&amp;` never acts as `auto`. &gt; why don't compilers treat auto as though it was written as auto&amp;&amp;? Essentially that idea was proposed and (rightly) shot down. See the tail end of the previous blog post: https://quuxplusone.github.io/blog/2018/12/15/autorefref-always-works/#for-extremely-detailed-informat
This comparison does even a worse job, it manages to compare a lib that's doing heavy use of concepts emulation, which is hard on build time, for "the good of science", not performance. Concepts is not even properly release by compilers, the lib is entirely on top of an emulator. This is nonsense.
Absolutely fantastic library
I'm using enums when the things in a vector or array are named things. The enums provide a self-documenting name for each thing in the list, and define the size of the list. The outside world can work in terms of those enums to access the slots using that self-documenting name. And, since my enums are IDL generated, I have value to name/text and name/text to value translations available so that I can log msgs where appropriate about accessing those slots and include the names of what was accessed. It's a very useful and practical way to do things, and I've long since worked out the mechanics of it.
If you are working in a group environment, then, sure. Take advantage of that. I'm not. But that doesn't mean my code isn't of extremely high quality. I mean, come on. I've been hard core beyond belief for 30 years. Do you really think I don't know the difference by now? I'm not working for someone else and trying to do as little as I can and still get paid. I live or die by the quality of my work, and I take it deadly serious. 
It's going to [work perfectly](https://stackoverflow.com/questions/30448182/is-it-safe-to-use-a-c11-range-based-for-loop-with-an-rvalue-range-init?rq=1). 
Python pulled it off, but c++ isn't python. An incompatible split would be a 20 year popcorn festival. C and c++ modules written from 50 years ago would continue to be used in conjunction with modern libraries, and the average c++ dev's job would become even more of a plumbing operation because the old libraries just won't go away even if breaking changes are made. People would find a way to jam the pipes together. I'm personally for it, and it is because I have seen the beauty that c++ could become, not because it would make my job easier. I realize that, and I am still willing to accept moving forward with breaking changes. 
&gt; If you want people who have concerns more aligned with your concerns about any feature or subset of the language, bring those people to the C++ Committee or ping people who you know are going to represent your concerns! &gt; &gt; [...] &gt; &gt; You must bring your representatives. You need to bring your companies, your code bases. [...] This is an absolutely fair point normally, but what bothers me is the lack of time to dedicate to something like this. Especially coming from the games industry. There's far too much red tape for this to be practical advice, as [this comment linked from the blog post points out](https://www.reddit.com/r/cpp/comments/a2o265/the_san_diego_2018_aftermath/eb38yor/?context=8&amp;depth=9) It's not as if people aren't coming forward with their grievances or contacting those in the know (see the aforementioned Twitter threads), but that the channels for getting anything **official** are too complex. As noted early in the blog post, someone has to sponsor, maintain, and push a technical paper, provide implementation for proposals, and/or attend standards meets; that's far too much time dedication for someone working well beyond 40 hours a week to maintain, and simultaneously too much work for a third-party sponsor to want to keep up with. It's logical, it makes sense, it keeps things well-defined and functional... It's impractical. What the solution should be is to be determined, but I believe getting a more inclusive feedback loop should be among the priorities. Anything more open, like repository issue lists or opt-in quick-discussion styled forums (as opposed to email groups), would work well as an inspiration. We're programmers, we can make better. And I'm saying this primarily to vent. The time I took to type this *could* have been dedicated to drafting a technical paper or test implementation, I admit; however, de-stressing is important for a healthy life and I'm piecemeal typing this between episodes of shows. (which also well-exemplifies the advantage of quick-discussion forums: low-cognitive, high-throughput)
&gt; In fact, in the case of Literal Suffixes for size_t and ptrdiff_t, the original author actually went through the entire Standardization Process, from initial paper to final wording in LWG. At the last minute, LWG shook their heads and say ‚Äúthis belongs as a language feature‚Äù, and sent them back to the start of the Standardization Pipeline at EWG. This is soul-crushing for motivation This was my doing, and I stand by it. The delay was undesirable, but the reset was necessary. The history was, nobody in L[E]WG noticed that doing this as a library feature was inconsistent with precedent: binary literals were also proposed as UDLs before I suggested making them a Core Language feature in C++14. I wasn‚Äôt at the initial reviews of the paper, and when it came to LWG and I was there, I pointed this out. And it‚Äôs not an arbitrary choice of following a mindless precedent - there is a major usability concern. Being a UDL would make it essentially unused (dragging in a header or module and adding a using-directive is too much activation energy). (As a library dev, I rarely say ‚Äúthis should be done in Core‚Äù, so the exceptions should carry some weight.) Yes, this took too many meetings to notice. We all should have noticed it earlier, including and especially me. But the right decision happened and the process eventually got the right result.
Not op but oh god do I hate auto itr = std::remove_if(vec.begin(), vec.end(), func); vec.erase(it); And everything else related to the `begin` and `end` functions. No other language I've used works this way, it's impossible to pipe operations in a straightforward way, and holy fuck is it verbose People tell me it's composable and generic but wow it that gross code to write
 &gt; Compile times are important, yes, but writing worse code to improve compile times seems incredibly backwards. It is entirely backwards; that‚Äôs the problem! Iteration speed is probably the single most important thing when it comes to creating a product. The faster you can iterate the more quality you‚Äôll achieve. C++ has garbage compile times. It‚Äôs a huge problems. Have to jump through a bunch of hoops and reach into a bag of tricks to ever so slightly reduce compile times is what‚Äôs truly backwards.
C++17 is a minor release just like `if constexpr` is a minor feature, until you save yourself from horrible C++11 template meta-programming nightmares with a couple of if constexpr's. 
You could do it with `if constexpr` 
I absolutely agree with the reset, and I certainly appreciate that someone took time to say it and put them on the right path, but my point is that it's still painful for the original author to do that work and then hit a setback like that. Things like this do happen, and it does serve as a motivation killer. Some of us (me and others) are still young and enthusiastic and will happily pick up a paper with a little prompting and get it back on the track, but my point was that given the Standards Process I can't really fault anyone for shrugging their shoulders and just saying "this isn't worth it for me". We try to make it so that doesn't happen, but... well. Every system has its cracks, and things do fall into them every now and again, and I felt like I should be honest enough to let people know the system isn't perfect.
The issues in this report still are valid as of today: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0158r0.html 
What‚Äôs the number of items at which you‚Äôd use an array / vector over simply a struct with named fields?
You can't: `if constexpr` will still check a branch to make sure it's valid C++, especially if it's not dependent on a template parameter or similar. `if constexpr(ON_WINDOWS) { DoWindowsThing(); } else { DoUnixThing(); }` will uncompromisingly fail to compile on either platform unless you're in a function template.
I think my hesitation is that generally in all these threads, when you mention a design decision that is deeply unpopular, like implementing your own replacement for STL functionality, you seem to double down on your decision, rather than switching to the more mainstream solution. There‚Äôs massive codebases that run all over the world, every day, in *spite* of poor architectural decisions inside of them. I‚Äôm not saying your code quality is necessarily not great, it could be excellent, but based on my experience senior devs when left in isolation will do things like re-roll their own standard libraries, which in the prescence of other devs would get shot down, but when you‚Äôre on your own is more appealing because everything follows your choice of idioms, even at the cost of time and bugs. I‚Äôm saying it‚Äôs worth it to consider trying to bring another dev up to speed on your codebase. If it‚Äôs excellent code, great. They‚Äôll be able to get up to speed quickly. But if it‚Äôs so conformant to your particular idioms that they have a difficult time, or if they strongly object to some major design decisions, then you‚Äôll grow as an engineer and your codebase will be better for it.
FYI (and this reinforces your point), your code snippet is incorrect. You need to say `vec.erase(itr, vec.end());` to trim the vector. Otherwise, you're erasing a single element which is bogus. The good news is that Uniform Container Erasure has been voted into C++20, so you will be able say `std::erase_if(vec, func);` and be done with it. (I designed and proposed this for Library Fundamentals v2, and Alisdair Meredith proposed merging it into C++20.)
Is it even possible to standarise on a build system and package manager?
 auto itr = std::remove_if(vec.begin(), vec.end(), func); vec.erase(itr, vec.end()); // ^^^^^^^^ FTFY
As far as concrete changes go, I would like to see a community effort around pre-reviewing papers before they're seen by WG21. We now have the "incubators" which is great, but that doesn't preclude pre-meeting work done in the open. (Groups like std-proposals aren't what I'm looking for; AFAICT basically zero useful things come out of that.) This subreddit is not an ideal place due to the volume of proposals, although maybe if we limited it to one a week, and only the first time a proposal is seen...
&gt; People tell me it's composable and generic Oh no! It‚Äôs definitely a known limitation of the design, which ranges are designed to solve very beautifully, but I hope we aren‚Äôt telling people that the ugliness of: std::sort(std::begin(container), std::end(container)); Instead of: std::sort(container); Was intended as the best API we could think of. It‚Äôs just that without concepts (which come along with C++20, enabling ranges), it was difficult to do.
Good point. Actually I was hoping that compile times would decrease in C++20, because Concepts seem to have the potential to be much faster than hacky template metaprogramming... If it doesn't, well that's a bummer.
In VS 2019, the debugger has a general ‚ÄúJust My Code‚Äù ability for C++, superseding the weird hack we used in `std::function` (now removed). We‚Äôll continue to improve debug codegen with `if constexpr` but that regression should be gone.
How so??
How true. This has happened in my code as well... I had some kind of mini animation system for a game that did some expression-template like thing to make an EDSL that looked something like this: //+ means combine in parallel and // &gt;&gt; sequence it. Each subtree will be animated for half of the time auto animationDesc = (moveTo(x, y) + rotate(180)) &gt;&gt; scale(0.7); auto animTask = makeAnimation(obj, animationDesc, 500ms); I think without if constexpr it would have been really really worse.
The first time I saw iota I said... ehat the hell is that name? I really think it should be changed. This is not a smart math people contest it is an API for people to get it as readable as possible.
Couldn't agree more. If C++ is a library language, it should be good at that. Any reasonable programmer should be able to understand what is going through. It's all good when you just have to use those libraries. But, when you are trying to solve a critical issue and it's already late at night and your debugger step in leads to a C++ library code....may god be with him. Having said that, I do like few or many of the language and smart pointer additions since C++11. The libraries (not all) on the other hand are just sucky.
It might be actually very slightly faster because there's no need to check that the declared type is the same as the one of the initializer.
&gt; Interesting that there is a significant difference Difference can be explained because the "curly brace" form is maybe lowered (rewritten) to the "equal form", which would imply a slight AST modification. &gt; I'm wondering what the compile time overhead for auto is It might be actually very slightly faster because there's no need to check that the declared type is the same as the one of the initializer.
Some people are actively hoping that a build system, and package manager are never standardized.
Huh, TIL. I thought `if constexpr`'s regular behavior was the same as its template behavior. Still, since we're talking about new standards, they could always change it I guess. 
Even rolling release distros don't update their compilers immediately. Gentoo is currently on 7.3 for their stable branch.
I think std-proposals is good as a literal soup of ideas, but that there needs to be another place for "I'm serious enough that I wrote a paper and I need pointed feedback about this". At present, while we have a Github for the C++ Working Draft, we don't have any kind of infrastructure for the papers themselves to facilitate pre-review before things get beat up at face-to-face's, unless the person explicitly requests review on the mailing list or it gets posted to some content aggregator (like here). I think a dedicated website with a comments section on papers would be a good start. Doing things like having the history of the papers easily accessible from one page, the current status of the paper (Last Seen: EWG, Jacksonville 2017 | Currently In: Coroutines TS / C++ Working Draft / etc.) would be helpful as well, because a lot of the posts on std-proposals are "is someone doing this thing?" or "did this work make it in yet?". Tagging papers for subjects and keywords ("allocators", "constexpr", "metaprogramming", "UDLs"), pinging authors about comments and putting up anonymized feedback, maybe even making it so there's a WG21-only section that is only accessible by participating standards members on each paper separate from the public-facing comments and documents to uphold the current privacy concerns... This is part of what I want to do Summer 2019, but that's a ways off and there's a lot of paper review that should be done before such a mystical, magical website comes about...
The fact that there are workarounds doesn't mean there isn't a problem. 
Unfortunately, the problem is not just parsing, but instantiating arbitrarily generic template code with tons of SFINAE to emulate something, that the language could provide easily in a native fashion. I'm really anxious to see compiletimes for ranges with language level concepts support.
Actually, TMP (sfinae) is extremely expensive. Modules will cut down on compile times significantly, but not to the point that you don't have to worry about templates code anymore.
No, it just means that every new feature becomes more complicated than it should be.
The moral is to not lie to the compiler.
c++ still does not have a decent straightforward string split hehe
&gt; Otherwise, you're erasing a single element which is bogus. And that element might not even exist :)
Arch Linux has [gcc](https://www.archlinux.org/packages/core/x86_64/gcc/) at 8.2.1 and [llvm](https://www.archlinux.org/packages/extra/x86_64/llvm/) 7.0.1
&gt; Thread per timer? Are you kidding? Came here to say this
Coroutines have a significant performance penalty and mix poorly with exceptions. They are not a free-for - all feature. People will get severely beaten until they learn when it is introduced to standard C++
&gt; Python pulled it off Barely, and arguably it hasn't finished yet.
It absolutely needs to be in the standard library
There already is a cleaned up version of C++ without backwards compatibility. It's called D and no one uses it.
Read my comment above addressing a similar counterargument. If the time to iterate with a slower to compile feature is still faster overall (even accounting for the compile time), then you should just write the better abstraction, and I would argue that any sufficiently bad example would mean we would never add any abstraction whatsoever in the name of compile times. Don't get me wrong, I hate long compile times and deal with it daily. I also love fast iteration speed. That's why I'm excited about modules and the proposal to add dynamic linkage for classes and functions.
&gt;It's called D and no one uses it. And one of the reasons is D2 that was incompatible with D1.
I don't know why this was downvoted. According to cppreference the single iterator version needs a dereferenceable iterator which means that you can't skip checking if `itr == vec.end()`. This is unlike the two iterator version which doesn't need to dereference `first` when it is equal to `last`.
That is exactly what I meant. 
Original author on `size_t` literals here. My experience on this has been a net positive. I got great feedback from Walter E. Brown, who championed the paper in several meetings. I also learnt about WG21 procedures and the great care and attention to detail required for something to be standardized. The LWG decision was correct and /u/STL was very friendly about in a private note. My reasons for transferring lead authorship to /u/ThePhantomDerpstorm were that I am a) not a professional programmer, didn‚Äôt have the stomach to write core language wording and c) have come to eschew `size_t` in most of my code.
It is not impossible, but I have became quite convinced it will never happen because the committee and the larger community do not actually want a package manager as they are unwilling to take the necessary path and instead focus on legacy codebases driven by a multitude of unmaintainable, over complex build systems.
Why would they do that?
That's not how it works, certainly Bjarne would welcome it. Just that no one actually has presented a workable solution
I'd like to use it but I can't find a concise doc telling me how to include it in my project. I have to compile my project also on Windows, so I would rather have a copy in-tree. Should I use a submodule? Also, I heard it is header only but there are some .cpp files under src/ Anyway, It's really a great library!
One GitHub issue per paper. Done.
Oh boi, I'm still slightly stuck with version 4
It should be. The standard library should make simple things simple.
Certainly Bjarne would also welcome a solution to world hunger, but that doesn't bring us any closer to a solution. There are too many obstacles towards dependency management at the moment, none of them technical.
30 minutes? Try eleven hours on a 256 CPU build farm. 
Hah, fair point, though I'm not sure it would be a stampede. 
Not forever, compilers don‚Äôt get maintained permanently. On the Committee are folks from Bloomberg and IBM. They have code bases that started out as C in the 1980s. They are never going to be rewritten from scratch, but are still maintained and incrementally improved, and maybe use C++14 in the same module as C89. We have the same thing where I work. If you drop backwards compatibility in a big way from the standard, I think you would end up with a fork of C++, purely off the back of the folks whose business demands are different from game developers or hobbyists. I don‚Äôt think that would be beneficial to the community.
That's all fine and dandy, but it doesn't scale very well. It's better when the compiler manages to warn there's a mistake :(
1....1....hours..... how many gigalines is it?
I think it's more important to standardize the *files*/*formats* for describing a build/package. It's not necessary to standardize an implementation, and I don't think it'd be a good thing either, it's very useful to standardize the *language* in which those things are expressed for interoperability (and IDE support).
Is standardizing the ABI really necessary? As a developer (consumer of libraries), I'd rather compile the sources myself to be honest. This way I can: - audit the code: manually, or with tools. - customize the code generation options: tuned for specific architectures, platforms, etc... - harden the generated code. - instrument the generated code to detect bugs: sanitizers. And I get a better debugging experience to boot. With a standardized way to describing how to consume a package and build its libraries I can throw *my* tools at it, with a setup customized for *my* needs. At this point, ABI compatibility with old libraries is of no concern; I will just re-compile them anyway.
&gt; really ‚Äúwhy is there a default argument on substr?‚Äù It may eliminate a branch in your code for 0 length strings, thus making it more robust (in theory). I think it's a bad deisgn but there are times when I'm glad to be able to remove a branch (like someone is giving me shit about formal verification and cyclomatic complexity).
I understand the dilemma, but at the end of the day to my ears is sounds like if the game industry can't send people to participate in the C++ standardisation process, then maybe they don't really care that much after all?
I guess there would be times where I would like to debug the library I‚Äôm consuming. But generally I‚Äôd personally rather not compile the sources myself, and for different compilers, or for the same compiler but with different flags. I‚Äôm not a library author per se but I can imagine having to wrap everything with a C api just to achieve abi compatibility is a PITA. That and the horribleness that‚Äôs __declspec(dllexport). Or not being able to pass stl objects because of abi incompatibility is just frustrating. Also a stable abi and a standardized build system would make package management a breeze. 
I'd love the answers to those questions too
[Is this concise enough?](http://fmtlib.net/latest/usage.html) Otherwise you could always use the vcpkg or conan packages, I guess.
See first sentence here: http://fmtlib.net/latest/usage.html. Adding the repository as a submodule is convenient, but you don‚Äôt have to do that. You can use it as a header-only library with a define (or via CMake - see the same link). 
Great, that's exactly what I need! Somehow I could not find it at first watch
&gt; That and the horribleness that‚Äôs `__declspec(dllexport)`. I work primarily on Linux, so I must admit not having to deal with `dllexport`. &gt; Also a stable abi and a standardized build system would make package management a breeze. A standardized build system and a standardized package description make package management a breeze with or without ABI compatibility; that's been proven with Rust's cargo. The only advantage of ABI compatibility is the ability to deliver ready-made binaries, which is mostly a concern for fundamental libraries (such as the `std`). For library authors, it's just impossible to distribute the library on every platform under the sun anyway... and even when they do, I've already had issues because a library was referencing a newer version of glibc than what I had (thankfully detected at load time), so I had to figure out how to recompile it even though I was using a "supported" platform. I seem the remember the same issue occurred on Windows, with each release of Visual Studio creating intentionally incompatible symbols, so that you'd need to prepare multiple binaries for Windows alone. To me, binary distribution of libraries doesn't make sense, and binary distribution of applications is best left to a customer-oriented package manager which can install system-wide uniform ABIs. 
Yes, a submodule works fine for me.
&gt; How can i get back MyEnum::Two and MyEnum::Three from mask without testing each bit ? You test each bit. But the better approach is just to pass around the `mask` value and test for the bits where relevant.
Inspired by recent talks by Odin Holmes, I believe we should shift the discipline and burden of knowledge from the programmer to the compiler. There should be a tool that can help detect such cases and suggest better alternatives.
I'm not trying to troll or just be the contrarian here, but why are compile times such a massive deal? Sure, I don't like the waste of electrons from having a simple example take 1 second to compile, but is that really such a huge deal? I would rather there be an up front "static" cost to a faster program, than having a less optimal program which is harder to maintain. What kind of compile times are you all talking about here that irk you? The largest projects I've built from source (binutils + gcc + glibc + libstdc++) take about 15 minutes, and it seems like, at the least, a minority majority of that time is spent between linking and miscellaneous build scripts/make commands. By "harder to maintain," I mean that you use the old "C"-style approach to a problem like pythagorean triples in nested for loops. I agree that the syntax required is a bit overwhelming and verbose, but I also thought that way when seeing things like `std::visit`, `std::variant` and code which uses integer parameter packs (e.g., using `std::make_integer_sequence`). I was so adamant about C++ "going in the wrong direction" when I first saw these things; however, I don't find them to be overwhelming when used properly. Also as a pedantic aside, I honestly think reading code that doesn't cuddle braces is much harder to mentally parse. I guess that could be just a consequence of me starting with Java as my first programming language.
My problem is in this situation, i don't know what i want to test it against, think of it as if `mask` was a list here, i just want to get all the enums contained in it, regardless of what they are.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
I'll check those out, thanks. If I understand correctly, I think I agree, but to me this is not a language problem but a tooling problem. For example, those 20k line cpp files that take 120 seconds to compile could automatically be split up by a tool without ever changing the language. Same with include maintenance. I often find that people conflate language and tooling. 
Is [P1329R0](http://wiki.edg.com/bin/login/Wg21sandiego2018/EvolutionWorkingGroup?origurl=/pub/Wg21sandiego2018/EvolutionWorkingGroup/P1329R0_OnTheCoroutinesTS.pdf) available somewhere not on wiki?
I added a scripting layer to a small game I have and oh God... my iteration speed grew orders of magnitude. And I only had to wait on the 20-40 seconds per compile (which is not so much for C++) but now think of changes of 1-3 lines and waits of 20-40 secs plus running. I became 10 times faster easily with all corresponding feedback when programming. Btw, thanks u/lefticus for that ChaiScript :)
Somewhere you are going to write `if (myListOfTags.contains(MyEnum::SomeTag)) { /* do something cool */ }`. In that spot, instead write `if (myMask &amp; MyEnum::SomeTag) { /* do something even cooler */ }`
I don't think you read anything i wrote, sorry but that is not what i asked. I am not asking how to test a bit but how to get the list of all enums contained in a variable, without knowing what to test it against.
Undefined behavior strikes again.
Such respect. You know the [XY Problem](http://xyproblem.info/), right? Making a list out of a mask word is probably your Y. It makes sense in a language where lists are first-class objects used frequently, but it's a bad match in C++ for what you've described. I know what you want to do. It has exactly the problem with it that you have seen, which is that you're going to have to loop over the enums and check each one against the mask, stick the enum into the list, and then work with it elsewhere. In fact, it's *super* ugly because there's no way to enumerate an enum, so you'll need to have a hardcoded switch (plus the branch wherever you're testing for presence of an enum). Plus any dynamic list is going to be allocated on the heap, and you'll need to pay attention to how you pass it so that it isn't copied over and over again. At which point, I have no idea why you'd want to use the mask word at all anywhere... just pass around the list from the start. But then you said you wanted to use the list like `myList.contains(SomeTag)`. And my point is that if you're using the list like that, you can just pass around the mask and test against it as I suggest. The mask is effectively a fixed-length list, and `&amp;` is effectively `.contains()`... except it's pass-by-register and search is constant time instead of linear, which is why C++ programmers sometimes use mask words instead of just lists of tags.
Let‚Äôs say you want to for-loop iterate over a std:map&lt;CustomerKey, SSLCert&gt;. What‚Äôs the type you use? No using auto or value_type because apparently those are bad/too verbose.
They have an _actual rvalue_ to bind to; you have an lvalue reference to a temoporary. This is called a dangling reference ‚Äì rather the antithesis of "working perfectly". In fact, if you read the comments on the answer you linked, you'll see that /u/notayakk has already singled out your exact example as "an important gotcha".
It certainly is a balance to be struck, and a complication of a language evolving over its lifespan.
There is a pretty simple workaround for that, suggested by Douglas Crockford himself: [https://plus.google.com/+DouglasCrockfordEsq/posts/RK8qyGVaGSr](https://plus.google.com/+DouglasCrockfordEsq/posts/RK8qyGVaGSr)
Yes please!
Indeed... Yep, I got gotcha'd. Sanctimonious replies in haste are often regretted...
[P0960R1](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0960r1.html)
Does JMC for C++ still require emitting a stub at the beginning of every function? This was a dealbreaker for me in VS2017 15.8, I can't tolerate that kind of impact in optimized builds. 
Does it work out of the box or do I have to pair it with other software to make it work?
I do it because of the level of control it gives me. I'm just not going to change that. I'd never have taken this whole project on if it weren't for that level of control. Nothing is a black box to me (well, other than the OS itself.) Nothing ever changes underneath me (other than the OS and I only use the lowest level stuff I can get away with.) No one in the field is going to install something and suddenly my stuff starts acting weird because it replaced some third party bit I depend on. And, since I'm creating a fully from the ground up portable system with a full virtual kernel, I have to be on the bottom. I cannot have other code underneath me or it becomes a mess. I can't make that code use my logging, my stats system, my networking architecture, my pub/sub system and so forth. I can't create a fully consistent system if I have to have a big chunk of someone else's code at the bottom. And, the thing I don't get is this... I go into a large company that has a really big code base. Out of that code base, the standard libraries stuff is probably like 1 percent or less of the code. The rest will be proprietary code that is unlike anyone else's. I, coming into that company, will have to put in a lot of time to learn how it works and understand it. That's totally commo. But, if I do that and just include that other 1 percent, I don't get how that's so different and so difficult to accept. Particularly given the huge benefits it brings to me. &amp;#x200B; On the other dev thing... That translates to 'pay someone a lot of money to bring them up on my code base'. That's far easier said than done in a small company. If someone else does come on board, again, the standard library stuff would be the least of his worries. There's vastly more other code that has nothing to do with that that he would have to learn, and there's no standard implementation of those things. Everyone's is proprietary. &amp;#x200B;
For most game companies the product is entertainment, not tech, and they are lead as such. (unless you're Epic, Nintendo, Microsoft, Sony, etc...) I don't think it's fair to measure technical interest by time that's hindered by industry business priorities. Non-tech leaders don't care about standards advancements that don't immediately impact their bottom line: visuals impact it, audio impacts it... language and process improvements? Not so marketable. &gt; We finally added C++11 support to our game in 2018! Hooray! Meanwhile developers are stuck with long schedules and are unable to volunteer their own time for language evolutions that very much impact us. This is only exasperated by the excessive amounts of time demanded by the committee to participate in any meaningful way. While I agree that game companies across the industry should invest more in their underlying technologies (as well as better work schedules in general), I don't feel that a lack of *corporate* investment in this case should exclude the inputs of the actual tech actors within an industry who do care- but aren't given time to show it. Which is why I mentioned the need for process improvements to give more opportunities to provide input. I feel that it would be far easier for one committee to improve their process than for an entire industry to change (which needs to happen anyways, but will be painfully slow-coming: more so than a c++ standard adoption) There may well be alternatives I'm unaware of, so any tips or suggestions would be greatly appreciated!
There's no fixed value. But I certainly wouldn't provide separate named methods to access 8 members of the same type. Doing it that way prevents any attempts to process them all in a simple way. Having them in a list and indexed by a named enum value means I can easily loop through them or any sub-set of them and process them in as compact a way as possible, while still having a self-documenting, compile-time checked means of accessing them. It works quite well. &amp;#x200B;
&gt; &gt; Consistent comparison (operator&lt;=&gt;) &gt; Is it really that difficult to implement this? Aside from the addition of the operator to the stdlib... it should just be adding a new operator. Haha, maybe. Why don't you show us all how it's done? :p I don't think it's _just_ adding a new operator, though. `&lt;=&gt;` has some peculiar rules about how other operators are generated or looked up based on its presence and the presence of other operators; not even move/copy generation rules are as complex as the spaceship rules, I think. &gt; &gt; [[no_unique_address]] attribute &gt; Why is this difficult to implement? Who said it was? You're consistently confusing "easy to do" with "someone decided that out of all the things they could be doing with their time that they wanted to go do that one thing more than anything else." :) If nobody decides that feature is worth their free time and no company decides that it's worth their money, it won't get done. _Doesn't matter if it's easy or not._ At some point - possibly closer to the official release of the C++20 standard - there will be people and companies that decide that "fully implementing C++20 in Clang/LLVM" will be worth time/money even for all the features they don't directly need themselves, but that time has perhaps not yet arrived. :) &gt; &gt; [[likely]] and [[unlikely]] attributes &gt; They already support GCC's __builtin_expect, which can act as likely/unlikely... The attributes function rather differently from the existing built-ins. The new attributes annotate the _branched paths_ (e.g. statements) rather than the _branch conditions_. I'd bet that the vast brunt of the work here is going to happen LLVM-side rather than in Clang. I'm most jazzed about `[[likely]]` because the VC++ devs will (probably) finally implement explicit branch-selection optimization and stop telling us to "just use PGO." :)
&gt; The largest projects I've built from source (binutils + gcc + glibc + libstdc++) take about 15 minutes Then those were small projects. The big ones go easily into the hours territory on distributed farms.
`ld` on a release build takes up ~24GiB of RAM when linking the executable. It's not small.
&gt; Nondeterministic Behavior of Generic Class I think this ought to be paid some extra attention to if "we" decide to change initialization any further. Nobody would think it's a good idea to create a `Foo::func(...)` with a bunch of overloads with different parameters that do different things, yet that's kinda what we're doing with constructors right now. And `auto(...)` would just add another method with a ruleset different from `{...}` into the mix. Having named parameters or named constructors seems like a better idea than to depend on initialization rules that have to be memorized and change in surprising ways if the code is modified.
You have (also) clearly thought about this for a while - what i'm currently doing (untill P0960R1) is the following - all Lvalue-construction - parenthesis only, then try to ensure (atleast somewhat good) class move behavior so as to (attempt to) justify all invoking of list construction (direct/agrigate etc..) as Rvalue assignment - thus (unhappily) accepting the strict narrowing requirements but (atleast) providing code which in invariant to the addition of future constructors. code on !
YES. PLEASE.
Why do you think should it not be in the stdlib? It's a universally useful utility function that works with all the containers. Your solution also ignores optional parameters that can be given to `std::sort` and because it is non-standard you couldn't expect the same sort of function to exist in other projects. So it's not that simple. If this is only because the simplicity of the function then lots of other stuff like min, max, make_pair, etc do not belong into the std lib either.
What I'm upset about is that it needs the gargantuan ranges library to solve a tiny issue. Not only does it invoke much of the problems talked about in the article but it also took about a decade longer than it could have. And ultimately we traded one minor inconvenience for another: having to go to the top and writing `#include &lt;range&gt;` when you want to conveniently sort a vector.
Compile times are mostly going to be solved by modules in c++20. In the meantime you can try precompiled headers, using SSD, and other techniques to workaround long compile times.
Don't use braces if you don't want to store the values in the object (e.g. array-like or POD-like type). That simple rule already avoids most (not all) hassle with braces. Uniform initialization was introduced to allow custom "container-like" types like std::vector std::pair to be initialized just like their native counterparts(c-array and structs). That is what that syntax should be used for and nothing else. The "use braces everywhere" was an attempt to give a simple universal guideline to c++ programmers, that just doesn't work, because c++ isn't a simple language. Adding yet another initialization syntax with yet another set of rules and yet another set of edge cases will imho not solve the problem, but add to it.
One of the biggest gotchas I ran into was boost::file system and std::filesystem treat appending an absolute path differently. /a/b + /c/d is /a/b/c/d in boost:: filesystem and /c/d in std::filesystem.
I think the are three main use cases for a standardized abi: - Closed source libraries (they do exist) - c++ interfaces in shared libraries that you want to update independently (that is already possible typ some degree of course) - Interop with other languages. Wether it would be worth our I can't say.
One of the more interesting findings from the article is that, if all you want to do is cast from a base class pointer to the exact type it's pointing to, then using typeid() is 3x faster than dynamic\_cast. The link to the Itanium discussion is interesting along those lines because it sounds like they chose to optimize for this scenario (for which no timings were obtained).
I think this is likely to be accepted. It is mentioned here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1018r2.html
Not just the compilation model, but the object and library formats on some platforms. They were never intended to be used with C++, let alone modern C++. Look at Windows DLLs. You can't write a C++ library on Windows as a DLL without making significant compromises upon template usage, which implies you can't use any of the standard container types. And a whole lot more is highly restricted, fragile or impossible. So you have to use header-only or static libraries, both of which also have important tradeoffs. The net result is that using C++ is drastically hindered by 1980s design decisions, and I've not seen any prospect for this to get better anytime soon.
The immediate fix that jumps to mind... std::path x('/a/b'); x /= '/c/d'; However, I don't think a 'chain' of [append](https://en.cppreference.com/w/cpp/filesystem/path/append) calls looks nice. IMHO
At the expense of interoperability with the rest of the world. Performance can be important, of course, but using nonstandard string types is a blight upon the lives of everyone who needs to use it with other libraries. One of the worst mistakes C++ made is making `string` a template. Look at `filesytem::path` for how it *should* have been done. Were it a simple class, it could be used pervasively, including without restriction in DLL interfaces. As it is, it's horrible. The number of times I've legitimately needed to specialise the type traits and allocator in the past two decades has been *zero*.
I don't know, but perhaps /u/TartanLlama does.
Note that the `-lstdc++fs` is both GCC-specific and going away once the ABI is stable (it will be part of the regular libstdc++). I wrote [FilesystemChecks.cmake](https://gitlab.com/codelibre/ome-common-cpp/blob/master/cmake/FilesystemChecks.cmake) to accomodate the for the future (ignore the Qt5 bits, which are for non-Boost, non-Standard C++ platforms where an alternative Qt5-based filesystem library is provided). I've run into similar subtle differences such as trailing backslashes on directories. But for the most part, a simple change of include and namespace does give you a mostly compatible replacement.
&gt; The problem is we most have to know which type is aggregate and which one is not! we have to memorize it! Or always avoid brace elision with `std::array`. That is, initialize `std::array&lt;int, 3&gt; arr = {{ 1, 2, 3 }};`. The outer braces are for the `std::array` and the inner braces are for its built-in array data member. &gt; auto c = int(get_it()) // or we can be explicit about the type This is a functional-syntax C-style-semantics cast, and is willing to reinterpret (from pointer to int, for example). I **strongly discourage** writing such code.
Welcome to the subreddit; however, I believe that this is off-topic.
`vcpkg` is alleviating some of this problem on the Windows side. Though it still insists on using `vc140` with VS2017 rather than `vc141`. Would be nice to have the option of using the latest and greatest for everything.
Yep, that's all correct.
The only thing that I haven't found a direct replacement in STL filesystem for is a line that says `fs::path tmp_dir = fs::canonical( fs::temp_directory_path() ) / fs::unique_path();`. The `std::filesystem` has no `unique_path()`, because the boost's implementation was deemed not secure and potentially predictable.
But a reference is nothing more than syntactic sugar for a pointer, while a view can be a more complex view onto data, such as a subrange or sparse set which couldn't be done with a simple reference. And in the case of `string` and `array` (and `path`) views, the underlying data can be raw memory, or a C variable, or a C++ object instance. Only the C++ object could be referenced, but the view presents a consistent representation for all of these data sources. And the view can be of immutable read-only data. At some level, I agree. It's an unfortunate limitation that `string` and `array` can't be directly zero-copy initialised from raw memory or C types without the need for any memory allocation or copying. That would perhaps remove any need for corresponding view types. However, since we don't have that option, the view types do elegantly solve a long-standing deficiency with the language.
For me complexity is a part of life as a high-performance program architect and thus more options just means more power - IMHO most people who are using c++ should really be using a much higher-level language which trades-off much of this complexity.
I think it's probably agreed by most that using named constructors is the best option - unfortunately in practice it's often the case that being able to (easily and inline) create any-old-thing from any-old-other-things (without touching implementations) is a really powerful motivator perhaps powerful enough to justify fairly seriously contorting the basics of the language... i think for better or worse it will likely go thru. c++ has very many masters.
For me, I really only generally have one question: Does it allow me to express more compile time semantics so that the compiler can watch my back all day every day in more ways? If not, then maybe it's something I'd be interested in, but it's not a gotta have it thing. All of the 'try to make it look like Javascript' stuff that's been going on, I really don't care much for it. And things that are designed to push more checking off to runtime, or that are designed to allow fewer words to be typed at the expense of compile time semantics/checking, I would almost never be interested in that. There's no need to be Javascript or C#, since they already exist. My mantra, as always, is we don't need more software, we need better software. The more mental CPU cycles we spend on stuff that the compiler could be watching for us, the less we have available for the bigger picture semantic stuff where things get really complex over time and changes. Let computers do what computers do well, and let humans do what humans do well, and get the most out of both of them together. &amp;#x200B;
I get what you're saying with respect to the realities of the devs in the trenches, and I sympathize with them. Another perspective: if the game industry is not willing to help pay for the development of the C++ standard in a direction that's beneficial for them, what makes you think other industries want to pay for them? Surely they'd rather fund their own preferred direction?
My C++ code base takes around 25'ish minutes on an 8 core system using the parallel build feature of C++, if I have to build from absolute scratch. However, I'll take that cost relative to making it less compile time safe. 25 minutes of rebuild vs a full day of trying to figure out some obscure problem in the field is well worth it. And of course you can greatly reduce the amount of rebuilding you do if you are smart about it by hiding details in all of the usual ways. OTOH, I also am willing to pay the price for more build time in exchange for not having crazy hard to manage header inclusion scenarios. Each library has a single main header that you include and you get everything in that library. Yes, it does cause more rebuilds, but you never have a situation where something changes and suddenly something is missing because it was being brought in by the thing that was removed, and there's a big mess to get that straightened out. And I don't have to do tests to insure that every header will work on its own. It guarantees that you can't include things in any wrong order or fail to get something you need. That public header of course brings in the public headers of any libraries that its public API depends on. Ultimately, I've found that to be a win, despite the extra rebuilding. &amp;#x200B;
Does anyone know why the standard filesystem seems to be so tricky and involved? 
The speed difference depends on the shape of the class hierarchy on Itanium.
[Great use of space](https://i.imgur.com/RJUASqt.png)
I have once looked into D community and I thought the biggest problem for D is that they are suffering the same fragmentation as C++. Even if they have a package manager, many things are still incompatible among dmd, ldc and gdc.
It's a typographic rule of thumb that you should only fit 2~3 alphabets on a line before it becomes hard to read.
I solve the whole linking drama like this: [https://github.com/kmhofmann/selene/blob/dab9d1ecc845678dcbbf6d85a069118573326873/wrappers/fs/CMakeLists.txt](https://github.com/kmhofmann/selene/blob/dab9d1ecc845678dcbbf6d85a069118573326873/wrappers/fs/CMakeLists.txt) [https://github.com/kmhofmann/selene/blob/dab9d1ecc845678dcbbf6d85a069118573326873/wrappers/fs/Filesystem.hpp](https://github.com/kmhofmann/selene/blob/dab9d1ecc845678dcbbf6d85a069118573326873/wrappers/fs/Filesystem.hpp) Works on Linux, macOS (where it *still* has to fall back to using Boost.Filesystem, thanks to Apple not porting things to AppleClang from upstream :-/), and Windows. Quite sure the presence of `-lstdc++fs` or `-c++fs` can be accommodated for in the future -- in the worst case, they'll be empty. What's worse is the renaming from what was `-lstdc++experimental` or `-lc++experimental`. One can't easily get around a) detecting whether libstdc++ or libc++ is used, and b) trying to detect which header/auxiliary library is supported, if any. (And I just noticed I don't actually support the &lt;experimental/...&gt; header yet in the library header... didn't catch that since none of my CI setups have a slightly older GCC/Clang version. To be fixed.)
And this is an "implementation" detail people have to be aware of using this library. I understood why boost did it the way they did with their filesystem library. Not at my desk at the moment, but I recall using similar library inclusions with llvm in CMake. If the PTB's make the filesystem specific name go away I would consider that a good thing.
You might want to use gcc instead for this example, because it's really to your credit: gcc 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Typeid: Avg elapsed time: 26.2 ms, stddev = 1.23969ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Dynamic Cast: Avg elapsed time: 82.95 ms, stddev = 1.09904ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Manual RTTI: Avg elapsed time: 52.55 ms, stddev = 0.604805ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Single dispatch: Avg elapsed time: 23.9 ms, stddev = 1.07115ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Double dispatch: Avg elapsed time: 23.2 ms, stddev = 1.36111ms clang 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Typeid: Avg elapsed time: 26.75 ms, stddev = 0.910465ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Dynamic Cast: Avg elapsed time: 83.75 ms, stddev = 0.850696ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Manual RTTI: Avg elapsed time: 23.45 ms, stddev = 1.05006ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Single dispatch: Avg elapsed time: 23.85 ms, stddev = 1.13671ms 25000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000250000025000002500000 Double dispatch: Avg elapsed time: 23.55 ms, stddev = 1.19097ms -flto and -O3 where used for both I have to say, I'm quite surprised that 2 virtual function calls are outperfoming the manual versions so much on my machine (ubuntu 18.10, i5 4440k). Good write-up. Side-note: you need to include &lt;numeric&gt; 
&gt; what makes you think other industries want to pay for them? Interesting question! One that I didn't pose ;) To restate- **My position is:** We need better systems for *official* input and discussion about the standards. Period. I am not suggesting any specific implementation, nor have I said a word about anyone paying for anything. Just stating a need which can be elaborated on, for or against, when addressed as stated. If cost of implementation or time investment required from *someone* to implement said systems is a concern, then fair enough- I probably misread the question. But as written, it seems like there was a misunderstanding somewhere. That said, I don't understand this: &gt; the game industry is not willing to help pay for the development of the C++ standard in a direction that's beneficial for them Why must it be an entire industry that sponsors the needs of the programmers? Individuals could very well bring shared concerns and grievances without being representative of, or sponsored by, the industry as a whole. Again, this seems to expect an entire industry to change for one committee. It's my view that we just currently lack the tools to participate despite our industry's shortcomings (which DO need to change, but will be far more long-winded) More so, industry direction shouldn't disqualify us as users of a system. To allow such input methods that bias discussions and standardization towards specific industries seems counter to creating a broad user-facing system. &gt; Surely they'd rather fund their own preferred direction? I actually 100% agree with this. I said as much in my first comment: &gt; [...] and simultaneously too much work for a third-party sponsor to want to keep up with
[Is this you?](https://imgur.com/gallery/X7NrD8a)
Just came across [this related tweet](https://twitter.com/pati_gallardo/status/1078969731615465472) and wanted to share. It's a discussion about more inclusive (in various meanings) scholarships to get more participants to standards meetings
I don't see why we cannot just: template&lt;class...Ts&gt; using callback=function_view&lt;void(T)...&gt;; struct Animal { virtual void downcast( callback&lt; Dog*, Cat*, Horse* &gt; ) = 0; }; template&lt;class D&gt; struct AnimalCRTP:Animal{ void downcast( callback&lt; Dog*, Cat*, Horse*&gt; cb ) final {cb(static_cast&lt;D*&gt;(this));} }; now adding a new animal type goes in one spot, and you can use overloaded lambdas, pattern matching, constexpr if, or custom types to handle the double dispatch. You aren't forced to create a class every time you do it. animal-&gt;downcast(overloaded{ [&amp;](auto*){}, [&amp;[(Dog* d){ pet(d); }, [&amp;](Cat* c){ runaway(c); }, [&amp;](Horse* h){ ride(h); } }); 
Thanks! This is a really useful data point. I ran some of the initial timings on gcc and saw they were about the same as clang, so I just stuck with clang because reasons.
Hi sysop. I'm not sure what you're meaning here, specifically? Can you elaborate?
Doesn't this approach touch the Animal class every time you want to add a new derived class?
string not being an actual primitive type is the mistake. It still feels like const char* is the actual string type, and string is just a wrapper. Partly that's because so many basic string facilities have been so poorly implemented in C++ - I/O chief among them. I know of few places that actually use iostreams, and it still shocks me how awful it truly is whenever I end up having to use it. And yes, string shouldn't have been templated. But filesystem::path is still pretty awful as of yet. Eventually maybe.
He means that you have a lot of whitespace padding on the sides of the content but then a large horizontal scrollbar for the code block.
Also, the page width does not increase with increasing font size (ie ctrl-shift +). Those of us with seeing problems use larger fonts and end up with lousy formatting. 
Game devs care about milliseconds not nanoseconds, it's high frequency trading people that care about nanoseconds :)