I don't get it; this just begs the question ‚Äì if one has a flag that causes drastic changes in semantics, why would one _not_ make it possible to detect that flag from the preprocessor? There's a macro for nearly every other `/Zc` flag; doesn't this seem, like, obvious? (I guess this question is rhetorical... ;-])
It's very helpful because it doesn't count as a full statement, so you can avoid braces in your loop or condition.
Thank you, I am from Finland and was typing english in a hurry, will fix it. I guess I can not edit the topic anymore?
I do use auto of course when it feels suitable. I try to push myself towards the modern C++ and I am currently going through the two books The C++ Programming language from Stroustrup and Effective C++ from Meyers after I have read with the first one. I do try to strip my mind from any old thinking, however things like Microsoft Windows API still forces me to do things in C-way.
I suppose MSVC will then support output its AST?
Clion is portable and in my opinion the best ide you can get Install on the usb drive if you want or copy the folder to it, execute the .exe or .sh and ready
I understand your viewpoint and it is a valid question. I know I will probably never be a person who works in a industry and produces large working systems. I mostly fiddle on OS level creating windows, handling events, read hardware stuff, create OpenGL or OpenAL contexts. Separate systems. I like to dig deeper and deeper. In Linux (especially Xorg) it is easier to write modern C++ with its API than with Microsoft Windows whos API forces me to still work with old C ways.
Eventually, yes. But that is mostly unrelated to this feature. 
Congrats on the release/update :) Speaking of 15.3, I started having some issues with static_assert after updating. Tried to slip in and get some attention after another user here was somewhat rude to a dev, but I think I got in too late. https://www.reddit.com/r/cpp/comments/6tnt8t/visual_studio_2017_version_153_released/dmgtlce/?context=3 Curious if two-phase name lookup and changes to how MSVC parses templates could end up fixing this... will have to try!
Will Networking TS be a good replacement for the Boost ASIO library? How would you rewrite the code for that?
Big talk, nothing backing it up.
Is this really something that is news to you guys? Suddenly there are 10 JavaScript programs where there used to be just visual studio. It is so extreme I have a hard time believing this is taking you by surprise.
I think they're both good at different things. Python teaches you the fundamentals of approaching problems like a programmer. It helps you logically understand how to solve issues you might have. C++ (or C actually) teaches you how to actually think about and understand what's actually going on -- how your computer is using resources, how memory allocation works... what's going on under the hood so to speak. Learning how primitive types were stored and how memory management worked really helped me grok at a deeper level what programming actually was, and made my code cleaner and faster. I love C++, it really helped me understand a lot, and as you say, it really does make intuitive sense. Everything (ok, *most* things) that it does, I can understand why it needs to do it that way, and I feel like I have a much better grasp of my coding. That said, I still **love** Python. For 99% of my applications, Python is appropriate. And as much as I love and appreciate what learning a lower level language has taught me, when I want to just bang out a small program to accomplish a simple specific task, it's just better and faster for me to use Python. I feel like Python is an extension of my problem-solving thought process, so I can just go from brain to working program very fast. Whereas with C++, I have to stop and think about things much more. Maybe time will narrow the gap, but for now I still put Python as my number 1. (Although C++ is number 2. And fucking Java and its stupid fucking JVM is dead last.)
&gt; IDE support for C++ is more mature and better for big projects anyways One of my favorite things about working with Python code is using the PyCharm IDE! 
Ofcourse Python is a serious programming language.
What good would a macro do? If there was a macro, you'd be altering your code to address this problem by allowing it to handle each case separately. Since the macro doesn't currently exists, you are by definition changing code. I would argue that you'd be better served changing your code to be agnostic to the lookup model used, instead of trying to handle each case separately.
https://en.wikipedia.org/wiki/Identity_element
&gt;In mathematics, an identity element or neutral element is a special type of element of a set with respect to a binary operation on that set, which leaves other elements unchanged when combined with them. I looked it up real quick before I asked and I figured he thought I was using a set, since that's similar to the python sytnax for writing one.
My personal use-case would be to give early errors for known problematic third-party headers (e.g. the current WinSDK), i.e. improve diagnostics. There also exists lots of code (e.g. in Boost) that works around MSVC deficiencies, and this code needs to be _conditionally_ disabled. But that seems a bit irrelevant to me ‚Äì I don't feel like I should should need to give a sales pitch here. It's not a huge deal, it just seems like an obvious omission to me and I thought there might be either more to it, or something I'm missing.
Okay, look, I just said that your initial value should be 1, because that's the neutral element for multiplicative operations. That's all, really.
You're best off using the [daily MSVC drops](https://aka.ms/DailyMSVC). I haven't updated them in a couple of weeks (vacation, you know) but the compiler I used when writing the blog post is the compiler that's up there now. I sent a message about your static_assert issue to our compiler dev lead and to Leo, who already commented. I'll let you know what we find out.
I don't understand what's happening. I was confused and you clarified, I thanked you and edited. Then you linked wikipedia and I explained my confusion and now you're coming off like one of us did something wrong.
/u/dodheim, I'm not saying you're wrong either : ) If it's any consolation, we've been working with the Boost folks since VS 2015 Update 3 ("14.3") to get the special-case MSVC macros removed from Boost. We still aren't two-phase clean with regards to all of Boost as of yet, but when we are we'll make sure Beman &amp; co. know. We look forward to the day when there are no MSVC-specific macros in Boost. 
Let's just grab a beer.
It's a date.
I'm not sure what the news is. You said you have: - A tiny project, a few KLOC - Lots of IPC with node.js service processes - Works well in VS 2013 - Performs horribly in VS 2017 To me that looks like a potentially huge problem with VS2017. I haven't heard of such a perf issue, no. But we don't frequently hear from developers who have a ton of node.js chatter. So I'd love to see your project if it's something you can share. We certainly want VS 2017 to work well for C++/node.js developers. But I haven't heard of any issues up until your post. If it's not something you can share, thanks anyway. I'll ask around it see if this is known, but that's a lot less actionable of a game plan. 
What about using a pi zero as an alternative for the mean time? It's cheap! Can't beat 10 bucks plus the SD card. I haven't done anything on it using cpp, just python. I don't see why it wouldn't work? It can be a cheap alternative for the time being that way you can work or chill at your place instead of spending mad hours at your school 
Eclipse+CDT.
We're going to _have_ an AST, but this is the first I've heard of _printing_ an AST. Maybe I didn't get the memo (which is entirely possible).
Yea, I was pretty greenhorn at that time and they wanted me to do it completely in the MATLAB language. If I would have known better, I maybe could have talked some sense into them. Then again, they probably wouldn't have listened to me...
What is CDT?
This sounds like something i'm looking for. Will try it out and report back.'thank you!
10 bucks? Will check it out
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6zkkqq/tips_for_someone_coming_from_c/dmvz5ys/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Very glad to hear about the progress being made on my compiler of choice. The blog post states: "We decided early on to follow an incremental path rather than rewriting the compiler from scratch." Microsoft has enough cash to pay an extra team to also rewrite the compiler from scratch. Why choose when you can have both?
Why should us normies care about clang when g++ produces faster executables and has far better compatibility?
Also generators that can use newer instructions more efficiently.
Well I had an internship and after finishing my Matlab program and optimizing it, it was still taking some time to make all the calculations (not something too bad, but it was supposed to run on smaller devices later without as much processing power). It took me maybe a couple weeks to redo it all in C++ (thankfully in optimizing the Matlab code I had classes that were fully type-annotated, making it easy to convert them to C++ classes) and the result was quite impressive: without even trying to optimize the C++ code (just stupid loops to replace the fully vectorized calculations I had), the performance was close to 100 times better. In this particular case, it comes mostly from how badly Matlab handles classes and indirection.
As Python has a much richer *runtime model* you will gain much easier information about a runtime error than you will in C++ - no stacktrace there üòâ As you have quite a *weak* type model in C++ (cast to ``void`` and ``nullptr`` and degenerated pointers) you can have lots of runtime errors the compiler won't catch. So to argue on top of the size of a code base is not a good foundation alone!
Look for the `-ast-dump` switch: http://clang.llvm.org/docs/IntroductionToTheClangAST.html
What do you mean you're going to *have* an AST? You haven't had an AST this whole time? What's the data structure used to represent a parsed translation unit?
https://blogs.msdn.microsoft.com/vcblog/2006/08/16/thoughts-on-the-visual-c-abstract-syntax-tree-ast/
Actually that makes a lot of sense given the compiler's lineage, but I really hadn't thought about it until now. For what it's worth, I've recently written tools using `libclang` (and contributed where it was lacking), and it understands Microsoft specific extensions pretty nicely if you feed it the right flags (`-fms-extensions -target x86_64-win32`).
It depends on the size of your cross platform interface. Less lines of code are always better.
Ah, young people underestimating the cost of a rewrite. In this case, it's not only time (same as money in do many respects ), and risk (*also* same as money in many respects), but the cost of having two competing implementations for a *long* time.
You're right, but that's already intrinsic to the language. You shouldn't use C if you don't want to/can't deal with more things. Python is by design lets you deal with less things. But there is nothing intrinsically wrong with a trivial operation like list creation being 6 lines of code. Your code is sensible, and next time I'll just call `create_new_list(sz)` etc and it'll be much simpler.
It's an eventual feature. Probably not specifically an AST, but serving the same purposes. 
&gt;I think python is a good replacement to shell scripting, but that's pretty much it. It is not suited for creating complex programs like C++ is. Hmm, I dunno. Some tasks are a lot easier in Bash than Python. There's some things that Python is a lot better at, especially if you need a real "program" or their libraries.
&gt;Have you ever looked at Boost source code? You need an IDE pretty fast to make it a useful endeavor ? I use Vim when writing code using Boost. Documentation snapped to the right, Vim on the left.
Wait so your reasoning that the phrase isnt political in nature is to give a bunch of examples where it's used in non-american politics? I mean i don't care about the phrase but thats got me a bit confused
Ooooo i've never heard of that but it's juicy! I'll have to keep an eye on it..
It wouldn't be much of a help at this point - if it ever is. The reason is that the definition of `/permissive-` is still fluid: some bug fixes get in, others graduate to become unconditional fixes. What the Visual C++ team has been doing is to ensure that for the vaste majority of the bug fixes, there are work arounds that work both in standard conformance mode, and in the non-conformance mode. The expectation is that you use the switch as a way to gradually move your codebase, as opposed to a binary thing. For example, the Standard Library implementation works both ways. The Windows SDKs are being fixed similarly. The VC++ team wants to help you to move to the conformance world, but it wouldn't want to assist you in building more debt :-)
Apparently that answer was written before IDEs figured out how to output 4 spaces when you push tab. Modern advances in IDEonmics has allowed complete elimination of any difference to the user between tabs and spaces, while maintaining consistency on the poor places that have 8 width tabs.
Exactly. Plus, the definition of `/permissive-` is fluid.
Use Intel's new instructions. Make available RDRAND as "engine" available to existing distributions. Make RDSEED available for seeding existing engines that need it. See the following references: https://software.intel.com/en-us/blogs/2012/11/17/the-difference-between-rdrand-and-rdseed https://software.intel.com/en-us/articles/intel-digital-random-number-generator-drng-software-implementation-guide NOTE: RDRAND is the dream generator. It has no software-visible internal state, is MT-safe (according to the links, each core has own engine), has no knobs and tweaks and it "just works" (it can occasionally fail but then you retry). 
NetBeans.
Catch me in my office :-) It is not a secret that the IFC format used for modules in the VC++ implementation -- which is based on IPR -- will be eventually publicly documented, to foster tooling in the C++ community.
Qt Creator should work.
By extra team, you mean a certain number of experienced C++ compiler writers... Surprisingly, there is a fairly limited number of those :-)
well MS got the money to just clone the current team
It is a C++ extension for Eclipse. However its one of the worst IDE's for C++. Its not made at all for C++. EDIT: My opinion is based on my experience from 6 years ago. It might have improved a lot.
To make the decompilation of your executable file harder, you have to pack/encrypt it via a hard-to-statically-reverse packer/crypter or obfuscate it on the binary level (e.g. https://github.com/xoreaxeaxeax/movfuscator). However, keep in mind that if your file can be executed by the operating system, there is no way of *preventing* decompilation per se. A sophisticated-enough decompiler will always be able to reverse it (at least in theory). Think of game/DRM protections. There is always a company that claims that their protector is undefeatable, until a group of smart-enough people breaks it. 
It's great for Windows users. :-]
ASIO is currently being updated to reflect the interface of the networking TS. Release notes are here: http://think-async.com/asio/asio-1.11.0/doc/asio/history.html#asio.history.asio_1_11_0
If at all, I'd like to see a macro that tells me that I'm in legacy/compatibility mode (/permissive- not set). Makes for more complex #ifdef constructs, but it would set the "right" defaults.
Why would you choose msvc? You'll have to tear clang out of my cold dead hands.
from the article: &gt; One extension that we are actively working on porting to Visual Studio 2017 is Image Watch. Image Watch is a watch window for viewing in-memory bitmaps when debugging native C++ code with built-in support for OpenCV types. We know many of you are stuck using VS2015 in order to use Image Watch and are working hard to address this. 
&gt; underestimating the cost of a rewrite And underestimating the cost of writing a compiler, of all things. That is *quite* a different beast than the average application/library written out there.
[removed]
Sorry, when I said: &gt;you don't know what to put in the header or not I meant the compiler doesn't know whether to put a function in the header or not. In some cases, you want to only expose certain interfaces and hide implementation so that other code will not break if it changes. Compilers cannot make this decision. Only the programmer. You can create new syntax to make this possible, but nobody has done that yet.
That's exactly the point of my question. Is it something in the niche that C++ is in, that brings confidence in your code? Making C++ programmers not wanting to test their code? 
I read most of the first article, but I don't think that his description of TDD is what most nowadays describe as TDD or it's "evolved" form, BDD. I think most TDD practitioners will agree with some of his points. That's why BDD was coined, to evade the "Test"-only focus some have on TDD. 
I usually implement each platform specific behavior into dedicated classes with a common interface (guarded by platform specific ifdef), then abstract them behind a single typedef/using declaration. This leave no room to ODR violation, while having no real downside.
I beg to disagree. When was the last time you used it? I use it everyday and it may not be on par with VS, but if you want something portable or need to run on Linux it's one of the best choices. I don't understand the hate people have with Eclipse. CLion is also Java based, from the same developers of IntelliJ and they don't get the same hate. NetBeans was also proposed and it's actually worse for C++, it's actually Oracle abandonware, but I don't see anyone saying anything.
What about the two-phase lookup in particular? It seems like it would be useful to library writers.
That's what I replied to. The tool (not the compiler, it's not the compiler's job) can certainly know which methods to put the in the header or not. First of all, all method declarations go to the header. Secondly, c++ only inlines methods that are one liners. So these method definitions should be put in the header. Classes can be labelled private or public, and private classes can go to the implementation file only. 
This should definitely be made a FAQ
And underestimating the cost of writing a **C++** compiler, of all languages.
But they'd risk leaking the cloned team since they haven't updated their polymorphic `clone()` method to return a `unique_ptr`, yet.
If you read a little further: &gt; We decided early on to follow an incremental path rather than rewriting the compiler from scratch. Evolving the aged MSVC code base into a more modern code base instead of ‚Äúgoing dark‚Äù on a big rewrite allowed us to make huge changes without introducing subtle bugs and breaking changes when compiling your existing code. 
So the compiler's been in the process of being rewritten to use a complete AST for 11 years now? Must be a large codebase.
Being polymorphic buys you nothing because it's not as if you're selecting what you're calling at runtime. What do you think would be the benefit of runtime polymorphism for this use? Using pimpl buys you the same thing as in non-cross platform case. It's orthogonal to the question of cross-platform code.
Doesn't compiling on the pi take hours?
https://stackoverflow.com/a/22786728/1495627 https://www.reddit.com/r/cpp/comments/2mcw30/eclipse_cdt_vs_qt_creator_for_general_c_not_for/ Netbeans is at least as horrible.
"X-Macros" are a code generation technique that involves include-ing the same file multiple times with different defines as "parameters". Not surprised that clang-format breaks them if it's trying to reorder includes.
&gt; not an IDE but a glorified text editor There's a difference? When does a "glorified text editor" become an IDE?
Large task, small task force or inefficient way of working... or a combination.
&gt; Yea it misses a few very small bits What? It misses C++03 two-phase look up, which means that fundamental C++ concepts like argument dependent lookup, overload resolution, etc. don't work properly under MSVC when used inside templates (that is, either they fail to compile, or they call the wrong functions, which changes semantics). If this was just a "small bit" it wouldn't take Microsoft 20 years to implement it. The fact that MSVC also doesn't support some C++11 features like expression SFINAE is more a consequence of not supporting C++03 two-phase look-up than anything else. So yeah, if you never write code that uses templates, then MSVC is mostly fine. If you write a header only template library of any kind, supporting MSVC is IMO a huge pain in the ass. Range-v3 could support it (that fork shows that is possible), but not even Microsoft wants to maintain that fork because it is a PITA to do so. 
I use this every day on linux. Works like a charm. Any specific things that don't work for you? IMHO it has the best code completion, syntax highlighting and refactoring you can get on linux.
msvc would be c++17 complete in the end of this year?
Qt Creator, and no second option.
I second this. Using this at work for what I'd call a large project (~5M loc, ~5000 files) and code completion and hints are blazingly fast.
or visual assist
Is it the verbose nature of C++ that makes you feel it's easier and more logical to use?
Seems they got serious about it first in 2012, so about 5 years. https://blogs.msdn.microsoft.com/vcblog/2015/09/25/rejuvenating-the-microsoft-cc-compiler/ There was of course also a few new C++ features implemented concurrently in that time. 
&gt; If you need help rewriting a template, send mail Maybe write a blog post with a tutorial? Is it basically making all non-dependent arguments dependent?
CDT is the official supported IDE for the majority of embedded OS development and GPGPU development outside Windows.
Not sure. Could be one aspect of it.
&gt; When does a "glorified text editor" become an IDE? when it has a semantic understanding of the code, supports project management (with eg "add file to project", "add library", etc), and has at least some RAD capabilities
Do you expect significant compilation time speedup when using modules with lot of templates? Caching whole AST nodes should help right?
You could make a new compiler, but why bother when there already *is* a new compiler on the block called clang? Microsoft haven't allocated full time developers to clang development like say Google or Apple. But they have contributed extensively to clang/LLVM's port to the MSVC ABI via documentation, publishing formerly internal bits of tooling, and lots of unofficial advice given via side channels. A lot of people, especially those here in this subreddit, think clang wonderful and msvc crap. But clang spectacularly fails to compile some important bits of Microsoft code for *architectural design* reasons i.e. clang was not designed to be able to compile that code in a reasonable RAM limit, whereas msvc was and is. MSVC is a much better compiler than most people online give it credit for. It's also a very old codebase, theoretically you can still link Visual C++ v1.0 code with the latest VS2017 stuff and it could still work. Which is an amazing engineering accomplishment conferring lots of value to lots of C++ users who tend to be silent on the internet, and for whom using clang won't be practical for at least a decade more or so.
Yeah, you are right. Though something like `(++i, i*i)` shouldn't be if I'm not mistaken.
&gt; RDRAND is the dream generator It is useful for cryptographic rng and is unsuitable if you need reproducibility. 
Command lines are part of your system. What are you using ? Linux ? Windows ? macOS ? 
I'm approaching this from only knowing MatLab. I'm on Windows but it's literally just saying "set CD to X"... "type ./configure then run that" etc etc. It won't run in a windows cmd prompt if that's what it sounds like I've asked.
Thanks as always, Andrew! I await news from ya'll wonderful folk :)
By that definition there aren't really _any_ C++ IDEs... Even the best "semantic" type features (e.g. "Intellisense" and refactoring tools) have difficulty with any moderately large C++ project. Also, often you don't even _want_ an IDE with its own build system (what you're calling "project management"); they're always harder to use and less flexible than something based on make or CMake, even when they use those tools as the "backend" they always add restrictions. "RAD" (which almost always means drag-and-drop UI design) is utterly irrelevant for non-UI work.
&gt; We look forward to the day when there are no MSVC-specific macros in Boost. Going to take a while then at the pace they are deprecating compilers. A little tongue in cheek :P But yes, will be great when newer MSVCs won't need macro hackery.
Yeah, somebody else raised that. I didn't think of it as I've always avoided comma as a general rule. It does introduce a sequence point. 
Sure this will work in windows? It sounds like it uses autotools which is common on Linux. Otherwise you can try the Visual Studio Command Prompt (search for it using the start menu search in windows) 
because the script said it needed to be run in C++ and I figured, probably best to ask people who know C++.
&gt; Visual Studio Command Prompt righto, thanks. Will try and download that and see if I make any progress on that.
&gt; By that definition there aren't really any C++ IDEs... Even the best "semantic" type features (e.g. "Intellisense" and refactoring tools) have difficulty with any moderately large C++ project. The best just use libclang, so they have as much "difficulty" as your compiler does. &gt; Also, often you don't even want an IDE with its own build system (what you're calling "project management"); they're always harder to use and less flexible than something based on make or CMake, one does not preclude another: see for instance CLion which is perfectly able to handle CMake as project system. &gt; "RAD" (which almost always means drag-and-drop UI design) is utterly irrelevant for non-UI work. Sure, but not all use cases tend themselves to IDEs.
Well, actually, we got down to two in the Boost Develop branch about a half a year ago. But yes, until developers stop using VS 2010 and GCC 3.x there will always be a bunch of macros. 
One simple reason: http://i.imgur.com/YXs5XS8.png
&gt; "type ./configure then run that" etc etc. you're in deep shit
&gt; rapid switching between writing test and writing application code (as in every few minutes - maybe half an hour) is the goal of TDD That's the definition of TDD right there. Writing two test methods or a whole test class before writing the corresponding implementation code is *not* TDD, by definition.
I knew that when I saw the only resource available for what I need to do... was in C++....
Aye, to be fair I could have mentions that MSVC is not the only compiler that has macro workarounds in Boost. If it is down to two that is great news, especially for the Boost guys.
&gt; The best just use libclang, so they have as much "difficulty" as your compiler does. Parsing the source code is but one part of "semantic" tooling. libclang doesn't help divine which source files are part of a particular target module, which macros are defined by the build system, whether to "autocomplete" the client or server's version of a particular type or macro on a shared header file, whether I'm writing library code that can see the internals of an "externally opaque" type or not, etc. etc. All things that any moderately complex project is going to run into. &gt; one does not preclude another: see for instance CLion which is perfectly able to handle CMake as project system. I addressed that. "even when they use those tools as the "backend" they always add restrictions". Thanks for ignoring it... &gt; Sure, but not all use cases tend themselves to IDEs. From what I can gather, your only use case for an "IDE" is a dead-simple desktop GUI app...
Yes. That's why I also mentioned RDSEED. As for RDRAND, it's "dream" in the sense that it will work for clueless people who just want to get a random number, now. And these people are surprisingly many (everybody who uses `rand()`). PCG seems to be a nice generator: http://www.pcg-random.org/
The Pi Zero is not even close to a laptop replacement for C++ development. ARMv6 is slow for compiling code if you have any sizable project.
Those answers are at least two years old. A lot changed since then. If your build is verbose, CDT can even "auto configure" itself for include paths and defines.
&gt; libclang doesn't help divine which source files are part of a particular target module, which macros are defined by the build system, whether to "autocomplete" the client or server's version of a particular type or macro on a shared header file, whether I'm writing library code that can see the internals of an "externally opaque" type or not, etc. I don't know when was the last time you used an IDE but apart from the opaque type one, they handle the cases you mentioned nowadays (mostly by having different contexts that you can switch for a given file, eg you can choose whether you are editing foo.cpp in the context of some CMake target or some other target and as such applying the relevant #defines). &gt; "even when they use those tools as the "backend" they always add restrictions". maybe, but it's still far more potent that what you can do with a text editor such as VSCode, which was the original argument. 
I don't recommend this, NetBeans os currently an Oracle abandonware.
You could pick up Vim. There's definitely a learning curve, but it's always going to be there on any machine you use.
&gt; I don't know when was the last time you used an IDE but apart from the opaque type one, they handle the cases you mentioned nowadays Only if you give them control over the build system (or manually reproduce all the relevant information in another form). On more complex products that's not practical. When you've got things like cross-compilation, cross-platform support, etc. going on, it's usually impossible. &gt; far more potent that what you can do with a text editor such as VSCode If/when it works, maybe. The rest of the time, using a "simpler" IDE/text editor means that you're less annoyed by broken/incompatible IDE features and can concentrate on actually writing the code. I'd rather have something basic that works than something "potent" that's going to require me to completely restructure my project to use it. A real-life example; when CLion was first launched, I decided to give it a go. While most things worked "OK", it's "feature" that automatically adds includes kept trying to add my hosts' standard library headers (by relative path to my project, no less) to my cross-compiled project (which already had the correct includes for its own standard library, where appropriate). That, among other less serious issues led me to reject it and go back to using a simple text editor with simple ctags-based "intellisense" features.
It does? In my experience, they tend to be roughly equal; although I've personally found Clang to be a bit faster for compile times, others have reported different results. It's very much project dependent, but at this point, neither is significantly better than the other. If you look at the C++ standards support chart, you can see that Clang's standards support is equal to GCC's with the exception of *one* proposal, which is supported but has been intentionally disabled by default due to a standards issue. Both Clang and GCC added `-std=c++2a` support around the same time, and Clang already supports some features in SVN (GCC hasn't updated their standards chart yet, so I'm assuming similar progress). Also, while GCC has concepts support, which is still WIP in Clang, Clang has added concepts support in 5.0, which [as of last month has not been attempted by the GCC team yet](https://gcc.gnu.org/ml/gcc-help/2017-08/msg00045.html).
No it won't. Look at this feature table and you'll see a bunch of stuff still unimplemented: https://blogs.msdn.microsoft.com/vcblog/2017/08/11/c17-features-and-stl-fixes-in-vs-2017-15-3/ I'm not sure I'd even expect another update this year. On the flip side, it's support for C++11/14 is looking pretty excellent.
I can't see a student compiling a major project. A student, not some engineer. So yea, I think it is a good alternative 
I would hope not? I don't see it taking hours for a school project. Either way, I still see it as a cheap alternative for a student
All compilers I know allow you to dump the AST in one form or another, it is useful for debugging some compiler bugs. Also, in a nutshell, modules is just serialization/deserialization of the C++ AST.
!removehelp
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6zjtwg/any_portable_ides_i_should_use/dmwlfna/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6zmra1/need_to_run_something_from_command_line/dmwlgzv/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I must have put a space somewhere!
Nothing can generate **your own** source code from your executable code. (Or rather, it can, but only through a **massive** accident.). Decompilers are used e.g. to help hackers to understand what the code does, and then, to work around "protections".
Side question: Is there any effort to bring the MSVC Preprocessor back into standards compliance? There's quite a bit of non-standards complaint behavior (expanding both sides of a ##, or recursive expansion for example).
It's only worth updating if: a: All your main platforms have a compiler that supports the new feature b: it will result in significantly cleaner code, better performance and/or fewer bugs of a particular type We upgraded to C++11 compilers for ranged-for, variadic templates, move, and lamdas. We don't use STL (unfortunately) but did clone unique_ptr etc into our own standard library. We haven't yet seen a big need to upgrade to C++17 compilers.
 If you ever get to talk to Youtube eng ask them what % of people working in YT actually like Python for large codebases. 
&gt; There is a lot of text manipulation and range comprehension. Your project is not common workload(assuming you are regular developer working on regular C++ project for $, hobby projects excluded) &gt;Functional programmers love UFCS, it's just how many people think/approach a problem. {"functional programmers", "many people"} - pick one :) C++ is verbose compared D in many examples specifically designed to make D look better, but IRL problems I have with C++ verbosity is const auto instead of just const, decltype being much uglier than D version(IIRC it is just variable.type), specifying types of lambda parameters when me and library code know it is const auto&amp;... So while I understand your view of C++ problems I disagree in their severity for regular developers. So what I am claiming here that you learning a language and hitting some verbosity problems for your toy project is not the same thing as working in a "boring" project for $ - you will not care about UFCS there, you will be irritated more by other things in the language. Also FYI quick sort in C++ is also short, if you use STL for building blocks(std::partition).
My opinion is probably outdated. It has been about 6 years.
TBH I would have hoped for diff tool switch within a compiler wrt this, since it is a breaking change and you know codebases of 20M LOC do not have the time to get developers to look over all the templates and check if nothing broke...
Nah, auto-mod has been skipping things occasionally recently.
&gt; Microsoft has enough cash to pay an extra team to also rewrite the compiler from scratch. Why choose when you can have both? Microsoft is a company not a charity. If you think that spending 5-10M $(my estimate for rewrite, others may disagree) is a wise business decision then IDK what to tell you... :D Because you know whining of powerusers on this subredit is not representative of general MSVC users. So yes I would want that MSVC has 5 more compiler developers working on it based on me being spoiled C++ developer, but if by some bizarre chance I was in charge of MSVC development I would do it pretty similar to what MS is doing ATM(with some notable minor differences obviously :P). 
I'm not quite sure what the purpose of this is. It shows how to implement some arguably weird JS behaviour in arguably bad C++ code. When moving some JS code over to c++ it needs to be rewritten anyway, so why not properly in the first place?
As a matter of fact, there is! We've been working on a new preprocessor. Look for a preview early next year probably. To answer the question everyone who doesn't write compilers will ask: our compiler is built in such a way that we can't just pick up another compiler's preprocessor. That seems surprising but it really isn't.
I think: float *pointer = new float; //requesting memory *pointer = 12.0; //store value cout &lt;&lt; *pointer &lt;&lt; endl; //use value delete pointer;// free up memory // this is now a dangling pointer pointer= new float // reuse for new address is really poor C++ style and shouldn't be taught that way. With the first explanation in C, it is good enough to understand the memory model. At most, I would put the equivalent in C++, but clearly and explictly stating that those are not good practices, and present modern and safe (without new typically) C++ alternatives to those arrays allocations.
But to your other point, it's down to two in the *develop* branch. So legacy is still an issue forever and ever. Getting the two-phase work completed should remove those macros as well. 
Could you tell me why you don't use STL?
Microsoft actually did attempt to rewrite the compiler from scratch some years ago (Phoenix), but the project mostly failed. My understanding is that it was trying to do too much as a general purpose compiler framework, which made it difficult to write new features and get to parity with the old compiler.
It's not for porting. It's for insights. So if a C++ programmer needs to use JavaScript, for example, it may be easier to wrap our head around when we realize that JavaScript's objects are hash tables, or that JavaScript's inheritance is delegating element access between hash tables, or that JavaScript's functions are hash table function closure objects. (JavaScript uses a lot of hash tables. :-P)
&gt; What do you mean you're going to have an AST? You haven't had an AST this whole time? From the article: &gt; In accordance with these original rules, previous versions of MSVC did very limited template parsing. In particular, function template bodies were not parsed at all until instantiation. The compiler recorded the body of a template as a stream of tokens that was replayed when it was needed during instantiation of a template where it might be a candidate. So, the body of template classes and functions was not available in the AST because they were not parsed yet. Makes tooling quite a bit more difficult :x
Haha cool! That's awesome. I read the beginning of the blog post and then scrolled through the list, but I didn't see this bit at the end! I really wonder what takes them so long, apparently some users even got it to work by just changing the manifest or something, but that didn't work for me. Anyway, great news!
I just don't know why people insist on writing blog posts that are repeats of introductory material that one might learn in a class, video, or book. Whether or not it's good style to use new/delete, I don't think many people in /r/cpp need them explained. Unless you're writing toy programs, overall allocation strategy and object lifetimes are the most tricky parts of memory management. It would be cool to see blog posts that had interesting things to say about either of these subjects.
Basically just one thing: do our officially-supported compilers support the new version? e.g., we're currently using VS2015 and Clang 3.9 at work so we're stuck on a subset of C++14. I can't use any C++ code - standard or not - that isn't supported by both compilers. When we eventually update to VS2017.3 and Clang 5 (or whatever is out and stable at the time), we'll start using newer C++ stuff. There's not really a good reason not to use newer language features if they're available. They make code cleaner, usually faster, easier to write, etc. The big costs are (a) training and (b) upgrading developer tools. If you're a team of several, both of those are going to be a lot easier to solve than if you're a big org like us, where we have probably 100+ engineers spread across 4 timezones and 3 continents (just getting the local dev tools upgraded requires coordinating with 4 IT departments and their budgets, and the CI infrastructure can sometimes take weeks to upgrade and get into a stable point).
FWIW: I would never use reference to pointer argument in my code
I've been meaning to get my own blog finally fixed so I can go over getting a block allocator for Vulkan working, and how that compares to doing the same for the CPU-side resources too. Agree that topics like this are a bit boring: I'd like to know more about the overall systemic design of a solid allocation and memory management system (which is apt, since C++ is good for systems programming, I guess)
I'm not that person, but there are two usual answers: 1. Working in gamedev. Everybody thinks their special allocator is better than the standard, so they rewrite everything to use it. This may or may not be worth it, but nobody questions the assumption. 2. Their codebase is old enough that the STL was shitty and immature when they started it, meaning they have a shit-ton of code using their own containers and utilities which are incompatible with the STL. (The view that the STL still sucks contributes heavily to #1 above.)
Right on both counts! I work with Unreal Engine, which was cross-platform before there were decent cross-platform implementations of the STL. Also, the engine supports script interop/reflection in a way which needs a known binary layout of containers, which you simply can't have with the STL. However, I wish they'd just allow it for some of the newer things, e.g. std::atomic, instead of trying to write their own (in the case of atomic... badly).
&gt;b: it will result in significantly cleaner code, better performance and/or fewer bugs of a particular type I agree with point A about platform support but once you have that what is the penalty of having some code using newer standards than others? The addition of R-Value move semantics could potentially be an issue but only if you were really doing some weird stuff that was already non-standard (MSVC at least used to bind temporaries to non-const refs). 
&gt; Delegating `unordered_map* __proto__ {};` is invalid C++ &gt; Each name that contains a double underscore (_ _) or begins with an underscore followed by an uppercase letter (2.11) is reserved to the implementation for any use.
&gt; I just don't know why people insist on writing blog posts that are repeats of introductory material that one might learn in a class, video, or book because having a "tech blog" is good on the CV
That sounds like a great topic, and one I would read with great interest. I feel like these are the sorts of things that make great blog posts for /r/cpp or the C++ community in general.
Quite. Or, you know, pass the object by reference, or possibly const reference, depending on use case. But by reference to pointer? I honestly don't think I've seen that in the 25+ years I've been professionally coding.
What I'm wondering is if "real TDD" is that much more common in other languages.
As others have said, 'when compiler support is available'. For my team, that means . . . GCC, Clang, MSVC, Intel, IBM XL, Cray CCE, and depending what mood people are in Portland Group (PGI) and Pathscale. We just made our last source release that won't require full C++11 support, last month. I actively curate http://en.cppreference.com/w/cpp/compiler_support so that I can refer to it and point my teammates to it as needed.
How do you typically implement stateful objects with static polymorphism though? Off the top of my head I can only think of two ways: header.h #if WIN #include &lt;Windows.h&gt; // Or attempt to duplicate typedefs for HANDLE #elif LINUX #include &lt;sys/types.h&gt; // Again can also attempt to typedef #else error #endif class bla{ #ifdef WIN HANDLE internal; #elif LINUX pthread_or_whatever_type internal; #endif void abstract_function_call(); } and then the pimpl pattern which basically just moves includes/typedefs and definitions into an internal class defined in a translation unit instead of a header.
It is not just about templates. Apparently, they literally did not have an AST data structure (afaik due to memory/performance constraints on the platforms the first MSVC versions had to run on).
Reason 3) You can't use exceptions. Also regarding 1): Contrary to popular believe, it is not difficult to write your own data structure that is more efficient in a particular usage scenario than the STL data types. The STL containers are general purpose containers and as such, the designers (of both, the specification in the standard as well as the actual implementation) did have to make a lot of compromises. When you know your specific requirements, the expected workload and hardware platform you can do a lot of usage specific optimizations/fine-tuning. The real question is not "Can you write a faster hash map than std::unordered_map?" But "Is the performance benefit worth the additional engineering and maintenance costs?" The part I'm always wondering about however is why those teams/companies outlaw the standard library all together instead of just augmenting it where needed.
Indeed. Passing a pointer by reference pales in comparison to the alternatives you've described. The technique in this article would mainly be useful if you're a student learning how to manipulate your own pointer based data structures recursively. 
Unfortunately virtual inheritance doesn't really work here. class cloneable : public clone_inherit&lt;abstract_method&lt;cloneable&gt;&gt; { }; class foo : public clone_inherit&lt;abstract_method&lt;foo&gt;, virtual_inherit_from&lt;cloneable&gt;&gt; { public: virtual void moo() {} }; class bar1 : public clone_inherit&lt;abstract_method&lt;bar1&gt;, virtual_inherit_from&lt;cloneable&gt;, virtual_inherit_from&lt;foo&gt;&gt; { }; class bar2 : public clone_inherit&lt;abstract_method&lt;bar2&gt;, virtual_inherit_from&lt;cloneable&gt;, virtual_inherit_from&lt;foo&gt;&gt; { }; class concrete : public clone_inherit&lt;concrete, bar1, bar2&gt; { public: virtual void moo() {} }; This fails to compile with the "no unique final overrider" error. 
&gt; I honestly don't think I've seen that in the 25+ years I've been professionally coding. I have seen it, I wish I did not :) Problem here beside that my brain has trouble figuring out it's semantics(I mean I can but when I read code like that it is a pause moment) is that it is easy to not see it in the calling code, because you assume it is taken by value.
The article lost me at not defining "covariance" before talking about it. I could look it up myself, but this is supposed to be educational, no?
No quick answer, sorry. Leo created a bug for us though. 
As was discussed before: Are you asking about writing tests in general or actual TDD? These are two different things.
/u/TheThiefMaster is correct on one count. I don't expect we'll get C++17 conformance this year, no matter how many hissy fits I throw. We'll be close, but not close enough for me. But I do expect another major VS 2017 update in 2017. 
I might be way off, but I would imagine you'd be looking at an order of magnitude more than that to write a new compiler from scratch. It's a huge technical undertaking. The type of devs that can write a C++17 don't come cheap, and there aren't that many of them. 
But can it run ranges-v3 ? ;)
We actually got serious about it maybe 3 years ago. But as we've started showing real benefits from the work we've been able to step up funding of the rejuvenation effort. It's always hard to sell a rewrite to management: &gt;"So our team's gonna do a lot of work to take this thing that almost works and create a new thing that also almost works! But one day--a long time from now--it'll be waaaaaaay better! Please continue to pay us!" Management has been fantastic in supporting the effort (thanks, /u/spongo2!) but we've also been careful to not ask for too much too soon. There's no way we could have just stopped cold and done this work all in one go. (Not to mention that we've had to make sure to be bug-for-bug compatible so we don't break old code...)
For some time it looked like you could have both (clang/c2) but unfortunately they stopped that.
And the fact that the compiler has just kind of added functionality for the past 35 years. You can point at Clang as an example of a well-designed parser. But it's only 10 years old. It's easy to be cleanly-designed software when you're brand-new. 
It's protected by an off-by-default switch so we do not consider it a breaking change at this point. And the feature isn't even complete yet. 
Thanks for the endorsement, /u/Z01dbrg. We do what think is best for our developers (and yeah, for the business of course.) 
this one is garbage
Let me say, I'm really pretty impressed by MSVC's conformance efforts. I've only been in C++ community for a few short years. My impression was that MSVC had no intention of ever fixing two-phase lookup, given how longstanding the issue are and how much standard windows code only works with the old way (or at least used to only work the old way?). So, kudos to you guys, this is really good for C++ as a language in the long term. I wanted to ask a related question: Does the MSVC frontend team plan to fix issues with `__VA_ARGS__` variadic macros conformance? There is a discrepancy between how gcc and clang do it, and how MSVC does it. Basically if `__VA_ARGS__` captures multiple arguments, and you forward those arguments to another macro, MSVC will pass a single argument to the next macro, and that argument will contain a bunch of commas. In GCC and MSVC, the commas will be broken up into separate macro parameters, so that it works a bit like "perfect forwarding" of function parameters. (From the standard's point of view I think the question is, are you supposed to rescan the `__VA_ARGS__` before invoking the next macro... not totally sure.) It is described as a long-standing bug in MSVC preprocessor by James McNellis here: https://stackoverflow.com/questions/21869917/visual-studio-va-args-issue More stackoverflow posts illustrating the issue: https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly https://stackoverflow.com/questions/32399191/va-args-expansion-using-msvc https://stackoverflow.com/questions/24894769/vc-vs-gcc-preprocessor?noredirect=1&amp;lq=1 If the stated goal of developers is "no more MSVC special cases in boost" or anywhere else, then this would be a great big bug to squash IMO. Is there any plan to do so? Or is this a "won't fix" issue? I only ask because I was under impression that two-phase was a "wont fix", so now you got my hopes up :)
Phoenix was an optimization/code generation/code analysis framework, not the compiler. If you'll allow the analogy, clang::LLVM what MSVC's "compiler"::Phoenix. In other terms, Phoenix was to replace c2.dll, not c1xx.dll.
I don't target a specific version of the standard so much as a short list of compilers. The code I write at work needs to build with Visual Studio 2015 on Windows and Xcode 8 on Mac, which effectively means most but not all of C++14.
That's something I'm happy to be wrong on üòâ
 To add on this. If there ever exists an executable file which is impossible to decompile then it wouldn't be executable. You can't expect the computer to execute instructions without letting it them know.
Thanks Andrew!
LOL, the Crysis of C++.
I see. Yeah, so a forum can be nice and it's very user friendly, but it creates a maintenance burden. Someone needs to administer and moderate it. A lot of forums attract spam historically. Maybe new techs like discourse are better, I'm not sure. I think a mailing list is less work to maintain. I guess I just use my gmail client, but I also am not actively following any of the standards mailing lists very closely, maybe it is hard to follow. Once in a while I do take a peek. Maybe you just need to find an email client that's nicer for this format.
Unless I'm wrong, it works: https://godbolt.org/g/osSDFS In the code above, there are warnings, indeed, because the same type appears more than once in the hierarchy, but those can be ignored IMHO because they are not important. In fact, they should be invisible (they are part of the scaffolding). Could you please share the complete code that produces the error?
I have seen C++ code with pointer to pointer. I even think it is common in certain circles. It always puzzles me that they don't use reference to pointer. Usually nullptr is not a valid argument. I wouldn't write either, but out of the two options I think reference is better. Nothing magical with pointers. One should prefer mutable reference over mutable pointer for all kind of non-optional output parameters. Still, I would rather return the result.
I've added a link on the previous article where the C++ covariant return type appears, and added a section explaining the problem, including a basic definition of the feature. I hope this will be clearer, now. Thanks,
I've never worked in an environment where a specification (or even understanding) of the problems at hand were complete enough at Day 1 that you could hope to write tests first. The only time I've done pure TDD is when working on "core" libraries (think std/boost). That said, writing tests with the final product (especially regression) are imperative to making forward progress, and I've never worked at a place without them.
I was actually meaning in the case where you needed to do some kind of work to be able to update to the newer standard, e.g. update tools / sdks etc. So you only put the work in to updating if it's possible (point a) and beneficial (point b), otherwise it's wasted effort. If your _current_ tools support the new standard already then the cost of updating is basically zero, so go for it!
Whatever the newest Visual Studio version supports is used. So currently, that's VS 2017.3. Then we find out what the minimal clang and gcc versions are that support the same set of features and use that on Linux and Mac.
Cool!
&gt; I've yet to find a method for testing computations that I'm satisfied with My experience is that this is because the domain space of testing computations is defined by the domain space of the input data, which scales out of control basically instantly (god help you if it's multidimensional). The best I've done is to write a bunch of checks on the data input and output and then to add test cases to bash all the "extremities" of the calculations (smallest values, largest, ect). Obviously you need to test whatever algorithmic constraints you have too (like a max_iterations if you're doing something iterative like newton's method). It's a horrible mesh of checks in my experience, many added only when things break. TL;DR - I'm in the same boat as you and wanted to commiserate. 
Thanks! I was trying to figure out this exact thing like a month or two ago.
Ah that makes more sense. Yeah I'm actually in that boat right now where my target platforms native/default shipped compiler supports C++11 but the compiler on the build machine doesn't!
Another reason is to write versions of standard containers which are as efficient as possible when compiled with optimisations disabled, which I believe is again a common game development thing. 
&gt; Basically just one thing: do our officially-supported compilers support the new version? Exactly, which means that the standard that introduces a feature is irrelevant. Only what features the compiler supports is relevant to users.
The code is C, not C++. There are like 2 posts in one: one about the chess code, and another for the Amazon stuff. Both are very interesting, but probably I'd split it into two, so readers can focus on the relevant part they are most interested in. In any case, good job, thanks for sharing.
I personally wait until the compiler at least gives the standard a real name (eg, --std=c++17 not c++1z). At that point, I figure it's stable enough and that I don't have to worry about things getting yanked or renamed as the standards process moves forward.
Depends on your plugin collection and which IDE features you care about.
You may want to consider this: http://www.copperspice.com/docs/cs_string/overview_why.html http://www.copperspice.com/pdf/CsString-CppNow-2017.pdf Trivia: there was a YT video of this presentation but now it is gooooone (I was thinking I am insane/melded multiple videos in my head, but google cache shows video existed)
Never made a `std::vector` of pointers? Sorta cheating to count generic code, but it does pop up a lot there where your values happen to be pointers.
Just for debug, really. Like, if you `-O3` your code, it's literally undebuggable. For a desktop application, turning off optimization means just a little more lag in a calculation, or maybe a longer calculation takes a minute. If you have that kind of unoptimized performance on a game, you can't play it well enough to actually trigger the bug in the first place. You literally cannot get to the first checkpoint in the first level of a game that takes half a second to render a frame.
You can put different levels of optimizations in your code though. You can undef debug before including the STL for example.
The proper support of C99+ preprocessor should be the "next big thing" after two-phase name lookup: https://www.reddit.com/r/cpp/comments/6zjub9/msvc_now_has_partial_twophase_name_lookup/dmwmgz4/
For sure. But schedules, man. Always schedules. :) I got out. I make biomedical research software. We do it right.
As an experienced professional I found the code in the blog excruciating. It's tempting to tear it apart. But as far as mewling beginner goes it's pretty good...especially given that most C++ curriculum is trash. I thus can't really fault it and you probably are indeed helping your classmates learn. I just wish you were learning a different style is all...don't let your professor dictate what you learn. Universities pay shit, the job is frustrating as fuck, and it's pretty thankless too. People who have a lot of choices don't tend to stick around and teach--you know unless you get one of those really rare ones that cares and can handle the fact that 90% of the students don't and not become engulfed in existential dread... Writing and tutoring are a couple of the best ways to learn, if you care more about getting things right than being right. I'd just suggest you branch out a bit beyond the box they're painting for you. Learn more current style and idioms. It's not at all useless to learn it the way you are, because a lot of code is terrible, ancient sludge, but C++ is way easier to learn and more powerful than that kind of thing really can be (why Z01dbrg is saying they would never use reference to pointer as argument anywhere).
Not expecting a quick miracle, so all good! I've just commented out the offending code for now until I hear anything :) Take care.
I think it also depends on the company, the bigger it gets the more the rules get stupid and immutable.
Could you expand on this because I don't think I see the relevance to the conversation.
Thanks for pointing this out!
Cool - just wanted to make sure.
You have never seen references on `std::unique_ptr`?
Thank you for the feedback. Like I said in the article: passing a pointer by reference or by value is something that I often see students segfault over because they pass it one way and interact with it the other way. I'm just hoping to clear things up. The application for this is narrow, because it's for students who are learning to roll their own pointer based data structures and need to manipulate them recursively. I definitely agree with your point on what we learn in school; and it's something that I'm actively trying to do. Perhaps this article is best left with the students who need to pass their coding exam where they have to manipulate a data structure in this way. They can't choose to use a more idiomatic, modern approach because the data structures course expects them to create their own pointer based data structures from scratch. No bells, no whistles. I'm curious to see your rendition of inserting to the end of a scratch LLL recursively because I feel the examples are quite simplified (obviously assuming not STL). Thank you again for taking the time to give me this feedback. I really do appreciate it. :)
So, i guess you can work around the lack of an explicit preprocessor symbol because you can easily write a test to check if u have one phase or two phase, and then use `static_assert` or `static if` as needed. Its almost the same as preprocessor symbol...
The table showing compiler support may be accurate for ICC, MSVC, clang, and GCC, but is *definitely* not up to date for PGI compiler and XL
C++17 outright **removes** features that were merely _deprecated_ in C++11. You'll want to make sure that none of your older libraries are hiding uses of `std::auto_ptr` or `register` or exception specifications...
(Assuming that you build with strict warnings...)
Just to note (and something I learned recently) is that GCC has on option for _exactly_ this use case: `-Og`. It enables all optimizations while working to maintain a sane debugging experience. I'm not a games developer, and I've no idea how effective it is, but it's something to try.
So this requires GMP? How is this different/better than Boost.Multiprecision?
Oh boy that 1) gave me PTSD :) Turns out nobody ever tries to benchmark malloc... and when they do, and get proven wrong, "it's about fragmentation"! In general blaming the STL and any style that isn't from the 90's is very strong among gamedevs. On one hand it's understandable, because until last gen C++11 was simply not a thing on any console. Clang really changed the ecosystem. 
&gt; In general blaming the STL and any style that isn't from the 90's is very strong among gamedevs. Yep! It's like they get paid by the line. "I hate debugging template code errors." Yeah? I hate debugging 35 classes of samey copypasta.
I'm working for a big company but our team is quite independent, we are roughly 10 devs but we are constantly upgrading to the latest release of GCC and Clang once a new version is available Sometimes there's small impact when upgrading to a major release (that are actually good, e.g. deprecated dynamic exception specification / register / auto_ptr, more error at compile time, enhanced warnings), but I think there's no good reason to postpone the upgrade Feature wise, we are not forcing ourselves to use everything that is new, but for instance moving to C++17 helped us to clean drastically most of our sfinae expression We also don't mind to use feature that are not yet officially standardized, the one that comes to mind is the concepts, but unfortunately as it's not supported by clang yet, we are not using it 
Or gmpxx.h which comes with gmp itself.
It's early for me here right now, but I'm fairly sure the OP was talking about raw pointers. Yes, I've seen references to smart pointers.
Vectors or pointers, sure. But I don't see where references to raw pointers comes in here.
I think I could probably count on one hand the number of times I've seen a case where pointer to pointer was actually required.
IMO it's better to just use a pimpl pattern here. "Saving a pimpl" is not typically very much, it's rare that some cross platform API like this is going to be used in a tight loop anyways, or that this difference will be noticeable. OTOH including `&lt;windows.h&gt;` is better done in a cpp file, because it leaks a bunch of macro definitions that will then pollute whatever cpp file includes it. Using a pimpl prevents that from causing a problem. It also reduces the size of all of the compilation units that include this one, since they can all avoid including windows then, which may speed up your compilation times significantly in a large project if this object is used in many places. ----------- If it's extremely important to you to avoid making a dynamic allocation and still use static polymorphism, there is an evil trick you can do. It's described by herb sutter here: http://www.gotw.ca/gotw/028.htm The article is a little out of date, but in C++11 the idea is, instead of "pointer to implementation", you create a buffer in the pImpl of fixed size and alignment, large enough for all of the platforms. In the cpp file for each platform, you static assert that this size and alignment is indeed enough. Then you use placement-new to construct the appropriate implementation in that buffer, forward all the calls to it, and destroy it explicitly as needed in the dtor. I've never seen anyone actually do this. Point is, C++ will let you do anything, even bad things.
That is actually a very good point. I'm a big fan of clean, readable code, and &amp;* is certainly one of those things that'd make me stop for a second and think "err, do what now?".
It's a wiki. Please update it if you have better information.
Just wondering about the terminology, how is it "header-only" if it requires linking the GMP library?
For someone like me, who has never programmed in JS, this seems like an interesting read.
Yes, it [depends](https://bluescarni.github.io/mppp/installation.html) on GMP. The [benchmarks](https://bluescarni.github.io/mppp/benchmarks.html) claim that mp++ is faster than Boost.Multiprecision. However the comparison is only done for the int classes. It would be nice to see a comparison of the rational classes as well. Also, I would like to see a comparison with GMP itself.
I'm asking actual TDD. Tests can be written with any approach to writing code. I am wondering how many are designing their code to be testable with a "pure" approach.
Looking forward to see this. I ran into this problem once when I ported some code from gcc to msvc and the new macro code that would run on both was double the size and an order of magnitude less readable. I try to avoid macros, but for stringification/compile time reflection there is not really a way around it at the moment.
&gt; How do you typically implement stateful objects with static polymorphism though? Light ifdef usage (localized to just a block of member variable declarations) as one option, if even necessary at all. A surprising amount can be done without even needing them. You can also just split up impl headers and depend on them in the same way you might do for the source files, and let your build system (CMake or whatever) sort it all out. In my experience, most of the platform-specific stuff can all just be stored as a `void*` and cast as necessary; a Win32 `HANDLE` is a `void*`, and you can stuff a POSIX file handle (`int`) into a `void*` via usage of `reinterpret_cast&lt;uintptr_t&amp;&gt;`, which covers a fairly large number of platform-specific types you might even care about in the first place. Some of those are abstracted via a single header/module, too. E.g., one might have a `PlatformCommon.h` that sets up type aliases for `HANDLE` vs `int` to a common `platform_handle` type or the like. Many stateful types are all about platform _implementation_ and don't even have data. An example that pops up for us here and there is naming things; our resources for various reasons have platform-specific naming conventions, so we have a handful of classes and free functions named things like `MakePathToThing(string_buffer&amp; buffer)` and then files like `makething_windows.cpp` and `makething_linux.cpp`. Implementation differences between platforms is far more frequent (in my experience, anyway) that anything stateful, especially with more modern libraries that already have the platform abstracted away.
Completely agree, I'm also against using it for every single variable. I reserve it for very special cases too.
 static inline uint32 read32(char *&amp;ptr) { uint32 value; std::memcpy(&amp;value, ptr, sizeof(uint32)); ptr += sizeof(uint32); return value; } That is handy for parsing binary buffers. On x86 that compiles into "mov" since on x86 unaligned loads are OK. It's just for convenience. I seen that pattern used in context like that. I wrap it with some extra abstraction myself along these lines: template &lt;typename Type&gt; class Pointer { protected: Type* p; public: Pointer(const Pointer&amp; pointer) : p(pointer.p) { } Pointer(Type* address) : p(address) { } ~Pointer() { } const Pointer&amp; operator = (Type* address) { p = address; return *this; } operator Type* () const { return p; } Type&amp; operator * () const { return *p; } Type* operator ++ () { return ++p; } Type* operator ++ (int) { return p++; } Type* operator -- () { return --p; } Type* operator -- (int) { return p--; } Type* operator += (size_t count) { p += count; return p; } Type* operator -= (size_t count) { p -= count; return p; } }; template &lt;typename Type&gt; ptrdiff_t operator - (const Pointer&lt;Type&gt;&amp; a, const Pointer&lt;Type&gt;&amp; b) { return static_cast&lt;const Type*&gt;(a) - static_cast&lt;const Type*&gt;(b); } class LittleEndianPointer : public Pointer&lt;uint8&gt; { protected: using Pointer&lt;uint8&gt;::p; public: LittleEndianPointer(uint8* address) : Pointer&lt;uint8&gt;(address) { } // read methods void read(uint8* dest, size_t count) { std::memcpy(dest, p, count); p += count; } uint8 read8() { return *p++; } uint16 read16() { uint16 value = uload16le(p); p += 2; return value; } uint32 read32() { uint32 value = uload32le(p); p += 4; return value; } uint64 read64() { uint64 value = uload64le(p); p += 8; return value; } half read16f() { Half value; value.u = uload16le(p); p += 2; return value; } float read32f() { Float value; value.u = uload32le(p); p += 4; return value; } double read64f() { Double value; value.u = uload64le(p); p += 8; return value; } // write methods void write(const uint8* source, size_t count) { std::memcpy(p, source, count); p += count; } void write8(uint8 value) { *p++ = value; } void write16(uint16 value) { ustore16le(p, value); p += 2; } void write32(uint32 value) { ustore32le(p, value); p += 4; } void write64(uint64 value) { ustore64le(p, value); p += 8; } void write16f(Half value) { ustore16le(p, value.u); p += 2; } void write32f(Float value) { ustore32le(p, value.u); p += 4; } void write64f(Double value) { ustore64le(p, value.u); p += 8; } }; // Usage: uint8 *ptr = ...; // memory mapped file, whatever... LittleEndianPointer p = ptr; float a = p.read32f(); uint32 b = p.read32(); int16 c0 = p.read16(); int16 c1 = p.read16(); p += 12; // skip 12 bytes ... The uload/ustore are not shown here but are as trivial as they sound. I also seen "byteswap-on-read" types being used in some places: template &lt;typename T&gt; class TypeCopy { protected: char data[sizeof(T)]; public: TypeCopy() = default; TypeCopy(const T &amp;value) { std::memcpy(data, &amp;value, sizeof(T)); } const TypeCopy&amp; operator = (const T &amp;value) { std::memcpy(data, &amp;value, sizeof(T)); return *this; } operator T () const { T temp; std::memcpy(&amp;temp, data, sizeof(T)); return temp; } }; template &lt;typename T&gt; class TypeSwap { protected: char data[sizeof(T)]; public: TypeSwap() = default; TypeSwap(const T &amp;value) { T temp = byteswap(value); std::memcpy(data, &amp;temp, sizeof(T)); } const TypeSwap&amp; operator = (const T &amp;value) { T temp = byteswap(value); std::memcpy(data, &amp;temp, sizeof(T)); return *this; } operator T () const { T temp; std::memcpy(&amp;temp, data, sizeof(T)); return byteswap(temp); } }; #ifdef XXX_LITTLE_ENDIAN typedef TypeCopy&lt;int16&gt; int16le; typedef TypeCopy&lt;int32&gt; int32le; typedef TypeCopy&lt;int64&gt; int64le; typedef TypeCopy&lt;uint16&gt; uint16le; typedef TypeCopy&lt;uint32&gt; uint32le; typedef TypeCopy&lt;uint64&gt; uint64le; typedef TypeCopy&lt;half&gt; float16le; typedef TypeCopy&lt;float&gt; float32le; typedef TypeCopy&lt;double&gt; float64le; typedef TypeSwap&lt;int16&gt; int16be; typedef TypeSwap&lt;int32&gt; int32be; typedef TypeSwap&lt;int64&gt; int64be; typedef TypeSwap&lt;uint16&gt; uint16be; typedef TypeSwap&lt;uint32&gt; uint32be; typedef TypeSwap&lt;uint64&gt; uint64be; typedef TypeSwap&lt;half&gt; float16be; typedef TypeSwap&lt;float&gt; float32be; typedef TypeSwap&lt;double&gt; float64be; #else typedef TypeSwap&lt;int16&gt; int16le; typedef TypeSwap&lt;int32&gt; int32le; typedef TypeSwap&lt;int64&gt; int64le; typedef TypeSwap&lt;uint16&gt; uint16le; typedef TypeSwap&lt;uint32&gt; uint32le; typedef TypeSwap&lt;uint64&gt; uint64le; typedef TypeSwap&lt;half&gt; float16le; typedef TypeSwap&lt;float&gt; float32le; typedef TypeSwap&lt;double&gt; float64le; typedef TypeCopy&lt;int16&gt; int16be; typedef TypeCopy&lt;int32&gt; int32be; typedef TypeCopy&lt;int64&gt; int64be; typedef TypeCopy&lt;uint16&gt; uint16be; typedef TypeCopy&lt;uint32&gt; uint32be; typedef TypeCopy&lt;uint64&gt; uint64be; typedef TypeCopy&lt;half&gt; float16be; typedef TypeCopy&lt;float&gt; float32be; typedef TypeCopy&lt;double&gt; float64be; #endif These are as easy to use as you would imagine: uint32le a; // "little endian uint32" uint32 b = a; // "native endian uint32" So.. the higher you go with abstraction the less you see *&amp;. The Pointer abstraction hides this completely as the pointer is member of the object and the member functions modify it through this but it's essentially the same thing just wrapped up nicely. 
3) is a faulty reason in a vast majority of cases. There are two good reasons to claim 3: * our existing code is exception-oblivious and we don't want to spend time changing that. * performance (we decided, through profiling and/or code size difference, that we can't do it) I am yet to see someone provide good data to support the latter. I have seen tons of complete tools with wrong data. Most common fool type is: "I took an existing codebase, turned exceptions on and it's bigger =&gt; exceptions are bad". Another common fool type replaces "return int" error handling with huge exception objects and complains. Third common fool throws an exception too often (e.g. 30% of passes through some piece of code throw). And so on...
a bit late, but navigating between files and navigating within a single file in sophisticated IDEs are equally trivial tasks (for humans) so it shouldn't be a reason for choosing a poor project structure
Well the description, in the beginning, is relevant to TDD as well. They don't just run writing tests for the sake of it. So you are writing unit tests regardless of the code design?
&gt; Verify that the reality meets the imagination in (3) A variation of "Virtual" TDD? :) So how is the verification process is done? Later written unit-tests or any form of testing?
So in your opinion, the lack of good tools is hindering C++ programmers from designing their code to be more testable? Were the SOLID principles more or less relevant when you weren't doing TDD?
That's also one of the reasons I wrote TDD and just writing unit-test. Tests in general and unit-tests, in particular, can be written at any point (their efficiency at those points is for another post).
Having successfully done a series of such refactorings in Python over the last year, I'd say that it's a wash for me. However, I'm pretty obsessive about unit tests, and those are what saved me. I cannot count the number of times I made what I was sure was a refactoring, ran the tests, and had a dozen failures...
Actually, those tasks are not equivalent. Remember, each CPP file builds its own copy of the universe, including each header file it needs. Consequently, navigation into many CPP files would require you to rebuild the world once for every CPP file to figure out whether that thing there is, say, a valid override or not. By reducing the number of files, it makes it easier for the IDE to resolve the includes just once, then work on the basis of that. The poor project structure is, unfortunately, predicated on the outdated compilation model. Once we get modules, things will be much simpler (C#/Java-like).
I believe they use flag to disable exceptions, so there are less stuff the program needs to do behind the scene (like when you enter a `try`block), and further optimizations should be available to the compiler (all you code become defacto noexcept).
 Those who live in the Qt/C++ world can use [tasks](https://github.com/mhogomchungu/tasks) library to do a bit of fancy things with futures like combining multiple futures into a single future and have them run sequentially(with .get()/.queue()) or concurrently(with .then()/.await()).
Eh... disabling exceptions on an existing codebase that uses STL, or the std::string, or the default operator new, leads to a faulty codebase, to one huge UB landmine. Why? Because all those things can throw, and there's no code supporting the throw. OTOH, enabling exceptions on a de-facto noexcept codebase produces exception-handling overhead where none is needed. Note that with the current exceptions implementations, there is no runtime cost to pay when an exception is not thrown. There is, however, a price to pay only when an exception is thrown. But for that, we are entering into the realm of *extreme* HPC. I posit: the amount of software that cares how fast some part of the software *does not work* is extremely small (I equate "exception is thrown" with "something does not work"). And for that small part, the performance hit can (and should) be lowered by changing the code to not throw for biggest offenders, until the performance is acceptable again. Code that has hard latency requirements on how quickly some action should be performed upon an error (e.g. upon a problem, stop irradiating the patient immediately, don't wait for the stack to unwind) can be rewritten to first perform that action, then unwind the stack. There is cost to pay for the code size, but to accurately measure that, one needs to have a functionally equivalent program without exceptions, and with. That is pretty much a rewrite of the whole thing. Hardly anyone does that so data is scarce. My guess that the difference on common platforms, when doing that, is around 5%[1]. I did partial rewrites, from error-return to "standard C++" with exceptions, STL and whatnot, and I even had cases of a reduction in code size (e.g. std::sort beats qsort). [1] 47.89 of all percentages found on the internet are invented on the spot :-).
&gt; STL, or the std::string, or the default operator new, Note that the codebase we're speaking about has no STL, thus no std::string, and may have custom-made `new` operator. Because of that, even if something could throw, they may not mind that "an exception = a crash" for the very few cases it will happen. Also, according to [this SO answer](https://stackoverflow.com/questions/13835817/are-exceptions-in-c-really-slow), the 0-cost model uses RTTI, which is also deactivated in (large C++) gamedev codebases. Last, the "0-cost model" may not be totally 0-cost, even if you don't throw at all. &gt; one needs to have a functionally equivalent program without exceptions, and with. That is pretty much a rewrite of the whole thing. Well, they already have written the latter. Even if exceptions were truly 0-cost, I doubt they'll rewrite to go back to exceptions - unless there is a massive codebase change.
Bah, this is a variant of `something**` parameters, not uncommon in some C circles. Typical (albeit rare and arguably poor coding) use case is a "side" output from a function, which should be on the heap, e.g. stuff get_stuff(params, side_info*&amp; i) { stuff result = get_stuff_impl(params, locals); i = new side_info(stuff, params, locals); return stuff; } However, in C++11, a reference to `unique_ptr` is better.
Could you elaborate on this?
Heh, you're right, though that's really three lines.
this means that this library isnt an ideal choice for a commercial project becauae of the gmp licenses
Yeah, those who already have exceptions-oblivious codebase are beyond redemption, I even say a variant of that in my first post here :-) I don't quite see why you mention RTTI. Yes, a typical exceptions implementation will use it (I don't know why 0-cost matter for that), but it's not as if that changes the meaning of the existing code. It will not be used otherwise, and one could even turn it off for translation units who don't need it, making it smaller. 
I have received an error: warning C4199: two-phase name lookup is not supported for C++/CLI, C++/CX, or OpenMP; use /Zc:twoPhase-. Do you have plans to support OpenMP ?
That doesn't guarantee anything. The `-std=c++17` option has worked for GCC for some time (with identical meaning to `-std=c++1z`), but support is still experimental. Specifically, library features such as `std::variant` and `std::optional` could see ABI-breaking changes before C++17 support is declared stable.
&gt; It enables all optimizations while working to maintain a sane debugging experience. It enables _some_ optimizations, as long as they don't make debugging too hard. So for example you still get inlining of small functions that you wouldn't want to step into in the debugger anyway, but you don't get completely unrecognizable generated code.
You should consider if you need ABI stability. GCC's C++17 support is still experimental, so for example `std::variant` and `std::optional` could be changed in incompatible ways between GCC 7 and GCC 8, or between GCC 8 and GCC 9. If you compiled code today using those types and didn't recompile it again next year you might find it doesn't work. If you want to be an early adopter you need to understand the risks.
No mention of smart pointers? This is not even good introductory material.
At this standard, it may count as a negative if someone bothers to read it.
I searched a bit more: I guess RTTI is needed to call the right destructor when an exception accurs. Regardless, even with the poorly-name "0-cost exceptions" implementations, entering a scope require a push_back on a list (to call for the destructors when an exceptions occurs). RTTI makes the .rdata bigger. Gamedevs didn't want any of those, so they get rid of them.
I think what /u/Plorkyeran meant was that a vector of pointers (std::vector&lt;Foo*&gt;) has its whole interface littered with functions that take or return references to pointers. And the same is true for a lot of other generic functions or types (think e.g. std::swap, std::advance aso.) 
operator[] returns a reference to pointer when the vector holds pointers.
Is it the concept of writing unit test beforehand that you don't find valuable?
Many methods of std::vector take or return references to elements. In the case of vector of pointers that would be references to pointers. 
That only matters when your toolset provider removes them.
Never used the Win32 API? I suppose that's C though....
Somehow I feel the only person who does "real TDD" is Uncle Bob ;)
RTTI is needed to find the right catch (thrown exception must match the type in the catch). I don't see what destructors have to do there? &gt; Regardless, even with the poorly-name "0-cost exceptions" implementations, entering a scope require a push_back on a list (to call for the destructors when an exceptions occurs). I hope you don't mean as in `list::push_back`? Because that ain't happening. &gt;RTTI makes the .rdata bigger. Yes, RTTI takes space. You know what else takes space? Incessant `if` statements of error-return code, which compiler turns into comparisons and jumps. So the interesting question is which is bigger; mere presence of RTTI means nothing. See, this is why I say that people fail to look at this with correct data.
I don't hate life that much...
&gt; I don't see what destructors have to do there? My bad, I thought of RTTI for the data that has to be cleanup-while knowing it wasn't that. &gt; I hope you don't mean as in list::push_back? Because that ain't happening. Not a `std::list::push_back`; [from this link](https://monoinfinito.wordpress.com/2013/02/05/c-exceptions-under-the-hood/), the unwinder needs to use extra data, which has to be written prior to the throw. For the RTTI vs if place: you're right that it needs more testing. There may be additional reason against RTTI too, like performance. (I man not knowledgeable on the subject, I only picked reasons I had read)
I made it two because I couldn't be arsed to press space bar eight times to indent it properly on reddit :P
Hello, mp++ author here. The library was created with computer algebra systems in mind. Its main goal is to optimize performance with small operands, while at the same time having the option to seamlessly transition into full multiprecision mode. In computer algebra, one often has to deal with objects containing large collections of integers/rationals which often have small values (e.g., coefficients in a polynomial, or entries in a matrix), and for which full multiprecision objects incur into noticeable performance penalties (e.g., due to heap allocation). So, in a nutshell, the library was born originally as a GMP mpz/mpq wrapper with 1) small object optimisation, 2) fast inline re-implementation of basic arithmetic primitives for small operands (1 and 2 limbs currently). I am currently adding support for a multiprecision MPFR wrapper, and eventually I'd like to have complex numbers and intervals as well (but that's still quite far away). 
Well, the library itself is header-only, but its main dependency (GMP) is not. So maybe I should reword that part of the documentation.
gmpxx.h does not do small object optimisation. For the use case for which mp++ was originally created (i.e., computer algebra systems handling large collections of integers/rationals) gmpxx.h does not offer much over plain gmp.h (e.g., you are still doing a heap allocation for every object you create).
I plan to add benchmarks for the rationals. I hope to achieve some speedup there as well, as I re-implemented all the arithmetic primitives with a particular emphasis on optimising the case when the operands are actually integers (i.e., denominator is 1). The benchmarks are actually comparing with GMP, via the mpz_int Boost wrapper. The Boost wrapper is used only for handling the construction/deletion of the GMP objects, but all arithmetics in the benchmarks is done via direct calls to the GMP API.
GMP has been included by Wolfram in mathematica for the last couple of decades, as far as I know: http://www.wolfram.com/legal/third-party-licenses/gmp.html I believe maple also includes GMP, so I don't see exactly how GMP's license is an obstacle to commercial usage. 
thanks for the info, thas very interestig! i dont understand all the details i just saw it was lgpl3 and gpl2 and guessed there were some drawbacks.
But do you "write" your users before writing your code ;)
No problem. GMP is LGPL, so you cannot distribute a modified version of it under a different license, but you can use &amp; distribute it in commercial projects.
&gt; One-size-fits-all thinking is bound to fail. Come now. Who can blame people who want a simple rule that just works? What other languages have two different ways of constructing variables - two different ways that you need to think fairly deeply to choose between, and which can lead to obscure issues where the code compiles but does something unexpected if you make the right choice? Can you imagine trying to explain this to beginner or intermediate C++ programmers? I have done it, and if you want to do it so people understand you, and giving people enough information to make an informed decision without overloading them, it takes a good half hour or more. I love C++, but this one problem (which is unavoidable because of backward compatibility) is a real pain point.
Slacker! :-D
&gt;They don't just run writing tests for the sake of it. ? &gt;So you are writing unit tests regardless of the code design? More like, tests are added after the final design is complete. 
RTTI data has a tree structure that reflects the is-a relationship between types (in this case, between classes in exception hierarchy). &gt;the unwinder needs to use extra data, which has to be written prior to the throw. You misunderstood this. the data is written by the compiler (no runtime price). As the article says: &gt;For each catch statement, the **compiler** will write some special information after the method‚Äôs body, a table of exceptions this method can catch and a cleanup table So when an exception is thrown, the runtime has to traverse the stack to find a catch clause that matches the thrown exception. So this is where the performance price is really paid, typically because this data is not often in any cache. I would guess this is, by a long far, the biggest part of the performance hit (but I never measured). Then, the above tree has to be traversed to see if there is a "match" between the thrown type and the catch. This is also a price to pay. However... All this should be offset with the following observation: * an exception means "an error"; how important is it that "doesn't work" happens fast? * if "doesn't work" happens too often, chances are, there is a problem with the code. Even if the code is subject to an attack (it doesn't work because something is pounding it intentionally). If that is a possibility, then surely the code should stop processing requests to it that fail too often. tl;dr: performance is a complex subject. I seriously doubt that e.g. the game industry at large actually has valid comparative numbers that show why exceptions can't be used. I rather think that its dislike for exceptions is due to a bunch of early attempts with suboptimal implementations, bad coding, misconceptions and, the biggest of all (and only valid one), inertia. BTW... People who argue that exceptions are fine, and make measurements, fall into a trap of throwing/catching a lot and measuring that. This often gives very good numbers. This is misleading because their use case is not typical. When a lot of exceptions are thrown, the RTTI data is in white-hot cache and is retrieved quickly, so exceptions look faster than they are in reality.
Well then, your speedup is great and I become more and more interested in how you achieved this. I have seen below that you want to implement a wrapper for MPFR as well. Have you seen the one by [Pavel Holoborodko](http://www.holoborodko.com/pavel/mpfr/)?
I indeed misunderstood who had to write the data; there is likely just offset + some pointer arithmetic. &gt; So this is where the performance price is really paid, typically because this data is not often in any cache From what I've read: yup, it's cache issues (I -wrongly- guessed it was writing to a non-cache-friendly location) &gt; an exception means "an error"; how important is it that "doesn't work" happens fast? I also fully agree that exceptions aren't supposed to fire often, especially for such cases. &gt; early attempts with suboptimal implementations, bad coding, misconceptions and, the biggest of all (and only valid one), inertia. Yeah, same: some custom compilers for previous consoles were likely less than ideals. I would be really interested in some benchmarks for large codebase to see how it really impact things.
That's a shame. As a rule I don't even touch something if it comes under the GPL license.
&gt; They don't just run writing tests for the sake of it. There are some misconceptions that TDD is just about writing tests with some imaginary goal of a 100% coverage or writing a test just to go through the motions. I'm not saying you're implying that, of course, but before writing a test you think about the design and specs as well as in other approaches to writing code, so that's what I meant by what I wrote. :) &gt; More like, tests are added after the final design is complete. What did you do when you encountered "untestable" code?
I'm not the author, but curious about why you will not touch anything that is under GPL. Too restrictive?
&gt;There are some misconceptions that TDD is just about writing tests with some imaginary goal of a 100% coverage or writing a test just to go through the motions I'm a TDD noob, but I don't think anyone is arbitrarily shooting for 100% coverage. I'm more saying that the projects I've worked on tend to be quite iterative; where the engineers work with the user/client to define the requirements, a prototype is delivered and then feedback is given and iteration occurs. Since the actual implementation details help define the design (in my mind/experience) it's hard for me to think of testing first. &gt;What did you do when you encountered "untestable" code? What's untestable code?
Pffft, 2? We have 4! (direct-, list-, copy-, and copy-list-initialization)
Nice read, you have my upvote. Unfortunately I never seem able to bridge the gap between these simple explanations and actual monad usage in Haskell, with bindings and side-effects and whatnot. I lose something from here to there and I find myself completely lost.
This article isn't about monads. Those are a different kind of entity than monoids (the topic of the article). The latter are conceptually much simpler to grasp and appear far more often. For instance, unsigned integers with "+" and booleans with "&amp;&amp;", "||", "==", or "!=" all form monoids.
monads are just monoids in the category of endofunctors. 
Yeah, what's the problem?
If only `std::initializer_list` didn't exist...
Yes. Despite claims of "freedom", the GPL and LGPL are not free at all and impose burdensome restrictions. Compare with the Boost license, which is a truly free license. This goes for my own libraries as well as the libraries I use - I will never release something under the GPL, nor will I ever write code that uses a copylefted library.
&gt; so there are less stuff the program needs to do behind the scene (like when you enter a `try` block) If your program actually does stuff behind the scenes when you enter a `try` block maybe you should use a compiler that wasn't written by cavemen.
&gt; Also, according to this SO answer, the 0-cost model uses RTTI, which is also deactivated in (large C++) gamedev codebases. Except `-fno-rtti` only disables RTTI insamuch as it isn't needed by the exception handling mechanism: &gt;Note that exception handling uses the same information, but G++ generates it as needed. [Source.](https://gcc.gnu.org/onlinedocs/gcc-7.2.0/gcc/C_002b_002b-Dialect-Options.html)
Sorry messed up my example. Fast typing, sloppy editing. Add implementations of `moo` also to bar1 and bar2, then it breaks: class bar1 : public clone_inherit&lt;abstract_method&lt;bar1&gt;, virtual_inherit_from&lt;cloneable&gt;, virtual_inherit_from&lt;foo&gt;&gt; { public: virtual void moo() {} }; class bar2 : public clone_inherit&lt;abstract_method&lt;bar2&gt;, virtual_inherit_from&lt;cloneable&gt;, virtual_inherit_from&lt;foo&gt;&gt; { public: virtual void moo() {} }; The rest of the code stays the same. [Demo](https://godbolt.org/g/unKchz)
I didn't feel that my comment should bring out the big guns, so to speak, with that particular fact, but I approve since it's technically correct ;)
I was going to ask for benchmarks relative to piranha, but I noticed you're the author of both! Bravo!
I might add tests to help develop the functionality that I'm tasked with implementing. But there is not place reserved for checking such tests into the main project. So they just stay on my machine which eventually get's lost. So it's not so much that the tests are actually deleted. It's more that these tests are considered a sort of a personal crutch required to compensate for my inadequacies as a programmer. Since the other programmers don't see themselves as suffering from these inadequacies, they don't need to make a place for the tests in the repo. And besides, it's extra work to do so. And half the time when a test fails, it's due to an error in the test itself. This last statement might be surprising to the advocates of testing. But it's true. I turns out that often its obvious to everyone what the functionality should be and so that writing a test would be a trivial exercise. But in practice it often occurs that not everyone is on the same page as to what a function should do. There are often non obvious differences in assumptions about pre-conditions, corner cases, error/exception handling and things like that. Often when there is a "bug in the test" it's really a manifestation of the above problem. Finally - and this is a big one - tests uncover bugs. And fixing bugs is more work - work that isn't appreciated. Fixing show a show stopping bug in shipped product gets one a lot of credit. Including a test which prevents the bug from ever getting into the product in the first place gets you no credit at all. You're tagged as a programmer who takes too long to get the product out. Since the product doesn't fail, now you've created the perception that the job is easy. The more you ignore professional standards, the more likely you are to get "promoted" to management at a younger age.
tl; dr, a set that is closed under some binary associative operator and has an identity element. Your terminology for describing the identity element ("neutral") is pretty weird and confusing. Just use the word "identity element". Another piece of standard terminology is that when an n-ary operator (binary or not) has co-domain which is the same as the domain for each of its inputs, we say that the set in question (the one that is equal to both the domain and co-domain) is closed under that operation. Some good counter-examples might be useful here too. For example, non-negative integers, subtraction, and zero, do not form a Monoid because non-negatives are not closed under subtraction. This fact explains why unsigned has major issues as a stand-in for a non-negative integer (I've planned to write about this for a while, haven't gotten around to it).
Anyone who is targetting emscripten and asm.js / the web is going to be very reluctant to enable exceptions. It is very difficult to support C++ exceptions in a javascript environment. They are not zero cost and cause massively increased code sizes, which hurts download times and JIT compilation times. It is by default off for a reason with em++. People need to stop minimizing the large variety of applications in which it's not appropriate to use C++ exceptions at all. This conclusion has been reached time and again by experts in many different areas -- not only for games. It would be nice if everyone could use all the language features all the time, but it really isn't the case and it's not productive to pretend otherwise.
&gt; It's easy to be cleanly-designed software when you're brand-new. On the other hand, interacting with aging codebases is paying well :D
Wasn't there a proposal for adding multiprecision integers to C++? Any update on that?
&gt; a set that is closed under some binary operator and has an identity element. Crucially, an *associative* binary operator.
What does transpilation to javascript has to do with this?! As for the conclusion of experts: show data. A long time ago, I have been looking quite hard, and what I could find was very underwhelming.
Good point, thanks. 
"Neutral element" is almost as popular as "identity element", especially in fields like abstract algebra. It is certainly not weird. The post also references the term closure with a link to its wikipedia page.
Googling around, my basic conclusion is that in abstract algebra specifically, neutral is slightly less popular than identity; I hadn't heard the term used but most of my experience with modern algebra is through physics. In other fields, "neutral element" is basically non-existent. You will not with remote comparable frequency hear the neutrality matrix in linear algebra, 0 as the additive neutrality in discussions of the field axioms during analysis, etc. So if you want to communicate with a broad audience, my point stands. I find it kind of funny that abstract algebra decided to give a second name to the exact same concept that already existed in so many other fields of math. Wonder what the historical reason for it is. The term closure was briefly mentioned later on, not at the beginning, but fair enough.
In fact i have never heard "identity element" for the neutral element when I studied algebra so I wonder where you got this from. I actually would call the identity matrix the neutral element with respect to matrix multiplication.
Sure, but monoids come from abstract algebra. It's a bit annoying that there are different terms for the same thing, but you could make the same argument about "abelian" or how some people use "covariance" and "invariance" interchangeably in some cases. In this case I don't think it's a big deal since he defines the term anyway. You'll understand regardless of which term you are comfortable with. Trying to require someone familiar with one to use the other is a bit unreasonable, in my opinion.
You're free to call it whatever you want, but as I said, the entire linear algebra world calls it the identity matrix. You can also google variations of identity and neutrality and you'll see that the difference in popularity is vast. Like I said, the term neutral is almost entirely confined to modern algebra. I've never once had someone fail to understand me when I said identity element, and never had someone say neutral element to me. This is in 4 years of math and 10 years of theoretical physics.
Is relative path appending associative though? /foo/bar is different from /bar/foo
Yes. Associative is not the same as commutative. 
Not trying to require anything, it's just feedback. If the author wants to make the article as easy to read as possible for as many people as possible, using identity will provide a small improvement, that's all. Totally their call. And they would *still* be using the more common term in abstract algebra, so I don't really see the relevance of your first sentence. Would be more sympathetic to that point if abstract algebra sources overwhelming use neutral over identity, which isn't the case.
BTW Google has a cool idea in their code style: no non const reference arguments(obviously ignoring std functions), so at the call site you see if the variable is maybe modified by function.
Yes, I confused the two.
Is this recursive? Seems like a pretty major limitation if it's not. Would also be better to save having to write out the member variables twice: struct Foo { DECLARE_VARS((int, x), (double, y), (string, z)); }; I also don't know why you have an inner macro as well as an outer one. The user shouldn't have to write JT_MEMBER over and over.
Yes, Microsoft's business model has proven to be successful. 
Its recursive. json_tools should be able to map any valid Json to a type hierarchy. That is a pretty nice idea, but when I started this I liked the idea that I could add the meta object at the end, ie. if I already had a struct and I wanted to parse Json into it, I could just add some stuff to the end of the struct definition. Also I like the look'n feel that the struct still looks like a struct; although with some ugly macro stuff at the end. The plain old struct part is in charge of the data, and the size of the struct and its layout is like it always has been, and then the macro stuff describes how to parse/serialise to Json. There are also additional macros like: JT_MEMBER_ALIASES(member, ...) that allows for several Json names to map to a member. 
During my math major, we mostly used the term "√©l√©ment neutre", which is french for "neutral element". I guess it depends on where you learned your algebra.
A verbose way to go about describing a supercategory of burritos, but overall worth a read.
https://www.reddit.com/r/programming/comments/4gtlfz/xoroshiro128_the_fastest_fullperiod_pseudorandom/
Nah, more doing the full design of the object that way. For some simple functionality I might actually write the test in advance if I've got the test code open in an editor anyway. I just usually have enough of an idea what I want my object to do and their behaviors are simple enough that I'll usually just write the object API first. The test is usually written before the first time I compile the object, though, and the test may drive me to add additional methods I hadn't initially considered, so it's still pretty close. If my objects are sufficiently complex, I still prefer to do my initial design on paper. Holdover from learning to program in the '80's, I guess. My instructors were from the punch card era and thought we should still act like it'd take overnight to get a list of errors from the compiler. I don't know if that attitude is still prevalent when they're teaching the kids these days. I don't see any on my lawn, so I assume they're not waiting around for someone to run their program deck through the mainframe.
Why is the existence of a neutral element important? The article doesn't seem to make any use of it.
Thanks :) I've spent quite some time tweaking the implementation of basic arithmetic for few limbs. I think the good performance is due to a combination of inlining, branch avoidance and use of compiler intrinsics. We can have a chat via PMs if you are interested in the details. I am aware of the MPFR wrapper from Pavel Holoborodko. In fact, the mp++ wrapper is going to be similar in spirit (e.g., it will choose the precision of a binary operation based on the precision of the operands, and, generally speaking, the wrapper will behave like a C++ floating point type whose precision is a runtime property of the specific instances of the class). Something that, I believe, will be new wrt Pavel's wrapper is that mp++'s will be fully move aware, meaning that it will be able to reduce drastically memory allocations in complex expressions by re-using the storage space of temporaries. EDIT: forgot to mention I recently added a 128bit floating-point wrapper as well: https://bluescarni.github.io/mppp/real128.html (it's not in any released version of mp++ yet, though)
Cheers! Didn't imagine I would find here someone aware of Piranha :) Might I ask if you are using it yourself and, in such a case, what you are using it for?
The last one I read was this: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3965.html I don't like that they allow for expression templates in the implementation... I believe there should be a way to disable them if needed.
Failure doesn't involve blame. What's wrong with warning about impending failure? Other languages such as JavaScript, Java, and Python have a built-in array, hashmap, or list type serving as a catch-all for aggregates and as an intermediary for data being passed into a structure. Converting from a built-in aggregate to a structure over class datatypes with constructors is tricky in any language. If we're comparing C++ with other languages, then stack vs. heap needs to be factored in as well. Only C, C++, and a few languages with heavy C++ influence (D, Rust) offer this choice. This also often results in code that compiles and does the wrong thing. C++ provides the *choice* to try and avoid separately allocating each object behind a pointer, and to try and avoid unconstructed states by focusing on the constructor instead of setter functions. If you really want to KISS, instead, you can put a default constructor in everything (invariants be damned) and always construct with `make_shared&lt;T&gt;()`. &gt; two different ways that you need to think fairly deeply to choose between Use braces for data aggregation. Use parens otherwise, when something in there is only like a function parameter. Use `=` when the object behaves as a pure value. Avoid it when it has subscribers or side effects. These things are easy to summarize. I'm referencing deeper concepts‚Ä¶ but these are really important things for all programmers to learn, and to keep in mind whether the language demands it or not.
Didn't say it is not called identity matrix though...
How did anyone manage to write software before the invention of nonsense? 
I see IBM has posted announcements and download links of XL 13.1.5 for Linux and 13.1.3 for AIX. I can't seem to find clear documentation of what improvements have been made in standards support in those (seemingly minor) releases. A little help please?
I've now done the best I can for PGI. It's now at least clear that current versions are complete for C++14, and part-way done for C++17.
I agree with everything you say. In order to get best possible performance, yes, you are going to have be much more subtle with things like constructors and copies. There will be non-obvious things you'll have to learn to avoid shooting yourself in the foot. I just think you should be a little easier on people looking for clear rules they can apply to constructors, is all. :-)
OK, so I found [this](https://www.ibm.com/support/knowledgecenter/SSXVZZ_13.1.5/com.ibm.xlcpp1315.lelinux.doc/getstart/new_features.html) from IBM, which suggests that the listings are basically correct, with possibly some better feature portability to different platforms the compiler supports in 13.1.3, but no additional features.
That's probably because the GPL was born to protect the user's rights, not the developer's
Indeed, you're right. IMHO, this is the same problem happening with this simplified code: struct foo { virtual void moo() {} }; struct bar1 : virtual foo { virtual void moo() {} }; struct bar2 : virtual foo { virtual void moo() {} }; struct concrete : bar1, bar2 { //virtual void moo() {} // if we un-comment this line, compilation succeeds }; (Amusing fact: If you remove moo from foo, then the error disappears, until you actually call the function) My understanding is that, in the case of clone_inherit, as it brings together two classes, bar1 and bar2, both overriding the same method moo from foo, but without overriding it itself, the compilation fails. I have no solution to that problem. If I find none in the next few days (and, by the look of it, I don't expect to), I will add, with your permission (and credit, of course!), a section on this specific problem. Thanks for the review! :-) 
Why not, optionally, have a parameter to provide the constructing lambda? In the case where the base class would not be default constructible.
When two-phase becomes a default feature (when /permissive- becomes default) all C++ features should be supported. Right now we‚Äôve scoped the problem to standard-C++ code without extensions.
I have extra info I can aggregate for 14.1 (though it‚Äôs currently in beta). Thanks for updating PGI. I wasn‚Äôt aware cppreference was wiki style ‚Äî my bad. 
Try VS2017 15.4.0 Preview 2.0 ;-)
&gt; What does transpilation to javascript has to do with this?! Well for instance I am working on a game project which targets 6 platforms: Windows, Linux, OS X, Android, iOS, Emscripten / asm.js. I only want to write code and use libraries that will run identically and ideally perform just as well on all of these platforms. Other people want to write code that targets game consoles. On some game consoles, exceptions simply aren't supported. https://softwareengineering.stackexchange.com/questions/113479/are-there-any-real-world-cases-for-c-without-exceptions If you want to target even more rudimentary things like arduinos -- they typically don't enable exceptions, because it's a bad idea to use them. In a much older but very well known example, there's the JSF air vehicle C++ coding standards, which also prohibit exceptions. This was written by a committee including Bjarne Stroustrup www.stroustrup.com/JSF-AV-rules.pdf They prohibit C++ exceptions because the application requires "real time guarantees" on the responsiveness of software -- if the program doesn't come back with an answer in X milliseconds, people can die. They found that when exceptions enter the picture they can't meet the running time guarantees with certainty, so they got prohibited. Even when doing simple things like, cross compiling a program for Windows using mingw, you can easily end up with exceptions that are not zero-cost. Mingw has several paradigms for exception support on windows: SJLJ, and Dwarf. MSVC uses "SEH", microsoft's structured exception handling. SEH uses support from the OS, and unfortunately gcc doesn't support SEH. The SJLJ option means "set jump long jump". That means whenever you enter a "try" block, the compiler will insert a call to setjmp. And a throw causes a call to longjmp. This means you do not have zero cost exceptions! Every time you `try` there is some overhead. The alternative, Dwarf, is zero cost to the best of my knowledge, but it doesn't support a 64 bit target, only 32 bit. So, many modern applications built with mingw use SJLJ and do not have zero-cost exceptions. There is a discussion here: https://stackoverflow.com/questions/15670169/what-is-difference-between-sjlj-vs-dwarf-vs-seh Edit: Well, in the course of researching this I now learned that as of 2016, mingw supports SEH. So that example is moot. But still, part of the point is that it took a really long time to get there. I think it's really great that we have zero cost exceptions for the most popular architecures, and the most popular build configurations. But I think it's also great that many libraries support building with `-fno-exceptions`. It's really nice when I can use the same stuff on all projects and all platforms and reason about the peformance everywhere in exactly the same way. It would not be good people if stopped developing libraries with `-fno-exceptions` support in mind because they thought it was no longer important, because they thought "zero cost exceptions" is the case everywhere, when it isn't true. All I can say is that in many of these "off the beaten path" project and targets, C++ exception support is disabled, and it's not considered controversial or a mistake. The people who worked on these platforms know how hard it is to get it to work with zero cost in their situation.
Oh, that's easy -- we just invented whatever nonsense we needed at the time. Only half sarcastic ;) 
?! The obligations under LGPL are the same whether you modify the code or not. The "LGPL [somethingsomething] modified [...]" is almost always wrong, and it's a myth that just won't die.
Probably because [semigroup](https://en.m.wikipedia.org/wiki/Semigroup) (i.e. monoid without identity) doesn't sound as cool as monoid.
[removed]
That's probably the easiest example to explain the difference. The first time you see non-commutative operations are for matrix multiply, and it's much harder to see it than with strings (unless you do matrix multiplies in your head).
In the vfx industry we have the 'VFX platform" which is a multi company agreed set of compiler and library versions. In 2018 we move from gcc 4.8.3 to gcc 5.3.1. Ground breaking. /s. At least well be able to use c++14 then. We just miss out on compiler bug fixes and optimizations.
You learned matrix multiplication before subtraction?
This wall of text contains only hand-waving, appeal to authority and **very** old decisions or a **very** particular target (e.g. the JSF pdf or an arduino). Do you know that a lot of Android is **a lot of** Java, which is **way** more expensive than C++ when it comes to exceptions? And I would *really** like to see how **any** library "run identically and ideally perform just as well" across e.g. desktop Linux and emscripten. I find this part quite preposterous. It tells me, in fact, that you really don't know much about the actual timings at all. tl;dr you have no data either. Edit: what are the -f-no-exceptions libraries that you use? Would like an experiment, think I can surprise you.
Good point, I didn't think about it because subtraction isn't usually defined as a real operation in this field but rather as the unary operator - and an addition.
It doesn't lead to undefined behaviour at all. Please don't confuse newbies.
Sorry to come back so soon, but I think I'm hitting an issue where the resolution order of template functions in one of my projects is behaving strangely since updating to 15.3 - ignoring specializations/overloads in favour of the base template function instead. I'll keep playing around with this, but can you think of a nice way to narrow it in on recent MSVC changes? I'll try with and without /permissive- for example to see if it's the changes to two-phase look-up. Having trouble reproducing it in a simple project - might be a combination of factors (eugh). If I can get a solid A/B test from before the MSVC changes and afterwards in a repo of mine, can I send that through somewhere? I'll do my best to make sure it's not just me of course; don't want to waste anyone's time. If there's an easy way to have 15.2 and 15.3 installed, I could compare the results a bit easier.
By "untestable" I mean code that is not designed to be testable. I would assume that when you write the test first you then design the code to satisfy the test, but it's not always possible to write tests to an already written code (e.g. legacy code).
OK, not entirely, pedantically accurate but I see what I should have seen. Yeah, I wouldn't count generic code.
It allows you to build it in a loop starting with the neutral one: T neutral; while (!done) compose(neutral, cur); Or take C++17's fold expressions: If you fold an empty pack with +,*,&amp;,|,&amp;&amp;,||,comma, the result used to be a given value - the neutral element. But since you can overload operators in C++ and change the mathematical semantics, this was dropped for all but &amp;&amp;,|| and comma.
&gt; try { &gt; return functionMap[className](); &gt; } catch (...) { &gt; return nullptr; &gt; } Eh?
On line 33 it would be better to use `map.at(key)` function to access the map, since the `[]` subscript operator will add an element if it doesn't exist. Where as `at` will throw if it doesn't exist.
so-so... 3/10 :-)
Yes, most compilers don't actually remove then, just turns them into a warning.
Yes this is exactly the problem. The difference with the simple code is that you are supposed to override `moo` in `concrete`, and if you do, the problem disappears, as it should. But in the full example you need to override `moo` in the generic template (i.e. in the class that first inherits both `bar1` and `bar2`), and you cannot, because the template has no idea about `moo`. There is no solution to this problem that I know of. Edit: yes please add a section.
I did a math degree as well and when I took abstract algebra it was always called the identity element. Not that it is definitive, but [wikipedia seems to agree ...](https://en.wikipedia.org/wiki/Group_\(mathematics\))
First, ``` try { return functionMap[className](); }catch (...) { return nullptr; } ``` could be: ``` try { return functionMap.at(className)(); } catch ( const std::out_of_range&amp; ) { return nullptr; } ``` Using `catch(...)` in library code is very very bad, because it hides errors when it shouldn't (for example, exceptions in the constructor of the built type). Second, the way you wrote the API forces the client code to check for nulls. When it comes to APIs, this is bad: it will force you to check the result for nullptr wherever you use the it. Consider throwing an exception instead of returning a nullptr. This will allow you to write error handling for the failure case only once, or multiple times, as necessary (but because you can handle exceptions down the stack, you will not _have to_ do so at every calling point). Third, the name used to index types is intrusive (you need to define a class hierarchy inheriting from stringable and BaseClass) which imposes an artificial relationship among them). Consider either extracting the name of a type into a traits class, or using boost::type_info to get a type name; This will allow you to get rid of the artificial inheritance constraint and make your factory more generic. 
&gt;this was dropped for all but &amp;&amp;,|| and comma For &amp;&amp; and || this is because in _normal_ use they are defined as always returning bool, so the neutral element is easy to identify. It was thrown out for "+" etc not because of possible change to the mathematical semantics but because the neutral element was defined as an int but could easily have been more appropriately a float or `complex` or anything else depending on use. An integer 1 is not much use if * is being used to fold a pack of matrices, for example. `+` being used for string concatenation is a _whole other problem_. Comma on the other hand is just used as trickery for expanding a pack into multiple expressions (normally used as a substitute for there being no syntax to expand a pack into one _statement_ per element) and the return value is normally unused, so void makes sense; and if it is used, a default of void for an empty pack is as sane as anything else.
From my point of view the current futures from the standard are hardly usable - others would call it completely broken. For further details I recommend to look into our paper submitted to the recent C++ standard meeting in Toronto: [P0676R0](https://github.com/FelixPetriconi/future_proposal/blob/master/proposal.md)
I would not say they not usable, but they are mostly usable for very simple tasks. 
"Factory" does not share a word root with "factor", but rather with "facture" as in _manufacture_. Therefore, setting a "fact*o*rer" makes no sense. Everything else about this is JUST FINE.
&gt; It was thrown out for "+" etc not because of possible change to the mathematical semantics but because the neutral element was defined as an int but could easily have been more appropriately a float or complex or anything else depending on use. An integer 1 is not much use if * is being used to fold a pack of matrices, for example Yeah, that's what I meant: in C++ you can overload operator+ without the need to specify whether or not you have a neutral element and what that neutral element is. &gt; + being used for string concatenation is a whole other problem. Yes, mathematically speaking it should have been *, as it is non-commutative.
Wow, there's some neat stuff there, those executors look cool for being able to do async processing and update the UI, although I guess you can already use signals and slots for that right now. And I agree, I have yet to encounter a use case where I would want to use futures, especially in a UI application. Something most of those tutorials fail to mention is that the `std::future` destructor blocks, basically you cannot get out of the function where you create the future as long as its calculation is running.
&gt;&gt; One argument for passing a future to the continuation is that the future encapsulates either the real value or an exception. However, this implies that everyone has to use the more complicated interface by passing futures even when an exception cannot occur. From our point of view this is against the general principle within C++, that one should only have to pay for what one really needs. I agree 100%
If it‚Äôs arbitrary precision why would you want to disallow expression templates?
Interesting post. Nitpick: conclusion: save -&gt; safe
... the things and the stuff
I wouldn't consider myself advanced either, but I thought I could give some thoughts on it as well in the hopes that more experienced people here can correct me, and we can all learn from it. This is by no means a comprehensive review, I'm just picking up some things that haven't been mentioned yet and that I've seen. return std::make_unique&lt;Templater&gt;(Templater()); Line 26 in your addFactorer function: This way it will construct two instances of the class, one of which is used to move construct the second instance only to be destroyed again while the second instance will properly be returned as a unique_ptr. You should just use it like this: return std::make_unique&lt;Templater&gt;(); std::make_unique does all the work for you in this case. If you did it your way, you could run into issues with classes that have their move operations disabled or if a move constructor does a lot of work or is faulty. It was pointed out that returning a null pointer in your factorClass function might be bad because it forces exception handling. An alternative solution might be to use std::optional or a corresponding alternative. That way you can return a basically empty object, and the user can choose to check for validity if they so want or just access it, which causes an exception to be thrown in case of failure. It forces another layer of indirection, which may not be what you want and may make the interface more complicated, but I think that it's a valid alternative that leaves the choice of how to handle the situation to the user. Another minor nitpick on the same function would be the possibility of using const std::string&amp; instead of just std::string as the parameter, or alternatively std::string_view if that's available to you. It has the advantage of avoiding a copy in this case and is pretty much how you should handle read-only string parameters by default. Then, in your Factory.cpp, consider marking the inherited and overwritten function toString of the class DeliverClass in line 15 with the override keyword. It just makes it more obvious and can help you with compiler warnings if you accidentally mistype a function name in the overriding process or something. Lastly from me, I think you should add a virtual destructor to your BaseClass. This is of relevance in this case since you're holding unique_ptr's of the BaseClass type with instances of classes that inherit from the BaseClass. The unique_ptr can't call the right destructor in this case. I have only criticised minor problems, not the overall design. I feel like in this case it's not really necessary to use strings to differentiate between the types at all. Consider something like this instead which makes more use of the type system: template&lt;class BaseClass&gt; class Factory { public: //enable_if is not technically needed since you wouldn't be able to cast the Derived type to the base class anyway //You can just as well use a static_assert to give a nice error message template&lt;typename Derived, typename = std::enable_if&lt;std::is_base_of&lt;BaseClass, Derived&gt;::value&gt;&gt; std::unique_ptr&lt;BaseClass&gt; factor_class() { return std::make_unique&lt;Derived&gt;(); } }; class Base{ public: virtual ~Base() = default; virtual std::string to_string() const = 0; }; class Derived_1 : public Base{ public: std::string to_string() const override { return "Derived_1"; } }; class Derived_2 { public: std::string to_string() const { return "Derived_2"; } }; int main() { Factory&lt;Base&gt; factory; const auto derived_1 = factory.factor_class&lt;Derived_1&gt;(); std::cout &lt;&lt; derived_1-&gt;to_string() &lt;&lt; "\n"; //prints "Derived_1" //Compiler Error! //const auto derived_2 = factory.factor_class&lt;Derived_2&gt;(); } This approach has several advantages in my opinion. It's simpler, and it's easier to refactor and rename the type with automated tools than to rename all instances of the string, although some modern tools can be a great help with that as well already. You can also rely more on intellisense, which knows the types but not their string representations. Additionally, you don't really run into an issue where you'd have to return a nullptr, unless something with the memory allocation goes wrong, but then you have other problems, and this solution will properly throw, afaik. Lastly, this also doesn't require you to register each class you want to be able to create manually, but instead automatically works with all classes that inherit from the BaseClass. Well, if that exactly was your goal, I guess this solution isn't really what you're looking for. I could imagine your approach in an environment where you're parsing data from outside the program, for example. If what I said came across as degrading, let me apologise. Your project is probably a good start to learn how all the pieces work together and may fit some use cases. My suggestions are meant as things to consider, alternate approaches. Maybe they give you a new perspective or new ideas to do things your own way. On the other hand, they're also made for myself. I wanted to give my own thoughts on the problem and get them on paper. It's always a good practice to think about how you'd do things other people did yourself.
Now I wonder if there is a region bias. Got to dig out my books and see if they are British :)
Too bad, myself-&gt;clone(); doesn't compile because I'm an instance of a class with a deleted 'clone()' function. There are too many interesting talks in parallel ...
In other words, do it like Rust? Why not change the iterator validity requirements? that would allow std::list to not have a sentinel node all the time. 
Is this something we could watch online?
yes, all talks have been recorded in the past years
I agree with you. C++ is a lot more easy to catch and there is a lot more you can achieve more easily. The only thing is that C++ tend to be a bit more complex then Pythonk, which makes it a lot more vulnerable to errors and bugs. Those can be detected using programs as checkmarx but it could sometimes take a long time and make you work slower.. Good luck.
It would be nice if you could somehow make it illegal to access something that has been moved from, and enforce that at compile time (a bit similar to Rust). This is an issue I encountered when we updated our library (Wt) to use more modern C++: previously, we would often add a widget to a container like this: Widget *widget = new Widget(); container-&gt;addWidget(widget); And still use that widget, e.g. to attach slots to its signals: widget-&gt;clicked().connect(...); In Wt 4, we changed `addWidget` to take a `unique_ptr` instead of a raw pointer, to indicate that this moved ownership to the container. However, if you weren't paying attention, you may rewrite it into this: auto widget = std::make_unique&lt;Widget&gt;(); container-&gt;addWidget(std::move(widget)); widget-&gt;clicked().connect(...); // nullptr deref! When porting our examples over, this was the most common bug, and all in all, it was still easy to find with a debugger or valgrind when you got a segmentation fault, but it would've been much nicer if the compiler (or static analyzer) could warn about this. I've tested a few, but I haven't found one that actually warns about this yet. Then, you could still prevent many bugs by using `-Werror=hey-you-re-trying-to-dereference-a-moved-from-unique-ptr-so-it-will-definitely-be-null-you-silly-person` or something in that vain. This was really the most common issue we encountered. We actually made `addWidget` return a non-owning raw pointer, so it's more like this now: Widget *widget = container-&gt;addWidget(std::make_unique&lt;Widget&gt;()); widget-&gt;clicked().connect(...);
Genuine question: are there reasons that current C++ move is better than destructive move?
Yes, that is why we are doing modules :-) The goals for modules are: 1. support sound C++ software architecture techniques at scale 2. isolation of components, so we reason about them - especially at scale 3. improved compile time -- everybody's popular request :-) 4. semantics-aware developer tools -- needed for productivity boost Compiling a module interface produces an artifact (IFC files) that contains logical instructions to the compiler regarding declarations etc. When a module is imported, that file is consulted bypassing preprocessing, parsing, etc. to directly re-construct the actual piece of information that the compiler needs. That rematerialization is done on a need-to-know basis. 
Further reading: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2002/n1377.htm#Alternative%20move%20designs Which concludes with: &gt; However the current proposal does not prohibit destructive move semantics in the future. It could be done in addition to the non-destructive move semantics outlined in this proposal should someone wish to carry that torch. http://howardhinnant.github.io/container_summary.html Which can be interpreted as a table of containers implemented with dynamic heap sentinel nodes vs those with embedded sentinel nodes. Except for deque which is just really complicated and I believe there's only one implementation that was designed with move semantics in mind (the other implementations predate move semantics).
I think some scenarios require non-destructive moves. Consider a function that does a conditional move: void maybeConsume(Widget&amp; w) { if (std::randint(0, 10) == 0) consume(std::move(w)); } If moves were destructive then how would the caller know if `w`'s destructor should be called?
Separate at from invocation or you can't know you're catching the right exception
This was the main part of this talk: https://youtu.be/hEx5DNLWGgA?t=29m14s
&gt; Consider an implementation of std::list that uses a sentinel node. As such, a list object is never empty which eliminates some branches in the implementation. But due to STL iterator invalidity requirements, the sentinel node has to be dynamically allocated. I don't get this. I understand the idea of a sentinel node, but why must it be heap allocated? The libc++ implementation has an end node as a member of the list itself. I do see that the msvc implementation allocates an end node rather than embedding one in the object though.
That was the spirit of our changes, yes. We were making all ownership explicit to align with the core guidelines. Does he address this issue of moved from `unique_ptr`s, because I don't recall him mentioning that? EDIT: Just rewatched a bit of it: you mean to say that a static analyzer that checks the core guidelines should normally catch this? I haven't tried the Microsoft one yet, maybe that works.
I have my information from here: https://www.reddit.com/r/cpp/comments/3a64no/standardizing_variant_difficult_decisions/cs9u1ja/
 void maybeConsume(optional&lt;Widget&gt;&amp; w)
Caller of `maybeConsume`? Simple - you can't destructively move `w` since it's passed by reference - you don't own it. So `maybeConsume` needs to only accept moved arguments. It's just that it's hard to enforce it in current C++ without breaking backwards compatibility.
Yes, and between that and using virtual inheritance, you now have two problems, and between that and the problem you used virtual inheritance to solve (diamonds), you have three. I'm only about 5% joking. While I would file this under "good to know", "unfortunate" is strong in my book. VI is only to solve diamonds of non-interface classes, and such diamonds are just something you should avoid about 99.9999% of the time. If you separate interface and implementation inheritance clearly (if necessary at the cost of a couple of extra classes) then it should almost never come up. Most examples of diamonds I know violate this principle.
And with this is now also the new website live... ... still got to do some css hacking for mobile/desktop. Some other data like past conferences will be imported later.
&gt; it's not always possible to write tests to an already written code That sounds like changing the design to facilitate the tests to me. If you're doing that you have to ask yourself if you're really operating on a level that's providing you a meaningful test. 
clang-tidy has a check for this, https://clang.llvm.org/extra/clang-tidy/checks/misc-use-after-move.html
The easiest way to test 15.0/15.3 is by using one of our [daily NuGet packages](https://aka.ms/dailyMSVC). Instead of using the "daily" package, which would be updated daily if only I had enough time to actually get the scripts on the build server done, you want to use a package from the official [visualcpp NuGet.org feed](https://www.nuget.org/packages?q=visualcpptools). The difference between D14 layout and VS2017 layout is whether the tools are lain out on disk as they were with VS2015 or VS2017. Either should work with VS2017, but the [15.0 toolset only exists in the Dev14 layout](https://www.nuget.org/packages/VisualCppTools.Community.D14Layout/14.10.25017). The 15.2 toolset and the 15.0 toolset were very, very similar. We didn't preserve the 15.2 toolset on NuGet: all the rest up there are 15.3 variants. 
nice article I upvoted it. I think this is a rich subject - as yet untapped. I've had something to say about this in my CPPCon 2016 presentation "C++, Abstract Algebra and Practical Applications" https://www.youtube.com/watch?v=632a-DMM5J0. Anyone who liked this post might find it interesting.
It's pretty much impossible to get this into C++ in a backward-compatible way, right? *Especially* now that move semantics have already been defined the way they have.
Why do you have such a slow pace in general? Is the MSVC team under-budgeted? What's the total number of engineers working on C++ compiler at the moment?
with limitations: https://www.reddit.com/r/cpp/comments/5qu8fr/a_guide_to_porting_c_and_c_code_to_rust/dd40vau/
Much like how `std::auto_ptr` was deprecated in favor of better alternatives, the standard could introduce `std::move_and_destroy` (modulo a better name).
I'm curious what the use case is for IBM, Cray, PGI, and Pathscale? I have never even heard of these compilers. My guess would be that the first two target supercomputer architectures?
They didn't. Software and nonsense are intrinsically correlated.
pretty much all pseudoRNG are build using following sequence: - enter state - chop-chop bits - make return value from freshly chopped state For example, take LCG uint rng(uint prev) { auto next = (a*prev + c ) mod m; return next; } xoroshiro128+ was made up in quite different way: - enter state - make return value - chop-chop bits which makes prediction of the state an easy task, for example. Speed is good, but this is wrong approach. Why they made is such - I have no idea
Cray is solely about supercomputer-like deployments. IBM also aims their compiler at usage on their big-iron POWER8 servers, used for all sorts of things. As for Pathscale, see [this](https://www.hpcwire.com/2017/03/23/hpc-compiler-company-pathscale-seeks-life-raft/) When you see 'PGI' read 'Nvidia'.
Well, one problem is that the compiler would have to track if a object was destroyed or not. For many cases that would not be a problem, but I guess it is difficult to specify the feature in a way that isn't too restrictive on the one hand, but doesn't pose unreasonable burden on the compiler in the corner cases. Would you allow destructive move through indirections (pointers, references)? How would it interact with exceptions? Control flow? Or would you only allow it for types created with (placement) new and leave it to the user?
Setting aside actual expenses of exceptions/no-exceptions in the generated code, my team has observed differences in compiler analysis and consequent optimization results when compiling with exceptions disabled. If the compiler can tell more about the control flow by assuming everything is `noexcept`, then it can make better transformations.
Is there some public group or discussion forum where that platform gets defined and revised?
There's no need to use `std::visit` in this example. if (std::holds_alternative&lt;std::string&gt;(v)) std::cout &lt;&lt; "string: " &lt;&lt; std::get&lt;std::string&gt;(v) &lt;&lt; '\n'; else if (std::holds_alternative&lt;int&gt;(v)) std::cout &lt;&lt; "integer: " &lt;&lt; std::get&lt;int&gt;(v) &lt;&lt; '\n'; else if (std::holds_alternative&lt;bool&gt;(v)) std::cout &lt;&lt; "bool: " &lt;&lt; std::get&lt;bool&gt;(v) &lt;&lt; '\n'; 
That's just as bad as the constexpr-if solution.
Personally, I think the problem is that while C++ can implement a lot of features in library, sometimes it really ought to have incorporated the feature in the language instead. Beyond boilerplate, language features generally lead to much more helpful error messages as well.
So if `v` gets a new type you just silently ignore it?
Or in another language: type Variant = String of string | Int of int | Bool of bool let v = String "hello" // or Int 3 or Bool true match v with | String s -&gt; printfn "string %A" v | Int i -&gt; printfn "int: %A" i | Bool b -&gt; printfn "bool: %A" b Even a beginner will understand the above, and it's much cleaner without any noise in the syntax. I don't even have to specify in which language the above code is written. And the most important point: if another case is added to the `Variant` sometime in the future, there will be a *compile-time* error stating that the match is not exhaustive.
I would definitely consider using a cleaned-up C++ for a lot of projects, especially if it supported easy integration with C99 (and maybe C++ code)
I think you're looking for OCaml üòõ
Did you consider something like `Widget *widget = container-&gt;makeWidget&lt;Widget&gt;(args);`? Superficially this is just sugar for `addWidget(std::make_unique&lt;Widget&gt;());`, but IMO it actually makes code a lot easier to audit for correctness and does a better job of nudging people into using a safe pattern.
Maybe I'm old-school, but I was taught that if you see a bunch of ifs like this, it really meant you didn't correctly use inheritance. I know inheritance is a bit of a dirty word nowadays, but it pretty much solves in a clean manner, the problem that std::variant solves in an ugly way. 
So two lines (implementation of `overloaded`) are missing in the standard library for `std::visit()` to be fine? They are even shown in the [example section on cppreference](http://en.cppreference.com/w/cpp/utility/variant/visit). I agree it's not easy to write these yourself, especially if you're less experienced, and it would be preferable to have them in the standard library (or even better, have pattern matching) but adding two lines of missing library code to your own is not *that* bad.
I use vim with YouCompleteMe and a bunch of other plugins. I've set the cindent settings to `set cinoptions=N-s,i0,=0,:0,b1,(0,W4,g0`. Tabs, which are sized at 4 spaces. For my code style I go with braces on a new line and a 95 character limit. Arguments go on separate new lines if I think it's too much to just chain them after each other and regularly indent. It's hard to describe how I do switches, so I'll just show an example instead: switch(option) { case OPTION1: { // code here } break; default: break; } There is a clear benefit to explicitly scoping switches, and that is that you don't leak variables to the other cases. I have some other style preferences, but they're not really indentation related. The most important thing to me is readability, which means keeping your lines short is arguably the most important.
Dedicated pattern matching: match (theSetting) { Setting::Str(s) =&gt; println!("A string: {}", s), Setting::Int(n) =&gt; println!("An integer: {}", n), Setting::Bool(b) =&gt; println!("A boolean: {}", b), }; C++17 visitation with overload: std::visit(overload( [](std::string const&amp; s) { std::cout &lt;&lt; "A string: " &lt;&lt; s &lt;&lt; '\n'; }, [](int i) { std::cout &lt;&lt; "An integer: " &lt;&lt; i &lt;&lt; '\n'; }, [](bool b) { std::cout &lt;&lt; "A boolean: " &lt;&lt; b &lt;&lt; '\n'; } ), theSetting); Doesn't seem that different to me if we actually make an honest comparison. Only complaint I have is that you have to put the variant last, where I would find it clearer to go first - but it's not nearly as big a deal as the article makes it out to me. However, there is a major advantage of pattern matching that isn't touched on in this article: returning based on case. Can't do that with `std::visit()`. 
Yes, that's intentional. Otherwise there will be a `else unreachable();` branch at the end.
Actually, I don't see anything wrong with the first option (struct Setting Visitor). Looks pretty transparent to me. 
To be fair, it's not *just as bad*: the regular if-else chain generates much better code than `std::visit` (another reason this should've been a language feature): [visitor](https://godbolt.org/g/i56j4P) vs [if-else](https://godbolt.org/g/viSuWH) vs [Rust](https://godbolt.org/g/jMPZkC). Edit: Since Rust uses LLVM, here's the clang version of [if-else](https://godbolt.org/g/DaCWMx) Edit2: If anyone's curious, the boost version of [visitor](https://godbolt.org/g/p5iGBQ) is much better
The Standard permits both dynamically-allocated and container-internal sentinel nodes. It's up to the implementer to choose which is better. I strongly believe that dynamically-allocated is better for list, due to its overall performance (low) and iterator stability (high).
&gt; OCaml # let square x = x * x;; val square : int -&gt; int = &lt;fun&gt; # square 3;; - : int = 9 # let rec fact x = if x &lt;= 1 then 1 else x * fact (x - 1);; val fact : int -&gt; int = &lt;fun&gt; # fact 5;; - : int = 120 # square 120;; - : int = 14400 I have no idea what is happening, but it looks fun.
&gt; because they decrease maintainability and are not easily extensible. not everything is meant to be extensible. I **want** to get compile-time errors everywhere if one day I add a type to my variants, because I **have** to handle the cases for my program to be correct.
That's kind of the only 'disadvantage.' But OCaml language is about 10% the size of C++14. So any average C++14 programmer should be able to learn it in a week and be productive in it.
I am a developer - yet I do not release my code under the GPL.
Well this version does not "require you to know about‚Äîand grok‚Äîthe new constexpr if syntax, along with type_traits fun like std::decay."
As others have said, `visit()` would be much better with a `std::overloaded()`, but it's still relatively clunky (or at the very least, noisy) compared to languages that have pattern matching built-in. It's also incredibly challenging to explain how it works to newcomers without dumping all sorts of other topics on their heads. As someone who's trying to encourage colleagues to use sum types more often, these strike me as a serious problem. Given the choice between sum types with no pattern matching, or neither of those things, I'd choose the former. But it's a sad state of affairs.
This: variant&lt;string, int, bool&gt; mySetting = "Hello!"; probably doesn't do what you think it does. Topic of your next ~~rant~~ post?
Boolshit.
Damn. Alright, I'm stumped - how does that get coerced to a Boolean? `variant&lt;string, int&gt;` doesn't seem to have the same issue. Is it because `"foo"` is a not-null pointer?
Std variant is a superior take on inheritance like modules is a superior take on header files. 
`char const*` to `bool` is a standard conversion, but to `std::string` is a user-defined conversion. Standard conversion wins.
There are two main issues with `std::variant`: 1) [it does not support recursion](https://stackoverflow.com/questions/39454347/using-stdvariant-with-recursion-without-using-boostrecursive-wrapper) and 2) it does not provide never-empty-guarantee (or as Boost put it, `std::variant` causes ["significant additional complexity-of-use"](http://www.boost.org/doc/libs/1_65_1/doc/html/variant/design.html#variant.design.never-empty)) But even if I can't use `std::variant` in real code, does not mean it is not suitable for your next hello-world application, I guess. Speaking about visitation, in my code it usually looks like this: boost::apply_visitor(*this, some_variant); // whatever, good enough
Damn. I'll have to update the post after lunch.
I don't think you have established in any way that your very first SettingVisitor is bad. Yes, the function definitions are slightly longer than lambdas...mostly because they're not anonymous. I'd love to know why you think this solution "gets even worse if we want our visitor to capture or modify some other state." And you go on to suggest that lambdas are somehow better for this, which I also take issue with. Why do I get the sense that the subtext here is that you think operator functions and overloading are lame and you're not cool if you aren't using lambdas or templates?
A comparison that would be honest to what that dedicated pattern matching supports would compare when there are if clauses in the match, destructuring, etc, besides the return from block feature, then one can see how much more to miss.
&gt; C99 interop would be a goal, C++ not, you can't even integrate C++ binaries from different compilers on the same machine FFS :( sure you can, on sane platforms. There is zero problem having clang and gcc compile half of a project each.
Far more than the syntax are the restrictions closures place on control flow. You can't use `break`, `continue`, `return`, or (heh) `goto` from within them, while you can with a built-in `match`. (This may be what you mean by "returning based on case"?) Another important difference is that built-in pattern matching can match on value rather than just type.
There's been an explosion of projects like this over the past ten years. Since you mention Rust, take a look at Zig, Jai, Pony, or even D.
Right, yes - I did not phrase that well at all. To clarify, it's not that I don't think pattern matching isn't better than `visit` or that I don't want it. It's that in the one example the post showed, pattern matching wouldn't really be meaningfully better. 
You could fix it by using `std::literals::string_literals` and going `"Hello!"s`. C++ is so fun!
&gt; 1) it does not support recursion The problem is that C++ makes explicit the fact that recursive types need to have some dynamic allocation at some point. Recursion is easy: just like you implement linked lists with struct node { node* prev; node* next; ... data ... }; you'd implement recursion in variants with some kind of pointer somewhere
...and you get to define it locally in a function if you need it only once. It doesn't seem that bad ... but then again, if you need access to outer local variables it requires more code. This kind of makes me wish, C++ supported a lambda expression syntax which results in overloaded operator() methods. 
I'm still sticking to that pair too. Geesus, reading here about all that Atom/VisualStudio makes me sick. 
Probably badly worded on my part. I would like to be able to disable them, as Boost.Multiprecision allows you to, but not to forbid them. The reason is that I want decltype(n1 + n2) to be some meaningful type, not some funky implementation detail of the ET machinery. ET have their uses, but in my opinion they don't play well with modern and generic C++.
I never have this problem, but that seems like an answer that is simple, easy and wrong.
Yep. Personally, I think C# has a really, really great design philosophy. When they introduce new features, they build them mostly in the library. But then they add a little bit of syntax sugar that makes accessing those library features clean. I'm thinking specifically of LINQ and async here.
A recursive discriminated union can't be a value, because its size is not constant. C++ has pointers and references, so recursive unions aren't necessary the way they are in other languages.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/704qf3/can_you_please_help_a_desperate_noob/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
The biggest problems with destructive move are two-fold: (1) runtime branches, where one branch destructively moves and the other does not. (2) destructively moving from containers. The second can perhaps be handled by simply not allowing it via tight ownership semantics. e.g., one can only destructively move values (never references), so moving from a container requires calling a function that erases the element and returns it by value, allowing you to then move it elsewhere. The first item is going to require some semantic expansion. Rust does it by tracking a moved-from bit for conditionally-moved values that have destructors, which is probably the best one can do without completely changing the lifetime rules and destructing the object earlier, which would be a very massive change to semantics.
Vertical alignment causes noisy diffs in version control. If you add or change a field that does not fit into the existing alignment, you have to realign the entire block--which leads to the whole block showing changes in the diff, making it difficult to discern what actually changed.
This might be the first time that C++ was accused of having too few features.
The analogy is wrong. 1) Deprecating a flawed lib feature and working but not perfect rather invasive language construct is not the same thing 2) It's not like destructive move wasn't a thing when move semantics were considered. Destructive move is not feasible without something like Rust's borrow checker and it looks like it was deemed impossible to introduce that kind of move into C++ without breaking a lot of working code.
&gt; Recursion is easy With some kind of pointer, can you rewrite [this code](https://stackoverflow.com/questions/39454347/using-stdvariant-with-recursion-without-using-boostrecursive-wrapper) with `std::variant`?
In this case I generally use a reduced version of PIMPL which only contains data. This avoids the additional function call indirection you get with full PIMPL (perhaps call it PDATA), which makes the static polymorphism approach static with regard to methods. Also note that it is possible to have a base class with static polymorphism by implementing some subset of the class methods in a file unconditionally included in the build.
Everybody has a very different list of warts. What one person hates, another person considers to be an essential feature. Your original post suggests a couple specific warts (slow compile time and type deduction rules), but what else would you like to clean up?
I agree with you here but also kinda disagree. C++ already has a lot of features, we can't accuse it to lack some. But the underlying problem is not that it lacks some features but that they are library features not built-in syntactic features.
&gt; With some kind of pointer, can you rewrite this code with std::variant? that's literally the question you posted. The "kind of pointer" is boost::recursive_wrapper&lt;T&gt; (which is sugarcoat on top of T*)
Why can't you use `std::optional&lt;std::variant&lt;...&gt;&gt;`?
Small note here; today's Rust stores that bit on the stack, rather than with the value.
Which functional language is that?
There sure is! https://groups.google.com/forum/m/#!forum/vfx-platform-discuss
Yeah. The usual complaint is that it has too many syntactic features.
Wouldn't call it superior. It's just different.
Why can't you [try it yourself](https://wandbox.org/permlink/KcQbb9rCPuogPpxO)?
Agreed. I've always wanted Rust to have more of a C style syntax. One of the projects I've always wanted to start (but never had the time (this would be a multi-year endeavor)) was a different front end for Rust that is more similar to C++.
`char *` is why we can't have nice things. Actually implicit conversion to bool is why we can't have nice things, but that's a whole different story.
Tbf, clang's [visit](https://godbolt.org/g/NSLuPf) is much better (similar to boost's, though still not as good as `if-else`). Btw, had to use `libc++` since it didn't compile with default stdlib.
What are the chances that they wanted to get a verbose version of it now so we get used to it, then get a nicer more evolved version in C++20? Sort of like the opposite of the auto_ptr or something? I feel like C++ knows what its doing so I'm content to just wait.
ML or some variant of it
If anything, *this* is what's wrong with C++. A type system weakened by legacy implicit conversions. &gt; Use compile-time conditionals, which require you to know about‚Äîand grok‚Äîthe new constexpr if syntax, along with type_traits fun like std::decay. In order to use the language's facilities, you need to know - *and "grok"* - the language's facilities? No way! The `if constexpr` variant (no pun intended) is perfectly straightforward and concise. Yet, he whines about having to *know about* it while glossing over the fact that he had to explicitly cast the string literal into an std::string when initializing the variant or get completely counter-intuitive behavior.
A very huge one is that a compiler feature can generate simpler and more direct code with better debugging support; not just eliminating template instantiation errors but also the runtime debugging is better without 20,000 lines of library gook to step through/around.
Thank you for replay :) I've changed exception handling and used [ShadyDC](https://www.reddit.com/r/cpp/comments/6zxx7c/rate_my_factory/dmznnjl/) suggestion to use std::optional as a returned parameter. &gt;It was pointed out that returning a null pointer in your factorClass function might be bad because it forces exception handling. An alternative solution might be to use std::optional or a corresponding alternative. That way you can return a basically empty object, and the user can choose to check for validity if they so want or just access it, which causes an exception to be thrown in case of failure. It forces another layer of indirection, which may not be what you want and may make the interface more complicated, but I think that it's a valid alternative that leaves the choice of how to handle the situation to the user. Reffering to third point. Yes, stringable class was missed idea, I've added, as a default, extracting the name using type_trails but my implementation is not an ideal solution (not workinging in all cases). But I think I'll resolve this problem soon.
&gt; Something most of those tutorials fail to mention is that the `std::future` destructor blocks, basically you cannot get out of the function where you create the future as long as its calculation is running. Keep in mind that _only_ `std::future`s obtained by calls to `std::async` have blocking destructors.
Thank for comprehensive reply, it was really helpful :) &gt;Line 26 in your addFactorer function: This way it will construct two instances of the class, one of which is used to move construct the second instance only to be destroyed again while the second instance will properly be returned as a unique_ptr. &gt;using const std::string&amp; instead of just std::string as the parameter &gt;overwritten function toString of the class DeliverClass in line 15 with the override keyword Right point. Changed. &gt;It was pointed out that returning a null pointer in your factorClass function might be bad because it forces exception handling. An alternative solution might be to use std::optional or a corresponding alternative. That way you can return a basically empty object, and the user can choose to check for validity if they so want or just access it, which causes an exception to be thrown in case of failure. It forces another layer of indirection, which may not be what you want and may make the interface more complicated, but I think that it's a valid alternative that leaves the choice of how to handle the situation to the user. Nice idea. I've used it. I've never used std::optional before so it was nice to have a chance to make it work. &gt;I have only criticised minor problems, not the overall design. I feel like in this case it's not really necessary to use strings to differentiate between the types at all. &gt;I could imagine your approach in an environment where you're parsing data from outside the program, for example. I think your solution could be helpful someday so I have added it to my Factory. But, my idea was to create mechanism to provide different classes in runtime, depending on configuration (in human readable form). Again, thanks for your effort.
F#
The `if` may allow the compiler to generate better code but at least `std::visit` generates *correct* code. And that's its only purpose. In case this isn't clear, using `if` doesn't enforce that all cases are handled. `std::visit` does. That is literally its whole point. Without this, we might as well use C-style unions.
You don't seem to like this answer, but you are looking for D: * calling into C99 and a subset of C++ is easy enough * a c++14 programmer can pick it up quickly * opt-out garbage collection, and also RAII, scope statements, built-in contract programming * definitely less warts (no vector&lt;bool&gt;, for one) * compiles much faster than c++ * uses modules * has very powerful templates with saner syntax than c++, as well as easy introspection (e.g. you can for-loop over the methods or the variables of a class) * not sure about the ABI
If you take the closed-world assumption. The int-string-bool variant is fine and dandy until you have collected lots of pattern matching code, part of it written by the users of your library. Then you get a request to add float to the mix. Revisit, modify and retest all of this code, and tell your users to do the same. Done? Great, now add lists. 
Could you explain your idea? Because I was thinking about a way to create possibility to pass parameters to constructor. One of the option was to provide list of arguments as variadic template in class definition: template &lt;class BaseClass, typename ...Args&gt; class Factor { std::map&lt;std::string, std::function&lt;std::unique_ptr&lt;BaseClass&gt;(Args...)&gt;&gt; factorMap; } it is not that bad solution but a little bit ugly because you need to copy all parameter type from base class constructor. I don't know if I'm not misunderstood you, but providing constructing lambda which must be stored in std::map as std::function with know number and type of parameter will provide only to construct class with const parameters known while registering a function to factory or with global variables. You will not be able to pass any arguments to factory.factorClass(const std::string&amp;).
How is using a C++17 feature (varadic using) with another C++17 (sum types) anything but expected? Or incorrect? I get the gist, it should be simpler with pattern matching but complaining that you have to use another feature from the same C++ feature set makes no sense to me. Can someone please enlighten me as to why this is causing the author so much distress?
I'm not a beginner and I have no idea what `Int of int` is meant to signify ?
`Int` is one of the three variant cases for `Variant`; `int` is a primitive type and designates the type of data for said case. The case names are basically irrelevant, and a bit confusing here; `type Variant = Foo of string | Bar of int | Baz of bool` is clearer if dumber, IMO.
It's often accused of lacking "basic" features such as variants, networking, graphics, filesystem. This example shows why - someone's always got to complain about whichever design choice was made.
Thankfully, there are ongoing attempts to have a language variant, not to mention more general pattern matching.
I see. Is this meant to allow having multiple variant members of the same type? 
&gt; it is not that bad solution but a little bit ugly because you need to copy all parameter type from base class constructor. No you don't need copy. Types in `Args` can be reference types and you can forward them. &gt; I don't know if I'm not misunderstood you, but providing constructing lambda which must be stored in std::map as std::function with know number and type of parameter will provide only to construct class with const parameters known while registering a function to factory or with global variables. You will not be able to pass any arguments to factory.factorClass(const std::string&amp;). That's correct. But I already see that you can now send the lambda as parameter to the registration function. This is great because you can mix the required arguments for construction sent each time the factory is invoked, and the pre-baked parameters. A bit like this: factory.register&lt;MyType, int&amp;&amp;, double&amp;&gt;([](int&amp;&amp; a, double&amp; b) { return std::make_unique&lt;MyType&gt;("preBakedString", std::move(a), b); }); double d = 8; factory.create&lt;MyType&gt;(5, d);
If I understand your question correctly, cases are named mostly to make pattern matching more readable; i.e. the concern is more about clarity of consuming a value than of constructing one, but regarding the latter it can help to think of case names as named constructors for the variant type. Also n.b. having multiple cases with the same associated data type is common ‚Äì in particular the scenario of having _no_ data, for the case of tag/enum types. EDIT: restructured for clarity
I'd say that it has too many leftovers from the C days, and from the pre-C++11 days. If we could remove some of the redundant and old syntax the language would be so much nicer and compilers would be simpler too.
Because getting started with variant seems to be accessible until you try to get something out of it and realize that the provided tool (visit) requires a reasonable amount of not-so-obvious boilerplate code. It's surprising in a negative way.
You could also have shown how to construct a build matrix with gcc-5, gcc-6, and clang-5, not just separately. That would be really useful.
.... This is approaching JavaScript levels of frustrating. I've always found C++ to only be usable with a few strict warnings enabled from the very beginning of the project, no implicit casts being one of them
CppCoreGuidelines and GSL
Maybe you could also enlighten us as to why you think that? Because I don't think of spreading pattern-matching code all over your source as 'superior'; rather, I'd call that a maintenance nightmare. 
The JVM is stupid but the slow as shit Python runtime is not?
&gt; In order to use the language's facilities, you need to know - and "grok" - the language's facilities? No way! You've got me - the wording there was pretty terrible, but I was trying to point a finger at `decay` and `is_same_v`. &gt; The if constexpr variant (no pun intended) is perfectly straightforward and concise. Perhaps `std::decay_t&lt;decltype(arg)&gt;` is perfectly straightforward to those of us who have been working in C++ for a while, but it sends newcomers down a whole different rabbit hole before we can get back to talking about sum types. &gt; he whines about having to know about it while glossing over the fact that he had to explicitly cast the string literal into an std::string when initializing the variant or get completely counter-intuitive behavior. Can I be dissatisfied with both?
Shameless self promotion, i made an alternative variant that doesnt have this problem https://github.com/cbeck88/strict-variant And does other things nicely too
&gt; Perhaps std::decay_t&lt;decltype(arg)&gt; is perfectly straightforward to those of us who have been working in C++ for a while, but it sends newcomers down a whole different rabbit hole before we can get back to talking about sum types. It's "pretty straightforward" as far as C++ goes lol. It's needlessly verbose, as almost everything in C++, and it's full of C++ idiosyncrasies, but it's hardly "everything wrong with C++." It's not a place where the language is broken. &gt; Can I be dissatisfied with both? Be my guest.
&gt; it's hardly "everything wrong with C++ Hyperbolic title was hyperbolic. Maybe I should have chosen something less clickbait-y, but I have a sinking feeling that it wouldn't have pulled the same amount of people into this discussion. :/
So, its easy to criticize, but its harder to propose an alternative. Has there ever been a standards proposal for sum types in the language, and visitation syntax? I dimly remember maybe stroustrup had a paper that talked abt this? Is that what u would rather see in c++20? There was already boost variant which ppl used for a long time successfully. Std::variant is different in important ways, but not all that different, esp. In respect to `std::visit`. So they changed some things but mostly followed the pattern of standardizing stuff that previously appeared in boost. Idk, i think ur article would be better if u also suggested something better or highlighted someone elses proposal.
Wikipedia[1] agrees that it is called either an identity or neutral element. [1] https://en.wikipedia.org/wiki/Identity_element
I think the problem is that you have to define a SettingVisitor for each different thing you want to do with your variant. Suppose you are using a variant in three different places in your code, in different ways, and only once in each place. Is it the best idea to write a named Visitor for each case? It doesn't make sense to define a struct that you are not going to reuse. It does the job, sure, but it confuses the *purpose* of having a named struct/function in the first place (reuse). Lambdas look like the perfect fit for the job: You need to do a specific thing with a variant, well you define a lambda right then and there. Of course, if your visiting pattern is recurring somewhere else in your program, it makes sense to define a Visitor for that. But if you want to use lambdas, you have to go to the pains the author described.
The problem is bigger than that. It comes from the fact that, in mathematics, there is no lingua franca, or a standard, to define those terms. They are all invented or made up, without major consensus. Indeed, using the most common terms is appreciable. Unfortunately, it also depends on whom these terms are common to, and again we are back to the original problem.
Those are libraries though, not syntactic features.
Sorry, what's the title of the article? How many times is identity vs neutral mentioned? How many times is identity vs neutral mentioned in the monoid article: https://en.wikipedia.org/wiki/Monoid, or the linear algebra article: https://en.wikipedia.org/wiki/Linear_algebra? I didn't say that neutral wasn't a term for it, I said that identity is way, way, more popular. Which the source that you chose to cite quite clearly confirms.
Not a fixed Abi. You would limit a much the number of platform you can compile the code. The calling convention is part of the abi, and the calling convention is platform specific. You'd have to choose your language's platform and it could never change! Maybe you mean a unique way to mangle names and a stable calling convention per platform? Also, I'd design the language to by default be zero-overhead, with a nice syntax and nice semantics. If zero overhead is the simplest, then your language is really good. If you decide to sacrifice performance because it would play nicely with one particular idiom, then as soon as the idiom is out of fashion, you will carry the weight of the overhead for the rest of the language existence. A well designed language should adapt itself nicely with new stuff.
Well, the website is titled "Bashing", so constructive proposals may be beyond the site's scope... But seriously, at the end of the article he did link to [this better alternative proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0095r1.html), which for some reason the standards committee hasn't selected.
Lets not forget the possibility of embedding more specialized languages in C++: https://github.com/andreas-gone-wild/snackis/blob/master/snabel.md
I think the same issue applies to syntactic features also; everyone wants it but everyone wants a different flavour of it. No matter what solution we end up with, some will say it's [too complicated | hard to read | too inflexible | ...] 
It might look a bit like this https://github.com/dobkeratops/compiler, the basics work but after a couple of months I decided it's futile to write a language without a community , so I didn't make any headway on the bigger plan (transpiling both ways such that it is really a ```C+-``` as you put it). my intention *was* that you could transpile a subset of C++ into this; thats how I was going to 'self-host' it. basically the idea was 'copy as much of Rust as possible, without breaking the ability to transpile a C++ subset'. I figure the reason it doesn't happen is everyone has their own idea about exactly what a C+- should be. The next most promising language for me is [jonathan blow's JAI](https://www.gamasutra.com/view/news/226070/Video_Jon_Blow_on_building_a_new_programming_language_for_games.php), but he doesn't like "dot-calls" , which I do like. EDIT: actually as I re-read your post, I see you *don't* want C++ compatibility; as such I think you just want an unsafe option for Rust. But from my POV, the ability to retro-fit into C++ projects (and use C++ frameworks) is more useful. Look how [swift](https://en.wikipedia.org/w/index.php?title=Swift_programming_language) builds on the existing [objective c](https://en.wikipedia.org/wiki/Objective-C) ecosystem in apple-land, or how interoperability of JVM languages allowed [kotlin](https://kotlinlang.org) to build android apps.
I see, I didn't notice it there in the footnotes. I think he could have given more attention to talking about the strengths and weaknesses of the alternative proposal than to the bashing. Sometimes the most convincing way to explain how something is bad is to describe something better. You're right though, I didn't read the article as carefully as I should have.
I personally think Rust gets the syntax just right. I've mostly used C and C++ but I accept that C/C++'s specific type declarations are a mess (e.g. how function pointers are written, how arrays work). preceding 'fn' is very pleasing because it makes the language so easy to search. grep "fn foo" will always find definitions of foo. grep "fn foo.*self" will filter just methods. pretty neat.
That's not what begging the question means... https://en.wikipedia.org/wiki/Begging_the_question
It's the only *modern* compiler that has them, though.
Be happy if you get c++98
I'd say that implicit conversion from bool to int is why we can't have nice things.
Honestly, the std::visit doesn't bother me that much.
&gt; In modern vernacular usage, "to beg the question" frequently appears to mean "to raise the question" (as in, "This begs the question, whether...") or "to dodge a question". In contexts that demand strict adherence to a technical definition of the term, many consider these usages incorrect. Apologies for not using purely formal speech on Reddit. ^/s
**Company:** [Magic Leap](https://www.magicleap.com/#/home). ( [Demo video](https://www.youtube.com/watch?v=GmdXJy_IdNw) ) **Type:** Full time &amp; contract. Internships available seasonally. **Description:** We are making a new type of wearable computing device based around a transparent display, that allows *unprecedented* interaction between the digital world and the real world. We have several hundred employees and over a billion dollars in funding. **Location:** San Francisco Bay Area. Florida. Zurich. Israel. New Zealand. Several other locations. Not all positions available at all locations. **Remote:** Likely not for new hires, but exceptions exist. Work from home with regular office presence permitted, and remote work common for established employees. **Visa Sponsorship:** Yes. **Technologies:** C++11/14. About to turn C++17 on. Primarily Linux. There are a wide variety of positions available with different requirements / preferred skills, from computer vision to user experience. Please review our [open positions](https://www.magicleap.com/index.html#/join), which can be considered a proper *subset* of our needs. **Contact:** Please direct message me on reddit. 
Here's the thing.. when you use a term to mean both something and what is essentially it's opposite, you lose the ability to concisely describe a specific thing and language suffers for it. There's already no way to say that something is literally true.. other than saying something like "it's literally true as in the dictionary definition of the word" because literally means literally and figuratively because people couldn't bother to learn that literally didn't mean "very". 
I used language accepted as modern vernacular; I don't have a problem with this, and I'm not really concerned whether anyone else does. ¬Ø\\\_(„ÉÑ)\_/¬Ø That said, thanks for pointing it out ‚Äî TIL. :-]
I disagree with the premise of this article: I don't think std::visit is meant for general consumption by every user of std::variant. Or at least, I don't foresee it being too widespread. I think the "constexpr if" pattern will be much more popular for everyday usage of std::variant. std::visit seems like it will be useful for people who are already deep into metaprogramming. 
&gt; (no vector&lt;bool&gt;, for one) Huh, as an embedded developer I actually consider the space savings of vector&lt;bool&gt; a nice feature!
This is simply the expression problem rearing its ugly head. Pattern matching is awkward when you want to add new types (while interfaces are easy as you simply make a new class that extends the interface). On the other axis, interfaces are awkward when you want to add new functions (you have to update every class that inherits that interface), while adding new functions with pattern matching is extremely easy.
Dynamic bitsets are a nice feature. Specializing `std::vector` in a way that breaks generic code when you give it booleans isn't.
Here's one thing I would definitely do -- forwarding references would not clobber the syntax for rvalue references. template &lt;typename T&gt; void func1(T &amp;&amp; t) { ... } // This only binds to rvalues template &lt;typename T&gt; void func2(T &amp; t) { ... } // This only binds to lvalues template &lt;reftype T&gt; void func3(T t) { ... } // This binds to anything, and parameter T will always have a reference type. T is a forwarding reference. I can't understand why they decided to make template rvalue reference not mean what it should intuitively. It's just yet another stumbling block for C++11 beginners. Is adding a new keyword really that big a deal?
I'm having an issue with '14.11.25712-Pre', figured this was as good a place as any to ask ‚Äî when using Clang 6.0.0-r313013 and linking with LLD, I'm getting the following linker error: &gt; lld-link.EXE: warning: msvcrtd.lib(loadcfg.obj): undefined symbol: `__enclave_config` &gt; error: link failed This doesn't happen with '14.11.25617-Pre'. Any idea what I need to feed to LLD to make it happy?
You know, I've been thinking about that. I actually did it in our ORM, Wt::Dbo, where you can do `session.addNew&lt;Object&gt;` and get a `Wt::Dbo::ptr&lt;Object&gt;` out of it. It was my colleague who did most of the changes, and I'm not sure if he's thought of it, I'd have to talk to him about that. In any case, it's not a disaster if that doesn't go into Wt 4.0.0, because adding `makeWidget` or something like that doesn't break compatibility.
Apples and oranges too much for my taste. Variant in no way helps with the runtime polymorphism, a very useful concept. It's useful enough that you see it in various C (not C++) libraries and even in the OS interface.
Yes, that's to be expected, but what code size/performance impact did you measure between two versions of the code? I hope you didn't just slap `noexcept` on the code that otherwise isn't (didn't rewrite it to be noexcept), and looked at the disassembly? That would put you in the "fools" category in my eyes, you know.
No problem, I "noticed" because I've been a big fan of this particular "alternate proposal" of a language-based variant since it was discussed at Kona in 2015, and was disappointed that it wasn't adopted for C++17. The paper does a better job than the article of laying out exact "before/after" examples showing the verbosity and non-intuitive nature of the current approach. Maybe it will be accepted for C++20... 
Not only compilers don't do it, but they can't as per the standard. The "moved from" is different from destroyed. Yeah, I hate that state too and Rust is superior in that area. To nitpick, a default smart pointer needs no checks, because `operator delete` does the check. I guess it's because `free` does it. (Don't know if standard requires a check though, just thinking out loud).
It would be a worthwhile project! I would take ABI requirement out, because * impedes optimizations * I am a big believer in cross-language integration tech. Intra-language is actually easy. * "formal" ABI is but a small part; interface evolution is **way** harder
&gt; but they can't as per the standard. The "moved from" is different from destroyed. but is it something that could be detected r.e. inlining. if if the move is along the lines of ```..blah blah blah .. src.ptr=nullptr;```, and the destructor is ```if (this-&gt;ptr!=nullptr) { ....}``` ... could this be detected by a statically inferable replacement of the variable with a symbol that represents a 'nullptr' ... by some fully general purpose mechanism, that might not even know anything about move semantics.. just logic in the compiler middle / back end. perhaps it's even possible at the level of LLVM .. I realise this is only applicable in cases where the code path is truly predictable. (conversely, a stronger version of move would also be preferable.. perhaps requiring an intrinsic 'forget(x)' as I think rust calls it )
&gt; To nitpick, a default smart pointer needs no checks, because operator delete does the check I wonder if its then safe to elide the delete call altogether if theres a statically known null value
Oh, I don't disagree: this is why I said this should've been a language feature. Since C++ is pretty big on zero-cost abstractions it's pretty sad that the correct implementation is a suboptimal one.
I would be careful there. That tagged unions withered on the vine in the imperative world for several decades is probably due at least in part to the opposite belief: that inheritance was a superior take on variants. I know Modula-3 dropped the variant records from Modula-2 in favor of objects and inheritance, saying that they "are more general than variant records, and they are safe". It would be a shame to come to our senses only to commit the opposite error. If nothing else, I suppose this very long detour in language design has given us imperative languages with much nicer tagged unions that the ones from the 70-80s. It would be nice if the same thing happened to inheritance.
Isn't the move assignment already being called if you were to just pass do_something(p); ? Since the unique_ptr can't be copied. I was under the impression that std::move is basically just a cast into a rvalue anyway, so the destructor would not be called. Now the unique_ptr's ownership is the function do_something. The destructor will be called when do_something's scope ends. I think if you were to do unique_ptr&lt;Foo&gt; p = ...; unique_ptr&lt;Foo&gt; q = std::move(p); p would now be null. There is an explicit member function for unique_ptr which can be used to check for this
Well, it's a trend: "If it can be implemented in a library then it must be". The language started with way to many features, many of them mistakes in hindsight. Now we have shifted to putting language features in a library (e.g. std visit)Pattern matching), and are starting to realize that some of them are mistakes (c++17 adds some pattern matching to the language). Hopefully the next step will be a more balanced language design.
For all the complaining, I didn't see anybody proposing how to deal with the fact that assignment in c++ is a "normal" member function. If it was actually destroyed, you couldn't assign to it anymore, and gone are the speedups of using move to sort a container etc ... is there an obvious solution to this that I'm missing? 
https://wandbox.org/permlink/B9QVmAg5es0BqSxU Runtime polymorphism has never been more beautiful.
https://wandbox.org/permlink/B9QVmAg5es0BqSxU Add a checkbox.
yeah p would be null (this snippet your present does illustrate the scenario I was talking about), but the point is the compiler will still insert a call to ```unique_ptr&lt;Foo&gt;::~unique_ptr&lt;Foo&gt;()``` and in turn ```delete p.ptr```. So my question is , after inlining etc.. can the compiler use a static inference that 'p.ptr==nullptr' to elide the delete call altogether. (does it have to be done at link time? incase the delete has instrumentation etc?) I wish we really did have a way to destroy/forget the value.
I'm not suggesting changing what move does: just suggesting compiler optimisation to eliminate a call, in a subset of cases where you can guarantee that the value really will be null. such logic could also presumably help give warnings (or even errors) if the moved variable is used in erroneous ways. It would be nice to close the gap with Rust here. Rust is demonstrating behaviour that we definitely want to replicate. I think it should be doable.
&gt; char * is why we can't have nice things. Would anything break if modern code just defaulted to using strings instead of char *'s? Only make const char * literals if the left side calls for it? There's basically no reason for them to exist any more except for backwards compatibility, and I think you could detect that.
hmmm I see, so the question is if p.ptr is nullptr and doesn't point to a memory address, does that still translate to bytecode instruction that the CPU executes? I am not super familiar with intel assembly but I've dabbeled in some motorola in the past and the format is usually &lt;instruction&gt; &lt;argument&gt;. At first thought an implementation could either 1. have a compiler check for special nullptr arguments and compile in a thrown error if that argument is passed 2. it could compile out that operation. 3. it could throw an error at runtime I'm not familiar enough with it to know if there is a standard behavior for that. Some of the stuff in the past I've been surprised isn't standardized and other stuff I've been surprised it is
yeah it would be pretty easy to verify exactly what the compiler is doing, as I wrote this I realised you might need to defer this to *link time* optimisation, because the C++ back end can't assume what 'operator delete' does (wont it be library code?.. it might have instrumentation in a debug build, e.g. counting the number of times it's called etc.) perhaps the only way to be sure is a real 'forget this value..' feature (to implement a stronger alternative ```std::move_and_really_destroy_the_value```), I wonder if there are any proposals.
I think a lot of code would break. Another bad thing about this is that `const char []` string literals are `constexpr` friendly, and `std::string` isn't because it may have to make a dynamic allocation. So a lot of `constexpr` string manipulation code may get broken. If it is binding `auto` to the string literal, it may defeat your "only make `const char *` if the left side calls for it" thing. I have several times used `auto` with string literals because I know it will become a `const char (&amp;)[N]` of the right array bounds on its own and save me a lot of typing.
fair point there
Nice post. You don't need the .sln in appveyor to build, I think you can do something like: build_script: - mkdir build &amp;&amp; cd build - cmake .. - cmake --build . --config Release
Why not have class std variant have an overloaded operator () that takes as many functions as parameters as the vsriant has types, one function per type? It's much easier to implement. 
If the compiler can prove it can get rid of a call, it will already. Proving it is hard though. Keep in mind that std::move doesn't actually move, it just enables moves. In your case std::unique_ptr&lt;Foo&gt; p = ...; // construct a 'Foo' in an owned ptr do_something( std::move(p) ); // consume the 'Foo' // unique_ptr&lt;Foo&gt;::~unique_ptr&lt;Foo&gt;() is now called on p, // can it be elided, by assuming the state 'move' leaves it in ? Two options: * do_something takes the unique_pointer by reference. Then we need the definition of do_something to prove if it was moved from or not. * do_something takes the unique_pointer by value. Then (semantically) the caller constructs a temporary value from the unique_ptr (with a move) and passes that temporary "by ref" (semantically) to the function. An optimizing compiler gets rid of the temporary (so no destructor call) if you don't do anything with the original afterwards, so there you already have your "optimized move". To know if do_something then moved from the temporary, we again would need the definition of do_something ... so, if the compiler has the definitions available, it already optimizes this stuff out. But without whole program optimizations, it often has not, and then it can't.
Not true at all, at least in theory. Have you ever heard [Homomorphic Computing](https://en.wikipedia.org/wiki/Homomorphic_encryption)? You can give encrypted unprocessed plain text data to your processor, so that it executes, and you get back encrypted plain text data back and decrypt to a processed data. In pratice homomorphic computing is very slow, and most known methods require your program to be very short (a few logic gates) but it's certainly possible from a mathematical point of view.
Correct answer: [Homomorphic computing](https://en.wikipedia.org/wiki/Homomorphic_encryption) If you have a very sensitive executable file that cannot be exposed to anyone, you're kind of doomed. Because as other answers said, if you encrypt your executable, then it's not "executable" any more. You may obfuscate it, but this does not guarantee and if there is a very determined, motivated and skilled attacker, they can comprimise your executable, at least to some degree. But there are actually cryptographic schemes that let you Encrypt your data to `E(x), E(y)` then execute to `E(x.y)` and finally decrypt `x.y`. This is called homomorphic encryption. If you know some algebra (group theory, ring theory etc...) you already know what homomorphism is, so you can already understand what I'm talking about and the power of such cryptographic schemes.
This might be the first time one line of code make me laugh out loud quite literally.
ReSharper C++ should have been the first item on that list.
Another trend is many people are using non-monospace fonts nowadays, where alignment makes no sense.
Funnily, this and constexpr-if solution explicitly enumerate the alternatives, just like pattern matching in ML, and nobody is complaining about that. 
Oh crap you're right, why didn't I think about this before! I just have to write bug-free code on the first try and everything will go smoothly!
I personally use switch on the `std::variant::index` member function and select the appropriate case. Otherwise `std::holds_alternative` is enough to me.
I agree with the general sentiment of this article. `std::visit` is way cumbersome than it needs to be. I gave a related talk and created a C++17 library for those interested: * [Implementing variant Visitation Using Lambdas - Vittorio Romeo [ACCU 2017]](https://www.youtube.com/watch?v=mqei4JJRQ7s) * [`scelta` - C++17 library](https://github.com/SuperV1234/scelta): zero-overhead syntactic sugar for variant and optional.
Isn't variant::holds_variant() way easier in the if constexpr method?
Ah, that would possibly be a quite interesting compiler optimization case (interesting because the situation should occur happen often).
*bitsets* are a nice feature. The following code is broken. Does-not-compile-broken. But just for `T == bool`. In real-world applications it might be hard to notice, and people might get weird compilation errors very much down the line: template&lt;typename T&gt; class Object { std::vector&lt;T&gt; values_; public: Object(std::size_t s) : values_(s) {} T&amp; get_value_ref(std::size_t i) { return values_[i]; } }; 
Ok, good example. Especially since I've written 4 separate GUI toolkits in my life (two while still in university as an exercise, one more as part of an emulator for the Amiga, and another which started life as a thin wrapper over win32, and by now is a full-blown GUI toolkit that supports win32 and x11. So I've written an _actual_ checkbox, rather than an academic notion of what a checkbox might be. My actual checkbox has: - 4 constructors. - 3 public functions. - 11 private 'override' functions to respond to system messages. - 1 additional private function. - 14 private data members. ... and of course it inherits from control. Adding a single function that pretends it's a checkbox, and only writes the word 'checkbox' to cout, is easy as pie. That's not a checkbox though. If you were to implement a real checkbox that way, all those private functions and members would have to become public, and all of a sudden the implementation of checkbox would become part of the global state, and could be manipulated in all sorts of unexpected places. That's lousy engineering. Moreover, you are skipping around the original question, which used variant and type switches. If you use a variant to implement a checkbox, you'd basically end up with a control class that can morph into any type of control. My toolkit supports 43, some small (like checkbox, at 280 bytes), some larger (like grid, at 1144 bytes). Any time you'd instantiate a small control you'd still pay the memory cost of the largest control though, so that's a rather wasteful solution. Type switches are a bigger problem. There are 37 events controls can override, if need be, so in those 37 places you'd find all of the relevant code to deal with all 43 controls. Instead of the control class being 1006 lines, it would end up at around 100,000 lines - containing the entire implementation of all the controls. In exchange for this total maintenance failure, you'd then get... Let's see: the ability to store controls in a vector (I have to make do with pointers to controls), and the ability to morph a control into a different type of control. Which I can do anyway, since I can replace those pointers as well if need be. So no, I do not agree that variant+type switches is a better solution. On the contrary: I think they indicate a design failure, and I believe that any system using them is essentially doomed as soon as it grows above a certain size, when those 'easy' solutions turn out to be completely unmaintainable. 
Quick thoughts/feedback: * Why is this library better than the other thousand already-existing argument parsing ones? Show a code example on `README` that "sells it". * Don't call it optional, especially when `&lt;optional&gt;` is standard in C++17. void newFlag(std::string name); void newDouble(std::string name, double fallback = 0.0); void newInt(std::string name, int fallback = 0); void newString(std::string name, std::string fallback = ""); * You take `std::string` by value and don't move it. If you want to observe, just take it by `const&amp;`. If you want to consume, take by value and then `std::move` it. * You use `newXXX` where `XXX` is a type. Have you considered simply using overloads here? * There's major code repetition in the implementation of the `newXXX` member functions. Templates are your friends. Option* option = new Option(OptionType::Flag); * Don't use raw `new`/`delete`. Use smart pointers. Actually, do you really need explicit dynamic allocation for argument parsing? * You're using `endl` everywhere. It unnecessarily flushes `cout`. Use `\n` instead. tl;dr: a lot of common "beginner" mistakes and of "modern feature" usage. I would advise reading "Effective C++" and resources on C++11/14/17. 
Instead of 'imagining', why don't you actually measure it? Preferably before complaining about it? Just write a function that allocates a resource, and then moves it. See if it makes any difference if you test the resource for nullptr after that (by timing it, not by looking at the assembly). And, of course, share your results here! There are some practical considerations as well. If a move really meant the lifetime of the object ended, code like `std::swap (a, b)` couldn't work anymore. And by extension, things like std::sort would also become impossible. 
For me personally: alignment makes big blocks of assignments easier to read, but that's ignoring the real issue which is that large number of local variables is a code smell.
&gt; why don't you actually measure it because I don't need to: **no work** is **always** going to be faster than **some work**. We need to profile when there's controversy over *what* work is faster, not to verify that '0' is less than 'some finite positive number'. thinking about it here, whats revealed is that it should be possible through some general purpose mechanism (i.e. identify a situation where a value is known to be some constant in a code path), as such it should be an optimisation that can yield benefits in multiple situations. Given the amount of C++ code in the world, it's highly likely that the effort in a compiler backend will save enough energy to make implementing it worthwhile, and done right it should be able to yield additional benefits (like the warnings about using an invalid value). or conversely, the machinery to do it might *already be there* (r.e. static analysers). &gt;There are some practical considerations as well. If a move really meant the lifetime of the object ended, code like I'm not suggesting changing the behaviour of std::move; just detecting where it *can* be treated as an end to the object lifetime. (but further to that, I think *another* stronger form of move would also be beneficial).
&gt; If you want to observe, just take it by ~~const&amp;~~ std::string_view. 
The python interpreter asks for the resources it needs. It doesn't start a whole fucking VM and provision 2 gigs of ram because it thinks you might eventually one day want that.
I love the no-fuss, to-the-point writing style. I was wondering, are there things that can be done with concepts that cannot be done with c++11 template magic (because `is_detected` is basically c++11) -- or that can be done orders of magnitude more easily? (I'm waiting for concepts for their self-documenting characteristics and the nice compile error messages, but I never found myself not being able to do something with c++11 template meta-programming that concepts would have solved easily)
Personally I think the simplest thing is not so much a fork of C++ but rather [cross-compiling into C++](https://www.youtube.com/watch?v=LmRTPweKGmQ). And yes, cross-compiling specifically into C++, not into C or some LLVM back-end. This way, you get all (well, most) of the features you while, at the same time, relying on mature and powerful compiler that lots of effort already went into. And you can add your own stuff, so if you want something like reflection or ARC/garbage collection, you can have your cake and eat it too. It wouldn't compile any faster than C++ itself compiles though, for obvious reasons.
With auto I'd try to make it a string and see if it causes any substitution errors, and then try again with a const charstar string. I don't think this is actually good behavior by default, though. Many new programmers get burned when they try to auto a string literal. Good point about constexpr... hmm. Isn't the C++ array class constexpr friendly? Maybe an array&lt;char&gt; then with syntactic sugar becoming the default string class? Honestly, it seems like a big mess to implement, but at the same time, charstars and #include are the two ugliest parts of legacy that C++ has to deal with.
You are doing the Lord's work.
Thanks Andrew for leading this fight (along with being our most stupendous EWG scribe)! I may routinely hack on clang, but only because of your compiler and its wonderful debugging IDE (that also keeps getting better! :) (and perhaps some historic inertia ;) Indebted.
With concepts, you can control which of the viable overload will be picked. The [example](http://en.cppreference.com/w/cpp/language/constraints) in the cppreference (see "Partial ordering of constraints") is pretty self-explanatory: it picks the more constrained overload if possible, even if the less-constrained is also viable. Also concepts as "more-constrained auto variables" (`int main() { Concept x = foo(); }`) was also reasonably impossible to achieve with SFINAE.
Well, actually, come to think of it, I've just realized that compilation is faster too. The reason for this is that when transpiling, all your classes/categories/functions compile to a single file. And this is where at least part of the C++ compilation process lies: each CPP file builds its own world. Since that world is only built once, you have less of a problem. With a precompiled header, you get even less of a problem. Ultimately, when we move to modules, things will become even more tolerable. It's just sad that macros weren't killed. Macros must die.
i.e. measure on Compiler Explorer (https://godbolt.org) with GCC or Clang at various levels of optimization.
https://www.reddit.com/r/cpp/comments/703k9k/stdvisit_is_everything_wrong_with_modern_c/?st=J7LS4Y3H&amp;sh=edd22b15
&gt; no work is always going to be faster than some work. Does it matter in an actual, real-life application?
I had no idea Concept x = foo(); was a thing. It's beautiful.
String literals already have the type `const char[N]` where N is the size of the string including NUL. Arrays just really love decaying into pointers.
I would already be happy if we could just [break all the eggs](http://scottmeyers.blogspot.de/2015/11/breaking-all-eggs-in-c.html)
I'm curious, how did you find the experience of patching GCC? I know historically GCC patching hasn't been a particularly nice experience as compared with, say, Clang. Has this changed?
&gt;no work is always going to be faster than some work. That's most certainly not true on modern CPUs, where work can happen in parallel, and where the cost of an instruction is about 200x less than the cost of a non-cached memory access. &gt;thinking about it here, whats revealed is that it should be possible through some general purpose mechanism It's called an "optimizer", and compilers already have them. Seriously, _measure first_, complain later. Right now you don't have a clue what's really happening, or if it would really make a difference or not. 
&gt;interfaces are awkward when you want to add new functions (you have to update every class that inherits that interface) Not if the default behaviour of the new function (in the base class) is identical to the old situation, before the new function got added. That way you only need to implement the new function in classes that actually need the new desired behaviour, i.e. all the places you would need to write code for anyway. 
I may not be up to date, but why is string_view better in this case? If he wants to use the full string this should make no difference, or does it?
I agree with everything you say, but please, be a bit more friendly in your approach. You might just have shot someone down so bad that they never want to contribute anything in fear of harsh critique.
Sloppy spelling: &gt; Abbreviated &gt; abdriviated &gt; abriviated 
The difference is that given void foo(const std::string&amp;); void bar(std::string_view); you have : foo("i like my bananas raw on apple pizzas"); whic allocates memory vs bar("i like my bananas raw on apple pizzas"); which does not
&gt;&gt; "no work is always going to be faster than some work" - &gt; That's most certainly not true on modern CPUs, where work can happen in parallel you're taking the piss now, surely. the absence of an un-necasery operation will allow other useful operations to fill those pipelines/slots. for example, from another hyper thread. &gt; Seriously, measure first, complain later. I will look at the assembly. the point is [Rust](https://www.rust-lang.org/en-US/) can do *no* work in this scenario, by design, [because it destroys the value](https://rustbyexample.com/scope/move.html) altogether, at the semantic level. What I want to verify is if they already jumped on this opportunity in C++, to do no work, in the case where no work is needed. (elide the call to delete if a value is moved, and never used again). I get the value of profiling, but that doesn't invalidate common sense. Profiling is for fine tuning and discovering things you *dont* know. my question is aimed at discovering what is already known about the state of existing compilers. I *know* C++ toolchains have had a huge amount of work done on them so it's possible this already happens, but if not, it's an idea for improvement.
&gt; Does it matter in an actual, real-life application? you create a capability in the tools, then it simplifies their use. If you can count on the compiler to do this, it changes how you can program. if it doesn't show up on the profiler , it might be because something else is slowing it down. Given the number of people on the planet using C++, we can work on every potential issue in parallel . We don't need to wait . This might be a scenario that recurs, e.g. a user checks 'if something is null', then calls something who's first act is 'check if something is null'. if you can automatically strip that out, it saves people having to check 'if they need to check.' .. or it could give you a warning about it, helping you to figure out a better architectural choice.
did you get the idea that the work for this should overlap with detecting if a moved value is used erroneously? thats another thing rust has over us. It eliminates thats class of errors at compile time.
&gt;I will look at the assembly https://godbolt.org/g/qcx1VG Apparently you haven't, because you can clearly see the optimizer does in fact remove the delete. 
i never said I had looked already. I was asking here, before I looked. I figure i'm probably not the first person in the world to wonder about this, so someone might know already. &gt; because you can clearly see the optimizer does in fact remove the delete. good, someone has common sense it seems.
I apologize if my tone seemed unfriendly/discouraging, that was not my intention. Is there any sentence I should rephrase? Can't see any particular offending statement.
&gt; and where the cost of an instruction is about 200x less than the cost of a non-cached memory access. yes I know all that too, i've seen scenarios where cache misses are "600 cycles". and that doesn't change the fact that **NO WORK IS FASTER THAN SOME WORK** .. indeed it makes eliding little bits of work even *more* important, because it might look simple, but it actually results in spilling something or whatever. Again thats' something I've seen.. an innocent little constructor clearing something, very little cost in assembly terms but it did exactly that (spilling), and the solution was , in line with straightforward logic, avoid doing the un-necasery work of *clearing* if you're about to *overwrite it with another computation*. there's a wide variety of scenarios, I've worked on platforms where branches are extremely expensive (by virtue of precluding other optimisations) .. which is why I've ended up being quite fussy about eliding checks (amongst other things).
I can't wait for this to be in the standard.
I'd get rid of files as a storage medium for source code. Files are the source of all problems related to build systems, build performance, dependency tracking, etc. They cause untold grief, even if we managed to build some impressive infrastructure around them. So what would I use instead? The answer is 'a database'. I'd store each class, each namespace, each global, etc. as a database entry. I would completely get rid of the notion of including things, but I would allow databases to refer to each other instead. The objects stored in the database would be dependency-ordered, but this would happen automatically, while you're working on it. For individual objects, precompiled versions may be stored with each object to allow (much!) faster (final) compilation. The database could represent pretty much all of current C++, with the exception of things that have file-scope (since there are no more files): statics, unnamed namespaces, etc. Also, extern would be redundant. I realize this is thinking way outside the box, but hey, you were asking for it... 
The least cumbersome way to use C++ variants for me is by converting them to pointers, eg.: Variant&lt;string, int, double&gt; my_variant = 10.0; if(const double *value = my_variant) printf("Double: %f\n", *value); else if(const int *value = my_variant) printf("Int: %d\n", *value); else if(const string *value = my_variant) printf("String: %s\n", value-&gt;c_str()); std::variant supports something like that with std::get_if.
I've found another scenario where it doesn't seem to (moving it into a function argument, i.e. 'a function which consumes a value'). So , seems there is infact more work to do. 
Awesome work. I want this feature *very* badly.
&gt; That's most certainly not true on modern CPUs, where work can happen in parallel Now the case where 'no work *isn't* faster than some work' is where you're comparing '*doing the work, and repeating the work*' vs '*doing the work storing it, loading it elsewhere/elsewhen*'. ...the counter-intuitive behaviour being that *sometimes* repeating work is faster (due to memory/communication cost). That doesn't breach the principle that *no* work is faster, in cases (like this) where we're talking about a redundant operation that can be eliminated entirely. And even then it's really comparing the work of loading/storing, vs the work of computation.
I haven't tried that check but my experience consisting of many years of disappointment with static analyzers tells me that if this "simple" test: #include &lt;vector&gt; #include &lt;iostream&gt; int main() { struct A{ std::vector&lt;int&gt; a{1}; }; std::vector&lt;A&gt; as(1000); auto b = std::move(as[100]); std::cout &lt;&lt; "use-after-move: " &lt;&lt; as[100].a.size() &lt;&lt; std::endl; return 0; } does not break it, this one definitely will: #include &lt;vector&gt; #include &lt;iostream&gt; #include &lt;mutex&gt; #include &lt;thread&gt; int main() { struct A{std::vector&lt;int&gt; a{1}; }; std::mutex m; std::vector&lt;A&gt; as(1000); std::thread([&amp;] { std::lock_guard&lt;std::mutex&gt; lock(m); auto b = std::move(as[100]); }).join(); std::cout &lt;&lt; "use-after-move: " &lt;&lt; as[100].a.size() &lt;&lt; std::endl; return 0; } and it isn't much harder to make the vector a global protected by the mutex, launch a couple of threads that randomly move some elements out of the vector, join them, and then in the main thread loop over all elements. I read in reddit often that one can retrofit Rust-like static analysis onto C++. Rust does not only catch these both cases at compile-time, but in both cases provides an explanation of what exactly did you move where and when, and why exactly what you are trying to do is wrong. Given the pace of development of C++ static analyzers over the last 20 years, any estimate of "C++ will catch Rust on static analysis" that is under 100 years is probably wrong unless many major unexpected breakthroughs happen in this field. We don't even know if we can ever get "use-after-move" always right in single-threaded applications, and this is just one of many possible types of memory-related errors. After covering all of those one would then need to cover C++ type safety errors (like using the wrong member of a union), which don't even have a timeline yet. And after that be able to do analysis on multi-threaded programs... Sure clang-tidy is better than nothing, but if you are using modern C++ clang-tidy is probably worse than nothing, because it discovers too few memory errors, and has a very large false positive ratio.
Curious: why doesn't the second has to allocate memory? 
&gt; It doesn't make sense to define a struct that you are not going to reuse. Sure it does. Not everything has to be reused. Your software isn't bad if everything isn't reused and reusable. &gt; it confuses the purpose of having a named struct/function in the first place (reuse) The only purpose of naming things is absolutely not reuse. Naming the constructs in your software can make their intentions clear, and is more maintainable, especially in a large project, if for no other reason than *you can actually find the thing by its name.* You also get all the benefits of having an actual struct/object. &gt; Lambdas look like the perfect fit for the job: You need to do a specific thing with a variant, well you define a lambda right then and there. Except you can't just do it the way the article settles on "right then and there". You jump into a header file to define your templates that keep the whole thing from falling down. I would also point out that if using lambdas for this, you have no good way to hold onto state between visits. A struct or object can easily encapsulate that state with member variables. You really don't need to use lambdas. If anything, the article shows clearly that in the current form of std::visit, they aren't worth the trouble. The author even says this, although he does so in the course of dismissing std::visit altogether, because he dismissed the SettingVisitor methodology out of hand because it's not cool enough.
My guess is int* ptr is the most common, but it does not really matter.
See the [the dedicated part](http://www.stroustrup.com/bs_faq2.html#whitespace) in Bjarne Stroustrup's technical FAQ.
std::string_view is basically the same than void bar(const char* str, size_t size); here the raw string isn't allocated, it's stored in the data of the binary. https://godbolt.org/g/nRZRxb 
*as long as you're not declaring more than variable at a time. Otherwise, int* x, y; will be very different from int *x, *y;
&gt; With concepts, you can control which of the viable overload will be picked. The example in the cppreference (see "Partial ordering of constraints") You can do this on C++11 as well using plain old function overloading: - The tool is [`range::v3::most_refined`](https://github.com/ericniebler/range-v3/blob/master/include/range/v3/utility/concepts.hpp#L217). - An example would be the [`reverse`](https://github.com/ericniebler/range-v3/blob/630fc70baa07cbfd222f329e44a3122ab64ce364/include/range/v3/algorithm/reverse.hpp#L61) algorithm, which uses [`iterator_concept`](https://github.com/ericniebler/range-v3/blob/f24d3490c0eddb51de20da1d3b14bf91a4a64ff4/include/range/v3/utility/iterator_concepts.hpp#L456) to [overload on concepts](https://github.com/ericniebler/range-v3/blob/630fc70baa07cbfd222f329e44a3122ab64ce364/include/range/v3/algorithm/reverse.hpp#L48). &gt; Also concepts as "more-constrained auto variables" (int main() { Concept x = foo(); }) was also reasonably impossible to achieve with SFINAE. The troubling thing is that depending on where a `Concept` is used, it can mean very different things. Using shorthand notation (not standardized, but also applies to the "verbose notation") void sort(Sortable s); `Sortable` means "_all types_ that implement `Sortable`", and `sort` is not a function but a _function template_. When used in return type position or as a variable name: UnaryPredicate foo() { return [](auto x) -&gt; bool { true }; } UnaryPredicate bar = foo(); here concepts mean that "there exists _a single type_ that implements `UnaryPredicate`". That is, `foo` and `bar` are a concrete function and a concrete variable, they are not a template function and a template variables like in the case above. Suppose that after we get some experience with concepts we decide that C++23 ought to support concrete types on function arugments, type erased by a concept. Well we couldn't use the same syntax as for `Concept variable` because on function arguments it already means "template"... This is close to borderline negligent programming language design.
Ah, i see. Thanks!
Static analyzers can't reach Rust level because today's C++ is simply relaxed enough for this kind of stuff to be undecidable. [Check this](http://nosubstance.me/post/dereferencing-null-pointers/) for example. Rust is stricter per design, which can allow for this to be decidable.
thanks for the link
In C, not C++, all declarations follow a certain form. In `char **args[]`, if I were to write `**args[3]`, the result is a `char`. (C++ adds references, which don't make sense in this way.) So as far as the parser is concerned, they make more sense by the identifier. I like putting them by the identifier, but it really doesn't matter. One identifier per declaration is a hard requirement, though.
It's not unfriendly, but it is a bit condescending. What you say implies that the author is not up to snuff. It is true what you say, but I believe it's said in a bit too direct manner.
*If* we're gonna have noexcept autodetection for *this,* wouldn't it be nice to have a general syntax that autodetects noexcept for functions (and non-abbreviated lambdas)? E.g. inline // cause if noexcept is gonna be part of the type system, this obviously won't work for forward-declared functions void foo() noexcept(auto) { // noexcept if and only if all statements in the function body are noexcept }
We actually slapped on `-fno-exceptions` to the compiler flags across the whole system (or equivalent on whichever compiler it was, possibly IBM XL). My recollection is a 5-10% performance improvement. In operation, this code does not throw exceptions, *ever*. Anything that would throw an exception is essentially fatal in our environment, so we just don't use them. We see a failure, and we `abort()`
&gt; The fact that we still handle dependencies in 2017 by literally copy-pasting files into each other with #include macros is obscene. So what does the author propose - just dumping everything redundantly on GitHub and make offline builds impossible?
It seems to actually be a review of clang-format, not what's new in v5.0 of it. Don't get me wrong, the article is absolutely right that if you're not using it already, then start. It's just not titled appropriately.
&gt; Pro-tip 1: Throw away your "coding style" document Unfortunately clang-format alone doesn't make this possible. There's still the issue of naming things. clang-format is almost entirely whitespace-only. There's a few exceptions like re-ordering includes, and now adding comments to closing namespaces. I think that clang-tidy does have checks though that let you provide regex for various things like classes, functions, etc and ensure that everything is conforming.
Sorry for the basic question, but what does the nested `noexcept(noexcept(...))` do? Why not just one noexcept?
That's a different feature, with different pros and cons from abbreviated lambdas.
xlc? I still have twitches from *that* bugger! üòäüòäüòä So... is that code using operator new, or STL, or std::string, or...? And if yes to any, did you use `set_new_handler` to modify operator new not to throw? If no, then your code was one big UB (in fact, it is one big UB even if you did change operator new). (With a low probability of actually invoking it, but still). So you are **exactly** the kind of fools that I speak of earlier (no offense meant üòâ). And yes, 5-10% improvement is reasonable, but! The code is functionally quite different! Before, it would clean up after itself (e.g. gracefully close any I/O etc). If you look above, you will see that I speak of exactly that. The "problem? abort()" strategy is valid, but only if the outside world is not affected by a sudden crash, and that is not a given. So for example, you leave a half-written file. Or a network client hanging for a long time. Or some transaction in limbo etc. But in general, it's a poor thing to do, it opens the probability to a whole other host of failures. It kicks the can down the road, really.
Exactly. The committee has been discussing reflection for years, and only recently got a general direction. Heck, Concepts took 9 years, and many aren't happy as it is.
Seems more of a combination of an introduction and the list of new features from v5.0
Does it do Whitesmiths yet? :P
Just wanted to post it looked a lot like the talk from Sean Parent, in my initial read i missed that you mentioned that specifically. Its great code by the way, so thanks for blogging about it! A talk of him that talks into detail about this is https://www.youtube.com/watch?v=QGcVXgEVMJg Would be cool by the way if you would add a link to the complete example code on https://godbolt.org/. Then people can immediately start playing with it as well.
This is awesome. Lambdas are one of the best features of C++11, and generic lambdas are my favorite feature of C++14. Having abbreviated lambdas would be my favorite feature in whatever C++ version they (might) get into. Now, if only we could have _Hyper_-abbreviated lambdas: transform(vec, out, [](auto&amp;&amp; x) =&gt; x * x); Could become: transform(vec, out, @(@1 * @1));
Thanks! And indeed, uploading an example on godbolt is a great idea. I think I'll do that and edit the article to add the link.
The proposal already supports this syntax: transform(vec, out, [](x) =&gt; x * x); which in my opinion is clearer than what you propose.
~~If most tooling around can't cope with the diverse conventions for how correctly to put a tab, it must mean something about them. Glad I'm not a tab person and can leave a machine's job for machines.~~
I believe it means that the lambda is noexcept iff test(x) is noexcept.
The current release of MSVC already has support for modules with the switch `/experimental:module`. The APIs over IFC and the documentation of the IFC format are in development and will be released when ready. I do not have ETA.
Sorry, don't understood this comment. clang-format handles tabs just fine (not that I am pro-tabs).
Travis doesn't supports ipv6 sockets and hence any tests or similar programs that need to run during your CI execution pipeline. I discovered it during building a personal project of mine ([network programming library](https://github.com/c10k/net/tree/develop)) and was a bummer since travis integrates nicely with github. For now I'm using http://www.wercker.com/ as alternative and you can see my config (which tests multiple compilers and versions and even runs valgrind tests) here - https://github.com/c10k/net/blob/develop/wercker.yml 
&gt; All polymorphism is done transparently, and hidden in implementation detail. Just so you know, this sentence is completely self contradictory. Transparent means that it is visible. The thing about this whole article is that it's predicated on a very rare scenario: that an interface defined in one place, will be met by a type defined in another place, by coincidence. This makes the adapter completely trivial to write, and then you are saving a lot of boilerplate when you use the technique presented. But much more realistic is that either the type was explicitly designed to that interface and inherits, or that the adapter is not completely trivial. In this case, this approach doesn't buy you much. I think the technique is very elegant, I always have from the first time I saw Sean Parent's talk about it. But I think for *most* classes, the benefits are relatively small. Your perks section is pretty unbalanced, I think. First, you should mention the detriments as well: you have to repeat every part of your interface 3 times. In traditional polymorphism, you would just write it once. Second, even most of the perks are not directly tied to Concept-Model, or at least not to the whole thing. Value based semantics come from storing a `unique_ptr` with the same interface, so you basically buy value semantics for one repetition of the interface. That has nothing to do with whether it's worth repeating the interface again, to make your polymorphism non-intrusive. Also, it none of this helps avoid heap allocation, in any way. The equivalent of this code: some_task t; // do some stuff with task t.stuff(); // maybe push the task if (condition()) { push(std::move(t)); With regular polymorphism is the same, except for the last line: if (condition()) { push(std::make_unique&lt;some_task&gt;(std::move(t))); It has the same heap allocations in the same places as the original code. Finally, I don't understand the supposed benefit of preventing the polymorphism from being "scattered" around. If you are writing types with the goal of meeting an interface, inheritance does this in a nice explicit way and gives you immediate compile errors if you don't meet it (as opposed to waiting until it's used). Bottom line this is a nice technique for certain purposes and worth knowing about, but I've found in general there is too much hype surrounding it. Structural polymorphism is nice, but C++ does not have it built in, and in most situations you are better off just use traditional inheritance.
From the article it seems like Rust bests c++ in terms of energy efficiency. I noticed that they are using GCC for the compiler. I wonder what it would be if they used Clang and maybe Intel icc. One of the nice things about C++ is it's maturity and that you can find several very good optimizing compilers and for a given workload, one compiler may do a better job optimizing.
The proposal mentions that the brevity of Elixir, Swift, and Scala are sacrificing in clarity, but I think that's just a matter of not being familiar with the syntax. When non-C++ programmers see code a-la `[=]() mutable -&gt; void { cout &lt;&lt; foo; }` the most common response is _???_. A C++ programmer who understands the lambda syntax and semantics knows perfectly well what the code actually does. In Elixir, the snippet `&amp;String.downcase(&amp;1 &lt;&gt; "-tag")` is idiomatic and commonplace. It's important to remember that these tiny code snippets never happen in isolation. The surrounding context always adds clarity: // What is the type of "foo"? auto foo = get_bar(); With snippets like this, _of course_ one would say `auto` reduces code clarity. But add some context and real variable names: void print_usernames(const Database&amp; db) { auto usernames = db.getUsernames(); for (auto&amp;&amp; uname : usernames) std::cout &lt;&lt; uname &lt;&lt; '\n'; } Looking at code snippets with placeholder names and no context is rarely a good way to judge code clarity.
I've read that too, most often when discussing visitors which are a pattern to cope with the lack of multi-dispatch. Of course, it's important to realize that efficient implementations of multi-dispatch for an open binary (one which loads binaries) is an open problem as far as I know.
&gt; clang-format is almost entirely whitespace-only I think I've misread that, you just meant focused on whitespace changes only, I think, regardless the type of whitespace. The thing is that there are some formatting styles that are hard to decide about when tabs are involved, like: fooo(a, b, c, d, e) { } For this style, most tools won't realize what the user wants before `d`, spaces or tab, both, reindent/realign arguments to attain tab-only, etc. I just use spaces, life is simpler.
Commenting the end of a namespace is probably the oddest rule I've ever seen in a style guide.
About the transparent part, thanks for pointing the error. I'll correct the meaning of the sentence. I still have to work my English! Also, for the avoiding heap allocation, just count the number of time in your code you're heap allocating some class because the class is polymorphic, but then never actually store it in a polymorphic context, like a type erased list or container. If you have none, congratulation, you have really well managed code! But unfortunately, it's not the case of most codebases. The number of time I saw Some class heap allocated because it *might* one day be sent into a particular function that happen to take everything as a `unique_ptr` is uncountable. Also, in the paragraph explaining how it avoid allocation, I explicitly said: &gt; There are also other strategies for avoiding dynamic allocation that I‚Äôll cover in other parts. I'll clarify this particular sentence too. Thanks for pointing this out. I planned to write about how be can use the idiom to use SBO, like `std::function` does. The part about writing the interface three time is unfortunately true. I'd really love the compiler to do this for me (hello metaclasses!) And lastly, maybe you don't see it as an advantage, but polymorphism implemented and used only in places where I actually need it makes the code easier to think about. Most variable can be carried as values instead of pointers all over the place, when you call a function, you know which function you actually call. But when using explicitly a polymorphic wrapper, you know for sure polymorphism is applied and needed here.
That diff between C and Fortran means I do not trust this. Something is wrong.
Thanks. It is a use of both the operator and the specifier. http://en.cppreference.com/w/cpp/keyword/noexcept
Rust scores pretty well, but tends to use a lot more memory. There's a whole section on time vs. memory and how to score them, and realistically it's going to depend on what you're trying to accomplish. &gt; If the developer is only concerned with execution time and energy consumption, then yes, it is almost always possible to choose the best language. Unfortunately, if memory is also a concern, it is no longer possible to automatically decide for a single language. 
Really? We were doing it before we knew it was a rule. It adds some clarity, especially if nested namespaces are not indented 
I think again one of the problems here is that two things are being conflated: value-based, and non-intrusiveness. Each of these costs you one full round of repetitions of all the interface. You can have either one without the other. &gt; Also, for the avoiding heap allocation, just count the number of time in your code you're heap allocating some class You are welcome to count; in the code I give above you still construct the original derived/implementation object on the stack, and then you move construct it onto the heap only if you push into the container. &gt; The number of time I saw Some class heap allocated because it might one day be sent into a particular function I mean, what you are describing is just a performance trade-off: if you construct the class on the heap as a `unique_ptr`, it's cheaper to pass to that function later. If you don't, you have to move construct the class. What you seem to be arguing is that somehow, regular polymorphism produces more of a tendency to heap allocate than concept model. There's no *technical* reason for this; if you want to construct a derived class you can just construct a derived class on the stack. If you want to argue that misuse of regular polymorphism is more common for some good reason, you should make that argument in your post, but you did not. &gt; I'll clarify this particular sentence too. Thanks for pointing this out. I planned to write about how be can use the idiom to use SBO, like std::function does. You can just write this as a generic class that combines `unique_ptr` and a stack based allocator. Again, this is totally orthogonal to Concept-Model. &gt; And lastly, maybe you don't see it as an advantage, but polymorphism implemented and used only in places where I actually need it makes the code easier to think about. Uhm, but you do need it when you are writing an implementation. Whether or not you explicitly inherit does not matter, this is trivial compared to writing out all of the correct function names, signatures, and implementations. &gt; Most variable can be carried as values instead of pointers all over the place, when you call a function, you know which function you actually call. I don't completely follow this, but yes I agree that value semantics has some value, more than non-intrusiveness, but this point doesn't help make a case for the later. Though, to be completely honest, I feel that the problem with the value semantic argument is that it's actually very hard to imbue full regular value semantics onto polymorphic types. Just try writing equality operators; rapidly degenerates into nasty double dispatch and RTTI. In most cases I don't feel like it's worth all the effort, just to make my polymorphic type behave mostly like a non-polymorphic type. I'd rather just accept that it's different, avoid writing boilerplate, and use it to get the job done.
&gt; Something is wrong. I suspect using the Intel compiler for Fortran would lead to a massive change. --- Beyond that, the programs are extracted from the Compiler Benchmark Game. The Compiler Benchmark Game explicitly attempts to re-create "real-life" scenarios, which may sometime prevent optimizations. On the other hand, said scenarios have not changed for years, so that a number of solutions have been optimized specifically for the current set of inputs (for example, finding a hash function which performs really well for a particular set and hash map implementation, but would provide dubious performance on random input). So, there are restrictions to keep in mind when evaluating the results, and surprising scores should be investigated (and explained), but it's otherwise one of the very few benchmark suites which are somewhat realistic, investigate a broad class of computations and are ported with the same rules to a whole lot of languages.
Actually, Rust is playing at handicap here: - LLVM tends to produce slower binaries than gcc in general (otherwise be sure that the C++ afficionados of the benchmark games would ask for clang++), - LLVM still cannot properly handle all `noalias` situations that Rust has (it's hoped that 6.0 and the NewGVN will allow enabling those annotations again), - Experimental features are not allowed, so Rust cannot use SIMD. At the same time, the difference between C, Rust and C++ are tiny, which is what really matters: it means the 3 are playing in the same ballpark, with individual implementation differences causing some fluctuations.
&gt; Rust scores pretty well, but tends to use a lot more memory. This is likely due to rustc using jemalloc by default, which tends to have a bigger initial footprint (per-thread slabs, ...). Normally you get better performance than with the system malloc, in a classic CPU vs memory trade-off. rustc could be instructed to produce a binary with another malloc implementation, and you'd likely see a drop in memory consumption and a surge in CPU time.
In our environment, the outside world isn't affected by sudden crashes. These are batch jobs run under scripts that see whether they succeeded or not. We are perfectly willing to accept that if we run out of memory, the world has ended, and UB can burn down our house.
After some years you don't care. All styles are easily understandable. If you want a rule, and consistency you should avoid int*. int* a,* b; (You should also use a auto formatter, then you can focus on the content)
Reading this I must agree and admit that every advantages I wrote in the article is perfectly doable without concept model idiom. Thinking about it, this idiom is a way of making these advantages simpler to use, maybe in some cases make them the default behavior, and sometimes enforce them. Not these advantages themselves. I should have written it with this in mind instead. You made me question my own opinion about that. And this is great. I'll work towards having a more balanced point of view about this in particular. 
Any energy savings are more than cancelled out by energy spent on online discussions of the placement of curly brackets.
From a theoretical perspective, like Stroustrup, I prefer int\*. However, I recommend against it. The reality of the C language is that * binds more closely to the variable name than the type name. The fact that most people now think this was a bad decision does not unmake the decision. Writing int* confuses non expert programmers about the way their code will be parsed and frankly provides very little benefit to the expert. 
Just don't declare several pointers on the same line and you'll be fine :p
Cool response, appreciate your open-mindedness. Look forward to the next one, especially how you implement the small stack optimization, cheers.
Thank you! &gt; You can do this on C++11 as well using plain old function overloading: I was pretty much sure it's possible (I didn't know it yet), however, I'd argue it goes under "or that can be done orders of magnitude more easily" from the initial comment :) &gt; Suppose that after we get some experience with concepts we decide that C++23 ought to support concrete types on function arguments, type erased by a concept. I don't think this exact feature is possible since the current concept is not a type (or typeclass, or trait), but a constraint/predicate for a type, so I can't see how to type-erase by it. However, this syntax occupying a spot for a trait/typeclass-like functionality is probably even worse...
Hmmmm... What are your sources for that? I haven't seen recent comparisons between GCC &amp; clang/llvm, which is why I'm interested....
Just don't declare several anything on the same line ever... :P
&gt; Writing int* confuses non expert programmers about the way their code will be parsed and frankly provides very little benefit to the expert. I don't know about you but I read "int *variable" as de-reference 'variable' and "int &amp;variable" as "take the address of 'variable'". Mixing the concepts for declaration and operations just adds to the confusion. At no point is "int* variable" ever going to be confused for "wait why isn't that de-referencing 'variable'?" because it's not valid syntax.
Mhm I saw the same title over at /r/rust today
I definitely do not ever read the * in "int *variable" as the dereference operator. That is simply not what that means in C++. In C++ the * in &lt;idendifier&gt; * &lt;identifier&gt; cannot be the dereference operator. The expression parses as a either a variable declaration or a multiplication expression.
Clang-format deals with tabs just fine. There are different options: You can have it mix tabs and spaces, telling it how wide a tab is and it will do all indentation and alignment using tabs, with spaces just to fill in areas less wide than a tab. This mode is of course an abomination and should never be used, and if you manually format code this way then _you_ are an abomination and should never be used. You can have it mix tabs and spaces according to the "tabs for indentation, spaces for alignment," rule. This is the long hypothesized but rarely observed 'correct' way to mix tabs and spaces, but which was never practical to do manually. It keeps the code correctly aligned no matter what width is configured for tabstops, allowing different users to view the code with their preferred indentation width. Still, until clang-format usage becomes universal avoiding tabs entirely is probably best: If your project doesn't have pre-commit hooks configured to reformat your code then you need to use spaces. If it does, then the reformatting will add tabs for you if necessary. Meaning that either way you don't need to configure your editor to use tabs.
&gt; I don't think this exact feature is possible since the current concept is not a type (or typeclass, or trait), but a constraint/predicate for a type, so I can't how type-erase by it. However, this syntax occupying a spot for a trait/typeclass-like functionality is probably even worse... We need to figure out how to do this to get to `virtual Concept`s, definition checking, etc. I also think that the current predicate-based concept cheking makes it close to impossible (or probably impossible, since every constexpr computation is a valid constraint, and constexpr can do a lot).
Is variant even a good idea to begin with? It's inefficient, and it only simplifies iterating over elements while making everything else more complex. [a variant with a string is bigger than a vector](https://wandbox.org/permlink/ZpSgdItTQ8L6kEKF). Wouldn't it be more efficient to just use a separate container for each type? I don't think it's going to be any more complex to use either.
Same with Pascal.
We use clang-format since a few months and I really really like it. It's not perfect - there are things it doesn't handle (which screws up how I'd like the code to look, optimally) but in general it's so nice to not have to care about that stuff while coding, I just hit Alt-Shift-F in VS code and my C++ is formatted. I did download the source and try my hand at fixing one of the things that has been bothering me: aligning member variables and comments when there's a line break in between them (and even keeping the same alignment over several structs. A simple solution was actually pretty simple but it had some bugs in it and I haven't had time to take another proper look at it. To see what I mean: https://twitter.com/Srekel/status/875839637008392196
After 25 years of writing C++, I do int* var, and I never declare more than one variable per line.
&gt; &gt; &gt; inline // cause if noexcept is gonna be part of the type system, this obviously won't work for forward-declared functions &gt; void foo() noexcept(auto) { &gt; // noexcept if and only if all statements in the function body are noexcept &gt; } &gt; actually, why shouldn't this be the default case ? eg void foo() { // noexcept if and only if all statements in the function body are noexcept and there is no throw }
clang-format is amazing. Finally, it's able to format my code better than I can do it myself.
Even the LLVM project does it.
Same in /r/C_Programming
Thanks for your comments - I agree with your points. Can you mention a use-case for holding onto state between visits? (Edit: It doesn't need to include code).
Nice article except this part: "My goal isn‚Äôt to disparage the folks on the ISO C++ committee who picked this approach." ;) But seriously sometimes I feel that sometimes ISO should stop and say: "We know it is pain in the ass to standardize anything, and it is much easier tell people to go to boost or some other library for helpers but we need to think a bit more about this from a PM perspective and think if this is something we would be comfortable selling to users that are regular developers. Like literally imagine you are in a room filled with average developers and your task is to convince them to start using std::variant..." In other words "it is only 6 lines of variadic templates" is not an argument. :) That being said I understand that C++ has limited manpower behind it, despite nominally huge number of people involved. I would rather have 10 experts working on it 24/7 (who are obviously in contact with ppl like Chanlder and STL) than what we have now. Unfortunately for me and C++ I am not a billionaire and I doubt anybody is willing to donate 5-10M$/year to make this happen. :) 
It's a feature that is implicit in abbreviated lambdas, as proposed, so if it's available there it should be available generally.
Great, nice to know that clang can capture those tab formatting related details, not many tools do it to that length. I will check those options. Thanks. Still, I don't see tabs-for-indent coping with that snippet formatting style. One would have to change format style somehow to bring tabs.
Only if before `d`, it's considered alignment, and it just fills it with spaces. Does it handle that?
Not all that surprising that an old, native language with highly optimised compilers is very efficient. What does surprise me is how well Java does (especially compared to C#). 
The standard isn't even official yet. We're ahead of the game! We have also been working on a number of older features. For example, two-phase. 
So I guess we are all winner!
Thank you! The whole team really deserves the credit here...
I can't believe ISO will introduce special syntax for std::forward abomination... It needs to be killed, not easier to use. Just joking obviously, I can believe it, this is exactly what I expect from ISO - making it less painful to do stuff you should not be required to do in any sane language.
Well out of 28 languages, they were in the top 5 And C was THE winner
Why would the author post on this subreddit if not to get (presumably legitimate) feedback? Being indirect would just be communicating poorly on purpose.
Yes, when there's a line break in the parameter list clang-format can align the trailing parameters up with the first parameter and this is considered 'alignment', so if you're using tabs only for indentation then clang-formatter will use spaces to get the parameter lined up right. Even before clang-format this has been called 'tabs for indentation, spaces for alignment.' One easy way to try out clang-format is the Visual Studio plugin found [here][1]. Dump a file named `_clang-format` with contents Language: Cpp UseTab: ForIndentation in the same directory of a file you want to format, open the file to format in VS and run the formatter (Tools &gt; Clang Format Document or Ctrl+R, Ctrl+D). You can turn on visible whitespace to see what it's doing: Edit &gt; Advanced &gt; View White Space or Ctrl+R, Ctrl+W. Other settings to try are `UseTab: Never`, and `UseTab: Always`. [1]: https://llvm.org/builds/
The third one is truly the best: - both other religions tend to accept you, or - both other religions hate you equally - being a pointer is important, so it should standout - leaves lots of room for `const` (in the right place) which should also stand out 
Well, it says "*one of* the most ..." so it's fair.
And where is your paper explaining how to cleanly get rid of `std::forward`?
Thanks. I do use clang-format for a long time in vim, but ignored that it had an option that would do the right thing for that case, most probably because I avoid tabs as I have to deal with other clang-format-lacking languages and depend on the editor which isn't much syntax aware.
a) I am not paid to write papers for ISO, so if you are gonna use argument of write a paper everytime somebody complains it is not really productive. I mean if you do not like some Chrome change nobody in theory stops you from forking it, but I hope you see that is not realistic. :) b) I have sent some ideas about removing need for std::forward to some C++ experts, they (politely obviously) said it was sh*t. So you could say I did 0.00032% of writing a paper that got rejected. :)
atta boy...
Maybe "abriviated" is an abbreviated spelling of "abbreviated"?
I like to say Herb likes to say Bjarne likes to say: There is a small language struggling to get out. :) Meaning that if you were to do a C++ like language from scratch you could make it much simpler. But you can not do that now due to backward compatibility. My dream that will never come to life is that they do CPrime but with a compiler from C++ to CPrime that maintains readability of code... But that will never happen. Too much money required to do that.
What I would love is a way of getting a canonical .clang-format that is *guaranteed* to produce the same output with newer versions of clang-format - as currently forcing a single specific version per project is a right PITA
That symbol is referenced in a structure in the Windows SDK (and also our CRT). Our linker just ignores the symbol if it doesn't have a definition. You would see the same error if you used an old MSVC linker with a new Windows SDK. I don't think there's a switch that will fix this for LLD. But LLD could just ignore the undefined symbol as well. If you search for "enclave" online, maybe in combination with Intel, you'll get a good idea of what's going on. 
[Here's Phoronix's recent comparison](http://www.phoronix.com/scan.php?page=article&amp;item=gcc8-clang5-znver1).
yeah sorry about that, I've got a mild case of dysorthographia. I've just merged a pull request to fix the typos.
Good grief, the guy wrote a compiler implementation for something that is still no more than a proposal, but his bad grammar is what matters. This is typical r/cpp: "I wrote a code that when run on a 386 machine it builds by itself a space rocket by repurposing all the pieces of the CPU and goes back and forth to Mars" r/cpp &gt; Could have used perfect forwarding in the creation of the propellers. ------ "Source code was leaked from heaven and turns out the human brain runs in C++" r/cpp &gt; We can agree that it is more like C with classes. On top of that, it uses google style guide which is a no-no for me. And that sleep function has some UB, capable of even activating motor functions because the compiler writers are idiots, and optimisations are leading to NeverCallThisSleepwalk() to be called, feeding a static nullptr with a reference to Walk(), even though sleep_main.cc never calls it. I think the compiler should rather kill the brain as defined behaviour.
&gt; Experimental features are not allowed, so Rust cannot use SIMD. Can you point out which of the C++ samples uses SIMD? Edit: the only one to use SIMD I could find on a quick check was the mandelbrot one. 
@DaMan619: TY .... I had actually glanced thru that article before, but since then both GCC 7.2 and llvm/clang 5.0 have been officially released, so I'm hoping for new GCC/clang benchmarks at Phoronix hopefully in the not so distant future...
It's all volunteering, unless you're lucky enough to be sponsored by, say, your employer. But obviously, that would require a good idea with benefits for that business. It sounds like your idea probably went as far as it needed to, so you followed the process correctly. 
As far as I know nobody's paid to write proposals. I agree that std::forward is cumbersome but If they solved a language issue with a library solution in the first place it's because they're never going to change the language around the issue. If it can't be killed let's tame it.
:) &gt; ... and optimisations are leading to NeverCallThisSleepwalk() to be called, feeding a static nullptr with a reference to Walk(), even though sleep_main.cc never calls it. That's a pretty astute observation of a serious bug, honestly. :)
Nice work, BTW. :-) It does look like a useful proposal. If you're familiar, what are the obstacles it's not yet made it into the language?
Well, Java is almost as old and optimized as C++ and it was designed with embedded/portable in mind from the beginning.
&gt; As far as I know nobody's paid to write proposals. Well not in a a way I can set up a stand in front of ISO meeting hotel and sell proposals("yo Bjarne, you need some language variants?") but there are people who are paid by their corporation to work on ISO and some of that work is writing proposals. Also technically Niebler was paid to work for ISO foundation but he was exception and obviously considering how much he can make in private sector ISO foundation was just reducing his loses not paying him in a way most people think when they think of getting paid.
YSK I'm biased, having written my own visitor implementation that does exactly this for a tree (although I made an interface that provides a Visit() procedure implementers have to create and can overload). Anyway. What if you wanted to visit all of the nodes in a tree (which has different node types) and even just count how many of each type there are? What if you want to aggregate parts of the data in your data structure in some way, perhaps in a more advanced way than basic math? It's useful really for anything you might do where the data in one item in your structure is relevant to the work you perform on subsequent items.
The reason why not is explained here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3207.htm This discussion is very illuminating regarding `noexcept(auto)`: https://groups.google.com/a/isocpp.org/forum/m/#!topic/std-proposals/ICtSsQxnPAw I had a case recently where I needed to use a template template, called with a variadic list of arguments, inside which was a big switch statement. It made it impossible to effectively use `operator noexcept` to carry the noexceptness of the template template parameter through. 
Or looking for the one relevant error message in the sea of compiler output
The day you hear that google switches their production builds from gcc to clang is the day that clang beats gcc in energy efficiency (at least for the scenarios/benchmarks that google cares about). It might happen some day, maybe even soon, but I don't think it has happened yet.
I feel the same way about the cache miss operator.
Are they counting the power used by memory? 
How programs are written FAR outweighs the differences in compilers.
Except for Fortran.
They already build chromium for Windows with clang, apparently... see [here, google groups]: (https://groups.google.com/a/chromium.org/forum/#!topic/chromium-dev/Y3OEIKkdlu0)
Yes, that's the orange bit at the top of the bars.
I've been toying with a variant that handles inheritance: Variant&lt;Button, Label, Textbox&gt; control; // each derive from Control control-&gt;render(); // calls virtual Control::render Because you often know all the derived classes beforehand.
I was lucky I think. I fell onto a patch that implemented [static_print](https://github.com/saarraz/static-print) (similar to pragma(msg, ...) in Dlang) that I used to understand the layout, used **grep -r** a bunch of times, read inline comments and that's it. The proposal only affects the parsing part of GCC and there are preexisting functions like finish_decltype_type(), finish_noexcept_expr(), build_static_cast() and build_noexcept_spec() that made my life easier. I only now struggling with exception specification because there is no preexisting way to update the exception specification of a method/function. As for change It's my first time at patching GCC so I couldn't tell you.
Has anyone ever tried something like this? Especially something like superoptimization? What are your impressions?
I don't understand the point of that. The lambda and function overloading replaces virtuals... The lambda can call w.render() if you want but I don't like that design because it is intrusive.
Ive never heard the term 'type erasure' used to describe regular polymorphic heterogeneity before. My understanding is that what you are calling the 'concept-model idiom' has traditionally been known as 'type erasure' (at least in the C++ world), and its definitely not new - there are lots of articles about its use in the STL in the form of std::function and the 'deleter' in std::shared_ptr. Sean Parent's awesome talk focused almost entirely on how it's a useful tool for removing ownership semantics, which he sees as a leaky abstraction, and allowing polymorphic types to be represented as simple value types and also used heterogeneously at the same time.
Or just one `#include` of boost.
I'm trying to build a large-ish app with Qt, and thus far the main issue has been trying to pass around errors across my signals and slots, and this looks perfect, and *much* better than my home-grown solutions. Also, if I can just plug a bit, it'd be neat to look at how this would work with [this error handling idea I had a while back](https://refi64.com/posts/an-idea-for-concise-checked-error-handling-in-imperative-languages.html) (NOTE: no clue why all the links turned pink on me... seriously Vue!?).
The code seems to be at https://github.com/greensoftwarelab/Energy-Languages/tree/master/C%2B%2B . 
put the * by the name so you remember that if you declare more than one variable, you need a * for each if you want them to be pointers. int * i, j; i is a pointer, j is not.
that's an advantage of templates/header-only libraries. You don't have to do link-time optimization and the calls can be optimized differently for different use cases, as opposed to built one way when the library is built.
I've been trying all day to build `blas` on Windows 10 using the `gcc` toolkit chain under MSYS2. Developing under Linux is a dream. Thanks to cmake, developing under Windows is bearable.
Yes, dog shit is different from bull shit. 
How can the outside world not be affected? Surely your batches have some visible output ? If they don't, then they don't serve anything. And if breaking down in the middle of the creation of that output can't be very good. I would guess that the situation is, rather, this: if something crashes and burns, there will be a manual intervention to fix it. So the cost/risk of not handling some errors is passed onto the user/support. Or perhaps, there is just a "delete all and start over", in which case, the cost is paid there. Or perhaps, it's simple database work where crashes are handled by the database's automatic rollback. But whatever. Nitice that here, we really entered the discussion of whether there should be error handling or not, which is kinda funny :-).
I really don't see how C is faster than C++ when any code written in C compiles and executes with no extra overhead in C++, and in C++ template metaprogramming can make things faster than C (see the sort call).
In my opinion, a few of these can be solved at the user (developer's) level. Of course, some are more fundamental issues. I think for a lot of teams, it's usually not a great idea to expose the entire language. If you have a bullpen worth of senior C++ guys, okay, maybe. But that's not reality a lot of times. This can help you: * Create a codebase that is easier for newer devs to pick up. * Avoid language lawyer code that only 1 or 2 devs can deal with (and they're both sick today!). * In some sense, a good framework/codebase enforces the safety you ask for. I'm not saying there's no purpose for a given language or library feature, but C++ is such a huge language, I think it's a good idea to tailor to the team/project you're working with. One of the more interesting aspects of my career has been seeing the variety of C++ written at different places. C++ codebases can be wildly different. More so than other languages in my experience. 
Yes. You can do just C and then something else depending on what you want. Just slightly unrelated question: What's the best thing template meta-programming can give to us? I know boost is heavy on it, but what is one thing that couldn't happen without it pragmatically speaking? TMP often feels very gimmicky to me being and old C-fart, but I am starting to accept C++.
From my point of view, TMP can offer the sort of inlining that C is not able to - things like duck-typing polymorphism (where you don't really have to do or emulate inheritance) and usually since templates are all about doing replacements, is a way of inlining things that are hard to express in C/basic C++ without function calls and pointer dereferences. That's why the C++ sort is faster - because you don't need to pass a function to the sort call, instead, it's automatically inlined as the code is replaced in place. The algorithm stays the same, minus the subroutine call. TMP is basically preprocessor on steroids, type checked, fully programmable. However, I think abuse of TMP (like boost does) is really bad. I personally hate Boost - their APIs became unreadable and programs with Boost require a super-computer to compile in a decent amount of time. But using TMP from time to time for specific, performance-driven goals is really worth doing.
Yeah, hear you on Boost. Hence my precautions for C++ :D I used to do lots of tricks on preprocessor with C, like reusing headers (in C files) with macros defined differently to avoid rewriting of identifiers. Basically had the initialization values of the identifiers typed in the header but only used in the . C file to get single instances, so that was kind of meta-proggramming on pre-processor. Wonder how expressive c++ template system is... If you want to avoid retyping your identifiers on implementation parts of your header, how would you do it, ie introduce identifiers and their default values in a header, but only create single objects, and provide access to identifiers without dereference when they are used. The header/linker multiple object inclusions were making it difficult on the past when I was toying with C++ trying to avoid preprocessor. Or does preprocessor still have its uses? 
I think the preprocessor is still used for some things, mostly related to interaction with command line (if X is defined) or with the platform (if you have SSE, if you're on Linux, if you're on x86 and so on). I haven't used the preprocessor for anything else lately. Now there's a trend of having header-only libraries in C++ - this makes them a bit more efficient, although the compile times grew exponentially. that makes the write-compile-runtest cycle quite slow, but you get used to it, I guess. If you follow the best practices of C++1* and use the enhancements, you get to write a lot more in one go, and it makes things less error-prone. So it's all for the best, I guess. So yes, it's worth having a look over what you can do nowadays with the language and templates 
&gt; 20,000 lines of library gook to step through/around. Just put a breakpoint outside the visitor and then one inside, then: disable 2 run enable 2 c
&gt; 2) it does not provide never-empty-guarantee (or as Boost put it, std::variant causes "significant additional complexity-of-use") Most of the time if you're in the situation where the `std::variant` is empty and the `std::variant` exists you've done something weird. I've used an implementation of `std::variant` in several real projects and never been in a situation where I was liable to see a half constructed variant. I think this is greatly over-hyped.
&gt; can the compiler use a static inference that 'p.ptr==nullptr' to elide the delete call altogether. No it cannot. The catch here is that move constructor of your Foo type (or any other type) is **not** required to set all of its ptr members to nullptr. You are free to choose the implementation which is most appropriate for your use-case. So no, std::move() is not a destructive move as Rust implements it and C++ compilers cannot infer any information implicitly from it. Another thing is if compiler or better said optimizer can make use of some heuristics to get rid of the additional call. But the language itself does not have any guarantees for this to happen.
Have you considered using the WSL?
Please read [how to format your posts/comments](https://www.reddit.com/wiki/commenting). Your post will be much easier to read and reach more people, if it's formatted properly.
The problem with BLAS is that it's written in Fortran and the only usable open source compilers are GCC and Flang (Linux only). If you don't want to build everything in MinGW, you can use the PGI Community edition and obtain MSVC-compatible binaries.
&gt;The catch here is that move constructor of your Foo type (or any other type) is not required to set all of its ptr members to nullptr. but in an inlined example, the compiler has the knowledge to detect if it did; as I understand it *is* required to set the members to a valid state. &gt; So no, std::move() is not a destructive move as Rust implements I'm suggesting that we detect the case where it sets it to null, and the destructor tests for null and does nothing if it is; potentially eliding *both* (no need to continue to use registers or memory to store a 'null', and no need to manually check them). &gt; Another thing is if compiler or better said optimizer can make use of some heuristics..the language itself 'the language' refers to the semantics; we regularly rely on *the compiler* to do work not specified by language semantics in order to achieve zero-cost abstractions. ultimately I don't care where it happens, so long as we can match the behaviour of rust in the appropriate scenario. This *should* be possible. I would prefer a true 'destructive move' to complement the existing move aswell, but it takes years to get addition to the language spec.
Benchmarks of Computer languages usually convert code from one language to another. No difference here and for example their binary-tree uses APR and f_c_pointer to allocate memory. This way you obtain a Fortran pointer instead of an allocatable which bleeds performance in a very nasty fashion. Their C++ code for the same example is the C code with printf replaced by std::cout and would you believe it, the C code is a bit faster.
Sorry the aligned version is more difficult to read :( We all have our preferences of course. Alignment breaks it for those using variable width fonts.
-&gt; by OP /u/PifPoof
Clang is the default compiler for the Android ndk. Android == embedded devices == seek for low battery consumption
As far as I know one of the main differences between the Java and C# JIT compilers is that C# does not profile or recompile code on the fly. So C# is stuck with optimizations that are valid for your average application, while Java can optimize specifically for each benchmark.
done, although I still feel code looks bad on reddit, but it is general purpose site, so I guess it is understandable...
Coming late to the party - but Python is exactly the tool for this job!
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/70b7g4/c_is_one_of_the_most_energy_efficient_languages/dn2u5q6/?context=3.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Well, where do you draw the line? The Top 5? The Top 10? The Top 30? This is ridiculous.
Did you use space or tab to separate the *?
&gt; I really don't see how C is faster than C++ when any code written in C compiles and executes with no extra overhead in C++ This is definitely not the case... char* x = malloc(10); In case someone else reads this, it doesn't compile, I'm not trying to make a string or anything like that. Just saying that "any code written in C compiles in C++" is false.
??? Google switched to clang internally years ago. But I agree that gcc still produces faster binaries in general.
You keep referring to ISO as if it was ISO (which stands for International Standards Organization) that develops C++ standard. The only thing ISO does for C++ development is establishing some formal procedure and checking the procedure was followed. The real work is done by individuals in WG21 (Working group 21), who are neither employed nor really controlled by ISO. So would be more correct to refer to "WG21" or "committee" (as it is traditionally called in the community) rather than to ISO as a whole.
&gt; who are paid by their corporation to work on ISO Not really. No one is paid for this for its own sake. They are paid by corporations to improve codebases. Then they see participating in C++ standardization as one of the ways to do that. That's rather long term investment, and only large corporations can afford that, but they expect it to pay back eventually. Also, Eric Niebler was not employed by ISO. It was C++ foundation, a completely independent organization. 
Such a string would actually go in the Small String optimization field, where you keep stuff on the stack. Calling malloc is a costly thing from the start anyway. And that code generates the same code in C and C++, btw.
Ah yes of course. I made this as a separate setting. :)
You should take a look at the title of the article in other languages. It's often a translation of neutral element. So yeah, regionnal disparity. You can't say it's called identity element *way more* than neutral element
Out of curiosity, what C++ project about as big, sophisticated and successful as chromium isn't a "big web of state"?
Great job
they don't seem to have it because they didn't implement it, not due to any technical reason.
What about taking it from a different angle and say *starting from straight C, what would you add from C++*? For me that would be * RAII (for automatic cleanup calls when a stack based variable goes out of scope) * method overloading including limited operator overloading * simple container classes with bounds checked access * a String data type I basically get this by using a C++ compiler but limiting C++ features I use.
Not sure I follow? My whole point is that this code doesn't compile in a C++ compiler... C++: https://ideone.com/z2gHy6 C: https://ideone.com/d3vGF5
Although they will always be necessary, raw pointers have become much less used in C++ code. References, `std::reference_wrapper`, `std::optional`, `std::unique_ptr`, `std::shared_ptr` and `gsl::owner` have replaced many reasons pointers were used. For those few remaining pointers you could define a `raw_ptr&lt;T&gt;` type template.
Yes it is an implementation detail. I think Microsoft made the choice to improve startup times, which suffer for Java implementations that have to interpret and profile code until the JIT considers it hot. 
You forgot a fourth possibility: `int*ptr` ;)
&gt; both other religions hate you equally My approach to handling every religion ever.
The point was that any code written in C has an exactly identical C++ counterpart, even if (in this particular case) it requires an extra cast. The cast, however, does not change the performance characteristic in any way.
Only if you pick the wrong one. 
The only case where I have seen this to be true is GCC 7 retaining std::auto_ptr in C++1z mode. Other features, and other compilers, have failed compilation when encountering a library or language feature which has been removed from the language, such as non-empty dynamic exception specifications. 
Why put dynamic allocation as a disadvantage in the classical inheritance approach when dynamic allocation is done even with this approach ? The advantage here is that the allocation is being done by the library instead by the user. Allocation still happens.
Is there an easy way for me as a user of libstdc++ to know when, for example, `std::variant` has become ABI stable and will remain so? Or is it simply easier to say, "Major compiler version bump, best assume the ABI has shifted somewhere"? Because presumably, minor version numbers will remain stable.
Auto-detecting the noexcept-ness of a compound statement is a new feature and is not available in abbreviated lambdas.
Nice write up. This summer I did something very similar for a portability abstraction layer. The only downside was we had to support C++11 only and not-so-great compilers (e.g. nvcc 8). It really helped clean up our implementation and provided much more expressive errors to end-users.
I think the reference implementation of BLAS was written in Fortran, but modern versions are mostly in native assembly or C so they can get maximal performance. It probably depends on which version OP is trying to compile, though.
[removed]
I don't know the technical details for C# but I thought using things like ngen it would produce optimised binaries (or assemblies) in the global assembly cache for that specific machines hardware?
&gt; &gt; &gt; &gt; &gt; Wonder how expressive c++ template system is... If you want to avoid retyping your identifiers on implementation parts of your header, how would you do it, ie introduce identifiers and their default values in a header, but only create single objects, and provide access to identifiers without dereference when they are used. Templates *are* the implementation most of the time. eg there is a single double-linked-list implementation for all your code (std::list&lt;T&gt;), a single dynamic array (std::vector&lt;T&gt;), etc
/u/dobkeratops yes, there is a cost in destructing moved-from objects and no, it's not (always) free. It can be measurable regardless of what the other comments are saying. If the type is trivially destructible then it does simply go away and the cost becomes free. I discuss this and have made some measurement of the impact in my "Practical Performance Practices" talk [cppcon](https://www.youtube.com/watch?v=uzF4u9KgUWI), [c++ now](https://www.youtube.com/watch?v=lNnBExDoNSQ). My general conclusion is that if you needed to move from a variable then you probably never needed that variable in the first place, and the temporary should be avoided.
and this causes a lot of pain, bigger and slower binaries, etc
I *think* that without concepts it's impossible to have a class template to SFINAE out if it is instantiated with a type that is not modelling the required concept (you can use a static_assert but that will not SFINAE).
Argh! Is somewhat more minimalist. https://github.com/adishavit/argh
The JVM JIT keeps track of which null checks are unlikely and may decide to catch a segfault instead, getting rid of a pointless branch. It also keeps track of types used to inline the most often called implementation, for example HashMap.get for calls to Map.get . The C# JIT is limited to the information it can get from the code itself, if a function takes an IDictionary it wont know which implementation to inline, unless the function itself is inlined to a point where the concrete implementation is known. 
Strange to see JavaScript and TypeScript when TS is compiling to JS. I guess there should be esnext (target of TS) and es5.
Indeed, dynamic allocation still happen, but the thing is, dynamic allocation happen only when calling the code that needs it. The way we use dynamic allocation encourages to only do it when needed. Also, the day you want to add SBO, it is done in only one place instead of refactoring all the user call sites. Maybe I'll cover SBO in another article of the serie.
The article towards the end gives impression that Monoids are order insensitive. Monoids are order sensitive but grouping independent. A op B is not necessarily same as B op A. In that case it would be a commutative Monoid. Once A,B,C,... are put in an order, they can be arbitrarily grouped while keeping the same result. If the order changes, answer may change. There're still N! possible orders for N members of a Monoid and hence those many possible results. 
You really should credit Walter Brown for coming up with this idea, and link to his (very good) CppCon talks explaining it. Part I: https://www.youtube.com/watch?v=Am2is2QCvxY Part II: https://www.youtube.com/watch?v=a0FliKwcwXE 
I'm trying to compile [LAPACK 3.7.1](http://www.netlib.org/lapack/). I just assumed that all implementations had a C or C++ interface to Fortran because that's what I remember from years ago. I didn't think to look for a pure C/C++ implementation. Thanks to your tip, I just found this page to [LAPACK++](http://math.nist.gov/lapack++/), but it says that it has been deprecated. Is there a C/C++ implementation you can recommend? Thanks in advance.
&gt; something weird There is `bool std::variant::valueless_by_exception()` to address exactly this situation. `std::variant` must be weird, right? What is weirder, writing a function no one will ever use or actually calling such function? This all depends on your definition of "weird", may be you like classes with useless functions, I don't know. &gt; I was liable to see Worldwide amount of bugs related to empty-by-exception problem in std::variant will be a big positive number. &gt; I think this is greatly over-hyped. For statement `a = b;` I expect 2 possible outcomes. Please explain, why do you need 3? 
&gt; It's not yet possible to have the spaces before the hash, but one can assume that a BeforeHash will be added in a very near future. I thought this is not allowed by standard or major compilers - the hash must be first character on the line or it isnt a pp directive
"Old style cast" is my new favourite warning :-). I tell you, in real world code, 90% of casts are a sign of a poor design. Therefore, they have to stand out like a sore thumb.
Now that you mention it, I do recall setting that up right after I acquired my current computer, but other than a cursory look, I've never used it. I was planning to use the shared libraries in MSYS2 to create a DLL that would allow me to call the BLAS libraries from a Windows executable. 1. Do you know if that's possible? 1. If it is, could I skip MSYS2/MinGW and compile the DLL using the WSL? 
&gt; they can't as per the standard That seems presumptuous. Surely if the move constructor and the destructor are small enough to be inlined, the compiler can optimize away `delete`s for pointers which have been set to `nullptr` by the move. Technically, the destructor is being called, it's just been optimized. As long as the observable effect is the same, the standard should allow it.
If you want to use some 'default' setting for sorting, why not overload `operator&lt;` for the type stored in `vec`? Then you don't even need the binary predicate, or you can simply use `std::greater&lt;&gt;`, etc...
On a similar topic, there are quite a few useful MSVC warnings among those not enabled by default (here is the [full list](https://msdn.microsoft.com/en-us/library/23k5d385.aspx)). IME it's worth enabling at least the following: * 4061 and 4062, which correspond to gcc `-Wswitch` and `-Wswitch-enum`. * 4263, 4264 and 4266, to warn about virtual function hiding and accidentally not overriding a base class method (this is, of course, mostly useful for code not updated to use C++11 `override` yet). This partially corresponds to gcc `-Woverloaded-virtual` which is not part of `-Wall` neither, but is worth enabling. * 4296, 4545, 4546, 4547, 4548, 4549: catch various "expression without effect" typos. * 4574 and 4668 which roughly correspond to gcc `-Wundef` (which is itself not enabled by default but definitely should be).
It's not really BLAS or LAPACK but have you considered Eigen, if it's applicable to what you're doing?
What does `old-style-cast` do? AFAICT, it lints on `(T)x` but not `T(x)`. Seems awfully stylistic to call "poor design" (even if I do prefer the latter).
I believe it warns about both `(T)x` and `T(x)`. In C++, you should instead use `static_cast&lt;T&gt;(x)`, `dynamic_cast&lt;T&gt;(x)`, `const_cast&lt;T&gt;(x)`, or `reinterpret_cast&lt;T&gt;(x)`. (Edit: Whether it warns about `T(x)` depends on whether it is a cast. e.g. `int(x)` would be, but `T`could also be a class which has a constructor which takes an `x`. I haven't double checked this, so it may be inaccurate)
Well, now that you mention it, I have to go back and check because I think I was actually trying to compile LAPACK because the implementation of Eigen I was trying to compile relied on it. Honestly, I'm so deep into compiler errors that I can't remember how I got here other than ultimately trying to compile OpenCV.
It doesn't. Try it ~ $ cat a.cxx int old_style_cast(unsigned x) { return (int)x; } int non_old_style_cast(unsigned x) { return int(x); } ~ $ g++ -Wold-style-cast -c a.cxx a.cxx: In function ‚Äòint old_style_cast(unsigned int)‚Äô: a.cxx:2:15: warning: use of old-style cast [-Wold-style-cast] return (int)x; ^
Good catch, I hope they extend the warning to the case you raise also
&gt; (which is itself not enabled by default but definitely should be) Great list! I just wanted to note, this flag breaks checking NDEBUG as NDEBUG isn't defined during debug builds by most build systems.
Why? What would `static_cast&lt;int&gt;(x)` have bought you there?
Yeah, not warning on t(x), that's a shame, but the IMO the vast majority of code will use the former. I rather called **all** casts (including normal C++ casts) a sign of a design error. It's from that standpoint that I think they should stand out like a sore thumb (and not be hidden behind an innocuous bracket pair :-).
It's just more consistent with the style that I have used myself and encountered in others' code. `T(x)` seems bad for the same reasons as `(T)x`, and IIRC, it has the same semantics, no? Being potentially a `static_cast`, `const_cast`, `reinterpret_cast`, etc. as necessary on a case-by-case basis? When you are doing conversions like `unsigned -&gt; signed` and potentially changing the value, losing data, doing something platform dependent, etc., it should be loud in the code IMO. Thus, `static_cast`.
It appears you are correct. I guess `T(x)` isn't a cast at all, it's a constructor call. Whether or not builtins like `int` have constructors is perhaps a grey area, but I believe you can at least treat them as though they do.
You're right. But it would be useful and easy to implement.
If the argument is that it might be a `const_cast` or a `reinterpret_cast`, etc. why isn't there a warning for just when a cast isn't a `static_cast`? Hardly anyone uses the others anyway. If the concern is that the cast might change the value, there already _is_ a warning for that (`Wconversion` and `Wsign-conversion`), and using `Wold-style-cast` penalizes conversions that don't change the value too. Not to mention the language would be perfectly happy to do the `unsigned -&gt; int` conversion for you implicitly; using `int(x)` already is the loud version. &gt; When you are doing conversions like unsigned -&gt; signed and potentially changing the value, losing data, doing something platform dependent, etc., it should be loud in the code IMO. Thus, static_cast. And other conversions are perfectly normal (take the low byte of this word) and don't need to be extra loud.
I would raise that to 99% as the few casts that end up actually being needed also need to be properly encapsulated. Maybe 99.95% ?
I don't know about LAPACK, but I've used OpenBLAS and Intel MKL in the past. Both have worked well for me. Do you need the linear algebra stuff or just efficient matrix/vector operations?
That's crazy. You need casts for basic integer operations, like decoding/encoding LE integers. If you can get away without them its only because the language has inserted implicit ones for you.
`Wuseless-cast` can be annoying. For example, if you cast a `uint32_t` to a `size_type`, it will warn on 32-bit systems but not 64-bit ones.
Oh I hope I didn't come across as claiming credit for this, that was certainly not my intention! There's already a link to the lib fundamentals 2 TS at the top of the page, but I'll put in a mention of Walter Brown and a bunch of the different talks on the subject when I'm not on my phone. Thanks. Edit: Done
&gt; Pro-tip 2: Don't discard Clang-format because a detail is missing &gt; ... &gt; From there, you have two options: &gt; 1. you can either change your coding styles (amount of work == 0), or &gt; 2. you can submit a Pull Request to LLVM, so a new option is added to Clang-format. I would add a third option: you can disable formatting for sections of code that clang-format might not handle to your liking with: //clang-format off ... //clang-format on 
&gt; One easy way to try out clang-format is the Visual Studio plugin found [here][1]. Actually now it's available from the extensions manager in Visual Studio. Also don't forget to enable "format document on save" in the Visual Studio clang-format options, which only applies when a .clang-format file is found. Disclaimer: I added this feature to the clang-format extension ;)
It's more obvious in a slightly different scenario: auto x = std::size_t("foobar"); This cast is so brutal that even `static_cast` cannot perform it, but due to the reckless nature of C-style-casts it works like this. This is one of the main reasons why I really, really want people to use brace-init.
The standard says `T()`, `T(x)`, `T(x,y)` are all [Explicit type conversion (functional notation)](http://eel.is/c++draft/expr.type.conv).
I've been doing it for over 15 years. It helps to avoid confusion over the scope of nested namespaces. Looking forward to having clang-format do this for me automatically.
You're thinking of OpenBLAS, that's written in C and assembly. The assembly part is written in AT&amp;T assembler, so you need MinGW to compile it, as VS can only interpret Intel syntax. The C part isn't any faster than Fortran, except for multi-threading being implemented (in fact, sequential Fortran will easily beat sequential C code due to auto-vectorization and lack of aliasing). That being said, only OpenBLAS is written in that. Most modern, commercial implementations are Fortran-based: Intel MKL BLAS, IBM ESSL, Cray Scientific Library, ... The only exceptions to this are cuBLAS and clBLAS, which are written in CUDA C and C++ with OpenCL. In the scientific field, Fortran is and will stay a major language and the state of linear algebra libraries (where Fortran shines the most) mirrors that.
This is how C++ got most of its warts to begin with. 
Google's cpplint actually catches the c style casting problem too.
&gt; If the argument is that it might be a `const_cast` or a `reinterpret_cast`, etc. why isn't there a warning for just when a cast isn't a `static_cast`? So, the guidance I received on this is basically, even when a C-style cast would resolve to a `static_cast`, it's still better to use a static cast. Because otherwise, for every subsequent reader of the code, you create a little programming puzzle -- which of the 6 possible casts does this boil down to? Are we absolutely sure it's not going to be some ugly reinterpret cast? Even though it's more typing it's better to just say exactly which one it's going to be. Because code is write once, read many times. So, at least in this logic, you should avoid ever doing a C-style cast where the compiler is going to "do the first of the 6 options that works". (I think this is the rationale behind this warning also.) Idk, at the same time, it's good not to take the rules too far. If it's really a simple example like yours where it's explicit what the type that's being casted is, and what the result is, and it's obviously just going to be a `static_cast`, no one later reading the code is going to be confused for a second. So I personally wouldn't want to get all code nazi on you about the example you gave with `int(x)`. But once the expression starts to get at all complicated, I personally would tend to prefer a `static_cast`.
It only breaks checking NDEBUG if you check NDEBUG incorrectly. You should be doing `#ifdef NDEBUG` or `#if defined(NDEBUG)`, not `#if NDEBUG`.
thanks!
Yes it did. I'm old enough to have programmed C++ when it was a pre-processor that generated C code that we fed to a C compiler. I think starting at the start and bringing forward those features that aren't warts.
Ohhhhh I misunderstood what it was doing. I completely forgot ifdef was a thing (even though I use it all the time). Thanks for the clarification, definitely turning that on.
Most of C++'s warts come from the demand for C compatibility, so starting with C doesn't solve anything.
Not really, WSL is a more or less genuine Ubuntu 16.04 environment that executes native linux binaries and at least by default you get a "native" toolchain that produces linux binaries. You could maybe cross-compile from linux to windows, but I doubt that is any easier than a native Windows compilation. My comment was rather meant as a statement that - for some use cases - it might be easier to just use the linux tools, which are now very easily acessible in Win10. But apparently your's is not such a use case 
file a report with gcc then
&gt; operator&lt; for the type stored in vec because generally speakingit is not true that you always want to sort based on the same predicate. 
I wish gcc would copy clang's `-Weverything` option 
Most importantly, and missing in the article: `-Wconversion -Wsign-conversion`
Looks useful! Is there a reason why it only supports one underlying type for all units? When working at large scales (e.g. earth sized) it can be useful to do some computations in double but others using floating point precision in a local space.
n-body: http://benchmarksgame.alioth.debian.org/u64q/program.php?test=nbody&amp;lang=gpp&amp;id=8 C++: 9.31s, Rust: 12.82s.
&gt; for Windows Can you even use gcc for Windows (without resorting to cygwim/mingw)?
From memory: - LLVM seems to be better in the numerical stuff, - GCC has better auto-vectorization, - GCC is better on "business-logic" programs (lots of branches/virtual calls). Clang/LLVM has been playing catch up for a while, but since both improve release after release...
To be fair, I think many of those libs were developed for Linux only and even then, the GCC ports to Windows will never have the quality that MSVC does. Sometimes if you don't choose the right technology, it can bite you in the butt.
Some people say no: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0726r0.html Problem with this is that IDK enough about this to have an opinion, and honestly it is just a theoretical exercise: concepts will make it into C++20 even if this paper is right. Too much effort went into them and too much ego is on the line for them to be removed even if they suck. :)
maybe there is a cmake template or something which can add all of these
It would an order of magnitude harder to implement, and an order of magnitude less useful. Harder, as we are talking a conjunction over substatements, and detecting try/catch blocks, determining if they catch everything thrown. Not as useful, because the noexceptness of code is far less obvious than a call, and fragile noexcept that becomes true/false at the drop of a hat is dangerous. I'd prefer `noexcept(check(true))` and `noexcept(check(false))` or whatever, where you get an error if you get it different than auto. In anything except template library code, `noexcept(auto)` looks poisonous, and there I hope there are bounded points of customization so the noexcept clause can be documentation. In comparison, `=&gt;` deducing noexcept is useful in user and library code, generates useful meaninful documentation, doesn't require parsing noexceptness of many statements while tracking throws and trys and catches, and is easy to specify and apparently implement. Don't get me wrong. I encourage you to implement and specify noexcept auto. But confusing it for something as useful or simple as this proposal seems wrong.
I think you mean, `char*` is why we cannot have nice strings.
Paragraph one implies that in some cases they lead to construction of a T, or am I reading the standard language wrong?
Yeah so I guess that's a pretty natural idea to make it become `std::array`. But you would also lose a lot with this change. Like, every C library that uses string literals would break, and you could no longer compile it in C++ mode. I guess you could try making it so that string literals work as they currently do inside `extern "C"` and as `std::array` in C++? But that might be more confusing for programmers than the current state of affairs. Also, again a lot of code would break, like, anyone that's using C standard library functions with string literals, in their C++ code. All that stuff, `atoi`, `strchr`, etc.. Unless you will make your alternative string literal type convert implicitly to `const char *` but that defeats the purpose.
This looks pretty cool. Is there a reason it can't be a header-only library though? Some of the places that it might make sense for me to use it are existing header-only libraries. Note that the other alternatives it competes with, that you mention, like Andrescu's `expected&lt;T&gt;`, `boost::outcome`, expected-lite, are all header-only libraries, unless I'm mistaken.
Allocators with state are only valid in C++11 under [these rules.](http://en.cppreference.com/w/cpp/memory/allocator).
I think the requirements are all met.
I did say 90% :-). Having said that, I rather think that your example falls into 90%, not the remaining ""I do need casting here" 10%. Care to explain what you mean?
Yes üòÅ.
From cppreference, stateful deallocator must be able to "deallocate memory allocated by any other instance of the same allocator type". From your description, that's not the case for yours? "A moved allocator will reset itself..." BTW, I don't see the relation to the implementation details of MSVC debug version and your problem, can you expand?
&gt; I tell you, in real world code, 90% of casts are a sign of a poor design. Yes, in particular the poor design of STL containers where `size()` is unsigned so it has to be cast to signed in every counted loop ;)
Easily 50% of cases, right? üòÅüòÅüòÅ
Then for the minority of code that needs that kind of shenanigans encapsulate that stuff into a small number of functions or classes and use pragmas to turn off the warning. This way the rest of the code gets the benefit and that one section can do whatever low level stuff it needs.
`-Weffc++`: All the greatness from Meyers Efficient C++.
&gt; any code written in C compiles and executes with no extra overhead in C++ There is a `restrict` keyword in C which helps compiler to better optimize code.
most compilers support restrict as an extension in C++ as well. However, this is a moot point. I'm yet to see a benchmark that shows a difference in C vs. C++ just because in C someone used restrict and they couldn't in C++. Same goes for the lack of auto-casting too, just to counter the possibility that mrexodia's point is made again.
Ok got it, it's my fault. After moving, my allocator cannot deallocate memory allocated by it self before. That's where the problem is.
I would wrap it in a function that does the cast only on 64-bit builds. I have a different problem with this warning - it shows issues in files autogenerated by Qt MOC (Meta Object Compiler). I'm not sure how can I silence them in those files. Maybe include them in the cpp file and wrap the include in pragma push/pop diagnostic? 
My ~~guitar~~ debug codegen gently weeps
There's probably many opinions on this, (such as the use of `auto x = ...` in C++, or `var x = ...` in C#). Honestly, I prefer seeing the `&amp;` as it avoid ambiguity when reading the code and it's clear what's going on and you don't have to *remember* that functions decay to pointers implicitly. Just my 2c!
Weirdly the "opposite" is also true - you can directly call a function pointer without dereferencing it first. void f(int); void (*pf)(int) = f; pf(); // (*pf)()
-Wconversion makes working with types smaller than an int virtually impossible. 
Ya I am not gonna declare more than one variable in one line too, for clarity.
Just starting programming so wanted to build up some good habits.
That is in my experience the biggest problem with the conversion warnings.
space
Where did we say that? That defeats the whole point of statefulness (and isn't what the spec says).
Per the direction of [LWG 2593](https://timsong-cpp.github.io/lwg-issues/2593), moving from an allocator must not alter its original value, but that issue seems to have missed altering the move assignment case.
 Which is incredibly important since we have templates. `f()` can be a regular function call, a call to a function pointer, or a call to some object's `operator()`. In the future, hopefully it will also work for member function pointers
I've encountered problems when not using the &amp; where - IIRC - a type was deduced to be a reference to a function rather than a pointer to a function (although that was in conjunction with templates). It might have been a compiler bug (probably not, but I didn't bother to check), but the easy thing to do was to just write the &amp;
In Walter's interview, he gave some advantages for the existence of multiple competing compilers. How big is the advantage of having different compilers for experimenting with various implementations for some TS? Are the implementations of one TS for different compilers usually done by the same people? At http://en.cppreference.com/w/cpp/compiler_support, it looks like each TS first starts on one compiler and after success gets "reimplemented" on other compilers. Is that so, or are there also TSs where different people race for a successful implementation on different compilers?
Robert mentioned some problems that tests in Boost are having. This week I heard that there were some discussions to replace Boost.Test, but I have not found any of those discussions on the internet. Is this true? Where can I find more information? 
Yes, I've tried to code around it before but it's just a huge pain. There's many intermediate operations that promote the result to int, so if you're trying to write an expression you end up with warnings all over the place. Even something as simple as `x += 4` will trigger the warning if x is not an int.
Whoops! You're right, I misquoted, from the default allocator explanation!
Ah, right! BTW, my reading of cppref was wrong, see else-thread. Funny, mistakes of some people (me) can help other people (you) üòÅüòÅüòÅ
That's DMR reaching out from the 1970s to help you. He said, "oh, you're applying parentheses to a function pointer, so there's nothing else you could possibly mean".
You could make your induction variable to be of type `size_t`. However, this not always helps. I have a codebase that is based on a framework with it's own containers that use `int` for sizes and indices. It is very painful to use them together with std containers and try avoiding warnings. 
Looking good! What are the main differences with [boost::units](http://www.boost.org/doc/libs/1_65_1/doc/html/boost_units.html)?
`-Weverything` is useless by itself. It only usefull together with a bunch of flags disabling various warnings. The problem is that `-Weverything` includes warnings about "too new" features. It also has warnings about "too old" features. This makes it almost impossible to satisfy in any non-toy codebase. 
True, but I didn't say anything about maintaining backward compatibility, neither did the OP. I was just suggesting starting with C as a known simple base of coding capability then enhancing instead of starting with C++ and removing. It would be possible to start with Pascal too as it actually has some of what I listed above, but it isn't as widely known by C++ developers.
... which is ugly as f
It also warns on proper behaviour of C++ standard. (`-Wpadded` anyone?) The point is that I can enable `-Weverything` and then selectively disable warnings that I don't care about (ie C++98 compat), but it lets me discover new warnings that I might care about. There is no way to do this with gcc.
Here is some insane stuff this enables: https://youtu.be/6eX9gPithBo (see towards the end)
Yes, I completely agree with you. It would definitely be useful.
&gt; isn't a cast at all, it's a constructor call No, that's wrong. It is a function style cast. 
Effective C++, and this flag hasn't been useful for a long time. Far too many false positives, and isn't a sophisticated enough implementation of the checks to be useful. I don't think it was maintained past a very early version of the book, either. Avoid. 
&gt; Effective C++ Damn you, autocorrect. &gt; Far too many false positives, and isn't a sophisticated enough implementation of the checks to be useful. I don't think it was maintained past a very early version of the book, either. Avoid. I have noticed some false positives. Pity it isn't maintained.
Yes, it should probably be documented more clearly. 
You can't pass a macro as a parameter.
Well, isn't 'gcc for Windows' mingw64, more or less? I have successfully built a from-SVN version of gcc within WSL (quite a while ago, even), so you could say it's kinda-sorta there... But I'm certain that self-built version of gcc can't be used to compile Windows-GUI using programs...
Can it handle things like natural units? If it can't, how complicated would it be to define such systems yourself using your library? Asking because I tried doing that with boost.units and it wasn't a pleasant experience. Additionally, is eV a predefined unit? 
 void foo() { } template&lt;typename Function&gt; void bar(Function &amp;&amp; function) { [function](){ function(); }(); } void qux() { bar(&amp;foo); } void quux() { bar(foo); } Accepted by Clang, rejected (quux) by GCC (qux is accepted by both). IDK which one is correct. Edit: It works in GCC with the capture list `[function = std::forward&lt;Function&gt;(function)]`, so probably a compiler bug in GCC.
Why are you using signed index in a loop? And what would be the semantics of a container with a negative size?
I wonder why that warning is not present here https://gcc.gnu.org/onlinedocs/gcc-7.2.0/gcc/Warning-Options.html Edit: Found it here https://gcc.gnu.org/onlinedocs/gcc-7.2.0/gcc/C_002b_002b-Dialect-Options.html
He might have been referring to Boost.Test being replaced recently with a much improved and refactored edition of itself after many, many years of no updates. The new edition has had a few teething problems and breaking changes, but I haven't seen anything like the complaint levels compared to before.
Or he may have been referring to Travis overload, or to the regression test machines not testing certain important combinations which caused Robert surprise because he broke Boost for a short while :) But Boost is no worse nor better than any twenty year old codebase.
Or even transform(vec, out, x =&gt; x * x); ? Damn this looks awesome, this would make lambda code so much easier to write and understand.
If it was a case of "ego", then perhaps they would have made it into the standard 6 years ago, after years of being championed by the developer of the language. Instead, what has been merged into the draft C++20 standard is stripped down from the original vision, and even stripped down slightly from the Concepts TS. Obviously, this contradicts the idea that the process is ego driven, or that there is a problem with discarding "effort". What I'm saying, basically, is try again. 
Yes, even better!
What I am saying is basic psychology. If I worked on concepts for 15 years I would think they are amazing and everybody who does not think that is an idiot. Bjarne and Sutton are probably much smarter than me but they are still humans. So like I said before guy that wrote that paper against concepts may be right or he may be wrong, but üéú in the end it doesn't even matter üéú. 
LOL - or any modern codebase for that matter
Rubbish. There isn't some big human flaw that we all have where we get stuck on ideas for years and blinded to criticism because of the investment we've made in time, or emotion, or whatever you're saying. It's not "basic psychology", or any other kind of hand waving. I would think that the reason they, and others, have spent so much time on the idea is because along the way they have received positive feedback, alongside criticism, naturally. And ultimately it appears there is a broad consensus. Were that not to be the case, I have no reason to believe that the whole idea would be dropped, even now. Your opinion to the contrary is simply your own weird bias. 
IIRC, the proposal for making PMFs work with regular calling syntax got rejected. I'm sure it wasn't accepted at least.
I think people here are more interested in code than in a video. 
Well, that is added to C++17, but not mentioned in the pre-version C++
int* foo; This is the one I prefer, but I suspect the designers of C did not because they allow this: int* foo, bar; Which declares foo as a pointer and bar as an int.
Sure... FYI Bjarne said he was wrong to oppose removal of concepts from C++0x at the time, I think he said "I was too close to it" or some phrase like that. IDK how to google for it since I remember hearing it in a video. If you are really bored going through every video of Bjarne on the internet in the past 10 years(assuming one I saw was not taken down) should prove me right. ;)
It was rejected as being half of a unified call syntax proposal. Which is silly, because it would have been insanely useful anyway.
So let's assume that you didn't misremember. You're saying that he wanted concepts in C++11 despite opposition, but that now admits that the consensus of others was correct. And you're worried about his ego, and that he will get what he wants despite criticism? I'm starting to doubt that you're being serious at all. 
Did they actually switch for their server software production builds? I know about their switch for chrome and android, but serverfarms is where a few percent efficiency become relevant (and I don't think the difference between clang isn't much more). I'm not questioning your statement, just asking for clarification.
It motivated me to change raw pointer members into std::experimental::observer_ptr. Other than that, it shows too many warnings for members not being initialized in the constructor's initializer list, even when it is unnecessary because of sane default constructors for these members.
&gt; Why are you using signed index in a loop? Because I need arithmetic on sizes. Because signed arithmetic is sane, i.e., no unintended conversions or wraparounds or crazy "impossibilities" like `a.size()-1 &gt; b.size()` when both `a` and `b` are empty. Because I don't want to rewrite comparisons from what feels natural to what is correct according to the language's insane rules. Because the core guidelines advise to use signed types for arithmetic and to use unsigned ONLY when you truly want wraparound (ES.102). Because OpenMP wants a signed index: https://stackoverflow.com/questions/2820621/why-arent-unsigned-openmp-index-variables-allowed &gt; And what would be the semantics of a container with a negative size? `size()` would still return non-negative values. What's the problem? Also, because the core guidelines advise against using `unsigned` to restrict the range (ES.106). Because `unsigned` still accepts negative values, only converts them to huge positive values. Because experts agree that using unsigned in the interfaces was a mistake (https://github.com/ericniebler/stl2/issues/182) IOW, many reasons to steer away from unsigned.
&gt; using the Intel compiler for Fortran would lead to a massive change. `ifort` is the Intel compiler for Fortran ? &gt; Compiler Benchmark Game No such thing.
Why doesn't the same logic apply to the member access operator? It seems like the -&gt; operator is useless. If you use the member access operator on a pointer what else could you possible intend than to dereference the pointer first?
It might still be useful to add `noexcept(auto)` that must give the right answer in "easy" cases, and results in `noexcept(false)`, or is implementation-defined, or simply ill-formed once the situation is too hard. It's true that it would really only be useful in template library code. But in such code, you very often have functions where the `noexcept` status depends on some template parameter, and you end up having to duplicate a ton of code or use ugly macros. In my experience, a huge amount of this duplication covers trivial cases. If I can write `noexcept(auto)` in those cases I would have like 90% less macros in my code. Then the standards committee can progressively extend the range of situations they consider "easy" as time goes on, without breaking backwards compatibility. Kind of the same way they increased the power of `constexpr` functions over time. There's various possible things that could happen if `noexcept(auto)` is used when the situation is too hard. But the situation is nicer because `noexcept(false)` declaration isn't usually catastrophic, it just means, we aren't sure that this function doesn't throw exceptions. It doesn't hurt any more than not giving a `noexcept` declaration at all. (False positive, where we deduce `noexcept(true)` when it actually throws, is far worse.) I guess as of C++17 there's the fact that `noexcept` status is part of the type system. So that would suggest you shouldn't make it implementation-defined what `noexcept(auto)` does in hard situations. But, I think you could just say, "once it's too hard, it must be `noexcept(false)` when using `noexcept(auto)`, with the understanding that in future standards the compiler might be able to deduce `noexcept(true)` here later." Or just say, it's ill-formed, could not deduce `noexcept` status. I agree with you that it's probably really hard to implement `noexcept(auto)` that handles arbitrary complex function bodies, try-catch blocks, etc. and they probably shouldn't standardize that without any existing implementation.
OpenMP 3 supports unsigned loop indexes and there are contradicting opinions in the linked github discussion. But thanks for the well prepared answer!
Arguing which style is better is 99% bikeshedding, consistency is what's mildly important. So just pick one and stick to it.
This should also work in C++. What compiler do you use?
Because of what is in my opinion the dumbest historical reason in the language: https://stackoverflow.com/a/13366168/1013719
A stricter version if this is likely to appear in C++20. http://en.cppreference.com/w/cpp/language/aggregate_initialization
It's going to be [in C++20](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0329r4.pdf). Plenty of compilers support it already, but MSVC doesn't.
Benefits: more readable code, if you don't know the structure. This could be done with comments though. Cons: Deciding how to treat variables declared in the wrong order, or variables not in the struct. It'd also be some overhead for the compiler.
I tried it with MSVC, but I didin't knew that others compilers support it, glad to hear that it will be added in C++20 ! 
&gt; But we then might loose some performances, because the struct's values are initialized then set. In many cases, there will not be any meaningful performance loss one the code is optimized. When the members are primitive types, this is going to become the same. When the members are class types, this will cost at most one move per member, assuming things are moveable. So, probably quite cheap. (Though not 0 cost.) I think there's a more general question, "why does C++ not have a named-parameters idiom". That would be useful not only for struct initialization, but any time we call a function. f(foo = 5, bar = 7); would be the same as f(bar = 7, foo = 5); This tagged initialization is basically the same as that, except only for aggregate initialization. (I guess it's different in that aggregate initialization is not technically a constructor...) Named parameters are a much loved feature of python, and I know I've read many articles about named parameters for C++, how would it interact with default argument values, etc. etc. I don't remember all the issues they raised but I think it's thought to be hard to standardize in C++. There are some workarounds you can use in C++, like using a "proxy object": https://en.wikibooks.org/wiki/More_C%2B%2B_Idioms/Named_Parameter (Note that that has basically the same performance as default-initializing the struct and then assigning to the members one-by-one, the only thing achieved here is that you can construct and initialize the struct in a single expression. I never thought of this as a failure of C++ to provide zero-cost abstraction though.) I guess that technique would be nicer if we can replace it with tagged initialization like you suggest though. Like if you can replace the proxy object with a simple struct and then do tagged initialization on it, in a single expression, then pass it to a function. --- Looking back through the writings about named parameter idiom -- the biggest problem with it is that, a function can be declared and redeclared many times, and the names of the parameters do not have to be consistent across declarations. They can be different in different translation units. And, these parameter names are not currently standardized. So if you called standard library functions this way, they'll have totally different names for different implementations. It would be a ton of work to audit and standardize all those parameter names. But with a `struct`, it can only have one definition, and the declaration never contains member names. The names of members can't be different in different translation units, or it's an ODR violation IIRC. So you can't have ambiguous member names with a struct. That's at least one bullet dodged by tagged initialization. I hope this becomes legal in C++!
Hmmm -- looking at [this proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0329r4.pdf), there is a wrinkle for C++: struct A { int x; int y; int z; }; A a{.y = 2, .x = 1}; // error; designator order does not match declaration order A b{.x = 1, .z = 2}; // ok, b.y initialized to 0 This is different from C, and they explain in detail here: &gt; [dcl.init.aggr] &gt; Change &gt; &gt; In C++, designated initialization support is restricted compared to the corresponding functionality in C. In C++, designators for non-static data members must be specified in declaration order, designators for array elements and nested designators are not supported, and designated and non-designated initializers cannot be mixed in the same initializer list. &gt; Example: struct A { int x, y; }; struct B { struct A a; }; struct A a = {.y = 1, .x = 2}; // valid C, invalid C++ int arr[3] = {[1] = 5}; // valid C, invalid C++ struct B b = {.a.x = 0}; // valid C, invalid C++ struct A a = {.x = 1, 2}; // valid C, invalid C++ This is unfortunate, it means we aren't really getting named parameters in the sense of being able to reorder the parameters at will and have the compiler sort it out. It just adds a small amount of extra safety where you explicitly say what member you think you are initializing at each position, which is less exciting. &gt; Rationale: &gt; In C++, members are destroyed in reverse construction order and the elements of an initializer list are evaluated in lexical order, so field initializers must be specified in order. So the thing I don't get about this -- why does it matter here? If an exception is thrown during the aggregate initialization, the aggregate object doesn't exist yet. We aren't going to call it's destructor. Why can't we just go back through the list of members that were actually initialized and destroy only those ones, in order. Is that really that much more work for the compiler? (Maybe the answer relates to what happens when you have initializer lists that reorder the class members? I also am not really sure why that isn't allowed, only that it isn't.) Edit: I read some more about it -- the issue is not, what happens if one of the members throws during construction. It's that there is only ultimately going to be one destructor. And if members are created in one order, but destroyed in a different order, that is considered bad. (I guess because they can potentially depend on eachother, but only in ways consistent with the initialization order.) Idk I guess I find this unconvincing. It seems like you'd have to go way out of your way to make a problem with this -- lots of times, there is no dependency among the members. Having some members capture others by reference or something is pretty strange anyways, and to have a problem the destructor of one would have to try to access another I think? And named parameter idiom is really nice. Need to think some more about this...
Then use `"Hello!"s`
I found the ones I last read: [Multi-Word Integer Operations and Types](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0104r1.html) and [Overflow-Detecting and Double-Wide Arithmetic Operations](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0103r1.html), both by Lawrence Crowl. The difference between this one and n3965 is that p0104 is bounded precision, so no allocator needed.
I'm really glad the difference exists actually - `-&gt;` can be undefined behavior while `.` is always well defined. In practice if you have code that mostly uses references instead of pointers, `-&gt;` makes it pretty obvious that there's something that could be null or could be deleted under you. Of course references can be invalidated too but still... it would be awful if nullable stuff used `.` like everything else IMO.
Whatever you do, don't use OpenCV for any linear algebra operations, it's horribly slow, even for small matrices. I'd really recommend you look at Eigen. It doesn't have any dependencies (in the default configuration - not sure whether it can be compiled with LAPACK backend...).
That's really helpful to know. Thank you for saving me some exploration time.
Just a tiny detail, you misspelled conclusion (Conlusion). Anyway, I'll probably try Clang format tomorrow :) EDIT: there a **two** many ways to write good C++.
On clang you can already reorder the names, so I don't know if this will be changed before C++20 or not. Its mainly allowed because it supports this C99 feature as an extension in C++. Ideally, the compiler could just reorder the initialization so it happens in the correct order. However, this makes it a little inconsistent as initializer lists are ordered from left-to-right.
I once had a similar problem in that we had iOS, Android, AppleTv, FireTV, and macOS as release targets with a single cpp code base. Honestly (and this depends on your team size) I don't think it worth trying to get the code validated for all platforms before cutting a release. We ended up tagging releases exactly as you normally would, git-flow for us, and when we decided a new version was ready, we would denote it as X.Y.0 and send that off for verification. If a platform passed verification, then we would publish it I that store. If a platform failed verification, we would fix the issue and bump the code to X.Y.1. This cycle repeated for every failing platform. The benefits here are that you are able to publish what you have ready and rework the ones that aren't. Apple, especially, has a lengthy review process and we've had to go back more than once to satisfy the issues they had which, in your world, might hold up the pipeline for the other platforms. As for managing actual binaries, we hosted them all on S3 straight from CI and wrote a command line to grab specific versions--just a simple wrapper around the aws cli.