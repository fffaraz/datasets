This got really wierd fast.
!remove
OP, A human moderator (u/blelbach) has marked your post for deletion because it is not appropriate for r/cpp. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/5tnh83/need_money_looking_for_someone_that_would_be/ddntcaz/,%20was%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
This [stackoverflow answer](http://stackoverflow.com/a/9086675/216111) seems to disagree (at least for c++11). &gt; If two pointers p and q of the same type point to different objects that are not members of the same object or elements of the same array or to different functions, or if only one of them is null, the results of p&lt;q, p&gt;q, p&lt;=q, and p&gt;=q are unspecified. and &gt; § 20.8.5/8: "For templates greater, less, greater_equal, and less_equal, the specializations for any pointer type yield a total order, even if the built-in operators &lt;, &gt;, &lt;=, &gt;= do not."
[removed]
So fucking mean
If you browse win32 app docs, you'll see that it's based around longs...
Easy, buddy. Someone answered your question already.
A warning on `std::int16_t x = 1; x = x + 1;` is not useful, but on `std::int16_t x = 32767; x = x + 1;` is. And it could be displayed _in case_ the compiler actually applies an heuristic _when_ it actually take advantage of the UB as overflow, that's what's at stake. Not simply to warn on all operations by which overflows can happen, even when it can't or which is not being detected and taken advantage of.
Personally I think cplusplus.com is pretty bad, but cppreference.com is excellent!
Hey I know that guy! This is one of my favorite episodes.
&gt; Every signed int addition may overflow and trigger UB. [Wrong]( https://www.reddit.com/r/cpp/comments/5tlnkr/undefined_behavior_in_c_and_c_programs/ddnu158/?context=1).
Those quotes prove what I said. Sorted containers use `std::less` etc. to do the sorting, which gives the guarantee of total ordering when using unrelated pointers as keys.
Regarding the second link, [this](https://bitbucket.org/eigen/eigen/src/488efc2159b504c7986fa56a5926ca133a59b5de/unsupported/Eigen/CXX11/src/Tensor/README.md?at=default&amp;fileviewer=file-view-default) is current and has formatting issues fixed.
But you stated that "the pointer comparison example is UB in C but not C++", which is not about `std::less`? It's about built-in operators.
Maybe doing the static analysis in debug mode might be valuable, but again, once optimizations kick in, you lose information, and generating a warning based off on the final result might not even make much sense to you when you look at the code.
I was refering to the pointer comparison example in the article: long *p = malloc(size(long)); long *q = malloc(size(long)); if (p &gt; q) The `p &gt; q` causes UB in C but not C++. In C++ it is unspecified (as shown by josefx's first quote)
Thanks! I copied this link from release notes, and failed to notice that it was really outdated. 
Lots of interesting stuff in the mailing. Any committee members know if there will be a Reddit thread with updates during Kona? /u/blelbach etc.
When? Many times `x + 1` is simply emitted as a calculated value by the compiler when it applies optimization grabbing the value of `x` it can reach and doing the sum, and in knowing the result, it could "optimize further" because of the overflow it has actually calculated. It's a pity that it can act to do that, but not act to show me.
Unspecified behavior is different from undefined behavior, it must still result in a well formed program but the standard does not impose any requirement on what that behavior should be.
Cap'n Proto does not require the compression step, it's just available if you somehow have tons of deleted or small integer fields. When it's off, Flatbuffers is still worse even from a CPU perspective, because cache lines are used for storing and loading those offsets. For Cap'n Proto to be worse you need the special circumstance of having lots of deleted fields. In fact, in a message with N fields, flatbuffers will need to store N offsets, while Cap'n Proto still store 0, so for the cache usage of Cap'n Proto to be worse you would need more than 50% of your fields to be deleted! Edit: realized this morning my reasoning is slightly off here. The offsets are stored in a single vtable per type, not per message. So the cost comparison is more complicated, my points about baking offsets into mov and not wasting any cache on them is still true though.
&gt; whereas if `x` were of type `size_t`, then that loop could walk forever Not if `M` is also `size_t`...
It's not just C/C++, this problem afflicts all languages these days. perl, php, whatever. Javascript is might not have this problem since (I believe) it converts all types to a single universal floating point type. But that just substitutes one problem for another.
http://eel.is/c++draft/intro.defs#defns.undefined
This is off-topic as it is not a C++ question.
install xcode and use clang
&gt; Obviously there is no existing default std::hash&lt;std::shared_ptr&lt;Film&gt; &gt;() But there is! http://en.cppreference.com/w/cpp/memory/shared_ptr/hash
All of the people I've heard share an opinion on this have not liked Xcode very much. CLion seems to be much more popular, and there's a decent chance OP qualifies to get CLion for free.
op will still have to install xcode command line tools, at least. fwiw, I use apple's clang and vim. also, sometimes gcc.
I find the "nasal demons" approach to UB, while humorous, should be avoided in a topic meant to educate and demonstrate UB. I much prefer the llvm articles on this topic http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html
What annoys you about Xcode? What about eclipse? 
Sorry, I don't understand what your problem is with hashing by the pointer value. If you have two shared_ptrs pointing at the same object then they will have the same hash because they're pointing at the same object.
From the sidebar: **For C++ questions, answers, help, and advice see r/cpp_questions or StackOverflow.**
You want to look into the third template parameter for `std::unordered_map`. The default hashing implementation isn't to your liking, so supply a different one. 
&gt; Bonus question No! If you do that then you will have two independent shared_ptrs that think they own your Film object so it will end up deleted twice.
Point of design patterns is to make it easier for people to understand the code and to make modifications to it. So you know connect in boost.signal2 is more descriptive than calling push_back on some vector... Additionally although this is unrelated to design patterns directly if you have a library that does only pubsub and that library implements a lot of nice features like grouping, throttling... you do not need to implement all that by yourself. If you did that your solution would grow a lot and would have much higher chance of bugs. 
thanks!
addition to current answers: If you are in college and have an option to get Win for free I would use Visual Studio inside Windows VM. Beside that for simple problems you can use online compilers(coliru, idone...). Potentially even better option is online compiler+ problem solving site like hackerrank where you have problems you can solve and the compiler is integrated in the page where you are solving the problem.
On a standard side of things I feel this guarantee should be removed? I mean if I understand this correctly all push/emplace_backs are slowed down to handle this obscure case... STL do you have an opinion on this. I know Stepanov and Parent would agree with me. :) 
&gt; two different shared_ptrs that point to the same Film should hash to the same score, because I want to map a Film to a Score, not a film_ptr to a score Those two shared pointers **will** hash to the same score if you used the default hash for shared ptr (which hashes the ptr itself). What's the rationale of whoever said you should not hash pointer values? I would guess less performance due to collisions? If yes, is that your case (did you measure)? It rather looks like you just think you need to hash the value here *because* python does it. That is not necessarily true. 
Simple solution: use `std::unordered_map&lt;Film*,Score&gt;` and use `get()` on the `shared_ptr` to get the raw pointer as the map key. As long as you don't delete the raw pointer manually, everything will play out fine. Just make sure that your shared pointers outlive the hash map (judging by your design where all are in a vector), this will be the case.
Signal and slots typically add lots o sugar to function pointers and even additional features like a message queue and/or multicasting. 
 I think, my point of view may differ from yours!!!
Eh - not really. Using a goto introduces a few unwanted side effects: * You're using a goto and macros instead of a safe dedicated construct. I tend to avoid them whenever they aren't absolutely necessary. * You can't reuse labels, which is pretty awful. Every single case for every single loop (EDIT: in a context) needs a unique identifier. * They cannot be checked for correctness or exhaustion. Breaking to a label from another loop - Good, go right ahead! Refactor/move/misplace something and now it's pointing to danger - compiler can't tell that it's wrong. Want to catch any possible break - you can't. Want to catch a normal break or a successful loop - well we're back where we started. And my recommendation is just what I'd want - someone else could probably make a case for expanding this to allow breaks to return actual catchable values. I feel like this feature (the overall idea) is worthy of some discussion.
Signals / slots can handle state, lambdas, etc.
Thanks! It was a last minute change and I misread the docs trying to remove that extra copy. Fixed.
 documentation for Qt's signal/slot is [here](http://doc.qt.io/qt-4.8/signalsandslots.html) Documentation of different ways objects can be connected is [here](http://doc.qt.io/qt-5/qt.html#ConnectionType-enum) Best thing about Qt's signal/slot system is that they are thread aware and everything happens in the right thread and it removes a need to manually managing threads using mutexes and other locks.
i *hope* c++ never evolves to become ”safe”. it's one of the beautiful things about it, that you can do strange things when needed...or for fun and exploration.
Agreed. Is the main issue the fact that it's a `template` while it doesn't look like one? If so, why is that problematic? I understand that there many important differences *(e.g. `&amp;&amp;` means forwarding-reference in the context of a template argument and rvalue reference otherwise)*, but I believe that they wouldn't cause problems during library development as you should know that functions taking concepts behave like templates. The trade-off of having a more verbose syntax is not worth it in my opinion - what I want to express is *"`sort` takes a `Sortable`"*, and the way I should be able to do is void sort(Sortable&amp;&amp; s) If I need additional fine-grained control and flexibility I can always use a more verbose syntax, but I want terseness, simplicity and lack of boilerplate by default. Not having abbreviated syntax would also create more differences between lambdas and functions: auto l = [](auto){ }; auto f(auto) { } // invalid??? Which would probably invite people to (ab)use `inline` lambdas to have terser syntax: inline auto f = [](auto){ /* ... */ }; inline auto sort = [](Sortable&amp;&amp; s){ /* ... */ }; 
Industry, especially gaming industry, will use their own graphics library no matter what the standard offers. This proposal is mostly for educational purposes (and maybe for some scientific/engineering, like to plot a function). 
That is truly awful. In C++, it is generally considered bad form not to initialise a variable at the point of declaration. If you're going to write these, at least get a dev experienced in modern C++ to check them before teaching others bad habits.
What really pisses me of about this is that the problem of distinguishing functions and templates is trivial: void(Sortable&amp;); // template void(sortable&amp;); // function The standard-library has naming-conventions. Use them! If people would stop with this bull-crap of inventing new naming-conventions for every new project, there wouldn't be any problem at all and all the code would look much nicer and more readable. There are exactly zero valid arguments against doing so and concepts could have been a way to force people into it. Now this is lost as well.
It gets to be a bit much https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md Like trying to list all the ways you shouldn't tangle a string...
The point is that the standard imposes no ~~restrictions~~ requirements on what the compiler may do when it finds undefined behavior.
If anyone understands the Modules syntax, please change the C++ code in https://github.com/steveire/ModulesExperiments to show me what changes when using the `export`, `module` and `import` keywords. I have not been able to figure it out.
Qt's signals are thread safe and can be queued. They also support an unmatching numbers of parameters. ( you can connect foo(bool) to bar(void) ). The older, macro-based syntax allow creating connections dynamically. In fact, all slots in Qt can be called by their names. But, under all that, signals/slots are just that : a vector filed with fancy function pointers. https://code.woboq.org/qt5/qtbase/src/corelib/kernel/qobject_p.h.html#QObjectPrivate::connectionLists 
Hi Jackie!!!
That is a misconception, the compiler cannot generate code that you have not written. Your interpretation of your code and the compilers might differ due to UB.
It can be usefull but tricky if people mix different break levels in a same loop.
By doing that, you are losing everything that `shared_ptr` brings: not worrying about the lifetime of the objects. Using `weak_ptr`instead of `shared_ptr`is possible, although it has the same non-issue as `shared_ptr`: the hash method that was supposedly missing, yet which exist.
std::exchange already does that (but returns the old value). In any case, it's the semantics, not the syntax of ternary comparisons that I really want!
I like this a lot
This proposal seems `strict`ly superior to all previous proposals on this topic. Nicely done!
Well, this may be how it is today with some parts of the standard library, I'm certainly not denying that. But things **are** changing a bit. Just look at SG14, for one example. The STL will never satisfy all nice applications but it's goal is and should be to cover most use cases, including games industry, finance, and so on. The C++ standard is certainly not built and advanced "mostly for educational purposes (and maybe for some scientific/engineering, like to plot a function)".
[Someone once proposed a `operator :=:`](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3746.pdf).
More-than-1-line explanation warnings, like there's already, for example, for errors of overload resolution.
is that first part just an extension from optimization being a halting problem in general? My reasoning is that optimizer simply ignores UB path or range when it sees it. I dont think we can find all UB easily.
Right, it's hard to prove whether UB occurs, which is why the compiler assumes the programmer knows what they're doing and makes the assumption that UB doesn't happen (eg. assuming the programmer doesn't pass null pointers to a function that dereference the pointer). Proving whether or not UB happens in cases like the example above is the halting problem, so an assumption is made instead.
This is just an advertisement
Can that simple list of callbacks be modified from within one of the callbacks? I've never seen an ad-hoc observer pattern that actually worked correctly in a threaded world. https://www.youtube.com/watch?v=RVvVQpIy6zc In fact, it can't work. You need something along the lines of how Qt handles threaded signals/slots.
The lectures are free, you just have to pay for the certification (whatever good that does anyone).
After clicking enroll, click on a little "audit" hyperlink on the bottom of the page so that you can access for free the contents of the course.
&gt; Just add a function pointer, tag index or some other field to the data type which handles that. You're implementing the Observer Pattern. There are MANY ways to implement every pattern. Not all look exactly like they do in GoF book...in fact most of those don't even apply anymore as in they're implemented poorly for today's needs. If what you suggest is good enough for your project's needs then by all means do it that way if you like. I still prefer other methods, but whatever. More complex versions add overhead in complexity of course. Qt has to be complex because it's a general purpose library/framework. It must match anyone's needs, not just a particular set of needs. This of course compounds complexity...but, assuming they got it right, means you don't have to worry about a lot of issues.
"Accept the damn modular arithmetic and learn how to use it." This is not a bad choice but you don't live in a vacuum. As you note, the standard library doesn't reflect this view - and very little code does as well. Even if you controlled all the code, you'd still have a problem. The modular arithmetic is limited to binary moduli and the size of the modulus varies with machine architecture and integer type. As it currently stands, it's only useful as an implementation of modular arithmetic in certain special cases. "You propose to write safe&lt;int&gt; len = ct.size()" Not really. "This is shorter than either cast variant, but nowhere as near as convenient as auto len = ct.size()" a) auto len is not the same as safe&lt;int&gt; len b) safe&lt;int&gt; is five characters longer. Is this a big issue? I can't give much more of an answer without knowing more of the context. The "safe" way would likely be comparing ct.size() to some other safe integer would would trap any incorrect behavior in any case. This isn't about adding a tool which you use to re-write your program. It's about moving one's thinking away from manipulating integer types to thinking about integers in an abstract/mathematical/real world sense and enabling the compiler to trap behavior where the this view conflicts the approximation of integers as implemented by the underlying computer approximation. "IMO, no library will be able to solve these problems in an elegant way until the std library is fixed." The problems of size_t are manifestations of the problem, not it's cause. It can't be fixed by modifying the standard library. It's caused by the implementation of arithmetic for various integer types and can only be fixed there.
&gt;The second is moc. In order to implement signals/slots, which honestly aren't even needed for 99% of GUI dev work (callbacks work just fine, and are even nicer with modern C++ and lambdas.) You end up having to add invalid C++ like "public slots:", throw in Q_OBJECT macros on your classes, create special build rules to run moc against all your headers, and fall back on run-time errors for bad parameters to SIGNAL() or SLOT(). Using them also means having to declare a function for every callback, although I hear Qt is better these days at binding lambdas. Since Qt5 you can write connections like this: connect(button, &amp;QPushButton::clicked, []{/*do something*/}); The pointer-to-member syntax gives you compile-time errors if you fuck something up.
So I hacked up Heath's code (linked above) to create a cmd script. If you compile this [gist](https://gist.github.com/AndrewPardoe/975a821aa3e865cfaf576fafbfdcbe2f) along with some headers from a NuGet package it will create a command script that sets environment variables to each VS install on the machine. NuGet packages are ZIP files so it's easy to grab the headers you want. You can then run the command script, change directory to environment variable and run the vsvars64 below that. Here's an example run on my box. C:\tmp&gt;program foo.cmd InstanceId: 0121b361 (Complete) InstallationVersion: 15.0.26206.0 (4222126368096256) InstallationPath: C:\Program Files (x86)\Microsoft Visual Studio\2017\Community Product: Microsoft.VisualStudio.Product.Community Workloads: Microsoft.VisualStudio.Workload.CoreEditor Microsoft.VisualStudio.Workload.NativeDesktop InstanceId: 22618a01 (Complete) InstallationVersion: 15.0.26206.0 (4222126368096256) InstallationPath: C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise Product: Microsoft.VisualStudio.Product.Enterprise Workloads: Microsoft.VisualStudio.Workload.CoreEditor Microsoft.VisualStudio.Workload.ManagedDesktop Microsoft.VisualStudio.Workload.NativeDesktop C:\tmp&gt;foo.cmd C:\tmp&gt;set VSInstallDir0121b361="C:\Program Files (x86)\Microsoft Visual Studio\2017\Community" C:\tmp&gt;set VSInstallDir22618a01="C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise" C:\tmp&gt;cd %VSInstallDir22618a01% C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise&gt;cd vc\Auxiliary\Build C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Auxiliary\Build&gt;vcvars64.bat ********************************************************************** ** Visual Studio 2017 RC Developer Command Prompt v15.0.26206.0 ** Copyright (c) 2016 Microsoft Corporation ********************************************************************** [vcvarsall.bat] Environment initialized for: 'x64' 
I'm sorry I'm a bit confused. Command line tools isn't a complier ? I thought that once you install the command line tools, you get the Gcc complier ? 
Because then you have to have an std::shared_ptr instance to perform lookups into a hash table, but you may not have one. You may just want to lookup the pointed-to object to see if it's already in the table somewhere and is owned by an existing shared_ptr, instead of creating two shared_ptrs that point to the same data but stored in different objects.
The compiler takes it from the other angle. If you dereference a pointer, and the pointer is NULL, this is Undefined Behavior. So when the compiler sees you dereference a pointer, it makes an annotation: "Pointer is not null". If you access an array of size N with an index `i`, and that index is out of bounds, this is Undefined Behavior. So when the compiler sees this access, it makes an annotation: "`i &lt; N`". Et caetera... The compiler does not attempt to prove that UB could or could not happen, which would require proving that a specific path of execution happen, and thus would require solving the Halting Problem (in general), it *assumes* UB cannot happen, and optimizes according to the assumptions.
As soon as you try to tackle the generic case, you need to answer questions like "should this be a sparse matrix" etc. It is probably best to avoid that. I would go further and not call matrix_2d a matrix at all. Call it a transform.
Conan looks very useful. Dependency management per project rather than per system is huge in my opinion. Thanks
"xcode command line tools" includes a compiler, as well as a bunch of other stuff (sysroots/SDKs). The included compiler in recent versions of it is clang. If you want to build things without installing the command line tools, you're going to have a very hard uphill battle. Building GCC without using Apple's sysroot, for example, is possible (as in, we do this at work) but quite difficult, hence being something I wouldn't recommend for someone new to it.
That's a good point. It all depends on how the program is structured overall, who owns the Film objects and how you pass them around (by value, by pointer, by shared_ptr, by some identifying key, etc).
There are a few things at work here. * Some things are in fact harder to do for C++ than they are for other languages. Dependency management for sure is an orders of magnitude more difficult problem for C++ than for Java. #Tradeoffs * Part of it is simply that it sounds like you don't really know what you are doing. For my self a 15 line makefile would take maybe 5 minutes to write. Sounds like you don't know Make. CMake, being a build system that was designed with make in mind is much easier to understand if you already know Make. * Conventional project structure? Simplified compilation? Are these benefits? The sound like tradeoffs that benefit the amateur over the expert. That is another thing to realize about the C++ community as a whole. The programmers that have gravitated to C++ have done so because they want a powerful toolset not because they want a simple one. This is why a language like Go, which was designed as a replacement for C++, got virtually no converts from the C++ community. Everyone would like a quicker project setup, but this is not something that you do everyday. So, C++ developers will tend towards resistance to anything that places restrictions on them in order to make a once per project task quicker (like conventional project structure).
You could check [CPPAN](https://cppan.org/). I have this [video](https://www.youtube.com/watch?v=VE9TJJfwDEY) with SFML example from last year. That's actually one-filer. If you have several files, you could create cppan.yml config near them with proper list of dependencies: dependencies: pvt.cppan.demo.sfml.graphics: 2 
I usually have a `extern/` with packages built and install locally. You can set the `CMAKE_PREFIX_PATH` to that folder in your project, and set `CMAKE_INSTALL_PREFIX` to that `extern/` when building dependencies, so the `make install` command install into your directory.
#include &lt;iostream&gt; #include &lt;stdlib.h&gt; int Average(int i) { static int sum = 0, count = 1; sum = sum + i; count++; return sum / count; } int main() { int num; do{ cout&lt;&lt;"Enter numbers ( -1 to quit )"&lt;&lt;endl; cin&gt;&gt;num; /*if number is not -1 print the average*/ if(num != -1) cout&lt;&lt;"The average is "&lt;&lt;Average(num)&lt;&lt;endl; }while(num&gt;-1); return 0; } 
In which paper is it?
The reason why the problem does not seem that big is because you still do not understand it. It is not just Linux or Windows that would need to be taken care of. It is every version of Windows ever made and every version of Linux ever made. On Linux we already have this. Each distributor creates a canonical set of packages that work together. So, C++ devs use this. On Windows the situation is more difficult because it is more difficult to tell what versions of libraries and the like a person has on their box. For this reason most people that deploy on Windows ship their programs statically linked against their third party dependencies. The intractability of this problem is exactly the reason that Java exists at all. What exactly do you mean by robust? The quality of robustness in software is the ability of a system to deal with erroneous input. Are you saying that Groovy (which is not strictly speaking a new language to Java developers. Groovy is a superset of Java) is some how more tolerant of erroneous input than Make? That seems unlikely. They are both programming languages if you specify the program incorrectly they both will do the wrong thing. As for Gradle being easy to use again your opinion on this has to do with familiarity. I have used Gradle and I find it to be extraordinarily frustrating to work with despite the fact that I know Groovy fairly well. I learned Make first so that is how I think about software builds. At the end of the day C++ devs are not stupid, nor are they fans of doing a bunch of busy work, nor are they fans of writing boilerplate. C++ is used for pretty different purposes than Java, Ruby, Python... The toolsets available reflect the purposes the language is put to as well as the constraints of the language (auto refactoring tools are difficult to implement for C++ because the type system is Turing Complete) . For instance no one really writes one off web apps in C++ so there are not really any tools that will bring up a quick web app skeleton like Rails has. 
I don't think `Sortable` is a great example, even though people like to use it. You'd usually want to take an optional comparator in most cases, which means that `Sortable` will be depending on said comparator type, in which case you can't use the terse syntax unless you write a new overload instead of using default function and template arguments. But then that just trades one form of verbosity for another. In your example, you'd also need to be careful to write your `Sortable` concept so that it can be satisfied by lvalue reference types. For example, if you need to check a nested type, you'd need a healthy sprinkling of `remove_reference_t` or equivalent. As to the "invitation" to abuse `inline` lambdas, those two are about the same length: constexpr inline auto sort = [](Sortable&amp;&amp; s) { }; template&lt;Sortable S&gt; void sort(S&amp;&amp; s) { }
[removed]
Cap'n Proto **absolutely** supports the same level of forwards/backwards compatibility as Protobuf (and presumably FlatBuffers). You can add new fields to a Cap'n Proto struct over time, and you get the same effect as Protobufs: old clients reading new messages ignore the new data, and new clients reading old messages fill in the missing fields with default values. It is true that FlatBuffers uses a convoluted vtable mechanism to avoid sending empty fields on the wire. This is an optimization trade-off. It may be faster / smaller in some use cases, but it will be slower / larger in others. The added bookkeeping to construct, de-dup, transmit, and dereference vtables is not free and it is not at all clear that it is worth the cost for typical payloads. Usually, a structure that has an excessive number of optional fields -- where most aren't used most of the time -- is probably mis-designed. Most cases I've seen make more sense as unions. Protobufs did not support unions until relatively recently, which led to the anti-pattern of excessive optional fields in a lot of Protobuf-based protocols.
I think syntax is much less important than semantics, assuming the syntax is not completely brain-dead. With a bit of experience you will forget about the syntax and what will become important is the conceptual model. Think about someone who only programmed in C++ looking at Bash for the first time -- the syntax would seem completely insane.
I find the answer provided in the FAQ to the question "How is this better than CMake?" a little lacking. I'd like to see an example of the CMake required to do a complex thing and the equivalent build2. I've not personally run into issues using CMake that I couldn't solve with a little conditional logic (to do the right thing on the right platform) and custom commands. Where CMake is lacking support for some compiler feature I can either write that support myself as a macro or call out to an external tool of my own design. Of course this is not ideal, but having these "escape hatches" are what allows CMake to thrive. There is always a way to make something work, even if it's ugly. This to me is a superior solution when it comes to supporting new compiler features. If I'm understanding build2 correctly, I would have to wait for build2 to support new compiler feature X whereas with CMake I can immediately incorporate a workaround. Also, CMake's out-of-the-box support for most popular IDEs is not to be understated. Is there any way, for example, to open a build2 project in Xcode? If not, in my opinion build2 isn't a complete toolchain -- it's missing a crucial link.
I don't see a contradiction, honestly. I am talking about what things mean, not how they look. You get over syntax quickly. I am not sure you can get over lack of a conceptual model. But, look, I appreciate that some people may find `build2` syntax foreign. Though I think if you undertand `make` and understand its limitations when it comes to handling today's complexity (see my reply to OP for details), the rationale behind the syntax should be pretty transparent. But then again, some people find a mix that "resembles a C like language and Bash" intuitive ;-).
&gt; it's not obvious to me why "sfml" is repeated on the same line with different case Right, I couldn't type the whole manual in there, could I? ;-) The capital name in `import` is a project name while the second is a target name. A project can conceivably export several targets. &amp;nbsp; &gt; I don't think that necessarily makes CMake more difficult to use, just more difficult to understand It makes it *impossible* to do things that the original authors didn't think of. Your only option is to build another black box.
&gt; Right, I couldn't type the whole manual in there, could I? ;-) The capital name in import is a project name while the second is a target name. A project can conceivably export several targets. Ha, no, but I find the equivalent CMake `project(sample)` to be self-explanatory where build2 is not unless there's a line not included in your example. &gt; It makes it impossible to do things that the original authors didn't think of. Your only option is to build another black box. If building another black box makes it possible then it's not impossible ;-) Again, I don't dispute CMake's ugliness but it does allow me to just get something done when I need to.
So it's build2 doing all the building itself or can I choose to generate ninja makefiles? Honestly I sceptical about build2 being fast as ninja. 
&gt;My problem with CMake is that it's all voodoo, you don't have conceptual model of the building blocks. +1 Even worse: it looks like declarative language, while it is absolutly not. I really like [qbs](http://doc.qt.io/qbs/language-introduction.html): &gt;import qbs &gt;CppApplication { &gt; name: "helloworld" &gt; files: "main.cpp" &gt;} 
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0587r0.pdf
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/5tyvun/anyone_knows_good_resources_on_data_structure_c/ddq598o/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; It is not just Linux or Windows that would need to be taken care of. At risk of flogging this dead horse, also every version of Solaris, and all those BSDs, and crazy OSes you've never heard of, on combinations of hardware that most of us have again never heard of. All the custom operating systems that 99.99% of the C++ programming universe will never see. All those embedded platforms that don't even have an OS and come with extra hardware just to be able to compile the code into something you can then blow onto the actual target hardware. And so on, and on. C++ is an abstract, platonic ideal that gets executed anywhere and everywhere (although since 2011, a little bit of memory model has had to be defined, for threading support I think). Trying to define a standard set of build tools would have to exclude real builds people are executing today. Compare this with Java, which defines a virtual machine. The universe Java exists in is deliberately defined universe, with all the benefits and drawbacks that comes with.
Makefiles are ok. The build tool should not be easy to use. In every big project the build process is the hardest part of the work. If you can not use makefiles in a light-hearted manner - just learn it a bit more. It's not rocket science. Really.
Yeah, thanks for the tip. However can you elaborate on why you do this? What's the benefit - compared to the alternative(s)?
You just highlighted why a lot of people prefer to use visual studio 
Wow, this looks great. I've been using Qt for a long time but never saw that. In my opinion, the QML syntax is the best declarative syntax I've seen. 
&gt;On Linux we already have this. Each distributor creates a canonical set of packages that work together. So, C++ devs use this. Not good enough (for development), not even close. As an example, say I want to use the POCO libraries. The current version of Ubuntu (16.10) has version 1.3.6 from 2009, i.e. 8 years ago! Actually, no. The version they have is `1.3.6p1-5.1build1`, which is like 1.3.6, but with 7 custom patches applied by the package maintainer! And that's not all! If for some reason you want to use this ancient version with a cmake-based project, the `find_package` command will not find it, because the required config .cmake files are not included in the package! Not to mention, what if different software needs different versions? So you're back to installing from source. Compared with this, every other langauge has a tool (npm, cargo, etc.) that manages dependencies *per project* and more importantly, it is *the library authors themselves* that create and upload the packages, not some 3rd party maintainers. Distro packages may be good enough for the end user, but are terribly inadequate for a developer. &gt;At the end of the day C++ devs are not stupid, nor are they fans of doing a bunch of busy work, nor are they fans of writing boilerplate. I think it's pretty obvious that the C++ ecosystem didn't reach its current state by choice. It is what it is because C++ is a really old language (not to mention its C legacy) that carries with it all this cruft from an era where we didn't have the tools we have today. It's not because C++ programmers *want* it to be that way, it's just that we have tons and tons of existing code and projects and conventions that nobody is going to migrate. Sorry for the ranty tone.
Yep, it looks great and the whole concept is kinda *proper*. QtCreator has qbs support out of the box. Recently qbs got generators of MSVC solution files (for better inregration with MSVC IDE, VC compilers were supported from the beginning).
Just to mention, in case people are unaware of this: Gradle team seems to be working hard to support C++ on it. See https://docs.gradle.org/3.3/userguide/native_software.html. They also have a video from a past Gradle conference in which they discuss specific needs of native project builds and what they needed to change in Gradle in order to support it: https://www.youtube.com/watch?v=KZdgxKe9wO8. About **CMake**: The main advantage I see on it is that it ended up being one of the closest things we have to a standard and ubiquitous tool amongst C++ projects and platforms (that's it: one of the closest things we have to a convention). I've worked on a project which had lots of external dependencies and at the time almost each one used a different build tool (Boost B2, Autoconf, QMake, CMake, custom build scripts...). That forced us to always need to learn and relearn how to configure, build and use each project using each one of these tools, which was really cumbersome. So although writing a `CMakeLists.txt` for a project is indeed a little bit tough sometimes, the easier "consuming" of external projects (configuring, building and importing them with `find_package()`) made up to it. Especially with the addition of *target usage requirements* latelly. I do believe that there is plenty of room for improvement in both the build tools and also the external libraries publishing/consuming tools in C++. But if any new tool is to be created in that sense, it needs to be able to get somewhat mass adoption. Otherwise, it might end up only making things tougher as it would be one extra tool to learn and support in your development environment.
**Company:** [PPRO Group](http://www.ppro.com) **Type:** Full time **Description:** At PPRO we’re pushing the boundaries of payments to accelerate the growth in e-commerce and make consumers’ everyday lives easier. We do this for our payment partners by using technology to abstract the complexities of a diverse array of local payment methods. Additionally, we offer prepaid cards and e-money accounts to both business and consumers as a better alternative to paying using traditional banks. To accomplish this, we have implemented our own transaction processing, card issuing and back-office systems, which are built by our in-house engineering team in Munich. The PPRO Group is an FCA-authorised financial institution headquartered in London and with offices in several European countries. Our team comprises a fast-growing, diverse group of people from more than 10 nationalities. We pride ourselves on our dynamic culture and our ability to deliver the best possible payment products and services to our partners and customers. PPRO people are characterised by their desire to succeed, their team spirit, their high energy and professionalism, and by their eagerness to find simple, elegant solutions to complex challenges. We are looking for Software Engineers (C++/Unix) to join our team in Munich. You will contribute to all phases of the software development process from requirements analysis to delivery, write high-quality, scalable and maintainable code for component-oriented, distributed systems, collaborate with your team members to continually improve working processes and share knowledge. **Location:** Munich, Germany; company language is English **Remote:** No **Visa Sponsorship:** No. **Technologies:** Modern C++11/14 on UNIX (FreeBSD), using clang and cmake. We also use Python, SQL, ZeroC Ice, crypto++ and some boost, but experience with modern C++ and Unix are the main requirements. **Contact:** Please send your application to engineering-job@ppro.com.
Still requires you to list the source files one by one.
&gt; What is target_link_library? Is it a function? Is it a macro? A thingy? Why do we call anything in out presumably-declarative dependency specification? ... why do you care ? Given this C++ code : int main() { return foo(123); } can you say what foo is ? A macro ? A global function object ? &gt; My problem with CMake is that it's all voodoo, you don't have conceptual model of the building blocks. Everything is specified here: https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html (or in `man cmake-buildsystem`)
Why is the constexpr version recursive and the other iterative?
no, please don't do this :( will somebody think of the windows and osx users ?
&gt; can you say what foo is ? A macro ? A global function object ? If I write this code? *Absolutely*!!!
I think you are being ridiculous. I need to *understand* what it means, not how exactly it is implemented down to such nuances.
&gt; Do you know of a way? No, I am genuinely interested why you chose two different programming schemes. &gt; It can't be copied wholesale because of the for loop state, … Can you elaborate on what gets copied where? I really like this approach of avoiding magic numbers via constexpr! Less need for defines and enums.
Yeah, it made the code much simpler also and I suspect faster too in this case. &gt; Can you elaborate on what gets copied where? I just meant the code, the reason the 2 had to be written differently was because for loops are not permitted in constexpr functions! 
&gt; Qt proper might not seem to get enough developer live at times. Qt is a hungry god that demands sacrifices
I was hopimg for a cage match.
I did know about that tool for quite some time, had [some issues](https://github.com/xiw/stack/issues/13#issuecomment-36355911) making use of it at the time. Development has stopped. It's the kind of functionality that a compiler that does that could embed to show when it does. [Not specific about code elimination alone though](https://www.reddit.com/r/cpp/comments/5tlnkr/undefined_behavior_in_c_and_c_programs/ddpvwy1/).
Why don't you describe what you are trying to achieve, there might be a better way. Right now this sounds like an infinite loop if said element is in the array. You could change the loop variable directly but I highly discourage this.
Extended constexpr isn't supported by Visual C++ yet, as far as I'm aware. I haven't really checked for it in the 2017 RC, though.
Do you expect anyone to use a library without understanding what the API does? That's even more ridiculous.
Yes, VS2017 has it.
rpaths are and always have been the devil incarnate.
The biggest obstacle to C++ is compiling things and getting dependencies into projects. And now we've got Docker, even compiled languages like C++ are basically write-once run anywhere.
It is possible to create a list of all sources in the project using CMake, but as far as I understand it, the option is generally considered poor style or an antipattern. I personally disagree, though. For some projects, it makes more sense to say "compile all the .cpp files".
It is considered poor because it freezes the list of sources to compile at configuration time causing direct invocations of the underlying build system to ignore new files that are introduced or files that are deleted. You will have to manually reconfigure the project to get the updated list of source files. 
oh yes, 1000 times yes. CMake is the most annoying build tool I've ever seen. It's like someone took the worst part of C (macros) and thought it would be a good idea to make a language out of it. 
I love QBS and hope they start pushing it instead of qmake as the default build tool for Qt stuff. Starting with an existing declaritive language really helped, and the tags/transformers thing works really nicely in practice.
And the STL has been updated to use C++14 constexpr. (We're still working on the additional constexpr added to the STL in C++17.)
There are many build systems for C++ but GNU make and CMake are the most common. If you are targeting Only window them Visual Studios might help to simplify some of the build if you are the only one working on the system or everyone has the same layout of source file and libraries. Cargo, GO tools and Maven might be simple to use to you but ask a total layperson with no programming experience to setup and use these tools from scratch with no guidance and I believe that might find the experience to complex and confusing. The general philosophy with C++ is you do not pay for what you do not use and if a build system adds something the run-time to help simplify builds that is an unacceptable trade-off. The one area that C++ is working to remedy in the standard is the confusing C legacy stuff.
It really sucks when one of your coworker adds a file, `git push` it, you pull it and you forget to run cmake.
* Make does not come with the default windows toolchain, visual studio (and not even with all distributions of mingw) * Even if it did, `g++ -o foo *.cpp -lz -lmy_library` would still require mingw, a correctly set path, and libz. (actually I don't think that g++ would work, you'd need at least a `CC=mingw32-g++` in some mingw distros), which excludes visual studio and the latest windows C runtime if I am not mistaken. 
Ah, sorry, patterns are coming in the next release of `build2`.
We have implemented (admittedly limited) rpath emulation for Windows (using assemblies/manifests) for being able to run tests without any `PATH` hackery. Now that is Satan itself. But, man, when you can just run tests at will, it's all worth it.
There are relatively few collisions (see http://softwareengineering.stackexchange.com/a/145633) but if there are, I guess you would need to modify the strings or use a modified hash which has less collisions or generates 64 bit patterns instead!
64-bit pattern may lower the chance of collisions, but not eliminate them. To do the latter, you would need [perfect hashing](https://en.wikipedia.org/wiki/Perfect_hash_function).
I am developing a build system called [Meson](http://mesonbuild.com). Its main goal is to make build systems [not suck](https://www.youtube.com/watch?v=KPi0AuVpxLI). We aim to achieve this by being extremely fast with a build description language that is simple and readable. A helloworld example looks like this: project('hello', 'c') executable('hello', 'helloworld.c') This is all that is needed to compile on Linux, OSX, Windows and other platforms with Gcc, Clang, VS and the Intel compiler. A few more sample projects can be found [on this page](http://mesonbuild.com/samples.html). Meson is currently being used by real world projects such as [GStreamer](http://gstreamer.freedesktop.org/) and [Pitivi video editor](http://pitivi.org/) and it is being considered by [Wayland](https://lists.freedesktop.org/archives/wayland-devel/2016-November/031984.html). We also have a multiplatform packaging system called [Wrap](https://github.com/mesonbuild/meson/wiki/Wrap%20dependency%20system%20manual), though, granted, there are not many packages yet available. We have native support for a bunch of languages, which makes it possible to do crazy things like a [Python extension module that uses C, C++, Fortran and Rust](http://nibblestew.blogspot.com/2017/01/a-python-extension-module-using-c-c.html). Feel free to try it out, you'll probably like it. If not, please let us know so we can fix things.
It would be voodoo if it wasn't completely documented. But each of those functions has a dedicated manual page. And the macros can be read directly if you want to see exactly how they are implemented. The OP is coming from the Java world. After having experienced both, let me say that CMake is a ray of sunshine compared with the horror which is Maven. It might be declarative, but it's incredibly poorly documented. It's all cargo-culting based upon what google finds on stackoverflow! I exaggerate, but not much. There's many a time I wished maven had such simple things as conditionals or an easy way to run a program without 40 lines of boilerplate. I picked up CMake in a week, after which I'd fully converted an entire project including all the custom autotools logic. Just by reading the manual and looking at a few other projects to see what the best practices were. Like any tool, there's some up front investment. But that applies equally to make, ant, maven, autotools, all of which are arcane until you've done some up front learning of the system.
Maybe it can be argued Cap'n Proto supports the same level of forwards/backwards compatibility, but you can't really make use of it for fear of bloating your binaries or requiring a copy/compression step. It's great that you have the luxury of saying that excessive optional fields is a mis-design.. welcome to the real world where I've seen that to be extremely common. So you can either have a format that requires programmers to be very restrained and have a lot of foresight, or you can have one that gracefully deals with all sorts of ways data can evolve at little cost. I know which I prefer. Note it is not just about being "optional", it is also about being default. It is very common for a large amount of objects in your total set that happen to have a field set at the default value (of e.g. 0). FlatBuffers can optimize this out too, even if the value is present in your source data. 
Equal hashes do not guarantee equality, so this means you still have to do a string comparison after the switch for each case. Also it seems weird to me that you use hashing (which is a lookup optimization) to solve a matching problem. This is the equivalent of having an unordered_map with string keys to lambdas minus the allocations. From a straight big O perspective, you are doing 2 linear operations (calculating the hash, then a string comparison after the switch to filter out collisions) when you could be doing a single linear operation (string matching, using a simple regex for instance or a prebuild DFA). Using a regex here is fine, because you already know all the keys (which is why this is a matching problem). Now, if you tested this and found that using hashes is faster even with 2 linear operations vs 1 linear operation, that is a different story.
the input string can collide with one of the cases, can it not? I guess we are assuming that we are only parsing well formed json files here? Otherwise different input strings can map to the same cases even if the cases do not collide with each other.
What I'm saying is that the state of the ecosystem is not a tradeoff that anyone consciously made. It is a situation that arose organically, caused by a lack of standard conventions and good enough tools in the past. For example the lack of any convention on project structure - it's not that people didn't *want* any, there just wasn't anything widespread, so people just kinda structured their projects in whatever way came to mind first. Nobody decided against using an existing convention in order to gain something else - there just wasn't any convention to use in the first place (and you can't make a tradeoff when you don't have any options to choose from). I believe people *do* want a good dependency management tool, there just isn't anything widespread enough, which makes it not very useful, etc., classic chicken and egg problem.
Yes it can. The standard places no requiremnts on the behaviour of a program which it does not define. UB is anything. Nasal demons, hard drive formatting, a cha cha line of ascii dancers. Time travel changing data prior to hitting the UB. Anything.
C++ can more or less do anything you want it to. The question is - does the service you're trying to talk to (Steam) support what you're trying to do, and what does the API (Application Program Interface) look like? You should probably check out https://developer.valvesoftware.com/wiki/Steam_Web_API Since it looks like it's a Web API, while possible to talk to in C++, you may find it easier / faster to use a higher level language like C#, Java, Javascript, etc. Ultimately I'm not sure r/cpp is the best forum to start with.
&gt; Gradle uses Groovy (a completely new language for most Java developers), yet you can do a ton with it despite not knowing anything about Groovy Maybe, if you know Java. But if you don't (like it was for me when I was adding some hooks to Jenkins) - it takes hours like for you and make.
That said, it's great that this technique (provides you use both of the colliding strings in the same switch statement) yields an error at compile time letting you know there was a collision, rather than giving you a potentially nasty surprise at runtime.
There is something else already
I know it's been nearly a month, but I figured I'd let you know that I made some major edits to my project using the method you posted. There's no doubt that it's a huge improvement over how I was doing things before. Thanks again for the tips!
I was just going to suggest meson. I love its declarative nature.
You can use the `vsdevcmd` script to configure the command prompt for required target platform: * 32-bit ``` "C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\Common7\Tools\vsdevcmd" -host_arch=amd64 -arch=x86 ``` * 64-bit ``` "C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\Common7\Tools\vsdevcmd" -host_arch=amd64 -arch=amd64 ```
Notice how the path to the batch file changes depending on the installed SKU... ;-]
Ah, yes, that is correct. But I think you should only use an if-else chain if you know all possible inputs.
Shameless plug. Opes-source peer-2-peer file synchronization app: https://librevault.com
&gt; I'll consider updating the slides to reduce the amount trickery. Obviously you should not be basing your slides based on feedback of one person (me) but if you know some junior C++ devs you may want to ask them what do they find easier to understand. I feel they will say similar things as me, but you can only be sure if you ask them. :) 
You only include the file containing the declarations (edit: and documentation); _it_ includes the file containing the definitions at the end.
I think that it provides better separation of declarations and definitions and also makes headers shorter. Most of the time I open a header I want to see just declarations and some docs. If I'm interested in implementation, I can navigate to the .inl file and look how the function is implemented. P. S. I mostly do this for long template functions, as for one-line inline functions, I usually just do it like this: inline int sum(int x, int y) { return x + y; }
&gt;If the thing is badly maintained - it's the sign that nobody needs that. Yes, not many Ubuntu packages depend on it (I found like 5). That kind of makes sense given that UI apps often use large UI frameworks like Qt or wxWidgets that already have all the same functionality built in. If you want to measure popularity, it has like 1800 github stars, for what it's worth. Plenty of fairly popular C++ libraries aren't even in Ubuntu repos at all. POCO itself is maintained just fine, with pretty frequent releases, last one being in December 2016 (and with github commits from even today). It's true that it's not "modern" and uses a somewhat old-school C++ style, but so do many other libraries. These days there may be better alternatives for its parts, but that wasn't always the case. Plus it's very portable, including good support for iOS, Android and Windows (including Visual Studio), which Unix-centric C++ devs often neglect. &gt;If you need the library - become its maintainer in Debian. That's how it worked in opensource. That's pretty ridiculous for many reasons. Especially when compared to other languages where you just add a dependency to `build.gradle`, or where you `npm install` a thing that the authors themselves manage. Distro packages are good enough for end users - not for development.
Fair enough, I just always think of inline functions to be a couple of lines at most, otherwise they should be non inline functions (this reasoning doesn't extend to templates).
Agreed. I've mostly seen ".inl" files used for template stuff.
That's the same guy who also wrote this: https://gist.github.com/bkaradzic/2e39896bc7d8c34e042b So maybe he's just behind chronologically?
Some 15 years ago I first saw the "pro" way of doing this, which is: what the author says, plus not inlining the inlined non-template stuff (rather, compile it in an appropriate translation unit) for faster DEBUG build times. A bit of work, but pays off eventually...
`inline` in C++ doesn't mean inlining. It's a way to have the same definition across translation units without violating ODR.
&gt; or features we used proved to be bad (like RTTI, exceptions, and streams), this guy lives in a parallel world
Actually I was trying to code atm cash dispenser's note denomination for the user input amount. And by now I have completed that I can share you my github if you wanna review or to have a look !
Yeah sorry this time, I'll post there from next time~
Extremely sad to see that people agree with his severely flawed message.
Wow, I really would like to laugh, but... as an embedded person myself, dealing with such people on a regular basis, I rather cry :-(
Keep it up, you are awesome.
I think what he meant was "emit a warning when compiler eliminates dead code based on UB". It seems to be the most common cause of UB bugs to me.
Usually for this kind of task I see folks using Python. I believe it's suitable due to being easy to use, powerful and productive.
&gt; How about Android, iOS, XBox. Yes, Android and iOS is on our TODO. XBox, CUDA, etc., -- contributions/external rule modules welcome. &gt; And how about cross-compilation? Emscripten? SWIG? Code generation? Yes. &gt; How will it integrate with IDEs? This is probably the iffiest part. One option is if IDEs follow Microsoft and provide a build system-agnostic mechanism like VC's "open folder". The other option is for IDEs to start using `build2` underneath. &gt; If I build a tool with libclang, how will I reuse my `build2` project definition? You will write a rule for your tool and distribute it as an external `build2` module. Proper support for code generators is one of the top goals of `build2` (we use them a lot ourselves, see [ODB](http://codesynthesis.com/products/odb/) for instance). &gt; [...] they make architectural decisions that make it impossible to move forward from other angles. What makes you think this is the case with `build2`? Our other main goal is to have a conceptual model of how things are built so that people can use `build2` for things we haven't even thought of.
&gt; two containers (e.g. vector) with two different allocators are two different types (i.e. vector&lt;int, AllocatorA&gt; != vector&lt;int, AllocatorB&gt;) Yes, the downside of policy-based design: http://foonathan.net/blog/2017/02/08/policy-based-design-problem.html It is easy to write a type-erased allocator though and the rest can also be improved, like the C++17 polymorphic memory resources (http://en.cppreference.com/w/cpp/memory/memory_resource) or my allocator model (https://github.com/foonathan/memory).
If those `print` and `sequence` are being used only inside one function, I see no problem in defining them inside that function. Defining them outside will only make user search for other calls to this functions, only to realize there are none.
\&gt; `for(auto e : container)` \&gt; Read-only use in loop body *Twitch*
&gt; I am very tired of ppl coding c++ as if it is still the 80s. Right from my heart. I do not claim to know everything, but sometimes I wonder about the resistance of learning stuff of some people. The sad thing is, the "new" stuff is not completely foreign, but an evolution of the language. People are lazy, they stick to what they "know", even if proven inferior/false, time after time after time. I think, this gets me the most... and it is always up to me to prove stuff.
For me part of the problem is that my last two jobs has at least in part involved mentoring developers and updating a code base or department to a different technical standard like moving their code bases to C++ or to a newer standard in C++. And one of my big problem is developers of all ages and educational levels who use vague claims of "safety", "simplicity" or something else to argue against any changes of their current code standards, architecture or language versions. I know it is hard to change, but sometimes people play at politics to stop change and I get frustrated from time to time (and I may occasionally went using bad language about these developers).
I've never really found std::transform to be that useful. I most often _want_ to use it in a way that will change the type, and that means I need to output to a separate range. Then I typically want to pass over the range again with a separate algorithm. The problem is that passing over multiple times is inefficient. transform seems like it is going to benefit substantially from the Ranges TS.
What book would you recommend? We've been using an online one called Zybooks and it's decent but I feel like it could be a lot better. And as far as finding something of interest, my problem right now is we're only learning console applications and I can't think of anything of use to me using just that, but I know I need to practice nonetheless. 
Well, a mobile app is out of our reach for a first approach and feedback at the moment. That is why we are trying the mail approach ATM as feedback for viability. About companies: the plan would be to be able to monitor the accumulated history to be able to make tools around this data. Imagine you can have some kinds of performance profiles for every developer in your company. We really think this could help, but we might be wrong or not considering other problems. Thanks for the feedback. 
make the function static and the user will know they only need to search that file.
By this logic, most of my private implementation methods would become closures. Suddenly my 10 line methods get bulked back up to 100 line methods, defeating the purpose of creating a new function/method in the first place.
Methods and functions are not the same thing.
Lambdas *are* functions. 
&gt; Distro packages are good enough for end users - not for development. My issue with the "good for developers" approach is that the user at the end deals with a thrown-over-the-fence binary or a ton of language-specific package managers (or even multiple package managers for single language).
&gt; scalar types &gt; using auto Also, in this case container is just `const auto &amp;` too. You have no way of knowing that it'll always contain scalar types.
Your comment doesn't make sense to me, given you were the one asking for something new Oo
While I understand that, I'm curious as to why you believe the distinction is so great as to need to point it out in tangent?
You are right, of course. But you can use them to separate logical blocks of code inside one function or avoid code duplication inside one function (in cases where it is not rational to move that code to a separate function). Consider, for example: void draw_circles_for_some_abstract_objects(const Collection&amp; objects) { auto make_circle = [](color_t color) { CircleShape circle; circle.setRadius( 150 ); circle.setOutlineColor( colors::red ); circle.setOutlineThickness( 5 ); circle.setFillColor( color ); circle.setTexture( some_texture ); return circle; }; for (auto&amp;&amp; object : objects) { // do some other things auto circle = make_circle( object.is_valid() ? colors::red : colors::blue ); // do more things window.draw(circle); } //cleanup things } Yes, you can write code in place, no problem. But moving that code into lambda just logically separates that code from other code. When you have this kind of code inside some big method, it is easier to navigate in it, and it is also easier to debug it.
`for(const auto e : container)` no reference, no sadness
:D I'm sure it's not too far off! Nobody I know would probably recognize `[](){}();` as valid C++.
Yeah :) But it's just a reference to [this joke](https://www.reddit.com/r/ProgrammerHumor/comments/2pzv2e/in_c14_you_just_write_auto_autoauto_auto_auto_the/).
Of course, but only if you're actually using that static analyzer.
there is no excuse for not using all the free available analyzers on your code as part of a CI pipeline
I think the real issue people have is what it actually MEANS to have the terse syntax. For example: void DoThing(RandomAccessIterator&amp; Begin, RandomAccessIterator&amp; End, RandomAccessIterator&amp; OtherThing); WHAT would you expect that the types of Begin/End/OtherThing be? Should they be allowed to be different types? As the Concepts proposal is currently, all 3 would have to be the same type. In my personal straw-poll at both CppCon and Issaquah, people are 50/50 as to whether Begin/End/OtherThing are the same type, or if they are different. Basically, does the above mean: template&lt;typename B, typename E, typename OT&gt; void DoThing(B Begin, E End, OT OtherThing); OR template&lt;typename T&gt; void DoThing(T Begin, T End, T OtherThing); Frankly, the fact that it is non-intuitive to about 1/2 of people makes it enough for me to nix the feature, and get let the rest of concepts in as acceptable.
I think you're incorrect that CMake doesn't have a conceptual model. I don't know build2 so I can't compare, but I do know CMake. It took a while to click, and from discussions with other CMake users that seems pretty common, but from my viewpoint there is a conceptual model behind the CMake interface. As a result, I didn't find the comparison convincing; you talked about concepts, but you showed syntax, and what you showed of build2 didn't show me the concept you were talking about, whereas I already know the concept behind CMake files. You then described what the bits of syntax in the build2 example were. For each thing you explained I could ask "Is it a function? Is it a macro? A thingy?" if you weren't already answering that.
I know what you mean. There is plenty of "reasons" not to change code. The only two things I encountered to resolve the problem is, to "simply" do the changes (myself) and hoping for the AHA effect. If this does not happen, it is "to expensive" or takes "to much time" to revert, so the better solutions keep creeping into the code. Although I must admit, alone against (sometimes) a whole team, is difficult and not always a success. The other thing is to find the technical strongest people on the team, convince one of them, then lobby. The problem is, I'd rather write software than play games... oh well. 
FWIW, this reflects a very common mentality in a certain segment of the games industry. There are entire conferences (e.g. Handmade Con) dedicated in large part to game developers that think C-with-a-few-upgrades is the superior language. So far as I can tell, it's mostly just a combination of big personalities in the industry who haven't learned anything new since the 1990's combined with the relative difficulty of avoiding some of C++'s unique problems (slow compiles, symbol bloat, etc.) that just aren't a problem in C. When [big celebrity devs say "C++ sucks" right in a CppCon keynote](https://www.youtube.com/watch?v=rX0ItVEVjHc) it's easy to start falling back to upgraded-C as your answer rather than _actually learning_ how to use C++ properly. :) That said, some of the concerns are very real. C++ _does_ compile more slowly and generate more linker overhead and potentially run more slowly than equivalent-ish C (esp. in debug builds, but also if you're not very careful about verifying that the optimizer does what you think it will) unless you put in extra time and effort. This is one of the reasons SG14 was formed. The linked article is throwing the baby out with the bathwater, but C++ could and should do a (much) better job out of the box. Honestly, my biggest gripe with developers like the OP are that they're just asking for crashy security-hole-riddled game servers in an age when literally every game is online. I've had examples of this myself in my job, with the tension between the folks whose primary job is getting major features running within the frame/tick deadline (a millisecond or two for most game systems) and my job in making sure our MMO isn't a crashy security-hole-riddled junkheap. e.g., the tensions between using the engine's standard utils based on boost::format (secure, but slower than the radioactive decay of carbon-14) and sprintf (roughly the fastest option there is, but about as secure as a papier-mache piggybank).
... there is no excuse either to not use CI :p
Well, it is easier to just write `const auto&amp;` instead of relying on CI to find the problem. My excuse is laziness :) Though it is worth mentioning that static analyzers can miss some errors from time to time, especially in templated code.
&gt; Conan doesn't serve you a cmake or a makefile. You still have to write down your own cmake file and then include Conan definitions into it That is absolutely false. Conan has particularly good support for cmake, but has other [intergrations](http://docs.conan.io/en/latest/integrations.html) as well.
I meant more that even by following a protocol, it's still possible to mess up the interface between safe and unsafe code. In Rust, this is particularly important because it takes a very different mindset to write unsafe code compared to safe code. Safe code you can mostly turn your brain off. Unsafe, you need to think of everything that could possibly happen 
The submitter spams lots of junk to /r/cpp
Ah, I see what you mean now. Thank you for explaining.
I see, and that makes sense. In my own code, I try to avoid "big methods." In your example, I would move the `make_circle`'s code to its own private method, even if I only use it once. That way when the backtrace shows up in the log, I will see that the problem is in either `draw_circles_for_some_abstract_objects` or in `make_circle`, as the latter will be given a symbol. In fact, I don't know how or if lambdas show up in the backtrace. It's never come up for me yet. I'll have to set up an experiment and see...
Why bother using bug checkers; writing bug-free code quickly becomes a habit. 
Doesn't this deserve a deeper integration in the language?
Doesn't this happen with most new big features in almost all languages? Inexperienced programmers learn about new x_feature, and they try to use x_feature everywhere, without considering if it is good or not. Sometimes it feels like people are trying to show off that they know how to use the latest features. The irony in this is that by overusing a feature they actually show that they don't *really* know how to properly use that feature. **TL;DR**: Calm down newbie, think it through before you start using the latest feature.
&gt; I would take this interface reduction one step further and make an un-annotated T* implicitly “not null”. Do you want your program to crash? 'Cos this is how you get your program to crash... 
I don’t think it is just inexperienced programmers. I’ll push new language features farther than I might think they should be used in order to see if experience confirms my intuition. Of course...not in all the code I write. But there are always projects where this kind of experimentation is innocuous. Or projects that I create for experimentation.
I'm sure you know the author is a former developer at a renowned MMO game studio. I really hope that this is just his personal opinion and not a general trend in that company/industry. Also, as far as I can tell, SG14 (whose mailing list I read cursorily every now and then) seems to be essentially running around in circles (save for that colony proposal).
Swift and Rust are new languages. They have the luxury to build their norm and culture as well as language design towards that direction. C++ is a bit older than that. We have to mind the existing norm when we build a better norm.
[removed]
Your post has been automatically removed because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/5ucg71/beginner_level_problem_with_square_rooting/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Good one! I'll see your `[](){}();` and raise you: `[&amp;](){;}();`
While the token `&amp;` doubles as an operator, this isn't an operator. It's simply syntax for a reference type.
Going to leave the sidebar here: &gt; For C++ questions, answers, help, and advice see r/cpp_questions or StackOverflow.
I haven't used cppcms, but I have used node.js with C++ backends before. I highly recommend it. The main advantage you gain from node.js is the community support. There are so much software for node.js it becomes easier and easier to add more features. node.js is also web native, which means it easy to integrate with other web services. As the web becomes more feature rich node.js will be one of the first things to pick it up. It's also not too hard to integrate it with C++ code. Since you are going to be pushing data to the clients, you should probably look at websockets. I have also used mongoose and websocketpp in the past. Mongoose works very well, but pay attention to the license. Avoid websocketpp.
Thanks for this resource, I've only just started my journey into parallel programming, I didn't realize the C++ side had extensive libraries for it!
Thanks for the detailed advice! I appreciate it. I haven't actually heard of mongoose until now. Those are some good points for node.js. I think even if cppcms was to out perform node.js on speed, the benefits of node.js outweigh it. Thanks!
You could use QT. There is a database interface for SQLite and others.
You could wrap your source in SWIG wrappers and use any "api frontend" they support. I am particularly partial to node though. Express is just so simple for this. I do recommend serving heavy assets via a static http proxy rather than through any node service. Nginx will happily do this for you 
Awesome thanks, I just checked out the thread support library on cppreference. So far I only have the MASM experience, just starting the higher level stuff this semester.
Another alternative might be [gRPC](http://www.grpc.io/), Google's RPC layer over their Protobuf serialization library. It's a piece of cake to use, and is the first RPC library that just did what I wanted. Depending on what the web stack is on the other end, there are bunches of language implementations, and the native JavaScript/HTTP1.1 compliant version is in alpha/beta right now.
For SQL, I'd consider [sqlpp11](https://github.com/rbock/sqlpp11). 
Have you read what I wrote? It doesn't serve (=generate) a cmake file for you. You still have to write your own cmake/make file and *then* integrate what Conan gives you (a settings file)
No point keeping an incorrect statement here
Why not just throw your data over a websocket? https://github.com/uWebSockets/uWebSockets
What's wrong with websocketpp ?
You have a point. I even kind of agree. But it does not relive us from facing these questions: What should the default be? How to deal with the existing norm when we build new norm? Imagine we now decide to differentiate owning and non-owning raw pointer at language level. How do we do that? 1. Three-type solution: Add an owning raw pointer type and a non-owning raw pointer type, and leave C pointer as it is. it feels redundant. It may also marginalize C pointer. 2. Two-type solution v1: Add an owning raw pointer type and let C pointer be non-owning. But it is just a stronger version of the status quo of GSL. 3. Two-type solution v2: Add a non-owning raw pointer type and let C pointer be owning. But it has been discussed like v1. We can even add more alternatives, like using qualifier/specifier/attribute instead of types. But we will still end up at three-way and two-way solutions. Going for language change does solve some problem. But it does not change the questions we face. It only turns these questions from how to design library to how to change language.
In the past I used CppDB for our save/load system in two UDK-based games successfully using dll-bind. It works with SQLite, PostgreSQL, MariaDB/MySQL, and ODBC. It's insanely fast as a save/load system specially if you use transactions. It works great on top of SQLite on windows. Here is the code: https://github.com/NuLL3rr0r/udk-rpg-save-load-system You can find the build instructions on Windows by looking for *.txt files in the same repo. For PostgreSQL database connectivity I'd use libpqxx as it's twice as fast and has many more features. Although it won't work on Windows as far as I know. I 'm using it for a non-game project at the moment on FreeBSD and Linux. Recently, we are developing a game in UE4 for Steam and we were looking for a header-only C++ SQLite wrapper. CppDB is not an option for us due to our decision to go with header-only libraries as it makes our lives easier by not maintaining a bunch of shared libraries for each platform we target. There are plenty of good header-only wrapper libraries for SQLite out their. At the moment I cannot recommend any of them due to the fact that I just started trying them.
+1 from me. Have used sqlite3 and curl in existing projects. The experience, once you install, and build the packages (both a breeze) is seamless. It's the package manager we've all been missing.
I've used boost::thread on both Android and Desktop (FreeBSD, Linux, Windows) successfully and prefer it over std::thread every other day. The big downside of std::thread is that it won't support interruption points and enabling/disabling them in critical sections as boost::thread do. std::thread were modeled after boost::thread but does not offer the same power. You can checkout the documentation to find the difference in detail.
Windows only but still incredibly useful. It is Windows that has been lacking in this regard. Acquiring and building libraries on Linux for example is never an issue 
[Please forgive me!](http://i.imgur.com/X7bEkuM.png)
Also if they're all the same type, it's shorter to use the normal syntax than the terse syntax anyway.
I like the idea of this paper, and I think it should get some attention from the community.
&gt; Acquiring and building libraries on Linux for example is never an issue Well, it is much better than on Windows, for sure, but there are issues, such as incompatible builds (see this [bug](https://bugs.launchpad.net/bugs/1588330) for a particularly enraging example). The bigger problem, of course, is that you use different package managers across various platforms. From the user's perspective, you have to learn various interfaces/quirks (`dpkg`, `rpm`, `pkg`, now `vcpkg`). From the developer's perspective it is even worse: you have to create/test/support all these different package formats.
&gt; Then I typically want to pass over the range again with a separate algorithm. Always measure. Sometimes two or three passes with simple loops is faster than one pass with branches. Depending of cache pressure, etc. 
Yes this is biggest drawback of vcpkg. But installing libraries globally is really easy using vcpkg. `vcpkg install tbb:x64-windows` `vcpkg install rapidjson:x64-windows` What other package managers offer the same on Mac OS or Linux ? 
After trying qt, i'm exploring wxwidgets, but i'm just getting started. Anyone who tried both wxwidgets and qt wants to share his opinion?
That one's even better! :-) My original one is "stolen" from somewhere as well, I saw it on some slides from one of the committee members, possibly it might have been Herb Sutter.
Well, one possible answer to your question is "Any" - apt, pacman, emerge, ... . Linux has had this for decades. Though all of them come with their own issues, some of them really big issues for C++ devs (e.g. ancient versions in official repos). So I do think vcpkg is great :-)
thanks, fixed.
&gt; vcpkg install rapidjson:x64-windows And what compiler/version is this for? &gt; What other package managers offer the same on Mac OS or Linux ? Well, since you asked, [`build2`](https://build2.org) `bpkg` does this for Windows, Linux, Mac OS, and FreeBSD, currently. The main difference is that it builds everything from source using exact compiler/version/options that you use for your application.
This will build using Visual Studio 2015 compiler per default. So you say that I can do something like this using `build2` `bpkg install tbb` and it will install and build tbb-2017 ? Unfortunately currently even installing `build2` is not that easy. 
Complete C++ noob. I will read through it regardless, every little helps.
&gt; STD::COLONY That would be great. Had to write my own colony-like data structure recently. Having it in standard would be a nice.
I think it should be the goal that the document is beginner friendly. After all, this document should be kind of the founding principles of C++. 
Still nothing there. 
caching issue... ... fixed.
IMHO the best parallel library for C++ at this time is [TBB](https://www.threadingbuildingblocks.org/). Even C++17 [Extensions for Parallelism](http://en.cppreference.com/w/cpp/experimental/parallelism) to do not come close to it.
It's only better when people care to upgrade. I read about people still using Fedora 16 today and wanting to use bleeding edge software...
I'm not very familiar with that proposal, but I did just now briefly skim over it. What's the gripe with those classes?
&gt; Adding 2D Graphics Rendering and Display to C++ Please no &gt; Reflect Through Values Instead of Types That would be awesome.
Swift probably more so than Rust, as noted in my previous comment much like OCaml or Haskell Rust doesn't cater to ubiquitous nullability as it's doesn't build on an existing ubiquitously nullable ecosystem.
&gt;[A Qualified Replacement for `#pragma once`](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0538r0.html) :( I _really_ wish we could just codify the various (imo not-that-common) circumstances that are known to or likely to break `#pragma once`, call those undefined behavior, and keep this proposed `#once`/`#forget` syntax except with an _optional_ identifier. Basically, promise `#pragma once`-like behavior for specific and reasonable cases while providing optional include-guard-like parameters for everything else. This proposal raises the issue that where _"it is common for headers to be copied"_ a _"programmer neglects to adjust the guard"_ but I don't think it does anything to address it. It also seems to suggest quite a bit of manual maintenance of these guards especially w.r.t. identifier namespace accuracy and version number grooming/incrementing/maintenance. AFAIK practically everybody has `#pragma once` available to them, and of course everybody has include guards available, so it would be swell if we could keep both options. 
Looks like reflection is one of the big things coming to the standard if the number of papers on a subject is any indication. I wonder if this will be stalled like concepts or ranges, or if it will make it into C++20
Is... is this like pip for Visual Studio? ...I'm sold.
If you have a websocket connection then they could send those parameters via the websocket instead.
Folks will probably want to know about these, if they haven't seen them before: http://www.cs.nott.ac.uk/~pszgmh/fold.pdf https://accu.org/index.php/journals/2182
&gt; So you say that I can do something like this using build2 If TBB were packaged for `build2`, then pretty much, yes. For example [SQLite is packaged](https://cppget.org/libsqlite3) so you can do (from the VC command prompt): &gt; bpkg create -d sqlite-release cc config.c=cl config.cc.coptions=/O2 created new configuration in sqlite-release/ &gt; cd sqlite-release/ &gt; bpkg add https://pkg.cppget.org/1/alpha added repository cppget.org/alpha &gt; bpkg fetch fetching cppget.org/alpha 13 package(s) in 1 repository(s) &gt; bpkg build libsqlite3 [...compilation...] &gt; bpkg install libsqlite3 config.install.root=C:\projects This is a complete session without any unspoken/magic parts (like where do the packages come from, what options were they built with, or where are they installed). After the `install` command you will have SQLite headers in `C:\projects\include`, import/static libraries in `C:\projects\lib` and the DLL in `C:\projects\bin`.
You never declare/define/assign `this` unlike every other variable in the language. It is not at all the same thing.
Good to know. I'll look into that. Thanks!
 Why would you use this or anything like this given that is platform specific? Doesn't it just make more sense to set up cmake/premake files for popular libraries--so that it works on any platform.. 
Websocket &amp; handling AJAX requests: Websocket is a separate standard and if your browser does that AJAX is not needed. Everything you can tunnel through AJAX you can tunnel through websockets if you design with that in mind. The data can be mostly the same (to be honest due to the way our data is delivered in the background we are currently using JSON as our serialization format). I have seen three operational designs: 1. The application pages (a configurable set of graphs of data from the product defined by the JS) are delivered as static information from the product and the data is then requested using Websockets and rendered on screen. The advantage here is that after initial delivery we only use CPU and network to deliver the data nothing else is going on that would not happen normally. 2. The product only delivers a basic JS library for getting/setting data on a very simple page which loads the actual multi-MB JS heavy UI web application from a separate site (allow us to release UI updates without having to update the products) and communicates with the product using the library we delivered to it. 3. A set of application pages with AJAX/jQuery and so on is put onto the product and a web application coded in C++ is loaded to answer requests. Personally if you can handle the technology and security I would go with 1 or 2. But my perspective is based on working in a very specific area with a specific type of products.
 Also the 2D graphics looks like someones hobby project injected into the standard :(
From their [FAQ](https://github.com/Microsoft/vcpkg/blob/master/docs/FAQ.md#why-not-conan): &gt; Cross-platform vs single-platform. While being hosted on many platforms is an excellent north star, we believe the level of system integration and stability provided by apt-get, yum, and homebrew is well worth needing to exchange apt-get install libboost-all-dev with brew install boost in automated scripts. We chose to make our system as easy as possible to integrate into a world with these very successful system managers -- one more line for vcpkg install boost -- instead of attempting to replace them where they are already so successful and well-loved.
&gt; I expect slower iteration because of data-dependences and more cache-misses This is the exact purpose of `colony`: fast iteration + no reallocations of existing elements. The complexity of erasure is still O(1) since you already know the position of your element, and insertion time is also O(1), so I don't understand the second part. And linked lists generally suck a lot when it comes to game programming (which will be the major user of this data structure): memory usage, cache misses etc.
Well Ubuntu is also slightly better if you're always on the newest version (not LTS), like 16.10 now. But yea, in general, all those repos have old/ancient versions of software - I agree it's a big problem.
According to my understanding and http://www.plflib.org/colony.htm#faq insertion and erasure are O(std::numeric_limits&lt;skipfield_type&gt;::max() - 2) in worst case. And the memory usage of linked list in this case would be the same as the memory usage of skipfield - one index of next element per element.
There's a way to use a private server? I really want to use it on my offline environment :(
&gt; I really wish we could just codify the various (imo not-that-common) circumstances that are known to or likely to break #pragma once c.f. this discussion : https://www.reddit.com/r/cpp/comments/4cjjwe/come_on_guys_put_pragma_once_in_the_standard/
Well yeah I did start that one, I'm not trying to hide which way I prefer doing it :)
Did you mean: "member function"?
Or you can use conan with all it packages on all OSes + all packages in vcpkg
Isn't that what a method is?
may be the worst recommendation here but I think that c++/WinRT is worth a look. I find this *"framework"* interesting because it shows how modern c++ can be used with modern OS API without the ugly Microsoft C++ Extensions (\^). *Adventeges:* Adopted by Microsoft - can't be more Windows than that Designed for modern c++ - can't be more c++ than that No stupid *^ references* - makes my eyes tear as someone who used c++/cli *Disadvantages:* New library - only a **preview** release Exists only to support the WinRT - the universal runtime is more limited than the classic runtime Unknown library - never heard of anyone using this library (told ya that's a bad advice) Here is the cppcon presentation of the library https://youtu.be/lm4IwfiJ3EU Edit: I hate markdowns
Boost is one on the many library that influence the C++ standard library, but by including everything in Boost the C++ standard is a bad thing. Boost include lambdas as a library feature but the standard included lambdas as a language feature. Boost is allowed to easily make mistakes but the standard should not easily make mistakes as it would . I also avoid boost unless there is something that is not in the standard.
I was this this was about improving itterators
Yep, we totally support usage in an offline mode! The only thing we need network access for is to download the source code or tools to build the package you've requested, however we cache _every_ download. So, once you've done a full build with access to the network once, you can rebuild those packages as much as you'd like completely offline. Another often-implied feature of a private server is adding private packages. This is totally supported as well! We get our list of ports from the ports\ directory on disk (no network access), so you're free to add as many as you like and check them in to your private fork. Whenever you'd like to update the "official" packages, you can use git to simply merge the changes to that directory from our GitHub. 
I'm very much not a fan of it, especially as it is based on oldschool canvas rendering APIs (Cairo, specifically) that literally every single major user of such APIs has tossed out and replaced (Chrome, Firefox, most UI libraries, etc. have all replaced or avoided Cairo). The problems with the form being defined by the 2D Graphics group are primarily: 1) Inefficient use of calculated state. If you need to define a path, then switch to another path, then switch back to the first path, the rendering system will need to recalculate path tessellation or use crazy heuristics for a cache. The same is true for brushes and basically all other state. 2) Keeping the state on the context object makes a non-composable interface. If I pass a context off to a helper function to say draw a debug overlay or something, I can't know what context state is left on the object. I either have to save/restore all the state (inefficient) or save/restore the state that the helper modifies (fragile). The helper can save/restore state itself, but the restoration could be pointless if the caller is just going to overwrite that state anyway. As a metaphor, this would be like waiting for Direct3D 12, Vulkan, Metal, and GNM to become standard and then designing a new API that looks like OpenGL 2.
If "most of us also speak EN anyway" you should stay here and not fragment the comunity
We created /r/cppit to help and provide info to programmers who don't speak English as well. Furthermore our channel serves both C++ related posts/news and programming questions for C++ newcomers.
Why are you compiling with sudo?
So... Why do you suppose you don't see these vtables being used in languages like C? Could save a lot of RAM not storing the null fields...
You don't have to spread false information to prove your point. iostream *is* slow, very slow, compared to printf. Does this matter in practice? Usually not. Even though I doubt he mentions allocators for this reason, the stl has issues with custom object alignement sometimes required for less mainstream architectures.
The game engine world...
Weird to see the Rook's Guide on there; it might be open source, but it's full of errors and woefully incomplete.
Your post has been automatically removed because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/5uidi1/can_somebody_help_me_out/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
While I'm all for the good parts being standardised (and improved during the process), not all of Boost is of the same quality in either its design or implementation. Some parts are also quite outdated, and I'd very much like to see them updated or replaced to use C++11/14/17 over time (e.g. MPL and variadic templates). Likewise I'd not be unhappy to see the stuff which was standardised for C++11 or earlier dropped entirely and/or replaced with compatibility typedefs for the standard stuff (example: tuple, cstdint, and memory functionality). It made sense to keep it for a transitional period, but keeping it around forever bloats boost with obsolete junk, and it would be nice to see that curated a bit more aggressively.
&gt; What other package managers offer the same on Mac OS or Linux ? All of them? dpkg and rpm offered significantly more functionality than vcpkg over 20 years ago, with multiple versions of the same libraries, fine grained splitting into development/library/debug/runtime/documentation/xxx subpackages, with fairly complex versioned inter-package relations. And it evolved a *long* way since then. Today, look at the multi-arch support dpkg offers. Or the FreeBSD ports/pkg system. These are all in a whole different league and operate at a scale of well in excess of 10^5 source packages and 10^6 binary packages with additional relation types, hooks, SAT solvers, established packaging and distribution workflows and infrastructure. Windows has nothing comparable; vcpkg is a small step along the way, but it's a far cry from Linux package management even in the mid '90s, and there's a lot of catching up to do for it to even begin to be competitive with these tools (it would be easier to port them).
I'm Italian as well, but I must agree with /u/JuanAG. No point in dividing the community. Beside, you are in a world of pain if you are a programmer and can't speak English...
For a quick command line build that won't allow one to make this error, try `make sourcecode` (will invoke `g++` -- or whatever is set in `CXX` -- if `sourcecode.cpp` is present in your current working directory). You can also adjust optimization &amp; standard settings, as in `CXXFLAGS="-Ofast -march=native -std=c++1z" make sourcecode` or even the libraries, as in `CXXFLAGS="-Ofast -march=native -std=c++1z" LDLIBS="-lwhatever" make sourcecode` Last but not least, don't forget about `Ctrl + R` and `Ctrl + S` (which makes re-typing the above unnecessary): - http://unix.stackexchange.com/questions/73498/how-to-cycle-through-reverse-i-search-in-bash - http://stackoverflow.com/questions/791765/unable-to-forward-search-bash-history-similarly-as-with-ctrl-r/ More: https://ss64.com/bash/syntax-keyboard.html
I want a `#pragma comment(lib, "some_weird_lib.lib")`, that will let me dynamically load DLLs on invocation. Instead of what is currently done where the DLL is loaded when the program starts.
I want a `non_copyable` specifier like `final` or `override`. 
I usually just roll out a makefile. It only takes a few minutes for a single directory project, and typing 'make' is a lot nicer than trying to remember compiler command line flags. I usually want one to build my tests, too.
Semantics?
No support for MinGW? 
We _always_ build debug+release because we are targeted at developers (not end-users) -- debugability is very important to us. We also already support VS2017! Because it's completely ABI compatible with VS2015, we put both compilers under the same triplets (`x86-windows`, for example). In the future, to support incompatible toolsets, we will create additional triplets such as `x86-v1XY-windows`. 
Highly relevant link: https://github.com/Microsoft/cppwinrt
I think there's always a place for local/national communities. There's different cultures, different purposes, ... sometimes it's just nice to be amongst own nationals. Or ask something in your own language, even if you are very proficient in English. People that don't go explore international communities like /r/cpp will not, no matter if there's Italian cpp-reddit or not. A lot of people are just not interested or willing to improve their English, and that's their problem (and annoying to all of us if we have to deal with them). It shouldn't be a reason not to have a local /r/cppit :-)
`= delete` your copy constructor and copy assignment operator?
I prefer `/DELAYLOAD`, but it would be nice if there was a way to do this from a `#pragma`. Then I could control the DLLs my code needs directly from the `#includes` instead of through the build system. Its the same motivation that gave us `#pragma comment(lib`
Do we know what the abstraction cost of ASIO is? For instance, for something like a highly optimized web server written directly on top of epoll, how much of a loss would it be to move to ASIO? Are there any benchmarks of such a thing?
Haha, if only we could. I knew someone must've thought of that solution well before I did :)
Not obsolete. Often the boost counterparts offer additional functionality not available in std. For example, boost::shared_ptr has defines one can set to enable extra debugging features to track leaked references, etc. I also like using the boost versions because I can count on the implementation being roughly the same across platforms. That cost for this is usually compile times because the boost versions need to carry a lot of machinery to support all of the different compilers. 
Similar to `boost::noncopyable` I would guess?
Well now I'm skeptical of the entire list and don't know what I should be paying atttention to :(
&gt; should still be true, so I'm just clarifying that I think they should make it standard defined, but limit the definition to what implementations currently and can reasonably/easily support. &gt; The problem is that there are still people contributing to ISO C++ who work on esoteric systems (e.g. IBM mainframes) that almost certainly violate your idea of what "the same file" means. :) There's also plenty of folks using crazy NFS mount setups and the like where `#pragma once` just doesn't work but include guards do, hence the unique identifiers in the proposal. No, the committee cannot just say that users in odd cases are on their own. The problem is that these users might want to share code or use libraries that want to rely on standard features (like any `#pragma once` replacement), but if the community starts writing libraries that only compile on Windows and *nix-like systems, these users are boned. Any code that is written to the ISO standard is expected to correctly compile and execute on _every_ system with a compliant implementation. Pretty much every single instance of "implementation-defined" is a wart on the language, and they are tolerated only because they are sometimes _required_ to make C++ useful (e.g., to properly support hardware in actual use - which yes, includes a lot of exotic architectures and not just x86 or ARM). This isn't hypothetical. These folks come out of the woodwork in droves whenever features like `#pragma once` are proposed for standardization. The version presented is the accepted works-for-everyone approach that won't result in standard-compliant cross-platform code that only builds correctly on mainstream OSes. Now, I write code that just uses and relies on `#pragma once` anyway because I'm a jerk and I honestly couldn't care less about exotic platforms because I write the kinds of things that nobody ever needs to run on a mainframe or microcontroller, but I also write gobs and gobs of platform-specific code in just about every library and app I develop. If you're in a similar boat, you already have `#pragma once` and there's no reason for you to ask the committee to standardize it in the first place. :)
Guessing by titles alone, the "preview" image got a good starting point IMO, if you look at it right-to-left: 1. "Introduction to Design Patterns" to see a bigger picture, 2. "Open Data Structures" to get a look on details, 3. "Boost Libraries" to learn when not to reinvent the wheel. As 0. "for the beginners" I'd look into "How to Think Like a Computer Scientist" - it seems small enough to not get bored, while covering enough topics to kickstart you into writing something useful.
&gt; You don't have to spread false information to prove your point I'm not saying iostream is faster than stdio, I'm pointing that optimizing that area of your code is simply stupid, IOs are slow super slow and must be minimized, just try to make IPC using stdout with printf/cout, good luck with more than 100k messages per second. &gt; the stl has issues with custom object alignement specifier alignas + custom aligned allocator, will fix 99% of these problems.
Unfortunately, I don't think the mongo-cxx-package is set up correctly or perhaps I'm misusing it? I've filed an issue here: https://github.com/Microsoft/vcpkg/issues/681 Now, that being aside, the more I use the tool, the more I like it. It makes command-line building C++ in Windows easy and the way everything magically links is nothing short of black magic.
English is my third language, I do not live in an english speaking country and I agree. Sure, you can find content in other languages, but depending on tbe language, order/order**s** of magnitude less and less up to date. Producing software that only works well for English speaking population, however, is inexcusable :-). i18n, motherfucker! :-)
You'd have a point if r/cppit actually picked up. But I find this very unlikely. Mods will have to work hard.
To everyone except pedants, yes. :) ("method" isn't a term in the standard, but everyone knows what you mean by it)
I'm talking stl in general, to allocate something different than pod in an aligned way within std vector/sequential container is a pain in the ass but that doesn't mean is not possible.
The thing for people to remember is that most hash maps are linear in the worst case (when they grow) and yet we still consider them constant time for many practical purposes because those worst cases are amortized away due to their rarity. Colony is similar. It's worst case is linear to the size of the skipfield, but bigger skipfields also reduce the likelihood of hitting the worst case. As with many real-world uses of algorithms, getting the best performance requires some tuning, and may even need a home-rolling version for application-specific tuning.
Thanks, I am glad you like it so far. &gt; It's hard because nobody working on existing software wants to covert their thing from cmake, autoconf, ... to another system. Our "master plan" here is to provide such kick-ass features that people just won't be able to resist. Like fast (both parallel and distributed) and uniform builds across all the platforms and compilers, proper test infrastructure, CI. For example, one of our next goals is to set up build servers for all the major platforms/compilers for cppget.org so if you upload a package, you get all the testing done for free (as in both resources and effort).
I'd actually be curious to know what grammatical reasons exist that we couldn't make `.` a synonym for `::` in name resolution contexts. Not that it would happen - the standard is resistant to change without strong motivation - but it's curious to me that we need `.`, `::`, and `-&gt;` while so many languages with very similar grammar can get away with just `.` for all contexts. I'm sure there's some grammatical baggage that makes it difficult or impossible (I can imagine that user-provided `operator-&gt;` in smart pointers is one of the big ones).
Serves you right for not using a proper build system and not committing your changes often. Actually, if this sourcefile was recently edited, why haven't you had it still open in the editor/IDE? And why the hell sudo?
&gt; In fact, my initial example should have read: &gt; process(false, true); &gt; But I confused the order of the arguments. Did you notice that? ...notice that you flipped the arguments while never saying what you intended the function to be doing? How were we supposed to notice that
is such problem addressed somehow in the Coding Guideline? or other places? a direct support from the language for such toggles would be quite nice.
I like the idea of using enums instead of bools in this case, but personally I would rather not see implicit casts to bool at all. Instead, one could just compare against the enum values that are intended. This also means that if you later decide you actually have three different cases instead of two for one of the arguments, you don't have to go and change every single call site.
I find this question really legitimate, especially as a soup guy...
This is an API to toss out a quick graphics application, to do a graphical Hello World, with just the standard library. Something like the old school libraries that used to come with compilers like BGI. It isn't an API to write bad ass AAA 2D games with the standard library.
Not in the standard, but I did implement [named arguments](http://jamboree.github.io/designator-draft.html) in Clang. I guess you will have at least designated initializers in C++20.
I certainly don't disagree that there are specific cases where the Boost implementation differs sufficiently from the standard implementation that it would be worth keeping around, but as a general point I'd to see them replaced by the standard implementation where that is possible. Some, like type_traits aren't fully compatible; I converted all my code at work to use the std traits a few weeks back. But others are completely replaceable.
Plain old C has got you covered, here. struct param {_Bool a, b}; void process(struct param); #define process(...) process((struct param){__VAR_ARGS__}); param(.b = true, .a = false); Even works with default parameters. This is one feature I really miss in C++.
That is also the approach I am using in our codebase. But I'm interested to learn about the pros and cons of this approach versus the casts to bool too...
"We were not supposed to notice that" is exactly the problem.
I would like to know the reasons why this feature hasn't been included into C++, honestly, just to learn, there must be strong reasons, any guru? When I have to make things within linux kernel modules this is one of the techniques you find everywhere.
It is unusable because it does not even support versioning of dependencies. There is always only a latest version of everything which will break your builds and make building your software from yesterday impossible (e.g. for a git bisect).
Works everywhere. Using an HTML Table for a public listing of documents isn't that bad as an option IMHO. Also the ISO has its own set of rules.
Thanks for reporting that issue, I think we've resolved it now.
Sorry, when I say support I mean _fully_ -- packages will be built with VS2017 when it is available (to provide the latest optimizations), and those packages are all safely (ABI) consumable from both VS2015 and VS2017.
The problem is that there are - to my knowledge - very few good examples of how to use the newest C++11 thread api, OpenMP, TBB, PLL, ... in the context of an (G)UI application with visual progress feedback. Can you e.g. show me a similar example as the sort routines app demoed in the [video](https://www.youtube.com/watch?v=qpGi8HSE35k) available in the [post link](https://community.embarcadero.com/blogs/entry/c-multi-threading-for-mobile-and-desktop-apps?utm_source=Reddit_Organic&amp;utm_medium=social)? The threading code should be cross platform whereas the UI code could be e.g. a traditional Windows desktop app.
The function isn't varargs; only the macro is.
There are no function varargs used in that code. It only uses variadic macros, which have no impact on type safety.
What's the problem with format flags à la ```std::setiosflags```?
This uses variadic macros, which are the source code substitution and not the runtime kind. You get a compile error if the arguments don't match. 
That's definitely annoying, but in this case I would place the blame with the developers rather than the tools. I remember helping out with the previous `c102` ABI transition in Debian and that was seamless. While libstdc++ does implement both the old and the new C++11 ABI, it does require a proper distribution wide ABI transition to make this foolproof.
If you're asking why c++ doesn't have variadic macros, I *think* someone here said a while back that c++ is moving away from making macros "better", discouraging their use in new code. I have no Idea about the omission of striuct initialization with named fields. 
A properly defined abstract binary interface.
You can say the same for IO, or networking. Also the commercial C and C++ compilers of yore didn't had any trouble shipping such 2D libraries with them.
It's not true of IO or networking, which are governed by POSIX or the BSD sockets api. This really only works on commercial platforms where the GUI is part of the OS.
My point is, we used to say that all these incompatible builds are Windows problems and we on Linux can link against anything and therefore use binary package managers. After this dual ABI fiasco we are now just like Windows and there is no going back.
That's not strictly true, IMHO. The dual ABI is useful for running old code on newer systems, but only when you have a dependency upon libstdc++ and ship your own libraries. It's useful for keeping proprietary binary builds functioning. When you have transitive dependencies on other libraries depending upon libstdc++ then you can't avoid a full transition to the new ABI for all dependencies. So I think the "dual ABI" is a red herring; from a distribution point of view, it's an ABI bump and needs the whole world rebuilding against the new ABI. The package manager can take care of this *if* you do a proper transition for each package.
Why would anyone down vote? CppCast is awesome...
My point was that any given OS has one way of doing IO, and one way of doing networking (though with bluetooth, for instance, this isn't strictly true). On the other hand, there can be many ways to do graphics. If there were a graphics api in the standard library, I would have to install a different version of libstdc++ or libc++ depending on what GUI I am using (Plasma on X11, Plasma on Wayland, Gnome on Wayland, Gnome on X11). Even worse, I have to link *anything* written in C++ against this standard library, even if it doesn't use the standard library graphics API. Alternatively, the graphics portion could be split into a separate library (i.e. a separate `.so` file), in which case it's now just one more C++ graphics library, and there's no real point in putting it in the standard.
I'm developing on a commandline interface where it's a very light OS does not have any GUI. So I'm stuck with nano.
thanks /u/mttd. I use makefiles for all other projects but this one in particular only needed a small modification. Hence I just used a single line g++ command
C++ has variadic macros.
That's naturally implementation defined because it's constrained by the platform.
&gt; Then we should remove all the other IO and network libraries from the standard as well, not all target platforms have support for them. Honestly I never understand what is the drive to get IO, network, filesystem in "the" standard library. Couldn't these be considered optional extensions ? The standard library should only be about pure computations.
&gt; any given OS has one way of doing IO, uh... no
&gt; My point was that any given OS has one way of doing IO, and one way of doing networking (though with bluetooth, for instance, this isn't strictly true). On the other hand, there can be many ways to do graphics. This is not true, unless you restrict yourself to UNIX and Windows variants, ignoring the remaining market of operating systems and bare metal deployments. &gt; If there were a graphics api in the standard library, I would have to install a different version of libstdc++ or libc++ depending on what GUI I am using It already happens, depending if one wants debug symbols or not, compiled with multi-thread support or not, 32bit or 64 bit, ..... It is just yet another combination. &gt; Alternatively, the graphics portion could be split into a separate library (i.e. a separate .so file), in which case it's now just one more C++ graphics library, and there's no real point in putting it in the standard. Sure there is, the point is exactly that, that it should be available in any implementation with some sort of 2D device available. How each compiler supports that, it is up to them. Also this is nothing special, C++ is way behind other languages in 2D support on its standard library. Which is quite sad, given that in the mid-90's all C++ compilers had such libraries.
Ugh, especially in these languages, the "default" network stack is sub-par at best and you are almost always better served with external libs for networking. It's only good for "quick'n'dirty" results.
*whom (Sorry.)
Little-known fact: you don't need a Makefile to use make. If you're just compiling a simple thing like that, just run this at the command line: $ make sourcecode CXXFLAGS+=-std=c++11 Less typing than your manual version and less error-prone. And setting up a Makefile for this is very simple: CXXFLAGS += -std=c++11 sourcecode: Yup, that's all it needs to be. I do this all the time for one-file projects.
Sorry, I must have mis-remembered. Thanks. 
How do you distinguish building against the MSVC debug RT from building against the MSVC release RT?
My native language is Korean, and I guarantee you, Korean is absolutely useless in computer programming, unless you're making some kind of i18n software.
Very insightful. Thanks.
If it is a third party library, by definition there is zero guarantee that it will be available everywhere a C++ compiler exists. It was that mentality with the standard library being just libc plus a few extras, that made mainstream developers migrate to programming languages with richer and more portable standard libraries. So nowadays many of us only use C++ to write libraries to plug into those languages, instead of full applications like in the 90's. I appreciate the work the ANSI C++ guys are doing regarding having a proper standard library for C++, as it should already have been part of C++98. Now just relax and wait for C++20.
That probably came across more critical than I meant it. In the end it makes sense because agreeing-on/building something more advanced/strictly defined might be more trouble than it's worth. But it's hard not to notice the double irony involved in software industry standardization committee using weakly structured data, and the reason being standardization on something better being difficult.
`{.withValidation = true, .withNewEngine = false}` is not valid C++. And not even C99. In C99 you can only use designated initializer during copy initialization, or you must specify the struct name: // C99: struct EngineConfiguration config = {.withValidation = true, .withNewEngine = false}; config = (struct EngineConfiguration){.withValidation = true, .withNewEngine = false}
You always need a build system but you can see that this become simplified when you can use `#pragama`s (like Boost) to select linkage based on includes. 
It is however valid objective-c++ because that's a goofy frankenlanguage.
&gt; A macro for every function name seems like a bad idea, unless we get a preprocessor that recognizes namespaces and classes first. The macro is just syntactic sugar. Totally works without.
But in my build system, there's one single action that is "linking" against a library, which will bring all the requirements automatically like include paths, dependencies and actually library to use. That's actually much better than hardcoding the name of a file in a header. Which one do you need: libfoo.so? libfoo.a? foo.lib? foo.dylib?
Well, even the languages themselves are in english. Every name in the standard library, every single keyword, is composed of English words (or shortenings thereof) separated by underscores. I can't even imagine how much it would suck to hear names like accumulate, lower_bound, partition, and not immediately have a good idea of what they do.
Euh... how so?!
There have been proposals. There will be more. It's harder in C++ than in C. C didn't have inheritance, doesn't have private members, doesn't have class members, doesn't have user-defined conversions via implicit constructors, etc. There's just a lot more little fiddly cases to get right in C++. There's also the question of whether it should be generalized to function calls, since it already (possibly) has to support constructors anyway.
**Company:** [ETAS GmbH](http://www.etas.com) **Type:** full time **Description:** We are looking for experienced C++ developers who have fun working in an international scrum team in the automotive area. More details can be found here: http://www.stepstone.de/stellenangebote--Software-Entwickler-in-C-Stuttgart-ETAS-GmbH--4169974-inline.html?suid=f8c70d0e-38f9-41d3-8c3f-67ea7085995d&amp;rltr=3_3_25_dynrl_m **Location:** Stuttgart Feuerbach, Germany **Remote:** No **Visa Sponsorship:** No **Technologies:** C++11, C++14, C++17 on Linux/Windows, experience with STL, Boost, Visual Studio 2015, Git **Contact:** Use link on our [job advertisement](http://www.stepstone.de/stellenangebote--Software-Entwickler-in-C-Stuttgart-ETAS-GmbH--4169974-inline.html?suid=f8c70d0e-38f9-41d3-8c3f-67ea7085995d&amp;rltr=3_3_25_dynrl_m) to contact us (reference code DE00510040)
This is off-topic for r/cpp.
Me too, but I use tmux or ctrl+z or multiple terminals, and there's no excuse for not using build system anyway.
I think the `maybe_unused` attribute is the wrong tool for this - attributes are for annotating functions (and possibly classes), and not to specify in your program that a return value might be unused. You mention `std::ignore`, which in my opinion is the right tool for this job, as it is used in exactly the same use-case when a function returns a tuple and you use `std::tie(val1, std::ignore) = func()`. But you don't mention what happened to it... is it not in the standard along the structured bindings proposal? Why not?
I've recommended the C++ Annotations to a few new guys who know how to code but don't know C++. It seems to have worked out well enough.
I'd be very interested if anyone has insight on solving this problem. I've taken to using "tagged types" for most functions where there are multiple parameters of the same type (not just bools), so I encounter it often.
What does the standard have to say about your build environment/filesystem though? What does it say (and I honestly don't know) about the path and file in an `#include` directive? Just that the "environment" should be able to use it to locate a file? I think this would be similar, just that the identifier-free form would be applicable to headers that your environment/compiler can uniquely identify, along with some examples of things that aren't explicitly supported. There are probably all kinds of ridiculous things you could do to your environment that would result in being unable to compile perfectly standards-compliant code. &gt; standard-compliant code would naturally get written without said identifier, and would cease to compile in otherwise standard-compliant environments Library writers would almost certainly use identifiers, just like they use include guards now. The code that would "naturally" get written without said identifier would exclusively be code that doesn't need an identifier (taking liberties with the word "exclusively" here, e.g. I'm assuming that if I published broken code on github nobody would want/need to use it and it would kind-of take care of itself).
std::ignore is not in the [proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0144r2.pdf) Section 3.8 &gt; &gt; Symmetry with std::tie would suggest using something like a std::ignore: tuple&lt;T1,T2,T3&gt; f(); auto [x, std::ignore, z] = f(); // NOT proposed: ignore second element &gt; However, this feels awkward. &gt; Anticipating pattern matching in the language could suggest a wildcard like _ or *, but since we do not yet have &gt; pattern matching it is premature to pick a syntax that we know will be compatible. This is a pure extension that &gt; can wait to be considered with pattern matching.
&gt; It's quite an omission not having such facility now at all with structured bindings. Destructuring != pattern matching. This is a job for the latter. &amp;nbsp; &gt; Is there even a proposal or some form of consensus about it yet? [Pattern Matching and Language Variants](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0095r1.html)
There's a good chance of a limited form of designated initializers getting into C++20. I think they've been accepted by EWG or something. With this and aggregate initialization: struct param {bool a, b;}; void process(param); param({.a = false, .b = true}); Source: https://botondballo.wordpress.com/2016/07/06/trip-report-c-standards-meeting-in-oulu-june-2016/
my intuition tells me that a function with four different flavors should instead be four different functions. call the one you need, and don't pay for the branches
So, I have to install clang_complete with the PR, nvim-completion-manager and neo-vim to achieve what is shown in the example videos?
Yes. clang_complete itself has default mapping that triggers the completion popup. If the default behavior feeds your need, there's no need for neovim and nvim-completion-manager. 
There're screencasts show [the difference between default behavior and the enhancement](https://github.com/Rip-Rip/clang_complete/pull/515#issuecomment-280556053).
Ah thank you very much for explaining! I now understand what the purpose of this is.
A pointer to who?
&gt;The current version of Ubuntu (16.10) has version 1.3.6 from 2009, i.e. 8 years ago! That's like saying Ubuntu is the only Linux distro, Arch Linux ships with [POCO 1.7.7](https://www.archlinux.org/packages/community-testing/x86_64/poco/), which was updated yesterday. If you're going to do development pick the right distro for it, Arch Linux and Gentoo are a great fit.
And coroutines do not exclude asio. Coroutines are about call stack becoming call tree with detachable branches as fitures. Asio is about not blocking for io.
Yes, but... Chances are that process(), validate(), those enums etc. belong in a namespace (or a class perhaps) anyhow. Conversion considerations are different for this use case. The whole purpose of the exercise is a type and a name for a flag. The article itself implements that conversion to a bool, and code clarity for those conditionals is fine (see my example).
How does it look like without the macro? 
 param((struct param){.b = true, .a = false}); The macro would also allow one to create default parameters.
The problem - identifiers in #defines need to be maintained. The solution - use a well known macro without identifiers and add to it identifiers that need to be maintained. Programmers around the world - "Wtf?" 
From experience, g++ is definitely not one of those commands.
You guys totally did! That's amazing lol.
The need to link against a library is due to missing function bodies. Taking this view, it would make sense that there is a `#pragma` around the function declaration. With #pragmas you only link against things you use. Its true that sometimes you need your program to be flexible and link against different extensions, I think this is a deficit of the existing system but doesn't have anything to do with the motivation. Which was the subject of my initial comment.
You can almost get there with a lambda :) process(param.b = true; param.a = false;); http://cpp.sh/546i
I used to use that make feature quite often, until I started using the -std=c++11 flag. Adding CXXFLAGS=... is way more typing. Is it possible to add some default flags to g++? 
It's also one of the rules advocated by the Qt coding standards that I found very sensible. Wrapper class is cool, not sure if worth the effort though.
&gt; From the point of a user just using structured bindings, I don't really care. You _should_ care, because you should want it done _right_, not half-assed or rushed. ;-]
I fully agree! I just have my reservations with respect to this particular thing.
Funny, that's basically how my game's code (which use a custom engine gluing different libraries) is setup.
$ :vomit: why cant you just use a normal keyworld? It is not like reflection is used once every 5 lines so it needs to be short.
You need to link because of missing function bodies in the headers of that library. To get the headers in your path, you need to tweak the buildsystem already. If that tweak brings you both headers and link the right library and deps automatically, then there is no overhead in the buildsystem or need for that pragma.
transform_if would be awesome, if it existed.
You should try vcpkg to install boost, i thing doing that it will work
Also there is problem that distributions can not be instancioned with char, signed char and unsigned char types.
Why not? That seems like a rather arbitrary restriction.
Why does this matter at all? Just pick int. You know the static_cast is safe since you set the range.
I'd suggesting reading the proposal and discussions leading up to it, e.g. on the isocpp reflector list. You're asking a lot of questions that have been answered many times already by people more qualified than myself. :) &gt; What does it say (and I honestly don't know) about the path and file in an #include directive? It doesn't say much, and that's the point. The compiler makes no rules that `"foo/lib.h"` and `"bar/lib.h"` are different files, but it does have lots to say about redeclarations; if those do map to the same file (which is super legal, and not that uncommon even on non-weird systems) then they must have protections like include guards to avoid errors. `#pragma once` requires that the compiler know whether the files are the same or not, which is not something the C++ standard is able to specify, since the C++ standard _does not_ require any rules about how paths map to files. &gt; Library writers would almost certainly use identifiers, just like they use include guards now. That is a huge leap of faith, for one. For two, why even bother standardizing `#pragma once` if a large percentage of C++ users aren't allowed to use it? It doesn't actually _gain anything_ by being standardized, since it'd essentially be an optional feature or have a very wide implementation-defined caveat. If a library is written such that it uses only features of the standard, it must work with every standard compliant compiler. That's the _entire point_ of there even being a standard in the first place. :)
Check this Stack Overflow Question http://stackoverflow.com/questions/41464356/build-boost-with-msvc-14-1-vs2017-rc
 For a few reasons, I can't link all 20 possible hardware devices in one exe. I control the hardware devices my code uses by changing a `#define`. Yes, I need to feed the folder to the build system but what actually gets build is now controlled by the `#includes` I'm actually using, which is nice.
I think Boost.CMake was forbidden because key boost developers hate cmake. Like people who support a football team and then hate the other team. To not look like zealots they usually explain that they HATE THE SYNTAX.
CMake is the tool for me. Boost build is not.
IIRC, it wasn't forbidden, it was just more trouble than anyone wanted to deal with. Boost Build and CMake are very different systems, some things are not trivial to port, and Boost has a _huge_ set of target platforms.
No actually, that's not a requirement, and shouldn't be. Many of my users use the cmake-gui, which they just double-click from their desktop. Asking them to open a developer prompt (even finding that shortcut...) and start cmake-gui from there is two steps to many, and unnecessary complicated: Users expect to double-click on the icon and then have it working.
I'll admit they don't seem to care about RC versions of MSVC, but they usually get really good support for the released ones shortly after they release. Edit: So, I found this: https://github.com/boostorg/build/issues/157 and apparently Microsoft has changed how you find VC.
Haha! I couldn't have said it in a better way. They're really late to the party, realising Boost doesn't work with VS2017!
Oh... (but I suppose that depends on the platform)
Because it's surprising, inconvenient, and inhibits generic code. The statement made by uniform_int_distribution is basically "we allow all integral types... oh, except for char and unsigned char". And for no particularly good reason, either. It's just arbitrary since, as you say, it *can* be implemented with int and a static_cast. But being able to manually implement it yourself doesn't mean it isn't inconvenient. Something that should be as simple as this: template &lt;typename Rng&gt; uint8_t rand_byte(Rng&amp;&amp; rng) { std::uniform_int_distribution&lt;uint8_t&gt; dist; return dist(rng); } Now has to become this: template &lt;typename Rng&gt; uint8_t rand_byte(Rng&amp;&amp; rng) { std::uniform_int_distribution&lt;int&gt; dist(0, 255); return static_cast&lt;uint8_t&gt;(dist(rng)); } It's annoying enough when the type is known, but is worse when trying to write something generic. What should be as simple as this: template &lt;typename Int, typename Rng&gt; Int rand_int(Rng&amp;&amp; rng) { std::uniform_int_distribution&lt;Int&gt; dist; return dist(rng); } Ends up turning into something equivalent to the following: template &lt;typename Int, typename Rng&gt; Int rand_int(Rng&amp;&amp; rng, std::enable_if_t&lt;!std::is_same_v&lt;Int, char&gt; &amp;&amp; !std::is_same_v&lt;Int, signed char&gt; &amp;&amp; !std::is_same_v&lt;Int, unsigned char&gt;&gt;** = 0) { std::uniform_int_distribution&lt;Int&gt; dist; return dist(rng); } template &lt;typename Int, typename Rng&gt; Int rand_int(Rng&amp;&amp; rng, std::enable_if_t&lt;std::is_same_v&lt;Int, char&gt; || std::is_same_v&lt;Int, signed char&gt; || std::is_same_v&lt;Int, unsigned char&gt;&gt;** = 0) { std::uniform_int_distribution&lt;int&gt; dist(std::numeric_limits&lt;Int&gt;::min(), std::numeric_limits&lt;Int&gt;::max()); return static_cast&lt;Int&gt;(dist(rng)); } And for what?
But then you (should) get compiler warnings.
Shameless plug. I've been working on this for a while: https://github.com/Orphis/boost-cmake You can build latest Boost and most modules with CMake. As long as CMake supports your toolchain (or you have a proper toolchain file), it should work. It's meant to be embedded in your CMake build scripts and will only make static libraries for now, using in the threaded variant. The reason is to seamlessly work on multitudes of platforms and compiler options without interfacing tons of various precompiled binaries (and deal with packaging and distribution of them all). Tested on VS2015, recent Xcode versions for macOS or iOS (universal builds), Clang for Android, GCC and Clang on Linux. I plan on checking for VS2017 support when the RTM is out, possibly fixing any CMake upstream issues there are to detect the compiler. Feel free to ask any question if you have any!
You just need to invert the dependency. A meta target "libfoo" that changes for each platform, its build scripts should handle that. Then in your app build script you link against it. It's simple and no overhead or special feature required in the compiler that way. And it's easy to maintain.
Just took a look, and it looks pretty awesome! Modern CMake usage with a simple and intuitive layout is a huge bonus. I attempted a similar project a while ago, but I never got it really working. The interdependencies between components was a chore to figure out, so I eventually gave up. Have you explored the idea of install steps and packages? I think getting a reliable and configurable CMake-based Boost build would be wonderful. Unfortunately, if what /u/Reallifeniceguy says is correct, it probably wouldn't get any further than a community project. Perhaps that could change if it can be shown that a CMake-based build doesn't have to be the horror show that it was back in the bad-old CMake 2.6 days.
Thanks! I believe that install() is the root of evil most of the time and in the context of modern cross-platform development and instrumentation like Clang sanitizers, relying on it is not going to work well. Sure, I could make it work for regular builds, but really, proper embedding is the way to go. Here's why: Boost was used in my previous company, prebuilt with some version of the compiler, some STL. Complex build scripts involved to get it working for all the platforms... Then we wanted to use Address Sanitizer. It worked fine most of the time, but we had some mysterious issues. I tracked them down to Boost Test and some other libs not compiled with ASan enabled. I tried to ship ASan binaries for some of the platforms and compilers and it greatly complicated the pipelines, the build scripts... Then, I had a look at what it took to actually build the libraries we were using and it was quite simple. So I just built them with CMake instead in a similar way to what I published now. All the false positives went away and the source download was done just once for all the platforms (instead of tons of various binaries), was way smaller, and took like 14s to build. I couldn't find any downside compared to the traditional approach. Because of that, I'm quite a fervent believer in large scale coherent build systems with requirements properly propagated and you can't get that with old fashioned builds installed on the system. In that scenario, the location or name on disk of the library doesn't really matter, it's known to the build system (CMake) and then propagated automatically. Edit: I forgot to talk about the modern CMake part! Yes, I believe that modern CMake is good and you can have easy scripts to work with. Unfortunately, because of various Linux distributions, projects conservatively keep compatibility with 2.6 or "2.8" (whatever that means, it's such a wide range of versions and features...). Some people asked me to write a guide to modern CMake development, I might do that someday, but I'm quite busy with other projects right now (cleaning up a big project CMake scripts to make it modern, and used later as a reference for other projects maybe). As for Boost, I simplified the build system to only include compiled libraries, so I can focus on those dependencies and ignore the header only ones. I added some tests for the compiled ones only to check I compiled them properly or didn't miss a file. It's not meant to be a full replacement, but I wouldn't mind if people tried to improve on it! I don't think it's a problem if it stays a separate project for now and in a few versions, if people like it and use it, we can try to do more.
I've also wanted to write about modern CMake usage, but haven't gotten around to it. There's a lot of outdated (or just plain bad) information floating about on sites like StackOverflow. Have you looked much into using `install()` with export targets? You can use that to do some incredible magic. For example: At my workspace we build every component (including third party libs) in multiple build configurations (Debug/Release/etc.), and `install()` each configuration, using a different path/filename depending on configuration (similar to Boost's tagged builds). When CMake uses `find_package()` on our libraries, it will set the `IMPORTED_LOCATION_&lt;CONFIG&gt;` for each configuration that we build, so we can quickly switch between using Debug/Release/etc. versions of a library, no need to know the filenames or include paths. I've recently been working on an `InstrumentedDebug` build type that uses Sanitizers and would be installed and used in a similar fashion. Because of installing all the different build types and CMake automatically handling the `IMPORTED_LOCATION_&lt;CONFIG&gt;` properties, setting your local project to use the `InstrumentedDebug` build type would immediately switch all the dependencies to use their corresponding libraries. It's a fairly robust system and has served us well for a few years, and gets better as CMake versions add new features. Old CMake code is the bane of my existence, and it's not at all hard to update older systems with newer CMake versions. I think the primary reason people use old versions is simply lack of awareness of how much better the new versions are, which translates into a lack of desire to update.
These guys are cool to listen to. I'm glad there's so many other episodes to catch up with
Yes, I have, but I'm not convinced it's still a great idea. Recently, I changed how LLVM was used in a project and switched to their exported file. Well, they didn't export the include directories on the targets, so you have to manually include it instead of relying on the imported targets... I want to look into that sometime and fix it, but I still have 24 hours per day, and that's clearly not enough :) In your use case, you have to go through an intermediate step to export and then import the libraries. In a big "workspace", everything is just there and you can use it. I guess you have a separate build folder for each library then when I have a super-build, so I don't run into this issue. It won't allow for mixing Debug and Release libraries in the same project, but that is a non common use case anyway... One problem with super-builds is that you can't just always build "all". In some CI pipelines, you may want to start naming the targets you want to build directly (or a meta target pointing at them). Otherwise, you may do a lot of unnecessary work. And yes, older systems are annoying, just like old build scripts. I just rewrite them in simple modern CMake that fits my embedded use in my project. Sometimes, I can contribute the changes upstream, sometimes it's just lost cause when I see they target 2.6 and will never ever consider bumping the requirements... It usually involves converting include_directories() to target_include_directories() and then add_definitions() to target_compile_definitions() or target_compile_options() or adding some FOLDER properties on the targets so that everything is well organized.
Summoning /u/stl... I really vaguely recall at one point you might have mentioned fixing this in the standard somehow. Any idea what the status of that might be? Heck, I'd actually be happy with *de facto* standard here even if it isn't de jure standard. Maybe you could just go ahead and fix it in MSVC, and convince Clang and GCC to do the same? After all, since `uniform_int_distribution&lt;char&gt;` is undefined behavior one of the valid outcomes is "it works, and does exactly what you think it does". :)
I tried, filing an LWG issue. It was rejected, although I disagreed. Maybe we should relent on the strict checks here.
This is 100% dependent on what format the data is in. Is it formatted or binary? Is it indexable?
Yes, but... C++/CLI beats writing p/invoke for stupid C shit and stupidly writing C wrappers around C++ APIs. Same for C++/CX. C++/CLI also writes IDisposable for you, that's a big PITA otherwise. So it's mixing two languages into one, so what?! (They say it's extensions; perhaps, for CX, but fir CLI, it's really another language).
As I've [said before](https://www.reddit.com/r/programming/comments/39wftk/the_trouble_with_bools/cs76qs3/) it's easy picking on `bool` but the same problem can happen for any type. **String** LookupEmployee("First", "Surname", "Middle"); **Int** SetDimensions(height, width, depth); // or is it Width, Height, Depth? 
Thank-you so much for this! This is exactly what I was really hoping would happen - to hear the pros and cons from someone with experience using it :) I can't thank you enough.
When are improvements for &lt;regex&gt; coming along?
I am keeping "scraping the religious war off my boot" for later :-).
No it doesn't. There's no reason `rand_byte` can't return `int`. And there's no reason `rand_int` needs to do anything special cased for chars. Doing any kind of calculation with chars or shorts is perilous and bug prone due to the Usual Arithmetic Conversions and Integral Promotions that automatically make things be ints whether you want them to be or not. If I only had a nickel for every bug I've fixed caused by people doing math with unsigned chars and expecting 255 wrapping resulting in negative array indices and other such shenanigans..... And anyway I don't understand why one would want this kind of genericity. Random number generation is a specific thing applicable to a small number of types. IMO this is genericity for genericity's sake rather than something broadly applicable. Finally your second example is way way more complex than you need it to be: template&lt;typename Int, typename Rng&gt; Int rand_int(Rng&amp; rng) { using DistInt = conditional_t&lt;sizeof(Int) &lt; sizeof(int), int, Int&gt;; std::uniform_int_distribution&lt;DistInt&gt; dist(std::numeric_limits&lt;Int&gt;::min(), std::numeric_limits&lt;Int&gt;::max()); return static_cast&lt;Int&gt;(dist(rng)); } I wouldn't have argued against making these work for chars in the standard, but I also would tell people to never do it even if it did work due to UACs/promotions.
I don't think so. Stroustrup's reaction is also telling. Of course it was a bit rude of Ramey to chime in again and speak out his comment, but Stroustrup shut him down in quite a belittling way. In any case, in my opinion, Stroustrup's answer to Ramey's question was very nice, fair, acknowledging and encouraging, I don't know what more Ramey would have liked to hear. And the few small shots that Stroustrup took at Boost in his answer, Ramey had it coming, by asking that question. In my opinion Stroustrup handled and answered this very well. (except for the very end, which was rude) (_edited to correct the name_)
The only random-related feature that made its way into C++17 is std::sample, used to draw a given number of elements from a sequence. The simple random number generation facilities (std::randint, std::reseed, + std::shuffle and std::sample with an implicit RNG) made their way into the Library Fundamentals TS v2 and are such likely to be incorporated into C++Next, but aren't planned for C++17. Among the tiny changes, UniformRandomNumberGenerator was changed to UniformRandomBitGenerator to highlight the fact that the generators are meant to generate random bits and not specifically numbers. There is also the removal of the old &amp; unsafe std::random_shuffle. tl;dr: nothing much in C++17 save for std::sample, but new features will probably be included in C++Next.
Pretty much the same approach a lot of companies have from what I understand! I did the same too at mine last year. I wonder why not so many people published their build scripts before to have a proper up to date CMake build system for Boost. I did publish mine recently and I would really appreciate if you could have a look and tell me if you could use it for your project. Not necessarily actually using it, just telling me if it would work or why it wouldn't! See: https://github.com/Orphis/boost-cmake 
Thank you ! The link is great. Solves 50 percents of problems ) &gt; Get everything: https://wg21.link/index.txt and with this it will be much simpler to build some app with searching possibilities .. Thank you again
&gt;questions that have been answered many times already I cynically believe that the real reason this gets shot down is because it started as a MS thing, and that explains why I've never been impressed by any of the answers to those questions :) &gt;It doesn't say much, and that's the point. It doesn't have to say much about this either. &gt; not something the C++ standard is able to specify ... _does not_ require any rules about how paths map to files We have `&lt;fileysystem&gt;` now and we have its `bool equivalent(const std::filesystem::path&amp; p1, const std::filesystem::path&amp; p2);` so it could be defined in terms of that or very similarly. &gt; That is a huge leap of faith I don't think it is. If library writers want their libraries used they'd absolutely do this. &gt; why even bother ... if a large percentage of C++ users aren't allowed to use it I actually think a large percentage of C++ users _would_ be allowed to use it, or at least _should_ be allowed to use it. &gt; It doesn't actually gain anything by being standardized, since it'd essentially be an optional feature or have a very wide implementation-defined caveat. The _vast majority_ of compilers support it already, and its use is very common, so I think it'd gain a lot by being standardized. It would also package its functionality and the identical functionality of include guards (in the identifier version) together in the same feature. &gt; If a library is written such that it uses only features of the standard, it must work with every standard compliant compiler. That's the entire point of there even being a standard in the first place. :) No argument there, but consider also that there are certainly _already_ things (dumb things, sure) you _could_ do to your environment that would cause you not to be able to compile standard-compliant code. I don't think the fact that some people (who somehow never show up to these arguments, even though everybody is sure they exist and there are a lot of them) are currently relying on some of those environments should prevent this from being standardized, _especially_ considering there is built-in support for an option (using an identifier) that would keep working for them.
http://www.specref.org/ has some search support for WG21 papers. It was added for https://tabatkins.github.io/bikeshed/, which some folks use to write papers (bikeshed automatically links to other WG21 reference papers). specref uses wg21.link's list of papers :-) It would be cool to get search capabilities directly onto wg21.link. Contact is: Bugs and feature requests: Send an e-mail to: Maurice Bos &lt;m-ou.se@m-ou.se&gt; Or contact me on IRC: M-ou-se on Freenode
Probably not the optimal nor fully recommended solution, but a vs2015 build of boost does work when used in vs2017 projects. If anything it complains it's an unknown compiler and points out that you should run the tests 
In this paper Walter E. Brown proposes to create a new TS group to analyze all random-related proposals. http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0438r0.pdf
Clion projects are driven by cmake. To add libraries you need to find how to do it in cmake - https://cmake.org/cmake/help/v3.2/command/target_link_libraries.html
I would make a more precise statement: undefined behavior is unsafe when it has unexpected negative effects on your program's operating environment. Following a wild pointer? Bad. Using std::rand? Probably intended and desirable. A corollary to your example is when I once had a tool that was occasionally crashing in our compiler tests. The error dialog was stopping the test runs. My "fix" was to capture all exceptions at the top level, discard them, and log the fact that it crashed. It was still undefined behavior, but it wasn't unsafe as I was happy to let this tool error once in a while--the OS process model protected what I cared about. "The responsibility for avoiding these errors is delegated to a higher level of abstraction."
https://www.github.com/OSSIA/API grep -nPR '\[.*\].*{' yields https://paste.ofcode.org/A4ffzJ3YJ5ja8WTjKNabK https://www.github.com/OSSIA/i-score cd base/plugins ; grep -nPR '\[.*\].*{' yields https://paste.ofcode.org/hkBgYPmmDGrKwcELtctSAa 
Out of curiosity and VS2017 aside. Why did you have to rewrite the building of some boost libraries in CMake? What I do and did successfully for years was doing a (in my case complete) rebuild of new Boost versions with bjam/b2 and then tell VS or CMake where to find these libraries. But the process of creating a set of binaries usable by CMake is not more than a single command that can be more or less directly copied from their "Getting started" docs. b2 just worked as advertised for this scenario. What was the pain point that lead you (or others) to rewrite the initial build of a new library in CMake, finding all the dependencies of that library and all that stuff, that is already done by Boost developers in Boost.Build? (Of course, from that point on using CMake is a lot easier than Boost.Build, but the investment to use CMake even for the initial Boost library build seems quite high to me. That's the reason I'm asking.)
I've had the same experience with YCM and used quite long clang_complete with vim until I found [rtags](https://github.com/Andersbakken/rtags) which works asynchronly.
Do we really need a meta meta build system? Please `build2`, save us from this bleak future!
maybe we should start investing in deep-learning webscale buildsystems 
Actually, if `std::uint8_t` exists, it is guaranteed to be either `unsigned char` or `char` (if that is unsigned). The reason is that `unsigned char` is required to have a maximum value of at least 255 and the next largest unsigned type, `unsigned short`, is required to have a maximum value of at least 65535 (so it can't be that).
I wonder who started the rumor physics grads can easily get jobs in software. Or in finance. Anyways, from what I understood, you have been getting phone interviews, right? That's great. Perhaps ask /r/cscareerquestions for interview tips.
&gt; auto_ptr wasn’t sufficient for all conceivable smart pointer scenarios, obviously. That's an understatement. Today's programmers may not realize that `vector&lt;auto_ptr&lt;T&gt;&gt;` was forbidden crashtrocity. &gt; I encourage you to check Loki’s approach. Which was intentionally avoided by Boost (and the Standard). I consider the insight to be deep: when everyone has a standard smart pointer of their own, no one does. That is, policies that customize behavior *and infect the type* lead to incompatibilities. If a library insists on using flavor X of a smart pointer, and you're using flavor Y, then you can't really communicate. `shared_ptr` avoids this trap by saying "you'll get Z behavior, and you'll like it". Significant customization is possible, but always in a manner that doesn't affect the type (e.g. custom deleters, make_shared, aliasing, etc.). &gt; we would naturally want std::shared_ptr&lt;Base&gt; to work with std::shared_ptr&lt;Derived&gt;. Surprisingly (?), the following code works fine: That is because the designers and implementers of shared_ptr have gone to extreme lengths to craft implicit conversions that simulate the behavior of raw pointers. This is almost perfect, limited only by the Core Language's power. An example where the simulation breaks down: when overloading on raw pointers, `meow(Base *)` and `meow(Derived *)`, calling `meow()` with `MoreDerived *` will select `meow(Derived *)` because that's a "shorter distance" conversion up the hierarchy. With `shared_ptr`, this would be ambiguous - we can only enable or disable conversions, we can't make them more/less preferred. However, overloading `purr(shared_ptr&lt;Vehicle&gt;)` and `purr(shared_ptr&lt;Animal&gt;)` and calling it with a `shared_ptr&lt;SaturnV&gt;` or `shared_ptr&lt;Kitty&gt;` is unambiguous, just like with a raw pointer, thanks to the extensive constraints powered by SFINAE. &gt; Luckily, there are specializations of std::atomic... functions that deal with std::shared_ptr. They are tricky to use correctly (because you can't call any ordinary operations simultaneously). My main suggestions for this article are that shared_ptr is easier to understand when you have an idea of its representation, and that make_shared and weak_ptr are important topics.
Wrap 'em all! https://ideone.com/rCXCA9
As a beginner this was a real issue, and made for a steep learning cliff, especially when I didn't understand how long software takes to write. Today on one of my projects, I have a lot of binary dependencies: libtiff, boost, ccgal, glew, OpenCV, CUDA, Qt + a bunch of OEM specific libraries. But, frankly, maintaining them hasn't really been too much of a burden. In my case I got something that works and doesn't give me trouble. No reason to switch. The only real deployment difficulty I've encountered is that CUDA code needs to be compiled for each specific generation, which needs to be updated annually. In fact, I've had a lot more problems dealing with ecosystems, in particular python, where deep dependence hierarchies and frequent changes to libraries make for difficult to reproduce environments (think frozen). In C++ you get a linker error, in Python you got a runtime error!
Hah. The OpenCL host API is a little rough. Although on the other hand, I found out recently that CUDA kernel code doesn't support any kind of simple native vector operations like x * y, whats up with that man? 
We would want to say that converting `shared_ptr&lt;D&gt;` to `shared_ptr&lt;B&gt;` has the same goodness as converting `D *` to `B *`.
Probably because its not an AVX style batch processing unit. The GPU architecture is closer to a barrelled processor where execution switches when a resources becomes unavailable.
Their support team is rather patronising ;) The reason it is off our table at the moment is due to the inability to point it to the repo or folder that owns the package. Having to publish a potentially buggy lib to the package repo is just a huge PITA. Use case we needed to make work - Have internal source libraries. Need to work on that and the app that consumes it. Wanted something similar to cargo.io where you could say "this lives at this repo/folder, pull it and build it". 
In Spain, my country, this has been happening for decades but not only with physics, mathematicians, other engineers (computer science here is an engineer) or even architects, anyone can program, you only need their special course of 3 weeks in wich you will become the new guru of Java/C# As you can imagine the quality is very bad, also the pay and most of the days you will have to work 3 or 4 extra hours, my beloved country has no interest in i+d so all the people with a degree usually program in this companys, i dont know how you call in english, here are called "carnicas" because every 18 months more than 90% of employees has gone so they need more flesh to start again the process, Everis, Indra, Accenture or Deloitte are some of the names And the ones who doesnt like programming will aply at a bank or doing hft stuff, you only need a master (doesnt care of what, only the paper saiying you had one) and you can work at banco santander or similars So, at least in Europe is not nothing new, i think it is because here the degrees are much cheaper so anybody can have one making that the market is satured so anyone can work on anything, in fact you need a Degree to work at McDonalds, yes, we have serious problems with that and probably what it is happening here is also happening there
Harvard is better than von Neumann, we use it because it is more cheaper, if we dont von Neumann will be another scientific more doing theorical papers By the way, von Neumann help to create the first "computer", today will not happen so having a physic making low level programing is a waste and the company loose money compared to a CS guy that knows and will make efficent and cheaper code to maintain and develop from day 1 and not slow down the team as a not programmer will do
You realize your compiler vendor can have integer types other than the ones the standard requires?
Here is a project that uses lots of lambdas for parallelism (LAMBDA and OMEGA_H_LAMBDA, all capture lists are [=]) https://github.com/ibaned/omega_h Its test suite uses exact comparison to previous results, so I'd be interested how resistant it is to mutation. Which tool do you use to parse and modify source ?
I'm curious about your statement about affecting the type. Isn't the custom deleter part of the type?
`shared_ptr&lt;T&gt; sp1(new T{args});` and `shared_ptr&lt;T&gt; sp2(new T{args}, CustomDel{stuff});` have the same type.
&gt; In last several months there were some fierce debates As well as the several months prior to that.. and years.. and decades. &gt; Exceptions on the other hand are what they are - exceptions Consider something less circular, such as "Exceptions report a failure to guarantee a postcondition" (cppreference has a brief exposition of that approach [here](http://en.cppreference.com/w/cpp/language/exceptions#Usage) , core guidelines have a [bit more](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#Re-throw))
We at least fixed the "multiline" bug (via LWG issue). MSVC++ and Boost say multiline is on, libstdc++ and libc++ say multiline is off. We changed the standard to require libstdc++ and libc++'s behavior and added syntax_option_type::multiline to request MSVC++'s current behavior. I still tell people to use Boost.Regex over our implementation if they can; one of these days I want to base our implementation off Chakra's engine.
I reached out to their support and I was told it was not supported. Then proceeded to tell me that " C/C++ is not Node, nor NPM. Projects are not pet projects, or startup projects, but big corporations that would very rarely rely on external sources for their builds." Was not very interested in digging further after that :) Do you have a link to what you mean by a build policy? I can't find mention of it in their docs. 
If you don't handle them, your program dies.
You should try [cget](https://github.com/pfultz2/cget) which will pull down standard cmake packages from github or other urls. It can also work with other build systems like boost or header-only.
The "reaction" was misleading. There were multiple stations for asking questions in the room. After answering my question (extensively) he turned to answer another while I was making my followup comment. Not a big deal - just a little confusing.
We do have both as it is, don't we? I hate the "exceptions are for exceptional situations" formulation, because one person exceptional is another's regular occurrence. I **very** heavily lean on the side of exceptions. To me, they are a code clarity tool. They accommodate what I ~~believe~~ see with my own eyes happens for a **vast** majority of all errors: clean up and get out. I further question how error-return can be used to report errors in a higher-level code, because the variety of failures of the possible underlying stacks seem to grow exponentially with the stack depth. I am yet to see a compelling argument that they hurt code size or performance in a relevant manner. In fact, I have, for code size, the opposite experience. I acknowledge that the speed (more: response latency in error case) can suffer, but believe that is only needed for very a small portion of all code, and can be accommodated by switching to error codes after the profiling has shown the need to do so.
If only everyone internally was using cmake... or some sort of standard build tool. The thing that originally drew me to conan was the ability to drop a quick conanfile.py file into run their existing commands with very little extra overhead. In an ideal world i could go from published package to in dev package by simply changing from a published version to a branch on github. ala crates in rust http://doc.crates.io/specifying-dependencies.html 
Ensuring they are handled can't be more trivial: have a try/catch in main and any thread functions.
My experience comes from working with APIs like OpenGL and OpenCL, which are (natively) written in C, and therefore cannot rely on Exceptions to deal with errors. It did not take me long—at all—to migrate over to C++ wrappers around those APIs that did provide Exceptions for those APIs. And some of those wrappers I wrote myself. I'm not going to litigate the general discussion of "exceptions vs error codes" as some kind of objective truth, since I know there are lots of scenarios where having a guarantee that no exception will be thrown is a major requirement for the software being written (or sometimes just the hardware's ability to have exception handling code written for it). But what I do find pretty consistently is that pretty much every time I work with an API that uses error codes, I end up writing a lot of code that looks like the following: void handle_error(int code, std::string function_name) { std::cerr &lt;&lt; "Something went wrong with '" &lt;&lt; function_name &lt;&lt; "': " &lt;&lt; translate_code_to_string(code) &lt;&lt; std::endl; } bool do_stuff() { int ret; ret = apiInit(); if(!ret) { handle_error(ret, "apiInit"); return false; } ret = apiConfig(API_CONFIG_SETTING_1, API_USE_VPN); if(!ret) { handle_error(ret, "apiConfig"); return false; } ret = apiCoverMyTracks(API_MASK_SELF_AS_NEIGHBOR, API_TRUE); if(!ret) { handle_error(ret, "apiCoverMyTracks"); return false; } ret = apiHackThePentagon(API_LAUNCH_ALL_MISSILES, API_TRUE); if(!ret) { handle_error(ret, "apiHackThePentagon"); return false; } return true; } When I'd much rather be writing code that looks like this: bool do_stuff() { try { api::Init(); api::Config(API_CONFIG_SETTING_1, API_USE_VPN); api::CoverMyTracks(API_MASK_SELF_AS_NEIGHBOR, API_TRUE); api::HackThePentagon(API_LAUNCH_ALL_MISSILES, API_TRUE); return true; } catch (api::Exception const&amp; e) { std::cerr &lt;&lt; "Something went wrong with '" &lt;&lt; e.get_function_name() &lt;&lt; "': " &lt;&lt; e.get_error() &lt;&lt; std::endl; return false; } } And I don't think I've ever encountered a situation where, given the opportunity to write the latter code, I've thought to myself "boy, it sure would be great if I were instead writing the former code!" Again, not saying there's no situation where error codes don't make sense, but as a general programming practice, I don't see why I'd *prefer* it, or why I'd go for a design that sought to use both.
Mostly only reasons why I personally never use exceptions or RTTI is because they inflate the ~~code~~ executable size by a big amount in my use case.
&gt; That's an understatement. Today's programmers may not realize that vector&lt;auto_ptr&lt;T&gt;&gt; was forbidden crashtrocity. &gt; Genuine question, how does something that seems so broken like this make it into the standard? C++ seems like good ideas 99% of the time, and 1% total wtf
I have friends who took physics that know basically 0 programming who've gotten jobs doing programming Hell I know compsci students that *cant* program that easily get internships
you can paint a house with a spray gun or a paint brush.
Maybe I'm missing something, but in the first case, couldn't you write something like if( !apiInit() &amp;&amp; !apiConfig(API_CONFIG_SETTING_1, API_USE_VPN) &amp;&amp; ! apiCoverMyTracks(API_MASK_SELF_AS_NEIGHBOR, API_TRUE) &amp;&amp; !apiHackThePentagon(API_LAUNCH_ALL_MISSILES, API_TRUE) ) { std::cerr &lt;&lt; "Something went wrong with " &lt;&lt; std::endl; } granted you lose your "e.get_function_name()" If you could overload based on return type this could potentially be kinda useful (a case where you care about an error occurring and a case where you don't)
You are getting interviews, so what is your opinion as to why you are failing them? I don't buy "lack of experience": That much would be clear from your CV and nobody wants to waste the time and resources interviewing someone they know is not going to get hired. It sounds to me like that is more like code for "you got busted embellishing your CV for if you had the experience you are claiming to have, you would not struggle with these questions." If you had problems getting interviews, then the issue would be in how you present yourself. The way you write your post does make you seem a bit aimless, applying dor anything and everything: studied physics, but want to work in non-physics software, want to work with C++, but recent projects were done in Python. I hope what you want is clear in your mind, and the more precise you can make it, the better. Are you applying for permanent or (paid) internship positions? The latter might be an easier way in.
This is not true. Conan is a pure package manager that just uses existing build systems, it is not another build system. It has a very good integration with CMake: http://docs.conan.io/en/latest/integrations/cmake.html and packaging libraries that use CMake is simple. It supports other build systems too, as qmake, premake, SCons...
We all are
I feel like "handled" should mean more than "caught".
i agree that Java does exceptions better. how are we supposed to know that a C++ function throws an exception other than the developer leaving a comment that says "hey btw this throws a exception and if you don't catch it your entire program will crash". it just seems like such a weird oversight in a language that is otherwise strongly typed. 
I worked with Java and by extend with checked exceptions and I can safely say that, while idea is nice, the implementation fails. Mostly because there is no way to describe generic method for N exceptions where N is dependent on method body. Without that, it hurts each time you use lambdas (and Java 8 Streams). It also does not really helps in general case because when you introduce new type of checked exception and throw it in method, you have to add it to each method declaration on the callstack all the way to the handling side. Last time I checked, checked exceptions were generally considered bad idea. I suppose checked exceptions can work, but only if your language designed around them, including metaprogramming facilities. I want code to force me to handle errors but I don't want errors to flood my method declarations.
Exceptions would be great if we didn't live in a world of C APIs.
I started to use conan for a benchmark suite that compares a bunch of libraries roughly in the same niche. In essence, this is an open source project relying on the public conan.io repository for dependencies. It's important to understand that conan, the tool is not the same as conan.io, the repository. (it's a bit like git to github). As a user of the public repos, my biggest pet peeve is the inability to see if a package is good or not without trying it out. The only indicator might be the download numbers, but that's obviously inaccurate. The website needs more distinguished uploaders, popularity scores on packages and possibly CI badges like on Github and maybe even comments. Also, some uploaders create packages for one platform only, which is usually not directly visible. These little improvements would save a lot of time. The bright side is that it's rather easy to create a package (if said package has a sane build process). It's even possible to build the tooling dependencies, e.g. I have a package that uses flex&amp;bison, which are also conan packages used during the build. I feel like conan is lately gaining a good traction and I'm very optimistic about its future. 
Yeah, it definitely doesn't have to be an either/or. I'm writing a library which internally throws exceptions on fatal errors, catches them in the interface layer, and converts them into error codes. All the resources I was using get released as the stack is unwound, and the client code doesn't get crashed, even if the client doesn't try to catch exceptions.
You can get a job with 0 programming skills doing programming if you go somewhere where they're willing to teach you
&gt; or some sort of standard build tool. C++ has a vast range of platforms. There are no standard platforms. Common platforms, sure, but no standard platform. No standard platform - no standard build tool.
I've never believed converting Boost to use CMake to be a worthwhile thing to attempt. Instead, I think there would be more benefit for everyone if current boost build would generate cmake config package files (like Qt does via qmake scripts) : http://lists.boost.org/boost-build/2015/11/28384.php Boost doesn't want to depend on CMake, and that should be fine. 
&gt; Exceptions report a failure to guarantee a postcondition That kind of overlaps with error codes though.
Exceptions are exceptional.
I usually have some top level catch. If the application is able to recover from the throw, display some type of error to the user and try to continue.
I think they meant that if the condition isn't met, the error should either be caught and handled or the program should die. In the case of error codes, it's very easy to just ignore an error code, and then the program may continue into unknown land and delete an entire database.
Boost doesn't need to have a hard dependency on CMake if they don't want to. But they could definitely provide build scripts for CMake easily to allow a perfect integration of Boost in various projects, just like I showed in the project I've linked a few times already here. The point is to be able to rebuild Boost easily with the rest of your application forwarding the right compiler, flags, instrumentation options, target options... Or to use universal builds on iOS easily. Or not having to learn yet another build system that is poorly documented and uses an unusual syntax when you need to target a different unusual platform. Building for the host platform is usually easy, crosscompiling currently isn't and there's no reason it should be like that.
It should then be referenced as the simplest way and put first in the documentation. People will always forget to use all the variables to integrate libraries properly.
 &gt; [rumors of Autotools support getting better on Windows] Sorry, but this won't happen. Autotools is fundamentally and irrevocably broken on Windows. No amount of lipstick can improve that pig. The only real solution is to transition off it.
Cross-compiling with Boost.Build involves adding a _single line_ to your user-config.jam or project-config.jam &amp;ndash; hardly arduous. ;-]
Oh yeah, can't imagine working on Windows either.
I don't normally get into the political correctness of computer stuff. Nobody is supporting oppression by having a master and slave replica, for instance. And I'm okay with white list, and black list. I mention this to establish I'm not overzealous. But "gyp" is literally an ethnic slur. It's no better than naming a project "kike".
[removed]
I would ask the gcc dev mailing list, see where they suggest you start. 
Aaah! Thank you so much! Now it all makes sense! :-)
At whatever level can continue its job (or properly back out). As \u\BillyONeal said, "If you don't handle them, your program dies". Knowing they are handled can be tricky, but the same can be said about result codes. And yes, I'd say "Handled" is more than just catch and ignore (most of the time)
&gt; This plus possibility of non-RAII code is a recipe for disaster. In new projects, you simply avoid writing non-RAII code. I would not have said that ten years ago but by now I am completely confident that with a decent style guideline, your average C++ professional can write large projects and never write non-RAII code (and not strain themselves one bit in doing so). &gt; because (unlike in Java) compilers don't have to verify declarations and enforce handling of exceptions. That is not a feature of Java - it's an anti-feature. :-D Indeed, there's an unchecked exception whose sole purpose is to package checked exceptions as unchecked, which is a tacit admissions that it didn't really work out.
No I mean the support for using msys within conan to call autotools. That's the only sane way to handle this right now. I wish some libs we use would transition from autotools but it's not likely something like ffmpeg will do that. 
But that doesn't mean all tools are the same. Using the wrong tool will take you a lot longer and get inferior results. (Use rollers to paint your house! It's a lot faster than a brush, and a lot less hassle than a spray gun... :-D )
based on [this](https://www.reddit.com/r/cpp/comments/5klmni/comparing_executable_size_of_c_exceptions_vs/) post, using exceptions results in smaller binaries than plain C with error codes. Interestingly also [this answer on stack overflow](http://stackoverflow.com/a/4334421/1691145) says &gt; A quick experiment (using GCC 4.4.3 on Ubuntu 10.04 64-bit) shows that -fno-rtti actually increases the binary size of a simple test program by a few hundred bytes. This happens consistently across combinations of -g and -O3 I assume you measured. Why do you think your use case gives different results than the first link above?
While I understand that ``unique_ptr`` by default uses ``std::default_delete&lt;T&gt;`` to delete the element, and ``shared_ptr`` captures the type (as mentioned in the article), the difference between the behaviour in the following code is very inconvenient. struct Base { ~Base() { std::cout &lt;&lt; "Base::dtor" &lt;&lt; '\n'; } }; struct Derived : Base { ~Derived() { std::cout &lt;&lt; "Derived::dtor" &lt;&lt; '\n'; } }; int main() { // This only calls Base dtor // This is undefined behaviour // http://www.gotw.ca/publications/mill18.htm std::unique_ptr&lt;Base&gt; up (new Derived()); // This calls dtors correctly (Derived followed by Base) // This works correctly std::shared_ptr&lt;Base&gt; sp (new Derived()); } I had another question: How does shared_ptr captures the ``Derived`` type in the constructor such that it is able to call the destructor correctly. I am guessing ctor number 2 from here is used above http://en.cppreference.com/w/cpp/memory/shared_ptr/shared_ptr. But how is ``Y`` known at the time of deletion? PS: Answer to the second question: http://stackoverflow.com/questions/3899790/shared-ptr-magic PPS: Is there a reason why ``shared_ptr`` and ``unique_ptr`` do not have same semantics?
Why do you even need a user-config.jam? You should just point at a compiler. Most of the user-config.jam I found on the web were *NOT* correct or not up to date anyway. So saying it's not harduous doesn't seem to be right.
Unless the error is reproducible ~~(and it isn't for me)~~, any bug report would be immediately closed anyway.
&gt; So - in your first example one operation can fail, but you can continue the execution, while in second you just saying "okay, that failed, time to wrap things up". This can depend on the specific API you're using, but in my experience, there are very few API errors I've encountered where the appropriate response was "Okay, Method A didn't work, so we'll try Method B instead!" This might be because the APIs I use are interfacing with GPUs, so an error usually means "this GPU doesn't have the functionality you need for this program; you are SOL." But even when I'm in a situation where I'm expecting to have one or more levels of fail-safes, I'd still rather write something like std::vector&lt;std::function&lt;void(void)&gt;&gt; init_functions { []{api::Init();}, []{api::Init(API_COMPATIBILITY_MODE_ENABLE);} } for(auto const&amp; init_func : init_functions) { try { init_func(); break; } catch (api::Exception const&amp; e) { std::cerr &lt;&lt; "'" &lt;&lt; e.get_function_name() &lt;&lt; "' didn't work: " &lt;&lt; e.get_error() &lt;&lt; std::endl; } }
I don't do microcontrollers. From my understanding, you can use GCC and C++ to compile programs for them. This means optimizations &amp; other improvements in GCC apply to a very wide array of microcontrollers. And c++ is still useful in microcontrollers because in C++ you have 0 space-time cost abstractions. There is a C++ talk talking about this with some microcontroller examples. I don't remember what it was titled. I think it was in 2015 or 2016 talks.
&gt; If only everyone internally was using [...] some sort of standard build tool. Just for reference, there was a recent debate on this issue here: [Where are the build tools?](https://www.reddit.com/r/cpp/comments/5tvhvg/where_are_the_build_tools/).
The bug report you've submitted seems to be missing any repro/info except `I get an incorrect integer result for some code, in release builds only.`
You may need to click 'Details' to expand this. And I added the repeat case as a couple of attachments, which don't seem to be visible yet. (It says something about it being possible for this to take some time.) So yeah, doesn't seem to be the greatest bug tracking interface in the world. Otherwise, you could also refer to the details I posted above. ;)
They're not as rare as I used to think a few years ago. I've encountered at least 3 ICE's in the last two years which were compiler bugs and then fixed. However I do agree that OP's code is not too obscure, so it is a bit unlikely to be a compiler bug - but it may well be.
I would recommend installing this KB: https://support.microsoft.com/en-us/help/3207317/visual-c-optimizer-fixes-for-visual-studio-2015-update-3 Though given dodheim's response, I wouldn't get my hopes up too much. You can also see if this is a bug in the ssa optimizer. https://blogs.msdn.microsoft.com/vcblog/2016/05/04/new-code-optimizer/ Try passing -d2SSAOptimizer- and see if it still reproduces.
Sorry for bad surname :-(
&gt; I am trying to figure out when to use shared pointer and auto pointer If you mean auto_ptr, never. Never ever. &gt; Is that just sugar over using the asterisk pointer No. You should definitely, definitely read up on them and any time you think you want a raw pointer, you almost certainly should be using a unique_ptr or shared_ptr.
Just in case it isn't clear to you or anyone reading your comment, the example you gave works fine when declaring `Base::~Base` as `virtual`. This, of course, comes with additional overhead, so it's not exactly an ideal solution. &gt; Is there a reason why shared_ptr and unique_ptr do not have same semantics? Overhead. unique_ptr has the exact same size of a naked pointer, which is a desirable property to make it a drop-in replacement for raw pointers (apart from the ownership semantics, of course). Adding a member for a deleter would increase the size. Calling a type-erased deleter at destruction would also incur some performance penalty due to the indirect function call. For shared_ptr, there's already memory overhead due to the reference counting block plus performance penalty for incrementing / decrementing the reference count. So the standards committee thought "We might as well throw a runtime deleter in there". It wouldn't be terribly hard to write a smart pointer with the ownership semantics of unique_ptr, but using a deleter like shared_ptr. For one reason or the other, it's just not part of the standard library.
Thanks Nimbal. So the reason for ``unique_ptr`` not doing the type erasure trick makes sense. (We don't want to pay any penalty for using ``unique_ptr`` over raw pointer. This is C++ afterall). The reasoning for doing this for ``share_ptr`` is probably not that sound. &gt; So the standards committee thought "We might as well throw a runtime deleter in there". First it creates differences like the one highlighted above. Secondly, if I just want to use ``shared_ptr`` for multiple owner semantics, I am paying the cost of virtual destructor now. (Not saying how large/small that is, it goes against convincing someone to use ``shared_ptr``) My 2 cents. 
OK, I don't see attachments yet indeed. Btw, try updating your VS to the latest version https://msdn.microsoft.com/ru-ru/library/mt752379.aspx and applying this hotfix: https://support.microsoft.com/en-us/help/3207317/visual-c-optimizer-fixes-for-visual-studio-2015-update-3 Maybe it'll help.
You are incoherent. 
Daily builds of the VC++ 2017 compiler still have the issue, so I'd be surprised if the hotfix helps.
&gt; Why do you think your use case gives different results than the first link above? Because it's a real world use case. Didn't use RTTI or exceptions before. Turned them off and executable size decreased dramatically - multiple megabytes, IIRC.
Go watch some CppCon videos on YouTube, Going Native 2012, 2013, CppCon 2014, 2015, 2016. Start with the keynotes, then talks with most views. There will be so many eye-opening gems for you there. :-) It's the best material to learn what C++ is nowadays.
References can be null or become invalid, if the variable they hold to changes or goes out of scope, like a temporary for example. Having said this they are way safer than raw pointers, specially for out parameters. 
This is a textbook example of a person who does not know how to work exceptions (in C++ or Java). In Java, one still has to write exception-safe code (that's what is really meant by "RAII" here) regardless of what the compiler says, because of unchecked exceptions. In C++, by and large, one does **not** need to know what exception might be thrown from where. Rather, one knows that everything, except an **exceedingly** small number of well-knows operations[1], throws, and codes accordingly. For that, the tools are plenty (RAII, scope guard, smart pointers etc). [1] primitive type assignments, non-throwing swap, C APIs, some last-ditch logging function.
Scott Meyers does a great job addressing these questions in [Effective Modern C++](https://www.google.com/search?q=effective+modern+c%2B%2B&amp;oq=effective+modern+c%2B%2B) (and his associated videos on youtube).
Thank you for the detailed explanation. 
New code should be using [Boost.Signals2](http://www.boost.org/libs/signals2/) rather than Boost.Signals.
Why than VS set environment variable with name %VS150COMNTOOLS% (when you run vcvarsall.bat)? Shouldn't it be %VS141COMNTOOLS%?
I put this to you: you turned on rtti/exceptions without changing a line of code, and compiled code became bigger. Yes? If yes: this is a pointless exercise. To see the actual difference, you would need to rewrite your code to be functionally equivalent, but use exceptions. You did not do that, did you? Edit: just saw [that I guessed right](https://www.reddit.com/r/cpp/comments/5v22e3/exceptions_and_error_codes_cant_we_have_both/ddzd86i/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=cpp)
There are long-winded ways to explain the latest smart pointers in the standard so I go with over-simplified view instead. std::unique_ptr can only have one owner. The idea is mostly that the dynamically allocated type has same semantics as values; when the uptr goes out of scope the object is deleted. This is convenient way to make your code exception proof; if someone throws all the object deletions are handled for you (look up RAII). uptr is ultra light-weight, just a simple wrapper for pointer that does call delete for you. std::shared_ptr is super-duper expensive abstraction for reference counting. It does not just keep reference count but the mechanism for keeping the count is thread safe. This makes it potentially VERY expensive. The reference counting object lives in the heap so it is separately allocated from the shallow wrappers to the object. This overhead is so significant that the standard invented std::make_shared which allocates the object and the book-keeping object simultaneously. If possible always use std::make_shared and std::make_unique! Look them up! (make_unique is c++14, so keep that in mind). 
What do you mean by one owner? Can I take a unique pointer and pass it in through a constructor to a child object, with the understanding that the child object can access it's members, but not destroy it?
C++ is arguably the best language for targetting uC's. There are many good reasons to use c++ and many reasons why it is still less common than C.
There's books on compilers in general and probably also on GCC in particular, but I'm not really expert enough to know which ones are really good. One that I have used that might suit your needs is Introduction to Compiler Design by Mogensen. It doesn't go too far into the rabbit hole when it comes to code optimizations and such, but does give you a pretty good general idea of what makes a compiler tick. Like I said though, I am not a compiler expert and also have not read any other books on it, so probably there are better sources out there.
Hmm, maybe it was std::bind that I was thinking of. I'd been continuing to use boost's implementations of both until C++11 became more mainsteam. I pretty much always use bind with signals, so I could totally see why I'd be confused there.
Be aware, though, that it is not reliable in the multithreading environment and may lead to data races. The `shared_ptr::unique()` is even deprecated in C++17. See [this proposal](http://wg21.link/P0521) by /u/STL and [this question](http://stackoverflow.com/questions/41142315/why-is-stdshared-ptrunique-deprecated) on StackOverflow.
They're quite handy and typesafe to boot. They go hand-in-hand with boost (or std now) bind, so you can do an assortment of nifty things like bind parameters into the callback that the signal didn't originally call for, or specify default ones that the original signal did call for. As long as it's all accounted for, the compiler will catch any type errors you make at compile time. You have to play some funky games to get them to play nicely with references, but I usually use pointers when working with them.
No; making copy of a unique_ptr is prohibited. You can release the managed object and create a new unique_ptr with it but that would be kind of dumb most of the time. Here's how you would pass parameter as raw pointer (you can also pass reference to unique_ptr, of course). // create int with value 42 stored in it in the heap std::unique_ptr&lt;int&gt; p = std::make_unique&lt;int&gt;(42); // you can "get" the raw pointer.. int *ptr = p.get(); You can pass the raw pointer around just fine. The p is the "owner" and responsible for deleting the object it manages. If you want shared ownership you want to use more expensive std::shared_ptr. So as I said earlier, the std::unique_ptr is just a simple delete wrapper so that you get the convenience that the object we point at gets deleted when the wrapper goes out of scope. 
Not a direct response to OP but based on the comments in this thread I though others might find this interesting: https://www.youtube.com/watch?v=fOV7I-nmVXw Fedor Pikus - The Unexceptional Exceptions 
You may want to touch on `std::shared_ptr&lt;void&gt;`, it's a cutesy. Regarding aliasing, it's unfortunate that the interface is so unsafe, as nothing prevents you from passing a pointer that is completely unrelated to the original object. I personally much prefer Sutter's approach in `deferred_ptr`, where has passes a pointer-to-member to the constructor, and the constructor derives the aliasing pointer from the original, guaranteeing they are indeed related.
Thanks for clearing that up. Clearly I have a lot to learn.
&gt; No, needing a tool to install some dependencies doesn't convert conan into a build system. But setting up your build system to rely on a package manager fundamentally changes the way the original build system works. No longer can the user just build the library using just cmake after installing its dependencies(either manually or using there favorite package manager). &gt; Even CMake is not a build system but a meta-build system. Yea to be pedantic than conan would be a meta-meta build system. &gt; Or just install your dependencies, get the include paths and library names from output text files and copy those into your CMake script. And this just proves the point that it fundamentally changes how the user builds the library in cmake. Cmake is setup to find dependencies without needing the user to use custom variables like `ZLIB_ROOT` or `ZLIB_INCLUDE_DIRECTORIES`. &gt; You can have CMake call git or wget (in fact it is common practice) I think is bad practice to do without first checking if the dependency is there. &gt; but that doesn't make CMake a control version system, even if you need git installed in your system for your CMakeLists.txt script to work. You can do a lot with cmake, but you have to go out of your way to make it do these things. Ultimately, a package manager and a build system should be orthogonal. So a build script should not be downloading dependencies even with a package manager. A build script builds and find dependencies(and even installs), and a package manger manages installing the dependencies.
Yes, a non-owning, nullable reference. But anything receiving one has to treat the pointer as non-storable as any layer usage of it might be after it has been destroyed. Unfortunately the choice between shared_ptr and unique+raw is largely down to expected lifetimes and how you need code to be. Unique_ptr has no overhead but shared has a cost
Disagree. It should have been noncopyable.
This is off-topic for our subreddit; I also recommend gcc's mailing list.
I recommand you to go and read the 2 articles Herb sutter wrote about smart pointers : [smart pointers](https://herbsutter.com/2013/05/29/gotw-89-solution-smart-pointers/) and [smart pointers parameters](https://herbsutter.com/2013/06/05/gotw-91-solution-smart-pointer-parameters/) . When I catched up modern c++ back in time, they helped me a lot to understand smart pointers.
This seems to be a setting for the package not the consumer of packages. It would require finding the package on disk, editing the source function (if the package doesn't already fetch from git) and then re install in the app. Not sure it will be usable but I will spin up a prototype, thanks! 
I hate core dumps, especially if they are triggered by undefined behavior and you only see `segmentation fault (core dump)`, and undefined behavior don't necessarily cause a crash. When a thrown exception is unhandled while you run the program through a debugger, you can be 100% certain that the execution stops and you can inspect what caused the exception. Technically, an assert would do the same I think... The amount of information you mentioned can all be found through a debugger afaik. Is there a particular feature you found on a core dump that is not there when using a debugger, or something about core dumps I just don't understand?
From cppreference: &gt; The result type generated by the generator. The effect is undefined if this is not one of short, int, long, long long, unsigned short, unsigned int, unsigned long, or unsigned long long. w... what? Why?? Argh. Well, at least my code works. For now. What were they thinking?
It's a worthwhile thing to attempt if you wan't to use it with the latest version of the most common compiler. But apart from that - totally not worth it.
&gt; it's potentially more efficient with memory. Also exception safe: foo( std::make_shared&lt;A&gt;(), std::make_shared&lt;B&gt;() ); foo( std::shared_ptr&lt;A&gt;(new A), std::shared_ptr&lt;B&gt;(new B) ) ); The first will clean up everything, the second could leak A or B if either throws.
Author here - When I was doing this work last fall, I couldn't find many comprehensive resources for strapping C++ onto a bare metal system. If anyone has any corrections, additional resources, or general advice, I'm happy to hear it.
Not that you're wrong, but I have removed all raw pointer use in my work codebase (when not using Qt... sigh Qt raw pointers make me itch for a more modern Qt I know the current system is fine, but it just looks so fragile especially when integrating with modern STL code). Either way when I want to reach for a raw pointer I use a reference or a std::reference_wrapper. I know the distinction is minimal but my OCD/Perfectionism is very happy with me and my fears of memory problems are often alleviated. If I use a smart pointer it is mostly std::unique_ptr that I move around. I am looking for a small personal project where I can experiment with having a code base that has no nullptr checks because null is not a valid state anywhere. 
If you are asking about smart pointer then you should never used `auto_ptr` as it is being removed because it was buggy we and limit the use of `shared_ptr` to objects the must have shared ownership. the default you should used is `unique_ptr` or scoped objects. C++ has a feature called **RAII (Resource Allocation Is Initialization)** were the constructor is responsible for creating resource needed by the object and destructors are responsible for cleanup of all resources owned and used by the object. Resource include threads, locks, mutexes, etc. and not just memory and with Java and C#. Also with container like vectors, lists, deque, sets, maps, etc. all have emplace functions the creates objects based on their costructor agruments
I liked this guide: https://www.gitbook.com/book/arobenko/bare_metal_cpp/details
I love that that exists, but I'm not 100% sure what it offers over a raw pointer, other than a semantic indication that it does exactly what I said, even to the point of offering the exact same interface, e.g.: if(foo) foo-&gt;DoStuff() Works whether foo is optional&lt;reference_wrapper&lt;T&gt;&gt; or T* edit: the above interface is wrong anyway, I should read docs more closely
Think about it &amp;ndash; where does that reference-count live? It must necessarily be on the heap, which in turn warrants an allocator.
Exactly. You can check out [this code](https://github.com/NovaMods/nova-renderer/blob/master/src/main/cpp/data_loading/loaders/shader_loading.cpp) for an example of how I do it. I use pure functions to read the data off of disk and build it into a `shaderpack` instance, then the `shaderpack` instance provides all sorts of useful operations.
Thank you for that link. That was interesting reading. I like that you don't make this into an object because it in reality does not have any interesting state. I think the only review point I would make is about those namespaced "global" non-const vectors which I would consider a bit of a "code smell" to use a phrase particularly in stateless functional code. They should be constants of some sort at least and if possible my team at work we would turn all of that into constexpr code or types with string names using the Typestring idea. But we are known to be a bit OCD about making the world goddamn const (according to the other teams we review).
I recently worked on a project using a PicoZed with Xilinx PetaLinux. It amused me that I had full access to all the latest and greatest kernel and compiler features in my embedded development, but I was limited to the outdated versions included in CentOS for the rest of the development on the project.
If you're getting as far as the hiring manager, you're either doing very well, or interviewing at tiny places. The hiring manager is nearly the last stop. And if you aren't what they expect from looking at your resume, someone earlier in the process screwed up badly. I do resume review, first and second round phone screen, and in-house technical interviews for a large technical company. We hire CS and non-CS majors for entry level roles, the difference largely being what we expect from the candidate, and the training program they go through once they are hired. (Every entry level hire goes through training. Everyone has gaps, and no one knows our software and processes.) If you are getting in the door, your resume is probably OK. But it can't hurt to make sure it's as computer technical as possible. It shouldn't be your CV. I'll be honest, I don't generally look at someone's github, unless it's something already relevant. Having one is good, just because it's a minimal technical bar. It lets me know you've done the basics. Unless it's something you are contributing upstream. That's way more interesting. And a huge plus for an entry level candidate. Work on being able to code in a live situation. Small, easy, basic data structure manipulation, usually. After all, I'm hoping the problem will be solved in about 15 minutes, and we can do another one. And still leave "time for you to ask me questions" Which you are also getting graded on. Ask about what I do, what you're going to be expected to do, what the team size is, what's my greatest difficulty in the current system. You could even ask me how to reverse a linked list. 
[removed]
ffmpeg does not use autotools. Their build system is just a giant hand-written shell script and a makefile.
OK. I can not argue about what you find nicer. :) I prefer the boost lambda in the example you wrote.
If you are good apply to FB, GOOG, AMZN. You do not need to know frameworks to get there but you should be very good coder, and know a lot of algorithms for the interviews.
Making them const definitely sounds good. Not sure I'll to with typestrings. Too used to regular strings, I guess
State Farm is a specific example I know. On one hand it shows the place can't attract top talent, on the other hand it shows that they are willing to invest in their employees. *would I work there?* No because I want to be surrounded by the best.
Maybe I'm missing something but you don't need to store the "separate range" values and instead use a lambda to output the final type.
It depends I guess on the University you go to as most these companies will only hire from the top 30, but if you are in that category then you shouldn't have a problem. I know a couple physics majors who were able to easily transition as they helped develop and code mathematical models.
The downside of null being an invalid state is that move constructors have to allocate, or leave objects in invalid states.
thanks dude
You might want to check out this posting (I'm not affiliated with the company, but they do sponsor our c++ meetup) https://www.quantlab.com/careers?p=job%2Fo2VH3fwr Also, be sure to come to the meetup https://www.meetup.com/North-Denver-Metro-C-Meetup/ Jason
&gt; Well, at least my code works. For now. Congratulations you've succeeded in showcasing exactly the wrong way to deal with undefined behavior.
it only runs on windows atm
There actually are two reasons to use owning raw pointers in a codebase, and one of them even makes sense: 1. Data structure implementations. 2. Legacy or stupid code. You know that you use raw pointer only as non-owning reference, but is it clear enough for others? Even if there are no data structures, there's a chance that you used `new` for some reason (what I call "stupid code" ;)). Edit: as for your question, no, I don't think so. Maybe you get exception when doing `x-&gt;y` on empty wrapper, instead of whatever-happens-when-you-dereference-nullptr, but I'm not sure.
Not to worry, review comments are suggestions for the code owner not dictates. 
For what it's worth when I had to report a bug with the 2017 beta/RC a month or two ago I found the process pretty straightforward.
Under no circumstances are you to read Modern C++ Design: Generic Programming and Design Patterns Applied : Andrei Alexandrescu (The Traitor (Works on D now)). It will blow you mind and you will use that new knowledge to introduce needless template based complexity that adds nothing.
**Company:** [Pix4D](https://pix4d.com/) **Type:** Full time **Description:** We are looking for a talented/senior Software Engineers to join our growing team in Lausanne. Pix4D develops drone photogrammetry software and products that enable tens of thousands of professionals around the world to process, visualize, assess and edit their own maps and 3D models. The ideal candidate should know that the most important characteristic of software is maintainability. As such, he/she strives to keep the code simple, readable and testable. Understanding the importance of testing and the definition of “done” is key for our team. [Full job description](https://pix4d.workable.com/jobs/421296) We also have an [open position](https://pix4d.workable.com/jobs/417202) for someone with relevant experience with CUDA/OpenCL. **Location:** Lausanne [Switzerland]. **Remote:** No **Visa Sponsorship:** Preference for EU Citizenship or passport. Negotiable. **Technologies:** - Required: modern C++ (we are currently limited to C++11 by MSVC 12, but we will move soon to a newer version (therefore, knowledge in C++14/C++17 is more than welcomed), modern cmake. - Recommended: We use many different technologies and frameworks and the knowledge on any of this would be considered a plus: Qt 5, Boost, CUDA, SIMD, OpenGL, OpenSceneGraph, OpenCV, AWS, ffmpeg, Eigen, Ceres, GTest &amp; GMock. - Optional: DevOps knowledge is also a plus. We use Jenkins for our CI system and we generate binaries for Linux, Mac &amp; Windows (gcc, clang, msvc). We mostly use Python for automatizing these tasks. **Contact:** Check out our careers pages (in the Description). 
I am nowhere near to advice someone, just assume I am suggesting. Wouldn't physics relates software kind of jobs make more sense to you ? I mean, you should compromise a little bit, of course in compare to someone which does have BS in CS and do know what is going on on software systems, software companies would prefer her/him. *but* you have very good chance too, for example, specialized software which is related to physics, in that space you are virtually in no competition. (and I estimate salary for those kind of jobs would be very good too). 
This is a good article, and I like the idea of at least modernizing the resource function to use a unique_ptr and releasing the raw pointer to a sink function you cannot change as it communicates intent. I think it's also worth mentioning that thanks to move, you can also now just pass resource types by value, giving the user the option to copy or move. This is useful when you want to avoid the extra heap allocation of creating the object managed by a unique_ptr. Edit: clarifications. 
I also develop embedded/systems applications. Only been doing it for about a year but still wondering what the best practice is for running my application on startup. I've tried from /etc/rc.local to making my own script in /etc/init.d/. Is there a better/safer way of doing this??
map+reduce was the main use case I was thinking of... or in C++ terms, transform + accumulate. Yes, you could use a lambda with a reference capture to implement the same thing... but then you should probably use for_each since you don't care about the output type. And if you're going to use for_each, why not just use a range-based for loop instead?
`IFacebookService::GetVal()` is private, but `IAchievementsService::GetVal()` isn't. Not sure if that's intentional. It's too bad that you need that parameter in `GetServiceName`... Maybe you could use the macro to define a template specialization for each `ServiceType`? Would make the macro more gross, but would potentially make the calling code nicer. You might also want to use `shared_ptr` to access services. Maybe not, but I just have a knee-jerk reaction to seeing raw pointers. Thanks for sharing this! I actually might use it in a couple of my projects... Need a way to manage dependencies a bit better
When is the next release? I'd like to try it.
Every time I see post like this, I tell to myself: "You should finally finish your ECS". I already counted 10 or so iterations of mine :P So please keep posting your ECS' - it motivates me to think about it. edit: usually when I sit to the code I end up rethinking how dependencies between components should be handled, but I'm closer and closer to final design. edit: do you have separate container for each data type?
In a few weeks. The two major features are [Testscript](http://codesynthesis.com/~boris/tmp/build2-testscript.xhtml) (done) and parallel builds (finishing off). Will post an announcement on /r/cpp.
I will consider removing the dependency in the future, didn't want to reinvent too many wheels for version 1.0. The benchmark is to add 1000 entities and iterate over them 1000000 times. The benchmarks were done with release builds without a debugger attached. I am quite new to benchmarking, which I've been told is notoriously hard, so I am entirely open to adjusting them to be more fair/accurate. 
They thought it would be more useful to have a mutating copy constructor, enough to damage the Core Language to make it happen. They thought wrong!
First, sorry for the late reply. I see where you are coming from and we've [considered this](https://www.reddit.com/r/cpp/comments/497mh0/inventing_target_triplets_for_visual_studio_clexe/). Specifically, quoting from that discussion: *Some suggested we also encode the runtime type (those /M options) though I am not sure: the only "redistributable" runtime is multi-threaded release DLL.* I guess the question here is what we are trying to identify, a toolchain, a build? In what situations would having the runtime type (debug/release, static/DLL, single/multi-threaded) be useful?
It's possible without macros. [Boost].DI deduces constructor parameters by default, you can check it out here -&gt; http://boost-experimental.github.io/di
sorry if this isn't what you're asking, but it sounds like your time would be better spent improving the design of your program can you find a way to hide more of the implementation details of each request type behind a common interface? if you do this right, then adding new request types is as easy as implementing that interface - and the rest of your program doesn't have to change at all
Sounds like you'd be better served grabbing some refactoring tool and refactoring all the cut and paste code in your program into a separate set of classes or methods. If you can't because there's business logic in the loading and display portions of the program, you might also want to think about explicitly moving the business logic somewhere else. That would help a great deal with your coupling problem.
Oh yeah, I've been there, believe me. I had to take care of a bunch of motif code a couple years ago. It had 400+ global variables in it, multiple variables for the same value in a lot of cases and business logic all through the display code. Never could make a dent in it. There was some rudimentary testing prior to deploy, but it was pretty much guaranteed that if you changed something there'd be some unforeseen side-effect that would pop up, possibly in a completely different application, three months later.
I keep making ECSs and starting over to make it better
I was rather thinking about bugs hit "in the wild", when there's no debugger. There, bugs must coredump IMO. In development, a debugger is preferred, I agree. For either unhandled exceptions (extremely rare, wouldn't you agree?) or bugs/crashes. 
It's more of a medium sized briefcase, but there's more modern components such as the newer features that are released, but then you get the problem of going from the newer .NET stuff to the older win32 stuff.
Odin Holmes is doing a lot for C++ on embedded: http://odinthenerd.blogspot.ca/?m=1 
Those quizzes were brutal... they mostly covered esoteric features, undefined behavior, and how the compiler deals with terrible code (variable hiding, etc.). I'd start off with FizzBuzz for the phone screen (via collabedit or something) to quickly check syntax knowledge, and, if they pass that (brace for 50% failure), I'd move on to a trivial algorithm and beat the hell out of it to see how deep they go into the language. For example, ask them to reverse a character array, and, after they finish an implementation, keep asking them to revise it: * Use pointer arithmetic * Use array notation * Use std::swap * Use std::reverse() * Use std::string and a reverse_iterator and on and on... if you get a blank look when you mention a reverse iterator or &lt;algorithm&gt; then you'll know their knowledge doesn't go very deep. For on-site questions, I've been hearing very good things about [**Elements of Programming Interviews: The Insiders' Guide**](https://www.amazon.com/dp/1479274836) ...there's a C++ and a Java edition. It's not an online quiz, but it has good questions that'll measure programming ability instead of just language trivia.
hnnnnnnngg potentially a rainforest themed robotics shop? :F 
One thing to consider is that boost container is header only.
Yes, but you're smarter than the average bear, Bryce! 
Update: The bug has been fixed and the fix will be available in the next major update. Backstory: An internal team found this bug a couple months back. We fixed the bug in Dev14/VS 2015 but because we also made a number of changes in the SSA Optimizer around the same time this bug fix was deemed too risky to make the RTW. Also, the workarounds are pretty straightforward. We're continuing to test the optimizer with the fix and expect the bug fix should be in the next major update of the compiler. 
I have written tools like this in Python with some modules I have gradually built up to do things like insert sections into text/source/XML files. It depends a lot on the actual structure of your additions and the changes needed to the generic code for each addition.
Nothing wrong with that if it's a learning experience.
Olve Maudal's Pub Quiz is always very tricky, you can learn a lot by giving it a try! http://www.pvv.org/~oma/PubQuiz_ACCU_Apr2016.pdf
Your post has been automatically removed because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/5vgze3/best_software_for_c_beginner_with_macbook/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
The way I handle dependencies is that Systems has a vector of Components type_indexes that i call componentDependencies. When an Entity adds a component it sends a message to my ECSManager class that retransmits the message to each instantiated system, each system then verifies whether that entity possesses every single component in the system's componentDependencies. Then just do the opposite when removing a component. I also intend to add an optionalComponents into the system, so that a System may specify that a component is not "required" but it still uses it in case it's there. I don't know whether I'm helping since your library might be quite different, but I hope this aids somehow.
I see that you have put great thought into optimization. It seems you have a single collection of entities right? How do you manage to avoid performance costs of filtering entities in every iteration for every "system" or equivalent? Edit: I think I understand, you just filter it once into a new collection of references. Don't know where I was getting at anymore. :-) Thanks for the explanation. Your approach is quite interesting.
Currently I also have a collection for each component/tag, and I am thinking about adding similar collections that the user can specify. 
Hacker Rank?
They use GCC 4.9.2. No generic lambdas = no fun.
I did contemplate writing it to stub out one side of the request to start off with, as on one side you can literally append to the end of the file and the request will work; but the other side is a little more tricky, as it requires putting requests into case statements etc. I'm not sure how I'm going to approach the user entering the request information. I'm going to speak with some of the principle developers and get their views of if this is even worthwhile or if it's just going to be a bug prone program.
Oh boy i feel your pain. We went through the same thing, but also did the refactoring afterwards (easier sincw you can generate the refactored code) You should be looking for templating libraries like StringTemplate for Java. Just remember to avoid mixing generated and manual code in the same file. We were also generating using some c# code, but I can't remember the name Edit: take a look at http://stackoverflow.com/questions/1518954/c-sharp-template-engine We used T4
 Aws::InitAPI(options); { // ... } Aws::ShutdownAPI(options); ...why not RAII?
With exceptions, RAII is the only way. ;)
Yeah, Amazon needs to hire some C++ Expert to review their code...
&gt;while they completely miss the idea of how powerful is the global package manager. What does it matter how "powerful" it is? It doesn't solve the real problems that exist in the real world C++ ecosystem. Problems that *are* solved in other ecosystems. &gt;Happens all the time. Non-developers installing applications from npm (or equivalent)? I don't think so. The only stuff I have installed globally from there is things like webpack, tsc, mocha, etc. - dev tools.
One thing that always bugged me in big o analysis is when people say things like "hash tables have O(1) lookup", and so are asymptotically faster than binary trees for this purpose. Hash tables are indeed better than trees in many cases but this analysis is not an explanation. More basically -- how can it make sense for pointer dereference to be an O(1) operation? When we're talking about comparing hash tables vs tree data structures asymptotically in c++, the only way it makes sense is if the main memory of the machine is growing asymptotically. No one is talking about using filesystem handles to access the data in the hash table. The assumption is, even these "asymptotically large" data sets are in main memory. This ties in with your point, everything on a real x86 machine, with bounded memory, is O(1). To do meaningful asymptotic analysis, you need to have in mind some kind of generalized x86 machine where the main memory has size n when the problem has size n, or something like his IMO. If main memory has size n, then presumably the word size is about log n. And then, pointer dereference should probably be O(log n) time or so. I don't think its physically realistic that a pointer of size log n could be dereferenced in O(1) time. And with that model, hash table lookup is O(log n) which feels alot more sensible IMO. Besides, real programmers know that dereferencing a pointer is sometimes vey exprnsive, esp. if it causes a cache miss. Thats half the reason to use c++ at all. Noone reads code thinking, adding two numbers is the same cost a dereferencing a pointer. Just a pet peeve, nice article :)
http://onlineprogramminghub.com/c++/data-types-in-c++.php &gt; boolean There's not data type with this name...
This reads like auto generated nonsense. 
It appears that the current prelease version of clang 5.0 SVN is C++17 complete. Of course, C++17 itself is not really complete yet, but it is in a feature freeze, waiting national body comments and defect reports. Standard Library support for libc++ is also not quite complete. With g++ 7 SVN also being feature complete, it's a good start for the year!
It's so weird to read "current SVN" in 2017... 
You can do something similar to what managed languages do and keep a table of references to a memory address. Once that falls to zero the placement delete operator is called and the memory is marked as available for reuse. 
apt.llvm.org 
whats the result of -0xFFFFFFFFFFFFFFFF (64 bit machine), try without compiler of course, then try to produce a list why your assumption was wrong.
I think he's not talking about cycles inside the syntax but cycles inside the structures that refer to AST nodes. E.g. turn this into an AST: int rec(int i) { return i&gt;0 ? rec(i-1) * 2 : 0; } The AST node for that recursive call will probably have a reference to the AST node of the function definition. That means its ref count won't drop to 0 when you throw away the AST since its being referred to by that other node. If, instead of relying on RAII or garbage collection, you just collect all nodes in a vector and run their destructors all at once, a thing like that won't happen. 
First, you should probably keep in mind that hash tables and binary trees are *abstract data types* (you might have had a course in college with ADT in the title or name of the textbook). So they *abstract* away the underlying machine architecture. It doesn't make sense when discussing an ADT to talk about things like machine word size, main memory, etc. Second, big O notation is a *limit* as the input size goes to infinity, so it makes no sense to say there is a different big O value for one size input versus another size input. Formally, big O notation is defined as such: if f(n) describes the number of operations needed to perform some task when the input size is n, then saying the task has time complexity O(g(n)), where g(n) is a different function of n, implies that there exists a value k &gt;0 and a value N &gt; 0 such that whenever n &gt;= N, f(n) &lt; k * g(n). This definition allows one to drop the scalar: O(n^(2)) is equivalent to O(2 * n^(2)). If you remember some calculus, you can also see why this allows dropping smaller terms: as n approaches infinity, n^2 + n will be dominated by the growth of n^2, so you can drop the linear term.
Nice article, not too long, accurate. I have often had even better success with an arena - like a pool except that you don't call the destructors when it's freed and the contents can be completely heterogeneous. Not having destructors forces you to put very plain sorts of data there (often you come up with some sort of string-like class which just wastefully reallocates from the arena when it increases in size). But it's astonishingly fast, and you can "free" the arena with "O(0) time". ;-) For things like webservers where you have a series of unconnected transactions where you steadily accumulate memory until you're finished and then everything goes, an arena is about as light a thing as you can get. I first saw this trick in Google's web server's code about ten years ago (no idea what they do today but suspect that's still there). I used it once successfully later in a hobby project though I really didn't do proper benchmarking...
A type in C++ has an identifier (not keywords). If you refer to types, refer to them by the identifiers or risk confusing everyone. You can't make shit up as you go along. Furthermore, the site overall seems to be just enumerating stuff without any explaining.
Great work! I hope VC will join the party soon...
The fundamental types are keywords as well as identifiers. I agree though that the page is confusing, I was just being pedant.
This ultimately leads to redundancies: the package manager does not know about the dependencies and the build system does not know where your dependencies where installed so one has to define these things twice which is also error prone.
I am currently evaluating Conan for my company, please share more about your CI issues and solutions. So far i do not miss any feature but i wonder how the conan_server will scale with a lot of binaries. I mean when building debug for multiple platforms this will end up with a huge amount of files. The other thing i am thinking of is how to deploy the binaries to a customer system. Currently we are using RPM/MSI packages but i would like to create binary-only Conan packages and have a Conan server running to install all dependencies of a "customer-package" which also would contain the configuration files.
https://aur.archlinux.org/packages/dpkg
Yep, I've used it before (even though it's not a great idea). 
Can't wait till we can say the same about CMake... ;-)
So is this just a C++ wrapper around a remote service?
Thanks for the information. Where do you draw the line, though, in terms of binary-compatible builds? Is building with different VC update versions (e.g., 14U1 vs 14U2) ok? What about using different Windows SDKs? One seemingly natural answer is "if the vendor (i.e., Microsoft) says they are". But then, we shouldn't be distinguishing between 14.0 and 14.1. To me it seems using some kind of a hash of the compiler signature, compile/link options, etc to identify a binary build would be the way to do it. This is what we do in `build2` to make sure when we say things are up-to-date they truly are: we hash and store the compiler signature and options each object file is built so that if you upgrade VC from U1 to U2 or change optimization from `/O1` or `/O2`, things will be rebuilt.
And then C++ gets bad rep because of code like this... :-)
Your post has been automatically removed because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/5vjyhp/beginner_needs_help_with_c_program/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
How is the solution in the article any different than using std::vector?
Note: Language not library.
Some of the chromeOS repos have pretty extensive mocking and test coverage: https://chromium.googlesource.com/chromiumos/platform/shill/+/master
I think this is part of the AWS SDK for C++. In contrast Qt also has a technology preview for text to speech.
Serious question: Why would I want to use this rather than just a std::vector and replace pointers with indices? 
Actually, it could get easier, since you don't need to use placement new. Just write node = pool.construct(token); 
What I'd really like is a doc that is just a concatenation of all the other docs. Automated. But I'm lazy I guess.
I did note that libc++ is not quite there.
 Very nice - thanks for making this.
&gt; Specialists in a niche technology can get paid very considerably more Seminars by the likes of Alexandrescu / Meyers / Sutter go for about $2,500 per person for a 50-100 person 2-3 day event. 
Not a big fan of folly as an example, it doesn't feel well designed, not clear distinction between library and interface, but of a bag full of stuff. 
How would the ToC be much different than the list of files I now have, since it is basically one file per topic anyhow? I suppose some stuff is buried in small_stuff and template_stuff, but mostly it is one topic per file. (this is not a rhetorical question - I really want to understand what would make it better)
boostache is a well designed library with its origin starting during C++Now, yet documentation is lacking: https://github.com/cierelabs/boostache 
&gt; international In a narrow meaning I guess, since you don't sponsor visas.
Fair point. As a hiring manager I'd expect you to knock out a response as quickly as possible on a phone screen. But, assuming the initial answer is correct, I find it very valuable to probe deeper and ask about different implementations. It tells me a lot about how deep your knowledge goes with minimal time commitment from either side... C++ is so huge that there's tremendous skill/style variation between developers, so you've got to find a good time/depth tradeoff in the first phase. And you can't expect phone screen questions to be representative of the work you're doing--it's just a 30 minute filtering process. FizzBuzz is not a problem you'll be solving on the job.
1) very costly allocation when the vector has to grow object_pool has the same issue 2) growing destroys cache locality Growing invalidates all pointers/iterators, but the new pointer has all elements stored contiguously, it means I can iterate them linearly. Pools on the other hand have no such guarantees, also u can't iterate objects allocated from pools a = pool.allocate() b = pool.allocate() c = pool.allocate() for(auto&amp; i : pool.items()) // this is not possible 3) a pool means that you can destroy an object, a the next allocation will get you cache-hot one. Yes, deleting std::vector elements other than the last one is ineffective, but the article mentions one condition: &gt; You need to allocate a bunch of objects having the same lifetime so this is not an issue
I still find it amazing that around 50% of people fail FizzBuzz.
Google for "structure and interpretation of computer programs" (aka SICP) It's one of the best books to get you into the right mindset Other than this: program, program and continue programming
To be clear, this is /u/fitzgen's post; I posted it here because I like to read this subreddit and figured that it might be of interest to you all.
For those who are curious, the LLVM project's Itanium demangler implementation can be found [here](https://github.com/llvm-mirror/libcxxabi/blob/master/src/cxa_demangle.cpp). I don't know how it compares to the GNU version mentioned in the article, but since it's written in C++ rather than C it's probably better ;-)
1. Create a new project, no matter what it is. 2. Then, do it better. 3. Repeat 2 until and sometimes 1 you're satisfied with your skills. 4. Profit! 
Thanks, I didn't think of sanitizers and other tools requiring special builds. Also I was thinking of reimplementing tools like bcp to copy the minimum requirements for a library. But I think as long as the whole source distribution is not changed it is not necessary to do such tricks. From your description of using the same Boost on different platforms (we use the one that's available on the different Linux platforms at work, so our challenge is to never use features not supported on the oldest platform supported) I understand that there sure is some effort for building Boost with CMake, but your task would be more complicated than the standard "build all on this platform" command anyway, so you might as well do it right and control all of your build process.
I think the best way is to find a job a work with senior programmers or someone who's willing to couch you a teach you good practices and tips. I believe you learn fast with the feedback of your peers. 
Well, /u/zygoloid/ is a clang developer so he should know :) If it's named clang SVN until 5.0 is officially branched, then that was inaccurate, but reddit won't let me change it. (although as I noted, apt.llvm.org builds the nightly packages as 5.0). In any case, the *next* clang release will in all likelihood be C++17 complete. 
Looking at the feature table, it seems like that the 4.0 release branch went into feature freeze before the final C++1z features (notably `constexpr` lambdas were being added.
I don't believe that this is on-topic for /r/cpp - as a general question it would seem more suited for /r/programming.
I was wondering what lldb does, and it has a 'fast demangler' [here](https://github.com/llvm-mirror/lldb/blob/master/source/Core/FastDemangle.cpp), and then falls back onto the demangler you linked when that fails. I would be interested if anyone knows how high quality either of these are.
[removed]
As someone who uses C++ professionally to write mcu firmwares, templates are a huge improvement over C for stuff like pin mapping and whatnot. However without concepts, it quickly becomes an unwieldy mess of incomprehensible compiler errors. I've started playing with Rust's Traits and it's a friggin blessing.
This post makes less sense than something generated with tensorflow.
So if a distribution of Linux doesn't update Boost, you will only use features available in that version? How about you statically link Boost instead then and use the latest? That is usually not an issue with proprietary software and may save you some issues with bugs already fixed in upstream you could be hitting, as well as getting newer APIs available.
[removed]
Keep in mind that not all of us here are native english speakers. I think I lost count how many times I made several mistakes when I tried to express myself. But at the same time I would prefer to be judged by my opinions rather than my grammar) 
flite is a C TTS library that is cross platform and completely offline... I was working on a C++ and cmake port for a little while and then stopped because I got bored, but I did get as far as coverting to cmake and getting it to build under both linux and windows if anyone's interested
[removed]
ok, i honestly never used coroutines in full potential so i cant actually put myself in your position, and i cant suggest an alternative for those. I am not saying boost is utterly bad and noone should use it. This is just a follow-up on message i wrote, and mostly my personal opinion, so no hard feelings. I am game developer now, and for me both from current and previous experience, i am trying to avoid it and find other similar solutions. This silly discussion above is mostly a result of local trolls, and lots of free time on my side. I am mostly lurking stack exchange so i am not really used to it yet. I had considerable amount of issues, but they were just a result of bad mix of fixed compiler version and huge codebase. Unfortunately we lost 3 workweeks fixing either app or compiler crashing during compilation of those. When i switched devteams i felt so much better :)
I don't think one ever stops learning C++, or any language for that matter. There are a lot of nuances and details that can take years to learn and decades to master. That being said - if you start fresh learning "Modern" C++ (based on the C++11 specification or higher) you could probably reach a pretty good understanding of it in a few months. Longer if you haven't had prior programming experience - it's not the easiest starter language, but there's no reason you couldn't do it. Job wise, I'm a developer at Microsoft working on API design and implementation for the Windows storage stack. It's interesting and challenging work. There are definitely jobs out there. Now, all this being said - you should probably head on over to r/learncpp or r/cpp_questions. This subreddit is mostly for news and advanced topic articles about the very latest in C++. (Please read the sidebar on the right for rules and some good links) Cheers, and good luck.
Because the person you are replying to works on (created?) the build2 buildsystem that competes with CMake. So [this FAQ](https://build2.org/faq.xhtml#cmake) covers why the maintainers of build2 think it is better than CMake.
My test for "do I know C++" boils down to "can I read the standard and understand 70% of what they are talking about". Of course that depends on whether, when I think I understand, I really *do* understand :-). As with so many things, "to know X" is almost entirely in the eye of the beholder :-).
I doubt. They seems to just give up two phrase lookup.
Just because it's my pet peeve ... your loop likely isn't doing what you think it's doing. It's probably reading in one more line then you want.
&gt; Boost introduces either little or no functionality that other libraries are not especially with modern standards (11,14,17, and custom extensions) Okay, so we're running with 'troll' then. At least now we know.
Quoting [someone who would know](https://www.reddit.com/r/cpp/comments/5dai1i/vs_2017_rc_is_now_available/da4h6xw/): &gt; Also, we'll have two-phase name lookup done early next year. We plan to be feature-complete for all 98/11/14 features in 2017.
The code in production is quite a bit more extensive than this. I just wanted something simple that would give some context. 
The first time I felt like I had a handle on C++ was when I understood RAII, the various types of constructors, and callbacks. I studied it on and off while I was in school. Things sped up a lot when I started working. And you're never done learning C++ (or programming in general). They're coming out with new standards every couple years, and there's a lot there already. You could familiarize yourself with the language features by finding an open source project and reading the source code. Don't worry too much about what the code is doing. If you see something you don't recognize, look it up. Focus on one thing at a time. There might be a lot you don't recognize, but it's important to go slowly and not get overwhelmed by trying to understand too much at once. All employers are different. I wouldn't try to make generalizations. As for work, I write networking software. I have a hard time coding outside work. I guess I get it all out at work.
Some nits: 1. The decomp declaration part has multiple issues: - The second decomp declaration table ("C++17/compiler") won't compile since you attempt to bind a non-const lvalue ref to a prvalue. - I'd also have used plain `auto [i, s] = /*...*/;` to show the "under-the-hood" thing, just to illustrate that `i` and `s` are always references, since that's somewhat unintuitive. - Also, in the third table, if you do `auto const &amp;` you obviously can't do `++i`. - For the customization part, `auto get()` is almost always bad. - `auto [ x, y ] = { 1, 2 }; // ??` is ill-formed. 2. The `if constexpr` part isn't quite right: you can't do `static_assert(false, "...");`. 3. The fold expression part forgot to escape the angle brackets of the *template-parameter-list*, so it went missing in the rendered version. 4. "inline Variables" is badly Capitalized. 5. "Guaranteed Copy Elision": `lock_guard`'s constructor is `explicit`. 6. `[[fallthrough]]` is applied to an null statement, i.e., `[[fallthrough]];`, not a label. 7. Lambda in `visit` example should be taking stuff by reference. 
Yeah, you need while (std::getline(stream, str)) { // Do stuff } 
I learned C++ thanks to some books borrowed from a technical school teacher and Turbo C++ 1.0 for MS-DOS, back in the early 90's. C++ just had a pseudo standard back then (C++ARM). Nowadays I mostly use JVM and .NET languages, with C++'s role being for writing high performance libraries or using the native APIs of those runtimes. As for learning, one never stops learning, regardless of the language.
If you're on Windows and don't need to be portable, you're probably better off using CreateFile-and-friends. Might be worth profiling FILE_FLAG_SEQUENTIAL_SCAN too.
&gt; No, because it doesn't grow by doubling, like vector does. vector requires shrink_to_fit at times, which sucks. A proper pool does the right tradeoffs here. Pool can, and the one mentioned in the article does grow by doubling
This was more what I was thinking, all of the code inside of the requests is pretty similar. I'm looking for any reading material around the subject if you know of any.
&gt; I'd also have used plain auto [i, s] = /*...*/; to show the "under-the-hood" thing, just to illustrate that i and s are always references, since that's somewhat unintuitive. Except that `i` and `s` are never references. See https://godbolt.org/g/rk2vXo. The declaration introduces id-expressions that *refer* to the sub-objects of the initializer expression, but apparently this happens as-if they are actually the sub-objects themselves, not references to them.
I think the problem is on your end, sadly.
Cool. How does it know the exact compile/link command lines VC is going to use? I don't think even Microsoft knows that anymore ;-). UPDATE: I am really interested to know, can one of the CMakers shared the details (instead of just downvoting). For those wondering what's the big deal, here is a result of an analysis we did on VC14 (prepare to scroll): LIB/{DEBUG{32,64},RELEASE{32,64}} CL.exe /c /ZI /nologo /W3 /WX- /sdl /Od /Oy- /D WIN32 /D _DEBUG /D _LIB /D _UNICODE /D UNICODE /Gm /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"Debug\static-vc14.pch" /Fo"Debug\\" /Fd"Debug\static-vc14.pdb" /Gd /TP /analyze- /errorReport:prompt test.cpp CL.exe /c /ZI /nologo /W3 /WX- /sdl /Od /D _DEBUG /D _LIB /D _UNICODE /D UNICODE /Gm /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"x64\Debug\static-vc14.pch" /Fo"x64\Debug\\" /Fd"x64\Debug\static-vc14.pdb" /Gd /TP /errorReport:prompt test.cpp CL.exe /c /Zi /nologo /W3 /WX- /sdl /O2 /Oi /Oy- /GL /D WIN32 /D NDEBUG /D _LIB /D _UNICODE /D UNICODE /Gm- /EHsc /MD /GS /Gy /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"Relea\static-vc14.pch" /Fo"Relea\\" /Fd"Relea\static-vc14.pdb" /Gd /TP /analyze- /errorReport:prompt test.cpp CL.exe /c /Zi /nologo /W3 /WX- /sdl /O2 /Oi /GL /D NDEBUG /D _LIB /D _UNICODE /D UNICODE /Gm- /EHsc /MD /GS /Gy /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"x64\Relea\static-vc14.pch" /Fo"x64\Relea\\" /Fd"x64\Relea\static-vc14.pdb" /Gd /TP /errorReport:prompt test.cpp DLL/{DEBUG{32,64},RELEASE{32,64}} CL.exe /c /ZI /nologo /W3 /WX- /sdl /Od /Oy- /D WIN32 /D _DEBUG /D _WINDOWS /D _USRDLL /D DLLVC14_EXPORTS /D _WINDLL /D _UNICODE /D UNICODE /Gm /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"Debug\dll-vc14.pch" /Fo"Debug\\" /Fd"Debug\vc140.pdb" /Gd /TP /analyze- /errorReport:prompt "dll-vc14.cpp" CL.exe /c /ZI /nologo /W3 /WX- /sdl /Od /D _DEBUG /D _WINDOWS /D _USRDLL /D DLLVC14_EXPORTS /D _WINDLL /D _UNICODE /D UNICODE /Gm /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"x64\Debug\dll-vc14.pch" /Fo"x64\Debug\\" /Fd"x64\Debug\vc140.pdb" /Gd /TP /errorReport:prompt "dll-vc14.cpp" CL.exe /c /Zi /nologo /W3 /WX- /sdl /O2 /Oi /Oy- /GL /D WIN32 /D NDEBUG /D _WINDOWS /D _USRDLL /D DLLVC14_EXPORTS /D _WINDLL /D _UNICODE /D UNICODE /Gm- /EHsc /MD /GS /Gy /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"Release\dll-vc14.pch" /Fo"Release\\" /Fd"Release\vc140.pdb" /Gd /TP /analyze- /errorReport:prompt "dll-vc14.cpp" CL.exe /c /Zi /nologo /W3 /WX- /sdl /O2 /Oi /GL /D NDEBUG /D _WINDOWS /D _USRDLL /D DLLVC14_EXPORTS /D _WINDLL /D _UNICODE /D UNICODE /Gm- /EHsc /MD /GS /Gy /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Yu"stdafx.h" /Fp"x64\Release\dll-vc14.pch" /Fo"x64\Relea\\" /Fd"x64\Release\vc140.pdb" /Gd /TP /errorReport:prompt "dll-vc14.cpp" 32: /Oy- /D WIN32 /analyze- 64: DB: /ZI /Od /D _DEBUG /Gm /RTC1 /MDd RL: /Zi /O2 /Oi /GL /D NDEBUG /Gm- /MD /Gy 
~~Just for completeness: if you are positively sure that you want stuff written RIGHT NOW, use ```istream::flush()```. Just be aware that writing to disk too frequently might slow down your software.~~ Edit: Mixed input with output. sorry.
`flush` does nothing for input. There's no way to explicitly reload an input buffer (but seeking usually does the trick).
This works? The declaration of parameter `i` on line 11 supplies no type parameter for `EqualityComparable` and gives no indication that `i` needs to be equality comparable with an `int`. Does the checker look at the body of a concept-using function to deduce parameter types, or is there something else in play here?
Too bad WSL is not available for Win 8.1. I can't upgrade to Win 10, every build so far makes my machine crash and BSOD, apart from other horrible bugs. And the auto-restarts, and the telemetry... And no surprise, back to 8.1 and it runs perfectly, no single crash. I reported all this to MS but every build of Win 10 worked same/worse. Makes me really afraid of Kaby Lake, when &lt;=Win10 is no longer officially supporter and drivers start not existing anymore for new hardware... Sorry for the digression. The article is really cool and I wish I could use it :&gt;
Works by luck. `EqualityComparable` does nothing more than just checks that objects of type `T` (in this case that would be `int`, deduced type of `i`), are comparable. If you'll call function `is_the_answer` with `std::string` as an argument, then code will not compile because there is no `operator==` for `std::string` and `int`.
Yeah gentoo did a good job making me patient. But honestly compiling clang is a lot faster than GCC and nothing compared to chromium
Yes, the example works, but it's really trivial. I added a better example from /u/CaseyCarter that shows concepts actually doing something useful. It's just that concepts can quickly become an eyechart. For a post about "let's learn concepts" I wanted something really, really simple.
The compile fails at the comparison inside `is_the_answer`. The call site is fine because the concept is satisfied: std::strings are equality comparable. Edit: I added a better example, albeit one that might scare beginners. 
Congratulations (and a little jealousy!) to all my friends who work on Clang/LLVM! PS: I don't care if you use SVN or Git. 
If you just glance at it, yes. But what I found reading Bjarne's paper (linked from the blog post introduction) is that concepts unfold really nicely as you read through them. 
This is fantastic. I think my fav is either "friend", "nest of the cute long long", or the "mysterious sfinae scroll". By the way, what do the labels on the compass rose represent?
http://www.boeing.com/resources/boeingdotcom/defense/c-17_globemaster_iii/images/c_17_hero_lrg_01_1280x720.jpg
&gt; When learned how are you using the language for yourself? Well ... I write software all the time (at work and at home). &gt; do you use it when you are not in your job ? Yes. My latest pet project: I got annoyed with windows file find, so I am writing my own command-line version. I am also creating a python project to manage C++ sources (list/search sources, generate boilerplate code for new C++ classes, etc). Both of these will probably not go anywhere, but I am still doing them. &gt; There is tons of precise information how to learn c++ but not something like : have I learned enough c++ ? A language is not _something you know_, as much as it is _something you are fluent in_. Consider the case of other languages. You can say you understand French/German/Mandarin/whatever+, but not necessarily that you know it. What would being fluent in English (for example) mean? Where would you set the bar? Capable of reading the language? Of writing a novel in it? Maybe poetry or technical writing? I am fluent enough in C++ to write code and understand it and debug it and so on. You may become fluent in GUI programming, DB optimizations, machine learning algorithms and so on. &gt; how do i know I came to a point I can find a job and what i am going to do ? Well ... simply look for one, go to a few interviews and see what happens... &gt; How do i improve myself ? Some ideas (not a full or complete list): Create your own projects or contribute to open ones? Work on code bases that you like? See some programming instructional videos?
I'm arrogant enough to give myself a 4 out of 10 in C++ knowledge. I've only been using it for about twenty years, though. https://www.slideshare.net/olvemaudal/deep-c/255
Thank you. And thanks for calling them 'nits', some are a bit bigger than that :-) - why can't you do static_assert(false, "...") ? - visit by reference - you say "should" because that's how the auto gets "translated" by the compiler, or "should" as in best practice? (I think you mean the former. So the string should not be const either) As for the "auto [i, s] are always references", ugh, it is so hard to get a straight answer to that.
Sweet pic. Relevance?
Look for catch and throw.
c / c++ programming
it's the pic when googling "c++17"
I don't know anything about C++ what does this map even mean? 
You could use [dear imgui](https://github.com/ocornut/imgui), which is a great immediate mode UI. Here are two resources talking about implementing such editor: - https://github.com/ocornut/imgui/issues/306 - https://gist.github.com/ocornut/7e9b3ec566a333d725d4
I want to learn C++ :) Please show me the ways~ 
I don't believe there is any reading material about this subject because it is highly specific. You will want to write a script which replaces occurrences of pre-defined text (i.e. stub identifiers) with appropriate text (i.e. implementation) depending on the context. To replace the selected occurrences one may utilize `sed` expressions or, if you feel like, you may write a (Python) script which goes over the file line by line and do the pattern matching by yourself.
Our company has 50 developers developing software for industrial machines and we use git. 
[Take the tour :)](https://isocpp.org/tour)
I would like to know too. I was thinking "B" being a predecessor language and "Ch" being a interpreted version of C++. While "r" is a language, it is not really related to C++ at all. And "Ttb" isn't a language as far as I know. So I'm probably going down the wrong road with this train of thought.
Better yet, we can show you a map to c++!
Also: threadsafe.
But curiously, `path` has a `+=` operator. 
Is it possible to have a Concept that requires a type implements particular member functions, like Rust's traits or Go's interfaces? Without this functionality it seems that Concepts are just another tool for library writers, not application writers.
Wait until someone publishes all the caveats, sharp edges, and pitfalls with Concepts. C++ might have "zero-cost" abstractions, but they definitely have a price.
As far as I can see from cppreference.com, this is possible. What I really want is the ability to write something like ``` template&lt;typename T&gt; class linked_list&lt;T&gt; satisfies Traversable {...}; ``` And even better (after the above): ``` template&lt;&gt; class linked_list&lt;EqualityComparable&gt; satisfies EqualityComparable; ``` to say that a linked list of `EqualityComparable`s is itself `EqualityComparable`, for instance (with some way to provide the `operator==` necessary). Basically what are called orphan instances in Haskell.
Compared to Haskell's type class syntax, it is pretty horrific. It's really a consequence of the template syntax being pretty bad to begin with.
From research online, CMAKE_EXPORT_COMPILE_COMMANDS does not work with Visual Studio generator. However, from a tooling perspective it's not as important as you could use a different generator (e.g. nmake) to get that output. Additionally, VS17 will natively support CMake without needing projects generated. For all the hate that CMake has received over the years, it seems to currently be positioned as the de facto standard for C++ development as it's the project format supported by Qt Creator, KDevelop, CLion/IntelliJ/Android Studio &amp; now Visual Studio.
&gt; yes, modern standards are usually considered to originate from boost, but are implemented in compiler, rather than in template hacks. what ? I'm pretty sure that all features of boost are implemented with the same template trickery in each compiler, except now different implementations (std::regex anyone) will have slight behaviour differences between standard libraries.
Are there any meaningful ways of giving the compiler hints as to which branches are more likely?
&gt; On that road, I would have put a carriage titled "Alexandrescu and Meyers". And Abrahams. :-[
Haha, that explains one thing - why [JF's](https://jfbastien.github.io/what-is-cpp17/#/9/2) works and mine doesn't. I guess I shouldn't do drive-by edits.
Thanks! Will fix soon.
This is off-topic for our subreddit. I'm not sure your question is even specific enough for StackOverflow.
I do not agree that #once is the solution. What other modern languages have this idea of having to manually prevent a file from being included twice? None. The real solution is to remove this mechanism from the language entirely. I see two solutions that are better than #once: * The default behaviour in C++ needs to be to include a (header) file only once no matter how many times its #include'd, and then if someone *really* needs to include a file twice then they can tell the compiler to do so with some #multiple directive or something. Why the default behaviour is to blatantly re-include files multiple times is beyond me and probably goes way back to the early C days. Considering the number of use cases where you want multiple inclusion vs single inclusion makes this default behaviour even more baffling. * Modules. Fixes a bunch of other problems as well as multiple inclusion. The reasoning in the #once proposal that "modules aren't here yet" is silly considering modules with either be in C++20 or at worst, C++23, and so this #once would *at best* be in the language for 3 years before being superseded by modules.
You know C++ reasonably well when you start to disagree with Scott Meyers (and have reasons to support why). Scott's books are great, particularly when you are learning. But the answer to _every_ C++ question is "it depends" (memory? speed? scale? maintenance?...), and a book can only cover the usual cases, not the exceptions.
its not Ch. but Eh., probably exception handling, and its not R but F, no idea for the Ttb.
Oooh, thanks for that. I made my own in Qt a few years back. If I had known about something like this a few years ago, I probably would have used it and saved myself a ton of fixing bad design choices. (And frankly, that one is visually prettier than mine.) That said, making your own isn't that awful in Qt. I used QGraphicsWidget and QGraphicsScene, and bound each visual node object in the Qt scene to a "model" class in pure C++ so it's pretty flexible. The model object was responsible for understanding all the behavior stuff and serializing itself to JSON so the UI could be ported to other kinds of apps if you needed to.
&gt; $2,500 per person I like the Three Amigos but I am honestly puzzled who pays that kind of money... My only theory is that people "scam" their company that it is super uber good for productivity and that the company pays. :) And before somebody tells me that I live in some poor country. Even if I worked in SV I would not be happy to burn that kind of money. cppcon fees are more reasonable. 
&gt; It's just that concepts can quickly become an eyechart. Of all the concepts I know, this one is the most....readable: template &lt;class I&gt; concept bool Readable() { return Movable&lt;I&gt;() &amp;&amp; DefaultConstructible&lt;I&gt;() &amp;&amp; requires(const I&amp; i) { typename value_type_t&lt;I&gt;; typename reference_t&lt;I&gt;; typename rvalue_reference_t&lt;I&gt;; { *i } -&gt; Same&lt;reference_t&lt;I&gt;&gt;; } &amp;&amp; CommonReference&lt;reference_t&lt;I&gt;, value_type_t&lt;I&gt;&amp;&gt;() &amp;&amp; CommonReference&lt;reference_t&lt;I&gt;, rvalue_reference_t&lt;I&gt;&gt;() &amp;&amp; CommonReference&lt;rvalue_reference_t&lt;I&gt;, const value_type_t&lt;I&gt;&amp;&gt;() &amp;&amp; Same&lt; common_reference_t&lt;reference_t&lt;I&gt;, value_type_t&lt;I&gt;&gt;, value_type_t&lt;I&gt;&gt;() &amp;&amp; Same&lt; common_reference_t&lt;rvalue_reference_t&lt;I&gt;, value_type_t&lt;I&gt;&gt;, value_type_t&lt;I&gt;&gt;(); } 
Your post has been automatically removed because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/5vuap1/help_with_project/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Exceptions also strongly hint that a branch is unlikely. You can take advantage of this by writing code like this: if (!ptr) try { throw 1; } catch (...) { // Handle nullptr... } Note: just kidding. Don't actually do this :P
Thank you for the example! What would a Concept look like if those callables accepted arguments of specific types (possibly also type-constrained)? For example, if `foo`'s signature was `int foo(SomeConcept const&amp; f, unsigned long ul)`? The applications for constrained compile-time polymorphism is huge in the embedded space. Currently I either give up type safety and deal with incomprehensible compiler errors, or use virtuals if I can eat the cost.
Why Meyers on the cart? Did I miss something?
Other languages probably don't compile the same way or were built with a solution to this problem already, so I'd give them a pass and say that's not a worthwhile comparison. I agree with both of your bullet points entirely, however your first one doesn't change anything: &gt; The default behaviour in C++ needs to be to include a (header) file only once no matter how many times its #include'd I think the real sticking point here is that there are ridiculous ways to include the same header twice that make it very very difficult to know the header is the one you've seen already. My point is that I think that instead of wringing our hands about those ridiculous ways, we should just say they're not supported/won't work. That's fine, because giving headers you're own unique identifier (basically what include guards do now) is the provided solution to that, so nobody is getting hung out to dry here. Otherwise, and while I agree: &gt; Modules. Fixes a bunch of other problems as well as multiple inclusion. I'm almost afraid to guess how long it will take my work to adopt this once it's available in a compiler we use. We have 2-3 million lines of code (depending how/what you count), so it's a major undertaking even to upgrade compilers -- nevermind switching everything to a giant new language feature like modules. Mind you, I don't expect any of this to actually get into the standard, but I wish it would or would have years ago because I'm probably looking at several years before we get the payoff from modules.
The Embarcadero iostreams implementation is pretty crap, you might be better off using OS calls directly.
Net beans with GCC (Cygwin x64) distro on windows, same on Linux. 
VS is a great environment to code in, if you're on Windows. Good C++ support, great debugger, good editor. some people swear by QTCreator not sure what specifically you're looking for 
https://scottmeyers.blogspot.com/2015/12/good-to-go.html
I always wonder, why it took so long for Microsoft to finally start to modernize (or rejuvenate if you will) the compiler? By the way, i look forward to the day i can finally use MSVC to build/test one of my projects ;)
Does PGO on MSVC do this?
vim
The advice I give to people starting out. For a while don't code. Instead chose vi and emacs and use them to begin your version of the Great American Novel using both. After you decided which one you like better really learn that one. Better a serious editor with good IDE like features then a good IDE with a mediocre editor. There are no great IDEs. That went away when C++Builder vanished into the etheric state it is now. Believe me. once you get used to an editor, you will become reticent to learn another. You just won't want to invest the extra time to change. There are other editors which could suffice, like Eclipse Atom, Sublime ... but they are not available everywhere. Maybe a stand alone graphical debugger like kdbg, but the basic functionality is the same in all. IDes have two major problems: they try to replicate functionality like grep, find, wrappers for version control and take a lot of developer resources from otgher more useful things. I am pretty much convinced this is wehat killed Smalltalk: good language but an ide was required and each version had to come out with it's own replicating all that functionality. The second is that IDEs do insist on creating their own versions of "projects " to build stuff. Effectively relegating your project to their ghetto. They also make it a pain to create small projects to play around with. 
Kinda sums up my working experience with Embarcadero. The project is in the middle of a conversion to vanilla C++, however things like this keep on delaying it. 
Thanks for the reply, that was my understanding as well. &gt; it's not as important as you could use a different generator (e.g. nmake) to get that output There is no guarantee that you will get the same output, semantically. For example, you may not have the same preprocessor macros or, even worse, some options (e.g., the `/M`-family) define their own. So you may end up analyzing your code in a different configuration (e.g., some fragments excluded due to macro definitions, etc). &gt; VS17 will natively support CMake without needing projects generated. Not sure how that helps with `CMAKE_EXPORT_COMPILE_COMMANDS` support. 
Blackhole project uses both googletest and googlemock for full code coverage. There are examples of mocking polymorhic classes as like as templated. https://github.com/3Hren/blackhole/tree/master
Neat!
It seems a reasonable fee. If a $100K programmer gets even 2.5% more productive (better, more idiomatic code, less technical debt) then this is earned back the first year.
I just write it perfectly each time :^)
Okay here's some stuff from my personal wish list... #1. A `namespace` like syntax for classes and structs so that you could reduce all the repetitious clutter in out-of-line definitions. For example, rather than: template&lt;typename T&gt; Foo&lt;T&gt;::Foo() {} template&lt;typename T&gt; Foo&lt;T&gt;::~Foo() {} // etc. Why can't you write something like: template&lt;typename T&gt; class_space Foo&lt;T&gt; { Foo() {} ~Foo() {} // etc. } #2. Another place where there is much needless duplication is in template specialization. No one wants to do it because every time you specialize, you have to rewrite the whole damn class. Maybe the `default` keyword could help here? template&lt;&gt; struct Foo&lt;int*&gt; { void bar(int* arg) { /* specialized code */ } void baz(int* arg) = default; // baz implemented as T* partial specialization or T depending on what is available // That is, the compiler looks for the next-most specialized version of Foo when it see default. }; #3. Optional named arguments for functions. #4. Hints to help compiler with auto-vectorization, parallelization, etc. (At least implement the `restrict` keyword for Heaven's sake!) #5. `constexpr` math functions. #6. A standard library implementation of whatever sorcery is needed to save a function and its arguments for deferred execution like in `std::thread`. (I tried to roll my own and it was a nightmare!) Okay I better stop. This is getting long!
I'm coming from several languages, and c++ is the first where forward declaration is needed, if you want the function further down. Do you know why this is ?
Historically, allowing the compiler to read the source code on a single forward pass through the code without ambiguity, without needing to hold the full source code in memory. Currently, it's a historical reason. It does offer a nice way to avoid cyclic references &amp; definitions - you cannot refer to things that aren't defined yet, so most cycles come out by virtue of not being able to write them.
OK, so a meta-question: In terms of process/workflow for releasing a new iteration of the Standard, what are the key points learned from the C++11, C++14, and upcoming C++17 releases? Do you see possible areas for improvement and if so, what are they?
If it helps, I haven't come across any proposals or std-proposals discussion about this that I can recall, at least not at all recently.
I am going to hammer this until my hands bleed, but: why no path::path(path::iterator, path::iterator)? 
On your fifth one: at least Clang calculates `sin`, `cos`, `tan` and such at compile time on `-O1`.
https://github.com/fbb-git
optional and variant capitulated, they are part of C++17.
You still need #include for C APIs/legacy and conditional compilation for conditional include/import. Plus expression to string conversion for assert.
I'll look into it, best get learning Python it seems!
I can tell already the answer to that one, and that is that Modules will mean MORE use of the preprocessor, not less, because the most convenient and least maintenance way to implement Modules is with the preprocessor. See https://stackoverflow.com/questions/34652029/how-should-i-write-my-c-to-be-prepared-for-c-modules/38227761#38227761. The preprocessor will continue to track the C standard, and there are slow and incremental improvements coming to that, so C++ will benefit from those too. The last remaining big C++ compiler vendor with a broken preprocessor is going to fix it next year, after which libraries such as Boost.VMD become MUCH more useful. So I'd expect to see lots more clever use of preprocessor in future C++.
&gt; first, last - pair of InputIterators that specify a UTF-8 encoded character sequence It appears it has to be a pair of character sequence iterators, which path::iterator is not.
The feature I would need/use the most would be an artificial scope with the possibility to leave using break. For example: scope { std::unique_lock&lt;std::mutex&gt; lk(some_mutex); if (planets_aligned) break; //do something while mutex is aquired } And yes, I know that I can make a function, with 10+ parameters, etc, etc. This would be way handier.
TL;DR, Elle is based on Boost, OpenSSL, zlib, curl, and a few other libraries which reinvented lots of stuff.
Agreed. I can't tell you how much debugging time labeled parameters would have saved. 
it's actually do { #pragma warning(suppress:4127) } while(0); and I hate this. When I look at some code containing do {} while () I should expect a loop, not an artificial construction to solve a specific yet frequent problem.
Can someone explain why this works? Where or how does the standard say that functions not yet defined can be referenced when they are member functions of a class? I've known about this trick for a long while, but never quite knew the reason, considering the forward declaration/single pass history of the language. 
Could you use a capturing lambda?
Did concepts make it in 17? Let constexpr be implicit so you don't have to sprinkle the keyword everywhere. Method call syntax on functions - think extension methods. Function call syntax for methods - sometimes code is more readable functional. Switch that can do type dispatch for a std::variant.
What are you doing that takes 50 arguments O_O?
At one point long ago, there was the idea floating around that in an if/else the positive cases were more likely to assumed to be true, meaning you could in theory give it a nudge in the right direction just by putting the common case first. No idea if that is still true, or if it was in fact ever true.. 
Plotting data often takes way too many options. Line color, line style, log scale or linear, x range, y range, connected or unconnected points, etc. There are reasonable defaults for every value, but every one is something that people might want to change.
How about you ask "Why don't you guys do a AMA (AUsA?) on reddit?"
So why don't you put them in a class having defaulted values? Then you send a single class instance or dict into the function.
Singletons are bad, and thus you should feel bad about your suggestion. :-)
&gt; Then you send a single class instance or dict into the function That's basically how Python named arguments work (i.e. with a dictionary).
I will try it, but it sounds promising. Thanks !
Ehh... CLion though. It has no "project files", it uses CMake and it has a pretty good editor. The only downside is that it's paid. On the other hand, I've never seen any problems on it that all the other IDEs have. And yes, I also use text editors (VSCode, Sublime and vim) so I know what I'm saying :P
That's not even remotely the same thing.
X-Post referenced from [/r/coding](http://np.reddit.com/r/coding) by /u/endersdouble [a heisenbug to share](http://np.reddit.com/r/coding/comments/5vvbys/a_heisenbug_to_share/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
&gt; If :) My feeling is that all those workshops are mostly feel good exercises and that true improvements come from working with expert coworkers and/or reading a lot of "boring and long" articles./books 
Test triven bevelopment? 
Why do you need hash function for symmetric encryption? auto encrypted_payload = symmetric_key.encipher( data, cryptography::Cipher::blowfish, // default: aes256 cryptography::Mode::cfb, // default: cbc cryptography::OneWay::sha512 // default: sha256 );
Thank you!
I think a lot of math functions are impossible to implement because they are required by standard to have some weird side-effects like setting errno and such.
Anyone wanna talk about c17 random memory corruption? I haven't heard of this
He's not really involved in C++ anymore. He's become the main D evangelist.
`client_threads` could be a vector of `std::shared_ptr` or `std::unique_ptr`. Using `new` in that case wouldn't be the nicest way to go about it, but it wouldn't be wrong.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/bashonubuntuonwindows] [\[x-post \/r\/cpp\] Compiling C++ with GCC on WSL using Visual Studio](https://np.reddit.com/r/bashonubuntuonwindows/comments/5w0gt9/xpost_rcpp_compiling_c_with_gcc_on_wsl_using/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
&gt; `return std::lexicographical_compare(data_, other.data_);` This won't compile (yes, I can always tell when people have written snippets that haven't been run through a compiler). The algorithm takes 4 iterators, not 2 containers. But in any event, this is silly - vector itself has operator&lt;() which uses lexicographical_compare(). As long as the vectors are of the same type, you should just use the operator. Same for operator==() being better than dual-range equal().
Yeah I heard that errno was a big stumbling block too. Seriously, errno needs to die. There are much better ways to do your error handling in C++.
#2: Oh wow I didn't know that works. Even seems okay in C++11! Thanks. #5: So why can't SSE intrinsics be constexpr? Am I missing something? #6: Ah so std::apply is what I was looking for. Welcome to C++17! :) 
Well that's sort of not the point though. If you can't actually call std::sin from a constexpr function, it greatly limits what you can do.
According to [this][1] they started around 2012, which makes sense; prior to the release of C++11 at the end of 2011, things in the C++ world had been fairly stagnant and there wasn't a whole lot of interest in improving C++ tools. So then C++11 comes out, there's a big spike in interest, and MS realizes that their compiler is in dire need of updating to deal with the new customer demands. I can easily imagine the project has taken this long simply because of what a huge effort is required to update a 30 year old compiler that doesn't even build complete ASTs. [1]: https://blogs.msdn.microsoft.com/vcblog/2015/09/25/rejuvenating-the-microsoft-cc-compiler/ 
Yes, yes it did. Single assignment, const everywhere on args and variables, function purity, use only those values you passed in args, avoid globals. Just what first comes to mind.
About 5, it would work for a separate function name where we provide a slow implementation and the existing name for fast runtime. A constexpr function has to work with the same body at both compile time and runtime, so using something like SSE wouldn't work for compile time and using a slower, naive approach that works at compile time would be a low-quality runtime implementation. Perhaps SSE has constexpr compiler hooks, I don't know, but compiler hooks like that aren't there for everybody. For 6, I'd recommend checking the cppreference implementation of apply. It's pretty small and straightforward if you use the index pack trick which has a standard library helper index_sequence_for in C++14 that would be appropriate here. 
That seems vaguely familiar. Guess I forgot about that limitation. It's also possible to split up the class on such a way that a smaller part could be specialized independently or provided as a policy (see policy-based design). 
Everything in moderation. FP has some good ideas but that doesn't mean you have to swallow the entire pill. Adopt what works and leave the rest. The real value of monads is that they give you a reasonable alternative to C++ exceptions. `future&lt;T&gt;`, for instance, is a great way of expressing task-based concurrency -- especially with the addition of `future::then()` (== monadic _bind_). If an exception occurs during the execution of the task, that exception is implicitly propagated through chain of `then` operations to the final `future.get()` call. The one somewhat unresolved issue is that the syntax around these objects can be clunky. You can write your own operators to clean it up a bit but it still leaves something to be desired. 
Well, that's why you add version checks in the build system and require version recent enough and have CI against the minimum and latest external libs that are available. And for your releases, you have a well defined version set in your build scripts and do a hermetic build from your build server.
And where the fuck is it?
"iloveyou.exe" won't sound suspicious at all
Wherever you want. She's docile.
MinGW Distro - nuwen.net - https://nuwen.net/mingw.html Compiler is **Windows x64-native**, and currently contains **GCC 6.3.0** and **Boost 1.63.0** 
[removed]
That's more about how the branch predictor in CPUs operates. When a branch is first encountered and the CPU has no information to help it make an informed prediction, it assumes that branches further down in the code are not taken. So if your compiler is not rearranging if/else clauses in the background (which older compilers do not do (and AFAIK modern compilers also do not do)), then you want to put the most likely case in the if. If the if/else is inside a loop, you only want to consider the first pass through the loop. Once the loop repeats, the CPU has additional information which it stored in the trace cache on the first pass through to make an actual prediction, and the if/else order is no longer relevant.
You know, all the work you make on modules will be 80% wasted unless they are portable between compilers and, in part, OS-es. Adoption of C++ among interested people suffers because there's no coherent ecosystem. Please take a look at Perl's PPM and CPAN. I *want* to be able to take a precompiled Linux package containing a C++ module for some library and use it with MSVC on Windows just by putting it in the right place in the filesystem, like with C# assemblies. Make this "just work" if the code uses only the standard library (so you also give incentive to people to adhere to the standard as much as possible), mark such modules as, say, "pure" (this would be a per-TU attribute; a module consisting only of pure TUs would be pure). Pure modules *must* be trivially transportable between platforms and compilers. For non-pure modules, make it possible that they have multiple implementations; a concrete one is automatically selected by the compiler if it exists in the module "package". Thus, non-pure modules will also be portable across OS-es and compilers as long as the code itself supports the platform and marks the code as being for platform X appropriately. Please put some kind of build-system and dependency tracking into the compiler, like Java has it. With modules you have the opportunity to kill make, cmake, autocrap and myriad of other half-useless crappy build system junk. Please make an effort and do it. [Yes, it'll be a slow death due to existing legacy code, but they'll be dead nevertheless eventually.] Yes, it requires thinking. Yes, it requires LTCG and abstracting away ABIs, calling conventions, etc. Major compilers (MSVC, LLVM and GCC) support this. Make it an optional component in the standard so that modern C++ is not held back by lazy vendors of braindead embedded compilers. Don't limit yourself just on improving compile-time performance. With multicore machines and build clusters, this is hardly a problem anymore, IMHO. Don't solve an outdated problem. (Yes, it'd be extremely nice if C++ had the compile speed of Borland Pascal, but binary compatibility across platforms and compilers is a problem that is several orders of magnitude as large.) 
There are three things that I have gotten out of exposure to functional programming for my C++ in specific and my programming in general. First and most importantly, is the emphasis on compositionality. Compositionality in this case means the ability to break big problems into small problems and not have the abstractions leak when you put together the solutions. People didn't get excited about pure functions and immutable state out of nowhere, they got excited because those make it easier build composable abstractions so it's easier to understand what their programs do and easier to build libraries for other people to use. [Parser combinators](http://theorangeduck.com/page/you-could-have-invented-parser-combinators) are an example of an explictly compositional approach to problem solving. Pursuing compositionality is a general principle which can guide solutions implemented in C++ as much as any language, but it was hard for me to think that way coming to C++ from C and Python. The second thing I got from Haskell was an understanding of how to use the tools provided for me in C++ to build composable solutions because I can think of how I would solve the problem using Haskell's tools or Haskell's problem solving approach and it frequently makes it easier to understand what sort of interface I want in my C++ or how I should think about problems. The conceptual focus of Haskell made it easier for me to see how to solve problems with aggressively compositional pure functional programming and also when that approach was awkward. It's hard to really push for that type of insight in more multi-paradigm languages because there is always an out and often multi-paradigm languages only have an optimized implementation of part of each paradigm. For instance, infinite recursion is efficient in Haskell where it didn't need to be in C++ because you can use a `while(true)` loop instead if you want to write a server. Finally, functional programming languages are often where research ideas are implemented first, so you can understand the core ideas behind C++ libraries and language features without being distracted by how they are implemented in C++. For me, I didn't really understand std::accumulate (or most of &lt;algorithm&gt; really) until I encountered foldl in Haskell and could see it without the baggage of the C++ iterator abstraction. In addition, playing with Adga made template meta-programming a lot easier for me because I could think about type-level programming in the abstract and then think about the implementation in terms of C++ template tricks once I knew what I wanted to write. Ubiquitous laziness also really expanded my algorithm toolkit in terms of generating infinite data structures and writing streaming algorithms. I wouldn't have thought about them if I hadn't left C++ because I wouldn't have understood the pay-off for building those abstractions so I just would solve the problem a different way. TL;DR: I think functional programming is valuable for C++ because I think understanding compositional design is important for programming in general and FP does it well, working in an explicitly FP language makes it easier to understand the FP in multi-paradigm languages, and there are lots of neat ideas in FP languages which you can bring back to C++ once you've grokked them in a language more suited to them.
[removed]
I won't argue with you. That is the right way to do it. But frankly, I've never seen a single company doing it right in *all* points. Might be that all companies I've seen so far suck. Might be that setting up a perfect CI with perfectly reproducible builds and testing these builds on different platforms costs more than occasional bugs on or rebuilds for some platform. But we're digressing very much now. 
&gt;doesn't the elimination of state mean that you get less performance? Generally yes. &gt;Passing all the parameters all the time takes more instructions than having them in my object's state... Functional progamming has types (equivalent to structs without methods). So mostly you are passing around types(references to structs). The problem arises when you mutable types would be a real advantage: An infamous example is the haskell quicksort: qsort [] = [] qsort (x:xs) = qsort kleinergl ++ [x] ++ qsort groesser where kleinergl = [y | y &lt;- xs, y &lt;= x] groesser = [y | y &lt;- xs, y &gt; x] qsort [4,1,3,8,2] (x:xs) splits implicitly the list into its first element (x) and the rest (xs). The where clause defines two lists : one with all elements(of the rest) less or equal than the first and one with all elements larger. Qsort returns the sorted lower list concatenated with the pivot element and the sorted larger list. (and qsort on an empty list returns an empty list.) Sadly [y | y &lt;- xs, y &lt;= x] is AFAIK defined as all elements of xs smaller or equal to x in the order they occur in the input list. That means they have to be collected into a new list. And the qsort is no longer in place and has to copy every element multiple times (on the other hand it is easily made stable). Many functional languages cache implicitly (memoization). But you as programmer hardly have any control over what is cached and when the cache is invalidated. You can work around any of this by using mutable types(often in the form of monads) but it is hard. Another problem is the passing around of small functions. Let's say you have written your own map implementation and you call it (trivial example): std::function&lt;int(int)&gt; add2 =[](int x){return x+2}; map( add2, my_array ); Now if you have defined map in the same file (or an included header) the compiler can probably inline (and even vectorize) the add2. But what if you map was defined in a different file or the actual action depends on user input. Suddenly the inlining by the compiler is not so obvious anymore. TL;DR: pure functional programming doesn't give you much control over performance. 
Install eclipse. go to the menu bar and seach for a thingy that lets you install packages (probably under the windows tab). install the cpp package. But you seem like you want to learn c++ so I recommend CLion or visual studio or another C++ oriented IDE. Eclipse isn't that good for c++. Also there is a [eclipse forum](http://www.eclipse.org/forums/) and a [subreddit](https://www.reddit.com/r/eclipse/) with a lot of activity for a subreddit with only 800 subs.
Yea I'm learning c++, is the free version of vs good?
It's got way more features than you'll need as a beginner and it's an excellent IDE. But I would still recommend you learn some kind of build system like Make and also to learn Git (which is integrated in VS). You might need to work with stuff like that. 
I don't think you need to "install" eclipse. I haven't used it in a while but as far as I remember you can just run it from the download directory... Some worth mentioning alternatives to eclipse are -Visual Studio (Only interesting if you use Windows. Don't confuse it with Visual Studio Code which is an entirely different software), -QtCreator (one of the most popular IDEs), -CLion (fairly new IDE, you can get it for free if you are a student) -KDevelop -vim with plugins, and many many more... 
&gt;But I would still recommend you learn some kind of build system like Make Forget Make and go straight for CMake! Learning how to write Makefiles manually is wasted time. It's good to know roughly what a Makefile looks like and to know how compiler and linker are called, but therefore you can just look at the output of any cmake generated project. Have a look at some [popular open source projects on github](https://github.com/trending/c++) to see how they do it. For example [google test](https://github.com/google/googletest) and [OpenCV](https://github.com/opencv/opencv) are well known projects which use CMake as their build system.
I just downloaded CLion didnt know they offer free for students, thx for the info
Yeah, but +1 for the shout-out to Boost.Algorithm. :-)
&gt; Locales that have a different character than . as decimal separator (e.g., the Norwegian locale nb_NO.UTF-8) led to truncated number parsing or parse errors. The parser now has been fixed to work with any locale Noooooo. If it's not a "." then it should be errors. Human friendly representation &amp; stored data should not be mixed this way. I don't want to deal with the mess csv has. It should be an error to use anything other than a "." for a decimal separator when saving data.
Ohh I see, you force "." as the only valid decimal separator for serialization &amp; parsing regardless of locale. Yes this is good. Maybe instead of saying "works with any locale" it's better to say 'ignores the locale and always uses "."'. Edit: I found the commit [Added locale-independent numtostr](https://github.com/nlohmann/json/commit/21cae35930a734fa726b1dc7cc73365b2b6c41b2). Yay everything looks good :). Edit 2: Never mind "ignores" is not a good word for it. As it doesn't ignore it but considers it. It converts using snprintf to string then checks the locale and normalizes to use ".". English is hard it's ok.
Hey this is great! I will definitely start using it. You mention repacking into new types to create type safety. Do you use something like BOOST_STRONG_TYPEDEF or http://foonathan.net/doc/type_safe/ ? Or do you do it yourself? If so, do you have any simple examples? I'm not good enough C++ programmer so I always end up worrying about creating overhead :(
&gt; Visa Sponsorship: No. Why? I was always under the impression US and UK are the ones with complicated work permit. 
Not if the outer scope needs to return something other than `void`. You'd have to have the lambda return an `optional&lt;T&gt;`. And then you have an immediately invoked lambda that returns an object just so you can check it to potentially return another object? Not a good solution.
Wouldn’t it be cool to have JSON parsing in C++ standard library... Haven though of suggesting this idea to a C++ standard committee?
You will probably have better success on /r/cscareerquestions
Excellent work. But does this library allow comments in the .json file? I was hoping that I could put some special parsing directives into comments so that I could destroy interoperability.
Ok thank you
Dude, I like you from the very first look at project's README! This world needs more people like you!
How does this compare to jsoncpp?
I love the lib, used it in many projects of mine. Thank you for it. 
Largely correct, except for: &gt; Passing in an rvalue results in T = std::string&amp;&amp;. No, it's just T = std::string. Use is_same to verify.
It really is one hell of a README 😏
Don't learn FP. Learn category theory. That's the crux of Haskell and what it's based on. Learning category theory isn't useful in and of itself but it's hard, weird and expands your perception on things.
It went both ways for me. I first learned C++ long before I ever toyed with a functional language (or even knew what that meant). The STL and function objects was my first taste of the basic idioms and Alexandrescu's Modern C++ Design was my first deep exposure to pure functional programming via template metaprogramming. When I started learning Haskell, I had the initial impression, "This is like C++, but slightly less powerful and way better syntax!" When I finally grocked monads, I really started to see how powerful Haskell's generics coupled with type deduction really were, and wanted more of those features in C++. In C++, its sometimes useful to be able to use SFINAE and the actual templated type, and I wish Haskell had something like that as an escape hatch, but even more, I'm looking forward to the day when I can write something like Haskell's `return` as simply in C++ as I can in Haskell.
it also supports cbor and messagepack? Wonderful.
Can it be? A third party library that actually follows the naming conventions of the standard library?
Oh, but why? If you have so many arguments that you forget what they're used for, put them in a struct and pass an instance of it to the function.
Well, it's a common practice, but it's not required. You're right, it serves no purpose in this post and just makes things confusing. I'll keep this in mind for my next post.
No, but I like it :-)
&gt; F and B =&gt; forward/backward That's correct.
I have been asked this question before, but I am not sure whether it would be worth the work to write a proposal. The whole STL consists of rather short and very specialized functions - a JSON library would feel weird. However, feel free to convince me otherwise ;-)
I have no idea. Maybe /u/agopshi can help, see https://www.reddit.com/r/cpp/comments/5w52tq/json_for_modern_c_version_211_released/de7ugge/
yes they are, I don't post it everywhere just where people might find it useful. And even the title is no way misleading. People who would really want to learn it would watch them. Is there anything wrong ? 
I once encountered such an [example](https://github.com/Oberon00/apollo/blob/9a56a512e58215c23315abf8425219a8a9dfe080/include/apollo/builtin_types.hpp#L203-L219): Given: template &lt;std::size_t N&gt; void f(char const (&amp;s)[N]) { puts("#1"); } void f(char const* s) { puts("#2"); } Which overload does `f("foo")` call? Answer: The non-template #2! Array-to-pointer decay just has so little influence on overload resolution. So how do you solve this so that #1 is called whenever possible? You need to make #2 a template too. Maybe like this: template &lt;typename T&gt; std::enable_if_t&lt;std::is_same&lt; char, std::remove_const_t&lt;T&gt; &gt;::value&gt; f(T const*) { puts("#2"); } But now it is ambigous! The solution in this case is to remove the `const*` from the template: template &lt;typename T&gt; std::enable_if_t&lt; std::is_convertible&lt;T, char const*&gt;::value&gt; f(T) { puts("#2"); } Only now does f("abc") call overload #1. If anyone knows a simpler solution, I would be glad!
Very good point, thanks! Fixed: https://github.com/nlohmann/json/commit/f1cd15ce7e66518007101e73bb91c87b27c13512
Instead of carrying your general purpose class you can make a specific struct or a class just for this chain of functions, without any member function code just fields. It will give names to your values and will allow you to carry state between them. People do this in C often, we erlangers do this all the time in Erlang. I've heard stories of arity 18 functions, those should be eliminated.
I'll accept most of the issues here as something is wrong with the installation, or something is up with your hardware/drivers, but the "and the telemetry" bit really irks me. What problems do you have against anonymous usage data, rather transparent ones at that (as in Microsoft tells you that it's collecting data, not silently)
About weirdness... maybe... but JSON parsing feels like kinda "must-have" utility, which other languages have built-in. I believe Stroustrup himself was articulating that C++ standard library is indeed very small compared to Java and others (I believe there was his talk with specific slide with chart comparing standard libraries, but I don't recall on which talk was it exactly). We have Filesystem, Networking upcoming, and even maybe basic graphics! Also, there was study group about Databases (sadly, _was_)... I do not know why JSON couldn't be yet another library in standard C++ toolbox. With Filesystem and Networking, it would nicely fit in, knowing JSON use cases... :) . EDIT: Also, I just remembered that C++ regex support is taken from EcmaScript standard, so JSON wouldn't be a lone stranger :-) . EDIT2: Here's link to the talk mentioning EcmaScript &amp; C++ standard: https://youtu.be/22jIHfvelZk?t=754
Stepanov too: he retired last year.
Too bad the syntax is fugly. I wouldn't write this monstrosity unless it really mattered: class Foo { public: std::string member; std::string member2; template&lt; typename T, // Parameter 1. typename U, // Parameter 2. // Template type checking. typename = typename std::enable_if&lt; // Check type of parameter 1. std::is_constructible&lt;std::string, T&gt;::value &amp;&amp; // Check type of parameter 2. std::is_constructible&lt;std::string, U&gt;::value&gt;::type&gt; Foo(T&amp;&amp; member, U&amp;&amp; member2): member{std::forward&lt;T&gt;(member)}, member2{std::forward&lt;U&gt;(member2)} {} }; If I saw this in my code, I would remove it, trivial efficiency be damned.
What advantages does this offer over lambdas? double a = 5; double b = 15; double avg = (a + b) / 2; could just be double a = 5; double b = 15; auto avg = [&amp;a, &amp;b]() { return (a + b) / 2; } Interesting library, nonetheless!
The syntax isn't that bad, ignoring the `enable_if` template&lt;class T, class U&gt; Foo(T&amp;&amp; member, U&amp;&amp; member2): member{std::forward&lt;T&gt;(member)}, member2{std::forward&lt;U&gt;(member2)} {} Although here it seems better to do Foo(std::string s1, std::string s2) : member(std::move(s1)), member2(std::move(s2)) {}
Surely the rules must be able to resolve `X&lt;T&gt;` without reference to the definition of `X`, or explicit specializations would create causality loops.
Via the `CXX` variable, which defaults to `g++`, but you can set it to whatever you want.
Basically, JsonCpp is exactly what you would expect a modern C++ JSON library to be. And then, nlohmann's "JSON for Modern C++" comes into the picture and blows your mind in regards to what you expected a modern C++ JSON library to be.
Ooh nice, didn't know it did that. Definitely using that from now on.
I think that's what I was looking for!
Do you know what reasoning MSVC takes to conclude that this case was ambiguous: template &lt;class T&gt; void foo(T, T); // #1 template &lt;class T&gt; void foo(T, identity_t&lt;T&gt; ); // #2 foo(0, 0); It's the only divergence I saw and I'm curious as to what algorithm MSVC actually performs. 
Very neat. Reminds of the type of thing that is commonly done in JS UI programming, and it's awesome to see it done in C++.
Agreed, it's a good question. But I don't think transform is more code - it's one line (or two if the lambda is larger and you wrap it to the next line). The for-loop is 2 or 3 lines (depending on your brace-formatting). std::transform will get an execution_policy in c++17, which should allow for easy parallel execution. That would be one advantage :)
OP mentioned less points because of `for` loops (this includes ranged by definition). To get a solution (and all the points) without loops was my interpretation. As a solution in general, using `&lt;algorithm&gt;` or even the parallel algorithms of C++17, could put library and compiler writers at an advantage of optimization. For this particular problem, ranged for loops can be optimized, of course, but this is a rather simple problem, not terribly common in in practice. Regarding the complexity, for example the cyclomatic complexity, the for loop would increase it, while `std::transform` doesn't. If in doubt, measure (or at least look at the generated assembly).
True, but this is a informatics olympiad, and they specifically make their test cases so that the lower complexity algorithm beats the adhoc solution. In this case, for n=100,000, O(log n) is way faster than a auto-vectorized O(n). Checkout [Problem C](http://2011.nwerc.eu/results/nwerc2011-problemset_2.pdf). It requires Fenwick trees, or you will run out of time.
&gt; True, but this is a informatics olympiad, and they specifically make their test cases so that the lower complexity algorithm beats the adhoc solution. Informatics olympiads should be marked as "considered harmful" then.
Are you sure about the speed? I'd be inclined to bet that even for n=100,000, std::vector would still be faster?
I've started the library mostly to help with C++ UI programming. I'm happy you like it!
Yes, indeed there are. But existing wheels have never stopped people from reinventing them :). 
They should atleast be taken with a gulp of salt. ☺
Clearly a typo: author intended to use `std::experimental::ranges::lexicographical_compare`.
Try it and see. On the linked page you can also find the input data used to validate the programs.
This is true! Don't write constructors you don't need to write! :)
This is awesome. Learn something new every day! Thanks.
How does it compare to RapidJson?
RapidJSON is definitely faster, but my library focusses more on a friendly and intuitive API. When you know `std::map` and `std::vector`, you should be able to program with this.
Thanks for the link. Didn't know about the fenwick trees. Learned something new today, yay!
VS just doesn't have the facilities to apply our company's code style, and I haven't found an extension that can do it either, so I just manually format it like everyone else.
Code::blocks... anyone?
how many uses does the critical section get? is it only for getting the message? but then the work is done outside the critical section? 
Random distributions will have fewer collisions while general purpose data will have more due to the fact that general purpose data tends to be very similar. I think.
Actually just wrote something similar. Great minds, right? XD Out of curiosity, any reason you went with an observe method rather than just overloading the mathematic operators to make the whole equation a "value"? Edit: Why the downvotes? I was just asking a question. 
Well I haven't profiled it or anything yet but I can see it's used in a lot of places. Not just for the messages being passed around. But also in various interfaces. It's a multithreaded program (up to 20-40 threads) and only has this single global critical section.. I'm thinking this must be a bad practice.
Is it common for large programs to have only a single global critical section in the whole application? No other critical sections/mutexes/semaphores are used in the entire program. Anywhere that data needs to be accessed in a multi-threaded way relies on that global critical section. Seems wrong to me..
A log(n) algorithm requires somewhere on the order of 17 accesses instead of 100000...
That's a good question. I actually spent a bit of time thinking about this. The main reason was to be able to overload it so users can pass custom updaters. You can do this: observe(my_updater, a + b); my_updater.update_all(); This way you'll control when the values are recomputed and from which thread. Plus you can do v1 = observe(v2); with the same syntax.
https://reddit.zendesk.com/hc/en-us/articles/204536499-What-constitutes-spam-Am-I-a-spammer-
I have though about implementing something like this before. To me it is similar to how hardware works and how one would think about programming in Verilog(or VHDL).
every random access is a potential cache miss, and that costs lots. 
This can make the code simpler and safer, but at the expense of (much) more thread contention, hence worse performances. This somehow defeats the primary purpose of multitasking, which is to make things faster... IOW, it all depends on your objectives. 
It's monads all the way down. :-)
For me writing a few programs in Clojure/ClojureScript really helped to understand just how good those practices are, and apply them a lot more consistently in my C++ code now. I learned to be much more observant about little side-effects like writing to member variables. But also the bigger picture, like trying really hard to put I/O in a tiny corner of an application where it can never be called by accident.
[removed]
It's not clear to me why he doesn't just grow the table at 50% load, given that he has experimentally determined that reallocation due to max element displacement is "almost never" triggered under 50%. Why keep that possible attack vector when just reallocating at 50% would be consistently performant?
The purpose of mutexes (critical sections) is to protect shared resources. Usually one mutex protects one resource. You say in your app there is a messages queue protected by one mutex. It's pefectly fine to have only one mutex protecting that one resource, regardless of application size. If however there are many different not related objects, all protected by the same mutex, then this maybe not so good but "it depends", see other comments. On one side having only one mutex eliminates dead locks, on other side it can be bad for performance. You have to analyze what happens during each lock, the amount of work done must be as small as possible and there should be no IO there (that would kill performance).
Not sure, but integers can hash perfectly — it's not hard to get a 1:1 hash function because the hash value has as many bits as the integer it came from. Data structures with more information content will always have hash collisions, which is kryptonite for simple structures that don't double-check actual values.
I also did something similar, 4 years ago: https://woboq.com/blog/property-bindings-in-cpp.html
Because max collision assumption lets the search function be missing certain branches -- the "look for the element code" is faster. There is a paragraph on it.
You are right, this example is bad but we didn't want to put to many new notions in the example. In real-life code, we use `elle::With&lt;elle::reactor::Scope&gt;`. 
The Wikipedia entry for [Fenwick tree](https://en.wikipedia.org/wiki/Fenwick_tree) links to [Prefix sums](https://en.wikipedia.org/wiki/Prefix_sum) entry which again mention the C++ function [std::partial_sum](http://en.cppreference.com/w/cpp/algorithm/partial_sum) which may be of interest.
Our build system ([Drake](https://github.com/infinit/drake)) takes care of everything for you (downloading, building, giving the compiler the right paths). Why? 1. We like self contained environments. 2. Some need patches (see the `.patch` files) to compile.
Because in the rare case that it would have been triggered before 50%, the "I don't need a bounds check on lookup" optimisation would be wrong.
(And in the "it's safe to increase the max_load_factor to 0.9 to save memory while only suffering a small decrease in speed" case, that failure would no longer even be "almost never".)
I appreciate no-nonsense Github repositories, but that's a bit too terse to my taste. There is not a single example. I don't even know the name of the class. I don't need fancy stuff, just something along these lines to test things ska::map&lt;int, int&gt; myMap; // What's the actual name? myMap[1] = 2; // Is the [] operator actually overloaded? myMap[3] = 7; std::cout &lt;&lt; "Retrieval " &lt;&lt; myMap[1] &lt;&lt; std::endl; 
Not introducing `With` and `Scope` makes sense, but I so no reason not to use `make_shared`, even if this is "just an example".
Good point, &lt;const X&gt; would make more sense in a lot of cases including this one. Maybe the name is bad because the value isn't shared; what's shared is the responsibility for disposing the original value. But shared_ptr's aliasing constructor does allow for a bunch of different pointers to exist that share responsibility for deletion of an original pointer.
Personally I would prefer to distribute the transform over multiple lines as well, and those lines are a lot longer (realistically the transform takes like twice the characters) thus take a lot more reading. The execution_policy is very interesting!
I disagree. A professional C++ developer should know 90%+ of those things, in my opinion.
The main difference from others is that it contains (pseudo) c++ code samples for each pattern instead of uml or textual description. Wrote it for myself, but it may be useful for someone else.
Something similar [was proposed for standardization](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3949.pdf) a couple of years ago under name `unique_resource` (but not the shared alternative).
Gotcha. A bounds check is generally very cheap, though. So it seems this is a tradeoff between "allow a catastrophic mode of failure" and "incur a minor cost per lookup". It seems to me that's optimizing a bit far &amp;ndash; like taking airbags out of a car because it can go slightly faster with less weight.
build2 using sha256 is good to know. TFA though mentions hearing "the news" (without a reference to any), and the article is not dated. It will probably be reposted the next time SHA appears in "the news". :)
&gt; It will probably be reposted the next time SHA appears in "the news". :) Haven't thought of this but now that you've mentioned it, I like the idea ;-) BTW, if you are wondering what will happen when SHA256 is considered insecure, check [this discussion](https://www.reddit.com/r/programming/comments/5wgdes/and_this_is_why_build2_package_manager_uses_sha256/de9u55d/).
Don't you have to construct the tree first? And these 17 accesses will not be in contiguous memory locations so each of them, in worst case, might be a cache miss, and a slow fetch from memory. Any way you are probably right but I wouldn't make an argument either way without measuring. :-)
&gt; I am sure most of you have heard the news by now. I haven't, and Google didn't reveal anything. Care to enlighten me? Thanks! _Edit: Second round of googling revealed you are probably referring to this: https://www.eff.org/deeplinks/2017/02/cryptographers-demonstrate-collision-popular-sha-1-algorithm_ And here the original blog post from Google: https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html
What didn't compile on msvc?
Holy crap, this is worthless and pure garbage. And the person absolutely hates spaces in their code. I'll summarize these tricks: 1) Proper use of constructors. `int i; i=4;` replaced by `int i=4;` 2) Use of in place math operators. `int i=10; i=i-6;` replaced by `int i=10; i-=6;` 3) Use of the ternary operator. If we have `int ans,a=13,b=15;` and we want `ans` to hold the max of `a` and `b` then we can write `ans=a&gt;b?a:b;` instead of `if(a&gt;b){ans=a;}else{ans=b;}`.
I made a BigInteger class for c++ and I thought I'd save y'all the time if you were about to make one for yourself
When I've implemented lazy iterators in C++, I've just left out post increment. I don't really care if it's wrong, insofar as the user will get a compile time error. It's just painful to implement, slow, and really post increment should never be used with generic code. Ranged base for loops use pre-increment, as you would expect.
It's not necessarily a bad practice, and it comes with a very big benefit of **simplicity**. One would of course suggest eliminating the threads instead, but on Windows sometimes that's hard to do. Lots of things have used giant locks successfully. The CPython runtime is a great example. Another is the FreeBSD kernel. As another poster mentioned, it all depends on frequency of use and length of time held.
There is a python itertool implementation in c++ [cppitertools](https://github.com/ryanhaining/cppitertools). With a range function :)
+1 for using a sane naming scheme.
Yes, we are focusing on getting the build system to the `1.0.0` state. Once that is done, packaging will follow.
[removed]
Maybe there is a more compact way to call transform that I'm not aware of, but here's what I come up with: std::transform(arr.begin(), arr.end(), arr.begin(), [](int e){ return ++e; }); vs for(auto &amp; e : arr){ e++; } Disclaimer: I haven't actually tested either of these, maybe I made a mistake. Also in fairness, it doesn't make much of a difference and I would consider both absolutely fine, but if I had to pick one...
Nice. Out of scope question: why require cmake &gt; 3? All my stable debian and ubuntu are still &lt; 3 and you're not the only one upgrading cmake minimal version. NB: I'm an autoconf/automake guy.
&gt;/ packages / jessie (stable) / devel / cmake &gt;Package: cmake (3.0.2-1+deb8u1) And [Ubuntu](http://packages.ubuntu.com/search?keywords=cmake) have cmake &gt;=3 since 16.04.
In a bunch of the functions, I have commented out a print statement that describes the error a bit. I though that was sort of bad practice, and I also hear that using exit() is a big no no. I didn't really know how to throw an error from an operator overload, so I just left it to the user. Any ideas on what would work/look best?
&amp;&amp; isn't bitwise. It's boolean. 4 and 8 are both "true". 
&amp; is a bitwise AND. Walk through every bit, set it to 1 if both are 1s or 0 if anything else. &amp;&amp; is a boolean AND. Boolean expressions are either 1 or 0, or rather on or off. A number is either zero or it is not zero. 4 is not zero, 8 is not zero. 4 &amp;&amp; 8 == on &amp;&amp; on == true &amp;&amp; true == 1 &amp;&amp; 1 = true = 1
Before you get told off by a moderator, as it says in the sidebar: &gt; For C++ questions, answers, help, and advice see r/cpp_questions or StackOverflow. But to turn this into something that *is* appropriate for /r/cpp: should compilers warn when performing boolean operations on non-`bool` types? GCC and Clang seem not to even with `-Wall` -- though Clang does warn if both operands are compile-time constants. I guess it might lead to a lot of warnings with C code, but it seems like it would also catch a lot of bugs (I use `&amp;&amp;` a lot more frequently than bitwise &amp;, so my fingers reflexively type the double-ampersand...) 
[removed]
I want this as a poster!
Not a good point. Conan.io lacks packages and package maintainers (like biicode). You are making the same mistake all over again.
https://github.com/stevenBorisko/BigInteger/blob/master/BigInteger.hpp#L51 Why have file writing logic in this class? Why not just provide an overload of the stream insertion operator so I can print out a BigInteger to whatever I want? https://github.com/stevenBorisko/BigInteger/blob/master/BigInteger.hpp#L53 Prints to where? Is there a magic file name stored somewhere? Looking at [this file](https://github.com/stevenBorisko/BigInteger/blob/master/Functions/PrintFunctions.cpp) I see that you print to stdout. Interesting, but again I'd much rather you provide a stream insertion operator (that responds to things like `std::ios::hex` and `std::ios::dec`) so I can print this wherever I want