&gt; What are the impediments to doing this currently? No idea, sorry. I would guess at a minimum that there's code out there assuming 2:1 maximum encoding unit growth UTF-16-&gt;Legacy that would have to be fixed.
I wonder if this approach would trigger some antivirus software. 
It has some built-in search paths though, predictably, this blows up for cross-compilation. So in `build2` we search for `.pc` files ourselves. This is actually pretty easy since by convention they are installed next to the library in the `pkgconfig` subdirectory. So if you can find the library itself, then you shouldn't have much difficulty finding its `.pc`. Now finding the library in a cross-compilation-clean way is a whole different story: you cannot use any built-in search paths and instead should extract them from the compiler. Which is what we do in `build2`.
&gt; the committee thought that std::string could mean utf-8 so std::u8string wasn't needed. Hmm... really? Did someone write up a proposal for this and get that response? I think a much bigger problem is that none of the basic_string member functions do the right in the presence of UTF-8. &gt; The fact that path(std::string) expects multibyte and u8path(std::string) expects utf-8 is rather mind-boggling to me. The design goal is that if today you have: ifstream is("filename.txt") and you change this to: ifstream is(path("filename.txt")) the behavior of the program does not change. This lets users have something that today does: X open_thing(const char *); and replace that with an updated API X open_thing(path); without needing to audit every caller to ensure they're doing the right thing. That means the ctor needs to be multibyte for char. I'm sorry that you find that surprising but it's the only behavior consistent with the rest of the standard library. &gt; when I see path::u8string() and path::u16string(), I assume that I can pass either to the ctor The assumption that path -&gt; string -&gt; path is information preserving does not hold on all platforms. It will hold for POSIX, where the native grammar and the generic grammar are effectively the same, and it will hold for Windows, where the native to generic transform is not information preserving (discards alternate data stream references), but it won't necessarily hold on more exotic platforms like z/OS. Unfortunately I don't know enough about such platforms to describe what will happen there but folks representing them said they couldn't have the generic -&gt; native transform be required to be information preserving.
To get any meaningful measurements you will need to modularize some real-world, non-trivial projects. And this is, how shall I put it, not exactly straightforward with the current state of implementations. We have started this work on `build2` itself but quickly ran into bugs/unimplemented areas in both Clang and VC. It seems the most promising way forward is "total modularization" without any `#include`'s, which, for any real project is not an easy task. But we will try again once the current round of patches/bugs we have submitted get accepted/resolved. Also note that while there were some performance numbers published/mentioned for "Clang Modules", I don't think they will necessary translate to Modules TS since Clang Modules are quite different semantically.
Note that the current MSVC++ behavior is a bug. Some of that is because this part of spec churned quite a bit between our implementation and now. This and many other bugs are going to need to be fixed before we move our implementation into std. Before we move this into std, on Windows, conversion from generic -&gt; native is a no-op. Conversion from native -&gt; generic needs to discard alternate data stream references and replace each sequence of *directory-separator* with /.
On what platform do you want to access things relative to the executable path? On Windows executable are typically in Program Files where user data better not be stored, and on POSIX executables are typically in /usr/bin where again user data had better not be stored. It being possible to append a component to a path is effectively a requirement on operating systems that want to support this library -- `current_path() / "abc"` has to work.
I see, so my plans to try a real project will help there. However I fear having to modularize dependencies. Would you say there are easy steps to do so? Like creating a module file just including all headers of a library, exporting them, then adding another file for the implementation like a cpp? Or do you mean that benefits will come only with code designed/organized with modules in mind from the beginning?
&gt; On what platform do you want to access things relative to the executable path All? If I need to read a file that I expect is sitting next to the executable, then I want the executable's path, not the current directory. This has nothing to do with user data. &gt; `current_path() / "abc"` has to work. Sure, but it doesn't make the path "cross-platform", which is that I was answering to.
The examples in the article is based on an (early) implementation rather that the capability of concepts, and it only picks examples that doesn't favor concepts. The author didn't even dare to mention that without concept we can only rely on enable_if, tag dispatch, and other hacks. Update: downvote me to oblivion as you like. I don't care. Oh btw, after the paper, Concepts TS was voted to be part of C++2x now, in case you don't know.
&gt; Did someone write up a proposal for this and get that response? Don't know, don't care. All I know is that we have `u16string` and `u32string` but no `u8string`, so `string` is ambiguous: could be utf-8, multibyte or arbitrary binary data. Which is fine, until all three start being used in the same API and then you have no idea what `string` should really represent. So `path()` interprets `string` as multibyte and `u8path()` as utf-8. We're basically in the same boat as with `wchar_t` and `wstring`, which specifies _character size_ but not _encoding_.
&gt; it makes using templates easier but doesn't do anything to help write them Being able to write `void f(Arg w, Args... f)` instead of `template&lt;typename Arg, typename... Args&gt; void f(Arg w, Args... f)` certainly did make writing templates easier when I tried concepts, but it seems that people in the commitee have incentive to collude with carpal tunnel syndrome medication providers. 
Did you consider using the "noexcept" operator to detect if an invocation is constexpr? (As mentioned here: http://en.cppreference.com/w/cpp/language/constexpr) 
&gt; The examples in the article is based on an (early) implementation rather that the capability of concepts, As far as I'm aware, he picked the most modern compiler that supports concepts. Can't do better than that. But sure, it could of course get better in the future. &gt; and it only picks examples that doesn't favor concepts. `sort` has been *the* canonical example for Concepts for years. It wasn't cherry picked to fail, it was chosen because it was presented as the model. &gt; The author didn't even dare to mention that without concept we can only rely on enable_if, tag dispatch, and other hacks That's simply untrue. The paper does mention the alternatives. 
It can definitely be used for non-system-installed libraries. I use it within a large project to easily pull in libraries out of the 2 dozen we have in the project. 
Hmm I see. So basically this sounds very similar to CMake's `libname-config.cmake` files and you have to deal with all the same problems. &gt; you cannot use any built-in search paths and instead should extract them from the compiler. Which is what we do in build2 I can't imagine how you do this - isn't this the chicken-egg problem - you're searching for the library to give the compiler the correct include and linker paths in the first place, so how can you use the compiler to _find_ the library?
Can you give an example that does improve diagnostics considerably?
IDK https://stackoverflow.com/questions/25617610/some-containers-in-stl-dont-have-find-function
 What kind of crazy ass shit is this math lib doing? I use SIMD code &amp; templates all over and don't think I have any files that take more than 2 or 3 seconds
What I claim(again without benchmarks do not trust me) is that unordered(sorry for not mentioning that) flat map will need to be entirely loaded into L1 cache when the lookup fails (because you need to go through entire map) For hash map you only need to touch a little bit of hash map. When you microbenchmark just the maps this is not a problem since entire map fits into cache, problem is if (again theoretical since no benchmarks == no credibility) in real code this slows down other parts of your program since your unordered flat map uses a lot of cache. 
I tried playing around with this approach before and couldn't get it to reliably run. I was able to get it to run under gcc but it required passing a known variable (ie you'd have to do something like in_constexpr(some variable used in your code)). [See here](https://wandbox.org/permlink/asU1BLdKcqqWsWZX) I could not get it to work when I tried to make it use a temporary variable though. [See here](https://wandbox.org/permlink/Nn6fzjNEudnRfx8a) I feel like has to be a way to use noexcept but I couldn't figure it out so far.
And I understand that... but what I'm saying is that, even if the benchmarks are indeed showing that unordered map is worst (which in a real case it's likely not), we should (maybe) take into consideration a formula between the benchmark and the O complexity. In this case for example let's say unorderd-flat-map beats unorder-map for lookup... but we know unorderd-flat-map lookup is theoretically O(n) and unorder-map lookup is theoretically O(1) on average. So those numbers already tell us the flat map will obviously be much more cache greedy. So I'm not saying your claims are wrong... on the contrary, I'm asking if there wouldn't be a way to also formally prove them by using the Big O complexity when evaluating said micro benchmarks :P
&gt;&gt; The examples in the article is based on an (early) implementation rather that the capability of concepts, &gt; As far as I'm aware, he picked the most modern compiler that supports concepts. Can't do better than that. But sure, it could of course get better in the future. Given that this implementation is not even up-to-date with the latest Concepts TS that was merged into C++2x working draft, there can be a lot to be desired. &gt;&gt; and it only picks examples that doesn't favor concepts. &gt; sort has been the canonical example for Concepts for years. It wasn't cherry picked to fail, it was chosen because it was presented as the model. Which as you mentioned can be fixed by better implementation. &gt;&gt; The author didn't even dare to mention that without concept we can only rely on enable_if, tag dispatch, and other hacks &gt; That's simply untrue. The paper does mention the alternatives. I did miss that part. If the author could give an excerpt from the current range-v3 implementation that was based on enable_if then I wouldn't miss that. /s I don't deny that we already have sufficient tools for generic programming. Most features of Concepts can be emulated by existing features. But I think the majority won't like them. EDIT: format and typo.
If something like AFIO (async file and filesystem) gets standardised in the future, we use the race free path design pattern throughout, thus allowing paths to randomly change and you not to get surprised by deleting the wrong file etc. Beman wanted to do the same for the Filesystem TS, but at the time two major OSs didn't implement the race file syscall extensions, and besides there was *considerable* disagreement as how best to interact with file i/o. The Filesystem TS as you see it today is not what anything would have wanted ideally speaking, and it's considerably pared down from what had been hoped for. But it's plenty better than nothing, and it'll do just fine until an iostreams v2 replacement starts to happen. First we need Concepts and Ranges finished. AFIO as one of the first bottom most layers will be ready for standards to consider by then too. We'll get there. https://ned14.github.io/afio/index.html
The Filesystem TS has a notion of a "generic path" and a "system path". The generic path, like the generic error category, is the POSIX form as POSIX is the portable OS spec after all. So always working in POSIX paths is safe, and will work portably. Conversion to system paths ought to happen automatically and on demand. AFIO is exactly compatible with the Filesystem TS on this, indeed it uses the Filesystem TS as well. But it does provide an "opt out" from costly path conversion using the exact same mechanism that Windows itself provides i.e. "\\.\" which turns on NT kernel paths. These are case sensitive, and use backslashes. You can still write portable code using the race free path API, this works just as well on Windows as POSIX and lets you avoid slashes.
The Win32 APIs will already do POSIX to Win32 path conversion, so you can safely feed them a forward slash using path and it all works well.
&gt; Given that this implementation is not even up-to-date with the latest Concepts TS that was merged into C++2x working draft, there can be a lot to be desired Which happened *after* the paper was written. Stop trying to make out the paper as if it's a dishonest argument. Given that one of the main motivating features of this particular iteration of Concepts was better error messages for users who fail to meet template preconditions, it's more than fair to expect the model implementation to give good errors for the model example. It's not, in of itself, unfair to point to bad error messages and ask what's wrong. 
Don't have access to GCC 7.1 right now so sorry. Actually the bad diagnose of reference implementation is no news. Andrei Alexandrescu had a talk a few years ago on this topic. It doesn't mean it will stay bad forever. The importance of Concepts is that it completes the mathematical model that STL is based on, and IMHO C++ is incomplete without it.
Purely for reference, AFIO doesn't implement directory iteration at all. This is controversial, the first Boost peer review didn't like that one little bit. But the hard truth is that the directory enumeration syscall is almost as efficient returning 100,000 entries as it is returning 1 entry. So AFIO's directory enumeration is a single-shot API, you supply a buffer to be filled with directory entries, and AFIO fills the buffer because that's what the syscall does. AFIO works very well with 10+ million entries in a directory on Windows, Linux, FreeBSD and OS X. The "slowness" of such large directories is a myth caused by resuming directory iterators. That design pattern needs to be abolished, it's a bad design pattern. It's much more efficient to read more than you need and throw away what you don't use, even with a 10M item directory.
That design choice passed several Boost peer reviews and the standards committee.
But it is wrong to pick a not-good-enough implementation as an example to argue that the tool is not useful. I just can't agree with the point that "Concept TS does not simplify Generic Programming." If the author likes enable_if and friends then fine, but don't drag me into that hole. EDIT: typo
I would look into it like this: for small n constant factor matters a lot. Even if unordered map is O(1) it does a lot of work. O(n) for 5 elements in unordered flat map is reading 5 elements, end of story. No pointer chasing, no hashing... And regarding formally proving my claims: they are as you say obvious from O analysis but the problem is to benchmark if they really do matter. So yes we know what unordered flat map of 123 elements for missing lookup will read 123 elements into registers(meaning cache also). That is obvious. What is not obvious if this will affect surrounding code noticeably. 
Billy is 100% right here. Both Boost and the committee were very aware that the char string interpretation difference between Windows and POSIX was very unfortunate, but *at the time* it was felt that semantic preservation was more important as Billy says. On Windows char strings mean ANSI, and changing that was felt would yield excessive surprise. Now, back during the final Boost peer review of Filesystem it was suggested that an underlying string type erasing implementation would be superior. So, you could feed UTF-8, UTF-16 or ANSI char strings or whatever to `filesystem::path`, and it would erase the difference into its public API. That design had a lot of backers, but eventually it was felt it would lead to too much inefficiency because the public API would have to do lots of on the spot tiny UTF conversions. So you ended up with the current Filesystem TS. Now, for AFIO v2, taking a better decision on `afio::path` was very hard. I sat on it for nearly two years before taking a decision a month or two ago to adopt a `path_view` which does erase the backing storage. Unlike the Filesystem TS, path views are not mutable and its observers simply return slices of the original. So the efficiency problem is eliminated, and we still get the ability to write UTF-8 path strings all the time and it all works exactly as expected on all platforms. The cost, of course, is that path manipulation is nothing like as rich, but then AFIO doesn't need rich path manipulation as (a) AFIO implements a race free path API and (b) AFIO extends the Filesystem TS, so if you want to manipulate a path, using `filesystem::path` for that. Obviously AFIO is not on the standards track yet, so all of the above is for information only currently.
&gt; I just can't agree with the point that "Concept TS does not simplify Generic Programming." Then let's see a use case!
Example: range based on enable_if vs. range based on Concepts.
There is already an open source library providing UTF-8 editions of all the Win32 APIs at https://github.com/thpatch/win32_utf8.
Intel's TSX-NI instructions are merely a bit of hardware assist for implementing transactional memory. They help out a lot, but they're not a proper hardware implementation like you can get on other CPU architectures. Hence you need to implement most of transactional memory in software, and there is some widely used library floating around which GCC and all the other compilers and tooling use irrespective of language. That's the bit which implements transactional memory at runtime, utilising full or partial hardware support where available, and falling back to 100% software if needed. If you disassemble some code generated by GCC when transactional memory is enabled, you'll see most of your memory access go via that external library. That's what introduces the slowdown for the average performance case. You can of course go ahead and write your assembler using TSX instructions directly, but I at least found the 1st gen Haswell implementation of those very disappointing, only made sense in very limited use cases. I sincerely hope 2nd gen is much better, but I've not had recent use experience.
`no_opt` to make a `constexpr` value force-read, then modify the binary at runtime so it has a different value. Relies on UB, and that no_opt blocks the UB from causing random mess from occuring. Talk about audit headache; every compiler version and flag change! Seems far easier to manually verify each location is calling one or another function. Not that *that* is easy, just easier! to 
You have access to it on godbolt.org.
Clever. I've seen a similar approach taken to implement the use of closures C, for APIs that want a function pointer as a callback but don't provide any way to pass in a user-data pointer. See e.g. https://stackoverflow.com/questions/7378772 http://nullprogram.com/blog/2017/01/08/ How did you choose the magic number 0x5EEEEEFF -- is there some reason to believe that those four bytes are less likely to appear as a word "by accident" in an ELF or PE binary than any others?
The `pkgconf` project looks nice. I actually added a cmake build to pkgconfig [here](https://github.com/pfultz2/pkgconfig). For windows, you need to install a [dirent](https://github.com/tronkko/dirent) implementation. The easiest way to install is with `cget install pfultz2/pkgconfig`.
Only if the path is shorter than `MAX_PATH`.
&gt; Don't know, don't care. Then don't say things like "the committee decided X" with no evidence whatsoever that the committee has even looked at X. The vast majority of answers to "why doesn't the standard do X" is "nobody wrote a paper proposing X". &gt;wchar_t and wstring, which specifies character size but not encoding Actually, `wchar_t` does not specify the character size, and I believe (but am not certain) the fact that it does not (and is different on Windows vs. POSIX systems) is a large part of why someone took the effort to standardize `char16_t`.
There's a difference between writing your own wrapper that you call instead of the XxxA functions and changing the XxxA functions themselves, exposing all the code in the universe currently calling the XxxA functions to UTF-8.
&gt; On what platform do you want to access things relative to the executable path? We load some plugin-like libraries (mostly built-in and installed, but still) like that. They're in a sub-directory wherever the executable is. &gt; On Windows executable are typically in Program Files Unless you had the option to install for current-user only without admin privileges, right?
We're mostly nit-picking at this point, but here goes. &gt; don't say things like "the committee decided X" I assumed that if `u16` and `u32` were added, then `u8` must at least have been discussed once. I may be wrong. I understand your frustration as an actual committee member when hearing this, but you should also understand mine as a user who has no knowledge of the inner workings of the standardization process. I'm also hearing "if you didn't participate, don't complain", which is not exactly constructive. &gt; `wchar_t` does not specify the character size I'll be more precise then: It specifies the type of the "char-like object" that a `basic_string` stores and, by extension, the size in bytes of each "char-like object". That's what I meant by "character size".
&gt;I understand your frustration as an actual committee member when hearing this, but you should also understand mine as a user who has no knowledge of the inner workings of the standardization process. You don't need knowledge of the inner workings of the standardization process to avoid claiming the committee has said something that it has not said. &gt; I'm also hearing "if you didn't participate, don't complain", which is not exactly constructive I'm saying "don't put words in the mouths of people who did participate, whether you participated or not."
Sure. My point is that you really can't and must not change CreateFileA() et al to suddenly understand UTF-8. Bad idea. But libraries exist, e.g. the one linked, which implement CreateFileU(). It sure would be great if Microsoft supplied *U() wrapper functions so those who want to can move entirely over to UTF-8 everywhere.
Depends on that new registry setting of course opting out per process. But sure, 95% of code on Win32 is limited to 260 chars, you need to prefix with \\?\ to opt out and that also opts out of forward to back slash conversion.
The magic number 0x5EEEEEFF was just a random constant with no significance. Since at runtime I start at the address of in_constexpr() function the first constant matching my custom constant _should_ be the value to return (assuming x86). As for the C closures, that was a very interesting read. I looked and saw the JIT post you had. Something like that could be really fun to try and do/use. 
TIL. I did not even know that this would be issue but that's a very good point :) Looking into it more, it looks like it might actually be problematic.
I find Concepts very helpful as a replacement for enable_if and for the ability to constrain functions (like non-template functions and copy constructors) that can't be constrained with enable_if. So, yes, they do help in writing declarations, but do not currently help in writing definitions. Earlier Concepts designs did intend to make it easier to write templates by enabling the compiler to concept check templates without requiring an instantiation (separate checking). Unfortunately, that goal has not been maintained beyond C++0x Concepts. I believe we could implement a reasonable form of separate checking that improves diagnostics for Duncan's example in which sort() is called with const iterators. Quite a bit of work is necessary to get there though and I fear that if that doesn't happen in the C++20 timeframe, then it probably won't happen at all. Something I've been thinking about is reserving the ability to add separate checking later by stating that a constrained function template containing expressions that are not required by its constraints is ill-formed, no diagnostic required. This would allow implementations freedom to diagnose such expressions. Based on feedback, we could then conceivably tighten diagnostic requirements later. This would impose an additional migration cost on adopting constraints for existing unconstrained templates though.
If you read through the metaclasses proposal, you'll see that it builds on top of Concepts. I do agree that metaclasses have the potential to be much more trans-formative though.
I don't have a good example, but see the following post for some previous analysis that indicates potential for improvements. http://honermann.net/blog/2016/07/18/refining-concepts-improving-error-messages/
&gt; My point is that you really can't and must not change CreateFileA() et al to suddenly understand UTF-8. Maybe, maybe not. Most code that deals with Shift-JIS correctly with the XxxA functions would probably be happy with UTF-8 instead. Code that assumes US english will be broken forever but the OS can't fix that :)
&gt; you need to prefix with \?\ to opt out and that also opts out of forward to back slash conversion And also be over MAX_PATH. Using `\\?\C:/foo/bar` works just fine. Makes testing for correctness here "fun".
I'm cold on this, but then I think very much like a corporate Microsoftie when it comes to backwards compatibility. I will say one thing though: the filesystem path consuming functions are not like other functions. The filesystem treats paths as blobs of bytes, it couldn't care less what's in the path so long as it doesn't contain a separator. I know Win32 imposes lots of legacy junk on that, but at the NT kernel level the filing system really doesn't care what you feed it: invalid UTF is perfectly valid as a filename. It's just bytes. So I'd personally accept if CreateFileA() suddenly accepted UTF-8. Damage is no worse than before there. But I'd still prefer a deprecation of CreateFileA(), adding of CreateFileU() and let's put all that multibyte support nonsense behind us forever.
I'm just about to finish this course: https://courses.edx.org/courses/course-v1:Microsoft+DEV210x+3T2017/course/ Has been a good introduction. I'm also reading this: Programming Principles and Practice using c++, by Bjarne Stroustrup Most important of all, i think, is a goal. Why do you want to learn c++? What do you want to make? For me, games. So I'm also doing the Unreal Developer course at Udemy. All up, its working well for me and I'm having a blast.
deleted ^^^^^^^^^^^^^^^^0.7940 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/75325)
&gt; my plans to try a real project will help there. Yes, and now you have a build system for it ;-) &gt; Or do you mean that benefits will come only with code designed/organized with modules in mind from the beginning? Yes, as soon as you have the same name imported as a module, say `std::string` from `std.core`, and included as a header, then things quickly go sideways. So, in our experience (and this is an important caveat), the best chance is to have everything modularized, from the grounds up. In `build2` we can do this since we have no external dependencies but there are still C/system headers which are causing problems. One thing that was suggested to me by the Clang folks is using "Clang Modules" for the standard/system library since the two module flavors should inter-operate. This way, even if some third-party dependency includes something as a header, it will still be imported as a Clang module. I personally feel that this is just jumping deeper into the rabbit hole but we might still try it if we get really desperate.
&gt; So basically this sounds very similar to CMake's `libname-config.cmake`[...] With important distinction being that `pkg-config` is a build system-neutral mechanism. &gt; I can't imagine how you do this [...] While compiler specified, there is generally a way to get system library search paths from the compiler. For example, for GCC/Clang there is the `-print-search-dirs`. Then you parse the user-supplied `-L` options and extract those paths.Combine the two and you have a cross-compile-clean set of directories to search for libraries.
Mutable lambdas aren't needed often in my experience, but they can be used to implement generator objects like random number generators and state machines. Copying is sensible as long as the mutable state is fully contained within the object: you get a clone with the same current state that can be independently advanced. 
What's wrong with using Qt Creator as a GDB frontend?
Sadly the OP couldn't answer my question what the purpose of his lib is. The readme lacks a clear description.
Seems to me like it works well with concepts rather than building on top of them? Can you expand a bit?
Wait how are concepts going to help with **non**-template functions? 
That's really helpful information, thanks a lot for taking the time to point this out.
You're welcome!
Non-template members of template classes, I believe. So you can make a template class that only has a copy constructor if the `T` of the class is copyable, for example. With enable\_if, the function _itself_ has to be a template.
&gt; With important distinction being that pkg-config is a build system-neutral mechanism. Good point! Regarding search paths, yes the compiler would give you the search dirs for system libs. But it wouldn't give you the search paths for libraries in user-specific directories, because it doesn't know about it _yet_, that's exactly the build system's job, to find these, and give them to the compiler via `-L` options. Example: I have libA installed in ~/libA, and I want to use this version of libA instead of the one in /usr/libA (or it's not installed in /usr at all). The compiler will not know about ~/libA unless the _build system_ somehow tells it about it.
Right, one way or another, someone has to tell where the library is/might be. There are several mechanisms for this in `build2`: bundling, specifying a non-installed build, or installed libraries. [This earlier answer](https://www.reddit.com/r/cpp/comments/6rc4tk/why_does_ninja_start_compiling_faster_than_build2/dl6a98e/) goes in a bit more detail.
Will there be videos?
*I will convince you that the problem is real and has practical implications* I don't think so.
I am not totally convinced of this either, but I stand ready to be educated. Looking at [[basic.compound]/3](http://eel.is/c++draft/basic.compound#3), we have--- &gt; Every value of pointer type is one of the following: &gt; - a pointer to an object or function (the pointer is said to point to the object or function), or &gt; - a pointer past the end of an object ([expr.add]), or &gt; - the null pointer value ([conv.ptr]) for that type, or &gt; - an invalid pointer value. If the memory location is "invalid", presumably the pointer must be an *invalid pointer value*. Then in [[basic.stc]/4](http://eel.is/c++draft/basic.stc#4) we are told of the consequence of an invalid pointer value. &gt; When the end of the duration of a region of storage is reached, the values of all pointers representing the address of any part of that region of storage become invalid pointer values. Indirection through an invalid pointer value and passing an invalid pointer value to a deallocation function have undefined behavior. **Any other use of an invalid pointer value has implementation-defined behavior.** (My emphasis.) So it seems the behaviour is implementation-defined, not undefined. [Footnote 37](http://eel.is/c++draft/basic.stc#footnote-37) adds some detail: &gt; Some implementations might define that copying an invalid pointer value causes a system-generated runtime fault.
I'd like to see some benchmark showing when linear map is faster than others
Alright... I'll be there!
Cannot emit pdb which means true postmortem debugging (from a crash dump) isn't possible yet
Google's cpp-btree (https://code.google.com/archive/p/cpp-btree/) will outperform both flat maps and std::map (the latter both in speed and memory usage) and maintain good performance at any size. Each node of the btree is essentially a small flat map, its size being determined in order to be cache friendly.
You might be interested to check out my Small Hash Optimization (https://github.com/greg7mdp/sho) which adds a small flat map as an optimization to any hash table. When the number of entries grows beyond a predefined size, it converts to a full hash table.
I was referring to the sed command you had for modifying the binary on-disk at your blog posting [here](http://saadahmad.ca/detecting-evaluation-context-inside-constexpr-functions/) -- sed will globally modify all instances of that word within the executable, even if they don't have anything to do with the \in_constexpr\ magic number. (Your README.md says that modifying the binary is still necessary for Windows...) [for the record, so as not to take credit where I don't deserve it, the nullprogram.com blog isn't mine.]
I mean, I believe someone if they tell me that Iterators aren't a perfect solution to the problem of "how do I traverse a collection?". But, much like the people who insist that "Object Oriented Programming was a Mistake" or some similar snake oil nonsense, if you try to then go further and argue that "Iterators are always wrong", you're just being silly and/or dishonest. 
If you search for 'requires' in the proposal, you'll see examples where generation is conditional upon the satisfaction of requires expressions. In other words, Concepts are used to query valid expressions and inform subsequent code generation. The proposal also strengthens Concepts in one nice way by enabling imposing a requirement that all instantiations of a type (declared with a metaclass) satisfy a concept. We don't have a nice way to do that today other than by writing a static assert for each such instantiation (potentially wrapped in a type factory).
We don't know the content of this talk or what will be said. It's ok to be sceptical, but it would be better to discuss this based on a paper, or slides, rather than speculate about what the author's view is. I too tend to distrust "everything is broken", but several times before I've had someone comment that about something I've been using, react that it works completely fine, only to listen to them and realise they have a point. Not often, but it has happened. Let's reserve judgement here.
In addition to what TheThiefMaster said, constraints can be added to functions that are not templated entities at all; for example, functions at global scope. This can be used, in combination with subsumption to provide alternate implementations of a function, depending on target properties, without having to rely on the preprocessor: template&lt;typename&gt; concept bool SpecialMathFunctionsAvailable = ...; void calc(int x, int y); void calc(int x, int y) requires SpecialMathFunctionsAvailable&lt;int&gt;; Of course, *constexpr if* fulfils this role as well, and arguably better.
I was looking through your project and I really agree that C++ needs more package management work. Especially semantic versioning that 99% of the company don't have, or get completely wrong. This is so frustrating seeing that other languages have it 'build it', I'm thinking scala, where it's really good. Is there a way to create a private package repository for buckaroo and deploy internal private libraries, with a semantic version attached to them? I went through the docs but couldn't find anything. Also, is there an easy way to create your own recipe for a library?
No.
Hi @dausama The easiest way to create a package is to create it directly on GitHub. Buckaroo integrates with GitHub so that you can import packages like so: buckaroo install github+username/project Guide: http://buckaroo.readthedocs.io/en/latest/github-package-guide.html This should work for private GitHub (Buckaroo uses your SSH credentials). I will check now to verify. Other providers (BitBucket, GitLab, plain old Git) are not supported yet. Other users have requested this feature, so they are planned for the next release. You can also mix-and-match Buckaroo packages and Git submodules right now. This allows you to keep code private using existing VCS. I will post an example of this in a minute. Do you mind sharing some info about your setup? Which VCS do you use? 
This talk was given on MeetingCpp last year as well. [Link](https://youtu.be/Yba5ncxzsnw)
Check those terms. You are making stronger assumptions on a lot of very common and very well understood concepts than are actually true and you are presenting one of the most fundamental and basic models in parallel and distributed programming as novel. Aside from that: You basically just made a C++ wrapper for MPI. Which is probably a good thing, but there are probably reasons not to that the more language oriented folk will chime in on (MPI standard dropped them at some point?) and Boost already has something (can't comment as i never used them) So probably a good first step, but get a better understanding of what you actually did for the purposes of your sales pitch.
it supports HLSL-like swizzling. You can do something like: float4 num1, num2; // initialize these probably num1.xyz = num2.zww; more info on hlsl swizzling at https://msdn.microsoft.com/en-us/library/windows/desktop/bb509634(v=vs.85).aspx it would be neat if they open-sourced that library. 
though, the swizzling bit might not be the slow part, but I imagine it could be one source of template explosion.
Looking forward to it! Thanks as ever to /u/philsquared for organising everything.
Why would this trigger antivirus software?
Because it's modifying the binary which I think it's common pratique in the malware business. 
Thanks for your comments. I am _not_ trying to present the BSP model as something new at all, I simply think most people in this sub (or C++ programmers in general) are more familiar with alternative forms of parallelization (particularly for shared memory), which is why I introduce it this way. I am well aware of the terms I use, although some of them are overloaded in different contexts. The 'sales pitch' is based on my own experience doing (mostly distributed memory) parallel programming over the last few years. Dismissing Bulk as an 'MPI wrapper' as you say, is in my opinion a gross mischaracterization. In fact, the novelty is that the same concise and easy-to-use parallel programming API can be used for shared memory, distributed memory and even many-core accelerators (we have an experimental backend based on [a previous research library of ours](http://github.com/coduin/epiphany-bsp)). This makes Bulk applicable in many different contexts, as an easily teachable alternative to platform- or modality specific parallel frameworks. Also, in my opinion, the API is elegant and very economic, especially compared to existing alternatives.
The reality is that you seem to have created a lot of terms and acronyms and it is very difficult to figure out exactly what it does or how it works. Also the link that you gave is broken. How much does something like this add to the binary size of a program?
Thanks! In introducing the library I should probably avoid the jargon, and focus initially on the usage. We wrote a blog post a while ago on binary sizes for a very memory constrained system when working on an early version of this library: http://blog.codu.in/parallella/epiphany/bulk/cpp/2016/05/06/parallella_cpp.html.
So do bulk offer a PGAS model of arrays, or does it rely on FOTRAN style coarrays? Secondly I noticed you have partitions, but it is unclear to me how these handle boundary data that needs to shared among multiple 'ranks'. 
I skimmed it and I don't see any numbers on binary sizes. If you want to introduce people to something new, you will have to make the broad ideas you want to communicate super clear. You can go into detail after you establish that. 
No shit. I'd put these words in the title for this "wonderful" technique.
 Ya HLSL swizzling is nice, I wish C++ had built in support for this type of thing. Still seems weird that it would cause such slowdown, and if it does, why would you use it..
On the platform we discuss in the post, each core has a local memory size of only 32 kB (for data, the binary, and the stack). We outline some tricks so that the resulting binaries using Bulk fit on the cores even in this extreme case. This means that, at least for our current target applications, binary sizes can be kept small.
There is no PGAS model in the sense that 'remote' addresses can't be written to directly. So yes, we have FORTRAN style coarrays; with bulk synchronizations required to resolve communication. As for the partitions, the short answer is that there is no support yet for boundary data. The longer answer is that the partitioning support is very much work-in-progress (and not yet 'officially' part of Bulk, which is why it is not documented) so we may add some mechanisms for this in the future. Suggestions are welcome.
What does "_at a boundary between elements_" mean ?
I've been that crazy guy before and used mutable lambdas with `std::generate_n` to populate containers with index-dependent values instead of just using a for-loop like a sane person would.
It would be great if you could synchronize across a subset of the world , or allow critical sections.
MSVC is often very slow at optimizing huge functions or functions with a lot of calls to inline. You can easily obtain such functions if you're using code-gens. In our case compile time for the file generated by bison was more than 5 minutes in release. Another example was a function that filled some table with strings and contained about 1000 or so `std::string` constructor calls and compiled for several minutes. Both were mere seconds on gcc and clang.
So, basically with concepts we replaced template call stacks to concepts "call stacks" in error messages? I'm not sure that the latter ones are usually shorter. Actually, errors for `enable_if` are very good in gcc and `void_t` trick is not very hard to use or teach.
Boost has a lib (QVM) that does this : http://www.boost.org/doc/libs/1_64_0/libs/qvm/doc/Swizzling.html
&gt; The bulk synchronous communication style does mean losing some flexibility, and comes with a (usually minor) performance penalty. This tradeoff is often well worth it. Curious, I've found anecdotally that trying as much as possible to *desynchronise* communication usually results in a noticeable speed up. Do you have any data for this claim? What sort of work are you benchmarking it on? And what sort of machines are you using as testbeds?
I am saying that indeed there is a performance penalty for 'synchronous communication', so my experience is in line with yours! The tradeoff I talk about is with respect to the points above that sentence (scalability, ease of programming, and predictability). In my applications (mostly scientific computing) the performance gains from using asynchronous algorithms are usually minor (or even non-existent!) and not worth it. This is however not at all true in general! EDIT: there seems to be some misunderstanding: I completely agree that, if it is possible, overlapping computation and communication can of course lead to performance improvements. However, there are many cases (including my own applications) where the nature of the algorithm or program prevents such overlap, and here the bulk synchronous model shines. In any case, our library targets synchronous communication. If this is not for you, and you require asynchronous communication, then go for something else.
Thanks for the details.
That's a flawed analogy, as I would not dream of cloning the entire GitHub. Pulling all of Boost is common on the other hand. P.S. I don't actually see a problem with Boost's growth. The thing is built once and then installed into a package/container/nfs dir/etc.
What are you running on? I've been frustrated lately with communication crippling scalability on very busy clusters, so perhaps this is not for me.
Okay, I think you are conflating two things here -- Concepts, the pie-in-the-sky idea and long term language development effort, and the current Concepts TS and implementation. The paper is saying that what we currently have is basically not helpful and not an improvement worth merging. The *tool* that we have now doesn't seem that helpful. You seem to agree that the current implementation is not good enough, so I can't understand why you are so critical of this conclusion. Now it sounds like, you want to say, the implementation we have is bad but the Concepts TS itself is good and worth pursuing. But, your argument is confusing because that part is not the tool. Also it sounds like you don't actually disagree with the findings of the paper. You agree that the implementation is bad. You only disagree that it indicates that the Concepts TS is not ready to be merged. However that's a hard sell for a lot of programmers. If something is too hard to implement then it isn't ready for standardization. I guess you should be focused on providing a counterargument to that. When you suggest that the paper or the examples it highlights are unfair, I think you are missing the mark and actually contradicting yourself.
I am extremely surprised you're claiming async doesn't buy performance. Almost all cases I've seen in my industry (large scale fluid dynamics simulations) the exact opposite is true. 
Absolutely not just you. Distributed scientific codes are very often stymied by synchronous communication. 
I agree. I worked with several scientific codes and most of my team's work was dedicated on making parallel applications' hotspots asynchronous. In addition, the fact that MPI improved asynchronous support and OpenMP implemented additional constructs for tasks should mean something.
Thanks for your comment. I admit that what I've written can be unclear and confusing. So let me give it another try. The authors argued in the paper that the current implementation of Concepts TS was not good enough for improving Generic Programming as we have now. They picked error message and Sortable concept as examples where Concepts TS was bad, and then said that existing tools were sufficient. What I agree: the current implementation is bad. No doubt about this point. But I believe it is focused more on concept checking than error message improvement. What I disagree: the rest of the paper. The authors used the Sortable concept as example that Concept is hard to use because it needs too many constraints. I'd say it is the way it is because the concept is complex enough that such long list of constrains is required to mathematically describe it. People get frustrated using plain templates because it is unconstrained, and now you don't like it the other way either? Then too bad for you. Also, it is true basic concepts are hard to describe, and that's why we need experts to define them for us in the standard, and with those building block you construct your own Concepts much easier. The argument that "C++17 already have tools for Generic Programming" is what I dislike the most. It is true in a sense, as we can do what is needed already, just like some of the features in C++11 could be done in C++03 because we had the tools. But they are primitive tools, more like hacks, and that's why we need better ones. Well, if you like enable_if and friends like the authors then fine. But they are what they are: hacks, because we don't have concept yet. Let me reiterate: The importance of Concepts is that it completes the mathematical model STL is based on. Most of the STL utility can benefit from Concepts that provides high level expression with compiler check instead of documentation in comments or ugly hacks if you insist to implement using enable_if as we have now, which the authors didn't discuss much. Concept is not good enough, but it does improves how we do generic programming, unlike what the authors concludes. EDIT: typo
I don't think it's controversial to say that constraining templates is an essential feature of modern C++. At the moment we have a means of doing so via SFINAE, but this is an ad-hoc solution that falls out of the template instantiation rules, with a couple of supporting library functions in the shape of `enable_if` and `void_t`. To me though it's an important enough feature that it deserves core language support. Arguing that language support is unnecessary because we have `enable_if` is a bit like saying we don't need lambdas because we have `std::bind`. Again, I think many people -- though certainly not everybody -- would agree with this. So the question then becomes, is the Concepts TS (or the subset recently voted into the working draft) the best approach to providing core language support for constraining templates? That I don't know. But I don't have anything better to propose, and neither does anybody else. We've been wrestling with how to do this since at least the early 2000s, and we've already gone back to the drawing board once. It's taken six years to get back to the point of putting *something* in the language -- and if we reject the current approach, it will probably be another six years before we get there again. Personally I'd prefer an approach that requires you to explicitly provide an implementation of a concept for a type, as you do with runtime interfaces, as you do with Rust traits, as you do with Swift protocols, and as you do with Haskell typeclasses. I prefer the idea of signature-based rather than usage-based requirements, again as all those other languages do. But those are just my thoughts, and I'm not privy to the design decisions that led to things being the way they are in the current implementation. Then again, I'm not sure many people outside of the committee are. Perhaps that's the problem.
Probably. All the C++::London talks are recorded and available on the [SkillsMatter site](https://skillsmatter.com/groups/10709-c-plus-plus-london#past_events) (you may need to have a SkillsMatter account to view them though, I'm not sure).
I'm guessing it's related to situations when you want to insert items into a collection. If the iterator points to an element and want to insert a new element at that iterator, should the new item go before or after the current element? Having a boundary iterator can be more explicit with intent - it would be akin of saying "I want to insert an item between these two elements".
Most likely because so far no one noticed that it's *this* what makes compiles be slow. Now that we know about it, we'll try to do something... :)
We do, this talk was given several times already. His point is that in the standard library, iterators are sometimes used to denote positions (e.g. find) and sometimes to denote borders between two positions (e.g. lower_bound), so we should make this distinction at the type level. It's a cute abstraction, but realistically not important enough to most people completely abandon the STL over it.
thanks for your answer, really appreciate it. My opinion is that it is great for individual users to rely on github, but for corporate users, who mostlu have private git repositories, relying only on github is a big no go. I would rather avoid submodules, as I have had a not too pleasant experience with them and they're difficult to get right. I do use git, and ideally I would like to create semantic versoned modules, in my private setup. For example: ApplicationA -&gt; libAv0.9.0 $$ libBv0.8.1 In this example Application A depends on two specific versions of libA and libB, which I would like to have deployed in my setup. It would be nice if I could deploy them to a specific location in my host, ie. /nas/libs/libA/libA.0.8.0, /nas/libs/libA/libA.0.8.1... Have you considered using play old rpms for it? What I have in mind is a bit like how sbt in scala handles dependencies and packages, which in turn heavily relies on how maven handles them. For instance in maven or sbt you can get packages from the maven public repo, but you can also use packages in your local setup. I really like the philosophy behing buck and your work for buckaroo, it would be great seeing it growing up and becoming a de facto standard for building C++ projects! Please let me know what you think about my suggestions! I'm also available on various IM messaging apps if I have not been clear enough. Thanks!
[GLM](https://glm.g-truc.net) has swizzling. The [manual](http://glm.g-truc.net/glm.pdf) describes it on page 9, but you just need to define `GLM_SWIZZLE` and you can do: glm::vec4 ColorRGBA(1.0f, 0.5f, 0.0f, 1.0f); glm::vec3 ColorBGR = ColorRGBA.bgr(); 
Not necessarily. You usually have a very little, platform-dependent, piece of code that establishes a well-defined and reliable set of roots and the rest of the code uses portable pathnames relative to those roots.
I didn't downvote you, but the reason you are downvoted is probably because you aren't providing any code samples, so it's hard to evaluate your claim for truth.
From GLM docs itself: "Enabling the swizzle operators will massively increase the size of compiled files and the compilation time" :(
Yes, I 100% agree. The end goal is to be able to mix-and-match public and private dependencies. We support GitHub now (GitHub is not only for open-source, main companies use private GitHub). We will add more integrations over time. File-system dependencies is an interesting idea. I have created an issue for that here: https://github.com/LoopPerfect/buckaroo/issues/115 Here is an example of using Git submodules with Buckaroo: https://github.com/njlr/buckaroo-with-submodules-example
never said it was free! 
thanks! we're on the same page. That is actually how we were doing it at a company I used to work for. We were using versioned rpms and leveraging fedora's rmps facilities to install packages. And is there an easy way to create packages from buck libraries? 
This is definitely on our list too!
KDE
Try LibreOffice. Your Code will be used by a huge amount of users. While the code base is quite unorganized, and the build times are abmyssal, it is very rewarding if you manage to get something accepted. As far as I know, there are "easy hacks" in the bugtracker.
Seconding this. I love KDE, sadly I'm not nearly qualified enough to participate in any useful way, but it's a great project with a lot of experienced developers working on it.
A Qt port of the LibreOffice UI.
Yes, a Buckaroo package is basically a repository that builds with Buck with some metadata. Here is an example: https://github.com/njlr/buckaroo-github-example As I said, only GitHub is currently supported, with more in the pipeline. The process for going from Buck to Buckaroo package is very simple: 1. Create a `buckaroo.json` file, e.g. using `$ buckaroo init` 2. Create a release tag on GitHub, e.g. `v0.1.0`
Qt. There are [many many many](https://bugreports.qt.io/browse/QTBUG-50992?jql=project%20%3D%20QTBUG%20AND%20issuetype%20%3D%20Bug%20AND%20status%20in%20\(Open%2C%20Reported\)%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20watchers%20DESC%2C%20priority%20ASC%2C%20updated%20DESC) open issues which have impact on software that runs on every platform. And you will likely be able to choose issues on a field that you like: 3d rendering, text processing, json / xml, [low-level or high-level networking](https://bugreports.qt.io/browse/QTBUG-14975?jql=project%20%3D%20QTBUG%20AND%20issuetype%20%3D%20Bug%20AND%20status%20in%20\(Open%2C%20Reported\)%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20%3D%20Network%20ORDER%20BY%20watchers%20DESC%2C%20priority%20ASC%2C%20updated%20DESC), [painting](https://bugreports.qt.io/browse/QTBUG-62418?jql=project%20%3D%20QTBUG%20AND%20component%20in%20\(%22GUI%3A%20Painting%22%2C%20%22QML%3A%20Declarative%20and%20Javascript%20Engine%22\)), [programming language interpreter / JIT](https://bugreports.qt.io/browse/QTBUG-62410?jql=project%20%3D%20QTBUG%20AND%20component%20%3D%20%22QML%3A%20Declarative%20and%20Javascript%20Engine%22), mobile support (android, ios, whatever), wayland, etc...
Yeah...., I don't think it's something a lone programmer not used to contributing to open source projects can tackle
[Godot](https://github.com/godotengine/godot)
I don't know if you are aware that there is already a widely known project named Bulk https://github.com/jaredhoberock/bulk. Cuda's Thrust uses this library as backend. Therefore, I suggest renaming your project.
deleted ^^^^^^^^^^^^^^^^0.8851 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/24238)
You could also look at the Blender source code, it has a ton of various bugs that need verification and patching. It requires familiarising yourself with the codebase, but it is a rewarding endeavour.
That's quite probably the case, but I would _such_ a big fan of them!
What what I've done, Qt &amp; Blender are really friendly to start in. Though Blender is more on the C side of things.
deleted ^^^^^^^^^^^^^^^^0.1411 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/22349)
&gt; sadly I'm not nearly qualified enough to participate in any useful way, nah, even small stuff like trying it and reporting bugs is good
Doesn't QT cost money for commercial products? Why would I contribute to a code repository that I couldn't end up using in a product for free?
It's open source
QT is open source under the GPL. If you want to make an closed source application you must pay for a different license. As long as the product you make is open source you will not need to pay (ie all of KDE).
&gt; Doesn't QT cost money? No. Qt is under a triple license GPL / LGPL / Commercial. And contrarily to what /u/GoldPanther says it's perfectly possible to make a closed source application with the LGPL version of Qt : https://www.qt.io/qt-licensing-terms/
excellent writeup - thanks!
Yep. Do you know what "closed consumer devices" mean ? It's stuff like DVD players where you can't update the firmware. If you make those, sure, you need to buy Qt. For everything else, desktop apps etc, LGPL is fine.
I take your point as not all of QT is licensed under the LGPL I'll leave my answer for simplicity. In particular I'm referring to Data Visualisation libraries and YOCTO recipes. https://www.qt.io/licensing-comparison/
&gt; Hmm, possibly.. could also mean mobile or any platform that's not desktop. no, it could not. You have to use Qt is under a license, LGPLv3, and LGPLv3 is quite clear in term of what is allowed and not allowed. To be legal, your app just needs to allow its users to replace *the LGPLv3* parts on their devices. Not possible on closed DVD player firmware, but totally possible on Android and iOS.
Yep! There are often ways to filter bugs on issue trackers on expected complexity. Some projects even have a 'great for beginners' type tag u/NanoCoaster . For example [Mozilla](https://www.whatcanidoformozilla.org) has [pages](https://www.joshmatthews.net/bugsahoy/) where you can input your skillset and it shows projects that could use your help. They generally use a `good-first-bug`tag. Even if that doesn't apply to KDE it can be helpfull to just replicate a bug someone described on different hardware or figure out the extent of older versions that display the bug to help narrow down when a problem occured, this gives you good insight as to what goes into actually contributing code fixes. Blender and Qt are other wonderfull projects that could use some love.
[removed]
I struggle with this as well but ***usefulness to others*** is maybe not the best criteria for picking a project to work on. There are a ton of projects that need developers because no one wants to work on their bugs. Either the bugs are complicated and require years of understanding of that specific project and its history or the project is just plain boring. 
Not true at all. You can make commercial/closed application for free using the LGPL license as long as you link the shared libraries. Even if you patch Qt yourselves you are only obliged to make the library source code available on request. 
Link to the subreddit: https://www.reddit.com/r/cpp_review/
This library is currently under review in r/cpp_review: https://www.reddit.com/r/cpp_review/comments/6rdkqz/review_of_bulk/
See my other [comment](https://www.reddit.com/r/cpp/comments/6ssf3o/coding_for_charity_what_are_some_c_open_source/dlfg560). Not all of QT is licensed under the LGPL. If you still think I'm mistaken let me know and I'll look into it and update my post.
Excellent indeed! Great work :) Thank you!
One I wanted to contribute to but never found the time is GNU Aspell http://aspell.net/. The licensing isn't the best but it's not AGPL3 and the codebase is small enough to be readable. It's actually quite good compared to more widely use libraries like hunspell and in need to more maintainers. The code is not modern C++ by any stretch but it seems well enough written to be translatable to a more modern version of the language. As I said, I haven't contributed to this, so it's more of a suggestion for further research on your side (mail the maintainers, ask them if you can help and how, readup on the issue of aspell vs hunspell and why the former is more widely used, look at the source code try fixing some of the bugs... etc).
It's OK. I don't care about downvotes. You asked for a use case, not sample codes, so I didn't bother to do so. Plus, one can just google them for the source code.
If you are interested in clang developments you could have a look at for example: [include-what-you-use](https://github.com/include-what-you-use/include-what-you-use)
You're correct if your point is that some modules are not compatible with LGPL, still is incorrect to assert that you need a commercial license to buils commercial Qt applications. 
I am gonna say that [Catch](https://github.com/philsquared/Catch) always welcomes help. However, the most interesting topic we have right now is decreasing compilation times and some pie-in-the-sky ideas (parallel test execution, multi-platform out-of-process test execution), so if you want to code up new features rather than refactor a lot, I'd look elsewhere.
While libraries like Boost will always be the golden standard of C++ libraries, I think a definitive "Good C++ Libraries" resource is sorely needed for the community. Boost has a very high barrier to entry, and (once a library is absorbed) the release cycle is now interlocked with the Boost release cycle. I hope that this takes off as *The One True Way*^TM for C++ libraries to become "community approved". Other languages have single-purpose libraries that are generally considered "The Best" for their respective domain. ie: _requests_ in Python. Now we just need a good way to _distribute_ these libraries...
100% agree with you, so if you want this to succeed, try to contribute to the currently running reviews!
Add more of the Core Guidelines to clang-tidy. Compact, manageable and pretty standalone task.
Awesome, I'm excited for C++17!
funny thing : I'm working on a machine learning library in C++ right now (with sycl) I'm not very experienced though and the main purpose of this project is to gain experience. I haven't done much yet and I'm still thinking about the global architecture. interested ? because I would really like to try working with someone 
Oh! I had completely missed the inline variables! It's always been such a pain to have to place those static class variables in the cpp file. It's of course even sneakier when it works fine at first, and then fails when you try to `cout` the variable because that's taking the parameter by reference (not value) and the reference cannot be found...
Why does this have to be on seperate pages?
It's funny in a bad way that all the answers focus on projects helping developers only instead of non-developers. I was expecting projects related to actual charity.
In some way, it would be good to have several "curator reviewers" following different rules and ways to follow the ones that match your use cases.
:'-)
that's...my main criterion though. I want to do something that helps people. of course secondary criteria will be how fun and interesting the work is. but I don't feel like contributing to blender (no offence to who proposed it)
just found a small optimization in the examples. Below code should call reserve before pushing back all the values. template&lt;typename T, typename... Args&gt; void FoldPushBack(vector&lt;T&gt;&amp; v, Args&amp;&amp;... args) { constexpr int nofArgs = sizeof...(Args); v.reserve(v.size() + nofArgs); (v.push_back(args), ...); } 
Once back home I'll search to see if groups making stuffs like Fold It, Seti At Home or other research software are allowed and open to having external devs help. Maybe start there? Also maybe ask directly to charity groups that interest you? (Maybe local is better) Another way might be to search see if there are already this kind of projects on GitHub, like maybe software to transform some Arduino or raspberry pi in something useful for people in need, handicapped or maybe even just citizen.
I made a shim of the Catch directives that I use. It removes all of the cli options and printing to get compile-times and code sizes to reflect only what my code is doing (heavy metaprogramming). [here](https://github.com/ricejasonf/nbdl/blob/3d71f865a3c3d120e800d0c05d333144f6ec5183/test/catch_main.cpp)
How about Chromium? There's tons of stuff to learn when working on it. If you don't have a problem with Google's C++ restrictions (no exceptions, etc), you should check it out.
Writing documentation, translating, creating artwork, doing community work - everything helps. Writing blog posts about the things you like is also something that would be very helpful on multiple fronts. 
Is that actually an optimization though? Imagine this pattern: vector&lt;int&gt; v; for (int x=0; x&lt;1000; x++) FoldPushBack (v, x); Now you are doing a reallocation for every element, which is far worse than the normal situation. See also under "Notes" here: http://en.cppreference.com/w/cpp/container/vector/reserve Of course this is not a proper use for this function, but similar access patterns could easily happen, and I don't think people understand how dangerous vector::reserve can be to your performance if used inappropriately. In this case I think it is just about always a pessimisation, unless the number of values in the function call is significantly greater than the number of values already present in the vector. 
To get more clicks show more ads. (I assume)
Fair point, didn't think about that scenario. So in the general case it's not an optimization.
There are no ads anywhere on that site. The pagination was just to make examples separate. But it's good feedback that most of people prefer a single page.
I really like that the platform allows readers to run anything inside the browser. So having runnable code samples is one option. Do you like such approach?
http://www.cplusplus.com/doc/tutorial/preprocessor/ Think of preprocessor as separate step which happens before compilation. What preprocessor does is replacing p# directives with C++ code which then processed by compiler. Something like this: file.cpp --&gt; preprocessor --&gt; file.i --&gt; compiler --&gt; file.obj/o --&gt; linker --&gt; file.exe/dll/so/... So preprocessor is it's own language.
Most compilers can run just the preprocessor, for instance with gcc you can run g++ -E -i foobar.c -o foobar.i to produce the preprocessed intermediate that would be normally fed to the compiler. Then you can experiment with -DMYMACRO or -DMYMACRO=foo to see what various things do. Apart from that, I don't know of any other tools to "visualize" what's going on.
There was this article the other day: https://blind.guru/qta11y.html He says: "If you want to write cross-platform accessible software: You definitely should not use Qt. And no other Free Software toolkit for that matter, because they basically all dont give a shit about accessibility on non-Linux platforms." Looks like there is a need that isn't commercially viable or interesting enough for anyone to work on. 
It's still an optimization. Per cppreference: &gt; If new_cap is greater than the current capacity(), new storage is allocated, otherwise the method does nothing.
No, it's terrible. `vector::reserve()` is the one member function that bypasses geometric growth - it is allowed to give you exactly as much capacity as you ask for, and typical implementations do. Constantly reallocating to Size + 1 is quadratic time.
I'm not the OP but I would be interested. I'm learning machine learning right now (have been using the basics for several years) and I've mostly programmed in Clojure, Python and Haskell, but I'm somewhat familiar with c++ and would love to do something hands-on together.
Anything and everything you contribute in code for Free Software, in the end benefits the whole humanity.
I'd like to consider myself a more advanced user of modern C++. though I haven't done much with machine learning (been meaning to but haven't had much time for it), but I understand the general concepts behind it. I'd be willing to help out with it if you'd like. Most of the stuff I do is in Robotics, but I'm always happy to help out with open source projects. I also have some experience in implementing python bindings for C++ code if that's something you'd be interested in doing. What platform are you developing it for?
I made a discord server if you want https://discord.gg/8Y5AEtq
I made a discord server if you want https://discord.gg/8Y5AEtq
Might I suggest as a tie breaker: A project you use. It's easier to keep working on something that you use as you understand it better and have a greater motivation.
I was expecting this: [There is a better way!](https://youtu.be/viejY6UZ5Bk?t=36)
 constexpr auto const MySuperConst = TConstant &lt;100&gt;; This is redundant, there's no need to specify the `const` qualifier when using `constexpr`.
I see no mention of rpcs3. Tons of work available and not that many people (20 people with more that 10 commits). If you don't know it it's the opensource ps3 emulator for windows &amp; linux. https://github.com/RPCS3/rpcs3
&gt;I should know how to use it well though. Why?
since it's dual licensed, doesnt that mean u need to assign copyright to them and do paperwork?
sure! I'll join it. I'm busy for the next week weeks but I'll try and hop on it at some point this weekend or next week.
Why are we taking an unsigned integer, using it to initialize a signed integer, adding that to an unsigned integer, and then passing that to a function that takes an unsigned integer. All this could have been avoided through the use of `auto`... 
I'd buy them a beer!
Do structured bindings only work with declarations? Or can you use them with already existing variables?
There is this [wiki page](https://github.com/pfultz2/Cloak/wiki/C-Preprocessor-tricks,-tips,-and-idioms) I wrote that goes over various patterns using the preprocessor. Its not really finished yet as I would like to cover parsing, but it does cover a little bit more than the cpp_magic article.
R++ can expand one level of macro at a time, which is nice. However, last I used it, I found a couple cases where it expanded all layers at once correctly, but expanded one at a time incorrectly, so that's something to watch out for.
&gt; But it's good feedback that most of people prefer a single page. Might be, but as there's no direct links I saw - you have to step through the parts in sequence, e.g. the obvious list of parts in the introduction doesn't contain links - I don't think that's the intent. I don't see any ads and Adblock Plus says it's not blocking any, but it does say that there's 18 whitelisted items and 2 hidden, and it does mention under "most active filters" gstatic, doubleclick and adsbox - I'm guessing that means there's tracking even though there's no visible ads. Which bothers me not at all - I have Adblock Plus on by default, but it took me a few attempts finding a site I haven't unblocked to compare against and make sure those "most active filters" are related to that site and not to my general browsing. Tracking is an annoying fact of life that I'm not going to blame authors for. 
Libraries like Boost make great effort and compromises to maximize compatibility with compilers. Just look at the version spread for [1.61](http://www.boost.org/doc/libs/1_61_0/doc/html/align/compatibility.html). This support can lead to ugly code. I'm not sure that's the golden standard, at least not to everyone's eyes.
If you're into electronic music or synthesis, https://github.com/supercollider/supercollider could always use contributors.
From my experience with the software it could also really need the help.
Krita! Check out r/krita for some recent issues, and bugzilla as well :-)
a few years ago, yep, you had to do paperwork but nowadays you just have to make an account on the website and check a checkbox
So maybe you can do #include &lt;cmath&gt; ... using namespace std; int N = v.size() + nofArgs; if(N &gt; v.capacity()) v.reserve(pow(2, ceil(log2(x)))); to round to the next power of two 
As far as what my memory tells me I've heard, partial specialization for function templates is disallowed because overloading is the right way to do it. However, I think it's clear that isn't always applicable.
The best thing someone could add for catch would be a test driver to integrate with Visual Studio and XCode. The integration with reSharper++ is great, but on my team we use VisualAssist. 
oh BTW it's cross platform windows/linux + nvidia/amd/intel/other asics
&gt;v.reserve(pow(2, ceil(log(x)/log(2)))); This makes my head hurt. log, ceil and pow operate on doubles. log is natural logarithm. Please, think of the children :(
Why not the more idiomatic (IMO) function call `operator()` member instead of a `static` function `_`?
there, happy ? :p 
Only a bit more. I just shrug at usage of floating point maths for things that shouldn't need it. I don't think there is easily accessible ilog2 in C++ though. Travesty.
hmm, I thought log had an int overload but it seems not. And quickly skimming the intel docs, it seems x86 doesn't have any kind of instruction for this so it's really a matter of choosing precision vs performance.
&gt; And quickly skimming the intel docs, it seems x86 doesn't have any kind of instruction for this It has. I think all modern processors have bit scanning instructions. There are also exposed as intrinsics on most platforms i know of. Like these: http://x86.renejeschke.de/html/file_module_x86_id_20.html https://msdn.microsoft.com/en-us/library/fbxyd7zd.aspx It is a pain to wrap these for so basic function though.
Generally, it is considered good advice to avoid relying on preprocessor magic as much as possible. As the language has evolved, the number of cases that really require resorting to preprocessor magic is dwindling. There are many pitfalls, and preprocessor macros are not hygienic. Avoid where possible.
I knew the vector.insert() range implementation is more efficient then pushing back separately, so i had a look at how they do it. I checked the vc++ vs2015 implementation and for a ranged insert in case of forward iterators it checks if the whole iterator fits into the unused capacity. If not it typically increases the capacity by 50% or at least up to the capacity required. That would be easy to implement here as well.
The Boost.Preprocessor library documentation is pretty good and the early chapters are a solid introduction http://www.boost.org/doc/libs/1_63_0/libs/preprocessor/doc/index.html
Given the comments on my original suggestion, i think this mitigates the problems with the previous implementation that were mentioned. template&lt;typename T, typename... Args&gt; void FoldPushBack(vector&lt;T&gt;&amp; v, Args&amp;&amp;... args) { constexpr auto nofArgs = sizeof...(Args); const auto requiredCapacity = v.size() + nofArgs; if ( requiredCapacity &gt; v.capacity() ) { v.reserve(max(v.capacity()+v.capacity()/2, requiredCapacity)); } (v.push_back(args), ...); } 1. it now reserves 50% additional capacity after detecting more capacity is needed or at least up to the capacity needed by the inserted elements. This should ensure using the FoldPushBack for 1 element cases also performs well. 2. now using auto for nofArgs to ensure the right type is used. 3. i based this on the vector.insert() range implementation of the vc++ stl (vs2015). 4. i know of at least one edge case that is not covered by this, namely nearing maximum capacity allowed in a vector, but oke.. (stl takes care of that)
The static has the advantage of not instantiating an object on which to call `operator()`. The purpose is to create a function, which we can't do with partial overloading, and a static function is the next closest thing to a regular free function.
I'm skeptical that this: template&lt;typename T&gt; struct is_pointer_impl { static constexpr bool _() { return false; } }; template&lt;typename T&gt; struct is_pointer_impl&lt;T*&gt; { static constexpr bool _() { return true; } }; template&lt;typename T&gt; constexpr bool is_pointer(T const&amp;) { return is_pointer_impl&lt;T&gt;::_(); } is clearer than the typical way of "emulating" partial specialization, which is just overloading: template &lt;typename T&gt; constexpr bool is_pointer(T const&amp; ) { return false; } template &lt;typename T&gt; constexpr bool is_pointer(T* const&amp; ) { return true; } (**Edit** the pointer overload needs to take a `T* const&amp;` to avoid decays giving false positives, thanks TC) The rules for selecting which overload to pick between overloaded function templates are the same as the rules to pick between class template specializations (the latter are described in terms of the former). As for why not partially specialize function templates? Overloading is sufficient. Specializing just opens the door for confusion: template &lt;class T&gt; void f(T ); // #1 template &lt;class T&gt; void f(T* ); // #2 template &lt;&gt; void f&lt;&gt;(int* ); // #3 f((int*)0); // calls #3 but: template &lt;class T&gt; void f(T ); // #1 template &lt;&gt; void f&lt;&gt;(int* ); // #3 template &lt;class T&gt; void f(T* ); // #2 f((int*)0); // calls #2 And that's explicit specialization too - can't imagine the examples you could come up with for partial. 
Reading this reminds me of [the single best sentence in the C++ standard](http://eel.is/c++draft/temp.expl.spec#7): &gt; When writing a specialization, be careful about its location; or to make it compile will be such a trial as to kindle its self-immolation. I don't know who was responsible for putting a limerick in the standard, but they deserve a medal. (In fact, that whole paragraph is amazing. Not only did it get through the committee in the first place, but it even got updated in C++14 to mention variable templates...)
Sometimes I miss partial specialization when I need to dispatch differently based on traits, which you cannot do on all compilers just with overload + SFINAE. template&lt;class T, class = void&gt; void f(SomeTransformedTypeFromT&lt;T&gt; t); template&lt;class T&gt; void f&lt;T, enable_if_t&lt;...&gt;&gt;(SomeTransformedTypeFromT&lt;T&gt; t); except that you cannot do this.
Jens, thank you for this awesome aggregation of links!
I know this may be the wrong sub reddit for this, but if you specifically want to publish on Android, C++ might not be the first choice. [Unity](https://unity3d.com/) is programmed in C#, supports publishing for Android, 2D Games and even allows to write C++ plugins. It also has a lot of Tutorials for every skill level and a huge store of assets.
This is one of the main selling points of Concepts.
&gt; it is my opinion that C++ is the most innovative language as far as unit testing goes. Sadly the author doesn't tell us the *how* or *why*  (And I am quite sure I would disagree ) But thanks for mention [FakeIt](https://github.com/eranpeer/FakeIt) - didn't know about that! I will give it a try - the lack of an good alternative zu gmock was always the cause not to get closer in touch with catch (imho a *unit* testing framework ist worthless without the ability of mocking) FakeIt might close the gap for this!
1. Learn C++ better 2. Learn the basic of game development. I don't recommend a full fledged engine like Unreal 4; yet. This was my first C++ book: https://www.amazon.com/Beginning-C-Through-Game-Programming/dp/1305109910/ref=sr_1_1?ie=UTF8&amp;qid=1502459253&amp;sr=8-1&amp;keywords=game+development+c%2B%2B You'll make text based games. As for a game development library, I'd recommend RayLib: https://github.com/raysan5/raylib As it's very simple. It doesn't have a lot of complex things provided for you already (such as Scene graphs and whatnot). You'll need to build that stuff for yourself. But it does give you the basics of graphics, audio, input handling, etc. I'd say once you feel comfortable with those two things, you should then move onto something like Unreal.
You should leave this up to the caller.
There are _very_ few differences between the C preprocessor and the C++ (basically the differences boil down to C++ knowing about the extra operators (`::`, `.*`, `-&gt;*`), allowing the use of `true` and `false` in `#if`-type expressions and recognising raw string syntax (and therefore not processing anything inside one). Therefore if you want to learn about the C++ preprocessor, anything written about the C preprocessor is more-or-less equally useful. Of course, many things that are done with macros in C are better done with templates, etc. in C++, but that's not a difference in the preprocessor per-se.
I am glad that (unit) testing comes into play in the C++ community. But I also strongly disagree about the cited sentence. Having done TDD and "Growing software guided by tests" in Ruby, JavaScript, Clojure, C# and Java projects before I had a hard time in the last months getting TDD introduced in a C++ project. Most effort had to be spent on getting C++ code "mockable". We use GoogleMock which is really great. But writing all the pure virtual interface classes for every class you need to mock in a test is cumbersome. That results in much "triplicated", not only duplicated, code: interface header, class header, cpp file. And if you need to mock a library you even need to write something like a wrapper or facade class with an interface for it. Theoretically such a wrapper for a library makes sense even without the need for mocking but if you develop your app in a big framework like Qt you have tight integration with it everywhere and you want to benefit from it. And Qt does not support mocking. At the end I still think one has to write tests and even better let their software grow with tests. It is the best method to develop reliable software I have seen in my 18 years' career. I hope that advances in the C++ standard will make it more enjoyable.
I agree, TDD is very hard in C++. Everytime I go from C# to C++, I'm always a bit put off. Don't get me wrong - I love the language! It's just that when I'm a used to frameworks for testing and mocking in C#, C++ has more friction. Heck, it's easier to test and mock in C than C++! Maybe with the metaclass proposal/paper, it would be easier. 
One thing is using a unit test framework on command line and in (continuous integration) builds. But isn't there another trend going on - e.g. in the Java world - in having your favorite unit test framework integrated into your favorite IDE, being able to code and run the unit test(s) as close to the code as possible? I believe Microsoft Visual Studio have "test integration" for C++ but I do not have any experience with it. PS. Just noticed the recent VC team blog post [C++ Unit Testing Updates: Announcing Boost.Test Adapter and Improved Google Test Support](https://blogs.msdn.microsoft.com/vcblog/2017/08/04/c-unit-testing-updates-announcing-boost-test-adapter-and-improved-google-test-support/).
Did you install C++? You have to choose it explicitly while inslalling VS 2017. 
Oh, yes, IDE Integration is necessary to have fun with TDD. QtCreator has plugins for their own test framework (without mocking) and for GoogleTest.
Hypodermic, the Dependency injection lib for C++, does some sophisticated analysis of C++ classes, constructors etc. at runtime. I was surprised how far they could go with C++. But for easy mocking one must be able to generate classes at runtime. Is someone doing something like that?
Yep, this is right. Check that you have `&lt;Visual Studio install location&gt;\VC\` directory, that's where the C++ compiler, headers and the templates live.
Looks like VS 2017 installed only those files that it needed for Unity.
I can't seem to find the VC folder where I installed the Unity engine. Like I answered manni66, it looks like only those files that were necessary for Unity were installed.
Rerun the VS2017 installer and pick the C++ package. It's one of the main packages and should be easy to locate.
I just selected the Windows universal package and modified the VS 2017. 
Ah sorry, I meant `&lt;Visual Studio install location&gt;\VC\`. Edited the post to reflect this.
As each compiler uses/creates it's own vtable format (which might even change on every release), I don't think anyone is even trying. It would be an extreme uphill battle; I mean you'd have to check, on every single compiler release, whether they changed something and furthermore maintain support with a few previous versions
http://i.imgur.com/sLc5grD.png The highlighted option is the one you want for C++ support.
 int a[10]; What's `is_pointer(a)`? &gt; The rules for selecting which overload to pick between overloaded function templates are the same as the rules to pick between class template specializations (the latter are described in terms of the former). Close but not quite. The rules that determine the type(s) being compared are different. 
With C++ it becomes crucial to interest yourself in what the industry experts have to say. The [cpp core guidelines](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md) are one such resource.
http://i.imgur.com/kPnRmR6.png By the time I received your reply I had downloaded the Universal Windows platform development workload. I find there are only options pertaining to DX in visual studio available now. Therefore I tried to download what you suggested and came across the situation as shown in the figure I have shared. As you can see the 'Modify' option is grayed out and is not clickable. Does this mean that I already have those that are checked?
Since you're already familiar with c-like languages, you could just start with a hello world program, then learn as you go along while working on a small project that would require knowledge in c++. I don't think I've ever really learned a programming language from a book, except the first one possibly. The internet is now the best resource on programming languages.
It says that you have instances of VS open. I just tested it, the button will be grayed out while other instances of VS are open. Close all instances and it should be installable.
Forget everything you know about idioms and take C++ as it is. Yeah, you can do the same with C++. That's how I learned.
These are advices I think are valuable for a C++ programmer. Some may disagree with what I say. So take these advices, but I'm not the absolute truth. Read guidelines and stuff, and advices form other, and more importantly, advices from yourself. You'll learn the best from mistakes and hard work. Learn value semantics and regular types. Don't get me wrong, classical OOP is useful and used quite a lot in C++, but is not the silver bullet people think it is. Sometimes using generic programming, or even concept programming result in better performance *and* better readability ( not having to deal with pointers and polymorphism ) I also tend to implement class hierarchy as an implementation detail. I hide to the user of my code that there's an interface, accepting any of their types if they satisfies some constraints and create an adapter for my interface. This make the user code less coupled with my code, because it doesn't have to extend the interface, and don't necessarily enforce dynamic allocation. Even with that said, you'll have to re-learn how to manage class hierarchy, polymorphism, and especially, manage memory. These days, memory management is not about newing and deleting memory, but about managing resources ownership and observers. When there's a new, you must delete. When there is an open, there is a close somewhere. Who has to close is the owner of the resource, who use it is generally an observer. Use RAII to close and open, the `}` will be your garbage collector. For *insert cultural symbol here* sake, use `std::unique_ptr` for owning pointer, and `std::shared_ptr` when you really need shared ownership. Don't use owning raw pointers, you're gonna have a bad time with those. However, observing raw pointer is okay, and it's about the only way to observe a unique pointer. References works too. One last thing, not everything needs newing and dynamic allocation, as opposed to C#. When you can use values and references, use them. For example, `std::string` and containers should not be used as pointers + dynamic allocation, but as value + references, unless you have very special needs. Even then, wrap the container in a structure and dynamically allocate that structure. Don't be afraid of templates, they are your friends. Learn to love them. 
I totally agree with you. I have the impression that it is quite common within the C++ community that lots of folks just do C++ and often even have knowledge of other languages - besides their old, some twenty years ago knowledges of C, Turbo Pascal or even Visual Basic. And then they claim such very questionable statements like C++ is so innovative in an area which the community has ignored for so many years... I think most impulses comes from minor used languages like Haskell, Clojure, Python, Ruby or Scala with different paradigms, where things like property based testing was invented or BDD has been streched to new limits by implementing DSLs that feels almost natural as natural language. C++ is inherently more verbose than most higher abstracting languages, so test code often is more bloated than in other languages what makes writing tests less comfortable. But it is ok, if you get used to it. But innovation I do not see at all! That's why I stated that it is sad that the author doesn't provide any examples!
I agree with the author that Catch (https://github.com/philsquared/Catch) is one of the best testing frameworks I've ever used. So easy to get set up as well. I highly recommend it.
I second that
In VS 2015 the test adapter feels not good at all (with googletest). You need to compile a project in order to get a list of your tests. The filter simply sucks - no bookmarking, no wildcard support. Perhaps this gets better in VS 2017? Always remember that test adapters in other languages and IDEs (some of them even don't cost any money!!!) provide a better experience. The VC++ compiler must really suck internally, as MS has much better IDE support for C#. Sadly they won't switch to CLANG - this would be a huge step and would surely improve tooling and real IDE support.
Your overload set is confusing (basically any competition between references and values is confusing). If you overloaded between `T const&amp;` and `T * const&amp;`, then that would be straightforward. (Edit: Although actually, I still forget how array-to-pointer decay in /u/tcanens example interacts with overload resolution here - one reason I prefer to avoid overloads for things like this.)
From reliable hearsay, the limerick was written by Andrew Koenig and the preceding paragraph was initially put together by Bjarne in an attempt to get the reader's eyes glaze over and miss it. (there are other easter eggs in the standard, too...)
CLion has pretty good support for GoogleTest, I can right click on a specific tests macro and have it set the right filter to run just that test. Bonus points for showing me the command line to execute, so I can send a oneliner to coworkers.
One big thing to note is how different the building process is. With c# it's all automated through a visual studio or the "dotnet build" call. In cpp you will have to write and maintain your own build scripts and environment which can be a headache coming from modern languages. I know visual studio attempts to encapsulate a lot of it on the windows side, but it is not as clean as c# and prone to breaking, and if it breaks and you don't already understand the entire build process it can be frusterating. I recommend learning to build from make and cmake in a nix environment before touching an ide that attempts to automate it (because it will break).
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6t30hm/how_to_get_started_with_c/dlhjb2x/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Need more hands? I'd be happy to get in on this.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6t24fv/best_advice_for_moving_to_c_from_c/dlhjezn/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Only experience I have so far with compiling cpp programs is using g++ to compile simple programs. I have no idea about how make and cmake works or even how a full project would be compiled(maybe header files are referenced in the main cpp file and they're all compiled from there?)
The Google Test adapter is a community GitHub project. See here the code for test discovery and note that it explicitly look in the compiled executables' debug tables to find tests (hence why compilation is required): https://github.com/csoltenborn/GoogleTestAdapter/blob/master/GoogleTestAdapter/Core/GoogleTestDiscoverer.cs The complaints about the compiler model seem superfluous since even primary competitors like QtCreator use googletest plugins that eschew compiler models in favor of hacks, e.g. regex scans in QtCreator's case (which avoids needing to compile first but has its own set of problems). See https://github.com/OneMoreGres/qtc-gtest/blob/master/src/TestProject.cpp Microsoft is - if anything - leading the charge in fixing these kinds of issues with the [Language Server Protocol](http://langserver.org/). Clang by itself doesn't really solve any of these problems in a satisfactory way as using the libraries and parsing directly, but with clangd (Clang's implementation of Microsoft's LSP for C++) it will be much more useful for various IDEs. Microsoft of course also has a C++ LSP implementation, though notably it's only so far used in VSCode and not VS proper (yet). Debates about the merits of UI are what they are; I'm sure the maintainers of either plugin would appreciate constructive input on improving their respective UIs or core functionality like test discovery. :)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [csoltenborn/GoogleTestAdapter/.../**GoogleTestDiscoverer.cs** (master  fd10b81)](https://github.com/csoltenborn/GoogleTestAdapter/blob/fd10b8110bf0f057ed73463223412502b1925272/GoogleTestAdapter/Core/GoogleTestDiscoverer.cs) * [OneMoreGres/qtc-gtest/.../**TestProject.cpp** (master  c9f265b)](https://github.com/OneMoreGres/qtc-gtest/blob/c9f265ba8262f266464ebe74bd008f6e488e70ed/src/TestProject.cpp) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dlhjg4s.)^.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6t1zna/visual_c_ide/dlhjgc3/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
nice.
The Dimov/Abrahams example from an old Herb Sutter blog illustrates this: http://www.gotw.ca/publications/mill17.htm 
&gt; // Don't use reinterpret_cast. It is hard to maintain safely. Hell, I'm just happy when people don't use C-style casts...
So hopefully a master on this will chime in. Otherwise there is plenty of good resources on the web. However, i'll give explaining it a shot. So you have used g++, and one thing you might have noticed is that it sucks. You have to manually add every file as an argument of the command and this becomes impossible to maintain with any reasonably sized project. So some smart people created "GNU make". It is a scripting language to make this building process a bit more maintainable. However, the devs viewed os independent building as out of scope and it was developed in the late 70's so the syntax is in my opinion garbage. So some smart people made "cmake", to automate creating make files. If I were you, i'd google a couple make and cmake hello world tutorials. I'd also research some conceptual information around compiling and linking.
Thanks, you explained that really well. I'll have a look at cmake right now.
cool ! you can join my discord (check the other comments) I have to admit I wasn't expecting 5 people too join I don't really know what to do 
Ill hop on around 6pm ish chicago time and be on all weekend. A safe starting point is to start discussing tools, meeting times, project management, and some high level requirements.
Better IDE support for C# is to a large extent C++'s fault. C++ is a nightmare to parse. Macros abound and #includes are everywhere.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6t1249/programming_my_first_game_with_c/dlhpg8u/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
... so... what do you do when you have a C API where you put callbacks that can take a void* as parameter so that you can do interesting stuff inside ? e.g. typedef void (*foo_callback)(float*, int n, void* context); void foo_add_callback(foo_callback*, void* context); ... void my_callback(float*, int n, void* context) { my_class* c = ???(context); } ... foo_add_callback(my_callback, &amp;instance_of_my_class); and you want to pass (and get back) a class instance as context. What can you do if you can't use any kind of cast ? Every operating system audio API works like this for instance. FFMPEG works like this. Many C network APIs work like this. etc etc... 
I think time is going to be one of the biggest issue, we probably all live in different time zones, 6pm is 1am for me, no problem right now because I'm on holidays but it might be problematic in the future
That's a `static_cast`. Casts, including `reinterpret_cast`, can be used safely in modern code. However, they should be avoided as much as possible, and they should always be C++ casts (which are intentionally verbose, obvious, and easy to grep for). C casts, *including functional-style casts*, should be avoided because they are visually subtle, and because they can take on numerous forms depending on the types involved (C casts are willing to do `static_cast`, `const_cast`, and/or `reinterpret_cast` functionality, according to arcane rules).
And `static_cast` is not a cast in the traditional sense of the word, as in a zero-cost operation, but another name for conversion. Except, just like what C++ casts are for a C cast, a `static_cast` is more greppable than other forms of conversion. Foo fobj; Bar bobj(fobj); // convert Bar bobj2 = fobj; // convert auto bobj3 = Bar(fobj); // convert // ... static_cast&lt;Bar&gt;(fobj); // convert 
Is there a way to enable the guidelines checker from a CMakeLists? And would it also work with the folder view?
Last I checked the Core Guidelines checker, it didn't support the _xp toolsets. Which unfortunately makes it fairly useless for me :(
I don't know if they use C++ or not, but you can always check out [Thorn](https://wearethorn.org).
Can we come to a conclusion - always write a (meta)test
 my_class* c = static_cast&lt;my_class*&gt;(context); ? 
What we really need is a catch adapter plugin for Visual Studio!
As usual, the fact that we're blogging about 15.3's imminent release means that we've been working on 15.x for quite some time now (my last feature for 15.3, the `is_invocable` rename, was checked in on May 15; my deprecation checkin on June 5 went into 15.x). This means that if you have requests or suggestions for our second toolset update, now's the time to tell us!
"The STL no longer marks functions as __declspec(dllimport). Modern linker technology no longer requires this." Where can I read more about this?
Nothing to see recently because "modern" in this instance is "in the last 20 years". Example: https://blogs.msdn.microsoft.com/oldnewthing/20060726-00/?p=30363/
&gt; If you forget to declare a function as dllimport... You **do miss out on the optimization** that dllimport enables, but the code will still run. So dllimport still needed. Since that time nothing has changed?
Not as far as I am aware (but I didn't make this change so I'm not positive -- this was /u/STL 's thing). That said, cost of crossing a DLL boundary is huge compared to a jmp instruction even if nothing has changed.
I'm not aware of good documentation about this - I learned about it through in-person conversations and checkin notes. Basically, `__declspec(dllexport)` and `__declspec(dllimport)` aren't mirror images of each other. Functions must be manually marked to be exported, through [`__declspec(dllexport)`](https://docs.microsoft.com/en-us/cpp/build/exporting-from-a-dll-using-declspec-dllexport) or [DEF files](https://docs.microsoft.com/en-us/cpp/build/exporting-from-a-dll-using-def-files). However, functions don't have to be manually imported. Having an ordinary declaration of a function is sufficient for the linker to say "let's try to link this function statically... it's not available, so I guess it must be provided dynamically". Here's where the not-very-well-documented behavior comes into play. The [`__declspec(dllimport)` documentation says](https://docs.microsoft.com/en-us/cpp/build/importing-into-an-application-using-declspec-dllimport) "Using `__declspec(dllimport)` is optional on function declarations, but the compiler produces more efficient code if you use this keyword." This used to be true in the past. Today, two things have changed. First, the extra cost is just not a big deal these days (it's very small, like an instruction or two, as I understand it - note that I am not an expert here). Second, the linker was improved a while ago to optimize this minor cost away, providing `__declspec(dllimport)` efficiency without the need for that marking. This optimization relies on LTCG being used, but our compiler relies heavily on LTCG to generate highly efficient code in the first place. (This was implemented in March 2009, and must have shipped in VS 2010.) We made this change to simplify our STL implementation, and enable some obscure undocumented scenarios (basically Windows wanted to replace some functions in the STL, which is hard when they're marked as `__declspec(dllimport)`). Note that **only functions** can have `__declspec(dllimport)` elided. **Classes** and **variables** must continue to be marked.
Ok. Thanks for the explanation. JFI, we do not use LTCG for our software, because it's difficult to configure its profiling (it requires several PC to run).
There's a difference between LTCG (Link-Time Code Gen, also known as Link Time Optimization for other compilers) and PGO (Profile-Guided Optimization). LTCG doesn't require profiles, it just requires more build time.
I just thought that LTCG did not give good results without PGO https://msdn.microsoft.com/en-us/library/xbf3tbeh.aspx?f=255&amp;MSPPError=-2147217396 (BTW something wrong with docs markup)
My google-fu took me here: http://www.apress.com/book/9781430258308 I did not read it and can't vouch for its content, but the chapters seem to cover what you asked. 
&gt; I just thought that LTCG did not give good results without PGO I think you're thinking of LLVM.
No, I am talking about Link-time Code Generation
My impression from speaking to the compiler backend team about performance is that they consider plain /O2 to be meh, LTCG to be good, and PGO to be ideal (but it is an ideal that is rarely achieved by customers due to the difficulty of profile collection). As far as I am aware, LTCG gives good results without PGO, and better results with. If you think about it, LTCG enables many optimizations that are good even in the absence of profile data. For example, cross-TU constant propagation. If you know that an argument is always true or whatever, you can optimize accordingly, you don't need profile data.
Right, and it's LLVM whose LTO doesn't give good results without PGO, not VC++. :-]
Thanks again
Unfortunately, the guidelines checker still produces many many bad warnings making it too difficult to use. For example, the 'use const' checker works fine for basic built-in types but not for typedefed types including built-in ones like size_t: const size_t len = sizeof(AnnotationHeader) + length + ((style == IndividualStyles) ? length : 0); warning C26496: Variable 'len' is assigned only once, mark it as const. (con.4: https://go.microsoft.com/fwlink/p/?LinkID=784969) Another problem is that it sees C-style casts everywhere: for (const CharacterConversion &amp;chConv : characterToConversion) { warning C26493: Don't use C-style casts that would perform a static_cast downcast, const_cast, or reinterpret_cast. (type.4: http://go.microsoft.com/fwlink/p/?LinkID=620420)
&lt;filesystem&gt; (C++17) please! :-) Also: P0452R1 (Unifying &lt;numeric&gt; Parallel Algorithms) and P0024R2 (Parallel Algorithms).
Meow, You can tell /u/STL wrote these notes!
I definitely wrote the cat-themed parts (and I wrote up Casey's work while he was on vacation). However, Billy was responsible for most of the STL fixes and their writeups. Meanwhile, Steve did the archaeology to figure out our C11 Standard Library support status, which I summarized.
Billy's working on all of that :-) Note that while we're adding the Parallel Algorithms to VS 2017 updates, our Filesystem TS implementation needs a binary-incompatible overhaul before it can become the Standard Filesystem implementation. Therefore that will ship in the next major binary-incompatible version of the STL.
ALL HAIL THE STRING PERF https://twitter.com/MalwareMinigun/status/814196564507799552
Current parallel algorithms implementation status -- note that this is subject to change following additional testing and similar, I'm just talking about what you see if you play with trunk. The following are "done" (but all the initial parallel algorithms are going to be somewhat experimental): * for_each * for_each_n (these use static partitioning) * sort (work stealing + quicksort) The following will not be parallelized. We built parallel reverse, and it was ~1.6x slower than the serial version. And then to make sure I was not just being stupid we tested with HPX's implementation and it was similarly slower. It seems the cost of getting the data onto other cores' L2 exceeds any computation cost. As a result we will ignore requests to parallelize algorithms that only move or copy elements (no comparisons): * copy * move * swap_ranges * fill * fill_n * reverse * reverse_copy * rotate * rotate_copy We currently only care about par, and have no plans to take advantage of par_unseq at this time. 
auto buffer = std::make_unique&lt;int[]&gt;(buffer_size); Please tell me this is MS fix and not the one suggested by GSL... I mean saving 2 pointers to not use vector... :/
Hey, in a couple of weeks it'll be XP's 16th birthday!
Code analysis is almost always heuristic-based You're going to have false positives with any analyzer. The question is where you strike that balance between missing a valid issue and reporting on perfectly good code. The first problem--typedefed types--seems a bit difficult. Our analyzers know about library types, but the typedefs probably throw things off. The second problem you mention just seems like a bug. I'm surprised you get a cast warning from that for loop. But as I said, this stuff isn't straightforward. If you're interested, there's a whole section in the blog post on [catching `const` violations](https://blogs.msdn.microsoft.com/vcblog/2017/03/07/check-for-const-correctness-with-the-c-core-guidelines-checker/) about a rule we implemented and decided not to enable because it fired too much on valid code.
LTCG has two main benefits over /O2: cross-module inlining, and a Whole Program Optimization (WPA) stage. LTCG is probably worth it just for the former, especially if you write modern C++ which has more small functions than old C-style code. 
Thank you, /u/dodheim, but I'll say that LLVM's LTO is catching up :) Thin LTO is especially neat because it more or less just does the cross-module inlining (and a little more optimization with hints recorded in the IL from the front end compiler.) In general, though, MSVC is better at whole-program optimization whereas other compilers often do better with global (function-scope) optimizations. 
Not just saving 2 pointers. As a reader I immediately know this array can't grow, and that size == capacity. Using the tightest data structure you can is good practice, it limits possibilities for readers.
Why not `std::array buffer&lt;int, buffersize&gt;;`
If you can that's even better, but then buffersize must be known at compile time.
Missing C++17 features that block 15.3 for me: inline variables (soft block, can work around it) and fold expressions (hard block, used to avoid recursion in meta-programming). Re inline variables, I think you already support it partially, namely the case of `constexpr` variables.
Is something like `-Og` will be implemented in MSVC? So optimizations that don't interfere with debugging are enabled? It makes running and debugging much faster (especially in game developpement) and I only need to disable it when I do deep stuff.
I think we have very different ideas of what it means to be blocked, but I'll ask about the priority of fold expressions the next time I talk to our compiler FE dev lead. Note that `conjunction` and `disjunction` may help you work around the need for folded `&amp;&amp;` and `||`. Inline variables are already on the priority list due to the STL's need for them. According to my understanding, `constexpr` does not yet imply `inline` semantics for variables as required by the Standard.
I'm not sure if the compiler backend team is looking into such optimizations, but I'll ask.
With blocked I mean that those are the remaining features preventing me from adding 15.3 to the list of compilers that can handle my C++17 projects. Perhaps similar to how a library writer can be blocked by missing compiler features? Non-native speaker here, so if I misused terminology, I am happy to be educated. Yes, know about `conjunction` and friends, but my use case is in applying runtime lambdas over a list of types (similar to Boost.MPL `for_each`)
I had the same trouble when using these checkers on Catch... Ton of false positives, meaning I just skipped entire categories of errors.
... Metaclasses :D :D :D More seriously, template&lt;auto&gt; would be so useful
Just a question, but is this behaviour still present? http://aras-p.info/blog/2015/12/11/careful-with-that-stl-map-insert-eugene/ 
Yes - fixing that is on my todo list (tracked by an active bug), but at a lower priority than other work.
I would use "hard block" to refer to things that cannot be worked around. For example, I can't add deduction guides for STL templates until my compilers support class template argument deduction. "Soft blocks" have workarounds. Fold expressions can be implemented with different metaprogramming, and not necessarily recursive (`make_integer_sequence` is very powerful).
OK, in that case fold expressions are not impossible but significantly harder to work around than inline variables (I can use a few macros to mask keywords and emit out-of-class definitions). Plus fold expressions just look soo nice :)
Great job guys, Can't wait for the release!
&gt; Code analysis is almost always heuristic-based So this means that hoping 2 whitespaces before variable to see if a mysterios char sequence(" const") appears requires compiling the code? :) I mean I know you work for MSFT but if you are gonna invoke "IT IS A SUPER HARD PROBLEM NOBODY KNOWS HOW TO FIX" explanation for every bug you will loose credibility. Also I am not sure that not using compiler for this was the right call but I believe people who make this decisions had good reasons. P.S. If you are interested in VS bug I hit recently but I could not get it to repro on small project so you will need to convince some developers to trust me and put monitoring for this: I changed static_assert(true,"bla") to static_assert(false, "bla"), IDE was happy to compile code (lol) and run the program but when I wanted to step into the function with this static_assert(false,"bla") IDE rightfully complained that I need to locate my_header.h (where this function with static_assert was) since I guess MD5 missmatch check worked correctly. So basically I moved on in my project so I can not easily trigger this bug any more since like I said small repro did now trigger it. But if you trust me :) or you have more reports of this you could ask devs to put checks in internal builds so you may catch this bug. So the check would be something like if the user clicks F5, does not modify the file in solution and debugger can not debug into the file since file is out of date this is a bug.
Sorry, what do you mean by the cost of crossing the DLL boundary? Once the DLL is mapped into the process address space, why should there be any extra cost for calling functions in it at all?
fold expressions and template&lt;auto&gt; are things I use far more than I probably should, definitely would like to see those come sooner rather than later!
Is there any hope of improving the efficiency of the &lt;regex&gt; implementation?
because stack memory is precious, heap isn't(aka it would work for 256 bytes but 256kb may give you stack overflow) 
I had a similar issue in the past and I used clang-tidy as a base for that. Added a new check using a script in the repository to write all the boilerplate, then implemented it and applied the suggested fix my code proposed. Can't recommend any documentation for that, but it should be a good start since there are tons of checks implemented and examples.
boost, It is super high high quality, but I bet there are bugs that need fixing. The only problem is that bar is quite high, so I would not do it if boost maintainers will spend more time telling you what you did wrong than if they fixed the bugs themselves. Obviously there is a learning curve, but what I say is that if you want to get into project for first month or so you will probably be a negative contribution because you need to ramp up and if you quit after a month :) it is kind of a jerk move to do.
When std::regex blows up due to excessive stack use (https://stackoverflow.com/questions/27331047/c-std-regex-crashes-in-msvc-during-long-multiline-match/30128172#30128172) I simply switch to lexertl. (Note that you require **.{+}[\r\n]** on Windows now instead of **.|\n** in that example) If your regex requirements exceed the DFA implementation (think lex/flex syntax but with support for non-greedy operators built in), for example you want to use captures, then you could consider parsertl too. I have even implemented parsertl::search() recently. This works surprisingly well (https://www.codeproject.com/Articles/1197135/gram-grep-grep-for-the-st-century). Usage of these tools has allowed me to be far more ambitious in the text processing problems I tackle at work. If you have any questions, feel free to ask. 
Thanks for information, I'll look into those. I guess the problem for us is that the same std::regex running under msvc stl is much slower than running on clang with libc++. In the past we've had to artificially limit some functionality on Windows based on this. We have since been able to refactor the regex in question to perform better on Windows, but the fact remains that's it's simply a less efficient engine than libc++. We've seen lots of great recent improvements in msvc stl in things like vector, string etc... It would be nice to see a regex overhaul soon too.
See this: https://github.com/pfultz2/Cloak/wiki/C-Preprocessor-tricks,-tips,-and-idioms Otherwise, try getting through the Boost.Preprocessor library. It seems like a pretty much ultimate preprocessor abuse.
Nice of them to reference clang, which is presumably how this checker operates. Standing on the shoulders of giants...
Our team would really appreciate such an optimization level. 
There are plenty of excellent examples in the clang tools extra directory. Follow the steps here: https://clang.llvm.org/get_started.html Make sure to do step 4, no need for you to do 5 and 6.
Yes, KDE is one of your best choices. The friendly and helpful community is very valuable, and most of the commits are reviewed. Through these code reviews, you will likely get above average quickly, if you are willing to learn. Choose Linux and Plasma as your Desktop Environment and start contributing patches to Plasma, or the underlying KDE Frameworks libraries. If you go this route, you will very likely also find very good jobs, if you are willing to relocate. You can get started by subscribing to [the kde-devel mailing list](https://mail.kde.org/mailman/listinfo/kde-devel).
If somebody cares... surprise surprise ThreadRipper is also trash for this particular benchmark: http://images.anandtech.com/graphs/graph11697/90039.png
We still have customers using XP on their lab control computers, regardless how Microsoft would like them to buy new licences. And believe me, being forced to use .NET 4.0 to target them also doesn't make me happy.
Yes, it would definitely be nice if MSVCs std::regex just worked better. Needless to say, I am not holding my breath! :-) There is still boost::regex of course if you want to go that route.
I don't care for Catch. It's not Catch itself that's the issue as much as it is the compilation time it adds. It's very slow to compile for some reason.
The new version did 13 more iterations (30 v 17) and took about half the time? Am I reading that right? Those are impressive gains. Is this your only test? Was this the result of only what was in the patch notes or was thee some algorithmic change?
What is the state of constexpr initialization of static const members. This: http://cpp.sh/9mnea
&gt; Therefore that will ship in the next major binary-incompatible version of the STL. Aaaw :-( I hope that'll be soon, as in 2017. I hope you can add that as a new toolchain to VS2017. Thanks for all the info and the great work to you and Billy :-)
For reinterpret casts, how can you do something like that? I want a type erased list of function pointer. Unfortunately, there's no`void*` for function pointer. template&lt;typename T&gt; using func_t = T*(*)(); std::vector&lt;void(*)()&gt; funcs; func_t&lt;int&gt; f1 = []{ return new int; }; // Is there a way I can do this without reinterpret casts? funcs.emplace_back(reinterpret_cast&lt;void(*)()&gt;(f1)); auto f2 = reinterpret_cast&lt;func_t&lt;int&gt;&gt;(funcs[0]); All function in the vector has different return type. If there's a way I can do this with static casts only, I'd be happy. Edit: more explanation.
Do you have any info for using clang/c2 with the latest version of boost? Would like to finally try it but having troubles with boost is sort of a stopper... 
Fixed it, thanks. My point about `_` still stands though - I am very skeptical that that is a clear solution. 
What I don't understand either from this article, and from the original talk (which is distressing), is *why* you would ever want to partially specialize function templates. Functions can be overloaded. In other words, you don't need to write: template &lt;class T, class U&gt; void foo(T t, U u); template &lt;class T&gt; void foo&lt;T, int&gt;(T t, int); Because you can simply replace the second one with template &lt;class T&gt; void foo(T t, int); End of story. No attempt to address this or even give examples of where going through the rigmarole with an implementation struct is preferable.
I'm not exactly clear on what you want to do (I have some suspicions) but I'm pretty sure that you can do it at least as easily without needing partial specialization, on any compiler.
Can you give an example where it's not applicable?
just throwing it out there, but you could always try to do it with a regex replace if the code you have to fix is not too complex.
/u/segfalt_WA would know better than I--he's our CMake expert. There's another post coming up (probably Tuesday) about using CppCoreCheck outside of VS. 
So here's an example: template&lt;class Plugin&gt; void RegisterPlugin(std::shared_ptr&lt;Plugin&gt; plugin) { gPluginManager-&gt;addFactory(plugin-&gt;getName(), plugin-&gt;getFactory()); } Now after a refactoring, we aim to support the "new format" of the plugin, that no longer uses `getFactory()`, but due to backwards compatibility we still have to support the old types. Note in either case the signature of function remains the same. I would design it as a function redirecting to class partial specializations. How would you design it? Relying on expression-SFINAE?
I'm not sure exactly what you mean by old type, new type. But if you can do it by redirecting to class partial specializations, you can do it by redirecting to overloads. And often the redirection isn't necessary. Can you write out a full minimal example?
So ok yeah, imagine the "old types" are user defined classes, that possess a function `getFactory()`, and the new types just don't have that. The real example is much more complex than this, but let's just say there is a function difference. Now what should we write in the overload? There is no way to know the names of user types, as they are not part of this library but user's code. 
I'd appreciate if you'd read my comment more carefully. First, I did not scream about "super hard problems" in all caps. And I certainly didn't provide that explanation for every bug. /u/nyamatongwe mentioned two false positives. I said the first was probably a heuristic failure--a "super hard problem" in your characterization. But I clearly said that the second looked like a bug to me. So I'm clearly not giving such ridiculous explanations "for every bug". After making that comment I sent mail to the dev who owns the const checker and asked him to look at that code. When I opened my mail, I saw that our dev manager, /u/spongo2, had already sent mail to the entire code analysis team with a link to this post saying "lots of useful feedback already trickling in on reddit". In my experience, this is typical behavior for the entire Visual C++ team. I'm not worried about losing credibility. I think developers who use our product know that we listen to feedback. I'd also like to defend my assertion that code analysis, being based on heuristics, is subject to false positives. But I have friends who will defend this for me: From [Coverity](http://security.coverity.com/blog/2013/Sep/gimme-a-break.html): &gt;For the Linux kernel specifically, we've observed that when there are multiple MISSING_BREAK reports within a single function, the false positive rate is much higher. .... Of course, the trade-off is that it might cause us to miss real defects, but we estimate that only 5% of the real defects in Linux are lost by this heuristic. It seems like the right trade-off for day to day scanning, with perhaps a more thorough review only rarely. From [Clang/LLVM](https://clang-analyzer.llvm.org): &gt;Static analysis is not perfect. It can falsely flag bugs in a program where the code behaves correctly. Because some code checks require more analysis precision than others, the frequency of false positives can vary widely between different checks. Our long-term goal is to have the analyzer have a low false positive rate for most code on all checks. And I can even find support from our friend Andrey at PVS-Studio, who wrote an [entire article about static code analyzers and false positives](https://www.viva64.com/en/b/0488/). I appreciate feedback on our product and I'm not trying to silence you or anyone. I'd just prefer to have productive, fact-based conversations. 
Didn't WannaCry teach them anything?
I'm not sure if I really understsand, but what's wrong with something along the lines of: template&lt;class T&gt; using has_factory = decltype(std::declval&lt;T&gt;.getFactor()); template &lt;class Plugin, enable_if_t&lt;std::is_detected&lt;has_factory, Plugin&gt;::value, int&gt; = 0&gt; void RegisterPlugin(std::shared_ptr&lt;Plugin&gt; plugin) template &lt;class Plugin, enable_if_t&lt;!std::is_detected&lt;has_factory, Plugin&gt;::value, int&gt; = 0&gt; void RegisterPlugin(std::shared_ptr&lt;Plugin&gt; plugin) Again though I think it would be easier to show you if you showed me a toy example showing what you're doing now.
Yes, yes, I know. Sorry to poke fun at /u/Ali1331 and your situation with my response! But I'll probably continue to do so. I won't be able to resist in a few years when we have interns who were born after XP was released who have to support developers who are still targeting XP. [Microsoft has always been very up-front about the lifecycle of their products, including Windows XP](https://support.microsoft.com/en-us/help/13853/windows-lifecycle-fact-sheet). The support you get from Windows and parts of Microsoft in the Windows ecosystem (like Visual C++) lasts far longer than the support you get from other operating systems. I'd love to show you supporting documentation but I don't think it exists. (E.g., [this question about Mac OS support](https://discussions.apple.com/thread/2199766?start=0&amp;tstart=0).) To be fair, some Linux vendors (e.g., [Redhat](https://access.redhat.com/support/policy/updates/errata)) offer comparable OS support lifecycles. So I don't think the company is trying to make a quick buck off of companies moving to a supported operating system. Rather, I think the company is trying to "save a few bucks" (actually, reallocate a few developers' time) by not having to make every technology work with every other technology. Making our code analysis framework work with XP (for the few developers that will not or can not move) means far fewer resources to make our code analysis work better for the vast majority of developers using supported operating systems. So sorry, but our code analysis doesn't work with the Windows XP-targeting toolsets. I feel for your pain, but it stops there. On the positive side, code analysis is all about correct code, which in turn is about reducing the cost of maintenance and decreasing the chance of bugs including security bugs. A customer who's still using Windows XP clearly isn't concerned about either of those things (especially security!) So maybe there's no market there anyway :)
Our code analysis framework is built on the MSVC compiler. We've mentioned Clang many times with regards to the CppCoreCheckers (they have an alternate implementation) but we didn't mention Clang in this post. So you're wrong on two counts. 
Probably trying to show off his web design skills
You got me with detection mechanism... good one. Though I don't think it is yet practical to use in projects (and does not compile on MSVC), it's indeed a better way than specialization. There is another use case of specialization: https://godbolt.org/g/WeMXbo I tried several times including tag dispatching, but none of those felt being an improvement. Edit: repost in case you haven't seen the edit
Because the optimizer can't see the code on the other side and thus has to consider all memory after the call as modified by the call. And it can't be inlined, etc.
&gt; The new version did 13 more iterations (30 v 17) and took about half the time? The time listed is per iteration, so iterations are twice as fast, yes. Because the improved version runs faster Google Benchmark needs to do more iterations to produce a confident result on how long the op takes. (The iteration count is listed so that if you look in a profiler with two benchmarks you aren't surprised when the benchmark the harness says is faster actually took more time according to the profiler, because it was run for more iterations) Check out Chandler's talk for more on this test harness. https://www.youtube.com/watch?v=nXaxk27zwlk &gt; Is this your only test? Pretty much. We have some more microbenchmarks but this is the only bit I would consider real-world. It turns out getting real-world stress tests for the STL's perf is difficult :(. If you have test cases that would be interesting to standard library implementers we would be interested :) &gt; Was this the result of only what was in the patch notes or was thee some algorithmic change? Just the patch notes over the last few releases. The optimizer folks haven't been asleep either in that time of course so some of the gains in that time can probably be attributed to them. 
I think [STL's comment](https://www.reddit.com/r/cpp/comments/6szojd/slug/dlhf4r6) illustrated some problems you can run into nicely. Anyway, the last time I wanted this was when my functions took `tuple&lt;Args...&gt;` in the template parameter list, but not the actual function parameter list, since all I needed was Args and they had to be separately contained from other template arguments. Granted by the end of that, I figured using a lightweight typelist type that I could take as a regular function parameter would be a lot less tedious. That is, tuple isn't meant for this since it also holds values, but it was already available, so it was my first choice. 
Someday it would be nice to derive something from Chakra's implementation, but at the very least such a change wouldn't be binary compatible. Boost.Regex' implementation crushes all of the standard libraries' implementations (by 20X in some cases), so at the moment we'd recommend using that instead. 
None of the standard libraries do a DFA implementation. Sadly this engine requires a stack slot for every potential backtracking location, and things like .+ allow backtracking on a per-character basis.
maybe a dumb question, but : why don't you (and not only you, but also other compiler vendors) just backport boost's implementation ? it's there, works, and is under a license that's as permissive as it gets. iirc there are small api differences but most of std regex is inspired from boost anyways.
That's the Java iterator model: an iterator doesn't point at an object, it allows you to get the next object or determine if there is a next element. This is also kind of how input iterators (but not forward iterators ) also work. They still point at an element, but that's often cached in either the iterator or the underlying stream. I would expect this talk to focus on the distinction between those two concepts and then propose something new? Different? Could be interesting. 
Yeah, in a previous project we did exactly that. In my current project we are not in a position to bring in boost so we are left with what's in stl. As u/doom_0o7 says, why can't you and other vendors just take the boost impl? Is it not the impl from which we got the standard?
As somebody who has authoritative knowledge on this topic, I don't entirely disagree. But the comparison to Rust isn't useful. Different language, different feature, different problems. No further discussion needed in that direction. Also, this isn't a language problem, it's a compiler problem. More, appropriately, it's a development environment problem. I think it's unfair to complain about the verbosity of diagnostics if you're not working in an environment that can filter and expand errors as you think they should be. Compilers are simple tools (however complex). Presentation of ediagnostics may be best left to an IDE. A direct analogy to event logging is not inappropriate here.
As to why we don't do that right now: 1. There are API changes, so it wouldn't be a direct port. 2. It would have to be _Ugly / __ugly-ized if taken out of boost. 3. It would be ABI breaking. As to why it wasn't done that way in the first place, I don't know, before my time. 
I think that's a fair assessment. But then I wonder, what language features help people implement functions? I honestly can't think of any language features specifically designed to help me wite e.g. sort. So maybe maybe the people saying what you've heard know some secrets of other languages that simplify generic implementations, but I doubt that. In fact, I'll say the opposite. The ability to add constraints (of any kind) to a generic definition makes implementation harder. Ask me how.
Brilliant. I'll be doing this for Beast to distinguish parsing errors from stream errors.
You could just specify Args explicitly to the template function directly by putting them first, you don't need a typelist type as long as other types are deduced.
We've accumulated lots of great improvements in the binary-incompatible "WCFB02" toolchain and we want to release previews as soon as possible, but there's still a lot more work to be done in both branches. The form in which it will ship has not yet been determined.
I don't agree at all. Generic libraries are written in terms of their concepts. Reflection might help with that. Metaprogramming solves an entirely different problem. 
Old Microsoft was very wary of open-source code, especially in shipping products. Nowadays things are completely different, but changing regex will be a major undertaking, just because it's so large and complicated.
There's a high-priority bug tracking that. It affects the STL too.
We're stopping development of Clang/C2 for C++ (it will remain at 3.8), and I recommend switching to Clang/LLVM. We're now testing the STL with Clang/LLVM 4.0, and I'll be upgrading that to Clang/LLVM 5.0 as soon as I figure out a problem with finding link.exe's location.
I see, sorry for the noob question, but you mean using exactly the original clang or a specific ms fork? How to use vc stl with it? Do you have any online pointing? I'd like to try to build our work codebase with clang for windows! 
Yeah, but neither libc++ nor libstdc++ went that route either.
Can you point us at the library in question assuming it's open? We are always looking for modern c++ test corpus
I don't own the backend stuff but as the ex-debugger dev manager I'm pretty interested in this. The way debugging has to be adjusted to work well with game dev is a topic of discussion lately. Can you tell me more about the scenario here? (unless /u/stl already knows in which case I'll just harass him on Monday) 
The current &lt;filesystem&gt; implementation does not support paths longer than MAX_PATH even when LongPathsEnabled is set in group policy and the process manifest specifies longPathAware (introduced in Windows 10 Anniversary Update) because parts of the implementation use statically sized MAX_PATH arrays internally. Will this be addressed by the eventual implementation of Standard &lt;filesystem&gt;? Additionally, is_symlink etc. does not appear to be actually implemented, in the sense it does not recognize NTFS symbolic links. Is this expected to change?
Well, I use this optimization profile for usual code/build/test workflow. I get something that is significantly faster than pure debug, so something closer of what I could expect when building it for prod. Also, since not all optimization are there, it's not so much slower to build. My use case is to be able to run the game at a higher framerate than 10 fps, test the functionality fast, and not having those debug loading time. Also, if I can trigger a bug that normally happen only in prod build while developing, it saves me a lot of time. If I can debug while the bug is triggered, than I'm super happy. 
Out of curiosity, does LTCG really have much benefit when unity files are used? (E.g.: UE4 uses this). I imagine this varies per project, but my personal experience is that any performance boost from LTCG was minimal and it just bummed out everyone since it added additional time to the link stage. We might've used it for final disc builds, but that was usually just done for the hell of it.
yep, so I guess (or rather hope!) that there is another valid reason for not using boost's implementation. 
Other than the boost::regex, the standard answer these days would be check out RE2.
NIH?
Yes, both will be fixed. We'll pass arbitrary stuff to Windows and if it handles long paths, it'll work. (We will not use `\\?\` syntax to attempt to activate long path support in the absence of the longPathAware machinery - that way lies madness.) Symlink support will also be implemented. These are some of the reasons we need to break bincompat.
According to my understanding, `-Og` contains those optimizations that don't result in debugger madness. For example, aggressive inlining and messing with the lifetimes of variables is hard to debug, but optimizing away conditions that are always known to be true might not be (for example, haven't checked).
I mean upstream Clang/LLVM. If you have VS installed, it tries hard to figure out the include/lib paths and use our STL. As of 15.3, our STL should work perfectly with Clang/LLVM with the exception of a handful of bugs we've filed against Clang, and the need to include `&lt;intrin.h&gt;` when using `&lt;atomic&gt;` because Clang/LLVM 4.0 doesn't support a compiler builtin that we need (this has been implemented in Clang/LLVM 5.0, whose release is imminent).
It's almost surely too much to ask, but it would be nice if this were better known, e.g. pointed out in the documentation. I did learn it on my own, after replacing `boost::regex` with `std::regex` in some performance-sensitive code, but it was a really bad surprise (BTW, but singling out msvc here, gcc was just as bad if not worse). I ended up rolling back the change pretty soon afterwards, but I wish I knew std version was that much slower before spending time on it... 
Thanks, will come back to that in the coming days. First, I want to try and test my code on the latest VS update myself. It's a 11 kloc template library compiling cleanly at `-Weverything` on Clang 5.0 and higher, as well as on gcc-7, and it has been pretty good at finding bugs and (rarely) ICEs in both compilers. There are some dependencies I need to clean up so that the repo is self contained. 
Possible typo: s/conde/code/
I hope you have thorough tests for your regex patterns to make sure your stdlib actually works with them. :-] My personal experience with `&lt;regex&gt;` has been that if libc++ or libstdc++ performs significantly better than MSVC for a given pattern, their output is also wrong (libstdc++ in particular will _happily_ hand you bullshit results).
Haha fair point. Yeah in this case it is actually really well unit tested. Otherwise I don't think we would have been confident enough to refactor it to fix performance on Windows. The regexes in question have evolved over 3 different generations of products we've worked on over the last 8 or so years, and the test cases have been passed down also, expanding as time goes by and more interesting edge cases are found and bugs are fixed.
There was more than one typelist.
This paragraph [was read out loud](https://youtu.be/ZLNq-4IiNTY?t=1m32s) by Michael Caisse at Meeting C++ 2016. 
&gt; For example, aggressive inlining and messing with the lifetimes of variables is hard to debug, but optimizing away conditions that are always known to be true might not be (for example, haven't checked). Not entirely, but close. We definitely want inlining, and that's the one optimization we turn on even in debug builds with Visual Studio. Inlining by itself is almost entirely harmless; all it requires is that the debug info map the generated code to its source, which is easy enough to do with most compiler frameworks. Optimizing out conditions is also _not_ acceptable, as one of the nicer tricks to pull in debug builds is to tweak global variables' values in the debugger to alter debug-only behavior; that kind of code folding is only safe if it's something `constexpr`-like that can't reasonable be changed at runtime. In C++ and especially more modern or more functional-style C++, the absolute biggest most essential critical optimization of all is inlining, and that one is totally safe for debugging so long as the compiler propagates debug info correctly. Inlining is important because of all the little helper/wrapper functions used everywhere. Something as simple as `std::move` for instance which serves _zero_ runtime purpose absolutely critically _must_ be inlined to avoid a rather absurd overhead in modern C++. Or take the popularity of smart pointers and the need to inline `operator-&gt;()`, or the importance of `std::vector::operator[]`, and so on. Further example: one of the reasons game developers so often rewrite even `vector` is just to make sure that the implementation of the code never calls helper functions without true necessity (e.g., implement `empty()` as `return _first == _last` rather than `return size() == 0`) to avoid per-function-call overhead and to ensure that their iterators are always just raw pointers (for which compilers produce usually reasonable code even in debug builds). The difference between inlining on and off can be the difference between an interactive application and a slideshow. The difference between enabling only inlining and enabling all other optimizations can be the difference between 20fps on a high-end dev machine and 60fps on a consumer machine; critically important for the "release" builds (be those the publicly shipping binaries or the internal "playtest ready" builds), but not so critical for day-to-day game engineering work. A very related item is that of build times. We want optimizations so our game runs well, we want only the optimizations that don't destroy debuggability, and we _also_ want only the optimizations that take relatively little time. Modern C++ can already be terribly slow to compile thanks to the heaps of (high-value!) abstractions like smart pointers, containers, function wrappers, and so on; the absolutely last thing anyone has ever needed is a slower compile. Good debug build optimizations would thus also be those that don't add a lot of passes or other expensive transformations or analysis to the compilation process.
&gt; I'd just prefer to have productive, fact-based conversations. The problem is that you can use equivalent of your argument about false positives for not doing any testing("linux kernel has bugs, so IDK why you are surprised our product has, idea to introduce testing is stupid"). I know most people do not think like this and your line of reasoning is often used by PR people, but we are not "normal" people here at r/cpp since most of us know all about statistics, fallacies, biases... So to be clear: this const size_t bla = 3; being broken and shipped is not result of some magical trade off between precision and recall, it is just a bug. A silly bug that should have been caught by a test and blocked the release of the feature. But I understand that it is tricky to think of this when writing tests since when you think typedef you think that it is not common, but unfortunately size_t is a typedef and it is used commonly. Note that in retrospect most bugs are silly, so I am not saying that people working on this feature are bad devs or anything like that. All I am saying that best response would be to say: "ups we will fix it". If you do not agree then I can just tell you what I said before: there is thin line between giving your customers 90% of the benefit of the product earlier and getting feedback and shipping product that is mediocre and using your users as free QA. tl;dr your best response to this would have been: "Ups, we should have done better, I will try to prioritize fix of this, should not be hard(tm)" instead of using justification that all static analysis tools have bugs. P.S. I am not hater of VS, I use it and recommend it to all my friends doing C++ development, especially if they are using certain other purple logo IDE. :) 
Well, I used is_detected to make it less verbose, it's very easy to write your own detection trait with the void_t trick, I'd be really surprised if that doesn't compile on MSVC. Also, if you implement is_detected yourself (it's like 5 lines), why wouldn't it compile? There are multiple ways to solve that use case of specialization. The simplest way conceptually is to leverage base to derived conversions. struct fallback{}; struct specialized : fallback {}; template &lt;class T&gt; T from_bytecode(ByteCode b, fallback); template &lt;class T, enable_if_t&lt;std::is_enum_v&lt;T&gt;, int&gt; = 0&gt; T from_bytecode(ByteCode b, specialized); template &lt;class T&gt; auto from_bytecode(ByteCode b) { return from_bytecode(b, specialized{}); } This is a common meta-programming trick. Indeed, you can simply define a class templated on an integer, that inherits from itself with a value one less (and specialize 0) to be able to basically use integers to directly specify priority. I don't really address the issue here of how to make `from_bytecode` work for a user defined type. You could easily add a tag type to the signature that would be templated on the type though. This would cause lookup in the type's namespace, i.e. ADL. Users of the system then only have to define a free function in their namespace with the correct signature; cleaner (IMHO) than specializing a template in yours. Here it seems a bit artificial but when you have functions that take types (like `to_bytecode`), you start to see some of the problems with class specialization vs the ADL approach. class specialization is exact, so you will never have the benefit of any form of implicit conversion. With the ADL approach, the customization is just a normal function typically, not a template specialization. So you get implicit conversions. Good discussion here: https://akrzemi1.wordpress.com/2016/01/16/a-customizable-framework/
Isn't /Zo option already doing this?
&gt; Fixed missing specialization auto_ptr&lt;void&gt; WTF?! I thought is it gone from C++17
Just for posterity's sake, Eclipse (CDT) also has built-in support for GoogleTest. 
One thing that I would find useful is to have `__forceinline` work in debug. Not necessarily for speed, but to avoid having to step through functions that are just wrappers.
Not really. More, afraid of the "viral nature" of the GPL. The world has changed a bunch in 10 years. 
Unity is really interesting. You should get most, if not all, of the benefits of LTCG just from global, cross-function optimizations. It's definitely on our radar. 
Thank you for your opinion. I appreciate that you always take the time to engage, even if I might disagree with your approach or some particular arguments you put forth. And thank you for supporting VS and Visual C++. 
That's correct. But we provide a C++14 mode (and an escape hatch that provides it in 17 mode).
Huh? Boost.Regex isn't GPL.
I totally missed context on that thread. Disregard my comment. Sorry.
lol -- nothin to be sorry about :)
Hard day in the sunshine. I need to stay inside, chained to a desk. 
Given the number of false positives as well as the marked decrease in compilation performance, I'm assuming you aren't trying to add the core guideline checker to your normal compilation process anyway. So one thing you could do is add an additional build mode, and equip it with a newer toolset and the checker, and run that at regular intervals. As for XP being old, I'm required to support it until one particular (and pretty expensive) ISA card finally breaks... It is hardly ideal, but the customer isn't about to spend $6000 or so just so I can use vcpkg. 
I found the [GCC manual](https://gcc.gnu.org/onlinedocs/cpp/) on the preprocessor to be quite detailed.
It only remains to show how to make those switch cases exhaustive :D
Actually, the GPL probably did play its part - just the other way around. Bah!
have you ever checked out natstepfilters? https://msdn.microsoft.com/en-us/library/dn457346.aspx
thank you for this detailed writeup. very helpful.
deleted ^^^^^^^^^^^^^^^^0.0081 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/23978)
Thanks for your detailed explanation. I wonder if the templated tag type is something like this: https://godbolt.org/g/6frpAM 
deleted ^^^^^^^^^^^^^^^^0.5700 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/27077)
I found this blog post really difficult to read, it's littered with English mistakes and bad grammar. I would advise you to revise your posts more carefully before posting, and sincerely consider to improve your English, run it through a good spell &amp; grammar checker program (please don't use LibreOffice, use a proper one), or get it proof-read by a native speaker. There was some interesting thoughts in that blog post once I got past the language issues! Mistakes are fine as long as they don't make it harder to read the content, which in this case they do, sometimes to the point where one needs to guess what you could've meant (e.g. "While under review for the standard APIs change, so that a 1:1 replacement is not always possible."). Just a few comments: * You often confuse it's &lt;=&gt; its * commas are used badly (in a "German-way") (e.g. "Even for new libraries I think, that authors") * hyphenation/word composition (e.g. "counter parts") * You need to learn that often ~~once~~one can't just translate the sentence structure and word order 1:1 from German to English.
i agree that the word order is not correct in most cases (i am not a native English speaker by the way). For me it's not that hard to follow, but maybe that is because my language (dutch) has a similar sentence structure. By the way, with your last sentence you probably meant: "You need to learn that often ~~once~~one can't just ...
Yeah, I think I was responding to Stephen's comment about being afraid of open source, while missing the fact that the thread referred to Boost. Fear of the GPL made Microsoft afraid of anything open source. 
Most of the time when I'm migrating to the standard library APIs the process comes along with a decision to raise the minimum required C++ standard for my source code. But I can imagine wanting to support both in favor of the faster compile times on supporting standards. Doing this for all my APIs independently seems like an annoying amount of extra work / code duplication though, perhaps this would best be expressed as a library that provides transparency between boost and std, rather than a coding practice.
That's not actually what they're telling you. They only tell you that your definition conforms to the constraints. That's entirely different than "properly constrained". I'm also not sure how you get from "properly constrained" to "won't break unexpected dow the road". You would only get there if you never touched those definitions once written. I think it would be helpful if a tool suggested constraints. I think it would especially harmful if the compiler tried to deduce and then enforce constraints.
Well, its not my first language, and some german grammar probably shines through.
It's just "concepts" now. Concepts Lite was the initial proposal. And the goal was never "to get exactly the kind of error messages that Rust is getting". The goal was to move errors from the instantiation stack to the point of use, and to try to improve the quality of diagnostic information. Somewhat ironically, generating quality information at the point of use can mean a greater quantity of diagnostics. Concepts can give very detailed reasons why e.g., `A` cannot be printed, including that `os &lt;&lt; x` is not valid expression. Most of your original post is concerned with overloading, which is entirely orthogonal. As soon as you have an unresolved overload, you're going to get a list of candidates and an explanation why each is viable or not. I think all compilers do that these day. This is where tooling is important. Compilers can generate a large quantity of high quality diagnostics. How a user best consumes that information has nothing to do with language features. 
I wish VS code also gets one as a plugin 
It's an unwritten law of the internet, that you need to make grammar and spelling mistakes in any post where you're criticizing somebody else's grammar and spelling.
deleted ^^^^^^^^^^^^^^^^0.9934 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/79076)
https://www.reddit.com/r/cpp/comments/5pqv8o/whats_the_best_c_beginners_book/dctj5k1/?context=1
deleted ^^^^^^^^^^^^^^^^0.8483 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/78775)
Does this choice make this implementation non-standard or does the standard allow an implementation to take the execution policy as just a hint? If non standard, does this mean you need to report to the standard to allow ignoring the policy?
Yes, the term for this is [Muphry's Law](https://en.wikipedia.org/wiki/Muphry%27s_law)
I've started to learn C++ from primer too and I really like it. It could be a little difficult for someone without prior programming experience. Getting MSc in Electrical Engineering (power systems, not really something related to programming) surely is bonus to me, so maybe my experience with this book is not relevant for some further readers. Anyway, I've red really good reviews and recommendations for this book so I if you do not experience some high barriers with it, go on ! 
"c++ primer" is a different book from "c++ primer plus", which gets considerably worse reviews.
Haha yea! So much :-) Even when you try extra hard. That was a case of thinking one word and writing another, and not even spotting it by reading it again. It definitely happens!
Yep, exactly.
I always find it sad when grammar and spelling mistakes are the number one discussion point of some post about something completely different. I see it often with German forums and rarely with English communities. It's fair to point out that there may be many mistakes and that something needs to be fixed, but not with it being only feedback besides "interesting thoughts" - a phrase as useful as no feedback. You provided good tips, yet the underlying tone comes across quite a bit elitist. Feedback can also be given in a way that doesn't shame the author about their English skills or make them insecure about writing their next post. Writing things doesn't come easy for everyone and writing in a non-native language makes it even harder. Putting yourself into that situation, how does it feel to get as most up-voted feedback "your writing sucks"? My point really is, that feedback about grammar and spelling-mistakes shouldn't take over the whole discussion of a post and that said feedback can be provided in more digestible way. I personally always try to follow the rule: First provide feedback on the topic and only then point out some mistakes. If you have nothing to bring to the discussion table you might as well just PM the author.
Roger that. I'm preparing a follow-up post publishing Tuesday and I will mention your remarks, if you don't mind.
I think the switching between APIs is a neat trick to keep in mind, however I question the usefulness of it for boost/std. The assumption that boost's API will translate nearly 1:1 to std's API seems a bit overly eager and providing potential fixes through more defines, etc. as mentioned makes me more wonder what the goal is. Is this really "future proofing" or is this more backwards compatibility? Once I have a std version, why would I need to keep the boost version around? And as long as there is no std version, do I have another choice than to use the boost version? I think, being ambiguous about the library in use, makes the code itself just harder to reason about and one would have to ensure that the behavior of both libraries are the same in all situations. Alternatively, I'd suggest to simply use the adapter or facade pattern, so you have known types to work with, while in the background the code can be changed.
https://www.reddit.com/r/learnprogramming/comments/188k8v/why_is_c_primer_plus_6th_edition_a_bad_book/ https://www.reddit.com/r/cpp/comments/5cdvme/c_primer_plus_why_are_people_against_it/ https://www.reddit.com/r/learnprogramming/comments/z1v31/what_is_wrong_with_c_primer_plus/ There's quite a few other links if you Google it. You're gonna learn from the book, it's just, to many people, not a good book.
Yes, my old team planned data structures and classes ahead of time. Our planning typically involves two or theee large rolling whiteboards, a decision log, and some really awful pseudo-UML. We talked about the roles of the class and what its interface / public functions should be. As we iterated on the designs, we thought about what tests we need, error conditions and handling, and where the class should live. We would question if the classes were getting too big, whether functions needed to be free, private, or moved to a utility library. Prototypes happened a lot. We throw things out if they didnt feel maintainable or caused us concern. Once we determined it was good enough, we would document it in our documentation too (generally Confluence) with some flushed out UML diagrams. Then we built it and constantly reviewed other's work.
Yes, my bad. My comment is pointing to C++ Primer from Lippman. Truth is that C++ Primer Plus is much worse book. I apologize for mislead. 
I did exactly what the author proposes in my codebase for boost::optional / std::experimental::optional / std::optional (and likewise for string_view); didn't have any particular problem and allows to target older toolchains. 
Any way to use clang-tidy on Windows, for Visual Studio projects?
I agree with you! Yes I could've indeed given more feedback w.r.t. the technical content of the article, "interesting thoughts" is in fact not very much of a discussion starter or feedback for that matter. I wasn't trying to come across as elitist. I'm not even a native speaker either. Maybe my post sounds a bit harsh (or let's call it _direct_). I've seen and read multiple posts of the OP and while he only has basic command of English writing, the posts are usually readable (and the technical content goes without question). This article struck me as particularly badly written and I really don't have much understanding for why someone who knows that he makes lots of mistakes not even runs his posts through a good grammar &amp; spell checker before posting a blog article to the public. So I guess I was trying to express that. Anyway - thanks for your feedback, I agree with most of it - and let's focus on the technical content :-) 
Will code generated with clang/LLVM using MSVC STL be ABI compatible with libraries compiled with MSVC? 
Well, usually I don't blog on Saturday evenings... ... also the topic is a bit more difficult to express then just writing about standard proposals or my own code.
It's more than a hint, because it places restrictions on the iterators, value types, predicates, etc. (the "element access functions"). However, the mechanism by which things are to be parallelized or vectorized at all is unspecified. Note "may be parallelized" which allows not being parallelized: http://eel.is./c++draft/algorithms.parallel#exec-1 There exists hardware where parallelizing and/or vectorizing these things would make sense (e.g. GPUs or Xeon Phi), but we don't target any of those in our implementation.
That's up to Clang/LLVM folks. I believe that is their intent.
I'm also an EE currently learning C++ on my own. Did you see any career changes after learning it?
I only took a quick look, but you seem to have virtual destructors everywhere, why. o:
every time I think "I know the big picture, I'll solve the problems as they come" I get reminded that it wasn't enough and lose like 3-4h of work because I didn't think of that one little thing that makes the whole design retarded. so plan as much as you can, I personally just use paper, or ms paint when the paper is to far away
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6tfcf3/c_primer_plus/dlkj5g2/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Yes, and if you need to bind a non-static member function you can do it like this: `std::function&lt;void()&gt; member = std::bind(&amp;MyClass::member, this)` Or if it needs parameters... `std::function&lt;void(int param)&gt; member = std::bind(&amp;MyClass::member, this, std::placeholder::_1)` and call like this: `member(1);` 
in that case checkout [doctest](https://github.com/onqtam/doctest) - it has a [strong focus on compile times](https://github.com/onqtam/doctest/blob/master/doc/markdown/benchmarks.md#compile-time-benchmarks) - I will also be [speaking on CppCon 2017](https://cppcon2017.sched.com/event/BgsI/) about the implementation of doctest!
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [onqtam/doctest/.../**benchmarks.md#compile-time-benchmarks** (master  b8d0e61)](https://github.com/onqtam/doctest/blob/b8d0e618d93f980e8ee742041cb6e97f630207e0/doc/markdown/benchmarks.md#compile-time-benchmarks) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dlkjh4f.)^.
Quick question. Do you speak a foreign language? Can you write in it? I honestly don't see the point of critiquing English grammar on a technical post. It's as if you are saying he shouldn't share his thoughts unless written out coherently and free of mistakes. Take it for what it is but let's not assume everyone speaks English fluently. That seems pretty self-centered. I find it rather offensive when all you have to say about the actual content is "some interesting thoughts." You have no technical insight at all here. At all. That was some interesting suggestion but I advise you to get your comments proofread by someone who can tell you whether it's a worthwhile thought to share. 
Lambdas are preferable to `std::bind` in terms of performance, size, error messages and (in my opinion) readability.
Hi, thank you for the glance :). It is mainly for extensibility purposes, I cannot estimate whether consumers of the library require inheritance from certain GL objects (such as deriving a render target from a framebuffer).
Yes. We'll never do anything in MSVC's STL to intentionally trigger ABI incompatibilities. In general, we avoid `#ifdef __clang__` as much as possible, using it only for compiler bug workarounds, missing features, or warning suppression, not anything that will affect layout. (In order to deliver features in updates, we've been forced to develop a very good understanding of what we can and can't get away with ABI-wise.)
I heartily recommend laying out some type of design for all but the simplest scripts. If you're writing some type of basic command line tool, for example something in the style of a simple bash command, then you may not need to design anything before getting into the coding. But in a non-RAD language, pre-planning usually proves well worth the time. I would at least create some sort of map that describes different classes you will need, how they will be used by any operational process, and how they will interact with each other. You don't need to code or pseudo code anything (although it does help some people), you just need to understand the structure of the application as a whole, and design it flexible enough to cover your challenges, so that you don't have to redesign it after deploying/shipping. Also, don't be afraid to start coding and then realize you may have to rewrite some areas. In the real world, that's going to happen sometimes. Just take it as a lesson in planning and start over sooner rather than later.
Even if your lambda looks like this: `[this](int val) { member(val); }`? Edit: If you're going to down vote me please leave a message and explain why. This isn't some politics thread or something like that where you're just downvoting people you disagree with. My question wasn't asked sarcastically.
And this is how you would use a lambda... std::function&lt;void()&gt; f1 = [this]{void_member();}; std::function&lt;void(int)&gt; f2 = [this](int x){int_member(x);}; 
It's both shorter than `std::bind(&amp;MyClass::member, this, std::placeholder::_1)` and gives more insight with less noise. So yea, sign me up.
&gt; allows to target **older** toolchains Exactly. I don't really see how this is "future proofing" anything. It's just making it backward compatible. It might only be "future proofing" if you put it in place before the `std` version even becomes available, i.e. you're ready whenever it gets available. But as long as there's a `std` version, supporting `boost` is just for backwards-compatibility's sake.
Well yes, but if you want to make software today, it has to work on the current debian stable which will be stuck on gcc 6 for the two next years, so no c++1z support.
And how is `[this] (auto &amp;&amp;...args) -&gt; decltype (auto) { return f (std::forward&lt;decltype (args)&gt; (args)...); };` is more readable than something like `bind_all (&amp;A::f, this)` (and it can be wrapped in a macro to be like `BIND_THIS (f)`)? And even then it doesn't work because lambda doesn't have sfinae on it and so it's impossible to pass it to a function that expects it (for example something like a qt-like signals that throw away unneeded arguments of a signal)?
So don't use `std::bind` and write your own function that bind all arguments (it's what is needed 99% of the time)
Or like this `[this](std::vector&lt;std::unique_ptr&lt;very_long_name&gt;&gt; x,std::vector&lt;std::unique_ptr&lt;some_other_very_long_name&gt;&gt; y){member(std::move (x), std::move (y));};`
&gt; Quick question. Do you speak a foreign language? Can you write in it? English. And Portuguese. If I was to blog in Portuguese, it would be much worse than OPs English writing. So yes, I can relate. But if I ever was to blog in Portuguese, I would use a spell &amp; grammar checker before posting. I think that's what gets me. Why can someone not spend the extra 5 or 10 minutes to do something so simple, which would substantially improve the whole post? It kind of offends me. Not sure why. Maybe my comment sounds a bit too harsh, agreed. In that moment, my thoughts were: "Oh, another blog post by this guy. Cool, very interesting topic. Hmm, it's really hard to read. Even worse than his previous posts. Why has he after dozens of blog posts still not figured out how to run a spell &amp; grammar checker?". It's like, you do something, then why not do it properly? It's so easy and just takes a few extra minutes (for a blog post you probably took a multiple of that time to write). I guess you could argue "If it annoys you so much, just don't read it". Fair enough. You are totally correct with your last two sentences, and it's a good point. I do think there is something worthwhile to share though. If it was my blog post, I would appreciate all feedback. I like being given direct and honest feedback. Be it about the technical part or the writing. That's how you improve. Not by people not mentioning grammar mistakes, just because it's a technical article. And in this case, they really hurt the readability of the technical content. Maybe not for you, but for me. (And I do appreciate your feedback as well - thank you.) Another commenter here is right though that I could've sent a PM to OP instead - that would've been more appropriate. I apologize. I'll see whether I can edit my post a bit.
What is `bind_all`? A functor with a variadic template `operator()` that simply forwards everything? Admittedly, that's better than the lambda but may only be used if you want to forward everything.
Yes, it just forwards all. Of course, if you need to do something non-trivial with arguments lambda is a way to go, but this case is (at least in my experience) is very common one.
I have, actually. But it's not very user-friendly. Visual Assist also has a [Step Filter](https://docs.wholetomato.com/default.asp?W506) which is what I use most of the time as it's way easier than searching for some obscure XML file to edit. And as others have mentioned, it is also a speed concern sometimes, so it would be killing 2 birds with 1 stone.
I almost always plan ahead. The tools I use for initial planning aren't really so different from what the Ancient Greeks used to solve tricky problems. I usually open a text editor and write bullet-pointed high-level notes describing what I think the code needs to do, then I sketch out important data structures, algorithms, and classes in pseudocode resembling the target language. If it's something where visual reasoning helps (directed graphs, or 2D/3D geometry for instance) then I also scribble diagrams and notes in a big graphing-paper notebook using multi-colored pens. I make sure to prominently label the page so it's easy to flip back if I need to revisit a problem or show a coworker my thought process. I basically start at a high level and eventually everything filters down as code in the IDE. (Then I throw half of that code out while testing, and the other half when I refactor it to actually make sense, but by that point at least I have actual running code to work with and chisel into something viable.)
Not affected by it. Such systems are not on Internet.
 [this](auto x auto y) { this-&gt;member(std::move(x), std::move(y); } No need to be so verbose in the types since it's either going to fail in matching to the std::function signature or fail in matching the ::member signature and both give you the same end result but the auto version makes it far easier to read.
The thing is, that `std::function`, as far as I'm aware it's hard/impossible to avoid a double indirection when you are dealing with function pointer objects. So, it's both better performance, and frankly clearer (if more verbose), to just use a lambda: std::function&lt;bool(const std::string&amp;)&gt; f = [] (const std::string&amp; s) { return s.empty(): }
If it's not on embedded hw somewhere without any sw updates, eventually something might get there. Oh, well. It's their problem.
Yes, I think you should probably think about what you're going to do before you do it
[this] (auto &amp;&amp;...args) RETURNS( f(decltype (args)(args)...) ) Where `RETURNS` does noexcept and sfinae for you, at least until [this] (auto &amp;&amp;...args) =&gt; f(decltype (args)(args)...) in maybe C++20
I personally think this is off-topic (it would be better suited to /r/programming as nothing about this question is C++ specific), but as it's gathered a few detailed responses, I won't remove it.
 Goddamn. That's thorough. What company or industry is this?
It might not be suitable for a toolset update, but in FASTBuild land we're dealing with a preprocessor bug when using a particular macro; https://github.com/fastbuild/fastbuild/issues/256 I'm not sure where the Visual Studio Feedback post / bug report that the user _myevan_ mentioned ended up, but this is a blocker for cross-platform compilation with FASTBuild + Visual Studio.
Reporting to the devs
I was fooled by this myself years ago when I bought/read Primer Plus when I had intended to buy Primer... Primer really is a much better book.
It depends, if I'm doing a quick project I normally have "proper structure/styling" in my head already and do it as I go, but if it's an actual important project that a bunch will build off of/for a client, you always prepare and design as much as possible ahead of time.
Thanks everyone!
Research-ey code for some contracting work that I cannot disclose. Honestly, it sounds like a lot of work, but it's better to pay that cost up front rather than have to redo it. It's particularly beneficial when you have a team that will be coding up the various pieces. It's one of those parts of agile that gets under-valued compared to waterfall (I am a scrum master), but truly accelerates delivery. 
I see you've thoroughly availed yourself of the koolaid. How do you know if someone is a scrum master? Don't worry they'll tell you. 
Being relatively new in learning programming, I think you should pick a language (not necessarily C++) and learn the fundamentals (data structures, algorithms), and if you're into making games, make a complete game, it doesn't have to be complex, any simple game is fine, you'll learn a lot from the process. As for a job in programming, there're always jobs, you just have to be good enough for the job :P C++ being a bit harder than the likes of Javascript.
For the literal answer, it totally depends on what job you want. Do you like C++ programming? Great, go get a C++ job. If not, learn something else like web dev. For the more general answer, it is extremely useful to know multiple programming paradigms, such as OO, functional, logic, etc. By learning another language you are learning a different way of solving a problem. This is great since programming is really just problem solving and the more tricks you have in your bag the better a dev you will be. On top of that, by learning more languages you can increase your general knowledge of computer science itself. You probably won't learn about threading or merge sort (etc) by doing web dev, and you probably won't learn about full stack development doing C++. By exploring as many areas as possible you're broadening your knowledge of computer science, and this never a bad thing. TL;DR: yes you should learn more languages and it's never a bad thing, but it's not *necessary*, depending on what job you want. 
Why so outdated version of standard?
dont blame waterfall when it is not in the wrong here. waterfall emphasizes upfront design. agile promotes increased communication between clients and within teams which of course leads to better design.
Actually, I've just started. The company's been moving me from one sector to another until I fit in to power system protection sector. Dealing with all these protection relays and getting more into their configurations, functions blocks and that stuff I get on idea to learn programming. For now, my knowledge is not that huge that I can contribute on some projects as I just get to classes. Thought after I "master" that part to start contributing on some projects. Time will tell what's gonna happen with my career. Anyway, i doubt it could be bad to learn new thing :)
You don't get it. By talking exclusively about English grammar mistakes, you are not talking about the technical subject matter at all. If reading a blog post riddled with grammar mistakes by a non-native speaker offends you, then I think I am being fully reasonable to be offended by your continuing to ignore the technical subject matter and engage in a completely unrelated tangential discussion. That gets me. The audacity of it. The lack of self-awareness. And yes, I guess I am guilty of that. I'm out. I'm done.
We run a large collection of vert.x (Java async framework) and I wish async framework/futures came with better tracing capabilities. The only project I've seen offer something was ParSeq by LinkedIn. 
default move operators but non-default destructors seems dangerous - wont you end up with double-deallocation of moved objects? Also agree that you should be more opinionated about what can be inherited from (generally almost nothing :) ) I started creating a sample project to see how I would wrap OpenGL (ES 2) to make it look nicer in C++11. Of note I created a python app that converts the standard Khronos headers into strongly-typed enums to ensure you could only use the correct flags with the correct functions: https://github.com/seshbot/glpp (I also used the same parsed information to create a documentation browser: http://cechner.github.io/ )
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6tk2eh/should_i_learn_more_than_just_c_for_a_job_in/dlleobd/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Nice! I'm a bit of a beginner when it comes to OpenGL, and one of the more unpleasant things about that API is that I find it hard to figure out what is going to affect what. As an example of what I mean, in one of your examples you 'use' a program before you can set a uniform variable. Would it be possible to build some kind of guidance into the library in case one gets this wrong (i.e. like a debug message saying 'you are setting a uniform variable but the program is not currently in scope')? Also, why stick with GLenum, instead of adding additional type safety through class enum? That would be a major step forwards for safe OpenGL use. And I'm not a big fan of passing strings as `const std::string &amp;`. A great many of those calls are going to involve string literals, and OpenGL is not typically a place where one wants to do additional allocations just to pass a value... Mind, I'm _not_ complaining about getting rid of all those unsigned char * OpenGL loves so much ;-) 
Thanks. I'm trying to move to software development at some point (hopefully something related to EE) so I've been learning C++ very slowly.
The only thing I have to say is that OpenGL support is a strange beast, and it gets a little all-or-nothing. For example, you show an example of how to get a shader program running. It's neat and clean and looks like it does a good job hiding some of the ugly details from an end user. It's great if your wrapper can make this easier. The only problem is that your wrapper really only deals with very, very entry level problems as far as working with OpenGL is concerned. Is getting a shader program to compile really the hard part about using OpenGL? It can be tricky and frustrating, but it's not really that difficult. The hard parts of using OpenGL are domain specific: how should I structure my application to use OpenGL in the best way to solve my problems? When should OpenGL things happen and in what order? What should I do on the CPU? How do I stream data to/from the GPU for optimal performance? How should I coalesce/sort light sources/lighting information if I'm working with deferred shading? Uniform buffers? Shader buffers? I feel like OpenGL is tedious, even after many years working with it. It's better these days, but yeah, it's still tedious. The thing is that most of the tedium is short lived, and it usually absolutely pales in comparison to the difficulties of solving the domain-specific problems encountered by a typical programmer who is using OpenGL for a specific purpose. tldr; these kinds of entry-level wrappers don't do very much, if anything, to make it easier to use OpenGL to solve hard problems.
by the way the python Khronos spec parser is here in case you find it useful: https://github.com/seshbot/glgen (I didnt know python very well at the time so caveat emptor)
All so people like the Kardashians can make more money.
I don't do a lot of planning on paper, but as a general rule, I figure out what kind of tasks need to be done, what the data is going to look like, and what the MMI will (roughly) look like, before mapping that to zero or more classes. The only tool I occasionally use in this process is mspaint, which I use for mocking up rough MMIs (usually it is pretty straightforward, but occasionally you end up with a large collection of elements that need to have some logical structure). I feel that using tools (especially UML editors) for software design is overrated. People using them tend to spend way too much time on details that can safely be left until coding time, as well as perfecting _diagrams_ instead of _the actual design_. I also feel that there is actually such a thing as 'over-designing'. You don't need to map out every function or every parameter thereof in advance; it is more than enough to have a (well-developed) idea of responsibilities within the system. Of course occasionally a bunch of boxes get drawn during team communication, either on the whiteboard or more often on a scrap of paper (that gets destroyed before QA realizes what we've done). In those cases we focus on responsibilities, rather than classes perse (there is a correlation, of course), and any resemblance with UML is purely coincidental. 
I think there could be something to be said for how C++ programmers would respond versus those primarily working in other languages. I'm not 100% sure what those differences would be - maybe aspects of planning that most benefit C++? Just a thought :)
Personally, I make only simplistic RAII wrappers around gl resources. Everything else is just a plain gl calls. More complex wrappers can hide what is really going on under the hood. GL has an ugly state machine there changing some parts can influence other parts. Hiding those state changes behind multiple layers of wrappers can make it too hard to debug.
I start with the data: where it's produced, how it's consumed, etc.. After thinking through data flow and events that are generated and how they'd need to be handled and how they interact with the data, classes usually just pop up. I mean, class design is not the goal in itself.. It's an implementation detail for the useful actions of the program.
I completely agree and hence attempted to make the wrapper as thin as possible: Almost every function consists of a single OpenGL call.
Hi, I tried to minimize such issues by using the direct state access (https://www.khronos.org/opengl/wiki/Direct_State_Access) interface for GL objects whenever possible: For example you do not have to bind a buffer first before uploading data to it. That being said, I will certainly focus on improving safer use in the near future. Thank you for the suggestions :)
I always feel folly, thrift and other fb c++ libs kinda javaish. Not so modern c++.
Quite the opposite; especially folly.
If it is a single gl call why do you need to wrap it? If there is a glBindBufferRange in code, everyone can see what is going on here. If there is a buffer.bind_range then you need to look up its implementation just to figure it is a single GL call but with a different order of parameters.
Ah, ok. So that means it cannot be used with OpenGL versions older than 4.6, then? (4.6 is the one that added all those bindless functions, right?)
YAGNI. You are pessimising your code for a benefit which is unknown. I would argue that you should instead seek to understand the advanced use-cases of your clients and implement those rather than some nebulous "maybe one day someone will do this". Also, inheriting from a frame buffer sounds like a bad idea. Edit: especially since these appear to be copyable, which is not something that plays nicely with inheritance.
In order to provide RAII and scope (not having GLints flying around). The order of parameters to functions never change in the library, it is the same as OpenGL. If a parameter is already contained as a template argument or member of the object, then it does not exist in the function declaration.
I think direct state access became part of the Core Specification in 4.5 so that should work too.
Depends on how you have created the vs project. If it's from qmake/cmake/etc, you can create compile commands json and run clang-tidy against that with some scripting and/or external tooling. 
I never blamed waterfall for anything. I merely mentioned that in my experience, agile processes don't do as much up front design (for the reasons you mention). Agile values emergent design, constant refactoring, etc., which is good, but someone needs to point the team in a direction and say "build this." 
Just wait until I tell you about Crossfit! It's actually in the Evangelism Section of the certification agreement: Thou shalt stand on the rocks and proclaim thou role in Scrum, for amongst the Developer Hordes, Servant-Leaders have risen to facilitate the rise from darkness." If there is koolaid around, I drink it. Let's talk about microservices and the cloud!
It would be nice if stepfilter could help with stepping through 4 wrappers for `std::function` each time to find out what it will call eventually. Sadly for all these filters(including VSX one) you can never get to nested calls to user functions. On a bright side "Step Into Specific" is really amazing VS feature.
Hey, I think I have used your documentation at some point when I was developing the library :) You were right about the move operator - destructor issue, if a valid handle is left in the moved object, a double deletion will occur. I recently fixed this and pushed a new version to the repo. Thank you very much for the notice.
I posted this article in particular because of how surprised I was after reading this: &gt;We were able to increase the peak CPU utilization of the Suggested User service from 1015% to 90% per instance. This enabled us to reduce the number of instances of the Suggested Users service from 720 to 38.
They are pretty cool, but they don't seem to handle capture groups. This makes matching certain things generically a bit of a pain (e.g. constructors/destructors). Not only that, but specific overloads can't be matched which means that the default std::move filter blocks both the range and non-range based overloads. They are fantastic for ignoring helper functions and operator overloads which is great for debugging code with lots of iterators or mathematical types.
/r/cpp_questions
I thought bind was going down the deprecation route?
Oh sorry , I am a NOOB ..
A very broad-brush question. If the competition mandates use of C++ then you should do it if you want to compete, I suppose. Do you want to, though? If you can choose the programming language what's wrong with the languages you already know? Does C++ promise some advantage over other choices? Are you competing who can come up with the answer in smallest amount of time? You can do pretty well with "less efficient" languages if you know what you are doing. Do you see yourself needing C++ after the competition? It's usually easier to learn something you have fun doing so give it a go and try to find out if you love or hate it. If you hate it, at least you know you don't want to work as a C++ programmer. Better to know earlier than later. If you are not sure it is not worth the effort it probably isn't. 
this is the "answer", anyone saying "yes you should learn c++" or "no you should not" would be making very strong assumptions on what you actually want (to achieve|from life|from c++)
Thank you so much 
Thank you 
it's just for declarations, and specific ones (you can't use it for declarations inside statements like `if (bool b = foo()) {}`. For existing variables there was `std::tie` already.
so, what are the boxes that will turn into a sexy green for this update ? http://en.cppreference.com/w/cpp/compiler_support#C.2B.2B17_features
For me '__has_include' and 'if constexpr'. Yet to test for myself.
We published [new feature tables](https://blogs.msdn.microsoft.com/vcblog/2017/08/11/c17-features-and-stl-fixes-in-vs-2017-15-3/) on Friday. :-)
Regarding [the recent post](http://www.fluentcpp.com/2017/08/11/how-to-do-partial-template-specialization-in-c/) (and [ensuing discussion](https://www.reddit.com/r/cpp/comments/6szojd/template_partial_specialization_in_c/)), I decided to write up my thoughts on the matter. I've used a `type` tag quite a bit in my own projects. Maybe someone else will find this useful?
 Do they list the compiler improves &amp; bug fixes anywhere?
Right here https://blogs.msdn.microsoft.com/vcblog/2017/08/11/c17-features-and-stl-fixes-in-vs-2017-15-3/
I previously (pre-C++11 being mandatory in my projects) did the same for shared_ptr, tuple, array etc. so I could work with C++98 at a minimum, with Boost fallbacks, or the standard equivalents when using a current compiler. I'll do exactly what you do for optional, any and filesystem when I start supporting C++17 optionally soon.
&gt;Futures : A better asynchronous  programming paradigm [\\\_()\_/](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3747.pdf)
Stephan, is there a document that details what is enabled using /std:c++17 vs /std:c++latest ? (I believe the former has been introduced in 15.3)
Currently there are no feature differences. It is detectable via a slight change in the value of `_MSVC_LANG`.
**Company:** [Kollmorgen](http://www.kollmorgen.com/) -- a [Fortive](http://www.fortive.com/) company. **Type:** Full time **Description:** Participate in SW design, development and maintenance for a PLC-based motion controller. [The product](http://www.kollmorgen.com/en-us/products/machine-controls/automation-platform/kollmorgen-automation-suite-components/) is mainly composed of a GUI application running under Windows, communicating with embedded devices using a variety of network protocols. The embedded device is composed of real-time software that is instantiated in various RTOS environments and HW platforms. **Location:** Santa Barbara, CA **Relocation assistance:** Available. **Visa Sponsorship:** No **Remote:** No **Technologies:** Some C++17 on Windows (though main focus in on C++11 and 14 features). Using some C++11 (GCC 4.4.2 with incomplete C++11 support) with QNX on some embedded devices. Most development uses C++, but we also use some C++CLI, C, C#, Python, and sh shell scripts. Other technologies include EtherCAT, Ethernet, Modbus, and Profinet. **Contact:** PM me for more information or questions. I'd be happy to receive a resume and forward to the HR department. Or to apply online click on link below. **Link to Apply:** [Software Engineer- KOL001225](https://fortive.taleo.net/careersection/external/jobdetail.ftl?job=KOL001225) (Full position summary available through this link to apply.) 
[C++ streams binary I/O](https://cristianadam.eu/20160410/c-plus-plus-i-slash-o-benchmark/) is still 3x slower than the C/POSIX one. Average c I/O took: 104.78ms Average posix I/O took: 101.92ms Average c++ I/O took: 331.8ms
Hello! Thanks for the feedback. A whole slew of const related bugs (including this one) were fixed but missed the release cutoff. I hope you'll give it a try when the next major release preview. I'll make sure we take a look at the second issue you mentioned - we shouldn't be issuing a warning for that conversion.
Is there documentation for which features are supported for different values of `_MSVC_LANG` or at least which Visual Studio version they correspond to? 
Do you have any news about this compiler bug https://connect.microsoft.com/VisualStudio/feedback/details/3123944/incorrect-initialization-of-subobject-with-pointer-to-member-type ? It is almost completely fixed in 12 years, a little left to finish
This says to me that they were doing things in a bizarrely inefficient manner in the first place. 
`_MSVC_LANG` is `201402` for `/std:c++14` and it is `201703` for `/std:c++17`. (These values are permanent.) Currently, it is `201704` for `/std:c++latest`; this will increase in the future. My feature tables are the authoritative reference for which features were implemented in which versions, and whether they are available in 14/17/latest mode or only 17/latest mode (indicated by the "[14]" note). MSDN's feature tables are derived from mine.
Does this have a compiler that doesn't break programs when turning on optimizations? Also VS2017 seems to have a lot more typing latency and in general be a lot slower with interactivity. 
I fixed the long-standing and massive regression in floating-point parsing performance a few releases ago, but overall iostreams perf is still a dumpster fire. I've filed this link as VSO#479545; libstdc++'s performance indicates that the problem is fixable, but we'll need to investigate to see where our (very complicated) iostreams implementation is being inefficient.
Thanks Stephan! Is there any distinction for VS 15.0 versus VS 15.3? (Is that the `03` hanging out at the end?) Or do we need to rely on `_MSC_VER` as well? Basically, how do I determine whether the compiler supports features added in 15.3? Edit: Stephan, not Steven :/
Replacing the C++ I/O code with the one below made the Visual C++ version 10x slower. std::ifstream in; in.rdbuf()-&gt;pubsetbuf(inBuffer.data(), inBuffer.size()); in.open(inFile, std::ifstream::binary); if (!in.is_open()) { std::cout &lt;&lt; "Can't open input file: " &lt;&lt; inFile &lt;&lt; std::endl; return; } std::ofstream out; out.rdbuf()-&gt;pubsetbuf(inBuffer.data(), inBuffer.size()); out.open(outFile, std::ofstream::binary); if (!out.is_open()) { std::cout &lt;&lt; "Can't open output file: " &lt;&lt; outFile &lt;&lt; std::endl; return; } out &lt;&lt; in.rdbuf(); Results for Visual C++ 2017.3 x64: Average c I/O took: 101.7ms Average posix I/O took: 102.26ms Average c++ I/O took: 1114.19ms And results with MinGW 5.3.0 x64: Average c I/O took: 101.38ms Average posix I/O took: 102.38ms Average c++ I/O took: 104.34ms
`_MSVC_LANG` is what `__cplusplus` "should" be. (We can't update `__cplusplus` until we've fully implemented C++11, otherwise the Internet would riot.) These values are the year and month of Standard finalization (Feb 2014 and March 2017) and are unrelated to Visual C++ versioning. The value of `/std:c++latest` will change over time - it will be distinct from and larger than all of the specific Standard versions available. (Our current behavior is that it is exactly one larger, but you should not assume this. Amusingly, this would result in a mythical "13th month" if a Standard is ever finalized in December.) For example, in VS 2017 RTM (15.0), when `/std:c++14` was the only specific Standard option, `/std:c++latest` was `201403` (one larger than the C++14 value). In VS 2017 15.3, when `/std:c++17` was added, the `/std:c++latest` value was updated accordingly. `_MSC_VER` identifies the toolset version. Previously, it didn't encode toolset updates - VS 2015 RTM through 2015.3 were all `1900` although we added many features. (You'd have to inspect `_MSC_FULL_VER` which was more complicated.) In the VS 2017 series, things are easier. VS 2017 RTM was `1910` and each toolset update increments this by one, so VS 2017 15.3 is `1911` and the second toolset update will be `1912`.
It's still active in our internal database as VSO#386799 and assigned to a compiler dev to investigate for the second toolset update, although I can't guarantee that it will be fixed.
That is language &amp; stl, I meant compiler optimization &amp; bugs
Uhm, no word about C++ in that blog post! :-( PS: In that blog post the MSVC team asks for feedback about the blog on the top. The first question is &gt; 1. How would you rate this blog post, Visual Studio 2017 Version 15.3 Released? and the possible answers are 1 to 5. So what's the scale? 5 is the best? Quite ambiguous ;-) /u/STL And follow-up question: Does that mean I can uninstall 2017 Preview, at least for now? Or will there be new updates there pretty soon-ish and I better keep it?
Looks like the ICE fixes for my C++ 14 code they said would be in 15.3 haven't made it. Guess back to clang :(
What were the bug numbers, and do you have preprocessed repros to verify? 15.3 forked for release months ago, so either (1) what we fixed wasn't exactly what was causing you trouble, (2) someone resolved the bug as "fixed in the next release" but after the 15.3 cutoff and we didn't communicate that clearly enough, or (3) the bug was clearly resolved as fixed in 15.3 but we screwed up somehow (missed the cutoff anyways, or didn't actually fix the bug for x86ret or whatever, etc.).
I've mailed our compiler devs (all of them, front-end and back-end), asking them to comment.
What will the version in that table be? Currently it's got 19 and 19.1 which I guess is VS 2015 (latest update?) and VS 2017 (latest update, but not the one today - i.e. U2)? Presumably it'll stay 19.1 in the table?
19.11 would be a proper way to identify VS 2017 15.3 where `_MSC_VER == 1911`. VS 2017 RTM (15.0), 15.1, and 15.2 all have the same C++ toolset, and therefore the same feature support. Only the IDE was improved.
the devcommunity links are also sufficient, /u/14ned . If you can use devcommunity via report a problem, that makes life slightly easier for us than the old Connect way.
I've pinged /u/spongo2 (Visual C++ Dev Manager) about your first point. I recommend commenting on the blog post itself about the survey ambiguity, that should get the fastest response. I am not exactly sure how the VS 2017 Preview channel works (as I work on the product itself without an installer, and I test specific preview releases in VMs), but we're busy working on the second toolset update and those changes will eventually be released in a preview form.
can you tell me more about slow typing? we track perf on that pretty closely and on average 2017 has similar performance for native code typing. do you have any extensions installed? any other languages active in your project?
I'm still impressed you got that username
 #include &lt;mutex&gt; int main() { std::mutex m1, m2; std::scoped_lock l{ m1,m2 }; } Fails to compile in C++17 mode. `std::lock_guard l{m1};` does not compile as well.
I have to say that this was an absolutely wonderful explanation of concepts. Your examples were great!
Good! The second issue occurs on many range-for loops. Here is a self-contained repro: #include &lt;string&gt; std::string CaseMapString(const std::string &amp;s) { std::string ret(s); for (char &amp;ch : ret) { if (ch &gt;= 'a' &amp;&amp; ch &lt;= 'z') ch = static_cast&lt;char&gt;(ch - 'a' + 'A'); } return ret; } int main(int, char* const[]) { return 0; } 1&gt;------ Rebuild All started: Project: C_Cast, Configuration: Debug Win32 ------ 1&gt;C_Cast.cxx 1&gt;C_Cast.vcxproj -&gt; C:\u\hg\pending\C_Cast\Debug\C_Cast.exe 1&gt;C_Cast.vcxproj -&gt; C:\u\hg\pending\C_Cast\Debug\C_Cast.pdb (Partial PDB) c:\u\hg\pending\c_cast\c_cast.cxx(5): warning C26493: Don't use C-style casts that would perform a static_cast downcast, const_cast, or reinterpret_cast. (type.4: http://go.microsoft.com/fwlink/p/?LinkID=620420) 1&gt;Done building project "C_Cast.vcxproj". ========== Rebuild All: 1 succeeded, 0 failed, 0 skipped ========== 
That's because we haven't implemented class template argument deduction yet. Please refer to the [compiler/STL feature tables](https://blogs.msdn.microsoft.com/vcblog/2017/08/11/c17-features-and-stl-fixes-in-vs-2017-15-3/).
&gt; We can't update __cplusplus until we've fully implemented C++11, otherwise the Internet would riot. :-D
Hm, that's true. I wonder why I was under the impression that this had to work. I thought it was working in one of the previews, but, apparently, I was wrong.
Thank you for the feedback. The author is a friend of mine - so I can't take the credit :)
&gt;redditor for 10 years Still surprised?
Do you have any example programs which break when optimized ? We would like to know of any to help improve the quality of the compiler...
You have a lot of protected data members. This is not a good idea - please read https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#c133-avoid-protected-data
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [isocpp/CppCoreGuidelines/.../**CppCoreGuidelines.md#c133-avoid-protected-data** (master  c2456cc)](https://github.com/isocpp/CppCoreGuidelines/blob/c2456ccf5b2d6659e91aa66688e207fcce700565/CppCoreGuidelines.md#c133-avoid-protected-data) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dlmojgk.)^.
By the way, his name is Stephan, not Steven.
Thanks for the repro. I'll add it to the bug.
yes
This blog post doesn't belong here.
Awesome post. I'm still getting used to a somewhat advanced C++, can you please illuminate me on what this code does: &gt; template &lt;typename T&gt; constexpr auto type = type_t&lt;T&gt;{};
I'll let the other mods decide what they want to do (I don't want to misuse my power, so I don't reflexively approve MSVC stuff - in fact, I've told my coworkers to avoid posting low-quality links like surveys). However, I believe that this is on-topic. While it is somewhat annoying that the Visual Studio announcement doesn't mention Visual C++, the release announcement itself is newsworthy to C++ programmers. It's a production release of a major toolchain adding new features, like a Clang 5.0.0 or GCC 8.1.0 announcement will be.
:(
That is a [variable template](http://en.cppreference.com/w/cpp/language/variable_template). Think of it like this: - `std::vector` is a _class_ template - `std::vector&lt;int&gt;` is a _class_. - `std::make_shared` is a _function_ template - `std::make_shared&lt;int&gt;` is a _function_. - `type` (from the linked post) is a _variable_ template. - `type&lt;int&gt;` is a variable. In the same way you can use `std::vector&lt;int&gt;` in any context where you can use a _class_, you can use `type&lt;int&gt;` in any context where you can use a _variable_. In the above declaration, `template &lt;typename T&gt;` is the template parameter list, `constexpr auto type` is the declaration, and `= type_t&lt;T&gt;{}` is the _initialization_ of the variable (from a default constructed instance of `type_t&lt;T&gt;{}`.
I had some issues with the git integration causing stalls in the UI thread, setting your version control plug-in to none might be worth a shot. I haven't tried it yet in 15.3 but I haven't seen anything to indicate it's been fixed either. 
When talking about this I prefer 'Conceptualize' over 'conceptify' as I prefer genericise over generify I am wary of the natural syntax because it looks too much like a regular function rather than a function template. Since templates must have their definitions available while functions do not I prefer obvious syntax differences between them. 
Thanks! I really do need people to submit their code samples, though. It would super help this effort! everythingcpp@gmail.com
FWIW, this is not just std::function, but any std algorithm too -- function pointers are slow because they can't (generally) be inlined. For example: bool compare(int a, int b) { return a &lt; b; } std::sort(array, &amp;compare); In this case, if the call to sort is not completely inlined away, and it often isn't, then the compiler is forced to call compare through the pointer because sort is instantiated based on the type of the predicate, in this case bool(*)(int, int), and not what it actually points to. Ie: the instantiation of sort is shared amongst all calls that take a function ptr to two ints. Using a struct or lambda guarantees a unique instantiation and will generally perform better. Along those lines, I once optimized some code by changing a few calls to std::sort that did exactly this, called through a function ptr, to use a struct instead, and the speed up was about 2x. 
Why, would you even care? I send some code 2 years ago if i recall rigth and the problem is still there in VS17
 #include &lt;variant&gt; #include &lt;string&gt; #include &lt;type_traits&gt; using VariantType = std::variant&lt;std::monostate, std::string&gt;; class Test { // compile error void bad() { //static_assert(std::is_copy_constructible_v&lt;VariantType&gt;); //uncomment the line will compile auto error = [this](const VariantType v0) { auto v1 = v0; }; } }; void good() { VariantType v0; auto v1 = v0; } won't compile now \vc\tools\msvc\14.11.25503\include\xsmf_control.h(25,1): error C2065: '__this': undeclared identifier \vc\tools\msvc\14.11.25503\include\xsmf_control.h(25,1): error C2227: left of '-&gt;_Construct_from' must point to class/struct/union/generic type \vc\tools\msvc\14.11.25503\include\xsmf_control.h(25,1): note: type is 'unknown-type' \vc\tools\msvc\14.11.25503\include\xsmf_control.h(26,3): error C2056: illegal expression 
I'll attest. In fact, I generally don't even post my own blog posts here. We really don't want to carpetbomb. 
Why not just remind the devs instead of attacking them?
Because i sent that code and the problem 3 times and i will not send it anymore, the first one didnt get to MS directly, instead it went to the company they has to handle the feedback and support for VS but i dont care, they should sent it to MS
I'm a Linux only dev and the amount of MSVC posts seems fine to me. Quite a bit of it is relevant to c++ in general anyway. Kudos for not just carpet bombing. 
It is the value of the type `type_t`. I have similar definition in my code but named differently. I call those `type_tag_t` and `type_tag` to avoid confusion. 
The point of trying is that it may (and most likely will) succeed. Exception objects are small; if the failed allocation was large, there may still be plenty of memory to handle the exception. Or maybe the compiler sets aside a small emergency store of memory from which it allocates exceptions (as described in the article). And finally, just because gcc is being autistic about it ("the standard doesn't specify what to do so we'll do the stupidest thing we can") doesn't mean that all compilers are. So no, there's absolutely no reason to ignore bad_alloc. At best you could argue that there should be reasons to better specify the behaviour in the standard. 
I built this tonight for the next release of my MinGW distro. Uneventful build, the only hiccup was that the `.tar` emitted `PaxHeaders` directories when extracted with 7-Zip. Switched to using MSYS2 `tar` and that worked fine. Haven't done the full "build the distro with itself twice" dance yet, but it's been many years since that ran into trouble (and that was with old mingw-org).
If you want a modern C++ async server application framework have a look at [seastar](http://www.seastar-project.org/).
I hope you wait for Boost 1.65 before your next distro release (RC3 came out today, final should be soon). :-]
Where is a detailed changelog? I know it's a bug-fix release, but which bugs have been eliminated? Edit: Found it - https://gcc.gnu.org/bugzilla/buglist.cgi?bug_status=RESOLVED&amp;resolution=FIXED&amp;target_milestone=7.2
If you're running on Linux then all memory allocations where the size is less than the physical ram succeed because it uses overcommit. All of them, whether there's enough available memory left or not. Your application will just crash when you try to use it. Also, unless you're working on an embedded platform (where you're not likely allocating anyway) you'll likely end up in a swapping death spiral long before you run out of memory.
Out of curiosity do you have any tests you'll be checking submitted code with? Might be helpful for folks interested in trying their hand at this, even with you accepting failed attempts.
Yep, that's what I was waiting for, and GCC 7.2's release surprised me!
Off-topic, but that's not a good description of autism and "autistic" is not a good word for the behaviour you describe.
I was hesitant to post it here (skimmed for C++ and there was no results) but I think that many devs like me had been waiting eagerly for the release and knew that there would be c++ goodies anyways so...
Probably outcome no 2. Your new builtin problem reporting tool appears to have no URL link mechanism, so here are my bug titles: - "internal compiler error in 15.3 Preview build 19.11.25325" - "internal compiler error in vs2017.3 19.11.25415" - "Internal compiler error in 15.3 Preview build 19.11.25505" Obviously add my name to the search if you need to. Anyway so long as the fix turns up in a Preview for 15.4 or 16 or whatever soon, I'm happy. I can't submit Outcome for its second Boost peer review until MSVC is working as I'm also trapped on trunk clang as release clang for Windows produces invalid binaries with my C++ :(. You may remember you released 15.2 just when the review began and it broke my frozen peer review build, so I'd rather avoid that this time. 
Your builtin reporting tool is a real pain to use for end users. Constantly logs you out. Search can't search by name of submitter. Can't send link to bug report to others so they can CC themselves. I hate to say this, but can't you just use bugzilla like clang and GCC do? It is actually designed for developers to be productive with instead of the constant faffing around.
Looks like https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81486 (Class template argument deduction fails with (), succeeds with {}) is still borked. Makes my template deduction code impractical. Hope they fix it soon.
&gt; Your builtin reporting tool is a real pain to use for end users. I could not agree more. Plus there's the fact that half my dev machines only have the build tools, not the IDE...
Nice! I have been waiting for this to get C++17 working with CMake. I noticed that Ninja is now the default generator for CMake projects, which in turn seems to use MinGW. How does this work with Intellisense, does it give correct information for MinGW builds etc? Also, I tried building my project (which has some errors in it) with Cmake + Ninja. Problem is that the build terminates when it runs into an error, but the error doesn't show up in the error list, which makes it pretty hard to navigate to. Is this just a limitation with the Visual Studio/Ninja integration?
C++ the language lets you write C++ in bounded memory circumstances without problem. Plenty of real world examples out. My last contract plus one was a real world example, and that's landing in shipping product right now. C++'s default standard library makes that problematic. You can get embedded standard library subsets which fix that. It's doable. But for most, simply don't use most of the standard C++ library.
Sigh, where to begin; and where is Don Hopkins when you need him? You might want to read up on Forth before you make more of a fool out of yourself; and PostScript; and the various languages used to program HP calculators. The opposite of what you're used to isn't always bad, or even worse.
&gt; If you're running on Linux then all memory allocations where the size is less than the physical ram succeed because it uses overcommit. You can restrict the process size using ulimit. A feature which is rather underused. If you think you have enough ram you can also globally disable overcommit. This has the drawaback that fork/exec called by a huge programm temporarly doubles memory use. 
Indeed I'm in the third group. I only use auto when using template variables or complex template types, but never to avoid writing an explicit simple type, as it makes the code much less readable! I prefer adding typedefs instead of using auto where possible (also depending on how much a type is used) 
Why is it that you compile it with itself twice? I would have thought compiling it with itself once would be enough to get any new efficiency gains. ... Maybe it would be better to compile it with an unrelated compiler first, then using the result to compile itself. That way you could side-step [this type of backdoor](http://scienceblogs.com/goodmath/2007/04/15/strange-loops-dennis-ritchie-a/)!
My experience with this is that it's largely due to localization formatting. If one plugs in a custom locale, like this one... https://github.com/cdglove/daily-fast_iostream/blob/master/include/daily/fast_iostream/fast_locale.h ...it's much improved. At the cost of localized formatting of course.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [cdglove/daily-fast_iostream/.../**fast_locale.h** (master  50bead0)](https://github.com/cdglove/daily-fast_iostream/blob/50bead02e18fa77dbf4cdf0610ccf5cd4d064e31/include/daily/fast_iostream/fast_locale.h) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dlndbil.)^.
Were there any improvements regarding android c++ development? I am kind of playing with it at home but the number of bugs and annoyances is astounding. Logcat view is often empy, suddenly debugger cannot attach, clang extra command line settings are ignored, googling workaround for buggy gradle used by VS to be finally able to sign apk was also a pain. Too much of them to keep programming enjoyable:/
 auto x = d + sizeof(T); &gt;The type of x is a mystery: is it signed or not? Integral or not? Could the value of d be negative? This example comes from James McNellis, who claims (rightfully) that using auto here is a really good way to get oneself into trouble. This example is somewhat of a straw man. If the type is important then this can equally be expressed in AAA style as auto x = std::size_t(d + sizeof(T)); Or auto x = std::size_t{d + sizeof(T)}; I would argue that this provides more information than std::size_t x = d + sizeof(T); Did the author intend for a type conversion to occur here, who knows.
64-bit modules had been broken quite a bit of time ago (in prerelease versions) and don't work in released 15.3 either  compiling import std.core; with cl -std:c++latest -experimental:module I'm getting: mismatching target architecture for compiled module interface for 'std.core' from 'C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.11.25503\ifc\x64\release/std.core.ifc'
&gt; You can restrict the process size using ulimit. A feature which is rather underused. That's not very useful. Most of the time, people restrict their processes by an application-specific limits: number of connections, sizes of the queues, lengths of the messages, etc. Then they just do an automatic test to confirm that the maximum configuration doesn't blow up the memory budget. And that makes sense because relying on the all-memory-limit is hard: when something specific (say, message size) eats all the memory and hits the ulimit then you've got an exception somewhere, but it's not guaranteed that it'll hit exactly there, plus you've got to support an apllication-specific cleanup logic. Also, you can't test the cleanup logic because you need to cover all the code paths (in all threads) that can throw multiplied by the number of things that are eating memory and are to be cleaned up in case of exception.
But if you're always declaring variables like `auto name = Type(a, b, c);` then this just looks like another variable declaration. I'm not sure that it provides more information - I wouldn't have even thought of it as an explicitly intended type conversion until you just said it and I probably won't think of that the next time I see this pattern. 
Cannot find a reference for this ATM, but as far as I remember `bad_alloc` is special, in the sense that the runtime allocates a single instance of it somewhere during startup, and always reuses that one. Even if `bad_alloc` was not special, `sizeof(bad_alloc)` is 8 bytes on my platform, hard to imagine a situation where even that is not available. Worth to point out that `bad_alloc` means that an allocation failed; it does *not* mean that you are out of memory: `new int [10000000*1000000]` will likely throw `bad_alloc`, even if you still have plenty of memory available. You might even have that much memory available, just not in a continuous region. 
I'd argue that sometimes using `auto` in place of a simple type can make the code more readable. E.g. constexpr auto id = 42; constexpr auto name = "xxx"; constexpr auto version = std::array{1,0,0}; is more readable than constexpr int id = 42; constexpr const char* name = "xxx"; constexpr std::array version = {1,0,0};
I completely agree, I was trying to point out that the problems with the example code are not related to the use (or not) of AAA. In practice, when I want to be explicit I would write auto x = numeric_cast&lt;std::size_t&gt;(d + sizeof(T)); template &lt;typename NumericTypeOut, typename NumericTypeIn&gt; NumericTypeOut numeric_cast(NumericTypeIn in) noexcept { assert(double(in) &gt;= double(std::numeric_limits&lt;NumericTypeOut&gt;::lowest())); assert(double(in) &lt;= double(std::numeric_limits&lt;NumericTypeOut&gt;::max())); return NumericTypeOut(in); } 
&gt; That's not very useful. The argument I responded to was that malloc will always succeed, which is wrong. Dealing with the error case correctly is a bit harder, but not fundamentally impossible. &gt; Also, you can't test the cleanup logic because you need to cover all the code paths (in all threads) that can throw multiplied by the number of things that are eating memory and are to be cleaned up in case of exception. Considering that the alternative is to just crash in all cases of memory exaustion it certainly wont make your program worse. Possibly helpfull: stop calling malloc/free new/delete all over the place, we aren't talking about Java after all and the non trivial amount of time some of the libraries I have to use spend in malloc/free is just not funny -(rant) especially not when it is a scenegraph constantly throwing away its cull datastructures as if it didn't have to build them again the next frame (/rant). 
Yes, indeed. In C++17 it is possible to write an elegant function that turns a function literal into a lambda, which solves this issue nicely.
The situation isn't near as dire as you suggest. On at least some versions of Windows (maybe all of them?), thrown exceptions are copied onto the stack. Destructors and catch blocks are "called" with all of the previous function frames still on the stack. Only when the exception is handled is the stack pointer popped. This has the unfortunate side effect that rethrowing an exception in a deep call stack can cause stack exhaustion. With the Itanium ABI (i.e. Linux, Mac, most everything else), the exception is pretty much forced to be on the heap. The compiler will emit a call to "void *__cxa_allocate_exception(size_t)" and "void __cxa_free_exception(void *)" in appropriate locations. However, the implementations of those functions aren't required to use 'new'. The LLVM implementation (libcxxabi to be precise) uses malloc, and if malloc fails, it goes to an emergency fallback store of 512 bytes. (https://github.com/llvm-mirror/libcxxabi/blob/847e4d2486074be3d0dd15e87bcbebb92cd0d064/src/fallback_malloc.cpp) Personally, I prefer the Windows way, but stack exhaustion is no joke either.
Seems like the sub disagrees with me anyways :)
Well I wasn't suggesting mods do anything, it's not like its spam or antyhing ;) I just felt quite underwhelmed reading it. Anyhow, looking forward to C++17 features in the future.
I will likely do so when I write the paper, but I do not want to bias how people write their own solutions. The intent is not to see if people /can/ use it well, but if people /will/ use it well, and how they will use it.
But what happens if you meant to define the variables as, for example, long instead of int and string instead of char* ?
Mmm non so sure about it, but depends from the context!  The most annoying to me, when reading code, is the auto from return type auto bar = foo() Because it's not immediate what type returns Foo. Of course if you have a ide it's ok, else not! 
Preach it brother, I submitted a bug once and never more.
&gt; we're busy working on the second toolset update and those changes will eventually be released in a preview form. Cool! I'll just keep the Preview then, and I'll be looking out for the next Preview update :-)
 constexpr auto id = 42L; const auto name = "xxx"s; ;-]
I actually did not know about the string one, that's cool :) However, i still believe that it would be clearer if you just write the type in this specific scenario. I guess it really is a matter of preference.
The `type_t` trick is neat, but I have strong misgivings about the use of ADL for customisation points (not just in this case, but generally). Libraries which use ADL like this reserve names *globally*, undoing the entire point of having namespaces in the first place. To me, the best solution is the first one, namely providing a struct and asking the user to provide specialisations for their own types. This preserves namespacing and allows different libraries with overlapping function names to coexist, and doesn't rely on the "magic" (and sometimes surprising) ADL rules. Unfortunately this approach does require a fair amount of boilerplate, which is why I proposed [P0665 "Allowing class template specializations in unrelated namespaces"](http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/p0665r0.pdf). With the proposed syntax, you would say: namespace my_library { class my_integer { ... }; template &lt;&gt; struct ::json_lib::serialize&lt;my_integer&gt; { static json_data to_json(my_integer m) { ... } static my_integer from_json(json_data j) { ...} }; } which is very much more pleasant. The paper received some encouraging feedback from EWG in Toronto, and I intend to provide an updated version for the next meeting. 
Been continuously finding little nuggets of C++ knowledge since about 1996..
You compile with itself twice and compare the results. They should be identical. No non-determinism in the compiled output. 
This is normally done to make sure that the results of two last compilations are identical. That means we reached a steady state.
&gt; All of them, whether there's enough available memory left or not Only if you enable full overcommit. It is not the default on Linux (granted, the default is not strict accounting either)
I recall running into that bug in gcc 2.95, where (on a strict commit-accounting OS), bad_alloc due to failure to allocate an int failed to allocate itself and terminated. We fixed it locally by setting aside a static buffer, and gcc did the same thing soon after. C++ is amazing at reliably handling lack of memory, though many third-party libraries are not.
Yes, I could have more accurately written "that's right but not very useful" about the ulimit and malloc. &gt; Considering that the alternative is to just crash in all cases of memory exaustion it certainly wont make your program worse. It'll certainly make it more convoluted. It depends on the use case, you've mentioned some kind of renderer, but my case is the google-style servers (that have fixed loads and resources, and should crash ASAP in case of a bug).
When I'm building a subsystem... I take a stack of plain white paper and some pencils and QUICKLY do UML... I make mistakes, I crumple it up and do it again... I spend 0 time worrying about how neat it is.. Instead I focus on what matters... and I iterate... I will sometimes have half a trash can filled with crumpled up UML... only when I am very confident in the design do I do a neat version.. and by that point, the UML doesn't really matter anymore....
You used to need to do this with Gentoo sometimes. It's not rebuilding the compiler with itself. All compilers that can do that do that automatically. But basically the result of one version of GCC gets linked with the result of another version of GCC and weird things happen. So you rebuild the whole toolchain with itself: linux-headers, GCC, ld, libc, GCC. And then everything else that you installed. Ideally, this wouldn't ever be necessary, and I don't know why it was. But maybe there are some nice things about compiling everything with the latest optimizations. Like deferring some optimizations to the linking phase (can't remember the name of that).
problem is that special std::bad_alloc instance is not used when _throw MyException()_ fails to allocate memory and your app (which could be completely correct and supposed to handle std::bad_alloc gracefully) will terminate. &gt; sizeof(bad_alloc) is 8 bytes on my platform, hard to imagine a situation where even that is not available. It does not matter what is the probability of this happening -- it **can** happen. 
I'll give that a try, thanks!
When comparing iteration specifically with other languages (because one must!), the implementation of `auto` is rather poor. Consider the typical loop in a languge such as C# or Kotlin: for x in y { ... } You get an automatic guarantee that, if `y` yields value types, `x` is a copy of that value and mutating it, while possible, is meaningless unless you just want to reuse the variable; whereas if `y` yields reference (`class`) types, those are returned by reference and you can mutate them. Simple, clear, predictable. In C++, on the other hand, writing `for (auto x : y)`, unless you're iterating integers, is typically *not* what you want to do. So you end up writing `auto&amp;` but then think to yourself - "hold on, should I maybe make it `const`, I won't need to mutate it, right?" - and so you make it `const auto&amp;`. And then you remember that it is *perhaps* even better to write `auto&amp;&amp;` because, who knows, maybe it's some mythical `vector&lt;bool&gt;` you're iterating and you want to be on the safe side, don't you?
I recall there being a proposal out there to change the meaning of `auto` in range-based for loops to `auto&amp;&amp;` to mitigate this problem. Of course, that just adds complexity since the ship has already sailed and I believe that's why it wasn't accepted. I've just gotten used to always using `auto&amp;&amp;` myself.
Well... In general case (ignoring specific implementation details) it makes impossible to write code (with exceptions) that is guaranteed to survive out-of-memory. You can do it in C, you can do it in C++ (as long as you don't call anything that can throw), but cannot in C++ (with exceptions). Seems like a problem to me -- can't write a mail server that is guaranteed to stay up, for example. Even if code is absolutely correct and gracefully handles std::bad_alloc -- it is still not guaranteed to survive. &gt; Personally, I prefer the Windows way, but stack exhaustion is no joke either. It is interesting that you mentioned this. I am right in the middle of writing a multi-threaded logic that executes multiple tasks in parallel and exceptions generated in worker threads are migrated to parent thread (also chained together in some cases) and rethrown. It was a surprise to me to discover that these operations cause exception object to be copied (with possibility of std::bad_alloc) with MSVC (but not GCC). It is not ideal -- I have to use extra try/catch blocks at various spots and I can't guarantee that original exception isn't lost (replaced by std::bad_alloc) as it migrates "up" through threads. It is much better than std::terminate(), though,
Ha, I don't think it can be changed now because it will break backward compatibility with code written that was explicitly relying on by-value behavior.
So, what is the behavior (according to C++ standard) of this function if allocation fails for _MyExc_ object?: void foo() throw(MyExc) { throw MyExc(); } 
how can auto be used in default initialization?
That's exactly one if the most important places for type deduction! Annoying to type and to read what is obvious by the name of the query function and the context... I would never restrict myself to anything because some hypothetical future dev has no IDE - imho that should not be relevant for does and donts 
Yes that's right but having functions full of auto declarations ihmo makes reading code slower, even if you have an ide! But I guess this topic is like coding style preferences  I wonder what are the best practices for other languages that have type deduction from more time... something to look at
Implicit in what I said was that you never throw exceptions, and never call code which could throw an exception. So problem solved. This is part of why there is a sizeable chunk of C++ users who compile with C++ exceptions globally disabled.
&gt; If you're running on Linux then ... I'll quote my reply to a similar argument: &gt; You can claim that modern systems (i.e. Linux) will kill such server before we reach this situation anyway. But (1) it is not an argument; (2) memory manager can be set to overcommit; (3) OOM killer won't be triggered for 32-bit app running on 64bit hardware with enough memory (or if app artificially limited memory allocation). So, yeah... &gt; you'll likely end up in a swapping death spiral long before you run out of memory. It is worse than you think... :-) Here is a real-world anecdote I had to deal with about 6 years ago -- we had a report application that was progressively using more and more memory every week (for business-related reasons). It was a 32-bit app compiled with GCC 4.4 (I think) running on x64 Linux platform with ton of memory (i.e. OOM-killer was never involved and swapping was never an issue). At some point we hit an invisible barrier and application simply could not complete -- it was running for 6-8 hours using CPU at 100%. It refused to die too. I spent a lot of time trying to figure out what is going on -- turned out it was malloc implementation. It had (still has?) concept of memory arenas associated with current thread and if thread runs out of memory in it's arena and can't get any memory from OS -- it was going for a hunt to other arenas. Basically, your malloc suddenly takes a lot of time to comb through memory to find that free 4-bytes block. We ended up migrating to 64-bit compiler and switched to ptmalloc. :-)
There will be new goodies coming in the preview channel for some time to come. It's always a preview of the next update. 15.4 will have lots of c++ features. It won't have any conformance work. The one after that will have the next set of conformance across language and library. It will hit the preview channel once 15.4 hits release. 
I didn't see anything in the blog related to: - if_constexpr - structured bindings What is the status of those features? EDIT: if_constexpr is supported! It's listed as "constexpr if-statements". EDIT2: Structured bindings are also supported. Yay!
True, but it seems there is one last hole left. I want to be either convinced that this is my imagination or that hole to be plugged. In your example that app could fail to throw another exception on another thread and hand-allocated buffer can't help you here
Both are present in the update.
Lots of C++ features, but no conformance work  I'm intrigued...
Well, problem is that GCC doesn't seem to have a choice here -- pretty sure you can't throw std::bad_alloc in this case (it would imply that every exception specification needs to be implicitly extended with std::bad_alloc -- and I am pretty sure this isn't mentioned anywhere in standard). &gt; At best you could argue that there should be reasons to better specify the behaviour in the standard. This is what I am gunning for, but don't tell anyone. ;)
It depends on what the function is. If it is a factory like `make_unique` or `u8path`, then I will use `auto`. If it is, say, `emplace_back`, then I won't.
It can't, but AAA advocates consider value-initialization strictly superior to default-initialization  that's at least half the _point_ of AAA.
I'm hopeful for that proposal. I was surprised when I wrote code on MSVC ~~(Which already supports it as an extension)~~ (Which supposed specializing structs without opening the namespace) and it broke on other compilers. Seems like a nice feature to have from the beginning. Oh well, hindsight. &gt; Libraries which use ADL like this reserve names globally, undoing the entire point of having namespaces in the first place. I agree and disagree. The purpose of namespaces is to prevent collisions and to subdivide APIs into logical components, allowing users to pull in just the parts they need via `using namespace foo` or `using foo::bar`. ADL doesn't invade the global namespace, and it only affects unqualified name lookup. If a `foo(bar::Widget)` function exists in the namespace `bar`, then you can be sure that taking the address of a qualified or unqualified `foo` will not be ambiguous (assuming the qualified `foo` does not name an overload set). `foo` has not invaded anyone else's namespace. The name is not reserved. In most cases, ADL isn't something C++ programmers have to worry about. It "just works." The big time to worry is when you are writing a generic library and the name of the function is very general, like `to_string()`. In those cases, you must decide whether you want ADL or not. The rules to ADL aren't that complex compared to the myriad other rules one must keep in mind when writing correct generic code. (As far as I can tell, please correct me if I'm wrong). I've rarely encountered anyone getting tripped up by ADL being invoked when it was unexpected, and when I did, it was solved with just a few minutes and a qualified name. ADL is definitely an ugly solution to a difficult problem. Hopefully we can see some kind of fix from UFCS in the future? Another idea: The "template struct with only static members" is a pretty common C++ idiom. What if we could have "namespace templates"? `template &lt;typename T&gt; namespace json_lib {}`...? *X-Files Music* (No idea how it would work, but it would be neat).
LTO, Link-Time-Optimization
I've only been using C++ for the past year, it feels like I'm drowning in a tidal wave of C++ tidbits and features... 
[Now the C# people want to have `ref` and `ref readonly` in `foreach` loops.](https://github.com/dotnet/csharplang/issues/461)
Don't drink from the hose, let the waters wash over and rejuvenate you.
First, I build GCC 7.1 (or whatever). Then I use that to build the whole distro, so its libraries are now built with 7.1 (like PCRE). Then I use that to build the whole distro, so now its programs are linked against libraries that were built with 7.1. I don't actually do binary-identical checks, since that relies on deterministic builds which MinGW hasn't achieved yet to my knowledge. (GCC's own bootstrap appears to do a limited form of this, "comparing stage 2 and stage 3".)
Link is dead? edit: it's working now
heh. there's not that much mystery. for each of the IDE and build streams of work we are currently doing, there will be an incremental update. (Linux, cmake, some platform updates, etc) blog post should be out Monday.
we have 2 stevens (me inclusive) and 1 Stephan on the team. someday /u/stl will see the error of his ways and repent!
thanks. I just sent this thread to the team that owns the feedback tool.
Problem here is not auto, its the name of the variabel and the function.
REPENT, FOR THE STEVE IS NEAR.
AAA ends up meaning there is no explicit vs implicit conversions. Consider: template &lt;class Duration1, class Duration2&gt; nanoseconds average(Duration1 d1, Duration2 d2) { // whoops: explicit ctor used auto ns = nanoseconds{d1 + d2}; return ns/2; } // much later... int i = ...; int j = ...; nanoseconds a = average(i, j); whereas template &lt;class Duration1, class Duration2&gt; nanoseconds average(Duration1 d1, Duration2 d2) { nanoseconds ns = d1 + d2; return ns/2; } catches the conversion from int to nanoseconds. Type authors want to use implicit vs explicit to help users. `auto x = X{y}` is not the same as `X x = y;`
Yep. But the important thing here is which behavior is default and which is optional. C#'s default `foreach` behavior probably satisfies the needs of 99% of the users, whereas in C++ land, it is in the interest of most users to write the lengthier `for (auto&amp;&amp; x : y)`.
Probably look at languages that *don't* have such type deduction, for example *Java*! It is a pain to always write out the *exact* type definition - especially at initilization sides; either by ctor or - like we discuss here - by a function return value. This is especially true for more complex types; to type a ``long`` is ok, but a ``Map&lt;Integer, String&gt;`` or a ``BiFunction&lt;String, String, Integer&gt;`` really is awfull!
Id argue that, for all intents and purposes, AAAA == CRA, and that all experienced programmers fall either into the AAA or CRA camp. Sure, there might be some crackpots who abhor even those uses of `auto` that youve showcased in your CRA position but (at least amongst experienced programmers who have ever heard about interfaces) this should be negligible. In particular, I doubt very much that James McNellis would fall into this camp rather than into the CRA camp. By and large, this position seems to be a bit of a straw man to show how unreasonable opposition to AAA is (and I say that as a staunch proponent of AAA).
Second one is explicit, the types intent are clear, the types text length are also short and overall all of these turns it more readable imo.
Let's see if I can address each question separately... :) The generator used to do the build doesn't have a bearing on how IntelliSense operates. We extract #defines, compile flags and #include paths from the CMake server that runs after a generation is done. If you're building on the Windows machine that VS is installed on, then we are using the Windows build of ninja.exe, not MinGW. Why do you think it may be doing something else? Error's in your build, either cmake or compile errors, will (should!) definitely appear in the error list. What type of errors are occurring? Any chance you can provide a small repro?
I believe that Gentoo would tend to rebuild GCC because the toolchain included with stage1 wasn't a rolling release. Nowadays your stage3 has fresh builds of all the moving parts (you may still want to rebuild GCC if you change any use flags) 
Alternatively, what I've always done is constexpr int id = 42; constexpr const char* name = "xxx"; constexpr std::array version = {1, 0, 0}; Granted, that does take a little bit more time and energy to maintain.
This is a tangent, but I wonder why is using `auto` faster for compilation times and by how much.
CMake detects "CMAKE_CXX_COMPILER_ID" as "GNU" and the first thing that appears in my output when I try to build it is: "&gt;------ Build started: Project: CMakeLists, Configuration: Debug ------ [1/29] C:\PROGRA~1\MINGW-~1\X86_64~1.0-W\mingw64\bin\C__~1.EXE" which is why it seems like it's using MinGW. CMake also selects my mingw libraries instead of the msvc ones, and I don't seem to get any link errors. One error was for example that i had tried to #include a file that doesn't exist. It didn't appear in the error list when I tried to build, but it appeared after I had opened the file in which the error was. I could try to setup a smaller project later and try to reproduce it, but I can't really do that right now.
There are some great improvements to the VC++ optimizer being released in 15.3, we are going to write more detailed blog posts about some in the following weeks. A short list of improvements: * The inliner is more aggressive, with better inlining decisions. This gives, on average, a 3-4% perf. improvement across several benchmark suites, up to 10% in some tests. More improvements that benefit especially C++ programs are underway. * An initial implementation of a new SLP vectorizer and improvements to the loop vectorizer for bitmasking operations. * The SSA Optimizer, [introduced in VS2015 Update 3](https://blogs.msdn.microsoft.com/vcblog/2016/05/04/new-code-optimizer/), now runs twice, with better handling of memory operations, address-taken variables and code with exception handling. CMOV generation is improved, more useless conversions are removed and many more patterns were added. * A significant improvement for float math optimizations under -fp:fast in the SSA Optimizer: * Strength reduction of POW library calls: pow(x, 16.0) is replaced by 4 multiplications, which is about 30 times faster. Multiple forms are supported: pow(x, N), pow(x, N.5), pow(x, -N), pow(x, -N.5). * Better detection and simplification of min/max/abs expressions. * A large collection of simplifications based on identities of the transcendental functions. * More arithmetic simplifications focused on removing MUL/DIV operations and folding comparisons. * A new partial dead-code elimination optimization, which moves expensive operations such as memory loads and DIV under the branch where the values are actually used. 
This was reported (https://developercommunity.visualstudio.com/content/problem/74313/compiler-error-in-xsmf-control-with-stdoptional-pa.html) late in the 15.3 preview cycle as a `std::optional` bug. When I implemented P0602, I factored a lot of commonality from `variant` and `optional` into the machinery in `xsmf_control.h`. This was a great idea, except for the compiler bug the xsmf_control machinery triggers with `noexcept(noexcept(this-&gt;_Construct_from(/*...*/)))` when special member functions are instantiated inside a lambda body. The next VS toolset update will have a workaround for this bug.
Hi tpecholt, We're currently working on updating the Android SDK and NDK to a newer version in Visual Studio for Android C++ development. We're aware of the logcat window issue, and will get to it soon. Can you elaborate more on the other issues you ran into? - Do you know in which cases the debugger fails to attach? Do you have a project or steps to reproduce the issue? This will greatly help us investigate the problem. - Can you share which clang command line settings you'd like to use? - Can you share more on the gradle problem too?
I'm getting compilation errors: C:\Program Files (x86)\Windows Kits\8.1\Include\um\combaseapi.h(229): error C2187: syntax error: 'identifier' was unexpected here
Anything C++/WinRT related, regarding feature parity with C++/CX?
Hi Predelnik, I'm interested in learning how you'd like the filter to work for std::function. Can you elaborate more on this? Also, I'm not sure I fully understand your comment on "Sadly for all these filters(including VSX one) you can never get to nested calls to user functions." I thought you could step over nested calls to user functions using VAX or VS?
Come on! I just spent half day compiling 7.1 on WSL :D 
That's a interesting viewpoint! Hadn't really reflected on this, that they differ more than in syntax
&gt; Yes that's right but having functions full of auto declarations ihmo makes reading code slower, who reads codes anyways ? just type and go from compile error to compile error until it works :p 
Now it would only be great if we could distinguish between 1910 and 1911 in the project files too! (Because /permissive- is unusable for me before update 3, and our build machine has to stay at update 2 for a while)
&gt; Alternatively, what I've always done is unless you've started coding in C++ two months ago I doubt you've "always done" `constexpr std::array`. at the very least `constexpr std::array&lt;int, 2345&gt;`.
Alright you knob. You know what I meant. :P
In the case of the array example, if I'm using a template class with a long signature in many places my first idea is usually to put a `using short_descriptive_name = long_ugly_template&lt;thing1&lt;thing2, thing3&gt;, ...&gt;` in the namespace with my code, vs. sprinkling `auto` around. I find this increases readability. If it's a one-time thing, on the other hand, I think `auto` is better.
The compiler is ALWAYS doing the work of figuring out the type of the rhs. Instead of then figuring out how to match that type with the explicit type on the left, with auto it can just slap the rhs type on the lhs directly, so it's potentially less work. Of course, the kinds of deductions that auto can do have gotten more complicated lately, so... well, I have no idea. :)
I think it's explained quite well in this moderately highly upvoted uservoice ticket: https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/4078610-make-stepinto-a-std-function-go-directly-to-the-i Basically we can step *over* certain skipped functions but we can't step *into* function calls skipping the uninteresting parts until we reach unskipped code. I don't know if it causes problem for cases other than `std::function` to be honest but for `std::function` I have to step through its wrappers quite frequently.
This might be a stupid question, but can you explain to me which of the two examples is best and why? I would say number 1 looks best, but you have a 'whoops' comment there.
ugh. i want to apologize for this. I'm going through the items in our database right now. it looks like you filed these in both Connect and DevCommunity. it looks like we fixed the bug for 15.5 on August 1st for the Connect bug, but none of these got properly marked as duplicates. We're looking through all of them and will figure out how to do this better for you in the future. 
I second! When is C++/WinRT going to be official? No news since October of last year.
&gt; exception specification They are deprecated since C++11 and deleted from the standard in C++17. Don't use them.
Can you expand on that? Sounds interesting
Me reading this article: &gt;Holy shit. &gt;Holy *shit*. &gt;Holy **shit.**
Sure, it's not quite trivial to write out here on the spot, but the basic idea is that you use a function with a non-type template parameter, whose type is deduced: template &lt;auto func&gt; auto make_lambda(); You could use it like this: bool foo(int); auto lambda = make_lambda&lt;foo&gt;(); `lambda` has a unique type, and is hardcoded (at compile time) to always call `foo`. That means that if you pass `lambda` into say STL functions like `find_if`, it will be more efficient than passing `foo` directly. As for the actual implementation, it's not terrible. Here's a full implementation that works in 14: https://stackoverflow.com/questions/35783785/function-to-lambda. With that struct you can write our function as: template &lt;auto func&gt; auto make_lambda() { return makeLambdaHelper&lt;decltype(func)&gt;::blah&lt;func&gt;(); }
My biggest problem with that tool is that it's very obviously aimed at IDE users. It's not fit for purpose for reporting compiler problems which anyone who remotely pushes the compiler will end up doing, a lot. And that's been the case for the twenty years or so I've been using Visual C++ in any case. Genuinely, the Visual C++ and SDK teams only need to go install Bugzilla on a public Microsoft server and we can all go use that. Separate from the IDE, separate from everything else in Visual Studio. I know you guys also run an internal issue tracker. It might be super nice if you could just expose the C++ category of that publicly. Even if we can't add our emails to get pinged when there is progress on an issue, I can still fire the URL into a daily page change service. It would be very useful.
I'm not sure if you are talking about this one but [/u/STL](https://www.reddit.com/user/STL) had a proposal ([n3994](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3994.htm)) that addressed this problem. It was rejected though. The proposed syntax was `for (x : y)` and it would be equivalent to `for (auto &amp;&amp;x : y)`.
15.5? For an ICE? Ok, at least it's a milestone. At least I know now to stop sending in bug reports until 15.5 lands. You gotta understand, I don't actually know if some ICE is caused by the same original problem or by some new problem. So you end up submitting duplicates without knowing that they are.
Tbh these kinds of problems are why I moved to Vulkan, and I don't regret it. There's a learning cliff, to be fair, but the reward for that is not fighting the state machine and getting to use neat things like renderpasses/subpasses, push constants, and multithreaded command buffer recording and resource transferring. I think writing wrappers for graphics APIs can be an educational exercise for learning how to leverage the API, but more importantly it can show you how *not* to do things, which has been the most educational part of writing my Vulkan wrapper tbh. 
~~One option is to differentiate between `$(PlatformToolset)` == `v140` vs. `v141`.~~ Another is to inspect `$(VCToolsVersion)`, which will be an IDE version such as `14.10.25017` vs. `14.11.25503` rather than a compiler version, but regardless contains enough information to make necessary distinctions.
Well part of that tidal wave was approaching C++ like I did embedded C, until someone nearly slapped me over the head and said "use `std::vector` and `std::array` you dolt" and that changed *everything* real fast
This is something I'm trying to muddle through myself, in my Vulkan renderer/game-engine. I'm thinking for pooling the IO state and information I'm going to steal Dear ImGui's method and update some kind of `VulpesIO` struct with key values. That still leaves me stuck with how to *get* values though, as I'm only using GLFW currently and haven't even tried to get Linux/Android support working (which I'd like to do, but I don't think it'll be pretty...) I've bookmarked this for a look back at some high-level architectural inspiration, though, so cheers for that 
Isn't most of C++ like the last paragraph you've mentioned? There are a dozen different ways to do the same thing and a dozen different pitfalls as well.
Working with any expression-template-based DSL also makes the difference very clear very quickly. ;-]
v140 is 19.00 (VS 2015's toolset). v141 can be 19.10 (VS 2017 RTM through 15.2's toolset) or 19.11 (VS 2017 15.3's toolset).
Ah right, I didn't think that one through. ;-/
Yes, but from what I understood from your link, in order for that to happen you need: 1) Be actually out of memory (not just `bad_alloc`) 2) Have *4* exceptions active at the same time in the same thread 3) Have *16* threads in total throwing at the same time. Saying "I should not bother with it because it's broken anyway" it's like saying: "Why bother checking for any error? If a cosmic ray hits the PC and flips just the right bit everything falls apart anyway".
Yes but range-based for is a relatively new feature. People spent lots of time in committees deciding how this works. The could have just looked at C# and realized it would be easier to have it work in similar fashion. But yeah it's a general trend in C++. I'm still amazed that you *can*, in fact, do `auto&amp;` when an iterator is typically typedef'd as a pointer...
Thanks, I didn't know about VCToolsVersion, that's useful!
&gt; The could have just looked at C# and realized it would be easier to have it work in similar fashion. I guess, maybe, until they realized that unlike C#, C++ has rvalues vs. lvalues and reference vs. value semantics that aren't decided on a per-type basis. &gt; I'm still amazed that you _can_, in fact, do `auto&amp;` when an iterator is typically typedef'd as a pointer... Why would this _not_ work? Range-for yields the result of _dereferencing_ an iterator, which has well-defined semantics; why would the type of iterator matter?
Cool! Can't wait for it! :-)
Wow, I couldn't agree with you more. Thank you so much for this post! I couldn't have said it better by myself!
No offense but the fact that you're not yet aware of this seems a bit weird to me. Have you never used the tool by yourself (or any of your colleagues)? Or are you using it and do you find it genuinely acceptable or even good? I can't imagine how any serious developer could use this tool more than once and find it good.
&gt; many devs like me had been waiting eagerly for the release and knew that there would be c++ goodies anyways Totally this. Sad that there is no C++ at all in the blog post so the blog post is indeed useless to read, but the announcement itself is very useful ;-)
Would have been a neat trick. Though I prefer it even more when there's no redundant parentheses, e.g., in Swift, you write `for x in y { ... }`, very sensible.
Well, to answer your second question, consider an experienced C++ developer. In the good old days, he would have to write for (vector&lt;it&gt;::iterator it = foo.begin(); ...) { it-&gt;bar(); } Then `auto` comes along and you can now write for (auto it = foo.begin(); ...) { it-&gt;bar(); } Note how `it` is still a pointer. And then, all of a sudden we have for (auto it : foo) { /* it is not a pointer, nor a reference */ ) So you see how the logic is broken between this step and the previous? Someone made a decision to introduce this inconsistency for the benefit of usability. All I'm saying is they could have easily gone the extra mile and `auto&amp;&amp;` the thing implicitly, which would have saved a lot of effort. Though I do appreciate that *if* someone wanted by-value behavior, they would have to find some alternative syntax to make that happen.
Timing out for me too.
The issue here is that you're using the x64-hosted tools. The IFC files for the standard libraries are compiled with the x86-hosted tools. If you open an x86-x64 command prompt (x86-hosted, x64-targeting) this works just fine. It also works in the IDE as the IDE always uses x86-hosted tools. This is a temporary problem that will go away before we ship a non-experimental version of modules. 
No idea who edited the page but shouldn't it show the features as "19.11", not "19.1" (which implies "19.10")? See STL's explanation below. I know the page has tooltips where it mentions which update they came in, but still, no?
&gt; So you see how the logic is broken between this step and the previous? It's not broken, it's different; given that a different loop construct is being used  no surprises. And I certainly _don't_ see how the iterator being a pointer has any baring whatsoever, which is all I was remarking on.
&gt; The IFC files for the standard libraries are compiled with the x86-hosted tools. If you open an x86-x64 command prompt (x86-hosted, x64-targeting) this works just fine. Can you please consider reversing this? Who's using the 32-bit compiler at this point..? &gt; It also works in the IDE as the IDE always uses x86-hosted tools. Vcblog has explained how to "fix" this multiple times; it's no secret, and frankly it should be taken for granted that people will be using the x64 compilers IMO. (EDIT: I'm assuming this is referring to the IDE-generated project file which doesn't set `PreferredToolArchitecture` or the like, as opposed to the IntelliSense engine itself being 32-bit. If not, apologies.)
I couldn't help but think *"Yo dawg..."*
The first example allows the user to call average with two integers, because the constructor of nanoseconds is called explicitly. The second won't compile in this case, because the nanoseconds constructor taking an integer is only called in an implicit context, and that constructor is marked explicit. Really, though, the average function should be written to take two std::chrono::durations. Having written an explicit construction, it was unwise not to have constrained the types of the parameters, and in my opinion this is the real issue with this code - effectively turning an explicit constructor into a non-explicit one by wrapping it in a template. But that isn't the point of this example, which is intended to show that AAA style can have unintended consequences because it takes away a tool, namely preventing unwanted conversions. I don't see that as a downside of AAA per se, just one of the times it's "almost" always, and in my experience in a large codebase is yet to come up in the real world. Edit: fixed up my wording to try and be clearer. 
Thanks for a nice description. I'm still trying to fully wrap my mind around the template voodoo, but in the meantime I'd like to try your sample code to get a better feel for this technique. The one issue is that I'm currently limited to C++03 (I know, I know, it's not my call). I'm having trouble figuring out if this code will work without 'constexpr auto' on type. 
That's the one! Too bad it wasn't accepted :/
One answer is for the function to accept two durations. I believe in this case, either form will work equally. This is a specific answer for this example, and I believe it covers a certain category of cases where AAA affects conversion in this way, but there may well be other examples without such simple answers. I can't think of one off the top of my head, however. 
As a side note on VC, it's referred to as WPO- Whole Program Optimization.
My first couple programming classes they didn't teach us about anything in the STL, and we used just raw arrays. Decent for learning pointer arithmetic tricks, but when I learned about `std::vector` my C++ world changed for sure haha.
You can still construct a tag type without using the variable template. Just use `type_t&lt;foo&gt;()` instead of `type&lt;foo&gt;`.
Mandating (in C++standard) std::bad_alloc (that can't fail) in this case will make me happy. But I am not sure about implications -- maybe it is impossible to achieve (or too costly).
Ok, how about this argument: leaving this case unspecified will prevent me from writing portable code. One implementation will decide to throw bad_alloc, another -- runtime_error. Is it a better argument?
"We designed this nice language feature for you, it is awesome! But you shouldn't use it if you want to write reliable code". :-) Yes, I know a lot of people (game devs, etc) disable exceptions or terminate on OOM. I happen to like them and would like to be able to write robust code that survives out-of-memory. 
(1) regardless how improbable situation is, if it can happen -- code is not reliable. (2) conditions you listed are for certain version of GCC. Other vendor may have other conditions (3) your conditions are incorrect -- you could achieve this situation with one thread. You'll need 64 active exceptions, or one big enough not to fit into 4kb block. And active exception is not necessarily the one that is currently "in flight" -- you could prolong life of exception indefinitely via std::current_exception. You could chain them, put them into vector and do other tricks. (4) 16 threads is nothing. I regularly observe hundreds in certain apps. (5) cosmic ray -- hardware problem, code relies on guarantees provided by language (and ultimately -- hardware). If any of these guarantees get broken -- you can't rely on your code.
Depending on the operating system (more specifically OoM killer), you can't even guarantee a program will handle out-of-memory situations in C.
Depends on how many sigma of reliability you need. If it's six sigma or above, you can't throw exceptions, or call malloc for that matter. There are validating compilers which restrict you to a reliable subset of C++. If you don't have a strong need for that sort of compiler, then I'd not worry, the chances of an exception throw killing your program due to OOM are tiny when compared to all the other things which can kill your program. It is literally statistically insignificant for you.
yes, i've used it. i am well aware that it is not where we need it to be yet for C++ scenarios which is why I encouraged the team that owns it to read your feedback directly so we can work together to improve it.
My team doesn't own that directly so it wouldn't be announced by us :) I'll drop Kenny a note to see if he has any public things to announce at this point.
It is significant to me -- from my standpoint code is either correct or not. No place for sigmas. Would you be happy if 4/2 gave you 3 in 0.000000001% of cases? I bet you'd want language to guarantee a correct answer. I understand your point about practical side of all this and I wrote a number of apps myself on assumption that _new_ never throws. It doesn't mean I don't want guaranteed behavior in situations where it makes sense.
True, but this misses the point. OOM-killer is a feature outside of language boundaries.
I would've just used `auto` there, since context makes it obvious you're replying to a knob.
Anybody having any luck using Armadillo in VS 15.3? I had to comment out inline operator std::complex&lt;T&gt;() const { T a, b; arma_rng::randn&lt;T&gt;::dual_val(a, b); return std::complex&lt;T&gt;(a, b); } within arma_rng::randn&lt; std::complex&lt;T&gt; &gt; in arma_rng.hpp because it's throwing error C2760: syntax error: expected ';' not 'identifier' during compilation. I had no issues with using Armadillo in VS 2017 before I updated to 15.3.
Code alignment is a no go for me - and uncle Bob mention this as a code smell in his book "Clean Code" iirc. It focuses more on the vertical alignment then on the real important context, which simply is found horizontally in each line.
I'll use the second one if these declarations appear separately. But I think when these declarations appear together, the second form just makes it harder to find the name being declared.
Btw, I might be sleepy, but why wouldn't something like auto e = new MyException(); // handle bad_alloc here throw e; work?
&gt; Doctor, it hurts when I do this. If exhaustion of the emergency buffer is a concern, then don't throw humongous exception objects and don't gratuitously lifetime-extend exception objects. I see no reason why an exception object should ever get anywhere close to 1KB - or 512 bytes, for that matter - in size (this is `sizeof`, not logical size).
I had a similar idea but took it one step further by differentiating between masks and sets to catch misuse of operator&amp; and ~: http://foonathan.net/blog/2017/03/16/implementation-challenge-bitmask.html
because "throw" allocates memory in unspecified manner. Which means it may or may not copy your exception elsewhere (in case of GCC it will) -- and that elsewhere is not unlimited. When that memory runs out, you'll end up not with std::bad_alloc (what you'd expect) but with std::terminate.
"Humongous exception" is just an easy way to demonstrate. Of course nobody creates 1Gb exceptions. But it doesn't mean problem isn't there. Now you may say that with small exceptions I should never have more than handful active. Well, here is the problem -- what is a handful? What is the magic number which is OK? I can have hundreds of threads. Each one can have few active exceptions at the same moment, I can prolong (caught) exception life indefinitely via std::current_exception. And etc. And while I am doing all this I really don't want my app to crash on me mysteriously.
isn't there a misstake in the second example in the AAA group? in the colum with auto: std::vector&lt;std::string&gt; v = { "I love", "my", "teacher" }; for (auto &amp;it = v.cbegin(); it != v.cend(); ++it){ std::cout &lt;&lt; *it &lt;&lt; ' '; } `auto &amp;it`is a reference to a temporary? 
you can try [alwsl](https://github.com/alwsl/alwsl), if you want up to date packages.
What about targeting android with cmake? Are you still working on that?
I did not know about the explicit bool conversion operator working implicitly in specific contexts (if, while etc.) I was recently looking at the proposal for std::expected interface and saw that this also had an explicit bool conversion operator. I really like the pattern of declaring these types of objects in if statements to reduce their scope. It looked to me like declaring the operator explicit would break this, but your post has cleared up why this will still work. Thanks for sharing! Edit: book -&gt; bool 
Maybe a tall order, but would it be possible to dump a bit more info when an ICE happens? For sure when I crash the parser it is easy to find the culprit, but this time I think it is the codegen, so making a minimal repro is difficult, when I only have a single line to start from (inside heavily templated code).
Sorry about that! Not sure why it went down
I used to use GLFW for my old project, but when I looked at for my current one it seemed like it didn't support Win10 and I didn't want to spend ages trying to wrangle it enough to get it working, so I switched to SDL. I used GLFW for what I imagine you may be using it for, Window creation and input, so I know it has at least some rudimentary ways to grab that input information. If you are using GLFW just for those two things I can recommend SDL for those too, it's fairly easy to get up and running, and it comes with a bunch of handy stuff (like the methods for grabbing input from controllers as well as key/mouse/etc).
So you want to prevent `someMask == myEnum::someEnumerator` and force me to write the less-readable `mask == make_bitmask(myEnum::someEnumerator)` to make the intent "very explicit"? I'd say that the `==` operator already makes the intent "very explicit"; it means "equal to" in all other contexts, what's so special in this case? If you don't think the `==` operator is "explicit" enough, you may as well drop it completely and have something like `bool is_exactly_exactly_equal_to&lt;E&gt;(...)`. Also, `if (someMask &amp; myEnum::someEnumerator)` produces a warning in many compilers (for good reason; it's very easy to mistype `&amp;&amp;` as `&amp;`), so extra brackets and/or an equality check (`(someMask &amp; myEnum::someEnumerator) == myEnum::someEnumerator`) should be used to make _that_ intention "very explicit". There are plenty of cases where `==` is the correct comparison with bitmasks; e.g. you have an optimised codepath for a common case where exactly one bit is set, the `someEnumerator` value is actually a named combination of bits, or it's equal to zero (no bits set).
It is very hard to reliably handle out of memory errors. Unless your program really needs, I think it is better to just terminate on OOM, similar to what happens when you run out of stack. Makes reasoning about the rest of the program much easier. Opens up optimization opportunities, etc. Here is a [good post](http://forum.dlang.org/post/kjcscn$1sap$1@digitalmars.com) by Walter Bright on why OOM should be unrecoverable.
I think you need to teach your autocorrect about "bool"s.
As much as I admire James, I think the example is a little disingenuous. Consider: unsigned int x = d + sizeof(T); With only that change, the questions are not necessarily solved. Now consider: auto new_offset = old_offset + sizeof(ArrayElement); This is the same as the original complained-about expression, but with names giving context. For me, a lot of the complaints about `auto` go away with good engineering practices: using meaningful names, and using meaningful types. For example, "s.version()" had better return a version object, and/or be usable in the same API where the version concept is expected: auto v = s.version(); d.update_version(v); In the level of abstraction at which the current function is, the actual types become immaterial and so it's not useful to spell them out fully. After all, that's no different from `d.update_version(s.version())`
Damn.. That's what I get for posting from a mobile device.
The first is just as explicit and clear. It only requires basic knowledge to know that a literal 42 is an int. Nothing there is implicit. 
Unfortunately VS 2017 15.3 has a codegen bug leading to heap corruption: https://developercommunity.visualstudio.com/content/problem/95067/vs-2017-update-3-codegen-bug-heap-corruption-after.html However I cannot wait for the bug fix: How can I revert back to the previous compiler version _MSC_VER == 1910?
&gt; if there is no way to reliably handle lack of memory -- what is the point of trying? Maybe it is better to terminate in new_handler [...] This depends entirely on the application domain. I have worked in an in-house developed financial investment platform, where the application cached results of database queries made over the network. When the application remained with no memory, it would purge the cache and try the operation again. This worked most of the time, and made the application faster on average. Still, for most applications, handling out of memory conditions is out of scope.
GLFW works fine on Win10, just as information. I'm using it for my game.
Not sure that "std::string const &amp;" is appropriate for a C API.
&gt; At least I know now to stop sending in bug reports until 15.5 lands. You gotta understand, I don't actually know if some ICE is caused by the same original problem or by some new problem. So you end up submitting duplicates without knowing that they are. Does this help you: https://blogs.msdn.microsoft.com/vcblog/2016/04/26/stay-up-to-date-with-the-visual-c-tools-on-nuget/ 
I agree. And why are the exceptions being caught at the platform-specific layer after the C API? I expected the Windows version of the platform-specific layer just to be a list of exports pointing to the C API.
Temporary's lifetime is extended to its reference fifetime, everything's good.
I think unintended ADL is more common in code bases that use the same naming conventions as the standard library and boost. Most people use some form of camel-case so there's less chance of conflicts.
1) If `operator&amp;(E,E)` returns an `E`, what is the value of `MyEnum::One &amp; MyEnum::Two`? I'd much rather have the result to be an empty `bitmask&lt;E&gt;`. Same for `operator&amp;(E,bitmask&lt;E&gt;)`. 2) I understand that using `static_assert` to rule out the comparisons between `E` and `bitmask&lt;E&gt;` might provide better error messages, but I would just avoid declaring the relevant operators and let the compiler complain. 3) I don't get the necessity of type `enumerator&lt;E&gt;`. Could you please elaborate?
~~~I think what is more important in this case is that `v` is within a not-shown block. Thus it stays valid until that block ends. The for loop is within that scope so the reference is perfectly ok.~~~ &gt; Temporary's lifetime is extended to its reference fifetime, everything's good. IIRC, that is only true for *constant* references.
Yes, there's a mistake. But not for the reason you said. The problem isn't that it's a reference bound a temporary. The problem is that it's a *non-const lvalue* reference trying to bind to an *rvalue*, which is ill-formed - the example simply wouldn't compile. But were it an rvalue reference (`auto&amp;&amp; it = v.cbegin()`), then the example would work fine and be safe due to lifetime extension.
in my understanding the `v.cbegin()` returns a const_iterator by value. so you are binding the (non const) reference to a temporary(a copy of the internal iterator). thus your `it` is a dangling reference. if `c.begin()` would return a reference, you would modifiy its internal state my incrementing the iterator. and that's of course totally wrong 
Gosh, I didn't see that. Thank you!
Footnote 2 actually discusses this. Nevertheless, Im not convinced that having a C API is the way to go: ABI compatibility sounds like a red herring when were talking cross platform. But maybe Ive simply been lucky so far. In a similar vein, I also dont see how dumbing down an API by using [string typing](http://wiki.c2.com/?StringlyTyped) helps (the example given replaces `std::filesystem::path` with `std::string`). Sure, I get that Emscripten doesnt know about this rather new API. But then the author links to another blogpost where they implement a thin wrapper for it. So  whats the point of not using it?
Sounds like your teacher needs to watch this: https://www.youtube.com/watch?v=YnWhqhNdYyk 
That's a good idea. Alas the current version is the same as the 15.3 RTM (https://www.nuget.org/packages/VisualCppTools.Community.D14Layout), but this method saves having to deal with IDE Preview installs. Thanks for the idea.
You've got to understand that the hardware only is only "correct" via statistical probability too, so it's irrelevant how correct your software might be. When deciding where to invest effort, which is scarce and expensive, you expend it on paring down the probability of unexpected incorrectness in those areas you estimate as the most likely to appear in some given expected lifetime for the software. Same as making cars. Experienced people and orgs get the estimation right almost all of the time in software, same as in cars. But occasionally we make a "black swan" mistake, usually due to a many order of magnitude misestimation of risk. But let's be clear, the days of writing fully correct software for any non-trivial solution for any realistic budget vanished some time in the 1990s. Nobody does it any more, not for airplanes, nuclear power stations, anything. It's all a statistical game nowadays. It's the only thing commercially feasible anymore. You of course are free to implement perfectly correct software in your own free time, but you'll need to (a) use milspec CPUs hardened against EMP (b) a milspec realtime OS, because none of the major OSs are up to it (c) milspec approved verifying compilers which implement a safe, formally verifiable subset of C and C++. You'll find progress extremely slow going, not least that the tooling will be ancient and little open source software will work on it, so everything will need to be written from scratch. But it's a very valuable niche skillset, if you can get security clearance you'll be employed for life.
Oh, no way! That's good to know, thank you :) I found GLFW to be a nice lightweight window creation thing but I gotta say, SDL has some features that it's very nice to have access to.
The reason I want to prevent `someMask == myEnum::someEnumerator` is to prevent recurring bugs where one performs such a comparison when what is really meant is `(someMask &amp; myEnum::someEnumerator) == myEnum::someEnumerator`. Making that comparison a compile error means that a legitimate use case (the one you're describing), must be enabled somehow, and that requires an explicit promotion from `E` to `bitmask&lt;E&gt;`. I decided to name that `make_bitmask()`. The example `if (expression &amp; expression)` producing a warning that you gave illustrates very well this kind of design choice. Because that expression is a recurring source of bugs, but still has valid uses, it was made a warning, but a more explicit syntax was offered for the valid case. I'm trying to do the same thing, and I'm now wondering whether it can be done using the same exact syntax. One question for you : when you say `if (someMask &amp; myEnum::someEnumerator)` produces a warning because of the `&amp;` and `&amp;&amp;` misuse, do you mean it as an example for bool expressions in general, or is it an actual diagnostic you get with the enum class implementation I blogged about? If it is the latter, would you care to tell me which compiler and compiler options you are using? Thank you.
That sounds very interesting, I'll have a read of your article very soon! Thanks!
1) The value would be `0`. 2) What would be the benefit of doing what you suggest? 3) The necessity for type `enumerator&lt;E&gt;` is to provide the `explicit operator bool() const` so that the result of `operator&amp;` can be contextually converted to `bool`. [Edit : formatting]
&gt; 1) The value would be 0 I know that `0` is valid value for `E` (because of [dcl.enum]/8) but it might not correspond to an enumerator. This is bad, IMHO. &gt; 2) What would be the benefit of doing what you suggest? It's just simpler. &gt; 3) The necessity for type enumerator&lt;E&gt; is to provide the explicit operator bool() const so that the result of operator&amp; can be contextually converted to bool. This necessity is moot if `operator&amp;` always returns a `bitmask&lt;E&gt;` even if one or both operands are `E`. 
You should check out [Boost.Hana](http://www.boost.org/doc/libs/1_61_0/libs/hana/doc/html/index.html). I think you'll like it.
Well, it'll only allocate memory/copythe pointer. Maybe you can check beforehand if there is enough free memory for the pointer but yeah, it's not very reliable. Not really sure why they've left it like that.
Boost.Hana is definitely powerful and beautiful, but it is way more powerful than the problems I'm trying to solve. 
I'm not sure `a == b` is more object oriented than `is_same&lt;a, b&gt;`. Or should template be kept difficult so that only the best programmers would ever touch it?
Any way to set those VS project properties from CMake?
&gt;the example simply wouldn't compile. It will on VC++. They probably wont be able to remove this extension anytime soon.
Is it just me or is Base a slow POS? 
It seems that in this way others cannot add new type traits. Also this might have its own subtlety: int x; int&amp; y = x; type(x) == type(y) // true or false? I don't think it's possible to distinguish between `type(x)` and `type(y)` in today's C++ (macro aside).
Callbacks are dangerous when you start dealing with multi-threading. You may not want to hang too much off of them. When querying input you're normally interested in the current state vs the state last input tick, so my preferred method is to normally just have two const accessible state structs that get filled and swapped each input tick. Everything can read input in a constant, thread safe, way. You don't run the risk of dangling callbacks and on the whole your code is likely to be significantly simpler.
why would ref type be a problem? https://godbolt.org/g/HpKGc3 clearly `int&amp;` is not `int`. Edit: I don't understand why this gets downvoted - just try yourself the difference between `decltype(x)` and `decltype((x))`
Is there a way to use both `/std:c++latest` and avoid deprecation of stuff in the standard library? I am getting weird errors from Boost.Test that uses the C++17 deprecated `std::binary_function` internally. I need the latest language features as well as Boost!
So [here](https://github.com/rhalbersma/xstd) is one small CMake project that works on Clang/gcc and for which Andrew fixed a VS ICE recently. It uses a lot of `if constexpr`, `constexpr` lambdas and other C++17 goodies. I have been busy setting up VS 2017 15.3 with this project, but I a) after 5 years of Linux development, I felt like a Windows noob again and b) VS didn't operate as smoothly as it should. E.g., even though I managed to get CMake to detect a hand-installed Boost, VS didn't propagate the Boost include directories to Intellisense, nor did CMake put them in the `target_include_directories`. Also, I had a mingw g++ compiler in my path that was picked up, and even after putting that path last in my environment, CMake still thought my Boost libraries were built with it. I had to manually override `Boost_LIB_VERSION` to get CMake to find the correct Boost.Test dynamic library. Note that I didn't use CMake from the command line, but from within VS! I would expect that unless I manually override a lot of settings, that no other compiler would be picked up from my path. It would be nicer if the VS+CMake combo defaulted to whatever toolchain VS is currently using, and not to whatever happens to sit on my hard drive. Other glitches were Boost's fault (C++17 deprecation warnings in `std:c++-latest` mode). I pinged STL about that.
Paging /u/spongo2. I've also sent mail to your Most Impressive Dev Lead. 
Here's another safe bitfield implementation: https://github.com/ned14/quickcpplib/blob/master/include/bitfield.hpp. It balances lack of annoyingness to use with improved safety. Nobody at Boost likes it, therefore it's probably fine :) Usage: QUICKCPPLIB_BITFIELD_BEGIN(flag) { flag1 = 1 &lt;&lt; 0, flag2 = 1 &lt;&lt; 1, ... } QUICKCPPLIB_BITFIELD_END(flag) ... flag myflags = flag::flag1|flag::flag2; if(myflags &amp; flag::flag1) ... if(!myflags) ... // This intentionally fails to compile: // if(myflags &amp;&amp; flag::flag2) // // That is because of operator precedence difficulties in say: // if(myflags &amp;&amp; flag::flag1 &amp;&amp; myflags &amp;&amp; flag::flag2) // // This works fine though: if((myflags &amp; flag::flag1) || (myflags &amp; flag::flag2)) ... 
FWIW, this was part 3 of a 3-part blog series by one of the C++ Core Checker devs (and me, a little bit.) 1. https://blogs.msdn.microsoft.com/vcblog/2017/08/11/c-core-guidelines-checker-in-visual-studio-2017/ 2. https://blogs.msdn.microsoft.com/vcblog/2017/08/14/managing-warnings-in-the-c-core-guidelines-checker/ 3. https://blogs.msdn.microsoft.com/vcblog/2017/08/15/how-to-use-the-c-core-guidelines-checker-outside-of-visual-studio/ The first of this series had [a separate /r/cpp discussion](https://www.reddit.com/r/cpp/comments/6t2y59/c_core_guidelines_checker_in_visual_studio_2017). We've also written up [new C++ Core Checker documentation](https://docs.microsoft.com/en-us/visualstudio/code-quality/cpp-core-guidelines-warnings) at docs.microsoft.com. 
`/D_HAS_AUTO_PTR_ETC=1` will restore it. Note that `binary_function` was removed outright in C++17, and Boost really needs to update their code.
VC also refers to this step as LTCG, Link-Time Code Generation.
CMake have CMAKE_CXX_CLANG_TIDY for clang-tidy, guess they would appreciate a patch that adds CMAKE_CXX_VS_CORE_GUIDELINES_CHECKER :D
ok. so last night we fixed a bug that makes our nightly drops nuget packages up to date and we have a drop up there that should have the fix. we're working on improving the test suite but in the interest of making sure we have this area cleaned up, can you please grab the drop and build your library? https://aka.ms/dailyMSVC. 
Great, just read the docs. I noticed that the checker supports caexcludepath. Perfect! As understand it its a deliberate decision to not offer this functionality for normal compiler warnings. Why this inconsistency? I would so much love to enable more warnings on my Windows build!
&gt;regardless how improbable situation is, if it can happen -- code is not reliable. You will never ever, **ever**, be able to write an application that is running 100% reliable with 100% uptime. Even if the application is 100% reliable you wouldn't be able to say the same thing about the OS that you use, or the hardware that you use, or the power source / backup power that you use, and so on and so on. So why bother care so much about this improbable situation? Even if this exact situation would arise it would be a side-effect of *another* problem. Why is the OS out of memory? Is your application allocating too much memory, leaking memory or is being run on a computer with simply to little physical RAM? Or is there another application that is running with any of the problems above?
We intend to enable a similar functionality for compiler warnings and deprecate (but not remove) CAExcludePath. If you have the time to create one, a [Developer Community](https://developercommunity.visualstudio.com/topics/C%2B%2B.html) feature request really does help us argue to get features funded. 
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/6u4526/how_to_compile_a_beginner_program_in_vs_2017/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
You lost me here: &gt; Moved-from objects may only be assigned to or destroyed, and nothing else (is guaranteed). Through no fault of your own, you've hit one of my pet peeves. This misunderstanding has caused me no end of time trying to correct. And here I go again... An object that is used with a standard-specified algorithm (such as std::sort or std::vector::insert) must meet the requirements specified for that algorithm. For example sort requires MoveConstructible, MoveAssignable, Swappable and LessThanComparable (using whatever comparator is supplied or defaulted). And sort will *temporarily* put some objects in a moved-from state. **Those moved-from objects still have to meet all of the requirements of sort.** Just meeting assignable-to and destructible is not sufficient. Now if you never use std::sort to sort your objects (or any other std algorithm), you can do whatever you want with your moved-from state. For objects defined in the standard, it is safe to perform any operation that has no preconditions on a moved-from object: guaranteed. This means you can find out the size and capacity of a moved-from vector (for example). And you can clear() a moved-from string. Etc. You just can't pop_back() a moved-from container because you don't know that its state is not empty. If you first check and find out it is not empty, then you *can* pop_back it.
Is there like an official guideline for what moving should and shouldn't do? Right now it feels like there's no clear rule for how moving should be implemented, and maybe this is why people are misinformed. My (perhaps misinformed) interpretation of moving something is you move ownership of resources and leave everything else as it was, i.e. pretty much copy everything over, but remove everything you would "disown" on destruction. So for a simple vector type (size, capacity, pointer) that would mean copy everything over, and then set the pointer to some value that means there's nothing to deallocate when the destructor runs. That would mean that the size and capacity members would still have their original values. Is this a faulty implementation?
&gt; Alas the current version is the same as the 15.3 RTM Well, yes, because that's the URL for _released_ versions of the toolset... Try http://vcppdogfooding.azurewebsites.net/nuget/ ;-]
It's `nuget install VisualCppTools.Community.Daily.VS2017Layout -source https://vcppdogfooding.azurewebsites.net/nuget/ -Prerelease` right? Because that's hanging for me here, hangs on "Installing VisualCppTools.Community.Daily.VS2017Layout 14.11.25615-Pre" whether from the command line or within Visual Studio.
Everything I've read has said something along the lines of ["moved from objects should be in a _**valid** but indeterminate state_"](http://en.cppreference.com/w/cpp/language/move_constructor). I have no idea if that wording is in the standard or not, but it's the one that makes the most sense. A vector with a pointer to nowhere useful and its previous size/capacity still set is no longer valid, so yes I would call that a faulty implementation. 
&gt; The first is just as explicit and clear. Nothing there is implicit. Can't agree. The second one explicitly tells the type while the first one is implicitly deduced. What does guarantee to you that constexpr auto name = "xxx"; is the same after a some years? What if the standard changes and it is deduced as std::string (or another string/character type) instead? Also the second one may be much clearer for junior programmers since they don't need to know all the compiler deduced rules. &gt; It only requires basic knowledge to know that a literal 42 is an int So it does require additional knowledge, that's why I consider the second one more readable, it is less error prone for all the programmers working in a codebase. 
What about writing a class for a bitmask, rather than writing all this stuff around an enum class?
TIL, thanks!
There's zero chance of the deduction rules changing in that manner because it would be a breaking change with too much impact on existing codebases. It would be madness to impose such a change and contrary to the policies of those in charge of the language. A std::string also cannot be constexpr because it allocates memory. And yes, it does require knowledge, perhaps one of the first pieces of knowledge that a well educated C++ programmer will learn, and something that it is reasonable to expect one of any skill level to know. So no, the second example does not contain any more information than the first, it is just written unnecessarily verbosely, and by doing so you introduce more opportunities for errors, and cause more work in maintenance down the line. 
&gt; Those moved-from objects still have to meet all of the requirements of sort. But only to the extent that these operations don't imply UB, right? I.e. couldn't the comparator legally return random true/false values given moved-from arguments? 
Hi Howard! Sorry for hitting on a pet peeve. And my apologies to you and other readers if the sentence you quoted reads as overly-factual. What I hoped to imply was "A useful mental model for move semantics is that...". (You may still disagree with that statement, OK.) I wasn't meaning to flat-out deny what's written in the standard though, so thanks for that correction. With regard to sort and the other permuting algorithms: what do they do to moved-from objects, other than reassign or destroy them? If an object meets the requirements of sort at the point of call, why does that mean that a moved-from object temporarily created inside sort still needs to meet all those requirements? To me, there is an analogy here with member functions having the ability to temporarily break class invariants. Re objects in the standard, I would be interested to know if there any instances where the ability to assign/destroy does not -- coincidentally, by my line of thinking -- entail the ability to use operations without preconditions. In order to properly destroy a vector, one must know its size and capacity; in order to properly destroy a string one must know whether it is SSO, so clear is also possible. This is what I meant by "some objects are well-formed coincidentally as a result of being partially formed." I hope to see you again in Bellevue and have a higher bandwidth discussion.
You could get away with returning random values when comparing a non-moved-from value with a moved-from value. However, comparing a value with itself *must* return false, moved-from or not. And comparing two values x and y must return values that are consistent with the strict weak ordering requirements as specified in [alg.sorting], whether or not either or both of x and y are moved-from. For example if x &lt; y returns true one time, it better return true again if the values of x and y haven't changed. And if a &lt; b and b &lt; c, then a &lt; c must be true, whether or not any of a, b and c are moved-from. 
Qt handles this already using QFlags: http://doc.qt.io/qt-5/qflags.html#details This implements the flags as a templated class, and 'testFlag' is used instead of '=='. It's mainly for convenience, I don't think it would catch the error that motivated the OP.
I understand your frustration. But please keep in mind that the number of devs in the backend team is very small compared to the number of bug reports. Some of them do get dropped due to lack of resources. But now you have the attention of a dev...
 _ /(| ( : __\ \ _____ (____) `| (____)| | (____).__| (___)__.|_____
&gt; with the exception of the constexpr auto version = std::array{1,0,0} which the right side explicitly tells you the type Really? You're okay with that even though it doesn't specify `int` (just like the others)..?
The standard guarantees that for std lib classes. [lib.types.movedfrom/1](http://eel.is/c++draft/conforming#lib.types.movedfrom-1): &gt; 20.5.5.15 Moved-from state of library types &gt; Objects of types defined in the C++ standard library may be moved from ([class.copy]). Move operations may be explicitly specified or implicitly generated. Unless otherwise specified, such moved-from objects shall be placed in a valid but unspecified state. For user defined classes, this is up to the user, but it considered good practice to follow the case. At least, the moved-from object should be destructible and assignable-to to be used with std library. There may be more specific requirements for some of std algorithms, as /u/HowardHinnant mentions.
I'd like to be able to write apps that are 100% reliable within boundaries defined by language. I can't handle failures in CPU, OS or other components of the system that language relies on. If you want to build reliable system -- you want to have each brick as reliable as possible. And while it is impossible to achieve in world of hardware (due to cosmic rays, etc) -- it is certainly possible to do in software.
To me that seems to be missing the point; alignment adds gaps between the different aspects of a declaration/definition (name, type, operator, value, etc.), which makes the line easier to parse horizontally. Instead of having to read the whole line, your eyes jump immediately and naturally to e.g. the type because you already know where it is  because alignment made it stand out, implicitly and without any mental effort.
I am getting close to the answer. Will update my post when I find it out. But it seems like a result of trying to accomodate both MSVC (which allocates exceptions on the stack) and GCC (which uses heap + emergency buffers). My problem is that even in this case -- you could still add extra requirements that will allow us to avoid std::terminate without major changes to both compilers. Why they didn't do that?
Not if you use `/permissive-`. :-D
I do understand all of this --100% reliable hardware is impossible. But 100% reliable software is a possibility and I see no reason why C++ should stop just one step away from this (esp considering C is already there). I always strive to write code that is 100% reliable (within boundaries defined by C++ language) and not a single employer complained so far. That is one reason why this discovery of this "weakness in C++" ruined my day. Don't care about being employed for life either. I could probably retire right now if I wanted. ;) And yes, you are correct on one account -- I do tend to rewrite a lot of things I touch. There is just SO much poorly written code everywhere. Note about realtime OS -- I don't need these guarantees and C++ is probably a bad choice for programming these systems. Maybe a subset of it... Can't really say, never coded in such environment.
Nice. Does windows.h compile with it? Edit: This would save me quite a bit of sadness if it was there sooner. Good to see it though.
By what measure is `is_same&lt;A, B&gt;` more difficult than `a == b`?
That example comes from a real world code base - Howard Hinnant's (creator of chrono). Maybe not his code directly, but code he worked with. I agree that the real issue is that the types aren't constrained, but that isn't always easy. And yes, as you mention, the point is that there can be unintended consequences. If you always do `auto x = X{y};` you've lost the implicit vs explicit distinction.
&gt; Etc. You just can pop_back() a moved-from container can -&gt; can't
I wouldn't call moved-from objects "partially formed". Most uses of "partially formed" (including Sean Parent and EoP, etc) mean _not_ a valid state. Moved-from objects should be fully formed (or well-formed), just no particular well-formed state, any well-formed state will do (but typically the one with the least resources and/or the one that makes move the fastest). This is why some of MS containers are not noexcept movable. IIRC their std::list always has an allocated node (even when empty), so the moved-from list needs an allocated node (because it must be well-formed). So that extra allocation means you lose noexcept. And then you get into the discussion of "destructive-move" and/or "partially formed".
Yes, thanks for the catch. Two little letters can make all the difference in the world! :-)
&gt; Everything I've read has said something along the lines of "moved from objects should be in a valid but indeterminate state". I wonder if stipulating "moved from objects should be in a default constructed state" would've been better? 
&gt; 100% reliable hardware is impossible. But 100% reliable software is a possibility ... If the hardware cannot be trusted to reliably execute a branch, then 100% reliable software is not possible. Doesn't matter what you write. &gt; That is one reason why this discovery of this "weakness in C++" ruined my day The requirements of this part of C++ is well understood. It was, and is, felt to not be important relative to other causes of program failure. As I mentioned before, if you really care about this, then don't throw exceptions. Plenty of C++ programmers don't. There should be improved facilities landing in C++ 202y for such programmers too. &gt; Note about realtime OS -- I don't need these guarantees and C++ is probably a bad choice for programming these systems. Any non-realtime OS cannot be reliable by definition, and therefore no software running on it can be either. Remember that non-realtime OSs offer no guarantees whatsoever that a timer will ever fire, or fire anywhere close to when you set it. That makes reliability hard to achieve. A realtime OS lets you say "if you don't call my timer callback within 1 microsecond of it elapsing, halt the system as correctness has been lost". Now that's a guarantee. Some would even say that any multi-core CPU system cannot be reliable by definition due to unpredictability of thread interaction, but I personally think that a step too far. Sure, lots of code out there has sloppy thread synchronisation, a lot of it in OS kernels. But I'd be fairly confident that a realtime OS like QNX running on a multi-core CPU can be predictable. You really would need a pure microkernel design though, it forces kernel devs to write correct unsloppy code. BTW C++ runs lovely on realtime OSs. QNX ships with the same STL as Visual Studio 2017, complete with C++ 14.
If you did that, you'd lose type safety as you would end up instantiating that bitmask using an integral type rather than an enum class' enumerator. Implicit conversions may once again bite you in the a$$ just as they did with unscoped enums.
1) You may be right. Although I haven't stumbled on any case where that was a problem yet. 2) Simpler to implement for sure. But I want to make it simple to use correctly. Providing meaningful error messages makes it simpler for the user, which is more important to me. 3) You may be correct, I'll experiment with this idea. Thanks for the comment!
&gt; If the hardware cannot be trusted to reliably execute a branch, then 100% reliable software is not possible. True, this is why when we write code we **assume** that guarantees provided by hardware hold, even though we know that sooner or later it will fail. And we say that this code is correct on **assumption** that hardware/compiler/etc it is used with work as promised. I don't understand what you are trying to prove. &gt; The requirements of this part of C++ is well understood. Can you explain them to me then? (So far no one was able to...) &gt; if you really care about this, then don't throw exceptions. From my current (admittedly incomplete) understanding I see no barriers for compiler to implement this particular case without std::terminate(). But for some reason those behind standard decided not to enforce this and leave it to implementation. Why? &gt; reliability Reliability's definition depends on context -- you are thinking "guaranteed to produce certain outcome in this timeframe", I think "guaranteed to continue working under given conditions" (i.e. not crash). &gt; BTW C++ runs lovely on realtime OSs. Interesting... How do you guarantee that this particular function is going to complete within 1ms? 
&gt; (macro aside). 
I've got a few questions and comments : 1) Is `flag` both constructible and convertible with the enum's underlying type in order to allow stuff such as serialization? 2) I like the fact you have `operator!`. I think this is a worthwhile addition to my own code. 3) In the usage you gave, did you really mean `if(myflags &amp;&amp; flag::flag1 &amp;&amp; myflags &amp;&amp; flag::flag2)` or should it have been `if(myflags &amp; flag::flag1 &amp;&amp; myflags &amp; flag::flag2)`?
No, I dislike that post thoroughly. Here is my answer point-by-point: 1. I did create multiple such programs and the biggest one of them (I wasn't alone working on it) -- an obscure stock trading engine which worked for ~6 months (in 2003 on Windows NT, I think) without restarts. It was restarted because traders started complaining that they aren't getting signals in last 30 mins of the day. Turned out that we were running out of memory and 'wrapped up trading day early' as it was designed to do. Since system worked fine for months -- people grew compacent and no one checked logs for a while, so it was unnoticed and everyone assumed that related algorithms simply didn't detect any patterns. 2. "No one tests those execution path anyway -- so lets not create them". Really? 3. "Running out of stack kills us, so lets die on out-of-heap too"... 4. Has nothing to do with C++, and personally (even after learning Haskell) I don't like functional languages. I just dislike writing code in them. 5. known issue and a reason behind "don't throw from destructors" rule 6. "This error gets in a way of optimizer -- lets sacrifice correctness for speed"... 7. "640kb of memory will be enough for everyone"... 8. I can also catch std::bad_alloc or use non-throwing version of 'new' -- what is the point here? Granted, point (6) has merit in certain cases (like gamedev) -- but why mandating it on language level? Add this possibility as an option. 
So, would you be happy if this application crashed one day instead of catching std::bad_alloc, flushing the cache and continuing doing what it was designed to do?
 #include &lt;type_traits&gt; std::is_same_v&lt;Long&lt;Templated&lt;Stuff&gt;, With, BunchOf, Arguments&gt;, And&lt;The, Other, Type&gt;&gt; Long&lt;Templated&lt;Stuff&gt;, With, BunchOf, Arguments&gt; == And&lt;The, Other, Type&gt;
It looks like the new '14.11.25615-Pre' package is using VS2017Layout, and its property sheet is _named_ VS2017Layout, but the property sheet contents reflect D14Layout; consequently, all paths are wrong. :-[ (cc /u/AndrewPardoe)
Sorry, but the idea of what appear to be types being compared with `==` would make me 'wtf' longer than it would have taken me to parse the one extra set of angle brackets... ;-]
Let me point out that `int` is shorter than `auto` :)
I think is a great idea. I will try it and see if it works well.
Not sure if this will be super helpful, but I was just now playing with compiler explorer to try to reason about RVO myself. I came up with this: https://godbolt.org/g/zBHegs Both clang and gcc removed the first assignment regardless of optimization level, which might be related to your issue. Interestingly with optimizations, clang doesn't optimize away the second assignment (see the call to copy assignment operator=), but gcc does.
You're trying to disable optimizations... Do you know what the O in RVO stands for? Edit: OP, you're going to need to dive into the assembly to find out what's happening with different tweaks you make. 
I've given your implementation a try, but I'm not sure I've been using it properly. When I want to check a dynamic value against a predetermined mask, I build the mask and then use it like so : auto myMask = flags::a | flags::c; if (myMask &amp; someValue) // if at least one of those bits is set if ((myMask &amp; someValue) == myMask) // if all of those bits are set I understand the reason you started down this path is that you wanted to prevent code such as `flags::a &amp; flags::b` which is always `false`. As a matter of fact, I guess deleting `operator&amp;(E, E)` is a good thing to do as the proper way to compare single enumerators is `operator==(E, E)`. But currently, with the `flag_combo` and `flag_mask` separation, the reason I started down that path is not solved by your library as it allows the following code to compile : auto myMask = flags::a | flags::c; if (myMask == flags::a) // I want to explicitly force the usage of operator&amp;(bitmask&lt;E&gt;, E) What are your thoughts?
Congratulations that doesn't answer the question.
Decltype?
&gt; As much as I admire James, I think the example is a little disingenuous. Consider: &gt; unsigned int x = d + sizeof(T); This shows intent though, which means that if d &lt; 0 you can likely say that its a bug vs auto where it is unclear. I don't disagree with you about the rest of what you said, but i feel like ditching the original author's intent with auto often makes it significantly harder to figure out I recently worked on a codebase which was 99% auto, and I would have absolutely killed for some intent expressed through typing. You might think that s.version is clear now, but eventually we all write bad code if instead of `auto v = s.version();` we had `int v = s.version();` or even `version_t v = s.version()`, you then don't have to go and lookup the type when it isn't clear due to context This is doubly bad if functions are specified with implicit type returns so you can't even figure out what the concrete type its meant to be returning is, when all you wanted to do was print it to the console to do some debugging now you have a multi stage crawl through the codebase This, at least for me, made everything 1000% more irritating to debug when there were bugs, and a conversion to concrete types even when they were obvious to the programmer from context was a massive aid. I can infer that `auto s = "some_string"s` is a string, but I don't need to engage my brain to figure out what the type of `std::string s = "some_string"` is
that's what you need: https://github.com/Manu343726/ctti
Could you please elaborate? I'm not sure where exactly the author is trying to disable optimizations, am I missing something?
He says he's using something called DoNotOptimize(), whatever that is, and volatile variables... 
&gt; DoNotOptimize Ah, I see, I missed that. Thought you're talking about the frontend invocation.
The problem with std::string, of course, is that you get stuck with a specific compiler in a specific mode. If the entire stack is an internal development this may not be a problem but if you expect others to also use this API it is a bit of a nightmare. Case in point, I received a DLL that uses std::string and std::vector in its API. Initially it was compiled with a compiler that was so old I couldn't even install it on Windows 7. Later I received a version for a more recent compiler, but even then I cannot compile in debug mode since it changes the object layout for std::string. And eventually I'll have to write my own wrapper DLL which I'll have to compile using an older compiler, just to decouple my software from theirs and avoid getting caught up in their bad design decisions. 
I suspected this might be the case, but it seems things are fixed in 14.11.25615-Pre.
Yes, sort puts objects into a moved-from state. _And then it assigns to them, which is always an allowed operation after a move, so they are once again in a valid state_. It's simply not true that "those moved-from objects still have to meet all the requirements of sort"; that state only lasts for as long as it takes to swap two elements, and after that all objects are again in a valid state and sorting can continue. I really don't see what the problem is here. Sort is not randomly moving from objects and then calling comparators or whatever on them; it requires a specific invariant on your data, and it will itself do nothing to destroy that invariant, for example, by moving from an object and then leaving it in that state. Have you seriously been spending "no end of time trying to correct"? Because that sounds like a rather bad case of rule lawyering to me... 
Could try the example on the Wikipedia page about RVO. Or maybe a really old compiler that doesn't do RVO, but they've been doing it for a long time (even before C++11)
https://en.wikipedia.org/wiki/Return_value_optimization
A single layer of complexity hardly defeats me. TMP is a powerful, Turing complete language with awful syntax. It takes a significant effort just to code most basic things in TMP. For instance, sometimes we would like to extract the return type from a function: //with TMP template&lt;typename T&gt; struct function_trait; template&lt;typename RetT, typename... ArgsT&gt; struct function_trait&lt;RetT(*)(ArgsT...)&gt; { using return_type = RetT; }; template&lt;typename T&gt; using return_type = typename function_trait&lt;T&gt;::return_type; //usage return_type&lt;decltype(MyFunc)&gt; I mean, how many non-senior programmers can get this right without doubt? but: //with type tag template&lt;typename RetT, typename... ArgsT&gt; auto return_type(TypeTag&lt;RetT(*)(ArgsT...)&gt;) { return typetag&lt;RetT&gt;; }; //optional, but can further simplify caller code template&lt;typename Callable&gt; auto return_type(Callable&amp;&amp;) { return return_type(typetag&lt;Callable&gt;); }; //usage return_type(MyFunc); //type: TypeTag&lt;return type of MyFunc&gt; return_type(typetag(MyFunc)); //same as above and not to mention this can be further simplified if the function signature itself can be wrapped by simpler definitions. The point I'm trying to make is TMP's funky syntax is one of the major obstacles for most c++ programmers I know (I'm in one of top game dev companies in Europe) to know its power, and for non-C++ programmers to get into the language.
I know the library and I like it
Similar situation here, getting stack overflows in debug builds due to a different bug with static_assert(offsetof()) and can't find a way to back down to 15.2.
Many serialization libraries require that classes have default constructors for deserialization. There are some workarounds, but they require lots of boiler plate if a class contains members without default constructors. Because of that most of the classes in my project are polluted with default constructors that should never be called outside of deserialization. It doesn't look like there is a good solution without a very strong reflection API. 
In this presentation about move semantics this is discussed in the hidden slides 7-17: http://becpp.org/blog/wp-content/uploads/2015/01/Bert-Rodiers-Move-Semantics-Part-1.pptx Nowadays most compilers unconditionally apply the RVO, the NRVO (Named Return Value Optimization) however typically only happened when certain conditions are met, so it is easier to test that one. The performance gain would be very similar for the RVO and the NRVO. Some more details can be found here: http://en.cppreference.com/w/cpp/language/copy_elision 
Sure, I'll add add a request. Or two.. :)
That's a good point about multithreading being an issue with this approach. I'll admit I'm not an expert in multithreading - so it's not something I'm really considering at this juncture. Switching it to a system somewhat like you said shouldn't be too hard if it does turn out I need to do that though - as I currently have an array (like you mentioned) for the previous frames input, so I can see the delta for held, pressed, released events, etc.
Isn't RVO mandatory since C++17 though, so no longer an optimization? It also affects whether the program will compile or not because with mandatory RVO you don't need a copy constructor to be defined, whereas with non-mandatory RVO it you need it even though it won't be actually called.
Congratulations you're sight impaired and mentally challenged. A) takes more characters to express B) require an header inclusion C) Weakness of ',' separator In a template parameter list you can find a lot of ',' with/within various other template parameter lists. which mean that the character ',' is not by itself sufficient to distinguish the two parameters of std::is_same_v. which mean that it requires to parse all of the first argument to find the second argument. which mean that it's more complex than to have two types separated by a character that is not present in both. I don't like this proposal and it won't happen, that's not the point. The point is that arguing that is not simpler is false.
That would rule out copy as a valid implementation of move, thus ruling out all the classes where move construction/assignment is not defined, or would degrade performance in some cases. 
Which of the optimizations are implemented in libc++?
There's good and bad news: The good news is that MS has thought about this and is offering bootstrappers for fixed VS versions. They offer one for VC 15.3 (Visual Studio 2017 Update 3) and one for 15.0. None for 15.2: https://docs.microsoft.com/en-us/visualstudio/install/create-a-network-installation-of-visual-studio#how-to-create-a-layout-for-a-previous-visual-studio-2017-release The bad news: It just doesn't work. The bootstrapper for 15.0 is malfunctioning. Lesson learned: Never trust a cloud service to handle backups for you. It might not be there when you need it. Now back to Visual Studio 2015 Update 3... PS: Fun fact: After downgrading to VS 2015 compiler warnings identified 2 real bugs in my code that VS 2017 did not see (not even the static analysis). Go figure.
Ah yeah! Good point.
Indeed in fact neither constexpr auto version = std::array{1,0,0} or constexpr std::array version = {1,0,0} compile with my VS2015 compiler it asks for the template type and size.
Maybe I get it wrong, but you could just return a vector or string from a function and inspect the assembly. You will notice that - given copy elision applies for the returned variable - it's moved (instead of copied) into the return value i.e the move ctor/assignment op should be called. Needless to say that moving a string is faster than copying it. (Except for SSO) 
No, but that shouldn't^* happen. `std::bad_alloc` in particular is a special case, as it needs to be allocated when there is no memory (so it is treated as a special case, possibly pre-allocated, or using a pre-allocated buffer). ^* If this is not happening (`std::bad_alloc` being throwable even in low memory conditions), you should change compiler (but I guess only if this is a real problem for your scenario).
Are we not being *just a bit pedantic* here? His text was correct, just not complete. 
People misunderstand the ABI compatibility of C++ toolchains with themselves, with C++ ABI compatibility across different toolchains, or even, against different versions of the same toolchain. E.g using clang with libc++ on Linux to compile some binary that links against a library compiled with gcc and libstdc++[*]. If you are lucky, your code will compile correctly, link correctly, and crash fast. If you are unlucky, your code will run ok and just run into UB when your clients are looking. I don't know if it is really the linkers fault here, but in these situations, the code should fail to link, 100% reliably. [*] replace this example with compiling the library with gcc 4.9 in C++03 mode and your code with gcc 7.1 in C++17 mode using two different versions of libstdc++. The exact same situation happens.
Added to which, you may (as the original article implies) want to interface to the API from other languages - I'm not aware of any other language that interfaces to the C++ ABI (probably because it varies by compiler/platform/compiler version/...).
Eliminating copies is one optimization the compiler is allowed to do even if there are side effects. With that in mind, you should be able to count the number of objects constructed and destructed by incrementing global counters every time a custom object is constructed or destructed. You can then count the theoretical number of copies vs the actual to see if your getting the optimization. This is important because RVO is so ubiquitous that is can happen even in debug builds. If you're getting it in non-optimized code you might have a problem making it not happen to be able to test the difference. 
&gt; I'm not aware of any other language that interfaces to the C++ ABI D is able to. Julia as well with a lib (https://github.com/timholy/Cpp.jl). 
Archlinux isn't in the future, arch is in the present. It's debian who live in the past. So supporting debian is supporting the past.
I suspect that in this case, "people" had absolutely no idea what was going to happen. They were upgrading an older application from C to C++, and on the whole I think they did a good job, but they certainly weren't C++ experts when they started, and this is an easy mistake to make. Look at all those shiny strings and vectors! That sure makes life easier! And it works fine all during development, because they are still a bit conservative and don't want to upgrade their development environment. So after _years_ of building this they present their work to the world... and nobody can use it, because they all have different compilers. I'd be pissed if it happened to me. So, then the question becomes... how can we fix it? Or maybe, "should we even try to fix it?" A complete fix, for all the possible types, might prove to be very, very complex. But maybe that is too much anyway; maybe it is good enough to provide a set of specific interfacing types for this purpose (distinct from the types in STL) that _are_ guaranteed to have a fixed layout, possibly at the cost of some performance and capability. Of course we already have the ability to work with C-style structs, but maybe some intermediate ground can be found between raw structs and something that is still RAII-enabled, yet also offers a stable ABI. Anyway, these are just some random thoughts on my part... Oh, and having the linker refuse to link would be a good first step. At least I wouldn't have wasted a day figuring out why my debug build wasn't working... 
Thats a good point. Luckily Ive so far been able to exert complete control over the toolchain so that ABI incompatibility is simply not an issue  everything was compiled and linked using a consistent configuration. I realise that this is not representative.
Toolchains are generally binary compatible in a platform when compiling and linking C code, so many developers just extrapolate and assume that this will be the case for C++ code. We can't expect that all C++ developers know about this, we can't teach them all about this, and most of them will never need this knowledge. So IMO this isn't the problem since this has no solution. The problem is that there is not a single piece in charge of detecting these errors in any C++ toolchain. This wouldn't be an issue if when you attempt to statically or dynamically link incompatible code you'll get an error of the form: "You compiled binary X with GCC 7.1, C++11, libstdc++, libstdc++abi, and are trying to link it against dynamic library Y compiled with clang 5.0, C++17,libc++, and libc++abi. This cannot work."
Please refer to Clean Code, Chapter 5, "Horizontal Alignment" on page 87f. if you are interested in details, why people might think completly the opposite of what you said! The *gaps* are more disturbing, as in reality there should be no gap in horizontal space! You should read the whole line, in oder to understand it. With a horizontal alignment, you fosters the eye to skim vertically, which makes you missing important aspects.
/u/14ned &amp; /u/dodheim, the package did install for me. But I never tried compiling. That yields this error: 1&gt;TRACKER : error TRK0005: Failed to locate: "CL.exe". The system cannot find the file specified. Clearly we still have a bug. Sorry, updates later. 
There is already an issue for this: Its is closed as Won't fix https://developercommunity.visualstudio.com/content/problem/12991/cmakeopen-folder-include-directoriessystem-still-h.html I'm adding a new one, more to the point and without the CMake noise.
OP isn't using C++17.
A valid and correct sort algorithm *could* move from an object and then compare it with itself. This would not be an optimal algorithm, but it would be legal. Stranger things have happened. One implementation of std::reverse once swapped the middle element of an odd-numbered sequence with itself. Smart? Not really. Correct? Yes. Legal? Yes. If you use any std-defined function, and that function requires MoveAssignable and RequirementX, and your moved-from object does not meet RequirementX, you are invoking undefined behavior by failing to meet all of the requirements of that std-defined function. I don't have a problem with you invoking UB, as long as you're not writing code that is critical to anyone (safety critical, financial, etc). I certainly will not, *under any circumstances*, allow such code on the projects I am involved with.
All of them shown on that link. libc++ was used to obtain those measurements.
Nope.
&gt; Does windows.h compile with it? Yes; well, the Win10 SDK does anyway, not sure about 8.1 or earlier ones.
Are you sure there is actually a real problem here? As I said earlier, exception objects are typically small. And it's not as if you will be allocating thousands at a time: each thread can have at most one in flight, so any strategy that preallocates a tiny amount of space in a thread-local context is already going to be just fine no matter what. This thread has some interesting comments on the subject: https://stackoverflow.com/questions/27259652/where-does-exception-object-have-its-space-heap-or-stack-and-how-to-access-it 
&gt; I hate to say this, but can't you just use bugzilla like clang and GCC do? You don't have to hate saying this. You can push on this as much as you like. You're the customer, your opinion is valuable. And there might be some people inside of the VC++ team who want the compiler team to use something like bugzilla as well. No names, of course. But heck, those people just might exist. Edit: Consider adding a [DevCommunity](https://developercommunity.visualstudio.com/topics/C%2B%2B.html) suggestion that we use an open bug-tracking system such as Bugzilla. We'd love to know how much community support there is for such an idea. 
That's because deduction there is a C++17 feature.
Still C++11/14 standards don't forbid RVO (or copy elision in general), so even disabling all optimizations may not disable it depending on whether the compiler considers RVO an optimization and not just baseline behavior.
I don't know of a way to downgrade the VS product, but there are archives of the MSVC toolset on NuGet.org. Search for visualcpp's packages. More about using the NuGet toolsets in a project/solution here: https://aka.ms/dailyMSVC. 
This isn't a permanent decision as much as it is a temporary artifact of where we are in modules development &amp; VS integration. It's a question of where we put resources right now. If it helps, some of us are still pushing to have VS use 64-bit tools by default : )
It's never a bad idea to send important bugs to people you know who work for the team. There are a few of us in this Reddit discussion already.
hence `-fno-elide-constructors`
Thanks. A more direct issue is more likely to get upvotes. And could you please send me a link to it? I want to be the first MSFT person to comment. 
We can turn any discussion into a C++ discussion!
I've generated `VisualCppTools.Community.Daily.D14Layout.14.11.25617-Pre.nupkg `and am uploading to the server now. I've tested that it installs cleanly and builds an impressive battery of tests (test cases may be limited to`int main() { }`).
Maybe CMake needs to implement something that let you do that without having to submit a patch.
That's why OP is using `-fno-elide-constructors`
&gt; If you first check and find out it is not empty, then you can pop_back it. Will `moved_from_vec.empty()` ever return `false`?
Yes, it can, with some allocators, and with move assignment, but no, not with std::allocator or move construction. That isn't specified for vector, but rather falls out from the implementation that must meet a great many constraints. That result can not be generalized to string, which is further complicated by the short-string-optimization.
Is it possible to build this into CDT eclipse? Or would Eclipse CDT need to explicitly support this?
Personally I would love to see a 'practical example' of how this library could be used on the github page, because the pitch in of itself "modification&amp;composition of objects at runtime" sounds like something I would like to get away from unless I'm specifically building, say, a scripting language VM. ...I"m quite certain I'm wrong and there are many wonderful usecases for this, as in, code examples, not closed source projects that use it. 
Would std::function(s) be able to be generalized for your purposes? 
I didn't know that, thanks. Unfortunately we don't work with a C++17 compliant compiler yet.
The [previous version](https://www.reddit.com/r/cpp_review/comments/6rdl80/review_of_dynamix/) is under review on r/cpp_review. With one very extensive review giving you a good insight into the library.
I know :) It is a pretty big task though. I am working on it, but unfortunately to really see the benefits of the library, the project needs to be big. Like... man-weeks-big. So far there are hints of use cases spread among the tutorials and examples. Still I acknowledge that an real-world-ish example will be highly beneficial. It will be my next focus for when I have time to work on the library in the following months
That's a good idea. Maybe enter a [Developer Community suggestion](https://developercommunity.visualstudio.com/topics/C%2B%2B.html) and send me the link so I can push internally? Note that there might not be much more we can do here, though we can maybe make a more transparent error message. The format of [C1001](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/fatal-error-c1001) is supposed to include the compiler source file and line number. It's often the case that this information isn't available, in which case, you won't get much information in the error message. * Sometimes the [ICE mentions a UTC file path, or C2](https://stackoverflow.com/questions/36005206/c-fatal-error-c1001-an-internal-error-has-occurred-in-the-compiler). This is an optimizer ICE. * Sometimes the [ICE mentions a cxxfe file path, or c1xx](https://stackoverflow.com/questions/35068935/what-pattern-matching-if-any-is-applied-for-c-variadic-template-function-inv). This is a compiler parser ICE. * Similarly, an [msc1.cpp ICE](https://stackoverflow.com/questions/17451978/fatal-error-c1001-internal-compiler-error-compiler-file-msc1-cpp-line-1794) is the compiler parser. If the ICE just says "cl.exe" there's no information available. 
Instead of reverting back to VS 2017 Update 2, you can consider passing "/d1ReturnUdtEHInline- /d2ReturnUdtEHInline-" to cl.exe. The flags will disable a recent improvement to the inliner which is causing this bug.
I don't think you can cast a `std::function&lt;int()&gt;` to a `std::function&lt;void()&gt;` back and forth. Also, `std::function` have quite some overhead compared to a plain function pointer. If I can have the same thing without reinterpret casts and without adding overhead, it would be great.
Isn't `std::async (std::launch::async)`basically required to create a new thread by the standard and so Microsoft implementation is non-conforming (I'll be happy if I'm wrong)? And if so, how it could be 14 times more efficient than thread creation? Anyway, in the current state (without `.then`, 'when_all', cancellation etc) `std::future` is pretty much useless. And in the future it'll be obsolete because of `co_await` that allows for much more lightweight futures.
But the RVO is part of the ABI on x86_64, so I'm not sure that flag will actually do what it says it will as it would have to actually *insert* extra calls to create temporaries. Other's more knowledgeable than I might be able to clarify that.
I need clarification on this because RVO is part of the ABI on x86_64, so I'm not sure that that flag will actually do what it says it will as it would, I think, need to actually *insert* extra unnecessary calls to create temporaries. 
Some minor typos: &gt;This can serioualy &gt;Lock free programming programming 
I think that one point that is danced around here, but never made explicit, to the great detriment of the article, is that a move constructor for a non-trivial type basically requires a default/nullary constructor. It doesn't have to be public, but it does need to be present. And fast, noexcept move constructors *are* operationally essential to an efficient STL. That means you are stuck with the following choice for non-trivial types: 1. a private, "toxic" default constructor. 2. a public default constructor. 3. Making it immovable. Most ownership will occur through a customized `optional` wrapper that allows moves by emptying out the optional (note that you can't use `std::optional` for this). The thing is that even if you choose 1, you still need to be able to destruct correctly. That means you need to know whether its toxic. So that state still ends up being there. Furthermore, in line with what /u/HowardHinnant mentions, you basically now have this implicit precondition with every single function call: not in moved-from state. This is not good. Color is not really a great example because any representation of color you can think of (enum, RGB, etc) will almost certainly be a trivial type where moving = copying, so you don't need a default constructor. And yes I agree not having a default constructor there is just fine. The really tricky examples in C++, are when you are dealing with things like a file. The most common non-trivial objects in C++ are containers, but those are fortunate enough to have an obvious default constructor. Does it make sense for a file object to only maybe hold a file handle? Shouldn't that be encoded by the optional type? But if it always holds a file, what does "movement" mean? What gets left behind? Or do they both point to the same file (moving=copying)? `fstream` is maybe the closest thing to a file object in the standard and it is default constructible. There are some clever things that you can do, but I don't know if there is a design in C++ for something like that which is really perfect. I think it's just a matter of making trade-offs and often (usually) having default constructor is still the best choice. On a final note even containers can be tricky. An interesting fact is that in C++17, the default constructor for `vector `became noexcept. Ok, this makes sense, right? But for many other containers, it can't always be taken as a given that the default constructor can be noexcept. And if it's not noexcept, the move constructor will not be noexcept either. But without a noexcept move, many things can be much slower. You may have some nasty surprises if you start playing with a `vector&lt;unordered_map&lt;K, V&gt;&gt;`. Edit: link to very smart people discussing the issue here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4258.pdf. Gratified to see that my comments above are not too far off.
[The standard](http://eel.is/c++draft/futures.async#3.1) is a bit hard to read on that. "_as if_ in a new thread of execution represented by a thread object" implying that it just has to behave like, followed by "The thread object is stored in the shared state and affects the behavior of any asynchronous return objects that reference that state." implying that there has to be a thread. 
Good observation about the relationship between default construction and move construction. Fwiw, survey (I think still current) of the different container implementations of move members, default construction and noexcept: http://howardhinnant.github.io/container_summary.html