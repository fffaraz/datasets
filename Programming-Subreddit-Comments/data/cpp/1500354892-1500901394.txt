He bypassed because it stops the program from running and change the registers to make the if true instead of false so when it is resumed it will be pass acepted He lets the strcmp happens and when it is done go to the cpu registers and change the result of the strcmp, the program will not going back and do again the cmp so it will be true always, if(0) is always true, if(anything except zero) is always false
Someone mentioned something about international treaty when it was brought up in LEWG, that could be it too. Edit: I think it was Guy! From the linked article: "Unfortunately ISO rules forbid recording, so the whole things has to be done by typing as people speak, which is not an easy task, as I discovered."
You don't have to apply, you can just notify them and show up!
you can't pretend like unicode is ever going to go away. ASCII is over. face it.
Got it ðŸ™‚ Thanks
Yes, and trim, trim_left, trim_right should trim inplace to avoid memory allocations. In addition, I want trimmed, trimmed_left, trimmed_right, which are const functions and return a copy. This way, I can still declare a new std::string as const.
https://github.com/jarro2783/cxxopts
`to_case_fold` as well. 
That looks like Objective C++ (or at least a library coming from the NeXT era or early Mac OS X). The interfaces must be callable from objective C
&gt; Desktop applications would be easier with JavaScript or Java, web applications easier with NodeJS, PHP, or Python, and even games are easier to make with C# and Unity Engine. They might be easier to make, but that does not mean they will be efficient too. Whenever you have to deal with tons of data, quickly, the languages you mentioned fail quickly. So try to build something where performance matters, and you'll learn that C++ can be a good choice.
Ah I see now, thanks for the explanation!
Argument parsing libraries that insist on parsing into global storage (e.g., gflags, and perhaps this one according to my skimming of the docs) have caused me great deal of stress in the past (and present). Why couple parsing a list of strings with a decision about how the results of said parsing is stored? I guess I should elaborate. Why might it be bad to have arguments parsed into global state? It's just argc+argv, right? It's not like that's likely to change during execution... It's not like anyone is ever going to reference that global state /directly/, making things "fun" to unit test. And definitely nobody /ever/ is going to refer to that global state from library code, right? Sorry for the rant. I work in a /really/ large codebase that is infected with the "let's map argv -&gt; global state" disorder. I'm seeing a therapist; hopefully I'll make it out of this alive :p
Additionally, such split function should operate on std::basic_string_view, not on std::basic_string.
You're missing the point. `std::string` can be and often is used to store utf-8 text but adding a method like `to_lowercase` basically guarantees that utf-8 text will not be transformed correctly since there's no way in hell the standard library is going to provide a fully conforming implementation. This is a problem precisely because unicode isn't going away.
Aha. Even then I would not use those interfaces myself but create a separate API on top of my own sensible API.
You pay for it every time you compile.
this JS script checks if the highlighted text is `P___` or `N___` then open wg21.link.
The option parsing is a very specific task by its nature. Like a logging or a config parsing. Any attempt to generalize it is doomed to failure in advance. Every full featured realization will be too complicated for most of the real use cases. And every simple realization will lack of some functionatiliy that seems impotant to most of users.
std::string understands no encoding. I think most operations do not have to depend on encoding, and adding encoding support might slow them down (because then they need to ensure that the strings conform to the encoding rules). There's [P0244](http://wg21.link/P0244) that deals with character encodings.
With hard-coded values, the compiler would catch the problem up-front (e.g., g++ says: " warning: integer overflow in expression [-Woverflow]"). As for the second, the point is that beginners typically *only* use literals as format strings--it takes forever to convince them that essentially any string will work, not just a literal. So no, most don't accept user input as format strings (sanitized or otherwise).
So... you're arguing that all header-only libraries are bad?
What real world usage does this function have?
https://github.com/tanakh/cmdline
That service is amazing and is the reason I could make this with so little effort. The idea was that because I find myself manually typing wg21.link/... all the time, I could make that part faster. Edit: Oh, I see what you mean now. I didn't realize there was actually a Chrome plugin for this. I thought you were referring to the URL shortening of papers. That's awesome!
No, just that they do still have a cost.
&gt; I have never once seen production code that used that function, but it's nice that it exists. std::vector has always had it from the time being as well: [std::vector::at](http://en.cppreference.com/w/cpp/container/vector/at)
Also [join](https://msdn.microsoft.com/en-us/library/57a79xd0.aspx).
`gflags` is nice, I used it at work. Though it seems to me like a bunch of macromagic tricks and I personally prefer objects over macros, so I'm not really happy with the syntax. Nonetheless, in terms of feature set and maturity, it's great.
This library has been submitted for review in r/cpp_review: https://github.com/igormironchik/args-parser
&gt; For example, iteration by code point or by grapheme cluster, normalisation, case conversion, conversion between UTF-8, -16 and -32 representations without having to use the godawful codecvt API, etc, etc I never understand why all of this is needed. In my opinion unicode strings should just be treated as opaque binary data you don't have control over. Just pass them and copy them.
Whats the point of fighting infinities so hard at C spec level? libstdc++ std::complex looks somewhat simpler (though i only took a glance), did you do any experiments with it? template&lt;typename _Tp&gt; template&lt;typename _Up&gt; complex&lt;_Tp&gt;&amp; complex&lt;_Tp&gt;::operator*=(const complex&lt;_Up&gt;&amp; __z) { const _Tp __r = _M_real * __z.real() - _M_imag * __z.imag(); _M_imag = _M_real * __z.imag() + _M_imag * __z.real(); _M_real = __r; return *this; } 
&gt; no copy operator: copying strings gets out of hand way too quickly. In my experience, the copy operator on vector and string is called by mistake 99% of the time. The preferred way to make a copy should be calling copy() Please no.
 % ls -1 /usr/lib/x86_64-linux-gnu/libboost_program_options.* /usr/lib/x86_64-linux-gnu/libboost_program_options.a /usr/lib/x86_64-linux-gnu/libboost_program_options.so /usr/lib/x86_64-linux-gnu/libboost_program_options.so.1.62.0 It isn't header only, and hasn't been in the 15 years I've been using it.
If I have not already boost in the project, I find https://github.com/jarro2783/cxxopts very useful. Small, Header only. 
Coroutines also fix this because you have somewhere for the string to live when you return the iterators into it
This is why we need package managers. Just so useful libraries can gravitate towards being 'part of the language' without actually having to go through all of this.
Interesting idea, but I think initializer_list might be a better choice than vector.
&gt;Accessibility program I applaud this idea. Good job. &gt;Like last year, the lounge track will include a meetup on diversity in C++, reaching out to other communities has made this years submissions, speakers and talks a bit more diverse then in the past. Way to go.
&gt; "Well, I've been doing this for X years so I know better" That's not what they said though. They're saying they've been doing this for X years and they still make mistakes. Maybe he's an incompetent programmer, but if you've been doing it for that long and still can't write secure code, then what promises do you have that other people with even less experience will do better?
Trim and split are the big two. Uppercaseifying things is also extremely common to simplify input, so it should be worked into std if possible. Only other thing I can think of is that one C string function that doesn't have a C++ equivalent... strtok?
You mean this? http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0707r0.pdf
You missed specializations for `complex`, `double`, and `long double`, which fall back to corresponding C types. https://github.com/gcc-mirror/gcc/blob/f0f5171608d68c3cb3aa6aa43d64814d4f9d67d5/libstdc%2B%2B-v3/include/std/complex#L1171 operator*=(const complex&lt;_Tp&gt;&amp; __z) { _ComplexT __t; __real__ __t = __z.real(); __imag__ __t = __z.imag(); _M_value *= __t; return *this; } And the article says that they tested `libctdc++`.
It is *impossible* to do case-insensitive compare without some additional information. 
While this is true for most applications, there is a lot of cases there it is needed. Anything that does text processing: text editors, for example, etc. 
Ah, good old undefined behaviour... Weird that it's a divide by 0 exception though, when it's not dividing by 0...
I would like to see starts/ends_with in the &lt;algorithm&gt; header so we can also use it with containers
Correct, one of the reasons we went with Java is that is what Buck is written in. Buckaroo introduces no dependencies that Buck does not already have. Buckaroo can in theory be used for languages other than C++, although we haven't targetted that yet. A big advantge of Buck is that it has good support for mainstream languages (and you can support more yourself using the Python scripting layer). Modern projects are often cross-language, and Buck respects that. 
In [his trip report](https://herbsutter.com/2017/07/15/trip-report-summer-iso-c-standards-meeting-toronto/) Herb said he's going to post it this week.
Agreed! This is why we chose Buck rather than building our own build system. Buck is package manager friendly because it has a concept of "cells", which are isolated systems of targets and dependencies with well-defined entry-points. There is a good discussion about Buck on this HN thread (https://news.ycombinator.com/item?id=13865400) and some of the Buck team also weigh in. Buck is absolutely production ready - it is used by Facebook, Uber, Dropbox, Gerrit and a few others. 
&gt; Iâ€™ll post a separate report on the metaclasses part soon, probably in the next week or so, with a link to the ACCU video. [Source](https://herbsutter.com/2017/07/15/trip-report-summer-iso-c-standards-meeting-toronto/)
Buck, Bazel and Pants are very similar, so I think it is best to just pick one. Buckaroo in theory supports any language Buck supports, although it is untested. Is your repo public? I would love to take a look. 
The Conan team have done impressive work in this regard, however I think that support for multiple build systems is a local optimum. Having many build systems and configuration options makes it difficult to reason about and optimize the build. If you have only one build system, then it can examine the process end-to-end and make guarantees such as reproducibility and artifact caching. But with many build systems glued together, the complexity explodes and it becomes near impossible to do this. Each link in the build chain becomes a black-box. If you have a legacy project that is very hard to port to Buck, then it is possible to wrap the old build in a small Buck file. Done is better than perfect! 
Soon.
They reimplement std::complex without any checks and claiming that it's faster. Well of course! Just compile std::complex with -fcx-limited-range and time difference will be negligible. Or I got it wrong in [my version](https://github.com/yekm/sandbox/blob/master/cpp/complexbench.cpp) ? 
&gt; I never understand why all of this is needed Some examples off the top of my head: * What if I need to ensure that a given binary blob is indeed valid UTF-8? * What if I need to compare two valid UTF-8 encoded strings to see whether they're the same? * What if I need to compare two valid UTF-8 encoded strings to see whether they're the same, ignoring case? * What if some random social media platform arbitrarily limits you to 140 "characters" per message, and I need to ensure that a given string is below this limit? * What if the username field in my database can store at most N bytes, but truncating the blob would result in an invalid Unicode string that would be rejected by other applications? * What if I want to take some search results and order them alphabetically (according to the user's locale, of course)? * What if library A hands me a UTF-8 string, and I need to pass it to library B which expects UTF-16? * What if file format A specifies that strings are stored on disk as UTF-16BE, and I need to read that in and pass it to library B which expects UTF-8? None of this is particularly outlandish stuff. Modern, 21st century languages handle all this much better than C++. In Go and Rust for example, strings are byte arrays which are always UTF-8 encoded. Swift uses UTF-16 internally for backwards compatibility with `NSString`, but this is largely opaque to the end user, and it's possible to get UTF-8 and UTF-32 "views" with a single method call. (Swift is also unusual as its "character" type is actually a Unicode extended grapheme cluster.) The fact that you need to use an external library like ICU to handle these things in C++ is an embarrassment that should be fixed. 
&gt;github link That's pretty strange. There doesn't seem to be anything in std requiring such implementation. [libc++ does the same](https://github.com/llvm-mirror/libcxx/blob/master/include/complex#L590). &gt;And the article says Oh yeah, i missed that somehow. I read that as his friend doing that only (but the results are provided).
Tutorials for everyone. Learn and work perfectly.
 &gt; Learning new obscure build system that noone uses I have to disagree with this. It is absolutely not a build system that "noone uses"; Buck is used by several companies, including Facebook, Uber and Dropbox. Additionally, it is easy to learn because it has nice abstractions written in Python, which is a language anyone (and certainly a C++ developer!) can grok. 
&gt; What if some random social media platform arbitrarily limits you to 140 "characters" per message, Then it's a problem for the social media platform for starting with an american-centered definition of "character". The notion of "character" just does not make sense for text, it's entirely cultural and subjective. Stuff like "What if I need to ensure that a given binary blob is indeed valid UTF-8?", "What if library A hands me a UTF-8 string, and I need to pass it to library B which expects UTF-16?" are the same than for any other library that works with specific data formats: would you expect the standard library to have ways to check if a stream is a valid JPEG, PNG, WAV ? Also ensure HTTP validity while we're at it ? No, we have existing libraries that do it. &gt; What if the username field in my database can store at most N bytes, but truncating the blob would result in an invalid Unicode string that would be rejected by other applications? ... because otherwise truncating is an acceptable option ? the only meaningful thing to do is to reject the input entirely. And you don't need unicode handling for this.
Okay... I guess I'll be more clear. The OP was concerned about boost being overkill if all you need is program_options. I was reminding OP that if all you use is program_options, then all you pay for is program_options, not all of boost. 
three options: * ignore them * provide a replacement map * provide a replacement characï¿½er
`initializer_list` was a mistake.
Professional palindromator here. People with words ask me all the time if they have a palindrome. I could use such a function dozens of times every year!
You're right. I've edited my post to correct that mistake while still communicating the point I was trying to make. 
&gt; would you expect the standard library to have ways to check if a stream is a valid JPEG, PNG, WAV ? If they were used as universally as strings, then yes. I don't think we're disagreeing with each other, necessarily: I agree that a string should be regarded as a binary blob in the same way as, say, a PNG-encoded image is (you certainly shouldn't be able to poke at random bytes as `std::string` allows, for example). What I'm arguing is that the need to handle such formats is common enough that it deserves standard library support. Or to put it another way: the lack of standard library functionality for handling Unicode leads developers to do the wrong thing in many cases, simply because it's easier. We should fix that.
Pass an output iterator like every other algorithm.
Until this article i have never ever heard of Buck. World certainly could use a good build system based on python, but problem here is not groking python but knowing ins and outs of build system, what it can and can not do and what are idiomatic ways of doing certain things. I am not even talking about IDE support here which is another nail in the coffin. CMake with all it's deficiencies works pretty good. Buck would have to do much better than CMake to stand a chance.
[Boost.Program_options](http://www.boost.org/doc/libs/1_64_0/doc/html/program_options.html).
Make people chuckle on Reddit...
I guess many just need it for a-z vs A-Z?
&gt; there's no way in hell the standard library is going to provide a fully conforming implementation Why not? If we're going to standardise Cairo...
docopt.cpp worked quite well for me. You specify the command line message and generates the legal grannar for the command line.
if someone needs generator - just use lambda. int main() { int start = 7; auto gen = [i = start]() mutable{ return i++; }; std::cout &lt;&lt; gen() &lt;&lt; "\n"; std::cout &lt;&lt; gen() &lt;&lt; "\n"; return 0; }
contains (even if it's just a wrapper for find and check), replace, and some algorithm shit
Hi Allan! This is correct, you don't even really need to notify them. You can just show up and participate. Your company doesn't have to pay unless they want to be a voting member (meaning planery only, you can still vote in WGs) in PL22.16 (the US ISO c++ group). Other country's orgs will have different cost structures. 
Have you ever been in a situation where you wanted to add a dependency on Boost.ProgramOptions but did not manage to? If yes, how would a package manager have helped in this situation?
I don't care as much about every C++ tool being in C++. I'd love some cross-language dependency management. With that said, what I do care about is ease of deployability (how many steps to setup a machine) and I feel like involving Java and Python is too much.
Sure, but when you're at the point where you can realistically get a completely new header/library added to the standard, finding an employer who supports this will be the least of your worries.
That reminds me, I've been hacking on a paper FOREVER trying to add another option. I have been leaning toward an initializer_list of string_view objects. Sadly, it requires an allocation (which has been my hang up) as well as making 'string_view' be a special type like initializer_list.
I was referring to the earlier discussion about standardisation of libraries, vs. 'merely' providing them through package management software of some kind. I hope you do agree that some better way to distribute C++ libraries is desirable, yes? One way is to add every library and the kitchen sink into the standard - meaning you get to deal with all the issues the OP listed. Another is to have a wide variety of choices available in packaged form, so you have access to a wide selection of up to date libraries, but are spared the trouble of having to build them yourself. 
There was a CppCon15 talk independently discovering these things.. it seems documentation is lacking somewhere. https://www.reddit.com/r/cpp/comments/3o3big/cppcon_2015_andr%C3%A9_bergner_faster_complex_numbers/ 
That additional information could be a default, obtained from the environment.
The `-&gt;` already does this automatically. It keeps chaining `operator-&gt;` function calls until it ends up with a regular pointer.
The `-&gt;` already does this automatically. It keeps chaining `operator-&gt;` function calls until it ends up with a regular pointer.
I think everybody here is missing the big one. The function that has the ability to completely change the way we think about C++. The function that will dramatically change the language, and what we can do with it. I mean, of course, eval(). std::string a ("int x=3; return x*2;"); int b = a.eval(); // b is 6 ;-) 
Currently, using Boost.ProgramOptions without building it myself is as easy as aptitude install libboost-program-options-dev Since you seemed to describe a "better future" that could be had if only there were a proper C++ package manager, I was just curious what specific problems people have, and how solving them would reduce the need to standardise libraries? The most common reason where people can't use boost that I'm aware of is projects with very strict guidelines concerning external dependencies, but these would apply equally to external dependencies delivered through a package manager.
I've done this myself more than once :)
**Company:** [Argo AI](https://argo.ai/careers) **Type:** Full time **Description:** Self-driving cars have the potential to improve road safety, reduce congestion and offer mobility to all. If you believe in this movement and share in our vision for how self-driving cars will change the world, join our team! **Location:** Bay Area, Pittsburgh, Michigan **Remote:** No **Visa Sponsorship:** Yes **Technologies:** Required: C++11, C++14, or the C++17 draft? Optional: Python, Java, Javascript **Contact:** PM or Apply on careers page
Not a fan. Personally not a fan of CMake at all, but the largest reason is that I think it's bad move is because it's a prerequisite that needs to be installed. The old build system might have had its problems, but at least you could easily invoke it from the command line and was self contained in the distribution.
Can confirm this was our #1 pain point to using Boost in an environment with non-standard compilers etc. I hope a good setup based on modern CMake is chosen.
wg21.link is even more awesome than people think. * http://wg21.link/P0001 - opens the latest version of P001 (P0001R1) * http://wg21.link/P0001R0 - opens a specific version of P001 (P0001R0) * http://wg21.link/LWG2876 - opens a LWG issue * http://wg21.link/LEWG2876 - opens a LEWG issue * http://wg21.link/CWG551 - opens a Core issue Note that some of these links may go to password-protected sites. Thanks to Maurice for setting this up and keeping it running.
Most 1-man projects will be limited by: *The time it takes you to figure out the logic and write the code to execute it *The time it takes you to package and possibly promote it *The time it takes you to design the graphics (unless you're really into graphics design, or don't have a gui) C++ is not a good candidate for any of those. C++ is good for software that you need to optimize, it's good for software that needs to be both fast and real time, it's good for large teams that may have different coding styles, it's good for projects that may wish to heavily change the coding paradigm they are using without changing the language, it's good for projects that need to be "kept alive" for dozens of years... it's really not a one-man language, that's not where it shines usually. That's not say you can't do personal projects in C++, personally I've used C++ with quality results and expediency for websites and small apps, but it's not the language in which one would usually code such things. 
Coooool!
...am I the only person who likes Boost.Build better? As weird as it could be at times, the flag switches and support for building different configs at once seemed to suit Boost quite well.
&gt; I think most operations do not have to depend on encoding, and adding encoding support might slow them down Then only use the hypothetical `std::utf8_string` when you need to pay for this performance. :) It would be nice to have this all rolled up into a `std::string`-compatible type.
great! I hope CMake gains some useful contributions from boost in the meantime
&gt; What real world usage does this function have? Oozing maximum irritation during stupid coding interviews.
&gt; can be and often is used to store utf-8 text Yeah but it's a real pain. Want to access character 7? I have to write a unicode parser to count the characters. Now I want to replace character 9 with another character of potentially different length? Urgh.
&gt; but at least you could easily invoke it from the command line and was self contained in the distribution. I really prefer installing CMake (and honestly I don't see a system where this is a pain... On windows it's supported natively by visual studio nowadays, and on linux / macOS it's just an apt / brew install away) than having to learn another set of commands / build options, for a *single library*. Also, nowadays CMake integrates with IDEs so you can see targets, etc... automatically. You don't have this with bjam.
Yikes, a decision like that makes communities lose people (as just happened with Rene). I still think http://boost.2283326.n4.nabble.com/Generating-CMake-package-files-using-Boost-Build-td4681626.html is the better approach to providing CMake integration to Boost users.
We're on the opposite end, not a single library which uses CMake and so will have to install it just for boost. Totally trust that it's easy to install, but at least for us it's just another tool which needs to be maintained &amp; kept up to date.
Without having enough knowledge on this issue to form an opinion - isn't that true for any major decision? No matter what, some people will always be disappinted.
I hope they won't be supporting CMake 2.8 or other ancient versions.
No you are not. In our team we despise CMake for good reason -probably because we (and our customers) are 100% on Windows and VisualStudio where CMake doesn't offer us anything but lousy, unmaintainable project files that we can't use anyway. So, CMake is forbidden as a dependency.
The minimum currently proposed is cmake 3.5 I believe.
Recent Visual Studios open CMake projects natively now.
I know. That doesn't help us in any shape or form.
For integration only, the approach you mention is perfectly suitable and maybe even superior. But I have to say that as a user of C++ libraries it is has long been very annoying to have to install a build system *specifically* for Boost to be able to install Boost. I have to have CMake already, so if it can be workable for building Boost as well as making Boost libraries available to other CMake-using projects, that would be preferable IMHO.
I just tried to Open Folder on my local Beast repository and it was a poor experience.
&gt;http://boost.2283326.n4.nabble.com/Generating-CMake-package-files-using-Boost-Build-td4681626.html The system proposed in this old mailing list post has been implemented by Peter: https://lists.boost.org/Archives/boost/2017/07/236851.php 
Yes, you're right. My reaction is probably due to my bias for my own suggested approach Â¯\\_(ãƒ„)_/Â¯.
Totally agree, in our company we have to use only the header-only libraries because of that.
He doesn't "crack" anything. He runs a program as root, then changes its behavior so that it starts a shell. The shell starts as root because the program he was running was running as root. GDB (also running as root) can change the behavior of the program running as root, because gdb itself is running as root. If gdb wasn't running as root, it wouldn't be able to change that behavior. Another way to change the behavior of a program you wrote is to change the code to do what you want, and run that instead. Another way to have "/bin/zsh" run as root when you already have root privileges is to type "/bin/zsh" at the command prompt.
Choosing one build system or another is inevitably going to alienate some segment of users, as seen in the comments here. Why can't Boost support both? Boost minds are the smartest in the biz, surely there is enough bandwidth to satisfy everyone without a need to pick winners and losers.
That matches my experiences. I have absolutely no problems with libraries supporting CMake. I just can't take advantage of it. In our team a library is ready to be used in a product when is has a *generic* project file which is *totally* agnostic of the VisualStudio version where it is used with, does *not* set *any* compiler option, and is *independent* of its position in the directory tree. Which means: we can put such a library anywhere into our product tree and compile it with whatever VisualStudio Version, bitness or mode we like without any configuration step or changes to the project file. Compiler flags, library dependencies and such are resolved globally for any given product. Our only build dependency is MSBuild that comes with VisualStudio.
How is that approach incompatible with the steering committee's announcement?
/u/Daniela-E &gt;CMake doesn't offer us anything but lousy, unmaintainable project files that we can't use anyway. So, CMake is forbidden as a dependency. You should try generating Visual Studio project files with Beast and tell me what you think. I'm a native VS user and the default generated CMake .vcxproj files stink. I have added code to my CMakeLists.txt for organizing the files in the project, perhaps it could suit your needs. 
+1 for gflags. Easy to use and under active development (last commit 7 days ago). I just wrote about [setting up gflags and glog](https://v1ntage.io/2017/07/15/episode-002-an-opinionated-start-to-a-cpp-project/) in a new C++ project the other day.
Interesting - I will look into it! It probably requires the latest version of CMake, does it?
Have you taken a look at [Hunter](https://github.com/ruslo/hunter) for C++ package management? It's written in native CMake which is (potentially) already a dependency for many projects. I just finished [writing a post](http://v1ntage.io/2017/07/15/episode-002-an-opinionated-start-to-a-cpp-project/) on setting up Hunter in a new C++ project.
Beast has the minimum set to 3.5.2 but that's just for finding the Boost packages. The actual code I wrote should work with much older CMakes. Try generating the .vcxproj in Beast using a new CMake, see if it suits your needs and then if you like it I can highlight the parts that you need and the steps, and you can try that in your own CMakelists.txt using an older CMake.
How would you like it if, instead of "building" the necessary Boost libraries you instead drop a single .cpp file into your existing build scripts? For example, lets say that you need Boost.System. Instead of building the static or dynamic lib, you edit your build scripts (any build system will work) and add one source file: $BOOST_ROOT/libs/system/src/boost_system_unity.cpp This .cpp will provide everything that would be provided by the static or shared library. And since its part of your build scripts it will use the same compiler settings and preprocessor directives. You can also get individual control over the settings for just that file. If you rather have a static or shared library with the Boost parts, you can add an extra target to your build scripts to build the library, and include those *_unity.cpp files, and have full control over the options used to build the lib. For example you could turn optimizations and symbols ON for Boost libs even though your application target specifies a debug build. What do you think? 
Great, finally a non-esoteric build system! (With the added drama of non-adoption rage-quit)
Currently one of the pain points about C++ is the lack of batteries-included standard library features (ex: networking, encryption, command line flags), but thanks to C++14, 17 and soon 20 things are getting better! I just finished a codecast and blog post about [starting a new C++ project](http://v1ntage.io/2017/07/15/episode-002-an-opinionated-start-to-a-cpp-project/). It covers package management, command line flags and logging so maybe you can find some inspiration there.
&gt; Coroutines also fix this because you have somewhere for the string to live when you return the iterators into it Coroutines don't fix this any differently than ranges. A string splitter range is more than capable of providing a place for the string to live. It's more than possible to fix all the problems, but only doing so with a fragile and possibly confusing interface that isn't directly obvious when it does or does not take ownership.
TIL that wg21.link even provides [a chrome plugin that generates links](https://wg21.link/chrome).
Interesting, a quick look at the docs and I couldn't see how this is the case. How do I tell Buck where my dependencies are installed? And how do I setup the toolchain settings(ie compiler, flags, etc)?
I personally prefer meson but I can understand why they choose cmake. It is a de facto standard.
Nobody prevents you from still using Boost.Build. It's just that for most users, which are using CMake, it'll now be easier to integrate with Boost. It will also be easier for new contributors to submit to Boost because they will likely already know CMake. To be clear; this whole thing is not motivated by the hypothetical technical superiority of CMake over Boost.Build; it is motivated by the fact that CMake, despite all its quirks, has taken over.
Actually there is not a lot of experts in the domain of current Boost build system as evidenced by the Boost developers mailing list and this announcement. What you see here is the effect of the vocal minority. The ones who are happy with the decisions usually don't feel the pressure to express it publicly.
The requirement of installing CMake sounds really trivial. It's used by many more libraries than Boost.Build. You also didn't explain why you are not a fan of CMake.
The files are only unmaintainable if you made them this way. There are some clean CMake files out there. Just like there are some clean Makefiles and horrible ones too. The Visual Studio CMake integration isn't *great* yet, but in the latest version it's good enough for a lot of workflows, and they are improving it at a fast pace too.
Try VS2017.3 preview. It's improved a lot. But also you need to be not doing things in cmake which cause trouble for VS2017. VS2017 requires very strictly correct cmake, it is intolerant of sloppiness you normally get away with. (Whether that makes "real world" cmake unusable by VS2017 is a separate matter)
It's strange how on that linked site it's impossible to know in some emails who wrote the answer to the announcement.
Yes, the subgroup is coming back with a couple of related proposals for Albuquerque with more detailed treatments.
&gt; What do you think? CMake with the [cotire](https://github.com/sakra/cotire) library can be used to generate unity build files
I think this was the conclusion also in the article, wasn't it?
Please add Remote, Visa Sponsorship, and Contact sections as requested by the template.
 &gt; Interesting, a quick look at the docs and I couldn't see how this is the case. The Buck docs do not cover cells yet (I must remember to send a PR...) but, there is a description of how they work here: https://github.com/njlr/buck-cells-example &gt; How do I tell Buck where my dependencies are installed? In Buck, dependencies are other Buck targets. For example, if your library `a` depends on library `b`from the same `BUCK` file, you might add this to your definition of library `a` : deps = [ ':b' ] If you depend on a system library, then you need to pass the appropriate linker flags. This is pretty self-explanatory. For example: linker_flags = [ '-lncurses' ] For platform abstraction, Buck also offers `platform_linker_flags`. The full documentation is here: https://buckbuild.com/rule/cxx_library.html &gt; And how do I setup the toolchain settings (ie compiler, flags, etc)? Regarding compiler flags, you can set them on a per-target basis, using `compiler_flags`, `preprocessor_flags` and so on, or you can set them universally using a "build flavour". Build flavours is a good way to configure things like debug builds. There is an example of setting up build flavours here: https://github.com/njlr/buck-custom-flavours Hope that helps!
A bit off topic but I tried that out the other day, because I'd love to be able to open the folder directly instead of having to generate a solution. However, we happen to have sources linked in from outside of the main folder, and as I understand it it's not possible to have multiple 'root folders' at once (like in for example Sublime and supposedly soon in VS Code). Would you happen to have any ideas if that's possible? (I've got an app in one folder and an SDK in a parallel one and want to open both at once but not the common parent.)
Just curious. What are better or competitive build systems out thereï¼Ÿ are they better or competitive with respect to CMake just for c++, or also other major languages?
I was wondering when that would happen.
&gt; CMake doesn't offer us anything but lousy, unmaintainable project files that we can't use anyway. I didn't t get this. I use CMake to generate visual Studio project files and it works like a charm. I'd much rather write CMake files than maintain a repo with the project files. I also have to target Linux and CMake is what makes that possible. I can't imagine how I'd do it without CMake. I'd have to maintain VS project files and make files separately and try to make the builds consistent which sounds like a nightmare. 
How would that `initializer_list` be constructed?
Same.
Agreed. In fact, I've always been amazed that Boost.Build isn't much more popular outside of Boost!
Hi Erich! Notifying them is preferred, that's why I mentioned that it.
Even better: make it possible to provide a custom fallback strategy! (And offer the above mentioned ones as default implementations) C# does a similar thing, Java also in a way.
I prefer #run from jai.
Thanks for your explaination
This is (for me) wonderful news to hear. Integrating boost builds into a cross-platform build system which needed to work on a good number of platforms was a fragile pain in the neck. First we need to [patch it](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/master/packages/boost/patch.cmake) to correct broken assumptions which aren't configurable without hand-patching. Then we need to [configure it](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/master/packages/boost/configure.cmake) which involves doing in two different ways depending upon the platform, and also hand-patching the generated project config jamfile so it will use the correct platform toolset, using a poorly extensible and fragile [mapping](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/master/packages/boost/superbuild.cmake#L6) from the detected compiler to the name used by Boost. Finally, we get to [build it](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/master/packages/boost/build.cmake). This is awfully complex, since we have to hack around with the options to get it to do something sensible since it hardcodes the library names for libz and bzip2 on Windows, again assumptions which are not always correct if you don't build these libraries as part of the Boost build (and in a large project, why would you when other components also use them?). Some of these options are also broken if you use them as intended (if the bzip2 and zlib include dirs differ, the build breaks); what you see here is a combination which was empirically proven to work. The logic is again different depending upon the platform, and it additionally requires hacking to match the conventions used by other projects (it bungs the DLLs into the libdir rather than the bindir, when they need to be on the `PATH`). The above horror of script files was what I wrote to deal with Boost.Build. It's in CMake, but the equivalent would need writing for any other build system as well if you wanted to support the same set of platforms. It's fragile and overly complex. Being able to build with CMake directly will reduce that to a few lines, which will be the same for every platform; no special-casing Windows/Unix builds or hacking around patching stuff.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [rleigh-codelibre/ome-cmake-superbuild/.../**build.cmake** (master â†’ 131af3b)](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/131af3b46081026930cd5a135792d0211cb9b973/packages/boost/build.cmake) * [rleigh-codelibre/ome-cmake-superbuild/.../**patch.cmake** (master â†’ 131af3b)](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/131af3b46081026930cd5a135792d0211cb9b973/packages/boost/patch.cmake) * [rleigh-codelibre/ome-cmake-superbuild/.../**configure.cmake** (master â†’ 131af3b)](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/131af3b46081026930cd5a135792d0211cb9b973/packages/boost/configure.cmake) * [rleigh-codelibre/ome-cmake-superbuild/.../**superbuild.cmake#L6** (master â†’ 131af3b)](https://github.com/rleigh-codelibre/ome-cmake-superbuild/blob/131af3b46081026930cd5a135792d0211cb9b973/packages/boost/superbuild.cmake#L6) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dke2x99.)^.
We might soon get hazard pointers in the standard as hazptr. There will be "I can haz ptr" jokes. Hopefully _in_ the standard.
As someone who does most of their programming on Windows, I honestly wouldn't mind if everything just used the Unix conventions. They are much more flexible and powerful while also being less verbose. Since most of the tools I use on the command line support Unix like command switches, being forced to switch back for the few builtin Microsoft tools is a bit annoying. Also, properly handling utf16 on Windows is arguably more important than following the other "conventions". Most of the libraries I have seen assume ascii or utf8 which is a non-starter. Of course this means either using the w* versions of the entry points (intrusive), or detecting Windows and manually grabbing command line arguments internally (more portable).
Finally! I remember discussions about moving Boost to cmake a decade ago. But I guess better now than never. Hopefully this will also improve the componentization of Boost. It's a pain that you have to import the whole Boost into your project even if you only want to use just one or two libraries from it. As an example of this problem see how Hunter tries to deal with Boost: each sub-library downloads the whole Boost, then builds it, and then copies that single sub-library into your project (https://github.com/ruslo/hunter/tree/master/cmake/projects/Boost). This is not the way to go in the long term. Each Boost sub-library has to be buildable/functional on its own.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ruslo/hunter/.../**Boost** (master â†’ c5e399c)](https://github.com/ruslo/hunter/tree/c5e399c086b22313a47fcb5571849db78d92ee45/cmake/projects/Boost) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dke3amb.)^.
Provide an **immutable unicode aware string type**. Internally it should be based upon UTF-8; methods to *de*- and *encode* into ``std::string`` should be provided. On top of that there should be the possibility to define custom fallback strategies in error cases like ignoring chars or replacing them. Library designers should be encouraged to build their APIS upon this type.
Does it look like a macro system for C++? (in the LISP sense)
Meson looks like another niche build system like many that were created in recent years. It has it's own DSL which I think is a mistake. A modern build system should use a well known scripting language (should it be Ruby, Python, or even Lua). CMake is not a modern build system and it has its own crappy DSL, but it's a de facto standard now.
&gt; which sounds like a nightmare. don't worry, it is. everyone who does this for any non-trivial project ends up reinventing their own build system at some point, except then it's shit.
Boost is easy: one download, and then you run a few well-documented commands. Try, instead, building OpenSSL from scratch some time. Or Pango. Or anything else that requires its own very particular compiler, comes with a truckload of dependencies, and offers precious little guidance on how to glue the whole thing together. A package manager solves the whole thing: it downloads the sources and all dependencies, compiles it in the correct manner, and leaves the includes, libs, and DLLs in a place where you can easily access them. It reduces days of tinkering to a five minute job. There are numerous ways to add libraries to C++: as part of the standard, as part of Boost, as a separately downloaded library. I disagree that every last bit of possible functionality needs to be in the standard. Take something like an XML parser: does it need to be in the standard? No, absolutely not. But should a library that parses XML be easily available to C++ programmers? Yes, certainly! So there appears to be room for a middle ground, one that would be very well served by a package manager of some kind. While Boost could be considered already a kind of package manager, I think it serves a different role in the C++ ecosystem: as a staging area for future standardisation work. That means the reviews are tougher than for a 'regular' package manager, and it won't take in useful libraries written in C (of which there is no shortage in the C++ world). 
To be honest, I'm not a fan of general purposes languages in build-systems. The company I used to work for had developed its own package-manager/build-tool, and it worked from a set of description files (in xml) which was purely declarative, and scaled to a thousands of libraries (dependencies). All the build systems I've seen that try to be "programmable" (make, CMake, SCons, ...) are actually much more difficult to use, if only because only the very program that interprets them has any clue as to what they are doing. Do we really need "programmable" build systems?
Your proposal solves the problem for users, which can now use find_package. However, it still does not make it easier for folks to submit libraries to Boost, and that is an important point as well. I've been in a few situation, one very recently at the Standards Committee meeting, where people decided not to submit to Boost "because of the build system". I think this is sad for both Boost and C++ as a whole, and we should try to solve this. 
Before i switched to CMake i worked on a project that had developers using multiple versions of Visual Studio, which meant having separate solutions and project files for every version. Every change had to be manually mirrored in each version, as well as the makefiles. Since switching to CMake i've never looked back.
No matter what build system you go with, please, please, please do the following: 1. ship pkg-config files 2. no, seriously, ship them 3. not having pkg-config files makes life a living hell for anyone who has to support Boost so just ship them already 
Some time ago it could be called niche but not really any more. It is being used by, among others: - systemd - GStreamer multimedia framework - GTK - many Gnome apps - X.org and soon Wayland 
Why? How often do you want to allocate a string instead of moving or reading it? Copying a `string` or `vector` is really rare in the code I work on. We do it a lot more because someone forgot &amp;, which is really a sad way to lose performance.
+1 - besides, regarding the project files, you're not *supposed* to maintain them. CMake maintains them for you, by deleting the old ones and creating a whole new set when you change the CMakeLists.txt... (CMake's VS project files are a bit ugly, it's true, and the solutions can look a bit of a mess, with that ALL and ZERO_CHECK stuff, and sometimes zillions of projects. But I've never found this a problem myself; once I've got a VS project that I can do F7 and F5 and F12 and Ctrl+, with, it's all good as far as I'm concerned. But it takes all sorts.)
As with all things, it depends on your use case. I've had a great deal of mileage out of using gradle to build code. I have a rather complex project with code from both java and C++, along with a bunch of resource files. I use gradle for the setup logic so new developers just run one gradle task and they have a beautiful development environment. It's also awesome for building because I need to compile the C++ code into a DLL, move that to a specific folder, then compile the Java code. All these operations have to work on Windows and Ubuntu Linux. For that project, using a general-purpose language as the build system has been hugely beneficial
And that they will do proper thing and add includes/definitions/build options to targets instead of using `add_definitions()` and friends and having people consume libraries through obscure variables. Amazing how many projects still get this wrong today.
This [trac ticket](https://svn.boost.org/trac10/ticket/1094) is my report from a decade ago, with a proof of concept patch to generate pkg-config data directly from the autolink data in the headers. Why didn't it get completed? Because it needed knowledge of Boost.Build to complete, and I didn't have the expertise. No one was willing to help out who did have the expertise, and so it stalled. I asked on the lists several times. This is a key example of people not being able to contribute desperately needed features because of the use of a baroque build system. Part of my beef with Boost.Build is that it's poorly documented, and only a handful of people have the expertise to use it. This is where CMake shines; it has copious documentation, hundreds of examples abound, and there are lots of people you can talk to who understand it, including the upstream developers who make a great deal of effort to engage and are very receptive and efficient in dealing with contributions. That support makes all the difference. It's not judging Boost.Build on a technical basis, it's having a critical mass of developers and users behind it which makes it a viable and growing ecosystem. Anyone who has used CMake in their own projects will be able to contribute to Boost using CMake, and that number of people is orders of magnitude greater than the number of Boost.Build users. I'm sorry that this decision will upset some Boost.Build maintainers and users. But when I was evaluating which build system to adopt for a family of work projects five years back I looked at many build systems from Autotools and CMake to SCons, Boost.Build and several others. I ended up using CMake after testing most of them. Boost.Build simply wasn't a viable choice then, and it's even less viable now. The cost/benefit of a custom project-specific build system may have made sense right back at the beginning of the project, but it was already causing pain a decade or so back, and I feel it's now more of a hindrance to use and contribution than a help. Learning a new build system to the point of proficiency requires a substantial investment of time and effort; learning CMake took a couple of weeks; becoming an expert took a couple of years of continued use, but I've used it on dozens of projects including contributions to several well-known open source projects. I would not have got the same amount of value and reuse across projects with Boost.Build to make that investment worthwhile.
&gt; where CMake doesn't offer us anything but lousy, unmaintainable project file Why would you want to maintain them directly? They don't even work across different versions of msvc let alone platforms. Freeing the dev from this is a major advantage of CMake. Do people safe the makefiles or ninja scripts from CMake too? I don't think so, but I have never asked around.
There is good number of boost libraries that are sensibly compileable in-place. Had to dig in boost vault for some code though to have it working ootb (dunno how it looks now).
- [HTTP](https://github.com/vinniefalco/Beast) - [JSON](https://github.com/nlohmann/json) Enjoy.
&gt; and soon Wayland xD Im sorry, seeing 'Wayland' and 'soon' in one sentence just cracks me up.
Anyone submitting changes involving new or renamed files would have twice as much work to do. Keep things simple, go with the de facto standard unless there is a good reason to do otherwise. 
CMake needs a -nolegacy switch or something similar to make this easier to deal with
There has been some debate as to the correct approach. But they are all variations of cmake 3 best practices more or less.
I think it's fair to say that this family of projects all occupy their own "niche" of sorts. It's a fairly insular group of projects which primarily work upon a single kernel.
Naive complex arithmetic will lead to incorrect results in presence of nan values. For example, 1 / (inf + nan * i) will generate nan values instead of zero due to the nan propagating through the implementation of the division operation. I used to think these were only theoretical concerns, but after spending some time trying to implement certain complex-valued special functions in C++ I quickly changed my mind :) Some more background info here: https://locklessinc.com/articles/complex_multiplication/ Python woes relating to naive complex arithmetic I reported a few years ago: https://bugs.python.org/issue25453 
Actually for many years, for customer's projects, I have just included the boost source - the part I use for that project - in the whole project. This means copying source but it also protects me from having boost change while I'm not looking, boost disappearing, etc. etc. It also lets me safe/send a compile source package that the customer can build himself or I can build later. It's never been a problem. I wouldn't recommend this approach for some project which is going to be updated periodically though.
I understand english is probably not your native language, so I'm not trying to be mean. the past tense form of read is ... read, only it's pronounced like 'red'. Yeah, it's dumb and inconsistent, that's English for ya :)
Any news on the spaceship operator?
I'm using Wayland since Fedora 25 and it works surprisingly well.
I'm not sure what you mean by "kernel" here but if you mean an OS kernel then this is not the case for Meson. For example one of the main reasons GStreamer chose to use it was to get a better cross platform experience in general and Visual Studio support in particular.
Absolutely not, they get thrown away as soon as the build completes and I blow away the entire build tree (or the continuous integration job cleans up). It's ephemeral autogenerated logic which is generated on demand for the specific platform/compiler/build tool. Same deal for all platforms and generators, including Windows Project/Solution files; anyone who is retaining them is fundamentally misunderstanding their purpose.
Er, sure they can. Look at things like std::count_if() for examples.
I'm not talking about meson. I'm talking about many of these being essentially Linux-only. Individual components might be a bit more cross-platform, but as a whole set these projects are developed by largely the same clique of developers. It's a niche which is somewhat insular in nature and somewhat divorced from the outside world in its outlook and practices (trying to state this charitably).
Of course they can, e.g. std::sort(container.begin(), container.end(), [](const auto &amp;a, const auto &amp;b) { return b &lt; a }); The part you're missing is that the type of a lambda is unmentionable, so you must either use a template type parameter (as in the above example) or you must use something that a lambda is implicitly convertable to, such as a function pointer (non-capturing only) or `std::function`. But in general the template approach is usually the way to go, because it gives the most flexibility. `std::sort` takes any callable, which includes lambdas, function pointers, `std::bind` objects, whatever. Edit: I missed the part of your question about it requiring C++14 specifically. The above example uses a C++14 polymorphic lambda, but that's not germane to the question; you can replace the argument types with concrete ones like `const std::string &amp;` or whatever the container contains and it will be valid C++11. There's nothing in C++11 that makes it impossible for a template type to match a lambda type. 
They don't even work in different folders. All the include paths and stuff are absolute.
Thanks for the edit, I immediately noticed that :) And for confirmation. I haven't actually tried it yet but I was suspicious it wasn't true.
Keep in mind that one of the goals/strengths of CMake is it's cross-platform ability. By picking an existing scripting language (like Ruby or Python) you're limiting yourself to the platforms those languages are available, unless it can be self-hosted (like Lua). You also need to take into account the idiosyncrasies of the various platforms (Default file locations, user directories, that sort of thing). At some point, it probably does make more sense to define your own DSL, even if that raises the barrier to entry somewhat.
That works only on very simple case though. And the state is less obvious.
That looks useful but can someone please ELI5 how to install it? I'm not enough of a deep Chrome guru to understand the sketchy instructions there.
What case doesn't it work on?
Go to chrome://extensions/, check 'Developer mode', then click the 'Load unpacked extension...' button â€” pretty much exactly what it says... ;-]
Your terminology is slightly incorrect. `sort()` takes a *function object* which is called with parentheses. Function pointers, classes with overloaded function call operators (including lambdas), and (obscurely) classes with conversion operators to function pointers are all function objects. (Technically, functions aren't objects so they aren't function objects, but things like `reference_wrapper` don't actually care, so considering functions as function objects is good enough for everyone outside the Standard.) *Callable object* is a special term, meaning "function object or pointer to member", which can be called with the generalized `invoke()` protocol. Newer things in the STL accept arbitrary callable objects (e.g. `std::function` does).
Yay!!! Awesome!
This is pretty much my whole experience, but I understand that I can only see what is near me. Combine that with the millions of software devs I will never meet and I can only see a tiny sliver of what happens. Just because it doesn't make sense to me doesn't mean lots of people aren't doing it.
You never have to hand-edit any header or library paths in VS, or any flags? Without CMake you basically have to do this over and over again on every PC and each time something in your configuration changes. That's hell.
So is this really happening or will the war now begin, certain people will quit boost, and the whole process will take several years at best?
WebAssembly is the way to go. Cutting-edge and C++ish.
I'm thinking for example about a parser or a turn-by-turn action solver for a game. Of course you could write it with a labda or a callable type, but the code would be convoluted (that's the current state anyway) compared to using yielding, which is what I meant by "work" but it was ambiguous.
Could you clarify how it would solve your problem? Does CMake provide generators for your non-standard compilers?
thx!
&gt; Each Boost sub-library has to be buildable/functional on its own. This statement highlights lack of knowledge of the implementation of most Boost libraries. You think every Boost library should be independent of Boost.Config? Of Boost.Utility? Of Boost.Core? To my knowledge, only a couple of Boost libraries have no dependencies on other Boost libraries.
Seems like quite a pessimization for most normal cases. I can see how guys may see it weird that multiplication by infinity yields complex nan, but really, why would anyone expect complex numbers to behave like reals (plain floats)? I wonder, how much domain, does the algorithm recover. Also, i think C++ _technically_ does not require this (though i just couldn't find relevant text, didn't search all that heavily).
No serial. :(
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6o3v7u/are_lambda_expressions_as_parameters_supported_in/dkel798/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
hi ! - yes, (in part) this is similar to a larger internal repo i manage. In meta you have all ansible repos, which git clone, make, etc. and place things in src/third_party/* https://github.com/senior7515/smf/tree/master/meta I've been looking to switch to bazel so I can produce .pex files for python, and jars in java for the RPC front. Right now I have a step that code generates code (my own code generator) https://github.com/senior7515/smf/blob/master/src/rpc/smf_gen/cpp_generator.cc#L260 So this generates RPC stubs like gRPC, Cap'n'Proto, etc. I wanna create multi-language RPC but have been lazy until I find a good solution since the multi-language repo I maintain has a meta system that I build - mostly using ansible and calls 5 or 6 other systems, but complexities arise, for example, from passing consistent compiler flags to all transitive dependencies. Same thing is true for code-gen'ed - binary configurations (no manual parsing), etc. I've only tested bazel lightly, but I like the fact that it has a large community - for say things like scala and python
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [senior7515/smf/.../**cpp_generator.cc#L260** (master â†’ 3cea5f6)](https://github.com/senior7515/smf/blob/3cea5f6c872b8196ff5503aa08e0b97050f78cbf/src/rpc/smf_gen/cpp_generator.cc#L260) * [senior7515/smf/.../**meta** (master â†’ 3cea5f6)](https://github.com/senior7515/smf/tree/3cea5f6c872b8196ff5503aa08e0b97050f78cbf/meta) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dken3u5.)^.
Finally! Would be nice to use CMake to build the libraries as opposed to the existing Boost.Build system. CMake has hit the nail on the head when it comes to a portable building environment/application.
Generators are for systems like GNU make etc, not compilers. What CMake actually provides most is consistency. For all its faults, it does support a fairly decent system (exportable targets) for informing downstream packages what you have compiled, how to link to it, where to find headers, and the same about upstream dependencies. If all packages in the overall build use CMake, this is pretty seamless. As for my original point, it was easier to figure out how to use CMake with Clang (back when that was new) and MPI compiler wrappers.
Meson is, in my view, *the alternative* to cmake. It is very well thought, the dsl does not suck and I have used intensively both cmake and meson. The big advantage of cmake in teams is generators and wide availability. I have also used in the past: autotools, scons, waf, tup and plain make. I can say with confidence that meson *is the best one*. But this is a subjective opinion and I understand the cmake decision. I wasted and still keep wasting some time fighting the cmake dsl. For me both cmake and meson work well with these differences: cmake can generate better projects for IDEs but meson has been working on that for some time. Wide availability. On the other side, meson DSL is easier, I never fought it. I know python so this might be an advantage. Also, Meson gives you more default targets and autodetections for coverage, sanitizers, unity builds and precompiled headers, which are useful and some of them from difficult to impossible in cmake.
I really think that metaclasses is the right approach. I think it is one of the best I have seen and a very good idea because it can solve so many real problems on the library side, which is better than adding to the language directly, can deprecate way more easily. Also, metaclasses make changes scoped and bounded per class, it is not a kind of global that affects or hijacks existing code in any way magically through reflection. I think this is a good property for teaching and keeping it sane. By this I do not mean we should add all of these into the metaclasses, but it is potentially useful for: - flag_enum - runtime_concepts - better variants - interfaces - enforced regular types - strong typedefs - comparisons by default - your own framework entity, such as QObject, or GameObject, etc. without writing lots of documentation on how to use it. And the list goes on. By the way, would metaclasses support inheritance? One more question if I may: the reflection syntax is going to be in any way like the one in the paper? I think it is easier to iterate with regular syntax than what hana does. I saw also a paper on adding constexpr containers through a constexpr_allocator. I also think it is a good idea and would make standard containers constexpr. Would need some tweaks though. I wonder what the alternatives would be with similar capabilities. 
Out of curiosity is Jon Kalb a Boost library author? Is he a Boost library contributor? But he's the chair of the Boost Steering Committee. What does that mean? What does the Boost Steering Committee do? It looks like they manage Boost's funds. So Jon Kalb is the chair of the committee that handles the finances of Boost. Great. He doesn't need commit access. So why is he (and the steering committee) instituting a CMake-only, no-Boostbuild policy, when some of Boost's user base cannot use CMake? Even when some of those user base is other Boost developers. Boost developers who actually write C++ code in Boost, not just manage Boost money. How does one get elected chair of the Boost Steering Committee? Do Boost authors, contributors, or even Boost users get a vote? Why did they vote to elect Jon Kalb to steer the committee? 
Rats live on no evil staR
This data is super useful for me.
Ebcidic coversion to ascii.
Basically the same points Vladimir (a principal Boost.Build contributor) made: https://lists.boost.org/Archives/boost/2017/07/237273.php 100% agreed on my part, but I'm not a Boost author.
Same post on the Boost.Users list: http://boost.2283326.n4.nabble.com/CMake-Announcement-from-Boost-Steering-Committee-tt4696935.html
Why do vs and later updates install tons of files to c:/? I have to increase c:/ space from time to time. Can't we just extract vs to any place?
The main advantage of cmake is that it generates build files for other build systems. With cmake I can build using visual studio, eclipse, make, ninja etc. etc. Another big plus is the ability to add sub-projects just by referencing their configuration. There's not really any direct competitors other than, say, qmake (which only makes makefiles afaik). A more powerful (but slow) alternative is scons. The configuration language is python, with some tools/variables to facilitate writing a portable configuration. Since it's in python, you're able to basically write your own configuration system (design your own configuration gui/cli, generate completely custom code etc. etc). However, scons runs the configuration script every single time you run it, and it does a lot more checking for "bulletproof builds" making it much slower the more preprocessing you do and the more source files you have.
Finally ;) Finally ;) Thank you guys, i was really waiting for this.
Thanks for this - it was a great read and, as you noted, a process not talked about as much as the results. Very insightful, and a bit exciting! Looking forward to C++20. Looks like this is the first post on your blog - interested to see what else you come up with, especially if you talk tech about CA/Total War. 
It appears your TLS cert has been revoked: https://www.ssllabs.com/ssltest/analyze.html?d=buckaroo.pm&amp;latest
I actually think it does make sense to have each sub-library be buildable on its own (with the obvious requirements that its dependencies are available). It's just that some modules would have extremely high usage (e.g. Boost.Core), as you rightfully point out. But conceptually, I'd like to be able to just pull e.g. Boost.Interprocess + dependencies and not worry about the rest if I don't need it.
Greetings. Welcome to the worlds of C++ and special math functions. I like the simple sum. This should be ok since the terms do not alternate in sign. Otherwise accuracy would be lost because of cancellation of significant figures. Consider adding the Hurwitz zeta function as your next calculator. In C++17 we added the riemann_zeta as well as several Bessel functions etc. I implemented these for the gnu c++ standard library along with others. I have a chaotic collection of special functions at https://github.com/emsr/tr29124_test. Keep learning. Look at http://dlmf.nist.gov for math. 
Ah, got it. So you are not asking that each library have no dependencies.
If you add a tolerance as an argument you could test the size of the term relative to the tolerance and exit the loop early - especially for large argument. This would speed things up. Near the pole at argument 1 it will still need MANY terms.
&gt; cotire I assert again, I understand the cmake decision, but, precompiled headers and unity builds in meson is a do nothing. It is provided out of the box. :) I just prefer it maybe. 
I think meson is going to occupy little by little the space from autotools. I think people from unix world will tend to prefer a bit meson and the rest will use cmake, especially windows users. Microsoft already started to support cmake directly. I still hope for more meson adoption, I just think it is better. But the market share is difficult to beat.
Keep in mind that Wolfram Alpha must serve many users simultaneously so the accuracy may at times be compromised in favour of better server performance. Nonetheless, congratulations on your hard work! Always strive to improve. Whether you choose to learn through the formal education system or through self-study, take the time to hone your study skills as they will be absolutely essential for success in fields that challenge you.
A surprising (to me) use case was simple: reversing a user-input string. You can't just do a simple `std::reverse` on the UTF-8 bytes to make this work. Even a Unicode string where all characters are in the BMP and stored using 16-bit characters can't necessarily be reversed using `std::reverse`. To make this work you have to be able to parse out the grapheme clusters, reverse those, then reassemble a string in the desired encoding. It's hard enough to do even simple things like this in languages (like Perl) that provide advanced Unicode support, but C++ gives a programmer little help here. Instead you have to fallback to things like ICU.
*sigh* That should be "comprises", not "is comprised of".
Well, the fact that Unicode itself is still constantly in development is sort of an issue there. UTF-8 can be nailed down, but what happens when going from Unicode 8.0 to 9.0 causes a program to change its behavior? There are (as always) solutions, but that adds even more complexity to a stdlib solution.
Does this happen with VS 2017? Some things, like the Windows SDK, are hard to install where you want them but most of VS 2017 can be put anywhere. 
Boost should have done this 10 years ago but there has been core members whose only contribution has been blocking progress to cmake. That their response to this is drama queen farewell announcements is very telling.
I would have hated it, because it means static linking in of (part of) boost into every executable I ship. I have 40-50 of executables, that blows them up in total size and they run slower because the OS needs to load and keep in memory that code 40-50 times over.
We finally killed CMake at work and I'm so happy about it. We moved to FastBuild, which has a little more maintenance burden for the build configuration but overall it's easier to maintain now that we've paid the setup costs and it compiles. so. fast.
well good luck. you'll certainly need it. i'll just go shepherding some cats, i wager i'll have more success. 
Top-down configuration is certainly not a best practice. It might work for application that would s not consumed by anything else but when libraries use that it just creates so many problems when consuming library. It's great when we can just slap source tree in our own code tree, add it using `add_subdirectory()`, link some library targets and be done. More often than not this isn't even possible for absolutely no good reason other than incompetence with CMake.
I'd go for a separate type, something like `std::text`, to enable a clean break for better (more-opaque) Unicode-supporting strings.
from my experience working on biicode, i.e. a dependency management tool that required libraries to be adapted to it, adapting libraries to work with your tool is not the right way. You will do a lot of porting work (with library adoption requiring that work, which raises the adoption bar a lot), find a lot of corner cases that will make you nuts (Booooooooost https://github.com/biicode/boost) and, definitely, a lot of potential users asking why they have to change their lib in order to support it in a platform they don't work with and don't care about.
You seem to be contradicting yourself. The standard specifies the interface. Standardizing the interface makes it fairly trivial to substitute one implementation for another. That standardized interface lets you use Boost's regex library instead of libstdc++'s without rewriting your code.
[Boost String Algorithms Library](http://www.boost.org/doc/libs/1_64_0/doc/html/string_algo.html) contains most of needed functions, but the library is too generic and sometimes it gets really annoying to get a function to work properly.
Yes, it's not my native language but I knew that read stuff. It somehow made into the text. I am going to fix the mistake, thanks ;)
Boost is commonly known as dependency hell, the ask is... just not that.
It's great to have those special math functions in the standard. So, is [n3060](http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2010/n3060.pdf) in c++17 already? Can we have [SphericalHarmonics](http://mathworld.wolfram.com/SphericalHarmonic.html) from it?
I'm using CMake to generate a compile_commands.json file that libClang can use as input to preprocess the codebase i'm working with. I don't know of any other build system that can do that.
What do you mean by some users not being able to use CMake?
So there are only 3 C++ compilers?
I've had to do this before as a workaround for a Linux kernel header that uses "new" as a variable name. It worked fine as a hack, at least.
Granted it was about 5 years ago since I had a look at it, but there were numerous problems at that point. Didn't handle pre-compiled headers, absolute path names in output ( usually not a problem, but just stupid ) comes to mind. I've also tried some libraries which uses CMake and it never seemed to "just work", auto magic for detecting stuff would fail etc. Also the CMake language was some sort of declarative type of frankenstein language which I found to be incredibly inelegant. In fact I think it's madness to invent your own language for a build system. We went with Premake which I found to be superior in every regard for us and been very happy with it since.
We didn't even have wording back then.
What's wrong with stringstream? I use it quite often when I need to construct strings. 
I'm going to let this thread stay, even though it is borderline. /u/Sampajama121 - keep up the good work and apply to [the C++Now student volunteer program](http://cppnow.org/student_volunteer_program/) when you are a bit older (junior in high school, at least).
actually cmake can be used as a sort of platform abstraction layer. They can write a simple script as before which downloads a dedicated cmake installation for boost if necessary (or even compiles it locally). but usually a cmake installation will exist on the target system. 
There are dozens of us! Dozens!
Is always interesting to hear new arguments on why signed are better and it seems there is one more argument every year, like some people are trying hard to demonstrate that they are right. First argument is pretty strange: if you need the difference between sizes you will need and absolute value anyway, if you need to know if one fits in another you compare sizes, not subtract them. Mixing signed and unsigned lead to problems, but if e.g. you need to perform binary operations signed numbers are undesirable for the task and you'll need to switch between them anyway. Wrapping behavior for unsigned numbers is common and desirable in embedded software. I looked in [following](http://www.agner.org/optimize/instruction_tables.pdf) document and I don't see how arithmetic operations on signed numbers are faster that on unsigned (from page 160 unsigned operations are as fast with some exceptions). Another common argument is that unsigned indexes lead to infinite loops, but not taking in count that in both cases loop maximum range was exceeded, only that with signed we can happily hide the bug for a while but an infinite loop will be hard to miss (but of course someone might think of this as error handling, based on signed overflow).
Interesting proposal (I hope it makes it into the standard). Can anyone tell me what `json_type::discarded` is for? The documentation doesn't specify.
&gt; It has it's own DSL which I think is a mistake. A modern build system should use a well known scripting language A modern build system should provide reproducible and parallel builds among other things. If there is a restricted DSL then build system can see what is dependent on what and will also prevent developer from introducing non-determinism. If a general purpose language is used then you have no guaranties about what build scrips are doing, what files are they reading, writing, are they generating random numbers, etc. Authors of build scripts will need to provide this information to build system manually which is error-prone.
Everything from llvm::StringRef: http://llvm.org/doxygen/classllvm_1_1StringRef.html
Did you know that most of Boost is header only anyway, so you end up doing that? Most of the compiled parts of Boost are actually quite small. If you can run 50 binaries but have then an issue with a few kB of space, maybe you're just using the wrong tools in your domain (limited embedded systems).
Organizing files in a project from CMake, or projects in your solution dates from a long long time ago. But you probably wouldn't want to use an older version of CMake as it doesn't detect newer versions of Visual Studio automatically (in addition to all the bugfixes that got solved).
Hm, why would you expect 1 / (inf + NaNÂ·i) to be zero? For all we know, NaN could be "inf - 1" and we should get -1/i or somesuch. Put another way, why would one expect a correct result if real or imaginary part are NaNs? I get that itâ€™s defined that way in the standard, but why?
I would have made msvc look very good in comparison, which is not an expected result.
That is not a feature of CMake, but of Ninja. Any build system that produces Ninja files can generate `compile_commands.json`. For example [Meson](http://mesonbuild.com) does it out of the box.
&gt; You never have to hand-edit any header or library paths in VS, or any flags? Of course not, that's the point. You put the flags in the cmakelists and cmake puts it in the .sln. If you have to change a VS solution, you're doing it wrong.
It always forces a copy since there's no rvalue overload for the `str()` member function. Or maybe they meant `&lt;strstream&gt;` rather than `&lt;sstream&gt;`...
Then what is cracking?
Hi Manu, thanks for comment. I'm not too familiar with BiiCode, but I thought that it required CMake? Writing a Buck file is incredibly easy for most projects, and it's just a file that can live alongside other build systems. For example, here is the Buck file for boost/graph: https://raw.githubusercontent.com/njlr/graph/develop/BUCK Buckaroo can inject Buck files into a repository, so it is not required that Boost incorporates them, although it would be fantastic if they did! In the future we are looking at making Buck files a generated target of CMake, in a similar way to Visual Studio projects. This would automated packaging for many projects. 
I think, headers only stuff matters little how much of Boost is header only, when stuff that isn't is measured in MBs. Was just looking at the sizes of boost 1.64 dlls built against the "universal" CRT of MS, total size is close to 8MB, and some big ones are e.g. `regex` (1MB), `log` (760KB), `program_options` (475KB). Statically linking in those, X times over, does not look like a good proposition to me (yes, I understand that not all will be linked in and that it is not X times 1MB e.g. for that `regex`, but still). Finally, note that my issue is both space and performance, which is adversely impacted by the "massive" usage of static libs. I think, a good exercise in how bad that can get is to try a system where everybody statically links to e.g. a C runtime :-).
Does it mean that eventually we'll have two different starts_with? And they would conflict if we'll have universal call syntax?
But your example does not show the complex, and common, use cases. Like having different compilation options, sources, deps, etc depending on the compiler, arch, platform, etc. The equivalent cmakelists to what you show there is a couple of lines too. My point is that if you work at the buildsystem level, you have to change or replicate all that compilation toggling logic. In your case would be worse since you're replicating the buildsystem. And that is exactly what Conan tries to avoid working as an external tool that just uses the existing buildsystem scripts.
I can't speak too much for Bazel, but Buck supports your use-case in the following ways: 1. Language libraries are supported via their various targets: `cxx_library`, `java_library`, `python_library` and so on. These language-aware targets allows Buck to do clever things, like only fully recompile Java libraries when an external interface changes. 2. Code generation is supported using `genrule`. A `genrule` is like a rule in `make`, but as part of Buck it benefits from caching and so on. You can do pretty much anything with a `genrule`. For example, here we use one to create a Debian package: https://github.com/LoopPerfect/buckaroo/blob/master/BUCK#L231 3. Global compiler flags can be controlled using "build flavours". You can define your own flavours easily. For example, here are flavours for debug and release: https://github.com/njlr/buck-custom-flavours/blob/master/.buckconfig Finally, here is a good video overview of Buck from the Buck team: https://www.youtube.com/watch?v=uvNI_E0ZgZU 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [njlr/buck-custom-flavours/.../**.buckconfig** (master â†’ e99d178)](https://github.com/njlr/buck-custom-flavours/blob/e99d178f6af17f1c8b6eab92cc8a8a18d79661e5/.buckconfig) * [LoopPerfect/buckaroo/.../**BUCK#L231** (master â†’ 50d9f0a)](https://github.com/LoopPerfect/buckaroo/blob/50d9f0adf031d77bc349dd886af78f7a8ce877a8/BUCK#L231) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkf5m8s.)^.
I'm somewhat partial to Bazel. It's *fantastic* if you control the whole build environment, though quite painful if you don't.
Mistake - i did mean &lt;strstream&gt;
About fucking time. And a proper github repo please.
I've been fighting to build boost in Cygwin for days to match my Ubuntu config. Ive given up supporting windows for now.
I use git submodule for other libraries that are sane... Its awesome.
Yup thanks. In fact bazel uses bucks Java code too. I was just saying that I like your approach and that it would be nice if it integrated nicely with other builds. Sounds like it is just buck. Thanks for the links. !! 
I think implicit global state is a very bad idea.
&gt; Why would you want to maintain them directly? Right - I don't want to. &gt; They don't even work across different versions of msvc Wrong, they do if you know how to. As I said before: our VisualStudio project files (and solution files, too) are *agnostic* of VisualStudio versions. They work with any IDE starting from VS2010 up to VS2017 *without any changes*. That's much better than anything CMake can offer to our team. Sure, cross-platform development requires tools that support that, but that's not our business. CMake has no clue how to create such VS-agnostic project files. May be one can teach it how to stamp out such files, but learning enough of CMake to teach it such skills probably takes orders of magnitude of time compared to what we do now if we decide to adopt another 3rd-party library.
So would I!
With regard to contains, absolutely!
Dunno about that one though, as it would involve invoking the compiler at runtime IMO.
&gt; You never have to hand-edit any header or library paths in VS, or any flags? Correct. We don't do that because we don't have to. And we don't want to. Project or solution files change because of code development, maintenance, refactoring, whatever, but *never* because of compiler flags, include paths or library paths.
The steering committee is made up of Boost developers, users, and community leaders. There's been a lot of FUD spewing on this point. They're tasked with breaking stalemates on the mailing list when the community needs it to move forward. This is exactly such a case and the last time this happened was when a decision to move to git was made. The committee was formed by the original creators of Boost and rotates in new members. The new members are voted in by the committee based on their willingness and ability to make decisions on behalf of Boost's original (and continued) mission: to improve the way people code. Also the committee is not preventing library authors from supporting Boostbuild going forward. The intent is to set the general direction of incorporating CMake support for both users and developers.
It's not 100% clear what the history behind Buck, Bazel, Pants and Blaze are, but here is my understanding: - Google built Blaze as a tool to accelerate their builds - Facebook wanted a similar tool, but Blaze is closed-source - Facebook hires some ex-Google engineers to build Buck - Buck is open-sourced - Google, wanting an open-source competitor to Buck, builds and releases Bazel - Bazel appears to use some source-code from Buck: https://www.facebook.com/buckbuildtool/posts/785918108144567 - Blaze continues to be closed-source because it has some proprietary Google components It would be great if someone from the Buck, Blaze and Bazel teams could confirm this! 
What "stalemate" do you think has been broken? The real stalemate is nobody caring what the steering committee thinks, or library authors refusing to implement anything, operate any part of the release process, and altogether not contributing to Boost any more. If it reaches that stalemate, your actions on the steering committee have hurt Boost more than anything in your "vision" wants to achieve. Basically, if Boost developers and contributors are against you (the collective "you") who have never contributed any meaningful code to Boost, you're only hurting Boost not helping it. The committee is either going to put in effort to achieve these things themselves (e.g. you David making actual pull requests and writing and implementing and building CMake support, taking over libraries and tools that authors have abandoned because of your decision) or they aren't - and then the committee is useless, and, forgive me for saying this, but so are you. Good intentions are useless. Actual work (the kind Rene and Vladimir have been doing for years) is all that matters.
It's not state. It doesn't change while the programming is running. 
Buck is conceptually quite different to other build systems. The Buck files declare the target graph, but global settings can be injected using "build flavours". Buck handles compiler flag configuration in two ways: 1. You can set compiler flags on a per-target basis using `compiler_flags` and `platform_compiler_flags`. You also have access to a turing-complete language (Python) for computing the flags if required. Docs: https://buckbuild.com/rule/cxx_library.html#compiler_flags 2. Global settings can be injected using build flavours. Build flavours allow you to pass settings to all targets in the graph, so for example you could pass `-g` to make everything build with debug symbols. In other words, Buck is designed so that you can inject global compilation settings into a project without modifying the build files of that project or its dependencies. You're right to raise this issue, because C++ libraries traditionally have complex build configuration. In truth though, using compiler flags is a bit of an anti-pattern. A better approach is to replace preprocessor magic with modern language features such as templates. If everything is built from source this becomes viable! (And interestingly, templates can also be faster https://hackernoon.com/comparing-the-compilation-times-of-templates-and-macros-d0a1b7264a17)
&gt; cout &lt;&lt; "\nIncorrect operation! Try again: "; &gt; cin &gt;&gt; num1 &gt;&gt; oper &gt;&gt; num2; &gt; solve(num1, num2, oper); There needs to be a `return` before `solve`, otherwise [the behavior is undefined](https://en.wikipedia.org/wiki/Undefined_behavior).
&gt; They work with any IDE starting from VS2010 up to VS2017 without any changes so you don't get the "this solution is outdated, wanna update it ?" when opening it in VS2017 ? 
Very good initiative! I actually tried to do it like you did, but i hit a few bumps along the way: * You need to tell clang the difference between system include paths and user include paths. This is especially important for the way includes are noted i.e. &lt;include.h&gt; or "include.h", when using tools like IWYU. * Options like forced include files are not in your approach (at my work we use them for precompiled headers and some macro's) * You can have property sheets which create the possibility to inherit properties, which makes determining the final includes even more complicated. It would be great if from an extension you could simply get the command line output that visual studio would generate.. My final solution is actually parsing the cl.command.1.tlog files visual studio outputs when building to ensure i get the same behavior using clang. Visual studio does not pass system include paths to the compiler, to mimic this behavior with clang i simply run vcvarsall.bat in the same cmd before calling clang or iwyu. The disadvantage of my approach is that the user should have build before using the extension.
split
No, I don't. That would ruin my day ...
&gt; I think a std::utf8_string would be much more usefull No, that would just be moving the problem elsewhere, not solving it. C++ needs a `std::text` data type that provides proper encoding-aware access to *text data*, [in the spirit of Martinhoâ€™s ogonek library](https://github.com/libogonek/ogonek). Such a type might well default to UTF8 code points under the hood but thatâ€™s a mostly irrelevant implementation detail: working on text should work regardless of physical encoding.
&gt; Want to access character 7? How often is that a legitimate use-case with real Unicode though? Proper Unicode text libraries (cf. Ogonek) donâ€™t even provide random access to individual characters, nor should they. Unicode text needs to be iterated over and transformed in well-defined ways, but random access isnâ€™t an easy or useful requirement.
Then implement your stings with copy-on-write semantics. It has been done before in C++ and other languages do it, although this has its own problems (which is why C++ implementations no longer do it). But providing a string type without copy constructor would be complete clusterfuck.
Right, but then you should be using a byte/whatever vector, not a *text* type. `std::string` is in this awkward in-between state between byte storage and (incompetent) text storage.
In my experience it was very easy to set up. Just `include(cotire)` at the top of `CMakeLists.txt` and `cotire(MyExecutable)` at the end. That's about it. Maybe just specifying preexisting `stdafx.h` header as a prefix header.
Yes, n3060 has been moved into C++17. So assoc_laguerre, assoc_legendre, beta, comp_ellint_[123], cyl_bessel_[ijk], cyl_neumann, ellint_[123], expint, hermite, laguerre, legendre, riemann_zeta, sph_bessel, sph_legendre, sph_neumann are in. I think gcc may be the only library that has them at this time. Boost has them though. The only things left out from the TR1 special functions were hyperg and conf_hyperg - the hypergeometric functions. This was because of implementability concerns.
Well, I installed today's CMake 3.9, set-up a playground for a test using Beast Version 83, told CMake where to find a prebuilt Boost 1.64.0 by defining BOOST_ROOT and BOOST_LIBRARYDIR, selected VS2017 using default settings, and pressed the Configure button. CMake complains about missing static boost libraries even though they are all sitting right there where I was sticking it's nose to. The instructions in the output window tell me to set BOOST_LIBRARYDIR - been there, done that.
&gt; Interprocedural optimization (IPO) is now supported for GNU and Clang compilers using link time optimization (LTO) flags That's very nice to have 
That gtest integration is a great new feature. Does anyone have a lot of experience using CPack for RPMs? I noticed that there are some upgrades in this version. Normally, when I make RPMs, it's through a CI system where it can be difficult to ensure that my project's dependencies are present on the machine. I could use Docker to solve this, but I tend to just have CMake configure a .spec file and use mock instead. It would be nice to have it all in CMake, but that requires all your dependencies already be installed on the build machine.
Great to know that we are not the only ones trying to figure out these things! We should be able to get all the command line information from the Visual Studio API, but as you already noticed it's quite tricky ;)
&gt; First argument is pretty strange: if you need the difference between sizes It's not _just_ difference between sizes. That's just an example. Just because you have values that will never be negative doesn't mean that no one will ever need to do regular arithmetic with them, including subtraction that can result in a negative number being the proper result. So the fact that those values themselves aren't negative isn't a good reason to make them unsigned. &gt; you will need and absolute value anyway, if you need to know if one fits in another you compare sizes, not subtract them. Who said calculating the difference was for seeing if one fits in the other? Say I've got a bunch of sizes and I want to delta encode them, meaning I want actual negative values. Or whatever. It's only an example. &gt; Mixing signed and unsigned lead to problems, but if e.g. you need to perform binary operations signed numbers are undesirable for the task and you'll need to switch between them anyway. &gt; Wrapping behavior for unsigned numbers is common and desirable in embedded software. Yes, there are a few reasonable justifications for using unsigned in some places. But like I said, in those cases keep them strictly limited to the areas in which they're justified and try to avoid exposing them to anything else. I'm not trying to argue that one should _never_ used unsigned, only that signed should be preferred, and that the valid range of values for a particular variable being non-negative is not sufficient justification to override that default. &gt; I looked in following document and I don't see how arithmetic operations on signed numbers are faster that on unsigned (from page 160 unsigned operations are as fast with some exceptions). The reason the people I listed recommended signed is because of langauge-level optimization opportunities, not machine level performance.
Try setting environment variables not cmake variables. I never use the gui
The open source C++ world seems to be converging rapidly on cmake as the default build system. Do you really think you can divert this momentum towards buck? Seems like a tall order.
At least with respect to the docker method the article is sloppy to the point one could confuse it with an advertisement. Docker doesn't require root to run, you are not limited only to the libs available through apt-get, docker has native windows support, disk-space is typically not a problem, as base images can be shared across projects and installing tooling a second time is a feature not a bug (libraries are not all you need for a project you also need compilers and its as important to keep track of compiler versions as lib versions).
Without CMake I can edit such things in a single property page, which is used by all projects.
Wow, you're completely wrong. CMake got that feature long before Ninja had it. 
Who rage quit?
I've had to implement it. As soon as you say "I need my UI to handle non-ASCII characters" then you are running into these problems. It isn't difficult really (especially if you can get away with code point counting instead of grapheme counting) just tedious. I don't have a wide sampling of when it's necessary.
I guess I should have said "exclusive to CMake" instead. But in context the point still stands: the original claim was that only CMake generates `compile_commands.json` but in fact every build system that exports to Ninja gets for free (you just have to tell Ninja to generate it).
Well I'm interested in some articles about unsigned numbers limiting optimization opportunities with some examples to better understand the implications.
Where do you get prebuilt boost? All I can see is source distributions.
If I recall, CMake became dominant following a similar pattern of adoption in the KDE community. I know I was originally skeptical about meson, and its use in these projects is absolutely what convinced me to try it. Maybe my mind will change, but thus far I am convinced it is simply a superior build system to CMake, at least for my needs.
Besides that i had a lot of problems getting my extension to work with different visual studio versions. I noticed you also created quite some wrappers around their api, which is quite annoying.
See 12.3 in the [CUDA C Best Practices document][1]. There are also some comments in part one of the article ["What Every C Programmer Should Know About Undefined Behavior"][3]. [1]: http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#axzz4nHnlZV2x [3]: http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html
i could be wrong about IWYU, but although it's a comment convention, using &lt;&gt; or "" does not denote the difference between user and system includes, at least as far as every single other static checker, compiler warning, etc, I've ever used. The difference comes from whether the include was found in a directory specified with -I, or -isystem. My experience is that &lt;&gt; vs "", other than convention, has zero difference beyond whether the directory containing the file is searched.
For now the extension just converts all the Visual Studio include directories to system include flags, so it does not matter if the source code specifies includes with &lt;&gt; or "". It works for both cases. 
After lots of staring at CMake traces and digging into FindBoost.cmake I found the reason for the failure: Beast's CMakeLists.txt insisted on Boost libs statically linking to msvc's runtime lib. Fixing that problem generates the project and solution files for **one** compiler version, **one** Boost version, and **one** bitness. E.g. the generated project file for the echo-op example (which has exactly one source file) is over 330 lines long, takes up 23k, and needs to be recreated with every change to configuration or dependencies. The hand-rolled equivalent supporting **all** compiler versions, **all** (fitting) Boost versions and both 32 and 64 bit is 1/10th the size, takes a couple of seconds to create, and needs never be touched afterwards. I'll stick with the latter ...
&gt; The calculator is actually more accurate than Wolfram alpha when you enter a number with decimals. [citation needed] I doubt Wolfram Alpha is giving incorrect results. Maybe you mean more precision, not more accuracy.
I maintain and create a Boost distribution for our team [here](https://github.com/DanielaE).
I don't understand why they don't use clang for their codemodel/autocomplete. I really like the design of this IDE but the bad codemodel and sometimes wrong code "improvements" make it unusable
Why start a response with "Wrong"? Its almost as if you don't want a meaningful conversation. All of the CMake scripts I have written in the past decade or so work with every version of msvc I have pointed them at. For one extra as long as you don't invoke and commands on the command line the scripts CMake scripts will also work by default with on Mac OS X, Linux and other windows compilers like MinGW (which produces faster executables in all my tests). &gt; CMake has no clue how to create such VS-agnostic project files. But why do you care? Just don't save the files, no other cmake output is intended to be saved. The makefiles, ninja scripts, code::blocks files and XCode projects I have emitted over the years never get saved This is all about single responsibility and not repeating yourself applied to build systems. If all you need is visual studio and know some black magic to share those configs across version, why did anyone even attempt to use CMake, its role in builds is to be the single store of build rules?
How do certain people microsoft to just work for them without the constant annoyances the rest of us have? It was so bad for me that I left microsoft land forever, I don't have windows outside a VM on any of my equipment and I fire up those VMs exactly as often as clients absolutely mandate.
&gt; As soon as you say "I need my UI to handle non-ASCII characters" then you are running into these problems. I assure you, there are better solutions than performing random access on specific characters. In fact, in virtually all cases all you need to do is *iterate over* the characters/graphemes/extended grapheme clusters to measure/display them. At any rate, naive random access will of course fail as soon as you are dealing with anything beyond a single codepoint (ðŸ‡ªðŸ‡º should display as a flag, not as two separate characters ðŸ‡ªâ€‹ðŸ‡º).
Hm? It doesn't change the fact that one implementation is better than the other?
It's been discussed before, either on blog comments or release threads here on /r/cpp. I think it boiled down to clang parsing being insufficient for the type of interactivity expected on incomplete code in an editor.
[this is what I mean](https://wandbox.org/permlink/jljmKU59aEOdcKL4)
I don't do GUI work and have no real stance on Qt but I thought there were some very strong opinions in this ep. 
We've used CPack to generate RPMs frequently. Why are you saying it requires your dependencies on the machine? Are you referring to shared library dependency detection? You can control that with [`CPACK_RPM_PACKAGE_AUTOREQ`](https://cmake.org/cmake/help/latest/module/CPackRPM.html#variable:CPACK_RPM_PACKAGE_AUTOREQ). We've had problems with DEB packages and shlibdeps before. BTW: I implemented docker for our builds, and it works like a charm. Highly recommended. Just two Ubuntu Xenial physical servers will build for as many different Linux platforms as we need.
[removed]
Meh, that's a sheer _ad_ for Rust. I'm interested in Rust, but this kind of no-information piece shouldn't be on /r/cpp. 
Yes, it probably does mean two different starts_with. And yes they would both be "in scope" with UFCS, but any version of UFCS needs to be good enough to "do the right thing" for cases like this, since tons of those cases already exist. ie vector.begin vs std::begin.
The same argument generally comes up again and again when discussing IDEs and compilers. There are two technical limitations: - Most compilers choke on bad input: diagnose and exit! However when writing code it's not suitable for compilation most of the time, and yet we expect things like *completion* to work. - Compilers are generally optimized for batch compilation: one file at a time! IDEs want low-latency, and therefore need to have a much more incremental process for dealing with "complete as you type". On top of that, there's also the issue that many IDEs have a somewhat generic model for all their actions used across languages; after all "go-to-definition", "find-usages", etc... are operations which require similar structures no matter the language. There's a cost in mapping a foreign data-model onto those structures, especially as the foreign data-model changes release after release of the foreign dependency. And of course, there's the cross-language issue. Clang is written in C++, but CLion is written in Java. There are C-bindings for Clang, but they are notoriously incomplete. And they would greatly complicate the debugging experience for the CLion developers because of the language barrier. And they would introduce potential segmentation faults (the infamous Internal Compiler Errors) inside CLion, which are even more likely in an IDE since most of the times it attempts to parse ill-formed code. In fine... to be usable a compiler have to be conceived with the ability to fuzzy-parse ill-formed inputs, and updating parses as you type, from the get go. And there'd still be a technical barrier.
Death to &lt;strike&gt;C++&lt;/strike&gt; Rust advocates! The language isn't bad, but the advocates should just drink their kool-aid and let the rest of us get on with life.
Well, yeah, that is a limitation of CMake. But the sources are well organized in the Solution Explorer (my original point).
It's better than it used to be, but by no means done. IIRC all external packages (cordova, android sdk, etc) still go to default places inside C: A full install will still take a lot more of space in C: than in your designed destination.
Rust community should clearly calm down with their "advertisement". Especially seeing how they slowly enter C++ conferences it seems more like giving them more incentives to think that we use C++ because of habit and we want see the light. Also if you look at comments in the article it's clear that most people are sick of this kind of articles as well.
*meh*. Weekly Rustvertisement. No relevant information in the article.
Can't say for sure. I would imagine though that if the NaN originates from indeterminacy in inf arithmetics, it makes sense to me that, as long as the other component is inf, the complex number is still considered an infinity as a whole.
I don't see anything useful for C++ developers in this article. It's just generic reiteration of "this feature of the language is unsafe, therefore it must die". I advocate for it being removed from this sub.
**Company:** [High Fidelity](https://highfidelity.com) **Type:** Full time **Description:** High Fidelity is hiring engineers with experience in C++ to work on Interface, the company's open-source VR platform. Interface is a client-server application that enables users to deploy their own virtual reality environments. The successful candidate will have experience working on complex interactive (user-centric) systems. We are looking for engineers excited to build their expertise in a specific area of our code and thrive as a key contributor to a large, ambitious open-source development project. The platform provides: high performance networking with low latency requirements using connectionless protocols and dynamic broadcast trees; 3D audio, including live mixing and room reflections; head, body, and hand tracking hardware API's; Physics engines; high-end 3D rendering, including complex character avatars with both dynamic and offline lighting. We are also continuing to develop the social tools and capabilities required for our user community to grow. We encourage our engineers to contribute actively to the design of the systems they work on to help build the best product possible. Our product management process is focused on prioritizing projects designed and scoped by the engineering teams that will implement those projects. We are an engineering-driven company with ambitions to build a world-altering application. **Location:** San Francisco, CA **Remote:** No **Visa Sponsorship:** No **Technologies:** C++, OpenGL, Github, Cmake, Qt **Contact:** Please visit our careers page https://highfidelity.com/jobs
Yeah, I say that the dependencies need to be there because the rest of my CMake has to run to get to CPack and the find_package calls fail. I should just use Docker for the builds. That's just one of those tasks that never gets priority.
And you're posting this here why?
If rust had come out 10 years ago it would be a very compelling alternative by now. But... it didn't. In recent years the C++ community has reinvigorated itself in to essentially a new language plus backwards compatibility with decades of software. I can also find C++ developers a lot easier than I can rust developers.
Really hope this makes it in.
Just FTR: Python is more portable than CMake. It runs on absolutely everything. The only thing that is more portable is Perl.
I do too. But seeing how much time concepts are taking since the initial proposal, I'm afraid we'll get metaclasses only in 10 years.
And that doesn't make defining the interface a mistake. ;-]
Well, first I got it as a main title in codeproject mailing list (which is telling how much this preaching is spreading around), second this subreddit and C++ community seems to be very friendly with Rust (even giving Rust platform on C++ conferences to promote Rust) when I don't see so much love on the Rust side and it's more like "we'll have our own language with modules and proper safety".
&gt; For C++ questions, answers, help, and advice see r/cpp_questions or StackOverflow.
&gt; And of course, there's the cross-language issue. Clang is written in C++, but CLion is written in Java. And they would greatly complicate the debugging experience for the CLion developers because of the language barrier. And they would introduce potential segmentation faults (the infamous Internal Compiler Errors) inside CLion, which are even more likely in an IDE since most of the times it attempts to parse ill-formed code. YouCompleteMe for vim is living proof that libclang is just enough to provide usable completion. And they don't have to do it in-process (YCM runs an out-of-process server as well). All in all, they are probably investing tremendous effort into shipping a C++ parser that sucks. If it worked well, no one would complain. 
ty
Clang is not perfect but it pretty much deals with invalid input. I've been using QtCreator (Built from trunk/master) with the clangcodemodel and it works really great. I literally cannot use CLion because I am already using lots of C++17 features which CLion does not support yet. Besides, last time I used CLion I've seen exception boxes popping up because it couldn't parse more complex c++ code.
[RenÃ© Rivera resigned](http://boost.2283326.n4.nabble.com/CMake-Announcement-from-Boost-Steering-Committee-tt4696934.html#a4696938)
CMake has policies, which could be used to implement this. Several (very) old features are marked as warnings or hard errors if they are used when a version is too new. Not sure if they plan on deprecating things like `add_definitions()` though. May want to take it up on their issue tracker.
I believe the `add_subdirectory(Boost)` case is also intended to be supported. (It's mentioned in the linked thread)
[Searching buck on /r/cpp](https://www.reddit.com/r/cpp/search?q=buck&amp;restrict_sr=on&amp;sort=relevance&amp;t=all) has a whopping 5 results, where two are at 0 upvotes, one is a question. Not having heard of Buck seems not too absurd. 
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
While this is valid code and working, it is too much burden on the library developer to think about. Expecting all functions in a class to overload `pair&lt;iterator, iterator&gt; operation() &amp;&amp; = delete` is absurd. Lifetime extension it is. Also, with that guideline you'd have to change _so_ much of the STL, like, even `std::basic_string::data`. Just reviewing that list of potentially dangerous functions would take a few years of WG21's lives.
I mean this is exactly what already happens for const and non-const. rvalue overloads already occur in some places like `std::get` on tuples, which is very much the same kind of thing as `operator[]` or `data`: a function that returns a reference to internal data. It's hard to even change overloads for existing functions of `vector` and `string` because it could break existing code. The rewards also are not as high, because it's rare to create a whole container and then only care about moving out a single element, because we tend to care about performance and that's really wasteful. But it would be fantastic if say: const auto&amp; x = *make_unique&lt;Foo&gt;(); Didn't compile, and it is quite simple to do so. Whenever I write classes, I try to minimize the number of methods that dole out internal references, but when I do have such methods I always consider rvalue overloads, the same way that you already consider const overloads.
I didn't say it isn't possible, but that it isn't feasible. In the end, you need to qualify all functions that return _some_ kind of reference into the object as `(const)&amp;` and need to forbid the `&amp;&amp;` overload. Most APIs have basic getters and setters. All getters need to be overloaded twice. Do _you_ want to do that for all your classes? `const&amp;` vs `&amp;` overloads are probably fewer. Also note that it would essentially double the number of overloads for pretty much every class. You don't want that. Reading the documentation alone would be a horror. You want some level of language abstraction to avoid getting references into partial objects. Having to handle that in _every_ class _manually_ is _not_ the solution.
Your post has been automatically removed because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/6obw9t/what_can_a_c_beginner_do_that_is_meaningful/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
That is very good advice to avoid getting into huge problems. If you are already drowning in one you have to do what you have to do. 
&gt; Most APIs have basic getters and setters. Uhm, most C++ APIs should not have getters and setters. This is not Java. Having a getter and a setter for a member is nearly equivalent to that member being public and is considered as an anti-pattern in C++. Like I said, you should not have that many methods that return internal references in the first place, outside of containers. When you do have those methods, in most cases the amount of boilerplate you need to write increases 50%: you go from writing 2 functions (const and non-const) to 3 (add the &amp;&amp; overload). Having to write the boilerplate is annoying but it's part of life in the same vein as const non-const. If you want the benefits of const correctness, I write the boilerplate. Similarly if I want the benefit of rvalue safety, I write a little more. Would be nice if a language feature removed that boilerplate. But that is the solution. Not lifetime extension. I have no idea what the mechanism would be for: const auto&amp; s = getVector().at(0); For example, to extend lifetime. We already have lifetime extension for sub-objects, but the elements of a vector are not formal sub-objects, just random locations on the heap, that vector is designed to own. You'd have to codify the notion of ownership into the language in a special way. E.g. make a magic special case and designate that `unique_ptr`'s pointed to memory constitutes a sub-object of `unique_ptr`, and then use `unique_ptr&lt;T[]&gt;` to implement `vector`.
&gt; How do certain people microsoft to just work for them without the constant annoyances the rest of us have? It's not a difficult concept â€” someone obviously took the time to learn MSBuild so they could write those project files. MSBuild documentation is as verbose as MSBuild itself, for better and worse, but it _is_ documented... It's not like they just got lucky and things magically worked better for them.
YCM is as good as it gets with current tools, but its completer is based on libclang, which API has some limitations for c++. I think some IDE's ([Eclipse CDT](http://www.eclipse.org/community/eclipse_newsletter/2017/april/article5.php)) are going to switch to clangd in the future, which exposes a LanguageProtocol API. When clangd integrates its own symbol indexer, would be amazing for vim.
I agree. Though my point was exclusively about how difficult it is to maintain a C++ standard library when there's so much to work on in it, hence the inferior performance.
&gt; people not using even the most basic OOP features such as inheritance OOP support is such a small corner of all the things we do with inheritance (mixins, policies, template implementations, type erasure, compile-time composition), it probably doesn't come up much.. I wonder if anyone will bring up a good modern example. &gt; Features like multiple inheritance feel outright scary and alien to me. Non-OOP MI is pretty straightforward, but even for OOP, what's so scary about `std::fstream`?
The problem is that when you say OO, what do you really mean? People often argue this without having the same definition. I think of OO as designing good objects. Good objects means excellent encapsulation, minimum necessary interface, and so on. This is very different from "Java style OO". First off, in Java you tend to have the easy possibility of shared references to objects, and getters + setters are far more common. So while lip service to data encapsulation may be paid, this clearly is not as critical. Second, is the whole subject of inheritance (the real elephant in the room). Which can be further separated into implementation inheritance, and interface inheritance (in C++, inheriting from a class that only has virtual methods with no implementation). To top this off, in C++ we have CRTP, and policy based design, which leverage inheritance to implement things best described as statically resolved mixins. Personally, I find myself using vanilla implementation inheritance, very rarely. It's kind of a rigid way of thinking about things, that breaks down quickly in most problems. A cat is a kind of blah. The famous square/rectangle example shows just how tricky these things are. However, interface inheritance is still extremely useful, because, well, separating things into interface and implementation is awesome. It's just a very nice way to bundle a bunch of related functionality modifying state, into one place. In short, polymorphism is fantastic, which you already know. The thing is though, whatever your personal preferences, you simply cannot always substitute compile time polymoprhism for dynamic polymorphism. For example, imagine a C++ program that is designed to read configuration files that represent predictive algorithms. The program reads the files, creates a corresponding object from a factory, and then using it to learn and predict against some dataset and saves the results somewhere. You will probably have something like this: class Model { public: virtual void learn(input_t x, output_t y) = 0; virtual output_t predict(input_t x) = 0; virtual ~Model(); }; The key point here is that every single kind of predictive algorithm will have some kind of internal state that is completely different, but you construct the absolute minimal interface that you need so that other parts of your program can interact with that state constructively. Implementing `learn` and `predict` on the object is absolutely necessary as they need to interact with that differing state directly. However, a function that uses learn and predict to do other things, like say `cross_validate`, should be implemented as a free function. As it happens, the C++ programs that I write are configured in extremely complex ways, and in many cases the only reasonable thing to do is to have interfaces and factories, and create instances of your objects from the configuration. So I do use it on a daily basis, and I still think it is a valuable technique. I will add though, that I rarely use interface inheritance, without a factory. Because if you don't have a factory, then somewhere in your code you are still stuck specifying the type at compile time, and at that point you may as well just use compile time polymorphism.
I don't have time to fully write this out, but I think you should examine your internal idea of OO design. Does it require inheritance? Interfaces? etc.. Also, check out the concept (heh) of Concepts in C++, which are the sort of interface contracts you touch on.
How do you produce 32-bit packages in docker, since docker doesn't support 32-bit systems? Cross-compile to 32-bit on 64-bit?
First, most problems have more than one solution, second OOP *is not* about polymorphism and inheritance, it's about data hiding and putting code and the data it manipulates together. Third, most of the time you don't actually need run time polymorphism, one common mistake is using inheritance for code reuse, where you should instead use composition, which by the way is as much OOP as polymorphism, and composition coupled with templates are more powerful than inheritance. If you look carefully STL is all about templated composition, that's why you don't see hierarchical containers. When you come from Java and similar languages without powerful templates you get that sense of "less OOP" because inheritance and polymorphism is all you have to work with. On top of that, run time polymorphism is significantly more expensive performance wise than compile time polymorphism and templating, making most developers, especially library developers, prefer the second.
In GCC: -m32 . In CMake: Make your own option, or use ${CMAKE_CXX_FLAGS} . in rpmbuild: --target i686 
You don't even need lambdas. An `InputIterator` is a generator!
To be fair, most other languages do this just fine.
You're probably looking at libraries designed to be highly generic so as to be useful in as many contexts as possible, and as fast as possible. A lot of more classical OOP approaches are incompatible with that. If you're not writing a library for wide consumption then judicious use of inheritance and virtual methods is probably the way to go, rather than highly templated stuff.
&gt; A modern build system should use a well known scripting language (should it be Ruby, Python, or even Lua). No. It shouldn't. **Fuck. No.** Invoke compiler. Run tests. Zip up headers and libs. I should not have to learn ${MY_COWORKER_FAVORITE_SCRIPTING_LANGUAGE_DU_JOUR} to do that.
Qt is very much OO Design. Its kind of the JDK of C++. I use Qt a lot, but with C++11 and following standards, you can see where Qt forces you to write more boiler plate code, just because its OO patterns. Derive a class you have too often, just to override one or two virtual methods. The Signal and Slots support lambdas, but thats about it. So you end up writing a glue layer which has classes handling Qt things, and offering a better interface. Like taking a lambda, instead of re-implementing the class for each new use case. QObject derived classes can't be templates, but you can add method templates, and have function templates taking them as arguments. Variadic method templates + generic/variadic Lambdas can ease things. Its not perfect though.
We don't build 32-bit packages, but it is entirely possible to cross-compile 32-bit programs from a 64-bit host. Using `-m32` with GCC may work, but for a complete solution, you probably want to write [a full toolchain file](https://stackoverflow.com/a/28047073). ([More about toolchains](https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html))
I've been doing this with a python script with "some" success. Biggest problem for me has been dealing with .props files correctly.
I didn't mean just C++ build systems. Every microsoft seems flaky to me. I get blue screens (Even in VMs), software takes forever when it does work, and their super expensive compiler and database always seem to benchmark slower than mingw and mysql for my workloads. I am pretty sure its that I am more experience with linux or solaris. I started with windows and it just seem to accrue problems and not ever work right. My first view times with linux weren't perfect either, but I could at least get to a state where nothing broke and it was stable. Now I work with Linux professionally and problems are easier to solve then when I worked with windows. A day of troubleshooting becomes an hour, a crazy bug is acknowledged as a bug by some open source community, not dropped into some microsoft queue to be ignored for years. And duplicating bugs! There are so many bugs that can't readily be duplicated (did you see the guy with the new ryzen that has slowdowns on some mutex deep in windows), when I have issues on a posix system they are readily duplicable and probably my fault and that means I can work to fix them.
Do they support makefile projects yet?
Implementation inheritance is a *very* special case in a software design. It's intended to support the case where we have some method (say `M`) defined on a base class (`Base::M`) that itself calls some further self-methods `A`, `B`, `C` internally, and if any of these methods are overridden, then we want these self-calls within a `Derived` object to ultimately call the `Derived` version rather than the one that was defined in `Base`. Clearly, this can require any self-method calls within the object to perform an extra indirection/dispatch through a vtable pointer, which is far from cheap in the general case! Some related notions which refer to the same or very close ideas are "[open recursion](http://journal.stuffwithstuff.com/2013/08/26/what-is-open-recursion/)" and "tying the knot". This is *not* 'recursion' in the usual CS101 sense, but it is a sort of "recursion" in how the bundle of available methods for any given type (or even any individual object!) ultimately gets defined. And it is "open" because you can always derive from a class and override the existing methods or add new ones.
A lot of those issues are not OO, but Qt issues.
VS 2017 preview 15.3
Glad to see that they have supported Clang's [thin-LTO](http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html)
I see. I don't ever do full installs--I'm pretty much a C++ guy. But if there are packages that can install elsewhere and won't, be sure to send feedback from within the IDE!
Would like to know this aswell! I've been trying to build and run SMFL but haven't gotten any luck. (Stuck on makefile process)
Personally, I've written a lot of complex high-performance code and seldom use OOP, meaning: I never use inheritance except to provide multiple implementations of the same high-level API (none of the methods are called at high frequency), I don't have many small objects allocated (I use arrays/vectors instead), I don't tie all code to data (the vast majority of code is in free functions that accept the data they need). This isn't just rusty old C code either, its fully C++11 with "almost always `auto`", using `std::shared_ptr` for memory management of arrays, etc. There is careful use of explicitly instantiated templates, but thats another story...
&gt; In Buck, dependencies are other Buck targets. So can it use dependencies from pkgconfig? How would it get the usage requirements from a library that wasn't built with Buck? &gt; If you depend on a system library, then you need to pass the appropriate linker flags. This is pretty self-explanatory. For example: I mean telling Buck where my dependencies are from the perspective of a package management tool. That is to say, my package management tool installs everything into `/opt/pkgs`. How do I tell Buck when it builds to look for libraries there? Not just the binaries which can be linked with `-l` but also the header files? &gt; Regarding compiler flags, you can set them on a per-target basis I mean setting the compiler and flags for the entire toolchain. That is, as a package manager, I want everything to build with a certain compiler and certain compiler flags without modifying build scripts.
Really, do they support properly skipping files in the debugger? I can't stand single stepping through the stl. Such a waste of time.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6odgdu/codeblocks_c_libs/dkgm1zv/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Take a look at Sean Parent's "Inheritance is the base class of evil" https://www.youtube.com/watch?v=2bLkxj6EVoM This was one of the most insightful talks on C++ and OOP design I have ever seen. If you have not seen it before, you will not be the same after watching the talk (in a good way).
I use objects all the time, but I've grown to realize that non-pure inheritance, 90's style (Java, C#, old C++) is just a mess. You generally want "traits" (pure interfaces or just template duck typing) + composition. Breaking up an object in user + used, instead of rolling them both into super and subclass is almost always much cleaner and much more flexibile; mostly because it reduces even more the size and possible interactions of internal state, and if designed properly it allows consumers of what would be a huge virtual class to consume only the the piece of it they need, reducing interface surface. It also allows you to design your objects around the data they contain and how it is used, instead than around their functions. Objects should always be about the data they contain! The typical example in games is Entity inheritance trees... you *will not* come out of it alive if you try to design it in a inheritance tree style. Component-based designs actually help you make better gameplay where rules apply orthogonally to anything that has certain data rather than manually crafting all edge cases. Example in the latest Zelda: some things conduct electricity; some are hot; some are equippable; some have health. The game has an example for all possible combinations and they just work as expected... inheritance will not let you do that. I didn't go through this change in style following an ideology, I just gradually found myself never using inheritance and polymorphism in designs I was happy about.
I think they are monitoring this sub, but to save time, you might get a quicker response if you email posters@cppcon.org. You don't have much time though, the deadline for poster submissions is July 23rd.
 &gt;The typical example in games is Entity inheritance trees... you *will not* come out of it alive if you try to design it in a inheritance tree style. Truth. I came out of my classes with Savitch thinking that these beautifully designed inheritance trees could solve any problem. Then I worked in the game industry for a while and all that nonsense had to be burned to the ground and discarded.
It is indeed, but the code is harder to write. Especially for non-trivial cases where it is much harder. That's why the next article will add iterators to the generator so it can be used consistently with things like the STL algorithms and ranged for loops.
Absolutely. Fortunately I got all wrappers to use the same source code and just reference a different version of the VS API. So there is at least no code duplication.
Actually the extension described in the article is our second attempt to tackle this. In our first attempt we've been trying to convert VS projects without using the API but gave up because there were so many places where flags could be hidden and so many variants of the VS project data format which has changed between the different versions of VS.
&gt; Is one exploitable memory safety issue per 10 years of active work good or bad? I think it's a good track record for a developer, but very bad for a serious project with a large number of developers. The individual developers are good, but the software is still Swiss cheese. Suppose you have 100 good developers who average 10 years of work between memory safety issues. Let's say memory safety issues live for 3 years on average until discovered and fixed. That means at any time, your software has 30 outstanding memory safety issues. That's plenty exploitable. Suppose now you enforce use of the newer, safe C++ features that reduce the error rate to once per 100 years of work for each of your developers. That's 10x fewer errors. But with 100 developers, this means you still have 3 outstanding memory safety issues at any time. That's quite likely still exploitable.
Memory safety issues don't have to be from manual `malloc` and `free`. They can be from subtler things like iterator invalidation. Or problems with signed/unsigned arithmetic. Or race conditions between threads. The new features reduce the error rate a lot, but the problem with large projects is that reducing it by 10x is not enough, it has to be effectively zero. If you reduce the error rate to once per 100 years of work for each developer, but a memory safety issue persists for 3 years until fixed, then if your project has 100 developers, at any time you'll still have 3 potentially exploitable issues. If your project has 1,000 developers, you'll have 30. And that's if your developers are *all* great.
&gt; The famous square/rectangle example shows just how tricky these things are. What is this? I don't think I've heard of it.
In that case, the correct type to use would be `uintptr_t`, but C and C++ do not guarantee that there exists such a type. Yes, on common architectures, it's possible to find an "invalid" pointer value, but there exist architectures where pointers have strictly more bits than any available integer type. And then there's another thing here: what would even be the use-case for `optional&lt;T*&gt;`? Why not just use `nullptr`? 
A square is a special case of a rectangle, so a naive developer might write: struct Rectangle { int width; int height; Rectangle(int width; int height); }; struct Square : public Rectangle { Square(int size); }; This leads to Square s(3); s. width = 5;//Odly shaped square Now Java solves this problem with setters and UnsupportedOperationException ( see java.util.List ). A different solution would require that Rectangle is non mutable, so while redundant the width and height of a Square couldn't be changed. 
Have you got some of your work you could share ?
OO design sucks because it leads to classes that are coupled in weird ways and boilerplate that doesn't pay dividends unless you make many of the same classes. Concurrency is hard because locking and unlocking on a per object basis is not efficient, now you wanted plymorphism but have to write containers. It makes sense for a game engine, but little else. With easy to pass function objects and lambdas, we can actually avoid some of the traditional cases where OO was requured.
I actually switched to using c# dynamics to do the same
Do it where it makes sense. Modern C++ is about mixing paradigms.
Well, ok then, take for example the stl stack definition: template&lt; class T, class Container = std::deque&lt;T&gt;&gt; class stack; Would you consider this to be 'object' composition ? Because personally I've used and I like this kind of pattern of injection functions and object via template parameters a lot, however, I feel like this doesn't fit into the definition of OOP (e.g. stack is not inheriting from its container, you can't access the container's previously public methods via stack's interface, if Container respects and interface stack can very well not respect that same interface). 
Just saw this comment while looking at replies for another thread - and wanted to make sure you're aware that full Matchers support (including documentation) is in Catch (for a few months now). https://github.com/philsquared/Catch/blob/master/docs/matchers.md
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [philsquared/Catch/.../**matchers.md** (master â†’ 7a22bad)](https://github.com/philsquared/Catch/blob/7a22bad76340f8b48094b46bc586e76ac9ea93ac/docs/matchers.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkgu48c.)^.
This and the circle/ellipse problem are an example of frequent interview questions to see if the candidate understands the Liskov Substitution Principle. https://en.wikipedia.org/wiki/Liskov_substitution_principle
'Inheritance' as you see it on a language feature level is implemented in terms of composition by the compilers. I.e. struct A {}; struct B : A {}; is an equivalent of: struct A {}; struct B {A a;};
An optional&lt;T&gt; represents any valid T value or the absence of a T. The first case requires sizeof(T) bytes of storage, the second case requires some further storage. Therefore sizeof(optional&lt;T&gt;) &gt; sizeof(T), unquestionably. The only exception is where T has invalid representations, and one of those could be used as the uninitialised value. I think this is your line of thinking - for the pointer example, say the first 48 bits are used for valid addresses, that leaves a huge number of values as candidates for "not initialised". optional cannot in general know what such a magic value is, however, and behaves as per my initial argument. 
Just to note (not correct), the reason that copy on write is no longer implemented for strings in C++ is because it is incompatible with the standard from C++11 on. https://stackoverflow.com/questions/12199710/legality-of-cow-stdstring-implementation-in-c11
No, Qt is based on OO. Some problems might be Qt specific, but OO is usually involved in some way even then.
Ta. I actually did see that last week and spent some time rewriting our matches to use the new version. Cheers for the work!
Let's step back a few steps. How do you get Clang to work on Windows and how do you make it work with Visual Studio? :)
&gt; what would even be the use-case for optional&lt;T*&gt;? Why not just use nullptr? The difference between "this value exists and it is nullptr" and "this value does not (yet) exist". I use that distinction constantly with std::map::iterator.
thank you i will try this out. 
&gt; but its completer is based on libclang, which API has some limitations for c++ Which ones? [Code completion API](https://clang.llvm.org/doxygen/group__CINDEX__CODE__COMPLET.html) looks rather feature-full.
A cool - thanks :-)
The correct approach is to also change the square's height when its width is changed and vice versa. 
In Java: Rectangle r = new Square(4);//area is 16 //halving the area of r r.setWidth( r.getWidth() / 2);//area is 4 Works just as expected. Guaranteed to surprise any user that expects to deal with a valid rectangle. 
&gt; the reason that copy on write is no longer implemented for strings in C++ is because it is incompatible with the standard from C++11 on The causality is the other way round: standard library implementors noted issues with COW (notably pointer invalidation) and stopped implementing it. That was *before* C++11. And *subsequently* there was a push to standardise on this iterator behaviour, and that happened in C++11.
Are they planning to?
This was taken from nlohmann/json. There, you can pass a callback function to the parser which decides whether or not to store a parsed value. In case a value is skipped, but needed for structural reasons, it is marked as discarded. This should be removed from the standard proposal.
GCC only enabled copy on write strings by default in version 5.1, released in April 2015 (again... Not a correction, just clarifying, there is no implied order of events just because of the causal link). 
&gt; I call it my billion-dollar mistake. (Tony Hoare about the Invention of NULL) And you have not only one, but two ;)
&gt; enabled copy on write strings by default You mean *disable*, right? And yes, correct. The timeline is messier than my simplification suggested.
yes please! a nice blog post explaining how to set-up latest clang for windows with MSVC / CMake would be so nice.
&gt; Substitutability is a principle in object-oriented programming stating that, in a computer program, if S is a subtype of T, then objects of type T may be replaced with objects of type S (i.e. an object of type T may be substituted with any object of a subtype S) without altering any of the desirable properties of T (correctness, task performed, etc.). The problem with this is that one needs to actually define "the desirable properties of T". For instance, with the rectangle / square example, is the non-coupling between `l` and `L` actually a desirable property ? In my opinion it depends on what you want to do with the rectangle.
it's the most valuable and mind-blowing feature, IMHO. also, it's somewhat inconsistent that there is std::string_view and no std::span :(
&gt; I feel like this doesn't fit into the definition of OOP that's because your definition of OOP comes from 1980's textbooks.
&gt; QObject derived classes can't be templates, uh ? of course they can. and with verdigris even classes with Q_OBJECT can be templates.
This seems to fix my issue with having to generate the project twice because only the ninja generator can create compile_commands.json.
I'm pretty sure you just install it and it works.
**Caution**: CLion 2017.2 still doesn't support C++14 constexpr[1], so don't bother downloading the free trial if your code uses C++14 constexpr. It's not just the usual stupid "yeah, I don't know what that keyword means, so I'll just ignore it" lack of support; it's (IMO) malicous "I saw the keyword `constexpr` in a place I don't understand it, so here are thousands of nonsense errors[2]" lack of support. [1] To be fair, they claim to support C++11 constexpr. [2] examples of the nonsense errors you can expect to see: Binary operator '&amp;' can't be applied to the expression of type ... Can't resolve type ... Can't resolve struct member ... Incompatible types in initializer. No matching constructor. No matching function. 
No, but the following should be true: `sizeof std::optional&lt;T&amp;&gt; == sizeof T*` because references can't be null yet the null-state can be encoded in the reference storage. For "reasons" `std::optional&lt;T&amp;&gt;` is not supported, but you can use `compact_optional` instead (which also allows you to encode this optimization easily). BTW Rust is able to go one step further. It allows declaring that a type can encode a null state, but that it will never have that state ([NonZero](https://doc.rust-lang.org/core/nonzero/struct.NonZero.html)). This means that not only `mem::size_of::&lt;Option&lt;&amp;T&gt;&gt;() == mem::size_of::&lt;&amp;T&gt;()` but also that `mem::size_of::&lt;Option&lt;Vec&lt;T&gt;&gt;&gt;() == mem::size_of::&lt;Vec&lt;T&gt;&gt;()` and this applies to any UDT recursively containing a type that allows storing a null state as well. [Check it out!](https://play.rust-lang.org/?gist=fbb4fae66355fe6b81f5ff0f5993db2f&amp;version=stable) Pretty neat eh?
From [here](http://llvm.org/builds/).
Every `cxx_library` and `cxx_binary` accepts an optional parameters called `linker_flags` and `compiler_flags`. Those can also be set globally in `.buckconfig`. Using `-L path` `-I path` you can tell the compiler where to search for libraries and header files. You can instruct pkg-config to return gcc/clang compatible list of Searchpaths and link instructions using `pkg-config --cflags --libs lib1 lib2 ... libN`. Now you can add ```compiler_flags = ['`pkg-config --cflags --libs lib1 lib2 ... libN`']``` to buck. Although this will work, I'm not sure if you still can guarantee reproducibility.
Yes it is. The stack is implemented using an "internal" container. It is *composed* with the container, and here we get a great example of how much more powerful templated composition is than inheritance, because I can change the container type *without changing the stack class*, at instantiation time. If the chosen container class implements the (informal) interface that the stack uses, the compiler will be happy and all will work. This is called duck typing. One advantage of this is that you *don't* need a fixed, static interface, you don't need to inherit or "implement" any particular class (or "interface") as long as your class provides the needed methods with the correct signatures it's going to work. This is actually how the first OOP systems, like Smalltalk, works (the duck typing part)
the normal window build of clang... kinda works on windows? https://llvm.org/builds/ as noted its last update was 26 June 2017
&gt; yet the null-state can be encoded in the reference storage Unlike pointers, references are not "objects", i.e., the compiler is not required to allocate storage for a reference. 
&gt; Unlike pointers, references are not "objects", i.e., the compiler is not required to allocate storage for a reference. Yet all compilers do.
Optional is provuded by the C++ compiler, so *can* know what an invalid pointer looks like.
The do when required by implementation issues. When they can avoid it, they do not. A global static reference can have zero storage, especially if the compiler can deterjije what it refers to at compile time. Unlike other types, you cannot get a pointer or reference to it, and thus it need not itself have an address even if it is used in a way that would ODR-use other types.
UNREADABLE. (or::just::me::hating::reading::semicolons) "Modern C++" is such a mess...
In this case, it's the desirable properties of both S and T. If it's expected that the length and width of a square are coupled (because otherwise it's not a square), then it's equally expected that the length and width of a rectangle are not coupled. Therefore, neither is substitutable for the other. These expectations vanish with immutability, and they do become substitutable, which is an excellent talking point.
Is shitposting your full-time job? * The library's examples have a normal amount of scope resolution operator usage. * The amount of `::` doesn't have any correlation with "Modern C++".
Yes, but OO doesn't mean "can't take lambdas", or "can't have qobject templates"
It is already there. You can export the graph scene together with your model's data
Wouldn't it be easier to just use the par or par_unseq execution policy with for_each algorithm ? (Of course this requires C++17 or higher, so may not be available for you...) 
Is it possible to install standalone clang on windows yet? (I.e. not needing a MSVC install)
Yeah, they are aware of it, so hopefully it will continue to get better. One odd case with those external packages was compatibility with non-VS-installed versions, which is not necessarily obvious 
No. 
Ah, fair enough. Tbh I find this kind of composition to be very helpful both with classes and with functions... so I guess it might be just a problem with my definition of OOP, I always so OOP as being focused on inheritance&amp;access-modifiers and assumed composition of any sort to fall in generic and/or functional programming, whether its applied to classes/types or to functions.
Don't be such a dweeb, and start using usings then...
Maybe just pass the n in your thread creation loop and have a chunksize of arraysize / numthread? That way you can offset into the array with n* chunksize, i.e. every thread gets a continuous block of the array to work with. I don't see the point of processing multiple chunks per thread. Maybe I'm missing something?
You can also build windows clang from source in order to get the latest version.
Yes, many cases for C++ 14 constexpr is still a pain-point to CLion (C++11 cases should be all ok). We hope to have it soon, not this time unfortunately.
I think you misread my post. I'm on "your" side of the "fence" too. I was asking Daniela-E (and the other people using VS solutions and not CMake) whether they never have to hand-edit any settings because it's a freaking pain in the ass, which is why you should CMake in the first place. I can't understand how they never have to hand-edit settings and how it's possible that it doesn't terribly annoy them, even doing it once.
We do plan. For that we need to solve some technical difficulties, like for example, how to deal with the project header files not listed in Makefiles, and some others. We constantly investigate possible solutions and discuss them internally, but haven't found any good solution to implement yet. But I'd like to assure you that we really would like to get Makefiles on board.
hmm indeed, I don't know why but I read "With CMake you basically have to do this over and over again on every PC" instead of "Without cmake" (certainly because I knew people who actually did this)
It is provided by the standard library; there is a difference. Also, 32bit systems are still quite common, which don't have free bits to use like that.
Did you see this pdf? It is linked on the poster submission page of CppCon. https://www.clear.rice.edu/comp400/9.pdf
Just use the C++17 parallel algorithms.
I modified some *.props and *.target files to make them compatible with vs 2017, they work for me, but I dont know how good are they for commercial use https://github.com/WubbaLubba/LlvmForVS2017
None of my real work; I work in finance so it's all prop. I have a blog; it's been a while since I updated it but I am working on a couple of posts, one of which is actually about how to setup an interface + factory nicely. http://www.nirfriedman.com/.
Not exactly. Having an immutable class does not preclude having a method like: virtual Rectangle Rectangle::changeWidth(int width) const; The idea is obviously to create a new rectangle with differing width. This is how a lot of immutable objects handle the desire for something like mutation. Obviously now we're back at square one where the desirable properties are contradictory.
The main thing you need for that to work is a linker. It should be possible to set up LLD for that. You may also need some libraries.
What happens when either the constructor of `std::thread` or the vector when resizing throws? Those threads aren't joined or detached and will die with the vector so I guess `std::terminate` will be called. 
everything is broken anyhow in that case. 
Also worth noting is Microsoft provides and experimental Clang support package as part of Visual Studio installation.
Yes is this possible to configure with CMake?
`T*` already encodes the optional semantic, so there's little reason to use an `optional&lt;T*&gt;`. `optional` and pointers even have the same basic interface: conversion to bool, and `*`, the "NullableProxy" concept.
Oh dear, this latest version of CLion seems to have taken a leaf out of QtCreator's book and will now refuse to perform analysis (for code completion etc) on header files that aren't specifically mentioned in a CMake target's source list. That is, it seems that you now need to say add_executable(myprog src/myprog.cpp src/myprog.hpp # this wasn't required before ) This is despite that fact that the CMake documentation specifically says that header files shouldn't be included in a target's sources, as they're tracked automatically by looking at `#include`s. This is pretty annoying and means that I now need to go and modify a *lot* of CMakeLists.txts to get code completion for headers in CLion. Why was this changed? Is there any way to go back to the old behaviour?
For those who are curious: the reason `optional&lt;T&amp;&gt;` is not allowed is that assignment was a bizarre case, where two groups formed with very different ideas of what was correct/intuitive. In addition, many people have said that `optional` should be viewed as a unary container, and containers cannot contain references. Const pointers also already largely fill the exact same role as optional references. The only sad/missing part, is if you want to have nice optional arguments, a const pointer requires taking the address, so it cannot be used with temporaries. But writing a `OptionalRefArgument` class is like 20 lines of code.
Shared ptr as a matter of fact?! Yuck! In high-performance code?! I don't believe you.
Your first problem begins with your choice of how to use the atomic variable: atomic&lt;uint32_t&gt; sofar{0}; auto f=[&amp;sofar, &amp;rg, &amp;numchunks, &amp;chunksize]() { for(uint32_t xchunk = sofar++ ; xchunk &lt; numchunks; xchunk = sofar++) { auto xstretch=rg.getRange(xchunk*chunksize, chunksize); // do things with xstretch }}; Unless the work done with `xstretch` is large enough, this program is going to get absurdly slow as each iteration of the loop has to make an [expensive] atomic update of `sofar`. And though we don't know what kind of work `rg.getRange` and the commented code are actually doing, there's an extremely high chance that this code is going to cause some kind of cache thrashing. We can fix both of these problems by reevaluating our design choices. uint32_t num_threads = 8; auto f=[&amp;rg, &amp;numchunks, &amp;chunksize, num_threads](uint32_t tid) { uint32_t start = tid * numchunks / num_threads; uint32_t end = (tid + 1) * numchunks / num_threads; for(uint32_t xchunk = start; xchunk &lt; end; xchunk++) { auto xstretch=rg.getRange(xchunk*chunksize, chunksize); // do things with xstretch } }; vector&lt;std::thread&gt; running; for(int n=0; n &lt; num_threads; ++n) running.emplace_back(f, n); //Note the change here! // place for status for(auto&amp; r : running) r.join(); The major design change here is that each thread is now responsible for a contiguous set of numbers to perform work on. So if `numchunks` were to equal 90, you'll get the following (running in parallel, obviously) * TID0: [0,11) * TID1: [11,22) * TID2: [22,33) * TID3: [33,45) * TID4: [45,56) * TID5: [56,67) * TID6: [67,78) * TID7: [78,90) This is a lot better than your code, where each thread *essentially* gets a random number [0,90), and has to operate on an atomic variable to get that number. It shouldn't affect the output of your code (unless the fact that each thread gets random assignments is critical to the behavior of your program), and there are few scenarios where it will have lower performance.
Can you show an example when this is the case for a reference but not a pointer? (to prove that it is the fact of it being a reference which makes it possible)
The quotes were only to avoid having to explain the different approaches to `optional&lt;T&amp;&gt;` and why it was removed. My point was only that if we ever have it, then the optional and the raw reference can have the same size.
If only I knew how to use cmake, or other build tools. :(
My bad I thought you were salty about its non-inclusion (I know quite a few people are) and that was the purpose of the quotes. In any case I guess I'll leave my comment as its informative, I'll edit it slightly.
OOP itself has changed a lot. The style that was commonly used and recommended of making a lot of use of inheritance has shown itself to be wrong and result in less maintainable code. Inheritance for re-use is very rightly now seen as "bad" where it was previously the goal of OOP. Re-use is better gained through composition and shallow inheritance hierarchies. When we C++ programmers limit the use of classes, put more stuff as free functions that are generic we're just taking that further, building through composition rather than inheritance. We aren't stuck in an OO only language so we don't have to go through the hacks you do in such a language to make "free functions" that aren't free. That's all really. It's an industry wide paradigm shift. The rules apply similarly to all languages. We are just in a multi-paradigm language so have more choices. We're not stuck with a single solution meant to solve all design and personnel issues.
All is good :)
&gt; back at square one hah 
I don't want to be harsh but we already got several attempts lead by mr.Sutter to redesign the language. Managed C++ - failed. C++/CLI - failed. C++/CX - failed. Now what? meta classes! With all due respect I would prefer if MS Windows devs lead by mr. Sutter solve their problems by themselves and don't twist the language every now and then. I don't need such features. WinRT/C++ is said to be consumed just fine with library solution. If someone wants to write WinRT component let him write the IDL. It's not such a big deal (and most of us will use multiplatform solution such as Qt or wxWidgets anyway). As for Qt it's true current moc step is ugly. But to remove it we don't need to embed a new language into C++. There is an article from Qt dev which summarizes what is missing from the language to get rid of moc and surprisingly it's not that much. Basic reflection facilities will do.
I dread `git pull` on CLion because the UI freezes for 2 minutes each time :( That being said, apart from said freezes on updates, it's quite a sleek IDE.
Code completion is only the tip of the iceberg, though. In an IDE you also want usages for example: call sites, overrides, derived/parent classes, ...
There has been no change in this area. What *has* changed is that files not included in the project now show notifications, rather than just appearing greyed out - perhaps this is what you are thinking of? Or are you really seeing a change in the behaviour of the static analysis?
(although, when the header and cpp have the same base name, CMake treats them as if you'd added them anyway)
I don't agree, because if optional has a value, one can assume it is valid, while T* might just be random garbage.
1. Yes. 2. None of the standard libraries have shipped them yet :)
Having seen usages of `boost::optional&lt;T&amp;&gt;` in the wild, I'm rather glad it was omitted from the standard. I feel like no matter which assignment syntax had been picked, it would have felt weird anyway.
Lol, there are no answers here. Also no new interesting questions though... Edit : the answers have been added now.
&gt; An `optional&lt;T&gt;` represents any valid `T` value or the absence of a `T`. The first case requires `sizeof(T)` bytes of storage, the second case requires some further storage. Therefore `sizeof(optional&lt;T&gt;) &gt; sizeof(T)`, unquestionably. Actually, this is not so clear cut. For example, imagine `std::vector&lt;T&gt;`: template &lt;typename T&gt; struct vector { T* begin; T* end; T* end_storage; } A vector with no capacity is necessarily empty, and therefore `begin == end == end_storage`. *Any* such triplet represents a vector with 0 capacity, therefore there are `2^(sizeof(T*) * CHAR_BITS)` different representations for the same logical state; but likely you will only pick *one*. Well, that leaves you quite a number of bit patterns that are never used at all! So, for example, if the constructor is `vector&lt;T&gt;::vector(): begin(alignof(T)), end(begin), end_storage(begin) {}`, a perfectly reasonable representation, then you can say that the "full-zero" bit patterns can be used to represent the *absence* of a `vector` in `optional`. This can be done with a simple template, actually: template &lt;typename&gt; struct BitPattern { static constexpr bool is_valid(void const*) { return true; } static constexpr size_t get_invalid_pattern_index(void const*) { assert(false); return 0; } static constexpr size_t nb_invalid_patterns() { return 0; } static constexpr void write_invalid_pattern(void*, size_t) { assert(false); } }; That you specialize for `vector` as: template &lt;typename T&gt; struct BitPattern&lt;vector&lt;T&gt;&gt; { static constexpr bool is_valid(void const* ptr) { static const char Pattern[sizeof(std::vector&lt;T&gt;)] = {}; return memcmp(ptr, Pattern, sizeof(Pattern)) != 0; } static constexpr size_t get_invalid_pattern_index(void const* ptr) { assert(not is_valid(ptr)); return 0; } static constexpr size_t nb_invalid_patterns() { return 1; } static constexpr void write_invalid_pattern(void* ptr, size_t index) { assert(index &lt; nb_invalid_patterns()); memset(ptr, 0, sizeof(std::vector&lt;T&gt;)); } }; Here you go, with something like this^1 you can use the invalid bit patterns to store information at no memory cost. ^1 *I do not think that memcpy/memset are constexpr, so needs tweaking.*
It could for any type whose alignment is strictly greater than 1, by using 1 as the magic value. For types whose alignment is 1, such as `char`, then in theory any address is accessible, and while this may not be a problem on current 64-bits architectures, C++ is also usable on 16-bits or 32-bits architecture where one can wish to use the full spectrum of the address space.
The lack of support for new C++ standards makes it unusable. I prefer VS because the compiler and IDE are usually on the same page when it come to language features support.
Of course I did, thank you. 
That Clang/C2 experiment for C++ is coming to an end. It won't be updated beyond Clang/C2 3.8 in VS 2017 15.3 (first toolset update), and will be removed in the second toolset update. Our STL will begin requiring newer features; currently we're testing against Clang/LLVM 4.0.1 but may require 5.0 depending on release timing.
NaN cannot really be considered as âˆž âˆ’ 1 or similar so some of these possibilities can be trivially sidestepped. Not to be nitpicky, but it is somewhat significant here: in many mathematics contexts, âˆž - 1 isn't a particularly well defined value. Not that there aren't ways to distinguish between infinite values, but they don't generally have the same granularity as integral values. For example, you can distinguish and classify infinite values of different cardinalities in a manner akin to the â„µ-numbers (â„µâ‚€, the least infinite cardinal, describes the number of elements in the set of natural numbers, â„µâ‚ is the next distinct cardinality, etc), but such constructs have interesting properties. Being that it is the number of natural numbers, â„µâ‚€ is transfinite or countably infinite, which is to say that you can assign to each element of a set of this cardinality exactly one natural number (or, in other words, construct a bijection between these sets). One consequence of this is that, as we know any other aleph number is inherently larger, any larger infinite quantity is not countable. Examining â„µâ‚€, you might consider your original decrement or, perhaps, something more drastic like, say throwing out most of the values. Just to go even past this, let's remove all composite numbers. Since 2 is the only even prime, this fact alone automatically removes 1 less than half the elements and - it is fairly obvious - once we start the multiples of additional primes, more than half the elements are composite and therefore are absent in the resultant set. There are an infinite number of primes as, for any prime Ï€, consider the value Ï€! + 1. Since the factorial of any number n is by definition a multiple of every positive integer i where i &amp;lt; n + 1, it's successor is trivially seen not to be divisible by any of them, save the multiplicative identity: 1. Ergo, either Ï€! + 1 is prime or there exists n`, a positive, integral, and prime - the 3 criteria for inclusion as the first two show it to be a natural number, our initial set, and the third shows it not to be composite and thus not removed in our construction - factor of Ï€! + 1. With this and the fundamental base case of primality, 2, the inductive reasoning to formally prove the set of all primes infinite is obvious. Additionally, as it is a subset we can intuit that a mapping from natural number to primes. Let's start said mapping by labelling the prime 2 with the natural number 1 (or 0 if that is how you definite the natural numbers). Then consider for each prime p labeled as n, a natural number i, find the first i such that the i'th successor to p in the set of natural numbers, designated p + i, is in our constructed set of primes and label it as n + 1, the successor to n. This mapping is trivially seen to map each element of our original set to a single element of the constructed set. As considering each successor in the former must consider each successive element of its subset, the latter, we can invert our labeling from n to p, creating a mapping from p to n that obviously maps each successive element of the former to successive elements of the former. This pair of mappings is therefore a bijection. The existence of a bijection between these two sets implies that for each element of either set, there exists a counterpart in by other set, these being the elements we denoted n and p or vice versa, showing that this set and a strict subset thereof must have the same cardinality. This non-obvious result means that while the strict subset of any finite set must have fewer elements to count, this is not necessarily the case with infinite sets, even those that are countable. In fact, the number of elements in any two sets sharing a common cardinality of â„µâ‚€ or above, we cannot distinguish the number of elements in one set from that of the other. Because of this, we can more obviously see that âˆž and âˆž - 1 are not distinct quantities. You might consider that âˆž is not so much a quantity as it is, rather, a property describing the cardinality of a set. This is one of the simplest properties of these numbers I can think of right now, but it shows why arithmetic including infinite terms is so difficult or unintuitive. 
Those kinds of bit patterns are what I meant by "invalid", which I put very simplistically. I didn't just mean those bit patterns which, for example, break the class invariant and could not reasonably exist in nature. I mean all those which are "unused". 
I believe youâ€™re entirely missing my point: *Because* `âˆž - 1` it might appear reasonable to represent that â€˜valueâ€™, arising from some calculation, as `NaN` (ignoring for the moment that the actual implementation would just give you `INF` again). Then dividing by some such badly-defined value should, in my opinion, also give you a badly-defined number, i.e. another `NaN`. Put another way: We donâ€™t know wherefrom the `NaN` in `âˆž+iÂ·NaN` came and deciding that `1/(âˆž+iÂ·NaN)` should be the same as `1/âˆž` seems entirely arbitrary to me.
Yes. My point is though that defining a simple helper struct which can be specialized `optional&lt;T&gt;` *could* know about the unused bit patterns and take advantage. *And actually, `variant` could too, as well any other kind of union.* Rust uses a simplified form (no `constexpr` there) with the `NonZero` trait, and therefore has a `Option&lt;&amp;T&gt;` which is pointer-sized.
OO is so uncivilized. I don't find a need for it.
If a compiler is able to determine what a reference refers to at compile time (so it can optimize the ref away), it wouldn't be different for a pointer. [Example](https://godbolt.org/g/iEU6ae).
Thanks for letting us know. Question: does Microsoft use Clang internally anywhere, or is it strictly MSVC?
Probably not. Not even Resharper++ has it. Main reason for me to use VisualAssist X.
&gt; is coming to an end &gt; currently we're testing against Clang/LLVM 4.0.1 but may require 5.0 I'm confused. What exactly is coming to an end? It will move from "experimental" status to "release/production" status? Or the experiment is terminated and Clang/C2 will be removed as a whole?
They are similar, but not equivalent. An `std::optional` holds `nullopt` if you [default initialize](http://en.cppreference.com/w/cpp/utility/optional/optional) it (e.g. `std::optional&lt;T*&gt; o;`), whereas a pointer is uninitialized (e.g. `T* p;`).
&gt; ${CMAKE_CXX_FLAGS} Don't use `${CMAKE_CXX_FLAGS}`. Use `target_compile_options`.
WRONG, it is a byte container PERIOD
It would be a tradeoff. You could save some memory, yes, but there is a runtime cost. The is-initialised check currently inspects a single byte (https://godbolt.org/g/KYt3CW - and I think this holds up to scrutiny looking at the specification, not just any particular implementation), whereas taking the specialised vector implementation as an example, it would have to compare three bytes because all of those bytes are needed to uniquely identify the uninitialised state. This may not be desirable. I think the standard design made the correct decision, given this, and they may very well have had other reasons too.
Not to nitpick (though this is Pedant's Corner if ever there was one), std::optional never holds a std::nullopt_t, this is just a type used to overload std::optional's constructor to tell it *not* to initialise the value it holds. Likewise, when default initialised, it doesn't initialise the value, just something that says "I'm empty" (a boolean).
You gain what you gain from optional anything - another state outside of the valid states of that type. This may not sound useful for T*, but it's possible that you might want to express something like "a pointer, which might be null, but which might not have had its value decided at all, yet". Weird, probably error prone, definitely not the default choice, but if it can be done, someone will need it!
If you do "clang++.exe -fuse-ld=lld", lld-link.exe will be invoked instead of link.exe from SDK. But I don't use "clang++.exe", because I can't debug (with Visual Studio) resulting executables (debugging works fine if I use LLVM\msbuild-bin\cl.exe to compile).
Sorry, I lost the second half of my original reply to a flaky phone and definitely lost/forgot a fair bit in reconstruction. Intuitively, I understand where you're coming from and wouldn't dream of falsifying the theory. On the other hand, though, intuition often breaks down and fails in consideration of infinite, infinitesimal or un(der-)defined terms, and this discussion concerns interaction between the three. One of the intentions behind elaborating on number of primes vs ints - lost in transcription or recall and reconstruction - was showing how many fundamental operations do not behave intuitively when operating on both infinite and finite terms at once. I'm not really sure how you would go about construction of a NaN such that the intermediate results, properly re-(distributed|associated|etc) in such an equation with âˆž could have a meaningful affect on the infinite term. This is my own failure in regards to knowing the complexities of IEEE floating points wrt NaN. My intuition teeters back and forth, but isn't necessarily useful in such contexts regardless. 
If I'm not mistaken, QtCreator uses clangcodemodel only for autocompletion, and still uses the old C++ plugin for code navigation.
I know. But being pedant is almost always good :) I think I should pay attention to the words I use.
You should really check out rtags. I use rtags + ycmd + spacemacs. All of the parsing of source is libclang based. It always understands complicated constructs, the error checking is nearly 100% reliable and only has a latency of a second or so (it takes about 10 seconds to first parse a file when you open it, but after that its fast). Rtags doesn't have every single fancy IDE feature I don't think, but everything that it has works pretty much perfectly. Goto definition, find references, search symbols, find overrides, renaming. I forget whether it has inheritance hierarchies. It makes a huge difference wanting to change a file, doing find references on it, and knowing with 100% certainty that those really are the only 2 places that it's called. Auto completion is extremely accurate but kind of slow (0.5-1 second delay). I workaround this by having the automatic popup completion be simple name based, and manually triggering semantic auto completion. Works fine for me. This is all in a pretty large, complex codebase. This is definitely harder to a setup than IDE but not by much. Honestly I spent hours getting Eclipse to understand my codebase, putting in paths to look for includes, etc. Other than getting familiarity with the tools themselves, all you really need to do is produce a compile_commands.json file. And if you do things like run clang-tidy nightly you already generate this file most likely.
Hi Phil, thanks for the response. What threw me was that for headers that are not directly included in a target, there is now a banner across the top of the editor saying "This file does not belong to a project target, code insight features might not work properly". I saw that, tried editing a couple of headers and got nothing, so assumed it was newly broken. But you're right, of course. I've just tried it again and it does work just as before -- provided I remember to wait for indexing to finish before I go running off to Reddit complaining about things! Apologies for the false alarm, and please change my whinge from "it's broken" to "indexing takes too long and makes Tristan think it's broken" ;-)
I just took a look at Hunger - I cannot get a feel for what this is. I think there must be a language barrier. I look at the quick start and I'm asking myself after each statement, "why?".
That is not how you do a relevant benchmark. Try with something that looks more like production code, otherwise you will only measure how good is the compiler optimizing trivial code. Maybe the std version performs better with more complex code, maybe not. 
Pretty sure visual studio 2017 natively supports Clang and even has a check box to install it when you install visual studio
That depends on whether or not you want control over the chunking size I suppose.
Granularity is what matters. What is bad is many `shared_ptr` objects pointing to small allocations, with high frequency constructor calls. What I have a very few (order 100 max) `shared_ptr` objects, each pointing to a very large array, with very rare constructor calls. This is basically using `shared_ptr` instead of `malloc` and `free` in high-performance C code.
Sorry for the ambiguous phrasing. The experiment is being terminated, and we aren't planning to update Clang/C2 for C++ ever again. (Clang/C2 may continue to be used for non-C++ scenarios, about which I know very little.)
I don't really know enough to answer this - we got a few internal teams using Clang/C2 and they might migrate to Clang/LLVM. I checked in Clang/LLVM's binaries for use by the STL's tests, but we don't use it to build our separately compiled DLL/LIB.
Q_OBJECT is what I mean, which is what you need to use to have signal/slot through moc. And ofc. copperspice and other solutions offer workarounds, but not even verdigris is part of official Qt distro. You'll just add the boilerplate of someone else to Qt to make it work.
Every OO framework/API you interface with, will have similar problems. Subclassing is a classic pattern in OO, and usually there is not a lambda interface offered in OO centric C++. Just because, these code bases often predate C++11. QObject is ofc Qt specific. But wxWidgets also has wxObject. Other Apis don't have this weak point, but still will offer you certain class hierarchies. Your one pattern to specialize things is then to subclass, classic OO. So if you have one abstract base class and 5 use cases, you'll end up writing 5 classes. When its just the difference of what you do in the few virtual functions, you might get away with implementing one class executing a callable in the virtual method.
Will Clang/LLVM be available as a Visual Studio install option *in lieu* of Clang/C2?
in structs maybe. but most of the time they are optimized out.
But none of that is a cpp problem. Further, the real problem is the legacy model of mocing, because for obvious reasons the moc at present can't deal with class templates 
I compared `T*` to `optional&lt;T*&gt;`. Even if the optional has a value, at the end of the day it's still a pointer, and it can still point to garbage. Just like if a pointer is non-null, it can still point to garbage. If you were comparing `T*` to `optional&lt;T&amp;&gt;` then yeah, this would be an advantage in principle but in practice quite a small one.
Well if they wanted a rectangle that guarantees that width and height are independent they should have asked for an IndependentlyDimensionedRectangle, which is a subclass of Rectangle, and Square certainly is not an IndependentlyDimensionedRectangle. The lesson I always got from those square/rectangle/circle/ellipse examples was that there's a lot more to an "interface" than the member declarations. If you define your rectangle so that square subclasses it then you'd better not act like the rectangle interface guarantees things it doesn't. If you want the rectangle to guarantee that the width and height can be set independently, then you'd better not subclass it with a type that can't support that interface. You can have either design without violating the LSP, but you have to decide what guarantees are actually made by the interface.
We have no plans to add Clang/LLVM to the VS installer at this time. That may change with sufficient customer demand.
No problem - I had to double check, myself! Indexing performance is getting better - but it's a hard problem with a limit to how well it can be solved.
Not with an (unicode aware) internal string type ðŸ˜‰
Then there is also the CRTP
so [[deprecated]] is getting [[deprecated]]?? :)
I've never been able to get it to work. At first, it would compile C but not C++ (it couldn't find the headers); then I tried manually specifying the VC++ STL paths (which I read clang++ uses on Windows) and it then refused to link. I tried uninstalling/reinstalling, and it then refused to compiler anything â€” not even the previously-working C code. I guess I was doing something wrong or it didn't like something else I had installed, but I never figured it out.
Very cool! I can't find any documentation as to what compilers are supported (and I haven't had a chance to dig into the module yet). Anyone know if there is a list?
No. The article maybe could clarify: the three attributes mentioned are being added but do not replace those already there.
Sure, then why does this thread exist? OO C++ libraries are typical a thing of the past. Legacy code, which isn't aware of lambdas and other new features. Qt is a good, and very popular example. Lots of legacy code bases are like that. What do you expect to face as a C++ programmer in the Industry? Only fancy, shiny modern C++? Because, you know, I'm already hearing these complaints from companies looking for C++ devs, that its difficult to hire new, young devs for OO C++ code bases. Ofc. subclassing isn't a "cpp problem", its just one of the centric patterns in OO everywhere.
Normally, I won't set such value in CMakeLists.txt. However, using it on commandline, like `cmake .. -DCMAKE_CXX_FLAGS='-m32 ...'` is useful. 
Just tried to compile QtCreator from source and it seems it won't work on ArchLinux as it doesn't support clang 4.0.1 yet. Oh crap. EDIT: Got it working, apparently I was wrong. Clang 4.0.1 works happily with QtCreator. I simply forgot to update some sources (Also needed qbs from master).
Generally: Subclassing isn't a problem at all. As to why this thread exists: because for some reason people think that C++ used to be OOP, and now is something else. Generic imperative or something. C++ was *always* multi-paradigm, but more importantly *OOP isn't having a "class" keyword*. The core of OOP isn't *how* it's implemented, but rather *what* is implemented, and new C++ features mesh well with older features and enable us to program with an OO paradigm. If we want that.
compact_optional is nice, wrt Rust: hindsight is a wonderful thing, so they can do a lot of things that C++ can not because it evolved with constraints of bw compat. 
Unicode-aware strings are not enough. You nned a locale too. Strings "i" and "I" are case-insensitively equal in English locale, but the exactly same pair of strings are not equal in Turkish locale. 
You won't get much of a response here. Try /r/cpp_questions. I think it should be something visual, which is why writing a little game is a common suggestion. Maybe a little image processing with OpenCV.
Try making a chat bot for telegram. There are a few libraries out there that work great. You could do all kinds of things. I taught my girlfriend some C++ with a bot that responds with relevant emoji based on the user input ("love" = â¤ï¸). Another idea could be to implement a calculator bot or a bot that stores a list of groceries and sends it back to you if you ask for it...
&gt; rtags + ycmd + spacemacs This is the best option really. It is amazing.
CMake has a bigger community, but I also find meson better and with enough critical mass to improve and maintained for adopting with no regret. That is why I use it. CMake is very frustrating at times. It can do many things and it is still workable, but that DSL made me waste so many hours and still does at times. Also, the cross compilation way is far better in meson. The default targets such as unity builds out of the box, precompiled headers out of the box, coverage out of the box, sanitizers, are things I need to set up with cmake. On the other side, CMake killer feature in my opinion is that it can generate projects for the main tools. I think VS support in meson is already decent, but cmake is the tool that can really do it and you know it will work. This is important for a big number of teams (but not for myself).
Toolchain file is better for that sort of stuff. https://cmake.org/Wiki/CMake_Cross_Compiling#The_toolchain_file
"Thank you for participating in this Microsoft Visual Studio computer-aided enrichment activity."
This is awesome. Count us in as happy recipients.
If you want to know about OO best read Design Patterns by the Group of Four.
Yeah, I realized that after posting. 
Because I'm curiousâ€”what do you mean by "the quite broken Win32 API"? Is there any write-up on what you mean by that? (Using NT APIs directly has always seemed like a bit of a black art to me.)
I agree on all points. Generators in meson can improve, but it seems prohibitively difficult for CMake to achieve the same level of polish that meson has by design.
Have you thought about retrieving the error messages using FormatMessage instead of hardcoding them?
I apologize for not adding the responses on time. I have added now. There's nothing to be lol'ed about, I took the effort to Interview Bjarne and he responded with some nice answers. I agree, nothing new but for the man who has been interviewed so many times, it's hard to find new things to ask. It's too easy to lol, but too hard to do stuff. Anyways I appreciate your feedback. I will definitely work on my questions on future interviews. Thank You.
Thanks for this interesting question. I am still using lot of OO code. At my workplace I have to deal with a huge legacy codebase mostly c++98. Personally I am working with c++11/14 wherever I can. I am very interested in new concepts and I want to encourage everyone here not to just share their experience but also to post some code or links so one can study these ideas in more detail. Thanks a lot
`FormatMessage` doesn't understand `NTSTATUS`.
I argue that all compilers represent references as pointers internally (the ones I am familiar with do) and that all optimizations that apply to references also apply to pointers (because the optimization backend never sees a reference, they get transformed to pointers before they arrive there). Everybody else in this thread is saying that this is not the case. So my question is: which optimizations do references allow _that pointers do not allow_? In your example, if you substitute the reference with the pointer or vice-versa, the compiler always generate the same code. This is not helpful since it supports my argument, but we cannot prove that I am right for all compilers. So what we need to do is prove that I am wrong for one example and one industrial compiler (gcc / clang). Failing to prove that I am wrong, kind of makes my argument right, or at least right in general. So what we need is an example in which clang or gcc are able to optimize a reference away, but if you substitute the reference with a pointer, then the pointer is not optimized away. We probably also need to go one step beyond and show (e.g. by filling a bug) that this is not a compiler bug, but that the optimization cannot apply to pointers for "reasons". This way, we all learn something.
Sorry, I just judged the post on what it had without answers. Next time I would recommend waiting with reddit until the post is complete :)
&gt; It could for any type whose alignment is strictly greater than 1, by using 1 as the magic value. Nice.
Well, it does for me. With FORMAT_MESSAGE_FROM_HMODULE and pointed to NTDLL.dll that is.
Thank you for the clarification! I guess it was an interesting experiment then. I'm not too sad to see it go. A few libraries I tried wouldn't work anyway because they got confused by MSVC and clang #defines and then couldn't compile because they didn't know which compiler they're being compiled with. I think those libraries also never got updated despite bug reports being filed, so the scenario was just too niche (and even for a library developer, the various #defines to do correct compiler detection were very confusing and MS didn't really help with clear information). Thanks! :-)
I honestly don't get why the use double brackets. [[, why not [attr] like c#?
I don't like IF statements at all. They look clumsy, they are biased (some of them evaluate with TRUE only 0.5% of times) and they fuck up your branch prediction. The program isn't stable anymore - thanks to branch prediction, you can't say that some algorithm will always run the same amount of time on the same data. I use IF only in the critical cases. In most times, you can write the same code without IFs and it will look amazingly pretty. 
Please, give us some examples. I am really interested to see samo good practice. 
why though ? the [binaries they provide](https://download.qt.io/development_releases/qtcreator/4.4/4.4.0-beta1/) work fine on arch.
Yeah but VS support for new language features is pretty poor too
Not really. It is very good for last standards.
There's no FormatMessage on non Windows systems either. Being able to interpret those error codes later on any system is nice!
`void warning_suppressed(bool b [maybe_unused])` already has a meaning.
Nice article. I love a good IF statement and writing them, like anything, is an art. Sometimes a simple beautiful branch is all you need. This way? or that way?
You can't say that some algorithm will always run the same amount of time on the same data anyway due to process scheduling, cache misses, page faulting and million other things. Branch mispredict will lose you only 10-20 cycles and is practically free if predicted. Cache miss will cost you much more and is more unpredictable (especially in presence of context switches and scheduling to other cores).
So basically don't try to be too compact when writing your code and only put one bit of logic per line of code. None of his examples were too bad but it seems like a similar problem to those discussed [here](https://www.reddit.com/r/programming/comments/6ogjie/do_people_write_insane_code_with_multiple/) 
can't tell if joke lol
that's not the same clang
I always like to spill long if statements into semantically meaningful locals. It's way better than writing a paragraph explaining the chain of relational and logical operators, unless of course it would be better to do so.
Fine, but I can only come up with some obvious examples, and if they are too obvious, forgive me. So, let's start with the simple stuff. Consider this: if(i &lt; 0) foo(); else bar(); The same goal can be achieved without IFs this way: const auto functions[] = {bar, foo}; const int funcIndex = (int)(i &lt; 0); functions[funcIndex](); //Branchless What's the point? I'll explain later. At first, look at the slightly harder version with the third condition to understand the principle: if(i &lt; 15) foo(); else if(i &lt; 104) bar(); else close(); The same goal can be achieved without IFs this way: const auto functions[] = {close, bar, foo}; const int funcIndex = (int)(i &lt; 104) + (int)(i &lt; 15); functions[funcIndex](); //No branch again Wait, don't throw rocks, I'll explain. Look what you have. You have a number of functions. You need to select a function to invoke. Instead of the regular "if a is zero, invoke this function, but if a is larger than twelve, invoke another function, and otherwise, invoke the third function", you use the simple "Invoke the ith function from these, *where* i is chosen by these rules". See, we have separated invokation and selection. Separation is always a good thing. Q: But "(int)(i &lt; 104) + (int)(i &lt; 15)" part is harder to understand! A: At the first sight, kinda. At the second sight, look at this rule: // 0 1 2 const int funcIndex = (int)(i &lt; 104) + (int)(i &lt; 15) ; You go through the equation from the left to the right, count the number of TRUE evaluated equations and stop after the FALSE evaluated equation or after ;. 0 TRUE evaluated equations =&gt; invoke 0th function. 1 TRUE evaluated equations =&gt; invoke 1th function. And so on. Pretty simple. Q: But in the really big if/else towers the index equations will bee *too* big! A: Don't ever write big if/else towers. They are always can be rewritten into something better. So, let's move forward. There are some branchful common functions: abs, min, max and so on. Well, the general advice is if you have some library that can do it fast way with SSE (SSE has instructions for these), use it. If not, use C abs/fabs, fmin and fmax whenever possible - these functions are *probably* optimized for branchless version. At least the compiler can recognize the use of them and replace the call with SSE instruction. Well, I think it's enough for the starting point (I don't think anybody still reads this and wants more). And OF COURSE, there are some exceptions and edge cases. 
https://www.reddit.com/r/cpp/comments/6omh1c/how_to_make_if_statements_more_understandable/dkilhe7/ ...still can't tell
OOP is a paradigm and not a dogma- at least that's how the world *should* be. If you have a problem and your solution works without sticking to OO design principles, there is no reason to use OOP features. What I love about C++ as a language is that it's up to you whether you stick to OO design or not.
What do you suppose the CPU is doing while waiting for the funcIndex to be evaluated in each of these cases?
As funcIndex evaluation and function calling are separated, I can always defer the function calling. So basically CPU is doing what I tell it to do.
Well, his flair is "Windows API is awesome". But can't tell whether that's serious...
I just noticed that too lmao
That's still branching, you've avoided explicit branching instructions that could be more easily picked up by the branch predictor and speculative execution, and instead replaced it with indirect calls which are harder to predict: two code paths vs the possibility of the entire memory space of the process being a jump target, though modern speculative execution can trace the jump target from being the result of a comparator. Not something you want to assume you'll always have.
Great write-up! Thanks for sharing Guy :-)
Indirect branching (switch statement is another example) is easier to predict. For example, Intel processors since Haswell almost never fail in indirect branching.
Ironically, I modified the webpage to increase the width on the article make it easier to read instead of having to scroll the code.
I really do think that WinAPI is a splendid hidden treasure, but it's too long to explain why is it so cool. After three months of Qt WinAPI wasn't just a leap forward - it was like interstellar travel. Well, maybe after a few hours I'll tell you, I like speaking about pros of WinAPI.
I'm not aware of any publicly available writeup. But it's definitely the case that some Win32 APIs are just plain broken, as in, fundamentally and unavoidably racy. They're also deeply inefficient when fed filesystem paths, going straight to the NT kernel API can double or treble performance when opening files for example, and things like enumerating directories are 10,000x faster. NT has no problem with 10 millions items in a directory for example, it's the broken Win32 API which makes `dir` hang in those directories. They actually enumerate complete with all possible metadata per item in under two seconds.
It already retrieves all the possible messages and hard codes those. If you're suggesting that we simply call `FormatMessage`, sure you could do that too. But the current approach is portable, and I specifically want Linux code to be able to parse binary logs from Windows where the NTSTATUS is dumped to the binary log. That said, a pull request optionally using `FormatMessage` instead probably would not be rejected by me.
It definitely does. It is how we generate the lookup tables.
Just use the binaries and be happy? You can still compile your code with clang-4.0 or any clang version.
You can key the table off state other than just the program counter. Good options, probably: branch prediction history, return prediction stack contents, previous branch addresses, etc. Then you get multiple histories per branch. "Just" more bits for the hash function, and some extra state to store, but good enough value for money that people seem to do it (or something based on similar principles): https://hal.inria.fr/hal-01100647 
&gt; So basically CPU is doing what I tell it to do. hahahahaha no
Yeah, doing pointer addition on a null pointer (or outside of the bounds of a single array allocation in general) is undefined behaviour. It's not guaranteed that `(T*)NULL - 1 + 1 == NULL`. ~~It's not even guaranteed that `(T*)NULL + 0 == NULL`!~~
If you just don't handle errors you can dump the conditionals too. Makes code coverage so much easier. All joking aside I've seen this pattern before used by "old timers" and while I didn't always agree with writing it myself, I did appreciate reading it later when I stumbled into their code. Maybe it's time I changed too.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6onrbg/write_program_on_c_that_load_cpu_and_coprocessor/dkipwl8/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; So what we need is an example in which clang or gcc are able to optimize a reference away, but if you substitute the reference with a pointer, then the pointer is not optimized away. Given this code: ``` int a; int&amp; ra = a; int* pa = &amp;a; void* p1 = &amp;ra; void* p2 = &amp;pa; ``` Taking the address of the reference is the same as taking the address of the object. The compiler doesn't need to allocate the space for `ra`. Taking the address of the pointer `pa` will force the compiler to allocate the space for this pointer. References are not objects. They're aliases. The compiler *may* need to use a pointer to represent a reference, but that's an implementation detail.
No, it cannot, because the standard guarantees lossless conversions between `uintptr_t` and any pointer, on systems where `uintptr_t` is viable/exists. What would you do with `std::optional&lt;T*&gt;` where the underlying value is an arbitrary integer?
for me your call to &gt;apply(v, 0ul); Is defaulting to "std::apply", so either change it to &gt;::apply(v, 0ul); or change the name to something like local_apply I still get a bunch of errors like: optional:1032:27: error: use of class template 'optional' requires template ?arguments template &lt;typename _Tp&gt; optional(_Tp) -&gt; optional&lt;_Tp&gt;; 
The `optional` is a lack of support for class template type deduction. At a guess, you're compiling new libstdc++ code with an older version of Clang (not sure if this feature has even been in a release yet).
I had a quick look and immediately stumbled into this: void push(const std::function&lt;void()&gt;&amp; functor) noexcept { { std::lock_guard&lt;std::mutex&gt; lock(mutex_); functors_.push(functor); } cond_var_.notify_one(); } sticky point is that _noexcept_ on a function... Edit: for those who care -- this function will kill your app if _push()_ or _functor_ copy constructor throws. 
yep, GCC 7's libstdc++ works with clang-5 SVN but not with clang-4. You have to use -stdlib=libc++ for now with clang on linux.
Do you have an example of something that is "fundamentally racy"? I could trawl AFIO looking for comments, I suppose. :)
I link to my [latest codecast](https://www.youtube.com/watch?v=TeeLEfT8nCU) in the blog post that shows how to build a project with Hunter. If you've used CMake before there are only two more statements that you need to understand: `HunterGate` and `hunter_add_package`. The `hunter_add_package` lines let Hunter know which additional libraries your project needs to pull in. And the `HunterGate` line locks down all library versions, so when you try to build your project a year from now it still builds as if it was today. Does that help you out?
I didn't know that. Are there any references or bug reports? Edit: I found https://bugs.llvm.org/show_bug.cgi?id=33488 .
 So, can you use the latest clang with visual studio now? I'd love to use clang and have it just work in VS-- with full debugger and IDE support. I know you can use the C2 thing, but its some old/custom version of clang, so not very useful-- Eh I guess it does not work, I tried http://llvm.org/builds/, and the installer can't find vs2017.
Yeap, depends on the case but I will normally create local variables instead of functions to evaluate a condition in an if. Depends on the reusability of the condition basically.
Been 4 hours already, I'm all ears about your opinion of WinAPI vs Qt as a well-versed Qt developer myself.
ADL is picking up `std::apply` because of the `std::vector` argument, and `std::apply` isn't sfinae-friendly - it's ill-formed if you don't provide something tuple-like. Just rename your function to not `apply` or qualify the call via `::apply`
This approach comes at the cost of possibly making debugging more annoying. The original way, you can easily see what the result is going to be, what is causing it to go that way, and it's easy to force it. Jumping into a function means having to remember more things. I still like it better than some code I saw this morning: if((value &amp; SOME_FLAGS) != 0) { } else { [code here] }
"The uniform course insects would have helped a lot of in juices" Um, wat.
I use CLion and it chokes on finding constructor calls when they happen from behind `make_unique` and `make_shared`, does rtags handle it? It's really annoying when refactoring :(
Well, the trick about `BitPattern` is that *you* (the user) are in control. If you indicate that your type has no unnecessary bit pattern, then the implementation has to use 1 more bit. Otherwise, you are in charge of making the check: up to you whether it's cheap or not. In any case, however, you decide where the trade-off lies, so it always suits your needs best.
N4659 8.7 [expr.add]/7: "If the value 0 is added to or subtracted from a null pointer value, the result is a null pointer value. If two null pointer values are subtracted, the result compares equal to the value 0 converted to the type `std::ptrdiff_t`."
Thanks for the correction!
Where is already stuff in the grammar that can start from single square bracket (lambdas, indexing, etc.), while `[[` is unambiguous.
life is pointless and nothing matters
The audio was not very clear at all, I tried my best to get the transcribe right. May have missed or got it wrong in few places. Will continue to fix.
What do you mean by choke exactly? Mark it as an error? Or find references? It seems like ycmd doesn't flag mismatches from `make_unique`, the reason is probably that the error is considered to occur inside make_unique, not your code. As for rtags, it does find the reference, but again, it finds the reference inside `make_unique`. So if you want to find all `make_unique&lt;Foo&gt;` occurrences it will not work. However, it does distinguish between different constructors. So basically, you can see for a given constructor, if any `make_unique`calls are going to that particular constructor. Not that bad. It's still not perfect, but it's ok and at least it doesn't get stuck on newer features or more complex code, just because. Also worth noting that spacemacs has an awesome interface to text based searches which are ag (or rg) based. So after verifying that a given constructor is used by make_unique, you could quickly find and sort through all of the make_unique&lt;Foo&gt; calls. Edit: rtags actually does have call graph functionality, which is pretty cool. It has a references tree. The problem is, that it also seems to get confused by the make_unique. It correctly finds the constructor reference from inside make_unique, but it won't let me trace back to the make_unique call. For most situations it is fine. I'll maybe raise this as an issue.
When doing a "search for uses", it completely ignore constructor calls occurring inside `make_unique` and `make_shared`: those calls are omitted from the list of all constructor calls returned by the search :( I guess it's a combination of templates and maybe argument forwarding, however it's extremely frustrating given how ubiquitous `make_unique` and `make_shared` are in Modern C++.
Ah, that makes sense. I'm used to `FORMAT_MESSAGE_FROM_SYSTEM`.
The most painful one for AFIO is `CreateFile()`'s racy handling of opening files which are pending delete. Indeed, AFIO v2 reimplements `CreateFile()` entirely, borrowing the implementation from ReactOS, but fixes the race. The other big one is renaming, Win32 does not permit a clean atomic rename which makes writing concurrency safe filesystem algorithms much harder. The NT kernel API provides an atomic rename operation exactly matching POSIX's atomic rename, works lovely.
https://godbolt.org/g/xkGXFj "Branchful" version got fully inlined and have 0 actual branches in asm (and only 3 instructions overall). "Branchless" version does no inlining and an indirect jump. And inlining gives you much more speed than no branches in case of small functions (if they're large you just don't care about branch speed)
This article should have been available to Microsoft C++ engineers who wrote Win32 APIs. All those if(SUCCEEDED(X)) chained together are amazing
I enjoyed listeng to it and look forward to some of your TBD guests. Also enjoyed listening to the previous one with Gosling.
You are awesome. This is one of my few dealbreakers when I interview potential hires. I give a test question that requires a 3-part predicate. Anybody who breaks up that predicate and refers to it by some nice logical label has a gold star for the rest of the interview. Anybody who crams the whole predicate inline gets challenged. If they answer something like "if I was actually writing this I'd simplify the predicate", we're cool, otherwise the rest of the interview stands a good chance of being very short. EDIT: If the justification is that this awkward compound predicate "runs faster", that's an immediate hard pass (unless they're an intern or junior or something)!! :)
Looks cool from the example. Hopefully once executors get into the standard, something like this will be provided as well (or at least trivial to implement). One API improvment I would suggest is to eliminate the difference between final and non-final tasks and support executing any (sub)task directly. Just pass the desired executor into a task's schedule method (or better yet, vice versa) instead of binding them together at task creation. 
You're trying too hard.
this guy compiles!
Obviously, and that option is available for user defined types today, by specialising std::optional. Your example, however, was std::vector, whose std::optional specialisation is unavailable for customisation by the user. The std::optional implementations I've quickly glanced at do not specialise for different types, but if they did, and they chose to do so for std::vector, the same space/speed decision would be made and would be out of reach of user code to alter. For more flexibility, you need a non-standard library type (compare folly::vector vs std::vector for a similar contrast). The OP was asking about std::optional - and so I maintain that it's design decisions are sensible. 
As someone from a C# background, I always point out to my young charges that they don't have an allergy to typing or they wouldn't have made it through university. Concision without clarity is worthless. Type the words, write the boiler plate, do the unit tests. But there we have the difference between C# and C++ in the real world. There has never been a moment in time where my C# applications, services, sites, etc. have been performance-bound such that the CLR couldn't work with code written for clarity and maintenance rather than speed. If you are using C++ you either need speed or a level of functionality beyond the CLR. You don't always have the luxury of long-cuts.
Real programmers only use conditional moves for control flow.
Real programmers evaluate all the branches and produce all possible results simultaneously.
Thank You, means a lot :)
That's a pity! Was just about to setup CI for clang/c2 after finally being able to update all projects to Visual Studio 2017 and fixing most issues.
Use Clang/LLVM instead and I think you'll be pleasantly surprised. More features, fewer bugs, and it works almost perfectly with the STL (one workaround is required for Clang 4.0 not supporting everything that `&lt;atomic&gt;` needs; they've fixed that in Clang 5.0 and I've baked the workaround into VS 2017's second toolset update just in case).
I think libc++ doesn't yet have std::optional though, I think I tried recently and couldn't get it to work in any combination with clang-4.
Thanks! Last time I tried clang-cl, I found it slightly less convenient because of missing LLVM-vs2017 toolset, defaulting to -fms-compatibility, and a linker error when using LLVM-vs2014 I couldn't immediately create a minimal repro for. Then I'll try again :-)
&gt; I tried my best to get the transcribe right You wouldn't type something like that manually, right? You used speech-to-text as a source for the draft? This usually goes really bad as it's too easy to miss nonsense or event offensive phrases.
TL;DR: Class hierarchies in programming languages are abstract concepts that don't have to mirror anything in real-life. But people get stuck on the assumption to the contrary and then write horrible code...
Yes. What I meant is, I might have missed few paragraphs on reviewing and fixing. No, I did not type that manually. Thanks.
Usually it simply means that if you use a derived class where a base class is accepted, you can also use the base class if it's not abstract. Square/Rectangle and Circle/Ellipse can only be mapped to such a hierarchy when they don't derive from each other but merely share a common ancestor.
I STILL can't tell if you are joking after reading 3 comments of yours.
How did you get your head filled with so much nonsense that you both say ridiculous things AND are cocksure about them? 
&gt; I did not type that manually. &gt; I might have missed few paragraphs on reviewing and fixing. The longer you keep doing it, the closer the probability of letting something really bad slip tends towards 1 :/
Are there any slides?
Does this work with error codes from another module, e.g. winhttp?
That seems like a weird deal-breaker to me. A 3-part predicate is hardly unfollowable logic. Also, splitting out 3 conditions into local bools means each condition will always evaluate (unless you prevent it with additional logic for each condition). Those evaluations might cost a lot depending on the code. So I'd argue it's not that clear cut, I'd just prefer to hear their reasoning.
I find encapsulation to be mostly a waste of time. If I could go back in time and tell my past self to just chill with the control issues, I would. Truth is, I have no idea how a piece of code will be used and extended; instead of making up arbitrary rules, I prefer encouraging correct usage by layering convenient api:s on top. Can't remember ever using multiple inheritance outside of C++ class in university. Besides constructors and destructors, I only use methods when I really need polymorphism; otherwise free functions give more bang for the buck by allowing multiple dispatch; and making the distinction means that method-calls may be immediately recognized as polymorphism at the call-site. Don't use exceptions either, automagic stack unwinding rubs me the wrong way. All in all, I'm probably breaking most rules in most books by now; and I have plenty of experience to back each and every decision up. https://github.com/andreas-gone-wild/snackis
"Only thorough unit testing would tell us" Boolean algebra is a thing you know.
I think you are just fine. I do run-time inheritance, sometime, but these times are when behaviors of objects that are determined at run-time. The goal of C++ in to keep your code simple. I would also consider cases where for sections of code where two commonly used interdependent classes or structs and there. I think that these are cases where inheritance can be used
Disclaimer: This is 100% my opinion and I'm human, and I'm flawed, and I'm just some schmuck on the internet, so take this with a grain of salt and not something I believe is a grand commandment that everyone should agree with :) I have yet to encounter a compound predicate that is more readable than at least some kind of logical assignment. To me, maintainability trumps all other concerns in code. I will never work in a less than pristine codebase again, so I don't allow standards shortcuts in any codebase I manage. Also, in my experience, programmers are notoriously bad at knowing what parts of their code run slowly. In my codebases, maintainability is a requirement and performance is a feature. Optimization is an entirely different process than coding. The best coders I have ever worked with (including some game engine badasses) are regularly surprised when they profile code. I'd say programmers are right about bottlenecks maybe 20% of the time, which means pre-optimizing during the coding process is a waste of time and contributes to messiness. In the interview, I am asking applicants to write, not optimize, code after a discussion about how important maintainability is to me. I'm fine if juniors aren't super clear on the difference, but I have no interest in paying senior people to write anything less than perfectly formed code. I've found as time goes on, it's been easier and easier to find people who agree with me and who "get it right" the first time around (at least in Austin). The day someone shows me a compound predicate that is faster for a human to understand then a labeled one, I'll revisit my opinion. As a result, the codebases I have worked in since 2009 have been super pliant, easy to modify, easy to re-architect, and bugs are easy to find and fix. It's changed my professional life. After all these years, I'm absolutely convinced that these desirable attributes of codebases I manage are the direct result of refusing to work with people who, among other things, think long compound predicates are an acceptable day-to-day practice. That's just me and just my experience, though. I don't think less of anyone who doesn't agree, but being strict about readability has worked out really well for me and I'm really stoked about it.
Does MSVC give good diagnostics from enable_if yet?
when will 15.3 be released?
I think in my industry (games) it's just completely unreasonable to expect clean codebases. 80% of our time is spent working with existing code that an ex-employee wrote several projects ago under tight deadlines and isn't unit tested, so the wisdom I've learned in this situation is that you're better off leaving that code alone rather than attempting to refactor for readability later on, simply because that code has been debugged and battle-tested through several rounds of QA, and it's very possible you'll introduce more bugs, even with code reviews. Of course if you're writing new code you should follow best practices for readability and maintainability, but when your industry has everyone from interns to seniors writing code and there are constant shifting goalposts for the functionality and tight marketing deadlines, real code is never as clean as anyone would like. So I prefer to look for engineers that are comfortable working with poorly documented and messy code, even though I expect them to write better code when they get the opportunity. Thanks for the detailed rundown too, that's completely fair enough and 100% agree on engineers not having an intuition for optimisation - I think the more experienced you are, the more you realise you can never really know without profiling. 
The release date hasn't been officially announced yet.
Interesting idea, separating tasks from their executors. However, my goal was to be able to have a single function call to re-calculate the graph. Without this it would be a bit cumbersome because the client would have to somehow go through all tasks manually in the right order. But this is something the framework should do for you as only it knows about level, priority, and ID of each task. We may still be able to have a single task class. A schedule call would then calculate all parent tasks and then the current task. If a parent has no executor assigned it will use the one from the child that called schedule(). I like this idea. 
I have to admit, it feels weird (in a good way) that a point release to VS is so... anticipated. Half the time I feel like I'm reading articles for a "VS 2018 Preview" or something. :) You and the rest of the team have been busy; thank you!
That is an interesting case and in theory, it is of course possible that push() and copy constructor throw. Even the construction of the lock may throw. However, the way this class is used make it extremely unlikely for these things to throw. The functor is only ever a lambda that captures a shared_ptr. I can only really see this becoming a problem when the OS is running out of memory or file descriptors in which case you have much bigger things to worry about. Or am I mistaken here? Are exceptions from this code much more likely than I think? Or am I simply misusing noexcept? 
I don't see the problem with your `Rectangle::changeWidth` in an immutable setting. A square can easily implement this method without harming its own squareness. So, you can pass a `Square` into a function that expects a `Rectangle` and all will be well.
Naming of template parameters in `copy_until` is wrong. You can't define `std::distance` between `__first` and `__last` if they are of `InputIterator` category.
&gt; First, most problems have more than one solution, second OOP is not about polymorphism and inheritance, it's about data hiding and putting code and the data it manipulates together. Is that really the case? I wouldn't really call Rust an OO language, but one can group data and implementation and get nice encapsulation, etc.
See how ugly it sounds: `IndependentlyDimensionedRectangle`, I think that's because you assume mutability by default. If, on the other hand, things are immutable by default, you have much simpler concepts floating around. An immutable `Square` is a perfect subtype of an immutable `Rectangle`, you don't need any word play.
Huh? C++14 [iterator.operations] shows &gt; template&lt;class InputIterator&gt; typename iterator_traits&lt;InputIterator&gt;::difference_type distance(InputIterator first, InputIterator last);
ctor of the lock won't throw (unless it's implementation is bad). There is no likely or unlikely in the coding -- code is either correct or not. function cctor can throw practically anything, not only std::bad_alloc. Lookup "exception neutral" code and similar topics.
I'm not necessarily assuming mutability by default, but if the interface allows mutability then that has to be addressed somehow. So, sure, another way might be to have an NSRectangle, subclassed by NSMutableRectangle and NSSquare. Then NSSquare is separately subclassed by NSMutableSquare so it doesn't have to share the interface of NSMutableRectangle.
Will it solve the problem whereby Visual Studio completely fails to start, with only a dialog saying "unknown error", on laptops that are running Checkpoint Endpoint security? 
You're welcome! We're pushing hard to be C++17 complete this year.
According to the standard the lock_guard ctor throws when mutex::lock throws which is indeed possible: http://en.cppreference.com/w/cpp/thread/mutex/lock This thread pool is part of the implementation details of transwarp and has hence a very specific usage pattern. For instance, push is called only with a function that wraps a lambda that doesn't throw and that only captures a shared_ptr. However, I suppose your point is to never use noexcept unless there cannot be any exception ever, even if the given usage doesn't allow for exceptions at runtime. Fair enough. 
I haven't heard of that problem (as I'm not an IDE dev). /u/spongo2 should know who can help you, though.
MS should unite with Nvidia and give VS first-class CUDA support. To this end, Xboxes can now be fueled by Nvidia GPUs so it's really a win-win. I would then no longer have to live in the world of VSCode as the IDE that doesn't fight me.
I just experimented. They're not bad, in my opinion: error: expression must have class type detected during instantiation of class "regulus::is_point&lt;Point&gt; [with Point=int]" 
Being undefined means the compiler is free to assume it won't happen, so clang is perfectly allowed to assume it can't be null. Both are correct :)
+1 to what SeanMiddleditch wrote, thank you very much! Is 15.3 Preview 4 (or whatever follows it when it's "released") going to be a 15.3 update to the VS2017 (not Preview), and binary compatible? Or how does the whole scheme work? Are the "VS20XX Preview" installs a preview to just the next point release to the current RTM VS20XX version, or will it at any point become the "next" VS, with binary incompatible changes?
The previews definitely excite me too. It's great! Are we meant to be able to seamlessly upgrade C++ solutions from 15.2 to, say, 15.3 Preview 4? 
The problem is rather CUDA's horrible "C-ness". I wish CUDA was more like modern C++ (and safe by default and all that). But you're right, good tooling would certainly help, but it would have to support remote execution &amp; debugging on Linux servers with CUDA cards to be fully useful, as it's often the case that you write code on e.g. a laptop without an nvidia card or even if you have one it'll have 1-2 GB of RAM which is not nearly enough to even launch certain things. So you'll be coding locally and then running on a remote Linux machine which got some heavyweight CUDA cards.
Yes, lock is allowed to throw in vague situations. But it won't (unless you break it's contract) -- throwing lock will make impossible (or very hard) to write reliable MT code. I bet they'll clarify this in C++17. (I typically add to my code smth like "this app works on assumption that std::mutex::lock doesn't throw if used correctly") &gt; I suppose your point is to never use noexcept unless there cannot be any exception ever Not really -- you should use _noexcept_ if you are absolutely sure you want to pay price for not letting any exception to escape and std::terminate() on exception is desirable behavior. You are also forced to use _nothrow_ on certain member functions if you want related STL logic (swap/etc) to take more performing path. Side note: noexcept generally doesn't make any sense on inline functions at all. Details are [here](https://stackoverflow.com/questions/21463119/does-it-make-sense-to-declare-inline-functions-noexcept/39242143#39242143).
According to my vague understanding (on all IDE topics), yes. I don't believe that you need to convert your solutions at all. Note that there can occasionally be source breaking changes due to compiler/library features/fixes, but we try to be gradual about it, and provide escape hatches or guidance when possible. `/std:c++14` (the default) experiences fewer breaking changes than `/std:c++17` which is receiving more new features.
The entire VS 2015 and VS 2017 series (RTM/Updates) are binary compatible. You should still compile everything consistently with the latest released version in order to get all correctness/performance fixes, but you can mix-and-match without anything bad happening. VS 2017 RTM was version 15.0. It received a couple of IDE updates, version 15.1 and 15.2. We're now working on releasing another update, version 15.3, which affects both the IDE and the toolset (this is the first toolset update; we're basically updating the toolset at the same pace as before, but the IDE is updating faster). As VS 2017's rewritten installer has improved things, and we've worked on speeding up our release processes, we're trying to make really sure that the 15.3 update is solid. Therefore, we've been releasing several previews of the update - hence stuff like "VS 2017 version 15.3 preview 4". At some point in the near future, we'll declare 15.3 to be done and final, at which point VS 2017 15.3 will be the latest fully supported version for production use. We haven't talked very much about the future of VS 2017 beyond this update, but I can tell you that there will be a second toolset update (15.x for some value of x; don't try to guess) that will get us very close to C++17 completeness (minus the `&lt;filesystem&gt;` overhaul, and hopefully not too much else). We are also working on a binary-incompatible toolset, in our "WCFB02" branch, with major improvements that we can't do in our normal WCFB01 branch (which has produced the VS 2015/2017 series). The incompatible WCFB02 branch will ship in some form, yet to be determined, but it will not replace the compatible toolset in VS 2017.
Yes. You can even install the preview and release channels side by side and switch between them to try out the preview. Please do if you can everybody. We'd love as many people trying out previews as possible
Do you have a dev community link I could look up? 
It on x86 yes, but other architectures can have NULL/nullptr to be != 0.
I've tried this and it looks like generating CMake .sln and projects with the Preview 4 used as a generator gives me `some properties aren't loaded correctly` error and has my newly-generated projects set to _Windows SDK Version_ 8.1 when they used to be 10.0.1xxxx.0 (can't remember exactly at this point and I can't force CMake to use the non-preview VS2017 to A/B test it), leading to some interesting compilation errors. Repo is [here](https://bitbucket.org/HateDread/rtg), generating from CMakeLists.txt at the root level.
Great library, I was actually wondering how to write the equivalence functions for a third party library at work, will take a look at your library when doing so. Curious about something. In your equivalence tests you are checking both that the error_category are equal or that their names are: if(*this == theircond.category() || !strcmp(name(), theircond.category().name())) ... -- https://github.com/ned14/ntkernel-error-category/blob/5890532151a50ef1172843a36f620852e36d23e2/include/detail/ntkernel_category_impl.ipp#L68-L72 It is my understanding that `std::error_category` has a contract of being uniquely identifiable in a process. It feels like a bad idea to not rely on this and to provide an API or "compilation mode" that would not guarantee this. If I want to provide an equivalence test to the ntkernel your library, does it means I also have to perform the comparison in the same way as you instead of just equality comparison? Is it actually possible for a header-only to provide standard-conforming `std::error_category`?
nope, just the podcast. Although there are some references on their website to talks of him: http://cppcast.com/2017/07/gor-nishanov/
&gt; Given this code: [...] Taking the address of the pointer pa will force the compiler to allocate the space for this pointer. Given that code both clang and gcc produce the exact same assembly independently of whether one uses pointers or references. I've tried hard to make them spit different assembly without any luck. Maybe you can provide your code that does produce different assembly for pointers than for references? &gt; References are not objects. They're aliases. The compiler may need to use a pointer to represent a reference, but that's an implementation detail. Nobody is arguing about this. What is being argued is that when an industrial compiler sees a reference T&amp;, the first thing it does is transform it into a pointer T*, and then it treats it as a pointer all the way down to code generation, resulting in pointers and references generating the same exact machine code. Whether that pointer/reference will need storage or not depends on the optimization pipeline, but whether it is a pointer or a reference doesn't affect the outcome of that. Since one cannot prove that this is the case for all compilers and all inputs, I said that: &gt; what we need is an example in which clang or gcc are able to optimize a reference away, but if you substitute the reference with a pointer, then the pointer is not optimized away. In the days of gcc.godbolt.com it is trivial for anybody with such an example to provide the evidence. That site also allows everybody to play with the code and get a better intuition of when an optimization triggers or not, and also is great for filling compiler bugs. 
It's called bikeshedding. /u/RjakActual is spending so much time worrying about if statements and they feel as if they're getting more done than they actually are. For example &gt; I have yet to encounter a compound predicate that is more readable than at least some kind of logical assignment. This is a red herring, they both tend to be reasonably readable, so agonizing over whether or not one is more readable than the other is just not useful. And don't get me started on the whole dealbreaker with respect to hiring. This is why job hunting is so shitty, you have people dreaming up these stupid ass tests because they think the decision making around an if statement is somehow indicative of that person's ability as a software developer.
It will compare with error conditions from any other error code category which implements comparisons to `std::errc` as the C++ 11 STL always tries in its comparisons to do a map to `std::errc` as a fallback.
Hey that's an awesome amount of information and details! Thank you very much! I'm of course already familiar with the binary compatibility of VS 2015 and 2017, but as you figured and perfectly answered, my question was aimed at VS 2017 vs. 2017 Preview vs. beyond. Thank you, and awesome work from you and your team. I'm looking forward to the final/production 15.3 release and then the WCFB02 branch preview! :D
&gt; It is my understanding that std::error_category has a contract of being uniquely identifiable in a process. It feels like a bad idea to not rely on this and to provide an API or "compilation mode" that would not guarantee this. An `error_category` which is not unique within the process presents problems if `operator==` is used as the C++ standard uses address comparison. Last time I looked, the only STL which 100% guarantees that `std::error_category` is a unique singleton in the process is Dinkumware's. &gt; If I want to provide an equivalence test to the ntkernel your library, does it means I also have to perform the comparison in the same way as you instead of just equality comparison? If you compile as shared library, and do not use `RTLD_LOCAL` in `dlopen()`, it ought to probably safe to do just address comparison. But as a library author you can't control your end users, whereas if you are an end user you can sometimes enforce not doing stupid things. &gt; Is it actually possible for a header-only to provide standard-conforming std::error_category? I don't believe so, no. I've informally spoken with a few WG21 members about this. They generally dislike singletons altogether precisely because of these sorts of problem. Fixing it at the linker level would be tricky, `thread_local` was already bad enough. It could be that Expected/Outcome/whatever will provide an alternative to custom error code categories for many header-only users, but until then we have what we have in the standard. (the other thing which could be done is to compare error category strings for equivalent like I do, but nobody on WG21 I spoke to liked that "hack")
Here's your example: volatile int x; static volatile int&amp; rx = x; static volatile int* volatile px = &amp;x; int main(int argc, char** argv) { rx = argc; *px = argc; return 0; } (you cannot attach a second volatile to `rx` *because it conceptually does not exist*) and the resulting assembly, which proves that a pointer *must* get allocated storage, but not a reference. main: mov dword ptr [rip + x], edi mov rax, qword ptr [rip + px] mov dword ptr [rax], edi xor eax, eax ret x: px: .quad x EDIT: this whole discussion is an academic exercise. It's not about what compilers *do*, but what they're *allowed* to do. gcc's ever more aggressive optimizations have broken programs exactly because they made assumptions like you're doing now.
Thanks for the example, you can play with it in godbolt here: https://godbolt.org/g/uBEqpE Note that you have a reference to a volatile, but a volatile pointer to a volatile variable so that your pointer and references programs are not equivalent. The semantically equivalent change is replacing `volatile T&amp;` with `volatile T*` and in this case both programs generate the exact same assembly. That is, your example does, again, prove that the compiler generates the exact same code independently of whether you use a pointer or a reference. We still need to find an example for which this is not the case. &gt; (you cannot attach a second volatile to rx because it conceptually does not exist) and the resulting assembly, which proves that a pointer must get allocated storage, but not a reference. The only thing this proves is that the program you wrote using pointers cannot be rewritten to use references, but since the programs are not equivalent, its a bit off topic. &gt; It's not about what compilers do, but what they're allowed to do. Compilers are allowed to optimize the storage of pointers in the same cases in which they are allowed to do so for references which is why in all the examples that have been shown in this thread, whether one uses a reference or a pointer is irrelevant for the generated assembly (and why compilers represent references internally as just pointers).
&gt; Note that you have a reference to a volatile, but a volatile pointer to a volatile variable. I know, that was the point and you're completely missing it.
Is that even using std::enable_if? As the error looks like you are missing a type in `is_point`, not in enable_if. Either way the message is not good at all, it should let the user know that the overload was disabled by enable_if which is much clearer than what's written as that seems only C++ experts would understand.
The point is whether given a program written with references, does changing their types to pointers alter the generated assembly? The program that you wrote with references can be expressed with pointers by just replacing `&amp;` with `*`, and in this case, the exact same assembly is generated independently of whether you use pointers or references. The program that you wrote with pointers cannot be expressed with references. It generates different assembly, but so what? So does any other program using any other set of types. This doesn't prove anything useful for this discussion. This doesn't disprove that compilers can replace `&amp;` with pointers internally, which is exactly what they do.
vs 2017 already support remote execution &amp; debugging on Linux if you choose linux c++ development workload
I know but this comment thread is about CUDA, does it really support debugging CUDA code on a remote Linux machine? I didn't try but I very much doubt it?
Yea the first problem is known both to CMake and VS teams already and I think CMake added a workaround to their code a couple days ago while the VS team will probably push the proper fix with the next update. This is it: https://gitlab.kitware.com/cmake/cmake/issues/17041
Look, you can do OOP in almost any language. Some just make it easier than others. GTK+ is famous for doing it in C, complete with polymorphism and virtual calls. There is CLOS for Common Lisp. The language may have *syntax sugar* for OOP but it's not needed. OOP is about how you organize your code and how you think about the problem, not how the programming language works.
Yes, this one: https://developercommunity.visualstudio.com/content/problem/31263/vs2017-fails-to-start-with-unknown-error.html In my case, it complains about package DSLTextTemplatingRegistry_x86.pkgdef. I can open this file in notepad using the same account. The version of VS I'm trying to start is 15.0.26430.16. My machine is an up to date Windows 7. A copy of the vile abomination known as Checkpoint Endpoint security is installed (it gets mentioned in quite a few questions about VS failing to start). My access rights on the machine are limited, and while the option to run VS as administrator is present, I'm not quite sure if selecting it will get me any additional rights. Certainly it doesn't help in starting VS; it ends with exactly the same error. The issue on the linked page is marked as closed in 15.2, but I have a laptop here that evidentally didn't get the memo... 
Cool, thanks! I wasn't too worried about the message, more my change in Windows SDK Version - manually changing it back for every project that gets generated would be quite the pain. I had to uninstall the preview so that the 'Visual Studio 15 Win64' generator would use normal VS2017 and not the preview. A shame :(
for that specific use i don't know
[Relevant image](http://i.imgur.com/FzSTkQj.png)
I think the issue of which VS2017 installation CMake picks (Preview/non-preview and from which directory) is fixed as well here: https://gitlab.kitware.com/cmake/cmake/issues/16846 The CMake guys are usually very slow adopting to new VS versions, probably nobody of the core devs uses Windows or definitely nobody is an early-adopter. However once you report these issues they usually get fixed quite quickly.
In code: template &lt; typename InputIterator , typename OutputIterator , typename Predicate , typename Distance &gt; InputIterator copy_until(InputIterator __first, OutputIterator __result, Predicate __pred, Distance __len) { auto __last = __first; for (; __pred(*__first) &amp;&amp; std::distance(__first, __last) &lt; __len; ++__first, ++__result) { *__result = *__first; } return __first; } You definitely can't use `__first` after `std::distance`. Because there is no multipass guarantee for [InputIterator](http://en.cppreference.com/w/cpp/concept/InputIterator) as opposite to [ForwardIterator](http://en.cppreference.com/w/cpp/concept/ForwardIterator): &gt; Unlike InputIterator and OutputIterator, it can be used in multipass algorithms. What do you think, how `std::distance` can be arranged? What a magic can allow it to be suitable for your `copy_until`? 
Yeah, the code I showed was the falsifier in an enable_if template signature. I just created a small [example](https://godbolt.org/g/XsZ83u) and you're right, the `enable_if` failures are pretty much useless. Bummer! The good news though is that it still fundamentally works which is honestly all that matters.
I won't disagree, there's quite a bit of C-like CUDA out there and it's honestly awful. However, if you manage to leverage Thrust correctly, your code can require few, if any, custom kernels. The more Thrust you can use in your code the better, as it's partially written in CUB and the CUB people are _amazing_. It'd also be pretty cool if first-class OpenCL support was also given to VS.
&gt; Yes, lock is allowed to throw in vague situations. This reminds me some _pure shit_ codebase i had pleasure to rewrite (well, refactor). It used following pattern to lock its mutexes: while(ptread_mutex_lock(&amp;mutex); I dont know, dont ask.
I'd like to make one clarification to both /u/sumo952's question and /u/STL's response: Binary compatibility is NOT determined by VS version, it's determined by MSVC toolset version. VS 2015.x and VS 2017.x shipped with compatible versions of the toolset: v140.xx in VS 2015, v141.xx in VS 2017 (as well as v140.xx to help with code migration.) These are compatible because they have the same major version number of 14. 1. We will be updating the v141.xx toolset sometime in the VS 2017 timeframe with a mostly-standards-conforming version. 2. We will also one day release a new, incompatible version of the MSVC toolset from the "WCFB02" branch that /u/STL mentioned. Because it's incompatible it will have a new major version number that is larger than 14 (possible 15 or 20.) The key here is that we can ship 1 and 2 in any version of VS. They might come out with a VS 2017 update (#1 almost certainly will.) They might come out with VS 2018, or VS 2019. Every modern version (VS 2015+) of Visual Studio is binary compatible with every other version because you can install and use the MSVC toolsets independently in any modern VS. If you want a compatible one, install one from the same major version family. If you want all the new hotness, install from a new major version. 
We work pretty closely with NVidia. If there are specific issues where CUDA needs better support, let me know and I can forward to the right people in VC++ or my NVidia contact. 
So do I, of course, I actually try FROM_SYSTEM first, and if it fails, continue to probe known DLLs for a message. See line 64: https://github.com/tringi/emphasize/blob/master/Resources/Resources_ErrorMessage.cpp
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [tringi/emphasize/.../**Resources_ErrorMessage.cpp** (master â†’ 28e74ea)](https://github.com/tringi/emphasize/blob/28e74eab53d536f5afc4087d1e55201310da76c3/Resources/Resources_ErrorMessage.cpp) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkks0ml.)^.
I had to double check the documentation that it's indeed supported to call it with FORMAT_MESSAGE_FROM_HMODULE | FORMAT_MESSAGE_FROM_SYSTEM. Nice shortcut.
Damn, I need this... Where did you start with reimplementing node? node.h/cc files and libuv? I find the first version of node.js quite readable but the project already gets quite cluttered at version 0.9.0, where they first used libuv, Can't imagine going through the last version of node.js...
Oh, I see. I hadn't looked at the code in the repo, I thought you were saying that `std::distance` simply didn't work with input iterators. I agree with your assessment. :-]
Unfortunately Thrust doesn't seem to be developed anymore, the last release is from March 2015. Cub looks cool in principle I think, I haven't heard of that effort yet! It doesn't look like a library for the everyday-gpu-programmer though... I have no idea how to use it (or even what exactly it does) from the Readme.md.
Is this defined in the standard or implementation dependent?
I don't know if C++ standard has details about memory layout of objects, but here is the very old paper by Bjarne Stroustrup about the implementation of multiple inheritance. Section 4.4 (Casting) is specifically about offset adjustment: https://www.usenix.org/legacy/publications/compsystems/1989/fall_stroustrup.pdf 
I have started by copying the API from documentation (version 4.0.0) implementing some C++ classes around libuv. The first classes I have been implementing where the non-stream based ones (UDP etc.) with pretty consistent behaviour. Then I've been starting reimplementing the stream based ones, notably TCP / net.Socket but I immediately realised that it was impossible to replicate exact behaviour without doing a source to source port of the stream class, which I did in a later rewrite. The same was done for the HTTP classes (IncomingMessage, OutgoingMessage etc.) that I have done source to source as well ignoring some features (Http agents and cluster module for offloading sockets to other processes). When I say source to source port I mean replicating the "logical sequence" of what was done in the code with something logically equivalent. Porting line to line between js used in node and C++ is otherwise very hard. I have then started copying some tests from the test suite to make sure that the api was behaving consistently. I have not been touching the core of the library since the 4.0.0 days, just been bugfixing stuff. Just out of curiosity some months ago I have been checking how much was changed in the latest version of node and for sure it would not be trivial to stay up to date with latest versions. What I would do is continue implementing the test suite to make sure that I can catch changes in behaviour to the external rather than implementation, as many of the changes and optimisations done internally to node.js are often Javascript specific and make little sense to be ported to C++. FYI I have been giving a talk on the subject at the Italian C++ conference in Milano not so long ago. You could read the slides at my GitHub (https://pagghiu.github.io/dev/A-Node-Like-Api-for-C++-italiancpp2017-en/). The talk is in Italian and the video is still to be posted online. I hope to add at least English subtitles to it, but the slides are in English if they are of any interest for you. 
&gt; C++17 std::string_view should now be the default (not only) type used for function parameters that want input string data Except when somebody expects a C-style string (ending with `\0`). Somebody wrote a blog post about this issue. `std::string` is pointer + length + `\0` at the end, in principle. But `std::string_view` is two pointers, in principle. These two principles are not the same.
Nsight is pretty weird to use. I use VSCode because in many ways it's more straight forward and easier. 
As far as I'm aware, the claim in the article that base classes must be laid out in inheritance order is false. 
Thank you for learning me this !
&gt; MS should unite with Nvidia When it comes to developer.nvidia.com, your statement can be translated as: MS should give NVidia a lollipop and a fuzzy teddy bear and a warm blanky and hold its wittle hand and show this $2B company how to write software. When it comes to developer tools, NVidia couldn't find it's own fundament using both hands and a flashlight. 
That's like asking what compiler flag you need to make (NaN + 4.0) / 2.0 be equal to (0.5 * NaN + 2.0)...
I'm reading the part of the thread with art Leonard (vs ide dev lead) as him saying it's fixed in 15.3 not 15.2 I don't have access to the handy dandy version number list on my phone. Can you try to install the preview build (sxs) and let me know if that works? 
I'll also add Scott Meyer's [Effective C++ for Embedded](https://www.artima.com/samples/effCppEmbNotesSample.pdf), which describes the implementation and memory layout of multiple inheritance, among other features, and which matches your nextptr question and answer. Though, Scott Meyer also had this disclaimer up front. &gt; Compilers are allowed to implement ... in any way they like. There is no mandatory â€œstandardâ€ implementation. The description that follows is *mostly* true for most implementations.
&gt; the C and C++ standards are out of touch with how modern hardware works. C and C++ aren't just for modern hardware...
Standard does not define this. I would be surprised if standard spoke of virtual pointer tables at all. Reminder: C standard does not speak of calling conventions or alignment, without which one can't define an ABI. And it couldn't, possibly, not unless it wants to be hardware-dependent (or otherwise platform-dependent). Similar with C++. Things are not specified for reasons of platform independence and performance implications ("if X is defined as [definition here], it runs slower on platform Y" is a general no-no). 
Your post has been automatically removed because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/6ozrkx/i_need_help_with_doubly_link_list_i_dont_get_why/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
The standard says (N4659, Â§[class.derived]/5): &gt; The order in which the base class subobjects are allocated in the most derived object (4.5) is unspecified. 
Well, libclang does not expose the full AST for c++. I have never found any vim plugin based on libclang, and please make me happy and prove me wrong, that works inside templates implementations. Not even [rtags](https://github.com/Andersbakken/rtags/issues/813).
More pertinently , there is a lot of different modern hardware around. E.g. ARM vs x86. 
Argh is a pretty capable but minimal lib: https://github.com/adishavit/argh
&gt; My main problems with an object centric design is that bundling your algorithms with data seems to get "viral" really quickly, classes tend to grow very easily and become very hard to shrink, **THIS IS WHY WE NEED UFCS, PLEASE PLEAASE PLEASE EVERYONE GET BEHIND THE PROPROSAL** then we can refactor code between class-methods/independant free-functions in decoupled modules more easily, without having to prematurely bake one path or the other into the syntax a lot of the draw to OO is the IDE dot autocomplete , which makes 'member-functions' much easier to find beyond that some other languages show features for extendable types in ways that are superior to class inheritance.. UFCS would be the best way to retrofit this into c++ (but there's traits/extention methods etc) &gt; my own possibly wrong coding style you should never think your coding style is wrong: Ommisions in C++ force sub-optimal workarounds. as the language evolves, the 'right way to do things' should become more obvious. inheritance is indeed problematic and doesn't quite handle the situation which other languages fix with 'extention methods'.
On windows, visual assist provides a similar feature - change signature, which would modify all related declarations and definitions from derived classes, though you still have to manually fix the body due to type changes. But actually, I feel that if your problem cannot be solved by simple text replacement, your code is in deeper trouble.
&gt; you simply cannot always substitute compile time polymoprhism for dynamic polymorphism. not sure it really achieves that but rust takes an interesting step of making traits related to both ('instantiate a trait object, or use as a template bound') . I think haskell might ? (i read that it assumes vtables, then recursively specialises to elide them where its knows which type is used..) . in both cases the assumption about the vtable location is different.
&gt; "your code is in deeper trouble." well the 'deeper trouble' is really that *'in 2017, header files still exist'*, but hopefully the modules drive will get there.
Modern IDEs (sometimes along with the help if add-ons like visual assist or resharper) provide many refactoring features, one of those is typically to change a method signature. Often this also checks not just in cpp/h, but also for code that uses the method and therefore need to change parameters too. C++ is way too complex for basing these kind of changes just on diffs.
Probably bugs in the code or pthreads implementation. ptread_mutex_lock can only fail in very well defined [situations](http://pubs.opengroup.org/onlinepubs/9699919799/functions/pthread_mutex_lock.html). It isn't too hard to structure your code to avoid them.
I'm on Gentoo, and even I won't build it from source :-P I did in the past, but the system-installed Clang can result in weird bugs. I recommend to just install from the binary, which has a bundled clang and qbs. The bundled clang is **not** used for compilation. It's only used for the code model and static analysis. (And you can still opt to use the system installed clang for static analysis.)
&gt;Probably bugs in the code or pthreads implementation Nope. Fresh code written in ~2014, on modern Linux. &gt;ptread_mutex_lock can only fail in very well defined situations. Yup, and none of them include situation where retry makes sense.
the idea of guiding it via diffs appeared to make such a tool available for command line builds, and the suggestion is not purely diff based - it would definitely have to use 'some other analysis' e.g to figure out what is a class , what is a function definition , declaration etc.
The site is nearly unreadable on the IPad. Is it so difficult to use HTML and not Javascript to write a website?
Anything that makes the language fundamentally more powerful is welcome in my opinion. Surely the whole point of meta classes is that they don't just solve one problem. I don't see how comparing meta classes to C++/CLI or C++/CX is in any way valid. Those are Microsoft technologies, meta classes are not. Frankly, C++ needs some ground breaking features to stay relevant. If you're not evolving, you're dying.
So you bring shitty progressive politics into the C++ world. How great. I considered going to this conference but since you made it political last year I would not go even if you payed me.
&gt; splitting out 3 conditions into local bools means each condition will always evaluate No. 
As someone who has done a lot of maintenance: keep doing what you do.
Why? The underlying type of a rectangle could be Square. 
There are no mature c++ codebases that I know of that use all these, however this is not a problem of the language, which offers the right tools for the job. 
The fact that they don't exist shows that the tools are not fit for the purpose as the tradeoff for using them is too heavy.
Yes. Unless you include the part of my sentence in brackets.
&gt; does the already exist [yes](https://vid.me/ML2Ik)
&gt; if your problem cannot be solved by simple text replacement so does *all* of your variables in your whole codebase have different names ? what do you do if you want to replace a loop index from `i` to `j` ? for instance here replacing the index of the first loop : for(int i = 0; i &lt; 123; i++) { ... } for(int i = 0; i &lt; 456; i++) { ... } already cannot be solved by "simple text replacement"
https://ideone.com/OCIruf So I tried. There is indeed always a runtime error. I would have expected the optimizer to optimize out the computation of condB if condA pass, because otherwise condB is unused.
Is there a proposal for UFCS to be added to the standard ? Would it not backwards compatibility in certain cases or add an extra layer of complexity&amp;slowness since now the compiler has to decide if obj.f() is a call to a method or a free standing function somewhere ? Further more, is there any way one can "get behind" a proposal ? Other than prototyping it in a home-brewed compiler version ? I mean, I'd be all for that, it would make code look much nicer and encourage data and algorithm separation but... I always assumed it only members of the committee that influence these changes... and it seems that most of those who would be large advocates for these type of changes have moved on to D and the current committee is in favor of not "bloating" the language for the sake of nice syntax.
I can't upvote this enough lol!
Are your class names / function names typically reused as local variables? I thought OP was talking about keeping those in sync between headers and sources. As for renaming, most IDEs are good enough to do that already. So what was the problem again?
What exactly is wrong with just using the newer, safer versions? What problem would this solve? 
&gt; Is there a proposal for UFCS to be added to the standard ? yes by bjarne himself and herb sutter, but rejected by the f***King standards committee &gt; Would it not backwards compatibility in certain cases they claim their proposals are backward compatible, because of their prioritising; (but maybe you're worried about SINFAE changing?). I personally suggest a compromise which requires 'explicit this' (like C#) to enable a function for UFCS, as a compromise for the people who don't want it .. i.e. *no existing functions would be enabled*. &gt; or add an extra layer of complexity&amp;slowness since now the compiler has to decide if obj.f() is a call to a method or a free standing function somewhere ? I'm not sure that would really hurt: it must built some sort of table of scopes quickly indexable by the symbol; the only case where there's *extra* search going on is if functions shadowed inbuilt methods, but I think through communication those cases would be eliminated (e.g. UFCS would be used to clean up 'kitchen sink' classes, or vica versa library writers could absorb the most useful 'extention methods' and over time users would eliminate theirs) &gt; large advocates for these type of changes have moved on to D I've never personally considered D .. isn't it designed to be primarily GC'd ? .. whilst I did consider Rust (being 'more different' there was more incentive to try it.. much better type inference etc). lots of us are captive to C++. Rust has many features that I like but I find I can still 'get stuff done' in c++ more quickly.. and it's all about existing sourcebases, momentum. I'm utterly shocked that people would argue against a compromise such as 'deliberate opt-in' , which would surely let both sides get what they want. If it's a clear syntax like 'explicit this' it could easily be banned in projects who's maintainers dislike it. we could even have it guarded by a compiler flag. r.e. 'private/public' etc I don't suggest giving UFCS private access. Crazily, some people have permanently associated ```a.foo(b)``` in their minds with 'evil OOP' , instead of just seeing it as 'a common way to put a parameter upfront' - and there's many languages now that have it *without* pure class-based OOP. I keep saying with this we can have our cake and eat it.. get the nice dot-autocomplete and chaining, *without* the coupling hazards of OOP. I would use this syntax everywhere if I could; i like having created the option to move things into objects later *if it is found to be beneficial* .. the key thing for me is the code being in a fluid state, because designs /best practices always evolve over time..
the problem is: **In modern C++, the inbuilt, neater syntax is for things you're not supposed to use.** ; that is a **waste** it should be the other way round - the compact, easy to write syntax is for the cases you should use most often, and progressively longer syntax should be reserved for 'rare occasions'. The smart pointers **look grotesque** compared to inbuilt syntax. (mess of angle brackets, nesting of critical information makes it look more secondary). Rust was amazing back in 2014 with it's sigils handling the common cases in a way that 'melted away' almost as much as * [] , letting your mind focus on reading the **program specifics**, rather than **verbose markup of trivialities**. e.g. *T should be 'unique_ptr' or something else more useful, whilst you have to write ```__raw_ptr&lt;T&gt;``` to do the unsafe stuff. (I gather some say ```*T``` has a valid use as 'optional reference', well, lets make ```*T``` equivalent to some sort of dedicated ```opt_ref&lt;T&gt;``` then ? but if its programable, we can discover by experiment what the best use is.)
I understand what you're saying now. But I disagree that it's an issue. The types in question live in the library - this was a very deliberate choice. They cannot, therefore, leverage special language syntax. Slightly longer wording, angle brackets etc. do not offend my eyes or affect readability, for me personally. Nor do I ascribe order of importance to language features based on brevity of syntax. I get why you might, but the fact that you do seems to be the real issue here. 
Yeah, the point I was getting at is if you have the expression "if (a &amp;&amp; b &amp;&amp; c)", where a b and c are some complex boolean logic, when a evaluates to false b and c won't be evaluated. If you evaluate a b c as local booleans before the if statement, they all get evaluated regardless (unless you check the value of the previous boolean as part of it). So it's less efficient when a b and c are doing nontrivial logic, and as a refactoring for readability it can change the behaviour of your program unless you're careful. So that's why I said it depends on the situation, and I'd rather just hear an engineer's reasoning one way or the other instead of enforcing a blanket rule that one way is always better.
I don't mean for that to sound accusative, by the way. I am just of the opinion that simple brevity isn't a justification for adding a new language feature. 
I doesn't seem like modules will remove the necessity of splitting method declaration/definition.
TIL. It probably happens because of ;
Though I don't believe there is a feasible way to add this, I must say that the very same argument can be said for not having operator overloading. What's wrong with using `.get()` instead of `[]`, `.out()` instead of `&lt;&lt;`, or `.pipe()` instead of `|` ?
The syntactic sugar provided by operator overloading is more sophisticated than just brevity in terms of characters typed. For example, it's much easier to type and read a chained list of `operator&lt;&lt;` calls than a bunch of nested `.pipe()`s. Also, the reader has an understanding of what `a[i]` probably means, regardless of type, whereas `.get()` could mean anything. The same goes for other operators, there is a conventional meaning and a richness that is lent to the code. 
&gt; Your example doesn't even make sense because arithmetic on NaN always yields NaN. And yet NaN never compares true to NaN. &gt; In a system programming language, I should be able whatever makes natural sense for my target, and more importantly I should also be able to do things like bit twiddling and reinterpretation as much as I want/need. Then do it with an integer type. You have never been able to do math with the null pointer value in C++, this isn't some new "committee-based nuance". That's not just a hardware thing either, the *semantics* of it don't make sense. Your question would make more sense if you were talking about problems with the null pointer being converted to an integer type (thus preventing you from "bit-twiddling") but the code in this instance was always improper.
Yeah, that is a perfect answer to why OP wants to have array and pointer overloading.
`T&amp;` can also point to garbage.
&gt;&gt; I get why you might, but the fact that you do seems to be the real issue here. The most convenient things to read and write should be the correct choices. doing increasingly dangerous/un-performant/un-recomended things should require typing more. using the best practices should require typing less. Surely the purpose of languages and tools (like computers) is to **make life easier** &gt; Slightly longer wording, angle brackets etc. it's not *slightly* longer, it's *a lot* longer, and more like 2 extra mental steps because it's a word *and a nesting level*. Those two extra mental steps should be conserved for **useful** information. More intelligent defaults will save us time and effort.
&gt;&gt; For example, it's much easier to type and read a chained list HAHAH *exactly* how you dismiss my suggestion - "it doesn't matter! it's just syntax, it's just a little longer. who cares about the extra nesting of those brackets."
Oh dear. I'm sorry, but I don't engage with people who start messages with all caps fake laughs. I just don't. 
depending how well templates/inference works, another 'better use' for the short syntax might be *infered* pointer and sequence/vector types, e.g. what if *T meant 'any smartpointer wrapping a 'T', infered from context', (a template template parameter / HKT) , and similarly '[]' meant 'any randomly addressable type. Obviously that would tie into future concepts ('a concept for a smart pointer, a concept for a randomly accesible collection class') Something like ... void foo(T* x, T* y) {...} // *T can be 'any smartpointer inferred by context..' void bar(T z[]) { ... } ---becomes--&gt; template&lt;template&lt;class T&gt; PTR&gt; // combined with future concept bounds on 'PTR'.. void foo(PTR&lt;T&gt; x, PTR&lt;T&gt; y) { ...} template&lt;template&lt;class T&gt; ARRAY&gt; // combined with future concept bounds in 'ARRAY'.. void bar(ARRAY&lt;T&gt; z) { ... } // see also proposals for 'auto' parameters and using 'concepts' as parameter types
So how would you actually implement std::array? After your definition, it's internal implementation would in fact be recursive... Also, what would "something safe" for pointers look like, in your opinion? Would that be unique_ptr? Shared_ptr? Something else, whihc we don't even have yet? 
I guess this will be a disastrous change to ABI.
And what about the case when I want to *actually* use pointers and plain arrays? And how this would work for variable declarations? And what advantage does this have compared to void foo(Pointer x, Pointer y); void bar(Container z); , besides changing the meaning of existing syntax and being less clear? EDIT: I am not saying that having nicer syntax for smart pointers won't be good, but I believe that it is not worth increasing language complexity just for that.
Operator overloading doesn't change the meaning of existing programs, though.
&gt; And what about the case when I want to actually use pointers and plain arrays? ... according to modern c++ that should be a minority of cases; my suggestion was to have 'intrinsic types' which recover those __raw_ptr&lt;T&gt; = old *T , something like that. &gt;&gt; , besides changing the meaning of existing syntax and being less clear? these could be replaced with things that are behaviourally equivalent, but safer (e.g your T foo[N] actually gives you a bounds check in the debug build,) or more convenient (helper methods) &gt;&gt; And what advantage does this have compared to The idea is to free up the *shortest syntax* for the *most common case*,whereas at present we tell people 'ok *T is horrible! don't use it! write unique_ptr&lt;T&gt; instead..' doesn't that strike you as inefficient???
it's a syntactic change, e.g. old foo(*T) === foo( __raw_ptr&lt;T&gt;) etc. old code should link fine. new code should be callable by old code. see ideas like SPECS which propose even more radical syntax changes whilst still being compatible.
&gt; the inbuilt, neater syntax is for things you're not supposed to use. I don't know about that. Personally I use plain pointers and arrays more often than smart pointers and std::array. To think of it, I rarely use std::array. And, unless I want to transfer ownership, I pass around plain old pointers.
&gt; So how would you actually implement std::array? After your definition, it's internal implementation would in fact be recursive... no; there'd be an 'intrinsic alias' of the original raw array and raw ptr, maybe __raw_array&lt;T,N&gt; __raw_ptr&lt;T&gt;, whatever the best name is . &gt; Also, what would "something safe" for pointers look like, in your opinion? Would that be unique_ptr? Shared_ptr? Something else, whihc we don't even have yet? well herb sutter says "T* has a legitimate safe use case in modern C++ as a maybe-null reference." , so one possibility is assigning T* to 'a smart pointer which represents a maybe null reference (unusable in any other way)'. Another more radical suggestion would be to allow it to become a HKT templated smart pointer, (see the proposals for 'auto' or 'concept' parameters) e.g. void foo(T* p) could be replaced with ```template&lt;template&lt;class&gt; PTR&gt; void foo(PTR&lt;T&gt; p)``` (but with concept bounds in future, anything that has semantics of pointer types). 
Why is that?
&gt;&gt; Personally I use plain pointers and arrays more often than smart pointers and std::array. Thats the problem I want to fix. The 'raw types' are supposed to be used *infrequently* (they're tools for building smart pointers and collection classes). The semantically rich templated types with safer restrictions and automatic behaviour are supposed to be the defaults for most of the code we write. '*T' can mean (and do) almost anything .. you're supposed to communicate the narrowed down specific use case formally in a machine checkable way to reduce the chance of error .. prevent functions being called in inappropriate ways. I like arrays that do bounds-check in a debug build. (over in rust they think *all* arrays should *always* do bounds checks but thats going too far)
&gt; You're living in a perfect world. no, I'm living in **hell**. I want to **create** a perfect world. Amongst other things, one barrier to modern C++ is that ```std::unique_ptr&lt;T&gt;``` is just fugly to read and write compared to ```*T```. 
good god. 'throw everything away and rewrite it in rust' or dinosaur/stockholm syndrome thinking over here. there must be a middle ground.
Call a spade a spade. ```std::unique_ptr&lt;T&gt;``` is **fugly**. I know that after having used 2014 Rust with the sigils. A programming language functioning like modern C++ *can* be made to look more elegant. 
&gt;&gt; because you come off as immature to say the least. I change my debating style other times.. today my patience has been exceeded many times over already (conversely trying to explain to people in Rust that you don't actually need bounds checks all the time , and explaining that c++ *could* be checked for safety). alternating between communities and different contrasting arguments is destroying my faith in humanity 
My read is that I cannot use this feature if I want to extend existing source files, or I will risk silently changing the meaning of some random code somewhere in the same source file. That doesn't seem appealing. Terse template notations are different because I know that the declaration `void f(const char*);` will have the same meaning with or without concepts.
&gt; My read is that I cannot use this feature if I want to extend existing source files, or I will risk silently changing the meaning of some random code somewhere in the same source file. That doesn't seem appealing. What I'd imagine is firstly just using it for new code, but then gradually refactoring older code to work with it. Interestingly in apple headers i've seen them write markup in standard libraries (it might be #defines) for 'nullable pointers' - I think such refactoring efforts would be in progress anyway to improve language interoperability (swift, and imagine if we could get more crossover with the rust community beyond just the C FFI) 
&gt; The 'raw types' are supposed to be used infrequently (they're tools for building smart pointers and collection classes). You are confusing `new` and â€˜raw pointersâ€™. I don't see anything unsafe that applies specifically to raw pointers. &gt; '*T' can mean (and do) almost anything And so can any operator.
&gt; I don't see anything unsafe that applies specifically to raw pointers. &gt; And so can any operator. safe patterns prevent inappropriate use at compile time ```*``` fails to communicate the specific meaning of each situation - you have to infer it (intuitively, which is error-prone) 'there's a valid safe use of *T as optional &amp;T ' .. but the point is *T doesn't specifically communicate that.. it wont prevent someone else from using it as an array or whatever.
This is off-topic on /r/cpp, try /r/cpp_questions. Make sure you format your code properly.
I think it's being worked on in forks. Even then, don't confuse stability with staleness ;) I know Nvidia's CUDA 9 is on the horizon and it's bringing C++14 features to CUDA so I'd expect a new version of Thrust to follow. CUB is a lower-level library that gives you better parallel primitives for writing your own custom kernels.
Sry noob here
&gt; Why is that? I don't pass `unique_ptr` around, because somebody could steal ownership from it. I don't use `std::array`, because usually I don't need it. Oh, and because you have to initialize it as `std::array&lt;T,N&gt;{{ {"1", 2, 3.0}, {"1", 2, 3.0} }}`. I mean the extra brace.
I think I understand what you're saying, but I'd probably word this a little differently. "Type declaration overloading" sounds really spooky scary, but I think what you're trying to say isn't quite what the post title implies. You want the built-in type declarations to represent smarter types, correct? For me, the idea I like most would be the changing of `T[N]` being a `std::array&lt;T, N&gt;`. It wouldn't introduce any runtime overhead, but you immediately get all the goodies from `std::array` that aren't available with built-in arrays. (Including the ability to insert debug assertions for bounds checking, etc.) To access the built-in array in this context, you'd use a compiler intrinsic like `__builtin_array&lt;T, N&gt;`. For array, this works well. It does get hairier with pointers, because what default pointer would `T*` become? Your best bet would be `observer_ptr`, as that had the closest semantics to a raw pointer. This would again let you insert custom logic (ie. Null checking). The biggest benefit to overriding types in this way would be _disabling bad behavior_. Array's ability to instantly and enthusiastically decay to a pointer is an unintuitive and often confusing behavior. It leads to the common beginner's misconception that arrays are just pointers. The biggest problem, and the one people are sighting most: the whole world falls apart if you change this in the language. However nice it would be, ABIs won't like this big of a change. There's such an enormous volume of old code written with these syntaxes that I doubt such a change would ever work. My brain is now turning, and may be turning for the next week or so, on "How could this be done in a backwards compatible way?" Who knows: maybe it's possible? C++11 introduced `std::initializer_list`, which, despite it's many flaws, is something close to your idea: a built-in syntax that resolves to a more complex type. Of course, since this introduced a completely new syntax (rather than overwriting an old one), there weren't nearly as many potential problems. I like this idea. It's pretty cool to think about. Not sure if it's possible, but it is cool nonetheless. A bonus: it would be awesome if string literals were their own type, instead of a plain character array that so quickly decays to a pointer.
I understand about unique_ptr - honestly, I type std::unique_ptr&lt;something&gt; pretty infrequently too, not only because transferring ownership is rare, but because I rarely allocate memory directly. More often, I'm using a container. And it's reasonable to expect well-written code to use containers or other abstractions more often than simple smart pointers. Function signatures, I find, very rarely contain smart pointers. This is part of the reason why I think this whole "problem" is an exaggeration. But do you really use raw arrays to save typing two braces? That is giving up a lot of potential safety for a miniscule saving in typing and readability.
The issue that I have with your proposal is that you are suggesting to assign terser (`T*`) syntax to a thing that is used less commonly (`unique_ptr&lt;T&gt;`), and longer (`opt_ref&lt;T&gt;`) syntax to a thing that is used more commonly. You can't use `unqiue_ptr` to refer to something that it doesn't own, and in some projects there might be no use for `unique_ptr` at all. And if you insist on introducing breaking changes to the language, why not just allow to assign built-in arrays and pass/return them to/from functions, removing `std::array` altogetherÂ¹? (Â¹ For other things that `std::array` provides, there are non-member functions) EDIT: I like the actual idea of overloading `*` and `[]` for user-defined types (something like `T *unique` akin to `T *const` would be cool), but I don't see how this can be done nicely. I agree that `T*` has too many uses/meanings (an optional reference, an iterator, C compatibility, and an actual pointer), but changing that would break virtually all existing C++ code.
I mean... I'm not trying to disagree with you here. I was more curios as to why it wasn't adopted, hence why I was asking if there is anything 'common folk' can do about helping with its inclusion in the standard :p. Also, as far as my experience with D goes, you aren't forced to use the GC... I believe most of the core libraries don't actually take advantage of it nowadays. Also, the GC is sort of nice because it basically allows you to write C++ with atomic shared ptrs without the incredible overhead a theoretical atomic shared ptr would involve. But I have used neither D nor RUST extensively and I'm diverging... the comment I was making is that some of the more 'avant-garde' persons when it comes to C++ syntax might have moved on to other language at this point... but In hindsight I don't know the community well enough to make that comment.
Here's one: void foo(int coordinates[3]) { ... } Trying to explain the horror of this declaration to a beginner is a great source of sadness for all who try to teach C and C++. Whether this change is implementable? Dunno. But this is a problem that it would solve.
Changing `T[N]` to `std::array&lt;T, N&gt;` would not guarantee zero runtime overhead, and runtime overhead isn't the only overhead to consider. I am struggling to find it (my mind may be playing tricks, but I don't think so!), but I think there is a current GCC bug causing a failure to generate`std::array`-based code less efficiently than raw array code (edit: there's not good reason why this should be). Binary code generated will be larger as well, because the symbols are bigger, compile time is likely to be a bit slower, and so on and so forth. Beyond this, they're just different types with different behaviour. `std::array` isn't just "raw array but safe". It has a completely different interface. This kind of global change strikes me as a very, very bad idea and everything would go boom. And what happens when you compile a header from a C library which uses a raw array or pointer? You can't change the header. How would you tell it to use the "real" old types? "extern unsafe"?
It _could_. Currently, the only reason it is ever _necessary_ is because using a function/type requires it to be declared prior to usage. So if your function uses a function defined later in the TU, and _that_ function uses the other function defined earlier in the TU, one of them _must_ be declared before the other is defined. IIRC, the only reason for these restrictions is that old compilers couldn't look ahead to see types/functions later defined. (Memory restrictions). Modern compilers can probably hand this situation easily.
[Sieve of Eratosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes) 
C++ is burdened by its legacy, yes. I think the best general approach to solving the issues this causes is to teach best practice and continuously revisit and update that best practice. Sometimes this will come in the form of new library features which take more typing. If this is truly a barrier to their use for a developer, then that developer's capacity to adapt is a barrier to their continuing development. Backwards-incompatible language features will never gain traction, nor should they.
Won't any change to a function body inlined in a module cause a recompile of all dependencies? Also, if two modules cross-reference each other, you'll have to put at least one implementation in a separate source file.
A pretty poorly made one at that.
Make me irrationally angry.
Is there any way to make `T&amp;` point to garbage without creating a pointer that points to garbage and de-referencing it, or dangling?
I get the same result with Visual Studio 2017 (15.2.26430.16). I also tried a range-based loop over a vector and got slightly better ouput: ; 51 : for(TestPair&amp; it : a) mov rdx, QWORD PTR [rcx+8] mov rax, QWORD PTR [rcx] cmp rax, rdx je SHORT $LN3@writeToArr npad 4 $LL4@writeToArr: ; 52 : it.first = v; mov r8d, DWORD PTR [rcx+24] mov DWORD PTR [rax], r8d add rax, 8 cmp rax, rdx jne SHORT $LL4@writeToArr $LN3@writeToArr: As you can see it's still loading `v` from memory on each iteration. &gt;One thing to note is that MSVC can do the aliasing analysis and produce a fast loop when the 'a' array is of a simple type such as int instead of TestPair. I wasn't able to reproduce this, I was still getting memory loads even when using `int` instead of `TestPair`.
Yes, the recompilation is unfortunate, but doesn't _necessitate_ (by the language) that the implementation be separated. If you want decent compile times, then yes, you _need_ to put the implementation somewhere else. Modules _should_ (if I understand them correctly) remove this dependency on the implementation of a function from the users of the function. Still, inliners will like to see the definition, if possible. There's a variety of ways this can be solved. The current paradigm of "one compiler invocation per TU" hinders IPO. IIRC, modules only export the interface, not the implementation. As such, cyclically dependent functions should be fine to be defined when declared. I haven't read the whole modules TS: it would specify the semantics in that case.
**Finally someone gets it**. thanks. &gt;&gt; the whole world falls apart if you change this in the language. I admit doing it in a backwards compatible way is very difficult, but I'd use the analogy of replacing syntax with #define .. just at a deeper level, but not as deep as 'overloading' which would have a fit if you tried to 'redefine' it. I suppose this would be an AST transformation, somewhere between the level at which #defines operate and the rest.. but just like a #define ,it shouldn't affect declarations seen *before* it. or perhaps its effect could be controlled by **namespaces**? - 'this definition of [N] only works within this designated namespace..' I haven't thought through all the possibilities or hazards but 'if we can put a rover on mars, surely we can redefine the square brackets in parts of existing c++ programs without throwing all of them away' Yet another idea might be to tie it to the types themselves: what if any T knows how [] and * should modify it. e.g. "star(int) --becomes--&gt; raw_pointer(int).. star(MyClass) --becomes--&gt; observer_ptr&lt;MyClass&gt; .." (default being 'builtin..')... this would allow a rolling refactor? (I was inspired by 'old Rust' where the sigil ~ wasn't really a type, it was a modifier that combined with other things in different ways to actually become a type. Many said that was confusing, but I didn't think so. you just had to read it as *'the owned allocated version of'* instead of *'an owned pointer of..'* ) 
Firstly, for historical reasons MSVC always assumes things which can alias do so. You need to mark things explicitly with `__restrict` to tell it otherwise. Secondly, MSVC is very unaggressive at inlining compared to recent GCC or clang. Sprinkling `__forceinline` around can help a lot. Thirdly, just because MSVC generates apparently worse code doesn't mean it benchmarks particularly slower. Pointless reloading from the L1 cache is irrelevant compared to stalling on main memory. So always benchmark instead of assuming from inspection of the assembler.
Yes, this is a case of "If I could rebuild C++...". I'd made built-ins a bit smarter. I'd remove the`T*` syntax entirely, make it `pointer&lt;T&gt;`, or something like that since pointer declaration syntax is a complete mess. I'd make built-in arrays more like `std::array`. I'd make string literals a more expressive type. We can all dream, right? The point here is "Wouldn't it be great it the default of the language was _do the better thing_?" It's all just a big reminder that computers and software development is all built upon sadness, tears, and regret. :)
 int * a = new int(); int &amp; b = *a; delete a; ?
&gt; Yup, and none of them include situation where retry makes sense. In programming there is no magic -- any misbehavior (and occasional good behavior) is always due to a bug somewhere. "Modern Linux" has them too :-)
That's dangling. 
&gt;&gt; This kind of global change yes there's a lot to explain, and I haven't figured out all the details - apologies - but I'm not proposing a global change, rather something that (like #defines) has a local effect. then you gradually refactor, moving the effect outward through the sourcebases. It could be controlled by namespaces? (see other ideas above) I know saying 'like a #define' sounds scary but we have plenty of other overloading going on making radically different uses of simple symbols.
&gt; Beyond this, they're just different types with different behaviour. std::array isn't just "raw array but safe". It has a completely different interface. This kind of global change strikes me as a very, very bad idea and everything would go boom. That's pretty much whole idea, though. The things we like about arrays (contiguous storage, subscript access, iterators, fixed size) can stay. The badnesses it inherited from C go away. Things like: - Declaring functions that take array parameters isn't possible. They're just pointer parameters. `std::array` as a function parameter works _exactly_ like a beginner would expect. - You can declare a reference to a C array, but you'll accidentally write a perl program in the process. :) - Arrays decay into pointers if you just look at them funny. - You can't copy C arrays. Copying `std::array` "just works". This code makes children cry: int foo() { int arr[3]; auto a = arr; static_assert(sizeof(arr) == sizeof(a), "Children are crying"); } It's the "do the right thing by default" that would be nice to have. The fact that it breaks the world to make this change is unfortunate, because it means we'll probably never get to have this in C++. We could follow Scott Meyer's advice and "break those eggs". Unfortunately, the "egg" we'd break is "the entire world". One possibility: A `#pragma magic` that changes the semantics of array and pointer syntax in a single file. This would be like `"use strict"` in JavaScript. And just like JavaScript, this would inevitably be the source of misery and woe for many generations to come.
one thing I saw in rust, they had 'slices', an elegant way of passing a pointer+size , which is what we should be throwing around more; [] could really be that. (I gather ranges of iterators would handle similar cases but there's the idea of 'something where you explicitely expect random access to be fast') ditto their 'raw array' was very pleasing, you could quite happily use [T;3] to pass coordinates etc around.. it's pleasing to be able to do that without needing the further dependancies of a whole 'point' / '3d vector' type
&gt; But do you really use raw arrays to save typing two braces? Not typing, just the fact that there are two extra braces bugs me. Yes, that's not a rational reason, those braces just bug me. And not because those are extra braces, but because they reveal that this isn't really an array, but a struct containing an array (for example, `std::vector` hides that nicely with a constructor taking an `initializer_list`, but `std::array` can't do that because it must be an aggregate). &gt; That is giving up a lot of potential safety Maybe. But the only unsafe thing I know of is that array decays to a pointer, which I avoid by using array_view/array_ref or std::begin and std::end (and sometimes, but rarely T (&amp;)[N]). And I don't use `.at()` ever (it would've been a lot more useful for me if it returned `optional&lt;&gt;`).
&gt; And if you insist on introducing breaking changes to the language i know there's a lot to explain and I haven't figured out all the details, but I don't imagine a breaking change; the scope of redefinition would be controlled (analogous to how #defines do not work globally; maybe it could be confined to a namespace, maybe it could be confined to modify specifically enabled types etc.. there's a lot of potential ideas. again what inspired me in rust was the sigil syntax wasn't an actual type like Box&lt;T&gt; /unique_ptr&lt;T&gt; .. rather 'a modifier that sits in the AST, combining with it's content to make a type'. you read it as "The owned/allocated version of&lt;T&gt;" rather than "an owned pointer to &lt;T&gt;")
Why is `int arr[3];` *your* default, and why can't your default change? That's what I'm struggling with. I haven't written anything like that for years. There isn't a 'thing' that has it as a default, the language, ide, or anything else. Just people. And they can learn.
your example of `std::initializer_list` is just brilliant! The same way we could have `std::string_literal` that can be used to initialize char array, or, used by better types.
Why does it matter if it's dangling or pointing towards the sun, in the end it's not point to the desirable location.
&gt; today my patience has been exceeded many times over already Then go outside, get some fresh air. Sleep it off. Come back with a clear mind.
Looks like it generates a shitpost from your initial run.
`gsl::span&lt;int,3&gt;` (though strong typing is a very, very good idea)
&gt; I was more curios as to why it wasn't adopted well it seems the community is divided. The arguments I get into: - [1] some say it's "encouraging OOP" (the ones who want foo(a,b) everywhere, having discovered C++ OOP is hazardous; a misconception that this syntax means OOP , when the change is to *fix* the hazards..) , - [2] whilst the other opponents (the ones happy with existing class behaviour) want to be more sure that ```a.foo(b)``` is found in the definition of ```class A{}``` (I try to explain: the new syntax for extention methods could be deliberately easy to search, e.g. ```grep "foo(.*this"``` ... and most people use IDEs anyway.) Bjarne's POV is good but he was opposed to 'opt-in' when I replied to his forum post. He wants it truly 'universal' (to make it easier to teach and use). My modification to the proposals is to make it a deliberate opt-in, as a compromise for those who don't want it. r.e. 'ease of teaching / using' I think something like a dedicated extention block (see swift) would be the best, because then it would look more like existing class methods. 
[removed]
Well, I already listed two ways in which references can go sour, all you had to say was "dangling". Yes, dangling is a way in which a reference can point to garbage. But for pointers there are many other ways in which things can go wrong, which they cannot go wrong for references.
&gt; I wasn't able to reproduce this, I was still getting memory loads even when using int instead of TestPair. Ah yeah you're right. There was some situation I was testing earlier where MSVC was optimising it properly. Will amend post later. 
&gt; Firstly, for historical reasons MSVC always assumes things which can alias do so. You need to mark things explicitly with __restrict to tell it otherwise. Do you have any more information on this? It sounds like a pretty poor decision to me. &gt; Thirdly, just because MSVC generates apparently worse code doesn't mean it benchmarks particularly slower. Pointless reloading from the L1 cache is irrelevant compared to stalling on main memory. So always benchmark instead of assuming from inspection of the assembler. There are some performance results in the blog post: test_class.writeToArray(): 0.000541 s (1.84977 B writes/sec) test_class.writeToArrayWithLocalVars(): 0.000380 s (2.63310 B writes/sec) As you can see, the version with local vars is significantly faster. And also, just because something may or may not show up in any particular benchmark is not an excuse for poor code-gen.
hehe . you might like to help in another thread where they are insistent that c++ is beyond redemption (pretty much due to the mere existence of raw pointers) - the exact opposite of your POV.
I haven't written like that for years either. I do that for the sole reason that _I don't want_ the behavior of the built-in array. It's the same reason we advocate _not_ doing it that way. The fact that this is the syntax encoded into the language for declaring arrays makes it _the language's default_. To get nice behavior, _we avoid what the language provides us_. Therein lies the rub. Yes, I know: This is _C_ that's providing the bad behavior. I'm not trying to argue that this change _should_ be made. I just really like discussing regrets. It's how you learn for the future. There's not even the slightest glimmer of hope in my mind that this would ever make it into C++, but it's a nice exercise in making myself sad.
It's not the language's default, that's an incorrect characterisation. It's the lowest level building block provided by the language. Seen like that, as well as historically from the point of view of C, it makes sense for it to have the simplest syntax. The mistake is in seeing that as the easiest to use/type and therefore default developer choice - that is down to education. We build higher-level abstractions upon these language features which are very powerful, with a cost of being more verbose, yes. I don't understand this as avoidance of other language features. It's using all of the language's many facilities to their fullest. This is what many people really like about C++, it has all of those layers available to the developer. And the result is, of course, imperfection and compromise and angle brackets and frustration. There's no magic bullet. Sorry.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
 Ya its particular problematic with lambda expressions, as MSVC will often fail to inline stuff that clearly should be inlined.. and doesn't support __forceinline on lambda. So you are forced to not use lambda in any high performance code.
who the hell puts a loop index as a global variable
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
Hi, I'm a developer on the VC++ optimizer. This is a known issue to us, caused by the lack of TBAA. The compiler has alias analysis, of course, but not based on types and here, due to lack of other info, one has to rely on the strict aliasing rules from C/C++ to optimize as expected. Sadly adding TBAA and having it on by default is going to break many programs out there - there is a reason why Linux builds with it disabled. Some examples of such issues are in one of John Regehr's posts: https://blog.regehr.org/archives/1307. I'm not saying TBAA will never be added, just that it's not an easy decision, since it can impact a large number of important projects in unexpected ways. Clang/GCC are certainly using TBAA here, since if you replace the TestPair* a pointer with int* a, both produce the same code with redundant loads as VC++. The reloading has to be done because one could have this: struct X { int* a; int v; int N; }; X obj; obj.a = &amp;obj.N; then writing through obj.a[i] would modify obj.N. Pointer aliasing is probably the biggest pain when trying to optimize C/C++ code, the freedom of being able to do anything in these languages also means you end up with a large amount of redundant memory operations and other missed optimizations. There will be more work on removing redundant memory operations in general - the next release of VC++ after 15.3 will have significantly improved redundant load elimination and redundant store elimination algorithms, for example. Thanks, Gratian Lup
The inliner is certainly too conservative, but there are many improvements being done right now. The 15.3 release will have the first part of this work, which shows really nice improvements (~5% average, sometimes &gt;10%) across multiple benchmark suites. Just doing more inlining doesn't always speed things up, having ways of selecting the right calls to inline matters a lot. &amp;nbsp; **Edit:** Speaking about improving performance, one of the most important things is using link-time code generation (LTCG/LTO) - too many popular open-source projects we use for testing don't use it. It allows for better inlining, devirtualization and a bunch of other inter-procedural optimizations that can easily give &gt;10% improvement. Profile-guided optimizations should also be used, basically every program at Microsoft relies on this since it usually gives &gt;15% perf. improvement over LTCG. As an example see the recent results from the CoreCLR team (https://blogs.msdn.microsoft.com/dotnet/2017/07/20/profile-guided-optimization-in-net-core-2-0) and Google Chrome (https://blog.chromium.org/2016/10/making-chrome-on-windows-faster-with-pgo.html)
&gt; Sprinkling `__forceinline` around can help a lot. Hold up there, too much inlining will bloat your code and cause cache misses. You don't want to sprinkle it around. Use it where you need it, and realize there's a maintenance cost in the future.
https://woboq.com/blog/verdigris-qt-without-moc.html
Yea, it's unfortunate that raw arrays aren't regular types in C++. But, we already have a solution to that: `std::array`. Yeah, it's a little bit longer to type `std::array&lt;T, N&gt; x` than `T x[N]` - although on the upside it behaves a lot saner as a type declarator and it's a quite a bit easier to understand: std::array&lt;T*, 10&gt; array_of_pointers; std::array&lt;T, 10&gt;* pointer_to_array; than T* array_of_pointers[10]; T (*pointer_to_array)[10]; because it follows the normal rules for types. Honestly, I don't see the argument that `std::array` is *so bad* that we need a *large* change the core language rules for what expressions mean in order to avoid using them. Ditto for raw pointers. There are entire code-bases whose style guide uses pointers instead of references for mutable arguments - good luck convincing them to change that practice? I'd encourage you to think a lot about the motivation for this kind of change and come back with stronger examples. 
Where does it say that you should be able to move a 3rd party code into a subdirectory of your source tree, do `add_subdirectory()` and expect it to work? Isn't building the external 3rd party library seperetly by itself, installing it either system-wide or into a custom installation directory, and then, in your project, using `find_package(name)` that utilizes `FindName.cmake` is the proper way of adding library dependencies to your project?
Logic error, I'm confused. Is python runs on absolutely everything, how can anything be more portable than python?
My biggest complain about CMake is that that you can't easily get a list of all build options available without grepping all CMakeLists files, which might be split in multiple subdirectories and which might required evaluation CMake in your head as some options might be Windows-only or Linux-only or GCC-only or only if Option5 is also enabled etc., or using the CMake GUI. Using autotools you can just `./configure --help` to see all the options available. Does Meson provide something like that?
&gt; The biggest benefit to overriding types in this way would be disabling bad behavior. What bad behavior currently exists with raw arrays and pointers? &gt; It leads to the common beginner's misconception that arrays are just pointers. I suspect this at least partly due to the fact that there are many teachers who *teach* this misconception. &gt; A bonus: it would be awesome if string literals were their own type Yeah, this would actually be great. But it would break the world. 
Hi Gratian, Thanks for the great reply. What about adding TBAA under an opt-in flag? We have a cross-platform code base that compiles (and works) with GCC and Clang with TBAA (e.g. 'strict aliasing') enabled. Having such an opt-in flag would allow us any perf-gains achievable with TBAA. And I think even a pretty basic algorithm would be able to handle the case in my blog post. I agree that pointer aliasing is a serious pain in C++, which is one of the reasons I created my own pure functional language in which this is not an issue (due to all values being immutable :) )
I agree. I'm playing around with pre-allocating the channel buffer as we speak. Another difference is the WaitGroup vs. sequential join in the benchmark.
Not sure but ninja files are non-recursive so I expect mesonintrospect should give you whole project things?
Poor mans counting semaphore :) Definitely an angle worth exploring. Can't think of a faster way to keep track of the threads from user code right now.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [andreas-gone-wild/snackis/.../**defer.cpp** (master â†’ 48146ba)](https://github.com/andreas-gone-wild/snackis/blob/48146baf746e1fa83206bc2a56317c9009b07cdb/src/snackis/core/defer.cpp) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkmn3xx.)^.
This is not CUDA but somewhat related: Is there any chance of updating ImageWatch to work with VS2017? https://marketplace.visualstudio.com/items?itemName=WolfKienzle.ImageWatch#overview Judging from the Q&amp;A and Ratings&amp;Reviews on that page, I am far from the only one holding back on upgrading to VS2017 because of ImageWatch.
Paging /u/spongo2! We've looked into ImageWatch before. It's not a straightforward task to get it working with VS 2017 but if you throw enough motivating customer requests /u/spongo2's way, I'm sure he can't ignore you!
Nice catch :) Despite it's simplicity it's even more useful than Go's built-in since it respects scoping.
I thought your cpu was a neural net processor; a learning computer.
&gt; What bad behavior currently exists with raw arrays and pointers? Regarding arrays, [see my other comment](https://www.reddit.com/r/cpp/comments/6p14d4/idea_type_declaration_overloading/dkm6g9b/). For pointers, the semantics are too overloaded. Is it an optional reference? (That's how most modern C++ programs use it). Is it a pointer to an array of objects? Not to mention the insanity of the pointer declaration syntax. Something like `observer_ptr&lt;T&gt;` could define `operator*` and `operator-&gt;`, but not `operator[]`, whereas `observer_ptr&lt;T[]&gt;` could do the inverse. In this way, it could prevent mistakes and "do the right thing" (most of the time). It took several rehashings of the same discussion with a friend on the strange relationship between pointers and arrays, and why they aren't the same thing, but can sometimes be used in the same way. His teacher taught modern C++, but never really got into the semantics of arrays at that level. Ideally, someone learning modern C++ wouldn't need to know about why arrays and pointers interact in the way they do, but this friend inevitably wrote a function a-la `void foo(int bar[3])` during an assignment and spent hours wondering why it behaved so strangely. &gt; Yeah, this would actually be great. But it would break the world. I know. I'm just talking about it to make myself sad that we can't always have nice things... :( In all this discussion, I'm not expecting anything to really change for the better. It's more me and others spinning wheels and ruminating on some of C++'s warts. Fortunately, we have better alternatives these days, but "wouldn't it be nice" if those better alternatives were just the built-ins? One can dream.
**Company**: [Interactive Brokers](https://www.ibkr.com) **Type**: Full Time **Description**: Hiring for multiple positions with no specified seniority level (we hire new graduates to 30+ year veterans). Interactive Brokers (â€œIBâ€) is the largest U.S. broker, measured by trades, offering direct-access electronic trade execution and clearing for active traders, institutional investors, financial advisors and introducing brokers. IB serves more than 400,000 customers in trading securities, commodities and foreign exchange in 23 currencies on over 100 market centers in 24 countries around the world. The firm leverages highly automated systems it builds for all aspects of its business to maintain a low cost base as the business expands. The business is in a stage of rapid growth. The core of the brokerage business is built around high-performance low-latency C++ applications running on distributed linux systems. See the links below for more details about the individual positions. *Financial experience is* **not** *required*, though it is a plus. **Locations** 1. [Greenwich, Connecticut, USA](https://careers-interactivebrokers.icims.com/jobs/1597/software-engineer---c%2b%2b/job) (headquarters: smart routing, tickers, order management) 1. [Mumbai, India](https://careers-interactivebrokers.icims.com/jobs/1164/senior-software-engineer/job) 1. Budapest, Hungary - tickers **Remote:** no remote work available **Visa Sponsorship**: US sponsorship available **Technologies**: Core libraries are in C++98/03, many groups use C++11. Applications are deployed on linux systems and are written using a make/gcc stack. **Contact**: pm me or apply through the link above; for Mumbai, apply through the link 
Nope, something else is going on. I switched to pre-allocated buffer and implemented the custom wait group but it's still mostly the same.
Cats and dogs...living together...mass hysteria!
Not far enough, I'd say :) Brilliant! 
Just wanted to mention that I managed to quadruple the performance which means that we're now twice as fast as Go's built-in channels. I have updated the code in the post.
I'm not seeing similar results. On my system (3.5 GHz Xeon, 6 cores/12 hyperthreads), I'm getting run times of about 40 seconds for your latest C++ version, but only about 5 seconds for the Go version. This is on a Mac; maybe you're using an OS with cheaper threads?
I'm on a Kubuntu/64bit desktop. Over here, the C++ version finishes around 2.5 seconds and Go-performance is the same as yours. 
FWIW, it looks like some groups at Google are investigating moving *away* from GN (and *to* bazel): https://groups.google.com/a/chromium.org/forum/#!topic/chromium-dev/wl1T6XX2gg8 EDIT: See "Differences and similarities to Blaze" here: https://chromium.googlesource.com/chromium/src/+/master/tools/gn/docs/language.md If I'm reading that correctly, it looks like GN is generally more flexible, whereas Bazel imposes more structure. With respect to their mentioning of "homogeneity", I suspect most of Google's server software is tailored to their standard environment, whereas Chrome has tons of conditional compilation as necessary for the multitude of platforms on which it must run. Where you can get by with with less flexibility, I'd bet that Bazel is the way to go, as: * I'd bet Bazel has more opportunities to cache build steps, and will run faster (especially when they publish the distributed build server stuff) * Bazel has support for many languages, so in polyglot environments, you can just use Bazel instead of maintaining both Bazel &amp; GN build configs * Because it has more users / use cases, I'd bet the code is more battle tested
This suggests that the benchmark is mostly measuring thread startup/teardown time rather than the efficiency of the channels. I'd like to see one designed so that the run time is dominated by channel get()/put(), but I don't know enough about Go to write the equivalent code.
&gt;I don't pass unique_ptr around, because somebody could steal ownership from it. Weak objection; if you pass a raw pointer then someone could `delete` it. 
That is one way. Old and cumbersome, and only way for libraries whose build scripts are terrible. Modern CMake advocates setting properties to targets, not to directories.
&gt; , but it's a nice exercise in making myself sad. awesome, I've achieved *something* with this post. but really can we think harder about ways this could be fixed with the power of 2017 technology . over in Rust their proposed solution is *throw ALL the C++ away* but we must be able to do better than that. What about the idea of foo[N] becoming like Rusts old sigil: not a type in itself, but a *modifier* ;the type it produces depends on the type given. If 'you haven't used foo[N] in years', that means supplying a new definition of [N] just for your types wouldn't break anything.
what the hell is this
&gt; compromise and angle brackets tangentially one thing that might be nice is if text editor could display this https://www.reddit.com/r/emacs/comments/6huzuq/is_there_an_extention_to/ .. but I want to go much further than that to fix the world.
Maybe something like sregex_token_iterator
&gt; It's not the language's default, that's an incorrect characterisation. what it is is wasted syntax space; what we see elsewhere is they've jumped on the opportunity to even make square brackets declare maps/dictionaries/hashmap more efficiently aswell. I also quite like scala's choice of angle brackets for typeparameters generally.. for some reason they're not as offensive the syntax for types does matter IMO because it's the first thing you read when trying to figure out how to use something it all adds up when you realise we had to resort to a named type just for pair&lt;A,B&gt; whilst that can be done straight off as (A,B) elsewhere (i know thats another problem.. hmm **we need to give the comma the same treatment**)
https://github.com/dobkeratops/compiler https://github.com/dobkeratops/compiler/blob/master/example.rs this was my attempt to do something constructive but realising the futility of building a new language (even one that we could transpile C++ to), I prefer to argue a lot now.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [dobkeratops/compiler/.../**example.rs** (master â†’ 7a6fa57)](https://github.com/dobkeratops/compiler/blob/7a6fa570b05379a4cbaf7d9460a32456065e75ed/example.rs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkn1a46.)^.
&gt; I don't see the argument that std::array is so bad that we std::array is *so good* that we want to type it more easily
This looks great! I am just about to start Arduino development. So, is it possible to use C++17 on Arduino? Are there any limitations, for example in the standard library?
Just in case if someone don't understand: It is an ASCII art, drawing a typical diagram representation of a chip. [Example](http://images.books24x7.com/bookimages/id_8662/fig6-1.jpg). With some meta-programming, the OP turns the ASCII art into a valid C++ constant expression.
I'm no C++ wizard but I think there is (at least) a mistake in your guide. You say : &gt; `capture list` captures the value of variable when lambda was called and not at time when lambda was defined. I wasn't sure about that so I tested it myself with the following code : #include &lt;iostream&gt; int main() { int a = 10; auto myLambda = [a](){ std::cout &lt;&lt; a &lt;&lt; std::endl; }; a = 14; myLambda(); } And I got 10, which means it should capture the value when creating the lambda. (You also forgot a semi-colon after your lambda definition) &gt; Till then, Sayanoara. You mean "Sayonara" ?
I'm confused with your reply, did you actually reply to the right post? I wasn't talking about any properties.
&gt; (blah | doh) -&gt; [derp] [herp] &lt;- (fart | cock) :D
Your post has been automatically removed because it appears to contain profanity or slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/6p53dy/did_i_go_too_far/dkn2v8f/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Feature request: please prioritize inline variables! It's so nice to be able to use a lambda to initialize lookup tables in-class only. 
No u, bad dog.
Looks like witchcraft to me! Someone hand me my pitchfork!
I might be using wrong term. What i have in mind is `include_directories()` applying includes to a directory and it's children while `target_include_directories()` applies those includes to targets. I call includes/definitions/build flags simply properties. Is there better term i should use?
Except that your idea completely breaks compatibility. Imagine if I decide that "T[n]" should be "array&lt;T,n&gt;", but I want to use a library that uses its own array class and defined it differently. Basically it would be completely unsafe to ever use the "nicer" "T[n]" syntax in a header file, since you have no idea what a source file may have defined that to mean. Overloading "\*T" is even worse. The standard already defines multiple different types of smart pointer for different purposes. It's pretty _certain_ that any large project will want to use more than one. That means that _different developers working on the same project_ or even _the same developer in different files_ may define "\*T" differently, which is just a recipe for confusion. If you really dislike typing "array&lt;int, 5&gt;" or "unique_ptr&lt;MyClass&gt;" so much, maybe C++ isn't the language for you... Personally, I find them quite easy and giving "hidden meaning" to C-style constructs is a pretty awful idea.
&gt; if you pass a raw pointer then someone could delete it. I was expecting this argument. Yes, they could. However, I believe that nobody in the right mind would call `delete` on a bare pointer, because that pointer could point to a static or a local variable, or a part of array, or memory allocated with a custom allocator. Stealing ownership fron unique_ptr isn't very reasonable also, but the worst it can cause is a null pointer dereference. Stealing ownership looks a lot more benign than using `delete` that everbyody vilifies. So I expect that it is more likely that somebody would try to steal ownership from `unique_ptr` than attempt to `delete` a raw pointer that they don't know where it came from. EDIT: I completely forgot to mention, I return (or pass as argument) an `unique_ptr&lt;T&gt;` when I want to transfer ownership, and `T*` when I don't, because otherwise it would not be clear when you were supposed to take the ownership and when you weren't.
&gt; There are some differences among compilers regarding the automatic detection of return types when you have more than one return statement because the standard doesnâ€™t guarantee the automatic detection of return types. wat
Ooh! Awesome that you have looked into it. /u/spongo2, pleeeeeease! ;-))) I know a few more people but they're unfortunately not the type who would post on reddit or email someone at MS about it. But if you have a look at the Q&amp;A and Ratings&amp;Reviews pages of ImageWatch on the VS marketplace (link see above!), there's soooo many people asking for it! :-) And I'm sure the people wanting it but not posting there is even larger (actually, until a couple of months ago, there was no messaging/posting facility on the VS marketplace page!)
You're right. Capturing at the point of calling makes no sense, think about returning lambdas from functions and calling them somewhere else. 
What do error messages look like if you make a typo?
That's the point that made me think twice about it, and test it myself :)
&gt; Sadly adding TBAA and having it on by default is going to break many programs out there A nice thing is that since GCC started enabling it, a lot of open-source software had bug reports submitted to them and crashes were fixed.
Bad bot
[removed]
Do not post questions to both r/cpp and r/cpp_questions. The latter exists for a reason. Read the sidebar. 
MSVC 2017.3 can't come soon enough :p 
One thing about returning lambda from function is that you have to be cautious about the scope of the lambda I think: I saw some code which used some lambda declared in a function for a POSIX timer, AFAIK this is an out-of-scope issue.
 auto myLambda = [](){std::cout &lt;&lt; "a" ;}; // ERROR Why would this throw an error? Can't you use string constants in lambdas? (On mobile, so I can't try it myself)
That is true, lambdas which capture obey the same lifetime rules as everything else, so you cannot for example capture a local variable by reference, then return the lambda, because the local variable will go out of scope. 
They're on the priority list because the STL needs them ðŸ˜ƒ
That code is fine, their erroneous code should have been `&lt;&lt; a` rather than `&lt;&lt; "a"`.
&gt;&gt;but I want to use a library that uses its own array class and defined it differently no, what i'm proposing works at an earlier syntactic level, more like #defines, but later:- lets explain in steps:- Imagine if you say #define A(T,N) std::array&lt;T,N&gt;, void foo(A(int,4) a); #undef A #define A(T,N) my_array&lt;T,N&gt; void bar(A(int,4) a); #undef A see how 'A(T,N)' is just local syntax; 'foo' and 'bar' can be called from the same scopes and will expect 'std::array&lt;T,N&gt;' or 'my_array&lt;T,N&gt;' respectively. foo &amp; bar show up with different name mangling. *now imagine foo[N] could be treated as a redefineable 'AST macro' in the same way.. (with an extra unamiguious __raw_array&lt;T,N&gt; type existing for compatibility) for inspiration, look at Rusts macros which operate on the AST in-between parsing and the rest. Imagine a new AST transformation/substitution phase in-between text and everything else. Basically imagine that we add the ability locally over-ride what AST node 'T a[N]' generates. &gt; If you really dislike typing "array&lt;int, 5&gt;" or "unique_ptr&lt;MyClass&gt;" so much, maybe C++ isn't the language for you yes apologising for this mess we have today makes you so smart, not. Its not just this, theres other targets to improve (like how commas are used). I do not accept things 'just because thats the way things are'. Software is the most maleable medium in this world. we should be able to improve it, ad infinitum. Rust back in 2014 demonstrated the same behaviour as modern C++ *but in a more syntactically efficient way* , more pleasant to read and write. BUT it was incompatible in other ways (e.g. they make *traits compulsory*, they don't have general purpose open *overloading*, they over-estimate the safety). I assert that **we should be able to get a language which is C++ compatible, but with the more efficient syntactic tweaks of rust** **infact I even wrote a prototype of demonstrating how this could work**: https://github.com/dobkeratops/compiler https://github.com/dobkeratops/compiler/blob/master/example.rs but getting traction for a *new* language is almost impossible. so I'm stuck here arguing with you (or arguing with the rust community about overloading and excessive use of bounds-checks and so on) For further reference see how Apple successfully mix [Swift and ObjC](https://developer.apple.com/library/content/documentation/Swift/Conceptual/BuildingCocoaApps/MixandMatch.html) (i.e. swift being able to use objC frameworks), and how [JVM languages can share class information](http://www.codecommit.com/blog/java/interop-between-java-and-scala). also see the idea of [SPECS](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.34.2481&amp;rep=rep1&amp;type=pdf) We should be able to make a syntax change that still generates code entirely compatible (both import and export) with C++. We should be able to make these changes **incrementally**, so it isn't a disruptive change (i.e., no, don't "re-write it in Rust", just gradually phase in better ways of doing things, so everything is still useable at every stage). 
Alright, thanks!
I don't think that this is restricted to lambda which capture: lambda themselves are objects which have a certain scope, and if you convert a lambda to a function pointer, it's very easy to try to use a lambda out of scope..
&gt; too many popular open-source projects we use for testing don't use it. I'd blame CMake defaults for that
&gt; Yes, they could. However, I believe that nobody in the right mind would call delete on a bare pointer, I agree it's an intuitive expectation, but its much better when the compiler can *rule out* as many mistakes as possible. It is great when we can communicate more details about the program in an unambiguous *machine-checkable way*, rather than leaving things to fallible opinion and guesswork. after all there ARE still old C apis being used which *do* sometimes return raw pointers that the user is supposed to free, how does someone know for sure that it isn't like that..
Regular priority, or burn-down-the-world-priority? :)
A lambda converted to a function pointer implicitly has static lifetime because it has no state â€“ it's _just_ a function.
I'm not talking about `include_directories()` though, nor any other properties. I'm talking about adding a library dependency to a cmake-based project. Please re-read my initial comment.
You don't need to repeat yourself. Of course it's going to work similarly to #defines; you already said you'd use the "using" syntax which already works that way. #defines are already "evil" and only put up with for compatibility. Here's an example for you, imagine I've got this in one file: #include "myheader.hpp" #include &lt;foo_library.hpp&gt; int myarray[5]; and this in another: #include &lt;foo_library.hpp&gt; #include "myheader.hpp" int myarray[5]; These could mean _vastly_ different things if both "foo\_library" and "myheader" define the syntax to mean different things. This is non-obvious and very, very bad. Changing the order of includes like that should not have such drastic effects. Yes, we already have this problem with macros, but at least using macros to redefine existing syntax is universally considered bad practice. Typedefs and "using" declarations don't have an "#undef" for exactly this reason; conflicts _should_ be an error. Just as bad, if I've got a bunch of old code that uses C-style arrays and I add some new library that redefines the syntax, suddenly everything breaks! So basically, what I said is true; you can't use the "better" syntax in a header file. You've completely ignored my point about the meaning of syntax varying between files, the fact that it hides programmer-relevant information and the confusion that _will_ cause. &gt; look at Rust We get it, you like Rust. It's not a bad language, but it's not C++ and shouldn't be what C++ aims to be. &gt; so it isn't a disruptive change It _is_ a disruptive change. It breaks existing code, which is the definition of "disruptive".
Thanks!. I corrected it.
Yup!. Corrected it. Thanks!. Regarding lambdas capturing values when they are declared ant not when they are called. Yes you are right!. When I wrote post I checked it with some sample code and it was not working that way. I am still trying to recollect it. Thanks for letting me know. I will correct it for meantime. 
Maybe it was something like: #include &lt;iostream&gt; int main() { int a = 10; auto myLambda = [&amp;](){ std::cout &lt;&lt; a &lt;&lt; std::endl; }; a = 14; myLambda(); } (Tthat would return 14, 'cause it captured the reference at the time of definition)
Different sort of force inlining. Some newer libraries like Outcome were deliberately designed to enable static pruning by the optimiser of large sections of potential code to execute because earlier up the logic chain, it is statically obvious that those combinations of code path can never execute. This *reduces* the size of the output binary, it's the opposite of bloating. MSVC does do the right thing here if forced. GCC and clang do the right thing without being forced. Microsoft are very aware of this, I sent them a repro many years ago, and they've been investing significant effort to close the gap. As another poster mentioned, a new optimiser landed a few versions ago, improved codegen lands in 15.3, more is coming soon.
The tradeoff for using them when starting from scratch is almost non-existent. What is heavy is converting existing codebases to use these tools. I.e. these codebases were started long before these tools became available, and converting the codebases is heavy. 
No ! Please god no ! If your first reaction when you so code is "how is that even valid C++" then don't use it. Think about the next developer that will have to use your code. 
Can someone explain what the compiler see?
&gt; #defines are already "evil" ... for reasons we don't need to copy in new syntax transformation tool - they're not controlled by scopes, they work at the purely text level rather than the AST. **Look at Rust's macros, the are not evil**, because they work on the AST. &gt; These could mean vastly different things if both "foo_library" and "myheader" define the syntax to mean different things. yes, absolutely. And why should that surprise you at all when **we already have all sorts of overloading and namespaceing going on** and whilst they may do different things, they should still conform to the overall expectations of an array (minus the retarded behaviour). We could make 'array to pointer coercion' a warning, to encourage people to actually write ```&amp;myarray[0]``` (compatible with old and new) if they really want a pointer. &gt; So basically, what I said is true; you can't use the "better" syntax in a header file. if it was controlled by scopes, you could enclose the 'new' use in a namespace or within members of a class. You can select the scope you want. &gt; It is a disruptive change. **It breaks existing code**, which is the definition of "disruptive". No, I've clearly explained ways in which that can be avoided. You say you understand , then you say this which proves you're still missing the idea. **it could be constrained to work within specific scopes** (e.g . within fields of specific classes, or within certain namespaces), or **it could be constrained to work on specific types** : when you write a new type you could say 'when used with the array declaration, it makes an array&lt;T,N&gt; instead of __raw_array&lt;T,N&gt;** , and old types are unaffected. (that again takes inspiration from Rust; ~ was not a shortcut for a template like 'Box&lt;T&gt;', it was a *modifier* that depended on what it wrapped to actually do something .. ~str , ~[T], ~T were completely different.. just related in the idea of 'single ownership , dynamic allocation'. Ditto, foo[N] could mean 'fixed size array semantics, but I don't care exactly how.') &gt;&gt; but it's not C++ and shouldn't be what C++ aims to be. It is 2017, we should have 'a language with all C++'s performance characteristics , but without crappy legacy syntax.' it should basically be possible to customise C++ to get there, so it's not a 'throw everything away' change. I'm picking on "array" a lot in this thread but there's others which all add up. Comma is next on my hitlist.
I understand, but these are related things. Why would anyone opt in for only consuming library after it is built and installed and using `Find*.cmake` instead of supporting both ways? It costs nothing. Flexibility does not suffer, its just that people may have to change some bad habits. Some projects bundle all third party dependencies and it is a very valid usecase.
Interesting, so I was wrong.. Do you have a pointer to the specification which define this? Thanks anyway.
I think you are right. I might have tried this. I was stupid I made that conclusion from this piece of code. Thanks!
You mean logs and a torch.
I think this should answer your question. https://www.safaribooksonline.com/library/view/effective-modern-c/9781491908419/ch01.html
C++14 [expr.prim.lambda]/6: &gt; The closure type for a non-generic _lambda-expression_ with no _lambda-capture_ has a public non-virtual non-explicit const conversion function to pointer to function with C++ language linkage having the same parameter and return types as the closure typeâ€™s function call operator. The value returned by this conversion function shall be the address of a function that, when invoked, has the same effect as invoking the closure typeâ€™s function call operator. &amp;hellip; A simple, everyday pointer to a function is returned, and functions do not have lifetimes.
bunch of operators and variables. Which part don't you get?
All ruined by #include "../include/mpl/util.hpp" #include "../include/mpl/collection.hpp" #include "../include/mpl/integral_constant.hpp" #include "../include/mpl/undefined.hpp" 
&gt; the standard doesnâ€™t guarantee the automatic detection of return types. It does for lambdas, and always has; nothing in that chapter says otherwise. (Regular functions/function templates didn't allow this until C++14.)
I got you now. I thought it from the perspective of reference and all in auto deduction. But that doesn't apply here. Thanks for correcting me. This is the reason why I love reddit. :)
&gt; And since the result is undefined, clang shouldn't just assume something, but actually check the value. How can it check the value when the value has no meaning? Pointer addition on a null pointer is not an operation with any meaning in the C++ language. It's the equivalent of me saying "multiply the pointer by six elephants and subtract a grapefruit, then give me the numeric result in dog years". We can try to give this statement some meaning if we wish, but it is outside what is defined by the standard. Ideally the compiler would refuse to generate any code for it. The problem with undefined behaviour is that the whole program is now undefined.
Thanks A LOT for your reply. That's always nice to correct one's mistakes.
And user-defined literals. 
Those are still operators. Or at least that's how you define them...
Please stop using bold formatting on random sentences, it's very hard to read. &gt; they're not controlled by scopes, they work at the purely text level rather than the AST ... ... and because they can be used to redefine existing syntax. I'm not against an AST-based macro system for C++, but I am against anything that can change the meaning of existing code. It's confusing, unintuitive and results in hard-to-find bugs and difficult-to-audit code. If you make all existing syntax redefinable, you've basically just made a crappy, overcomplicated version of LISP. &gt; they should still conform to the overall expectations of an array Not possible. You can't make a new array class that's compatible with the C array (in a well-defined way). If you're compiling C++ code that's meant to be called from C (via `extern "C"`), that's vital. What about code that's got to compile with either a C or C++ compiler (e.g. in a header file)? &gt; if it was controlled by scopes How? How do you specify a namespace on the line `int myarray[5];`? Replacing it with the "full" syntax (i.e. `mynamespace::array&lt;int, 5&gt; myarray;`) is not good enough; what if the library changes the name of its array class? Something like `int array mynamespace::[5];`? And you think `array&lt;int, 5&gt;` is ugly and too hard to type.. &gt; No, I've clearly explained ways in which that can be avoided. Nope, you've (only just, in your latest reply) handwaved something about it being controlled by scopes. Nothing like the level of detail needed to "clearly" explain anything. &gt; It is 2017, we should have 'a language with all C++'s performance characteristics , but without crappy legacy syntax.' It's 2017, we should all have jetpacks. "A language with C++'s performance characteristics, but with a different syntax" isn't and will never be, C++. You're quite welcome to define (and implement; talk is cheap) such a langauge. Maybe WG21 will pick up a few of your ideas. If you want to program in Rust, program in Rust. If you want to implement some of Rust's idioms in C++, do that. Don't expect a very well-established, deliberately conservatively specified language to make radical, breaking, changes to suit your whims.
For example 2 in the structured bindings section, i think you also need to specialise std::tuple_element for things to work properly for a custom type.
Personally I like Visual Assist more. VA tries to stay away from understanding your code, and focuses on language-level aspects. To me, [Goto related](https://docs.wholetomato.com/default.asp?W478) and [VA Outline](https://wholetomato.fogbugz.com/?W187) are the killer features. Usually I don't enjoy being told what to do, but when I do need code analysis, I use [CppCheck](http://cppcheck.sourceforge.net/) or [CppDepend](http://www.cppdepend.com/features) 
Any details about the release date gor 15.3?
&gt; "but I am against anything that can change the meaning of existing code." thats why I keep saying 'the application could be controlled by scopes'. you have to *opt in* to enable it, e.g. *within* a clearly marked namespace, or within a clearly marked class body. maybe you couldn't read that because I kept writing it bold, sorry.
&gt; the application could be controlled by scopes And I repeat, in bold, because you apparently can't see it otherwise: **HOW?** With a syntax example, showing how I can choose the "array definition" from one particular namespace.
&gt; How? How do you specify a namespace on the line int myarray[5]; the [N] 'AST macro' would be defined within a namespace; you 'bring in' a definition just as with any other functions by bringing that namespace in. if you don't, you get the default; e.g. // mynewworld.h namespace MyNewWorld { define_syntax foo[N] = my_array&lt;T&gt;; } // mylegacysource.cpp #include "mynewworld.h" int foo[4]// == _raw_array&lt;int,4&gt; namespace MyNewWorld { int bar[4]; // == my_array&lt;T,N&gt; } //mynewsource.cpp #include "mynewworld.h" using namespace MyNewWorld; int baz[4]// == my_array&lt;int,4&gt; Not so far off from the use of templated collection classes which themselves are in namespaces, e.g. we can write '```using namespace std```' , then ```vector&lt;T&gt; == std::vector&lt;T&gt;``, or we can make our own ```template&lt;typename T&gt; class vector { ...}``` and use that instead (which is kind of why they put it all in 'std::' .. to avoid clashes with local refs) .. and the 'using' mechanism allows selecting between them, *in the same program*. or put it another way - just imagine ```foo[4]``` had already been written ```placeholder_array&lt;int,4&gt;```, and through our choice of namespace (or default) we're just assigning the symbol ```placeholder_array``` at will. and yes, 'using namespace std' in a header is a nightmare, unless you constrain it. e.g. namespace MyWorld { using namespace std; // ok from here on because it wont leak // but anyone else including this header can choose to enter 'MyWorld' // by saying 'using namespace MyWorld' // and get all it's settings without manual prefixing each time. } 