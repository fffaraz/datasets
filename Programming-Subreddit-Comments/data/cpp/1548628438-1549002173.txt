First make sure it’s in C++. I say this because each chip vendor determines the language and provides the compiler. In 1993, using C++ for embedded wasn’t as common as it is today. 
A better name, IMO, would be `delegate`, either modifying the member object, or modifying function and pointing to the object. It would also be nice as it could be used outside of this context to implement function aliases. `int foo() = delegate foo2;` or use a more common syntax like `int foo() : foo2;`
I know you're joking, but we could use T^ as the non-owning pointer syntax.
&gt;have you tried passing argument into it? have you??.. if you had you would see this is not working as intended. There are two errors here -&gt; `(*argv + argc)`.
&gt; const is the replacement of #define in C++ when dealing with constants ehmmm... not so sure about that
I learnt C++ from that book, it was good at that time.
How's the F-35?
The whole auto thing makes no sense to me, except where it's needed for template magic. As far as I'm concerned, auto just makes it harder for the compiler to know your explicit intentions and hence to enforce them. &amp;#x200B;
It's effectively a way to make native XAML-based UIs for desktop apps, which people have been begging for since Win8. The 'P' in 'UWP' is not involved, just Win10.
It’s true that x64 provides more architectural registers, but register renaming is highly effective (as I understand it); I am uncertain as to how much of a benefit remains after accounting for that. (Which doesn’t affect my opinion that everything should be 64-bit, and x86 should vanish. charconv in particular strongly benefits from wider math, even without using fancy vector instructions. I wish I didn’t have to spend so much time tuning x86 codegen.)
A ```constexpr``` that doesn't receive arguments is simply a ```const```.
I have another reason. The name observer\_ptr implies to me observation (i.e., read only). However, this pointer is writable which means the value can be manipulated. 
First of all, their best are as good as anyone else in the world. However, I have noticed that for average students, they have quite a rigid system that makes it really hard to change things, and also budgetary restrictions. A side effect of this I have noticed is that they have a tendancy to expect answers to exactly match the text and don't accept optimisations on the basic algorithm.
&gt;As far as I'm concerned, auto just makes it harder for the compiler to know your explicit intentions and hence to enforce them. It's not harder to the compiler that just does what you explicitely stated: use the type the compiler knows about. The main critics of auto are with reading the code, so it's basically the human's deduction that can cause issues. (just in case: personally I evaluate case by case if I want to use auto or not)
I'm sorry I just saw that after my break from doing school. Just applied the fix and thanks I appreciate your input.
TFoo&amp; fooUpdate = getFooObj(); auto fooUpdate = getBarObj(); &amp;#x200B; Which of those has an error that doesn't get caught by the compiler? If you don't tell the compiler what type you expect it to be, then it cannot tell you if it's not that type. If the subsequent code is such that both a Foo and a Bar object meet the syntactical requirements, then you will never know it was wrong. And that may not be terribly unlikely to happen. It certainly is likely enough that I'm not about to take the chance just to save a little typing. &amp;#x200B;
I can understand someone not liking abuse of auto ````auto n = 1``` - did you mean int? did you mean unsigned int? did you mean size_t? It's too vague (in my opinion). But for things like [for-loops it's invaluable.](https://gfycat.com/JauntyScornfulBrontosaurus)
&gt; As far as I'm concerned, auto just makes it harder for the compiler to know your explicit intentions and hence to enforce them. If you don't mind entertaining me for a moment - can you tell me the explicit type you'd use in place of auto in the following: ```for (auto&amp; item : std::map&lt;std::string, std::mutex&gt;())```
Did you really PM me a link to dictionary.com as an authoritative reference? https://www.merriam-webster.com/dictionary/architecting It would appear that the I.T. community's insistence upon using the word has finally swayed Oxford, however. https://en.oxforddictionaries.com/definition/architect
I said above, except for those places where template magic is required. Though of course my stuff doesn't use nearly so verbose template stuff like the STL does, so that kind of thing isn't nearly so much of an issue issue for me. And of course you could have done a using statement above that if you wanted to make sure it's safe, and used that aliased type. If I have any sort of type like that that's not just one or two ad hoc local uses, i.e. it's important to the code in general, I would typically alias it anyway to make it more self-documenting. That doesn't lose type safety but reduces clutter and increases semantic for the reader. 
What if my explicit intentions are "use whatever type is actually needed/returned here"?
Sorry It didn't post properly. Ill fix the screen shot and get you guys my code &amp;#x200B;
It all depends. In your case (if you intended to or not I don't know) your "auto" example is initializing via copy. Most likely you just forgot to include the &amp; but maybe not. In the case of ```const TFoo&amp; = ...getFooObj()``` if getFooObj() doesn't return TFoo but something implicitly convertible to TFoo it's going to compile just fine and implicitly convert to TFoo. It also depends what you're using the variable for. The variable name is not compile time checked which means it effectively has no correlation to the variable type so expecting getBarObj() to return Bar is just a bad assumption.
To me, that is Javascript thinking. You either know what it is, you know what it's derived from so you can use the base class, or it's in a template and you know the template parameter 'type'. There's never anywhere, unless you are passing around void pointers (which is pretty questionable) that you don't have some 'type' that you know it's going to be. If you have that information, and providing it can reduce potential errors because the compiler has more information and can better watch your back, then why on earth not use it? &amp;#x200B;
Can you show me an example of how you'd write that? What I gave was just the easiest to type. In any case I can think of the type of the map would be outside the loop defined somewhere else maybe with an alias.
The fact that we're half a decade or better into investigating this capability and these sort of questions are still up in the air is mind-boggling. The fact that it's a real consideration of whether modules might be standardized without the experience of applying it the standard library is SUPER sketchy. I know I'll be sitting down with my organization's standardization representative this week.
Yes, I forgot the ampersand. The odds of implicit conversion (if you are intelligent and don't go around creating lots of conversion operators and you use explicit liberally) is far less of a potential issue. I don't use ANY conversion operators at all. Every single one I ever used in the past bit me for exactly that reason, where some unexpected chain of conversions was found. But that's a different issue. Whether or not that happens, providing an explicit type is clearly going to \*increase\* the odds of the compiler catching an error. And it's damn more likely that getFooObj() returns a Foo object than a Chicken object. &amp;#x200B;
"Creation of foo.bmi is a byproduct of the compilation of foo.cpp. When compiling foo.cpp, the compiler will emit a foo.o and foo.bmi. As a consequence of this design, foo.cpp must be compiled before bar.cpp!" Worse, foo.bmi may be the result of compilation of quux.cpp. Module names are purely logical, with no physical representation on the filesystem. 
I'm no STL maven. If it's impossible, then of course it's impossible, and do what you have to do. I'm not here to argue minutia or whether the less common or exceptional counter example means the whole concept is invalid. That's internet silliness I'm not going to get into. Ultimately, unless you are doing this stuff for academic reasons, then it's a game of inches and the folks who waste less time chasing bugs, other things being reasonably equal, are more likely to win. So, for me, anything that lets me be more explicit, so that the compiler can watch my back, I'm going to do it. The 2 seconds it takes me to explicitly indicate the type is nothing compared to having some customer look at you with loathing for a few days as you try to figure out why the product is dead and he's not getting business done. &amp;#x200B;
Both your quote and your other remark describe Fortran90 (and on) modules. Are they considered to have been a mistake? 
&gt; To me, that is Javascript thinking. That's naive – in C++ your `string` is still a `string`, you just didn't have to tell the compiler because it already knew that; whereas in Javascript your `string` can become an `int` at a whim. So similar! /s
["Prove to me that it is impossible to write a fast C++ dependency resolver"](https://twitter.com/jfbastien/status/1089590970893578240) is such an utterly batshit insane position to take. I guess he views C++ modules just a masturbatory exercise and so it doesn't matter if anyone can actually figure out how to implement them as long as the committee can just blame the tool makers for just not being smart enough?
Use /r/cpp_questions and read the guidelines.
While I agree that there is a problem with having no mapping, what would happen if it is not added now but it is added later? Would this be possible? I do not think pulling out modules would be nice since it provides, in theory, better compile times, yes, but not only that: much better isolation.
This is a good question, and one that has been considered. In short: Adding a required mapping after-the-fact would necessitate changes to the lookup semantics of existing implementations: A breaking change. The lookup of `#include` directives is not standardized either, and only after a lot of trial and error do we finally have general agreement on how the lookup should be performed (although implementations still differ in a few edge cases). Attempting to standardize `#include` lookup now would break existing implementations. Nevertheless, we are able to make `#include` work (for the most part). However: The lookup of `#include` is fairly cut-and-dry, and there is a mostly obvious way to understand `#include` in terms of the filesystem. Module names have no bearing on the filesystem, so implementations converging on a lookup method is not likely. If an implementation were to enforce a mapping between module names and file paths, we'd end up with multiple slightly different standards, and a later mandate will break compatibility. If implementations instead rely on the presence of a BMI with a name for the module, we end up with the serialized compilation of the DAG, and performance goes in the toilet.
That wouldn't confuse people coming from C# _at all_. ;-]
&gt;How do we make sure that #include &lt;foo.hpp&gt; resolves when compiling bar.cpp? It’s simple: Make sure foo.hpp is present in a directory on the header search path list. We do not need to do any additional pre-processing. Doesn't this require step require you parse to foo.hpp (or have already parsed it before-hand), since it literally copies the content of header-file? It seems like there is no ordering requirement, but really there, it's just hidden. In terms of implementation, how is that different to a module? Can't you simply parse the `foo` module to extract the interface (i.e. without compiling the implementation), e.g. from a header-file or simply via skipping text in the implementation/interface (for one cpp file). And then whenever you refer to `foo` in `bar` you simply refer to a stub/place-holder to where the implementation of `foo` is required. Now to actually compile to this temporary state, can you not compile each module now independently with 'stubs' and then once everything is done merge/resolve it all via an additional step in the compilation process (i.e. now you have to step through the dependency graph in topological order). I have my doubts on whether this would be faster, but it seems like the only feasible way to do compilation in parallel for chain-like (degenerate) dependencies. Though I wonder how frequent chain-like dependencies happen in the real-world anyway, and how large they can get? Surely the dependency graph will not be as crazy to truly hinder performance too much? Perhaps I'm too naive.
The ordering is a case of the compilation of distinct TUs. A header file needs to be _parsed_ before it is used, but it does not need to be compiled in its entirety. The process you've described is essentially the "lazy" BMI generation that we're looking at in SG15, wherein a compiler processes the module interface as-needed when someone imports it, and it will only need to parse the publicly visible aspects that define the interface. In essence, yes: This is how I would like to see it work (where the BMI is generated lazily). The trouble comes that you need to know where the definition of the module interface lives, and thus bringing back the problem of the module/file correspondence. You can either enforce a mapping between the module filepaths and the module names, or you can generate a manifest/mapping file that describes this relationship. The manifest/mapping _could_ be generated by a build system up-front, but you're back at the issue of dependency scanning an arbitrary number of files which requires using the compiler's preprocessor to get it correct. The only solution in that case is to create the manifest by hand, which I do not want (and I doubt many people want it either). Thus, my preferred solution is to use filepaths with module names.
I see, thanks for clearing that up.
To be honest he seems to be a guy who lives in the clouds of abstraction, far away from the mundane world of those who have to suffer the consequences of long compilation times. 
&gt; The current design and implementations require there to exist what is known as the “binary module interface” I'm not sure the specification requires that a BMI exist. As far as I know, the compiler could store everything in the memory of a separated process, that schedule and share data between compilers. I don't think there would be any compiler doing that within the next decade, but nothing in the current specification prohibit that. --- Reading this blog post makes me confused. I watch Nathan Sidwell's talk about it's "Oracle" how the mapping can be extended by tooling and such. I followed the build2 development and module support seem doable when designing the build system with modules in mind. Why one side seems to say that the problem is solvable but so many others says that the mapping problem will kill modules? For example (as far as I know) Build2 run the preprocessor on every cpp files before compiling anything in order to save it's hash. It then re-use the preprocessed file to compile it. It takes a bit more time to start compilation, [but can result in a overall speedup](https://build2.org/faq.xhtml#ninja). In the example given by the author of this blog post, should the same be doable too but with a small extra step to extract the module name? And with modules, running the preprocessor can be very very fast because the number of needed header will dramatically be lower. In fact, the mapping can be done before compiling anything. The compilation order can even be established in advance and compile in parallel for each possible steps. Then only a bunch of (very small) preprocessed files will be compiled, with a known compilation graph. That graph can be stored and re-used for the same configuration. Right now, I agree there is a concern with tooling *given today's tooling*. I might be wrong but it does not seem so far fetched we can develop tools that can deal with modules. On the other hand, I do not expect tools designed for a modular world to be available soon if all we have is a TS.
will modules actually help those template heavy, header only libs?
This is the first succint summary of the module lookup/dependency concerns I've personally seen. This seems like a real issue to me, although I'll want to hear what those involved with modules have to say. (keep in mind that I'm not very involved with modules)
I had basically a similar question, I'm not really sure why this problem is such a big deal. The example he gives, he suggests that there is some processing step that goes into the module, beyond just having a definition file (header file). He called it the BMI and suggested that, to even import a module, the BMI must at least exist, but that would require ordering or some sort of explicit dependency marking. I don't understand why the .bmi needs to exist, or if it does, why it also shouldn't just depend on the existence of the interface (MIU) file. The way I see it, ordering still doesn't matter. import foo results in looking for the bmi, which doesn't exist yet, so it goes and generates the bmi, which as long as the header file / MIU file exists, it generates it and continues compiling. So yea, I don't really understand why this is such a big deal, maybe someone who understands this process better could explain it. As far as: &gt; A Sisyphean Scanning Task I have to admit, I don't really follow the C++ spec changes and haven't read about how modules will actually be implemented, but was the plan really to do away with header includes completely? You might be able to have stupid levels of slow module building but once it's built, you don't have to do so again right? I thought one of the huge advantages of modules was that they were essentially compiled once (unless you're actively developing it) I suppose I haven't really been that excited about modules because I tend to break things up into separate libraries for a similar effect on my own projects.
I think this analysis is missing the fact that most of the work and still be parallelized because most of the work in a modern compiler is code generation. I have not read the proposal in detail but from this blog's description it sounds like it should be possible to produce the BMI for foo.cpp in a separate step before generating machine code. I imagine that this is how Java and C# have to work -- dumb fast compilation to byte code is necessary to import, but then the slow AOT or JIT compiling of the byte code is done later. BMI doesn't even require generating a full byte code, just a description of the interfaces. If it can work for Java and C# at scale I don't see why it can't work for C++.
&gt; I don't understand why the .bmi needs to exist You are justified in this question. In fact, a valid implementation of modules need not require an intermediate "BMI"-type files at all. The issue arises that you need to give the compiler a way to resolve `import foo` to some set of declarations that will be exposed in the importing TU. The current answer to that is to have a BMI with the name `foo.bmi` (or some other file extension). If this file does not exist and we want the compiler to load the interface from the MIU, we need to provide a way for the compiler to map from `foo` to the source file that defines the module. This mapping does not yet exist, and is one of the desires of SG15.
We're well over a decade into this.
Ahh got it. Immediately, compiler flags comes to mind for me, where you specify module name and MIUs. Or just a new keyword that defines a file as part of a module of a specified name. I thought his main gripe though, was the circular dependency. If you have two modules and one application, and module Foo depends on module Bar, the fact that there was a compile step was the problem. Thanks for the explanation, although this seems like a pretty trivial thing to solve. Hopefully it won't ruin modules like the article name suggests.
It's on Github under the name [CustomTabNames](https://github.com/personalmountains/CustomTabNames). You'll have to build it yourself for now.
Wait, a module has to be compiled before it can be imported? What happens if module foo imports bar and module bar imports foo? Is that just not allowed? While that is admittedly a sign that your code structure isn't great it is still a pretty huge limitation that isn't found with the current system (where foo.cpp can #include bar.hpp and bar.cpp can #include foo.hpp).
I'm not an expert, but doesn't C# manages somehow without binding between namespaces and filenames? I mean "starting process is slow on windows" is solvable by daemons - why manifest generation is such a problem?
You're missing the rest of the context: &gt;*const* is the replacement of #*define* in C++ when dealing with constants. *const* will do what #*define* do plus type check, while #define only works as a textual replacement without any type-safety checks. C++11 introduced what is called *constexpr* which is a regular *const* plus ensuring that an integral value is a compile-time constant. They mention constexpr, and when it came in, so they are correct. Quote properly. &amp;#x200B;
I personally love PVS-Studio which is expensive, but they have a free version available. This is a static code analyzer which is very useful. It does a way better job than any other static code analyzer that I've tried and integrates beautifully with Visual Studio. If you do scan a solution, don't double click the issues found and then the free version will last forever. I've used it for years now without any issue.
How dare you impugn the honor of C++17 &lt;charconv&gt;? I demand satisfaction!
&gt; If the subsequent code is such that both a Foo and a Bar object meet the syntactical requirements, then you will never know it was wrong. `error: fooUpdate has ambiguous type. Hint: replace "auto" with a type name` 
Isn't this basically the same problem as linking? We don't know what .a or .o defines a function, either. Admittedly, linking is frustratingly slow, so it might not be a positive comparison.
Microsoft and Google have such experience on their compilers and they are the ones driving the modules design. All the modules FUD that I have read so far, seem to be basically people wanting to keep doing everything they do with header files, while at the same time getting modules and not changing anything on the build infrastructure.
You made me look it up. It's becoming more like perl every day. How many way are there to do ::atoi now? 
I started to get into a discussion about this with /u/gabrieldosrios about this last year but it kind of petered out before getting a satisfying answer: https://www.reddit.com/r/cpp/comments/7a3t2w/common_c_modules_ts_misconceptions/dp7at25
This isn't real right? No one could possibly care so much about something so inconsequential while being so aggressively wrong. Go get offended for no reason somewhere else 
I don't know how relevant it is, but one obvious difference is that parsing C# is dramatically faster and it doesn't have to deal with the preprocessor. I'd guess though that the real answer is just that modular compilation and the traditional one-file-at-a-type build setup that gcc/clang/make are designed around aren't compatible. This has an obvious solution of just not doing that, but maybe "you can't use make to build c++ modules" isn't considered an option?
The thing is, #include dependencies are one-to-one mappings with files found in the search path. It's trivial to resolve these since the name of the file is specified. When you include a file you don't have to preprocess the whole set of files found via the search path. With modules there is no filesystem mapping, an implementation does not know in which of all the available files in the search path the module is declared. This is the point that the author is making here. A file could have a completely irrelevent name and possibly require preprocessing before exposing it's module nature. What is an implementation going to do? It seems as if the only possible solution in this case is to preprocess the whole set of files available in the search path. 
Java has straightforward maps from class and package names to paths. Not sure how it works in C# though. 
Thank you; I hadn't seen the isocpp survey nor the 2018 JetBrains survey, that I recall. :)
&gt;I have not read the proposal in detail but from this blog's description it sounds like it should be possible to produce the BMI for foo.cpp in a separate step before generating machine code. That might help, but I'm not sure how much. One module could very well depend on others, so the BMI generation process will be bottlenecked by the dependency graph. Worse still, since the dependencies are (presumably) determined from the cpp file, the list of dependencies will likely be bigger than those for a public header file. These considerations might have little bearing on the feasibility or benefits of a module system, but I think all of these issues need to be considered carefully before standardizing modules. Given that the most compelling purpose to have modules is to improve compilation times, I think it needs to be demonstrated with an implementation that modules actually improve compilation times BEFORE standardizing it. The standard experimental implementation (in something like a clang extension) should also be made public.
You don't need to know where a function is defined during compilation. You don't need to know during linking either, it's just something you find out during linking. 
The talks on how build2 implements modules from CppCon kinda did the opposite of alleviating my fears. I am utterly dependent on incredibuild/fastbuild at work to be productive. It's difficult so see how we'd adopt modules for anything but the most trivial library projects (and those are *rare*) if the mapping problem is as bad as it sounds. I'm not a build engineer, so I'm just hoping everything turns out alright and that modules don't turn into a feature that's widely ignored. [unrelated] @vector-of-bool: The fancy fade transition when clicking on links to other articles on your blog breaks Ctrl+Click (win10 firefox)/ Cmd+Click (macos chrome) to open in a new tab. Idk if it's a theme thing or w.e.
&gt; most of the work in a modern compiler is code generation [citation-needed] :)
They don't avoid the need to instantiate templates in every TU that uses them, so no.
Watching your last talks I though it was CTAD!
Use PCH. Modules don't do much beyond that in their best case.
&gt; most of the work in a modern compiler is code generation. Source? I have significant amounts of code which are definitely no more than 50% code generation - as evidenced by -fsyntax-only runs.
Seems unlikely they would do any better than PCH.
When I created the site locally I included a non-https mathjax URL and that worked fine (file:// doesn't trigger the ban). Since it's published on github pages it's served via https and now browsers block the mathjax include. I just added the missing 's' and now it'll hopefully render the formulas correctly.
Not merged yet. But you can start from here: https://github.com/mattkretz/gcc/tree/mkretz/simd
That's not nearly the full story either. The ability to assign a value as a compile time constant is a relatively small part of const. Const function arguments and member variables are (imho) WAY more important, and those achieve functionality that can not in any way be approximated with #defines. If the writer meant that defines can be removed and const (or constexpr) used instead, that is (mostly) correct. I think that would be the wrong way of thinking about it though.
Lazy generation of BMIs sounds like it would just create headaches in the long run, because it means that the compiler is trying to take over some work that the build system would traditionally do, without coordinating with the build system. Today, you just tell the compiler to build a source file and it does that, possibly looking in some other locations that you tell it about to find headers. If it also had to create BMIs on-demand, it would need to know where BMIs are stored (which it would have to anyway, similar to include paths), where source files corresponding to those BMIs live (normally the build system's job), what settings you need to use to build those files (normally the build system's job), how to tell when the BMI file is already up to date so it only regenerates it when it has to (normally the build system's job), and how to parallelize lazy generation of BMIs (normally the build system's job). I'd much rather figure out how to make a build system keep the BMIs up to date, than have a mediocre build system built into every compiler. Plus, many newer build systems keep track of fine-grained dependencies automatically by running the equivalent of strace() on each build task. Having the compiler go out and build a random subset of upstream modules would confuse this kind of build system pretty badly. 
Please would someone fork clang already and create abi-compatible cleaned up c++-like language already... I bet we all want that. Especially the part where someone else does that hard work.
I mean it's the same problem from the perspective that you need to find some information that was generated by the compiler from a cpp file. It's different information that's needed, but the process looks kinda similar if you squint a little.
I am quoting Walter Bright's [post](https://www.reddit.com/r/programming/comments/akizzy/c_modules_might_be_deadonarrival/ef5mmef/): &gt; D made the decision at the beginning to have filename==modulename specifically to avoid this problem. Any idea why C++ does not simply copy the Design of D modules in this regard since this seems to work well for D. (This is also the [preferred solution](https://www.reddit.com/r/programming/comments/akizzy/c_modules_might_be_deadonarrival/ef5pany/) of vector-of-bool.)
Not if what you want to express is to use whatever is returned. There is no ambiguity for the compiler. There is for you.
What problem exactly modules solve? Will they help with "all-project-recompiles" if some common header cahnged?
To me it's not vague at all: it's literally int. There is no ambiguity. If the person coding is serious and I read that code, I just assume they are explicitly using the type of the expression on the right. Things get less clear with less clear expressions. auto vec = vec3 * rotationMatrix; What's the type of `vec`? Do I even want to know? What will I do with it? Mostly pass it to another function that will check it's type anyway. The meaning of the expression is clear to me but not the type and that's ok for me (in particular in generic library code). I guess being at ease with that kind of code depends on the context, like if it's generic code or not, if we are manipulating the resulting object directly here (accessing the members) or if we just pass it to another function etc. auto thing = thingManager.find("a"); That is super ambiguous for me (not for the compiler). Other than I will not get a reference, I know nothing about `thing`. The name is ambiguous whatever the domain concepts the library/app is modeling. It's a sure result when looking for something that might not be there. So... Is it a pointer? An optional? An iterator? Something else? Etc. Thats what I don't tolerate. But as said, it's only ambiguous to the reader.
&gt; auto vec = vec3 \* rotationMatrix; This is a particularily bad example as some of the popular math libraries actually use lazily evaluated template expressions (or whatever the correct wording for this is), which led to subtle bugs in production code when combined with auto... &amp;#x200B; It was actually part of the coding guidelines of my past employer, that you were not allowed to use auto for mathematical expressions (and rightly so).
I'm the writer and indeed I'm only talking about constant variables. I expected that this part could be understood in a different way. You're right *const* has many more important usages, but in that paragraph I was talking about optimizing by making the compiler calculate the constant (if needed) and do type-check while insertion. 
You'd think modules would be easy to solve, but it turns out that some people can't think of obvious solutions. Sanely, copy what D does, or even Python ~ couple module name to a file path, and nothing more or less. 
In compiling, there are a few discernable steps in the process: - Parsing your code into correctly resolved inter-class/function links, and instantiated templates where necessary - Optimizing the visible classes and functions with the known information we have - Exporting this into code for your target platform. A simple invocation of your compiler does all three. If you run it with LTO, you run only the first two during compilation, and then the last two during linking. (Yes, you optimize twice, basically). The first task has a big complication, which is that how your file parses depends on whatever came before it. The problem is that your file may parse differently every time, so this time-consuming task must be re-done every time you load your headers. This can be optimized if you either know it's always the first (which is what PCH accomplishes), or if you know it must be parsed independently anyway (what modules do). In that case, you can run step 1 separately, and then run step 2 separately. This is 1 gain from modules - we now get "headers" with very clearly outlined function &amp; parseability, without outside pollution risks. To do this though, we need to have all the imports from one module to precompile the next - the dependency tree problem. The second that we need is that given an import, we need to know where to find the BMI (and potentially the source file generating it). This is currently "magic", where by some unknown instance your build system causes the right build order &amp; dependencies, and magically makes a module map come to life, which does this mapping for your compiler. This is the problem with modules right now, and from what I can tell this does not look favorable to be resolved at Kona. I'll be going there to hopefully help out to get it resolved, because I really want the #1 result from modules to exist, and #2 is for me a secondary thing.
The full story is that its target audience is for embedded C programmers, who overwhelmingly still use defines for constants. That is the biggest issue to address for that target audience.
When it comes to the AAA I'm not concerened about the compiler, but much more about my coworkers. I think using auto lets you write faster code, but it takes longer for others to read/understand/being able to change this code. especially when it's entirely clear what the types are (or the type might actually matter). &amp;#x200B; The worst offender (and I said this a couple of times on reddit already), is actually using auto as argument for non generic, non-trivial lambdas... which I have seen way too often for no good reason. IDE will stop providing auto completion, so it's really a big step backwards for being able to write some code a few seconds faster.
&gt;In short: Adding a required mapping after-the-fact would necessitate changes to the lookup semantics of existing implementations: A breaking change. Wouldn't putting modules in a TS help there? Modules would be around, and could be used and tested, but breaking changes to implementations are still allowed.
It didn't adhere to the UNIX philosophy. It didn't do one thing, and it doesn't do it well. It's also designed for fighting wars of the past, and not the future. Nothing to do with it being programmed in C++.
So what you are saying is that the standard needs to define the lookup rules for `import foo`? Given that the standard tried to avoid talking about file systems and so on I find that very unlikely. Why isn't it possible for the three major implementations to come together outside of the c++ standard and decide on one? As long as the standard doesn't prevent a same solution, I indeed don't see, why this should be a concern of the c++ standards document.
This comment is pure gold. Thank you! 
Java has classloaders, only the default one maps directly to the filesystem. 
My response to vector-of-bool is a solution that permits this to some degree. D does not have to target some operating systems lacking more than one level of directory access, or that do not have a real concept of file extensions (such as zOS). As a result P1302's approach was designed to tackle this, and allow us to tackle the naming scheme at a later date while also not absolutely wrecking the ability to port existing code bases to modules as soon as possible.
&gt; I'll want to hear what those involved with modules have to say One easy solution to the problem would be to break compilation into an additional phase. Does the current wording impede generating bmis independently without actually compiling to object files? A dumb buildsystem could just generate all bmis upfront and than give them all to the compiler when compiling modules to object files. This would not require any mapping of files to module names. It would work very similar to the linking step today. A smarter buildsystem could try to extract dependency information from the bmis and than start compiling object files as soon as the required bmis for a module are all ready, to increase parallelization. It would only hand the required bmis for a module to the object file compilation step. It could even cache the dependencies for the next build (in hope they did not change significantly) and order the bmi generation so the actual module-object-file compilation can start as early as possible.
Just not allowed, see wg21.link/P1103 .
Sounds good; the observe_ptr should only come into play if it can automatically become null on destruction of observed object, without this it's just noise. Otherwise my solution is the same as it was before smart pointers, that is comments at declaration: // owns and // does not own 
100%
Nope (or let's say: very unlikely)
Maybe I'm too naive, but how is the lookup any more problematic, what current build systems already have to deal with. Depending on a libray means I have to tell the compiler/linker, which include directories to add and which binaries to link to my project neither of which need to have any obvious connection with the libray in question. So just as I'm telling cmake today, what files and include directories belong to a certain target and on what other targets it depends, why can't I just tell cmake, what files make up a module and what other modules it depends on?
I feel [this proposal](https://groups.google.com/a/isocpp.org/forum/#!topic/std-discussion/-We7z1p5L44) might be a good alternative. But I wrote it myself so I'm biased. Unfortunately I cannot post on my own topic anymore: Google believes I'm sending bulk email to the list (five whole messages!) and marks it as spam :-( &amp;#x200B;
Sure, templates are not necessary for many problems, right tool for the right job.
Yes, obviously I was talking about the one that maps to files. If you're using another classloader that will have its own way of mapping classnames to classes.
While C++ is generally much more type-safe and at the same level of efficiency, C still uses fewer resources to build the project which is very critical to embedded system design. I think C++ is fine for systems with large amounts of RAM and ROM, but otherwise (as in the majority of cases) you have to mainly use C and ASM.
\&gt; Unfortunately I cannot post on my own topic anymore: Google believes I'm sending bulk email to the list (five whole messages!) and marks it as spam :-( &amp;#x200B; This is actually because Google decided to silently reduce the number of people a google group can have to about 500. It's not you. It's entirely Google and people are working to fix it. But I'm not aware of any details beyond that.
OK, thanks for your answer. Modules is too important to get it wrong, it would be a nightmare if e.g. half of the C++ community would avoid modules due to a serious design flaw that cannot be fixed later on because this would break backwards compatibility...
&gt; C still uses fewer resources to build the project which is very critical to embedded system design Are you talking about compiling project _on_ microcontrollers (not _for_ microcontrollers)? Otherwise why would you care how much RAM does c++ build systems and compilers consume? 
&gt; Modules is too important to get it wrong, it would be a nightmare if e.g. half of the C++ community would avoid modules due to a serious design flaw that cannot be fixed later on because this would break backwards compatibility... &amp;#x200B; Absolutely agree. I've been mentioning these issues for well over a year and while people paid attention a year ago, the community ended up waiting to see where merged modules would take us. We have some options, but we *need* to fix these small issues before modules are... well... *merged* into C++20 &amp;#x200B;
C/C++ should probably have used a python-like module system that from the beginning, but I'm not sure such things existed or were thought to be needed at the time. As far as I'm concerned, the way that the compiler finds all the source files belongs right in the language. Modules would make that possible if done right. When you use one, the compiler could search the current directory, then the command-line specified ones, then the sys defaults. The current 850 build systems add so much confusion. I suspect it actively discourages people from using libraries, despite the fact that the compilers are great at removing unused stuff. You might even be able to ditch makefiles completely, if you made module-ified wrappers or forks of all your dependancies.
I'm glad to see this issue with modules is getting attention. I've been talking to the people working on modules for a long time and explaining why buildsystem maintainers need to be part of the design discussion https://twitter.com/steveire/status/1072904573675814914 Bizarrely, me explaining this on the CMake list was celebrated as 'Talking to the CMake folks' https://twitter.com/jfbastien/status/1072882728788361216 https://cmake.org/pipermail/cmake-developers/2018-August/030819.html I hope for the best! :)
Is it just me or is the proposed `std::aligned_storage_for` in [P1413R0](open-std.org/JTC1/SC22/WG21/docs/papers/2019/p1413r0.pdf) the same as the already existing [`std::aligned_union`](https://en.cppreference.com/w/cpp/types/aligned_union)
Thanks for the correction!
I agree. it's easier to be efficient in C. C++ can be as good as C, it's just really easy to make it much slower by accident. e.g. you add a destructor to class A. one line even. suddenly all classes that use A also have a destructor. And the compiler wants to be "smart" so it inlines this destructor everywhere and suddenly your code size increases by 100* the size of your destructor's instructions. oops. so you say __attribute(no inline) or whatever the syntax is. compiler is too smart... decides to ignore you. wtf. another example... you use inheritance once. OH no! your class has a new pointer right in the best part of your cache. and so your super hot member is no longer the hottest member. and btw all your function calls are now a long read. oops another example. you throw in one function. it's really a jump... you catch in the same function! welp now your whole code is way bigger. and you have to write noexcept everywhere. and also suddenly things randomly STD::terminate and that's never supposed to happen. what about templates? each instantiation is adds a new binary implementation... fuck so yeah c++ is fine as long as you avoid inheritance, cries/dtors, exceptions, and templates. which means not using the STL. or just use c 
That information is kind of useless for tooling when it does not apply in the general case. The java language specification itself has several examples with multiple top level classes in one source file and compiled binaries can end up in any form as long as one of the available classloaders can load them. It is a nice convenience for humans when source path -&gt; package name and while that can be immensely important to the user it doesn't help the tool developers. 
that does appear to be what u/TheAngel37 wrote, but I think she/he meant that C++ binaries are usually a lot larger than C binaries. Often the C++ "equivalent" of a C program won't fit on a chip. BTW it's imprudent to ignore the resources required for compilation. c++ has a compilation time problem for medium/large projects
I remember the ordering being mentioned by the author of GCCs module implementation in his CppCon talk last year. I assumed they were aware that this may become problematic later down the road. 
&gt; Module names are purely logical, with no physical representation on the filesystem. So is a target name in my cmake file. What is the problem?
It's supposed to be the best/fastest one though. Especially for floating point.
Modules have limitations on them which may make them easier &amp; lighter than pch. Ex: #defines in the includer don't affect the module, but do affect the pch.
The C++ standard has no understanding of this 'file' thing of which you speak. 
That's mostly because embedded programmers don't like to turn the optimizer on. The compile time for C++ is a tradeoff. Compile times are nothing compared to wasted customer time due to bugs and security issues that would have easily been prevented with higher level features.
Sounds like you don't know what you're doing.
It's worth repeating the the C and C++ standards are also silent on the matter of how header files are discovered by `#include`, or even whether the argument to `#include` is a file at all -- we have just developed a convention over the past 45+ years that `#include &lt;foo.h&gt;` looks in well-known places on the filesystem, a search list that is augmented by passing `-I` flags when invoking the compiler. (`#include "foo.h"` does the same, but searches relative to the current working directory first). This being the case, I wonder if module mapping is something that needs to be explicitly specified in the C++ standard, or could also come under the banner of "technically implementation-defined, but in practise everyone does it the same way"? So let's say we are compiling a module `acme.bar`, which depends on the interface of module `acme.foo` via an `import` declaration. * First, the compiler looks for `acme.foo.bmi` in its BMI cache, the location of which could be given as an argument to the compiler when invoked by the build system * If that exists and uses the same options as the current build (an internal hash of the current compile flags/defines placed at the beginning of the BMI), then the compiler uses it. Otherwise, it looks for a file named `acme.foo.&lt;ignored_extension&gt;` (or `acme/foo.&lt;ignored_extension&gt;`) in a list of search paths; this could default to `[${CWD}, /usr/include/c++_modules, /usr/local/include/c++_modules]`, augmented by a list of paths passed to the compiler with a `-M` flag, akin to `-I`. * If that cannot be found, a compilation error occurs; otherwise, the compiler builds both modules, and outputs both `acme.foo.bmi` and `acme.bar.bmi` to the BMI cache directory. Now, if we have a third module `acme.baz` which also uses `acme.foo`, we go through the same process; but if `acme.bar` has been compiled first, then `acme.foo.bmi` will already exist, and will be used. But the build system can attempt to compile `bar` and `baz` at the same time; both will try to write `acme.foo.bmi`, and whichever finishes first "wins". This is doing more work than we really need to, but allows for more build parallelism as the author wishes. This requires a convention that the file defining the module `acme.foo` is named `acme.foo.&lt;something&gt;`, or perhaps has the path `acme/foo.&lt;something&gt;` relative to one of the `-M` paths. But this doesn't seem particularly onerous, and it's more-or-less what we do today with header files. As far as I know the C++ standard is silent on the subject of TU file names (or even whether a translation unit *is* a file with a name), so making this a requirement isn't something the standard need concern itself with. Odd platforms with unconventional file systems still have the freedom to do things in odd ways if they need to, but the 99.9% of developers using Windows or Unix will still be able to get builds that are as parallel as they are today. But perhaps I'm being spectacularly naive, in which case please do let me know :).
Why not just copy Haskell's .hi file design? I really don't understand what's the problem.
Yes, I wrote to Herb Sutter (the list owner) and he told me about the problem. It's rather frustrating: I really think the language can move in this direction and it would massively improve all of our lives, but unfortunately I cannot even go and answer questions about it :-( 
Turning on optimisers makes it very difficult to debug embedded systems because the disassembly code will not be in the same order - an interrupt can change volatile variables and suddenly the code no longer does exactly what it was intended to do.
Even so ~ this is a new feature of an upcoming standard that can have an understanding possibly bolted on in the form of `module` and `import`, no?
You're not the only one to say this! I have an email in my inbox that I intend to respond to today about this. I really don't think it's the same, but I'm happy you hear your thoughts. The point of the proposal is to simplify the API so that there is only one way to use it and that way is the correct way (and by correct I mean follows the pattern that the vast majority of uses follow). The leading byte count and the veriadic typenames of `std::aligned_union` make sense for the type, but they don't make it as easy to do the right thing. Passing in a `0` to the first template argument just to meet the API is very awkward and not readable. It forces future readers of the code to ask what that number means and if the author really intended to make use of a size-0 buffer (even though that isn't possible in C++). I get why `std::aligned_storage` and `std::aligned_union` were standardized the way they were (to capture all possible usecases), but that doesn't mean the APIs are good for the average usecase. Also, there's a semantic difference. Aligned storage is intended to be used with one particular type while a union is, well, a union. If the author only passes in a single type argument, does that mean they intended to leave the door open for other types to be passed in as well down the line? Or is it simply that they probably should have used an aligned storage but chose aligned union because of the auto-alignment implementation detail. Essentially, the author's intention becomes ambiguous. I probably should have addressed this in the paper. I instead focused entirely on `std::aligned_storage` because my proposal was simply a typedef on the existing type. I very intentionally did not propose a new type with a separate definition because the intention is to make callsites more obvious and readable without actually changing the underlying type. 
Do you want to manually create target for every single file in your build? No? Then that's the problem.
The people that vote on the standard are preventing it presumably.
You are doing that already (consider the translation from cpp files to .o files - each `.o file is a target in make and the connection between those names is purely by convention). Just because the build system has to generate those names explicitly doesn't mean you have to actually, manually type them instead of using some sort of generator. Also, there is absolutely no need for a one cpp file per module approach, but I guess that comes down to personal preference. 
:(
If you change the interface of a module, then everything that uses that interface will need to be recompiled. There's really no way around that. That also goes for `inline` functions and things that are implicitly `inline` like templates, `constexpr` functions and member functions defined in-class, if I understand correctly. So not really any change to the status quo, except that we could potentially be a bit smarter about noticing that e.g. adding comments doesn't actually change the binary interface. The potential benefits that modules will bring are nicely summarised in the linked post.
\&gt; e.g. you add a destructor to class A. one line even. suddenly all classes that use A also have a destructor. And the compiler wants to be "smart" so it inlines this destructor everywhere and suddenly your code size increases by 100\* the size of your destructor's instructions. oops. &amp;#x200B; If your class needs a destructor then it needs one in C or C++. But in C if you didn't have it everywhere (by hand, but with the same code size increase) then your code is incorrect because you're not tearing things down correctly. I think most of the C vs C++ comparisons that come out in favour of C only do so by implicitly allowing incorrect programs in C, versus correct ones in C++. \&gt; another example... you use inheritance once. OH no! your class has a new pointer right in the best part of your cache. No it doesn't. If you're talking about the vtable then that only happens if you use virtual functions. The same happens if you use virtual functions in C. However in C, the idiomatic way is one virtual function pointer per virtual function. The C++ way uses a single pointer to a struct of virtual function pointers. That is shared among all instances of the same class, so C++ tends to be more memory efficient than C if things are done the normal way. &amp;#x200B; \&gt; another example. you throw in one function. it's really a jump... you catch in the same function! welp now your whole code is way bigger. and you have to write noexcept everywhere. and also suddenly things randomly STD::terminate and that's never supposed to happen &amp;#x200B; Yeah exceptions do cause code size increase, though many of the comparisons are against C code which doesn't clean up properly after itself. But exceptions are not ideal. &amp;#x200B; \&gt; what about templates? each instantiation is adds a new binary implementation... fuck Same as macros. Which C has. C++ templates are a formalisation of what we did with macros all the time anyway. Those problems are also reduced with a good compiler (with whole program optimization and identical code folding). You also have a choice with templates how to implement them, choosing the fastest or smallest implementation. The language can't know which you want, and ultimately you have to choose that in C. Do you want a slow function like qsort with no bloat or a much much faster function like std::sort which can give code bloat? &amp;#x200B; &amp;#x200B; &amp;#x200B;
They are already in a TS.
And there's also [https://blog.conan.io/2018/06/11/Transparent-CMake-Integration.html](https://blog.conan.io/2018/06/11/Transparent-CMake-Integration.html) which I just discovered.
I've read through the article and all (66) messages posted so far and I have to say I'm not sure I understand both what is intended with modules and why it is needed. My C++ background is old, been many years that I've been a Java developer so that perhaps accounts for my confusion, but how is it C++ doesn't already have the same capabilities as Java or even needs them? Java defines namespace by package name and class by file name. Generally source code is arranged in a directory structure that mimics the package name (though strictly speaking that is not required). C++ has namespaces that could be arranged in the same source structure and are the equivalent of packages. Java uses the *import* statement to identify dependencies. When Java compiles it looks at the defined classpath for referenced imports, if it finds a compiled class it uses it, if not and it finds the source file it compiles it then uses it. C++ uses the #include and looks in all locations defined for include files, much as Java looks at the classpath. C++ doesn't require the dependent code be compiled until linkage time, it works of a declaration file (usually a header). So about the only thing I see different here is when Java needs a class that isn't already compiled it simply compiles it and uses it because Java treats a .class file like C++ treats a .h file and if it doesn't exist, Java creates it. C++ developers generally create the .h files themselves along with the source file which gives the compiler everything it needs to compile. Linking is a different beast, Java with dynamic class loading technically does runtime linking whereas C++ does that via shared libraries. So back to the original question, it the desire to have C++ be such that we only need to define a source file, then when the compiler is compiling one source file **foo.cc** and **foo** imports **bar** that the compiler will find the source **bar.cc** and extract the header type information automatically and possibly compile **bar.cc** automatically?
I see. Yes, I think your paper should definitely contain a discussion of possible alternatives. Personally, I'm still not sure if it is worth it, because it is not all that common use case and too close to existing functionality. Now, if you added a type that let's me easily **and without UB** construct and retrieve the object in the storage (e.g. with emplace, get and destroy member functions that perform the correct new/casts/launder calls), that would add a real benefit to me, but of course, it would also be much more difficult to bring into the standard.
Oh. Shouldn't they stay there until some experiences have been collected modularizing stuff? Why then push for integration into the core language, if they are both a TS, but not used enough to say how well the idea does? 
&gt; SG15 has only had face-to-face meetings. The last meeting, in San Diego, was useless as the chair was absent and people were too busy getting caught up since the prior meetings to have any useful discussions. With no SG15 meetings outside of those at the official WG21 convenings, the members thereof have difficulty staying up-to-date and collaborating on work. In addition, many times that SG15 has attempted to raise issues they have been shot down as their work is considered “out-of-scope” for the C++ language. &gt; &gt; A Tweet about the pre-Kona mailings spawned discussion of C++ modules and p1427. [Questions were raised about who to trust regarding module toolability.](https://twitter.com/horenmar_ctu/status/1089542882783084549) &gt; &gt; This discussion culminated in an eventual call for SG15 to [“STFU”](https://twitter.com/rodgertq/status/1089580076729982976?s=19) unless they can provide code samples that prove the problems they outline. If there's one thing to learn from Rust, it's to do exactly the opposite of this.
&gt; destructors.. yes of course sometimes cleanup is necessary (not always!). doing it well by hand means I will goto cleanup instead of having many exit points. which makes the function smaller &gt; inheritance... yes I should have said virtual functions instead of inheritance. my bad. in c I tend to avoid inheritance all together. of course you could recreate the same or worse things to virtual calls &gt; templates... macros... yeah but sometimes it's silly. like templating on a lambda or on an integral type. of course sometimes that's what you want but it's so easy to have another instantiation and that messes up the binary &gt; time (faster) vs size (bloat) it's a trade-off... the language shouldn't choose for you
I like to think I do! but even if you are the best dev, there's probably a newbie who's going to use your code one day, and that's when i want it to be really hard to abuse my code
We'll see where the discussion at Kona takes us :) This is my first time writing proposals for the standard and my first time attending a standard's meeting. It wouldn't surprise me if I have to go back to the drawing board before it passes LEWG+LWG (if ever).
In .NET an Assembly (aka .DLL) is the lowest unit of compilation. So the compiler matches the *using* statement with the set of available namespaces from referenced libraries. This is what many are against, giving build knowledge to the compiler.
That's likely a symptom of accidentally relying on undefined behaviour in your code. Likely because you don't utilize the optimizer, so you manually "optimize" the code yourself, and introducing the undefined behaviour you blame the optimizer for. Meaning the code, even if unoptimized, will eventually screw up, but on a customer site, instead of your debug session.
Parsing is the problem you are missing. Java only parses the source code once, then any client of a specific library only needs to read the public symbols table from the jar file. Whereas in C++ the header file will be processed every single time there is an #include. With the increase in template usage, this leads to logarithmic complexity in build times. There are pre-compiled headers, but they are a brittle solution with different outcomes depending on the compiler.
It is called D.
&gt; e.g. you add a destructor to class A. one line even. suddenly all classes that use A also have a destructor. And the compiler wants to be "smart" so it inlines this destructor everywhere and suddenly your code size increases by 100* the size of your destructor's instructions. oops. I assume you added that line for a reason. Likely because your program was incorrect before you added that line. So now you have a choice between correctness and a small binary. If you dont care about correctness, i suggest having no code at all. &gt; another example... you use inheritance once. OH no! your class has a new pointer right in the best part of your cache. and so your super hot member is no longer the hottest member. and btw all your function calls are now a long read. oops You mean someone used the `virtual` keyword. Inheritance alone doesnt cause the problems you described. &gt; another example. you throw in one function. it's really a jump... you catch in the same function! welp now your whole code is way bigger. and you have to write noexcept everywhere. and also suddenly things randomly STD::terminate and that's never supposed to happen. Why are you compiling with exceptions enabled on a resource constrained platform? And why are you surprised about `std::terminate` being called when you, apparently carelessly, spread `noexcept` all over the code-base? &gt; what about templates? each instantiation is adds a new binary implementation... fuck Whats your alternative? Macros? Or maybe implementing every variant by hand, with cryptic suffixes to differentiate all of them? All of these are problems you can solve yourself. For example, every compiler i know of lets you disable exceptions. If you want to have a jump somewhere, use a goto as a last resort. If you sometimes need a bit of code to destroy an instance of a class, but dont want it inside the destructor, dont be shy about spelling it out wherever its needed. Or create a derived class with that bit of code in the destructor. Or create a custom deleter and use it with `std::unique_ptr`. Now, all this may not apply to esoteric compilers or those from before this decade, and if thats the case for you, fine. Do what you have to do to get a working product.
&gt; it doesn't have to deal with the preprocessor. C# does have a preprocessor.
But actually, it will be very easy for a newbie dev to abuse your code. eg, a newbie will not know the ins and outs of your custom-rolled cleanup routines and cause a bug, whereas a proper constructor/destructor will make sure that can never happen. If you don't use templates for generic code, you force newbie devs to reimplement the same function for each new structure type, leading to them getting it wrong, and also introducing the same binary bloat. Or you force them to use function pointers and casts to and from void pointers. A lot of destructor and template stuff disappear at compile time even at a low level of optimization, so your concern is unfounded.
Indeed. Also, don't forget, that my answer is just a single opinion of a random guy on the internet.
&gt; they'll forget to cleanup... would you rather they make the code too big due to dtor inlining or forget to clean up? which would they probably catch without asking for help? &gt; templates ... generic generic code doesn't require templates or macros. don't see why the rest of that paragraph is true. there are lots of libraries in c... &gt; the compiler will fix my dtors and template stuff compile a medium/large c project. then rewrite a struct with a dtor and see what happens.
D's nuts.
What's the diff between your master and gcc\_example branches?
It is not a clang fork, it does not have a compatible ABI and GC is optional in theory, but opting out is inconvenient enough in practice that we can count it as non-optional. Be nice if it was D, but its not :|
I think he means comparing the result of GCC 1.27 compiled by GCC 1.27, versus GCC 1.27 compiled by the latest GCC.
&gt; you need dtors there are lots of ways to clean up. not all c is incorrect &gt; virtual.. yes I should have said virtual &gt; don't use exception yeah.. that's what I'm saying. you shouldn't use them. another example of an unusable c++ feature for embedded &gt; generics look at a c library. of course what you say is not the best way &gt; use go-to instead of exceptions e.g. out of a lambda? a generic function? you cannot always &gt; do what you have to do exactly. if you can use c++ and not stress constantly then go for it. 
`enum` is the replacement for defines. `const` (or `constexpr`) is not. It can still take up unwanted space/needs to be stored in memory (if compiler can't prove it's address is not taken anywhere).
 Need to get off the high horse, accept that module &amp; file names need to match(do what everyone else does ffs). Those using antiquated OS without filenames can keep using &lt;= C++ 17-- they are probably 0.00001% of C++ users, seriously, why the hell would you hold C++ back for that??
I understand the compilation model, but I'm not really understanding how this is an issue. Java is still technically parsing the .class files as well it just happens to know where the specific symbol tables are in the structured format.
&gt; Otherwise why would you care how much RAM does c++ build systems and compilers consume? I'd care if a project of reasonable size can't be compiled in a reasonable time on a reasonably powerful PC. Though, I'd argue that's not specific to embedded and probably not what was meant (since in embedded you can often allow for a build to run for several days if it means the final executable would fit all the constraints perfectly and you don't need to do that often).
I'll have 10 to 100 times as many used headers as I do source files. A change in order of magnitude is not just a charge in degree, but in kind. Expressing how to build all those isn't feasible the same way that using libraries is. 
I think the freestanding guys are starting to do a good job. They seem to be putting things in the direction that freestanding should not be an all-or-nothing feature. Depending on environments the restriction can be real time interrupts or forbidden soace overhead or no heap allocations (or customized ones). I think that well-done, C++ can eventually replace C in this space since I see the implicit object lifetimes and better suppor for type punning coming, volatile deprecations and others.
If a module were library sized, that might be feasible. But the design goal is to be header sized. So manual process that's just barely ok with libraries doesn't look as good. It's also not a personal preference, since I use far more c++ code than I personally write. My company might be able to make some style guides, but there's a lot of open source to influence, too. If `import boost.asio` is the best that can be done, modules hasn't done it's job, in my opinion 
&gt; &gt; you need dtors &gt; there are lots of ways to clean up. not all c is incorrect That was not my position, so good job defeating a straw-man. Im saying you either needed the code inside the destructor, or you didnt. If you didnt, why did you add it? If you did, why are you complaining? &gt; &gt; don't use exception &gt; yeah.. that's what I'm saying. you shouldn't use them. another example of an unusable c++ feature for embedded I dont think anybody is seriously suggesting you use exceptions on microcontrollers. But so what if its unusable for embedded projects? Other projects may benefit from exceptions, C++ does not have to be usable in embedded environments in its entirety. &gt; &gt; generics &gt; look at a c library. of course what you say is not the best way I dont know what youre suggesting. What library should i be looking at? &gt; &gt; use go-to instead of exceptions &gt; e.g. out of a lambda? a generic function? you cannot always Holy shit. No. You obviously cant always use a `goto`, but this applies to C as well. So maybe keep the comparisons fair, and dont blame the language when you want to do something insane and it blows up in your face. C++ already tolerates far more insanity than many are comfortable with, but using exceptions to `longjmp` three layers up the callstack might just be a bit too much. I suggest rethinking the design that forces these types of constructs.
That would impose severe implementation and optimization problems.
I’m not sure how much embedded systems design you have done, but typically there are hardware considerations that you need to take into account that aren’t visible by the compiler, which means most of the things that are done are what you would call ‘undefined’ in a regular programming environment. Strong knowledge of the computer architecture is imperative to manipulate the hardware, so the behaviour is defined on the developing platform. I understand that this is a C++ forum, but embedded systems need the software to be procedural and bloat-free. Adding classes, templates, and other C++ only features are usually avoided, and good practice is often ignored to minimise resources and maximise speed. Compilers are often primitive, normally can only compile ASM and C89 - you would be lucky to have a C++ compiler at all, let alone a modern one. Arduino is the exception to this (.ino is a fancy C++), but industry will typically use a DSP or a more sophisticated MCU. What you already end up with (assuming you are a good programmer) is highly optimised, and so any further optimisations will have negligible impact on performance but like I said can have effects on the procedural order of the code. I am not against C++ - rather, it is my favourite language. It has such a broad variety of uses and is used everywhere in industry. It’s fast, it’s challenging and there are so many ways to use the language. However, the one thing it isn’t is light. C is still king for embedded systems, and the C++ that exists on them may as well just be C.
Again, current build systems already automatically generate build rules on a per file basis according to some non-standardized rule set, that may or may not be changed by the end-user. Why does sich a rule set now have to be standardized in the language? Just because someone has to manually tell the compiler "this module is created by that/those files/can be found here" doesn't mean that the en-user has to do anything manually at all. &gt; It's also not a personal preference, Personal preference includes the personal preference of the people in charge of your companies style guide. The current module design explicitly allows for multiple implementation files for a single module and it also enables aggregate modules and I think people will make use of that. But all that is besides the point: As I said. Even if you create a separate module for each file: putting in place a rule for that in your build system of choice should be simple.
A BMI could cache template instanciation.
Politics, and egocentrically thinking that they know better than everyone else? It's what happens with committees controlled by the arrogant. At least, this is what seems to be happening from my point-of-view.
What I'm saying is the mapping can be done on the fly at a very low cost. It would be done after the preprocessing of all files, and just before compilation. When all files has been preprocessed, a very simple parser can be made to extract the module name. That parser can be run in parallel. When extraction is done, you have the whole graph and the whole mapping, plus every preprocessed file. You just have to run the compiler in the right order, and even run in parallel when possible. I'm not a build system expert, but in my mind it seem quite simple. Of course, a build system supporting modules must be a bit smarter than run a simple list of compiler commands.
&gt; The thing is, #include dependencies are one-to-one mappings with files found in the search path. This is not mandated by the standard. If you write `#include "foo/bar.h"`, the standard does not specify how or where to find `bar.h`. This is entirely a implementation defined behavior. I do not see why the same cannot apply to modules too.
Is your code pathological? Have you made it 100 layers of templates deep? ;)
I think if you think about this for half a second you will realize that with non-pathological source files that this must be true. To verify yourself dry compiling some projects with optimization turned off and see the difference. Code generation involves an enormous set of NP problems fought with an enormous set of hand-tuned heuristics. Parsing by comparison is a cake walk, even for a screwed-up grammar like C++.
The failure case is really obvious: `std::bitset&lt;65&gt;`, which is very conceivable.
The problem with PCH isn't that it's not helpful, it's that it's so damn hard to get set up that people rarely use it.
How is variadic if you limit the number of cases to 5?
1. The code supports up to 5 cases. If you want it to handle more cases. You will have to copy paste line46 - line55 a couple more times. 2. Not sure by how much the compile time will increase, I am guessing a lot. 3. The below 2 functions `s0`, `s1` should generate identical asm code. using InputT = std::uint8_t; using OutputT = std::uint64_t; constexpr InputT cs[]{1,3,5,9,7}; constexpr OutputT os[]{7,8,7,5,3}; constexpr OutputT d{999}; //constexpr OutputT s0(const InputT&amp; input) { switch (input) { case cs[0]:return os[0] + input; case cs[1]:return os[1] + input; case cs[2]:return os[2] + input; case cs[3]:return os[3] + input; case cs[4]:return os[4] + input; default:return d; } } //constexpr OutputT s1(const InputT&amp; input) { return variadic_switch_with_default_case&lt;InputT,OutputT&gt;( input, std::integer_sequence&lt;InputT,cs[0],cs[1],cs[2],cs[3],cs[4]&gt;{}, [&amp;]{return d;}, [&amp;]{return os[0] + input;}, [&amp;]{return os[1] + input;}, [&amp;]{return os[2] + input;}, [&amp;]{return os[3] + input;}, [&amp;]{return os[4] + input;} ); }
Ye, having a fixed limit of `numCases` is lame I know, but you can increase the limit.
I've heard that the bmi format will be compiler specific such that they would have to exist for each compiler.
Well, thinking logically, this would mean you can access book2.book1.book1.book1.book1.book1... ad infinitum. I'm not sure your computer has enough memory for that. What are you actually trying to do here?
...and having now written all that, I perhaps need reminding of why having, say, 32 cores all racing to produce the same `base_module.bmi` is a good idea? If we have proper dependency information, we can use one core to compile `base_module.cpp` and the other 31 to compile any other available TUs which don't depend on `base_module`. Once `base_module` is finished, we can unblock everything else to build in parallel as before. Can we actually extract such dependency information cheaply? Well, [Build2 seems to manage it](https://build2.org/article/cxx-modules-misconceptions.xhtml#build), so I don't see why CMake/Meson/etc shouldn't be able to as well. It would be useful to get /u/berium's opinion on this :) 
They want code to... prove that it *cannot* be done? They want a negative proven?
No, you cant. The compiler needs to know the amount memory that requires an object, and since the class Book contains a Book, you need an infinite amount of memory, which is not possible.
The standard says nothing about how to locate include files either. It does't even say that given `#include &lt;foo\bar.h&gt;`, bar must exist as a file. What we have now is currently a convention, adopted by compilers. So I fail to see why similar convention cannot be adopted for modules. I also do not understand why it is the compiler's job to locate BMI's and not the build system's. I can easily imagine a protocol where upon encountering a import declaration the compiler goes and asks the build system to give it the corresponding BMI. So it goes like this: &gt; Compiler: I see a `import foo`, can you give me module foo? &gt; Buildsystem: Either a) Here is the path to module foo &gt; Or b) I don't have module foo yet, I will spawn another compiler instance on this machine to compile something else and go back to the original compilation once foo.bmi is available This I think completely removes all the double parsing / preprocessing problems. Nor does it require ahead of time DAG construction. It does however require the build system and the compiler be able to talk to each other. I do recall seeing a paper for a protocol for similar communication implemented by GCC.
I understand your point and thanks for commenting! I just read on a book that if you declare an object inside the class in which belongs, you can directly access its private and public members. Maybe I misinterpreted ?
Yup. This is exactly why compilers will be able to cache template instanciation: each compiler has an internal representation of an instantiated template. A common format would require parsing and transformation to the internal representative, making it slow.
Worse in that modules cannot have circular dependencies. Headers/sources can on the implementation side.
You are correct but it should not be itself. 
&gt; or even Python ~ couple module name to a file path Importlib is quite a bit more complex than that. The path to module mapping is just a default behavior that can be extended with custom finders/loaders. 
Clear as crystal! Thanks a lot both!
It is not related to your problem but yes, you can access members of another instance in a method of the same class. int Foo::bar(Foo a) { a.privatemember = 0; }
It makes me lose a *lot* of faith in the committee when I see a member (two, actually) take a hard line stance of requiring people to prove negatives.
// Yes you can, here an example: class foo { int x = 23; public: void func() { foo bar; bar.x; } }; &amp;#x200B; int main() { foo bar; bar.func(); } // As you can see, in foo::func() we are \*declaring an object inside the class in which belongs\* and we can directly access the private member "x".
No, because the object would be of infinite size. Each Book object would contain a Book, which would contain another Book, and so on. You could make it a pointer: Book *book1 = new Book("Animal Farm", "George Orwell", 1945); This compiles, but it will cause infinite recursion when it runs. When you create book2, it would allocate a new object to assign to book2.book1, but this object would also allocate new object to assign to book2.book1-&gt;book1, and so on. 
Interesting. :)
Usually the compiler generates the dependency information for a TU as a side effect of compilation. That can't work for modules because it's too late if the DAG has changed. Before compilers were generating this info, there was usually a pre-scan step, and everyone hated it because it stalls the build. And if it wasn't done by the compiler, it was often wrong, leading to subtle or not so subtle build problems. Or it was moved to be on demand, so people waited until they noticed the build was broken to rebuild dependencies. Which was long after the build was badly broken. 
I've been bringing that up for a while now. I think the only solution is to have a traditional source file with private implementations requiring circular dependencies. It's dumb. We should be able to preprocess modules to make thin interfaces, eliminating that problem. But... nope.
Most of the work CL does is sending the code to /u/BillyONeal and /u/STL, and waiting for them to compile it manually. Not sure how Clang emulates this. Citation: me. :)
So the act of importing it may modify it?
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/akor0o/unix_help/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; which means most of the things that are done are what you would call ‘undefined’ in a regular programming environment Can you please show some examples of that?
Is it even trivial to force GCC/Clang/CL to emit IL that is not yet optimized? They usually emit object files.
No, it could cache template instantiation needed for the entities defined in the module interface.
-O0, -fsyntax-only
I ran into a fun case in Java where a seperate class loader got started, and I had two instances of a class. Had to debug why updating a static variable wasn't working.
Sounds like one part of a proper solution is requiring partial compilation, generating thin, partially compiled module interfaces first.
Ever since C++17 we’ve had wording in the standard for what a file and directory is thanks to the addition of the filesystem API :v
I suspect it will end up standardized like `#include` - implementations will look for modules by name/filename.
Does that work in Clang and MSVC/CL? 
Two big things: 1. If you are going to make a project that deals with images, you need to put some images on the project page. 2. How is this different from using a short script and something like image magik (or some compositing program) ? 
1. Class files are effectively self-contained in Java. Bringing in a class file doesn't necessitate loading and parsing other class files at compile-time, as they only have thin references to other classes. 2. Class files are already parsed and compiled. Bringing in class files in Java is effectively similar to linking in C++ (at runtime), and at compile-time is similar to bringing in a precompiled header. 3. Java mandates type-elision. You cannot have deep, nested templates in Java that eat up compilation time, as generics don't work that way in Java. Java generics are *way* weaker, but way faster to compile. When you `#include` a header file, you are literally copy-pasting that header file's code into your code. Any `#include`s in that header file are also handled that way, so every translation unit ends up massive. This is all raw source code that must be parsed and compiled. For compilation, in Java, another class is looking in a precompiled (though not optimized) class file looking for symbols. That's it. The symbols are already there, in a nice, easy-to-read format. When you are loading them, it's similar (the JIT handles the rest underneath). The equivalent, as said, would be precompiled headers in C++, but even then it doesn't eliminate the template issue.
You deleted the comment, but the way to do this is to declare a pointer/reference to the type of the object being declared. Because a pointer/reference has a known size. This is common in a naive LinkedList class where you have a `Node` that has a `next` member which is also a `Node` Something like: struct Node { Node* next; };
I am spreading the event through many channels and universities are definitely among them! Thanks for your suggestion and hope to see you at the conference.
That's not strictly true: [http://eel.is/c++draft/cpp.include#3](http://eel.is/c++draft/cpp.include#3)We know about source files, and we know about searching for them. In implementation defined ways. # include " q-char-sequence " new-line causes the replacement of that directive by the entire contents of the source file identified by the specified sequence between the " delimiters. The named source file is searched for in an implementation-defined manner. If this search is not supported, or if the search fails, the directive is reprocessed as if it read # include &lt; h-char-sequence &gt; new-line with the identical contained sequence (including &gt; characters, if any) from the original directive.
Check out Michael Park's version of this: https://mpark.github.io/programming/2019/01/22/variant-visitation-v2/
The difference I see, at the moment at least, is that presently the two compilers supporting modules require them to be precompiled, which adds another layer. %MODULE_FILE% -&gt; %MODULE_INTERFACE%, %SOURCE_FILE% &lt;- %MODULE_INTERFACE%, whereas headers are just %SOURCE_FILE% &lt;- %MODULE_INTERFACE%. The standard doesn't mandate this, of course. It does, however, complicate things a bit. Right now, the compiler itself handles the includes. Either the compiler needs to handle the lookup, or there has to be some deep communication between the compiler and the build system... and I don't see either solution being adopted universally between GCC, ICC, Clang, and MSVC/CL.
I've been tinkering with it in two approaches. Approach #1 is starting with Clang and just reworking C++ from there as a separate, cleaner dialect. I don't have a name for it. `(++C)++`? My separate approach is a separate language altogether. Implementing the parser and the basic compiler elements, and then jamming the AST into LLVM. That's the 'clean' approach, but obviously takes more time and is more difficult. Interestingly, I *also* haven't fully solved the module problem in *either* approach yet. Unless you go full-on Java or C# (which isn't wise, here) you can end up with module pre-dependencies, and I *do* want to support circular-dependencies in modules which makes it harder (trivial for Java or C# given the nature of objects there). A big question also becomes - should the compiler be handling significant parts of the build? My personal opinion is that building should be standardized, so 'yes'. That simplifies things (sorta). If you instead rely on separate build tools, it makes it more difficult to handle such things. The build tool lacks information the compiler has, and vice-versa.
Twitter is terrible at subtlety. I understand where JF is coming from. It seems wrong that a good scanner can't be written. And I think he's right, scanning for \`import X\` is not hard, nor is \`module X\`. There's some issues with the pre-processor, but it's probably manageable. The snag, and this is the non-obvious part, is that isn't quite enough. Depending on the module foo doesn't say what specifically you depend on. Existing build technology really likes physical files and timestamps to track things. You have to put together that quux.cpp produces the bmi for module foo that bar.cpp is depending on. And it's right now open research on how to do it without causing other problems, like compiling quux.cpp multiple times. Or finding quux in the first place. 
One example is memory allocation. If we consider a display, an area of the CPU has its own partition of the RAM dedicated to the display (VRAM). A pointer to the VRAM start address is used to locate the area, but the memory isn't ever allocated (using malloc/calloc) as expected by convention, but the addresses for the whole VRAM partition are manually assigned to a colour value. There are several areas of the RAM allocated to different hardware parts depending on the architecture of the MCU. Here is a snippet of a GBA test program, which works on a similar process: /* Screen Dimensions of GBA are 240 x 160 */ #define WIDTH 240 #define HEIGHT 160 #define NUM_PXLS 38400 /******************************************/ /* Assign the Memory Address Locations */ #define MEM_IO 0x04000000 #define MEM_PAL 0x05000000 #define MEM_VRAM 0x06000000 #define MEM_OAM 0x07000000 /***************************************/ /* To Distinguish RGB15 codes from conventional unsigned short. */ typedef unsigned short rgb15; /****************************************************************/ /* Any global variables go here: */ volatile unsigned char *ioram = (unsigned char *) MEM_IO; /* Put ioram at address MEM_IO. */ volatile rgb15 *vram = (rgb15 *) MEM_VRAM; /* Write pixel colours into VRAM. */ /*********************************/ /* Main Function */ int main (void) { /* Local Variable declarations */ register int i, j; /*******************************/ /* Write into the I/O registers and set the video display parameters. */ ioram[0] = (unsigned char) 0x03; /* use video mode 3 for a 16bpp bitmap in VRAM. */ ioram[1] = (unsigned char) 0x04; /* use BG2 for the 16bpp bitmap. */ /**********************************************************************/ j = 0; while (true) { for (i = 0; i &lt; HEIGHT; ++i) { vram[i + WIDTH*j] = 0x1F; /* Red */ if ((i + 1) % 8 == 0) ++j; } } return 0; } /*****************/ Notice that neither `ioram` nor `vram` have any memory actually allocated, and don't need to because this 'undefined' behaviour is defined on this platform by the hardware.
Thanks for your feedback. To 1: That's true, There's already a sample project and I'll add some images to the project page. Maybe even a demo geo-server where one can see the result. To 2: I did this and it took two hours to convert a big scan I have (over 12000x8000 pixel) - and that on a relatively low detail zoom level (level 13). It would take nearly a day to convert it into tiles where I can zoom further in (e.g. zoom level 15). The problem was that image magick had to load the large scan image (over 100MB) over and over again (I assume other applications/scripts would do that as well). Now it takes like half a minute to convert it into several hundreds of thousands of tiles with high zoom levels. There's probably a way to do this with scripting but all the calculation logic was not that fancy and I decided to do it in C++ using OpenCV.
I know they think of it as the better C / better C++... but it's not really a better version of those. It's a pretty distinct language, and hasn't yet found itself to be fully appealing to C or C++ programmers.
I don't see why this example code wouldn't work if compiled with a c++ compiler as-is. Could you elaborate, or provide a different example?
It's required to include a source file, how that is found is implementation defined. It definitely isn't going to include a file named 'xyzzy.h'. 
The version I was using - most of the supposed benefits are checks to happen during the build &amp; testing (compile-time, not all operations supported, runtime asserts): #ifdef NDEBUG template&lt;class value_t&gt; using observer_ptr = T*; #else 
It's full day on June 15.
For the readers who missed: [P1108 web_view](https://wg21.link/p1108)
I never said that it wouldn't work in C++, just that this is undefined for a regular project, but well defined in an embedded project. Sorry if there was any confusion.
What undefined behavior this code can lead to in your opinion? If you think that writing through volatile pointers without allocating memory is undefined then [you are wrong](http://eel.is/c++draft/dcl.type.cv#5). Or can you present another example please? 
If you're violating the contract of the type, and the very purpose of the type, why would you expect everything to work out OK? If I reinterpret_cast it to double I'm also going to have a bad time.
People shouldn't be learning about languages ~ but learning about the core concepts that C / C++ / etc, share. Start simple, and work your way up. I started with Python, because it keeps things simple. When I wanted more speed and power, I started researching C and C++. Right now, I'm using D, because of its bounds-checking features, which was the biggest lure for me.
Unless you can prove that there is no teapot orbiting the sun, you‘re argument is invalid.
&gt;I will definitely try to work with text macros. This looks promising to me. I think I need such a supplement. There were situations when it was necessary to reproduce the actions accurately, but I could not do it for some reason.
Awesome. I’ll take a look. Thanks. 
&gt; observer_ptr doesn't add any functionality Indeed, it takes functionality away, which is the main point. It does add some initialization semantics.
The reason this is not possible is that the compiler command line flags used to compile foo.cpp into quix.bmi can (and will!!!!) Be different than the command line flags used to compile the CPP file that wants to import quiz.bmi. So no, its not possible to write a simple parser to find the module name.
The TS is significantly different than the Merged Modules proposal that is advancing. Although, not in places that affect what we're discussing here. And also, no one quite got around to implementing that, either. 
Grab the cmake module cotire. include it and you're done.
Modules don't leak things out. You (can) get exact control of what names are exported from the module. You don't with headers or PCH. 
Never heard of it. I'll take a look.
The classloaders, though, are runtime things. The compiler will still check using the default classloader? 
I really hate to be that guy, but are the bugs caught by this tool common in "modern c++" code bases? I can't remember the last time, when I had to manually lock and unlock a mutex as opposed to use a lock guard/ unique_lock. And the "simple" sync requirements (you always need to hold mutex X when accessing variable Y) are rarely what leads to bugs and or are well encapsulated anyway. Most of the bugs I've seen revolved around "clever" synchronization strategies using atomics, lockfree datatructures or situations where you sometimes needed to lock the mutex and sometimes not depending on some precondition. One warning, that could become verz important is imho https://docs.microsoft.com/en-us/visualstudio/code-quality/c26138?view=vs-2017 A couple of years ago, the much bigger problem, was substandard performance of c++11 synchronization datastructures in msvc, which then drove people to the more low-level and easier to missuse win system calls.
\&gt; yes of course sometimes cleanup is necessary (not always!). doing it well by hand means I will goto cleanup instead of having many exit points. which makes the function smaller You can always goto the end of your function in C++. C++ makes your program more likely to be correct. You can put in manual effort to make it short too. In C you always have to put in that effort and it still doesn't help with correctness. \&gt; yeah but sometimes it's silly. like templating on a lambda or on an integral type What's silly about that? Linear algebra libraries like Eigen do that, and it means that things like dot products are fast since there's no loop, just a list of multiply-accumulates. And it checks for correctness at compile time, disallowing invalid multiplications etc. \&gt; it's a trade-off... the language shouldn't choose for you It doesn't. You can choose how you implemet your templates.
I don't have an installation to test against, but something like this? You might need to write something for reading the file though, as I don't think ifstream can be read with begin and end and it would need to be for this. ``` int main(int argc, char** argv) { const int cols = 14; // Split file into 14-byte chunks per row for_each(ifstream{argv[0], ios_base::binary} | chunk(cols) | take(20), [](auto&amp;&amp; chunk) { for_each(chunk, [](byte v){ printf("%02X ", v); }); printf("%*s %s", 3 * (cols - chunk.length), "", // Padding string{ transform(chunk, [](char c) // Replace non-printable { c &lt; 0x20 || c &gt; 0x7E ? '.' : c }) }.c_str() ); }); }```
Honestly, a lot of the module discussion and problems I've seen seem to be a problem of "trying to be everything to everyone". It seems like we could have modules today if the C++ committee didn't try to solve every single problem with modules and instead made them opt-in with progressive features being added. For example. A first and easy restriction of modules would be "templates and macros aren't allowed in modules" Do that, and you have nearly trivial module implementations that could be done today. Then work from there. Figure out something that would work for templates first, then macros later. Swallow the elephant a bite at a time rather than all at once.
&gt; dtors... yes but if you didn't need dtors that's another c++ feature you're not using. I think it's good c++ style to use dtors, but often bad for embedded systems &gt; what library gmp is a nice example. there are many. even stdlib.h &gt; don't be dumb I was providing an example where you might want to use an exception. if you can't, then yes, you'll have to rethink your design.
&gt; use go-to in c++ you can, but it's bad style and devs often prefer not to (for good reason) &gt; some c++ temares are good no disagreement here. I am saying that these templates increase binary size which might not be your design goal &gt; c++ can be as binary small as c yes. that's like the first thing I said! the point is that there are subtle ways to introduce a lot of bloat. ways that are considered good style in c++ docs/community etc. and that's not always bad... it just may be a bad path to take to what you want.
If I can prove that there's a whole planet that has plenty of teapots, and that the planet and all its teapots are orbiting the Sun, does that prove the argument invalid, too?
&gt; You might need to write something for reading the file though, as I don't think ifstream can be read with begin and end and it would need to be for this. Range-v3 comes with `istream_range` for this purpose.
I've read (or skimmed?) the paper and most comments so far. It's very, very complicated - and any implementation is going to be very, very complicated to use. Not a great thing. One thing I'm missing from all this is how DLLS/Shared libraries might fit into all this. These "modules" currently exist, are "supported" by all (or maybe most) C++ implementations but are not recognized by the C++ standard as far as I know. They address a number of the issues that modules are meant to address - separate/decoupled compilation, hidden local symbols, etc. They do bring a bunch of problems - which seem similar to the ones that modules are expected to bring. Compile time switches (C++ has a lot of them) have to be in sync and a system needs to be built to ensure that they are, ABIs have to be maintained in sync, circular dependencies aren't really supported (probably a good thing), extra efforts are needed to share header only code, symbol visibility needs to be addressed (currently it's not in the standard and different compilers do it differently). The bring some issues that modules don't - e.g. multiple address spaces. But aren't all that complex compared what modules is starting to look like. It's been mentioned that this approach has worked with the C# family of languages. To summarize it seems to me that many of the advantages and problems regarding modules have already presented themselves with these libraries - and they've never been addressed in a standard way. Perhaps it might be more productive to spend efforts to resolving current issues with DLLS/shared libraries before embarking on a much, much, more ambitious effort. Perhaps usage of DLLS/Shared libraries in combination with PCH can solve most practical problems in a way that most people can understand. If the goal is to be able to compile a 6MLOC program consisting of header only templated code with out having to manually (re)factor it to permit this, I'm afraid we're going to be disappointed. &amp;#x200B;
Well I'm still not really following your reasoning here. Are you saying that in C++, using memory that's not be acquired with the new keyword is undefined behavior? If so, I suppose your right in the sense that the standard itself doesn't specify what the behavior needs to be (aka, undefined behavior), but everyone who knows that also knows that they need to confirm what behavior their compiler of choice is going to implement. So it's undefined as far as the C++ language standard, but it's *not* undefined as far as the platform being compiled for and the compiler being used. Therefore, I don't really see this as a good example of &gt; which means most of the things that are done are what you would call 'undefined’ in a regular programming environment As this is, for all intents and purposes, a regular programming environment unless the person making the judgement call doesn't realize they live in an ivory tower of language purism. Hell, you're not even using inline assembly here, so it's not *that* weird.
&gt;Those using antiquated OS without file extensions can keep using &lt;= C++ 17 For that matter, they could just keep using headers.
&gt; (I assume other applications/scripts would do that as well) I wouldn't assume this and the times seem strangely long to me, but it is still understandable to do. You can always use smaller image loaders like stb_image if you ever want to make a stand alone program that doesn't depend on openCV.
&gt;&gt;they'll forget to cleanup... &gt;would you rather they make the code too big due to dtor inlining or forget to clean up? which would they probably catch without asking for help? They are more likely to notice the resulting binary was too big. It's unlikely in the extreme that a newbie is going to catch a single place out of many thousands of places where the cleanup logic was left out. Even professionals with decades of experience still occasionally program themselves memory leaks to debug a month or a year later. When in doubt, prefer correctness over tinyness. If you claim that the platform you're targeting can't support the cost of doing things correctly, then get a new platform, or make your program do less things. 
https://en.wikipedia.org/wiki/Russell%27s_teapot
There are a large number of “legacy” C++ code bases that are slowly moving to more modern C++. This tool could still be useful for such hybrid systems.
I’m struggling the think of a use case where this would be the best solution. How does this handle code that is highly dependent on garbage collection?
I completely understand everything the class file brings and how the preprocessor works I was trying to understand why it is a problem, but I think you answer it in a sidewise fashion &gt;but even then it doesn't eliminate the template issue. I don't generally write templates. I'll use them from things like the STL but for myself I don't do any template programming. For me the header is generally nothing more than a typical interface definition and maybe some constants so including that in the compilation unit of a source file isn't a big deal. So modules are a solution to over complicated template programming?
In short, yes, we can, but that will require a new compilation model (and which is what CMake/Meson/etc have an issue with). For details see [this part](https://youtu.be/cJP7SSLjvSI) of my CppCon 2018 talk.
While this is true that's a pretty weak outcome after all those years.
&gt; For example. A first and easy restriction of modules would be "templates and macros aren't allowed in modules" Do that, and you have nearly trivial module implementations that could be done today. You know that even some "primitive" types are sometimes defined as macros, right? Or if not defined as macros directly, are typedefs that are conditionally defined based on macros. I think you'll find this is not an easy restriction to enforce.
The ambiguity isn't between the compiler and the code, and it's between the compiler and your intentions. It cannot know your explicit intentions unless you express them. And the point isn't what you did the first time. The point is, when someone reads this two years later, how can THEY know what you intended? They can't fully, because you didn't make it as explicit as you could have. I mean, I don't even depend on precedence of mathematical operations, because I don't want to (or want someone else years later to have to) know for sure if I understood those precedence rules; or, even if I did, that I got them right that particular time. Explicit parenthesis means that that is absolutely what I wanted to happen. No one has to guess. When you are building significantly sized software, you have to both grow it substantially over time and maintain it over all that time. The odds get more and more against you as you move forward, so every single little thing you can do to make sure that intentions are understood is a potentially big win. &amp;#x200B;
how do you know when your binary is too big?? that's pretty hard to measure since you don't know the optimal size
Modules are a 'solution' for the fact that `#include` is really very brute force, and tends to bring in a lot of implementation details (including includes and such) - think when you `#include &lt;Windows.h&gt;` - it brings with it all those macros, etc. Modules only expose what you export.
It isn't clear that modules and file names need to match. It's less clear that they should have to match. Why should I have to pull down a cached bmi and put it on the filesystem at all, for example. It would be easy enough to define a protocol between the compiler and a given build system that says that, for this limited circumstance because I have a relatively simple build system, they will match. I do not then have to have them match in my build system. So in this case my high horse and thinking that I know better than you (referencing your other posts) is only that I think I know what I want better than you know what I want. I don't particularly want a defined name-&gt;filename mapping. The simple obvious solution you propose is suboptimal for my use case. I'd rather allow it, but not require it.
I don't need to imagine. Eiffel, Ada, .NET, Delphi, D all support modules, have generics without type elision, do support primitive types and compile blazing fast compared with C++.
If it's hard to measure, then why does it matter? I get people at work complaining to me that the size of their binary increased by %1, because I fixed a bug in a class they used. It's not like the binary in question was going to be deployed to a resource constrained system. Just a bog standard x86_64 intel machine running Linux. Correctness is more important than the size of a binary, but it's much more likely (in my opinion and experience) for people to notice that the binary is big than for people to notice that their code has a (subtle or not) bug that'll eat their customers data for lunch.
Yeah, but they don't do textual import.
Well, then such a system doesn't fit your particular usecase. Fair enough. I dare say it would be a minority usecase, though. Modules matching a filename/path is just a very sane solution. Little to no mental load is required. They make it very simple for the compiler to figure out where your desired module is. Especially if you're adding a custom include path. 
[I think](https://stackoverflow.com/questions/945971/use-a-custom-classloader-at-compile-time) you can pass a custom classloader to the compiler using one of the java runtime flags. However even without classloader trickery we still have the issue that the class foo.Bar could be defined in bar/Bar.java. The compiler only seems to enforce that the class name (of a public class) matches, not the package path. Whatever the java build process is doing cannot rely on path/to/File.java -&gt; path.to.File .
I still wonder what was wrong with precompiled template headers.
1% is a lot. imagine increasing your energy bill by 1% or the cost of ram by 1%. is that worth the benefit of style? of course you can measure it! but that would require you to write both implementations. which sometimes is fine, but in a widely used struct may be difficult. also, small losses may seem ok, but ten years later is out of control. if it's not going to a resource constrained system, then this whole discussion is not applicable! that's the whole argument for c! see the top of this thread! &gt; correctness is more important than size.... in general I agree, but if it's too big to use, then it's a moot point. ... btw resource leak is not subtle in most cases. see valgrind/similar
Amazing stuff. You should share GitHub repo link so we can stalk it. Clean approach is probably more future-proof as long as we maintain abi compatibility and can link to other c++ libs. Nim does some things great btw. Macros working on ast, user-defined operators, great cpp compatibility (it compiles to cpp). I wonder if it would be possible to be able to use instantiations of c++ templates from another language. Almost perfect compatibility would be an important selling point.
This is a really broad question also you should go to [https://www.reddit.com/r/cpp\_questions](https://www.reddit.com/r/cpp_questions) instead
I don't see why this is a problem. If incompatible flags are used between many translation units then it doesn't work today and won't work with modules (GCC actually check for incompatible flags in the BMI). Compatible flags such as defines, include directories should not affect the BMI in the eyes of the importer.
That’s not how you format code on reddit
6 by my count: [`std::atoi`](https://en.cppreference.com/w/cpp/string/byte/atoi) [`std::stoi`](https://en.cppreference.com/w/cpp/string/basic_string/stol) [`std::stol`](https://en.cppreference.com/w/cpp/string/byte/strtol) [`std::from_chars`](https://en.cppreference.com/w/cpp/utility/from_chars) [`std::sscanf`](https://en.cppreference.com/w/cpp/io/c/fscanf) [`std::basic_istream::operator&gt;&gt;`](https://en.cppreference.com/w/cpp/io/basic_istream/operator_gtgt) (combined with [`std::istringstream`](https://en.cppreference.com/w/cpp/io/basic_istringstream)) And that doesn't count third-party options like [boost:lexical_cast](https://www.boost.org/doc/libs/1_69_0/doc/html/boost_lexical_cast.html) or a full parser library. And I still feel we're missing one: a type-safe `sscanf` replacement a-la fmtlib.
&gt; You know that even some "primitive" types are sometimes defined as macros, right? &gt; Or if not defined as macros directly, are typedefs that are conditionally defined based on macros. Primitive types don't have to be defined by macro or typedef. They are part of the spec. Doing that is an implementation detail that can (and probably should!) change. &gt; I think you'll find this is not an easy restriction to enforce. Maybe. It might require quite a bit of extra effort on compiler writers to enforce this sort of thing. However, it can't be harder to do than inventing a special intermediate language. Even if that is where things need to end up, supporting easy to static compile things would make it a lot easier to start work. 
So I was skimming this and wasn’t really interested until this caught my eye: HTTP Server for QML This sounds great. Would it require specific browser settings? How would it work?
I'm thinking `import&lt;c++&gt; "header.h"` or such for bringing in C++ code, which would present it as a special module.
So, I've been adding some move semantics stuff to my code base. Something that I ran into that sort of bugs me, but I may be missing something is the requirement to force a move for anything but temporaries. Take a fairly common scenario you might run into in a back end messaging middle-ware'ish thingie... 1. You have a pool of msg objects to avoid heap churn and overhead 2. You grab a msg from the pool, put it into a queue for processing by a worker thread. 3. The worker thread processes it and puts it back into the pool One really obviously convenient thing is to use a specialized pool oriented unique pointery type class. I created one of those easily enough. It will get an object from the pool, it uses move semantics to insure only one copy if it is ultimately kept, so you can use by value semantics on the queue and the copying isn't an issue. And it puts it back into the pool when it destructs. But, it's semi-useless because the queue doesn't know from move semantics, it just copies and assigns things. Collections are already crazy complex enough, so having to have it support either move or copy semantics by introducing some new intermediate node storage wrapper thing and whatnot, I just don't know if that's worth it, for either overhead or complexity reasons. I guess I could do it from the outside, on an ad hoc basis by wrap the unique'ish pointer itself in some secondary wrapper or something, and have the outer wrapper impose move semantics on the inner one. I guess you could do a template which generically provided that outer move forcing wrapper. But, by that time, it's be just as easy to use a counted pointer class that already exists. Since add/remove from the collection would already be protected in such cases a non-thread safe counted pointer could be used and maybe less overhead than a wrapper around a wrapper. But, still, it would be ever so nice if, in a class in which move is implemented and copy/assign is deleted, that copy/assignment automatically used the move versions. I'd be happy to accept that this means that no constant objects can be involved, i.e. they can all be modified as they are copied into the collection. Attempts to copy const objects into the collection would just fail at compile time. And of course access to the elements in-situ has to be via reference since copying one out would destroy the one in the collection, but most collection classes are going to do that anyway. Even if that required some explicit indication in the class declaration that this was to happen, just to be safe. &amp;#x200B;
Because they promised Modules will automagically know what files make up a module and somehow reduce compilation times despite the extra work involved. 
Sure, if "everything they do with header files" means things like "maximize build parallelism" and "build without processing each TU twice." That hardly seems like FUD.
How do you handle help? I have a function that just takes the option number as the help option, and when an argument matches that option's argument string, I then dump out the contents of the struct. It's fairly simple, but it does the job.
getopt is really basic and it's not even cross platform, you have to provide a copy of it yourself, so really the only decent option is to write your own.
why does that sound great?
Fact.
Hi, How does it compare to xframe? And a couple of short code snippets in the readme would really help!
This is exactly what's happening in my job - we're slowly replacing the use of an ageing critical section class with std::mutex.. sounds boring but its actually very satisfying
&gt;would you rather they make the code too big due to dtor inlining or forget to clean up? Hmm... would I rather customers get buggy code, and have their hardware fail at the worst time, or a few bytes here and there? What would my customers care about the most. &gt;generic code doesn't require templates or macros. No, but templates make them a lot safer, and easier to be safer. In C, all you have are macros. Or copy-and-paste. &gt;there are lots of libraries in c... Yeah, and lots of them use function pointers and void pointers to be "generic". Including handrolling vtable implementations that cannot be inlined by compilers. &gt;compile a medium/large c project. then rewrite a struct with a dtor and see what happens. If some structure of yours has a cleanup function, and writing a destructor increases size compared to manually calling that cleanup function, then you demonstrably have missed a cleanup function call somewhere. I'm afraid you've been shipping buggy code to your customers.
Which is the whole point of having modules in first place. 
Since your flair says Nvidia, why did they kill Cg?
Being able to serve QML instead of html/css is in of itself great. Also I find QML saner to work with than html/css. 
Languages born with modules don't process each module twice, and many e.g. .NET do actually compile their modules in parallel. So yes it is FUD, because while it might not be possible with existing build scripts, Unity builds and what have you, it doesn't mean that it wouldn't be possible to do the same with some C++ related tooling improvements.
That particular problem is easily achieved by a map of module name to file name - a trivial map in this case - that the compiler can implement or communicate with. It is a tooling problem, not a standardisation problem. Why do you believe that this rule needs to be encoded into the standard and therefore get in the way of my use case? Especially when files in general have largely been left out of the C++ standard up to now.
Can't those members be delayed at the airport?
File- name&gt;Module is how mu build scripts build C++. They mimic modules to with precompilation. Trying to add automatic PCH generation as well.
enums are another replacement for defines. It is not the only one. And the problem with enums is that they are implicitly convertable to ints. enum classes, on the other hand... Not all constants are simply some kind of flag. Some constants, as used by embedded programs, are memory addresses. You can't use an enum or enum classes for those. Or they're a constant quantity of something, not a flag. With constexpr, you can even have a constexpr of a wrapper type, such as a safe integer type. You can't do that with enums.
Everyone wants a new compilation model. Nobody wants to be responsible for it.
I was geniunely surprised at how bad that CImg header file is...
Languages born with modules also don't have the same parsing requirements as C++, where the full types of imported entities must be known to proceed. Features there do not automatically apply to C++ modules. Please do describe these "C++ related tooling improvements" that would fix the issue, because people here would be *extremely* interested to hear them.
Is there a good way to precompile template-heavy headers or modules?
But I said imagine a language that *also* did textual import.
re the first thing... I'm assuming "they" is a junior Dev and "you" are a reviewer. idk about you but I might not read the assembly of every line of junior Dev code. I damn sure will catch a missed cleanup in c code. and re the last thing... I that's not true. the lines of c++ code is not proportional to lines if binary. that's kind of the point of high level languages... they do more for you. which is good outside of severely constrained envs. an example of this that I already used is dtor inlining. a more indirect example that I also already used is common c++ style e.g. multiple exits from a function and let the dtor clean up vs go-to cleanup 
Shared libraries are great for many use cases (including speeding up incremental builds _a lot_). Sadly there are people on the committee who are actively hostile to shared libraries using reasoning that goes along the lines of "our corporation does not use shared libraries, therefore nobody in _the entire world_ should use them either". An actual point against is that a shared library (or a static one for that matter) can not represent TMP functionality (even std::vector). It needs to be compiled for each user of the code individually. Modules, like precompiled headers, can store that information.
One thing that should be kept in mind is that during the San Diego meeting it was briefly discussed during breaks that we will need a concept of a shared library at some point to satisfy some module deployment issues. We won't see them until at the very least C++23, but we can't avoid it anymore.
Another take on [http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0960r0.html](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0960r0.html) ?
Industry standards, who needs them, AMIRITE?
What an awful policy. Glad you might still be able to do something.
Those were my thoughts. I can't think of how object lifetimes would be "translated". Sounds like a terrible idea and I don't see any use cases either.
I've heard about this proposal, but as far as I can tell, it only solves the inability to use aggregate init or `std::initializer_list` initialization with helpers like `make_shared`. I don't think it addresses any of the other issues I mentioned. It doesn't even want to address designated initializers (`std::make_shared&lt;T&gt;(.a = 1, .b = 2)`) because "designated initializers are a C-compatibility feature, and that syntax is not a thing in C". When I mentioned that my proposal allows aggregate init, it also allows designated initializers.
Yeah. Like javascript! 
Even reviews can miss things, especially if under time crunch. Having the compiler review your cleanup is a much more sure way of reviewing. I'm not sure what your last point is. Of course lines of C++ code is not proportional to the library. One destructor can be called many times. But the point is so is your cleanup function. There must be objects in your code that requires some kind of cleanup, or a state reset. That's what a destructor is for, and cannot add more lines to the binary than if you called that cleanup/state reset function in their correct places. Seriously, if adding a destructor to replace that cleanup function adds more instructions than calling the cleanup, you should review your assembly to see where that destructor call adds more code and really make sure that destructor call isn't necessary. For objects that don't require cleanup/reset also doesn't require a destructor, so again, no cost there.
&gt; I manually review postings &lt;3
xframe is also a great implementation. I have a star there. xframe is based on two other libraries. What I like more about my implementation: \&gt; It is completely self contained. \&gt; It is much more flexible in terms of covering all built-in and user defined types \&gt; Also I like the interface better. It is more similar to Pandas.
Ada, Delphi and D also support textual import.
I agree a review is not watertight per the inlining point I made previously, a dtor can and often does bloat more than a function call. you can try to avoid inlining but the compiler may ignore you. happens especially if you are trying to use other optimizations. or if the compiler generated the dtor (or move ctor or assignment etc etc)
I'm not with nVidia, but I would think it's because continuing to try to maintain their own language in the face of a growing ecosystem of tools for compiling various other languages to and from SPIR-V makes is a losing proposition. 
You mean like generics in Ada packages, for example? My desired "C++ related tooling improvements", based on experience with other module native languages, are exactly want you don't want to have. So the bottom line to me feels like "modules yes, but please keep our compilers stupid".
&gt; 1% is a lot. imagine increasing your energy bill by 1% or the cost of ram by 1%. is that worth the benefit of style? Who's talking about style? I'm talking about correctness. In my example, the bug was something that destroyed a customer's session. I was assigned to fix it. The root case was a bug in a base class deep in the class hierarchy. Fixing it *required* adding more code that was reflected in the ultimate size of the binary. People complained that the binaries were bigger. They tried to convince me that the binary size was more important that not destroying a customers ability to do what they pay us for. They lost the discussion. &gt; of course you can measure it! but that would require you to write both implementations. which sometimes is fine, but in a widely used struct may be difficult. also, small losses may seem ok, but ten years later is out of control. I didn't say I couldn't measure it. You said it. See the following quote from you: &gt; how do you know when your binary is too big?? that's pretty hard to measure since you don't know the optimal size &gt; if it's not going to a resource constrained system, then this whole discussion is not applicable! that's the whole argument for c! see the top of this thread! "It's a resource constrained system" is not, by itself, a valid justification for using a poor language like C. That statement, taken in isolation, is equally valid as an argument that you should use C lang as it is an argument that you should buy yourself better hardware. &gt; in general I agree, but if it's too big to use, then it's a moot point. If the code is too big, it's not the fault of the language. It's the fault of trying to to make the hardware do more than it's capable of, either because you're abusing the language, or because the hardware was never sufficient to begin with. &gt; ... btw resource leak is not subtle in most cases. see valgrind/similar Riiiiight. Sometimes it's obvious, sometimes it's not. C makes it a lot easier to screw up than C++, for "idiomatic" code of the two languages that attempt to do the same thing in the same general way. Destructors are fool-proof (for certain levels of fool), "Oh, btw, remember to always put a jump-label at the end of your function, and make sure you always clean up each of the variables that you put on the stack, or allocated on the heap, before exiting." is not even expert-proof.
I feel like we're going in circles here but one repsonse.. sometimes buying better hardware costs a lot more than writing more lightweight code. and yeah those examples of cleanup would be caught by valgrind... wouldn't even make it to review
I think there actually are good uses for things like this. If you use shared_ptr in place of garbage collection you could get stuff working rapidly, even if it is sub-optimal. From there you could do refactoring with a solid amount of work done for you.
Bjarne Stroustrup (et al) in [P0939 - Direction for ISO C++ ](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0939r2.pdf): "we must aim for coherence: among standard-library features; between the standard-library features and the built-in language features" Bjarne Stroustrup in [P1428 - Subscripts and sizes should be signed](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1428r0.pdf): " std::span’s index and size types are signed" Contradiction spotted. 
Correct me if I'm wrong, but my understanding is that the way the BMI is generated is independent of the command line flags of the compiler invocation that imports the BMI. You said &gt; What I'm saying is the mapping can be done on the fly at a very low cost. It would be done after the preprocessing of all files, and just before compilation. When all files has been preprocessed, a very simple parser can be made to extract the module name. That parser can be run in parallel. But I don't think that you can actually do that because the preprocessed output of the module may be different for the compiler invocation that generates the module than the invocation that wants to consume it. In fact, the article that the OP points to explicitly points this out as a problem. For you to have a build system that works in the way you describe requires either: * that your build system parses and pre-processes *every single file* that you might want to compile every time you invoke your build system, with complete knowledge of all command line arguments for the compile. * Or that your build system cache this information to disk somewhere, and that's *not* attractive, since it also means you have to distribute this cached information along with your build artifacts with anyone you're collaborating with, or they have to be able to regenerate it all themselves when one of the files gets edited. For case #1) For some build systems this is possible, but this is *not* possible for other build systems. I am the maintainer of one such build system at my day job. It's *not* possible for the build system to know the command line arguments that were used for every possible c++ file on the system. In fact, we have things pretty segregated so that the cpp files of one dll/.so are incapable of seeing the cpp files of another. Only explicitly listed header files are visible. For case #2) I don't want to cache this information. That sounds like a pain in the ass. Not happening.
We can have those abilities with modules in C++, they just require changes to the module design rather than (just) compilers. That's what people are trying to get at here.
shared_ptr doesn’t work as a drop in replacement for garbage collection. Garbage collection can handle reference cycles.
&gt; I feel like we're going in circles here I agree &gt; sometimes buying better hardware costs a lot more than writing more lightweight code. "writing more lightweight code" doesn't imply that using C lang is the answer. &gt; and yeah those examples of cleanup would be caught by valgrind... wouldn't even make it to review Have you met many C programmers? Ohhh the things I've seen. Ohhh the arguments that I've had. C programmers with 20 years of experience make worse mistakes than C++ programmers with 2. Not all of them, I hope. But all of them that I've ever worked with.
&gt; Primitive types don't have to be defined by macro or typedef. They are part of the spec. Doing that is an implementation detail that can (and probably should!) change. I agree with you. Compiler vendors may not. &gt; Maybe. It might require quite a bit of extra effort on compiler writers to enforce this sort of thing. However, it can't be harder to do than inventing a special intermediate language. Even if that is where things need to end up, supporting easy to static compile things would make it a lot easier to start work. *shrug*. Couldn't say. Was only trying to point out that it might not be that simple. That's all.
Ok that's a thing!
Yeah HTML/CSS is too low-level for developing GUIs in my experience. One could do that, but hey, it's just not designed for that purpose. Having to write JavaScript to complement CSS for proper layouting is defeating the sole purpose of splitting the two languages.
But, the question is, does it come with a kitchen sink?
Beats me, before my time.
In my opinion, `variant` has the significant usability downside that for simple, single-dispatch operations, you need a visit. Suppose all animals have an `eat()` function. Then compare: // OOP dispatch for (auto&amp; a : animals) { a-&gt;eat(); } to // variant visit for (auto&amp; a : animals) { std::visit([](auto&amp; a) { a.eat(); }, a); } I often find it inconvenient to use `variant` for this reason if my types have a lot of common members.
&gt;Pattern Matching is such a well written and easy to read paper. Thank you so much for your kind words! &amp;#x200B;
My AVR C++-written binaries are smaller than the C variants.
Nailed it.
Haha, thanks! This is encouraging to hear. The current target is for C++23, but the committee is very busy with the influx of papers. We'll be doing our best to make that timeline though!
Memory barriers exist for a reason. Stop writing buggy code and relying on optimizations being disabled for it to run.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
There is no undefined behavior here, as it is using `volatile` correctly, as volatile effectively marks the address as outside of the purview of the abstract machine.
Very cool, though I'm not quite sure how I would implement this today and I do think that this should have been around a long time ago. It might be good for setting up a REST API for a service already written in Qt, but there's already so many tools to do that if you just need a simple API interface. Being able to build and send a JSON response is about all the example code can do so far. At this point, I would need much more than that so hopefully this keeps moving forward.
I often write AVR in C++. It's *not* hard for heavily templated code to compile to *smaller* than equivalent C.
Perhaps you should stop defining objects before initializing them so you don't have redundant destructor calls.
It works on new Reddit, actually
I've done my share of audio development for games and at least this feature set doesn't even come close to the actual requirements of an audio api (at least for that purpose). Why not add std::3dengine that is really great for drawing cubes or std::webserver that makes it really easy to serve static html pages? Because that's the scope of the examples at least. 
Give an example of where a destructor bloats code where manually calling cleanup functions does not.
If it's a resource-constrained system, you know what your limits are.
&gt; It is much more flexible in terms of covering all built-in and user defined types Could you perhaps give an example of this? 
&gt; mobile, etc.. Now normal reddit falls into "etc"?
Not true. GC is a deal breaker for a ton of real time, low latency stuff. Pretty much all actually.
This isn't apples to apples, because in OOP you would have to have a base class where you defined the virtual function. So here's the real comparison: // In Animal interface virtual void eat() = 0; // Anywhere, Animal is an alias to variant&lt;Dog, Cat, ...&gt; void eat(Animal&amp; a) { std::visit([] (auto&amp; x) x.eat(), a); }; And then you can just call `eat` as many times as you want, equally easily with either approach. Clearly the variant form is still a bit more repetitive but it's not a big difference and it only shows up once per common member (rather than once per common-member usage point). And there are advantages in the fact that it's not centralized and such.
&gt; D does not have to target some operating systems lacking more than one level of directory access, or that do not have a real concept of file extensions (such as zOS). Erh... Ok. But maybe we should not as well in C++ ? I mean, anybody using a "modern" version of C++ already uses a recent compiler with a decent OS, or has a way to cross compile. Also p1302 says the build-system needs to see the `export import`, but then it still needs to parse it. And do apply preprocessing. Back to the issues mentioned in the article.
Could use Boehm.
You can implement a GC using something similar to shared_ptr.
Actually, it's a good policy -- it allows us to stabilize the compiler so that we can release something that won't blow up the world (so to speak). With a constant stream of new features we wouldn't be able to achieve this. That said, we don't tend to be overly strict when enforcing the rules; for instance, if the problem above can be fixed with a non-risky patch, we can still go through with it. But I won't know until I have the fix.
I realize that, but the point I'm making is that even if something translates to C++ that isn't final, there is still a lot of work that can be saved. It isn't as if it is translating to a binary form that can't be changed.
Hey, that's not bad! It also requires free function calls instead of member functions calls (worse for auto completion), but that seems worth it to make multiple dispatch way cleaner.
go back to python mr soyboi webshit.
I think you're overestimating the cost of optimization and underestimating the cost of preprocessing/parsing in the general case. Like, if what you're claiming is true, why would anyone bother with precompiled headers? The reality is they are often used, and often result in much faster compile times, and all they're eliminating is time spent preprocessing and parsing.
Nice try
&gt;or if the compiler generated the dtor (or move ctor or assignment etc etc) If the compiler generates a dtor, then that means you needed one and are otherwise using the object incorrectly. A compiler will generate a dtor, for example, when your object contains an object with a non-trivial dtor. That means your code will have leaked something if you didn't manually call the cleanup code for that containing object. It really sounds to me the code bloat you've discovered is actually highlighting where you should have called a cleanup function (or otherwise properly specified non-ownership semantics).
You have a very good point here. It seems more like a design deficiency of the STL shared pointers and containers. You can achieve all your goals with the use of callback based constructors. See: https://godbolt.org/z/TtA2rM * constructor only has to be available to the functor. * you may initialize however you like. * factories are no worries. The best part is that we don't have to change the language. So no possible parsing problems and ambiguities. A special syntax might save us some characters of boiler plate. Not worth the effort if you ask me.
A very raw idea (so please feel free to point out it's many likely holes!) for the `new` problem might be to allow `operator new` overloads to return values besides `void*`, which would then also suppress implicit construction. You could then use a smart pointer tag for controlled construction: constexpr struct {} unique; using unique_t = decltype(unique); template &lt;typename T, typename... Args&gt; unique_ptr&lt;T&gt; operator new(unique_t)(Args&amp;&amp;... args) { return make_unique&lt;T&gt;(std::forward&lt;Args&gt;(args)...); } // usage example auto ptr = new (unique) foo(arg1, arg2, ..., argN); //decltype(ptr) == unique_ptr&lt;foo&gt; This still combines I think with other `new` overloads if the constructor versions forward to other operators, e.g. // gross generic version you'd find in the standard header template &lt;typename T, typename... NewArgs, typename... Args&gt; unique_ptr&lt;T&gt; operator new(unique_t, NewArgs&amp;&amp;... new_args)(Args&amp;&amp;... args) { return unique_ptr&lt;T&gt;(new (std::forward&lt;NewArgs&gt;(new_args)...) T(std::forward&lt;Args&gt;(args)...)); } // usage example auto ptr = new (unique, align_val_t(16)) foo(arg1, arg2, ..., argN); The non-`void*` versions should be assumed to work with RAII and not require a corresponding `delete` I think, too, which simplifies the design space a bit.
It's probable that this means you can make a server in QML, rather than use QHttpServer to host QML web pages. QML isn't strictly for UI stuff
&gt;I find QML saner to work with than html/css. I've never heard of QML before, but after quickly looking it up, stuff like this seems awesome. Item { Rectangle { id: myRect width: 120 height: 100 } Rectangle { width: myRect.width height: 200 } }
Why would you do that though? Modules are supposed to be independent blocks that don't need other modules to be imported separately to work.
I agree, have your build system make the modules, the compiler should just error if the module isn't there.
&gt; use go-to in c++ &gt; &gt; &gt; &gt; you can, but it's bad style and devs often prefer not to (for good reason) It's bad style for both C and C++ though.
At first glance, the side-by-side "Class Properties" example makes C++ look bad, but there's a comment on the C++ side that's not on the C# side and they are using 'System::String' on the C++ side and just 'string' on the C# side. You might wonder if C# has namespaces from looking at that example. In my opinion, the side-by-side example needs work. For those interested in source to source applications, I'm working on a [code generator](https://github.com/Ebenezer-group/onwards) that [outputs low-level C++](https://github.com/Ebenezer-group/onwards/blob/master/example/zz.exampleMessages.hh) based on [high-level input](https://github.com/Ebenezer-group/onwards/blob/master/example/example.mdl). 
Are you implying that anyone with a C# codebase doesn't know that `string` is shorthand for `System.String`? The examples aren't to show what C# looks like... ;-]
Because that's how all other languages work. It's pretty rare to not need anything from another file.
I seriously don’t understand why Qt feels that they need to reimplement everything, from the C++ standard library to a web server. Everybody knows they won’t be able to maintain it and keep up with security bugs. They should focus their efforts somewhere more useful. 
 class ClassProperties : public System::Object { std::string mPublicPropertyField; public: std::string get_PublicProperty(); void set_PublicProperty(std::string value); ClassProperties(); }; There's probably more that could be done, but just doing this much makes the C++ side look better.
Alternative implementations are useful. For instance, check out how many components of boost become part of STL. If every one thought glibc was good enough, we wouldn't have musl or alpine. I agree with your point. There should be regular auditing on the implementations. With a big name with full time employees like Qt, I expect it to be present.
But modules don't have to be a single file. If you start having modules than depend on each other in a weird way, you're just asking for trouble.
Interesting! Does it also work with placement new? I guess due to forced rvo, it just might...
Sometimes Billy is playing poker while I'm playing Magic. That's where fatal error C1001s come from :-)
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/akoetu/can_you_declare_an_object_inside_the_same_class/ef89crf/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; Politics, and egocentrically thinking that they know better than everyone else? &gt; It's what happens with committees controlled by the arrogant. &gt; At least, this is what seems to be happening from my point-of-view. You've got the point. Trying to satisfy everyone's need can only lead to nothing.
We already do that with headers and source files, though.
Original rules or with the stack?
True, but that doesn't mean it's a good idea. Trying to force sane behaviours is a good thing.
You can try [extern template](https://isocpp.org/wiki/faq/cpp11-language-templates) by manually specifying commonly-used instantiations.
Why? Just why?
Sounds like adding a new switch to the compiler which basically tells it : Exit with some documented/standard error code and produce a listing enumerating the unresolved module dependencies when I try to compile you is a great way to transform the compiler into the tool you can launch in parallel for any translation unit in order to build the dependency graph in a perfectly accurate manner.
What! Have they done anything to harm the stack?!
Magic Arena, I live at HEAD.
My point is that a totally unenforced contract may as well be called a convention. Ideally we'd have tooling to enforce the contract. That would add a higher amount of value potentially worth the very measurable cost. As it is, we contract for modern use of raw pointers - it's effectively the same as observer. I get that large code bases with legacy code are stuck in the middle and that embedding more info in code helps disambiguate it. I've got a few million lines of code that model the problem. That's a temporal state though. Code is either left behind or modernizes over time. Companies could trivially put their own solutions in place during migration. My issue is the long term cost - particularly compile times - for something we're talking about standardizing and pushing as a best practice. 
Exactly. We all know our code gets better with newer compilers, but I'm curious about exactly how much better. re we talking about a 25% improvement over, say, 10 years. or a 2000% improvement over the same time frame? The former is basically irrelevant, the latter is very significant.
Well, I think I was sleepy. They split HTML and CSS but Javascript is a glue between them to allow dynamism. So touching Javascript for layouting is not defeating purposes. Yet in Qt you can do more layouting in QML without Javascript, so I still believe Qt is more elegant.
I'm concerned about security. When is it a good idea and when not?
It was required before the STL becomes really useful, but nowadays it is far less relevant
The "lock must/must not held while calling X()" warnings still seem useful, particularly with locks that are held higher up because it would be too expensive to automatically do so around every individual operation in a sequence. 
They created it. The original rules did not have a stack, thus why interrupts were a thing.
Hahaha, you had me scared they got rid of it! :P
Which would be useful to resolve in any case.
Doesn’t this only work if the type has a single argument constructor?
Going back would be fun. I dislike newer cards (newer meaning 2005 on). Having to read three paragraphs on every card everyone plays is unfun. Also, I still dont understand Planeswalkers.
So they should deprecate it. It feels clunky to convert back and forth from STL data structures and Qt ones when building a Qt app. 
Qt isn’t improving on what’s out there though. Nginx is going to do everything this does but 100x faster and more secure. CMake can do everything QMake does but it’s 100x more flexible and well documented. The STL can do everything QT’s collections can do but 100x faster and more efficient. 
They used to matter a lot more than they do now, for precisely the reason I gave. I work on a large C++ code base that has never bothered for exactly this reason.
Clang is not better either. It is 5 years it silently crashes with a SEGFAULT if one instantiate a static template data member of a template class.
I think in c it can really come in handy much more often than in c++. yes, you shouldn't use it everywhere
an alternative to a non trivial dtor would be using a char array instead of a string for example. or you might decide you don't want a dtor call because you moved out of the object right above where it goes out of scope. so many edge cases. compilers are really good at the general case
But often the only reason you use it in C is because you lack the ability to do it cleanly like C++ would allow.
yes but you might not notice that your function is a few bytes too big. that alone is ok, but it adds up
You measure the total size of the resulting binary, and as long as that meets your size requirement, then why do you care?
suppose your dtor is 12 bytes (aka a few instructions -- short in c++ or c!). then your function exits in two places because you like early returns (which can be fine and maybe this bloat isn't your concern). well a go-to will be a jmp so 5 bytes. so your total cleanup is 17 bytes with inlined cleanup or 10 if you call a function. that's compared to 24 with two inlined dtors. this is the kind of thing you wouldn't probably notice for one function when you measure, but across 100 functions it's a kb of completely wasted space. and btw even on a large system probably abuses your cache.
redundancy is not the only source of bloat
send an example
I think in some of my other comments I've shown that there are advantages to go-to besides "style" I agree tho -- if you have the space and don't care, then definitely write as cleanly as you can.
two main reasons: my instructions may share space with my stack/heap (if I'm using that). in which case the more free space, the more I can do at runtime. also, if i work on this product for 15 years, it's much easier to write very carefully each day rather than "wish I had a few more kb" and "find optimizations" ... especially if you know you regularly push up against limits. so in summary, don't be sloppy today because you'll wish you hadn't tomorrow.
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0847r2.html Omg, the best proposal!!!
&gt;So they should deprecate it. Backward compatibility 
Instructions normally reside in ROM, and you will have to forcefully move those instructions to RAM (and corespondly tell the linker to link to RAM instead of ROM) for your code to eat into your stack/heap-space. More ROM code does not eat into your RAM, otherwise systems with 8kb of ROM and only 2kb of RAM would not be able to run. If you work on a product for 15 years you should definately be more concerned about correctness. Going back to something you wrote 10 years ago and trying to remember why something works (not how...why...) is much harder than going back and looking at how (not why) something works. Optimizing binary size really is no different than optimizing for speed: you must measure before you do something, so you know what to change. Then you can apply compiler-specific flags to turn off stuff like inlining or loop unrollment etc. But chances are that with clear code that a compiler easily can optimize, the compiler will do a better job than you in the first place. Abusive magic code is hard for compilers to optimize. 
Yet I cannot thing of anything more real time than military deployments or factory automation. PTC and Aicas have lots of happy customers using real time Java. Just because a programming language has a GC it doesn't mean it is the only way to deal with memory.
Currently I know of only 3 compilers (all on Linux/Windows) that implement C++17 in full. Is there a reason to believe C++20/23 will be better supported?
multiple qt versions can live side by side.
not every chip distinguishes between parts of memory. even if you have separate rom, the icache is a concern I'm not saying don't be correct! there's lots of correct c code out there.
How this will help to users who would like to migrate to a newer version if that version has changed API?
I guess in this case there aren't any standard webservers. One of their main use cases are embedded devices and you want to limit dependencies as much as possible. Some of the stuff needs to be kept for backward compatibility. Other stuff have slightly different semantics. Qt containers are copy-on-write for example. I believe that for Qt6 they are looking to align better with modern C++ though.
&gt; https://en.wikipedia.org/wiki/Russell%27s_teapot I raise with Elon Musk's Tesla Roadster Hypothesis.
I like the idea that my app can easily implement web interface for users. Like qbittorrent. Using nginx in this scenario is overkill and still you will have to build some basic API.
I work with people who complain that Qt 5 removed Qt 3 support.
"Everything should be backwards compatible forever, durrrr" Enough with the fallacies.
Not sure, why you are getting downvoted - unfortunately, what you say is true. However, it rarely matters in my experience. First of all, the compiler only needs to prove that the address is not taken in the current translation unit which is relatively easy. Second, even when you use enums or #defines the address can be taken (e.g. a function taking a `const&amp;`) and then the compiler has to create a temporary variable on the stack. That is rarely better than having the constant in ROM. And of course enums can't be used for more complex types at all.
**Company**: [Laboratory of Plasma Physics](https://www.lpp.polytechnique.fr/) **Type**: full time **Description**: The Laboratory of Plasma Physics a public research lab strongly involved in the development of high performance computing (HPC) codes for simulating plasma dynamics observed in laboratory experiments and astrophysical environments. We are currently developing a code (named "PHARE" [https://github.com/PHAREHUB/PHARE](https://github.com/PHAREHUB/PHARE)) that aims at solving plasma kinetic dynamics on multiple refined grids on tens of thousands of core. We want this code to be open-source, flexible enough to be used by a broad international scientific community. We look for a passionate C++ developer, who has experience in developing HPC applications. You will join a small team, who love C++ and numerical simulation, and will participate to all aspects of the project, from the design of the code architecture, to the implementation of components and their tests, etc. and participate to the team's publications. No need to speak/read/write french. Contract would not start before sept. 2019. **Location**: Laboratory of Plasma Physics, Ecole Polytechnique, Palaiseau, France. This is a very international environment. **Remote**: We like when things are done and done well, no matter where they were done. Often it's better to sit together, talk and draw on the white board... but we respect everyone's freedom to work from home from time to time and choose its own work hours. **Visa Sponsorship**: No need in France. **Technologies**: We work on unix (mac, linux). We use C++17. The code depends on the C++ AMR library [SAMRAI](https://github.com/LLNL/SAMRAI), and the I/O library [HDF5](https://support.hdfgroup.org/HDF5/). There's a coupling with python via [pybind11](https://github.com/pybind/pybind11) on its way, thus we also need python (3). We use [TeamCity](https://www.jetbrains.com/teamcity/)for continuous integration, CMake to build the project, [Google Test](https://github.com/google/googletest) for tests. **Contact**: contact us at [nicolas.aunai@lpp.polytechnique.fr](mailto:nicolas.aunai@lpp.polytechnique.fr)
You're presuming the compiler won't just optimize it to the same thing as a goto (it will). After a move, the dtor will be called but the optimizer will elide it if it does nothing.
The thing is: Most of the time, it is probably better to just replace the old stuff with the safer primitives than sprinkling annotations around the code that still don't give you 100% confidence. Of course, if you have to maintain binary compatibility that might not be possible and the same is true if your code is written in a way that such changes would propagate through the whole codebase, because the threadsafety isn't properly encapsulated.
It doesn't catch the really tricky cases yet, but there is no harm at all in making these easier cases warnings. There are always developers of different experiences and abilities on every team and this prevents junior devs making silly mistakes.
Qt is switching to CMake as default build system, so they acknowledged that. &gt;The STL can do everything QT’s collections can do but 100x faster and more efficient. That is not true. Qt's collection classes use implicit sharing and copy on write (thread safe). That makes it easier to pass QVectors and such around, the data is only copied when you access it in a non-const way. So there a are different use case for e.g. std::vector and QVector and both have their merit. Also you can use both Qt's collection classes and the STL in the same project. There are not so many Qt APIs that take QVector and such as input and you can always use e.g. QVector::fromStdVector().
Are you a top manager in Qt Company? Do you know what their commercial customers needs? Obviously not, because as a commercial user of Qt I can say that now, when Qt is 27 years old, stable API is 1000 time more important than things like using std::vector over QVector. May be indie developers with 6 months average product lifetime would like to get rid of all old stuff which they don't use and make the library simpler and smaller, but they are not the driving power behind libraries like Qt. Any deprecation is almost always painful because **the library is successful** and many people use different features. Just look at the stories of QBS or QScript: deprecation of them costs a lot of reputation to Qt Company/management. 
My idea is to have both. We need to experiment with a potential HttpServer QML Item. But also one of the use cases is to provide an easy way to serve a Qt build of WebAssambly keeping a WebSocket connection alive to make the communication between client and server easier.
The project is still in the early stages. But we are concerned about security as well; we'll try to do our best.
I'm open to suggestions :)
Well then, that's a very neat idea jsfdez
There is no HTTP Server in the standard library. There is not even a network stack.
Unfortunately, this is common in our user base.
&gt; I believe that for Qt6 they are looking to align better with modern C++ though. We're having a lot of discussions about this. My point is always the same, try to synchronize as much as possible with the STL.
Why not?
Forgive me if I'm misremembering, but I think C++20 concepts make this a lot better: template&lt;typename T, typename U&gt; concept Same = std::is_same_v&lt;T, U&gt;; void f(Same&lt;std::string&gt; auto... strs); If you want types convertible to `std::string` (implicitly or explicitly), it's a matter of substituting `Same` for a more appropriate concept.
option 4: a little bit of type erasure - https://godbolt.org/z/k_-lJb
They force us to pollute namespaces with all the things included in all the headers.
I did a quick test and it seems that for 10.0.17763 the answer is no.
It is not a replacement of nginx. It is more like micro-framework for web
it often will do these optimizations but you have to check every time (or just write what you want). there's lots of edge cases where it can't elide the dtors especially if you have volatile members where it cannot do any tricks for loads/stores. you alone know actually in this case don't bother with that load/store.
What are you using currently for that purpose?
Qt containers, and many other Qt types, have Copy on Write, which is very useful with their signal/slot system. Also, someone may argue that, their api, is more elegant. Their hashtable for example.
&gt; Why isn't it possible for the three major implementations to come together outside of the c++ standard and decide on one? That would be illegal.
That's not C++20 concepts. [This is](https://godbolt.org/z/CknAE8): template &lt;Same&lt;std::string&gt;... Args&gt; void f(Args &amp;&amp;...);
 template&lt;typename... Ts&gt; using AllStrings = typename std::enable_if_t&lt;std::conjunction_v&lt;std::is_constructible&lt;std::string, Ts&gt;... &gt;&gt;; template&lt;typename... Ts&gt; using NotAllStrings = typename std::enable_if_t&lt;!std::conjunction_v&lt;std::is_constructible&lt;std::string, Ts&gt;... &gt;&gt;; template&lt;typename... Ts, typename = NotAllStrings&lt;Ts...&gt;&gt; void f(Ts...) { static_assert(false, "Not all arguments are strings"); } template&lt;typename... Ts, typename = AllStrings&lt;Ts...&gt;&gt; void f(Ts&amp;&amp;... xs) { } using namespace std::literals::string_literals; f(std::string(), std::string_view("sdfdsfsd"), "fdsfkldsfnlsd", "sdfdsfsdf"s); //yay conversions now work! If you want it all (besides the implementation being in a .cpp file) you could also use a mixed approach like this. Overload resolution only works since one version doesn't use universal references, not sure if that could lead to some issues? &amp;#x200B; I'm still a fan of only using static assert, since it gives you a compile time error, which is even more important than having the requirements visible in the signature. I also fixed the AllStrings condition to allow converions - that was just a wrong implementation imho.
I think the terse (auto) syntax with has been voted in on the last committee meeting, I just think this hasn't found it's way into a compiler yet?
&gt;P1072 specifically does not propose to add the resize_default_init method to vector [...] because it’s unclear whether a general facility would be useful, whereas anyone who works with parsing of character data (e.g. receiving packets from a network socket) knows the use-case for string::resize_default_init. So there's apparently a clear use case for `resize_default_init` for parsing character data, but there's apparently **not** a clear use case for `resize_default_init` for `std::vector` for parsing binary data? Wat.
I think a 25% improvement is overly optimistic, much less a 2000% improvement. The code is already heavily optimized (much of it by hand using ASM). Plus, since optimizations can't affect the semantics of the program, there's a limit to how much the compiler can really do.
Isn't that just an issue with the structure of the header file though? If I look at the windows.h file in the MinGW distribution it is a master header file that has a collection of conditionals that include other header files as appropriate. That type of conditional inclusion could also be required in a source file importing modules. If you drill into one of the included files as an example, winnt.h what you see is a collection of forward reference function declarations, typedefs, structures and constants defined. The macros are in-line replacements for functions so either you use the macros or define additional function calls. Even in a module approach you need the forward declarations and structure declarations. You need the typedefs to go with the function declarations. You could ignore the #define constants but then you'd be using magic numbers or magic strings in your code. If there are details in these headers that should be hidden and only available to the windows library that it goes with then those source files could include both an internal header and the external header. If the information is needed by consumers of the windows library then it has to be defined in the header. So you have the same level of control over what is exposed do you not? What is magical about modules that makes them so much more needed than what is currently available? I'm trying to understand exactly what problems modules are looking to fix that a properly defined header file and associated .so /.dll file don't already provide? 
The `initializer_list&lt;value_type&gt;&amp;&amp;` constructors of `flat_map` all immediately delegate to a constructor template talking a `const Container &amp;`. For example: flat_map(initializer_list&lt;value_type&gt;&amp;&amp; il, const key_compare&amp; comp = key_compare()) : flat_map(il, comp) { } If they took `initializer_list` by value, this would be just infinite recursion. Likewise, the `make_optional` example present no problem: lvalue `initializer_list` arguments would simply go to the `const Container&amp;` overloads instead. &gt; _Never wax poetic about a single overload without considering the entire overload set._
This is my understanding.
&gt; C++ continues to not-have (and likely will *forever* not-have) a way to encode “any arbitrary template with any arbitrary parameters.” &amp;#x200B; I was discussing this some time ago and wondered what the difficulty with such a feature would be? You'd only be specifying that a template parameter could be either type or non-type. Great for metaprograms that wish to forward arbitrary inputs or extract specialisations of arbitrary template templates.
This is based on my somewhat limited knowledge of xframe. So correct me if I am wrong. In xframe you have to know the types contained in an xframe instance at compile time. In my case you can add/remove any columns of any type at any point in the process to any instance of DataFrame. Another thing I like more about my DataFrame is that it is easily extendable. For example, if you have some proprietary data analysis algorithm, all you have to do is to put that in a functor and call one of the \`visit\` methods. From my limited knowledge of xframe, I am not sure how easy that is in xframe. That brings me to another point that xframe interface is just too complicated for simple people like me.
Impressive! Might eventually replace the MEAN-stack with this instead for cleanqt :)
Good clarification, but I (think I) got it right first time. Of course you need Javascript in the browser for anything more than pure static design. But to do it for non-web programming is like killing bees with a flame thrower. Especially in the context of C++ programming. On the other hand, there are a lot of web designers than can leverage his knowledge to program in cpp, C# and so on. That's cool.
In general Qt is a fantastic framework. For a long time a complete true cross-platform Cpp framework including threads, signals, slots, GUI, IDE... you name it. It's a shame these constants changes of license and owner make companies reluctant to adopt it.
For one application, when I moved from GCC 4.1 to 4.4, the throughput of the generated binary jumped 6x. For another application, when I moved from GCC 4.9 to GCC 8.2, efficiency jumped 10%. This latter statistic though is on code that had already been pretty thoroughly optimized. 
They do, they'll optimise for code size
I'm so sad about namings and special cases we have. What is the best way to say that default initialization for char *is not* '\0' ? How to explain that string::resize_default_init() is needed to make things faster, but vector is not in the same category ? Can someone list all possibilities program will have by using resize() or resize_default_init() ? 1. `string&lt;char&gt;().resize(10)` is same as string&lt;char&gt;().resize(10, char()). char() is value initialization that leads to zero initialization. We have char* that point to 10 '\0'. Reading this memory is not UB. Explaining this to someone new may lead to assumption that any T() is zero initialized. 2. `string&lt;char&gt;().resize_default_init(10)` fills 10 chars with default initialization that leaves chars in indeterminate state. We have char* that point to 10 bytes of unknown things. Reading this memory is not UB. And, wait, there is: 3. `string&lt;unsigned char&gt;().resize_default_init(10)` is same as above except reading from this memory is, most likely, not UB, thanks to special cases with default initialization for unsigned char and std::byte. That is one more point that we have few things in C++ for experts-only usage. And not accepting coroutines because they are not beginner-friendly - should not be argument. Is std::string beginner-friendly ? 
Why are designated initializers contentious?
Hey Jose. Quick question: If I compile a Qt app to wasm, can this app running on the browser make get/post requests to a webserver/rest api? Or is this feature not there yet?
Both are C++20 concepts. The problem with yours though is that it won't accept something like: std::string s = "hello"; f(s);
Almost all the plugins that I use are already written here. I want to add: [GitHub Extension](https://marketplace.visualstudio.com/items?itemName=GitHub.GitHubExtensionforVisualStudio). The GitHub Extension for Visual Studio makes it easy to connect to and work with your repositories on GitHub; [Deleaker](https://www.deleaker.com/). Deleaker is a Visual C++ extension and standalone application for memory leak detection - memory, GDI, and handles so far. [SQLite](https://marketplace.visualstudio.com/items?itemName=ErikEJ.SQLServerCompactSQLiteToolbox). Connect to SQL Server Compact 4.0, 3.5, SQL Server and SQLite database files in Visual Studio 2012 and later, including the free VS 2013 and later Community Edition
So where is this language level memory barrier feature? What is its syntax? Or are you going to say "Use the OS primitives"? What happens if there is no OS?
.
You can use the access token/passkey idiom to work around the `private`/lack of `friend`ship issue: template&lt;class Creator&gt; class access_token { access_token() { } friend Creator; }; class widget { public: // Only widget can create access_token&lt;widget&gt;s, so you can actually only // call this constructor from inside widget or if you have been passed a // access_token&lt;widget&gt; (which you are allowed to copy). explicit widget(access_token&lt;widget&gt;); std::unique_ptr&lt;widget&gt; make() { return std::make_unique&lt;widget&gt;(access_token&lt;widget&gt;()); } }; This idiom is a bit like turning friendship into an object that you can pass around.
The visit refactor didn't make 15.9; it has been in every VS2019 preview.
There are more clever ways to do that though with actual code though. Encapsulate the mutex into a class so that it returns a unique type when it's locked; when the type is destroyed the mutex is unlocked. Now have any functions that require the mutex is locked take the unique type by reference.
ATM I think only WebSockets are supported.
Yes, /u/mcypark and I discussed his visit refactoring a bit at the San Diego WG21 meeting, and I had previously chatted with /u/quicknir about it on Slack. The MSVC implementation is less recursive and more preprocessor-spammy so as to be friendlier to our optimizer, but the approaches are fundamentally similar. I suppose I should credit /u/cppsage - if that's Matt Calabrese's reddit username - for describing switch-block visitation to me in the first place. 
QMake predates CMake. If you wated to compare build systems the better comparison would be QBS, which is significantly nicer to use than CMake, but is unfortunately now deprecated due to it not really seeing widespread adoption. As far as the containers, the containers do a couple things that the standard library doesn't: * Guaranteed ABI. * Implicit sharing * QString is actually a usable string class and knows about unicode. I also haven't seen any benchmarks that Qt's containers are particularly slower than the standard library ones. They do provide considerable convenience though.
The real problem with templates regarding code size is that as soon as the template is non-trivial (this means the vast majority of STL for example), it's extremely difficult to reason exactly what is happening codegen-wise. Another (though lesser) problem is that compile time specialization can encourage generating multiple almost identical versions of the same code without it being obvious at a glance.
I hate how I can fairly easily write a type trait to test if a class is an instantiation of a template with an arbitrary number of type parameters but as soon as it has a non type you can’t tell anymore. 
Compiler bugs are off topic
When did the `Concept auto` syntax get in?
This past meeting, San Diego in November.
my issue is there's no practical method of encapsulating a mixture of types and non-type - the language makes it very hard for the two to intermingle. I imagine something like the following to be possible: &amp;#x200B; template &lt;magic...&gt; pack {}; template &lt;template &lt;magic...&gt; class C, magic Head, magic... Tail&gt; auto constexpr first(C&lt;Head, Tail...&gt;) \-&gt; pack&lt;Head&gt;; where you can pass in any arbitrary pack of types and non-types (or any template for that matter), and have the first element returned to you in a simple wrapper type.
I may have misunderstood your situation and please ignore this comment if so but I would like to clarify something. You say ImageMagick *had to load the large scan image (over 100MB) over and over again (I assume other applications/scripts would do that as well)*. This suggests to me that you are cropping each individual tile from the large, original image in a loop. I would have thought that you only process the original file (you say it's 12000x8000 and larger) *once*, when you crop it four times into the next zoom layer. This new zoom layer, composed of four images, can be further subdivided to process the next layer of sixteen images and so on in a standard recursive way. I suspect this would be much faster than what you reported (measured in minutes and not hours hopefully).
Speaking of Bills, why does Billy Gates co-own the top floors of the mandalay bay hotel with the Islamic terrorist Al Waleed Bin Talal? The Las Vegas massacre was pretty fatal.
Thanks! I'm open to providing suggestions :) One thing would be simple HTML5/JS widgets - which is mentioned by someone else and touched on in your post as serving QML - but using native C++ code as well. At least maybe something simple like QImage (QPixmap), QTextArea, Q___Layout as a first set of tools. From what I've read, the WebGL and WebAssembly implementations seem to be specifically lacking multithreading support, and having a thin client interface w/multithreading support would be awesome - even if that processing is done server-side. This would be huge in scope though, and also somewhat competing with projects that are already part of Qt. I'm not sure if there's a straightforward way to take advantage of the aforementioned interface tools for the UI part while taking advantage of this new set of classes for the HTTP/WebSockets communication for server-side multithreaded processing. I haven't looked into if/how you spawn processes for responses yet, but some kind of sandboxing would be great for scalability. For examples, on Windows I would prefer to launch a web server as an ISAPI dll that spawns a new process for every connection. Otherwise, a single client can cause the whole server to crash and stop responding to everyone. Another big one would be session handling, as well as easy connections to Qt Network Authorization.
Because QT feels the need to remake everything. At this rate they might end up releasing their own OS.
I think you got something wrong here. The expression `foo(...)` is doing some kind of initialization of foo, but then the fact which kind of initialization / which constructor was used somehow gets lots and the arguments end up being passed to std::make_unique, which will inevitably forward them into `foo(...). What would happen if you did `new(...) foo{.a=0,.b=1}`? You should see how any solution that ends up calling the current form of `std::make_unique` will not support aggregate/designated/initializer_list initialization. The idea to overload new is a possibility but I think it must involve a factory object automatically generated by the compiler (invented lambda). Maybe this: `new(std::new_unique) Foo(...)`. Which would work very similarly to my original proposal, just with different kind of syntax / operator definition, e.g.: ``` template&lt;typename Expr&gt; auto operator new(unique)(Expr expr) -&gt; std::unique_ptr&lt;TForExpr&lt;Expr&gt;&gt; { using T = TForExpr&lt;Expr&gt;; return std::unique_ptr&lt;T&gt;(new T(expr())); } std::new(unique) A(1, 2); std::new(unique) S{1, 2, 3}; std::new(unique) factory(); class optional { .... template&lt;typename Expr&gt; optional &amp; operator new(emplace)(Expr expr) { reset(); new(&amp;_storagePtr()) T(expr()); _has_value = true; return *this; } } optional&lt;T&gt; opt; opt.new(emplace) T(...); ``` 
This is true and my proposal is actually based on the idea of factories, it just goes further by also proposing proper syntax instead of having users write lambdas for the most basic initialization tasks. It wouldn't be so bad if C++ had a terse lambda syntax like \`() -&gt; T()\` (like C# and Java have), but currently you would need to write \`emplace(\[&amp;\]{ return T(); })\` which I do not consider a sufficient improvement to be worth changing things.
Sure, but I'm interested in improving the language to make these hacks unnecessary.
Reshaper
&gt; I think you got something wrong here. The expression foo(args) is doing some kind of initialization of foo, but then the fact which kind of initialization was used gets lots and the arguments end up being passed to std::make_unique. Hah, you're totally right. The idea formed in my head that "`new` doesn't have this problem" but that got lost as I whipped out a half-baked sample snippet. :) The factory function idea isn't bad. Another option might be to explore some more refined forwarding mechanics; the problem really is that when you make a forwarding function, you can't constraint the forwarder's arguments to those expected by the forwardee, and that in turn breaks deduction and other things. It'd be nice to somehow be able to say "`vector&lt;T&gt;::emplace_back(args...)` will forward to T(args...)" in the declaration of `emplace_back` itself so that the compiler can constrain `args...` within the signature of `emplace_back` itself. A totally strawman syntax might be something like: template &lt;typename... Args : T(Args...)&gt; T&amp; emplace_back(Args&amp;&amp;... args) { return _do_emplace_back(std::forward&lt;Args&gt;(args)...); } The idea being the `typename... : T(Args...)` in the template parameter declaration, which is meant to mean "the `Args...` parameter pack must forward into the expression `T(Args...)`, with the intent that that'd be enough for the compiler to do useful deduction (no idea if that's reasonable off the top of my head, though).
&lt;charconv&gt; is nice! Are there any plans to support types other than `char`? It would be nice to have overloads for `std::from_chars` and `std::to_chars` for `wchar_t` strings, especially on Windows. I mean, I suppose you could just convert the results over, but I figure that would somewhat defeat the purpose of it.
Inventing new kind of template parameters seems like overkill. It's much easier for the compiler to generate a factory function as if one wrote a lambda, and doesn't require language changes beyond a certain very specific syntax that doesn't affect other parts of the language.
I think it's more that doing this correctly for vector is difficult in the allocator model, because vector calls allocator construct and destroy. String already doesn't deal with throwing element types or construct or destroy, so making it work there without breaking other things is easier.
BTW, I've edited my original post to use a new-based syntax (it's the same as before but with new in the middle).
In Visual Studio 2019, there is 64-bit debugging. You might find this blog post helpful: [https://blogs.msdn.microsoft.com/vcblog/2018/12/18/out-of-process-debugger-for-c-in-visual-studio-2019/](https://blogs.msdn.microsoft.com/vcblog/2018/12/18/out-of-process-debugger-for-c-in-visual-studio-2019/)
I personally prefer types. Indeed, I've added a `Mutex&lt;T&gt;` class to our C++ codebase, shamelessly adapted from [Rust's `Mutex&lt;T&gt;`](https://doc.rust-lang.org/beta/std/sync/struct.Mutex.html). The usage is super simple: class Foo { public: int get() const { auto locked = mState.lock(); auto a = locked-&gt;get_a(); auto b = locked-&gt;get_b(); return a + b; } private: class State { public: // methods only callable under lock int get_a() const; int get_b() const; private: // data }; Mutex&lt;State&gt; mState; }; And by cleanly separating which methods need to acquire the lock (those of `Foo`) and which don't (those of `Foo::State`), it is also very good at getting re-entrancy: `Foo::State` cannot call any `Foo` method which would attempt to lock again. The implementation is dead simple, too, with `Mutex::lock` returning a `MutexGuard&lt;U&gt;` (either `T*` or `T const*`) which is movable-only and releases the lock in its destructor. It's not a panacea: you can accidentally leak a reference to the inside of `Foo::State` if you are not careful. Still, it makes writing locking code much easier.
Unless I'm missing something I don't see how that would be an issue: The entire construction doesn't really make sense for anything but trivial types so it should be ill-formed/UB when used on an instance of an instantiation of `std::vector` unless the first template parameter is trivial.
construct and destroy are things that can do things even when the value_type is trivial.
And parsing isn't the only important use case. In GROMACS we have an kernel important to the overall app performance where `v.reserve(n); for(...) v.push_back(...)` is 38% slower than `v.resize_default_init(n); for (...) v[i] = ...` (the kernel filters a vector so everything other than the push\_back loop is cheap). We ended up emulating `resize_default_init` by using `resize` with an allocator which does default init. But that isn't a nice solution. So I would very much like to see `resize_default_init` also for std::vector.
&gt;check out how many components of boost become part of STL That's because Boost is intended as incubator for STL 
You might want to check out www.webtoolkit.eu Around for couple of years already and very mature.
&gt; easy to solve Only people with lack of understanding of macros and build process in general believed that, the rest were either skeptic or expecting to be surprised by geniality but found wishful thinking. The current proposal of modules not only requires the compiler to meddle into preprocessor business, it also forces it to meddle into build process stuff. If you want lower compilation times clean your headers and get rid of your coupling issues. There is no magical wand to fix your shit for you. 
PS: the biggest problem with using a `DefaultInitAllocator` to achieve the same thing as `resize_default_init` is that it isn't possible to move out of a `std::vector&lt;T, DefaultInitAllocator&lt;T&gt;&gt;` into a `std::vector&lt;T&gt;`. If the Allocator-extended move constructor of vector (`vector(vector&amp;&amp;, const Allocator&amp;`) would be extended to also support moving between vectors with different allocator types, then this would be resolved and it would be an OK work-around for not having `resize_default_init`. Maybe that could be a solution given /u/BillyONeal's reason why `resize_default_init` for vector is likely not achievable.
So it's like the opposite of [`destroying_delete`](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0722r3.html)? (`constructing_new`?)
Just roll your own man, it really isn't that much work. I wrote a UTF-8, UTF-16, and UTF-32 version (8 and 16 are just wrappers around 32)
How are you guys handling the shortest option btw? I've got a little section in my if else for it, but beyond that haven't fleshed it out yet. I've been thinking about either running both (which would take a long time comparatively) or maybe just choosing one, and I'm not really sure which is the best option.
I think it’s *achievable* but specifying it in a way that doesn’t break the allocator model is complex. Probably means putting some constraints on the allocator / constraints like “if your allocator construct/destroy do anything you get UB”.
It will, it will take by value (copy) 
If you got this to output 3d printer files it would be great.
Last I checked, the version of "designated initializers" proposed for C++ doesn't allow this cool hack. The C++ proposal intends to require that all the designators appear in the "proper" order, just like member initializers. So the C version has two advantages that C++ won't have: Ability for the library to "default" some parameters to things other than `0`. #define http_set_cookie(http___handle, ...) \ http_set_cookie((http___handle), (http_cookie_args_s){ .max_age=3600, __VA_ARGS__ }) // OOPS! In the syntax proposed for C++, this wouldn't be allowed. Ability for the user to use the facility at all, without knowing [the struct fields' declaration order](https://github.com/boazsegev/facil.io/blob/master/lib/facil/http/http.h#L133-L156). http_set_cookie(request, .name = "my_cookie", .name_len = 9, .value = "data", .value_len = 4); // OOPS! In the syntax proposed for C++, this would have to be written as http_set_cookie(request, .name = "my_cookie", .value = "data", .name_len = 9, .value_len = 4); 
The storage looks ok to my eyes, but I get issues with the initalization, which only seems to take rvalues under gcc8. A struct set up with #pragma pack(1) doesn't suffer from that issue. &amp;#x200B; int i{500}; PackedTuple&lt; char, int &gt;( 'a', i ); // fails PackedTuple&lt; char, int &gt;( 'a', std::move( i ) ); // ok &amp;#x200B;
So basically it's the same as asserts (or actually is weaker than asserts) but they apply to overrides as well(?)
you mean industry standards such as [GENIVI for automotive which are based on Qt](https://at.projects.genivi.org/wiki/pages/viewpage.action?pageId=11567879), [security standards compliance](https://www.prnewswire.com/news-releases/the-qt-company-and-green-hills-showcase-significant-advancements-in-integrated-automotive-hmi-platforms-300579554.html), and [widespread use in medical devices](https://www.qt.io/qt-in-medical/) ?
The logic of the tile generation is not thaaaat simple, because the position of the tiles are on fixed geo locations but basically your attempt would speed it up. I was pretty curious about the speedup and manually measured the time image magick needs to cut a 6000x4000 tile from the original image (which would be the first iteration) and it took 23 seconds. To generate this zoom level would therefore take 1.5 minutes, because image magick must generate four tiles and therefore is called four times. For the next zoom level I took the 6000x4000 image and cut a 3000x2000 tile out of it, which took 5.5 seconds. This zoom level would also take 1.5 minutes. The next level would take about 1.6 minutes, the next 1.7 and the last I tested (with an tile size of 375x250px) would take 1.8 minutes. All of these values were manually tested for one tile I cut out and therefore aren't that accurate but I was impressed that it wasn't getting faster towards the end (even though the last tile took only 0.1 seconds). It may take like 15 minutes (I just assume that) to generate 13 layers which is still more than my image2tiles program. I'm also thinking of more optimizations using multiple threads. Things that aren't possible with simple scripts. However your attempt is way faster than my first script, so thanks for the suggestion :)
zapcc essentially did just that, as I understand. This is also Rust's approach to improving compile times, since Rust has the technical equivalent of template instantiation overhead but dialed up to an 11. zapcc pretty much died on the vine due to being a perennially out-dated proprietary fork of clang. Rust seems to be doing fairly well with their caching; they have their own de-facto-standardized build system that can play along however necessary, though, which probably helps.
Well, slow-_er_. Streaming data to/from a common binary format is not at all slow, but it depends on the in-memory format needing to be careful designed for serialization (which incidentally, tends to _also_ mean that it's designed in a data-oriented fashion that makes use at run-time faster, too). That said, the templates wouldn't be cached _in_ the BMI because that would imply the BMI is mutable based on dependents, which would be bad, so the instantiation cache can be compiler/version specific while the BMI could still be a common format.
Yes! I was thinking about STL, however STL is based on triangular surface mesh, and I'm only having voxels (cubes). Any ideas on how to convert voxels into triangular mesh (or some alternative file formats)?
If you're _only_ solving `new`/construction problems and not all forwarding problems, sure. :)
If you read the other replies, you'd see that they predominantly don't.
Not relevant to your content, but the markdown you use doesn't work outside of the Reddit redesign.
Like you say, the misaligned access is the show-stopper. But, the show must go on... For the storage, an array of unsigned char (or std::byte if you prefer) is simpler and would guarantee no internal padding, though there may be padding at the end of the containing class: std::byte bytes\[(0u + ... + sizeof(Args))\]; instead of std::tuple&lt;std::byte\[sizeof(Args)\]...&gt; I don't know if tuple is specified to have no internal padding for byte arrays like this. Anyway, tuple is a super complex dependency best avoided if possible (inheriting from tuple is questionable too). Once you remove tuple as the storage, its only other use is as a type-list to lookup the Ith type for get&lt;I&gt; (your use of std::tuple\_element\_t&lt;Is, Types&gt; in the init function can be replaced by 'Args'). So, you could switch the use of std::tuple\_element\_t for something simpler. Here's a version using byte-array storage and the tuple dependency replaced with a homespun 'Ith type' getter. [https://wandbox.org/permlink/YLMi7AipwGUBK7sW](https://wandbox.org/permlink/YLMi7AipwGUBK7sW) [https://godbolt.org/z/2JGjcj](https://godbolt.org/z/2JGjcj)
Do you still have this GROMACS benchmark around we can use? It seems likely that we could reduce some of that cost. Also note that if you tested on VS older than about 15.7 or 15.8 (don't remember which specific release) vector::push_back was not inlinable because we had the realloc code in there. Factoring that out resulted in huge wins in some benchmarks by allowing the non-reallocate case to get inlined into callers.
The SFINAE and the `static_assert` solutions are not exactly equivalent to the original problem `void f(std::string const&amp;... strings)`. For example, if `f(i1, i2)` simply returns `i1+i2`, the SFINAE solution would fail for `f("1", "2")`. Another solution (in case a simple wrapper function is acceptable) template &lt;typename ...Strings&gt; void f_impl(Strings const&amp;... strings) { // body of f... } template &lt;typename T, typename ...&gt; using First = T; template &lt;typename ...Args&gt; void f(Args&amp;&amp;... args) { f_impl&lt;First&lt;std::string, Args&gt;...&gt;(std::forward&lt;Args&gt;(args)...); } Now in the body of `f_impl` one can be sure that all the `strings` actually are `std::string`s and implicit conversions to `std::string` will work.
That can happen but it's not super common if you declare simple constants as `static constexpr int x = 5;` or whatever in header files, between inlining, the tendency to not pass simply types by reference, etc. That said if you really want to avoid that storage, a much better approach than an enum is still just a constexpr function returning the value. With the enum approach after all you are implicitly utilizing a conversion, which conceptually is like a function call (just not really, because it's a built in). A function like that will actually work for any type, unlike `enum`.
Marching cubes or Surface Nets.
It's well known that basic_string is better vector: it has SSO, concatenation facilities... ^^
It always works. That's because this essentially does `new(ptr) T(function())` where `function()` returns type `T`, and since C++17 this does not involve copy/move. This is basically guaranteed copy elision at work. 
It's /u/mattcalabrese. :-]
&gt; and since C++17 this does not involve copy/move To be clear, since C++17 it's _possible_ to not involve copy/move, but it depends entirely on how `function` forms its return value.
The source code is available at the following address: [https://github.com/PardDev](https://github.com/PardDev) 
Thank you good sir.
Circular dependencies are never a good idea...
You are looking for: C++Now 2018: Alan Talbot “Moving Faster: Everyday Efficiency in Modern C++” [https://www.youtube.com/watch?v=J9yVA341zrw](https://www.youtube.com/watch?v=J9yVA341zrw) He gave an identical talk at cppcon 2018: [https://www.youtube.com/watch?v=EovBkh9wDnM](https://www.youtube.com/watch?v=EovBkh9wDnM) but imho the C++Now version is much better. 
Thanks a lot! This was the one
You're welcome, mate!
I would say avoiding circular dependencies is a benefit, not a limitation 
You... fix your code base ? I don't believe it is an impossible job to do on any code base of any size. If you are lucky, it's just fixing compile errors around. Using the last available dependency is good practice and part of a software lifecycle.
There isn't that much of HTTP server as an API around there. Having one which integrate seamlessly with you "GUI toolkit" is nice, for coherency and dependency management. Just because they do "other stuff" doesn't mean they are slacking off on the core features. Actually making something (like QtCreator, GamaRay, etc) of their SDK is pretty good idea and is a sane way of pushing the SDK forward. &amp;#x200B; And BTW, it's spelled "Qt".
Not too familiar with modules yet, but I don't understand why an *interface* file is compiled from a cpp and not from a header. Would it not also be much simpler then? IMO a finished module should contain the linked code, but the bmi should be just the interface... I assume I'm missing something?
`constexpr` function is not better than `constexpr` variable in that regard. `consteval` function however wouldn't have such problems.
https://gcc.godbolt.org/z/R8ET7C. As you can see, constexpr function and enum have identical code. You can also see that in both cases the generated assembly is quite a few instructions longer... in fact, you end up with a larger binary when you use an enum or constexpr function because the extra instructions take more space than storing a single integer.
When I change `const auto a` to `auto a` both compile just fine. When I do: const std::vector&lt;int&gt; b(1) b[0] = 1; I get an error in both. I view this as consistent in GCC and not in Clang. Unless there is an obscure part of C++ that I am not aware of.
I totally get why, but it's funny to see an image in big text: \*Game Tutorial 5: Drawing a Triangle\*
Yeah, that's because it's a bit low level, but I hope you'll enjoy it anyway! :) 
I believe I have some code that will handle this from a somewhat recent project. Send me a PM and I can share and/or help you with an implementation. 
I haven't heard anyone talking about that. Implementing charconv from scratch is an enormous amount of work, so that's out for users. Wrapping the char functions is slower than desirable because you would need a temporary char buffer, and "smash non-ASCII characters" conversions, and if you want to get the pointer return values right, that's even more work.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/al6t3b/good_book_or_online_resource_to_learn_c_quickly/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Decimal shortest is powered by Ryu, with additional code for floating-point values that are large integers in fixed notation. Ryu is insanely fast; the large integer codepath is moderately fast and we're working on it. At no time do we perform two kinds of conversions and select the shortest. For `chars_format::general`, which needs to select between fixed and scientific, I implemented a test that selects the format ahead of time, once we know what the exponent is. Decimal shortest is cool for a couple of reasons: (1) it is amazingly fast, and (2) it is super pretty. The fact that it preserves the bits is a distant (3). The aesthetic behavior is non-obvious - it turns out that if a user provides you a value like 17.29, then Ryu will emit exactly that, and not any extra garbage afterwards. If your numbers are being generated by mathematical processes (e.g. sqrt) then the shortest round-trip doesn't really help make things nicer for humans, but if the numbers are provided by humans, it is super nice. Using something like the default of precision 6 is the worst of both worlds: it is slow, it is ugly for human-provided numbers, and it loses data for numbers that actually have more digits. Precision should be used in only a couple of cases (when you need the entire mathematical value of a floating-point number, or when you are intentionally truncating data to fit a narrow column, e.g. 3 decimal digits). Hexfloat shortest is custom code that I wrote; it wasn't especially hard. (Hexfloat precision has a nice rounding trick that I developed with Billy.)
As the author of the earlier flat_map proposals: I had numerous people approach me at CppCon and our GDC SG14 meetings asking for split storage based on their implementation experience. In games, of which the code is typically not open source, so finding those implementations is of course more difficult. This feedback informed my papers and I likewise passed that on to Zach's. Performance of container operations is not the only concern. Being able to grab a `span&lt;value_type&gt;` from the underlying storage can be hugely useful, for instance.
How could I make it work? I use triple backticks for the code.
I'm actually really surprised charconv made it through the committee without anyone mentioning that! Especially given the new character types introduced in recent standards.
As far as I know, on the old design, the only way is to start each line with four spaces (and put a blank line before/after your block).
D uses alias as a template parameter for this. What about using “using” in C++? But using in C++ just forwards types. Alias in D works with anything.
Just from reading this blog post, it makes me wonder if it really makes sense to standardize this at all. It's a pure performance optimization as I understand it, and the standard does a lot better with "good enough" performance then it does with highly optimized ones. For a couple of reasons. First, everyone's performance tradeoffs are different. Second, you tend to be constrained by these overly generic interfaces that many people writing ultra high performance code don't value very much. What's the killer use case for this data structure that makes it worth standardizing, or is it just a half performance measure?
But passing `int` by `cont &amp;` is already a code smell. Also, if you use `void bar4() { foo(5); }`, you'll see that it'd generate the same code. So it further shows that it's the same code/behavior as with `#define`. Also, if you have several constants with the same values, they'll be duplicated in such usage. So yeah, many different trade-offs/gotchas.
Thank you for reading my response and writing such a detailed reply. It makes sense to me that each layer would take approximately as long as the previous layer because every layer you process in this recursive fashion must read exactly 12000x8000 pixels and also write out exactly the same number of pixels. This is a very rough intuition and doesn't account for overhead such as compression in the image formats but the numbers you've provided seem to agree. The original loop, on the other hand, would write out 12000x8000 pixels but read 12000x8000x*n*, where *n* is the number of tiles. This would explain the very long times required for higher zooms using this technique. Please note that I did not mention this to critique your image2tiles program or its raison d'être. I may use it myself one day as it relates to my work. Good luck to you and keep up the good work.
Why do you have an object initialized at that point, then?
I don't need 99% of the symbols `Windows.h` brings in, some might conflict with other code, and you cannot namespace it or otherwise hide it. Headers make it difficult to hide implementation details, and make it easy to pollute the namespace. Modules resolve that.
In C++, you should be using the atomics library - `atomic_thread_fence` or `atomic_signal _fence`. You can also use the C stdatomic library, which are named the same but are in `&lt;stdatomic.h&gt;` rather than `&lt;atomic.h&gt;`. GCC and Clang also have builtins for this, and you can also declare a compiler-side memory barrier with `asm volatile("":::"memory")`. Often, specific platforms/architecture SDKs also offer libraries for that. Memory barriers have nothing to do with the OS, so I'm not sure why you brought that up.
Qt has offered a complete cross platform C++ library for many years. As far as I know there's no other C++ framework that has ever offered the same (and for free, though the commercial licenses have always been a problem). The key words are "complete" and "cross-platform". Including sockets, threads, signals, slots, GUI, IDE with RAD for the GUI, basic sound, strings, OpenGL/DirectX support, embedded browser. Many of these features are still unique. Standard C++ threads were introduced in C++11 and not mature until C++14. STL string features are still today inefficient.
In tech there is a lot of age bias against older programmers. Change "15 years experience" to "over 5 years experience". My post is as serious or stupid as you choose to consider it.
In my tests, the majority of the member functions of the container library end up code folded, as the underlying structure and code don't change per instantiation. `std::vector::size` and `std::unordered_map::size` were folded into the same function, regardless of type.
well I'm only 38yo, but yea I remember when my 60yo father was looking for a job in the field, took him 18 months...
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/al7lu4/best_book/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Nice! Always on the hunt for a good coding blog, especially by programmers who've seen a thing or two. Looking forward to some great posts. 
Avoiding circular dependencies is good. The language forcing you to avoid circular dependencies due to technical limitations is not.
I don’t understand what this does from the examples. I expected it to just stack the cross sections, but the star/bolt example is more than 2 layers deep.
just checked the github site for this book, used google translation to english, it look fine.
Yes, I should have used: ``` template &lt;class...Args2&gt; //requires std::is_convertible_v&lt;Args, Args2&gt; &amp;&amp; ... packetTuple(Args2 &amp;&amp;...); ``` 
thank you! I will do my best not to disappoint :)
and using a std::map for an enum is really overkill so...
Because in the case that a % 2 == 0 (I.e. a is even), 0 implicitly converts to false, so the string is not assigned the value “odd”. If a % 2 == 1, this is implicitly converted to true, which results in the if being executed. Feel free to PM
agreed, from the standpoint of performance. but it is a clean solution in my opinion. definitely not as fast as the X Marco approach though :)
r/cpp_questions/ P.S. Non-zero integers are contextually convertible to boolean true.
As far as I can tell: You are prompting for a number and then assigning it to a. You then initialize the variable "res" to even. The next step is where the magic happens. The first part of the statement is `if (a % 2)`. This will return either 0 or 1. This is because `a % 2` is really asking what the remainder of `a / 2` is. This is a great way to check if a number is divisible by 2 (i.e. even). Even numbers are divisible by 2 so the remainder is 0. Odd numbers are not divisible by 0, so the remainder is 1 (e.g. 7 % 2 == 7 - 2 \* x + y == 7 - 2\*3 + 1. 1 is the remainder because we have to add it to get back to the original number). Next, because we are saying `if (a % 2)`, the compiler is smart enough to cast this integer (0 or 1) to a boolean value. The compiler will set *anything* that is not 0 to true. &amp;#x200B; From here I am not a huge fan of the syntax. Saying `if (a % 2) res = "odd";` is the same as saying if (a % 2) { res = "odd"; } Which sets anything that is an odd number (`a % 2 == 1`) to true, we will assign res to "odd". From there, we just output some stuff to the console. 
Thanks for the suggestion. Will post my doubt in that thread 
Thanks for the explanation.
That was so simple and greatly explained. One question though, is this convention is safe while using in big programs or should I use the If-else convention?
+1 ... and it doesn't take much [many lines] to implement one on top of std::lower_bound [possibly std::sort] and get's as good as it gets with just that.
I'm definitely not an expert, but you could put this in a pretty simple function like: bool evenCheck(int a) { return !(a % 2); } This basically takes your int and checks the remainder. Because that remainder is 0 (A.K.A false), you can take the negation of that to return that it is true. I usually go for something like: bool evenCheck(int a) { return (a % 2) == 0; } but they should both do the trick.
I suppose that's one way to statically check it, but I'm not in favor of polluting internal interfaces with tag arguments, even if the optimizer makes them zero-cost. Might also be a problem with operators. One library I've worked on has a large internal, performance-sensitive API surface that is used on two similar but different platforms where only one is multithreaded. Annotations could be macro-wrapped, but extra arguments would be more troublesome. 
Just figured I'd comment on your post about how ```shrink_to_fit``` sucks. It's your solution that is significantly inferior in every possible way whereas the standard's approach is optimal. That may seem blunt but it's worth understanding why. First your ```shrink_to_fit_test``` is too pessimistic since it assumes that an implementation either always performs the shrink or never performs it when actually the situation is a lot more complex than that. The whole purpose of the standard leaving it up to the implementation is that for small vectors, forcing a ```shrink_to_fit``` potentially wastes a lot more memory than not doing anything at all. The reason is two fold, first because the default allocator doesn't allocate memory less than 16 bytes to begin with. In other words a vector of 10 bytes takes up just as much memory as a vector of 1 byte so forcing a ```shrink_to_fit``` in that situation doesn't save you anything. Furthermore with your implementation it actually ends up potentially causing you to waste 4 kilobytes of RAM because your solution forces a new memory allocation and given how virtual memory works, that allocation can result in having to fetch a new page, so you've now wasted 4 kilobytes of physical RAM for nothing. In situations where something like this seems fishy, it's best to review the literature on the issue to understand why the committee made the decision they did. Sometimes they do screw up, but that's not the initial conclusion you should jump to. Take a look at how GCC implements ```shrink_to_fit```, it delegates the operation to the allocator so the allocator can decide whether to carry it out since the allocator knows more about the memory system than ```vector``` does. It also implements the operation in a way that can avoid any reallocation period and it does its best to provide the strong exception guarantee. Basically there is no situation where you would want to use your implementation of ```shrink_to_fit``` over the one provided by the standard library.
Yeah no... I wouldn't hold up the medical device industry as any sort of good example of robust standards emerging from a competitive market process. They are in the position to charge $5,000 for an LCD display which you can get the same thing for in Costco for $249, but the medical device version has a "certificate of approval" which is really another way to say that the government has eliminated competition in the market. Similar issues of regulatory capture exist in the automotive industry but I'll let this rest since it is veering decidedly off-topic.
Try it and see? Also your formatting is messed up - ``` does not do code in normal reddit.
Maaaaaaaaartin, that’s you!!! 🙌🏻😎
- If `llvm::SmallSet` performs hash-based lookups then it's a `std::unordered_set` replacement, not a `std::set` replacement. - Given `llvm::DenseMap` is an `std::unordered_map` replacement, it's strange to compare its iterator behavior to that of `std::map` instead - "SSO heap storage" is an odd phrase – what does the heap have to do with it?
&gt; and get's as good as it gets with just that That depends on whether your iterating or doing lookups more often; if the latter, eytzinger layout can offer even better cache locality.
No worries, glad I could help!
I have an almost-bachelors-degree from a noname Central European technical college. I work as a senior software engineer with C++ and I have a feature in ISO C++11 with my name on it. Do I wish I had a masters or a PhD? Certainly. Am I succesful considering my schooling? Definitely. So it is all up to you. 
If you don't pass it in by const&amp;, it just all gets optimized out, including the integer. So it's pretty simple: if you never use it by reference, then the constexpr int is the same as the enum or #define. If you use it a single time by reference, then the constexpr int is strictly better because the binary is actually smaller (4-5 instrs &gt; 1 int) and there are fewer instructions in the runtime path. There is no situation whatsoever that I can see where #define/enum buys you anything at all over a static constexpr int, in optimized builds. If you have such a situation, please show an example.
Great stuff
Not only that but alignment requirements must be taken into consideration. If I need my values to be 64-byte aligned say, inserting them into a map where they must be colocated with the key inflates my data by a factor of two.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/al90l2/career_possibilities/efbxz30/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Thank you.
Not really, the way to handle such things IMHO is to make the code be seemingly completely identical in both cases. The only difference is that depending on the platform, you either use a real mutex or a null mutex where all operations are no-ops. That way clients too can write totally portable code. The extra argument maybe isn't totally ideal from the perspective of the single threaded client but it's not a huge negative either, and it's even a form of future proofing, it gives you significant freedom as an implementer to move the API from single to multi threaded without even breaking client code.
Forgot this option: Just use the parameters without checking the type, e.g. store them in a string array if that is what's needed. If it compiles then fine, either you were passed strings or objects convertible to strings. If it does not compile, well, then the user will know the parameters had the wrong type.
&gt; requires proxy types Considering future possibility of convenient AoS -&gt; SoA conversion using reflection, it would be nice to have more experience of proxy references in standard...
at which point?
&gt; `std::to_chars is locale-independent, non-allocating, and non-throwing` Oh, hey! That's actually really useful.
That sounds pretty cool actually!
If the object is initialized, it gets destroyed. Why is it initialized if it needn't be?
I have a flat_map implementation that works like the proposed one (but it's not the proposed one due to the different allocator): https://github.com/foonathan/array/blob/master/include/foonathan/array/flat_map.hpp I've had very good experiences, especially with the separate iteration.
Interesting, as it is rare to see benchmarks for containers populated with few elements (we often see the opposite). 
Wow! I'm not even offended but it sounds like you are and I had no intention of offending you! Have fun architecting.
Yeah, I've got FoundationIO set up to use my String2Number (really more generic name for String2Integer and String2Decimal) functions for both a replacement for atoi/atol/atoll etc etc etc and as the number formatters for my string formatting functions. When it comes to precision, I'm planning on begrudgingly supporting it because there are some cases where people do need strings to be a certain length, tho it's heavily discouraged. For supporting the Shortest formatting option, using a heuristic makes sense, tho I still need to figure out a way to get the number of digits the string needs to contain, and honestly I don't want to rely on Ryu for that part, I want to figure out for myself. the Mantissa is all negative powers of 2, the only hard part is figuring out at what point it stops making a meaningful difference.
I completely agree, that separate storage is the more convenient and natural model, so I hope it makes it into the standard. At the same time I do believe that there should be at least one production quality open source implementation of any type proposed for the standard out there before it gets into the standard. That way, people can gather experience with it and evaluate the interface design decisions.
Given that we already have 3 keywords for generic template parameters (typename, class and auto), I feel it would be better to reuse one of those - class being the primary candidate due to the status quo against using it over typename. With the exception of some complicated SFINAE, I expect such a change to be backwards compatible (though newly written user code would be able to misuse the new functionality it offers to break pre-existing library code). Would definitely be worth a test implementation to validate that though.
&gt; First, everyone's performance tradeoffs are different. Case in point: I also maintain a data structure I call a 'flat map', but it is not a sorted vector. Rather, it is a red-black tree that uses a vector for its node storage. Links between nodes are stored as offsets into the vector, rather than full pointers. I need this thing because I store a lot of data in it, and while inserting data at the back is the most common case, there are plenty of cases where data gets inserted near (but not at) the front (basically, think graphs: usually data gets added at the end, but sometimes we extend the history length to see more older data, which then gets inserted near the front). Just sorting it again and again and again would be incredibly bad for performance. It still does pretty well with caches: it needs 16 bytes for the control structure, and whatever you need for the key and value. My most common usecase has an 8-byte key and 8-byte value so it all aligns nicely. Tree walks are sped up by way of a right-sibling link. And as long as data comes in in order of timestamp (almost always) it will also sit in the vector in the expected order. 
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
Maybe, because Qt is not a GUI toolkit.
&gt; a quality implementation should implement that as a couple of pointer swaps, and ta-da, we have the “awesomer” buffer-ownership-transfer primitive described above! I've legitimately pined for a standard way to do this. Just let me rip the guts out of a vector and do with it as I please. I'll write all the move constructors or whatever other boilerplate is necessary. Just let me do it.
![](http://assoc.tumblr.com/post/459536318/weakness-of-stl-over-plain-c-types) ![](http://assoc.tumblr.com/post/411601680/performance-of-stl-vector-vs-plain-c-arrays)
&gt;You... fix your code base ? It is not a fix, because using stable API is not a bug. &gt; I don't believe it is an impossible job to do on any code base of any size. Of course it is possible, but it doesn't bring any new value to any product, so why should one spend resources on this? &gt;Using the last available dependency is good practice For indie/home projects - yes, but not for anything serious. 
&gt; The STL can do everything QT’s collections can do but 100x faster and more efficient. That is not even close to be correct. For example STL collections don't have copy-on-write, so it is very easy to find a case where Qt would be much more efficient. Also in most case STL is a part of compiler and regardless of being a part of the standard the performance can be very different, it is not the case with Qt collections.
&gt; One thing would be simple HTML5/JS widgets - which is mentioned by someone else and touched on in your post as serving QML - but using native C++ code as well. At least maybe something simple like QImage (QPixmap), QTextArea, Q___Layout as a first set of tools. I'm thinking in a way to do it. So far the way to go for this is Wt. Some experiments I'm planning are easy integration with Angular.js and React.js to make easy to communicate with the web server using Qt WebChannel. Of course one of the targets of this is to provide an easy way to distribute (and connect) your applications using WebAssambly versions of Qt. &gt; From what I've read, the WebGL and WebAssembly implementations seem to be specifically lacking multithreading support, and having a thin client interface w/multithreading support would be awesome - even if that processing is done server-side. WebGL streaming does not need multithreading in the browser. And it's executed as a normal C++ application in the host, so proper threading is supported. About WebAssembly, we are working in threading support. &gt; I haven't looked into if/how you spawn processes for responses yet, but some kind of sandboxing would be great for scalability. For examples, on Windows, I would prefer to launch a web server as an ISAPI dll that spawns a new process for every connection (or at least one for every few connections). Otherwise, a single client can cause the whole server to crash and stop responding to everyone. It's planned. &gt; Another big one would be session handling, as well as easy connections to Qt Network Authorization. Planned :)
Fastlog?
It doesn't really follow too many "modern" C++ practices, or does it? Just had a quick glance at the source. Also worth mentioning it's Windows-only and Direct X. I think to create a "real" game engine, one would abstract the rendering layer. You seem to couple Windows/DirectX quite tightly to your game?
I would recommend [https://github.com/gabime/spdlog](https://github.com/gabime/spdlog) . We use it production and I think it's a very fast and mature framework (which works with the fmt library :-) ).
One thing I'm noticing over and over again (also in VS2017) is that the ui spuriously freezes when running clang-format (manually or automatically). A UI that is freezing up non-deterministically is really not something I'd expect to see in 2019.
What's wrong with glog?
It depends on your needs, project size, used plugins etc. I´m using VS2017 Community on an older Haswell i5 (3,2GHz), with 16GB RAM and a 256GB SSD. Developing a game engine using C++, this serves me well. Plugins are PVS-Studio, clang tidy, and sometimes ReSharper C++. My project ist not really large at the moment, but a slower computer would handle it well, too. Since this is my private allround computer, I´m using it for gaming, graphics, music, browsing etc. More is always better, but not really neccessary.
Majority, sure. But can you guarantee that? Can you look at the code and say with certainty ”this will not increase the size significantly compared to C-like implementation”? I recently cleaned up code to use a non-allocating vector-like container (span). It should have compiled almost identically compared to using a pair of &lt;float *ptr, int N&gt;. Instead it brought in 20 kB of extra code of which I have no idea about and no way to find out where it’s even called from. I was able to use an undocumented hack to remove that, but there is no certainty of what other changes that hack caused. This is an example of what I mean by reasoning about the generated code. The average case is fine but that doesn’t help you much when any random thing can suddenly cause bloat that’s next to impossible to figure out the root cause for.
It's pretty brutal build time wise if you use it throughout a large project.
if you were really programming*large scale C++* , you would not ask.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/albw17/whats_a_good_wayresource_to_learn_c_as_someone/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I've used log4cpp before and it was fine. But as with testing frameworks i'd probably recommend just using a simple selfmade solution for most things. A log file to write in will do just fine for almost everything. 
&gt; If `llvm::SmallSet` performs hash-based lookups It doesn't. From the article: `It uses a simple linear search`. &gt; Given `llvm::DenseMap` is an `std::unordered_map` replacement, it's strange to compare its iterator behavior to that of std::map instead In many cases both ordered and unordered map is an option. It is the differences between different maps that dictate when one is appropriate and another is not. It makes sense to compare `llvm::DenseMap` to `std::map`. But for clarity, it might have been clearer to say `like std::unordered_map but unlike std::map ...` in the article.
Dell XPS15 9560, 32GB ram, 1TB SSD and an I7 CPU. I work for a large tech company in the realm of 3D CAD using C, C++ and C# I use a Dell XPS15 9560 for personal projects too with the same ram/cpu but smaller ssd. Both are more than up to the task. How it suits you depends on the work load.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/alc4y3/what_are_some_tips_for_someone_starting_to_learn_c/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Is it easy to pull out DenseMap for use with GCC or ICC? It’s annoying that I’m developing with Clang on a Mac but have to run my code on an HPC system witout Clang.
Yeah, you've right, mate! At the moment Windows and DirectX seems to be coupled quite tightly, but because it's the beginning! If I were focused at the beginning into abstracting the rendering layer, the rendering of the first triangle would have required at least 10 parts! The project is almost done under the hood, simply I trying to show its various parts from the simplest one to the hardest one! The abstracting of some layers, like the rendering layer, will be topic of the next tutorials! Anyway, thank you for your interest!
I can be a shitty laptop, but using a cloud based build and a cloud based IDE
Devenv.exe of VS 2017 is 32bit. I have an older i5, 16GB RAM and 256GB SSD. Biggest solution at my work is some 160 projects. It's OK for me. SSD is important though.
Oh wow! thank you for the comment. I had no idea :( and now I feel silly about my first blog post :( Hope you don't mind if I copy your comment to my blog and rework the implementation to perform shrink\_to\_fit on the right type?
1. It's not "a pure performance optimization". The current associative containers cannot be implemented to give the same guarantees. It also don't have the same interface or guarantees than any container currently in the standard. 2. It's a better default than std::map. In my opinion in the majority of situations you either need a flat map or a hash map. If you are not sure, start with a flat map. In rare cases you need std::map but it's definitely super rare. 3. It's about standardizing a container that a massive lot codebase reimplements (because std::map is rarely appropriate - see the proposal for a beginning of a list). That's a very common construct (not necessarily the adapter version but the idea of a flat map) and that's exactly why it should be standardized. The "killer" use case is all cases (IMO) are so common it hurts: - you don't have a massive amount of massive objects to store as values and their location in the container is not important (you won't use pointers to the values); - you need to go through the keys or values in addition to be able to search; So I strongly disagree: It is a better default associative container and should be used in doubt, in the same way you use vector in doubt. It's not about adding an optimization.
Oh, absolutely. In [the GitHub repo used to generate the benchmarks](https://github.com/s3cur3/llvm-data-structure-benchmarks), you can find the `lib` directory, which has all the LLVM library code. It's easy to drop in and use with any compiler. (We're using it at work in MSVC and GCC without a problem.)
std::clog
I'm using a Ryzen 2700X with a 512GB SSD and 32GiB RAM. I'm developing software for processing huge images, and testing on different platforms in various VMs, so having all those cores and lots of RAM works well for this. But for developing and compiling code in VS2017, it's huge overkill. Anything from the last decade will likely suffice for that purpose. You gain in build parallelisation, compilation speed improvements and greater memory headroom, but these are niceties, not essentials. I'd go for a nice 4K monitor over more RAM, for example. Makes looking at code all day much easier on the eyes.
There's something hilarious about an insane person with recreational outrage wandering in to a professional programmer forum and crying over nonsense. Maybe you should come back in a few years when you are an adult.
I agree that the LLVM containers are closer to their `std::unordered_` counterparts... but the sad fact is, most people (myself included) tend to choose a container not really thinking about whether they need the ordering guarantees or not. In [the CppCon talk advocating these containers](https://www.youtube.com/watch?v=vElZc6zSIXM), Chandler Carruth laments that `map` is so many fewer characters than `unordered_map`, and will always "win" for that reason. :/ True story: at work, we have this stuck in our global PCH: `template &lt;typename Data&gt; using vec = llvm::SmallSet&lt;Data, 8&gt;; // it's shorter, so you know it must be faster!`
Great, thanks for the link!
Can someone please explain the issue at interface boundaries for llvm::Smallset?
Do the tutorials have a similar ratio of exclamation marks? I just want to go in prepared.
Putting on my WG21 and Boost hat, split storage ought to easy to achieve with inline storage. Just store a `span`, or even a pointer, or even a shared ptr, to the external storage. You can even do this with the existing `map` or `unordered_map` by using a custom allocator to have the node allocation happen from elsewhere to `malloc`, albeit that most STLs have unpredictable allocator usage, thus making this hassle (it's far easier to maintain to just use Boost, then there's a single implementation everywhere). So for me at least, I'd need to see some *really* compelling evidence why built-in split storage is necessary. I'd *far* prefer to see layers of abstraction, so a very low level in-line indexing algorithm, then *on top of that* an entirely separate split-storage abstraction, then on top of that something like a LLFIO based memory map based store which is TLB and page fault aware. I'd also like those three layers of abstraction to come with zero overhead over a single monolithic design. To my current best knowledge and experience, that always ends up leading to Boost.Intrusive, which is unsurprising. A slightly modernised Boost.Intrusive I very much think SG14 should be proposing for standardisation, rather than these once-off bespoke container designs. As an aside, LLFIO comes with a basic algorithm and containers library none of which is proposed for standardisation, but is there merely to demonstrate that filesystem storage stuff **can** be plugged into generic STL algorithms and containers **if** those generic STL algorithms and containers are designed right (which in the current STL, we are close but not quite there, we specifically need the current allocator design to go away). I'm very, very keen for SG14 to be pushing hard for that sort of fundamental improvement. I am far softer on once-off bespoke containers. I think they solve the wrong problem i.e. what we really need is a reboot of the fundamental design assumptions behind the STL better suited for contemporary hardware and contemporary C++ use patterns i.e. a STL v2. Go big, or go home. That's my euro cent on this topic, anyway. 
You need to learn how to write tech blog before posing it on Reddit.
&gt; It's not "a pure performance optimization". The current associative containers cannot be implemented to give the same guarantees If it's not a pure performance optimization, can you explain what the exact differences are? &gt; It's a better default than std::map. In my opinion in the majority of situations you either need a flat map or a hash map. **If you are not sure, start with a flat map**. I'm sorry but this is simply terrible advice. A good default is something that offers good performance in all cases. A flat map may perform 10% or 20% better when you have a very small number of entries, but for even a moderate number of entries (&gt; 100) its performance on insertion will rapidly degrade and it will quickly start performing 200%, 300%, etc worse. Unless you definitively know with certainty, that for *all time*, the container will remain small, using a flat_map is the opposite of future proofing: it's future time-bombing. The sorted characteristic itself is incredibly rare. I've actually never needed a sorted associative map structure; Chandler (or maybe it was Titus) say something similar that they didn't find a single use case in reviewing god knows how many lines of google code. I think he said something to the effect that there were always better intermediate opportunities to perform the sort rather than on each element as it came in. &gt; It's about standardizing a container that a massive lot codebase reimplements Yes, but they may well reimplement them slightly differently based on their needs. If it makes more sense for them to separate key and value storage because they care a lot about performance and its slightly faster, they're going to do it. They aren't going to care that it doesn't match the generic interface easily. &gt; So I strongly disagree: It is a better default associative container and should be used in doubt, in the same way you use vector in doubt. Again, this is just bad advice, bolstered by an unbelievably bad analogy. Vector is the right storage+iteration container in like 95% of cases. There just isn't anything in the associative map world that is as dominant. But. Hash tables provide good performance for all container sizes, ranging from slightly sub-optimal to best as size increases. It's the best choice for any container that is now or might be in the future, size &gt; 100 (approximately), and for any container whose size is simply unknown (e.g. in any library or generic code). Hash tables simply are the best "default" associative map choice.
From the blog post &gt;since the preallocated size is part of the template type &gt; &gt;// BAD: Clients cannot pass e.g. SmallVector&lt;Foo, 4&gt;. &gt;void BAD_hard_coded_small_size(SmallVector&lt;Foo, 8&gt; &amp;out); &gt;// GOOD: Clients can pass any SmallVector&lt;Foo, N&gt;. &gt;void GOOD_allows_any_small_size(SmallVectorImpl &amp;out);
I'm sorry, mate. You've right, I'll try to reduce this ratio! Regarding the tutorials, don't worry, they don't have a so high ratio of exclamation marks. :)
Is this like the old phone prank where kids would call a random number and ask for the same person a bunch of times, then a final person would call and pretend to be that person, asking if there has been any calls for him? 
In general inlining decisions are made in the larger context of the caller as well, not just the callee (c_str). So I can't say we *always* inline c_str(). I whipped up a very simple example and saw that c_str() is inlined even under /Ob1 /O2 /EHsc (and /Ob3 too, of course). We're putting this in the preview exactly to see its effects on real-world code. I suppose you have a case where we really should be inlining c_str and we aren't? I've pointed my colleague Terry to this thread, the implementer of /Ob3, in case if there's more context to add.
Hey barfyus! Ob3 actually is the same as regular inlining, but with the internal thresholds needed for inlining decreased. It's actually not a new heuristic (although we work on those constantly too) - it just makes the existing one more aggressive. I highly suspect your ICE is an out of memory error. Increased inlining increases memory usage, and for a large project, that might tip you over the limit. Have you tried using the 64 bit hosted toolset?
Try adding /d2inlinelogfull:TopLevelFunction (or /d2:-inlinelogfull:TopLevelFunction to the linker when compiling for LTCG) to see a high level reason of why a function was or wasn't inlined. The inliner in MSVC is organized as a set of legality checks followed by a set of heuristic checks. If any of the legality checks fails, the inline doesn't go forward. If any of the heuristic checks succeed, the inline proceeds. Ob3 increases the threshhold for one of the heuristics. Another thing to consider: each inline function has a budget. What could be happening (and I'm just guessing) is that Ob3 might have increased the likelihood of inlining stuff before the c_str call, causing the function to run out of budget before it evaluated c_str. In a similar fashion, perhaps the same inline decisions were made, but those earlier inlines are now larger because of Ob3. Stuff gets weird when you start approaching the budget limits, and a change like Ob3 (changing a heuristic's internal threshhold) isn't going to make you inline more or increase the overall budget (that's a separate, hard limit to prevent size explosion) - it'll just shift around what takes up that budget.
I would aim for at least quad core, 16GB Ram, and 512 GB SSD. 
At work (in gamedev) we talk about this sort of thing as being the "noise floor" in performance... it's the kind of thing that's so pervasive that no *individual* code site will ever show up as a hot spot in a profiler, but if you were to replace *all* small uses of `std::vector` in the hot loop, you might actually get a substantial performance boost.
&gt; It is not a fix, because using stable API is not a bug. using deprecated one is. If an api changed, it should be for good reasons, otherwise you may complain to the maintainers. &gt; so why should one spend resources on this? Again, the old api may be broken, bugged, unsafe whatever. Also it also allows you to move the new version which may bring more value to your code-base, that's your call. &gt; Using the last available dependency is good practice Define "serious". Too many "serious" have been breached because nobody updated them and their dependency soon enough. Modifying code that work should not be a huge hassle.
Yip, recently did that for a [flat implicit kdtree](https://github.com/degski/KD-Tree).
Current Qt API is not broken, unsafe or deprecated, so for the beginning try to justify changing Qt API from Qt collections to STL.
I also pretty much never use map/sorted vec-- just checked the codebase I work in. Associative map was used twice, both in neither case was it needed, a hash would have done the job. Actually had more valid uses of multi\_map than map..
devenv will spawn as many processes for compiling and linking as you can handle, so it can use much more memory than the above statement would let you believe. And having a lot of free ram for disk cache is also useful, as Visual Studio writes stunning amounts of intermediary data.
TIL that some people who choose to write C++ actually try to reduce the amount of typing required.
&gt; If it's not a pure performance optimization, can you explain what the exact differences are? I think it's better to read the paper, it's written specifically to answer that. But a short summary is that it's like using a sorted vector, but with an associative-container interface. Comparing an array based container to a node-based container is pretty easy from this point (if you are at ease these data structures). It's not much more. &gt; I'm sorry but this is simply terrible advice. A good default is something that offers good performance in all cases. No, a good default offers good performance is common situations or if you don't know enough about the actual use. You cannot ever make anything have good performance in "all" cases. Otherwise we wouldn't be discussing. &gt; [...]: it's future time-bombing. Your analysis assume extremes conditions which are neither proven to be problematic, nor means that you should keep a flat_map if you go there, nor are as common as you seem to imply. If you use a flat_map and see such obvious issue apparent (because you test these extreme situations, right?) then you'll quickly know that this model cannot work for your case. If it's clear to you from the beginnig that flat_map is not the right tool ,why use it? Inserting small objects in the middle of a reserved flat_map can be fast. If it's not, change it. I repeat, based on my experience in several industries (games, embedded, robotics): it's a better **default** in general than `std::map`and it's not only a performance thing. I disagree that a hash map is match the most common use cases for an associative container. It does match the most important though (there is a nuance here). flat_map is not the best "in all cases" because as said before, it's not what I mean (or "in doubt use std::vector" mean) by "default". It's also important to clarify, when teaching it, that it's not because it's the best default that it's the best for your case. Currently when I see code from people around me (mostly scientists, not C++ experts) that chose an associative container, they look at the standard library and obviously chose what is named after what they want to do: std::map, bot then I end up fixing their horrible performance by replacing it with a flat map or a hash map (the second makes sense surprisingly less often, mostly because of the need to iterate). Of course that's opinion based on (varied) experience, I'm sure if you stay in one specific domain you will think otherwise (as an example of when it's not a good default for your domain). And measure, measure, measuere (if you only focus on performance). &gt; The sorted characteristic itself is incredibly rare. That's just an implementation detail, not something most people are looking for, of course, and that's never been my point. Most people don't care how things are stored in containers as long as the requirements are there. (BTW I'm not even sure if the fact that the content is sorted is mandatory in flat_map, looks to me like it's a side effect of the requirements). &gt; Yes, but they may well reimplement them slightly differently based on their needs. My understanding is that they basically all look the same with one major difference: either it's a vector of pair, or 2 vectors. Other than that, they offer the same thing, which is why there is convergence in this paper. The other details (mostly related to allocators or not, which standard api to match etc) have been studied and discussed through the last 3-4 years if my memory is correct. The fundamentals didn't change a bit. I don't have an opinion on the 1vec or 2 vecs difference: to me it still solve the same issues; if as other commented there is an advantage with an adaptor of 2 vectors, then I'm in, personally, because it will end up almost the same interface and guarantees I care fore anyway. &gt;Again, this is just bad advice, bolstered by an unbelievably bad analogy. I'm (obviously) not convinced otherwise by your points. I don't think it's a bad analogy because it's not an analogy, it's literally the same advice than why we suggest vector by default (again we don't have the same definition of default but...). &gt; Vector is the right storage+iteration container in like 95% of cases. There just isn't anything in the associative map world that is as dominant. That's part of why flat_map is a good default. It's a(or 2) vector. You're basically advocating for it. &gt; But. Hash tables provide good performance for all container sizes, ranging from slightly sub-optimal to best as size increases. It's the best choice for any container that is now or might be in the future, size &gt; 100 (approximately), and for any container whose size is simply unknown (e.g. in any library or generic code). Hash tables simply are the best "default" associative map choice. We disagree but agree on some things. As I said before, most of the time you either need a flat_map (small sized data and not that big count) or a hash map (we agree it's powerful). A hash map makes more sense in some specific situations, like when you are not iterating. The difference of situation is quite clear so I agree that when you know you will just use it as an index, probably the hash map is better. But that's not the most common case, in my experience and when reading other's code online. (also a massive lot of situation requires both searching and going sequentially in the container, it's surprisingly common, in particular with small set of data ;) ). What I am saying is that if you don't know clearly the situation yet (because you have too many unknowns) then using a flat_map first is your best bet except if it's "obvious" that you need a hash map, of course. Also your count to chose or not the hash map means nothing: the truth is in some cases a flat_map will beat a hash map but that depends a lot of what's inside, what types, what size of elements etc and the cache line size. So I wouldn't go by your rule, I would measure instead. But again, it's not only a performance issue. You still can't go sequentially in a hash map without hurting a lot. &gt; Hash tables simply are the best "default" associative map choice. As said before I disagree. :) I suspect it's mostly because we don't work on the same kind of projects though. 
IIRC there are some subtle differences between `inline` and non-`inline` templated functions.
&gt;WebGL streaming does not need multithreading in the browser. And it's executed as a normal C++ application in the host, so proper threading is supported. I looked it back up, and it looks like one drawback that they mention is scalability issues. I would have to dive into the documentation again but there was a separate major requirement we had that was missing from WebGL streaming with no (currently) planned support. &gt;About WebAssembly, we are working in threading support. This would be great, however, it relies on multiple third parties also putting in effort for multithreading support. I also wonder whether this will be the future vs. how much web browsers are wary of a web application being given access to 100% of a client computer's resources. &gt; Sandboxing, sessions and Qt Network Auth - planned Excellent! Honestly, these last few features above should be enough for me to start playing around with it. Instead of just database queries it would allow me to have an API that does a lot more heavy lifting server-side.
Yeah. The fact that flat_map may be standardized before a circular buffer is pretty unbelievable.
FWIW, here's an improved 'Ith type' getter, to replace std::tuple\_element\_t [https://godbolt.org/z/KOXX\_p](https://godbolt.org/z/KOXX_p) #include &lt;type_traits&gt; #include &lt;utility&gt; template &lt;typename T&gt; struct type_identity { using type = T; }; template &lt;typename&gt; struct IthType; template &lt;std::size_t... Is&gt; struct IthType&lt;std::index_sequence&lt;Is...&gt;&gt; { template &lt;typename T&gt; static T get(decltype((Is,(void*)nullptr))...,T*,...); }; template &lt;std::size_t I, typename... Ts&gt; using Ith_t = typename decltype(IthType&lt;std::make_index_sequence&lt;I&gt;&gt; ::get(std::add_pointer_t&lt;type_identity&lt;Ts&gt;&gt;{nullptr}...))::type; static_assert(std::is_same_v&lt;Ith_t&lt;0,bool,int[5]&gt;, bool&gt;); static_assert(std::is_same_v&lt;Ith_t&lt;1,bool,int[5]&gt;, int[5]&gt;);
What nonsense is this?
I'd be interested to see how boost's flat_map and flat_set compare to these, too.
I can take a to-do to add those. :)
It would have been more interesting if they would have compared their implementations to other third party equivalent types. For instance small_vector to boost and folly equivalents, and dense map to google's dense map. 
The major battle I'm fighting within my organization is getting people to use *any* non-`std` container. I imagine the Boost, Folly, EASTL, etc. variants will all perform "close enough" to each other that picking any of them over the `std` versions is a win. (But I could be wrong!)
as I said, if you move out of an object, then the dtor may be unnecessary. but it would still be called in c++. presumably you initialized said object to use it before you move out of it. maybe the compiler will figure it out and skip the dtor, but your dtor may have a volatile load that has to happen.. so it wouldn't be elided
I like "Deaths by a thousand cuts" ;) Each individual use is so tiny it barely registers as a blip on the radar, but altogether you're looking at a solid X% of the overall run-time, spread all over the place.
Its true, many std containers perform poorly under certain conditions. I know for heavy insert cases, switching to google:dense_map instead of std::unordered_map has yielded 5x performance increases. Too often I see these discarded under the explanation of library X may not be well supported in the future, so therefore we should make our own.
*C++ Beginner's Tutorial 42: Hello World*
&gt; I think it's better to read the paper, it's written specifically to answer that. But a short summary is that it's like using a sorted vector, but with an associative-container interface. Uhm, yes, I understand that. So, it has the same interface as `map`? When a second data structure has the same functionality as the first, but has a different implementation, what exactly is this other than a pure performance optimization? Do you want to offer something specific that it does differently beyond providing different performance trade-offs? &gt; No, a good default offers good performance is common situations or if you don't know enough about the actual use. Yes in common situation**s**. Notice the s. Very small maps are one common use case. Larger ones are another. There's nothing fantastically rare about maps with 1000 entries. We should pick a data structure that offers good performance over the whole range. That is, a hash table. Saying to optimize for the most common use case is a fantastically primitive way of approaching defaults. If 60% of maps are size &lt; 100, and 40% are size &gt; 100, this does not mean we should pick a data structure that offers 20% better performance for the former, at the cost of 500% worse performance on the latter. The basic problem with flat_map as a "good default" is that you have far less to gain than you have to lose. Gaining a small amount often is not a good trade-off for losing tremendously less often, in the vast majority of applications. You then go on to say that the sorting is only an implementation detail, and that's not the point. If you don't care about the sorting aspect, why on earth are we comparing to `map`? 2003 called, it wants it default back? If we don't care about the sorted characteristic there's no reason to even mention `std::map` at all. &gt; My understanding is that they basically all look the same with one major difference: either it's a vector of pair, or 2 vectors. Sure, because they are trying to be simple and generic, and aren't necessarily optimized to a specific use case. Imagine this example: keys are small, but values are large, and insertion is very important. How can we beat these naive implementations? Also, how about ordering: is it better to store the array in sorted order, or in BST order? You seem to have glossed over this trade-off and assumed it's just sorted earlier but BST order (i.e. the median first) leads to significantly more cache friendly behavior on access. You say there is "convergence" in this paper but this paper discusses only the API, not the implementation at all (which is also part of the point of the blog post). &gt; That's part of why flat_map is a good default. It's a(or 2) vector. You're basically advocating for it. I'm not advocating for it in any way. I think you need to read what I write more carefully. &gt; Also your count to chose or not the hash map means nothing: the truth is in some cases a flat_map will beat a hash map but that depends a lot of what's inside, what types, what size of elements etc and the cache line size. So I wouldn't go by your rule, I would measure instead. But again, it's not only a performance issue. You still can't go sequentially in a hash map without hurting a lot. How is it not a performance issue? Hurts in terms of what? Does the hash map smack the user if you go sequentially? Like when I said the data structure was a pure performance optimization and you said it wasn't, you seem to exclude clearly performance related matters from performance for reasons I don't understand. I think the fundamental point you're missing here is the difference in performance variations. Hash tables offer good, fixed performance. That immediately means there's an upper bound on how much you can gain. And in practice, flat maps only beat them out by moderate amounts, even for small data. On the other hand, there's no limit to how bad a flat map can be. When you're looking for a good default, you want something that minimizes your exposure to "performance risk"; pathologically bad performance in somewhat unexpected use cases. If you then want to optimize code paths for specific input data, that would be the time to start introducing flat maps. Of course, then they are in competition with many other specialized techniques (e.g. perfect hashing). If you think we should simply optimize for the most common case, the most common case is probably *very* small hash tables, with &lt; 10 elements, right? So why not just a flat map with linear instead of binary search? Far better insertion, and better access even for primitive types up to 10-15 elements. Obviously, linear search is not good general purpose because it offers mostly only minor savings in ideal circumstances and access (usually the most prioritized operation) drops off so badly in performance. The same arguments that lead to flat_map as a default over hash table, lead to linear search as a default over binary search. The arguments are wrong either way. &gt; I suspect it's mostly because we don't work on the same kind of projects though. I significantly doubt that's the reason.
Do you have any experiencing with insertion performance in an Eytzinger layout. I am thinking that adding a sorted list of elements into an Eytzinger layout would regularly cause "catastrophic" rebalancing. Say, you start from: 1 2 3 4 5 6 7 + 8 =&gt; 4 2 6 1 3 5 7 . . . . . . . 8 As you keep inserting, the "right" sub-tree will keep expanding and at some point you will want to re-balance and shift the 6 in place of the 4 which seems to me like it requires moving over half the elements. Am I missing something?
&gt; There is no situation whatsoever that I can see where #define/enum buys you anything at all over a static constexpr int, in optimized builds. If you have hundreds of named values, with large chunks of them being consecutive, creating constexpr variables for them might be a big churn. And just to be clear. I'm not advocating the use of `enum` over `constexpr` (especially indiscriminately), I'm just saying that it's the (more) direct replacement for `#define FOO 42`.
&gt;Also, the people who are most interested in speedily converting vectors to strings or vice versa are probably the same people who are least interested in pulling in all of &lt;ranges&gt; (with its 8-second compile time) just to get the facility. Um, wow. Would anybody be ok with an 8 second compile for a header? There's pch, but still.
Well I don't have any issues with it being standardized, I just doubt I'll use it much/at all.. But I also pretty much never uses other contains such as list/deque/set/map.. Even std::vector has mostly being replaced by a custom version that allows for uninitialized resizing and other such micro optimizations.
You can update your blog of course and I notice you did update it. An even faster implementation that uses less memory than your update would be the following: template&lt;typename T&gt; void shrink_to_fit(std::vector&lt;T&gt;&amp; v) { v.shrink_to_fit(); }
Could you post an imgur album or something with the benchmarks? The little slideshow widget is awful on mobile.
Here's the complete set of images generated by the script: https://imgur.com/a/QljLj7R
Thanks!
Great! Thanks! That's AWESOME!!! :-)
No problem, mate!
I can't wrap my mind around it. What exactly is achieved by wrapping that into template if you call a member function nevertheless?
There is nothing acheived, my comment is intended to be mostly rhetorical to really emphasize the point that you shouldn't implement your own ```shrink_to_fit``` and simply use the one provided by the standard library, always.
I'm not! Qt API is fine, for the most part. What I'm saying is that you should not exclude yourself from a framework and it's update just because it has changed. It's bad practice and philosophy.
&gt; So, it has the same interface as map? No, clearly not. std::list and std::vector do not have the same interface, do they?
&gt;everything I find is pretty outdated Considering that logging is most probably a solved problem when it comes to needs a student could have, we'll need to know what that means in your head in order to give you better guidance. Otherwise, you'll get generic answers.
Could you explain a bit?
Are you referring to the blank post above, or the benchmarks?
printf is my favorite library
A and B don't have the same interface, therefore C and D don't have the same interface. How does this help? I also obviously don't expect every single method to be exactly identical. But most discrepancies I would expect to have to do with managing the capacity/size, which works a bit differently. They still will both have iteration, at/get/operator[], upper_bound/lower_bound, insert, emplace, etc, which are largely identical or differ in very minor ways. list lacks operator[] which is one of the most vital pieces of vector's interface. So your analogy fails pretty hard. If you have some interesting, specific, meaningful, bit of API difference (beyond ownership/size/capacity related) between flat_map and map, let me know.
What does this do that a database doesn't?
They have different interfaces for the same basic reasons. Or, like the other guy said, read the paper. The most obvious ones are the ones being heavily discussed in this thread: const key_container_type&amp; keys() const const mapped_container_type&amp; values() const And here's a couple obvious ones you'd see right away if you read the paper: &gt; The flat_map class supports random access iterators. &gt; except for the requirements related to node handles (21.2.4) and iterator invalidation (21.6.1). &gt; A flat_map does not meet the additional requirements of an allocator-aware container 
Security is often a good idea.
It doesn't have anything to do with databases at all, it's about client/server interfaces. The 'database' bit was just to provide it with something semi-realistic to do, which would be extracting data from a server. That data may be in a database, it may not. It may be getting pulled down from some external server and cached in the local server. Even if it is in a database, it may not be one directly accessible by clients. &amp;#x200B;
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/alhzlf/noob_question_about_header_files/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
The first one is just exposing some internal detail of the storage layout, and obviously it's far from set in stone. It also doesn't make any real difference to the usage of the data structure, i.e. it's not an interesting difference, API wise (though it might affect performance considerably). The random access iterators is pretty interesting, thanks for sharing. On the one hand, it gives the flat_map a somewhat unique usage where you can quickly get the Nth largest value in the collection for changing N; not possible with std::map. On the other hand, I think it locks in the internal data layout to proper sorted order and excludes a BST ordering with the median first (slower for access). Which comes back to my original point; the perils of writing a one-size-fits-all version of a data structure, most of whose purpose for existing is to act as a moderate performance optimization. Still, at least this means there's one unique thing it can do.
10 years ago I used to work with omniORB. Today I think the [Real-time CORBA with TAO(TM) (The ACE ORB)](http://www.dre.vanderbilt.edu/~schmidt/TAO.html) and the [TAOX11 - The CORBA implementation for C++11](https://taox11.remedy.nl/) is a good CORBA. But why not just [gRPC](https://grpc.io/) nstead which is an RPC extension on top of the protobuf?
The missing question: how does using more branch logic affect your icache when you have other code too?
In fact, devenv.exe is just the IDE which invokes the compiler. The compiler, cl.exe (which loads the c1xx.dll frontend and c2.dll backend) has all combinations of 32-bit and 64-bit hosting (the programmer-user's machine), and 32-bit and 64-bit targeting (for the end-user's machine).
My code base is the opposite of that. Unlike the standard thing, it's not a conglomeration of third party code. It's a single, highly integrated, custom code base that doesn't use third party code (with two very wrapped exceptions), or the standard C++ libraries or STL, and completely encapsulates the underlying OS so that the bulk of the code is written to my own portable 'virtual kernel'. &amp;#x200B;
Agreed! I'm not entirely sure how you'd measure this, though... Any ideas for what a good toy problem would be to benchmark? The golden standard of benchmarks, of course, is to *actually* replace all the containers in your production code with the new stuff, and measure perf on the real input. I'm not sure what a good, representative test would look like for something in between that and what I have now.
So this is basically redoing proto buffers?
No error bars because they're thinner than the lines?
&gt;&lt;CIDIDL:Interface&gt; &lt;CIDIDL:ClassIntf CIDIDL:Name="VideoDemo" CIDIDL:InterfaceId="E1CCC9CBCFA1FDAF-29A8F7CB04763265"&gt; ... &lt;!-- Query a record by name --&gt; &lt;CIDIDL:Method CIDIDL:Name="bQueryByName"&gt; &lt;CIDIDL:RetType&gt; &lt;CIDIDL:TBoolean/&gt; &lt;/CIDIDL:RetType&gt; &lt;CIDIDL:Param CIDIDL:Name="strToFind" CIDIDL:Dir="In"&gt; &lt;CIDIDL:TString/&gt; &lt;/CIDIDL:Param&gt; &lt;CIDIDL:Param CIDIDL:Name="recFound" CIDIDL:Dir="Out"&gt; &lt;CIDIDL:Object CIDIDL:Type="TDemoRecord"/&gt; &lt;/CIDIDL:Param&gt; .. &lt;/CIDIDL:Method&gt; How this XML based interface description intended to edit? I think direct XML editing is not simple. Just look at the alternatives, you want to describe something like this: TBoolean bQueryByName(TString strToFind, TVector&lt;TDemoRecord&gt; recFound) Which is in gRPC roughly this: service VideoDemo { rpc bQueryByName(QueryRequest) returns (QueryReply) {} message QueryRequest { string strToFind = 1; } message TDemoRecord { string col1 = 1; int32 col2 = 2; } message QueryReply { bool returnValue = 1; repeated TDemoRecord = 2; } }
I can imagine something like: * create an array of function pointers, each of which points to a function composed of random code. * use a biased RNG to choose which function pointer to use, so that some of them are more common than others, at least within batches. * alternate between calling the to-be-tested code and the random-function-pointer code. So the larger the icache imprint of the to-be-tested code, the slower the random-function-pointer code will run? I dunno.
I've just graphed the data coming straight from [Google Benchmark](https://github.com/google/benchmark). They have internal logic to continue running iterations of the tests until the results stabilize. I'm not aware of a good way to get a standard deviation off that data... if you have one, though, by all means let me know and I'll put it on my to-do list for updating the graphs.
I don't find XML that complicated to edit. But, anyhoo, using a standard format means it would be available easily for other uses without having to bring along its own parser. And of course the XML parser does a lot of checking for you, so that's code not duplicated over again. I get your point. But I think the advantages in this case are worth the use of XML. &amp;#x200B;
Well, that's missing a lot of the point of it. You are talking about the flattening/resurrecting bits, which is just a side issue here. But, no, it's not the same. The data remains binary, there's no need for the high overhead and error prone process of conversion to and from a secondary format. But it still deals with endian issues as well, so it has the benefits of remaining binary and handling endian conversion. This is not meant to talk with external systems. It's for programs written in this system to talk to each other. In those cases where external exchange is required, then something like XML or whatnot would need to be used. But that's a whole other issue. &amp;#x200B;
Haha, that's what I am always telling people. There's no way you can outsmart a group of dedicated people who were reasoning about the solution of a problem you just decided to solve from first glance. Inventing wheels should be always prohibited except for educational purposes :D
&gt;But why not just [gRPC](https://grpc.io/) instead which is an RPC extension on top of the protobuf? Because you like [message oriented middleware](https://en.wikipedia.org/wiki/Message-oriented_middleware). I'm [working on that](https://github.com/Ebenezer-group/onwards) for years now. If you are using C++ on multiple tiers of your application, I think the code is simpler than what the OP posted.
Maybe writing it as a blog post series?
A nice talk by Bjarne. We have heard most of it before, but this time Bjarne seems to be really comfortable and gives a very good and humouristic talk and a good Q&amp;A session afterwards that is also quite instructive.
Then this is serialized data structures that are always in their flat form?
&gt; The standard didn't make this part of the STL non-binding so that the people who work incredibly hard to implement it could be lazy How *dare* you impugn the laziness of STL implementers? We are at the top of the laziness priority queue. Look at what we did with garbage collection! I demand satisfaction. The duel will be scheduled when I get around to it.
I haven't used that library before but maybe this section helps? https://github.com/google/benchmark/blob/master/README.md#reporting-the-mean-median-and-standard-deviation-by-repeated-benchmarks
I'm not sure what your definition of 'flat' is. But it's not just the data written out as a blob or anything, no. Each class that supports binary streaming implements a 'streamable' mixin interface. They use that to write themselves out and read themselves back in. They do versioning, don't write things that can be recreated or initialized upon reading back in, sanity checks of various types, compression, whatever is appropriate for the class. They of course recursively invoke the streamable mixing on any of their streamable object members, and write out to the stream any of their fundamental members. Ultimately it all comes down to fundamental types and strings in the end. The binary streams handle endianness of fundamental members and enums, and strings are done as UTF-8 which is endian neutral. &amp;#x200B;
My first idea would be to use libevent or libev directly, with the minor inconvenience of constantly translating between c and c++. If we are to bring a dependency, let's at least bring an industry standard with a proven track record of performance and security?
Make it like Java and I might come back.
They're still there on old Reddit; there's some markdown around them that may be hiding them on new Reddit.
What do you say to rile up a group of C++ devs?
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/aljzln/console_application_help/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Why are you even here?
There is already a language like Java. Guess what is its name? Java.
This isn't async, message oriented stuff. This is for strictly client/server work. It's for the many, many work-a-day types of remote interfaces that large client/server software products need. It's not asynchronous, at least not from the perspective of the clients making the calls. It's purely synchronous to them, as though it was a local call. &amp;#x200B;
Maybe I misunderstood, but you are saying that ArrayMap is basically an array that you do binary search on to find stuff? That is, it's basically the same as the proposed flat_map, also on the reddit front page. And it only beats unordered_map for 8 elements or less? That thread is full of people claiming that flat_map should be the "default" choice for associative maps because it performs better for most collections.
&gt; I'm not sure what your definition of 'flat' is. Existing in one solid span of memory. &gt; They use that to write themselves out and read themselves back in Is that different from serialization? I remember seeing this before and I just can't figure out what it does that can't be done with already established and better designed libraries. It really seems like you are reinventing the wheel but not making it round. 
Any serialization scheme creates a blob of memory because the point of it is to transmit it in some way. It doesn't matter how the individual bits of data are represented, it still creates a blob of memory to move from here to there and then load back up into memory. The way the individual bits of data are represented only matters at each end to the classes that represent that data, or if you are exporting that data to third parties. For an ORB, you aren't exporting the data to third parties, so the format 'on the wire' is an internal detail. &amp;#x200B; As to already established libraries, my ORB code has been around for almost twenty years, well seventeen at this point, so I think it's pretty established by now. And it's very, very round. It's not some external system that has to be shoehorned onto my code. It's integral to the system. It's incredibly clean and tight. &amp;#x200B;
Nope, that's exactly my understanding as well. I have no usecases for this that involve mutation of any fashion; the data either comes out of a database already ordered properly such that I can do a bulk sequential insert into the underlying storage, or it's sorted after initial insertion and never modified after. It's not something I use often by any means.
By my understanding, the implementation should be similar, but 2 things worth noting: 1) My ArrayMap is really a toy implementation designed for simplicity and ease of understanding... it’s not been finely tuned for perf. 2) In [the HN discussion of this](https://news.ycombinator.com/item?id=19035160), one commenter noted that the powers-of-2 sizes for the containers may be causing pathological performance for the binary search. I haven’t had a chance to dig into this deeper, though. 
&gt; It doesn't. From the article: "It uses a simple linear search". Also from the article: "*when you’re below the preallocated size, and only moves to fancy, higher-overhead, guaranteed-efficient hash-based lookups at larger sizes.*" If it requires that your type be hashable and equality-comparable, but not orderable then it is clearly a replacement for `std::unordered_set`. Don't misquote to support your position.
Do you guys feel C++ as become bloated now or trying to do too much ? Most business use C# or Java today and less and less C++... most of the web work is all done in Javascript with some framework, it's like C++ as lost it's place today.. I know its still extremely powerful but maybe it starting to become a niche language like in 3d games, financial industry etc What you guys think ?
Wow, that should really encourage people to spend lots of money creating software to sell.
&gt; That is, it's basically the same as the proposed flat_map, also on the reddit front page. This stores key and value together, the proposed `flat_map` does not.
Good to know. Maybe if it's not too difficult, you might consider swapping in boost flat\_map instead. It's probably the first off-the-shelf flat map I would turn to.
&gt; the powers-of-2 sizes for the containers may be causing pathological performance for the binary search Long, overly-verbose, but IIRC good read on the topic: https://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/ Caveat: I haven't read it since it came out.
He spends a lot of time at the beginning of the video talking about the strengths and weaknesses of C++ and where it is useful today. It’s a pretty good overview. 
yes but what you think about it
Most modern software is very large or just provided through cloud services so reverse engineering isn't often a real threat. Also reverse engineering itself might not hurt the developer because the reverse engineer would have to do a better job marketing their version of it which is hard to develop further also.
I empirically looked at that with the [kdtree](https://github.com/degski/KD-Tree) quoted above with the result you expected already, a small change can/does make a catastrophic difference [even worse for a kdtree] and just starting from scratch [rebuilding the tree] is just as fast [and avoids potential implementation bugs] as one can do away with anything complicated/sophisticated.
Errm... Are these exotic OSes used for actual delopment? Do they already have full C++17 conformant compilers? Or are they just targets for cross-compilation? If the latter, then it looks like tying all the marathon runners just to allow some old fart who never heard of it to compete.
Than you didn't get the point of discussion: changes in API should have some serious reason.
Java and C# use a virtual machine. You can't do bare metal or embedded development with a tiny foot print with those. C++ has the ability of providing a reasonably good default implementation of things in the standard library and let the developer the possibility to replace those with his or her own.
Usually the goal in reverse engineering is not to understand and reimplement the entire functionality of the application. Quite often you have specific questions about the application's functionality that you want to answer. Anyway, RE has plenty of benign applications that can benefit from techniques such as this one, including malware analysis and vulnerability discovery.
There is a question in QA session about "virtual function table modified at run-time in malware". Is it possible to do that in C++?
Very weak arguments from Bjarne. The ability to prevent deletion/increment is way more important than providing pointers for C, which will not go away anyway.
&gt; observer_ptr doesn't add any functionality over T*, so I don't think it is a compelling argument. It takes away functionality from T*, which is just, if not more, useful.
The feature that forbids deletion is invaluable: it allows for writing interfaces where the intent is clear. By inspecting code that accepts observer_ptr, one can be sure the object that is pointed to is not deleted. This is invaluable.
Qt is the best (it is not QT, QT means QuickTime). 
I've used a sorted flatmap style container thing several times within the past year. Small **n** is common for a lot of gameplay code, and if you have to support a wide variety of gameplay systems (more and more common because of service games) you want to make sure those systems are keeping it tight. Not trying to minimize the footprint of these things is like being the guy who won't take off your backpack on the subway during morning rush hour. That said, I agree that it might not be worth standardizing if as you say, there will be endless variations in implementation to debate. Cue obligatory "we need better packing and build tools so it can be trivial to include a libcontainers4gamez library".
 auto vtable = *(uintptr_t**)this; vtable[4] = union_cast&lt;uintptr_t&gt;(&amp;SomeClass::SomeMethod); Done. `union_cast` is some vile template trickery: template&lt;typename T0, typename T1&gt; inline T0 union_cast(T1 input) { union { T1 input; T0 output; } u = { input }; return u.output; }; Needless to say a well-formed application should never-ever do this. If you are doing this kind of thing you must absolutely know what you are doing and what implications of this are.
I hope that your Mutex interface is using the nodiscard attributes =) Like this : [[nodiscard]] MutexGuard&lt;U&gt; Mutex::lock();
This might be a concern in your code base, but in none of the code bases I've worked in - neither pre, nor post c++11. Seriously: How many c++ functions do you have that take a pointer as an argument in the first place? How many of those call delete? Do we really want to make changes all over the codebase just to catch that handful of cases? Isn't it more economic to ensure that all owning pointers are encapsulated inside a proper raii class? Also, I keep saying this over and over again: If you don't know what a c++ function does, you have no business calling it and what few c++ functions I've seen that would call delete on a pointer argument (more often that happend in a destructor that called delete on a member) were all called delete/erase or had a similarly obvious name. Finally, how would you actually envision the transition? Have a dual API for years? Have conversions/casts between raw and observer_ptr all over the codebase?
&gt; If you are doing this kind of thing you must absolutely know what you are doing and what implications of this are. As with any UB. ;-]
Unfortunately porting existing codebases to modules will require certain effort anyway. Especially in cases of bizzare codebase layouts. What really troubles me is that committee here puts legacy codebase needs far before new project needs. Could you please give direction to specific arguments of choosing that strange multifile module design over module==file approach? My personal experience with golang (which uses similar multifile package approach) is that it often becomes hard to determine where specific functionality came from. In case of C++, module can be assembled using files from multiple arbitrary locations.
Since you asked for personal opinions, here is mine: Yes, I feel C++ has crossed a point of "too complex", "too many pitfalls" &amp; "too many bad defaults". I'm also not impressed with a lot of the attempts to improve the situation. C++ just has a lot of baggage due to irreversable design decisions. To be more concrete: It saddens me that to this day compilers won't warn about rule-of-three violations even though compiler-generated copy constructors and assignment operators are *deprecated* in certain cases since 2011. It saddens me to see how inconvenient the use of tuples and variants are compared to built-in support in other languages. It saddens me to see the need for so many 45+ min C++ conference talks that spend all their time to showcase pitfalls and surprizing C++ behaviour. There's an entire talk about initialization. There's an entire talk about possibly surprizing lifetimes of objects you should be aware of to avoid dangling references. As a C++ programmer one might be tempted to justify that complexity since C++ is pretty powerful. I did. But even Stroustrup is known to have [said](http://www.stroustrup.com/bs_faq.html#really-say-that) &gt; Within C++, there is a much smaller and cleaner language struggling to get out This is more true than ever. And yet, I'm still using C++ because there's no other language that completely replaces what C++ allows me to do. Rust comes pretty close, though. And to be completely honest, me learning Rust influenced me in how I see C++ today. I used to be enthusiastic about C++.
To locate certain include, you only need a set of include paths and include's subpath. With modules, as they are designed now, you may have // file: project/firstlib/main.cpp #ifdef PYTHONIC module pythonic; #else // PYTHONIC module cppish; #endif // // ... // // file: project/secondlib/main.cpp #ifdef PYTHONIC module cppish; #else // PYTHONIC module pythonic; #endif // // ... // // file: project/application/main.cpp // modpath: project/firstlib // modpath: project/secondlib module app; import pythonic; // // ... // where you need to fully parse and compile **all** accessible modules **before** you know which one to choose for your import.
&gt; Java and C# use a virtual machine. You can't do bare metal or embedded development with a tiny foot print with those. not that I like it but... the chip in your credit card almost certainly runs Java
You can just do C-style cast, can't you?
Regardless, `SmallSet` doesn't appear to do that after having grown up either. The article appears to be wrong about that, unless there is a version mismatch. Can u/tylerayoung explain? From llvm source: class SmallSet { //... SmallVector&lt;T, N&gt; Vector; std::set&lt;T, C&gt; Set; 
Well-spotted. :-] Certainly not the implementation I was assuming, or would have chosen.
Nope. But they solve: 1. Include &lt;fancyheader.h&gt; and get half the Boost into your scope (with 5min-per-TU compilation) just because of private: boost::mpl::whatever&lt;...&gt; storage; 2. Get your Qt project stop compiling when you start embedding Python just because Qt defines "signals" macro, and Python has "signals" member in one of its public structs 3. Include contents dependent on where you actually include them. Includes reparsed independnetly for each TU. 4. ...
~~Why do you even need union_cast? Can't you static_cast that?~~ Apparently not. But you can do it with static_cast if you declare the vtable to be an array of member pointers: https://gcc.godbolt.org/z/XkJabk
How are these numbers computed? The raw table lists numbers of iterations, so does e.g. `2 ns 2 ns 274761938` mean that a total time was measured and then divided by 274761938 to end up with 2 ns per iteration? If so I don't understand why the timings are integer values, comparing 2.4 ns and 2.6 ns is something else than comparing 2 and 3 ns. I must say that I find the graphs to be quite unclear, maybe a log scale would help actually showing what's going on at low x values, or having separate graphs for the low x-values. A summary table with some numbers backing up the TL;DR would be even better. &gt; SmallVector is a big win for a win for emplace/push_back at sizes up to the preallocated “small size,” and not a loss beyond that I don't see any output for `BM_vector_emplace_back` in the graphs nor the list of timings? &gt; SmallVector is a big win for random reads at sizes up to the preallocated “small size” until you get so many elements preallocated that you start passing beyond cache lines Which graph should I be looking at to see this?
My work machine is an 8-core Xeon W-2145 with 64GB of RAM and about 3TB of NVMe SSDs. All hooked up to a distributed build system, which means that building our full 6MLoC code base takes a couple of minutes at most, a large part of which is the linking step because it needs to be done locally. I try not to think about how expensive it all is. This is for game development at a big studio, I have a hard time imagining your project being actually "large scale" if you're asking this question. This machine is complete overkill for anything I would ever attempt doing on my own or with a small team.
&gt; The first is that returning a string by value in readline is pretty inefficient as it makes a heap allocation each time it returns it. Not very zero cost abstraction IMHO. Sure it is, because you don't pay for that if you don't want it! Easily 90% of the time I read a file, I'm doing it once, the file is small, and I just want to read it into memory without fuss. &gt; That is somewhat painful so a simpler alternative is to provide an interface that takes a lambda, and calls it with a string for each line, again reusing the same buffer. 
Oh wow. This is exactly why i used `union_cast&lt;&gt;` :)
Do you have some sources for this? (Not trying to call you out, just genuinely interested)
/u/TheThiefMaster showed that we indeed can. Once you see his reply you will know why i did what i did here heh.
https://stackoverflow.com/questions/47731005/practical-use-of-java-cards https://en.wikipedia.org/wiki/Java_Card and for some actual history look at the end of this : http://www.ub.utwente.nl/webdocs/ctit/1/0000014d.pdf
Number of cores is first. Some good ideas at [Pcpart Picker] (https://pcpartpicker.com/builds/) Get the biggest amount of cores and not SSD but M2 or NVME. SSD ATA is 600 MB/s and NVME is +4,000 MB/s. If you will be Coding for Linux then it is a good idea to get at least 32 GB memory So 16 GB can be shared between one or two virtualbox. I run One Ubuntu and one CentOS side by side inside Windows 10 all connected through NAT service. Also pay particular attention to what your target machines will be. If you will be coding to run on newest Intel and you got a Rizen you might be off a few features (unlikely but it happened to me in the past). If you will be doing 3D graphics (Unity, Unreal, SFML, Open SceneGraph) get at least a 1050 discrete card if you are getting a laptop.
You can't _legally_ do it with `static_cast`, but `union_cast` isn't legal either, so.. why again?
I missed this! This is intended as a file utility to handle the 90% most common cases, no more. If you can see a way to allow memory mapping without making it more complex for simple users, lay it on me! :-)
I've postponed my studying of Rust, partially because I fear that I would never look at C++ in the same way... But I think I should have finally learn it, even just for sake of being better programmer in general.
Together with spdlog, this one is top of many lists https://github.com/3Hren/blackhole
Thanks for this list! There's some good stuff there, but I decided that these ideas were out of scope of my project, which is just opening, reading and writing files and then closing them. It's a shame that that list doesn't seem to take pull requests (there are over 100 in the queue...)
If you ended up using this, then there's a new version that's backwards compatible but has several more convenience features...
&gt; It saddens me that to this day compilers won't warn about rule-of-three violations even though compiler-generated copy constructors and assignment operators are deprecated in certain cases since 2011. actually if I'm not mistaken the latest GCC and maybe Clang actually introduced warnings about those and started fixing their own standards libraries. 
I've been working remotely for US companies on and off for over a decade, and know lots of other devs who do the same, especially here in Ireland where working directly for US firms and getting paid in US dollars is quite common in our tech industry. Nobody employs directly, that would be daft. Indeed the EU has a specially built legal vehicle for exactly this situation, the single person incorporation, that lets EU citizens trade with extra and intra EU businesses as a business, whilst paying all EU specific taxation in full EU-side. It's also trivially easy to set up a US ACH to EU SEPA bridge, so the US firm pays you by US ACH exactly like any employee. Most US startups aren't aware of this at the beginning, and think to try to employ you directly. Their legal counsel usually then slaps them hard. You then gently guide them through the hoops that need to be jumped, the specific wording clauses and contract structure needed to keep both the US and EU sides of things happy. It can take a few weeks of back and forth, but once heads are wrapped around what is needed, contracts get signed and it's off to the races. I can't speak of non-EU jurisdictions, but any country with a comprehensive double taxation treaty handles withholding taxes just fine. Over here in Ireland, our top marginal tax rate on income is 65%, which you reach at surprisingly low income levels. Me personally, I'd *just love* to pay a mere 30% of my income in taxes!
I always use a 64-bit toolset. When I tried different configurations compiler complained about missing `-bigobj` option for a failed project (it did not require it with previous VS versions for this particular project), but even with this option `-Ob3` still fails. I also think that it is memory-related issue.
Within the EU, one EU country cannot employ directly someone with no tax presence in another EU country. So if the US company has a base only in Ireland, that Irish base can't employ a French citizen living in France, for example. EU single person incorporation is by far the easiest way out. The French citizen self incorporates, and the relationship becomes a business-to-business one with full French taxes paid French-side. This is not to say that plenty of people don't cheat the system. There are loads of French people who "live" in the UK but actually live in France, specifically so they pay UK taxes and not French taxes which are much higher. But that's tax fraud, and that's on them. That said, I've seen almost zero enforcement of people evading tax like that within the EU. Luxembourg and Switzerland in particular is rife with it. I do know that here in Ireland, our tax authorities view money coming into the country by any means as an unalloyed good thing, and enforce rules particularly lax on Irish people working for EU firms and ignoring the residency rules. I don't doubt it's the same on the Continent, as EU countries view their neighbours as competitors on tax, always trying to undermine them.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/alp64a/help_me_with_my_c_school_program/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Bytecode-based programming languages, such as Java, C#, etc. decompile almost perfectly back to source code (modulo variable names). This did not stop Java to become one of (if not the) most popular languages for commercial software development. If really needed, there are automatic code obfuscators (see javascript world, for example). In any case, hiding your source code is never enough to protect your market share. You have to always innovate, run forward ahead of competition.
I updated the blog. Check it out, you'll like what I did with the shrink\_to\_fit post ;)
why not just use memcpy, it's well formed and is what bit_cast(probably in c++20) will do. 
Learning is not bad. Learn as much as you can. But when u deliver an app I am afraid that in many cases you will still have to stick to C++. I do not find C++ bad at all the same way I do not find Java bad. You hear all these complaints all the time, but the truth is that they are both useful in their areas and the complexity they got is the consequence of the language evolution + backwards compatibility. As for learning Rust, I would recommend D better (yes, D). Its metaprogramming is incredibily powerful and useful. Sometimes I am doing C++ and I think: in D this is a no-brainer. Rust will get a bit in the way at first, especially the borrow checker. I aee Rust more of a theory thing (still useful is you master it, like Haskell) and D like the “pargmatic get things done”.
I still don't understand, why c++ doesn't allow me to forward declare the whole public interface of a class without exposing the private members natively and instead forces me to use the pimple idiom, which not only makes the code more complicated, but depending on the exact implementation also produces runtime overhead due to the additional pointer indirection, dynamic memory allocation and/or code that is more difficult to optimize for the compiler.
no wonder why it's slow to pay with these /joking
because the compiler needs to know the size of the class for stack &amp; other allocation, and that is affected by the memory layout of its members - including the private ones.
Thanks for answer :)
This problem is not as straightforward as it seems at first. &amp;#x200B; struct A { virtual void M(); private: ... }; struct B { virtual void N(); }; struct C : A, B { }; void f(C &amp;c) { // Compilers need to know the relative location of the B sub-object to generate code for this call. // The location depends on the size of A. c.N(); } } &amp;#x200B; (sorry, I can't figure out how to format this as code on Reddit) You can't do without knowing the data members in all cases. This might be part of the reason why you can't declare just the methods in C++.
I think C++ has lost its place for application development and has become the current owner of a few niches like you state. The days of full stack GUIs fully implemented in C++ are gone. I have moved into Java/.NET around 2006, just getting hold of C++ when those languages need a little extra help from the lower layers. Recently Kenny Kerr was complaining that even with all C++/WinRT improvements, most devs make use of .NET for the UI, including the Windows UI team. And Microsoft is the only desktop OS vendor that still gives C++ a place on its UI frameworks. So I see C++ becoming the modern Assembly so to speak, where the upper layers are done in managed language (even if AOT compiled), with C++ being left for drivers, GPGPU, 3D APIs, real time audio and similar tasks. From my point of view, even if I would still do full time C++, keeping track of all ISO C++ changes, specially among revisions, and how each compiler handles then implementation specific cases has blown up the complexity budget.
Tickets booked. See you on conference. :)
Disney released an open source expression evaluation library that sounds like what you want.
Java and C# do have a managed runtime, the virtual machine approach is just a possible implementation. However there are implemetentations that fully compile to native code ahead of time, just like C++. Just like there are C++ interpreters as well Leaving here a couple of examples for Java and .NET: https://www.ptc.com/en/products/developer-tools/perc https://www.excelsiorjet.com/ https://docs.microsoft.com/en-us/dotnet/framework/net-native/ https://docs.microsoft.com/en-us/xamarin/mac/internals/aot
Ignoring the fact that you could e.g. specify this manually via attributes (and then the compiler checks if thos match the actual requirements), the compiler only needs to know those things where an object of that type is created or deleted. As long as You only pass around existing objects via reference or pointer that isn't a problem. We have done those kinds of things in C for ages and there is no fundamental difference between a c or c++ function `foo(Bar*)` and a c++ member function `Bar::foo()`. Yet in the first version I only need to forward declare bar but in the second I have to provide the complete definition of `Bar`.
https://github.com/wdas/seexpr/ if you're looking for it /u/Misrta 
I admit, I don't know all the intricacies of how multiple inheritance mixed with virtual member functions works, but I could live with the fact that forward declarations only work for cases without multiple inheritance (I believe that is the actual problem due to the necessary offset calculation right?). That would satisfy 99% of my use cases. 
You shouldn't have two `FILE*` because the only class here that has a `FILE*` is `Opener`. Hopefully this clarifies it: http://coliru.stacked-crooked.com/a/03cdebde5633ceea.
Ah, I get it now, but that upcast is pretty unexpected. :-D But it should work. Since I have you on the line (so to speak) - would all the parameter pack stuff in your sample work in C++11? 
No problem! Thanks for sharing your stuff and being receptive.
&gt; Cons: The lack of a modularized standard library may inhibit adoption of modules or lead to implementation-specific standard library modules. Isn't that what is normally supposed to happen in a standardization procedure ? Implementations should start by testing potential organizations, and once there is something that looks like a consensus, with real-world usage, it should be formalized in the standard.
&gt; It also doesn't make any real difference to the usage of the data structure I don't know about you but I use `.keys()` all the time in other languages. In C++ I end up using map_keys, which is less efficient than this would be, involves more headers, and the syntax is clunkier.
As bloated as it may be, C++ gives us two important things that most other mainstream languages don’t. Direct hardware mapping gives us the ability to write higher performance code than managed or interpreted languages with the tradeoff of being more dangerous. Yes, you can run really really fast, but you’re going to have to hold these scissors while you’re doing it. This is necessary in markets like games, simulation, high performance trading, an others. But performance can also mean reducing power usage which is hugely important in mobile devices where battery life is an issue and server farms where bad performance can cost many extra millions of dollars in electricity usage. More importantly though, C++ gives us zero overhead abstractions (don’t pay for what you don’t use). This is critical in hard real time systems where you need predictable worst case performance. Especially in safety critical real time systems like medical devices and vehicles (land, sea, air, and space). “}” beats garbage collection. 
You can use the fast pimple idom which at least shouldn't have an indirection overhead, but it's rather ugly.
A virtual machine in itself is not an impediment to writing small and efficient code (see Forth).
Kudos to the speaker or ACM for not saying “Using **AI** to recover...”
From what I understand, yes. Development happens directly on these operating systems.
Thanks! Though the question about their compilers' standart support remains.
I find [ExprTk](https://github.com/ArashPartow/exprtk) really useful.
It seems like the confusion comes from just looking at *headers vs modules*. You have to consider *linking*. In a sense, modules may make *compiling as hard as linking*. But I don't think modules make compiling any harder as linking. I'm all for the suggested module conventions. That's totally sensible, but I don't think the world ends if C++20 doesn't include them. Make a build system that tells me it will solve all my dependencies for me if I follow these conveniences - I'll happily comply. Or even better - make a build system that is fast if I follow these conventions, but slow if I don't. I do not expect from a build system to solve module dependencies. It's not like build systems are solving linking or even include path dependencies on their own - even though the granularity there may be a bit different.
Looks exactly what should be used there, thanks!
Why? In which case would it be an added benefit and how is this case particularly different from any other limitation, which the language specifications impose?
One of the points is to eliminate need for handwritten headers I guess.
You might not need them but if the module developer is exporting them because others might need them you get them anyway, right?
It doesn't, actually. It would be a nice warning, as it is likely unintentional, but note that it is safe without: - The destructor of `MutexGuard` releases the lock, so `mutex.lock();` will lock/unlock. - It is not possible to access the inner state without a `MutexGuard`, so after writing `mutex.lock();` you cannot accidentally have a data-race. 
&gt; just starting from scratch [rebuilding the tree] is just as fast [and avoids potential implementation bugs] as one can do away with anything complicated/sophisticated It certainly avoid bugs, however **most of the times** it is going to be slower to rebuild from scratch than just add a single value: O(N log N) vs O(log N) is a non-trivial difference.
I really recommend learning Rust to improve your C++. Specifically, I recommend practicing enough that ownership/borrow-checking rules become second-nature... so that you can apply them (although not as perfectly) to your C++ code. Before learning Rust, certain pieces of C++ code would tingle my spidey-sense, but after scrutinizing them I could not think about a scenario where they would actually break... and would hope they were correct. Now, I just apply Rust's ownership/borrow-checking rules, and not only do I spend less time determining whether a particular construct is safe or not, I am also more confident about my judgement.
The thing, though, is that C++ is not the only language purporting to give both direct mapping and zero-overhead abstractions: - Jai/Rust/Zig offer both. - D/Nim offer both, albeit with a GC. *Note: Unsure about Ada/ATS, but they would probably want a word, as well.* C++ is by far the most popular of the bunch, of course.
This would be a massive pita as in, in practice, everyone would roll out their incompatible std modules. It really is a worst-case scenario.
I would be nice also that STL implementers move their non-public symbols into a sub-namespace `details` or whatever to avoid polluting the `std` namespace :)
Well, the point is to perform a shape interpolation between given sections. The code produces a set of intermediate images that interpolate in between star and lightning (or whatever).
Not to forget C, it also has zero-overhead abstractions and is much simpler as a language than C++. Granted, so are its abstractions!
Indeed!
Why display it as a 3D stacking instead of an animation?
Thanks, I'm kind of busy right now with other projects. Feel free to do a pull request on github.
Indeed, and I’ve been meaning to spend some time learning Rust but, you know, life... I suppose another thing worth mentioning is backwards compatibility. I don’t know how the other languages handle it, but C++ goes through ridiculously painstaking effort to preserve backwards compatibility so that the code you write today will still work tomorrow without having to revert to an old compiler. That’s certainly a major point of consideration when choosing a language for a long term project. 
&gt;Yeah, maybe I'll rename actually
What is this “C” you speak of?.. :)
If we're talking modules, implementation details ideally just wouldn't be exported.
https://i.imgur.com/mg5VJPb.png that's cool
Well, with modules they could simply not export any symbols that should not be exported. Only the documented exported symbols (+ extensions) will be available.
There are many ways to implement it. One way is to use an embedded scripting language such as Lua, Python or scheme. Another approach is to implement a parser that returns an AST Abstract Syntax Tree. The AST can be represented using the composite design pattern and the AST evaluator can be implemented using the visitor design pattern. The easiest arithmetic expression evaluator to parse and implement is the reverse Polish Notation. I have built a simple one using the RPN approach that can be seen in this [gist](https://gist.github.com/caiorss/c7db87df674326793431a14006aa21f6), it can be test in the [online compiler](https://repl.it/repls/WiltedHoneydewClients) with REPL . The hardest par of implementing an expression evaluator is writing the parser due to ambiguity of operator precedence rules. &amp;#x200B; Example: The following expresssion, evaluates sqrt(30 \* 30 + 40 \* 40) or sqrt(30\^2 + 40\^2) &amp;#x200B; EXPR+&gt; 30 dup * 40 dup * + sqrt stack: 50 &amp;#x200B;
Yes, I think performance wise it should be near zero overhead (you still have the indirection in debug mode), but as you said: Ugly/complicated as hell. Also, if you want to combine pimple with inheritance/polymorphism things get even worse (actually next to impossible).
Isn't that already the case? If not, why not?
Awesome! By 'fairly soon' are you estimating days, weeks, or months? I'm actually in the middle of addressing compile times at work and I can potentially skip writing our own tools if this is released soon enough.
John is working on it. With contracts, modules and a million other things on his plate (including but not limited to a new marriage and some old injuries requiring serious attention) things can slip a bit. 
Shoutout to getting married. Shoutout to staying health.
Would you recommend reading Large Scale C++ Software Design (1997) today? It seems like it might not be worth it since it was before many revisions to C++ (especially C++11).
I suspect a segmentation fault. strlen returns an unsigned integer and an unsigned os always greater equals 0. In the next iteration after checking 'h' against 'z' the loop tries to access memory that most likely is not allocated. And then BOOM!
GCC spoils it prog.cc:6:18: warning: comparison of unsigned expression &gt;= 0 is always true [-Wtype-limits] while (--idx &gt;= 0) ~~~~~~^~~~ 
Your code has UB because `--idx &gt;= 0` is always true since `idx` is unsigned.
"Almost always auto" Kappa
BOOM! using auto is like fucking without a condom. You either know what your using very well or you might get something you really do not want.
We may have differing opinions on whether C has abstractions, to start with.
`idx` is a `size_t` variable, so if `c` isn't found in `str` the `while` loop never exits. This mean that at some point one will access `str[UINT_MAX]`. &gt; What does lastIndex() return? Not `-1`. &gt; Does it core dump? Who knows. It's UB and we know nothing about the environment this is being run in. Does it have an OS?
And _that_ is what's so great about turning up the warning levels.
That's a very reasonable suspicion, but it almost certainly does *not* seg fault. Consider this line: if (str[idx] == c) We're really just adding a number to a pointer like so: if (*(str + idx) == c) When idx flips over past zero, it becomes 0xffffffffffffffff. Run it in a debugger and you'll see that adding 0xffffffffffffffff to a pointer is the same as adding -1 (which sort of makes sense). So we're not reading at the end of memory; we're reading before the start of the string. If we find the character before we reach zero (which is pretty likely) we will not go boom!
It's thoughts on physical design are as apropos today as when they were first put forwards. Witness that the material has successfully been used in his various presentations over the last couple of decades plus.
Cevelop.
I do not know CLion well enough to comment, but I would personally prefer Visual Studio Community over Eclipse any day and night of any week. The debugging is like comparing night and day between those two. If you have linux interests I can also recommend a look at QtCreator (not tied to the Qt library as such) and KDevelop as other IDEs that might be better there.
I don't think people who use std in the first place would reject the standard modules when they get added.
Yes, the condition in the while loop is always true, but but the function almost certainly *does* return (and I think it's same on any OS). `idx` turns into `UINT_MAX` (or `ULONG_MAX`?) when it crosses zero, but when you add that to `str`, the net effect is like you subtracted one. So we end up searching the memory *before* the string and odds are we'll find the character at a random location before we reach zero and core dump. So lastIndex() returns a semi-random negative number!
I'm thankful that they do!
Believe it or not, the original code didn't use auto. I added it for the challenge!
&gt; That’s certainly a major point of consideration when choosing a language for a long term project. Backward compatibility, and the ability to compile code written 10, 20 or 30 years ago, is indeed important. I would argue, with the benefit of hindsight, that C++'s approach is flawed. C++ compilers allow specifying the language version at the level of a *whole program*, including not only the final binary, but also all its dependencies. The number of discrepancies between versions therefore have to be kept to a minimum, lest we observe an ecosystem split between users of C++03 (still... I know) and users of later versions, which in turn results in the clunky language that we have. A better approach is to, instead, allow versioning each library (or file) independently, guaranteeing ABI compatibility between the various versions when compiled with a given compiler. This would allow using a C++03 library *as-is* in a program otherwise compiled as C++17 (or even C++2a), without having to keep weird syntactic oddities such as [The Most Vexing Parse](https://en.wikipedia.org/wiki/Most_vexing_parse), and without fearing the introduction of new keywords such as `yield`. Actually... it would even allow compiling C as C, rather than stumbling on the differences between C and C++. Backward Compatibility does not have to mean Piling Up Cruft Forever. *Hindsight, hindsight...*
C++: `std::cout &lt;&lt; "hallo“sv.find('z') &lt;&lt; '\n';`
Surely, C++ deserves more love. The reason why C is more used is that there are more compilers available for low end microcontrollers, namely 8 bits and 16 bits and also C++ compilers are harder to write and C++ requires a more expensive runtime than the CRT C-runtime. However, 32 bits microncontrollers and embedded processors are becoming more powerful and cheaper, specially due to the ARM company that allows any company under a proper license embed ARM CPUs in their devices. 
Unfortunately, I'd have to pay several thousand USD to see how standards conforming they are. They could very well have ported clang though!
Personally I use mostly Clion. I also like Visual Studio IDE but not for c++, especially due to the Microsoft c++ compiler regularly crashing on "Internal compiler errors" without any debugging info except for the line number where the crash occurred.
This is why I'm looking into switching.
Yes, but they're then contained to that module rather than the global namespace.
I'll just mention that there's a [library](https://github.com/duneroadrunner/SaferCPlusPlus) (shameless plug) designed to help enforce a Rust-ish style strategy for achieving (high performance) memory and data race safety in C++. &amp;#x200B;
Not particularly hard. The OS uses memory mapping behind the scenes generally, anyways. Wrap file handling in thin structures that keeps the pointer from the memory map around. Not hard to keep the normal file operations around when doing that, since they're pretty easily abstracted to just an offset and size. The main difficulty is that there are two (well, four) implementations of memory mapping of files floating around - Win32, Linux's version, BSD's version, and OSX's version (might be BSD's?). The latter three are pretty similar, the former's not. They're all pretty simple, though. They all effectively do the same thing, though - create a memory mapped view of a file handle, and give you a pointer that represents it. Also nice because the user can specify access semantics like 'I will be reading this sequentially/random access/without a cache' and such.
Awesome! Good stuff Indeed. But I was under the impresion that nowadays most people use qtquick/qml instead of Qtwidgets directly, no? Thouhts?
&gt; If we find the character before we reach zero (which is pretty likely) we will not go boom What do you use, MS-DOS? My Linux segfaults at 0x400000 and Windows at 0x13F820000.
 bstaletic@Gallifrey ~ % gcc foo.c -O0 bstaletic@Gallifrey ~ % ./a.out -3697 bstaletic@Gallifrey ~ % gcc foo.c -O1 bstaletic@Gallifrey ~ % ./a.out -3744 bstaletic@Gallifrey ~ % gcc foo.c -O2 bstaletic@Gallifrey ~ % ./a.out -4 bstaletic@Gallifrey ~ % clang foo.c -O0 bstaletic@Gallifrey ~ % ./a.out -4 bstaletic@Gallifrey ~ % clang foo.c -O1 bstaletic@Gallifrey ~ % ./a.out -4 bstaletic@Gallifrey ~ % clang foo.c -O2 bstaletic@Gallifrey ~ % ./a.out // Infinite loop `-O2` and `-O3` behave the same.
\- include of STL string should be "#include &lt;string&gt;" (w/o ".h") \- operator \[\] does not do bounds checking of string container. \- strlen doesn't take UTF/multibyte characters into account \- `while --idx &gt;= 0` \--&gt; checking zero-length string - why? ...just off the top of my head. Code seems ripe for problems I just can't put my finger on ALL the issues. &amp;#x200B;
I do believe you! The mix between the good old string.h and the modern auto is actually a bit strange when i think about it. 
Personally I use CLion. I love jetbrain IDE's and they are pretty great. They have a lot of plugins and features as well : Alt+enter to fix errors (will include files, cast stuff for you, etc), ctrl+p to see the parameters of the function, alt+insert to auto generate code, etc... It's basically God's IDE LMAO 
The "always auto" camp that knows what they are doing would have written `auto idx = int{ strlen( str ) };`, but I don't see why one should do that instead of `int idx = strlen( str );`
It really doesn't matter. Personally i don't like eclipse it just feels slow (maybe that changed since i last used it). I'd probably recommend sticking to visual studio, it's popular and if you'll ever work with C++ on Windows you'll most likely have to use VS. So it is always nice to know your way around it. 
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews, or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/alt3x8/from_where_should_i_start_to_creating_actual/efgxk8t/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
They're including Standard C `&lt;string.h&gt;` for `strlen()` which is correct. There is no use of `std::string` in the example. (`ssize_t` is non-Standard, though.)
This sort of response is not really useful without an explanation. Why do you like it? What does it do better than other options?
And that's why we all hang out on this subreddit. TIL...
Qt widgets is fine for desktop applications. If you also want to port for mobile then qtquick is definitely the way to go.
Right now there's no pollution in the sense that our implementation details are `_Ugly` or `__ugly` identifiers which can't be mentioned by users. (We would definitely like to be able to stop using ugly identifiers, of course.)
Hello, always auto camp here. I just wouldn't have written strlen at all. What is this, 1992?
It's been long since I last used Qt (unfortunately), but yes, I'm more fond of Qtwidgets than Qml. Kudos!
I'm using mostly CLion for work. I don't have much experience with VS Community, but the few times I've tried it the experience has been pretty horrible - its IntelliSense is just _much_ slower without being any more accurate than what JetBrains is doing. VS apparently has an overall great debugging experience; I've not used it enough to form an opinion on that, but that to me only really makes it an extremely heavy weight graphical debugger. The main thing I'm doing is still reading and writing code, and if I don't get responsiveness in those areas I just don't care what else your IDE could hypothetically do for me. CLion is far from perfect of course, and I have some complaints about how their refactoring works (with some features being buggy or just straight up missing), but it's still better than what comes with VS.
Fair point, my mind was in a weird C mode with C++ notion of `auto` (reminder that C also has `auto`, but a different `auto`). Still `auto idx = str.size();` doesn't change anything and you would still need `auto idx = int{ str.size() };`.
`for(auto it = str.rbegin(); it !+ str.rend(); ++it)` The error is iterating with an integer index.
Personally I prefer Visual Studio with Resharper C++. When I tried CLion it had lot's of problems with false positives from the program's error highlighting which bothered me to no end. This may have gotten better by now though. Also it didn't give very good info when hovering over code, which is a feature I use a lot in Visual Studio. Eclipse seemed to have really poor CMake support last time I looked into it which was a deal breaker for me.
&gt;which can't be mentioned by users What does this mean exactly? I was under the impression that it was possible to invoke these "ugly" methods directly.
I would recommend Visual Studio Code.
VS is usually the path of least resistance. It also has the best debugger interface. CLion will require that you learn some CMake, which easy but only if you happen to use the right tutorials. CLion will probably eventually be very good, but when I tried them a year ago they were still catching up. I don't know anybody who uses Eclipse anymore. Not even for Java.
"can't be" in the sense that it isn't advised to.
I read somewhere few months ago that during VS2019 lifetime there will a major upgrade of the toolset along v142, breaking the compatibility with the previous ones but allowing some kind of revamp. Is it still true? And what can we expect from this? 
The downside of this binary compatibility is that we do not get bug fixes and performance enhancements that would result in an ABI breakage. For those of us who build our dependencies from source anyway, binary compatibility is not an advantage, and losing out on the bug fixes and performance enhancements is a disadvantage.
There's also `str.find_last_of(c);`. No naked looping, no chance of messing up anything, no naked iterators...
rewriting code is time-consuming a lot of it gets written it 3 years
Undefined Behavior / Implementation Defined
I have used CLion, Visual Studio, Eclipse, and Visual Studio Code in the past. I still prefer Visual Studio. Second and third place to me are Visual Studio Code, and CLion. My big issues with CLion is performance (it is written in Java) and it's code completion is not as good as Visual Studio due to JetBrains writing their own C++ parser which has trouble with advanced C++ constructs. &amp;#x200B; So my recommendation is go with Visual Studio Community Edition. 1) It has the best code completion. This is really helpful, when learning and is useful even if you are experienced 2) It has the best debugging experience for C++ &amp;#x200B; &amp;#x200B;
Ugly identifiers are reserved for use by the implementation (compiler and standard library), unless otherwise specified. For users, mentioning an ugly identifier triggers undefined behavior.
Overall, good job! I've been debating writing an article like this for a while as I've found the existing literature to be mostly useful for experienced game devs and less useful for neophytes or devs from other disciplines. I'm glad you beat me to it. :p That said, I take issue with the very first question's phrasing. :) &gt; What is an Entity Component System? Please do not say "an ECS." That grammar implies that ECS is a singular System of Entity-Components, which is of course untrue. ECS is an architecture of Entities, Components, and multiple Systems. The single "a"/"an" is incorrect. Refer to it as "the ECS architecture" or "the ECS paradigm." Otherwise your attempt to remove confusion is inadvertently contributing to it. :) I realize this is nit-picky, but I've found being precise with this kind of stuff really helps people understand quicker and more thoroughly. Anecdotally, I've been consistently rewarded by correcting junior developers who say things like "templated functions" instead of "function templates," as it seems that using the correct language helps them better grasp the concepts involved. Likewise, I see a lot of people throw around the term ECS when they're talking about what you call EC, because they have "a system" that manages Entities+Components. Being exceedingly clear that System is no more singular than Component when we're talking about ECS might help avoid that problem. &gt; What are examples of ECS implementations? You correctly list Unity ECS in this list, but I'd recommend clarifying that Unity ECS is _not_ the same Unity's default `GameObject`/`MonoBehaviour` object system. People have been confusing Unity's model with ECS since the term was first coined, and this is probably the most frequent "FAQ" answer I've personally had to give over the years, including to C++ committee members (it doesn't help that if you Google "Unity ECS" now you find a ton of videos and docs about their actual ECS add-on, which confuses people who are not otherwise intimately familiar with either Unity or the concepts of ECS). &gt; What is the difference between ECS and OOP? While not a comparison I actually like, it comes up enough to be worth referencing: some people find that describing ECS in terms of a relational database is enlightening. Components are tables, Entities are the collection of related rows between Component tables, and Systems are the logical procedures (aka SQL) operating on the Component tables. I also find that your description of OOP, while typical, is probably not the best way to describe OOP. It's more about behavioral composition than about object identity. That is, interfaces describe what an object _does_ or how it _behaves_, not what it _is_. Object identity and is-a relationships are more related to type theory and value-based programming than they are with OOP itself. &gt; Is ECS considered to be faster than OOP? I don't like the question or the answer, as phrased. OOP _solves a different problem_ than ECS does. ECS isn't any faster than OOP when you're trying to solve an OOP problem that ECS can't even address. :p A notable example is that in many ECS frameworks, Systems are very much OOP constructs, while still being part of a highly-efficient ECS architecture. I'd rephrase to make it clearer than your'e talking about OOP-style Game Objects rather than OOP as a paradigm. &gt; What is the difference between EC and ECS? You might want to show some example of what you consider an EC. Perhaps also example somewhere the goal of component-based design (aka dynamic aggregation) to explain what EC is. &gt; A system is a function (code) I'd phrase as "A system is logic and code (a function)" since not all architecture use just a plain function for Systems. &gt; Can I reuse the same component for multiple purposes? Your answer is specific to implementation. Not all ECS implementations strictly use nomimal typing; it's entirely possible for a Component type to be identified as a nominal type + tag, allowing for the same underlying type to exist under multiple unique Component type identifiers. Some ECS implementation do this by requiring a wrapper/child type, e.g. `struct ComponentA { vec2 pos; }; registerComponent&lt;ComponentA&gt;()`, but others can separate this data, e.g. `int ComponentA = createComponentId(); registerComponent&lt;vec2&gt;(ComponentA);`. There are of course tradeoffs (explicit types result in more template/generic instantiations and more compilation/linking overhead in some languages, but provide more concrete naming and a somewhat simpler usage pattern). &gt; Do I have to read/write component data inside systems? The answer here likewise presupposes all ECS implementations work a particular way, which isn't true. It's also misleading to say that it's "not recommended" to do something without qualification. The code one would write for editing/inspecting/debugging data stored via ECS patterns may use "not recommended" patterns for _run-time_, but there's no good reason to push people to avoid it for developer purposes. &gt; The number of entities is tightly coupled with how many objects your game or simulation has. While I know what you meant, this sentence is a big vague given the language you've tried to define elsewhere. Namely, clarify what "object" means in this context. &gt; In Reflecs ECS This appears a lot. It's fine to reference Reflecs a bunch, but this mostly reads as an advertisement. If your goal is to help unmuddy waters for everyone, you might want to use more varied examples for all the answers where you only reference Reflecs.
We're planning to release a binary-incompatible toolset in the future (final naming TBD; we've been calling it "v20" or "WCFB02" for the libs). The timeline and release mechanism are also TBD; it may appear as an optional toolset in the VS 2019 installer. However, the binary-compatible v142 toolset will **definitely** remain the default for VS 2019. The v20/WCFB02 toolset will fix lots of long-standing bugs and improve performance - basically all the stuff we are holding back due to ABI concerns right now.
[It says 2021 on Amazon. 😲 I wonder how accurate that is?](http://i.imgur.com/zXCzq1P.png)
&gt;due to the Microsoft c++ compiler regularly crashing on "Internal compiler errors" Are you using the more newer version of MSVC? If yes, is the issue still there? If yes, have you filed a bug report? I do not really work at Microsoft, but if you find a bug, notify them and they end up solving it, it helps us all. &amp;#x200B; Also you can choose to use the compiler of your choice. For example, many individuals get away with using LLVM/Clang. It's quite easy to configure MSVC to use Clang-CL or LLVM/Clang especially if you are using CMake based projects. &amp;#x200B; I think the LLVM Project also has an extension with MS. Do check that out sometime if you desire to return to the dark side? &amp;#x200B;
You could always use LLVM/Clang( another compiler) + CMake( which CLion also uses) instead of MSVC in VS
Yeah, we know it's a tradeoff. The olden days of being able to break ABI with every major version were very nice for library implementers, since we could improve object representation, change and remove separately compiled functions, and fix major bugs without restriction. Many customers also found it massively burdensome and many of them would lock themselves to ancient MSVC versions in response. Preserving bincompat is a headache for implementers (mostly because of what we can't fix; we've gotten pretty good at figuring out what we can fix, and how to do it in an ABI-preserving way - the amount of stuff we can fix is surprisingly large, even eliminating base classes is possible), but it's one we willingly accept because it makes most customers very, very happy. We're hoping that our upcoming binary-incompatible toolset will address the different but totally valid concerns of customers like you (and relieve many of our headaches about being unable to fix ancient mistakes), but there's a lot of work that remains to be done before we can release that - migrating the changes we accumulated from TFVC to Git, implementing even more fixes (e.g. my refactoring of iterator debugging hasn't touched deque and vector&lt;bool&gt; yet), implementing compiler changes, and figuring out a migration story that makes the bincompat break less disruptive. If anyone wants to help us, improving individual companies' and the community's build process will make migration easier. Basically, the more customers that are like you - building with the latest toolset, with the latest final Standard version, with maximum strictness, and able to fully rebuild all dependencies on a moment's notice - the easier it is to release source breaking and binary breaking changes.
Visual Studio and CLion are both solid options. VS has better debugging, CLion has better cmake integration and is cross-platform. 
I thought "reserved" meant you cannot create new user-defined ones, not that you cannot use pre-existing ones?
Crazy thought experiment here: What if we didn’t have to import anything and we just get std for free? If ‘std’ appears in your code, the import happens automatically. Similar to option 4.3 but without the need to actually import it. 
On Windows Visual Studio is the most easy solution, if you don't have much experience with setting up compilers etc. Install it and use it. No need to install a separate compiler, setting up paths etc.
I'd say most of the people would keep their "tested, tailored to their need" implementation. It has been quite popular back in the days when everyone had their own implementation of vector or string before STL got popular and later on mutex, atomics, condition_variables before c++11/c++14. Problem is that many of those legacy code still exists, even though they have the same or even worse implementation than the one that we got standardized. Manly because it's hard to rewrite a lot of code and do the proper refactor. Because of that I would avoid ideas like the one we are having here.
Clang too. $ clang++ -Weverything test.cpp test.cpp:7:18: warning: result of comparison of unsigned expression &gt;= 0 is always true [-Wtautological-unsigned-zero-compare] while (--idx &gt;= 0) ~~~~~ ^ ~ I don't know why this is not in `-Wall`.
Interesting! I'm using Red Hat Linux on a x86\_64 machine (Intel Xeon). I compiled with g++ 4.8.5 like so: g++ -g go.cpp -std=gnu++11 -o go On my machine, `str` is 0x400bd3 and the loop ends when `idx` is 18446744073709551058. Add those together and you get 0x4009a5, which isn't far from your segfault. So it looks like I've been getting "lucky". ...however, I just updated my program to loop through the alphabet and I *do* get a segfault when the character is 'k'. And the memory address is 0x3fffff. Did yours actually crash on 0x3fffff?
This is really great. But I think it's worth mentioning that it comes with a lot of caveats. (maybe they're necessary?). We're still only talking _binary compatible_ when two libraries are built with the exact same compiler flags. There are quite a few compiler flags that break ABI compatibility. Which ones? MS won't say and it's hard to find out. (There are a few obvious ones of course like debug/release, non-AVX/AVX2, and the like). Also it's only "forward" binary compatibility. Meaning a library built with 14.20 or 14.16 is **not** ABI compatible with a library/app built with 14.0 or 14.10. So basically if you want to **ship** a library that is ABI compatible across all "binary compatible VS versions (2015.3/2017/2019), then you have to build using the lowest toolchain, which is 14.0 (and you're limited to the C++ core/library features of that version). If you're willing to forego 2015.3 compatibility, you could build with 2017 14.10, a very sensible compromise - but that toolset is not available from the VS installer. You have to download &amp; install a whole separate (old) version of VS 2017 that contains exactly this toolset. The lowest toolset that you can get from the VS 2017 installer is 14.11 (from VS 15.4). Which isn't that bad I guess. Now the problem becomes much larger because in VS 2019 Preview, the lowest toolset that you can get (apart from the rather ancient 14.0) is 14.16. So you can't ship a library with VS 2019 that is ABI-compatible from 14.11 onwards. You have to choose 14.00 (ancient compiler/library!) or 14.16 (rather new, lots of people excluded from consuming your library!). Please make the 14.11 toolset available in VS 2019! And please let me know if I got anything wrong. I would be happy if I got some of that wrong actually because it could only mean things are better than they look like to me! :-) I mean these are already great improvements compared to where things have been with Visual Studio 5 or 10 years ago! But I think C++/VS still has a long way to go in terms of ABI compatibility.
Private members still impact the layout of the class. Simple example: struct A { int x; private: double y; public: int z; }; Type of `y` impacts what offset must compiler take for `A-&gt;z` even if you pass everything by pointers.
It is still slow (eclipse), there is a lot of issues with it as well especially when you need to have proper syntax checking and/or dark theme which is consistent. It always take a lot of time.
What c++20 features are available with the new compiler? I've looked for a list recently and didn't find anything.
&gt; Did yours actually crash on 0x3fffff? Yes. What I am trying to get at is that there's no reason to have access to every address from zero to your .rodata. Linux process maps start at 0x400000.
Wow. How the heck does clang make an infinite loop? The exit condition must be optimized out!
So it's far more likely to segfault than I thought. Good to know.
Neither the post title nor the link explains what Catch2 generators are. 
Would be great to provide a snap app for linux, pretty easy to package 👍
It means you can't mention them in any context whatsoever. They might be macroized or magical in some way.
"Can't be" in the sense that doing so triggers undefined behavior, at which point anything can happen, and if you complain to your implementers, they can point to the relevant bit of Standardese and resolve your bug as by design. (In practical terms, `_Ugly` machinery can change at any time without notice, so taking dependencies on it means that your code can be broken by updates.)
Yes, latest VS preview version seems better. The problem is that, from my previous experiences, each time an update appears, the probability that it breaks existing code is much higher than with other compiler vendors. Honestly, I have never filled a bug report for MSVC either because they were already reported or because they were juste too random. I don't even see how to write a useful bug report saying that moving some non related code lines/functions/includes in a file can produce or resolve an ICE ? The message provided by the compiler in case of ICE just sums it up *" To work around this problem, try simplifying or changing the program near the locations listed above. " :* each time I read this, I ask myself if the developers are trolling me. &amp;#x200B; I've never try clang with VS. Is it ABI compatible with MSVC ? 
Surely I didn't mean you buy them just to check. Thought you may have heard something from someone. My humble experience (AIX 8-10 years ago) is that such platforms tend to lag decades behind mainstream.
In C++ you should include \`&lt;cstring&gt;\` instead of \`&lt;string.h&gt;\`.
They allow to execute parameterized tests - https://github.com/catchorg/Catch2/blob/master/docs/generators.md
Great feedback! I'll process it and edit the FAQ. Some thoughts: &gt;ECS isn't any faster than OOP when you're trying to solve an OOP problem that ECS can't even address This is very true. I'll update the FAQ to reflect this better. &gt;Not all ECS implementations strictly use nomimal typing I'm not sure whether I agree. While it is possible to reuse a \_type\_, this type has to be registered with an ECS under a different identifier / handle / ... If the component does not have a singular meaning, you can't write meaningful systems. So while the programming language can use structural typing, the "ECS type system" has to be nominal. A tag + a type is effectively a nominal type system ;) &gt;The answer here likewise presupposes all ECS implementations work a particular way, which isn't true. \[in response to whether components must be edited in a system\] I see what you mean, and I agree. However, regardless of which ECS implementation you use, chances are always higher that in-system modifications will be faster, since the implementation \_could\_ optimize, whereas outside-system modifications cannot theoretically be as fast. Additionally, you could argue that in-system modifications are "trueer" to the ECS paradigm, but that's more of a philosophical debate. Perhaps using "recommended" in the general sense here is too strong of a qualification. &gt;While I know what you meant, this sentence is a big vague given the language you've tried to define elsewhere. Namely, clarify what "object" means in this context. Yes, you can tell I was trying to find a way to explain, without being self-referential. I'll think of a better way to describe this. &gt;It's fine to reference Reflecs a bunch, but this mostly reads as an advertisement. Good point. I struggled with this a bit, as I want to talk about ECS approaches, but do want to point out that it is not the \_only\_ way to do things. I would like to also highlight how other ECS frameworks approach things, but as I'm less experienced in those, I also don't want to misrepresent them. If you have experience with another ECS framework and you know how they address some of the highlighted topics (or others) let me know and I'll add it. Maybe I'll just replace "In Reflecs ECS" with "There are implementations that" or something like that.
I use CLion, but I rarely work on windows, mostly on osx/linux. Plus I have enough ram to feed the beast. :)
My position on that is "Meh." Except for the "additional overloads" of `&lt;cmath&gt;` etc. and a few headers that provide C++-specific types (e.g. `std::byte` in &lt;cstddef&gt;), there is really nothing to be gained from `&lt;cmeow&gt;` versus `&lt;meow.h&gt;`. `&lt;meow.h&gt;` must provide `::meow` and may provide `std::meow`. `&lt;cmeow&gt;` must provide `std::meow` and may provide `::meow`. (In MSVC's implementation, `&lt;cmeow&gt;` absolutely provides `::meow`.) That is, `&lt;cmeow&gt;` doesn't avoid "polluting" the global namespace. If you like `&lt;cmeow&gt;`, feel free to use it. But `&lt;meow.h&gt;` isn't wrong (for C Standard Library headers, not abominations like `&lt;iostream.h&gt;`), and telling people to stop using it isn't very productive.
For an ICE, you should report a bug with a preprocessed file and complete command line. That's almost always sufficient to reproduce the ICE and fix it. Clang on Windows is ABI-compatible with MSVC (and they use MSVC's STL so the library is compatible too, not just the compiler).
What benefit would this possibly provide other than making compile times invisibly balloon due and being forced to document which parts of code are pulling in std
Isn't that just the same thing that happens if you use C++ namespaces inside header files?
Good guess, but totally wrong. With `-O2` or `-O3`, the `lastIndex` is exactly what you think it would be for a valid function, but `main()` is surprising. main: .LBB1_1: jmp .LBB1_1 `lastIndex` doesn't get called. There's no `"hello"` allocated. `main` is just replaced by an infinite loop. Here's my guess at the clang reasoning: - Loop condition is never false. - UB never happens. - The only way to exit the loop is in the case where the string does contain the character. - Both, the character and the string are compile time constants *at the call site*. - Since there might be other, valid, uses of `lastIndex`, compiler can't turn the actual loop in an infinite loop. - But *at the call site*, invoking the function *is* an infinite loop. - The function also has no side effects. - Therefore, the call to a function that will result in an infinite loop (remember, no UB), can be optimized away and replaced with an empty infinite loop at the call site itself.
My god. The clang optimizer is like that spoon in The Matrix.
I don't understand why C++ doesn't simply figure this out itself. Why forward declare at all? All the information is right there in the source, and the computer is really good at extracting it automatically instead of making a programmer do it by hand. 
Is there any public discussion of what optimizations we can get with a changing ABI?
A notable one is that we'll get EBO without having to opt into it on a per-type basis. However, the bug fixes that we can't have because "we" demanded bin-compat are of more interest, personally. ;-]
Yes, though macros can't be scoped, and namespaces won't work properly with C headers. Another advantage is that modules are *also* effectively precompiled headers. You don't have to parse the header again and again, and all of its dependencies.
If you are on Windows, which I assume is the case because you are considering VS, setting up a non-MSVC compiler can be a bit of a pain especially as a beginner. So for that reason I'd recommend Visual Studio. Visual Studio also has really nice CMake support these days if you want to be able to build your projects on other platforms or use open source libraries hassle-free. And CMake + ninja builds a lot faster than a VS project + msbuild in my experience. I'd recommend a proper IDE over a basic text editor, especially as a beginner, for the debugging support. VS Code is also pretty nice these days if you don't need a massive IDE, and it still has good debugger support.
Using CMake in Visual Studio is really good these days. I don't notice much of a difference working on CMake based projects as opposed to VS Solutions.
I tell people to call `std::meow` instead of `meow`, so I need to tell them to use &lt;cmeow&gt;. But I wouldn't dare tell a guy who wrote a bunch of the STL headers, any whose name is literally STL, what he should do.
Issue is that you will have to parse through the whole file for std:: to see which imports are required. And it could also lead to maintainability issues because it would be challenging to reason about which std modules are imported by a given source file without reading every single line of code. Currently it's easy to manage which sources use which imports, because it's all clearly laid out at the top.
Is there any downside to just sticking to the convention of always using `&lt;cxxx&gt;` headers? In my view it's much clearer - it means "hey I'm using a C-header in C++", and additionally "and if there's anything the C++ compiler/library need to take care of when I use that C header, they'll do it for me - there must be a reason this `&lt;cxxx&gt;` header exists". If you use `string.h` then it's particularly not clear to a beginner what this is. Is it the same as `&lt;string&gt;`? There's no indication that this could be a C-header, apart from maybe that it's `.h` and not `.hpp` but not everybody uses that convention. Anyway just bikeshedding I guess, not really worth the time... :) I just think it's less confusing, particularly for beginners or people who haven't seen it before.
Billy totally overhauled the multithreading headers. If we can drop XP/Vista targeting, we'll be able to overhaul them even more. I've removed dynamic memory allocations for iterator debugging bookkeeping. `call_once` is more efficient. We'll be able to slim down the STL's DLL by removing dead code (e.g. bogus floating-point conversion code, unused Filesystem TS code). We might be able to make RTTI more efficient. We would be able to retune `deque` and reimplement `unordered_meow`.
You have to say `std::strlen` or `using`, that's about the only downside.
Why do people think VS debugger is so good? I would choose to use LLDB over VS debugger any day if I could. Visual Studio for me is just bloated and buggy.
&gt; A tag + a type is effectively a nominal type system ;) Very fair. :) I'd then just clarify in the FAQ what you mean by "type" then because that's a very overloaded term for us. :) &gt; However, regardless of which ECS implementation you use, chances are always higher that in-system modifications will be faster And that's I think a better way to phrase it in your FAQ. :) &gt; If you have experience with another ECS framework and you know how they address some of the highlighted topics (or others) let me know and I'll add it I'm not intimately familiar with the publicly available ECS. I honestly really didn't like ECS for a while because all the open source implementations (at the time) were terrible things I'd never have inflicted on a shipping production game. :p I think Unity has done some talks on their ECS approach that are publicly available which would be illuminating I think, as it will almost certainly become the most widely-used ECS implementation in the world. I'd have to revisit your list and see what I can expand when I have some time. RemindMe! 2 Days "collect some public ECS implementation notes for ajmmertens' article"
I will be messaging you on [**2019-02-03 00:22:28 UTC**](http://www.wolframalpha.com/input/?i=2019-02-03 00:22:28 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/cpp/comments/alrh62/im_putting_together_an_entity_component_system_faq/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/cpp/comments/alrh62/im_putting_together_an_entity_component_system_faq/]%0A%0ARemindMe! 2 Days ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! efhefvb) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Slight correction: these are IF-NDR, which is more serious than UB. Simply having one of these in your program makes the entire program UB, not just that code.
Just partial support for the spaceship operator for now
If str is not null terminated the strlen will segfault. And you're going to underflow your size_t. 
I just don't ever write nonsense like `while (--idx &gt;= 0)`. That's a recipe for disaster.
So there's a lot of people here who have tried CLion and weren't fans of it. I use it for my day to day work. It is rapidly being developed so recently there has been better integration with debuggers as well as remote development which is super handy. It's also getting better with some c++17 constructs. If you develop on Linux and your project is already in CMake then I would recommend it. Otherwise, I would go with visual studio.
&gt; If anyone wants to help us, improving individual companies' and the community's build process will make migration easier. For us, it was night and day difference when we stopped treating the VS solution files as first-class data and started treating them as temporary things to be erased and rebuilt at the first sign of changes/trouble. Using almost any external tool to define how the solution files are meant to be built and then having it scan the source folders and output the correct solution files means we can change out which compiler version/toolset is being used at a moments notice and it's pain free and instant for everyone working on the repo to switch over. We're currently using FastBuild but only because it was the first thing that we came across when we started looking at abandoning manual management of the solution files.
This is obviously a very hypothetical question, but given that the C++ standards committee seems to be slow on this: what do you think are the odds that msvc/gcc might eventually work on the beginnings of abi compatibility with one another? Or is this simply too much work and breakage?
Is `__cplusplus` macro an exception to that rule? That macro follows `__ugly` convention.
Who would do that work, and why would they want to do that work instead of using Clang on Windows?
Can you share a rough estimate of how many issues are waiting for the ABI breaking change? Also, is there anything conformance related that is being held up?
&gt; There are quite a few compiler flags that break ABI compatibility. Which ones? MS won't say and it's hard to find out. It's hard for us to enumerate too. For example, as far as we're concerned, `/std:c++14` is binary-compatible with `/std:c++17`. This mostly adds classes, non-member functions, and non-virtual member functions, all of which are super safe. It also removes and deprecates some things. It changes a very limited number of return types. We don't do wacky things like change data member layout based on Standard mode. *However*, there is nothing stopping user code from doing such wacky things. To pick on one example, I believe that Abseil switches between its `string_view` and `std::string_view` depending on Standard mode. This is a **recipe for doom** in that it sets up ODR violations in code using such a varying type.
Fixing module imports from one fixed set to a different fixed set would be trivial to automate. Someone would make a clang-tidy plugin and it would be a non-issue.
Also feature-test macros are completely supported (previously `__has_cpp_attribute` was missing because I'm not a compiler dev), and [P1008R1](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r1.pdf) "Prohibiting aggregates with user-declared constructors" was checked into VS 2019 16.0 (not sure which Preview). In the libraries, C++20 `remove_cvref` is available, and C++17 `to_chars()` hexfloat shortest/precision was added in VS 2019 16.0.
for the slow people like me: IF-NDR == Ill-Formed No Diagnostic Required 
Maybe 50 implemented and 50 remaining to be implemented. Surprisingly, very few features have been blocked by ABI (I thought Filesystem and Special Math would, but Billy and Casey found a way). There are a couple of constructors that can't be constexpr due to ABI (mutex and error_category).
&gt; unordered_meow Is that a container for cats?
Implementers can give a list of macros with reserved names that they guarantee to be safe to use. In this case, the standard says it should be defined, so it's an even stronger guarantee.
Is XP runtime support officially dead?
That looked interesting, and then I tried looking at the single header in GitHub...
Implemented with boxed types.
I fully support the addition of `unordered_meow` but only if we also add a `cats_cast&lt;&gt;`.
So: the c abi is super useful because you can distribute compiled binaries and they're basically guaranteed to work. Lots of free but closed source tools work like this, and additionally the C api is what is targeted by all language ffi interfaces, I suspect at least partly due to abi concerns, even when the language might support much of c++ style semantics. This means that overall, tools tend to have an annoying C interface even if internally its a c++ api With even a small well defined subset we'd see tools able to target that instead of a pure C api which would probably remove the need to build C++ apis on top of a C api, which is potentially on top of an internal C++ api. The other big advantage is that often developers do not provide binaries for one of the major compilers while providing a C++ api, where maintaining a port or a full C translation may be too much work. Having at least some C++ available to them would be great Clang has massively improved the situation though, but overall I think that the lack of a common abi doesn't help Don't get me wrong its almost certainly a massive amount of effort more than its worth heh, I'm just curious
The first bug is as others stated, unsigned integers are always `&gt;= 0`, but another bug is that if `str` is the empty string then `strlen` returns `0`, and so `--idx` would wrap around to `SIZE_MAX`. To resolve both bugs, change the loop condition to `while (idx--)`.
I guess that will continue our pattern of skipping every other upgrade.
This is one of the reasons I'm not a huge fan of `auto`.
You mean `purr_cast`? We also need `std::scritch`.
so what do you prefer on osx / linux?
I went to try the `extern template class ...` trick in my code base and ran into an issue. When I instantiate the class in one of the .cpp files using `template class ...`, it tries to instantiate all methods of the class, even those that wouldn't compile. A simple example would be to try this with `std::vector&lt;T&gt;`, where `T` doesn't have a copy constructor. // 1.cpp #include &lt;vector&gt; struct S { S() {} S(S&amp;&amp;) {} }; extern template class std::vector&lt;S&gt;; int main() { std::vector&lt;S&gt; v; v.push_back(S{}); return 0; } // 2. cpp #include &lt;vector&gt; struct S { S() {} S(S&amp;&amp;) {} }; template class std::vector&lt;S&gt;; error: call to implicitly-deleted copy constructor of 'S' _Tp __x_copy = __x; ^ ~~~ 2.cpp:9:21: note: in instantiation of member function 'std::vector&lt;S, std::allocator&lt;S&gt; &gt;::insert' requested here template class std::vector&lt;S&gt;; 
Its architecture dependent, might die on ARM might be fine on x86. Im not expert tho
Usually not a problem. Uncached memory on ARM has issues with unaligned reads. Don't bother until it's proven to be an issue.
So what you're saying is I should just use Clang and dump MSVC and the VS suite?
&gt; If we can drop XP/Vista targeting I mean, the toolset would be released already with those two OS have been unsupported for at least two years, even on Extended support. Even if you are unofficially supporting them, I doubt anybody using them would need to update to VS 2019 plus a new toolset.
No - I'm saying that if you want to link with MSVC-compiled libraries, either use MSVC, or use Clang which has done the extensive work to match the MSVC ABI. Why would you want to use GCC instead? And I say that as someone who has used GCC on Windows at home for over a decade.
Holy crap, I new Eric as an intern
Don't understand your question, can you post code? &amp;#x200B; If you have storage and you use placement-new &amp;#x200B; \`\`\` std::aligned\_storage\_t&lt;sizeof(T)&gt; storage\_; &amp;#x200B; new (&amp;storage\_) T(); \`\`\` &amp;#x200B; it's often fine because \`std::aligned\_storage\_t\` defaults to \`max\_align\_t\` which is supposed to be the maximum alignment of any fundamental type on your architecture. So, probably the alignment is already larger than the alignment you need. &amp;#x200B; If the alignment of the storage is not as large as \`alignof(T)\` then the above is UB. &amp;#x200B;
Hopefully.
It would have been MS's job to come up with a standard packaging system decades ago. Third party libraries were the only reason we were trapped in VS2010 back then. Conan seems to fill the dev side of the gap now.
Finally they split up the compiler packages based on the target platform. Now let's hope there are no spurious dependencies as usual. Just saw another weird extension in VS2017 the other day which I couldn't uninstall because of dependencies that shouldn't be there.
"*Don't write portable code until it's proven to be an issue.*" But why, when it's so easy to do correctly the first time?
There are a few reasons. One is hinted at when you say the "LLDB" debugger -- an internal detail, rather than name the IDE that enables a good experience. In the end, what matters is the experience you get from the IDE (even if neglecting the internal details). It turns out that the major game developers using C++ (as an example) have come to a nearly unanimous conclusion that things work out better when you use Visual Studio. When it's in place, all the major debugging features are used a lot every day during development of every feature by every team member and everyone is typically in agreement that the IDE's working great (does this ever happen with an IDE in any other situation? I haven't seen it). \[Bonus details: I've been on teams using VS where the average per-dev bugfix rate is &gt;5 bugs per day.. and I've also fixed bugs on my first day on the job on teams where I can just load up the project in VS and get to work.\] When the deadlines are tight and the competition is merciless, you want to go with the thing that keeps helping everyone get stuff done on time. It's a bit unsatisfying to base the bulk of the argument on past experience, but that's not a bad rule of thumb (especially when you're putting down your time and money on something) when there's a wealth of experience. That's mostly about a team experience. For an individual experience, you get to benefit from tight integration between the toolchain and the IDE (which has historically also been really annoying due to its proprietary nature.. but it's always paid the bills).. since you can start with a sample project and the debugging works properly from the beginning. Then all you have to do is just never, ever, let that experience degrade (degradation in debugging experience resulting from certain code changes ought to be treated as a critical regression) and you're all good. On the whole, it's a different experience than I've often (most often actually) had with other IDE's+toolchains.. where getting it to really, actually work consistently and properly in the first place and deciding how I'm going to set it up has sometimes been a chore that dragged on for days. On the subject of debugging, it's always interesting to me how important it can be to get the IDE integration set up from the outset in order to get the best experience from the tools (Have you ever been on a team that is largely anti-debugger and they seem to have no concept of its merits.. since whenever someone tries it out on their existing codebase, they don't have a good experience on code which isn't well suited to the debugger? I have -- several, horribly-frustrating, times). That applies to use of Visual Studio as well.
Many uC manufacturers base their IDE on Eclipse.
IIRC it has a few false positives, so it was decided to not enable it in -Wall.
He's a great addition to the team :)
Yeah, he was pretty obviously going to go far :p
The O's might be quite different, though. There's another thing, important for games f.e., run-time will be always O(N), while the insert road gives different results every time. B.t.w. bench-marking shows that in the 2D case, plain linear search beats all for up to 50 elements.
VS isn't perfect but definititely the best option when using the right plugins (the standard Intellisense just feels ridiculous after using VAX or Resharper). But anyway you should use CMake+Conan and generate an IDE project from that.