Depends a bit on what you consider compatibility. Have you read the mail linked in this topic? He makes it pretty c clear that he does only care about what gcc does and not what the standard says (at least with respect to this topic)
Linus doesn't care about the standard and doesn't complain about the standard being as it is, so why should he work on changing it. He simply says, that what is relevant for Linux Kernel code is the documented behavior of gcc and what what is written in some standard's paper
Are there any hard numbers (e.g. benchmarks) about how much per furnace average programs gain when they can assume the programmer doesn't violate the aliasing rules?
This is exactly what jclerier said. The important point is that f(no-)strict-aliasing doesn't make a difference here, while restrict would.
The biggest change in this version is the compile time checked conversion between FlaggedT's types. While a *Positive&lt;int&gt;(-2)* will throw exceptions on runtime, construction of wrapped types from one another are now checked at compile time. A function requiring a *MoreThan&lt;vector&lt;int&gt;, 2&gt;* can now accept e.g. a *MoreThan&lt;vector&lt;int&gt;, 14&gt;*, but will reject a *MoreThan&lt;vector&lt;int&gt;, 1&gt;* at compile time. There's also conversions between less related types. *Positive&lt;int&gt;* can be built from *FlooredInclusive&lt;int, SIZE&gt;* if SIZE &gt; 0. This makes it possible to use very restrictive types as less restrictive within the code base. A *FixedRangeInclusive&lt;vector&lt;int&gt;, 100, 130&gt;* always passes as a *MoreThan&lt;vector&lt;int&gt;, 10&gt;* etc.
What part of this understanding / explanation will cause the most issues? Genuinely interested.
[What are move semantics?](https://stackoverflow.com/q/3106110)
I thought you c++ guys enjoyed slamming your heads into your desks? That's why header files are still around, right?
The IEEE-ish semantics they bolted on for complex arithmetic spring to mind...
You're not wrong
angle brackets as template lists introducers is one of the worst decision that was made when designing templates. They clash with the inferior sign and are the reason why in a dependent scope we have to write "template" before a templated name. Parenthesis are simpler, cleaner and have only one meaning. The point of modern metaprogramming is to move away from template metaprogramming and with it's its orendus syntax. &gt;foo(a) bar(b); // Is 'foo' a metafunction here? foo could also be a macro but the result of foo can only be a type and it's clear.
thats just not true
&gt; But standards aren't there to be "good". They're there to make things portable and interoperable. In that case, standards often have to choose the suboptimal. While that may a valid argument in general, it's hardly a valid argument for type punning being undefined behavior in C++, where _at worst_ it could be simply considered _implementation defined_. There is absolutely no reason why union-based punning shouldn't be allowed in C++ at least for standard layout types, with the specification that something union { struct { uint32_t x, y } u32_2; uint64_t u64_1; }; will lead to architecture-dependent results.
I've seen this discussed a few time here before. There are a many, many reasons why the committee works the way it does, and why an open forum like mailing lists and github-like management of the standard cannot work (even though the standard is hosted on github). The process of changing C++ is not "heavy" until it gets to the ISO level. The process is conceptually very straightforward: write a paper, discuss the paper, update the standard. It would be fair to say that the process is not perfectly democratic, but there are good reasons for this. First, the committee: The committee is comprised of people with a broad and deep knowledge of the language and---very importantly---who (or whose country or whose employers) have a vested interest in the longterm direction and evolution of the language and its use. There's a very good description here: https://isocpp.org/std/meetings-and-participation It is also worth noting that the ISO C++ Foundation also offers travel assistance in certain cases. https://isocpp.org/about/financial-assistance-policy I think that keeping the size of the committee relatively small is important. The mailing lists are already quite active, but the discussion is usually very focused (and it's still very easy to miss important discussions/details). Making the mailing lists even more open (like, say, std-proposals), would make the signal-to-noise ratio untenable. The size of the committee also affects our ability to host meetings. I think Rapperswil is the largest yet. We had 160 or so attendees. For a host, that means finding and negotiating hotel rates, providing coffee and snacks (which can get expensive), hosting a dinner, providing AV equipment, etc. Making the committee larger may inhibit participation and will reduce our ability to make progress. Second, papers: Papers are the committee's form of issue report and pull request. Anybody can submit a paper. However, in order for it to be considered, your proposal's concerns will need to be shared by many other stakeholders in the language. Papers with low stakeholder buy-in don't move forward. Ever. FWIW, I've written papers like this---I think most committee members have. I cannot recommend the experience of trying to convince the committee, in person, that your pet idea is worthwhile when it primarily addresses your needs. 0/5 stars for that experience. Writing a paper provides a focal point for having discussions and making decisions. A (good) paper introduces specific topics, new language features, changes to behavior, occasionally deprecations, etc. that we can be considered. Good papers will also distill a significant amount of discussion (online and face-to-face), alternative designs, and user experience. The std-proposals mailing list is a good way to begin introducing and discussing new ideas. Committee members post and reply regularly. Good ideas discussed there have turned into well-supported proposals. That said, most proposals discussed on that list do not move forward for various reasons (often serious technical problems or the potential to break valid code). That would be the same outcome regardless of the process used to change the language. Third, meetings: Meeting face-to-face has a number of advantages over mailing lists discussion. Mostly, we go to meetings because can get more work done than we can online. Also, voting. There are some other important issues, discussed below. I think the more important aspect of meetings is that you actually have to discuss your ideas in person, and those are very different than the kinds of discussions had on open forums. We don't suffer ad hominem arguments (your feature is bad because you're dumb), and because there is no anonymity, there is significant personal jeopardy in making such arguments. Discussion at meetings is (usually) quite collegiate. It's worth noting that the ISO and ISO C++ Foundation codes of conduct apply to committee mailing lists as well. Fourth, legal: At least in the US. The committee provides a framework for collaborating on a shared "product" in a way that avoids the appearance of violating antitrust laws. Most companies wouldn't participate if their legal counsel thought there was a chance of being accused of antitrust laws or colluding e.g., to fix prices. Employees are allowed by companies to contribute to a shared technical specification because the committee works to ensure that this appearance is maintained. Meetings also provide a protected environment for employees to discuss certain aspects of proprietary technologies and information that may help inform the technical specification. A company may be otherwise unwilling to make that information public. We have prohibitions against live-tweeting, photos, and live-streaming meetings because of these protections that our meetings afford. Anyway, that's my view of this. Most of it seems right, and I don't think we could effectively develop C++ as if it were an open source product.
`foo(a)* bar(b);` Now we have a problem... 
Direct LKML link: [https://lkml.org/lkml/2018/6/5/769](https://lkml.org/lkml/2018/6/5/769)
&gt; So him throwing shade at standard committees members is precisely because he doesn't participate in standardization. What is it with people gatekeeping on here and implying that only those participating in the standardization process have any right to even have an opinion on C++? Nothing Linus could possibly have said could reinforce his opinions on C++ more than reading through the comments on this thread calling him out for not spending weeks and months of effort trying to improve a language that *he doesn't even use*, and then trying to act like it's his fault if C++ ends up still defective.
Oh, that's easy. You need the super-spaceship operator (`&lt;=^=&gt;` with plenty of bikeshed options) that returns a constexpr std::string that is == if the values are equal. I don't see what's so hard... üòÅ Only half kidding. We mapped all of our objects to a "KeyString" format such that memcmp of the key strings had the same total order as the original objects. This includes json-like dynamically-typed objects and correct numerical interleaving of int64, double, and decimal128, so I know how hard it is but that it can be done. It is really useful in so many cases.
sure, but that requires a function. And function take space in the binary. I was able to reduce the size of a library I work on by a whopping 5 megabytes thanks to replacing such for_each calls by fold expressions for instance.
What is not true? That Linus Torvalds is one of the programmers?
Linus is 4th in number of commits, 2nd in lines modified, has made commits all the way from the start until now in 2018. I'd say he's a pretty significant part of the project.
Wow, that's surprising. Can you link to a source about that?
How about just adding the "EXPORT" keyword to the install command for the targets? Or some variation thereof? Everything necessary to know for the typical use case is already defined in the build by this time.
It typically takes a lambda, like so: ``` tuple_for_each( tp, [&amp;](auto f) { std::cout &lt;&lt; f &lt;&lt; " "; }); ``` and the generated code is exactly as you'd expect from the hypothetical `constexpr for`: https://godbolt.org/g/mWHXoP
yes, at -O3. Development isn't typically donee at -O3. In debug mode : - I hit more than once the windows 65535 symbols per lib... actually it's fairly simple to do if you have a few variant visitations - Not a lot gets inlined - even with GCC / Clang's -Og. Every random template instantiation, even dumb stuff like std::forward will take some space in your binary. At some points I had multi-gigabyte static libs on linux. Even when starting from a ramdisk, gdb would take ~1 minute just to load the symbols. 
that linus has any emotion for cpp other than hate and phobia
If you are on windows just use vcpkg 
Could you provide an example of lvalues and rvalues which designate the same object? Genuinely interested (so I can fix my notes :) )
Reminds me of my snippet for learning move semantics. #include &lt;algorithm&gt; #include &lt;cstddef&gt; #include &lt;cstring&gt; #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;vector&gt; // this class follows the rule of five class Blob { public: // default constructor Blob() : size_(1024 * 1024) , data_(new uint8_t[this-&gt;size_]) { std::cout &lt;&lt; " default constructor" &lt;&lt; std::endl; } // default constructor Blob( const std::size_t size) : size_(size) , data_(new uint8_t[this-&gt;size_]) { std::cout &lt;&lt; " argument constructor" &lt;&lt; std::endl; } // copy constructor Blob( const Blob&amp; other) : size_(other.size_) , data_(new uint8_t[this-&gt;size_]) { std::cout &lt;&lt; " copy constructor" &lt;&lt; std::endl; std::memcpy(this-&gt;data_, other.data_, this-&gt;size_); } // move constructor Blob( Blob&amp;&amp; other) // notice it is not const, move requires this : size_(other.size_) , data_(other.data_) // notice we just take the buffer { std::cout &lt;&lt; " move constructor" &lt;&lt; std::endl; other.size_ = 0; other.data_ = nullptr; } // copy assignment operator Blob&amp; operator = ( const Blob&amp; other) { std::cout &lt;&lt; " copy assignment" &lt;&lt; std::endl; delete this-&gt;data_; this-&gt;size_ = other.size_; this-&gt;data_ = new uint8_t[this-&gt;size_]; std::memcpy(this-&gt;data_, other.data_, this-&gt;size_); return *this; } // move assignment operator Blob&amp; operator = ( Blob&amp;&amp; other) // notice it is not const, move requires this { std::cout &lt;&lt; " move assignment" &lt;&lt; std::endl; this-&gt;size_ = other.size_; this-&gt;data_ = other.data_; other.size_ = 0; other.data_ = nullptr; return *this; } // destructor ~Blob() { std::cout &lt;&lt; " destructor" &lt;&lt; std::endl; if (this-&gt;data_) { // need this check to prevent double free std::cout &lt;&lt; " delete" &lt;&lt; std::endl; delete this-&gt;data_; } } // get size std::size_t size() const { return this-&gt;size_; } // get data uint8_t* data() { return this-&gt;data_; } private: std::size_t size_; uint8_t* data_; }; // this function returns a temporary so you don't need std::move() Blob make_blob() { Blob x; return x; } void print_section( const std::string&amp; msg) { std::cout &lt;&lt; std::endl &lt;&lt; msg &lt;&lt; std::endl; } int main() { print_section("expecting default constructor"); { Blob x; // don't forget, Blob x() is a compilation error } print_section("expecting default and argument constructor"); { Blob x; Blob y(10); } print_section("expecting default and copy constructor"); { Blob x; Blob y(x); } print_section("expecting default constructor and copy constructor"); { Blob x; Blob y = x; } print_section("expecting two default constructors and copy assignment"); { Blob x; Blob y; x = y; } print_section("expecting two default constructors and move assignment"); { Blob x; Blob y; // remember what std::move() does is simply cast y from Blob to Blob&amp;&amp;, // treating it like an rvalue (i.e. temporary) and matching the // operator=(Blob&amp;&amp;) function. x = std::move(y); // below will compile and work but is undefined behavior because y has been // gutted and moved into x so y should be considered a zombie std::cout &lt;&lt; y.size() &lt;&lt; std::endl; } print_section("expecting two default constructors and move assignment"); { Blob x; x = make_blob(); } print_section("expecting ten default constructors"); { std::vector&lt;Blob&gt; x(10); } print_section("expecting default constructor"); { std::vector&lt;Blob&gt; x; x.emplace_back(); } print_section("expecting default constructor and copy constructor"); { std::vector&lt;Blob&gt; x; Blob y; x.emplace_back(y); } print_section("expecting default constructor and move constructor"); { std::vector&lt;Blob&gt; x; Blob y; x.emplace_back(std::move(y)); } print_section("expecting default constructor and copy constructor"); { std::vector&lt;Blob&gt; x; Blob y; x.push_back(y); } print_section("expecting default constructor and move constructor"); { std::vector&lt;Blob&gt; x; Blob y; x.push_back(std::move(y)); } return 0; } 
int i = 0; int&amp; a = i; int&amp;&amp; b = std::move(i); `i`, `a`, and `b` all refer to `i`.
`i`, `a`, and `b` are all lvalues. ;-]
I found the article clear and well thought out. Thanks.
Given that the process is fragile and slow, I hope compiler vendors will start implementing static exceptions before the committee does it's thing. 
&gt; In which case could the compiler use a possible "temporary" overflow to screw you? Ignoring the fact that a compiler is NOT an adversary attempting to screw you... My question is exactly the opposite; that is, does the use of `-fwrapv` open up new optimizations that are not possible under conventional rules. For example, imagine `3 + y -5`. If overflow is undefined behavior, then `y` must be `&lt;= INT_MAX - 3`, while on the other hand, for `y - 2`, y must be `&gt;= INT_MIN + 2`. The validity domain of `y` changes even though mathematically speaking both should yield the same result. `-fwrapv` removes the validity domain, and therefore unambiguously opens up the possibility for the compiler to transform `3 + y - 5` into `y - 2`; is this kind of transformation valid without `-fwrapv`?
How is that a worse than that: foo&lt;a&gt;* bar(b) In the above foo can be a variable declaration, a templated using or a metafunction. Compared to: foo(a)* bar(b) here foo is only either a function or a metafunction. 
 int a = 1; a; // lvalue that refers to a std::move(a); // xvalue that refers to a
&gt; who in his right mind would define NULL as 1 (or any other number)? I guess this is technically not exactly the same thing, but if you assign either `nullptr` or `0` to a pointer-*to-member* gcc will actually assign `-1` :P
*Beep boop* I am a bot that sniffs out spammers, and this smells like spam. At least 65.12% out of the 43 submissions from /u/pinxin appear to be for Udemy affiliate links. Don't let spam take over Reddit! Throw it out! *Bee bop*
Assuming it has proper build, you can just do cd LIBRARY_FOLDER cmake -Bbuild -H. -DCMAKE_BUILD_TYPE=Release cd build make -j MOAR_CORES make install You can ask cmake to build directly to a folder, but I don't remember the flags on top of my head, so you get this recipe instead ¬Ø\\_(„ÉÑ)_/¬Ø You might also want to select `RelWithDebInfo` build type instead, so that the libraries are debuggable, provide `CMAKE_INSTALL_PREFIX` to install to a custom location, etc... 
Sure, both of your examples are good reasons to use free functions. I didn't say never to use them, I only said that functions that can have a minimal (private) scope, but do not rely on member data, should probably be private rather than free. And what the OP is trying to do is clearly pushing far beyond the boundaries of what's reasonable: he doesn't need a set of free connection functions, he needs connection objects with state and everything. 
It should not matter. The addition operator is commutative for any integer in C++ whether it is signed or unsigned, i.e. `a+b+c` can be reordered however the compiler sees fit so you would expect `a+b+c == b+c+a` to be `true` every single time (when no overflow is present). The only thing that changes with signed integer overflow is that, if overflow is present, then not only is `true` a valid answer but so is every other answer^(1) but since it is always a correct result the compiler can still replace `a+b+c == b+c+a` with `true`^(2). 1: And even things that aren't answers such as crashing the program, throwing an exception, doing I/O etc. 2: The fact that a compiler is allowed to apply a certain optimization does not mean that it will. Clang and ICC do but interestingly GCC doesn't but does once you pass `-fwrapv` (https://godbolt.org/g/nmaEy2). We can in fact use this to construct a benchmark that runs is infinitely faster with `-fwrapv` (http://quick-bench.com/JEdgzvza9quUratQXoJHRhXYOv0). This wouldn't necessarily work in other compilers though.
I think the compiler can do the transformation. The fact that overflow is undefined does not mean that it has to blow up. `y - 2` yields the same result on the valid ranges for `y`. What happens if there is an overflow is undefined and hence the result does not matter there. (And the compiler is allowed to give you the result you expect.) I think undefined behavior allows this optimization even on a hardware where an overflow would produce some hardware interrupt. The compiler can assume it does not happen and make the transformation anyway. So even for `y = INT_MAX` this would not trap. (That would be the case where UB bites you, if you expect the trap to happen.)
The validity domain in this case only applies to the user. The compiler is allowed to assume that all values will fall within the validity domain. To take your example, yes the compiler is allowed to make that transformation, as there are three possible outcomes: 1. `INT_MIN + 2 &lt;= y &lt;= INT_MAX - 3` is in the validity domain: `y - 2` is the correct answer. 2. `y &gt; INT_MAX - 3`: Any answer is valid including `y - 2`. 3. `y &lt; INT_MIN + 2`: Any answer is valid including `y - 2`. You will in fact see GCC, Clang and ICC all don't care whether you pass `-fwrapv` (https://godbolt.org/g/2ZP6ic). 
It doesn't look like the rules were made explicit, so I'll give a go at it. I hope this helps answer your question. Given this as a starting point: class base { public: virtual void public_foo() { ... } protected: virtual void protected_foo() { ... } private: virtual void private_foo() { ... } }; Let's add one more method inside of base: class base { ... void call_stuff() { public_foo(); // ok to access one's own public members protected_foo(); // ok to access one's own protected members private_foo(); // ok to access one's own private members } }; Now let's derive: class derived : public base { public: void public_foo() override { ... } protected: void protected_foo() override { ... } private: void private_foo() override { ... } }; This is fine so far in C\+\+, so let's move on to where the accessibility matters: class derived : public base { public: void public_foo() override { base::public_foo(); // ok to access base class public members } protected: void protected_foo() override { base::protected_foo(); // ok to access base class protected members } private: void private_foo() override { base::private_foo(); // error - base::private_foo() is private } }; Note that because the base class private\_foo() is private, the derived class is not allowed to call it. This is the same rule that applies to private member variables. Finally for completion: void outside_the_class_hierarchy(base &amp;b) { b.public_foo(); // ok to access public members b.protected_foo(); // error - can't access protected members b.private_foo(); // error - can't access private members } In C# or Java, it isn't always clear if the derived implementation of a method is expected to call the base class implementation. One practical use in C\+\+ is that you can disallow calling the base class method by marking it private, and communicate that a base implementation may be called by marking it protected. NVI can be used for those times when the method must be called.
You might be the best bot I've ever seen. &lt;3
Yes, in this specific case they don't, which is why I was reluctant to include the example to start with. The question I asked at the beginning is: are there cases where `-fwrapv` enables potential optimizations?
In short: No, `-fwrapv` only guarantees a certain behaviour on overflow, whereas without `-fwrapv` any behaviour is allowed on overflow (which includes wrapping). See [my other answer](https://www.reddit.com/r/cpp/comments/8pyq94/torvalds_on_aliasing/e0g0bik/) for slightly more details.
So rvalues are just data? I've always been confused about the difference.
That's assuming that the compiler can elide the memcpy() call. This is not a valid optimization if the direct CPU load operation requires alignment since memcpy() has no alignment requirements of its own. Here's a 32-bit ARM example where using memcpy() forces a copy through the stack: https://godbolt.org/g/rfgtrW ...and here's an example where using memcpy() to work around Intel's broken intrinsics design results in a stack copy with MSVC: https://godbolt.org/g/nA1a5y The `bit_cast` proposal is a step in the right direction, but not comprehensive. 
I don't see what ads could gain by switching to WebAssembly, it is not going to replace javascript and the dom. However I am afraid about hidden datamining scripts being more profitable.
 void g() { int x; x; // lvalue std::move(x); // rvalue } struct S { S&amp; self() { std::cout &lt;&lt; this &lt;&lt; '\n'; return *this; } }; void f() { S().self(); // lvalue , same object as S() which is rvalue // address of temporary object printed S() = S(); // assign to rvalue } 
You have 2 different definition of portability. What is "funny" is that you are basically defending the standard letter, while redbeard0531, who identify has being on the C++ committee, is implicitly using the pragmatic (and actionable!) interpretation. And well, thinking more about it, this is reassuring to a point, and probably actually more logical than funny, to know that some in the committee are really doing what they should, trying to understand and standardize existing practices, and taking into account classic problems. One of the key point of Linus is that standard are not sacred texts. If they have problems, they should be worked around, or the standard should be changed, etc. And existing implementation must obviously be taken into account, because the theoretical strictly compliant but not more compiler does not exist, and probably would not even be very useful. It is wrong to the highest point to consider that once a topic has been standardized at minima, existing legacy practitioner in this area should conform to strict compliance of the new norm or should somehow be considered at fault. Threading did not officially exist in strictly conforming C and C++ until a few years ago. Did that prevent from threading existing at all and should implementation have pretended that it actually does not exist, in order to "optimize"? That would have been retarded. If someone took a dependency at that point on a behavior considered perfectly fine by absolutely everybody at that time, and that now that feature is technically UB (that might BTW be because it is merely non-portable on really exotic architectures, rather than intending to invoke nasal daemons from the start), should that person feel bad about it, or consider that MAYBE in some case the committee was smoking good stuff? It depends. I see no reason to think it should be absolute in either direction, so one can totally gives their opinion and maintain their project according to what the think is sane. Especially critical projects, maintained by one of the greatest engineer. BTW, it would be useful if a rational for the standards was maintained and published. Maybe less people would be interpreting it has a religious text, then. And everybody could go back to engineering, rather than pretending we have a formal text that we must model check against unconditionally. 
As one of the authors of P1089, I was the one presenting it in Rapperswil, and will be one of the ones presenting its followup in San Diego. Please, please send us a description of your real-world experience, as data helps strengthen our case. You can find our email addresses in the paper.
A compiler is not a platform.
It is convenient for compiler writers.
Very cool. I can imagine in the future, these type invariants (that express a requirement, not perform an action) might be expressed similarly to contracts: template &lt;typename T&gt; using Positive = T [[requires x: x &gt;= 0]];
1. I thought, and I think this is widely believed by a lot of people, that char was a universal aliasing type for RW -- and I think that main implementations doing TBAA (gcc and clang) are handling char like this. Do you have any reference from where we can easily infer that this is only for guaranteed in a portable way for reading? 2. On the contrary, it seems that there is actually nothing *written* in the standard that really allows you to memcpy to a completely different type. But tons of people are stating that this is the safest way, so it is probably very low risk to do so. 
Can't you just continue your work as a library and kindly ask stdlib vendors to ship it ?
If the compiler can not prove that the alignment is correct, it must obviously use another construct than trying to use naively aligned instructions for the access. But there is *nothing* that prevents it from being smart and avoiding the memcpy anyway, as long as the behavior of the program would stay the same, conforming to the as-if principle. And after all, if you are attempting to type-pun, and your alignment is more strict on the destination type, you can not possibly, reasonably, and portably, do a better job than the compiler. Whether it manages to elide the memcpy() (but use more careful instructions for known localized accesses) or not is a quality of implementation issue. Oh and BTW for what we are talking about, the access *must* already be localized to a degree, because if simultaneous access is required to the original object and to the new one, then you also obviously can not elide the copy (but even then std::memcpy can *always* be implemented by something else than a call instruction to a library function, as far as the standardize language is concerned) 
Torvalds speaks as if he is some kind of a deity, I can't get halfway through his drivels. Whatever point he's making, it gets lost in the dickishness and assholery.
You could have generic free functions (connect, disconnect, test) that are templated on an InternetConnection parameter, which could be a concept you define. Then your concrete classes for Wifi or 3G connection need to model that concept, but they don't have to inherit from anything.
This course seems good, why doesn't it have any reviews?
Well, some other languages seem to be doing fine without those constraints. Some of your point are even in *favor* of more cyber openness: like too many people is bad because it requires too big physical meeting places? Well the problem might be that the work depends *too much* on physical meetings. 
&gt; And after all, if you are attempting to type-pun, and your alignment is more strict on the destination type, you can not possibly, reasonably, and portably, do a better job than the compiler. This is true. However, I deal with a lot of situations where the alignment of the source pointer is greater than the alignment of the source type and compatible with the alignment of the destination type. This occurs frequently in vectorization and serialization scenarios where either the source is known to be over-aligned or explicitly aligned with a pre-loop. There is not currently a standardized and effective way to communicate this to the compiler for a guaranteed zero-overhead result. 
At -O2 too. I agree that if you target -O0 or -O1, things look different.
Yes, and I also remember that, but (a) the anti EEE lawsuit was completed years before introduction of the _s functions, and (b) EEE kinda depends on the functions being ones users like, and users almost universally hate these. I agree the lack of cross-platform support for these APIs is unfortunate, but I also hate the "implicit sized buffer" APIs like strcat and strcpy with the fire of 1000 suns due to the rampant buffer overflow bugs they encourage.
I am unaware of any such proposal.
Yes, but all the data for the string lives in the basic\_fixed\_string itself (I'm assuming), not a separately allocated pointer.
To be pedantic, I'm not sure it would even be possible to *guarantee* a zero-overhead result in this case. At least not in the way guaranteed copy elision was guaranteed for example. After all, if you build without optims, most probably your implementation will always call memcpy. You would like a notation to to classical type-punning in place, at source level. Sadly, it is now forbidden in the name of "optimizations". Yet it prevents you from doing your real targeted optimizations, that would maybe ironically optimize way more (and have faster debug builds)... And now I'm wondering how often this situation happens.
&gt; Here I create two simple strings s1 and s2. I join them and I put the result (a temporary string, i.e. an rvalue) into std::string&amp;&amp; s_rref. Now s_rref is a reference to a temporary object, or an rvalue reference. There are no const around it, so I'm free to modify the temporary string to my needs. **This wouldn't be possible without rvalue references and its double ampersand notation.** I don't think so. std::string s1 = "Hello "; std::string s2 = "world"; std::string s = s1 + s2; s += ", my friend"; std::cout &lt;&lt; s &lt;&lt; std::endl; This works fine.
A code generator running with optimization off is not likely to have any aliasing issues due to not caching any values, though, so it may have _less_ issue dealing with a construct that just tells it to do a direct load. To be clear, I don't like using memcpy() for this as, even codegen issues aside, it's annoyingly verbose. An ugly consequence of the current situation is that you can't write an optimized memcpy() replacement in C++ since strict aliasing bans the typical optimizations done in memcpy(). There is more generally a disturbingly increasing use of compiler magic to handle sticky situations in standard library implementations, such as optimized strlen() not being usable in constexpr functions. Using compiler magic makes these solutions inaccessible to non-stdlib code. 
You may also check my general purpose library [ASL](https://github.com/aslze/asl) which includes HTTP client requests, HTTP servers, JSON, XML, etc. And does not rely on curl or anything. The HTTP part is quite simple and does yet not support things like authentication (you could probably do that manually with whatever headers are needed).
Oh come on, stop scaring the guy. Yeah, it's a complex beast of a language but it's very much possible to write simple, modern elegant code. You don't have to use super advanced language features. If you'd stay within a reasonable subset of the language it is not dramatically harder than most other languages. Assuming he has good colleagues he can learn a lot from them, and let them guide him. 
Yes, true :-) I know, in case of Linux we all know his opinion about C++...
There's almost no difference between the terms "undefined" and "implementation defined". In a standards document, they may as well be the same thing. The standard doesn't define it, hence, "undefined". C++ already has the as-if rule, so compilers are still free to implement a definition if it doesn't break other things. C++ separates "undefined" and "ill-formed". If C++ made union punning ill-formed, then you may have a point, but it doesn't, so you don't.
Who said he doesn't have a right to an opinion? Why are you gatekeeping and denying my right to have an opinion about his opinion? C and C++ both have problems with aliasing, so C++ is relevant in the discussoin whether or not Linus uses C++ or not. And yes, he does use C++ on his Subsurface program.
&gt; One of the key point of Linus is that standard are not sacred texts. They're not sacred texts. They're *standards*. They're there to be adhered to for maximum portability. If you don't want maximum portability, then don't adhere to the letter of it. Standards should be updated. But while a standard is currently where it is, it should be followed as close as possible. We're lucky in programming to even have an international standard, especially one that explicitly lays out an as-if rule. In some industries, you don't even have the luxury of deviating from the standard.
Clearly we should do like javascript does and add another null type. Maybe `undefinedptr`? ^^^^/s
Your code can compile to additional instructions though
&gt; (1.1) A glvalue is an expression whose evaluation determines the identity of an object, bit-field, or function. (1.2) A prvalue is an expression whose evaluation initializes an object or a bit-field, or computes the value of the operand of an operator, as specified by the context in which it appears. (1.3) An xvalue is a glvalue that denotes an object or bit-field whose resources can be reused (usually because it is near the end of its lifetime). (1.4) An lvalue is a glvalue that is not an xvalue. (1.5) An rvalue is a prvalue or an xvalue. Are you seriously suggesting that this is a good explanation to a person that is getting his head around rvalue and lvalue for the first time? 
Yes. It has the advantage of being simple and accurate. If the reader doesn't know the meaning of any of the other terminology then they can look that up and come back to here. 
 * Blob's copy-constructor is not exception-safe: the object is left in a bogus state if the `new` throws. * `// this function returns a temporary so you don't need std::move()` , there is no temporary involved here, however there is a rule that when the return statement names a local variable of the function, there's an implicit `std::move()` applied, and also this is a copy elision context. * `if (this-&gt;data_)` is not required in the destructor. * `{ Blob x(); }` is a function declaration, not a compilation error * `// below will compile and work but is undefined behavior` - it's not undefined behaviour, you correctly implemented the move assignment operator so that `y.size` is `0` after the move. 
Well, you can literally say that about everything formally described. How about you try to learn what is [directional statistics ](https://en.wikipedia.org/wiki/Directional_statistics?wprov=sfla1) by looking up terms you don't understand? Ofcourse, my example is an exaggeration but I am trying to illustrate a point that formal and accurate descriptions are not always the best educational resources. Another example is how are we though maths at school. Weren't you given inaccurate information ( totally false in fact) that you cannot do a square root of negative number when you were a kid? There is a good reason for that. Learning process is often much easier if you make certain concepts inaccurate but easier to understand. Their is no reason that once feel confident about the topic, you revisit it and update your knowledge with more accurate information. 
THIS. NEEDS. TO. DIE. Seriously. This is the kind of thing that should be implemented as a library, just not as a _standard_ library. And the main reason for it is that a library can be thrown away once it has become obsolete. Ask yourself what a 2D library would have looked like 30 years ago. Would you still use it today?
Agreed. I hear that some few people may even be working on serious proposals for San Diego. Although that wouldn't target 20 :(
Teaching that planets have circular orbits -- not great but acceptable, because an internalization of that can easily be tweaked to take into account elliptic orbits. Teaching the geocentric model -- bad, because to accommodate reality the whole thing will have to be thrown out and start again. This article is like the geocentric model, it conflates expressions and objects (amongst other egregious errors). For someone to read this article series and then go on to properly learn about rvalues etc., they would need to wipe the slate clean and pretend they never read this.
`place`/`unplace`. There should be symmetry between the making of an object to be initialized and valid, and the unmaking of it.
I wasn't arguing about the OP's article. I was claiming that resource you provided falls short of being a great introduction. 
Thanks for the corrections. For the copy constructor, if `new` throws then won't the object be unwound and the `bad_alloc` exception propagate to the calling context, and wouldn't that be "okay" in the sense that there's isn't any dangling data, the object being copied remains untouched, and it becomes the caller's responsibility the failure of the copy constructor? Unless there's some rule regarding exceptions in the copy constructor I'm not aware of. 
My last comment was in response to your last paragraph about inaccuracy versus ease of understanding. IMO formal and accurate descriptions are better than geocentric model type descriptions, for adult learning. Undoubtedly there also could exist a "circular heliocentric orbit" level explanation for rvalues and lvalues, although no such one has been offered on this thread so far. 
Sorry, I meant to say Blob's copy-assignment operator is not exception-safe. (Will edit my earlier comment). E.g.: Blob b, c; try { b = c; } catch(...) { std::cout &lt;&lt; "assignment failed\n"; } // error when B's destructor runs, (or any other operation using b.data_)
[Stroustrup's original meeting notes](http://www.stroustrup.com/terminology.pdf)
&gt; IMO formal and accurate descriptions are better than geocentric model type descriptions, for adult learning. Its hard not to agree with most part of this statement. But mainly because it seems to be using "straw man" type argument, to claim that the alternative to "accurate description" is a description that is absolutely incorrect ( and not in regards to details). Also, if it comes to your opinion about adult learning vs children learning ( I know that I am totally offtopic now) , I respectfully disagree as well. Perhaps it works for you to learn everything through its formal definitions, straight from an encyclopedia. Unfortunately it doesn't always for me, and it doesn't to many other adults I know. Intuitive understanding is a powerful concept that allows people to visual things using ideas that they are familiar with. In my experience it works for children as well as adults.
There are some things that make sense to put in `std`, and many things that don't.
&gt;Also, if it comes to your opinion about adult learning vs children learning ( I know that I am totally offtopic now) [Here is an introduction](https://en.wikipedia.org/wiki/Andragogy) . Obviously, individuals have a range of learning styles that work better for them. Adults are more self-aware in this respect.
So he targets a slightly different standard, which is GCC C. In many ways, it is a superior standard because there are less undefined things so there are less surprises. Giving too much freedom can lead to many headaches and/or bugs.
This is also very useful for string operations, because you can do optimizations when you know both strings don't go over each other.
It probably will not confuse the compiler, but it does confuse a human reader like me.
If you read this, Guy, thanks for your work. I can definitely understand the frustration of putting so many hours into something because there was good initial reception and then ever declining support. That sucks. However, hopefully the package management stuff will work out. Then it is less important for certain things to be in `std`, and perhaps your work would still be accessible.
There's a huge difference between 'undefined' and 'implementation defined'. UB means anything might happen. Proverbial nasal demons. Halt and catch fire. Implementation defined means it \_is\_ defined. sizeof(int) is ID. It has to work the same way all the time. 
But GCC isn't written for Linus. That's my point. GCC is written by contributors for contributors, and they want a compiler that also works for them. And the only way they can work together is if they all agreed on the same standard. Giving too much freedom *either way* can lead to many headaches or bugs. Freedom to have any vendor specific behaviour is too much freedom. That's why we have this whole mess - because vendors wanted their own behaviour. So either we have many vendor specific extensions with who knows how many compatibility issues, or we leave certain minimal corners of the language undefined so that at least people can work together.
Most of what Linus needs to make Linux works is what almost every compiler (even msvc) written by sane people do. Integer wrap around and union type punning works on the three main compilers (some flags may be required). GCC is also very strict about not breaking code when it changes, I'd say even more than the standard (like you had to explicitly opt-in to C++11 even years after it shipped). You make it sound like anyone can change GCC however they want, but they don't break people's code that rely on their documented features.
&gt; But standards aren't there to be "good". They're there to make things portable and interoperable. In that case, standards often have to choose the suboptimal. Sure, but "good" could also mean features that encourage portability and interoperability. "Good" also means clarity or something that is not open to random interpretation.
Fixed.
AS a pointer is unsigned, I would to call it UINTPTR_MAX, in 2's complement, that's -1.
His new job gives him all the creds in the world, from scratch, so I wouldn't worry if I were him. The existing code-base probably already made lots of decisions for him, limiting the language naturally. 
Wait? You mean, you don't use the [BGL](https://en.wikipedia.org/wiki/Borland_Graphics_Interface) anymore?
You're right. Then I guess LT argues that Clang/LLVM does what documented gcc does. AFAIK VC does the same. THEN, the next step is to say that the std sucks and is wrong, because all compilers that matter do not do what is stated in the std. There is a point.
Sorry if you think I'm argumentative, but that's not what jclerier said IMO. &gt; where a and b point to the same object, while this is disallowed (more precisely, use of each pointer is disallowed if I'm not mistaken) if a and b are tagged restrict ... It's not dis-allowed, you have no guarantee what will happen, i.e. UB.
&gt; Sadly, it is now forbidden in the name of "optimizations". Yet it prevents you from doing your real targeted optimizations, that would maybe ironically optimize way more (and have faster debug builds)... Yes, sad indeed. 
It would be worse if we have both
&gt; You make it sound like anyone can change GCC however they want No I don't. In fact, the opposite.
I'm talking about effective differences. There are almost no effective differences between the two. If the standard says something is undefined, but the compiler is able to define a behaviour with a flag, then for all intents and purposes, that behaviour is implementation defined.
Disallowed doesn't mean it doesn't compile. Where did you get that impression from?
&gt; And the main reason for it is that a library can be thrown away once it has become obsolete. Things in the standard can be deprecated and then removed.
This is very disappointing. I really wanted a standard and simple way to put pixels on screen. I would have been quite okay with the minimal alternative proposed in P1062, but to have nothing at all, well, thats disappointing.
Examples without dokumentation? Include stdio.h?
Quite a bunch of 70's cpu architecture used non-zero values for NULL.
Not just math libraries. We really need standard way to do common advanced bit manipulations. E.g. wanted `popcnt` as well...
Ah, true. Mixed up l/rvalue references with the value categories.
Yes. Maybe an "official" extension for a simple 2D GUI interface should be launched, but not standardized.
If the functions don't rely on member data and is only needed in the implementation of some of the class member functions I wouldn't even make it private. I'd just make it a static free function in the .cpp file implementing the member function that requires it.
I think the reason people including myself think the memcpy technique is allowed in c++ (I have no knowledge about the lifetime model in C) is because you are copying the bytes into an already constructed object - hence you are not actually punning the type of any objet.
This is about the C standard. Not the C++ one
That's definitely an option. The issue is that without a low-level graphics library in std, we'll remain in a https://xkcd.com/927/ situation where that's just one more competing standard. The benefit of P0267 was that it would give a cross-platform default option that works for most of us. The drawback is that it's extremely unlikely P0267 would have worked for most of us. A lot of the motivation for P0267 is explained in P0669 (http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/p0669r0.pdf) and most of the responses to the "nobody's going to use it" questions are centered around the usefulness of P0267 for teaching computer graphics in universities. That's a laudable goal, but P0669 does not address the fundamental question: does it need to be in `std`. I didn't know about P1062 (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1062r0.html) until I read this post, but ultimately I agree with it. They do suggest something similar to what you're suggesting: write it as a lib and ship it, but they recognize that C++'s lack of proper package management is an issue there and urge the committee to address it. I'm a bit sad because I had hopes that the Graphics track would find a sweet spot to give a common foundation for graphics library that isn't just another graphics library, but maybe that was a pipe dream. 
&gt; // move assignment operator &gt; Blob&amp; operator = ( &gt; Blob&amp;&amp; other) // notice it is not const, move requires this &gt; { &gt; std::cout &lt;&lt; " move assignment" &lt;&lt; std::endl; &gt; this-&gt;size_ = other.size_; &gt; this-&gt;data_ = other.data_; &gt; other.size_ = 0; &gt; other.data_ = nullptr; &gt; return *this; &gt; } This leaks memory.
Everytime I see if (ptr) delete ptr I just want to kill myself.
Can you be a bitore specific, what kind of code you'd like to allow? You end up very quickly in a situation, where your compiler can no longer do any kind of TBAA. In case this is not clear: Alias analysis is used by the compiler to prove that a write through e.g. one pointer can't change the value he is about to read through another pointer, so it can reorder them or chache a previous read. Now there is a lot of debate whether TBAA is worth the hassle (msvc doesn't do it at all), but it is not like those restrictions on type punning come out of nowhere.
There is the concept of de-facto standard and while it is not a great situation, it is often more important for compatibility to adhere to de-facto standards than to actual standards (doesn't mean Linus cares about either of them).
If I'm not mistaken, [P0553][https://wg21.link/P0553] might still make it in time for C++20.
I don't think this really works as a proposal without C++ gaining some kind of proper package manager / build system. It definitely shouldn't be part of the language standard. C++ isn't Python.
Move back !
&gt; E.g. wanted popcnt as well... Adding ```popcnt``` to the standard (STL) is more difficult because some CPUs have a hardware POPCNT instruction while other CPUs don't. For this reason an STL popcnt implementation would need to check at runtime whether the CPU has the POPCNT instruction before using it. Unfortunately this runtime check causes a significant slowdown which is not acceptable for many projects and so these projects would still use the popcnt compiler intrinsic or assembly language instead of the popcnt function from the STL (to avoid the runtime check). Personally I think that popcnt should remain a compiler intrinsic (instead of being added to the STL). But it would be awesome if the compiler intrinsics would be standardized in C++ so that you could use the same popcnt intrinsic using all C++ compilers. In order to facilitate programming with compiler intrinsics it would also be nice if the C++ standard supported checking for CPU instruction set extensions (e.g. ```hasPOPCNT()``` on x86) so that we don't need to mess with ```CPUID``` anymore. The Rust programming language did this, they standardized the compiler intrinsics [std::intrinsics](https://doc.rust-lang.org/std/intrinsics/) and I think that is a good decision.
So what's actually the correct way of type punning? Casting?
Unfortunately the only proper way is a memcpy. As a side note, I personally think type punning via unions shouldn't have been disallowed in C++.
Totally agree. I call them "the two pages programmers". Ther goal is mainly to write blog and code MUST be short. The readibility does not count for them. The absence of readibility can even be a good point because they can write some blogs on them
Looks really easy to use. Does it handle ssl? Does it make async calls? Hope to try it this week.
i don't really get this library tbh. it seems you are trying to make simple things simpler, by adding more abstraction. take for instance non-negativity. imho you lose a lot of beauty compared to the language-independent if(x&gt;=0){...} look how concise that is. best of all: it doesn't need a library and everyone can read it. also what's up with the sorted vector wrapper? if i want a sorted data structure, i have choices already. 
Any example? You memcopy to a different variable?
&gt; Can you be a bitore specific, what kind of code you'd like to allow? Literally just reinterpret a bit pattern as a different type. A typical usage for this for example when reading/writing multi-byte data with different byte order from native: there are functions that do the byteswapping on integer types, but there are none that do it on floating-point type, so a typical way to read byte-swapped floats is to do something like: `union punner { uint32_t i; float f } p; p.i = be32toh(source32); return p.f`. A lot of manipulation of floating-point numbers can be achieved more easily and faster working on their bit representation than on the floating-point value itself (fast inverse square root anyone?) Another use cases is fixed-point implementations, where there are cases when you want to look at the whole thing as a scaled value, and others when you want to manipulate separately the integer and fractional parts. In other cases you may have have hardware where certain types have preferential treatment (e.g. textures on GPU), so there are methods to e.g. access types like `struct int4 { int32_t x, y, z, w};` in an efficient manner, but the type you actually need is `struct customtype { int32_t v1, v2; int64_t v3 };`, so what you do is treat the thing as an `int4` everywhere except when you need `v3`, in which case you _reinterpret_ the same variable as a `customtype`. There are heaps of cases where using type punning is simply the most efficient way to do things. Now, if you want to be standard compliant, you _must_ do it with a `memcpy`, and that's completely ridiculous, because there should be absolutely no need to rely on the standard library for something that can be done natively (and no, ‚Äúthe compiler will optimize it away anyway‚Äù is not a good answer, because if the compiler will optimize it away anyway, why is there a requirement for the `memcpy` in the first place? _and_ there are context such as GPU programming where the standard library is simply not available at all). Heck, with the present situation in C++ there is no standard-compliant way to do compile-time endianness detection, which should be trivial to do. Type punning is so important for performance that OpenCL C not only explicitly has support for it via unions (like C99 does), but it even provides a set of functions _specifically_ designed for it (the `as_&lt;type&gt;()` functions). &gt; You end up very quickly in a situation, where your compiler can no longer do any kind of TBAA. I disagree. To get to the point where the compiler cannot do alias analysis you would have to go through _other_ layers of undefined behavior. For example, you might have the `punner` type above and then a function `void dosomething(uint32_t *input_int, float *input_float)` and if you use it as `something(&amp;p.i, &amp;p.f)` you'll be violating the strict aliasing rule _in the scope of the function_, and that's really no different than passing two pointers to overlapping memory areas to `restrict` function parameters (in C; e.g. using `memcpy` instead of `memmove`). &gt; Now there is a lot of debate whether TBAA is worth the hassle (msvc doesn't do it at all), but it is not like those restrictions on type punning come out of nowhere. Except for the part where union-based type-punning is actually part of C99 (and the ‚Äúunspecified behavior‚Äù in that standard was actually further clarified in the C11 standard), so compilers _theoretically_ already should be able to handle this. Honestly, IMO the real issue with it in C++ is the potential complexity associated with data layout for arbitrary types, but there is absolutely no reason why it shouldn't be supported with standard layout type. And in fact, I would argue that there is no reason to support it explicitly via casts instead of having to go through unions: we could extend `reinterpret_cast` for it, _at the very least_ for types themselves if not for their pointers (as in: allow `reinterpret_cast&lt;float&gt;(int32_t)`). 
I feel like I'm not in the loop enough to have a great sense of where everyone as coming from, but I took a look at this and [P0973R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0973r0.pdf), linked early on ("Coroutines TS use cases and design issues", Mar 23, by two of the three authors on this). One of the things *that* paper keeps harping on is the fact that coroutines *could* provide a nice syntax for functions with `expected` returns, and that the TS doesn't provide a good transition path: &gt; In practice, we have been unable to prevent users from using macros to abbreviate [getting an `expected`, testing for error, and returning the error]: ... Perhaps more problematically, [the requirement to use either `co_return` or `return`] makes it substantially more difficult to migrate existing macro-based error handling code to use co_await instead, because we must introduce co_await and eliminate return in a single atomic step. But then in this followup work, they propose a solution that basically forces a completely different and uses-multiple-new-language-proposal features for defining a new *function*? How the heck is *that* supposed to provide a remotely reasonable transition path forward? And then any function where you want to deal with `expected` you have to write using this new syntax that makes it look like every other function ever written in C++? Actually I'm not sure how confidently I say that, because I'm not sure the paper actually ever provides an example of what such a function looks like in their suggested most-terse form. (The close they seem to get is a lambda.) On the other hand, that very fact seems somewhat telling to me...
There are channels for users to complain about the standard, but complaining "into the void" isn't really one of them. In any case, Linux does not use C, it uses gcc-C, mainly because from a pragmatic point-of-view kernel devs only need to complain to gcc developers. 
The only real issue I see is that the API is in C and thus sucks from a modern C++-perspective. It would however still be better to have that than nothing (AKA: what we have now).
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8q85ga/question_help_with_starting_a_project/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Great list, I would add 3 points : * Don't be afraid of the lack of garbage collector. You don't need a garbage collector when you don't produce garbage. * you don't need to think about memory management anymore (really). Instead, you need to think about lifetime and responsibility. Until when my object should live? Who's responsible for cleaning up the mess. * Do not use exception to control the flow of your program (like in python). Exceptions are raised in exceptional situations (and that should stay the way to go). Welcome on board :)
Yeah, exactly. Given two pointers float *a and int *b, you need to do memcpy(b, a, sizeof(float)) to alias a as integer (assuming they point to valid memory).
Can pointers be the same? Is it possible eg to treat 32-bit int as 4 chars without creating unnecessary copies?
&gt; The benefit of P0267 was that it would give a cross-platform default option that works for most of us. Any API that allows to write pixels to screen directly is everything but a "cross-platform default option that works for most of us". It's maybe good for some teaching assistant, but no relevant drawing lib works like this as of today since it's so damn inefficient.
Why should there be? Symmetry shouldn't become and obsession. At some point people just need to drop their OCD. It's not healthy.
I'm sad for the humans you put the effort into writing this library, but happy for the c\+\+ language. A good standard package manager would be much better than adding stuff that \*\*will\*\* eventually be deprecated in not a long time (unless algorithm, containers, ‚Ä¶). C\+\+ is already a big language, it doesn't need to grow with those kind of proposals.
&gt; A lot of the motivation for P0267 is explained in P0669 (http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/p0669r0.pdf) and most of the responses to the "nobody's going to use it" questions are centered around the usefulness of P0267 for teaching computer graphics in universities. I've been involved in a few university-level graphics courses at different institutes, and I'm a bit surprised at this. I only know of one course that spent any amount of time on 2D graphics, and that part revolved mostly around drawing lines and maybe blitting regions, plus a few other very fundamental methods (the course was discontinued around 2008). All the other courses revolved heavily around 3D graphics, and used OpenGL (in its various incarnations) pretty much from the get-go. The content of the courses varied quite a bit (focus on theory, fundamental rendering methods, common 3D techniques, or even on more-or-less state-of-the-art real-time algorithms). Either way, I never really saw how any of the proposed graphics libraries would fit into the courses that I'm familiar with. There's another problem - all courses I've been involved in use C++, but in none of the instances had the students gotten a prior introduction to C++. So the graphics courses typically had to set aside a bit of precious time to give a "crash-course" in C++-survival ... and subsequently limit the amount of "advanced" C++ that the students would encounter. While that's somewhat of a more fundamental problem IMO, moving to a more C++-centric library would be a hard sell in the face of that.
Char is a bit special. You can reinterpret_cast every type to a (const) char* and back again.
\&gt; Report it as a bug to third\-party library authors if a library does not support clients to use CMake. CMake dominates the industry. It‚Äôs a problem if a library author does not support CMake. This is not nice. It is not a bug if a library author does not support CMake. Library authors are free to use any build system they want, it is their choice, they are putting time and effort and releasing something to the public for free. It is good trying to collaborate with them, suggest contributing such cmake scripts and help maintaining them. And even if they don't want to have any CMake code in their repos, it is their choice. I strongly favor CMake, I am happy every time I find a CMakeLists.txt in a repo I want to use. But still, this is not a bug, and it will never be (unless the C and C\+\+ international standard committes standardize CMake as the build system... :) ). Respect library authors.
who says a pointer has to be unsigned? There are architectures with signed addresses. The transputer by inmos, for example.
So can you give me a full example of it? Somethng like `float read_as_float(const int&amp; x)`?
Yes - library authors are free to do whatever they wish (me being one), but I mostly agree with what is written in that gist. Perhaps an "issue" would be a better word than a "bug" - that could be the start of a contribution adding CMake support - or at least an indicator that someone feels there is a need for it. I also think sometimes strong sentences like that are there to make a point - CMake has really turned into an industry-standard.
Sure float read_as_float(const int x) { float t = 0; memcpy(&amp;t, &amp;x, std::min(sizeof(int), sizeof(float))); return t; }
Can things like this be optimized? Like a I have an int and use such function - will it get inlined to nothing or make an actual copy of the variable?
That depends on your compiler and your optimization level, but GCC will usually optimize out such punning; the CPU itself doesn't care, so the generated assembly doesn't contain the punning.
Yes, it is useful to remind yourself from time to time that unless you are paying them, authors of open source projects *do not owe you anything*.
Perhaps it would be better to suggest contributing the CMake support instead of just opening a bug. Those kinds of things are better accepted when you put your own effort in it.
All what you describe, is somewhat ok, for virtual methods with default implementation. What if base private virtual = 0? Like in NVI/template pattern? You can expose it by overriding as public. It can **easily** be done accidentally, by putting it in public section. And you may not notice that. Thous breaking design encapsulation. True, it will only be exposed in derived class. But intention of template pattern is to hide implementation. By allowing **instantly** to change visibility of virtual, compiler does not help us here. Though it may seems just like a little bits, in large codebase checking/enforcing implementation visibility for template pattern, may be tiresome to do manually. I found using passkey idiom helpful in enforcing encapsulation, as I wrote previously: [https://www.reddit.com/r/cpp/comments/8p1s70/overriding\_private\_virtual\_method\_as\_public/e0840k1](https://www.reddit.com/r/cpp/comments/8p1s70/overriding_private_virtual_method_as_public/e0840k1) . Even if you do accidentally override it as public, you can't call it. Actually, this is even more stricter then NVI \- you can't call implementation virtual method even from derived class, that implements it! Thus enforcing to using that virtual method as interface only.
I don't follow the Linux development or LT. Does he actually care about clang at all?
I recently migrated one of my personal projects to CMake from plain make files. I had never used CMake before and was migrating the project specifically to learn the tool. All the guides I read pretty much did everything that this post tells you not to do. However this guide tells you "not to do X" but doesn't tell you how you *should* be doing it. For example telling me "Get your hands off `CMAKE_CXX_FLAGS`" is all well and good. The reasoning given makes sense but how am I meant to defined the compile flags instead? If I google it every guide uses `set(CMAKE_CXX_FLAGS...)`
This reminds me of the mess called systemd which pressured it's way into distributions by opening tons of bugs against all sorts of libraries for not supporting it. It's despicable and I hope 'cmakers' won't be so pathetic.
I'm on ubuntu, but from what I've seen libraries like opencv are pure hell on windows that's why most devs use linux, right? Not to start a war or anything. :)
I would add one more advantage to variant version \- you can use template functions. Useful with something like ranges.
&gt; (unless the C and C++ international standard committes standardize CMake as the build system... :) ). why would this be needed ? Other languages communities without international standard commitees have no problem reporting bugs to libraries if the libraries don't use a commonly used package management system in their language ecosystem.
&gt; how am I meant to defined the compile flags instead? target_compile_options(mylib {PUBLIC or PRIVATE} -fmybuild-flag) however I would say that generally setting build flags is a bad idea; most aren't portable and you quickly end up in a mess of if(MSVC) else if(NOT APPLE AND CLANG and CLANG VERSION &gt; 3.7) else if(NOT APPLE AND CLANG and CLANG VERSION &gt; 4) else if(APPLE AND CLANG AND...) ... endif() because every compiler version will have subtle bugs relating to such flags. 
There‚Äôs a bug in your comment: it isn‚Äôt written in French. Whenever you write a comment in English, please also write it in French as well. French is really an international standard. It‚Äôs the official that the EU government operates in which covers almost 30 countries. I‚Äôm being facetious, of course, but I hope my point is clear.
So lets agree to disagree ))
https://codingnest.com/basic-cmake/ And part 2 The reality is that even CMake "gurus" don't quite agree on specific details, and there is always question of purity versus pragmatism. I favour pragmatism for my projects (couple of dependencies, building on the big 3 platforms using big 3 compilers), but I can see why e.g. the Bloomberg guys have much stronger preference for super clean cmake files, to save themselves from going mad :-)
&gt; unless the C and C++ international standard committes standardize CMake as the build system... :) ) Please no, and I say that as a person who has properly installable and includable CMake build on his projects.
English is, ironically, the lingua franca of the programming world. I'd wager there are many projects using French/Russian/Portuguese/Chinese/etc. naming conventions where people have suggested rewriting in English to reach a larger audience. But I'd also wager in most cases that the request will go nowhere because they're deliberately avoiding English for their own reasons. Is there a parallel with CMake? *shrug*
Obamurri might be too harsh but he is right that users who want to use networking lib now probably don't mind the executors bit much. AFAIK executors are needed for aligning the implementation of networking lib with other std lib parts and it doesn't need to be available right now as demonstrated in boost.asio. So why is it being blocked? Is it expected the networking API will change significantly because of executors? Why is not possible to hide executors under current post, defer api?
Yeah right...
Well portable or not, you still have to set them somehow and repeating them for each target isn't feasible or a good idea for a large project. AFAIK there isn't yet a good mechanism 
&gt; If I'm not mistaken, P0553 might still make it in time for C++20. Personally I would prefer a solution similar to Rust: * Standardize the compiler intrinsics. * Provide functionality to check for CPU instruction set extensions in standard C++ (so that we don't need to mess with ```CPUID``` anymore). As far as I can tell the [P0553R2](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0553r2.html) proposal (&lt;bit&gt; header) does not have a good solution for hardware instructions that do not exist on all CPUs like e.g. POPCNT on x86. It seems to me that if I want my code to be portable across all x86 CPUs I would need to check using ```CPUID``` if the CPU supports POPCNT before using ```std::popcount```?! This does not seem like a good solution to me. Maybe the ```CPUID``` check could be integrated into the ```std::popcount``` function but I don't like this solution either because that check would significantly hurt performance and I guess many people would continue using the POPCNT compiler intrinsic to avoid that runtime check.
The goal of those papers is to use the compiler intrinsics when available and to fall back to another algorithm otherwise, which is done through a compile-time check (which most likely depends on a -march compiler flag or equivalent). That's basically what many code bases already do.
You know, this is all well and good, but can you please provide examples as well: If I shouldn't use X, you better come and say : Use Y and here's an example of proper usage of Y. 
There's always the big hammer of `add_compile_options()` but it's very common for large projects to have a central utility/core library that every other component links against. All it takes is setting `target_compile_options(&lt;that central library&gt; PUBLIC &lt;all your important flags&gt;)`.
@tcanens that's not true, we have found such types in real code, with an average frequency of one such type per 3 MLoC. And we broke them :)
`Shuffled&lt;T&gt;` Unlike the rest of them, this one doesn't make a ton of sense to me. Being shuffled isn't an inherent property of the data, it's property of your knowledge about the data. It's only shuffled until you look at it, after which it's not. That said, for the general approach I've always found this to be more trouble than it's worth. I've done things like this before for a NotNaN type and it was never clear that the additional typing and cognitive overhead was worth the trouble. Additionally, MSVC at the time would generate bad code when small/single member structs were passed as arguments to functions. This is probably fixed by now (it was over a year ago), but at the time it was really annoying. (Worth noting, for my case peppering `__forceinline` on affected functions ended up giving a 30% speedup as a result -- it was only an issue when the function wasn't inlined).
Of course I was joking. I don't want it to happen, and I am pretty confident it will never happen.
Something tells me some of the schools that still use Turbo C++ do still use this.
Cmake is an abomination and must be abolished asap. 
&gt; "if all you have is a hammer, everything looks like a nail" Makes sense to use each programming language to their strengths, no? With Python being a higher level language it hides a lot of grisly details like memory allocations and layouts, but the price you pay is the speed. Something like a bot using praw wouldn't need that speed anyway since it's not exactly CPU/IO bound so it makes sense to use Python here, right? Use C++ when you benefit from the static typing and the performance, with things like databases, games, graphics processing etc etc. And even then, you can use the performance critical C++ code as a python module (see "[Extending](https://docs.python.org/3/extending/index.html)" in the python manual), which gets you the best of both worlds.
The reason Python exists is to do more in less time with less code than it takes with something like C++. That isn't a huge secret. Of course you lose performance going with Python over C++ but that is often an acceptable trade-off, especially with internet based applications where by far the slowest aspect will be the network. C++ does not have a built in UI toolkit so any kind of graphical application will mean using a third party library. If you want to target multiple platforms you can do with something like Qt. If you are only targeting Windows then yes you will use the Windows API (Win32, MFC). And yes C# is an alternative to C++ and one which many people go with these days when you don't need the on-the-metal performance of C++. 
I work on software that is 400+ Visual Studio projects. I keep reading to &gt;Forget the commands add_compiler_options, include_directories, link_directories, link_libraries. If I am to forget these blanket commands, how in the world am I to maintain the same compiler options across all projects? Unless my scenario would be considered an exception to the rule?
So how do you define this behavior? For example, do both types have to be equally sized? If not, what do you do about uninitialized bytes? If yes, does the padding between both types have to match? Independently of whether the answer is yes or no: the std doesn't talk about padding, and if the padding bytes do not match you might get uninitialized bytes from one type into another and you are again in the "what do you do with uninitialized bytes territory".
&gt; This feel impure It shouldn't. As you said, C++ is not a platform, when you choose to do something in C++ you also have to choose a platform, or build it yourself. Having to choose between Qt, the windows API or whatever is part of the language.
Indeed, RAII is central to the language. I probably should have made that an explicit bullet point instead of just mentioning it. &gt;Do not use exception to control the flow of your program (like one would in python). I would argue not to do it even in Python... :)
OK, then this is like standardizing the compiler instrinsics which is a good solution.
It's fantastic that for the first time we have something that works and has become the industry norm. I'm not going back to SCons or autotools or VS solutions
&gt; So how do you define this behavior? For union-based type punning, exactly as it is in C99, or even better C11 which clarifies this further. This includes provisions for the fact that padding bytes have _unspecified_ values. `reinterpret_cast` would only make sense for types with the same size (and possibly alignment), so there even wouldn't be an issue with that. &gt; Or in other words: if you want to make the behavior here defined you will soon realize that 1) you had to add some restrictions, and 2) you need to also define things like the layout of all types (which is left unspecified to allow implementations to optimize layout). No, the standard doesn't need to make the behavior defined, it can keep everything implementation-defined, except for the fundamental principle, which as stated in C11 is: &gt; If the member used to read the contents of a union object is not the same as the member last used to store a value in the object, the appropriate part of the object representation of the value is reinterpreted as an object representation in the new type as described in 6.2.6 (a process sometimes called ‚Äò‚Äòtype punning‚Äô‚Äô). This might be a trap representation. Now if the issue you want to raise is that C++ does not have the concept of trap representation if not for floating-point types, that's _again_ something that is just as simple to extend.
No, but Clang cares about gcc compatibility, aiming to be adrop-in replacement for gcc.
Can they be programmed with std-complying C++?
It's not fantastic because it does not work as well as it should have been. It's mediocre at best. 
That's what I'm trying to find out as well!!!
Because a lack of symmetry makes in non-obvious what's going on and why. 'Placement new' was a terrible concept in the first place because 'new' implies the creation of an object whereas we are merely initializing it. I don't see why either placement delete or new keywords/functions for the concept would be problematic.
The ENIAC possibly to, and don't forget the Babbage Analytical Machine of course. Check your calendar.
It was actually proposed in SG15 already
&gt; ... which **would** mean it does not compile ... Possibly to many negations here. I added emphasis now, yes it compiles, you're on your own. 
Good point. There probably isn‚Äôt a parallel with cmake, I just disagreed with the spirit of the comment I was initially replying to.
Linus Torvalds, the creator of both Linux and git, just complains without doing anything. Ok.
What are some of these things that even the CMake "gurus" don't quite agree on? I would be curious. I looked at both and I can see many consistencies but I am probably not knowledgeable enough to see the differences and their pros/cons. It would certainly help an "intermediate-level cmake person" to know about these differences and the "why's".
I don't think so. At least I haven't seen a paper proposing such a thing. And, FWIW, at the meeting in Rapperswil there was a pretty strong sentiment against this idea.
I like bit_cast because it adds safety. You can not cast to/from non-trivial types and you can be sure that the types do match (which they might not do in your example).
&gt; if(x&gt;=0){...} The question is what to do in the else case. What if x is passed to another function that also requires it to be &gt;= 0? What if there's 10 more such functions or classes that hold x? Will you check for &gt;= 0 everywhere? Will you throw if not? Who handles the exception? Sorted can be used for more containers than just vector. Also the properties of a map might be worse for an application than using a sorted vector. What if other code requires a vector? Sure, there's cases where it's not required, but there's also cases where it's way nicer to have that option. 
It's certainly sad and unfortunate that so many hours have gone into this and it's not being pursued. But it feels to me like the people involved in these graphics proposals did not listen to the many objections that were given because these objections/conflicting opinions were given in forum posts (of the 2D graphics SG), /r/cpp, etc., and not as official papers. So this seemed reason enough for the people to ignore these voices ("If you have an objection, you have to submit a paper - otherwise, be quiet and move on"). Then these voices got louder and louder until they could finally no longer be ignored. This is at least what seems to could possibly have happened from an outside perspective. Is there some truth to that? If yes then there's certainly the procedure to blame to some extent (we've read this a lot here recently, committee process needs to be more open to non-paper comments, more like the Python process, etc.). But also the people championing this/these proposals seemed not to have listened much to "the opinion of the internet", or much too late.
I figure it's worth a try, especially if it comes with a pull request, but I wouldn't expect many people to welcome the offer, especially if it's presented like the gist: "CMake dominates the industry. It‚Äôs a problem that your library does not support CMake." 
I raised this [same question](https://www.reddit.com/r/cmake/comments/8p43q0/questiondo_people_really_like_cmake/e09k8dq) in a recent /r/cmake thread. It leaves me very confused, because I feel like maybe I am missing something and there is a better mechanism for handling global setup. Or maybe you're right and this oft\-repeated advice just doesn't apply to large projects.
I also think that having this functionality as the standard would push hardware vendors to implement the functionality in instructions.
Thanks for catching that.
Library authors might be interested in adoption. So it makes sense to file a ticket from the perspective of a user who would like to adopt a library, to comply with CMake build system. It shouldn't cost a lot. Perhaps it's not a bug-report but rather an feature request.
&gt; you still have to set them somehow do you ? which flags are you thinking of ? I'd wager that a large majority of them would already be abstract in a cross-platform way by cmake: -fvisibility, -std=whatever, -pthread, etc etc...
:-)
well, the problem is that 70's machine are still in use today in big banks, insurance companies, etc. IBM did some active lobbying at multiple countries's national standard commitees so that the [EBCDIC](https://en.wikipedia.org/wiki/EBCDIC), almost 50 years aldo, encoding would still be supported by the newer C++ standards - thankfully it did not pass.
This seems like a summary of the sources cited, notably [Pfeifer's talk](https://www.youtube.com/watch?v=bsXLMQ6WgIk). In my opinion the YouTube talk is easier to follow because it provides examples to explain some of these rules. 
You could as well round up your question more, but my impression having seen some landscape is that C++ looks faster at run-time, and slower at development-time for given tasks. One needs to think about which one you care more about.
Both Linux and git was born from a frustration with what existed at the time. In both cases Linus' solution was to create something new from scratch instead of improving what already existed. I'm sure if we all worked like that, the world would be a much better place.
No problem. Here's another one: // default constructor Blob( const std::size_t size) : size_(size) , data_(new uint8_t[this-&gt;size_]) { std::cout &lt;&lt; " argument constructor" &lt;&lt; std::endl; } You probably want to mark this constructor explicit, unless you actually want this code to compile: Blob x = 10;
&gt; look how concise that is. best of all: it doesn't need a library and everyone can read it. As I understand it, it‚Äôs conceptually similar to the CppCoreGuidelines recommendation to use ‚Äònonnull‚Äô
In my opinion the thing that C\+\+ is lacking (though it is getting better) is a bunch of the useful library stuff baked into the standards of other languages... Here's some of the stuff I have in my standard library: light actor model implementation, low level bit manipulation stuff, dynamic library support, exception types (includes stack traces), raii files, file locks, functional programming utils, json support (just use nlohmann's json), logging, lru\_cache, md5 &amp; sha hashes, memory maps, nullable type, path \+ filesystem stuff, process support, generic server support, sockets \+ ssl sockets, basic statistical tools, timer, udp sockets, uuids, variants. I also have an HTTP implementation that gets included with nearly everything these days... Anyhow with all that stuff I feel super productive...
Thanks, also a good catch.
I don't know if it's better than using the global flags or not but I'm trying to have a base, empty library (with no sources, or one with a stub cpp so CMake doesn't complain) that I link to every other library in the project. Then I add flags, link libraries, add include directories etc. PUBLICly to that library so all the dependent targets get those.
Linus doesn't really use C++; it sounds like his goal was really to argue a point, here, for the sake of a kernel commit. That ultimately amounts to something more meaningful than mere complaining. The post isn't even really about the C++ standard, but the C standard, which he explicitly states is garbage because by his belief language standards *in general* are garbage; he dislikes the lack of specifics that are left up to implementations.
&gt;Actually I'm not sure how confidently I say that, because I'm not sure the paper actually ever provides an example of what such an expected-returning function looks like in their suggested most-terse form. (The close they seem to get is a lambda, on page 15.) On the other hand, that very fact seems somewhat telling to me... This paper proposes making the _only_ way to create a coroutine being using lambda syntax, FYI.
I don't necessarily disagree with you, but there are lots of things in the standard library that have become outdated, so that alone is not a reason.
My guess (didn't do any research on it) is that this (write a paper o be quiet) mainly happened towards the end when the graphics proposal was already well on its way. There is a lot to be said, about the effectiveness of the current standardization process as a whole, but a fundamental problem is always, that people only start to complain when things get real and by then it is often not really feasible / extremely expensive to make fundamental course corrections.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8qb6l5/how_do_you_learn_c_in_2018/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
This is pretty much at the same level as a youtube comment that states the classic "to all the haters who downvotes this: can you sing better?"
Thanks for the confirmation. I probably could have been clearer in what I wrote, but I did realize that the standalone function coroutines were written in terms of a lambda (e.g. the example on the bottom of p15).
Should targets propagate its requirement, or should C++ standard setting be left to the end user with compile error if he picks wrong? Should compiler flags be set strictly in toolchain files, even though that means a crazy rat nest of logic if you have multiple ones, or should you break the purity a bit and set "reasonable" flags inside the CMakeLists itself? etc
Should targets propagate its requirement, or should C++ standard setting be left to the end user with compile error if he picks wrong? Should compiler flags be set strictly in toolchain files, even though that means a crazy rat nest of logic if you have multiple ones, or should you break the purity a bit and set "reasonable" flags inside the CMakeLists itself? etc
I was talking about the (not well received) suggestion on SG15 mailing list back in February.
&gt; Can't you just continue your work as a library. That's reasonable, if the library is of sufficient utility many people will adopt it anyhow, this would justify considering it for inclusion at a later date. And if you can't convince many people to use it, then it's not worth forcing it on people via standardization either. 
A lot of that already in boost library which is a first dependency for any non-baremetal c++ project.
I don't know about Linus, but in my own projects, I've been using -fno-strict-aliasing, otherwise, Bad Things happen. I've been trying to improve on this for a while in the past, but at some point I kind of stopped bothering. The most natural way to do this has been declared UB by the standard, and I think I'll just keep using -fno-strict-aliasing. There's really nothing more natural and "elegant" than a simple cast and assignment, IMO.
There will be several proposals regarding Networking in San Diego. At least one will propose decoupling Networking from Executors in some way. That would seem reasonable, IMO.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8qc17x/need_help_understanding_this/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I'm hopeful that `[[assert:]]` will be the future and replace the abomination that is this naming-convention-violating-macro, but we will naturally have to wait for a few more years before it is finally standardized.
I would say yes. This one is more flexible in several ways: - It can invoke its own handler given to the compiler when building. (The proposal does give motivation for why this shouldn't be replaceable at runtime.) - It can specify a checking level (default, audit, axiom). - It's separate from other behavioural changes that happen based on `NDEBUG`. In a contracts world, we should be using the corresponding `[[expects: ]]` and `[[ensures: ]]` to state our assumptions. The new assert syntax fits right into this. One thing I haven't seen, however, is the ability to attach a custom message in a more logical way than with `assert`. To me, this feels like a worthy future addition.
OK - here is why contracts are a bad idea for C++. I haven't looked at the specific contracts proposal. I'm commenting on contracts in the context of C++ in general. Contracts are assertions inserted into code which verify that the assumptions made by the program are in fact true as the code is executed. These have been classified as preconditions, post conditions and invariants in line with original ideas regarding proofs of program correctness. This code can be a significant execution cost. So implementation of contracts in C++ has generally taken the form of code which is only executed when the program is built in debug mode. Many languages/libraries implement them (e.g. MFC which I'm familiar with) in exactly this manner. They are almost never used by programmers. I'm guessing that this is due to a couple of issues: * They are extra work to write in every function * They slow down execution way to much * when a function is refactored - divided, the work and run time cost is doubled. * they are only invoked for debug builds. * they are no help once the program is fixed. So, if not contracts what? We not have a language with very strong typing rules - C++. This was not widely available until the advent of C++. It certainly wasn't considered when the ideas behind contracts where being originally being promoted. With the current typing facilities of C++, we can now build types which enforce invariants at compile time. This takes some effort, but less than contracts. These types have zero runtime overhead. This idea is slowly growing. I've made presentation at CPPCon 2017 https://www.youtube.com/watch?v=632a-DMM5J0 about this idea. Also Ben Dean has made some well received videos which relate to this. And of course there is the idea of "Concepts" (which I prefer to "type requirements") is related to this. As is my latest project, safe numerics (see http://blincubator.com/bi_library/safe-numerics/?gform_post_id=426) , which I'm trying to finish. To summarize, my view is that we already have much better facilities to help enforce program correctness than contracts. Including them in C++ at this point is a bad idea which will set back the progress of C++. Robert Ramey
It was doomed to failure from the start, this was the permanent impression I got years back when I saw the first Herb presentation over this.
I guess this is the proposal you're referring to: http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/p0542r0.html
Python and Java have lots of libraries built-in. C++ doesn't by default. You can add libraries to C++, piece by piece with librul, libuv, etc, or you can use a full featured library framework like Qt or .NET. With Qt C++, developing praw would go about as fast as Python.
About your concerns with Conan and the need of modifying the CMakeLists, this blog post could be interesting for you: https://blog.conan.io/2018/06/11/Transparent-CMake-Integration.html
Anyone got an ELI5/TLDR? This presumably has something to do with Contracts and is likely coming in C++20, but nothing that would be in C++17, correct? So that question is a bit premature I think. Even the most modern code base would be C++17 at that point, but not use an experimental/upcoming feature that's implemented in one or two compilers at best.
I'm not sure C++ was popular at the time, and the transputer probably wasn't popular enough for someone to write one anyway, but I'm pretty sure that nothing in the standard prohibits pointers from being signed. The relevant excerpt is basic.compound paragraph 3, which states that *"The value representation of pointer types is implementation-defined."*. Actually, I think technically this doesn't even stop you from having signed pointers for ints and unsigned pointers for std::strings &gt;:)
Cool, thank you very much for these examples!
There's a couple threads on the graphics SG mailing list, I can't be bothered to find these links again but they were linked here several times in the past. I'd be curious if these threads were already opened "too late", or whether they were just "ignored" for too long.
I know what you mean. Two pages of templates, lambdas, and algorithms, and in the end they have added two integers together. But hey, they have done it in a way that's \_much\_ cleverer than any of us mere mortals do it!
That would be a good rationale, however I think [basic.types] makes it very unclear whether it is legal. Because it explicitly allows it for same (trivial) types, but says nothing about different types. In which case the usual interpretation (short of anything else allowing it) is to consider that the unspoken cases are illegal and UB. It might be that the 4. makes it legal, especially given the note "45. The intent is that the memory model of C ++ is compatible with that of ISO/IEC 9899 Programming Language C", but I'm not 100% sure. Edit: thinking more about it; also memcpy is supposed to be the same as the C one, and C seems to C allows type punning with it, so maybe we can consider it imports this property at least on C compatible types? 
Based on the article, this sounds awesome. I'm tempted to read it and see what sort of audience it caters to.
The way I see it, the question's context is all major compilers supporting contracts. It appears to be from a design point of view more than a technical one. That is to say you run the risk of incompatibility for any new feature you use, but that doesn't help with why you specifically would or wouldn't use the new contracts facilities. By the way, contracts were merged into the C++20 WD in the last week, so it's very likely this will be standard.
Well, once you have "learned" it using those better ways, for advanced purposes you need to relearn it from the definition. Because honestly the better ways barely scratch the surface of all the details, and details matter strongly. Especially in C++, where details sometimes (often?) involve traps full of nasal daemons.
That a weird thing to say, given dev during the pre standard era were non negligible and with sometimes really big changes (introduction of templates...), the 98-&gt;11 transition was astonishing, 11-&gt;14-&gt;17 makes the language finally somehow reach the level of the competition for moderately abstract programming (modulo poor syntax and UB madness everywhere and too little help from the compiler to get any *serious* guarantee of avoiding them), and now dev seems to continue on track with the 3 years evolution rhythm. 
Using URLs like http://wg21.link/p0542 always gets you to the latest revision of papers (in this case revision 4).
My explanation was independent of [basic.types]. Afaik, memcpy treats it's operants as if they were char arrays, which allows it to write to them. It just copies the bitpattern it finds in object a of type A byte by byte into object b of type B. As long as that bitpattern is valid representation of an object of type B (probably B also has to be trivially copyable) everything should be fine. Would be nice however, if the standard could be clearer on such things.
&gt;**Avoid custom variables in the arguments of project commands.** I mean for simple cases I agree, but if you have to aggregate source files over multiple directories for a single library, it's a lot cleaner to do with a variable.
Well, once again that *would* be a good rationale to make the standard says so (if it does not already, because we at least agree it is not clear). But the question is: *does* it says so? Because we have tons of good rationales for why some UB are insane (shift by number of bits of the object comes to mind), yet according to the standard they still are UB...
I had the opportunity to review few chapters of this book. Based on that I can say that it will be greatly helpful for people trying to solve some problem using modern C++ with the help of available libraries. If not help them solve then at least get started with. A quick start guide if you will, covering quite a number of domains.
I think what I said is a logical consequence of what is currently written in the standard (not just of the intention, but the actual wording), but I certainly won't try to search the relevant pieces on my mobile - sry ;). 
You should look at the specific proposal; for the most part you've confused contracts with "std::assert by another name". What you're suggesting is Type Safe programming, and that's something that both Very Good and Completely Orthogonal. The bottom line is **contracts exist**, and are part of the unalterable interface of the function, regardless of how you enforce them. If your code only works on non-empty lists, and you give it an empty list, that code will never work. Of course you could create a type called `NonEmptyList` and a conversion from all STL types that implement list concept, or you could write `[[expects: list.size() &gt; 0]]`. And if you're creating a new type for every contractual obligation between caller/callee, all your arguments for "is extra work" disappear; you're just shifting the work to developing and maintaining a larger type hierarchy. Type Safety when applied to your code base will indeed cut down on the number of pre/post-conditions you would otherwise need to express, but unless your types are extremely fine grained, you're doing to have types with run-time checkable invariants. Both Type Safe system and Contracts make make the contract between caller and callee explicit and enforced, but only Contracts can be turned on or off as needed. In the proposal you have 3 contract levels (default, axiom, and audit) and 3 execution levels (off, default, and audit), so it's not true that "they only run in debug mode" -- when they run is up to you as a designer. While it's true that if you have N methods that all expect non-null inputs, you may want to have all of them declare and check that constraint, whereas if you had a NonNullableType you'd only need to check that once. However there's nothing in the contracts proposal that stops compilers from performing "constraint elision": where the compiler can convince itself the caller expected its input to non-null, and the input is unaltered, then there is no need to re-check non-nullity; in fact that's the first obvious extension of this feature. And lastly you write "they are no help once the program is fixed" which I can't believe any experience programmer would ever state. If you think things "stay fixed" once you uncover one implicit contract mismatch I don't know what to say. At best you describe a good practice everyone should already be doing, which some people might be lazy about and avoid by going "contract mad" -- which would still be better than what they do already, which is pretend no contract exists. At worst you don't seem to understand that contracts exist no matter what, and advocate an extreme practice that's equally destructive to a sane code base.
That would indeed be interesting (as I said, my comment was just a guess). Afaik, Guy also wrote a timeline of the proposal's progression somewhere that could serve as a baseline. 
I strongly encourage proper CMake usage because it is far more likely to work with different compilers and different platforms compared to other tools I've tried; that is the full reason of it. However, *modern* CMake is still insufficient. CMake 3.12, currently unreleased, is slated to [allow linking OBJECT libraries for usage requirements](https://gitlab.kitware.com/cmake/cmake/issues/14778). Every non-trivial project I've seen would want this capability because making shared/static libraries for everything is undesirable. I can't believe how long it's taken for such a feature to land! But, at least it is landing so thank you to everyone who worked on that. Back to the point of modern CMake though: the CMake community has been very negligent with regards to training. There are approximately 2 articles aside from this one I found through Google that teach this sort of thing and they have big gaping holes where novice and even intermediate users will have no idea what's going on. One should not need to be an expert with CMake to write working, mostly-correct CMakeLists.txt files; there needs to be more and better training.
What exactly would be the difference between this "official" extension and any other library out there if it doesn't get standardized?
What exactly is absurd?
That's pretty cool. I could see this format being cool for teaching introductory programming, but it would be very difficult to make it practical.
It's actually R5 that was merged, wg21.link is a bit slow to pick up new papers :)
1. The CPU doesn't need to know where padding bits are, though a specialized masked\-cmpxchg instruction could be told statically by the compiler. 2. That's not relevant: the compiler only needs to normalize padding bits for \`atomic\`, so on store, exchange, and cmpxchg, on the incoming non\-atomic parameters. You don't need to normalize all padding bits for non\-atomics, though you can. BTW it's just "normalize", not zero, all we need is consistency. Your claim of "overhead" will need serious data to back it up: normalization can occur on a stack copy of the non\-atomic parameters (for cmpxchg), or all in register for store and exchange, and that's pretty much free. The alternative is what we have today: totally broken. So I agree with P0528, but I'm its author so I'm biased. Then again the entire concurrency and parallelism group of the committee agrees as well, so I can't be \*that\* wrong. This paper was a total no\-brainer.
&gt;(a temporary string, i.e. an rvalue) &gt; &gt;slams head into desk repeatedly I think he is referring to the result of operator\+( ... ) when used on the two strings being an rvalue
Yes, agree. Presenting it as a feature request, and better, offering to contribute is great. Letting them know that it would be easier for a large part of the community, it is worth (they probably already know, btw). But at the end, library authors are aware about the adoption issue and it is their decission to accept/implement/merge them, and that should be respected.
Even in a modern code base it will take some time until you can use it, but when it is available: Yes (I always prefer language over library solutions) and in many cases you should probably use expects and ensures instead
Haven't read this book, but I've looked at the author's other book - "Modern C++ Programming Cookbook" (author is Marius Bancila) - I received a review copy. Based on what I read of "Cookbook", and what Scott Meyers wrote, sounds like a good use of $10. Both books go into C++17, I'm kinda mostly still stuck on C++14, so some of the material was in front of me, but that's how I learn...
This is what I do too but I use an [interface target](https://cmake.org/cmake/help/v3.11/command/add_library.html#interface-libraries) so it needs zero sources. Depending on your design you might want to guard the flags and include directories with $&lt;BUILD_INTERFACE&gt; so that information doesn't propagate to external consumers.
Hmm, attributes. Am I the only one who is allergic to them?
R5 isn't publicly available; wg21.link reflects that, so it's a feature, not a bug. ;-]
I didn't read the full document in detail but from what I understand it can be useful for compiler to do away with the checks. assert doesn't even included for codegen since it will be removed during the parsing when optimizations are enabled, however for contracts it seems that compiler can still use the information but not actually execute the runtime check. Not actually Contracts but you can check: https://godbolt.org/g/WM491i 
is it only me, or did Meyers mean irony?
Because \`bit\_cast\` gives you a brand new object, which \`reinterpret\_cast\` does not. You can't just magic away the aliasing: imagine then passing the cast object to another function in a different TU. \`bit\_cast\` also has requirements which cannot retroactively be slapped onto \`reinterpret\_cast\`.
Note that R0 of P0528 took that approach and added a has_padding_bits type trait; however, EWG told us to "just make it work", so we did.
&gt; Additionally, MSVC at the time would generate bad code when small/single member structs were passed as arguments to functions. This is probably fixed by now (it was over a year ago), but at the time it was really annoying. It's part of the calling convention ‚Äì it's not changing.
BTW.: C++ will finally get a standardized library facility for that: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0476r2.html
on va pas dire non hein !
As you said yourself, I don't see the advantage compared to a simple `add_compiler_optiins`
&gt; Anyhow with all that stuff I feel super productive... I'm sure you are, but that seems like reinventing a lot of wheels...
&gt; you can't really make an interface for a function Huh? C++ offers no less than 3 ways of using free functions to define interfaces: 1. The C function pointers. 2. `std::function` 3. Non-type template parameters. For example: template &lt;std::size_t (*strlen_)(const char*) = std::strlen&gt; class BasicClass { ... }; using Class = BasicClass&lt;&gt;;
You hit the nail on the head. The spirit of a certain request bears meaning.
Killing a program on an allocation failure
That doesn't mean that changes to the language should be considered because they help people learn C++ by reading the standard. That's a terrible reason to make a language change.
I think you're missing context for the wider discussion which led the experts of LEWG to vote in favor of this change. I expect there to be a follow\-up paper which explains why it's a good idea.
Let me guess, it's going to be along the lines of 'the compiler can do some cool stuff if it can assume allocations never fail'? Arguments against: A) huge BC break B) see A C) see A D) allocation failure isn't uncommon on embedded. Life can go on. E) the compiler cannot possibly know which allocator is being used, LD_PRELOAD can be used to insert your own F) 'linux will kill your program on OOM': overcommit can be turned off It's a dumb idea, sorry
I like them, but _haaaate_ the double-bracket syntax.
I have yet to see a type whose C++17 aggregateness led to an actually plausible problem scenario. The real-code examples in the LLVM codebase, as reported on the reflectors, are: - A stateless RAII type that only does work in the destructor - Two structs that are just a bundle of public data members with a defaulted default constructor and a couple member functions. - Two stateless CRTP base classes. - Namespaces-masquerading-as-classes, with all static members. 
&gt; will naturally have to wait for a few more years before it is finally standardized. You mean before it's implemented in all major compilers? Because it's already standardized (well, C++20 officially didn't happen yet, but it's voted into the draft).
No, it's standardized once the standard is published officially. A working-draft is not a standard and there have been cases where stuff was added and removed again without any publication in between. (I'm waiting for this happening to concepts again, because the day that we get concepts can't possibly be nearer than the day of GNU/Hurd being the dominant OS.
OK, but this is already much less elegant and more verbose than just setting them via \`add\_definitions\`.
Give me something that has better cross\-platform support and more library finders, and I'll happily oblige.
That's what I said (with clarification in the parens). How many times did this (voting out right after voting in) happen? Especially for a non-controversial feature?
We're *not* "merely" initializing it. There is no "it". Before new is called, the memory doesn't represent any object so you can't "merely" initalize it. We are literally creating an object, even with placement new. The fact that the process is itself non-obvious argues against the symmetry because there is none. One creates an object, the other just runs the object's cleanup function. They are not opposites.
[Here is the first problem](https://books.google.com/books?id=nl9dDwAAQBAJ&amp;lpg=PA192&amp;dq=The%20Modern%20C%2B%2B%20Challenge%20pdf&amp;pg=PA10#v=twopage&amp;q&amp;f=false) &gt;**1. Sum of naturals divisible by 3 and 5** &gt; &gt;Write a program that calculates and prints the sum of all the natural numbers divisible by either 3 or 5, upto a given limit entered by the user. and its provided solution &gt;The solution to this problem is to iterate through all numbers from 3 (1 and 2 are not divisible by 3 so it does not make sense to test them) up to the limit entered by the user. Use the modulo operation to check that the rest of the division of a number by 3 and 5 is 0. However, the trick to being able to sum up to a larger limit is to use the `long long` and not `int` or `long` for the sum, which would result in an overflow before summing up to 100,000: int main() { unsigned int limit = 0; std::cout &lt;&lt; "Upper limit:"; std::cin &gt;&gt; limit; unsigned long long sum = 0; for (unsigned int i = 3; i &lt; limit; ++i) { if (i % 3 == 0 || i % 5 == 0) sum += i; } std::cout &lt;&lt; "sum=" &lt;&lt; sum &lt;&lt; std::endl; } So, what exactly is "modern" in this solution? Is there an actual philosophy? Or is "modern" just compiling at/near head? There is nothing in the preface that establishes what "modern" is. I'd expect expressive, zero-cost abstractions. Ditch the raw loop for an algorithm. Even better, find the sum in constant time using the formula for the [sum of a series](https://en.wikipedia.org/wiki/Summation). I suppose abstracting this into a function may be considered by some as *post-modern".
For me the upside is, I know what it's going to do when I link it with a project. I don't know what `add_compiler_options` do. Which targets does it add it to? Every sub directory? Only the targets in this file? Will it retroactively add them to the already defined targets? If multiple files can set these flags, do I have to track where they come from? Can they be overwritten? I'm sure there are well defined answers to all of these questions, but I think using a specific target for this is quite intuitive. You can't add flags to it from a different file, it's going to add the flags to all the targets that you linked to etc. I agree using the global versions is going to get you up and running faster, but I think following a structured way usually pays off in the long run.
It‚Äôs also useful to remind yourself that as an open source developer with a public bug tracker you are effectively inviting your users to submit issues. I think most would agree this would be a feature request, not a bug, but these days they all go in the same tracker. After all If no one asks for the feature how else can a dev assess the demand? Respect all developers
I feel that real-world C++ is generally fast, slower to develop and messy to work with. By messy I mean, that most of the open-source projects are very varying quality and everybody writes their own style (many layers of oop, c with classes, extreme meta-programming etc...). Obviously you can make progress faster with python, as long as performance is not an issue. There are also lots of hidden costs due to the lack of static typing in python. Other than that, you don't really have to tie yourself to a platform. There are nice, multiplatform, libraries that handle all the user interaction for you. Like SDL is great for creating games, and QT for non-game software.
Can you expand on that? In what way does cmake make -fvisibility cross platform? What does that do on msvc?
assert/CHECK and logging macros are the two biggest reasons I still use macros. Once this and hopefully fmt ships, I can start dreaming about having a macro-free codebase.
I have 3-5 libraries combined into just a few apps and have the same questions. Nothing feels ‚Äòright‚Äô.
Nothing worse than a debate about semantics. You should be above that.
Can that be done for future continuations as well? Will there be any papers on that?
So that it jumps out üòù. I originally used it as is suggested in this thread, but then I found that most junior devs had trouble with it. A for loop has this syntax: for( init; cond; repeat; ) I feel: if( init; cond; ) Is very easy to explain/ self explanatory. Whereas the evaluate init as bool can trip people up. There is just too much symmetry with the (sometimes) redundant condition of a for loop 
You jumped on the guy who directly replied to a comment of yours making it out as though _he_ was shitposting, because you apparently don't know how threaded forums work. This isn't "semantics", it's failing the basics of communicating on the internet. :-]
None of what you said makes sense. You're trying to litigate what whether "it's not good for C++" is an answer to "name me a developer who doesn't need contracts". You're degrading the quality of discourse. We are all worse off for your tangent based on a perception of "jumping on" someone. 
&gt; I remember many colleagues refusing to build with xlc for bluegene's power arch and instead use a gcc cross compiler That's just idiocy. For scientific code, xlc blows gcc out of the water.
&gt; You're trying to litigate what whether "it's not good for C++" is an answer to "name me a developer who doesn't need contracts". No, I'm saying that the word "me", written by the author of a very widely used library, is an answer to "name me a developer who doesn't need contracts". Good god; I honestly can't tell if you're trolling.
Which also doesn't answer the question, because we all know the idiomatic use of "name me a X that Y" isn't sufficiently answered with "Bob". Come on man. You know how discourse works -- quit degrading it.
Also, a good chunk of the presentations are also available. You can see the links here: https://github.com/boostcon/cppnow_presentations_2018 Things without a link do not have their slides available. If you have no slides please send them in or make a pull request so I can add them (I'm the volunteer managing this stuff).
Here's the snippet from the book itself, &gt;**Who this book is for ?** &gt; &gt;*Are you trying to learn C\+\+ and are* ***looking for challenges to practice what you're learning****? If so, this book is for you. The book is* ***intended for people learning C\+\+****, regardless of their experience with other programming languages, as a valuable resource of practical exercises and real\-world problems. This book* ***does not teach you the features of the language or the standard librar****y. You are expected to learn that from other resources, such as books, articles, or video tutorials. This book is a learning companion and challenges you to solve tasks of various difficulties, utilizing the skills you have previously learned from other resources. Nevertheless, many of* ***the problems proposed in this book are language agnostic****, and you can use them when learning other programming languages; however, in this case, you won't be benefiting from the solutions provided here.*
He gave us Linux and he gave us git. Not fair suggesting that he's a lazy bloke ;-)
Oh wow, thanks.
There is an *it*. Raw memory, which was either static, on the stack, or dynamically allocated (and that dynamic allocation having a symmetrical deallocator). Placement new initializes the memory as a type, creating an object there. The inverse, which currently calling the destructor is, functionally eliminates the object. They are certainly opposites. After the destructor is called, the object no longer exists.
Compilers can't just wily-nilly optimize away implementation-defined behavior and anything that touches it, which is an absolutely massive difference. https://godbolt.org/g/BbVgsL
In my experience turning off strict-aliasing has resulted in fairly minor performance gains. Both firms I've worked at that heavily rely on performance to make money keep it off since we can mark spots where it's a problem with restrict.
I have a similar question in conjunction with cross compilation and the use of toolchain file. The document says, &gt; Don‚Äôt use target_compile_options to set options that affect the ABI. OK. Maybe I should set those `-mcpu=...` etc flags in toolchain file instead. But, &gt; ## Cross Compiling &gt; &gt; ### Use toolchain files for cross compiling. &gt; &gt; ### Keep toolchain files simple. But what should or should not be written into the toolchain file, especially when `add_compiler_options` and `CMAKE_CXX_FLAGS` etc. are off-limit? How do I set the `-mcpu=...` flags? These two guidelines are definitely not helpful.
It's hard to detect "sarcasm" sometimes, but i think the following is a good example: Unless there have been big changes to the STL since C++14, there's no gender_from_image" functionality in the standard library.
I think that passing through a char array would also work. Might be useful if your compiler has a good optimizer but doesn't treat `memcpy` as an intrinsic.
Modern is not a very good word to use in any context, today's modern is tomorrow's old. Totally unrelated and off topic. Close to where I live, there's a scooter rental place: "Moto 2000", was cool in 1999 (like Prince). 
&gt; The issue is that without a low-level graphics library in std, we'll remain in a https://xkcd.com/927/ situation where that's just one more competing standard. The benefit of P0267 was that it would give a cross-platform default option that works for most of us. I am afraid that I have to disagree. P0267 did all sorts of image loading and drawing stuff that meant it wasn't just a low-level library to allow building other stuff on top of. (Like everything else in std.) It tried to do so much that it was shooting for being a jack of all trades, master of none, and wouldn't have been useful for many sorts of graphics applications. As a result, it was a feature that would only be useful for writing technical debt. I still think a simple image2d vocabulary type that can unify all the various "xkcd927" libraries by giving them a simple way to interoperate is all that should go into the language. Then all the cairo stuff in P0267 like path_builder can just be adapted top draw into the standard image, and stuff like wxwidgets and Qt can easily get a compatibility shim to be able to display the result in an interactive window, and stuff like OIIO can get a wrapper to allow saving the image to a zillion formats, etc.
&gt; Compilers can't just willy-nilly optimize away implementation-defined behavior and anything that touches it They can, as long as they document that that's the behavior. The necessity for this documentation is the difference between implementation-_defined_ behavior and unspecified behavior.
This whole discussion arose from Linus' use of union-punning, which is undefined behaviour in the standard. Yet, Linus is using the GCC implementation defined behaviour, especially with its use of the -fno-strict-aliasing flag. From that point of view, which is the only point of view relevant to this post, they are the same. You are completely arguing my point - Linus is being unrealistic in his criticism of the standard because he can't understand that the standard has to cater for more than just his use of the implementation of a language for one compiler.
There is no it *before* the placement new. It was clear from the context. Do I need to spell out everything?
I'll take VS solutions over CMake everyday.
No, you just need to have a cogent argument.
Learning how to comprehend a simple sentence in a wider context is a basic requirement. If you refuse to do that, no argument will ever be cogent for you, for you fail basic comprehension. "Symmetry, because symmetry" is not an argument. "It's non-obvious" is not an argument either. Languages should not be designed by OCD.
A more "modern" solution would be to not use explicit for-loop and instead use ranges I guess: using namespace ranges; int sum = accumulate(view::ints(1, limit+1) | view::remove_if([](int i){ return (i % 3 == 0) || (i % 5 == 0); }), 0); 
Should be `filter` rather than `remove_if`. :-]
Homework goes to /r/cpp_questions. Read the rules before posting. 
Honestly I thought I was in that sub because I've posted there before -_- Thanks for the heads up.
The standard will enter feature freeze after the next meeting. The chances to have either concepts or contracts removed from the draft are extremely low.
It seems that neither gcc or clang is smart enough to either go full unrolling or to compute the sum directly. Clang can get the unconditional sum just right but it doesn't like when you throw in some condition. It is also interesting to see how the tests for modulo are replaced by bitshifts with gcc. Also when manually unrolling the loop, you get some weird code https://godbolt.org/g/bXJKdJ
It supports async requests and callback. You can read about it at the documentation page. OpenSSL will be supported in future. https://whoshuu.github.io/cpr/
It's fantastic as it's a large improvement to how it used to be. As a heavy CMake user of course there are tons of things i get annoyed with and wished were better
Not sure I follow, what do you mean? You just list all the files as usual, but add the path prefix?
More efficient crypto-currency mining on other people's systems without their consent.
Finding an answer to those questions costs you about 30 seconds of googling and reading &gt; Adds options to the compiler command line for targets in the current directory and below that are added after this command is invoked. This approach makes sure that all libraries and executables in my current project are build with the same settings so there is e.g. no abi missmatch and I don't have to remember each time to link the config library. Also, I'm of the school that libraries should not set any compiler options in the projects the are linked to. So target_link_libraries(my_exe my_lib) should not suddenly change my build config (like turning on warnings, changing the c++ standard etc). Nothing more annoying than having to track down, which dependency added a specific compile flag to your build. At most, they should communicate their requirements (e.g. at least c++14) and define library specific symbols. I also don't see why the approach with a config library is more structured in general. It just adds a lot of edges to your dependency graph with the same result as using global variables if you don't forget to link it somewhere and potentially very hard to find bugs is you do (abi missmatch) Where it imho does make sense is if you have a common config library across your whole company that you can get from a centrally manager packet server/repository. That is something global variables can't do for you, but I'm not sure how common that is. In the end, I see this as a matter of taste. 
Ok, if your co-workers do indeed have less trouble with the redundant version, then of course that is what you should do. Luckily for me I didn't have any problems with it yet (not that common in my code anyway).
The hard truth is that you Sonnetimes have to set platform/competitors specific options and also that cmake someone's just doesn't have an appropriate platform agnostic setting. 
Enabling and disabling specific warnings is one common use case for compiler flags. Optimization settings are another. Also Everytime a new compiler version or flag comes up it usually takes some time before it is covered by cmake.
I totally agree, and as a maintainer myself, I really appreciate all kind of feedback, feature requests, etc., not only bug reports. What I disliked is trying to impose providing support for a build system or a tool in general, no matter how popular it is, reporting it as a bug. It is clearly not a bug, and telling the maintainers it is a bug (so it has to be fixed) it is not a nice approach, not to say it is even a big aggressive.
All the more to stop you using them, Little Red Riding Hood. :)
Sounds good. Is that public? You may also check my little general purpose library [ASL](https://github.com/aslze/asl) which includes HTTP client requests, HTTP servers, JSON, XML, sockets, threads, processes, logs, dynamic libraries, directories, etc. HTTPS is supported with help from the *mbedTLS* library. That is the only external dependency (optional).
Thank you for your job - there are some nice videos out there.
Yes, there's a separate paper for that.
&gt; C++ Requests is a simple wrapper around libcurl One problem with libcurl is it spawns a new thread for each request (or batch of requests). This can be an issue for some people.
Don't be afraid to use the right tools for the job, just because they aren't the ones you use by default. There is nothing wrong with a central `add_compiler_options` at your top level cmake file. It is the most straight forward and maintainable way to ensure all your projects are compiled with the same language and abi related flags (there is a cross-plattform version for setting the language standard though). `include_directories` and `link_libraries` are a different matter though. Both of them are needed as a result of dependencies between project components and you should use `target_link_libraries` to express those. `target_include_directories(libA...)` should only be used to express, what include directories belong to libA. The include directories of your dependencies will be automatically forwarded via `target_link_libraries`. 
Isn't that because both those functions take a `limit` as an argument? Unless you know what `limit` is, the compiler can only assume that it goes as far as `unsigned int::max()` and can't compute the sum directly. And loop unrolling only goes so far. Maybe the compiler determined that the size of the loop unrolling, for up to `unsigned int::max()`, wasn't better?
Just use \`target\_sources()\` then on the main library with a full path. Or make sub targets for each component that are interface libraries with sources added using \`target\_sources(... INTERFACE...)\` and link those. I find that cleaner.
A free function can always takes a base class reference or pointer (raw. For now, raw pointers are used for non-owning pointers.)
So what would be a better syntax?
The *is not a bad compiler* takes into accout that part. The problem is when you want to compile a huge application in Fortran70 or C89 written by a physicist 8 years ago. If it does not work straight away you just cant afford fixing everything for just a compiler.
I don't find it so weird that they don't go full C\+\+17 craziness in the first problem. You don't want people to stop reading your book on page 1.
Unfortunately there isn't one, to my knowledge. That doesn't mean we shouldn't say anything about how bad cmake is. 
How can it be that 'it's fantastic' and then ' there are tons of things i get annoyed with'? I get the latter, and I can't say it's fantastic.
The only C++17 feature they would have to use is `std::lcm`.
`//` and `&lt;-` already have meaning in C++.
I would say the industry standard is actually \`make\`. That does not mean every single project has to be done in plain makefiles. Autotools has worked for decades and it still does the job, so I understand developers wondering why to put some effort if people is already fine with that they are currently providing.
It happened to variadic `std::lock_guard` and `std::default_order` in C++17 because the committee judged that the features weren't worth the ABI break they introduced. Variadic `std::lock_guard` was replaced by `std::scoped_lock`, but as far as I know, nothing was introduced to solve the `std::default_order` ABI problem.
What is the meaning of `&lt;-`? Less-than followed by unary minus?
‚ü¶ and ‚üß of course.
The million dollar question. I think we've exhausted the available options on standard keyboards.
Why? There are still potential duplicated character operators, eg `%%` and unused `@` and `$`.
I think he's referring to the Core Coroutines proposal.
something visual? expects: _/ a &gt; 5 \_ assert: |-- b &lt; 4 --| ensures: \_ b != 5 _/
Oh, sure, thre's plenty of room to use other characters, but none that form a nice 'bracketing' pair, like, er, brackets. I always find the cognitive load is far lower when something self-contained is of the form "`OPEN` body `CLOSE`", where `OPEN` and `CLOSE` are different but complementary. Just a personal preference. The double-bracket syntax satisfies that constraint but is fairly noisy visually, so eh, I just need to suck it up I guess. In 6 months I'll probably have habituated and won't care.
The improvement is fantastic. If everything which has aspects that can be improved upon is considered bad, then everything sucks. That isn't a particularly useful worldview.
Great. Ever since I saw the talks from you and hartmut on HPX, i've thought that was such a nice way to do concurrent programming for things like fluid simulation. 
That's the point - it's not willy-nilly and it's not going to kill your program in mysterious ways on a compiler upgrade or if you accidentally massage the optimizer into seeing it.
It works fine for the classic sum from 1 to n. There are many ways to deal with the overflow, so you could definitely make it correct. Clang generates the code just fine: https://godbolt.org/g/9y4fUx
Sure, if you specifically limit your comment to GCC and type punning unions for use in the kernel and a single compiler. But that's very different than the statement that there's not much practical difference between implementation defined and undefined behavior.
The improvement is fantastic, but it still sucks.
Correct. Basically, if you gave &lt;- special meaning, you either have to make sure it's only useable in locations where it couldn't have been used to produce a legal C++ program before, or risk breaking existing code.
Nope, don't know much about co-routines.
Do it
Maybe next week.
From my experience if you are not a guru coding in Java or Python will be much more easy and quick with less bugs With C++ you will have a slower and more painful time coding except if you are a guru with more extra pain in form of library dependency or the linker problems that will arise sooner or later with the inconvenience of having to wait between compiles times So if you want the project done and rigth use Java, Python or C# (.net Core works on Linux and OSx natively now), if you want to learn more or code for fun while using the most from the hardware (the only real reason from choosing C++ among others) use C++ but remenber the experience will be worst because you will have many extra problems 
$ character has been proposed for metaclasses
\&gt; Ask yourself what a 2D library would have looked like 30 years ago. Would you still use it today? We still use \&lt;iostream\&gt; or \&lt;locale\&gt; to some extent, even if it is outdated. And we still use \&lt;cstdio\&gt; when \&lt;iostream\&gt; isn't outdated enough. And nobody suggests it was a mistake to include them in the standard library. 
r/cpp_questions 
For me, the C++ coding I do is really C with Classes. I do use the container classes, but mostly I write C code and only occasionally create classes and then predominately as plain old data objects. I often use struct versus class to make content public by default. I take advantage of RAII and method overloading by using C++ but otherwise my code is mostly C. 
Ah! My apologies! I debated where I should post this, actually! I thought I was submitting to the correct place, as my question was more about the nature of C++ language... rather than a specific question about C++ code. Maybe I go ahead and delete this post and maybe repost on /r/cpp_questions? 
Wow, that's interesting! I think that's similar to how I was approaching C++, in this initial learning phase. I was curious what kind of programs you generally make with C++? 
Wow, that's interesting! I think that's quite similar as to how I find myself approaching C++. I was curious what kind of programs you generally make with C++? 
Ah! My apologies! I debated where I should post this, actually! I thought I was submitting to the correct place, as my question was more about the nature of C++ language... rather than a specific question about C++ code. Should I go ahead and repost this on /r/cpp_questions? (Or since it is here now, maybe I should wait a bit... and then repost on r/cpp_questions...) But yes, my apologies about posting to the wrong sub. 
&gt; For me, the C++ coding I do is really C with Classes. I do use the container classes, but mostly I write C code and only occasionally create classes and then predominately as plain old data objects. I have no words to express how much I ***hate*** your type. Please stick to C. 
Your question was too long and I didn't read it all. If you are going to use C++, you should learn to use the capacity of C++. Embrace the flexibility it gives to you, embrace the templates, embrace polymorphism. If you feel it's too much to take, head to learncpp.com to get a friendly tutorial, and jump straight into Effective Modern C++. Don't write C. C++ is far more sophisticated than C.
Amen
Minimalism comes from refinement. You can't choose what the minimum is - the problem you're solving, and its context, chooses for you. Start with what you're used to, but ask is there a better way. Functions are a good place to start. Except for macros. Never start with a macro.
The two most important features of C++ are templates and RAII. Between the two of them, you can effectively write what superficially appears to be "pure C code" but categorically is _not_. "Modern C++" means using templates and RAII to your advantage at every opportunity and today's C++ implementations (C++11 and beyond) provide 95% of the tools you'll ever need to make that happen: `shared_ptr&lt;&gt;`, `unique_ptr&lt;&gt;`, `vector&lt;&gt;`, `find_if()`, `sort()`, etc. To the extent that object-oriented design is relevant at all, it is often just a means to an end. Rather than high-minded sprawling class hierarchies, you have instead small, tactical designs that have a specific narrow purpose.
Of course it's fine. I do something pretty similar. C++ has a culture that gets pretty hostile to the idea that maybe not everybody likes their particular tower of toys, but you'll be travelling well-worn ground either way.
Also, the phrasing "divisible by **either** 3 or 5" sounds like exclusive or. The provided solution adds all numbers divisible by 3 or 5 (inclusive or) though.
They are hoping to get rid of exceptions entirely, at least in STL, in the hopes of making it more attractive to people who do not use exceptions today, such as embedded developers. I think they are badly mistaken: the people that don't use STL today because it has exceptions won't suddenly change their ways. The people who do, will need a new container library that doesn't randomly kill their process just because there is a shortage of memory. You are right \- it's a dumb idea. I'm amazed it has gained any traction.
Seriously, you **hate** someone who doesn't use all of your favorite features of what I consider to be an overly engineered overly complex system because I have no need for the complexity? That is....dumbfounding. I would be happy to stick with C if it supported RAII and function overloading. I can get reasonable container types for C but using them adds to the memory management issues that C is prone to have, that is why I prefer RAII.
Using `memcpy` this way is optimized on all the compilers I've tested on, when optimization is enabled at least. It results in the same code as using a union. https://godbolt.org/g/W4hmgK 
Your approach is not too different from mine. When writing C++ I stick to templates, RAII and the standard library containers and algorithms. I use namespaces more often than classes, unless I need runtime polymorphism, otherwise a namespace does a good job at categorizing and controlling access to variables and functions.
Writing nicely organized functions is fine, and if every small problem you need to solve is well solved with a function, then just write lots of functions. Whether to use classes or functions isn't about how big/complex something is. It just boils down to whether you need a class. Do you just need to group data together? Use a simple struct. Do you need to maintain invariants over multiple accesses to a group of state, or provide potentially polymorphic behavior over state? Then you need a "proper" class. In sufficiently large, complex projects these problems almost always show up, which is why you almost always need classes, to some degree. And it's why so many large C projects just reinvent a class/object system anyhow. Instead of thinking about "old school C" vs "modern C++", I'd suggest looking at specific situations. When you frame it in those general terms, what tends to happen is that programmers romanticize one approach or the other, and make that part of their identity. Instead, consider yourself an engineer trying to solve specific problems. Problem: I need to free memory, close files, sockets, perform other cleanup, whenever I exit the function. But the function has multiple exit points, because of e.g. error handling, or other reasons. C Solution: use goto. C++ Solution: use RAII. Which solution do you prefer here? In most (all?) cases, I'd argue that RAII is just a better solution, engineering wise. With goto, you are basically re-implementing what RAII in the language gives you for free; the ability to call destructors properly when scope exits, no matter how scope exits. It is a problem in C++ that there are many shiny toys, and people overuse them. It's a problem in almost every language other than C, to be honest. But that doesn't mean you can't write simple C++. And C is going to suffer from its own problems of so many things being re-implemented by users of the language, or even poorly implemented in libraries because the language just doesn't give you the tools to do it well. Again, I'd suggest coming up with some concrete examples of problems, and what you envision the "C" way and the "C++" way to be. You may find either that: a) Many C++ developers will tell you that what you consider the C way is a perfectly fine way of doing this problem, and is no less modern C++ for being simple, or b) The C solution has all kinds of drawbacks compared to the C++ way that you hadn't considered.
Both C and C\+\+ have always reserved identifiers starting with an underscore, or containing double underscores, and any use of these has always been UB. I'd say that leaves quite a bit of virgin territory for new keywords to be created in: just start them with an underscore. Of course a few people might be inconvenienced because they are already using disallowed names, but their inability to follow a simple rule should not stop progress as a whole. Besides, all is fair in love and UB. 
FWIW, I have a vague plan to respecify `span` in terms of the Ranges design after the Ranges merge completes to make it more consistent with the other views.
My advice is to try and implement some basic computer science constructs with c++, like if you are writing a UI for a game or utility, try a functional reactive approach, or try an object oriented approach, but something fairly language agnostic. Write a mutable class for something and try writing an immutable variant with pure functions as interface. I would honestly stay away from trying to do language oriented learning. Like you can implement publisher-subscriber with runtime polymorphism via classes, or free functions, or at compile time with constexpr or templates. It‚Äôs kind of important, but only after you understand the construct, then you can apply it with C++ techniques. It‚Äôs good to try and apply various c++ techniques to build the same CS abstractions. I like looking at Closure, Haskell, and other tutorials, so that I don‚Äôt get blinded by C++isms. If you want fun, quick, and a bit dirty feedback, check out dear imgui. One of the funnest ways to quickly experiment with c++ in a mostly paradigm agnostic way. You can quickly write a procedural, object oriented, or functional reactive UI and have fun mixing and matching.
No worries: let's just say you're not the only one that says my communication style can be overly verbose and wordy at times! (It's ironic that I'm asking if I can approach C++ in a more minimalist fashion, given how I'm so overly wordy at times!) -------------------------------- But yes, as you suggested, I will certainly pay attention to a modern C++ style. Since I'm learning C++ now's the time to do it... But for now... I feel myself thinking that I might end up being more "minimalist" and slightly more towards the older C-like style, as compared to a lot of C++ programmers. Anyways, who knows: that could change as I learn more, and pay attention as you suggested to modern methods. 
Just like we have an OS\-less subset of C\+\+, we could have an 'extended' superset that includes libraries that are not subject to the rigors of the core language development, but are expected to be available in quality implementations. 2D graphics would make an excellent library for this extended ecosystem.
&gt; The solution to this problem is to iterate through all numbers from 3... Am I the only one here who expected an *actually* smart solution here, like sum of a series? Not like it would be related to C++ in any other way than using your brain to analyze the problem before jumping into coding...
Maybe you want to put a cross over it, but I still think that all of the above is rather unsatisfying.
Yup... that's exactly what I was afraid of... ! This is the precise issue that I noticed, which lead me to post this question here... Just to see what people thought about my frame-of mind regarding C++... and whether or not there is a place for me in the C++ community. -------------------------------- HOWEVER... I was just wondering however: Do you actually mean that you truly hate me as a person in general (for having that stylistic preference) ? Or did you perhaps mean that you hate working with people like that on projects? ------------------------------------ If it's the later, then I can TOTALLY see where you're coming from! I can imagine that your working on a big project, and along comes someone who codes completely and radically different approach from the general style of the rest of the project, that everyone else is following! So in a case like that... if I were you, then ya! I too would hate working with me as well! But just keep in mind, that I would never impose myself on a big project in such a fashion like that. (I wouldn't except a job in the first place if it meant coding in a way that was completely wrenching to my own style, as that would not be fair to the others.) 
What about reading the post ? " Polls of members‚Äô positions were taken regularly, and eventually there was sufficient consensus to stop pursuing 2D graphics in the standard. " &gt; Ask yourself what a 2D library would have looked like 30 years ago. Would you still use it today? Points, rectangle, regions, bitmaps, patterns, copybits, pen, text, the 16 transfer modes, pictures and polygons, clipping... Just went throught the 1984 Apple QuickDraw reference (that's 34 years ago -- you can find it in Inside Mac Volume I), and well, I think people would be totally using today for simple immediate rendering graphics / UI. Of course, there was only 8 colors and no transparency, but I think that in 30 years, the standard would have evolved quite a bit. Not saying that Graphics in Std C++ are a good idea, just saying that you underestimate the relevance of old apis.
Yup... that's exactly what I was afraid of... ! So this is the precise issue that I kinda sorta noticed, which lead me to post this question here... I just want to see what people thought about my frame-of mind regarding C++... and whether or not there is a place for me in the C++ community. -------------------------------- HOWEVER... I was just wondering: Do you actually mean that you truly hate me as a person in general (for having that stylistic preference) ? Or did you perhaps mean that you hate working with people like me, on projects? ------------------------------------ If it's the later, then I can TOTALLY see where you're coming from! I can imagine that your working on a big project, and along comes someone who codes completely and radically different approach from the general style of the rest of the project, that everyone else is following! So in a case like that... if I were you, then ya! I too would hate working with me as well! But just to reassure you: I would never impose myself on a big project in such a fashion like that, if it's obvious I'm not going to fit in. It wouldn't be fair to everyone else working on the project. 
Your asserts should provide full backtraces (not just the line number of the assert), and it should somehow allow for picking out the value of expressions. For example, if you want to assert that f(x) &gt; y, your assert should print out the runtime values of f(x) and y when the assert fails. assert() doesn't provide this; if [[assert: ]] does (I know nothing about it), then use it, otherwise the answer is: use neither.
I would vote for \*\*properties\*\* as a language feature. Some C\+\+ compilers already implemented it: Microsoft Visual C\+\+ has \`\_\_declspec(property) \` and the old Borland C\+\+ Builder had something similar. But being compiler\-specific, no\-one uses that. Other languages include built\-in properties: D, C#, JavaScript, Python, PHP and probably others. If all of those can use built\-in properties, why can't C\+\+?
Thanks for your great response Bill! From what you describe, it sounds to me like the EXACT same approach I want to use/develop with C++. So that makes me happy to hear that others use this approach... and that I might not entirely be viewed as some kind of heretic-blasphemous-freak in the C++ community! -------------------------------- But yes... since your approach is similar to what I think mine will be... I also wanted to ask you what types of programs you generally work on? Also, what would be your absolute favorite type of C++ project to create, if you could work on anything you wanted? -------------------------------- FINALLY... In addition to C++, do you find that you need to sometimes use other languages on occasion in your career? Or do you mostly stick with C++ almost entirely? Right now, I'm really loving C++, so if I could stick with C++ as my main and primary everyday language, I think I would be really happy with that! As a second language I was thinking Python... But C++ really grabs me, much more than Python. There's something I don't fully like about Python, in addition to not liking the fact that it's usually interpreted rather than compiled! -------------------------------- Which... leads me to yet one more last question (if I can sneak it in here, and if you have time to answer)... If a python library is written in C, or C++, can I then use that library in my C++ program? If so... then great! I'll have access to some extra libraries, without being forced to use Python if I don't want to. (Although I imagine there are already a lot of alternative libraries specifically for C++ that might be even much better than the Python version, so I'll always look for those first.) 
When you see an article about lvalue/rvalue references, move semantics, etc, you can ctrl-f for "expression". If expression doesn't show up, then it's not going to be good. All of these pages that purport to explain "for beginners" tend to actually make things worse and more confusing. It is better to be 100% clear from the start. "rvalue references" refer to type, rvalues refers to expressions. Expressions have a type (which is commonly described as having the reference-ness stripped off just before being considered), and they have a value category. What references can bind to what expression, is determined by the type of the reference, and the type and value category of the expression. You explain that, and you enumerate the most common examples of expressions, and what their value category is (an expression that consists of a named variable, an expression that consists of a function that returns by value, etc etc).
He's a zealot and a jerk. There's lots of very talented programmers who write C++ in style that is essentially C with only those features from C++ that they consider strict improvements over C. What features you choose to embrace come down to what's best for you and what's best for your project (and established practices, which you correctly point out). See also Orthodox C++: https://gist.github.com/bkaradzic/2e39896bc7d8c34e042b The author of sokol_gfx shared their thoughts related to switching almost exclusively in C over C++ in the last year: http://floooh.github.io/2018/06/02/one-year-of-c.html. The part about language "anxiety" resonated with me particularly - where thinking of the best way to use the language to solve the problem becomes a distraction. 
My day job involves mostly Java coding, but what I've been coding recently in C++ are 3D OpenGL based applications. The OpenGL libraries themselves can be used from straight C, but tools like glm and assimp are C++ so a C++ compiler is needed.
Isn't it time that we do survey of architectures that are supported by currently existing C\+\+11 compilers, and adjust the standard to match what's actually there? I appreciate that a future architecture might be 'exotic' in some way \- but if it wants to have any kind of connection to the world as it exists today it will match existing standards, not just for C\+\+, but for \_everything\_ (both hardware and software). That means it's fundamental properties are going to be a pretty precise match for existing technology. And making endless allowances for non\-existing hardware on which 99&amp;#37; of software won't run anyway is just needless complication. There aren't going to be non\-power\-of\-two integer processors anymore. There will be no further ones complement CPUs (if ever there were any). IEE754 is here to stay. Maybe it is time we accept those things.
Just FYI, the Core Coroutines proposal suggests `[&lt;-]` for something at least somewhat analogous (I don't feel like I understand the alternatives well enough to make a stronger statement) to what's called `co_await` in TS coroutines.
Single underscore names (unless followed by a capital letter) are only reserved in the global namespace. Something like `class MyClass { int _x; ... };` like I've seen, or `namespace mything { void _helper(); ... }` are legal. So that would lead to either double underscore names (consider `__attribute__`) or underscore capital (like `_Pragma`).
After thinking on it a few more days for me it basically boiled down to: 1. We are going to have some sort of static reflection in language 2. Eventually someone will be willing to do type modification/generation through it 3. It have to be somehow expressed in the language And if we just look around current language version, we already do the same thing (in more restricted manner) and syntax for that already exists; it's simple, widely used, and easily interpreted by human readers. This doesn't mean we *have* to make it exactly this way, however when the wheel is already invented, maybe it's worth a consideration when we choose our path.
If the generation of suboptimal code because of aliasing worries bothers you, maybe the answer should be to stay away from unions, rather than legislating this usage as UB. I'd consider accidental UB to be a far worse outcome than accidental performance loss.
Thanks for the link to the one year of C post, it was a good read. 
How can you feel special if you don't over engineer everything?
Quick question... And I hope this isn't too dumb but working with C++ and python I've come to the conclusion that the newer context-manager style employed by a lot of code in python 3.6 is actually quite similar to handling things in an RAII type of way. Am I missing some large difference here? Again, excuse me if I am saying something completely stupid.
&gt; Unfortunately there isn't one, to my knowledge. But then it makes no sense to abolish it. CMake isn't perfect, but it's better than having no CMake.
It's completely fine. It's better that you spend your time on doing something productive and actually solving problems than continuously looking to make your code "modern" for no apparent reason whatsoever. Just do what you're used to doing, and if something becomes problematic, you can always look up your problem and see if there is a feature that helps you solve that.
Oh that isn't a problem, my day job is Java and if there isn't at least 7 levels of indirection in the code it doesn't pass a code review. :)
IMO it would be pretty cool if we used \`@\` to support a uniform way of specifying allocators, it's annoying that STL containers will often support a custom allocator but which template parameter is the allocator is anyone's guess. The problems multiply when you try to do something more complicated like make a variant that has support for something like \`boost::recursive\_wrapper\` but also wants to allow you to specify a custom allocator used for the \`recursive\_wrapper\` allocations. I don't have a concrete proposal and it's probably really hard, but I don't think I'm the first one to think that there should be a uniform way of specifying allocators. We could also make it so that \`@\` can be used as another way of spelling "placement new" if you use \`@\` with a pointer instead of an allocator or something.
&gt; Both C and C++ have always reserved identifiers starting with an underscore, or containing double underscores, and any use of these has always been UB. I'd say that leaves quite a bit of virgin territory for new keywords to be created in: just start them with an underscore. Of note, C has already used this to create new keywords. [10 of C's 44 keywords](http://en.cppreference.com/w/c/keyword) start with an underscore and capital letter. Note the capital letter part: the rules about reserved identifiers are a bit more complicated than most people realize.
The mythical standard package manager is even more difficult to pull out than the graphic API.
&gt; Some symbols have been duplicated to create more operators [...] so we could as well get [...] ``` // ``` [...] operator//() Mmmm... WCGW? I also suggest: ``` operator\\\\() ``` and ``` operator;() ``` :-)
I have well reasons to be hateful. The people that are sympathising with the oh-so-called """orthodox""" C++ are just downplaying the language to be what it isn't. Do _not_ be afraid of your tool, learn to harness it instead. Once you go past the trivial parts of the language, you slowly start to understand why C++ chose to have features that you are trying to avoid. &gt; Stylistic preference C++ is far more sophisticated than C. The legacy style is error prone, not type safe and vastly unreadable, unmaintainable and creates magic "no-go zones" within a codebase. Sooner or later you'll learn to use lambdas instead of function pointers. You'll use templates instead of legacy style void pointers. You'll use `constexpr` instead of `#define`. You'll use `std::unique_ptr&lt;T&gt;` more than legacy pointers, you'll use `std::make_shared&lt;T&gt;` instead of `new` . These are just good practices. You don't get to "pick" how C++ is. The "old school" people invest more in re-inventing the wheel while others solve the actual problem. One more reason to avoid this "old school" fraud is that you'll also find out how the standard library (and others) makes use of many of these concepts. In order for you to read, understand and debug others' code you can't afford to conveniently choose not to learn those concepts.
My dream is to one day remove all the `if` statements in the codebase. Honestly, using `if`s is a sign of a bad design. 
Speaking as someone whose code generally contains as few if statements as possible, there are edge cases where this actually \_is\_ true ... 
ok, `//` was not very well written about but you know, it has different purpose compared to `/`
1. Libraries mostly. In the past three years I worked at 4 different companies doing mobile, desktop and embedded. I was mostly focused on the library part, and so I‚Äôve done a lot of C, C++ and ObjC. 2. Sometimes I have to do GUIs, and I have worked on two libraries meant for Android, so I have done some Java, as well as JavaScript and Swift. At one company they were using Xojo, which was pretty much trash but it had one advantage: It required you to write plugins in C++ if you needed to extend the language with additional functionality. So I loved that part about it. Most of the time that was wrapping the native APIs to get around deficiencies and bugs in its own frameworks. Since those APIs are typically procedural C APIs, I found no need to use classes and stuck to the C++ features that added actual value. 3. If the Python library has the typical .h or .hpp and exposes a C or C++ API then yes.
This, But unironically
Hah you def don't need to be so apologetic :-). Yeah, your observation is spot on, context managers are quite similar to RAII (and they're amazing, use them all the time!), there are some important differences as well though. RAII lets you move resources between scopes easily, without calling cleanup code. I.e. you can have a function that opens a socket, returns it without closing it. In this case, responsibility for the socket has passed from one function to another. Not really possible with context managers. The other key difference is that with context managers, is that they give you more power with how error handling is done. In a context manager, the __exit__ method receives information about the exception thrown in the body of the with statement, and then can try to make intelligent decisions about that. In C++, destructors don't know if they are being called due to an exception being thrown, or normal exit. This in turn (along with other factors) means that you almost never want your destructors to throw. So you are more restricted to cleanup that never fails, in python it is much easier to handle cleanup that itself can fail in a context manager. But in simple, common, use cases (I want to open a file and ensure that it gets closed no matter how I exit the function) they end up working exactly the same.
Telling people that it's unacceptable to not throw more memory at a problem in every case, and acting like the existence of a feature justifies its mandatory use, marks you as more zealot than intelligent. You don't HAVE to use function pointers at all. If your object is comparable, for example, either build that into the object, or decorate it with a comparator instead of carrying around a lambda that you loosely associate with the object. If you're using std::unique\_ptr\&lt;T\&gt; or make\_shared\&lt;T\&gt; on objects that aren't being copied, you're doing things badly. If you use these features without thinking about what they mean and why you need them, you're copying as sloppily as the people who make buggy code in C. I suspect this is all projection of unnecessary rage from somewhere else, that has little to nothing to do with the actual coding practices of the person you hate.
I think all of those actually (can be made to) conflict with existing syntax.
Aaah thanks so much for your excellent reply. And I had no idea about the `__exit__` (and accompanying method `__enter__`) in withs. This changes everything. Have a great day! 
Here, take a `unique_ptr` and stop leaking.
C++ destructors can access information about whether the stack is being unwound and what the current exception is, but people don't use it all that often because it's generally better to design your classes so that it's not necessary (and because lots of people don't know about the library facilities that exist).
operator:-)()
You can access current exception, and the number of currently active exceptions, but this does not tell you whether the destructor is being called as a result of exception exiting the current scope, or e.g. as a result of the destructor of another class running code that normally constructs and destructs your class. AFAIK the only way to do this is to capture the # of active exceptions in the constructor, store it, and then compare to that number in the destructor. So I don't think it's just a matter of preferable design or ignorance of available facilities; the available facilities can also easily used in such a way as to not give exactly correct results, and are just convoluted in general. Which is fine, this is directly related to C++ destructors being more flexible.
Not the author of the post before, but I do hate "c-with classes" programmers if they make code unnecessarily complex or slow, write invalid c++ code, write overly complex macros etc... The problem aren't people that don't use every c++ feature out there. The problem are people that a) refuse to learn c++ beyond c with classes and thus don't know when the language would offer them a much simpler/less error prone solution and/or b) people that don't try to learn common c++ best practices. Btw.: I hate "modern c++ programmers" if they make code unnecessarily complex or slow, write overly complex and generic templates or have to use every new toy just because it is there and not because it makes sense, etc... Both ends of the spectrum are annoying.
&gt; If you're using std::unique_ptr&lt;T&gt; or make_shared&lt;T&gt; on objects that aren't being copied, you're doing things badly. Honest question, when exactly did I say that the objects aren't being copied? Why did you make up that premise? And how exactly telling others to learn about it translate to "not knowing what they mean"? And yes, another concept of modern C++: the move semantics. Good luck conveniently forgetting about this and teaching the ""orthodox C++"" bubble. 
If statements are a code smell, period.
Funny you say that, working with the opengl shading language any form of branch introduces expensive hits in the graphics processing. Because of that you try to avoid if or switch or loop constructs. Years ago I used to program micro-controllers that had limited memory, like in the 16k range. We coded in C and compiled to the target processor and would look at the generated assembly code to see if we could optimize it. For instance a construct like flag = 1 might use 5 or 6 bytes to generate a series of move instructions whereas an operation like flag |= 1 might only use 2 bytes as OR was a builtin operation. I realized when reading about what not to do with shader programs that I have gotten *lazy* in my old age. 
Wow that sounds like some fun and amazing work you do! I've always had a fascination with coding, related to GUI's. Designing and implementing a new GUI is a very artistic and enjoyable kind of project to work on. It also really makes you think a lot about the end user, aesthetics and design and appeal, and how people will interact with your program. (I think some of the best innovations in GUI these days are coming from the video game sector, rather than the OS sector!) -------------------------------- Also: I'd love to see a really good GUI (for Widgets and basic graphics) become a de facto standard for C and C++. I was reading that a lot of modern projects and embedded devices that need a GUI, end up just using Qt... But Qt has some annoying/expensive licensing issues (I guess they have to make money afterall!), so I suspect that might hold Qt back from becoming the ultimate de facto for now. I'm surprised a big company like Microsoft, Google, or Apple, didn't buy and snap up Qt... and then just release it to the public domain and keep it maintained. Microsoft bought Nokia afterall! But looks like they missed a few of the really good bits, like Qt! 
just use rust
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8qkuyb/beginner_question_about_include_and_chevrons/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
And summarily rejected. 
Qt is really good but licensing cost can be an issue even for big companies. I know there is GTK+ and wxWidgets but never used them. C++ needs a GUI toolkit but then again it also needs an XML parser and we don‚Äôt have that either.
&gt;C++ is far more sophisticated than C. That doesn't equate to better though. &gt;Sooner or later you'll learn to use lambdas instead of function pointers. What if you don't need function pointers? &gt;You'll use std::unique_ptr&lt;T&gt; more than legacy pointers, you'll use std::make_shared&lt;T&gt; instead of new What if you don't need new? As the link that /u/hahanoob provided that C developer had 13 total memory allocations in his code. Using stack based objects and RAII you may never need new at all. But if you need those things, if your code warrants them, then use them. But attacking someone who doesn't need them and doesn't use them isn't a productive way to present your opinions. &gt;You don't get to "pick" how C++ is. Actually you do as evidenced by the many other replies in this thread that all have varying ways of using C++.
Loved it on my MS-DOS days.
&gt; What if you don't need function pointers? Don't use it? I'm suggesting OP to use lambdas or closures instead of function pointers? Why are people twisting this message? &gt;What if you don't need new? Read above?
We also have situations like these, from avoiding compiler bugs in how certain intrinsics signal errors, to making sure certain instructions (like SSE instructions, for example) are not generated, to simply making sure a piece of code is as small as possible, etc. 
No is the answer.
[P04116R0](https://wg21.link/P04116R0)
CMake gives you all the tools to handle visibility on all platforms that support it : * GenerateExportHeader to create a file with macros that allow you to explicitely mark your functions as exported (since it's specific compiler attributes) https://cmake.org/cmake/help/v3.11/module/GenerateExportHeader.html * https://cmake.org/cmake/help/v3.11/prop_tgt/LANG_VISIBILITY_PRESET.html and https://cmake.org/cmake/help/v3.11/prop_tgt/VISIBILITY_INLINES_HIDDEN.html to set a default on the target if the compiler supports it (MSVC has no option for it) * the "nuclear option" if you have an old linux codebase where you always relied on everything being exported by default and want to port it to windows : https://cmake.org/cmake/help/v3.11/prop_tgt/WINDOWS_EXPORT_ALL_SYMBOLS.html 
&gt; Enabling and disabling specific warnings is one common use case for compiler flags. and then you add a warning that only exists in GCC 7 and people make issues on your tracker because they can't build anymore on debian stable. &gt; Optimization settings are another. I am of the opinion that optimization settings should be set by the person building your code, not by the build system.
Couldn't it also be instantiating a template with a negative constant as a parameter?
A large number of major code bases do use the STL and disable exceptions; the practice isn't limited to embedded developers. The goal is to allow these disparate environments to hew closer to each other and the standard.
Believe it or not, the world does not revolve around you. I was suggesting that maybe someone ELSE, you know, a person who ISN'T you, might have a use case that doesn't make sense with the dogmatic world you live in. Those people exist, they're out there in the world, and they have actual experience. It had nothing to do with whether your advice applied to you, but rather to do with whether your advice necessarily applied to everyone else in the world, which it doesn't. "not knowing what they mean" isn't something I said at all, and it doesn't even refer to an idea conveyed by anything I wrote. At no point did I even bother challenging your holy mastery of the bible of C\+\+\[current\_year\], even though that's clearly a major part of your self\-identity and challenging it really pisses you off. Move semantics are great, they're fantastic! If you don't need to eke performance out of the copying of objects because that's not how you designed your code, then you really don't have to use them. Again, you seem to be under the impression that if a feature exists, it's horrible to not use it, but everyone codes in a proper subset of C\+\+. No one uses the full standard.
&gt; Do you need to maintain invariants over multiple accesses to a group of state, or provide potentially polymorphic behavior over state? Then you need a "proper" class. No, not really. Structs and free functions work fine, though I wouldn't particularly recommend it, and wedding yourself too tightly to the state-means-class viewpoint makes it easy to overlook more holistic, data-oriented sorts of approaches.
I wish the word "minimalist" could be erased from programming lexicon... In any case, could you define what you actually mean by "minimalist" and "old-school" here? "old-school" often means more code than if using modern features, so it's hard to claim that it's generally more "minimal". Honestly, I think your mindset is wrong. If you don't know C++ very well yet, don't go into it thinking that you're going to commit to one particular style. Learn how the various features and STL classes work, and if you decide not to use them, have an informed reason why. Some decent points are made by "C with Classes"/"Orthodox C++" folks, but there's also a lot of horseshit out there about the supposed perils and overt complexity of Modern C++, which really boil down to the individuals commenting disliking changes to what they're used to, and clearly having never tried to use the new features they criticize for any reasonable length of time.
&gt;AFAIK the only way to do this is to capture the # of active exceptions in the constructor, store it, and then compare to that number in the destructor. Yep, this. This will always give accurate results AFAIU, but it definitely is a bit convoluted, and it is easy to na√Øvely or accidentally call `std::uncaught_exception()` and get subtly incorrect behavior. Also, hi! Didn't catch your username when I first responded. Funny to talk to you on Slack and catch you on Reddit the same day.
If you're writing code mainly as a hobby or something, whatever works for you is OK. If you're writing as part of a team, whatever works for you and your team is OK. That being said, I'd suggest getting familiar with one of the C++ unit test frameworks and try writing unit tests early and often. They'll save you time and regressions and show you where your design might not work as well as you want it to.
I'm getting a contact Java high just by reading that. :\^)
They deleted 2 of the Tony Van Eerd. Those we're bound to some of the best material.
Having an exception-free STL is only about making _STL_ palatable for people that won't use exceptions today. It does nothing for making _exceptions_ palatable to them, so they will have no reason to suddenly start using those. Again: nothing will change; this proposal royally screws one set of C++ programmers without any benefit for the other set. Oh look, people that already have abort-on-OOM are also not happy with it: https://internals.rust-lang.org/t/could-we-support-unwinding-from-oom-at-least-for-collections/3673/28 
I already mentioned structs (and by implication free functions), but structs simply don't maintain any invariants among members, you can easily set any individual member from any point in the code. You can of course try to have a convention to only use some set of free functions to operate on your struct, but in general there's no reason to have that convention when you have a built in language mechanism to make it easy to enforce it with the compiler. I'm not sure what "holistic" means in this context other than a synonym for "unencapsulated". As for data-oriented, I associate that with changing your layout in memory for improved performance, which is obviously important in some cases. But if performance is not an issue, if you have an invariant between difference pieces of data, the right thing to do in C++ the vast majority of the time is to put them in a class and encapsulate it. In C, of course you'll have to choose between an opaque pointer (which has significantly more and more universal performance cost) and convention, but in C++ you don't.
Or you could do it the easy way. int limit_3 = (limit-1)/3; int limit_5 = (limit-1)/5; int limit_15 = (limit-1)/15; int sum = (3 * limit_3 * (limit_3 + 1))/2 + (5 * limit_5 * (limit_5 + 1))/2 - (15 * limit_15 * (limit_15 + 1))/2; (Just be aware of overflows... &gt;.&gt;)
&gt; I'm not sure what "holistic" means in this context other than a synonym for "unencapsulated". Insofar as you consider classes encapsulation and encapsulation classes, there's no getting around that, but that isn't where I'm starting from. In my eyes a holistic design is one that looks at the big picture view of the whole problem at once, and starts by figuring out how to feed it, how to lay down the data so that everything is exactly where it needs to be at the time it is wanted. In contrast, the design I see more often with a classes-first approach is to take the problem *description* and carve little chunks off at a time, making what are perhaps neat little packets of functionality, but leaving the routing to grow (and tangle) organically.
If we were to start using `@` and someone wanted to complain that their keyboard didn't have an `@` key, well, they can't email us to complain, so we are fine.
That's because sending text to stdio doesn't look much different today than it did 30 years ago. The point is that graphics is a field that is a lot more complex and a lot more dynamic than printf() and std::cout. A 30 year old graphics API would be tied to things like indexed color palletes that are no longer applicable.
Very good reasoning here
I don't disagree with what your saying here, anyone who writes code that is unnecessarily complex or slow is a problem, and that is in any programming language. I personally don't like macros in C or C++, and it isn't that I refuse to learn C++ beyond the C with classes model or C++ best practices. I admit that I think that coding in the templating language is a bit overkill and way more complex to comprehend than most macros I've seen, and I'll likely never do template programming myself, but where others have done so and provided a nice capability that I could use, I will use it. It is actually the opposite, I've worked quite a bit with the latest capabilities and have determined, at least for my own programming activities, that I just don't need them. When working on a team I follow the defined models and standards, but for me personally, a C with Classes model meets my needs. I have actually been coding off and on with C++ since before it had its own compiler and we used tools like CFRONT to parse C++ and spit out C code to feed the C compiler. Now based on the link the /u/habanoob provided, that write up got me thinking about some of the features I do take advantage of in C++ and that I might not even need those and could go straight C, but I would rather use C++ as a C with classes approach then to create an object oriented simulation in straight C. 
The might come to the next WG21 meeting and # you to dust or / you to pieces. 
Ok now you're contradicting yourself. I said I use a small set of C++ capabilities that I need and find useful and your response was vile, now you're saying don't use them if you don't need them. 
Thanks for the share.
Agreed and it is unnecessary too, because as far as I know there is a solution for timeouts for all APIs that ASIO touches on all major OS platforms so there really is not even a good reason to use a deadline\_timer. I ended up having to ditch ASIO usage for a serial line protocol because it was guaranteed to have fallouts and lost packages (it is running in a very bad electrical situation) and I had freezes and the like many times. I ended up going for a rather simple thread \+ nonblocking polling and a pipe2 call to enable me to kill it if it is in a poll and I want it to end early. Worked much better than ASIO unfortunately (it is a problem because now I have more code to maintain rather than just using a library that I use extensively in other places). 
Wow, sanity. How refreshing.
why not a `shared_ptr` so we can share the love
Ahh, I forgot about it: Big O notation (without caching): Map has search complexity `lg(n)`, sorted array also. 1) But If we want sorted array to be useful, that array must be sorted. You've forgot about resorting cost after insertion. Let say we make one iteration of insert sort so `n` vs `lg(n)` of map. So there are some pros for std::map. With thinking about caching: 2) unordered_map's memory is also contiguous and there's really good amortized complexity. 
Because sharing is just another form of leaking. 
**Company:** Exegy, Inc. **Type:** Full time **Description:** Exegy, Inc. has a full-time internal Sr Software Engineer opening in St. Louis, MO. The senior software engineer participates in the design, implementation, documentation, and debugging of time critical, multi-threaded, distributed applications that interact with Exegy‚Äôs custom hardware acceleration devices. Successful candidates should have a strong work ethic, work well with others in a team environment, and the ability to quickly learn complex concepts relating to market data processing systems. Qualifications C, C++, Object Oriented Design, familiarity with Linux operating systems (RedHat, CentOS, SUSE Linux), and experience with debugging tools. Preference will be given to candidates with experience in STL, network programming (sockets), multi-threaded application design and/or real-time software development. Exegy provides ultra-high performance hardware-accelerated computing appliances that efficiently process and enrich market data for the world's leading financial organizations. Exegy's unique market data products respond to financial organizations' growing demands for low latency, reduced cost of ownership and flexibility. **Location:** St. Louis, MO. **Remote:** No **Technologies:** Strong background in algorithms and data structures. Strong skills with Object Oriented Design and concepts. Excellent C &amp; C++ programming skills Experience with scripting languages (shell, Perl, Python, PHP, etc) Experience with debuggers and complex debugging techniques. Knowledge of profiling, memory leak detection and other code analysis tools (ex Purify, valgrind, Bounds Checker, V-tune, CodeAnalyst, etc). Understanding of Unix/Linux operating systems and programming environments. Understanding of distributed systems and data structures, both in design and implementation. BS or higher in Computer Science or Electrical Engineering and/or relevant industry experience. Low-level operating system knowledge and/or device driver development a plus. **Contact:** dblack@exegy.com
Is there an alternative that does this?
Don't strive for minimalism; strive for elegance instead. Being overwhelmed by C\+\+ is an unfortunate fact of life for newcomers to the language, and slowly building up your knowledge of the language (and by extension, your use thereof) is perfectly fine. But don't turn that into dogma: don't restrict yourself just because you've chosen to restrict yourself. Learning something new, occasionally, keeps it fun!
I thought facebook folly had an implementation, but now I can't find it. They definitely have a backtrace implementation that you can leverage, at the very least. It's not very hard to write the rest on your own, but you could look around and see where has it. I wrote one at the company I work at and I use it extensively.
`@attribute` is fairly common in other languages (Python, Java, ...), and I think I'd find it nicer than `[[attribute]]`. But it would have conflicted with Objective C I guess?
There is nothing wrong with using a simple function if that is the best / a good solution to your problem. What doesn't make sense is to minimize the use of c++ features out of principle because often this can make code more complicated and/or error prone
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8pjs8j/what_is_this_weird_constructory_syntax_cc/e0kdh9h/?context=3.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Perhaps you should avoid attempting to deflect criticism by resorting to personal attacks.
Do you know why it was rejected?
Why is that, and what part exactly has been rejected? I read most posts and trip reports posted here on /r/cpp but haven't read anything about metaclasses in the last couple of months. Apologies if I've overlooked it.
I like the use of `@` for decorators in Python. However the fact that `@` got (ab)used for matrix multiplication in numpy is one of the ugliest things ever. I'd rather write `a.mul(b)` than `a @ b` (what the hell is that supposed to mean to anyone not already familiar with it...), but then concatenated/nested expressions get so ugly so quickly...
I'm not entirely sure... I wasn't at the meeting where this was discussed. There are some older systems that provide headers that use `$` as part of an identifier (I found at least one in a GCC header with some dtrace stuff). Maybe somebody who was there, or who knows why will comment.
Only the use of `$` as an operator has been rejected. The metaclass work is a bit quiet because of some we've had to rethink/retool some of the core features (like static reflection). It'll ramp back up over the next year.
I'm confused. The subject is contexpr. Any sorting is going to be happening at compile time, and the data structure is going to be constant. There is no insert. That's how constexpr stuff works.
I swear your mindset just comes from insecure C programmers who don't want anyone to have the best of both worlds. Seriously people can program however the fuck they want, why restrain yourself? There is nothing wrong with what OP is doing.
&gt;and then you add a warning that only exists in GCC 7 and people make issues on your tracker because they can't build anymore on debian stable. Perhaps Debian stable's version of cmake is too old for "modern" cmake to work on anyway...
Are you trying to troll me? If a problem needs a function pointer, I suggested OP to use a lambda or closure instead. If a problem does not need function pointer, don't use it. How does this translate to "use a small set of C++"? I'll use the full potential of C++ as soon as I get the chance to exercise them?
Please don't make posts with vague titles. 
I think so too. I worry that graphics is just the first such large library that will be rejected with one of the many excuses being ‚Äòwe need a package manager not a library for ABC‚Äô. As a mostly passive observer I sure hope some good comes of this fiasco... (To be clear I am not claiming this graphics proposal should have been standardized. only that a standardized graphics API would be nice. Nor am i posting this to argue and defend my opinion)
We also have TS‚Äôs now. There is no rule that says they must be merged.
Great points! Moore‚Äôs law was alive and well for 25 of those years. We long since traversed into the era of good enough computing. Not once has the UI of an application I‚Äôve been paid to work been my value add. I just want a standard UI in c++ that is cross platform so that I can focus the parts that pay the bills... I‚Äôm sick of writing and debugging language inter-op layers.
It's clear that mistakes were made when adding features to C++. The members of standards committee even agree. I can't talk about &lt;locale&gt;, as I've never used it. Criticizing &lt;cstdio&gt; or &lt;c...&gt; is maybe a bit unfair, because easy interfacing with C, and an easy "upgrade" path is a goal of C++, so supporting the traditional C headers seems ok to me. About graphics specifically, I feel that tight integration with the hardware is important, and the hardware moves fast. OpenGL has been around less than 30 years, and almost all of its original features or ways of doing things are deprecated now. DirectX, Metal, Vulcan, you name it, they're all younger than 30 too. 
Can you tell me what's C++Now? Is this an organisation like Pycon or cppcon? 
What about string literals as operators? As in: &gt; "my operator"op UDLs without underscore are reserved by the library, and we can just give `op` a special meaning.
:)
Why don't you have any info about people of colors or women on that page? 
I think there is no need to value initialize T?
My feeling is that LT could say exactly the same without making people around feeling insulted.
Well, yes, but if you scream and shout you get noted (looking at you Trump).
&gt; Because `bit_cast` gives you a brand new object, which `reinterpret_cast` does not. That's also the case in my 2nd example. Today that's an invalid use of reinterpret cast so no programs would be broken. 
Last comment was about comparing 1 to 1. ( (constexpr)std::map should have shorter compile time and because allocated on stack its memory also should be contiguous) I just want to say: there's some other reason why choosing good container is important. 
Username checks out
Why not use [bit fields](https://en.cppreference.com/w/cpp/language/bit_field)?
Not sure, how your post is a response to mine (have you read more than the first few lines?). My opinion: people can program however they want as long as code quality doesn't suffer and they deliver on time.
One issue with this is that using the underlying type of the enum may not be big enough. If you fill a char-sized enum with 256 values you need a 256-bit sized type. I think you should let the user specify it and default to size_t, not the underlying type.
Good question. I think enums allow a bit more flexibility. For example, if you need to store a piece of data that indicates which option to set, it's easier to do with an enum (can't store the "name" of the bitfield, and you're not permitted to take its address or offset either IIRC)
&gt; and then you add a warning that only exists in GCC 7 That's what version checks are for, just like there are different flags for msvc and clang. . More often it is the other way round anyway: I have to temporarily disable a specific warning on a new compiler because we compile with -Wall -Wextra on GCC and clang and the code base isn't yet clean with respect to that specific warning. 
I would assume because they do not care about the gender or race of the volunteers, as long as they are passionate about C++ and want to lend a helping hand :)
in my opinion it is not clever to hide the power of two part behind this template. it eliminates the underlying mechanic of the whole bitwise operators and may be bad for someone that doesnt have a real grasp of what bitwise operators actually do.
Pixel 2 and many CrOS devices ship clang built kernels.
WTF. Someone really has high level of humour.
What?
That would not be enough as I can assign arbitrary values to the elements of an enum enum class Color { red = 0xff0000, green = 0x00ff00, blue = 0x0000ff }; In the example above, the type would need to support \~16M bits!
Cool, thank you very much for the reply!
So I've been thinking about a compile-time way to catch this, and seems like something along the lines of template &lt;typename X&gt; constexpr bool is_valid() { return static_cast&lt;typename std::underlying_type&lt;X&gt;::type&gt;(std::numeric_limits&lt;X&gt;::max()) &lt; std::numeric_limits&lt;std::underlying_type&lt;X&gt;&gt;::digits; } would do the trick (need to call it as part of the enable_if) 
Not if the two types are of equal size indeed, but then there's also no need for the std::min. It's in case the result type is larger, in which case the upper bytes would be uninitialized. Granted, an int is usually 32 bits, but that is not guaranteed by the standard.
It was a figure of speech that meant 'please give us something saner'.
Bit fields are awkward if you need to deal with more than one field at a time. 
Correct, the solution in the book is wrong. It should be: if (i % 3 == 0 &amp;&amp; i % 5 == 0) So 15, would be the first number, which is dividable by 3 AND 5.
Bit fields are not very good in terms of portability (signing, endianes).
I did something similar some time ago to wrap in an acceptable way the RabbitMQ library for my needs at the time. [here](https://github.com/daedric/commonpp/blob/master/include/commonpp/core/Options.hpp) is the code and here is an [example](https://github.com/daedric/commonpp/blob/master/tests/core/options.cpp). The generation of the operator&lt;&lt; is the only thing you did not do however you added the external operator which is a nice idea, however I would have put them in a special namespace to avoid this side-effect in a general code. 
\*\*Company:\*\* [ArangoDB](https://arangodb.com/), [ArangoDB Career Page](https://careers.arangodb.com) \*\*Type:\*\* Full time \*\*Description:\*\* We are looking for a C/C\+\+ Windows Developer (m/f) to take care of the Windows version of ArangoDB. ArangoDB is a native multi\-model NoSQL database. It combines the power of graphs, with JSON documents and a key\-value store. Oh, and did we mention it is open source? \*\*Location:\*\* Cologne, Germany \*\*Remote:\*\* Yes, but not preferred \*\*Visa Sponsorship:\*\* No \*\*Technologies:\*\* Currently using C\+\+ 11, C\+\+ development using Visual Studio, Windows \*\*Contact:\*\* Apply via [Full Job Description](https://careers.arangodb.com/p/4f3df03efaca01-c-c-developer-m-f) on the website or send an email: gudrun@arangodb.com
Loosely related question to r/cpp: what's your opinion on: typename = typename std::enable_if&lt;std::is_enum&lt;option_type&gt;::value&gt;::type vs `static_assert`?
strong typedefs would be such a win in terms of code correctness
&gt; Again, you seem to be under the impression that if a feature exists, it's horrible to not use it, but everyone codes in a proper subset of C++. No one uses the full standard. No one said that and you are attaching a straw man here. The argument wasn't "Not using a c++ feature is bad" but "if a problem can better be solved with a particular c++ feature it is bad style to not use it just because you arbitrarily restrict yourself to a certain language subset". I.e. no one will blame you if you don't need to create an object in the heap and thus not use std::make_unuque, but if you need to create an object in the heap, you better use `std::make_unique/make_shared` instead of `new` 99.9% of the time (there exist some rare cases, where `std::make_unique` doesn't work/isn't useful)
This is the same idea as projections in range-v3 right? Nonetheless it looks great and I wish I could use it (I develop on Linux but we also need to build using msvc)
I would argue that reinterpret_cast is the correct way
In order for this attribute to be useful, you would have to put it practically everywhere. Wouldn't it be better to have a debug compilation mode or static analyzer pass to perform the check automatically on each and every functions, without having you put an attribute?
I think it can be easy to confuse the complexity of the language with the complexity of programs written in it. A lot of what the standard is currently doing is adding complexity to the language so that our code can actually be simpler. For me, using C++ as an improved C doesn‚Äôt lead to simpler code. Simpler C++ code comes from using a lot of the C++ specific features and especially some of the newest features in a straight-forward manner. And relying on good libraries to hide the complexity when it comes to stuff that we don‚Äôt yet have a simpler way of doing.
WHY DON'T YOU HAVE ANY INFO ABOUT PEOPLE OF COLORS OR WOMEN AT THAT PAGE?
yeap, there are only simulations: [https://github.com/jm4R/explicit\_tuple](https://github.com/jm4R/explicit_tuple)
Nice post history you got there, bud. Never would've though that C++ community has such vocal pedophiles, but oh well... 
I don't understand why you want an attribute for this. It should just be a compiler warning that is always on - and maybe an attribute for opting out. We get so many defaults wrong!
I don't understand why you want an attribute for this. It should just be a compiler warning that is always on - and maybe an attribute for opting out. We get so many defaults wrong!
I do not find a justification to do that.
Writing `if constexpr(is_constant_evaluated())` is not correct. That function is used to check if we're in a context that requires constant evaluation - of which `if constexpr` is one. So that's equivalent to `if constexpr(true)`. All of the examples in the blog get this wrong. There is also no Russel's paradox of constant evaluation if and only if not constant evaluation. It's specified in terms of requirements.
Not just correctness, but also better optimization: two `std::string &amp;` parameters might refer to the same value, but a `Name &amp;` and `Email &amp;` are inherently different types and allow for the compiler to optimize the code as such. It's a [negative-cost abstraction](https://www.youtube.com/watch?v=SWHvNvY-PHw).
Not trying to troll you, trying to understand you. Me: &gt; For me, the C++ coding I do is really C with Classes. I do use the container classes, but mostly I write C code and only occasionally create classes and then predominately as plain old data objects. You: &gt; I have no words to express how much I hate your type. Please stick to C. Me: &gt;What if you don't need new? You: &gt;Don't use it? So first your saying if I'm not going to use C++ features then I should *stick with C* but now your saying *just don't use* features I don't need. Which is exactly what my original post stated I was doing that you then commented on. 
I dislike exposing the bitwise operators to the users. They are highly unsemantic in this case and expose a very low\-level\-view on the topic. I'd prefer something like this: enum class color {red, blue, green}; using colorset = my_set&lt;color&gt;; auto cols = colorset{color::red, color::green}; assert(cols.is_set(color::green)); assert(cols.any_is_set(color::red, color::blue)); assert(cols.all_are_set(color::red, color::green)); assert(cols.none_is_set(color::blue)) cols.set(color::blue); cols.unset(color::red); assert(cols == colorset{color::blue, color::green}; This is just so much clearer than checking for `cols &amp; color::red` (the ampersand means ‚Äúand‚Äù in normal language, not ‚Äúis any bit set in both arguments?‚Äù) and allows for tons of intuitive and useful versions like above.
You seem to be arguing that you can't trust someone to understand the language. Where do you draw the line then? 
What's your opinion of BOOST_STRONG_TYPEDEF?
cppnow.org http://lmgtfy.com/?q=c%2B%2Bnow It's the best (and oldest?) C++ conference going. It is small - max 150 people. Which means speakers tend to be much more approachable. Mostly run by the same people as CppCon. It tends to be a bit more on the advanced-topics side than CppCon, but there's always something for everyone.
Not if I "!, you're dead" them first.
Taking a dependency on the very heavy libcurl for making such simple requests seems overkill...
/u/ihcn I would love your feedback on "resumable expressions" http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4453.pdf please email me vinnie.falco@gmail.com Thanks!
I've been happily using something similar based on [this](http://blog.bitwigglers.org/using-enum-classes-as-type-safe-bitmasks/) implementation for some time.
It is macro\-based implementation which is bad. There were many probes to simulate strong typedefs in C\+\+, almost all of them has limitations. My own implementation: [https://github.com/jm4R/explicit\_tuple/blob/master/CheatSheet.md](https://github.com/jm4R/explicit_tuple/blob/master/CheatSheet.md) During writing this code I met some problems. For example: * operators ‚Äì if you have own type `velocity` and `time` (both based on float), should it be possible to divide `vel/tim`? What should be a type of result? `acceleration`?
&gt; the ampersand means ‚Äúand‚Äù in normal language, not ‚Äúis any bit set in both arguments? To play devil's advocate: anybody who is accustomed to using flags of this based on regular enums and just wants a more type-safe version of the same likely wants it because they prefer the terse semantics and compact in-memory representation. That's certainly why I have a version of something just like this in my codebase.
What if you disable operators by default but allow users to explicitly define them themselves?
Of course they can do it, by deriving and adding operators, but it makes some pain to write it. Maybe it should be easier. But I have no any concept‚Ä¶
I think this suggestion for it being an attribute was for it to mirror `noexcept(auto)` exactly. It's effectively a trial of `noexceot(auto)`. That said, you are absolutely right that noexcept validation would be better as a default rather than an exception
Oooohhh I just read your user name. Why do they need to be special? Or why are they different from 'people' as it says on all the pages. They are people right?
&gt; As the time window for new features is closing, it is now very improbable that modules will still make it to C++20 [...] Maybe I've missed some obvious signs, but I didn't get such an impression. The impression I got is that if enough progress is made for the next meeting, another attempt to merge (perhaps a subset) of the merged proposal into the standard is plausible. There is also talk of an out-of-band meeting to work on modules that could help with that. I felt that everyone recognizes this is a cornerstone feature so special accommodations regarding cut-off times could be made.
\&gt; Well, some other languages seem to be doing fine without those constraints. Can you give some concrete examples?
Yes, sure, it's still possible Modules make it in some shape or form. I wouldn't rule it out at this point. I just think it's improbable. But that's of course my very subjective personal impression.
Why is that a concern? The client shouldn't have to concern themselves with the implementation. I don't think it's wise to code the internals of our layers of abstraction to the specification that the least competent engineers should be able to understand them.
That is very, very sad information. Modules would be the biggest improvement since C++11 announcement
It says that this is the first time we have CamelCase in the standard. Is that really correct? Hasn't the iterator categories always been CamelCase? Making concepts CamelCase just seems to stay consistent with the past.
How much of Ranges are we getting? It's just the basics right? Versions of the current algorithms that take ranges instead of iterators. That's great and all but it's with actions/views and piping it becomes really exciting and useful!
Don't strong typedefs start to work a lot better with the `get&lt;N&gt;(t)` utility functions and generic definitions of library functions in terms of object members?
I would go further and say it would be really nice if `noexcept(auto)` was the default. I.e. the absence of `noexcept` qualifier should imply `noexcept(auto)`. Think about it, if it's possible for the compiler to infer a function noexcept-ness, why would you not want the compiler to go ahead and just do that? I believe there are already rules in the Standard to infer noexcept-ness for things like default constructors. Making such inference rules wider should substantially reduce the need to use `noexcept`.
Well, if it would have caught the problem that caused a spacecraft to crash into Mars then the pain would have been worth it!
Yeah you need to be able to inspect the amount of enum values at compiletime. The max value of the underlying type would be way too large in most cases.
But my main point was that using the underlying enum type is just not useful the way it works right now. User configurable type that defaults to something large is already a lot better. If you can get the actual maximum it would be perfect but I don't think its possible.
In their description: yes. In code: no. e.g. We have `InputIterator` as a notion, and `input_iterator_tag` as an example of code that exemplifies that notion. Concepts, however, are going to be actual compilable C++ code, expressed in CamelCase.
Seems like concepts might become the only significant change in c++20. What really annoys me: Modules and coroutines have been in the making for years (if memory serves right even before c++14 was standardized). Viable implementations have been produced and tested but now suddenly (ok, rather 1-2 years ago) people come up with all their concerns about the fundamental design aspects that will block them for yet another release cycle. I understand the desire to not bake the "wrong" design into the standard, but at one point people have to accept a good enough solution, otherwise nothing will ever get done (don't get me started on concepts). It seems to me the committee is losing its ability to ship major features (imho the last time it demonstrated it was c++11)
I believe so far we don't get anything, but yes, that is the most likely outcome.
Yup, it seems like they are losing the ability to ship new features every 13 years.
I'm always counting from c++03 ;)
CamelCase has always been the C++ convention for templates though. Yes I agree this convention is now elevated a bit. Bit it's still consistent with the past
They're underpaid and generally oppressed by white male developers.
&gt; The parts that were presented to LEWG were section [...] 4.3 (‚ÄúTreat heap exhaustion specially‚Äù ‚Äì make the currently throwing version of new a fatal error instead ‚Äì this would allow most of the std library to become noexcept!). The room voted strongly in favour (almost perfectly unanimous) of both these ideas. That was the part that was surprising. I am surprised too! I had always seen strong push back in replacing `std::bad_alloc` with `std::abort` (or similar) in discussions, mostly from people interested in embedded/kernel work. For the kind of projects I work on, usually running on Linux with overcommit on, `std::bad_alloc` has never made any sense. The only few times I saw it was because the size requested was so massive (underflow...) that the allocator gave up. Has there been any discussion of the performance impacts? I would expect better code generation in many cases: - some libraries have more optimized paths if operations are `noexcept`, - the compiler should have more room for optimizations (less execution paths), - the generated side-table for exception unwinding should be smaller (less throwing paths). 
With C++17 I would vote for using static_assert and constexpr if for type validation instead of SFINAE with enable_if. Other than that, nice article.
I agree with you. I just wanted to elaborate a tad for the benefit of others who aren't as well-read :-) 
This is the wrong kind of tasks for a generic strong typedef library. If you want to work with physical quantities I'd recommend using an actual units library.
Is it camelCase or PascalCase though?
Nothing is set in stone yet, but the basic concepts and enhanced version of &lt;algorithm&gt; (constrained calls, projections, range-based overloads) seem almost certain, according to Eric Niebler on Twitter. P0789, which proposes the first dozen or so Views, is also well advanced, so I‚Äôd be surprised and disappointed if that doesn‚Äôt make it into ‚Äò20 as well.
At this point, it's bad style to assume every programmer is going to agree with you that they even ought to be comfortable enough using every feature of C++. I agree, they should generally make use of features which are an improvement, but if they're not familiar with those features, and their project doesn't interface with people who want those features, and they don't have learning those features as a goal, it's ludicrous to HATE them.
Modules are certainly most expected, but the Contracts also look interesting :)
This was only an example. More generic question is - should all possible operators be avaliable for a) 2 typedefs b) typedef and underlying Type c) 2 different typedefs with the same underlying Type? Because from my intuition, operator+ should be defined for a), but operator* should be defined for b). But I am not sure if my intuition will be simillar for such library users.
Assuming we are talking about numeric types, I would agree with that intuition. Generally, I'd say the Chrono library serves as a very good example (including the distinction between time points and durations)
lowerCamelCase has never been used in C++. Well except back in the days when people made up their own naming conventions.
I think delaying module will hurt the C++ community. Sure modules should not be an half-assed feature, but the lack of module will hinder adoption, development of new small and large scale projects and will further contribute to the fragmentation of package consumption and development. C++ desperately need modules to stay relevant. More so than concepts or even reflection. I really hope that they will make it to C++20. If they don't, then I hope all implementations will offer them so we can use them before 2023 ( I'm looking at you clang! )
That's good to know! I never even bothered to check and just carried over my Java habit, I'm ashamed to say!
*sigh* I didn't want to have to go here, but... &gt; over 100 pages of high-quality wording I think it's a bit misleading to categorize a proposal that has never been reviewed by the library wording group or any library wording expert "high-quality wording". As discussed during the meeting, there were a variety of flaws in the P0267 design that had been raised years ago and were never addressed. A large part of the opposition came from the lack of high-quality wording. Some people have tried to make it sound like P0267 was fully baked and ready to go, and the committee said "no graphics". That is not accurate. The facts on the ground are: * P0267 was not sufficient baked and not sufficiently "C++". Fundamental issues had been identified with the proposal and they were not addressed for years. The people who provided that feedback got fed up and stopped giving it. People started leaving the room when the proposal was being presented, because their feedback was ignored. Go count up the number of total votes on P0267 straw polls as a function of time. The proposal was no where near ready to ship. Thanks to Guy Davidson's efforts, it recently had started to improve - but it had a long way to go. * There is no consensus on the committee to prioritize a 2D grspgics. There is a large part of the committee that is opposed to this direction. I decided it was time for that majority to have a voice. Either of these two points would be enough to warranty re-evaluation of the further investment of committee time. Both of them demand it. Here is a summary of the specific, concrete issues with P0267: * No fully batched rendering API, which artificially limits implementation strategies and forces an inefficient rendering model. * Not designed like a standard library. basic_path_builder is a clone of vector with some extra member functions. Do we really need another string? The point and geometry types are not cleanly designed - the value type is not parameterized, the matrix element access members (m00, m01) are weird and do not follow precedent. * The current proposal is needlessly opinionated about color models. I will repeat this again: **multiple library wording expert have estimated that LWG would need to spend an entire meeting of time to ship P0267 as TS - and it's not even in LWG yet. Are you willing to spend 1/6th of the time LWG will spend on C++20 working on graphics instead? ** (Otherwise, great report.)
Hmm, thanks for your opinion. I will maybe extend my library to address this issue. But the main part of it was to build `type-driven tuples` anyway.
Glad to see you've seen the light! Using lowerCamelCase in C++ is as silly as using snake_case in Java
It is PascalCase, many know not
Clang? GCC made the most progress currently AFAIK .
The way it is going, I bet Java will be getting proper AOT compilation on OpenJDK and value types before ISO C++ and all major compilers finally get modules.
Python, Rust. Plus probably most "young" languages. Some even have multiple implementations (Python has plenty of them). They are developed online. It's not different from a software project actually. So *maybe* in the C++ case it would cause a problem in the ISO context, more than any theoretical issue. In which case maybe ISO is more part of the problem. Arguably I know of one big technical problem in the ISO framework in the context of programming languages: we are not supposed to continue to use old versions once a new one has been published. This does not match the practice, nor has been taken into account in the most recent development, on the contrary (MSVC has been sort of forced to introduce a version param whereas they previously did without) 
Sorry that wasn't clear. I meant that we will actually have user-facing *identifiers* named with CamelCase defined in the actual standard library. That wasn't the case before as far as I know, with very few exceptions such as std::ios_base::Init which starts with a capital `I`. 
Oh yes. I meant `PascalCase`. Sorry.
I have now also fixed it in the actual blog text. Thanks.
Java will turn into Kotlin. And Java in most scopes is not competitive to C++.
Anyone knows what about static reflection? I'm tired of writing `EnumToString`- and `StringToEnum`-like functions.
Thanks, Bryce. This is a very controversial topic and the feedback you give here is great. It seems to me that actually, LWG being such a bottleneck is a huge problem in itself, regardless of the issues of P0267, and I wish I knew how to fix that.
I like to call it `TitleCase`
Not a fan of Contracts or Ranges?
We moved last week to send the Reflection TS to PDTS ballot. So, expect it to be published some time this year. And hopefully to get reflection into C++23.
Herb is simultaneously proposing adding `try_` methods for those that allocate, which would allow people who care about memory exhaustion to avoid termination.
Yah, it's becoming a big problem. I'm not sure how we fix it. We need to pipeline better.
Also, just to make it clear: I am not saying that the existing Graphics proposal is perfect. However what I do see is that the way this played out will probably result in a massive discouragement for anyone to undertake large proposals for C++. And that's a loss imho.
Can I ask what (m/f) means in your job posting?
I've seen at least 2 different proposals of it. Do you know which one is the nearest to be merged? PS: 23 is freakin too long :(
&gt; needless complication A lot of what is complicated about C++ is either due to compatibility with C, long-term backwards compatibility (it's amazing how much old code is still in use), or edge cases that are not obvious without extensive consideration. I think C++ simply can't impose more restrictions with something like this without breaking one of these things. I think this is one of the main reasons why everything passes through the standards committee at a snails pace :(
Hi, please update your post to follow the template. You are missing Visa Sponsorship.
So kind of like a TS then.
Hi, please update your post to follow the template. You are missing Visa Sponsorship
Hi, please update your post to follow the template. You are missing Visa Sponsorship
we already know what happens next. Roughly all opensource libraries built on top of glib use g_malloc (which terminates) instead of g_try_malloc, and reliable software has to write its own libraries.
Yes, the current TS draft http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/n4746.pdf is the "template metaprogramming" approach of Chochl√≠k/Naumann/Sankel. So this one will go out to a TS publication and thus technically "nearest" to be merged, although it's not near at all yet and the other (value-based) approach by Sutter/Sutton is also still actively worked on afaik.
Reflection is still a very exploratory space. We have no idea yet whether the TMP/type-based approach or the value-based approach is the way to go. So it would have been a mistake imho to consider any of it for C++20. It's just too early.
I wouldn't call either of them a major feature (full ranges more than contacts though). And let's see, what part of ranges ends up in the standard.
Contracts are amazing and I am really looking forward to using them.
I wouldn't say we are "delaying" modules, I believe that almost everyone on the committee wants them and thinks they should have been ready yesterday... but we cannot merge anything into C++ until there is a single modulers proposal that has the consensus of the committee.
So what's stopping you from just wrapping your types in a class? Doesn't that give you the exact same benefits?
Well it wasn't easy for Concepts which had been in the work since 2006 and was turned done in the form of C++0x and then again for C++14, and now it's finally here. It's never easy, but I believe caution is preferred to avoid shipping a flawed design that requires breaking changes to fix. (And you can tell I'm a Concepts fan.) I think the C++ standard is so important to the industry that stability is crucial. I'm keeping an eye on the discussion regarding language and standard library directions.
Code verbosity really. It'd be nice to be able to define a new type without it having to be a struct, e.g. using distinct Centimetres = double; using distince Inches = double; // assignment Inches operator=(double d) { return Inches{d}; } // conversion Inches operator=(Centimetres c) { return c / 2.54; }
If you're writing your own reliable software, why wouldn't you use `try_push_back`? What did you do before when `push_back` throws?
No guarantees, but it looks like full ranges will be there...
Those guys mostly use ADA, as far as I know, and anyway, they stick with compilers that are at least a couple of decennia old. Operator overloading is not on the approved list of features for flight software, to the best of my knowledge. 
Is there a good discussion somewhere of what compelling benefits they bring over `assert` (or custom assertion macros)? Like I know of a couple, but they seem pretty relatively small to me, so I'm not getting why lots of people bring them up as this awesome feature they finally have as opposed to it being "just another" incremental improvement.
Yeah that's what I meant. MSVC and GCC are advancing pretty good on module support, whereas Clang is the only one of the major compiler that didn't really invested in a module implementation.
How I Post My Article on that website?
IIRC add_compile_options doesn't help when you want different flags for C and C++ sources (but CMAKE_C_FLAGS / CMAKE_CXX_FLAGS does).
roll back, drop request, take next batch, drop caches, choose another space/time tradeoff, there are many ways apps handle OOMs.
The cmake library finders I've used have been pretty low quality, and I end up writing my own in the end anyway. So for me, cmake doesn't provide any cross-platform advantage over waf/scons/autotools/tup/rake/gmake/etc. I do like the pretty colors and readable build output that cmake gives out of the box. I honestly think that's why cmake beat scons and others 15 years ago.
I sat in on Herb presentation, and intend to be there when he takes the whole paper in -- can you clarify for me your concern? I don't believe you finished the first sentence, and the second sentence is nothing you can't detect and accomplish from a return value instead of an exception...
That sounds great. If I can help in any way then let me know.
Return value works when there is no call stack between the action ("open file", "price a mortgage", "run a data storage query", "preview page for printing", "") and error (a call to make_shared, vector constructor, etc). Herb's paper lists this as "Big operation that causes lots of little-but-related allocations", and suggests using `set_new_handler`. And then what, longjmp? How do I go to the appropriate handler? How do I guarantee destruction of everything that was allocated so far?
What you've described are *concrete types*, and are not difficult to implement with template typedefs, with template parameters representing dimensionality.
Were the breaking chnages in the iterator concepts (proxy iterators, input iterators) merged?
Is there any way or proposal to determine if a specific argument is known at compile-time?
Please don't post vague titles.
Named template parameters would also be pretty swell. It's on my C++ extension to-do list for my GCC and Clang forks.
template &lt;size_t N&gt; operator /**/ (const char (str)[N]) ...
-memotive-extensions
Only when the JVM gets written in Kotlin, until then it is mostly irrelevant for the majority of Java developers. https://trends.google.com/trends/explore?q=kotlin,Java
Doesn't really matter... how would you disambiguate // from //?
Shouldn't it be @tribute?
My one issue with RAII is that it can sometimes be non-obvious what's going on or what will happen. I've always been fond of C#'s resource scoping with `using`, as both the resource, it's lifetime, and the scope are very obvious.
That would be really nice (personally I'd already be happy if msvc could finally compile ranges-v3)
Throw an exception?
You could use [Boost.HOF](https://github.com/boostorg/hof), which works for MSVC and can be installed independently from boost(with just `cget install boostorg/hof`).
Well, for example, contracts can be used as hints to the compiler to simply assume that a precondition is true and to optimise for it (the violation of a precondition becoming UB in this case). So they can lead to better optimised code in many cases. `assert` can't do that. Previously you could do such things only with compiler-specific builtins, such as MSVC's `__assume`.
The only difference is the reporting mechanism (exception from new, or bool from `try_`) -- you still have a chance to recover; under his proposal you're just rotating the problem space.
Not `-femotive-extensions`?
I don't think there is any generic answer to these questions, nor should a library or even a language feature attempt to. The user should be able to decide which operations they want to embed into. I do a strong typedef where you can opt into various operations using CRTP, which then just forward to the underlying type. It's slightly verbose to write the CRTP policy classes but it's a one time cost and honestly you are using the same couple dozen 99% of the time: comparison, arithmetic, &lt;&lt; &gt;&gt;, hash, etc. Obviously it would be nice if that boilerplate weren't necessary at all, but at least it's library only boilerplate.
Could you not just build a value based approach on top of the TMP/type-based approach? If so, you would gain a lot by adopting the TMP paper.
Camels can have two humps, you know.
I wasn't in Rapperswil, and I'm happy to be corrected, but as I understand it: * The fundamental low-level concepts in [P0898](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0898r2.pdf) will be merged into the IS * Eric Niebler's "Deep integration..." paper ([P1037](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1037r0.pdf)) was reviewed by LEWG, who agreed to it as the path forward. This abandons the previous `std2` direction in favour of putting all the new stuff in a nested namespace `std::ranges`. No existing code will be broken: iterators that don't meet the new requirements (such as being `DefaultConstructible`) will still be able to use the old algorithms in `std`, but not the new versions in `std::ranges`. * [P0970](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0970r1.pdf) "Better, safer range access" was agreed to by LEWG as well, except they want to make things even safer by forbidding calls which may return dangling iterators (the proposal used a `dangling` wrapper to warn users about this). * [P0789](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0789r3.pdf), which adds some of Range-V3's Views to the *Ranges TS*, had its wording reviewed by LWG. I assume the paper will be revised and "rebased" on top of the other ranges proposals for San Diego. I think that about covers it, but like I say, I wasn't in Rapperswil so this is all second hand information -- take it with a pinch of salt.
I personally am saddened graphics is dead, but the writing was on the wall tbh.. But modules... come on. If they miss c++20... I get there may have been legitimate issues with the original TS ... but the optics from my POV is that the only problem was macro related.. (please correct me)
FWIW, my opinion is that the Python language dev process is a good example of what not to do. There are quite a lot of PEP's that go through that are very difficult to follow and read, are also of very poor quality, and end up creating a big mess for everyone else with very little oversight. The process as a whole has led to gratuitous breakage that has made it extremely difficult to install and use python packages in diverse environments in practice. The 2\-to\-3 transition has been awful and for a lot of people it has basically killed the language. Python is supposed to be an interpretted / JITed language that is easy to use and install, but instead it has become as difficult if not more difficult to write code that will work on all the linux distros in use at your company than it is to write C\+\+ code that does the same. I like Rust, and I'm loathe to criticize it TBH. I also don't know enough about the language dev process in use for Rust. I also think Rust, while promising, is still pretty immature, having only become nominally stable very recently, and it's not like the language doesn't have warts already. I'd like to see where it goes before drawing conclusions about the health of their dev process.
&gt;The solution is to not do that. In our snippet above, we should have created two member functions, perhaps then_f and then_v, where the first unconditionally returns F(*this) and the second unconditionally returns F(this-&gt;value). Just let the user call the one they want! Meh, I've used an approach that works quite well and doesn't require having multiple function names. The solution isn't to check the type of the `Callable`'s first parameter, it's to check if the `Callable` can be called with a parameter of type `T`. This avoids any issue with cvref-qualifications or anything really. Can `f` be called with an `int` as it's first parameter? If yes, do `X`, if no, do `Y`.
Oh, you wanted to enable the language extension. I was just telling the compiler that the CPU that you're targeting has the emotive instruction set. `SMILEY rax, rdx`
Curious, does `-fmodules-ts` not work well?
I'm not sure I see how you can recover from `new` if the exception is replaced by termination. I thought that's what was being discussed.
You recover by return from `try_push_back`, not exception from `new`.
I'd recommend reading the [Atom proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0947r0.html) for some more.
&gt; Meh, I've used an approach that works quite well and doesn't require having multiple function names. I actually literally just spent an evening a week ago trying to get something like what's scoped out in TFA. For me, your solution wouldn't work (directly), because both versions that I wanted were both callable from the other type because of implicit conversions. (At least one direction, and probably the other as well, shouldn't have been possible according to me, but it's not my library...) However, they still needed to do very different things. The eventual solution though wasn't too far away, though it took me a while to get at -- I use an additional wrapper class to "use up" the one allowed implicit conversion. So if I had classes `S` and `T`, then `foo(S)` could be distinguished from `foo(T)` by testing whether `foo(wrapper&lt;S&gt;())` is legal, where `wrapper&lt;S&gt;` has an implicit conversion to `S const &amp;`. The `S` version would let the argument go from `wrapper&lt;S&gt;` to `S`, but for the `T` version couldn't then go to `T`. But I was also OK with the problems talked about in TFA. My version I think should work for either by val or by const ref, and it shouldn't be possible to do it with a non-const (or R-value) ref. Using `auto` might be convenient but there needs to be *some* way to disambiguate, so it's not like you can elide all type information (and the type names are short and sweet).
Beyond a certain size, I believe that libraries should be *separately* implemented and proposed for Standardization. Trying to combine "this should exist" and "this should be part of the International Standard" for libraries (which aren't deeply integrated into a compiler) is trying to do too much at once. Compare/contrast Filesystem, which took a proper path (Boost, TS, Standardization) and still resulted in a whole bunch of work - imagine if we had tried to do it all at once. The result would have been an overloaded L[E]WG and a dramatically inferior library. Some libraries are special and are so deeply intertwined with the rest of the STL that they can be directly proposed for Standardization (Ranges is the obvious example, and even it has prototype implementations). Things like graphics have no special Core/Library dependencies and should absolutely start as open-source libraries, whether part of Boost or a separate effort.
I've read the question multiple times now and couldn't not understand what this is supposed to achieve. Anyway, this looks like terrible overengineering and would most likely not pass any code review.
Noexcept is part of your function's interface. You don't want to make something noexcept if it might throw in the future, even though it doesn't at the moment.
I'm interested in your scenario because implicit conversions won't matter or affect the outcome if you're using `if constexpr` as in the article. It will be an issue if you're using SFINAE because two potential overloads could match and it's difficult (although not impossible) to specify a priority between two matches. But `if constexpr` allows you to prioritize one over another trivially. At any rate, as I said, I'd be happy to know what the specific issue was if you could share some minimal example. Truth is I use this technique quite often in one of my codebases so I'd appreciate knowing any downfall that could bite me later on.
Were you referring to the existing `new` when you said exception from new?
&gt; I understand the desire to not bake the "wrong" design into the standard, but at one point people have to accept a good enough solution Exactly. There will never be 100% optimal solution for a feature, because that is simply not possible. A solution that is optimal for embedded systems might be suboptimal for another domain and vice versa. `std::unordered_map`, for example, is *good enough*. It's not perfect, it has flaws, but it's reasonably efficient for probably 90% of all cases. However, I think binary compatibility is the real problem the committee has to face sooner or later. Designing a feature is hard, designing a feature that needs to be compatible with older versions of C++ and can possibly never be changed or corrected in the future is nearly impossible. 
As disappointed as I am that we won't have a graphics library in c++20, I do agree that it needs to be a working library(on many platforms) first. If one looks and most libraries of this scope, the interface changes to accommodate the actual use.
First, `if constexpr` isn't possible because it's a C++11 library. Second, I don't know how it'd help -- the `if` would still need to decide based on *something*. If I dispatch on whether `foo(s)` is callable, that has the same problem. Dispatching based on extracting the first argument type would have worked (with `is_same`), but I could have SFIANEd that as well, no `if constexpr` required; the decision to go with the wrapper was based on that being simpler than writing a `first_argument` trait. At least AFAIK, both my wrapper callable thing or a first argument trait would have worked with both `if constexpr` or with SFIANE. Anyway, I'm happy to share. I am not a C++ expert -- I consider myself proficient, but to an extent that proficiency starts petering out with recent standards and once you start doing moderate metaprogramming stuff. Feedback would be appreciated -- like I said, I'm not sold on this approach. (This is part of a PR but I likely won't get feedback for quite a while.) The library in question is nlohmann's [json](https://github.com/nlohmann/json) library. The two classes are `json` and `json_pointer&lt;json&gt;`, or any corresponding specializations. (It's named a bit inconsistently. There's a `basic_json` type of which `json` is a specific instantiation. `json_pointer` is parameterized by the `basic_json` type it corresponds to, but it's not named `basic_json_pointer`. Going forward I'm just going to call `json_pointer&lt;json&gt;` just `json_pointer`, and you should read `json` as any specialization of `basic_json`.) The two classes are implicitly convertible to each other, or at least they look like they are in the SFIANE context. Even looking again, I forget how `json_pointer` gets to `json`, but `json` has a "[convert to anything implicitly" function](https://github.com/nlohmann/json/blob/develop/include/nlohmann/json.hpp#L2825). That function usually can't be successfully instantiated and will eventually hit a `static_assert` if you try with `json_pointer`, but I don't know how you'd tell that from a SFIANE context. (Even if it's possible, I bet it's uglier than my wrapper. :-)) I'm writing an extension, and at one point I need to be able to specify a binary predicate that I call with both a `json` and `json_pointer` object. However, I suspect almost always only one of these will be relevant -- the client will want to pass a predicate that only looks at the `json` parameter or only looks at the `json_pointer` parameter. So I've got three overloads -- the binary version, the `json` version, and the `json_pointer` version. Anyway, the commit where I introduced the metaprogramming machinery is [here](https://github.com/EvanED/json/commit/d7677fca6f2eabb5259a41a4ad6e0c2522162cd5), the functions were renamed from their "separate names" version [in this commit](https://github.com/EvanED/json/commit/fe09e0dd6bd08f0a4b4e8eaeae71bb59977f7ce2), and you can see example uses in test code at the second link.
And issues of compile time gets more critical with new features like Concepts and enhanced `constexpr`. If nothing of Modules don't get merged to C++20 that will be really a pity which may discourage users to exploit the new features. :/
BactrianCase vs dromedaryCase.
There is some event,only during this event it is allowed to update (part) of class data. Data MUST be updated (we must somehow ensure in this) in this event. In particular, imagine list if objects: struct Data{ Time start_time; Time end_time; // basically end_time is start_time of the next in list void update(Time new_start_time); } during `update` start\_time and end\_time must be updated. And ONLY during update. All other time it is read only. Thats all, if simplify. 1) I want to ensure that data is mutable only in specified by design places. 2) I want to ensure that data is actually updated. Actually, template pattern in example do the job... What my long "method" should achieve in compare to template pattern \- you may add some logic in update \- do something before and after change. Plus returning Done is cheaper then tuple of actual values. In my "long" method you can create SetX, SetY only from Data, thus you can decide who can update data only from Interface class. And as you can see interface method `update` must return `Done` \- it is returned only be set\_x, set\_y, which you can access only having SetX, SetY (that's passkey idiom). You must return `Done` from virtual method, otherwise you'll have compiler error (wit`h -Werror=return-t`ype), in this way we can be sure that setters called. But probably that's overengineering indeed..... *Maybe not...*
Could you provide an example where this comes into play? I don‚Äôt understand how signing/endianness would have any impact.
Yes, that makes sense to me.
Okay. Prove it.
`Data`, as you've shown it, is fine except that the data members should be private. Then another class has a `Data` data member; simple composition and done ‚Äì what am I missing?
`assert` only help with catching runtime bugs, and are often disabled in release mode so they don't help. You can ask your compiler to check for them in debug and/or assume they are always true, which can improve performance. It can help when you ensure no overflow is going to happen for example.
Exactly right. There have been proposals in the past to add strong typedefs with sound anti-aliasing. If I recall the strawman syntax was meant to look like strong enums; and looked like: [template &lt;typename T&gt;] newtype NotInt : [ public | private | protected ] T [ = default | = deleted | { // api that will be forwarded to underlying T }] So you could do: newtype Name : std::string = default; which means copy the api of std::string into a new type called Name. You could also selectively pull declarations into the newtype: newtype Name : std::string { Name(); Name(string const&amp;); const char * c_str(); }; and then only those three functions would be usable on Name (which would be transparently mapped to std::string's versions of them) This was deemed too radical at the time (around the time of c++11) so we scrapped the proposal. 
I've thought about doing something similar in my own code base but have always been scared of exploding build times, so I still manually pre-hash. Also: constexpr uint32_t x = crc32::compile_time("\xA0"); &lt;source&gt;:95:17: note: cannot refer to element -161 of array of 256 elements in a constant expression table[static_cast&lt;uint8_t&gt;(_cur) ^ _str[INDEX]] ^ (_cur &gt;&gt; 8), // 2. Add '_str[INDEX]' to the current crc value Reminder, chars are often signed.... 
Actually, there are different concepts you can use to choose what set of operations you should implement. * Identity type: can only equality compare -&gt; most things should go there by default * Point type and duration types (like in `chrono`). You can subtract time points and you get a duration, you can add or subtract durations together and you can add or subtract a duration to a time point. See also pointer types and pointer diffs. They are always two, no more, no less. * Arithmetic type: can actually do operations on it (like a regular int). Not very common in practice. You might want to use a unit library to get the units right after multiplication or addition. I suggest you question yourself if you're thinking of using a strongly typed dimensionless arithmetic type.
Why is the table copy/pasted in there, instead of computing it? And really, you don't need the table at all for the compile-time version.
The work of using it would have to be less than the work of catching mistakes through review. The mutation restriction seems like the more usable part of it. But the mutation requirement is very interesting. I've sometimes been frustrated by being unable to statically express things like this. I think you might be bumping up against higher concepts in type theory and compiler systems, things like linear type systems, which I only slightly understand.
No, not on anything real. Though at this point it probably makes sense to go start working on support the merged TS rather than the original.
Sure, we could pretend that we can visit the source code of every library linked in a project and replace every call to push_back/insert/reserve with if(!try_xyz)) throw, strip the noexcepts, write our own tests, and keep maintaining the in-house forks, but STL has throwing constructors and operators, which the paper conveniently sweeps under the rug with a promise to "Re-specify each language operation and standard library function that could throw bad_alloc today". There can be no try_std::vector(n), and no try_f(v) where f takes it by value.
So mark the function as noexcept(false).
I agree, but sadly, filter is [deprecated](https://github.com/ericniebler/range-v3/issues/87).
It's not that C++ is on it's way out for application developers, it's that desktop applications will be replaced by webapps for most cases. Certainly it will in the corporate world.
&gt; We have no idea yet whether the TMP/type-based or the value-based approach is the way to go. Are you sure? Because I'm pretty sure EWG decided for the value-based approach last time.
You want to be OOM safe and use random open source libraries at the same time? I have to think you're working at cross purposes there. As Herb points out it's actually really hard to recover from since any attempt to recover may itself try to allocate. I guess you're saying you had a tiny hope in hell of just try-catching whole blobs of foreign code, but it still seems to me to be a brittle way of actually handing OOM. In video games we allocated the entire available space and budgeted it out. You can always throw anything you want when your custom allocator pool is exhausted.
Until you forget to do so for one function and this "feature" bites you
Interesting... In that case, the predicate is backwards.
Before C++ starts adding keywords with underscores, I'd rather we start using unicode, like Perl 6. You'd be able to special-bind a few in every editor without much problem. 
I prefer *split* (headers and sources in separate directories).
I prefer *combined* (headers and sources in the same directory).
I don't care too much, I just don't like needless abbreviations and cryptic names.
I see zero reason why to put the in separate directories. 
I have done the opposite a while ago and it's not convenient IMHO. If you use an IDE and have lots of files you'll have very big expanded list for include and source so if you search a file (not knowing it's real name, so unable to use shortcuts) you'll have to scroll or eventually fold one or more directories. This gets even more complicated when you have more than one project in your application.
You nailed it: separating public headers from private headers is key. The public API is called out cleanly. The `include` directory is what gets exported in a Conan package, or otherwise used by consumers as the directory to include. Implementation details that are not supposed to be exposed, don't get exposed. And, as a developer or as a reviewer, changes to `include` need extra scrutiny with regard to breaking changes. Of course it's still possible to make a breaking change just within `src`, so this is not a 100% black-and-white thing. It just calls out an extra level of scrutiny for edits to `include`, where in the *combined* strategy developers/reviewers have to do more thinking to figure out whether changing a function signature is a problem. If you know it's an implementation detail, and not part of the public API, no problem.
I prefer split just because it's the only way to quickly tell a private header from a public one, otherwise you have to come up with some weird naming or, worse, something more creative.
Looking at some of the other comments, the survey question probably needs to be correlated with some other environmental factors. * How many consumers does the library have? * How distant are they? (e.g. consumed solely by a tight-knit team, vs. consumed by folks you have no direct contact with) * How tolerant are consumers of breaking changes? For some environments, extra discipline =&gt; extra hassle. For others, extra discipline =&gt; more sanity for everyone, library developers and library consumers alike. But people are in different environments.
Public headers in include/, everything else in src/
Split so that private and public headers can be separated even though I rarely have private headers. However, I like that I can treat `include` as the public interface and, barring configured headers, the folder can be installed by just copying it to the prefix.
Use a proper IDE that allows you to define a virtual folder structure. 
Split As others have said: Public headers in include, everything else in src
Yeah right, which is why ChromeOS was forced to adopt Android native apps and officially expose the Linux userspace as means to gain adoption beyond US school system. I have been writing **new** Windows desktop applications the last 4 years. Now every company has jumped into "my world is the browser" mentality.
Lol - I burned myself in some low level code like this... just today. I was wondering why clang didn't warn me that the condition would never be true.
The main problem is that C standard (not sure if there are any differences in C\+\+) do not define how bitfields are implemented. Compiler can do anything with them, including padding. When you use them to just play with bits and you don't care how they are stored, there could be no issue for you. But when you use them to play with hardware registers or port communications, the way that bits are stored play huge role. In some embedded platform there is something called bit banding, but this is something completely different.
Not every company, but the writing is on the wall. At the places I've been working on application development keeps shifting to webapps little by little every yesr
What if I just want certain settings for just one .cpp file? 
I get the impression that I'm relatively amateur compared to the average user in this sub, but I'll give my thoughts anyways. ...split all the way. It's mostly for personal/aesthetic reasons: Separating the headers and implementation files helps keep them 'separate' mentally. But in a practical sense, it more or less halves the size of the directories and makes it at least a little bit easier to navigate and find necessary files. That being said, maybe I'm misunderstanding something, but the idea of enforced code layouts isn't really that appealing to me. Java has that, more or less, and it's always been a headache for me to get things working properly in a new project. It's very useful to have flexibility in directory layout..
Split with public header in includes and private headers in src. My opinion might change to merged with the apparition of modules and the keyword export.
In Qt, everything is in srce... but there is also an include folder where each public header is a single line statement that redirects to the full .h file located in scre. Nice design for a library.
IIRC there was a working implementation (but I guess it was too young to get any real feedback)
Everything in combined. No public / private api difference for your main code - if you want a "public" api put some C bindings somewhere. The easier to change /override for your users the better.
You need to clarify what you mean, I put all sources and private headers in one directory (src), but all public headers go elsewhere (include/libfoo)
I agree, I always had problems with languages with forced directory layouts. It's frankly one of the reasons I use c++ - just lemme organize my project however I want.
To separate the public headers from the private headers. That is, your API headers from you local private ones.
It depends, if the abbrevations are idiomatic it simplies things. Such as 'include' for the public headers directory. And src is pretty explanatory.
But that is what I would called a split layout. src/ class.h class.cpp include/l libfoo/ public_header.h
So how about c\+\+21? Breaking the release train is more important than not getting modules until c\+\+23. Even Tim Sweeney (of Epic Unreal Engine) tweeted that he would prefer that than not get modules.
To clarify, this is what I would call a 'split' layout src/ class.h class.cpp include/ libfoo/ public_header.h
Damn that would be fantastic :( why is it too radical ?
Interesting, didn't know about this. Are you limited to use macros though for lifting?
Woah there, buddy. I'm talking about SYSTEMATIC OPPRESSION of women and people of color. And you want proofs??!? As I though -- you're bigots, sexists and racists.
Is this poll for libraries out applications?
because the whole logic behind bitwise operators ONLY makes sense when used with actual bits. if you only have enum "states" you want to combine for example, it is not logical in the first place to do OR operators when in reality you mean to combine them (or AND operators for filtering).
My preference is * Public headers are in `include/foo` although it does not have to be in a project root directory. For example, I find the modular Boost structure non\-disrupting with its `{boost_root}/boost` for all headers in one and` {boost_root}/libs/{library}/include/boos`t for headers of an individual library. * All the rest of files are structured in logical and convenient manner. The traditional `{foo_root}/src` convention has worked well, but I think it is more important how the files are named and structured below. My technical reason for `include/foo`: allow a build configuration to find public headers of a library using pointer to a **single** **location**.
You can have your build system export public headers, together with other build artefacts that have to be installed.
Actually, I'd prefer to see a single default directory structure, even if it isn't optimal for everyone. 
A standardized directory layout has the advantage that build-/package-/analysis tools can better understand your project structure without needing extra infromation (like explicitly specifying necessary include directories) 
I use split only _if_ there are public headers.
If you are talking about msvc: I really hate that MSVC doesn't adopt the "real" folder structure by default. Nothing wrong with having the additional ability to create virtual folders, but the default should be to mirror the code structure. Luckily I'm using "open-folder" by default these days
Where is the poll?
I believe that if anything can be standardized in C++, it's cannot be the build system (because of very strong feeling for / against any of them), but probably a standard simple layout that can then be built by any build system in a consistent manner. It is was is done by Rust for example (https://doc.rust-lang.org/stable/cargo/guide/project-layout.html) For this reason, separating *include* for public headers and *src* for the rest, is the only way to indicate which part of the code is the public interface and which is the implementation. As an example, I did several prototypes using conventions like that to define project layouts and the corresponding plumbing using different build systems to build such projects (https://github.com/berenm/CMakeBuildPackage, https://github.com/automeka/automeka). Also, from these experiments, I now believe there needs to be a way to differentiate TU that should be compiled as executable from the other TU that should be compiled to libraries as it is unreliable to parse the files for main functions. Rust does it as well by using specific folders or filenames to distinguish executable from libraries.
This would be great for compiler enforced documented template contracts. If you need a forward iterator then you add exactly the functions that make a it forward iterator and the compiler can check that those functions can be forwarded to the type and provide a clear error message when it cannot. Having some stamp syntax so you can reuse those groups of functions (so there is actually a std::ForwardIterator identifier you can use for that) would be even better.
Split with public headers in `include/&lt;projectname&gt;/`, sources and private headers in `src`, it's convenient because you can just add one line to your CMake file to add an installation rule for the public headers and be done with it.
Really? I believe they decided to encourage further work on the value-based approach, but I don't know anything about us deciding "for" the value-based approach in favour of the type-based approach, and that would also be very inconsistent with publishing the type-based approach in a TS. But for the record, yes I also prefer the value-based approach, I believe the resulting code looks much more like normal code and is therefore more user-friendly.
It's a common mistake to split the files in an include and src folder. When you are developing your API, you put everything together. One folder `source`. BUT, when you ship your product, you just give a way the header files in a clean `include` folder. The advantage of having everything in one folder is enormous, when you sometimes browse the source code (e.g. Github) you can easily jump from header to source and vice versa. The reason why people were using the splitting approach was, they were no proper build tools in the 80s/90s. So they just copied the whole folder. Now you can easily get the list of all header files (exclude the private header with a suffix, e.g. `_p.h` like Qt does) and copy all the files **TL;DR** Never split files, always together. Only when you install the files, they are split.
You need to upvote OP's respective top-level comments in this thread (they're a bit hidden between all other comments).
Ah yes that's possible that I misunderstood what they actually did :) I agree
The amount of development might be unprecedented (I can'T tell), but the progress (in the ISO standard) starts to lack behind what happended in the 03-11 time frame (of course real comparison can only be made after release of c++20). Just think all the major features that were introduced in c++11. From the top of my head: - A standard memory and threading model, together with (basic) library support (threads, atomics, mutexes etc) - Introduction of constexpr computation - Move semantics and noexcept (including forwarding references) which was applied accross the standard library - Variadic templates - lambdas - Significant extensions to the standard library (chrono, type traits, tuple, smart pointers ...) c++14 and 17 brought a lot of nice incremental improvements on that, but very few really new/major features (library evolution might be on par, but imho not the language side of things). c++20 may outshine c++11 or not, but considering how much more people are working on the standard compared to pre c++11 times I can't hide my disappointment and maybe the higher number of participants is exactly the problem, because the standardization process just doesn't scale properly (too many cooks with different opinions). 
all *.cs files in same folder. Joke aside, headers go in dedicated folder, the rest are in the same folders. The reasoning is that I usually want to look from navigate from x.cpp to x.h and the other way around, and having them in different directories makes it annoying. I know there are tools, but sometimes you do the manual, files-system level work using for example TotalCommander.
Yes, good point. I think /u/parla has done this with his comment.
Even if that is the case, you are just in a way agreeing with me that C++ has lost its place in the GUI wars, be it via other managed languages or as you say the browser VM, regardless if browsers are implemented in C++ or something else.
I agree with this approach. If the project is big I usually split into modules instead of src/inc. Splitting into separate modules is especially useful in embedded domain in which you can split drivers into separate folders like: `/can` `*.c` `*.h` `/lcd` `*.c` `*.h` In smaller projects it is not necessary.
Thanks 
Seems to overcomplicate things. This means I can't just clone a header only library and add it to my search path, I need to install it as well.
You know, I haven't put much thought into that yet, since I never really started a serious C++ project from scratch. The split layout seems like a good idea, putting the public interface in `include`. In Wt, C++ sources and headers are all in `src`, with the public interface in `src/Wt`. The headers in `src/Wt` are the ones that get installed.
Agree yes desktop applications written in C++ isn't very common anymore. And .NET and Java is next to lose their GUI applications. WebAssembly seems like a promising runtime, then we can compile native code to wasm and slap a WebUI ontop
ACtually, I would have liked that idea very much, but I guess it wouldn't be possible to gain consensus on that now.
Like this? :) http://blogs.autodesk.com/autocad/autocad-web-app-google-io-2018/ 
I prefer combined when working on non-libraries or code that is not made to be a dependency for someone else or public. I tend to use split when the whole project is a library I want others to use. I don't think there are good "technical" reasons for split. However it does drive the user of a library to quickly understand what is considered public interface and what is implementation details. It also helps with tools that don't understand the script describing how to build the project (one can point them to the include directory and not be exposed to implementation files when intellisense search for header files for example). I also use combined for header-only libraries. So to me split is just convenient when exposing open source libraries to others. It's an affordance enabler, if you see what I mean. As an example: in all my gamedev and libraries I develop on the side, I use combined. That one library designed to be used by several open source projects, I use split (it's annoying but it does help users, it have a complicated implementation). If there was a standardized way to declare what is public and what is private, all tools could rely on that and the question would then not be important anymore. 
I kind of agree but I see this not working when the user get the source code from a repository, not a package manager. In this case the user receive the code in the layout you used to develop, not the installed one. Therefore, if you want to distribute open source code, you still have to use the layout that you think the user need. Which in what you describe is split, not combined.
That is compatible with saying that you put public headers in 'include' because a binary offers no public headers.
I agree that separation makes navigating your source more difficult but what if some of your headers are public and some are private? Now you have to sift through them to separate them during install phase. 
Can you rephrase this in code? Just to be 100&amp;#37; sure that we talk about the same thing :) My vision is : class Data{}; class A{ Data data; }; How A should .... hook/react/implement on update "event"?
I also use combined with header-only libraries, as said before, so I don't think that's exactly the same. In don't use "include" in this case.
For header-only libraries, I'd still stuck to the same rule: public files (i.e. public headers) in include and private files (i.e. private headers and source files -- if any) in src. But in this case there simply aren't any source files, only headers living in either include our src.
Yes, I was tempted to add a third question along the "and what will you do come modules" lines. I think the natural thing to do in the split layout would be to put public module interface units into `include` and private in `src`.
Where would a header-only library include a private header?
Both. There are a couple of answers along the "split only if there are public headers" lines. Note also that an executable could have public headers (e.g. for plugins, etc).
&gt; 4.3 (‚ÄúTreat heap exhaustion specially‚Äù ‚Äì make the currently throwing version of new a fatal error instead ‚Äì this would allow most of the std library to become noexcept!). The room voted strongly in favour (almost perfectly unanimous) of both these ideas. That was the part that was surprising. RIP C++
This is the only correct answer.
I appreciate the hard work everyone in the committee does but it's really disappointing to see modules not making it's way to C\+\+20 :( 
&gt; The work of using it would have to be less than the work of catching mistakes through review. I'm clearly missing something/doesn't see... What kind of mistakes will hard to catch? Or you mean, there is just too many boilerplate? For me personally \- only mutator passkey composition looks bloat (because of Done part)... but it can looks \+/\- as `SetXY : SetX, SetY` with metaclasses. Maybe because I wrote this \- it looks like a pattern to *me...* &gt; The mutation restriction seems like the more usable part of it. But the mutation requirement is very interesting. Actually that's the **only two** things that I'm showing... Maybe it needs another names? Like, instead of Done =\&gt; Mutated, Updated.... SetX, SetY =\&gt; MutX, MutY
Agreed.
Yeah I mean as a user both are fine honestly. As a developer, whatever I guess.
You're right a header only library probably considers all header as public. So then I would place them all in the 'include' folder to make that clear to the user libfoo/ include/ libfoo/ header.h tests/ test1.cpp
&gt; well Yes that's what my comment refers to. Compile the native code to wasm and add a webui ontop.
&gt;constexpr uint32\_t x = crc32::compile\_time("\\xA0"); The compilation times obviously can increase. However, nowadays I think that compilers are quite efficient and they can cope with this efficiently, but as always it depends on your particular context and how heavily will you use this. Regarding the compilation error, you are right, there is a missing static\_cast in the general case when accessing the table. It should be table\[static\_cast\&lt;uint8\_t\&gt;(\_cur) \^ static\_cast\&lt;uint8\_t\&gt;(\_str\[INDEX\])\] Thanks, Nice one!
The table is needed in any case, run\-time or compile\-time versions (Check the code at line 95). However, I could have created the table at compile\-time. But in order to save compilation time I decided to include it pre\-calculated. Just consider that for big projects there are cases in which you actually want to reduce the compilation times as much as possible. Even it could be the case that you don't want to use the compile\-time version for this reason.
Split for my projects, together it is a library i want to use because i prefer to import only one header to my project
Same folder because it becomes a pain on a split setup to look at the code (e.g on GitHub) and get some tools working with this setup. For shipping its quite easy to generate a include folder with only headers (even only public ones) using cmake.
yes, sure :) m=male or f=female, since we are in Germany obligated under equal opportunity law to underline that we are open for applicants from both gender. Some also add m/f/x to show they are open for the whole gender spectrum, but it's not common yet. the origin of all this is the crazy German language with its generic masculimum and the masculine, feminine and neuter articles and nouns
&gt;and I end up writing my own in the end anyway. Please submit them. CMake is open source, and just like any other open source project, it lives from contributions. &gt;The cmake library finders I've used have been pretty low quality I've had luck so far. Also don't forget that CMake provides good defaults and good functions that finders can use. That's why good finders can be really short. &gt;So for me, cmake doesn't provide any cross-platform advantage over waf/scons/autotools/tup/rake/gmake/etc. I guess it depends on which tools you use, but for me, the abstraction of compilers and flags (for example when building shared libraries) is pretty useful. Especially if you use it "right", and think in terms of targets and interfaces, it can simplify things a lot.
Having directory structure tied to the internal of your code (namespaces, ‚Ä¶) is annoying, but I don't think that having your directory structure tied to your interface is a problem. It should even help you to design the interface.
I re\-though about that. In fact it should not change a lot compare to what I do currently. Export everything (.h only) from the include directory, and keep your internals (.h and .cpp) in the source directory.
&gt; You want to be OOM safe and use random open source libraries at the same time? Not when I controlled a robot arm that might hit a human on the head, sure. Very carefully when I provided financial services or data persistence. But even for casual applications that stand to lose data in case of a crash, as long as the "random open source libraries" are reasonably exception-safe, it just works. Because C++ made the right choice and treats memory as a resource by default. Example: open a file too large in Notepad++, it will OOM in the guts of scintilla and bubble up for nppp to give the pop-up to the user. I am sure I could fuzz more than a few bugs out of it with a test malloc, but in practice, my already open files stay open. &gt; As Herb points out it's actually really hard to recover from since any attempt to recover may itself try to allocate. "may", sure, someone also "may" write exception-unsafe code. It doesn't happen in reality. One of the first bugs I dealt with in my career actually was similar: a telecom client managed to OOM on a 4-byte new and back then the compiler was dumb enough to use the heap for the exception object. 
What about split but nested: it is a variant of the structure of Bloomberg bde project with private headers in a dedicated directory. Project directory/ interface_header.hpp interface_header.cpp interface_header.test.cpp private/ #(or source) private_header.hpp private_header.cpp private_header.test.cpp Subproject directory/ interface_header.hpp interface_header.cpp interface_header.test.cpp private/ #(or source) private_header.hpp private_header.cpp private_header.test.cpp # Pros: 1. Easier project nesting 2. Interface files are directly accessible in the project root directory 3. No circumvoluted directories "project/subproject/include/project/subproject/" 4. Compact hedear/implementation/test file view 5. Easy view of what is private and what is the public interface of the project. Very usefull for shared library, (ease so visibility declarations or dll exports declarations) # Cons: 1. More suited to cmake projects.
I don't think it's too radical as such -- many people agree that some form of strong typedef would be nice. As I understand it, the stumbling block is around how "strong" you want them to be. For example, should there be automatic conversion from `NewType&amp;` to `OldType&amp;`? If the answer is no, then you have to potentially duplicate an awful lot of code in order to make `NewType` useful; if the answer yes, then you can pass a `NewType` wherever an `OldType` is expected, which makes them much less safe than many people would want. 
Do you put .inl files also in src/ or do you not use them at all?
We don't use .inl files.
Why is it so (capital letter start)? Some historical reasons?
Was that addressed at me or at Forricode?
The is the only correct approach: - deploying the library, you simply copy the contents of the include folder and are done. - using the library, you write #include &lt;libfoo/public_header.h&gt; - you cannot misuse the library easily by including private headers. With combined approach, it is not clear what is public header and what is implementation detail. You loose a lot of information and control, that will cost a lot of time to sort out when trying to deploy a library to someone else. When using DLLs, there are indeed combined approaches where a smart script parses all headers containing the DLL_EXPORT_MACRO and automatically create the public interface, but this is not standardized. Therefore, the manual split is best solution: well-defined and it just works.
I figured that's what was meant. It's going to take a bit of getting used to for me. I left my Camel Casing back in Haskell land.
Right, it was addressed at u/Forricode.
Many other things like linear algebra, geometry, multi\-dimensional arrays, strong types with physical units, unit testing, etc. seem much more reasonable for std. 2D is high level, the library would never be a standard but rather one option among others, and any application using it would still have dependencies to other libraries for reading/writing image files, showing them in a GUI, etc. It would also introduce types into std such as vectors, 2D arrays, colors, specifically for graphics, even though these types are also needed for many other uses. Similar to how std::chrono with the duration types.
I prefer to keep sources and headers in the same directory. I create directories for "topics" instead. My rational being that I can easily list headers or sources using automated tools (ls *.h, ls *.cpp) but I can't automatically list by topics. And if you want to do both then you have to either go for: topic1/ include/ src/ topic2/ include/ src/ Or: include/ topic1/ topic2/ src/ topic1/ topic2/ Both leads to deep hierarchies which are painful to navigate, especially when working outside of the IDE. The issue of private headers is problematic though. In a combined setup I have seen different solutions, but no silver bullet for now: - use a suffix in private header names (for example private headers in Qt end with _p.h) - create a public include/ headers whose file only forwards to the real include - split out things manually at install time
Like I mentioned, private headers get a suffix `_p.h` , i.e. `MyClass_p.h` All files with this suffix will not be installed. The buildmanager will perform this logic. The benefit is you also see it from just browsing the source code. Explicit is better then implicit. And I mean, even creating the files, `.h`, `.cpp`, then I have to move in the explorer to the corresponding folder structure and place there the other file. Don't do it, keep things simple! PS: Qt uses also this approach and...yeah it works.
I prefer combined. The reasons behind split are doomed when some of your headers are generated during build time ‚Äî you no longer have src and include directory, but bunch of other directories and from everyone you want to publish specific headers.
What I mean is there's a table-less version of the algorithm that can be used instead for the compile-time algorithm. This should still be plenty fast in compilation speed. For the run-time version I'd still use the table version, of course, and whether or not you generate that table is a design decision. However, I just had to fix a CRC32 implementation that had a mistake in it's table, so I'm a fan of generating it. That also lets you make the algorithm more generic and work with more than just CRC32.
I have an application which does currently handle memory exhaustion. It's a server application. Each request to be processed is wrapped in a try\-catch block. If any exception is thrown during the processing of a request, then all of the memory associated with that request is released (because we use RAII properly), and a message is logged to say that this particular input cannot be processed. The server moves onto the next request. So I'm not doing anything special to handle out\-of\-memory. But *it just works*. It works because out\-of\-memory is handled the same as any other error. As it happens, I'm working at the moment on a bug report where the server ran out of memory processing a particular input. It was creating a vast number of copies of some of the data, for a reason which is specific to that particular input \- that's a bug that needs to be fixed. But the server didn't crash. It noted that the input could not be processed, and was able to process subsequent requests correctly. There's no way I could use `try_` methods to do this: there must be thousands of functions in the code where memory is allocated. I could never justify the time needed to replace all of these calls, even if that was a good idea. (And it wouldn't be a good idea anyway \- the only way you could do this in reality would be to create a wrapper for the STL which had the old throwing interface, and not use the "new" STL directly at all.) I'm prepared to admit that the code probably isn't 100&amp;#37; exception safe. I expect that there are several functions which, if their memory allocation failed, would cause a crash. I would agree that it's very hard \- probably impossible in practice \- to do this perfectly. But perfection isn't a requirement. What we know for sure is that there are several cases where bad\_alloc *has* been handled correctly, and those would be broken if memory exhaustion terminated the application.
Ah just found it, it is two years since last commit if it is the correct one https://github.com/cristianadam/io2d/commits/master 
Some build systems are not as versatile as others and I believe (might be wrong) that the goal is to be build system agnostic. Just a thought.
In the past, I have configured a shortcut in my editor to take the current file path, strip the extension, and replace it with .h or .cpp then open the file. This gave me a low cost way to navigate between header and source. That becomes a little bit harder with a split layout.
A major argument against this is if you change a header's being public or private, you need to move the header, which in turn may mean (depending on directory structures) that you have to change every single #include of that file. If you just put everything in src/ and specify in your build system what the public headers are (and their structure), you don't have this issue (though there are disadvantages as well).
&gt;There is talk at SG15 of coming up with a common build description which I think will also require a common header/source layout. Why do you think it requires a common directory layout?
If a compiler can turn a precondition violation into UB then the whole thing is pointless. It should abort the compilation.
Hi there, Yes I understand and agree with you to a certain extent. I had it calculated at compile time before but the context in which I use this is so heavily making use of metaprogramming constructs that I had to reduce compilation times from all potential pieces of code being this one a clear one. Apart from that and it could definitely be self contained and generate the table. Thanks for the comments
g++ *.cpp works for me
He really lost me when he went through some old-style code and "modernized" it (around 14m). double rms(const vector&lt;double&gt; &amp;v) { double sum = 0.0; for (vector&lt;double&gt;::const_iterator i = v.begin(); i != v.end(); i++) { sum += *i * *i; } return sqrt(sum); } Not pretty, agreed. --- double rms(const vector&lt;double&gt; &amp;v) { double sum = 0.0; for (auto i : v) sum += i * i; return sqrt(sum); } Looks good to me. Well done. Modern C++ is much better! But wait, then he rewrites it as this: --- static double accum(double partialSum, double elem) { return partialSum + elem * elem; } double rms(const vector&lt;double&gt; &amp;v) { return sqrt(accumulate( v.begin(), v.end(), 0.0, accum)); } which then becomes... double rms(const vector&lt;double&gt; &amp;v) { return sqrt(accumulate( v.begin(), v.end(), 0.0, [](double partialSum, double elem) { return partialSum + elem * elem; } )); } What are you doing? That's not better! The first refactoring with just auto and the range loop is great, no one has to look twice at that. These other two versions are *terrible*. The `begin()` and `end()` cruft is reintroduced and now there's even a lambda and the code is more convoluted. I can't quite make out if he thinks the last two versions are good or bad... but it doesn't seem like he's totally denouncing them, which I think he should. Is this considered good modern C++?
As a general rule, named algorithms should be preferred to ad-hoc loops: Sean Parent's well-known "no raw loops" guideline. For something trivial like this though, I agree, the algorithm version is not more readable than the range-for version. (Also, the `begin()` and `end()` "cruft" will hopefully go away in most cases in C++20. But not for `accumulate()`, because that's in `&lt;numeric&gt;` and not in `&lt;algorithm&gt;`, *sigh*.)
Pre-calculate your per-input memory needs and see if the allocation fails before proceeding -- there's a number of ways to do that. Since this is still just a proposal, one can suggest a convenient API for "is there enough memory for me?"
I definitely prefer combined. The (somewhat unfortunate) reality is that a lot of headers contain a non-trivial amount of code in C++. So much so that there are probably more new "header only" than "traditional" libraries out there. The lines between headers and code that "does something" is beyond blurred by now that separating them makes no sense. It will be interesting to see how this develops once modules come out, potentially it will make sense to have some sort of new separation method at that point, but as it currently stands there's just no point in treating header files like they're only descriptive.
There is no reason you can't put the public directory on your include path and then just do submodule/header.h like you would for private headers. There is no reason that they can't even have identical structures.
C\+\+ compared to Java have many differences, like: \-C\+\+ was designed for systems and applications programming, extending the C programming language. \-C\+\+ supports pointers whereas Java does not pointers. \-C\+\+ supports operator overloading multiple inheritance, but Java does not. \-C\+\+ is closer to hardware then Java. \-Everything is an object in Java. \-Thread support is built\-in Java, but not in C\+\+. \-Java does not support default arguments like C\+\+. \-There is no goto statement in Java. \-Java was created initially as an interpreter for printing systems but grew to support network computing. \-Java has method overloading, but no operator overloading just like C\+\+. \-Java does not support unsigned integer. C\+\+ compared to Javascript: \-The main diffrence between these two is that while Java is a programming language, Javascript is scripting language. \-Javascript is a prototype\-based scripting language that is dynamic, weakly typed, and has first\-class functions. \-C\+\+ is a general\-purpose programming language. It was developed from the original C programming language. \-C\+\+ is a statically typed, free\-form, multi\-paradigm and a compiled programming language. C\+\+ compared to Python: \-C\+\+ is compiled language, while Python is an interpreted language. \-C\+\+ has been around for ages (1983). \-Python is younger as it was created in 1989 and all its releases are open source and freely usable and distributable, even for commercial projects. \-C\+\+ is slow to write, error\-prone, and frequently unreadable, while Python is known for its writability, error reduction, and readability. \-C\+\+ creates more compact and faster runtime code, and it's already the language of choice for 95&amp;#37; of embedded system code, while Python may be less efficient at runtime, during development it's much more efficient.
The newtype proposal was pushed into an "alternatives" section of a bigger proposal paper (N3635), modified heavily to ensure it solved the parent paper's goals (implicit conversions, etc), and removed from subsequent versions of that paper entirely. The paper was trying to add restrict semantics to c++ and was (rightly) rejected by the committee eventually because C-style restrict is a terrible way to deal with this problem. newtype is a much more general approach but the authors wanted a more compact proposal to fit corporate goals. 
You're able to exhaust your physical RAM with notepad? Stack is also memory. I'm not going to try to compare two different individuals evidence for them. If you want to keep `new` throwing, you should write a paper.
It is hard to scale, but we do our best.
I thought there was a more recent one, but I can't remember where
It's usually not possible to detect contract violations at compile time. As far as I understand you will be able to decide, through compiler switches, how you want contract violations to be handled. In a debug build you probably want it to stop and print where the violation happened, but in a release build you probably don't want to pay the cost of checking for contract violations, but that doesn't mean you need to let the compiler do optimizations based on the assumption that the contracts hold if you're not comfortable with it.
I'm sure of that. And I hope my post didn't read as criticism of the people on the committee. Maybe one problem is that far too few people are payed to actually "make c++ better" rather than "solve a particular problem for our company" and that there are too many stakeholders with different interests.
If you've programmed in many other languages, the concept of accumulate (or `reduce` as I prefer honestly because it's what it is) is a common idiom.
&gt; Like I mentioned, private headers get a suffix _p.h , i.e. MyClass_p.h All files with this suffix will not be installed. What I don't like about this is -- what if nearly all of your headers are private? Then you're adding that (IMO) ugly decoration in the almost-everywhere case. That probably works well for a library, but not for an application. (Applications can still have public headers because of plugin interfaces or whatnot.)
Is there any proposal to add range versions to &lt;numeric&gt; algorithms?
&gt; However split source has the advantage of being easier to install as you just copy the include/ directory to the installation. IMO installs should be more selective than what you wrote (though maybe not what you mean). With what you wrote, if I'm developing on a library or just making a patch for internal use or something, edit a file in `include/`, and then install -- now all of a sudden `$PREFIX/whatever/` will contain editor temp files and crap like that. IMO, really the installer should selectively copy from *within* the `include/` directory, not just do a `cp --recursive` equivalent.
I could see using something like the last version iff I passed in the begin and end iterators so I didn't have to tie the data being stored in a vector. If it is always going to be stored in a vector then I much prefer range loop version.
&gt; If you've programmed in many other languages, the concept of accumulate is a common idiom If you've programmed in many other languages, you're hopefully used to much better syntax for that kind of thing than what you can do in today's C++. If you could do something like `return sqrt(v | map([] (int x) { return x * x; }) | accumulate)` I think *that* is better and more readable than the first refactoring. I know ranges gets you something kinda close to that, though don't know enough about it to know exactly how it'd look. Much more syntax on top of that and it will turn worse IMO. (OTOH, there's the proposed `=&gt;` syntax for lambdas, which would make it better. *No* idea what the status is of that proposal or if it stands a chance.)
/r/cpp_questions. and incliude the error message.
* TOCTTOU race conditions galore * You can't necessarily precalculate -- what if it depends on input you received yet? Or what if to calculate the amount of memory you'll use requires using that memory, unless you want to make an estimate that's conservative by perhaps an order of magnitude? * How do you figure out how much space all your data structures that you don't maintain implementations of will take?
You're proposed syntax doesn't specify the reduction operation. I agree that pipes are elegant and they do show up in other languages (Elixir had them IIRC). Everything in C++ gets a bit uglier because with the exception of the ranged based for loop, you need the `begin` and `end` iterators everywhere. 
Also, are you targeting C++11 or C++14? The relaxed `constexpr` rules in C++14 would let you write the CRC as a loop. You could probably unify the run time and compile time versions, honestly, and (from what I've heard, at least) it'll compile even faster.
Thnaks. Will doo
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8r31ph/c_help/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Yikes! That‚Äôs not something you want to discover through learning in the field. Thanks for explaining. 
I think /u/berium is referring to a proposal the I volunteered to write when he says "common build description". While I have a proto-prototype (really just an example build file) that I've shopped around to a few people, I haven't written up the real proposal yet. The core idea is that the source files, libs, bins, and their dependencies should be specifiable in a human read-and-writable declarative format that can be easily consumed by all (or at least most) build systems. (I should have more details to announce in a month or two, so please don't swarm me with questions yet, unless you want to volunteer to port this to a build system. I already have volunteers for SCons and CMake.) I want to be *very* clear about this: I have *no* intent or interest in requiring a common header/source layout. If I'm going to put time and effort into this, it is because I want it to improve the lives of C++ developers (including myself), which requires that it actually get used. Mandating a specific file layout is a "boil the oceans" proposal that has very little chance of being widely adopted outside of new, small projects with no external dependencies. I'd rather not waste my time on that kind of proposal.
C\+\+11 is the target
The point isn't that the compiler is getting to do UB. It's that the *non* precondition violating calls will be faster. The behavior of those must be preserved (between contracts provided and not) because there's no UB. Aborting the compilation doesn't preserve the behavior of executions that don't violate the contracts. Here's an example. Consider the following code; we'll start without contracts and a "traditional" demonstration of how UB allows optimization. void use(int * p) noexcept { int x = *p; ... } void foo_with_check(int *p) noexcept { if (!p) return; ... } void caller(int *p) noexcept { use(p); foo_with_check(p); } In this case, assume that `use` and `foo_with_check` get inlined into `caller`: void caller(int *p) noexcept { int x = *p; ... //1 if (!p) return; ... //2 } Because of the dereference `*p`, the compiler is allowed to assume that `p` is not null. (More precisely: the standard imposes no restrictions on the behavior if `p` *is* null, and the optimizer is only constrained by the as-if rule on correct executions.) So it can eliminate the check, leading to better code: void caller(int *p) noexcept { int x = *p; ... //1 ... //2 } What's really fun here is that if `x` is unused, it's actually allowed to elide the dereference even: void caller(int *p) noexcept { ... //1 ... //2 } because, again, the null case was always UB so the optimizer is unconstrained. Also fun is that it doesn't matter what order the calls were -- `caller` could have called `foo_with_check` and then `use`, and the same optimizations would have been possible, even though the check occurs before the dereference. In essence, `foo_with_check` is too general *for this particular use case*. The check presumably still needs to be in that function in case it's called from other contexts, but here it's redundant. So we're happy the optimizer removes it. Anyway. A "problem" is that the compiler needs to be able to know that `use` triggers UB if passed null. So with this: void use(int * p) noexcept; void foo_with_check(int *p) noexcept { if (!p) return; ... } void caller(int *p) noexcept { use(p); foo_with_check(p); } it won't be able to do that. But suppose we put a contract on `use`: void use(int * p) noexcept [[expects: p != nullptr]]; Say the compiler still inlines `foo_with_check` void caller(int *p) noexcept { use(p); // [[ expects: p != nullptr ]] if (!p) return; ... } Now we're in the same place we were originally -- because `p == nullptr` would be a precondition violation, the compiler is allowed to assume that the program never does that, and can optimize away the check as before: void caller(int *p) noexcept { use(p); ... // from foo_with_check } Now, there is a difference here -- you're relying on *the programmer* to correctly specify the contracts -- but this is maybe not as big of a difference as it initially appears. There have been security vulnerabilities "because" of the original optimization example above (especially if the compiler then determines that `x` is unused and elides the dereference of `p` entirely, so now there's no check and no crash...), so the "implicit preconditions" in the actual code may be incorrect as well.
JavaScript fuckery.
Right now there is the advantage of C++17 parallel algorithms being easy to use once you use an algorithms version. He could have also used double rms(const vector&lt;double&gt; &amp;v) { return sqrt(inner_product( v.begin(), v.end(), v.begin(), 0.0)); }
&gt; that you have to change every single #include of that file. If you have to change your includes after moving a header between public/private then your header directory structures are wrong.
&gt; Your proposed syntax doesn't specify the reduction operation. I'm assuming that `accumulate` would implicitly use `+`, because it does. :-) I should have specified an initial value of 0, and unfortunately it looks like with ranges you can't pipe a range into accumulate. (Maybe there's some adapter that would allow it?) But this [works](https://godbolt.org/g/yUpgnV): double rms(const std::vector&lt;double&gt; &amp;v) { return sqrt(ranges::accumulate( v | view::transform([] (double d) { return d * d; }), 0)); } and that's only a *little* worse than what I said. I think that's pretty competitive with the original refactoring. &gt; Everything in C++ gets a bit uglier because with the exception of the ranged based for loop, you need the begin and end iterators everywhere. And the fact that everything but the bare loop example gets worse and the bare loop example doesn't mean that it can very easily be the case that the algorithm approach is better in other languages and the bare loop can be better in C++.
I'm not greatly in favor of that. I already have to cut and paste the symbols for degrees and omega when I need them, and those I could just stick in a constant if it bothered me enough. Having to type weird symbols for language constructs sounds horrible. Another option is to declaring compliance with new keywords: `[[standard:c++20]]` (or something similar), to turn them on conditionally. At least that way we can do it piece by piece, and only enable it once we know a piece of source is ready for it. This also offers a way forward for deprecating features without immediately causing massive pain in old source bases... 
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
I believe Christopher di Bella is working on it, although no promises.
I guess you're probably looking for the [P0267 reference implementation](https://github.com/mikebmcl/P0267_RefImpl)
This is my approach. 
I do not mean to be negative but I think the lib is overkill. We could flag types in so many ways that the combinatoric explosion could be terrible. I think simple solutions are more compatible, do not depend on a library and will generate lighter code, probably. I could understand that you can use Positive\&lt;double\&gt; for something critical. But in most cases, I would stick to @pre in doxygen docs in the interface as documentation and \[\[expects : x \&gt;= 0.0\]\] or assert(x \&gt;= 0). That will work everywhere with no dependencies. I can see some usefulness for some libraries in certain contexts, but I see this, for general\-purpose use, as overengineering. I hope you do not feel to bad about my feedback. Just my two cents. I do not mean it is useless, just that this is too narrow\-cased and also can have drawbacks such as introducing another dependency or generating bigger code when you can use alternatives that will work well enough in practice.
Keep in mind we're not talking about "low on memory terminates", we're talking about "**out of memory** terminates". If you're in a memory constrained environment, you're still obligated to manage memory wisely. There's at least two distinct use cases where OOM is likely: * memory constrained machines (embedded) * memory unconstrained with unconstrained input If you're working in memory constrained environments you have a back-of-a-napkin calculation of what your components will need to use, even if that calculation isn't precise -- you're going to need spare heap anyway. If you've budgeted 1MB for input, and your allocator exceeds your budget, your custom allocator can throw or do anything you please. If you're working with unconstrained input on a 32GB machine, and you manage to exhaust all that memory, this is highly indicative of a logic error (invalid inputs), as the parent admits. If you don't want your service to go down, and you haven't implemented instance redundancy (ie. have a lot of processes and a healing daemon), you can do sanity checks within an order of magnitude, and then buy 32 more GBs. Do you have a more precise use case? I think use cases make the most persuasive arguments.
I prefer folder hierarchies matching namespace hierarchies, to avoid name clashes in files. That is, if working on the foo-bar-lib library: include/foo/bar/lib/ nested/ public_include_0.hpp public_include_0.hpp public_include_1.hpp src/foo/bar/lib/ nested/ private_include_0.hpp source_file_0.cpp private_include_0.hpp private_include_1.hpp source_file_0.cpp source_file_1.cpp This means including like: #include &lt;foo/bar/lib/public_include_0.hpp&gt; #include "private_include_0.hpp" Rationale: 1. I used to work in a company in which developers pulled in ~1,000 libraries as dependencies. 2. C++ compilers use a "path" system to search for includes (`-I /path/to/lib1/include -I /path/to/lib2/include ...`). The result is that if everybody just chucks their header files in `include/`, then in case of name collision between dependencies, pain ensues. To ensure that name collisions do not occur even without coordination, the path to include was made to reflect the actual library hierarchy.
I think Matt has a reddit account, not sure if how much he pays attention to it though u/mattgodbolt
Brilliant explanation!
Our C++ codebase is modularized, so we actually have company/module_a/cpp/include/company/module_a/*.h company/module_a/cpp/src/*.h|cpp company/module_a/cpp/tests/*.cpp company/module_b/cpp/include/company/module_b/*.h company/module_b/cpp/src/*.h|cpp company/module_b/cpp/tests/*.cpp Where company is our company name. Other code is in vendor/. Code is namespaced with namespace company { namespace module_a { Then you include public headers with #include &lt;company/module_a/file.h&gt; And private with #include "other_file.h" Then we manage include paths and linking with cmake, enforcing no circular dependencies and so on. We also enforce clang-format, so there's never any style discussions.
My very shallow first impression is no OSX support(X11 from macports doesn't count) and autotools makes me cringe. But that is literally 10s of looking at it.
Came here to say that. `sqrt(inner_product(...))` is the right way to do this and the most readable one. Fold-operations are nice, but to general for cases like this.
The best advice I could give you is: after a small theoretical of the language, choose a small project and write real code. Keep searching for good solutions at your problems when you do not know how to write or approach a piece of code. Learn these as general purpose: \- Data abstraction in C\+\+ and value types. \- OOP style with virtual functions (you need to know this still even if not as fashionable as it used to be). Learn smart pointers along the way. \- STL containers. \- STL algorithms and \*function objects\*, which lead you to \*lambdas\*. Try to not write too much boilerplate when you can use the algorithms library, so study the library first to get an idea of what to find. \- Subtopic worth a comment: const\-correctness. Extending your knowledge: \- I recommend to have Boost at hand as a second\-standard library, especially the following: more specific and optimized containers, boost asio library for network programming, Boost Beast for http programming. If in games or real\-time concurrency, the coroutines library (and maybe even fibers), boost.optional and boost.variant if you do not have C\+\+17 available, boost.future for higher level programming with async results \+ continuations (std::future does not have .then()). Beware that configuring the futures library in C\+\+ is a bit of a mess (the macros you need to define etc. gave me some headaches before). After all of this, if you want to go deeper with C\+\+ strenghts: constexpr, customized memory allocation in containers and some compile\-time metaprogramming (such as if constexpr) are worth a look. But my route for a beginner with programming experience would be: \- Get an overview of data abstraction and value types. \- Get an overview of STL. \- Try to code some kind of project and research as you go: where should I put my header files? Should I code this function this way? Which container should I use? \- Absolutely understand function objects combined with algorithms to get a feeling of the flexibility of customization. \- When you cannot find something, visit Boost and go to the available libraries, maybe you can get something you were looking for. The rest of libraries that you need, just as you go (Qt or WxWidgets for GUI, {fmt} if you want fancy formatting, nlohmann::json for json...) As for the build system, the standard is CMake, but I recommend myself Meson. I also recommend to learn coding with editors, not fancy IDEs. It will get a bit to get used to, but you will understand what happens at each step. If you are invested in it, you will do it.
Absolutely! I'm just showing you can do it with the non explicit loop style some folks prefer. I'm much more a loop guy, the code with the explicit range for is how I'd write it myself. I try not to be too strongly opinionated about which approach is better as I never want to start debates about which is better. Both have advantages, I wanted to show how you could phrase it either way. The compiler treats them all pretty much identically anyway :-) Thanks for linking me, I wouldn't have spotted this otherwise!
And to be clear, I'd never denounce any code, unless it was clearly wrong, using undefined behaviour. It's not my style to preach the right way to code: I've been strongly opinionated before and there's almost always something to learn from trying doing something the other way :)
Very similar if not identical :) Note, if you have the chance to use C++17, you can actually use: namespace company::module_a::nested { } For namespace declarations, which is identical to the previous `namespace` repetition, without the boilerplate.
I want to get rid of the difference between headers and source entirely, and use only modules that specify exactly what is intended to be exported and what is not.
Thanks for the response Matt!
Can the operators be binary? // stolen from perl 6 template &lt;typename T&gt; auto operator "¬Ø\_(„ÉÑ)_/¬Ø"op (T x, T y) { return rand() % 2 ? x : y; } void f() { auto drink = "coke"s "¬Ø\_(„ÉÑ)_/¬Ø"op "pepsi"s; }
I absolutely agree. My benchmark tends to actually be C# with linq. For example Math.Sqrt((double)numbers.Sum(n =&gt; n * n) / x.Count()); 
&gt; Keep in mind we're not talking about "low on memory terminates", we're talking about "out of memory terminates" -- every last single byte is gone. That's not what `bad_alloc` means. `bad_alloc` means *that allocation* failed. That *might* be because every last byte is gone, but it might be because you requested an enormous allocation. It might be because you requested a large allocation and your memory is fragmented to hell. &gt; you can do sanity checks within an order of magnitude, and then buy 32 more GBs. Great, we'll just buy our customers all an extra few gigs of RAM. I'm sure that'll go over well with my higher-ups. (Isn't C++ supposed to be the language that you go to if you care about performance and resources?) &gt; Do you have a more precise use case? I think use cases make the most persuasive arguments. There are several in this discussion, which you seem intent on ignoring. For example, "I have an application which does currently handle memory exhaustion. It's a server application. Each request to be processed is wrapped in a try-catch block. If any exception is thrown during the processing of a request, then all of the memory associated with that request is released (because we use RAII properly), and a message is logged to say that this particular input cannot be processed. The server moves onto the next request." Or look at [Herb Sutter's deterministic exceptions paper](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0709r0.pdf) and the discussions in section 4.3. For example, "Microsoft Excel, File &gt; Open huge\_spreadsheet.xlsx: Big operation that causes lots of little but related allocations. Today, 32-bit Excel can encounter heap exhaustion when enterprise users try to open huge spreadsheets. The status quo way it is handled is to display a dialog to the user, and for support to recommend using the 64-bit version of Excel. In this model, only a minimal change is needed to preserve the same behavior: Install a new\_handler that invokes the existing failure dialog box display code. If the program wishes to continue if the whole large operation fails, it can make the allocations performed as part of the large operation use nonthrowing allocations (new(nothrow) and try_ functions)." Except he's completely glossing over the last part would often mean extensive code changes to those in that category. Next bullet: "Microsoft Word. Today, Word wraps STL with try_ functions such as try_push_back." Why would MS bother maintaining a STL fork with those `try_` functions if handling allocation failures gracefully wasn't important? Look, I don't have as much of a dog in this fight as it might seem. I *do* think that the proportion of C++ programmers who would actually be affected by assumptions that `new` can't throw is quite small. Despite my response to the "buy more memory" comment above, I am not in that group, so I can't give you my own use cases. However, I suspect that even though the proportion is very small the absolute number is moderately high, and those people (1) would be affected *extremely* adversely and (2) are probably among C++'s core demographics. So I think you are being *much* too dismissive of those people's concerns. "Just rearchitect your program" is not a solution.
I like \*both\*. But it depends on the build tool that is used. For example, for personal projects and open\-source libraries, the split method works great with build tools like CMake. It makes it impossible to include private headers by accident. But for some other professional projects, I might use Bazel. Then, the build files are actually specifying what headers are exported and which aren't and enforce it when users consume your library. It's convenient, safe, all files are in the same folder, it's easy to switch from one to the other, so why not? But right now, not all projects are a good fit for a Bazel build, so I wouldn't quite recommend it at all times.
Please, please don't do the first one. It's an insane mess of include directories, that's not only hard to navigate for humans but also for compilers, build systems and IDE's. When I started at my company, they had option 1) and the first thing I did was to switch to option b). It's just much, much saner. &gt;use a suffix in private header names (for example private headers in Qt end with \_p.h) &gt; &gt;create a public include/ headers whose file only forwards to the real include &gt; &gt;split out things manually at install time But why so complicated? Especially the "forwarding" doesn't even work, because that requires you to ship internal headers \- otherwise the includes won't work for the users of your library. It's pretty simple: 1. Get as much as possible into the src/ directory 2. Put only the public interface headers into the include/ directory 3. Prune and reduce dependencies and put only what's absolutely necessary for consumption of your libraries into include/ .
If you think I'm ignoring you there's no point in continuing. For my part I no longer believe you're arguing in good faith.
How about using the "refactor" abilities of any modern IDE?
I'm at least *trying* to argue in good faith. I will at least point out -- note that I am not dcrc2, who posted the original response to you in this thread. So if we're done, at least don't put than on him. But the reason I said I think you're ignoring the examples in the thread is because dcrc2 posted a fair bit of detail about the way they're relying on bad_alloc exceptions. I'd guess that your proposed solution to that problem (preallocate and reserve) more likely than not just doesn't work for at least on of the reasons I described. Even if it does work, it'd require threading that allocated pool through a bunch of code that presumably doesn't have it right now. That's not a small task. Your responses to CubbiMew saying "just use `try_`" are overlooking the fact that that will often be a monumental task to replace all potentially-allocating calls with the new `try_` versions and stop using any allocating constructors (good bye "don't use two-phase initialization"). In dcrc2's case, he says "there must be thousands of functions in the code where memory is allocated. I could never justify the time needed to replace all of these calls, even if that was a good idea." So from my perspective, I think you're just appearing very dismissive of what I think are very real concerns and problems of that small group of people, almost saying "your problem isn't a problem."
but is the optimization here often worth the trouble?
You can simplify the `add_test` to simply `add_test(NAME plain-run COMMAND vector-test)`. Since `vector-test` is an executable created by CMake it will properly handle finding the location. 
At that point, you are duplicating your structure which you have to keep in sync on the filesystem. Why not simply put all files in tree, and list the public headers in your build system? The build system can then automatically duplicate the folder structure, even e.g. pruning folders as they become empty. You then make things public/private simply by adding removing files to a list. Seems more straightforward to me.
One thing that you can do about warnings if you are either alone or working with very disciplined team-mates is to set them via the environment during the initial project-generation. Personally I have the following line in my shell-configurations such that I always have a sane environment: export CXXFLAGS="-fdiagnostics-color=always -Wall -Wextra -Wpedantic -Wconversion -Wsign-conversion" alternatively you can do prefix the first cmake-command like this: `CXXFLAGS="..." cmake ..`. The good news is that cmake will remember this and you won't have to pass it ever again, the bad news is that it will remember the initial setting and you cannot easily change it at a later point. The other issue you face if you put it into your general shell-environment is that all projects from other people that you try to build will pick up these options as well; given how many people are unaware of the last two, you often get tons of warnings (I still recommend to use them though!). Finally: This works if you work on the commandline on Linux (and probably any other unix-like-system), I don't know anything about windows at all. (The best solution to that is of course to not use an inferior os. ;-))
Arguing for A doesn't mean ignoring B. In fact it's the sort of intellectual exercise used in the Socratic Method that is effectively used in committee: first you argue your points (ideally with use cases), and then *after* you make a decision. I heard their uses cases and it informs my opinion, which by the way includes the point that on paper it's easier to use outside libraries if you can just catch a `bad_alloc` (and the paper calls that fact out explicitly). But Herb's paper isn't just "let's get rid of `bad_alloc`", it's a holistic suite of changes that go together to make a full argument, and you weight the pros and the cons of the whole thing together. `bad_alloc` just means a failure to allocate, but the paper itself actually discusses OOM; including the fact that on systems with overcommit and virtual memory, it's effectively impossible to fail to allocate, and with an OOM killer, termination is already observable behavior. Moreover, it's a paper, and for a good standard to result it needs all counter arguments fully formed and presented for discussion. "What's your use case?" is the act of unearthing those counter arguments. If I say "but you can do X", that's a prompt to say why X is prohibitive or impossible.
One issue WRT `@` which was raised in [at least one recent paper on operators](http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/p0611r0.html) is that it is used pervasively in Objective-C, and therefore Objective-C++.
gcc has a very cool thing named function multiversioning, which allows the compiler to use optimized x86 intrinsics when available on the runtime, or to fallback otherwise: [https://gcc.gnu.org/onlinedocs/gcc/Function-Multiversioning.html](https://gcc.gnu.org/onlinedocs/gcc/Function-Multiversioning.html) Note that while gcc actually honors your arch setting when using `__builtin_popcount`, `__popcnt` with MSVC *assumes* that SSE4.2 is available. I learned the hard way. Thanks Microsoft! &lt;3 AFAIK, icc (intel's C/C++ compiler) can do this automatically. gcc doesn't seem to support that sadly... I'm curious if anyone ever gave it a shot and noticed possible issues? /shrug
Too late to rant, std::span::index_type is [broken in the very same way](http://en.cppreference.com/w/cpp/container/span). 
&gt; Sean Parent's well-known "no raw loops" guideline. I 100% disagree with this guideline. Besides bloating your object code with various random instantiations it just generally makes everything less clear to other readers. 
There's still a chance to change that, the proposal hasn't been merged in yet.
I agree that consistency is the most important thing, implying that this really should be `std::size_t`, but even if we put that aside: Most arguments against unsigned integers seem to revolve around problems that are easily fixed by using `-Wconversion -Wsign-conversion` in GCC and Clang (note that these are not part of `-Wall -Wextra -Wpedantic`). AFAIK `/W4` should do something similar with VS(?). 
&gt; But not for accumulate(), because that's in &lt;numeric&gt; and not in &lt;algorithm&gt;, sigh. Lol - another great example for the "Vasa"... How can that be? I sincerely hope &lt;numeric&gt; gets in there as well...
Well you can get close to that with `Eigen::Vector`.
The committee has already stated that making index_type unsigned was a mistake. Though it's also questionable that whether making future index_type in std signed is the proper fix as it breaks intercomparability among types.
It sounds like a really strange advice to me if trying to understand a version with a named algorithm takes say three times more than the loop-based version.
Signed size types are an abomination! Why would anyone think they are better?
I heard them say that it is a mistake, but did they ever elaborate why it is a mistake? I haven't had an issue with size_t being unsigned to this day.
The core guidelines have some good stuff in there but the problem with it is that it's not particularly helpful to people who don't already know what's good, what's bad and what the trade offs are, so it ultimately ends up being more of a crutch that keeps newcomers in a "local maximum" so to speak. What I mean by this is that you have a lot of well meaning C++ developers doing everything they can to learn proper, modern C++ from the big names like Herb Sutter. But a lot of people just end up memorizing the rules without really understanding them and so they are unable to differentiate the good well established rules that are an obvious benefit from rules that are more... how shall I say... discretionary, context specific, rules where the author is exercising artistic license. Now really solid and experienced C++ developers know how to differentiate between them, but then again those same solid developers don't really need to read over these guidelines all that much, they mostly are familiar with these through experience. It's a tough situation overall and makes learning C++ quite difficult. I personally have just become really skeptical of much of the advice professed religiously about what good modern C++ is, and I'm almost afraid to confess I basically just stick to absolute simple, basic stuff, and write my C++ like C but with classes, RAII and namespaces. It's not a popular opinion to share online where I feel everyone wants to express how they use the niftiest and newest C++ meta programming technique... but I think despite the blog posts and C++ articles and guidelines professing to just write simple basic stuff... my approach is significantly more popular than it gets credit for if you look at actual real world and widely used C++ projects. Look on github at the most popular C++ applications and you'll see a world of difference between how industry actually uses the language, and how the so-called thought leaders tell us we should be using it.
Yes, that's generally the main plus. But quite frankly, I don't see a huge difference. Compare, for example, a simple CMake project with a Java one (built with, say, Gradle or something) In CMake, your configuration to find the include directory is just 1+ line(s) of text. `include_directories()`. It's explicit and very clear that you're searching in *this/these* directories for your header files. If you're doing something a bit weird (I do weird things a lot, good for learning experiences) then this is super convenient if you have to, say, switch to a different directory. You just change the line of configuration. Compare to a Java build - your configuration *is* your layout. Want to try a different include directory? Better start renaming folders. It's not like you're magically removing the necessary configuration or the learning curve. I'd argue you've made both of them worse: Renaming and creating a bunch of arbitrary folders takes time, more time than adding in a line of configuration. And figuring out how to get your directory structure *exactly right* so that the build system with be good with it is... well, I found it harder than CMake, at least. But, of course, that's just my experience, (and inexperience with Java) and that's why I prefer non-standard layouts.
One of my pet peeves with Core Guidelines is the advice to not use `#pragma once` simply because it's non-standard.
I'd like to give my vote to this one. If your project gets bigger than, say 20 sources, one folder gets way too bloated imo.
There are lots of reasons *I* think they're better. * There are certain iteration patterns and such that are far more convenient to write if `0-1` gives `-1` instead of `SIZE_MAX` * An analogue of the first point, there are certain mistakes that are relatively easy to make with unsigned size types and hard to make with signed size types * Uniformly using signed size types means that you could do (admittedly very unusual and/or un-C++y) things like arrays where `[0]` is in the middle of the array and negative indexes are valid, or Python-style indexes where `[-1]` is the last item in the array * At a conceptual and readability level, I think `-1` makes for a better sentinel "invalid index" value than `SIZE_MAX`; despite the previous points, I think it looks more like an invalid index. * The fact that signed overflow is UB can allow for "better" code generation * The fact that signed overflow is UB can allow for better *error detection*, which is actually somewhat surprising so I'll explain a bit more. I assert that overflow on `size_type` is almost always going to be an error -- the one exception I can think of is if you are correctly relying on `0-1` to produce `SIZE_MAX`, e.g. to terminate a loop. That's true for both signed and unsigned overflow. However, a tool that wants to soundly report errors in your program -- like ubsan -- can only do so for *signed* overflow. That's guaranteed to be an error by the standard. Unsigned overflow is *probably* an error but may not be who knows maybe the program is relying on it. So said tool can't soundly report unsigned overflow as an error. This is reflected in ubsan's defaults, for example. None of those reasons is like *super* compelling, but together I *do* think they're more compelling than the arguments I know *for* unsigned: * `unsigned` can represent a larger range of values. I don't find this very compelling because the only time this could matter on even unusual systems is when you are counting single-byte objects. If you are counting two- or more byte objects, you can't have enough of them. And are you realistically ever going to be using more than half of your memory with a single container holding single-byte objects? * It's better documentation for the programmer -- after all, `v.size()` will never be negative. I think this is a decent reason, but I think it's outweighed by the other readability stuff above. Finally, I'll say that I'm *not* convinced that the reasons in my first list are enough to outweigh the fact that `size_type` is unsigned everywhere else in `std`. That may be the most compelling reason overall to keep `span`'s `size_type` as signed. I don't take a position on that; I don't think I can talk intelligently about it. (E.g. I've not tried to use `span`, so I don't know how common the problems are.) But that's not really what you were asking about from my reading. Even if it was, the counterargument to *that* would be that while consistency is good and inconsistency is problematic on its own, it's also true that if I bang my head on a cabinet door, I don't keep running into it deliberately just because I've now set a precedent. If you keep doing the same thing you won't break things that rely on the current behavior -- but it also means you can't fix broken things.
`std::basic_ios::init()` also exists as a different thing, and someone in the distant mists of the pass decided that uppercasing the I on a thing that isn't meaningfully part of the API was the right way to resolve the name conflict.
Arithmetic on unsigned integers is more bug-prone. A newbie who doesn't know the signedness of size_t, or a professional on a deadline, can easily write code like the following: auto targetIndex = v.size() / 2; auto minAllowedIndex = max(0, targetIndex - 3); auto maxAllowedIndex = min (v.size(), targetIndex + 3); And then they get a 4 billion showing up. I know this can happen, because I've shipped code to production like this. Recently. I'm not proud that I did, and I certainly should have tested better. However, the intent of this code is obvious, but it breaks for a reason that a newbie will consider arcane.
I wish we could just: double rms(const vector&lt;double&gt; &amp;v) { return sqrt(accumulate(v, 0.0, +=)); }
That's not the only reason to avoid it. Essentially all compiler writers will tell you the same. It cannot be made to work reliably and should be avoided for that reason alone. Coincidentally, the inherent unreliability is why it isn't standardized.
Not exactly a rebuttal, but `-fsanitize=unsigned-integer-overflow` exists in (at least) Clang. 
Yup, that's why I talked about ubsan's defaults and was careful to stress "soundly" in there. :-)
In real life, the most used compilers support it just fine, and in the eventual case someone needs to compile code that uses it on a compiler that doesn't support it, there are tools that convert `#pragma once`s into header guards automatically. So there's literally no reason not to use it, if that's what you meant by unreliability. :P
Clion JetBrains
&gt; (I mean, is `gsl::not_null&lt;gsl::cwzstring&lt;&gt;&gt;` really better than `_In_z_ wchar_t const*`?) Well, one is portable C++ and the other is MSVC-only C... Gee, I wonder? Ranting is fine, but I hope people aren't expecting arguments of technical substance here.
Something works great in 99% of all cases in all major implementations? No, definitely not good enough for the Standard. Something else does not play nicely with the rest of the Standard Library? Meh, good enough for the Standard. This is why we can't have nice things.
One opinion I haven't found in this thread is that if your little python project grows, it can be tricky to manage (one programmer's opinion here). For example, in C++ if you want to change the type of a variable, the compiler / type system will give you a hint about most of the places you need to rework. In python, you change the type, and then you kinda gotta manually trace around the code and make sure you're good everywhere. Same thing applies if you e.g., want to remove a member from a class. I'm sure there are counterexamples to the above, but it's one situation I've found myself in a few times working on a large python codebase. 
One argument I find missing for signed is that given such a value as a parameter, the precondition is obvious: can't be less than 0. With an unsigned type, the same precondition either wouldn't exist, letting the error of an underflowed value slip by to a later point in the program, or be something counterintuitive like `value &lt;= INT_MAX`, counting the second half of the range as invalid. On one level, I really like the ability to easily detect these underflows as early as possible through preconditions. On another level, having a signed type when only unsigned values are allowed defies all logic. This could be improved with strong types that don't exhibit the same eager conversion problems as built-in types: `index{2} + 3 == index{5}`, `index{1} - index{2} == difference{-1}`. That all said, this certainly requires more code both for defining the types and for using them. Given the trade-offs, I'd normally be pretty fine with the band-aid fix of making them signed, especially if that kind of slicing is a future possibility. However, I think trying to integrate a "dumb signed" type into a "dumb unsigned" world will backfire immensely.
Because then subtraction works as expected. 
The problem I have extending the "only positive values are possible" argument much beyond readability (and I hit on that aspect already) is I actually don't view that as much of a precondition at all. Suppose we're talking about `vector::[](size_type i)`. The *exact same values* (0, 1, ..., size()-1) are valid whether size_type is signed or unsigned. Yes, the condition you'd write to test that is a bit simpler in the unsigned case (`i &lt; size()` instead of `0 &lt;= i &lt; size()`), but the same values are valid. The same values are invalid. In terms of the bit patterns, there is no information at all that is conveyed by signed vs unsigned. I don't think that's necessarily the whole story, but I *do* think it's *very* suggestive. &gt; On another level, having a signed type when only unsigned values are allowed defies all logic. I think part of what I'm trying to say is that conceptually I think the size shouldn't be thought of as `size_type = integer` (for some flavor of integer, no statement regarding signedness) but `size_type = value of integer | invalid of integer`. So then `(value 5) - 1` would yield `value 4`, but `(value 0) - 1` would yield `invalid -1`. That's at least true if you want `size_type + 1` to yield another `size_type` (or `index` according to your following example) and not just a straight integer type. The reason is that doing arithmetic on an index doesn't necessarily give you another index. Actually, now that I type that -- I think that actually reifies a lot of what I'm thinking here, and a lot of the arguments I tried to make before. If all you did with indexes was get an index, store it off, and use it -- I'd agree that unsigned is appropriate. But that's *not* all you do to indexes. You add and subtract them. You increment them. And that means you *very often* end up with invalid indexes on purpose. (Any loop `for (size_type i=0; i&lt;c.size(); ++i)` will briefly result in the invalid index value `i == c.size()` at the end of iteration.) And that means you have to look at not only what better represents indexes, *but what better represents that mix of indexes and invalid values* -- and for the reasons I gave above, I think signed values do that better. &gt; index{2} + 3 == index{5}, index{1} - index{2} == difference{-1} And this doesn't help with respect to my argument above, because `index{2} + 3` may *not* legitimately result in `index{5}`, it could result in `invalid{5}`.
... the heck? Their argument: &gt; A natural, portable implementation of span is as a pointer+length (or alternately, two pointers). The maximum addressable distance from a pointer can always be represented by std::ptrdiff_t so it is the correct choice. It's undefined if the difference between two pointers is greater than ptrdiff_t. It certainly does not describe the *maximum* addressable distance - that's impossible. 2^63 &lt; 2^(64).
"unsigned can represent a larger range of values." Not true. Unsigned and signed int can both represent an equally sized range of numbers. The prime differences are in the min and max bounds and whether over/under flow are defined. Maybe also whther sign extension occurs on shifts.
I do it like this: foo\_project include foo\_project a.h b.h source a.cpp b.cpp Then I do includes like this: \#include "foo\_project/a.h" \#include "foo\_project/b.h" \#include "bar\_project/c.h"
Everything together but have your build COPY public headers to the install path
Have an upvote came here to say the same
&gt; "unsigned can represent a larger range of values." &gt; &gt; Not true. You're technically right, but my statement can be easily repaired: unsigned can represent a larger range of values *that are not facially invalid*. `-1` cannot possibly be a size of a container, but numbers between `SSIZE_MAX+1` and `SIZE_MAX` could be, theoretically, on some platforms .
&gt;Similar to what Java `sort` does when it detects inconsistency in the supplied comparator. VS implementation of `sort` also does this on debug, it's pretty cool.
Enabling signed conversion warnings is certainly a good idea, regardless of span. But the problem is that, in the case of span, this results in a lot of verbose useless casting back and forth. Personally I've switched my own span to unsigned and seriously hope this will be done to `std::span`, too.
/u/NoGardE gave a good example of mixing signed and unsigned types leading to bugs that are unavoidable because of index_type being unsigned while some operations of counts require signed type. More discussions at https://github.com/ericniebler/stl2/issues/182.
&gt; The exact same values (0, 1, ..., size()-1) are valid whether size_type is signed or unsigned. Yes, and it's nice having a type where one side of that is guaranteed with some kind of loud error otherwise. This prevents the need to repeat that condition in every function that wants to fail fast if given such a value. I feel that rather than this `index` type saying "this type is unsigned", it says "this type supports values that could be a conceivable index to some range". Of course this still means the functions check the other end - more on that later. I share your logic that these types would have some kind of invalidness baked into them. In my picture, `index{n}` would have a precondition that `n` is positive. `index{n} - 1` would also have a precondition that it doesn't end up in the range of definitely-not-an-index values. It's the same idea, but rather than a sort of ADT like you describe, this would have the validity as a class invariant and fail any operation that tries to make it invalid in a way it can detect. While this wouldn't cover `for (index i{size(c)} - 1; i &gt;= 0; --i)`, I already prefer writing this as `i-- &gt; 0` as it currently works cleanly for both signed and unsigned - an idiom of sorts. As range tools develop, this kind of manual index iteration should get less common, too, with an algorithm providing the index as a lambda parameter instead. I'd prefer detecting an index becoming -1 over allowing the use of the first iteration example. You astutely point out that this isn't so easy with the other end of the spectrum where an index is too large. It's conceivable that a system could be built where this is also a class invariant, with the index checking the container's size when changed and the container telling the index when the size changes. However, this is very complex and costly in comparison to not guaranteeing the upper bound. It could simply hold a reference to the container and have a mechanism to force checking before passing it to a function. However, this is still rather complex and also ties the index to a particular container, which prevents some other uses. All in all, I don't believe it's practical for the upper bound to be baked into the index type and caught at the point of the addition instead of at the use of the final index value. The functions taking an index would need their own precondition to check against the range size. It might be repetitive, but they have the ability to painlessly check it against the size at that point and it doesn't include repeating the more trivial `&gt; 0` check as that can easily be included in the type. I would personally prefer this half-invariant over preconditions needing to repeat both sides like we get with built-in types.
me too, i end up casting it to a (signed int), which feels like I'm fighting the language.
Can you elaborate on this? We are currently having discussions about using it in our office but this hasn't come up yet. 
I don't see why not.
This right here. So many fucking casts.
&gt; It cannot be made to work reliably IIRC there is some *extremely* weird edge case involving symlinks, and that's the only case. And I have a habit of using a lot of symlinks a lot, but I've still never hit it.
I think techniques like this are an idea whose time has come, whatever form they end up taking. Here is my own architecture that does something similar and integrates nodes that keep state, a gui to build program visually, a separate visualization process and low level exception isolation while keeping everything lock free and asynchronous. https://github.com/LiveAsynchronousVisualizedArchitecture/lava
CMake files should be a common scripting language - not it's own. ruby, python, tcl, perl, js... anything! ....ducks runs away...
Incredible how most people here seem to think index types should be signed, with the main argument being "it prevents bugs". If you don't understand how unsigned vs signed integers behave, you're going to have a lot more problems real soon. 
Pragma once fills a sorely missed need and has worked for years on all the compilers most people care about. The committee has been extremely and inexplicably wrong headed in refusing to standardize this, and are basically contributing to a fork in the language where many library authors will use it because it is simpler and more maintainable, some holdouts won't, meanwhile every compiler is free to give a slightly different implementation which will on rare occasions bite us in obscure ways. The committee should do the work of standardizing a replacement for include guards that is DRY and doesn't fail silently if two long all caps tokens mismatch in a hard to spot way. The committee is letting down the community by failing to do this year after year.
The CMake DSL has a limited set of features, designed for building C/C++ applications. A more powerful/complex language like Python would allow people to write byzantine build scripts. The fact that the CMake DSL has limited capabilities is a point in it's favor, as it pushes people towards writing simple and limited build scripts. A build script should not be doing complex logic.
Keeping public headers separate means you can make accidentally grabbing private headers impossible. 
I see your point. But does it preclude having a python like language with limited features? The DSL should help you get quickly up to speed but stop at "the edge of the world".
Good. But "for programmers" as opposed to what?
[This](https://stackoverflow.com/a/34884735/91642) is what I was able to find. Apparently, ensuring that two header files with similar names aren't included twice or one of them accidentally omitted is a hard problem.
Well we're getting modules, this will eliminate the need for `#pragma once`.
&gt; there are tools Research and replace regexp you mean?
&gt; The committee has already stated Oh, really? Do you mean to say that the committee voted on and published something as a body, or do you mean that one or more members of the committee disagree with unsigned size_t as individuals? There's a pretty big difference.
You're almost never going to index something that would require the unsigned value, but you are quite likely to mess up and end up with `SIZE_MAX` . With the contracts now in the standard, you can make it UB or throw an error if the value is negative.
It doesn't work the same on all of those compilers. For example, are pragma's statements or expressions? What does it mean to write for (size_t i = 0; i != n; ++i) #pragma whatever { // do stuff } Well? Is the program well-formed? Is that block enclosed within braces a loop body or is it just some other block of code?
"People expect numbers to work like numbers" - Tony Van Eerd "Nicely put" - Bjarne Stroustrup (from recent emails on this topic)
Actually, no! I saw a really neat converter on Github that deals with special cases and makes sure to automatically generate uncollisionable header guards.
&gt; With the contracts now in the standard, you can make it UB or throw an error if the value is negative. So you can write out a contract for every parameter to every function that accepts a size rather than just using a type whose bounds naturally models the quantity being modeled?
Writing code on a phone is really painful, I would definitely advise against writing something with more than 50 lines. Just the boilerplate to handle the input will take a while to code. It's most likely possible, but you're better off using a computer because the typing will be much easier and you have access to better tools.
Well, so far the committee has said yes to span being signed, so we are on the way to "published something". And that's with many vocal "pro-signed" members actually arguing for unsigned, because consistency is more important. This isn't exactly the committee either, but very prominent C++ figures: https://www.youtube.com/watch?v=Puio5dly9N8 see 9:50 41:08 1:02:50 
The standard functions would have a contract, you wouldn't have to write it yourself.
It comes down to symbolic/hard links and essentially no way to tell if some file has already been opened in some specific setup involving such links.
I believe several committee members have said that it was a bad design to make index_type unsigned (you can check out the panel session of cppcon from 2013-2016, sorry I don't remember exactly which year. I do remember it was Chandler Carrueth making the comment). But there is no proposal on amending the wording of standard. I believe such breaking change too fundamental to attempt, even it's a good thing.
How do you get any collision if you simply copy the full path?
It's not a matter of understanding unsigned numbers. It's a matter of them being yet another pitfall and opportunity to make a mistake; yet another place where you have to be perfect. Besides, I think that's a somewhat disingenuous statement anyway. I know of at least [six arguments](https://www.reddit.com/r/cpp/comments/8r5b3k/this_from_gslspan_is_a_good_example_of_why_the_c/e0op2vm/) for signed size types, only one (okay, maybe one and a half) of which is "it eliminates bug causes."
&gt; Apparently, ensuring that two header files with similar names aren't included twice or one of them accidentally omitted is a hard problem. Not two header files with similar names, but *one* header file that is accessible via multiple paths (e.g. by symlinks, hard links, or copying it (e.g. as part of a process of installing to an `include/` directory) and then `#including` both that and the original location). I don't think these are particularly compelling examples personally (especially the last), but maybe some disagree.
No idea. I just remember reading it did something like that ü§∑‚Äç‚ôÇÔ∏è
I think this is overthinking things. "`#pragma once` must be specified while at global or namespace scope." Boom, done. Sure, it's not fully general, but it hits 99% of cases and there's still the old include guards fallback for the others. (OK, I'm exaggerating there with "boom, done", but point is that you don't need to solve what your code means.)
I like using [waf](https://github.com/waf-project/waf), but its manual is pretty byzantine on its own.
This depends somewhat on the nature of the problem you are solving. I think the single most important question is: do the distributed workers need to communicate with each other or is the work they do independent?
I could use some Cmake advice. I've got a project that eventually builds and will install to the system directory a shared library. I want to add a version number somewhere in the project CMAKE such as other projects which import it will be able to check for the version. For example. * Sub-project - builds a bunch of useful tools as a shared library and installed * Main project CMakeListsts.txt find_package(subproject 1.1) 
This is terrible advice.
When can I see a picture of this framed?
I guess you could try to make contrived examples that would break it on purpose but regexp work when you're working with normal people, not satan.
&gt; would allow people to write byzantine build scripts If I had one word to describe CMake byzantine would be a pretty good choice!
Cmake is really good for blank-slate C++ projects that only have dependencies that can be built easily with `find_package()` or `add_subdirectory()`, especially if you have an IDE like CLion that will manage CMakeLists edits for you. if not, I don't blame you for using a different toolchain... even if im not sure what that toolchain entails. 
In addition, current hardware is unimaginably far from ever having a span greater than 2^63 bytes. You would need 33 million computers, all with 256 GiB of RAM, all networked together and existing in a single address space to even have your address space cover 2^63 bytes, much less any single allocation out of it. So for the foreseeable future (which is a lot further out than 2038), every single valid offset will fit in a signed 64-bit integer, so there is no benefit to making it unsigned.
Judging from lots of reddit stories, I think it's safe to say most people don't work with normal people üòÇ
While `size()` should never logically be less than `0` it is logical for some indexing schemes to use numbers below zero; therefore it makes sense that for these types the `size_type` ought to be signed. Some people think it will also be better for the other types as well, but without a full stdlib implementation that goes that way it's hard to argue since we can't experience it both ways easily.
Fair enough. I found that it is impossible to code for satan, you can just do the next best thing: `git blame`.
Starts off by saying it will not talk about user-defined literals, and then talks about a case where it would be appropriate to use them (`std::string` literals) 
Just to clarify, the talk of a common build description (or API) predates the Rapperswill meeting and several individuals have expressed interest or said they are working on something in this area (see the SG15 mailing list archives for details).
 &gt; The reason is that doing arithmetic on an index doesn't necessarily give you another index. Right, just like how in geometry, the difference between two points is a vector (i.e. (non-negative) distance and direction), the difference between two (unsigned) indices is usually a one dimensional vector represented as a signed integer, where the sign is the direction. So what you really want is for the subtraction operator taking two `size_t`s to return a signed integer. One way to achieve this is by using C++ replacement classes in place of `size_t` and (signed) integers. For example, the SaferCPlusPlus library (shameless plug) provides such [replacement classes](https://github.com/duneroadrunner/SaferCPlusPlus#primitives). 
Unsigned ints doesn't model non-negative numbers, they model modular arithmetic which is a poor fit
I used to share your view. Here is what made me change it: On x86_64, the upper 2^63 bytes are reserved for the operating system so an array will never cross that boundar. So `std::ptrdiff_t` is just as good as `std::size_t` (except signed so arithmetic works properly, and hence, in fact, better) so in that case your statement is incorrect. That leaves us with 32-bit: On Windows 10 at least, I can't even get a buffer that large (yes, with a large address aware executable and `VirtualAlloc`) so if you want to be cross platform you can't. Even on Linux where `std::malloc` will give you such a buffer GCC warns about it. And ultimately what is the point? You can only have one such buffer, and even if you have a large dataset that fits into 2^32 bytes but not 2^31, you are probably better off just using a 64-bit process, instead of trying to cram it into a 32-bit one. (If you happen to know of such a problem then please share, and perhaps you can persuade me into accepting that there at least exists cases where `std::ptrdiff_t` is not sufficient.) Ultimately I believe it was a mistake for the standard containers to use `std::size_t` and that the correct choice would indeed have been `std::ptrdiff_t`. Whether `span` should make the same mistake for the sake of consistency is another discussion (I personally believe, the answer is still no).
First of all, no one is taking away from you the ability to use arbitrarily fancy/nonstandard project layouts but the fact is that a lot of (maybe most) c++ projects already follow some FS convention anyway, so why not strive for a single unified convention that everybody (tool or human) understands without looking at the information in the build file (maybe that information could even become optional then).. Second, the other nice thing about embedding some project properties in the Filesystem is that it is build system agnostic. My IDE might be able to query cmake for the include directories, but what if I want to use meson instead? And many editors probably can't even talk to cmake.
Yes as soon as you actually use the unsigned as a number (quantity) you need to cast it. Everything would be simpler and more predictable if sizes used signed ints instead of pretending unsigned is the same as positive numbers, which they absolutely are not. The crucial aspect is that you need to consider a number together with its operations. If unsigned modelled non-negative numbers then any arithmetic involving minus would need to return signed, as the set of non-negative numbers is not closed under subtraction.
This is a great quote and sums it up nicely. Unsigned doesn't behave like numbers because unsigned doesn't model (non-negative) numbers. 
&gt; std::ptrdiff_t On a bikeshedding point, I'd have much preferred a `ssize_t`. It's both more descriptive of how it's being used and shorter.
The most common answer (at the moment) seems to be to split out the public header files into a separate include directory, which is what I'm currently doing too. I wonder however, how this might change with modules.
I would agree. I just wish it was in the standard, and not only POSIX.
I'm generally in favor of signed sizes, however, I've recently found myself writing the following code: `assert (i &gt;= 0 &amp;&amp; i &lt; itemCount)`. If `itemCount` were unsigned, I wouldn't need the first part of the check. This is basically the _only_ disadvantage of signed sizes I can come up with. 
It's actually less descriptive - it's not obvious what the first "s" stands for. I would have to read the C++ documentation to understand it. 
It seems to me this calls for a new type, `index_t` that like `ptrdiff_t` is implementation defined. This would allow compilers to warn on (or disallow) subtraction operations. And inappropriate implicit type promotions too?
Why would you want to forbid subtraction operations on an index type? What if I want to write: `int v = a[i] + a[i-1];`
My counterarguments would be: * The really dangerous thing are unknown unknowns. If you see `ssize_t` and don't know what it is, that's a known unknown. * It's *really* easy to google and find out what it is * Even if you don't know what it is and don't look up what it is, you can still likely make a good enough guess as to 90% of what it is * You learn what it is once (hopefully), and then you know it; `ptrdiff_t` is a bad name forever * I don't even think `ptrdiff_t` is a very good name for a newbie *either* I'm sympathetic to people coming across something for the first time and think efforts should be made to be sure that things are understandable for them. But at the same time, I think it's possible to swing too far the other way, where by bending over backwards for people who don't know something you make things worse for those who do.
&gt;First of all, no one is taking away from you the ability to use arbitrarily fancy/nonstandard project layouts If this is the case, then sure, I don't care. However when I used Java, you really *had* to have your project set up in *this specific way* or there's pretty much no way to build and run your application. (Even building without an IDE was a pain in the first place, but hey, Java...) That's what I'm talking about when I say I don't like enforced directory layouts. And that would take away your ability to use your own layouts. I don't care if there's an encouraged layout, or bonus features you get from using a specific layout; as soon as you start to enforce it, that's where it becomes a problem.
Yes for this reason using unsigned as a crude approximation of non-negative integers is ill-defined. It's like trying to treat a timestamp (a point) with the same type as a time interval (a distance)
Right. Not subtraction in general, subtraction between two `index_t` types. Subtraction between an `index_t` and a signed integer is fine. Or more ideally, you'd want the subtraction of two `index_t`s to return a `ptrdiff_t`. 
How long do you think until sizable code based will switch to modules? They are not even standardized yet and I wouldn't bet big money on them being in c++20. "We are getting &lt;insert favorite big proposal&gt;" is not an argument against anything.
Presumably someone tried to write an actual piece of code with std2:: here and there and realised what an abomination that would be?
No you cross-compile. You develop on a computer and deploy the app to iOS.
pragmatic in general are compiler specific (that is their purpose). Pragmatic once is sufficiently supported to be more portable than a lot of actually standardized c++ code.
Yes, but isn't this a one in a million problem that can easily be fixed if it happens? As long as it fails at compile-time, I'd rather have a simplified solution for the 99.99% case and fails maybe once per decade than sticking to those arcane include guards everywhere.
&gt; a thing that isn't meaningfully part of the API True. Who remembers all member inherited functions from ios? It's so well hidden most beginners don't even know whether `std::cout` is a function or some weird globally-accessible object