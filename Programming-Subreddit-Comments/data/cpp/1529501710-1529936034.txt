Yeah, I'm not a huge fan of the extra verbosity. ```#define AUTO decltype(auto)``` ? 
That is true but not exactly pertinent to the OP. I was merely explaining the merits of the feature and chose a simple pointer check over something convoluted. This was so that it would be more immediately apparent as to why the feature can be beneficial.
&gt; As a point of comparison it also implements a plain linear search using STL std::vector. How about as compared to `std::binary_search()` which seems like a more equitable comparison. P.S. If the author is reading, the proper word (and homophone) is `plane`, not `plain`. 
I'm guessing because this is an unsorted container, and `binary_search` requires a sorted container.
You know, this might be an easy compromise for the terse concepts syntax debate. A concept name makes it an implicit template, so require a set of empty `&lt;&gt;` to make it explicit.
More like regular C++, python or java, where you can have things like object.at(object.size()).something() How to spot a Lisp fan...
We'd also need a version for when the foils are locked down (i.e. not in attack mode): `-=-`
It's the [goes to operator](https://stackoverflow.com/questions/1642028/what-is-the-operator-in-c).
Most PGO implementations apply profile information to the source directly or compiler IR post-creation because they are correlated to the source line numbers in most cases. This requires the source to be present and is more of top-down approach. The article is saying that you don't need source for BOLT because it constructs CFG info from branch and label instructions in bottom-up fashion.
Will CMake ever get a nice, simple option to build with the static CRT (/MT option) in MSVC?
LUA (or something else) as the scripting language instead of the current one - what happened to that old attempt and is there any chance of reviving it? If yes - what would be the transition like?
what are points that you recognize to be flaws of CMake, and do you intend to tackle them in the near future ? 
Precompiled headers support? And especially the ability to reuse a precompiled header between targets? (see [this](https://github.com/nanoant/CMakePCHCompiler) for reference)
Why is CMake's philosophy to ignore errors if possible? An example: `set(CMAKE_CXX_STANDARD 17)` is not an error for cmake versions that don't know C++17, but is rather silently ignored, even with `CMAKE_CXX_STANDARD_REQUIRED` set.
How much of the engineering is working on the cmake executable vs providing better .cmake scripts for various tools/targets/libraries?
Why does cmake have its own language and weird syntax? I'm only a beginner and it seems like for everything i want to do with cmake i have look up some commands online just to setup a project. Wouldn't it have been better if it worked like scons? I know, hindsight is 20/20 but i feel this is a huge inconvenience and it puts a lot of people off. 
I have a question :) In a multi-configuration generator, like VS, having Debug and Release configurations, how can I build a package with CPack, which would contain artifacts built for \*all\* configurations? Thanks 
Any chance of ever getting fine grained scoping on the cache variables? As my project grows and I have to import more external libs, the cache keeps getting more and more disorganized. Would be cool if you can ever achieve the level of organization of Kconfig for example.
Do you think that C++ language will ever see a standardised build / package management system? Do you think that the language would stand to benefit from these things?
I'm a little unclear on the use case for a random number that never changes.
Any room for improving the performance of the configuration/generation steps? I've heard it takes a looong time to run CMake for the LLVM project. That's why projects such as [this](https://github.com/cristianadam/cmake-checks-cache) exist
The CMake developers have an ongoing discussion on how to implement precompiled headers in CMake ( https://gitlab.kitware.com/cmake/cmake/issues/1260 ) and all the problems that come from generator expressions and integration with multi-config generators. Personally, I think getting CMake to support modules is a better long term solution at this point.
Really want to put C++ modules into production. When is that going to happen on the cmake side? 
That doesn't sound correct to me, CMake should error out when it encounters the first target that tries to use that language with something like: ``` Target "foo" requires the language dialect "CXX17" (with compiler extensions), but CMake does not know the compile flags to use to enable it. ```
&gt; This is analogous to stating that T* is preferred to {unique,shared}_ptr&lt;T&gt; It's not analagous because the point in question of where to put const is solely about readability. The type of pointer you choose affects the behaviour of the program.
There are still a low level things that C++ should receive. Like static reflections or coroutines. No need for adding high level libraries for std.
Hey, I'm most'y just reposting a comment from a few days ago (edited to change context/clarity) but: CMake still doesn't truly support clang on Windows. They only support the clang-cl via the llvm installed VS toolset that uses the VS 2015 STL even on VS2017. It'd be great to see the native windows clang++ working. There's a bug tracking this here: [https://gitlab.kitware.com/cmake/cmake/issues/16439](https://gitlab.kitware.com/cmake/cmake/issues/16439) And some work on a branch here: [https://gitlab.kitware.com/dutow/cmake/tree/clang-gnulike-support](https://gitlab.kitware.com/dutow/cmake/tree/clang-gnulike-support) Are there any updates here? Is there anything folks could do to help?
What do you think about [Meson](http://mesonbuild.com/)?
Will CMake scripting allow true case-insensitivity? ``` find_package(protobuf) vs find_package(Protobuf) ``` The former won't work on OS witch case-sensitive filesystem. CMake as a multi-platform meta-builder should not that leak such a low-level detail bubble up to public interface. CMake (end)-users should not be required to understand that the argument to `find_package` translates to a file name. CMake users they should not be required to take care of platform-specific filesystem requirements. If this is behaviour is by-design, then it is a flaw in the CMake design.
You know somewhere out there someone things empty() empties a container.. they are in for a surprise.
We have had many iterations of improving performance of the configuration/generation steps (time spent with cachegrind) . I would encourage projects like the one you reference to try and upstream ideas like that. I don't see any reason why some sort of cache like that could be incorporated into CMake proper. -Bill Hoffman
If I am starting a new open source embedded project (cross compiled ARM target code), and I want to support people building on Windows, Linux, and Mac, is CMake right for me? Or would it just be easier to set up the build in a Docker container?
Not sure this is quite the right venue for this question, but hey, you said AMA so here you go. :-) Is there good documentation for someone who wants to hack a project that is built using CMake but doesn't need to actually edit the CMakeLists.txt file? Covering things like -- when do I need to re-run `cmake` as opposed to just the build files? Or I think I've seen uses of `cmake --build` or something -- should I be using that instead of running `make` directly?
Will CMake maintainers propose an official **CMake Coding Guide** with recommended guidelines for *scripting style* as well as best practices for *designing CMake configurations* (ie. promoting the Modern CMake conventions that prefers working with targets instead of variables, etc.) If not, why?
I don't see why it couldn't. I think many projects that need that use string replace on the flag variables. However, a higher level API to turn on and off static run time linking would be easier to use. Do you have an API that you would like to see? -Bill Hoffman
It isn't more clear and introduces a translation layer people have to think about. Last point is implicit is default but that is confusing when you have implicit in half the places.
So in general you should never have to re-run cmake after editing files. CMake adds hooks to the underlying build system to re-run cmake if any of the input files are modified. As far as `cmake --build` goes I highly recommend it over the underlying build system. First it makes any scripts easier as they can handle Ninja/Make/VS better, and with 3.12 you can pass parallel flags in agnostic manner.
How do you see some of the competitors to CMake?
Please do not assume that someone wanting to use something means they are qualified, able, and willing to contribute to it. 
I was attempting to integrate [Catch2](https://github.com/catchorg/Catch2) into my project as an [external project](https://github.com/catchorg/Catch2/blob/master/docs/build-systems.md) (CMake section). I wanted to utilize its automatic test registration CMake script. This issue was when I declared the [include](https://github.com/catchorg/Catch2/blob/master/contrib/Catch.cmake), I was getting the error because the repository is only downloaded during the build step, not the configuration step, hence the included file was no where to be found. Is there way on the horizon to be able to download projects during the configuration step?
Great, thanks for the answer! &gt; CMake adds hooks to the underlying build system to re-run cmake if any of the input files are modified. Does that include CMakeLists.txt itself? Or should it just magically work?
:) I actually had a quick discussion about this with Brad King yesterday and fixing this is possible and it is totally behaviour by accident. I haven't worked out all the exact steps required ( and all the policies that will need to be added ) but it is on my personal roadmap of things to fix. So far the steps in my mind are: 1. Add Policy that states &lt;UPPER:name&gt;_DIR is preferred over &lt;name&gt;_DIR for config modules 2. Add Policy that states &lt;UPPER:name&gt;_FOUND is preferred over &lt;name&gt;_FOUND for find modules 3. Add Policy that states if exact case is not a match we search for all upper-case, than all lower case. 
If you could release CMake 4 tomorrow which doesn't need to retain compatibility with older versions, how would it be different?
Thanks for linking this cool Wikerpedia side, hadn't heard of it. 
It would be great if you could provide an official “Getting started” “the basics” or “This is the recommended, correct and intended use with a small but non trivial example.” Right now if I go to cmake.org the closest thing are a link to videos or a book from kitware. I realize you need to make some money but I shouldn’t have to buy a book or take a class to get started. So I guess my question is what do you recommend for someone looking to get started? (Including examples with best practices)
It does. 
I just wanted to say thank you! We use CMake in our company to build a pretty big CUDA/C++ project, and your native CUDA support simplified our build scripts immensely. I think there are still quite a few minor issues for the polish, but it's already very useful.
So, when I started CMake over 18 years ago, the obvious option for a language was TCL. With hindsight, I am glad that was not used. The main reason it was not used at the time was to increase the portability of CMake itself. CMake depends on only a C++ compiler. Since the primary target for CMake is building C++ projects, you will always have a C++ compiler and will then be able to build CMake. In addition, having a DSL for CMake means that projects are not able to implement a private version of CMake for themselves. The key features of CMake get incorporated in the C++ code of CMake. scons is actually a good example of this. Many projects using scons end up with project specific versions of scons with custom code that is hard to move to other projects. KDE actually picked scons over CMake and switched back and the remains of that effort became another version of scons. When KDE switched to CMake, many features were Incorporated into CMake that have benefited thousands of projects over the years. All that said, there are some plans to allow for Lua as an extra language. -Bill Hoffman
LUA support is on the back burner of our wishlist, but we have not forgotten about it. We've identified a chunk of policies/deprecations in the CMake 3 series that are necessary to add LUA support. Although full LUA support will not come before CMake 4.0, CMake does now support [LUA-style long brackets](https://cmake.org/cmake/help/latest/manual/cmake-language.7.html#bracket-argument). Generally speaking, we don't feel that it's maintainable to have an arbitrary number of input languages to CMake, so two options is probably enough. We also take backwards compatibility very seriously, so CMake script (the scripting language) will never be deprecated or removed.
&gt; with 3.12 you can pass parallel flags in agnostic manner This is great!
Okay, I've written a long list of questions, and before I could post them my laptop died. The first one regards compiler-warnings. Every competent C++-programmer agrees that they are essential and yet CMake does not provide any support for enabling them short of manually setting the compiler-specific flags. Is there any chance that you could introduce something like pre-defined warning-levels that result in somewhat remotely comparable warning-levels across compilers without having to manually set them each time. I imagine a syntax like `target_warning_level(targetname 4)` which could for example expand to `-Wall -Wextra -Wpedantic -Wconversion -Wsign-conversion` on Clang and GCC and to `/W4` (?) for VC. They don't need to be exactly the same by any means, but I'd like to not have to throw around these things myself or manually pass them during project-setup. The second one regards GLOBs. I know that rerunning CMake on every build takes a few split-seconds, but I really, really don't consider manually listing all files to be acceptable. The issue is that GLOBs suck in many ways, including syntax-wise. Is there any chance that you may allow to add files using `add_target(targetname "src/*.cpp")` and just rerun cmake on every build for those who prefer it. (There would be no need to slow anything down for those who keep manually listing the files themselves.) 
The short answer is yes :) Long answer: CMake performance during configuration and generation are dependent on a couple of different factors and all of them can be improved. So looking at the problem in more detail I can think of the following improvements: Configuration: - invoking underlying compiler - Offer some way to batch try_compiles to reduce overhead of launching compiler - macro invocations - In general avoid if you can use a function. Macros require more parsing and with generator expressions that can add up - synchronous execution of CMake parsing [crazy] - I have brought up around the water cooler that in theory CMake could do deferred try_compiles and support something like 'async_subdirectory' to allow for parallel execution. Lots of this is - smarter CMake C++ code - std::string_view is going to help. Pool allocator might also be worth looking at Generation: - Multi threaded generation parsing / execution [crazy] - Another crazy idea, but I believe in theory CMake could parallelize generator expressions. A low hanging fruit would be to break `file(GENERATE)` into 2 steps ( buffer, write ) and parallelize the writes. - smarter CMake C++ code - See above 
That's a false dichotomy: cross platform CMake build scripts work greatly together with Docker containers
And if there happen to be two different packages with the "same" Foo vs foo name on that one system that supports case insensitivity? 
 set_target_properties(my_target PROPERTIES COMPILER_RUNTIME {static,dynamic,none}) so that it maps to the relevant /MT, /MD on the VS side, and the relevant -static-libgcc, -static-libstdc++ or even -nostdinc to not get any relationship with the platform's runtime.
Why still no package manager ? Why it is a pain to manage projects and dependencies in c/c++ ? So my real question, any plan to refresh/modernize the whole cmake project? 
isn't what this man page is ? https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html
Let me get you back to you on this. This is going to require :words:
Just want to say to you and any other Kitware engineers reading that I personally really enjoy using CMake. Thanks a lot!
&gt; CMake depends on only a C++ compiler. Language != parser. In choosing to implement its own parser for portability, CMake still could have chosen to implement a parser for any language, or language subset. The chose to roll their own. 
I think [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) is what you want. I wouldn't be surprised if there are some examples of other projects out there doing exactly that for Catch.
Interesting, you might be right. I can't reproduce it on my machine, but I remember running into this issue on one of our Travis builds -- all was green, but only because C++17 was not enabled.
**Company**: ScyllaDB **Type**: Full time; remotes welcome Description: ScyllaDB develops an open-source, high-performance, distributed NoSQL database, also (total coincidence) called ScyllaDB. ScyllaDB utilizes the asynchronous I/O engine Seastar, which we also develop, to drive million of operations per second on large multi-core machines with fast SSDs. The stack includes everything from custom memory allocators, through a user-space TCP/IP stack using dpdk, through high-level concepts like query parsing and compilation and maintaining materialized views in synchronization with the base table. Take a look at https://github.com/scylladb/seastar and at https://github.com/scylladb/scylla; if you like what you see you'll enjoy working with s. **Location**: Israel; San Francisco Bay Area; and many remotes around the planet (13 countries and counting) **Remote**: Very much, most of the workforce is remote **Visa Sponsorship**: in special cases **Technologies**: C++17, C++ concepts, boost, asynchronous programming, distributed systems, future/promise, C++ coroutines (eventually), JIT **Contact**: jobs@scylladb.com
cmake-buildsystem is a great place to start for best practices for designing CMake configurations. as far as CMake Coding Guide, not having one is a problem. One of my goals for 2018 is to write a blog post on recommended CMake Coding Style. I wonder if that would be sufficient? My worry about making it official is that it will be endless bikeshedding as we try to get consensus over things like 2 space indentation ( I might be wrong ).
Fair warning: if this thread turns into a Rust/C++ flamewar, I'm locking it.
Use [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) See [googletest](https://github.com/google/googletest/blob/master/googletest/README.md#using-cmake) for how to do this at configure time. The idiomatic way to depend on Catch would be with `find_package`, though
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8shfom/how_to_improve_this_bst_editor_i_am_new_in_c_but/e0zqz4u/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I thought the build system that everybody loves to hate was the autotools.
Excellent, thank you.
Hmm that is good question and we really don't track those kind of metrics. We could use the MR for CMake ( https://gitlab.kitware.com/cmake/cmake/merge_requests?scope=all&amp;utf8=%E2%9C%93&amp;state=merged ) and see what files churn the most often that would be useful. But we can't infer any time from number of changes. Anecdotally I think the core developers spend more time on Gitlab reviewing contributors changes, issue triage, and discussion than most people realize.
As far as I know in ancient version of CMake before it even understood `CMAKE_CXX_STANDARD` as a special variable for setting the CXX_FLAGS, it would silently do nothing and proceed along. Just replace `CMAKE_CXX_STANDARD` with `MY_TYPO_PROJKECT_CONFIG_VAR` and the question still remains valid and is the source of many hours of debugging build/test failures.
I only assumed the qualified part.
Is there a page somewhere that does a comparison to explain reasons why one would prefer xmake to other systems? I appreciate the reasonable support of wildcard globbing. This is such a critical thing it's shocking to me how few build systems will handle it. And if you don't you're garbage. Implementing the scanning of the directory tree is a nice _idea_ but it looks like it wasn't fully implemented. It only handles one directory level? How does it know which source files contain my unit tests and therefore should be compiled with different flags and the resulting binaries executed as part of the build? I suspect it doesn't know this. 
Is modules in CMake in the roadmap? If so, when can I expect using GCC's module TS implementation with CMake without huge pain?
Another question, and this may just be a misunderstanding of my part, but I'd like to see some more clarity and options around the set\_target\_properties command. Sometimes it seems to reset the property, sometimes it seems to append. In the case of LINK\_FLAGS\_PUBLISH for example, it appends. In the case of FOLDER it resets the property. There's also no way to actually reset a property to not having been set as far as I can tell. I've had issues with that regarding to Folder where I've written helper functions to auto-generate a folder structure. A separate issue (For context using VS2017 generator), but I'm fairly sure there's something wrong with source\_group(TREE), as I ended up needing to write this function: [https://gist.github.com/playmer/91a7642a4aec34e465a1d270f0f07c01](https://gist.github.com/playmer/91a7642a4aec34e465a1d270f0f07c01) To make it a little easier to use. As I need to run it on targets/sources which might have mixed files directly below the target and files in folders below the target. Without this (as in only calling source\_group once, with the prefix set to either "" or " " CMake will do different things, sometimes not giving any folders/filters, and sometimes generating "Source Files" and "Header Files" filter/folder for the files that are directly below the target.
&gt; when I started CMake over 18 years ago, the obvious option for a language was TCL I've been around for more than 18 years and Tcl was the obvious solutions only for Unix GUI applications (with Tk). Why not i.e. Lua? 
That is a good idea. I know that this has also been requested on the CUDA side for allowing better control over `cudart`.
The one you get is dependent on the search order of directories just like it is currently.
When do you think CMake is supposed/preferable to be used?
&gt; Many projects using scons end up with project specific versions of scons with custom code I used to think this was a positive. I loved all our custom `scons` magic. We had wonderful builds. &gt; code that is hard to move to other projects Yeah. Felt bad man.
I would say that a language like Python is appealing not just for syntax, but also because it includes lots of useful libraries. If CMake were to only have a python parser without the extra parts of python I still don't think it would satisfy what people are asking for, but maybe it would. Also, extracting the python or tcl parser from those projects and incorporating them in CMake would be difficult to maintain. This is why something like Lua is appealing because it is small enough to include with CMake. That said we would never drop the CMake language as backwards compatibility is too important, but at some point we might also allow for Lua.
I find myself, uhm, "unappreciative" of how TLS interacts with std::thread - I'd expect thread construction and destruction to also construct and destruct the associated TLS, but alas, that's not how it works. There is a real risk of C++ getting obsoleted by younger, more aggressive languages that are willing to cut corners. They may be incurring technical debt, but that might not matter to C++ by the time it comes due, especially since C++ already has a reputation for towering technical debt as things stand. Anyway, that's just rehashing an old argument by now, so let's not worry about that. At this point I see more of a future in developing the external ecosystem, i.e. get ourselves a good set of libraries for specific problem domains that are external to the standard but pervasive, at least across the major platforms. Boost, basically, although I would want to see less focus on maintaining compatibility with older C++ versions. And I don't mind unpopular opinions. I seem to hold enough of them myself ;-) I'm guessing that's mostly because my focus is a little different from other people in this group: we write applications, not libraries. They are not required to be 100% portable, but they must get the job done to a high degree of reliability. We very specifically design for error recovery - it's the pervasive design principle throughout the software. For the record, I work in the space industry, and I'm in charge of a piece of software that monitors and controls the health of spacecraft during ground-based testing. The software takes data from thousands of sensors, performs a mountain of calculations, and then decides on power settings for various heaters located on or near the spacecraft. It controls enough power that it can burn a test specimen to a crisp, and your typical piece of space hardware costs a few hundred million euros, so we care about reliability. So far we've had 20 years of operation without any mishaps - and hopefully it stays that way. Anyway, I've now gone completely off-topic, so thanks again for the explanation, and let's hope we'll get something good in the end!
how would/should `find\_package` handle this setting, since it's frequently necessary to link with libraries that were built with compatibly CRT settings? Short of a naming convention for libs, I'm not even sure how to tell if a library was built with a particular CRT setting.
&gt; CMake, the build system that everybody loves to hate. FWIW, I've never felt hate for CMake. It's made my life a lot easier. Thank you for doing what you do &amp; keep up the good work!
I normally do this with 2 build directories instead of 1. With 2 build directories you can use `cpack --config &lt;src&gt;/MultiCPackConfig.cmake` where MultiCPackConfig.cmake looks like: ``` include("&lt;build_dir_release&gt;/CPackConfig.cmake") set(CPACK_INSTALL_CMAKE_PROJECTS "&lt;build_dir_release&gt;;&lt;project&gt;;ALL;/" "&lt;build_dir_debug&gt;;&lt;project&gt;;ALL;/" ) ``` For a single build directory I don't know off the top of my head what the `CPACK_INSTALL_CMAKE_PROJECTS` should look like to get everything installed properly. 
&gt; how would/should find_package handle this setting, since it's frequently necessary to link with libraries that were built with compatible CRT settings? same way that find_package handles debug / release libraries ? 
I gotta ask, when will precompiled headers support land for cmake? I know cotire does a great job of automating things, sufficiently so that maybe cmake need never support them directly. Still, it would be nice if support came out of the box.
You can do basic linting of CMake by using the '--warn-uninitialized' and '--warn-unused-vars' command line flags. That will catch some of these.
Python-like syntax even missing many of the libraries would still be better than yet another custom scripting language. And just about any scripting language would be better than what CMake has.
My .02: whenever there's a bikeshedding decision, just make one and move along. The best formatting guideline is the one that everyone follows because there's a tool which formats to you and lets the engineers think about the real issues instead of how to indent lines. clang-format but for cmake would be a godsend
I think people are looking for small examples in modern CMake, such as [https://cliutils.gitlab.io/modern-cmake/](https://cliutils.gitlab.io/modern-cmake/)
You're right. Our reference documentation is pretty strong, but we still have work to do providing examples of best practices on how to write CMake build systems. We recognize this and it is a priority for us to get it right. For someone getting just getting started with CMake, I recommend that you use the ["Modern CMake" commands (aka Usage Requirements)](https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html#build-specification-and-usage-requirements) whenever possible. The benefit here is that usage requirements (meaning include directories, link libraries, compile definitions, etc.) are specified at the target level rather than the directory level. This finer-grained control allows such requirements to correctly propagate to other dependent targets. More simply put, use commands like these: `target_compile_definitions` `target_include_directories` instead of these older commands: `add_definitions` `include_directories` If you're looking for a good example of a Modern CMake build system, check out vtk-m. [vtk-m](https://gitlab.kitware.com/vtk/vtk-m). This is a real project (not a simplified example) so it handles a lot of issues &amp; options that might not be applicable to you.
&gt; clang-format but for cmake would be a gods That would be amazing! I also need to check in on [cmake_format]( https://github.com/cheshirekow/cmake_format) &gt; just make one and move along Imposter syndrome at work. I don't feel like I have the position to decree anything for the entire CMake community.
IMHO, for multi-configuration generator there is not much reason to have multiple build directories... -- this is the nature of the generator to have few configs in a single dir... Nowadays I use a workaround which works in the single directory via custom install script which installs the other configurations, so \`cpack\` (doing install) pack them all together ...
Is it in the roadmap to make exporting packages easier? Unless I'm out of date on the subject, to properly export a package you have to create some `.cmake.in` files, for example, a xyzBuildConfig.cmake.in with some `find_dependency` in it, add an export target with the namespace, install the config file in the right location, include some CMake scripts like `CMakePackageConfigHelpers` and call many functions... Only a small amount of libraries actually managed to do it *properly*. A one, or two liners to export a package would be wonderful.
Regarding any guidelines in a blog that would become some kind of authority, it would be nice to consult the community and see if we can agree on anything. I've seen many talks recommending things I strongly disagree with and there is no good way to change those recommendations. For example, one recommends to use macros when you are declaring something in the local scope, otherwise functions. No, just no, please. A function should be isolated and have clear documented side effects. No leaking variables by design. Any outputs should be intentional using the classic functions like add\_library, set(... PARENT\_SCOPE).
To be honest I had not heard of it. I now know it existed now, but TCL was at least in my circles more popular and the one I looked at. 
I want to also take some time and thank a couple of CMake developers that are not from Kitware, but without their effort Modern CMake would not exist. - Stephen Kelly - The primary motivator and author of usage requirements and Modern CMake! - Daniel Phifer - By teaching Modern CMake! - Craig Scott - Just being amazing at doing a massive amount of CMake code reviews!
The "seeds" are [`__TIME__` macro](https://docs.microsoft.com/en-us/cpp/preprocessor/predefined-macros) and `constexpr counter`, [`__TIME__` macro](https://docs.microsoft.com/en-us/cpp/preprocessor/predefined-macros) changes every time you compile the code, therefore the random numbers do change.
Regarding the so called *modern CMake*, an extremely useful tool would be something like `cmake-tidy`, a linter and static checker similar to `clang-tidy`. Did you ever considered something like that?
&gt;C++ has no real concept of UTF8. Honestly? That's probably preferable to what almost every other language has done. Most "unicode supporting languages" have elevated code points and assumed they can be used as "characters", which is grossly incorrect and leads to all sorts of stupidity. At least in C++ you have to choose to a) Ignore unicode or b) pull in one of the heavyweight libraries that gives you real unicode support. For most people, who are only ferrying strings from their translation file to their display API, choice (a) works just fine.
First, thank you for CMake, it saved me so many headaches. Not really questions, more thinly veiled feature requests, that would get rid of the more arcane invocations of cmake and the need for wrapper scripts. * can we have `CMakeLists.cmake` as an alternate spelling of `CMakeLists.txt` * can CMake bail on in source builds without writing any files * and/or be able to create a build dir for itself * can we have better control over command line options * i.e. let me write cmake --use-tool if I have an option(use-tool ON) in my CMakeLists * can we have better toolchain file handling? * toolchain files that include other files to keep them DRY * select project specific toolchains from main CMakeLists * system toolchain files * i.e. make something like --toolchain org.llvm.7.0.0 work * have standardized directories for toolchain-xyz.cmakes * support for sanitized toolchains * i.e. something like --toolchain clang.asan * support for find\_package to find the right binaries for each toolchain * some standardized directories where users/package managers can install the packages per toolchain
What about something like? async_try_compile(... TASK check_header_bar) async_try_compile(... TASK check_header_foo) wait_for_all(check_header_foo check_header_bar) if(HEADER_FOO_FOUND) ...
As C++ developers we fully understand the need for a C++ module system. Since CMake already supports Fortran modules such a module system should be feasible for C++ too. We are currently working on a timeline to make this happen, but we don't have any concrete timeframe to share at this time. 
I don't see it as a red herring, but a pragmatic decision made in the early CMake days. We (Kitware) were tasked by [NLM](https://www.nlm.nih.gov/) to build [ITK](https://www.itk.org) on Windows, UNIX and Mac. CMake had to work and be as portable as possible from day one. Regardless, CMake language is here to stay as we certainly don't want to break any of the projects depending on it. Lua integration could happen if there was a champion for it in the future.
What is your opinion on Symantec/Norton's "Smart algorithm" constantly falsely flagging your "CMakeCCompilerId.exe" test as "Heur.AdvML.B"? My IT director to has a fit every time it pops up, and I have to show they its a false positive. I have filed tons of false-positive reports with Symantec and they come back each time telling me its been white-listed but it pops up again the following month.
Do I have infinite funding to do all the work to change cmake :) Personally I have never thought about it since it will not happen. 
I wrote that documentation a few years ago, but didn't complete it for the windows side. Commenting so I can hopefully remember to dig that out tomorrow.
Is there something from the IDE developers that they aren't giving you to make better communications with them? I like CMake a lot but I really wish CLion would give me better code completion. I always forget the arguments for some commands and I have to look them up every single time.
&gt; I find myself, uhm, "unappreciative" of how TLS interacts with std::thread - I'd expect thread construction and destruction to also construct and destruct the associated TLS, but alas, that's not how it works. No, that's how it works. At link time, all the `thread_local` data is assembled into a special section in the binary. On thread creation, a copy of that section is made, and any dynamic initialisers run on it. The problem is that the *entire* set of `thread_local` data for the whole process is made available to every thread. That's acceptable with a few hundred threads, though I might note that in some cases on 32 bit platforms people have already run into trouble by making multi-megabyte arrays thread local for example. But for millions of threads, it doesn't scale, not just in memory terms, but having to init the entire thing for every thread. &gt; There is a real risk of C++ getting obsoleted by younger, more aggressive languages that are willing to cut corners. They may be incurring technical debt, but that might not matter to C++ by the time it comes due, especially since C++ already has a reputation for towering technical debt as things stand. Anyway, that's just rehashing an old argument by now, so let's not worry about that. Sure, but almost all of those younger languages come with lock in due to the single supplier problem. That won't worry people initially, but it becomes a real pain down the line if your startup becomes successful. And younger people forget how much of a pain single vendor lock in was in the 1980s and 1990s. The corporate sponsor of the major younger languages does not have your best interests at heart. They are not sponsoring the language for free, they want you hooked and dependent on them so they can milk you later. They may not shaft you now, but they certainly may shaft you later. The Java folk have started getting a taste of this recently, and that's a *huge* ecosystem, but Oracle can still shake down and harass those with deep pockets. That kind of blackmail based hegemony cannot happen in C++. That drew a ton of people to C++ in the 1990s because memories of being exploited were still fresh. Such memories have since faded, but let's be clear here, why do you think each of the major tech multinational made their own systems language rather than adopting a single "younger language"? If they wanted a new systems language to solve something C++ did not for them, they'd all have landed on some shared post-C++ choice. They fact they all were willing to invest hundreds of millions of dollars in their own proprietary systems language, none of which are particularly different to the others, means they think it's a good bet for a hefty revenue stream down the line. They think that C++ is hard to monetise, but something slightly better than C++ might be a better bet in a winner takes all outcome. For example, whoever invents the next Java gets a big cash cow. It's highly likely that C++ will continue to steal ideas from the upstarts, and provide a reasonable clone of those within five years. That suits its multinational sponsors well, and avoids lock in. It'll be interesting to see how things pan out. &gt; At this point I see more of a future in developing the external ecosystem, i.e. get ourselves a good set of libraries for specific problem domains that are external to the standard but pervasive, at least across the major platforms. Boost, basically, although I would want to see less focus on maintaining compatibility with older C++ versions. The state of packaging and distro is frustrating. If somebody would write a $5 million cheque, a permanent solution could be delivered within the year. But that's not how C++ works, unfortunately. It's volunteer driven, and that's mainly through volunteer-until-exhausted, which is a poor way to do good engineering. 
This is a never ending problem with all the anti-virus companies that false positively flag the cmake.exe or our small c++ verification programs. I feel your pain, as we have to report these all the time to. If anyone can help reduce this pain please teach me how!
Many people who made CMake are not Kitware engineers :). See https://github.com/Kitware/CMake/graphs/contributors for example. 
On windows Ive been using vcpkg with pretty good results. https://docs.microsoft.com/en-us/cpp/vcpkg
Some projects run a dual-buildsystem approach, e.g. with CMake and autotools. The outcome is never identical and just adds yet an other variant to software packages ("what build system did it build?"). Do you have any recommendations on playing nice with other build systems \_output\_ wise? For example, do you recommend to create .pc files in CMake projects, in case downstream are a few autotools users one wants to play with?
Now we just need walk-type, sprint-type and possibly jump-type.
I like that syntax. My initial mental version was way more complex ( and most likely impossible ) with CMake treating variables as futures.
One of the difficulties with using existing languages is backwards compatibility. While Lua is a very compelling candidate because of it's "embeddability", it still sufferers from older Lua code not being valid with newer Lua interpreters. The same is seen for python 2 -&gt; 3. Despite many of the shortcomings of the CMake language, backwards compatibility has always been a dominant feature of CMake. You can \*generally\* use old CMake code with new CMake and it should still work, albeit sub-optimally and with deprecation warnings, but still work. Given the lifespan of CMake, few other interpreted / embeddable languages an make that claim.
An interesting topic. A typical academic paper. Very wordy, hard to follow, lacking in clarity of purpose and conclusions. Will have zero impact int they real world. On the upside, will look good on the authors resumes. I find it ironic that a paper on documentation is pretty much indecipherable. I also find it ... disturbing that two of the authors work for google, and yet we have [Abseil](https://github.com/abseil/abseil-cpp) from the same company. Abseil purports to have [documentation](https://abseil.io) but in this excludes all the information that a programmer would require to actually use the libraries. The only way to actually use them is to troll the source code looking for the API. There are many components which are re-implementations of standard library / boost components, but there is zero explanation for the motivation of why one would choose these versions over the alternatives. It's a pathetic effort and sets a horrible example. For the first time ever, it makes me question the actual competence of developers at google. Who can possible find this acceptable? The sad part is that there are examples of how to do this. There is the [cpp reference](http://en.cppreference.com/w/). There are the original [STL docs[(http://www.rrsd.com/software_development/stl/index.htm) which are still relevant demonstrate the most useful and convenient way to document an API. Finally there are my own (pathetic?) efforts [here](https://www.youtube.com/watch?v=YxmdCxX9dMk) and [here](https://www.youtube.com/watch?v=ACeNgqBKL7E) to describe practical ways to create useful, correct and complete library documentation. Robert Ramey
I love you Steve :)
I think we need to see how C++ Modules TS looks when it's done. Brad suggested that something like the Fortran modules support in CMake could be adapted for C++: https://cmake.org/pipermail/cmake-developers/2018-May/030663.html However, that seems to rely on extraction from already-preprocessed sources being possible. As far as I know, that is not possible if modules can export macros. http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1052r0.html The GCC implementation currently allows the caller to specify a script to help with that. Personally, I don't think C++-Modules is compatible with buildsystems of today yet.
I agree, Modern CMake would not exist without the help of the greater Kitware CMake developer community. I am glad you enjoy using CMake! -Bill Hoffman
Is there a simple way to test available linker flags in CMake? I would like to do a platform-independent \`-Wl,--exclude-libs,ALL\` on my libs to capsule dependencies (default on Win, XCODE\_ATRRIBUTE on OSX since it lacks this support in \`ld\` and \`LINK\_FLAG\` target property on Linux).
That happens to already be valid syntax.
It seems to me that every build system that people use is hated.
Shhh, don't spoil the fun.
Yes, that was recently suggested at the last meeting. Might work!
Why not C++17?
Because one version of C++ is better than 100 separate dialects. And MACROS are evil.
&gt; Is there any chance that you may allow to add files using add_target(targetname "src/*.cpp") and just rerun cmake on every build for those who prefer it. This isn't precisely what you're asking for, but CMake 3.12 adds support for rerunning `file(GLOB)` commands at build time. If the GLOB output changes CMake regenerates the build system. At the very least, this solves the classic problem of `file(GLOB)` not noticing that you added a new source file. See [the documentation for the `CONFIGURE_DEPENDS` option](https://cmake.org/cmake/help/latest/command/file.html#glob) for more details. That being said, we still do not recommend using GLOB to collect a list of source files.
Did you ever fix the buffering problem with parallel builds? Doing output a line-at-a-time is seriously not that difficult, even in shell scripts, and is guaranteed to never cause problems as long as it's less than 512 bytes (4096 on Linux). `make --output-sync` is the most pointless flag ever. Did you ever fix that bug I filed where executability wasn't checked when searching PATH, breaking disabled transparent wrappers?
Adding to this, any thoughts on any of the other modern build systems? ( QBS, build2, etc. ) Is there anything that CMake could learn from them or anything that they need to learn from CMake?
I suspect but can only speculate that some of the virus developers use CMake as a build system and/or use the curl library. This makes cmake trigger false positives. Not sure what the fix is other than continuing the game of wack-a-mole and reporting the false positives as soon as we see them. -Bill Hoffman
yes but only if MACROS are in all caps. /s
Have you thought about marking all the workflows deprecated which are not target-based? `add_definitions` and manipulating global `CMAKE_CXX_FLAGS` for example. Or, as an alternative, is there a strict mode/linter?
I will be going to cppcon this year to learn more about these efforts ( and just subscribed to SG15 mailing list yesterday ). &gt; Do you think that the language would stand to benefit from these things Yes. &gt; C++ language will ever see a standardised build / package management system I am in the group that build and package management are two separate systems. Personally as a developer a C++ only build system would be insufficient for me as I work on projects where C++ and other languages ( C, CUDA, FORTRAN ) are inside the same library / build. At the package level I have the same questions as I would want to be able to tightly integrate Python into my package ( C++-&gt;Python, Python-&gt;C++ ). Maybe I am in the minority of C++ developers, and most projects are purely C++ but these are the questions I would love to get answers on. 
&gt; Deprecate implicit capture of this via [=] please someone stop the insanity. I just checked and I have roughly ~600 `[=]`-capturing lambdas in the project I work on, most of them using a `this` pointer of some sorts. I don't remember one case where it was a problem.
no go away. Shoo! Shoo!
&gt; can CMake bail on in source builds without writing any files &gt; and/or be able to create a build dir for itself Being able to bail and cleanup like this would require some work and would be need to become a component of the project() call I expect. &gt; can we have better control over command line options &gt; i.e. let me write cmake --use-tool if I have an option(use-tool ON) &gt; in my CMakeLists This would require CMake to double parse the project to grab all the `option` commands and would have problems with dependent options. Maybe a proposal where you had a side by side file that had high level options, and that would be something talking about &gt; toolchain files that include other files to keep them DRY I thought you could use `include` in a toolchain file. &gt; select project specific toolchains from main CMakeLists You didn't hear this from me, but you should try before the initial `project` call. &gt; support for find_package to find the right binaries for each toolchain I defer to /u/chuckatkins about this as he has done lots of work around find_* and toolchains. 
&gt;Is there any chance that you could introduce something like pre-defined warning-levels that result in somewhat remotely comparable warning-levels across compilers without having to manually set them each time. That's a relatively straight forward concept for gcc, clang, and msvc, but it doesn't really map to the dozens of other compilers that CMake supports (PGI, Cray, IBM XL, etc...). Also considering how the warning level maps to other languages... You get the idea. While it's doable, it's an incredibly subjective measure that means something different for every supported compiler. The things that get compiler abstractions are typically more objective, i.e. there's an explicit way to do it and it really only means one thing. Take C++ language standards, for instance. Saying you want to enable C++17 with no extensions isn't really up for interpretation as to what you mean by that, so a simple: `set(CMAKE_CXX_STNDARD 17)` `set(CMAKE_CXX_EXTENSIONS OFF)` Will set the appropriate flags on pretty much every compiler CMake supports (`-h std=c++17` on Cray compilers for instance). Saying that you want to enable "warning level 3" is abiguous at best.
Haha, I was just wondering what the reactions would be over here. Ofc its pretty trivial.. ;)
I 100% agree that the current process is too complex. I believe it was /u/steveire that did the initial work on CMakePackageConfigHelpers and the related documentation ( https://cmake.org/cmake/help/v3.8/manual/cmake-packages.7.html#creating-packages ) which really helped improve exporting. I could see people coming back and incrementally improving this work to make the process easier. If people have explicit ideas on how to improve `CMakePackageConfigHelpers` lets talk about it. 
But we need it as a standalone binary operator if we're going to defeat the Empire.
The projects that I generally work on use 'Ninja' on the packaging machines for faster build times. So I sidestep the issue :(
`CUDA` as a language in 3.9+ (for visual studio) is fantastic, **thank you**. I have two simple questions: 1. Is `CUDA_ADD_CUBLAS_TO_TARGET` still the "modern" way, or does `CUDA` as a full-fledged language enable something different? 2. If not, would it be reasonable to make it some kind of compile feature? There are a number of these kinds of libraries that could be worked on separately. E.g., `CMAKE_CUDA_KNOWN_FEATURES` would get something like `cuda_cublas` and `cuda_npp` features?
&gt;can CMake bail on in source builds without writing any files I would fully support this. It has created no end of problems on many of the projects I work on for users and developers alike. &gt;I thought you could use include in a toolchain file. You can. I do it all the time. The toolchain files are just a CMake script that ends up getting evaluated at a very specific time before the CMakeLists.txt. As such, you have the full facilities of the CMake language available to you (in scripting mode anyways, not "project" mode), including `include`. &gt;can we have better toolchain file handling? I'd like to hear more about this. At first glance, it seems to me like most of the items you listed work as is so I assume you've got some use cases in mine that don't. Could you try to elaborate on the features you've listed some?
While the target-based approach offers many advantages there are a lot of projects that have gotten along fine without it for many years and don’t necessarily need to change. So long as the older approaches are in widespread use we shouldn’t deprecate them without good reason. - Brad King I personally agree with Brad. Personally, I don’t agree with the presumption that all CMake users would be willing and able to dedicate the resources needed to entirely re-write their build systems if we deprecated all those commands.
Has anyone ever suggested providing a SpinLock implementation? Executors... meh
That's a TIE Interceptor or TIE Advanced. A TIE Bomber would be {oo}.
Why is pkg-config still a second-class citizen in CMake? You can't use the new dependency style (`Foobar::Foobar`) with them but instead need to use the ugly `target_link_libraries` et al commands. Or at least the documentation does not even hint at the fact that this should be possible. Also, all package finders should try to find the flags using pkg-config first and only if that fails try to detect thing on their own. At least in distros the `.pc` files are the source of canonical truth and I have heard several complaints from distro maintainers that CMake's custom detection logic quite often fails whereas just using pkg-config would work.
We're obviously biased, but we think CMake should be used whenever you begin a new C/C++ project. When using CMake, you describe your build in a single text format. This allows you to generate native build files for a variety of platforms and environments.
 1. You can now just do `target _link_libraries(&lt;&gt; PRIVATE cublas)`. If the library has device symbols you need linked you have to use CMake 3.12 as we just fixed that bug. 2. We have talked about adding targets for all the cuda libraries ( https://gitlab.kitware.com/cmake/cmake/issues/17816 )
Basically, it will reorder .text segment, right? What about .data and/or .rodata?
All macros should be rendered by your IDE in Comic Sans. They are that bad.
Timely question. The CMake development team is currently talking about how to better support pkg-config and import targets. I am not part of that discussion as it is outside my area of expertise, but I recommend you join the discussion. - https://www.mail-archive.com/cmake-developers@cmake.org/msg18558.html 
I hear you on the impostor syndrome. One idea: it could follow clang-format's design and have reasonable defaults while letting a .cmake\_format file override to custom settings.
Z95 Headhunter mode?
excellent, thank you!
I made it shorter. `struct X { X operator()() { return {}; } };`
They don't have any special caveats.
`int get_rand () { return 4; }`
Why not use a hash of __FILE__, __LINE__, __DATE__, __TIME__, and __COUNTER__ if it's available (which it should be in most or all compilers)?
If you or a third party compiles code into a static library, and you link it into your application, the compiler will have no opportunity to optimize that section of code in the context of your application. Since this disassembles all instructions in the binary, and rebuilds its control flow graph, it can rearrange blocks of instructions according to the profile, regardless of whether they came from a user's code, a static library, handwritten assembly code etc. You could implement something like this in an ordinary compiler-linker toolchain, but no such implementation exists. It makes more sense as a separate, post-link step, because it means you're actually, directly optimizing the binary you profiled, and you don't have to wait for a full recompilation to optimize your binary for layout.
I created [the Meson build system](https://meson.build) years ago and one of the reasons for it was that CMake's dependency system was not great. I don't have much to add on the discussion apart from "copy what Meson does exactly, it's the best solution we (including all the other people who have contributed) have come up with".
&gt;Did you ever fix the buffering problem with parallel builds? The Ninja generator works awesome for this! It's designed to be a highly parallel build, tends to produce faster build times than Makefiles, and you still get the nice summarized output.
You can to some extend. There's the `IMPORTED_TARGET` parameter for the tools in `FindPkgConfig` to achieve precisely that. It could possibly be documented more clearly but it's doable. What is currently not possible is to use `find_package` to utilize `pkg-config` automatically.
Yeah, absolutely - the open-source community backing is a fantastic thing to have for a software that so many companies depend on. Thanks!
To expand on /u/zackgalbreath answer here my lower level thoughts. 1. The Ninja generator really depends on the state of ninja. To get FORTRAN modules to work properly we have to rely on a fork of ninja that adds the ability to re-scan/build? the graph while executing it. So for C++ modules we will need to get this mainlined for proper support. I think this is a place we should talk with the meson team as I believe they require ninja to. 2. The makefile generator will need some serious engineering effort. I would need to look at my notes from the last time we internally talked about this. In theory we could re-use the work we did again for FOTRAN but IIRC that won't be optimal at all for C++ 3. MSVC / Xcode . Should be `easy` as we just push all the hard work onto the IDE
So that's a no.
&gt;Did you ever fix that bug I filed where executability wasn't checked when searching PATH, breaking disabled transparent wrappers? If it's still a problem, do you have an open Merge Request for it? I'd be happy to review it if you do :-)
I reckon modules will also need to be finished before it's really possible to plan that. Right now, there are some concurrent proposals for C++20 and it's not really clear in what form modules will become part of C++. I don't think spending much time to get a TS to work makes sense.
How exciting! How exciting!
Oh boy.
&gt; As far as I know, that is not possible if modules can export macros. The merged proposal places enough restrictions on the import preamble to make this possible.
It'd be awesome! Submissions / merge requests are welcome! :-)
No xkcd link? https://xkcd.com/221/
Why would you be opposed to `file(GLOB ...)` if cmake will rerun if the glob changes? I thought that was the problem with it.
Why does CMake use its own language instead of being based on an existing scripting language such as Python, etc.?
Only tangentially related but what's the progress with [std::randint](https://en.cppreference.com/w/cpp/experimental/randint)
I haven't checked to see if the Windows binaries have been digitally signed by Kitware Inc. Signed binaries are more trustworthy :) This requires Kitware to purchase Authenticode certificates. There used to be companies that would give free certificates to opensource projects...
if appropriate
Interestingly, most of the updates to the compiler and find modules end up coming from downstream projects that address a shortcoming, and push the change back upstream. This frequently happens with Find\*.cmake modules. And while there are some great CMake contributors outside of Kitware, even none of us here are full time CMake developers and work in varying capacities on other projects. So often CMake features end up getting developed to solve a particular problem that a specific project that one of us has, but that would be super useful to the broader user base. If it's something substantial that will take a major development effort then we need to try to find either a customer willing to pay for it or an external contributor wanting to work on it (cmake server mode, for instance). That didn't really answer your question but it's related as to how we end up working on the different aspects of CMake.
What do you think about buck as a build system for C++?
Where does the phrase "Tony Table" come from?
Have you guys used Hunter and what are your thoughts on it? (I wish it was part of the core language/system) The main pain-point Hunter fixes is namespacing project targets. Is there a reason namespaces don't exist in default CMake? Without them calling add_subdirectory() on an external project is playing with fire (longer explanation here: https://geokon-gh.github.io/hunterintro.html)
To clarify: What's not working right now is `clang++` building against the MSVC backend, i.e. when you install the LLVM toolset and don't want to use `clang-cl` - using Clang with MinGW works fine. The code generated by `clang++` targeting MSVC and `clang-cl` is the same afaik.
I'd like to know when, if ever, CMake plans to support c++ modules from the modules-ts
I am not aware of anyone taking the work of dutow and getting it ready for merging into CMake. Fundamentally we want this, but since hasn't become a blocking issue for a CMake developer the effort has died off. Large features like this need a champion ( internal or external ) and without they generally get pushed to the back burner.
When using CTEST with `--test-load` a few days ago, I observed that it's skips test entirely if the load is to much. However, most uses of load averaging (eg., gnumake's `-l`) don't prevent at least one thread from running. It simply doesn't start new threads if the load is too high or it's running max jobs. Since the intent to run at least one job at a time is implied by the execution of CTEST itself, could you consider changing the behavior of `--test-load` to ensure the the tests at least get run?
What sort of changes are you hoping for?
We recently switched compilers at work, and included in there was two or three of that *exact* error. (We basically simultaneously upgraded MSVC, switched from GCC to Clang on non-Windows, changed from libstdc++ to libc++, and changed from compiling almost all as C++98 to C++17, so I'm not sure which of those did it. :-))
How do you feel about adding support for custom commands and custom targets to depend on **install** target? It is useful for CTest targets (run integration test on installed project) and run_the_program target (**ninja run** that compiles, installs and executes the project).
They were invented (or at least named) by Tony van Eerd (the guy whose Github this is on).
CMake has some basic linter abilities ( '--warn-uninitialized' and '--warn-unused-vars' ). I am not 100% certain that the current internal parsers of CMake make it usable as linter with the same quality as something of clang-tidy. It might be easier for an external linting parser would have a better time using the CMake formal language specification ( https://cmake.org/cmake/help/v3.8/manual/cmake-language.7.html ). Though this might have problems depending on what you want to lint ( generator expression evaluation? ) 
Thanks for mentioning - he made the talk entertaining and covered the topic nicely.
Where should i start learning cmake? Just the docs or are there any other good resources? Thanks for the answer btw
I love hearing this. Everyone should learn from Python's mistake.
Any opinion on the CMakeLists.cmake rather than .txt question? I would also really like that.
That certainly sounds like a bug! ctest should always run the tests specified, not skip them based on load. Please file a bug report so we can work on and track the issue.
Please do not add support for Lua, thank you. And no other script languages ever. 
 I have looked at the documentation of Meson and was very impressed with how everything is organized. I think that CMake should have this kind of using documentation in addition to the API reference docs. I also started to look at the Build 2 source code, as I was curious about how it is handling globing and modules. I have not tried to use Meson or Build 2 for a project, however. It is really hard to find time to try out all the different C++ build and package solutions. Some of this is also domain focused, as most of my programming is HPC focused I gravitate towards products used by that domain. 
Please don't post random wikipedia C++ links. If we want to see them, we know where to go.
Hello, thanks for your contribution so far. One of the biggest problems that we face is that all the modules in FindXXX.cmake do not share the same quality and in different Linux distribution behave differently. The classic issue that we face is the need for 32bit libs in 64bit system and based one the system some libs cannot be found by the related Find. Any thoughts on that. 
So you think it's cute to waste people's time so you can see their reaction?
I think they do mean plain, as in simple or ordinary.
I know of a code base (which shall go unnamed) that has a global `#define abstract = 0` and `#define interface class`. This is why we can't have nice things.
Thanks for the suggestions. The value of the macros will not change inside a function(templated or not). const char* abc() {return __LINE__;} This function returns the same result no matter where you call it.
I don't know if that's the solution, but the obvious worst problem with CMake is the documentation. It's written so that you can understand it only if you know if you know how it works already. 
My first job had this build system where our a group of engineers did enough work where the makefiles that developers had to (rarely) touch were stupidly simple to use. My current job has it nice where we will over ever have to worry about building in Windows, for Windows, and never anything else due to the nature of the work. So we let Visual Studio handle all of the building for us. That said, I never had to use cmake. What am I missing?
I can see why you would want this. I think you are going to run into problems with the global install target and how the is implemented inside CMake. But hopefully I am wrong. It seems that what you really want is support for &lt;build&gt;, &lt;install&gt;, &lt;test&gt; on a per target basis. I honestly don't know if this is technically possible. Either way you should open an issue/feature request on CMake, this might be easier than I expect.
I think it'd be great; I can appreciate the consistency it would provide. Unfortunately, for most developers it's not really much of a pain point so there's not a substantial force function to develop, iterate on, and carry the feature through. 
Being merged.
Hi, thanks for this. First of all, I've been trying to figure out how to handle large recursive dependency trees using "modern cmake" recently. I think I figure it out, and I posted it as an [answer on stackoverflow](https://stackoverflow.com/a/50929353/2125915](https://stackoverflow.com/a/50929353/2125915). Is it correct? Is there a better way to do it?
I will personally advocate for any MR that adds support for CMakeLists.cmake as an entry point. A quick scan of CMake leads me to believe you would need to change: - cmake::SetDirectoriesFromFile - cmake::DoPreConfigureChecks - cmake::GetSystemInformation We would want to prefer `.txt` over `.cmake` as that would not break backwards compatibility. 
In all honesty though, yes it would be great to have but there are quite a few hurdles to making this happen.
The "C" in CMake stands for "cross-platform", so it follows that a primary benefit of using CMake is its ability to generate native build files for a variety of platforms. You can imagine that trying to keep handwritten Makefiles, XCode projects, Visual Studio solutions, etc. all in sync. is a challenging and error-prone task. So instead you can write your build system in one place (CMake) and get support for these myriad platforms for free. Even in the case where you're only targeting a single platform, CMake has some nice features: install rules, unit testing, static and dynamic analysis, code coverage, system introspection, and finding 3rd party dependencies just to name a few. Another benefit of using CMake for your build system is that allows you to offland some platform-specific implementation details onto us (the CMake team). You don't have to worry about updating your project files when a new version of Visual Studio comes out because that type of knowledge is built into CMake. I would also argue that CMakeLists.txt files are easier to read, write, and maintain that handwritten Makefiles. Of course that's subjective and partially dependent on what you're already experienced with. Your motivation is correct though that in the best case you aren't modifying your build system very frequently.
I gotcha, I might look into it then in the next couple of days.
What's your biggest regret when int comes to CMake that you are now stuck with because of legacy compatibility reasons?
So I have proposed [MR 2155](https://gitlab.kitware.com/cmake/cmake/merge_requests/2155) that changes the way `option` works so that it doesn't create cache entires or clear any existing normal variable with the same time. This will help with the keeping the cache as small as possible. I know that other CMake developers have thought about how to change the behavior of adding cache entries which modifies/clears normal variables. I want to see this work happen sooner than later. I don't think that even long term CMake could handle nested caches where each project has a separate database as that would break some fundamental presumptions of CMake.
I like that 3.12 is offering some of the functionality. I use GLOB and I don't mind rerunning CMake, because it's still a lot faster in my projects than manually adding new files. To be honest, this was the reason why I chose to use CMake, because I was sick of manually editing makefiles.
The point od using something like Lua, isn't to be compatible with existing scripting. Its to avoid weirdness that custom build languages like CMake get. It would have been fine to peg at some version of Lua and keep that forever.
`__COUNTER__` will. While it's not standard, I don't think any modern compilers don't support it.
If I'm not mistaken, people use CMake because it outputs different project files for different IDEs, and these project files must be machine-generated every time the cmake project file is modified. Wouldn't it make more sense to just write a standard makefile following certain conventions and teach the IDE how to read that makefile?
Chiming in, especially thanks to everyone who contributed to making this happen: &gt; The release cycle for 3.12 kicked off last week. The major improvement in 3.12 is to allow object libraries to be used transitively. 
This is not as easy to implement as it sounds though. For projects only using C++ it's sufficient to add a flag and achieve linkage - for projects using mixed languages this is very difficult to implement on the other hand. CMake needs to learn the libraries linked by the C/C++/Fortran/... compiler implicitly to know how to link a library built with one language from the linker of the other one. This step takes place when a language is enabled for the first time. By permitting this to be a property, it would be necessary to learn the different compiler libraries for all possible linkage modes and account for them separately.
https://godbolt.org/g/vao5F4
Personally? I think CMake should have competitors. A community the sizeof(C++) has factions that have different needs from a build system. Having multiple build systems to choose from allows projects to use the one that best suites their taste. Now this obviously goes against the ideal of C++ having a build system as part of the standard. If/When C++ gets a build system as part of the standard, I 100% recommend using that whenever possible. But not all projects will be able to switch to that once introduced, and having other options will be necessary for them.
Didn't know about the sanitizer options, thanks a lot! Is it possible to enable them programmatically via CMake? Anyway, I will look into the parser code, the first step would be to provide some sort of API similar to libclang. Maybe some sort of API to specify AST checkers would do the trick. Thanks for the reply!
I am totally out of the loop on the work that is happening between the CMake developers and IDE teams on code completion information. I can't remember if people are thinking about using `cmake-server` for that work or looking at the other code completion API's. 
Yeah, you have to handle that differently. https://godbolt.org/g/si5J8b Or however you want to do it. __COUNTER__ just needs to be uniquely expressed every time, otherwise it will just be embedded into your function.
I also wanted to ask a similar question. Assuming I have a large dependency tree, and all of the nodes on the tree are using modern cmake, my dream is to specify only the direct dependencies of each project as a concise list of addresses to git repositories, and have cmake recursively build those dependencies for me, *automatically ignoring duplicates*, and link its targets (including transitive link, include, compile flags) all in just a few lines of cmake code. I'd also like for it to give the same safety guarantees that I'd get by manually building and installing the dependencies and doing a `find_package()` + `target_link_libraries()`. Is this ever going to be possible? Do you share this dream? Clearly ExternalProject is never going to work for this, but is the new [Fetch Content](https://cmake.org/cmake/help/v3.11/module/FetchContent.html#module:FetchContent) module aiming to address it? Why is the syntax so darn verbose?
&gt; Is it possible to enable them programmatically via CMake Not that I am aware of. You should file a feature request for this since that sounds totally reasonable to me.
https://godbolt.org/g/g1Q4gk This is the code I stole from [C. Esch](https://stackoverflow.com/questions/6166337/does-c-support-compile-time-counters). Only works on MSVC for some reason. The good thing about having `constexpr int counter()` as a templated function instead of a macro is that, Now I can have the counter as a default templated parameter. template&lt;bool ABC = counter()&amp;1&gt; void print() { if counstepr(ABC)printf("yes"); else printf("no"); } int main() { print();//prints yes print();//prints no }
Yet another application of black magic from [cwg issue 2118](http://b.atch.se/posts/non-constant-constant-expressions/): https://wandbox.org/permlink/Mr3jN4d4112zNiXy I forgot how to make counter work, so no real entity tracking, but I think it's doable with deduction guides. Not sure if there is some fundamental issue with making destructors templatable (wasn't there a paper about it?). So, I don't really see why it would be impossible to implement on the compiler side.
TLDR: Is it a goal to eventually do dependency management well from within cmake? Assuming I have a large dependency tree, and all of the nodes on the tree are using modern cmake, my dream is to specify only the direct dependencies of each project as a concise list of addresses to git repositories, and have cmake recursively build those dependencies for me, *automatically ignoring duplicates*, and link its targets (including transitive link, include, compile flags) all in just a few lines of cmake code. I'd also like for it to give the same safety guarantees that I'd get by manually building and installing the dependencies and doing a `find_package()` + `target_link_libraries()`. Is this ever going to be possible? Do you share this dream? I think it's obvious that ExternalProject is never going to work for this in a scalable way, but is the new [Fetch Content](https://cmake.org/cmake/help/v3.11/module/FetchContent.html#module:FetchContent) module aiming to address it? Why is the syntax so darn verbose?
Wrt. threads, I meant that std::thread doesn't actually construct a thread, it allocates one from a pool (so you may get a newly constructed thread, but you may also get one that was constructed some time ago). In the last case, TLS won't be constructed, you'll get the data as it was when the thread was last returned to the pool. That may not be what you expect... 
&gt; for projects using mixed languages this is very difficult to implement on the other hand maybe I'm missing something but given foo.cpp and bar.f90 you only need to do for instance g++ foo.o bar.o -static-libstdc++ -static-libgfortran in order to link with both static runtimes. CMake needs to learn about these flags one way or another so I don't see any added complexity with multi-language targets
Original bug: https://cmake.org/Bug/view.php?id=13071 Original patch: https://github.com/o11c/CMake/commit/a7402e8bd3cd9b955d7483769e712938656a2233 Which works, but still isn't really correct. Back then I didn't know about `access`, which simplifies everything (and even works with ACLs, which are used more and more these days). *Unlike* `stat`, the Windows version of `access` doesn't know about executability ... but different windows/unix implementations are definitely a good idea anyway. For the Windows case, it should use `PATHEXT` probably. But I haven't had access to a Windows system for *years*.
Just wanted to pop in and say thanks! Keep up the great work.
Right? The problem is that too many people want too many things out of their build system. Some what to be able to run arbitrary commands against the OS. Others want to have it just build things with no or minimal configuration required.
In general I like CMake my only wish would be to get a fastbuild generator so we could build our code in a distributed manner, no need for distcc+ccache or goma, incredibuild etc..
I haven't had to work on a project that uses buck, and haven't looked at in any real depth. I scanned the very nice documentation when it was released and was impressed with lots of the concepts behind it but never really looked at how they are doing C++ building/linking, incremental builds, or reproducibility. 
Had to be at least one of the Tonys!
Also to reply to the follow-up question of "if you are part of build system team why aren't you looking at other build tools"? You are correct people that working on a project should be aware of what other people are doing in that space. In general I am aware of the concepts that buck delivers on ( reproducibility, non timestamp incremental builds, etc ) and have looked at how other projects have tackled these issue. But most of my work in CMake is focused on other problems ( how can we better support CUDA? ) and that drives what research I do.
almost as bad as a `std::vector&lt;bool&gt;`...
Offtopic: I am a little confused, how os VTK-m related to VTK, what is the difference?
Currently I am doing this to get closer to what I want, but it has its own issues add_custom_target( Install_ COMMAND "${CMAKE_COMMAND}" --build . --target install WORKING_DIRECTORY "${CMAKE_BINARY_DIR}") add_custom_target(run WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin USES_TERMINAL COMMAND ./exampleApp DEPENDS Install_) Currently I am not able to do **DEPENDS** on **install**. I have opened the issue and tried my best of describing why it might be handy. https://gitlab.kitware.com/cmake/cmake/issues/18110 Thank you for taking your time to do the AMA! And thanks all contributing to CMake, it does makes life much easier. 
So I forwarded this question to some CMake Kitware developers to provide multiple view points. &lt;Robert / Me &gt; High level: I have been exposed to CMake for about 10 years, and in that time, the flaws of CMake have either become background noise or pits that I avoid without thinking about it. One of the reasons that I wanted to do this AMA was to talk to users of CMake and get a sense of what issues other people have. So with that context out of the way, here are some of the flaws of CMake that I want to fix. The first is CMake’s inability to specify link options. This is a long outstanding issue with CMake, and actual Modern CMake has made it more problematic, as while the global ‘FLAGS are sent to the linker, the ‘target_comile_options’ are not. The upside is that CMake 3.13 should have the [MR](https://gitlab.kitware.com/cmake/cmake/merge_requests/2033) that adds in a new ‘ target_link_options’ command. Thanks to external contributor Marc Chevrir. The biggest flaw to me currently in CMake is the way that the normal and cache variables interact when cache variables are added. I strongly believe that adding cache variables should not modify any existing normal variables. This is most often seen with the interaction of adding sub projects that use `option`. For CMake 3.13, I want to have `option` not add a cache variable if an existing variable exists with that name. That way, users can do the following: set(PROJECT_SETTING OFF) add_subdirectory(&lt;sub_project&gt;) #actually have sub_project obey PROJECT_SETTING In addition, the entire existence of the CMake users package repository and the associated ‘export(PACKAGE)’ command has also caused me pain, and I really should bring up if we can deprecate the feature, or am I alone on this? Lastly, as was brought up by other people, I think that the current verbosity and tricks required to get a correct exportable and relocatable find package has to be simplified, if we want more projects to provide config files. This AMA has given me a couple of ideas on small improvements we can make to the process, but I worry I won’t have the time given all the other work I need to do. &lt;/Robert&gt; &lt;Brad King&gt; CMake’s Package Configuration Files, which allow find_package(Foo) to work without a find module, were invented independently prior to the widespread use of pkg-config. They work really well for handling dependencies between two separate CMake projects but are opaque to non-CMake tools. There has been some discussion on the user and developer mailing lists about how pkg-config might be used to address the problem. &lt;/ Brad King &gt; &lt;Chuck Atkins&gt; Ability to consume exported CMake targets from non-cmake projects (related to pkg-config stuff but maybe not) [ Note from Robert: We should see if we can provide wrapper scripts to cmake-server to provide this information ] `find_*` commands live in isolation of one another. It’s entirely possible in a find module now to have a header file from one install and a library from another end up as the output of a single find module because find_file and find_library don’t coordinate. This becomes especially messy when you get parts of a system-installed version and parts of a private version of something. The inability to restrict find_library to only find static or only shared &lt;/Chuck Atkins&gt; 
I would like to work on cleaning up some of the error messages. Sometimes, when CMake cannot find a package, it is not very clear to the user what to do to fix the problem. So, it would be nice to provide a good way for errors to get to CMake users. CMake developers know where to look, but I think we could do a better job helping out the person who just wants to build a package and maybe has never even heard of CMake or used it before. \- Bill Hoffman
LTO+PGO?
&gt; All that said, there are some plans to allow for Lua as an extra language. I'm strongly against that. Yes, there's vocal people on reddit about how "cmake language sucks" but I don't care. That's not an argument. All languages suck. Lua sucks too. Replacing sucky A with sucky B is pointless. Also, I don't want the act of compiling, linking and packaging my software to itself *be* software. Software that would have to be maintained. That would be ridiculous. 
VTK-m is an associated project to VTK which targets accelerators/exascale systems for HPC. So redesigning algorithms so that they work on CUDA and high count CPU designs such as KNL/Thunder X2.
We don't have the ability to change IDEs. We did have the ability to create input to popular IDEs. That said Microsoft Visual Studio now has official support for CMake and are talking directly to a CMake process. QtCreator is using the same mechanism to communicate with CMake as well. So, in some ways that is happening.
Sure, if you use the same compiler for both, here GCC. If you mix g++ and say Intel Fortran, it requires you to link all the Intel Fortran runtime libraries explicitly. In the same way, using a Fortran compiler to link executables requires you to pass the C++ runtime as libraries.
Do you ever intend to support multi-architecture builds? The most common/consistent blocker to CMake adoption for us is that the amd64 builds need x86 bits to be deployed together (e.g. a 64 bit debugger extension needs the DLL to inject into 32 bit processes), or less commonly the reverse (e.g. the x86_amd64 cross compiler needs the 64 bit CRT compiled alongside the 32 bit compiler binaries).
Indeed, just make one and move along is a golden rule here. It's better to make 'wrong' decision, than no decision at all.
&gt; It's written so that you can understand it only if you know how it works already. To me, that is because the CMake docs currently are mostly a reference manual, while a prose documentation is needed.
This sounds really good! Thanks
Sorry I haven't used hunter, I think the author is contributing some stuff to CMake proper. &gt; Is there a reason namespaces don't exist in default CMake CMake supports '::' namespace syntax or is this something different in hunter?
Oh, right, it's in Library Fundamentals v2. I expect another batch of things from LibFund to be merged into C++20. Not sure if std::randint is among them.
I don't know what you're trying to say.
As far as CMake provided modules I would say that now the bar to adding a find module is higher than it was previously. This is good as it means new find modules will work properly but it creates a tension with providing support outside of config files for users fo CMake. If you know of provided find modules that don't obey [FIND_LIBRARY_USE_LIB32_PATHS](https://cmake.org/cmake/help/v3.8/prop_gbl/FIND_LIBRARY_USE_LIB32_PATHS.html) please report them as issues. 
Holy crap, I had no idea you can do tables like that in markdown! My god!
MSVC (under the settings you're using) is likely creating a separate instantiation of that template every time for some reason, whereas GCC and Clang parse and resolve it once. I would very much like there to be something like `std::constexpr_counter()`. There's actually a number of things I'd very much like, but that's one of them.
In the Microsoft fork of CMake we can find this [Win10MultiPlatform](https://github.com/microsoft/cmake/tree/feature/Win10MultiPlatform) feature. Maybe it could be upstreamed. Please ask your colleagues :) 
Do you want to build the x86 bits and amd64 bits inside the same project, or do you just need to ship an existing x86 lib alongside the amd64 one? 
I agree with /u/mloskot CMake’s documentation is mainly reference API docs. It describes language behavior and features rather than how to use commands. Writing user-level documentation is a constant challenge; personally, I find that writing API-level documentation is significantly easier and therefore that is what I do. Stephen Kelly spearheaded an effort to provide some material under cmake-buildsystem(7). This documentation tries to gently introduce the components of a CMake buildsystem, but it could be improved to cover concepts such as include flags, compile options, etc. 
&gt; As far as I can tell, this isn't even mentioned in &gt; the documentation page It is, in the first paragraph: &gt; Usage requirements from linked library targets will be propagated. Usage requirements of a target’s dependencies affect compilation of its own sources. 
It is always nice to hear that people are using the CUDA features. Thank you for testing them out.
Looks like there was some work done on this ([https://gitlab.kitware.com/cmake/cmake/issues/15294](https://gitlab.kitware.com/cmake/cmake/issues/15294)). It does seem to be stalled, but if it could be made to work it sounds like a good feature for CMake. -Bill Hoffman
I am happy that we have stolen that crown from autotools
&gt; Our reference documentation is pretty strong As a cmake user, I can't agree. Too many times have I looked up a specific cmake built-in function or variable only to be greeted with insufficient information for what I need. Taking a look at the cmake docs right now reveals that some of my previous gripes have been documented in newer doc versions, though I'm definitely not the only one who thinks that cmake's documentation needs a lot of improvement.
Ohh I can see why this stalled out. Changing behavior inside `SystemTools.cxx` is most likely going to have some push back as that is a sub project that is shared among many of other projects. On first glance this change seems reasonable but I expect the way to change this is to add the `FileIsExecutable` method but do the check higher up in CMake where we are calling 'FindProgram'. I expect that this approach will also allow us to have a policy for this change in behavior.
I think the downvotes came because the syntax of Rust differs. It's fairly straightforward to write a single program that is both valid C and valid C++. How does one write a program that is both valid C++ and valid Rust?
I think that my answer to 'doom_Oo7' about the flaws of CMake is a pretty good summary.
What's the recommended approach for building Mac app bundles?
Not a question, but a comment; you're brave for facing the rowdy unwashed masses :)
I'm vocally opposed to a 2d graphics library *in the standard*, however, it may be interesting to see how a 2d lib in modern c++ evolves and is used. Whether that is as a standalone GitHub project or a boost library, I don't think it really matters :)
I think I'm not the only one that feels relieved that 2D Graphics is out of the way. I think many of us wanted C++ to provide more out-of-the-box, but `std::graphics` wasn't quite what I had in mind. I think that boost is a good platform for a library to mature to proper practices and real world use. I feel that `std::graphics` had not a proper direction, mainly because it was not a real product, designed under a proper set of demands. It felt mostly like an academic exercise. 
No, the downvotes are because of the idiotic shoving of rust in everyone's face
I'd be careful with that and it's automatic fixing. I just used it on a project of mine using Qt. It corrupted the headers with it's fixes. At least one fails to compile.
That may very well be a viable option moving forward. Pegging a to a fixed version of any thirdparty library carries a heavy maintenance cost down the road though. It's less of an issue with libraries that cmake uses since we can update those and our internal usage of them but on something as user facing as the language, you could envision the problems a few years from now when Lua has a breaking change and CMake now has to depend exclusively on an old language no longer maintained by anyone. At least with a language maintained by CMake itself, as ugly as it might be, we can continue to simultaneously add new features to it and ensure backwards compatibility with existing CMake code. I feel your pain though. I'd love to be able to have real expressions and function return values to use in if() blocks. The unfamilliar and sometimes wierd language is probably the most often cited issue people have with CMake.
Well, there are two distinguishing features of a boost library. First, it goes through the formal review process. This alone has tremendous value because of the high quality feedback given to the author or authors. Second, by going into Boost it automatically enjoys a wider distribution than it would otherwise get. There are plenty of commercial development environments where the allowed external libraries are curated. Places that accept all of Boost but little else will become potential users of Boost.Graphics.
Can't be that memory safe, after all, in the time it took you to scroll down and make a comment you already forgot that this is a question about C++!
Are there any plans to allow multiple different config at the same time? Context: Like my game engine builds for webGL(via emscripten), android, various desktops, etc. Now my current solution is to create a single build directory for every one of them and then build them all one by one(I have .sh files for that). That gets quite cumbersome though and there is quite a lot of bloat in my directories because of that. I've tried to mess around with recursive targets, but that's just insane. 
Don't know why it wasn't called `is_empty(). Who'd thought it was a good idea?
I like this proposal. Kids and adults like to see something visual when learning, to make stuff intersting. Having a simple graphics library would be a nice feature in a library like boost which generally can be installed on the system (from package manager). Otherwise, expecting someone just learning C++ to find and download and install a c++ library of his own can be a hard task. 
Thank you for the CUDA bindings my dude
I'll see what I can do if I get bored during the holidays then!
MSVC is the one that aggressively applied `[[nodiscard]]` to the STL, in C++17 mode.
&gt;it would be nice if someone could write a post-mortem Well, I don't think they've actually conceded yet. They may feel that they're just a few years behind schedule. :) Perhaps it's a situation where it's sort of easy to not concede. I mean, they are making some progress. The checkers now seem to be, as the other commenter said, a useful linter tool at least, compared to the (not to be mean, but imo) train wreck they were not too long ago. And there's also the fact that the original stated goal is non-specific enough that it's hard to argue that it won't be achieved at some point. The original goal was to detect all (memory) unsafe code with a "10&amp;#37; false positive rate". That 10&amp;#37; figure is of course arbitrary and it's not really clear what code base it would be measured against. So with no real restrictions on the number or nature of false positives, the checkers should eventually be able to enforce a safe subset of some kind. The issue is whether that subset will be useful/practical/acceptable. My assertion is that if they ever do achieve an "acceptable" safe C++ subset, that subset will fundamentally resemble the SaferCPlusPlus subset much more than the one implied by the current Core Guidelines.
#define yesexcept(x) noexcept(!(x))
OpenRCT2 has `#define interface struct`, but it's days are counted. We are removing those things, at least some of them to get rid of `common.h`
That was my best guess but my memory was a bit hazy. Thanks for your work, and for the help with the bug catching! :-)
I've pushed hard to more committee proposals to have before/after tables, as they clearly explain proposals better than a bunch of prose. After all, C++ proposals are about changing the language or library in ways that hopefully make programming better. So show how it does. Eventually a few committee members started calling them Tony Tables. (I blame David Sankel in particular. I think he started it.) At this point they are "unofficial" requirements for at least library proposals. If you don't have a table, someone will ask for one. 
The idea would be that you would define it in a totally subjective way on a best-effort-basis. If a compiler doesn't offer substantial warnings, then the higher levels would be equal to the lower ones. This is the kind of thing that most developers using strange compilers should be aware of. But for the 99%+ who use something somewhat common, it would be of huge value. I don't use a build-system because I want it to exactly mirror what the compiler does; if that is what I wanted, I'd just call the compiler directly. I want one that abstracts these details away from me, picks sensible defaults and makes the 95%-case easy.
Hm, It's certainly not short and could be better, but I wouldn't call loooooong. Especially compared to the time required to compile the whole thing =) I remember it taking 2-3 minutes on a cold run, though I did it a few months ago so I could misremember. Consequent reruns (when I add new plugin or change compile options), however, are rather fast. Like 1 minute at most. A fraction of the overall compilation time considering I rerun cmake much less frequently than recompile llvm.
What it actually should do? Draw vertex graphic? Manipulate bitmap? Create window? Cairo-like or SDL-like?
Well that's the great thing about a Boost library. These questions would be answered in the review process. And there can be more than one graphics library.
CMake own documentation is so limited that without this repository: https://github.com/Akagi201/learning-cmake and this [book](https://github.com/Akagi201/learning-cmake/blob/master/docs/mastering-cmake.pdf) in particular, I would not be using CMake at all. However, with this repository, it is good enough and I am happy using CMake. So, for me the issue is that the only **good** documentation is either proprietary and expensive to purchase, or very hard to find.
They are part of the same project. The debugger extension DLL and the "helper that gets into 32 bit debuggee" DLL are part of the same component, one doesn't make sense without the other.
Agreed. Here's an example https://cmake.org/cmake/help/v3.0/command/set_directory_properties.html. This says absolutely nothing about what the properties are. no links to any relevant information on how cmake uses them. You're just supposed to make a lot of assumptions about cmake internals. You're also supposed to understand cmake cacheing and clean targets.
Please do so. I am only asking about memory safety in C++. Some people have mentioned Rust, but that is not what my question is asking about. So thanks for mentioning it but I'd rather talk about C++ here.
So it is. I've read that page several times, but it never stood out. It kind of assumes users are smart enough to figure out what "usage requirements" are and click on the link. It'd be nice, if it was expanded slightly to "usage requirements (such as ...)", and also if [this section](https://cmake.org/cmake/help/latest/command/target_link_libraries.html#libraries-for-both-a-target-and-its-dependents) mentioned the other variables that also get propagated in addition to the link flags, ideally with some example code that also mentions `find_package()` somewhere, since these two commands are kind of intricately linked. I think the ultimate solution though, is to have a new command which is self-documenting, and encourages modern cmake. Something that looks like this would be far more intuitive IMHO: ``` add_library(foo ...) find_package(libbar REQUIRED) # potentially header only library with some target mybar target_add_dependency(foo libbar::mybar) # package_name::target_name ```
Nice!
Can you apply those anywhere you want in the library? Or does the standard specify where? I don't see them in cppreference for example.
Is there anything that would fit the bill, which we could take a look at?
Makes sense. I find memory safety to be an extremely hard topic, in particular for multi threaded code. I've coded an `array`, a `vector`, and a `small_vector` using the linters already and will probably see how things improve next year. I had to disable a lot of the lints for specific lines of the implementation, but then when working on the tests they catched some mistakes which is cool.
A bit late to the party. I am a fan of CMake and am interested in moving a large monolithic Jam build process to CMake. We make use of a lot of generated code though. Currently it seems like https://samthursfield.wordpress.com/2015/11/21/cmake-dependencies-between-targets-and-files-and-custom-commands/ the only/"best" approach to creating a correct chain of targets from the final output to the inputs to the generator. I have a function that takes hundreds of my "model.xml" files, parses some of their content, generates between 8 and 20 files, and I collect all of these as the output to the `add_custom_command` and mapping them to a `add_custom_target` with a unique name using String::MAKE_C_IDENTIFIER based on their filesystem path. I then use this unique target to link it to a high-level target like "model-gen". Then I can link the "model-gen" to some hand-crafted code using the generated code. I guess my issue is, am I SOL for having this done at configuration time? To properly replace the existing build system I need to be able to see both the input files and generated source code in msvc solution files or any other IDE. I have seen https://github.com/Crascit/DownloadProject and now the recent Fetch module; perhaps I need to understand what these are doing to have these steps done during configuration.
Can you make one for class non-type template parameters? 
Congratulations! :D
Well the original 2D Graphics TS proposal has a complete implementation, I believe this is the repository https://github.com/cristianadam/io2d
Yes, because they emit warnings only. We'll try to prepare a paper to add the nodiscards to the library, it's just 3000+ occurrences.
&gt; whereas GCC and Clang parse and resolve it once. If compiled with gcc or clang, `int counter()` will always return 1 &gt; I would very much like there to be something like std::constexpr_counter() That would be awesome, I like this idea.
Good luck. Maybe you should propose using [[deprecated]] in the library too.
&gt; If compiled with gcc or clang, int counter() will always return 1 Yup, that is what I said.
Oh I believe you mean this one: [https://github.com/mikebmcl/P0267\_RefImpl](https://github.com/mikebmcl/P0267_RefImpl)
&gt; std::mt19937{ std::random_device{}() } Why do people default to using the heavier engine if they're not going to seed it properly..? This is a fine way to seed a `std::minstd_rand`; this is not a fine way to seed a `std::mt19937`. Just use `minstd_rand`. ;-]
That one does look more recent
I feel that the status quo around deprecation and removal is acceptable and I don't want to make changes to the Standard that suggest that any vendors are doing anything wrong. MSVC emits deprecation warnings (via `[[deprecated]]`) for C++17-deprecated Library features, and implements removals in C++17 mode. (This will continue for C++20). Other vendors don't emit deprecation warnings and continue to provide removed features, as permitted by Zombie Names. This is perfectly acceptable.
What's your recommended way of dealing with dependency management? Should we use cmake find_package to find (1) third party libraries, as well as (2) git submodule builds, and (3) system-installed vs hosted libraries? Should we just use another package manager to install system libraries and find all packages in a predefined location?
&gt; It'd be really easy to implement. There'd be complications in that the counter would be compilation-unit local, though with LTO you might be able to finagle it to treat the whole program as one CU. That's a pretty badass statement to make. Are you a compiler implementer? &gt; `std::constexpr_cached_index&lt;size_t N, T&gt;(const char (&amp;filename)[N])`. The latter would return a sequential index, and would cache it in the given file atomically. If the file already exists, it would atomically increment it and return the original stored value. I would implement it like so. template&lt;char...cs&gt;struct ConstexprString{}; std::constexpr_cached_index&lt;size_t N, class T, class/* ConstexprString */ FileName&gt;()
I'm very thankful for your efforts -- I think the name is badge of pride.
That _can_ be how `std::async` works when passed `std::launch::async | std::launch::deferred`, but that's absolutely not how `std::thread` works.
&gt; That's a pretty badass statement to make. Are you a compiler implementer? I work a bit on GCC and Clang. A built-in like that isn't hard to implement. Making it non-local to the CU is more of an issue.
IMHO requiring project developers to maintain the complexity async waits, etc. of the build system, and the subsequent complex error messages that must be produced seems gross and complex. I feel like an environment-configured async operation, rather than in a CMakeLists.txt is the right approach. Then you're allowed to let the CI server build differently (probably synchronously?) than developer machines
your argument would be stronger if you didn't link to documentation that's over four years out of date.
Any chance for a cmake debugger? As somebody who works on a couple massive cmake projects I'm now pretty strongly anti-cmake due to the homemade-but-not-full-featured scripting language.
&gt;Is this ever going to be possible? Do you share this dream? I think it's obvious that ExternalProject is never going to work for this in a scalable way, but is the new [Fetch Content](https://cmake.org/cmake/help/v3.11/module/FetchContent.html#module:FetchContent) module aiming to address it? Why is the syntax so darn verbose? As the author of FetchContent, I'll give my personal point of view. I do share that same desire and where I can, I'm working toward making FetchContent provide the ability to incorporate dependencies seamlessly and handle complex dependency hierarchies. I agree the current usage of FetchContent is too verbose. Declaring the dependencies is as minimal as one can make it already, but the population part is more complicated than it needs to be for simple cases. I first wanted to get the basic capabilities out there and see how the community reacted to it before making too many assumptions about how people would use it. The functions provided by FetchContent at the moment provide the lower level building blocks, but I hope to add a higher level function (hopefully in time for CMake 3.13) which makes the population part a single call, which will certainly make it much more concise and friendlier for the average user.
Any suggestion of an API or detailed behavior for this? Remember that when doing a try_compile, you usually wanted to check synchronously if it worked or not at once to use some feature or use a flag. But maybe it could work with a generator expression with something like $&lt;TASK_RESULT:check_header_foo&gt;. Obviously, generation wouldn't start until all tasks are done. 
I’m not sure what modern cmake is exactly, but if it contains commands to avoid you could start with a text scanner for that :)
I see there have been some commits lately but wasn't Boost.GIL the thing from 2003 or so that hasn't really been updated since. Maybe I'm confusing it with something else.
The answer to that is often they don't handle static/dynamic or debug/release. I'm not aware of any that handle debug/release, and static/dynamic is iffy in my experience. The ZLIB module, for example, only supports the dll.
&gt; can we have better toolchain file handling? Documentation on how to actually write those things..
I didn't choose to be born as an abomination!
Yeah... I was hoping to protect the innocent (or guilty?).
Any chance of the ExternalProject having a full configure time counterpart? The download portions got FetchContent, but what I really want is the ability to download *and build* a dependency at configure time, without having to resort to complicated super project things. This is possible currently by using fetchcontent and execute_process, of course, but then you're just reinventing the build logic, and error prone. And before 3.11 you could generate and execute a project using ExternalProject at configure time, which is similarly error prone and reinventing the wheel.
Those older projects, if they happen to be library projects, essentially screw their users hard by not doing the right thing. Integrating the target-based and legacy components is a PITA of astronomical proportions. At work, I maintain about 50 cmake build systems for open source packages that either use obsolete cmake or use autoconf. We may eventually release some of it. The big one is Octave and all of its library dependencies that are needed outside of centos+epel, which currently works without any trouble on centos7, mac os 10.13, and windows 10 with both mingw and msvc 15.7 (no cygwin, lol). We use it mostly for a monolithic ninja build of everything, and gives some hope that perhaps automake will actually die one day. If only the people who use automake were willing to just let go. As much as cmake is a pile of legacy crud now, automake is a Sierpiński pyramid of it in comparison.
Gosh no! A blog post? Blog posts are disposable stuff that quickly turns obsolete. Write something that lives in cmake repository so that it can be kept up-to-date and people could contribute. It can be a blog post, just have it live in actual cmake repo, not anywhere else.
That's why it can't be a blog post. It needs to be a doc page in cmake repo, and it needs to be something that can be easily contributed to.
I used scons. It's slow. Abysmally slow. Impractically slow. Meh.
At some point I was cheering for KDE to use scons... I regret this very much. It was a shameful lack of insight on my part.
Once I decided to just let go of the apprehension, the language was something that took me about two days to understand, including being able to figure out some odd quirks like the `${${}notmacro}`. Cmake BNF syntax is quite concise and very easy to grok. If it's the "most often cited issue" people have with CMake, then it's not because it somehow is hard, but because people genuinely don't want to do the proper job of understanding it and they don't realize it. I mean: the `cmake-langauge` page is how long?
With a blog it can the opinion of me as an individual or Kitware as an entity. Making it part of CMake even the manual carries with it the presumption and weight that this was formed by consensus by at least the CMake core developers. That will take significantly more time than writing an opinion blog post.
&gt; At work, I maintain about 50 cmake build systems for open source packages whose upstreams either use obsolete cmake or use autoconf :(
What sections of the ( very old ) book would you want first ported to the CMake documentation? I look at the book and see everything that is out of date, and can't see the useful bits anymore.
I truly don't understand what's the big deal about a language whose syntax takes 8 pages to explain. It sounds like a bunch of BS to me, frankly said. Cmake is a very small language, yet sufficiently expressive to make it work. I'd say if anything else should be added to cmake, it should be Rexx, not Lua. Rexx seems to have much less spiritual and impedance mismatch, I think.
With the cost of re-running the glob scaling by the number of source files instead of CMakeLists.txt files you can run into situations where checking for a change has significant cost. This becomes an even more noticeable if you are using non-local drives. 
You are 100% correct. This is not as easy as it appears on a first glance. But I am still in the camp that thinks it is possible. I know that some of this information is already captured as part of the compiler detection as we have to record the implicit libraries ( as you mention when linking fortran/c++ ). What we would have to do is test build each language with shared/static/none and have those as separate properties. From that point it is simply the nightmare of figuring out the correct link line when building a mixed language library.
Your example is not necessarily UB, it depends on the implementation and how much memory it reserves. If you had an explicit `reserve()` call before you took the reference of the right size, it would be completely safe. I'd say this would go in the "no raw pointers" rule, since the reference here is acting in the same way.
I answered that here: https://old.reddit.com/r/cpp/comments/8sie4b/i_manage_the_release_cycle_for_cmake_the_build/e0znc7r/
\&gt; Is there a page somewhere that does a comparison to explain reasons why one would prefer xmake to other systems? There is no relevant comparison article. \&gt; I appreciate the reasonable support of wildcard globbing. This is such a critical thing it's shocking to me how few build systems will handle it. And if you don't you're garbage. Wildcard matching is supported, for example: add\_files("src/\*.c") \&gt; Implementing the scanning of the directory tree is a nice *idea* but it looks like it wasn't fully implemented. It only handles one directory level? How does it know which source files contain my unit tests and therefore should be compiled with different flags and the resulting binaries executed as part of the build? I suspect it doesn't know this. This is just an auxiliary feature. It is only used to quickly generate a xmake.lua file.
Can you point me to where in meson I would find that logic?
That is great news. 
well, the C++ ones anyways. In all the other language communities I'm in (Rust, OCaml), the build tools are loved.
If it's on github, a simple search can find the offending repos.
Thanks for your reply. I can handle the verbosity if it's just a temporary thing, but one innate disadvantage with FetchContent that I see is related to the safety part that I touched on in my question: &gt; I'd also like for it to give the same safety guarantees that I'd get by manually building and installing the dependencies and doing a find_package() + target_link_libraries(). Due to the use of `add_subdirectory()` in FetchContent (at least in the docs), there is the possibility for variables and compile options to "leak" from parent to child, or even from child to parent if it uses `PARENT_SCOPE`, which can potentially lead to bugs that would not exist in a manual build. Additionally, I believe the targets from the child will be exported (by default), right? i.e. those targets will pop up in `make install` and `cpack` invocations, which is often not what we want in static libraries. Do you recognize these short-comings of the `add_subdirectory()` approach, and is addressing them a concern of yours? I guess the only way to get around it is to take things a step further and build the dependency tree during the configure step instead of just cloning it.
I guess that's sensible. I'd still probably prefer using the GLOB, since maintaining the file list is also a cost. Since the benefit isn't that large, I'd probably just use the glob
Then have it as a gist, evolve that way, then submit to CMake repo once fleshed out. But please make it something that can be worked on, and of course no, the comments plugin at the bottom of the page isn’t it. 
I think my memory fails me there. I would need to check my makefiles to give you a real answer.
What about this: https://cgold.readthedocs.io/en/latest/
I understand why CMake can't change the language drastically this late in the game. But has any thought been put into a transpiler so a simpler language could be compiled to CMake, or perhaps lua bindings to the underlying commands. I think everyone agrees the language is a time-consuming, unnecessary hurdle. CMake could ultimately replace 90% of makefiles if the language were simple enough.
I also expect they can simplify the API of random numbers. For example, \`std::shuffle\` since C++11 requires a \`UniformRandomBitGenerator\` as the 3rd argument, which is really verbose for regular uses (when I don't need advanced tools for better shuffled random numbers). In the example of [std::shuffle](http://en.cppreference.com/w/cpp/algorithm/random_shuffle) from cppreference, I have to write two extra lines for a random device and then an engine. Make our code simpler and more expressive, plz. ps. Here is a better API in Library Fundamentals v2: [std::experimental::shuffle](http://en.cppreference.com/w/cpp/experimental/shuffle). Hope thay can make it.
What is the right/easiest way to deal with the fact that I have several executables which use many of the same source files? See [https://stackoverflow.com/questions/1388877/how-to-build-object-files-only-once-with-cmake](https://stackoverflow.com/questions/1388877/how-to-build-object-files-only-once-with-cmake) for a very simple example. The answer there (create a library) is not a good option for me since I have a dozen executables (more actually), which share different subsets of 1000's of source files. I don't want to add everything to a library or libraries since then I either build way too much (one big library) for a given target that I'm testing, or I make a library for each possible combination of executables which would be way too many libraries (and too hard to maintain). This is one of the problems keeping me from switching to cmake, and I feel like there must be some simple solution that I haven't been able to find. 
Ah, it was an implementation bug in MSVC! I didn't realize that. Thanks!
I have been in the discussion in gitter but it seems this is stalled. I think this would be the last big feature to make cmake complete to build large projects. I like cmake a lot and thank you for the work you have done :) 
Boost.optional is ~20 years old and yet people seem very happy with its presence in c++17
How can I make the configuration step faster? For 7000 targets with Ninja generator, what should I be expecting?
&gt;All that said, there are some plans to allow for Lua as an extra language. Man, this would be really amazing... CMake's macro language always messes with my head too much, and Lua seems like the perfect fit for a build system. 
There are two models, I think: - Aimed at maximum versatility, and GPU acceleration be a secondary concern at best. This is what Cairo does, although an implementation of the same functionality (but not the same API) exists for NVidia GPUs as the path rendering extension. This type of graphics is excellent for desktop applications. - Aimed at maximum performance, and versatility be damned. This type of graphics is excellent for games and other places where it makes sense to offload drawing to the GPU. *I do not believe these two models should be supported within a single library.* They have different users and conflicting goals. I'll talk about desktop rendering a bit more since I know something about it. Cairo's operations generally consist of two passes: one where a set of data is produced describing the object to be rendered (which is not a good match for a GPU), and another where that data is combined with the data already existing in the target bitmap. This last pass should run just fine on a GPU. One thing that's important to desktop applications, and that Cairo supports well, is creating printed and PDF output. Cairo supports these as different drawing targets that support the same rendering primitives as you would use on the screen, allowing you to reuse screen rendering code for printed output as well. One criticism that could be leveled against Cairo is that it puts too much data into its drawing state. Paths and patterns, at least, should have been separate objects, allowing applications to save those for reuse in future drawing passes. This is something that could be changed comparatively easily, though. I also believe it would be much faster if its inner drawing loops replaced all those pseudo-virtual function calls (it's all function pointers and C) in the most critical loops by templated code (with at least the combining operation and the source pattern as template parameters). 
The back-end in the [recent reference implementation](https://github.com/mikebmcl/P0267_RefImpl/tree/master/P0267_RefImpl/P0267_RefImpl) is cairo. It provides vector-graphics, like in "... Operations in cairo including stroking and filling cubic Bézier splines, transforming and compositing translucent images, and antialiased text rendering ..." Holy Moses, why would one (as a beginner, who seem to be the target user) like to do that, P0267 seems to try and do the same. Luckily, it got shot down.
optional is a rather simple concept though, with a small surface, everyone could easily write their own, standardising that avoids that and therefor seems a good idea.
Probably because Google always shows old cmake documentation. I like your recommendation :)
&gt; I think I'm not the only one that feels relieved that 2D Graphics is out of the way. Nope, P0267 seems to be written/proposed by someone who hasn't even ever attempted to write even a meager pong-clone.
They put it in the final version of the C++98-standard, but because everybody reads the latest draft (as the real thing costs money (WHY?)) nobody knows about it.
I only tend to use it for iterators.
I agree that it would be a better boost library than a standard library, but even as a boost library I have some concerns: Who has the skills to maintain it that has a long track record of committing into boost? After the original authors poop out, do we just fob all the maintenance onto John Maddock? This seems like a huge effort, and the last thing Boost needs is another library that springs to life in a fit of enthusiasm and then dies. Myself, John Maddock, and Paul Bristow have been discussing a way less ambitious project: A boost svg library. If the goal is education, I think an svg library has way more potential to fulfill this goal along with being useful to professionals. Whether or not we have the mojo to bring this to life is the relevant question . . .
Actually, it was written by people that write games for a living.
That's another one that got fixed in 3.10... \*sigh\* Are we supposed to assume that docs from 3.10 apply to 3.0? &gt; You're also supposed to understand cmake cacheing and clean targets. No, you're supposed to nuke the build directory every time you change something other than the source files. /rant
Why do vI as a beginner care about the backend? My guess would be that most other backends are even more powerful. I don't see your point.
&gt; Actually, it was written by people that write games for a living. I was obviously not serious about that, but then the question becomes: "What's the usefulness to add P0267 to the standard"?
Yes, this is active and there is progress to report... in brief, part of the Lifetime rules I showed at CppCon 2015 are now shipping in CppCoreCheck (including diagnosing most of the examples in my talk), and are in the process of being updated to diagnose `string_view` and `span` dangling as well. I plan to give an update in my CppCon talk in September and might blog about it between now and then.
Deprecation doesn't have to mean breaking things. It might just mean outputting a warning and having the same warning in the docs. As a relatively new CMake user, it would really be helpful if the docs could more prominently indicate when there's a newer/better way to do something, what that is, and briefly why it's better. It would be helpful to see the same warnings for in-house-but-created-before-we-knew-what-we-were-doing library builds. If you want to keep your old CMake stuff you could either live with the warning or possibly set some option to ignore them altogether. You'd basically be saying "yeah, this still works but there might be a better way to do this."
&gt; Why do vI as a beginner care about the backend? Agreed, they wouldn't. P0267, roughly speaking, seems to just be wrapping cairo, which as a library undoubtedly has its merits, but it's hardly where [SVG-graphics] you would start to learn to program a small game. &gt; My guess would be that most other backends are even more powerful. Yes, with higher levels of abstraction and possibly written in C++. My point is that cairo as a backend choice directs P0267 into a direction that IMHO is a waste of time (and it seeems thhe majority agreed with that).
Thanks, fixed the link.
That would be Daniel Pfeifer. 
The reason to look up older documentation versions is the minimum cmake version your project supports. Otherwise you're assuming that newer docs apply to older cmake, which may not be true.
&gt;My point is that cairo as a backend choice directs P0267 into a direction that IMHO is a waste of time (and it seeems thhe majority agreed with that). Just to make sure: Is that opinion based on an inspection of the API in it's most recent form or some comments on the internet (I rarely work on graphics so I can't really tell one way or the other) 
Oh, and they did implement asteroids in space with it ;) Does that qualify?
Could you please take a look at another directory there called "coregraphics"? The phrase "The back-end in the recent reference implementation is cairo" contains a factual error. There are two backends at the moment, they are absolutely interchangeable. There is no "the backend" in RefImpl.
Without a clear mission statement about the purpose of that library I don't see the point of putting it in boost if there are already other well established and more powerful libraries out there. With such a clearly articulated purpose I'd very much like to see it.
Again the best trip report. I wonder how he's able to get so much detail from so many different groups.
Shouldn't those fundamental design questions be answered before the review process?
What's wrong within existing libraries out there? What do you expect such a library to make better than the others (Honest question, I've very little experience with graphical applications, but I feel those questions should be answered)
hi, zapcc developer here. The zapcc sources has been updated to include explicit LICENSE reference, for example https://github.com/yrnkrn/zapcc/blob/master/tools/zapcc/Client.cpp
So how would you specify e.g. the build instructions in a toolchain independent manner, without effectively standardizing cmake (or something similar)
The [cairo examples page](https://www.cairographics.org/examples/), IMO, says it all, no need for more comment. FF doesn't even use cairo anymore, and is already 2 rendering engines further.
hi, zapcc developer here. LLVM/clang are very rich in data structures, probably more than 1000. Using files with zapcc mode of operation would requires serialization of all of them. Not saying it's impossible but zapcc is already very complex. 
You can try xmake. [https://github.com/tboox/xmake](https://github.com/tboox/xmake)
You can try Intellij-IDEA/VSCode + xmake to support CUDA.
I scanned the proposed API in the [latest proposal paper](http://open-std.org/JTC1/SC22/WG21/docs/papers/2018/p0267r7.pdf).
That there are more current backends is not very relevant as they idea would be (that's in the paper) that suppliers would provide there own implementation (f.e. Dx on windows instead of OpenGL) targeting various hard-/software combinations.
Thanks, good to know.
I recall the intro video about the GSL (Guideline Support Library) and the idea of a zero-cost typedef for an ownership pointer that would enable a static checker to validate all pointers marked as owning are properly released (no memory leak), and warn about view (i.e. raw) pointers being used in unsafe ways. I recall MS sponsoring this effort and I think clang-tidy was going to be the front-end for this checking. Is this what you have in mind? I am so interested in how this is progressing. I know clang-tidy has some of the core guideline rules implemented, but I do not know if that includes anything in the GSL.
We do seem to be mis-understanding each other on this topic, because I do agree with you on this point 100%. Here's the "Hello World" example from SFML. #include &lt;SFML/Graphics.hpp&gt; int main() { // create the window sf::RenderWindow window(sf::VideoMode(800, 600), "My window"); // run the program as long as the window is open while (window.isOpen()) { // check all the window's events that were triggered since the last iteration of the loop sf::Event event; while (window.pollEvent(event)) { // "close requested" event: we close the window if (event.type == sf::Event::Closed) window.close(); } // clear the window with black color window.clear(sf::Color::Black); // draw everything here... // window.draw(...); // end the current frame window.display(); } return 0; } If you would cut the comments it's very shorty and simple.
Thanks, good to know. I've had the impression that too many opinions (not just on this particular topic)) arest based on "I've read somewhere that..."
Absolutely. My point was that calling P0267 a "cairo wrapper" conveys a wrong information.
Well, formally yes, practically no.
https://en.wikipedia.org/wiki/ANSI_escape_code should get you started. An escape sequence is a special character string which gets interpreted by the terminal. There are certain libraries that build an abstraction based on those, for example ncurses for linux, which basically give you a TUI framework.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8spp5q/what_am_i_looking_for_seperate_console_outputs/e11bhnt/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Since CUDA doesn't support the latest Visual Studio 2017 15.7 compiler, I looked forward to the new CMAKE\_GENERATOR\_TOOLSET options in CMake 3.12. So I installed the 3.12-rc1 and generated a project with -T "version=14.13". Unfortunatly cuda\_compile\_ptx() fails with C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.14.26428\include\yvals.h(301): fatal error C1189: #error: STL1001: Unexpected compiler version, expected MSVC 19.14 or newer. 8&gt;CMake Error at cuda_compile_ptx_1_generated_XXXXXXX.cu.ptx.Debug.cmake:211 (message): I could get it to work by manually adding the 14.13 include directory to CUDA\_NVCC\_FLAGS with set(CUDA_NVCC_FLAGS "\"-IC:/Program Files (x86)/Microsoft Visual Studio/2017/Professional/VC/Tools/MSVC/14.13.26128/include\"") But thats just a hack.
Well, as I said, I don't have experience with GUI applications except some qt (and I played a bit with cinder) so I can't really comment on the technical aspects of the proposal and thus don't really have a strong opinion on wether it should be put into boost or the standard (generally speaking I would have liked to see a simple, standardized, "good-enough" graphics api). However, when reading through this and other threads or blogposts, there are (sometimes non-technical) arguments on both sides that I don't find convincing or don't understand. Those are the posts I tend to comment ob no matter if they are in favor or against.
Bsck then I just added CMake support for the project and moved things around a bit. Now the original proposal has CMake support, another back-end, tests and samples. I should update my branch, and get back at my original goal - add a Qt back-end :)
I think those against take the "it has to be fun for beginners" as a starting point. If you're a beginner, it is (and should be) fun to write a game (as a learning exercise). If the tool given to you by the standard library is a very low-level library dealing with SVG, that is definitely not fun, i.e. it misses the point completely. It's, according to cairo's web-site, a.o. used as a rendering engine in GTK++, LibreOffice and (but that's outdated) FireFox, those pieces of software are far removed from my first pong-clone.
Again, I don't care about Cairo. I care if the proposed library would be easy to use for me or not and if it has a chance to be implemented with a decent efficiency on modern hardware. If you tell me "The proposed API is horrible because XYZ" Then we can have a discussion of see if we have the same opinion about that and how important those points are.
cool! this is great news, I can't wait to watch your talk :) Could you comment on what has to be done to enable these for `span` and `string_view`? Like, do they need new compiler support? Or is it just about adding annotations to their internals? If it's the later, is there a document somewhere explaining how to write your own data structures using these annotations to enable the linting to work?
Meta question: what is the strategic purpose of this AMA? Surely you’re a busy person, and your coworkers have also committed to helping out. Is there a perception that CMake, as a project, has poor public optics? Isn’t being utilized fully? Are you interested in garnering feedback from users? Not trying to be combative at all, genuinely curious. Thanks for doing the AMA.
I recently held a meeting at work, where we talked about the difference between `clear()` and `empty()`. Also that you should prefer `empty()` instead of `size() == 0` and that you don't need to call `clear()` in the constructor...
Imagine a new programmer who has just learnt how to use std::vector and some stuff and now wants to make make a snake game. There is currently no library which is 1. Popular 2. Easy to install 3. Easy to use 1. Boost will make the lib popular 2. Because boost is available as a package in many package managers, it is easy to install. 3. The library was designed to be beginner friendly. 
The docs are mostly reference-level, not tutorials, so they are not good place to start. I'll instead shamelessly plug my posts https://codingnest.com/basic-cmake/ https://codingnest.com/basic-cmake-part-2/
&gt; Stephen Kelly spearheaded an effort to provide some material under cmake-buildsystem(7). This is a great piece indeed. Funny thing is that I have learned about it ~2 weeks ago from Stephen via #cmake channel on cpplang.slack.com. I blame the "bad" man-like naming of files/URLs. Who seeing `cmake-buildsystem.7.html` would have thought that will be such a great tutorial, rather an in-depth description of CMake build system architecture. Small things can have an impact :) 
&gt; There is currently no library which is ... SFML meets all your criteria. It’s popular and easy to use, and is available on apt-get, brew and vcpkg. I don’t particularly want or care for a beginner friendly library in the standard. If it’s too simple, only beginners will use it making it all but pointless, IMO
This is a good book, but to me it shifts the weight towards lower priorities. When I look at it last time, most of the most important parts from the modern CMake point of view (targets, usage requirements, install) were empty, marked as TODO, while the chapter about variables is overwhelmingly total and complete (possibly, even more than the CMake reference manual) 
Dirty flag pattern is not great performance-wise, if you have many objects and the "update" is fast, since you branch on your dirty flag in a tight loop. Sorting or otherwise separating and then acting on homogenous data is better cache-wise. Otherwise, caching in-object means that you have to abandon `const` on a normally non-mutating function (`update`) or to make the member variable `mutable`, but now making your object `const` is a half-lie (threading). Feels to me that this would solve a problem in a "soup-of-objects" type of architecture, which I think is rarely the good architecture (IMO make your systems smart, not your objects).
I've done bat script on windows to call vcvarrall in x86 compile with cmake then call vcvarrall in x64 then compile same project with cmake. Both version use cmake install in same directory to generate our binary references. It's almost ok for CI, but painful/tricky to debug when you need both.
First, thanks for all your work and for the object library stuff in particular which we use a lot. I am going to try the enhancements right away. I am wondering if it now possible to build a shared/static library and an object library in the same output Visual Studio project. At present we end up building all our object files twice once in a library project and once in "object library" project. This is a a waste as the "object library" project seems to be in Visual Studio terms exactly the same as a the static library project. So the put my question another way is it now possible to use the object files built in the Visual Studio static library project as if they are an object library? 
obligatory ~~xkcd~~ abstrusegoose: [https://abstrusegoose.com/strips/pop\_pop\_phys\_phys.png](https://abstrusegoose.com/strips/pop_pop_phys_phys.png)
People wrote several comments about that. Also, you can check other posts from his blog on reddit. E.g. https://www.reddit.com/r/cpp/comments/8huy3u/stditerator_is_deprecated_why_what_it_was_and/
This is not the standard. I agree about standard. This is boost. Also, I did not know of SFML till I was on the C++ slack. My anecdoctal evidence is not a proof but this is consistent with what I have observed among people who are new to C++ programming in my work environment. They will know about boost within first month of learning C++ due to its pervasiness. So, while SFML might be popular within the people who already know about graphics library, boost (in my opinion) will be more popular for people who know nothing about graphics libraries in C++. 
Hate? It's the best tool currently available!
When I wrote FetchContent, I explicitly prohibited the ability to build at configure time because that goes against the way CMake should be used. The configure stage needs to be fast because it can be retriggered automatically when files used by the configure stage are modified. A time consuming configure step is typically a strong point of irritation with developers. I would urge you to reconsider structuring the project as a superbuild instead if you are unable to incorporate the dependency's build as part of the main project's build step using FetchContent. If you are truly set on this path though, you may find some help with ctest's build-and-test mode, but this is unlikely to make you popular with other developers on your team in the long term. ;)
If you can define the commands that create the generated files using `add_custom_command()`, then there should be no need to do all this at configure time (and indeed that would be undesirable, you want the configure stage to be fast). I thought generated files showed up in IDEs but maybe it depends on which IDE. The functionality provided by DownloadProject and FetchContent (the former is the forerunner of the latter) is intended for downloading things during the configure stage, but not for building things at configure time. An example of a situation where you might want to download during configure is to be able to have direct access to targets the downloaded (CMake) project defines by simply incorporating it into the main build using `add_subdirectory()`. There's also a special case which I don't think applies here, but I'll mention it just in case it is of interest. If the external project builds a tool that is used to generate CMakeLists.txt files that you want to incorporate directly into the main build's configure stage, it can be a bit more tricky to do, but the following link gives one way to do it (and we've been using this at my workplace with good success for some time now). https://stackoverflow.com/q/36084785/1938798 It's a pretty specialised corner case, but maybe it's helpful to some.
You raise some valid observations, so let me see if I can clarify them. &gt; Due to the use of add_subdirectory() in FetchContent (at least in the docs), there is the possibility for variables and compile options to "leak" from parent to child, or even from child to parent if it uses PARENT_SCOPE, which can potentially lead to bugs that would not exist in a manual build. In most cases, you _want_ the parent variables to affect the child. You want the child to be built with the same settings as the parent so that they are consistent. If you need to have the child built with different settings, the parent is in control of that and can set the relevant variables first before it does the population step. If you want to prevent contamination in the other direction, you can put the population in its own subdirectory or do it from within a function. Either option will prevent any `PARENT_SCOPE` from going any further out. At my current company, we have complex project hierarchies with shared dependencies. We put each dependency population in its own subdirectory to keep it isolated. Some of the dependencies require setting some variables before calling `add_subdirectory()`, often to overcome problems with projects that had not considered being incorporated into other projects directly like that. This has worked well for us in practice. &gt; Additionally, I believe the targets from the child will be exported (by default), right? i.e. those targets will pop up in make install and cpack invocations, which is often not what we want in static libraries. This is a common problem, but you can set the [`EXCLUDE_FROM_ALL`](https://cmake.org/cmake/help/latest/prop_dir/EXCLUDE_FROM_ALL.html) directory property to prevent them being built unless something else needs them. I believe this also prevents them from being installed by default too, but you'd need to check. There is a similar option for `add_subdirectory()` but that has a more checkered history with problems for the Xcode generator (I think those are fixed in recent CMake versions though). You can also set the [`CMAKE_INSTALL_DEFAULT_COMPONENT_NAME`](https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_DEFAULT_COMPONENT_NAME.html) variable before calling `add_subdirectory()` if the child project doesn't use components for its install rule. Then you can simply exclude that component in your main project's packaging, etc.
For what is my understanding, modern CMake is a set of vague best practices like relying on targets for everything (no more variables, even for dependencies via Find* or compiler settings) and so on. I don't think that naive text processing would be enough, probably a proper AST would be needed to check that stuff.
I didn't think that could be right, so I looked up the standard, and you are correct: `std::thread` is permitted to adopt a currently unreferenced "spare" thread, and any thread local storage is *not* reinitialised. I certainly have not been writing my code to handle that these past ten years. I suspect neither have most people. Thanks for pointing out to me my mistake. Now I must go retrofit all my code ... arse ...
...what about making ASYNC the default ? I'm pretty sure it would work 99.99995% of the time.
well, yes, and yet it is already fairly useful as is. 
Well main is where all your code has to go, so looping on main will always rerun all your code. However nothing is stopping you from putting the game logic into a different function and looping that function instead.
&gt; There needs to be a simple 2d graphics library in a well-known library collection set. And then we will get even more complaint posts from the "muh boost is bloated" crowd. 
I think SVG is still too ambitious. I am strongly in favour of a C++ graphics library which implements exactly the feature set of HTML canvas, and not a touch more. In favour of such a proposal, HTML canvas is well specified by W3C standards. It specifies a minimum level of input, text rendering, color and sprites. As the C++ standard already does, one can refer to external standards documents in the standardese, so in any proposed standardised C++ graphics support, one says (effectively) "whatever W3C HTML canvas standard says on this point" etc. What would be particularly cool in such a proposal is that a live web browser renderer backend is completely feasible via WebSockets. One choice of backend would send the calls via a mini localhost web browser (probably implemented using Boost.Beast) to be issued by Javascript in the web page. It would not be fast, but plenty good enough. Other backends could be a lot quicker. That's been my recommendation for some time regarding standardising C++ Graphics. Even HTML canvas support would be a lot of work, but much less than SVG support.
You are not allowed to call main from your code. : See [[basic.start.main]/3](http://eel.is/c++draft/basic.start.main#3): &gt; The function main shall not be used within a program.
&gt; In most cases, you want the parent variables to affect the child. I'm not totally convinced that this is true. If I'm building my project `Foo` and it depends on a completely unrelated library like `protobuf`, then I probably don't want my project's variables to affect the protobuf build. If both libraries happen to define some variable `MAGIC_NUMBER` and they set it differently then it could lead to some serious headscratcher bugs. If I do need to modify the protobuf build to suit Foo's purposes then I would like to do that by explicitly passing it some cmake variables at it's configure step (like you can do with ExternalProject's CMAKE_ARGS). &gt; If you want to prevent contamination in the other direction, you can put the population in its own subdirectory or do it from within a function. Either option will prevent any PARENT_SCOPE from going any further out. At my current company, we have complex project hierarchies with shared dependencies. We put each dependency population in its own subdirectory to keep it isolated. Sorry I don't really understand, are you saying that you have a subdirectory for each dependency, which contains only a CMakeLists.txt that performs the population step (and possibly setting some variables)? This doesn't sound super appealing, it'd be nice if the library could take care of it for us.
Didn't think of that at all, thanks
Use https://www.reddit.com/r/cpp_questions/
Yeah, but the default way of ending up at old pages is * google XYZ * click link that says XYZ from cmake.org * look at documentation for oooold version of CMake, because that's what google gave you What the poster is arguing for is for a banner that says "this documentation is not documentation for current version, go here instead"
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
That's easily countered: Boost never was meant not to be bloated. That is pretty much its function right now: take in all cool stuff that shouldn't be in the standard, because the standard would be (more) bloated. Or because the stuff is so new that the standard will need years to catch up.
So we should get rid of every other programming language out there too? Better to just have C++.
This is the kind of thing that made me wrap the STL. There are just too many pitfalls. Erase-remove idiom? Fuck that.
&gt;The biggest flaw to me currently in CMake is the way that the normal and cache variables interact when cache variables are added. I strongly believe that adding cache variables should not modify any existing normal variables. This is most often seen with the interaction of adding sub projects that use option. For CMake 3.13, I want to have option not add a cache variable if an existing variable exists with that name. That way, users can do the following: &gt; &gt; `set(PROJECT_SETTING OFF) add_subdirectory(&lt;sub_project&gt;) #actually have sub_project obey PROJECT_SETTING ` It might be worth adding a new command/scope to help with this sort of thing (or even to make working with `FetchContent` easier. Maybe something like `add_subproject`, which also places all `install()` targets into the parent scope, but nothing else (this would reduce the chance of examples or tests being built)
&gt;I just checked and I have roughly ~600 [=]-capturing lambdas in the project I work on, most of them using a this pointer of some sorts. I don't remember one case where it was a problem. Presumably because you expect the behaviour - but it's extremely unsafe to capture `this` when a user isn't expecting it.
As one of the "boost is bloated" crowd, I have to say that for me the problem is not the number of libraries in boost but the dependency graph between them. Particularly annoying is that many (maybe most) libraries directly or indirectly depend on libraries that have been made completely obsolete by newer standards (the worst case is that they use heavy template and macro trickery to emulate stuff that by now is natively supported by the language). I can't speak for others, but I'd have no problem with a mostly self contained c++17 graphics library in boost.
&gt; ... take in all cool stuff that shouldn't be in the standard, because the standard would be (more) bloated. Most, if not all (recent, which is measured in years) contributors to Boost target for their library to be in the standard at some point.
The most important point in my view is that has the right scope. Putting pixels on the screen seems too limited. Yes, SFML, it does all that, plus it's well documented (very important for a beginner) and is also written in a consistently comprehensible way, extending it is easy. To me it's almost a no-brainer, but alas, simple is often not the way of the standard.
&gt; Presumably because you expect the behaviour - but it's extremely unsafe to capture this when a user isn't expecting it. but why wouldn't the user be expecting it ? it's the semantics of this construct. By that logic copy construction should not exist since an user could reasonably be expecting that const auto&amp; my_big_struct = result(); could potentially make a copy. 
Putting pixels on a screen isn’t really much use either. You realistically want to draw images, support text, window management, input handling, and probably audio. All of those operations are probably worthy of a full blown library on their own!
&gt; ... libraries directly or indirectly depend on libraries that have been made completely obsolete by newer standards ... There was an (un-submitted) proposal (and [implementation](http://blincubator.com/bi_library/cxx_dual-2/?gform_post_id=1597)) to deal with this particular issue, it got the cold shoulder and it dis-appeared of the radar. 
Im surprised to see future.them merged but executors have not been merged yet... I though .then was dependent on executors. What changed?
Well /u/ruslo is a busy guy. He is the creator of the Hunter package manager (https://github.com/ruslo/hunter). I'm sure he'd love contributions to the docs.
`[=]` means capture by copy, but the enclosing object (pointed at by `this`) is captured by reference (because `this` is copied, not the object itself) - it means you can't immediately tell when reading if a variable used in a lambda will actually be a copy (because it was a local) or a reference (because it's actually a member that's implicitly being accessed via `this`). It also means you can't tell if a `[=]` lambda is safe to keep around after the lifetime of the object that creates it, without checking every variable it uses to see if one is actually a member of `this`. Without `[=]` implicitly capturing `this`, you can guarantee a `[=]` lambda is independent of the object that creates it.
&gt; [=] means capture by copy, but the enclosing object (pointed at by this) is captured by reference (because this is copied, not the object itself) yes, and this is consistent. what would you expect if you had auto self = this; [=] { self-&gt;whatever(); } a copy of the this pointer, not of the object, right ? If it's a problem why have `this` at all ? at this point just do OOP C-like with explicit context pointers. 
Where in the standard are you seeing this? Thread's constructor states that it creates a _new_ thread of execution.
&gt; There needs to be a simple 2d graphics library in a well-known library collection set. The basic problem here is that computer graphics (even in 2D) is not simple, other than in the most simple scenario.
Your documentation is written for people who already know a lot about CMake. From personal experience this is at most one person per team. Most developers just copy and paste what is already in their project and don't bother even trying to search for an original solution. I am that one somewhat-experienced person and even I struggle with finding relevant information in the official documentation. Stackoverflow is a better documentation for CMake than your official one. A prime example for what I mean: Your main help main tells me nothing about what CMake is, why I would use it or how to start using it: [https://cmake.org/cmake/help/latest/index.html](https://cmake.org/cmake/help/latest/index.html) I would recommend the following changes: * Keep the reference documentation, but put a user-oriented guidebook in front of it. This guidebook starts with a simple tutorial and provides solutions to the most common issues. Avoid walls of text. No one reads them anyway. Link to the reference documentation instead when it makes sense. * Include complete, working example projects people can copy and paste from. Just go to Stackoverflow to find ideas for these examples. * Improve the usability of CMake GUI. For most developers this is the main interface to CMake, not the command line, not CMakeLists.txt. Help -&gt; Help doesn't solve the issue that CMake is difficult to use if you have never used a meta build system. When something goes wrong, it could provide relevant links to the guidebook or the reference documentation for example.
Perhaps too much at times.
The problem is that `this` gets captured even if it's only used implicitly: `[=] { if (thingy) whatever(); }` Does that capture `this`? That depends on the definitions of `thingy` and `whatever()`.
&gt; The problem is that this gets captured even if it's only used implicitly: but why is it a problem ? why should it work differently in a lambda scope than in a normal scope ? has there been an occurence of a high-profile CVE caused by this ? it's trivial C++. [=] copies what's in the scope, and the `this` pointer is in the scope. If `this` was a reference to the object then sure, it would make sense, but here it just complexifies the language rules for no additional benefit.
GIL is actively developed, and a maintainer is also active in the #boost channel on Cpplang Slack: https://github.com/boostorg/gil/commits/develop
No, because the library is already written. The reviewers can decide if it "does what it should do."
After some testing, I slightly adjusted our CUDA support in juCi++. One can see the flags added for CUDA files here: [https://gitlab.com/cppit/jucipp/commit/b47d971ca96e29e090fd40cd6f8d76e8f12dac41#b39bf70b724deb4f99925c2fa237047a46bfaab3\_145\_144](https://gitlab.com/cppit/jucipp/commit/b47d971ca96e29e090fd40cd6f8d76e8f12dac41#b39bf70b724deb4f99925c2fa237047a46bfaab3_145_144), and this can be useful for other projects that uses libclang as well. You can now use the same `CMakeLists.txt` file for both compiling and parsing CUDA sources. For instance, my example `CMakeLists.txt` now looks like this: cmake_minimum_required(VERSION 3.1) project(cuda LANGUAGES CXX CUDA) add_compile_options(-std=c++11) include_directories(/opt/cuda/include) add_executable(cuda src/main.cc src/gpu.cu)
The library has a mission statement, it was designed with specific purpose and focus. Remember this is a TS and a complete reference implementation.
&gt; Dirty flag pattern is not great performance-wise, if you have many objects and the "update" is fast, since you branch on your dirty flag in a tight loop. Sorting or otherwise separating and then acting on homogenous data is better cache-wise. Well, true, but that's I guess not a general case... &gt; "soup-of-objects" type of architecture Sorry for my ignorance, but what is this?
\&gt; GP &gt; I thought you could use include in a toolchain file. \&gt; You can. I do it all the time. I probably made some mistake back then. Will try again. \&gt; I'd like to hear more about this. \[toolchains ...\] so I assume you've got some use cases in min\[d\] that \[I\] don't. I don't think so, the use cases are supported in principle, i.e. I am able to get them to work, but they often spill implementation details to the user. My grieves are minor details with the UX and some default settings. e.g. right now to use clang-svn on my mac I do something like this: `cd build.clang-svn &amp;&amp; cmake -DCMAKE_TOOLCHAIN_FILE=~/devel/toolchains/clang-svn.cmake -G "ninja" ../repo` What I would prefer would be to is this: `cmake --toolchain=clang-svn -G "ninja" .` and then have cmake find me the toolchain `.cmake` file from a list of default locations like e.g. `${CMAKE_SOURCE_DIR}/cmake/toolchains/`, `~/.cmake/toolchains`, `/usr/local/share/cmake/toolchains`. Bonus points if cmake can pick up system toolchains by their system name like `org.llvm.7.0.0svn` on macOS without someone having to write a toolchain.cmake I do not care for the individual spellings of those dirs, as long as they are standardized within cmake, so that package managers and users know where to find them and can depend on them. I am not yet sure what the right model for sanitizers is, currently I lean towards a toolchain, i.e. have a toolchain with a SYSROOT set to a dir where all dependencies have been build with the sanitizer.
I have a general design question regarding writing CMake scripts. The CMake guidelines advise two things : 1. Don't use add_definitions / include_directories / ... but use their per-target forms instead, target_compile_definitions, etc. 2. Don't create functions to replace add_library, add_executable in a project with a custom one. So, my question is : what should I do if I have e.g. 200 targets to which I want to apply the same flags / includes / etc ? Just copy 200 times the target_compile_definitions, target_include_directories, etc ?
I used N4659. It says specifically that *"Constructs an object of type thread. The new thread of execution executes ..."*. It doesn't actually say that thread's constructor creates a new thread of execution, and elsewhere in the wording it is made clear that `thread` is no more than a unique reference to some thread of execution. Therefore, a conforming implementation might recycle old threads, ones without a `thread` instance attached to them if their `thread::id` does not match any other, without resetting TLS. For the record, I find this to be a defect in the standard text, and it ought to be clarified, specifically to unambiguously say that thread construction always equals the creation of a new thread, where "creation of a new thread" is specifically detailed to mean that thread local data gets destroyed and reinitialised even if the thread id remains the same. Right now, the wording does not require that either.
&gt; why should it work differently in a lambda scope than in a normal scope ? It _does_ work differently in a lambda compared to normal scope. If I try to manually capture a copy of "thingy" (say in a functor class) I will always get a copy of "thingy" even if it's a member of the class constructing the functor. The same is _not true_ for lambda implicit capture.
SFML can't do what cairo/skia do. It can't work with vector graphic. That few OpenGL wrapper primitives produce jaggy and buggy result. Try to render font with them (if SFML even can fill paths now) - it's nothing in compare to skia. Anti-aliasing is a great thing for vector graphics. 8xMSAA will not save you.
Thanks for the suggestion, I hadn't thought of using canvas. Forgive me, but I think we didn't have quite the same vision: Are you suggesting that we write a browser-style renderer for canvas, or use *a* browser's renderer for canvas, and just generate strings of text that the browser can parse? The SVG library we are thinking about, based on https://github.com/pabristow/svg_plot, simply writes text to disk, and then you need independent rendering software to visualize it. 
Those are good points. The authors of the graphics proposal do seem very passionate about it though. The question of maintenance is something that could be explored during review.
Thanks for the shout-out! Some others that stand out in my mind: * Alex Neundorf ** Introduced CMake to KDE and maintained the buildsystem there for years, giving CMake a boost * Clinton Stimson ** Maintaining the Qt UI for running CMake * Nils Gladitz &amp; Rolf 'Eike' Beer ** Giving user support on IRC and code contributions 
This is the problem. Nobody can say what they want. Someone else said they want to make a snake game. You want vector graphics. I want hardware accelerated rasterisation.
and again, has anyone ever been bitten by this ? even the most rudimentary text editor shows members and locals in different colors.
I want this library only for the most simple scenario. Like, if it can do more with simple interface, great, but if it cannot, then I'll sacrifice complexity for simplicity for the new users. 
&gt; I explicitly prohibited the ability to build at configure time because that goes against the way CMake should be used. Limiting it for how *you* think it should be used is wrong, IMO. FetchContent goes against that ideal anyway, as well as file(download), execute_process, etc etc. they all do things that can take a long time. At *request*. CMake won't do it itself. no reason to keep building dependencies from that, if developers want to. What both FetchContent and the like have in common is *they don't rerun if they don't need to*. No reason the same can't apply for building. Why have FetchContent at all if we shouldnt be doing anything with it at configure time, especially downloading it? Whats the point of downloading sources if you shouldnt use them? If your adding them to your project, ExternalProject already exists and is the same thing, if its more advanced download options(like git and unpacking archives) then file(download) or something should have been improved instead. Now we have two ways to download things. &gt; I would urge you to reconsider structuring the project as a superbuild superbuild isnt some magic thing that makes the time go away. all it does is move it into the build stage where you can't do anything useful with it. It only needs to be built once either way, and after that it's fast. Configure time or build time. Either way you spend the time, it just matters where, which is less than important. &gt; incorporate the dependency's build as part of the main project's build step using FetchContent ..that sentence makes no sense. FetchContent doesn't run at build time, and it also doesn't handle dependency building. That whole sentence is the exact opposite of what you apparently designed it for. Did you mean ExternalProject? I don't like ExternalProject. It clutters the IDE projects doesn't it? And it runs at build time, which is too complicated for something that should be simple. I shouldnt *have* to restructure my project just to do this. superbuilds just move where the time is. Plus at that point it isnt useful in the project anymore. &gt; If you are truly set on this path though, I am. I've made my own system that previously generated and ran a CMake project that used ExternalProject, but now uses the much simpler FetchContent and execute_process. it even generates an imported target for you. Tell it the name, main header file, library file(if applicable) and it handles it. Downloads content(except if its already downloaded, FetchContent handles that) Builds the dependency, and then does a very simple check on whether the header file exists to determine whether to build or not. Provides the imported target. Done. Simple, fast, easy. No reason for it to cause a slow configure. No reason to ever rebuild, either. Dependencies are static, they shouldn't be changing much. when they do(ie updating) just delete the folder and it'll get redownloaded. It's kinda like hunter, except much simpler. and easier to figure out how to use. Less than pretty, but it works for me. I only build on windows though, so no package managers and whatnot. I also, like hunter, have a root directory outside of the build directory where dependencies get downloaded and built, so deleting the project build dir doesn't make you have to rebuild everything. configure still fast.
And again, that means you have to look at the body of the lambda to determine if it can outlive the containing object or not, having an implicit-copy (`[=]`) capture list is not sufficient to make that guarantee. I've seen a few coding standards disallow implicit captures because of this risk.
Whoops, I have corrected that thank you.
Like /r/kalmoc has been saying repeatedly, what is your most simple scenario? Before you define that, there's no way of knowing whether P0267 fits the bill, and then we still did not discuss whether your simple scenario is useful in a realistic scenario (which I think should be decisive, for the inclusion in the standard).
&gt; This is the problem. Nobody can say what they want No, the problem in that term "2D Graphics" is TOOO broad. At the end ALL graphics is 2D - I see it on my flat 2D monitor!
I found some code where someone wrote vector.size() &lt;= 0 for an empty check. Just in case unsigned numbers some how ever start being negative I guess.
You can use usage requirements to propagate the flags. So you construct an interface library with your common flags, and includes wrapped in a $&lt;BUILD_INTERFACE&gt; [here is an example](https://gitlab.kitware.com/vtk/vtk-m/blob/55eafbafee0bf515777191c55c638dd3bd81b4f4/CMake/VTKmCompilerFlags.cmake#L142] and link that publicly with your common targets which will than propagate this information to your internal consumers. If you have 200 targets that share no common parent you can look at adding a `proj_add_library`|`proj_add_executable` that does the repetitive work as you are outside the common use case. 
The benefit of not having the glob is huge on large projects. Think: I changed one file and want to rebuild. This might take second or two and is an extremely common thing to happen in most workflows. With globs in a large project, it may take tens of seconds to minutes just to check that no new files have been added... and you have to do this check every single time.
I've pitched the idea of targeting the HTML canvas feature set at the authors of the Graphics TS, and anybody who would listen, for some months now. Its three big advantages over SVG are (i) it's much simpler (ii) rendering in the major browsers is much more consistent (iii) it provides for simple input (mouse clicks etc). Same as the Graphics TS, multiple backends are possible, but the most obvious is a web browser rendering a page on localhost with a Javascript WebSockets connection executing Canvas operations sent to it by the C++ program, and with mouse clicks etc sent back. One could very quickly implement other backends based on SDL2, Cairo or Skia in next to no time, because the feature set offered is so trivially simple. For me personally, a pure SVG output isn't sufficiently interactive to be useful as a graphics library, nor as a training library. For teaching, the student needs to be able to click something they've written, for it to react according to their programming. Also, down the line, it would be super amazing if somebody developed a web IDE and debugger which generated WebAssembly from C++ via LLVM. Then the backend would be "native" i.e. render graphics straight to the browser. And your C++ teaching tool would be any web browser. Now *that* would make C++ far much teachable.
&gt; I think SVG is still too ambitious. What you state afterwards completely takes that apart. &gt; What would be particularly cool in such a proposal is that a live web browser renderer backend is completely feasible via WebSockets. One choice of backend would send the calls via a mini localhost web browser (probably implemented using Boost.Beast) to be issued by Javascript in the web page. Ok, now it gets simple, I need to know about WebSockets, Boost.Beast, and Boost.ASIO (I pressume, and other stuff that gets pulled in) and well, why not learn Javascript as well. The idea (Bjarne's) was that this would be cool for beginners, they want to write a pong-clone (or whatever clone). SFML enables them to do just that, with support for graphics, fonts, sound, networking and some other bits and bobs thrown in, the stuff you need (in one comprehensive library) to do just that.
&gt; please someone stop the insanity. I just checked and I have roughly ~600 [=]-capturing lambdas in the project I work on, most of them using a this pointer of some sorts. I don't remember one case where it was a problem. I think this is motivated by a combination of two historical mistakes: The first is that if references had existed at the time that `this` was designed, then it would have been a reference, not a pointer. As things currently stand, `this` has mostly reference semantics: the standard guarantees it can never be null, you don't perform pointer arithmetic on it, you can't rebind it, etc. The second is that `[*this]` did not exist when lambdas were first introduced. This oversight likely stems from the fact that `this` is a pointer, not a reference. If it was a reference, no one would have missed including this feature when designing C++11 (I think that goes to show how "functionally equivalent" constructs can lead to very different reasoning). So if you think of `this` as a reference, `[=]` should be the same as `[=, *this]`, not `[=, this]`. I'm assuming this deprecation is in hopes of making that the case in some future standard. (I'd guess C++26) The standards committees hold a long view on C++, so they likely see fixing this oversight now, before it's so ingrained that change is impossible (see `this` being a pointer for an example of that), as worth the pain, since it will make the language more reasonable down the road.
I feel like there should be "template" projects on github that one could just fork to get started. For example, for a header only library, dynamic library, binary that can be run, etc. Ideally, with all the niceties of having `make fmt`, `make tidy`, and reasonable compiler flags for most compilers.
Yes that should be possible even before CMake 3.12 ( but with more complications as you couldn't easily get the properties of the object library ) Pre CMake 3.12: cmake_minimum_required(VERSION 3.11) add_library ( objs OBJECT a.cpp b.cpp ) target_compile_features(objs PRIVATE cxx_std_11) add_library ( combineObjs STATIC $&lt;TARGET_OBJECTS:objs&gt; ) target_compile_features(combineObjs PUBLIC cxx_std_11) Now with CMake 3.12: cmake_minimum_required(VERSION 3.9) add_library ( objs OBJECT a.cpp b.cpp ) target_compile_features(objs PUBLIC cxx_std_11) add_library ( combineObjs STATIC ) target_link_libraries( combineObjs objs) 
Minor correction: [P1023](http://wg21.link/p1023) "`constexpr` comparison operators for `std::array`" is listed as "undergoing wording review". In fact LWG approved the wording in Rapperswil (all six words) and it was voted into the working paper in plenary. (This is listed correctly in the C++20 changes at the top of the page.)
I would counter with const left is a lesser cognitive load when reading large amounts of code. Is this true? I don't care constleft4life.
Totally agree, wish I could give you more than one upvote.
Is networking even useful without async support? How will you do timeouts?
I'd say I do not idle myself, although if I have free slot to contribute, I rather focus on cleaning Find-modules in the CMake upstream. My point was also that the book presents CMake from slightly different angle, though also oriented towards a reference manual, IMO.
There's a better proposal: [randutils][1]. [Here's][2] a more full description including a comparison with the randint proposal. [1]: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0347r0.html [2]: http://www.pcg-random.org/posts/ease-of-use-without-loss-of-power.html
The limited facilities of the Rand int proposal are one reason the randutils proposal is better and should be adopted instead.
The current proposal states: "The reference implementation of this paper provides a software implementation of the math and rendering support classes. This is based on the Cairo library; indeed, so far the reference implementation has been based on Cairo. However, it is now possible to provide an implementation more appropriate to the target platform." I know (now) that you would like to ignore that, but that's what it says in the paper (Rev. 7).
&gt;Without \[=\] implicitly capturing this, you can guarantee a \[=\] lambda is independent of the object that creates it. While I personally support making \`this\` capture explicit, this is a bit of an exaggeration. As long as the language still has raw pointers, a \`\[=\]\` lambda can't guarantee safety.
One of the common patterns you see with `try_compile` is: check_function_exists(longjmp HAVE_LONGJMP) check_function_exists(setjmp HAVE_SETJMP) check_function_exists(sigsetjmp HAVE_SIGSETJMP) if (NOT HAVE_SIGSETJMP AND HAVE_SETJMP) check_symbol_exists(sigsetjmp "setjmp.h" HAVE_MACRO_SIGSETJMP) endif() So without so type of barrier you can't execute the `if` before two of the try_compiles finish.
Your employment prospects are likely to be higher with Java on your CV. 
&gt; Without `[=]` implicitly capturing this, you can guarantee a `[=]` lambda is independent of the object that creates it. While I personally support making `this` capture explicit, the above statement is a bit of an exaggeration. As long as the language has raw pointers a `[=]` lambda can't make safety guarantees. 
Where do you get the time to write all these papers!?
Sorry, but I disagree with your assessment... Ignoring the fact that \[thread.thread.constr\] says "the new thread" not only in the Effects section that you quoted, but also in the Postconditions ("`*this` represents the newly started thread") and Throws sections, \[basic.stc.thread\] says that `thread_local` variables are destroyed upon thread completion and \[thread.thread.member\] says that `thread::join()` has a postcondition of thread completion – how is this ambiguous? I'd check Anthony Williams' SO posts before writing a defect report, as I'd wager this isn't the first time this has come up. ;-]
Previously, other developers have looked at adding either an external debugger that understands CMake (e.g., cmake-dbg) or extending the cmake-server code to handle debugging. The major problem to adding a debugger to CMake as I understand it is that we don't keep full back traces of the state of execution due to the overhead of the information. Therefore, tracking what lines modify a variable would be challenging for a debugger. So that is a round about way of saying that I don't expect to see a CMake debugger anytime soon. Maybe somebody will come by and realize a smart/fast way to add debugging support to CMake. So in the near term you have to use the following flags to get CMake to spit out more information: - -Wdev - --debug-output - -trace-source=&lt;file&gt; - --trace, --trace-expand - --warn-uninitialized - --warn-unused-vars 
To be clear. I am not advocating anything for the standard. Only for boost, probably never to get into a standard. And my most simple scenario is set a resolution (say 80 points by 50 points) to plot points (pixel, so it appears as a square). I point out that, because I would like to have a 80x50 is a manageable number for someone dealing with pixels for the first time. Set colour to each pixel. This should be the core library. This should be accessible. I know, this is an extremely inefficient method. But in the starting, this is nice to grasp. Next, in the library, one can provide functions to draw a line, or draw a square or circle, but those are extra functions, which internally should use basically coloring of the pixel method. (This is so that the internal functions do not produce better results than what a good programming student can write.) I personally am in favor of this library only from teaching point of view, nothing else. For everything else, I just expect people to use other available libraries. 
I started writing the Rapperswil papers about eight weeks before the meeting. I get exactly two hours per weekday to write those, and my personal email, and all my open source stuff. It's the time I'm on the train to/from work each day.
Some would feel that socket i/o ought to have a timeout parameter. Like say the i/o functions in P1031 *Low level file i/o*, which take a `deadline` parameter which can specify an absolute deadline, or a relative-to-now deadline.
Not that I am aware of. Hopefully Marc Chevrir work for CMake 3.1.3 to add the `target_link_options` command might make this easier to encapsulate in the future.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8sruuh/need_help_linking_static_lib_using_gnuwin32/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; `[=]` should be the same as `[=, *this]` Honestly, this seems like a mistake as well. That could introduce a scenario where one forgets that a particular variable used in a lambda was a member not a local, and all of a sudden the entire current object is copied into it. I would wager that this would often be contrary to the developer's intent. More often than not, their intention was probably to just capture a copy of that particular member. And if the copy constructor had side-effects this could be especially nasty for a beginner to debug. Capturing `this` and `*this` are both super powerful techniques, but they're also somewhat more advanced. I think `[=]` is best reserved for simple, safe, straightforward captures.
CMake developers talked about this in the following threads: -https://old.reddit.com/r/cpp/comments/8sie4b/i_manage_the_release_cycle_for_cmake_the_build/e0zrad4/ - https://old.reddit.com/r/cpp/comments/8sie4b/i_manage_the_release_cycle_for_cmake_the_build/e0znd3u/ - 
Where did you get any of that from? The backend implementer needs to know that stuff. The C++ user just writes something like: ``` auto canvas = std::canvas(); auto ctx = canvas.getContext("2d"); ctx.fillStyle = 0xFF0000; ctx.fillRect(0,0,150,75); ``` The equivalent in Javascript: ``` var canvas = document.getElementById("myCanvas"); var ctx = canvas.getContext("2d"); ctx.fillStyle = "#FF0000"; ctx.fillRect(0,0,150,75); ``` 
Putting pixels on the screen is exactly how some people would like to start out thinking about graphics. Even if it is a very short lived experience, even if they never use the method again. It is a method which people new to programming who will probably never use graphics again might understand. (I am advocating this as a teaching library, not much else.) 
As far as seeing the generated source code in MSVC have you tried using the [`source_group` command ](https://cmake.org/cmake/help/v3.11/command/source_group.html)?
It hinges on "new thread of execution". I find that insufficiently specified as to what that means. I want *exact* wording, and the current standard text is ambiguous. It'll get fixed up anywhere when they implement Executors. For which they need to solve the TLS problem i.e. only the TLS which an Executor declares itself to use will be made visible to the Executor. Which, despite everybody trying to avoid that, is the least worst solution available. We'll have a similar problem to chew through for memory maps in that objects can have more than one address, or can be inaccessible despite still existing, and so on.
I know. Doesn't mean I have to like it :P
The error you make in your logic is that you assume the beginner wants to write a pong-clone, using C++ in a browser (why? Use Javascript directly), there are plenty (shit-loads) of good games doing just that [written it in JS]), using the browser's rendering engine. 
'Trawl', not 'troll'. Lol
There's a pretty lengthy paragraph defining exactly what a thread of execution is in \[intro.multithread\], and \[thread.threads\] normatively reads "_Note:_ These threads are intended to map one-to-one with operating system threads"... Better wording would be nice (as in lots of places in the standard), but *under*specification isn't exactly the problem IMO, and it would take a Machiavellian reading of the current wording to arrive at other than the expected/desired behavior. I agree that Executors will make this moot, eventually.
&gt; Well, true, but that's I guess not a general case... What is your use case? It is the case for gamedev, I'd say. &gt; "soup-of-objects" type of architecture OOP-like architecture, smart autonomous objects interacting with each other (there was a blog post recently here: https://floooh.github.io/2018/06/17/handles-vs-pointers.html) I think that coupling this metadata to your data will make your architecture more difficult to follow and reason about. Your architecture gives no guarantee that something is up-to-date, you have to always "update" before you access the derived values. But updating mutates your object, so now you have to reason about thread safety and where exactly data is changing. What I am proposing instead is that you make your data dumb and the systems that act on it can care about thread safety / caching / etc. . In this way your Triangle doesn't need to care if it needs to be cached or not. (Imagine you only need to cache a certain subset of your triangles)
Thanks for taking the time to share your views on this. You make some worthwhile observations and I'm glad you've managed to find a system that works for you. I do intend to continue collecting observations of how people are using FetchContent, so your input is helpful. Cheers!
Ping /u/philsquared :)
No, it's different ends and goals. I personally see no reason why we need competitive graphics in the standard. Competitive graphics evolves too quickly to be standardised. Not the goal of standardisation, which is to standard stuff which hasn't changed in at least a decade. So, that leaves "pong clone" use cases, and for that we need a feature set which everybody can agree is standard practice. The HTML canvas feature set is definitely standard practice. Hence that's a good feature set to choose. If later one, extensions to that feature set are considered worth doing, they can be standardised later. But let's aim low, to begin with.
I have not, I will now; thanks!
&gt; No, it's different ends and goals. I personally see no reason why we need competitive graphics in the standard. For the moment there are none. &gt; So, that leaves "pong clone" use cases ... That Bjarne's use case, pong-clones (and hopefully a bit more creative, plenty of examples (wriiten in JS) around the web). &gt; The HTML canvas feature set is definitely standard practice. I did not say or imply that this would not be good, but I fail to see how HTML-canvas features help me write a native, f.e. windows app. &gt; If later one, extensions to that feature set are considered worth doing, they can be standardised later. But let's aim low, to begin with. But if it turns out the basic assumptions are wrong, it will take another 15 years to get rid of it again. Aim low seems reasonable, but the goal (from what I get from Bjarne) is that one should be able to write a cross-platform pong using std libraries. If there is not a comprehensive solution to achieve that goal, beginners will not attempt that (with the STL, at least).
That’s a good point about the never variables. I’m convinced; I agree that neither `this` nor `*this` should be captured by default with `[=]`
I don't know if anyone has thought about a source to source compiler for CMake. It would be interesting challenge as CMake only provides an executable so you would have to generate a single input file for CMake to parse. The obvious downside to a source to source compiler is that CMake configure times would only increase. 
\&gt;For me personally, a pure SVG output isn't sufficiently interactive to be useful as a graphics library, nor as a training library. For teaching, the student needs to be able to click something they've written, for it to react according to their programming. d3.js uses Javascript events to make the SVGs interactive. Presumably there would still be some JS to be written to make the features interactive. Obviously, we're now out of scope of a teaching library.
Have you tried CMake 3.11? It has improvements to generate time for all generators especially custom command / targets. The cost of writing targets is fairly dependent on what the exact type each one is, so I really can't tell you how long 7k targets should take without more information.
&gt; cuda_compile_ptx The `FindCUDA` module is based entirely around custom commands and isn't aware of what value `CMAKE_GENERATOR_TOOLSET` is set to. If possible I recommend you look at moving to the native CUDA support as that is aware of the toolset and also can compile PTX code by using `OBJECT` libaries and the [CUDA_PTX_COMPILATION] property. (https://cmake.org/cmake/help/v3.12/prop_tgt/CUDA_PTX_COMPILATION.html)
&gt; by using a code similar to ... Your snake is gonna be moving fast, very fast :-). Actually, you'll just be seeing random dots on the screen. Don't dis-agree with you, though, SFML does all of that, and more (you **can** actually write a decent game in SFML).
Correct: It **had** a mission statement and purpose: Being the graphics library for the standard library, shipped with every c++ toolchain in c++20 or later. Apparently that's no longer a realistic option. If it is now added to boost it becomes just another graphics library and I'd like to know what specific use cases it should enable and why someone should use it instead of &lt;my_favorite_graphics_or_gui_library&gt;. If those questions are answered the boost reviewers can determine if the library meets those goals or not.
I agree. I forgot to put sleep. :-)
Good question. I covered it as part of my answer to another question about what I see as the flaws of CMake, but given that I have used CMake for about 10 years I don't see all the issues that new users have when starting with CMake. So I wanted to use this AMA as a way to talk to users of CMake and get a sense of what issues other people have. Additional I think the AMA was a great way to teach people about the history of CMake and why CMake made some design decisions. Also it allows people to peek behind the curtain in a way and see who the CMake developers are, and what they think about when trying to improve CMake ( Backwards compatibility, Policies, Policies, Policies ). 
Sure, where there's a will there's a way. But for me at least, SVG is too vector-based in any case. Any abstracted out C++ API would thus be very "vectory". What I like about the HTML Canvas API is that it's equally insufficient at everything equally, if that makes sense. But ultimately, any successful Graphics TS comes down to whomever puts in the work to make it happen. Which includes stakeholder engagement outwith C++ experts, who unfortunately voted for the Graphics TS early on, when perhaps they hadn't fully thought it through yet.
You mean outside of setting `MACOSX_BUNDLE` on the `add_executable` :) I think the recommendations are to make sure you are using `MACOSX_RPATH` and using something like BundleUtilities to make the final bundle. I would need more information on what parts of the process you currently have questions on.
I don't expect to see multiple configuration build directories happening anytime soon. You will have to continue to use multiple build directories and some wrappers to build them all.
&gt; That's Bjarne's use case, pong-clones (and hopefully a bit more creative, plenty of examples (wriiten in JS) around the web). And I, and I think most of WG21, would get on board if the proposal were sufficiently limited in scope to do just pong clones, there was a reference library with lots of use experience, and we were standardising existing practice on something which is highly likely to remain fixed in stone forever. None of the proposed Graphics unambiguously ticked all those boxes. &gt; I did not say or imply that this would not be good, but I fail to see how HTML-canvas features help me write a native, f.e. windows app. This is the graphics TS. Not an Application TS. Separate thing. (I personally think we need an Application TS, very ripe for standardisation, but god help whomever proposes it) &gt; But if it turns out the basic assumptions are wrong, it will take another 15 years to get rid of it again. Aim low seems reasonable, but the goal (from what I get from Bjarne) is that one should be able to write a cross-platform pong using std libraries. If there is not a comprehensive solution to achieve that goal, beginners will not attempt that (with the STL, at least). There are plenty of Canvas based games which demonstrate the featureset is sufficient for many use cases. I agree there is a gap between Graphics and an application, but that's a separate issue, and TS. I'd definitely welcome seeing a proposed Application TS, something which standardises event loops, command line processing, and so on. It's not like those haven't been written in stone for twenty years or more by now.
If the language cannot provide functionality one needs then people are going to use generators like Bison or Qt MOC. Not to degrade these tools at all because they're great, but at least macros are part of the C++ standard, so people will very likely reach for them first. When reflection and metaclasses are standardized, a lot of macros and third party tools can be removed entirely.
Alright. Thanks for the reply!
LOL - right 
As a developer I would rather packages have 1 build systems that works really well than 2 partial build systems. If that means they choose autotools I would be sad, but that is still better as a consuming developer. As far as playing nice with other build systems _output_ wise. If you are able to create ` .pc files` for your project and have users that need them I say go for it. At more of a research level I love to see somebody tinker with cmake-server to get it to generate .pc files for an already installed CMake project.
You totally did this wrong. You should have come in and been like, "here's some stuff that's super easy in D, and I don't think you can do it even in modern C++." Then everybody would have jumped on it. As you've phrased it, it just sounds like homework. :P
&gt; What is your use case? It is the case for gamedev, I'd say. Update objects which is outdated. Think of "desktop"/"mobile" application. Objects at least not small (100s of bytes), update is at least somewhat computationally expensive. Scatter object and have separate raw data, and "computed" data is not an option. Objects should be as simple interface-wise and as encapsulated as possible. One example: You have texture. Texture have text in it. You can change text, color, bg color. Each frame update, you check does set(text, color, bg\_color) changed - if yes - redraw texture. &lt;text,color, bg\_color&gt; - small, but texture is not small object, and update act - is expensive . Non-optimisation dirty-flag examples: You have settings screen list. You need to save/apply settings only if settings changed. You have document. If document changed - you must ask user for save before exit. You have search field, each 300ms you check does it changed, if yes send net request. Actually you can't fully rely on dirty-flag in this cases (unless you have separate one for each case), but you can rely on epoch. This things, kinda can be solved with reactive programming... But with epochs it seems to be shorter/celarer.... &gt; Your architecture gives no guarantee that something is up-to-date, you have to always "update" before you access the derived values. Why? Triangle interface hides how exactly ab, bc, ac is get. Make them lazy/cached is implementation details. You don't even have to know about updates existence. &gt; But updating mutates your object, so now you have to reason about thread safety and where exactly data is changing. Sometimes you really do can afford executors:) and single thread runs. Show how in Triangle example cached data can be outdated. You need lock on object to use in threaded env, how caching can be a problem? &gt; What I am proposing instead is that you make your data dumb and the systems that act on it can care about thread safety / caching / etc. . But how much life is easier when Triangle care about this itself, and I don't have too :) I agree that for small objects, it is faster But it's easier to reason about behind this wall of abstraction, when you just draw, get\_ab, get\_area. Not sort, update, find in result array get.
As /u/fat-lobyte stated even if you go the docker container you are still going to want to use a build system to build the project. How do you expect users to consumer your project? Are you providing a library or an executable? How will they know where to find your project files? &gt; I want to support people building on Windows, Linux, and Mac If you want them use the system or the compiler of their choose you should look at CMake. In general you should't consider docker to be a build system, but a way to have a collection of consistent reproducible build enviornments which could rely on CMake.
Sorry, I don't think I explained myself clearly. We do use what you suggested already. The problem with it is the the solution that gets created has two projects one to build the object files and one to link them into a library. This is a bit sad as the project that builds the object files actually builds a static library anyway. This replication of projects is OK for a small number of project but our solution has about 150 libraries so doubling that to 300 kills Visual Studio to the point it's not usable. So to explain what we are trying to do. Sometimes our clients want the 150 "component" libraries or some sub set of them, other times they want a "combo" library which we produce by linking the thousands of object files produced by building all 150 projects using the OBJECT style projects. So because 300 projects is unusable in Visual Studio we create two separate solutions one to build the "combo" library and one to build the components.
&gt;I truly don't understand what's the big deal about a language whose syntax takes 8 pages to explain. It sounds like a bunch of BS to me, frankly said. Once you know it, you know it. Let me provide my 2 cents: I would like the build system to be simple enough that I can ask my intern, or a junior dev, to add some feature, add their new file to the build, and put up a PR, without needing to ask for help when they make a mistake. The problem with all of these "stringly-typed" scripting languages is that everything is a string, and nothing is an error, no matter how stupid it is, and so the ultimate error message may end up being some complex compiler or linker error that a junior dev typically can't make sense of. It blows my mind that something like the cmake scripting language can become a part of an "industry standard" tool, when it is so easy to misuse it and so much harder than it should be to figure out what the problems are. Yeah, once I know it, I know it, but that doesn't help everyone else.
What I'm saying is that even a wrapper can provide a significantly different API experience than what is wrapped. For every argument where you say "It is bad because Cairo..." I have to check wether or not that is also true for the wrapper and I have no desire to do that. I am interested in learning, what specifically is wrong with the API of the proposed library, not in what is wrong in Cairo and thus may or may not also be a problem in this library. I'm not saying it doesn't make sense to expect that it exhibits the same short commings as Cairo, but I'd like to argue about actual problems, not expected/likely ones. It is just like saying "Don't by this phone, it's predecessor had a short battery life" - that doesn't help anyone. A more useful statement would be "Don't buy this phone, because they didn't fix the predecessor's short battery life".
Ha! Yes, I saw this when Adi first posted. I don't remember if it broke (like my Python version) when meetup switched their interface. In any case, getting the CSV of all attendees and parsing that is probably easier now.
Cmake has a few issues, but why not clean it up and standardize it? At least it is a proven solution...
&gt; "does what it should do." Wasn't that exactly the question? What should it do?
[CMake 3.9](https://cmake.org/cmake/help/v3.12/release/3.9.html#id7) added the ability to export, import, and install OBJECT libraries. So project A can build and export/install the object files and have project B import them and use them to build a static library. I hope that simplifies your use case.
So we are back to having a single tool? BTW.: The c++ committee is barely able to work on one language. What makes you think it can handle a second one?
&gt; has there been an occurence of a high-profile CVE caused by this ? Should we be waiting until there is one and deciding at that point "oh now it's been too long and it's time to fix that mistake"? &gt; [=] copies what's in the scope, and the this pointer is in the scope Except it *doesn't* copy what's in scope. It copies locals that are in scope but a *reference* to member variables (via this).
We have multiple compilers all implementing a language spec, right? A package format should be tool-neutral as well. As for your second question, unfortunately you have a good point there...
Please notify us when code samples are ready. I would love to see comparison. (I bet on C++!)
Haha! No matter how it sounds, my question is still genuine and not homework, just curiosity. Maybe I will post some code samples at some point if I have some time. I want first to figure out how to deal with the examples, and, yes, it is super easy in D but in C++ it is a bit more challenging, at least with the current state of affairs.
The one I find more challenging is the RPN calculator, since it generates code on the fly. :)
Yes, but now we are talking about standardizing the proprietary language of a single vendor/tool and tell everyone else to essentially deprecate their own products. And while I like cmake, because it is already the de-facto standard and thus well supported it is imho far from being the best technical solution for the job. But yes, the lack of progress in whatever would get standardized is my main concern. 
&gt; my question is still genuine and not homework, I don't mean you're in school and asking us to do your homework, but this sounds like programming busywork.
&gt; It copies locals that are in scope but a reference to member variables (via this). but there's no such thing as a reference to a member variable ? it's always going through `this`.
&gt; examples Your examples are not bad, but they don't really demonstrate a need to go beyond a flag. I think you need something that is expensive to compute, but cheaper if you can reuse the current solution somehow (don't know a good example right away, may be something with a DB). &gt; Why? Triangle interface hides how exactly ab, bc, ac is get. Make them lazy/cached is implementation details. You don't even have to know about updates existence. &gt; Show how in Triangle example cached data can be outdated. You need lock on object to use in threaded env, how caching can be a problem? I got a bit confused with the many examples you give. If you put the logic in the accessor, then it can't be outdated. If you have `void update_XXX()` and you expose the results as members, then they can be outdated. However putting caching logic into the accessor means that &gt;&gt; you have to abandon const on a normally non-mutating function (accessor) or to make the member variable mutable, but now making your object const is a half-lie (threading). &gt; But how much life is easier when Triangle care about this itself, and I don't have too :) You will end up with Triangle, UnCachedTriangle, ThreadSafeTriangle... and all users of Triangle will need to know of this.
That is not really *on the fly*, code is generated statically in compile time: static foreach (c; "+-*/") it's possible with e.g. `constexpr/pack expansion`
It doesn't surprise me one bit that D's examples can't be repeated in C++ as nicely and elegantly, because examples are almost by definition intended to be small snippets of beginner/intermediate level code and D's examples will be written by those who are at the top of their class with the D language. No one is going to particularly care to write those same snippets of code to promote C++ because frankly C++ is beyond the phase of trying to promote itself for wider adoption. C++ experts aren't working to promote C++ in and of itself the same way D experts are, C++ experts work to actually use C++ to write very sophisticated systems. Strictly speaking, D is a more powerful language than C++ if all you care about is the language. It also has a much better language improvement process and community involvement that manages to adapt the language within reasonable time frames and avoids bike-shedding. C++ can learn from this (but it won't). The issue with D isn't the language, it's the tooling, environment, debugging, bugs, performance, libraries, and the fact that it's historically had many issues with stability that makes it an insecure choice for doing work that is consequential over a long period of time. For personal projects, even professional projects on a small to medium scale, D is a fine language to use. It is also suitable for many niche related work as well. I think in many ways it's a lot better than C++ as a language and many of its contributors are in fact some of the most knowledgeable C++ experts who got fed up and tired with how stagnant the C++ language is. But for projects where push comes to shove because actual money or jobs are on the line, you need the best tools available, the most battle tested systems. You do not want to be using a compiler basically written by a small handful of guys who work at their own pace, with tools that are fragile and where many of the tools are one person's hobby. You also don't want to be in the situation Sociomantic is in, they are likely the largest users of D and they're stuck on a 10 year old version of the language and have had a lot of struggles adapting their codebase from the old D1 to the current D2. That's just time and money wasted.
&gt; auto&amp; x = m_whatever; Why are you using `auto&amp;`? "I" asked for `[=]`, not `[&amp;]`. And `auto x = m_whatever` copies, doesn't give you a reference. `int local; auto &amp;x = local;` also gives you a reference, so maybe `[=]` should capture locals by reference too? (`decltype(auto) x = m_whatever;` also copies of course, so no help there.) &gt; you wouldn't expect ... to only capture the "x" variable so why should this be different for this ? Because syntax matters.
Well, the compile-time array sort was [easy](https://godbolt.org/g/cnhjC6) ;)
I expressed myself incorrectly. I meant on the fly at compile-time. Which is generated before running , of course :)
Nice use of ranges. One error you have in practice is, that not all RSVPs show up. So having a checked CSV file as input might be better. On the other hand, its a good way to promote people to RSVP.
Well, actually D does care about stability more than it used to. Your assessment is quite correct to me. I think it cannot compete against C++ tooling because of the size of the community. But, in fact, I see it as an alternative and it is improving over time. But it cannot possibly be as battle-tested as C++ is. The bar is very high in that sense.
I am even more impressed about the include!!! How does that work?!
That eventuality is, in fact, accounted for and handled by the ability to get the next name (by typing “more” at the prompt). 
You'd have to ask /u/mattgodbolt about that :)
&gt; Why are you using auto&amp;? by habit :p it's unrelated to my point, I should just have put `m_whatever` and `this-&gt;m_whatever`. And yes, `auto x = m_whatever;` copies, but the = of assignation doesn't have the semantics of the [=] in lambdas, which is "take the values in the scope and make a copy of them". Member variables are not in scope, their `this` pointer is. Else you would need to have the following code making a copy of a `struct foo` to be consistent - and in my experience there's much more possibilities of bugs when the semantics aren't consistent than when it's the syntax which is not consistent. struct foo { int x; } *f; [=] { printf("%d", f-&gt;x); }; 
Of course, the tried-and-tested “Linux sucks because it can’t do XXX” approach: http://bash.org/?152037
Better yet, the right internet approach is to trash and shame, it would be like "Modern C++ is so bad is not used in dlang.org". :D
&gt; but they don't really demonstrate a need to go beyond a flag Oh I forgot that, thanks. With dirty-flag. struct SettingsList{ bool dirty; onStart(){ dirty = false; } set_resolution(){ dirty = true; } set_gamma(){ dirty = true; } update_frame(){ if (!dirty) return; dirty = false; // need update texture, redraw canvas, etc. draw_canvas(); } onExit(){ if (!dirty) return; dirty = false; // Now what? IT IS dirty, but we already drop flag. Need another one // duplicate in same places. Need dirty2. // And we need duplicate for EACH feature. save(); } }; SettingsList settings_list; // All below biased for comparison with Epochs :) // Somewhere after. // [Maybe not exactly dirty-flag job, but it would be nice if that work.] // Another flag duplicate? was_dirty_on_exit?? if (settings_list.dirty){ show_message("Settings changed."); } // Latter, we want to periodically save our Settings, if they chnanged (think autosave) // Hypothetical. Probably you just save whole previous Settings and compare with current IRL. if (settings.changed_since_last_autosave){ settings.save(); } With Epoch. struct SettingsList{ Epoch epoch; set_resolution(){ epoch++; } set_gamma(){ epoch++; } Epoch frame_update_epoch; update_frame(){ // equivalent to: // if (!frame_update_epoch == epoch) return; // frame_update_epoch = epoch; if (!frame_update_epoch.update(epoch)) return; // need update texture, redraw canvas, etc. draw_canvas(); } Epoch exit_epoch; // probably don't need this... If exit once... onExit(){ if (!exit_epoch.update(epoch)) return; save(); } }; SettingsList settings_list; // Somewhere after. Epoch show_message_epoch; if (show_message_epoch.update(settings_list.epoch)){ show_message("Settings changed."); } Epoch autosave_epoch; if (autosave_epoch.update(settings.epoch)){ settings.save(); } Well, biased comparison, but I think it shows where it can shine :)
I fail to understand how you're missing the point here. The argument should be whether or not the canvas feature-set is sufficient, not about what the code itself looks like. Yes, it looks like JavaScript's canvas API, most likely because there's no reason to reinvent that wheel. Your arguments seem to be reactionary, and not based in actual concerns like "would a Canvas-like C++ graphics API be a reasonable inclusion into Boost/the standard library."
If you have a logical grouping of these source files you can place them in [`OBJECT` libraries](https://cmake.org/cmake/help/v3.12/command/add_library.html#object-libraries) and than have your executable uses these object libraries. This becomes cleaner with CMake 3.12 too as it allow syntax like: cmake_minimum_required(VERSION 3.9) add_library ( objs OBJECT a.cpp b.cpp ) target_compile_features(objs PUBLIC cxx_std_11) add_library ( combineObjs STATIC ) target_link_libraries( combineObjs objs) 
This question has been asked and answered numerous times. Just grab a copy of "Tour of C++" and you'll learn most of the important concepts of (modern) C++. I don't like "transition guides", the concepts, idioms and practices of C++ very much differ from C, so you should just pick up recommended C++ books. 
`.then` hasn't merged into the IS. I think the trip report may be referring to other parts of the Concurrency TS, which may have been merged in Rapperswil.
Yes. See [P0514](https://wg21.link/P0514).
The `add_subproject` idea is an interesting idea it halfway between external project and add_subdirectory. I have no idea of it is possible to implement with how we manage the global 'install' target and the global cache.
It's a horrible hack that only works in Compiler Explorer; it's definitely not standard C++ and only works for one level of include...
&gt; Is there a better way to do it The better way is to have the packages generate export configurations and use that information. I answered a fairly similar question on the CMake mailing list where I provided some source code ( http://cmake.3232098.n2.nabble.com/Re-Install-libraries-defined-in-INTERFACE-targets-td7597569.html#a7597687 )
One way to do it is to get started (https://isocpp.org/get-started) and take a tour (https://isocpp.org/tour) just like people from other programming languages. A special C to C++ transition is possible. After all this is how C++ is invented and gain popularity historically. But I have yet to see any learning material that satisfy me. I don't think I would recommend this path right now.
Please post code reviews to /r/cpp_questions
The rules are in terms of a generalized Pointer and Owner concept. A capital-P Pointer is a non-owning indirection and T*, iterators, ranges, span, string_view etc. are treated equally. A capital-O Owner treats unique_ptr, vector, string, etc. equally. Basically, if you get a Pointer from an Owner, the Pointer is assumed to be invalidated when you do anything non-const to the Owner you got it from... that's the default rule which can be overridden.
[Dilbert did it first](http://dilbert.com/strip/2001-10-25)
Still impressive.
Oh, it's about 10 lines of hacky JavaScript :)
Let's be clear. The HTML canvas *feature set*. So we implement a C++ API that can do each of the things that HTML canvas can do. There are two parts to such an API feature-set match. * It should be easy to take *as input* requests from an HTML interpreter that is requestiong HTML canavas operations, and implement them using the C++ canvas. * It should be easy to connect *as output* requests from a C++ program interacting with the C++ canvas, and forward them over to an HTML canvas implementation on a web browser. These are two distinct and objectively measurable ways you can match the feature-set of HTML canvas. They are related, but not the same thing; it would be easy to imagine an API that pulls off either, both or neither of the above while still matching the feature-set of HTML canvas. 
I'm not sure you're cut out for a career in sales…
Spot on
Darn. Feels like we’ve been waiting on .then since c++14 (14 ‘finished’ 11)
Looks like here, in case anyone else was curious what 10 lines of hacky JS looks like... https://github.com/mattgodbolt/compiler-explorer/blob/master/static/compiler-service.js#L117 (Called [from here](https://github.com/mattgodbolt/compiler-explorer/blob/master/static/compiler.js#L388))
&gt; Another question, and this may just be a misunderstanding of my part, but I'd like to see some more clarity and options around the set_target_properties command. Sometimes it seems to reset the property, sometimes it seems to append. `set_target_properties` should always set the property and never append. If you need to append you have to use the `set_property`. You are correct there is currently no way to reset/unset a property. &gt; `source_group` It is totally possible that this is a bug within CMake. It looks like other users have reported (non obvious errors / bugs with `source_group`) [https://gitlab.kitware.com/cmake/cmake/issues?scope=all&amp;utf8=%E2%9C%93&amp;state=opened&amp;search=source_group]. I have never looked at how `source_group` is implemented so I really can't tell how easy this would be to fix.
Thank you very much, I'll do what you recommend. Sorry for asking something that has been treated before. Cheers :)
There is not much logic as such. You basically do this: some_dep = dependency('depname') executable('foo', 'foo.cpp', dependencies : [some_dep]) Dependencies passed with the `dependencies` keyword argument can be found with pkg-config, custom logic (Boost, etc) or they can even be internally defined. They all work the same way.
That's the one :-) nice tracking skills!
LEWG hated the member-based design with a passion. Holding your breath is not recommended.
It's a joke about parentheses, which is pretty much all non-Lisp programmers know about Lisp.
I don't use Ruby much, but I think they're the only ones who've done it right (imho). Functions querying state are suffixed with `?`, so it would have been `empty?()`. (or just `empty?` because idiomatic ruby omits the parens for methods with no arguments) Reading it in code triggers the same part of our brain as when reading a question in text. It even makes grammatical sense. Unfortunately, I think that ship has sailed for C++, because of the ternary operator.
Would it be possible to make a change where you make a per-project install target (that is only for targets from add_library, etc) and the global install target depends on all add-project install targets? I’m not as familiar with the underlying implementation so I don’t know if this is even possible.As for project specific caches, the only thing I can think of is to create a PROJECT set_property option and if it was added via add_subproject all GLOBAL settings go to the project specific cache. Any calls to get_property GLOBAL would lookup in the local PROJECT then in GLOBAL. This way it won’t break compatibility with existing projects, even if the internal cache is a bit more splayed out. On an unrelated note, has any thought been given to adding some sort of automatic CMAKE_MODULE_PATH setting so I don’t have to manually append PROJECT_SOURCE_DIR/cmake in every project? It could also be something set in the call to project() if it isn’t named cmake/
That's why we now have WSL. Someone said "Windows can't do this."
It should meet the criteria for a boost library to be accepted
Is there anything left in the article after this misunderstanding fix is applied to it?
`#define maybeexcept(x) noexcept((constexpr_rand() &amp; 1) ? (x) : !(x))
`__random(__implicit, __explicit))`
Immoral code is fine if you aren't using the Orthodox extension or `-fmorality`. Or HolyC, I guess.
Stalllint?
Which are? Afaik you can't submit a library to boost without explaining, what it is supposed to do can you? I have the feeling we are going in circles
You need to clarify what your C skills are like. My advice would be very different for someone who can use C to write an OO ABI stable plugin architecture, and discuss the different possible implementation choices and their pros and cons, from someone who can sometimes remember to check the return value of `malloc`. C++ provides a few big things for a C expert. * RAII. Resource Acquisition is Initialization. Know all the "goto cleanup;" code? In C++ you can roll the cleanup code into your resource aquisition code using destructors. * Pre-written object model. `virtual` and `new` and methods etc. It is an implementation of a common object model pattern that existed in C prior to the existence of C++. It supports interfaces, mutliple implementation inheritance, virtual single dispatch, non-virtual methods, (limited) information hiding, and both stack and heap based allocation of objects. * exceptions. * `template`s, which are a form of code generation that is less insane than using macros. `template`s exist within the language, not before the language, like macros do. Both `template` types and `template` functions exist, and are massively powerful. Using C++ `std::sort` is faster than `qsort` and no harder because of it. * Slightly stronger type safety. `void*` doesn't implicitly cast, funtions have to have signatures, and casting operators that are a bit safer and more explicit about what they do. * A larger standard library, including some simple template data structures so you don't have to hand-roll yet another linked list. * [tag:C++11] syntactic sugare, like lambdas, `nullptr`, and range-based `for(:)` loops. * Move semantics, as part of having complex value types with non-trivial copy semantics and RAII. A `vector&lt;int&gt;` is a dynamic array of `int`s. This can be *copied*, but that requires allocation and O(n) work. It can also be *moved*, which requires no allocation and O(1) work. In C++ if your generic code (written as a template) is move-aware, it can do the right thing for a `vector&lt;vector&lt;int&gt;&gt;` as it does for an `int` or a `vector&lt;int&gt;`, and in fact `std` library algorithsm are move-aware and generic. * References. These are *aliases* to other values. You could think of them as a `const` pointer to possibly `const` data with some syntactic sugar on them, but they are really aliases in some fundamental ways. They are not nullable, and cannot be "rebound" do another object. In exchange, the compiler can often erase them from existence. Most of these bullet points could take a small library of books to master, but I think that is most of the touch points.
east `const` best `const` west `const` worst `const`. vs `const` east best `const` west `const` worst `const`. I rest my case. 
/u/shevegen Here is my answer to your programming post ( since it is locked ). &gt; . How can I do the equivalent "./configure --help"? I still have not found out. To me, cmake is simply less user friendly than GNU configure-based systems. CMake doesn't support --help where we can list information on what options a project support. I was talking with Bill Hoffman about this exact problem and we started to bounce around ideas on how some future version of CMake could handle this. 
You mentioned the word 'joke'. Chuck Norris doesn't joke. Here is a fact about Chuck Norris: &gt;Chuck Norris can binary search unsorted data.
Sure, macros are useful. I wrote one recently that used - ### - __VA_ARGS__ - goto - variadic templates - comma operator The above is an example of both their usefulness (nothing else within the language could solve the problem), and their evilness. So whenever you use them, weigh it against their inherent evilness. (And, implicit, for example, fails that weighting test in my mind)
Epic!
I love CMake. It's the best thing that happened to C++ since it was created. There truly is a before CMake and an after CMake in the world of programming.
[Here](https://godbolt.org/g/QiSRhs) is my approach at the hexdump one. {fmt} isn't really necessary though, [this](https://godbolt.org/g/FDNZwx) is essentially the same thing without {fmt}. I suppose it would be possible to use more ranges-v3 in there e.g. by replacing the for loops with a ranges-v3 `for_each`, but I think that would just bloat the code at this point...
You are living up to your username, thank you.
&gt; because we don't have a solution yet. Apparently nobody out there is working on one, or if they are, there is either too much fragmentation, or they are simply not getting enough traction. And that's something where standardisation could help a lot. Prior to Rubygems, we wanted "CPAN for Ruby", but there were multiple competing solutions, and none of them gained any traction after many years. Tired of waiting, five of us wrote Rubygems in a few hours at the back of a hotel bar. It wasn't technically superior to any of the solutions that came before it, but we managed to get Matz's blessing on the project, and it was eventually included as part of Ruby. If we had waited for a winner to emerge, we might still be waiting for a package manager for Ruby. I think the same will be true of C++: if a packaging (i.e. source code distribution) system is not standardized, we will be waiting a long time for the world to come to an agreement on a de facto standard. 
Well, we're in a C++ sub so we can do better than non-programmers.
Been there, done that. The trick with old open source packages if that they rarely introduce new files. So once you write clean CMake files for them, it's easy to upgrade when a new version is released. It's not that bad, really. And the advantages are so big, that they outweigh any alternative (trying to maintain a bad interface with heterogeneous build scripts).
"The library must be generally useful and not restricted to a narrow problem domain." https://www.boost.org/development/submissions.html 
My answer to that as someone who has *built* a few things with CMake and used to build *lots* of stuff made with the autotools^(1) would be to point towards `ccmake` or `cmake-gui`. Actually, as much as it'd be sometimes nice to have shortcuts for common options (`cmake --prefix=...` instead of `cmake -DCMAKE_INSTALL_PREFIX=...`), as a builder I came to prefer CMake projects over autotools projects on that point. ^1 For a while I was maintainer of a collection of software installed site-wide at a custom network path, so we needed to build basically everything there with a custom `--prefix`.
You just need to have a 3rdparty or vendor folder and do all your FetchContent from there to isolate them from the rest of the project. And yes, you need to be able to pass variables to builds you include in your project as some of them will require configuration. And in theory, you could have options from your project leak into the other projects and bad things happen, but in practice, I haven't seen that happen a lot. To answer the other question, EXCLUDE\_FROM\_ALL will prevent installation of 3rd party libraries too, which is what you want usually.
The Tiny RPN example is neat, but hardly a good example of "real" code. It's only generating about 4 simple lines of code, meanwhile increasing the complexity of reading the code quite a bit. While I appreciate the code reuse in the example, this is more readable (and easily portable to C++ without macros): void main() { import std.stdio, std.string, std.algorithm, std.conv; // Reduce the RPN expression using a stack readln.split.fold!((stack, op) { switch (op) { case '+': return stack[0 .. $ - 2] ~ (stack[$ - 2] + stack[$ - 1]); case '-': return stack[0 .. $ - 2] ~ (stack[$ - 2] - stack[$ - 1]); case '*': return stack[0 .. $ - 2] ~ (stack[$ - 2] * stack[$ - 1]); case '/': return stack[0 .. $ - 2] ~ (stack[$ - 2] / stack[$ - 1]); default: return stack ~ op.to!real; } })((real[]).init).writeln; } Moreover, if you wanted to add operators that aren't built-in to D, you couldn't use the original method anyway. That example isn't meant to show how D can create high quality code, it's meant to show how D's features work. That code generation syntax IS extremely useful, and would likely allow for more readable code to be written, when used properly. tl;dr Your challenge is arbitrary, and doesn't effectively critique's C++'s shortcomings with respect to D.
Windows can't be Debian, Ubuntu, openSUSE, and Kali simultaneously! 
By "these checkers" they probably mean address sanitizer and the like. They exist and make your code a lot safer and better but they are not "as good" as Rust in this regard.
Learn the standard library and use it. If you face a problem of any kind, most probably there is already an algorithm or data structure for it in the standard library. Also watch Sean Parent’s C++ videos
How would it help differentiate between different "verbs"? I.e. `is` vs `has`?
Considering the vague notion of files in the include system, is this really not standard-compliant? :p
If you are interested in an higher-level wrapper of the `CMakePackageConfigHelpers` macros, you can check [`YCM`](https://github.com/robotology/ycm)'s module [`InstallBasicPackageFiles`](https://github.com/robotology/ycm/blob/master/modules/InstallBasicPackageFiles.cmake) and its function [`install_basic_package_files`](http://robotology.github.io/ycm/gh-pages/git-master/module/InstallBasicPackageFiles.html#install_basic_package_files). Check https://github.com/robotology/how-to-export-cpp-library/blob/master/CMakeLists.txt#L118 to see a practical example of its use. 
Fair point, the standard is pretty vague on this!
&gt;In addition, the entire existence of the CMake users package repository and the associated ‘export(PACKAGE)’ command has also caused me pain, and I really should bring up if we can deprecate the feature, or am I alone on this? You are not alone on this. I personally find the CMake users package repository and the ‘export(PACKAGE)’ command quite dangerous and confusing, but it is difficult to convince non-expert CMake users not to use it if [CMake documentation](https://cmake.org/cmake/help/v3.5/command/export.html) provides no feedback on whatever it should be used or not. See for example this Eigen's bug: [http://eigen.tuxfamily.org/bz/show\_bug.cgi?id=1386](http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1386) . Several users affected by problems due to the use of `EXPORT(PACKAGE)`, but one of Eigen's developers correctly observers: &gt;But I'm not really a CMake expert, so I have no idea, whether one should or should not do: &gt; &gt;`export (PACKAGE Eigen3)`
I like this idea, that's what they do over at boost
Similarly, I thought all std customization points should be prepended with `std_`. ie you call `std::swap(x,y)` it turns around and calls (with ADL) `std_swap(x,y)` I'd accept either. 
How is `std::thread` a horrible abstraction?
My preference: the standard could provide a class template for you to specialise for your own types, for example: template &lt;typename T&gt; class MyVector { ... }; // P0665 syntax template &lt;typename T&gt; struct ::std::range_interface&lt;MyVector&lt;T&gt;&gt; { static auto begin(MyVector&lt;T&gt;&amp; vec) { return vec.first(); } static auto end(MyVector&lt;T&gt;&amp; vec) { return vec.last(); } }; Then the standard library `std::begin(T&amp;&amp; t)` could call `range_interface&lt;uncvref&lt;T&gt;&gt;::begin(forward&lt;T&gt;(t))`, or give a nice error message if that doesn't exist. No ADL required, and no possibility of collision with existing names, because everything is namespaced. (Yes, I know this is basically Rust traits or C++0x concept maps. I think it's the best pattern for customisation points, and I don't know why it isn't used more frequently.) 
You aren't going to write examples of things that your language is shit at, it's going to be all the niche meme features that you never actually use but sound cool.
What's the difference between alias this and inheritance? It seems identical to non-polymorphic inheritance. struct Point : std::array&lt;double, 2&gt; { double dot(Point rhs) { ... } };
My issue with prefixes is that we went away from prefixes with namespaces. Prefixes, just seem so C'ish, and adds yet another reserved class of identifiers to C++. In addition, if you want to design customization points for your own code, you cannot use the prefixes technique. Using an empty type in a namespace you control works for both std customization points and for non-std library customization points.
So e.g. to make `push_back` a customization point, the implementor of a container need to define a `push_back(std::customization_point, MyContainer&amp;, ValueType);` in addition to the existing `MyContainer::push_back(ValueType)` and ensure that the two functions behave the same? Is the benefit worth the extra work?
Actually you don’t need to do that. The default implementation in std will look like template&lt; class T, class V&gt; void push_back(customization_point, T&amp;&amp; t, V&amp;&amp; v){ forward&lt;T&gt;(t).push_back(forward&lt;V&gt;(v));} So you would only have to do one or the other but not both. 
Basically, it's too low-level – to make effective/efficient use one generally wants some sort of task pool, so everyone ends up having to reinvent the same wheel or use a 3rd party library. It's not horrible for what it is (though there is the whole stack size thing...), but what it is is a low-level building block not really suited for direct use.
Thanks :) I know details about the Evolution Working Group because that's the room I sit in during the meeting. For the other groups, I just listen to their reports during the closing plenary session.
Thanks, fixed!
&gt; By and large, asking what C++ (and by that I mean C++2011) does better than D is as self-contradictory as the question, "If you pay a professional to clean your house, what are the places they'll leave dirtier than before?" Whatever of value was that C++ could do that D couldn't, it has always stood like a sore thumb to me and Walter, so almost by definition there's nothing C++ can ever do that's not within D's reach. https://softwareengineering.stackexchange.com/a/97270/105679
Hah... One of the jobs I had during college was in a corporate IT department. At some point the manager had learned this trick. If he had something that really needed to be done, he'd come back to the area where a bunch of geeks worked and basically say "I bet you can't...." And whatever it was would be done before whoever was challenged left for the day. Of course his challenges we're sometimes things like "I bet you can't generate a report of the top people browsing porn on the corporate network" and not language challenges, but the spirit is still there.
I find the examples genuinely well-thought in general to oresent features that can be useful in real life.
I guess that not exposing the inheritance and being able to hide the definition of that array. 
&gt; I know it's obviously better as it makes the same but in a simplier, cleaner and more elegant way, but as I say, I never tried to migrate from one to the other. There are quite a few thing is there. "obviously better" will depend a bit on what you're doing, and what your trade-offs are as to ease of debugging, available (std) libraries and how proficient you'll get in C++ (it's easy to write in-efficient code in C++ if you go about things the wrong way). "simpler", IMHO, C++ is harder on any level than C, you **will** fight the compiler on a regular basis (f.e. getting your code const-correct (and compile). In C++ you can not just simply cast things away, particularly CV-qualifiers). "cleaner", definitely! "elegant", usually, but it [the code] can also be quite (sometimes very) verbose, and depending on the compiler you'll be using, error messages can also be very verbose and regularly very confusing. I suggest to use Clang/LLVM if you can, it gives IMO, the best diagnostics. I learned C++ after C, and found it to be hard, mainly because of classes, just couldn't get my head around them. Many books start talking about virtual inheritance at some point, which just confuses the issues. Skip the bits on VI and use composition instead.
Hi, thanks for the comments! If you would like to get the result of a task, use the method without silent\_ prefix. This will give you a future which will be ready when the associated task finishes.
Thanks for the comments! Yes, we are currently working on some of these points. By default, Taskflow has its own threadpool implementation. Of course, supporting custom threadpool / executor is in out to-do list. Regarding task cancelation, we havn't done this at this moment because we believe the best way to implement this is at application level. Methods without silent\_ prefix will return a future which can be passed to different tasks. Task priorities are a quite interesting point, but this really depends on the dependencies of the graph. If the dependencies are too large, adding priorities make less sense. Having an event notification is a good idea. We will add this to our TODO list. I believe Cpp-Taskflow is now Visual Studio friendly.
God Eclipse sucks. Really wish Clion would up their game. Even though they are the best, they are still behind intellij in many ways. CUDA would be great, but a deeper refactoring ability that understands namespaces would really be nice. I'd reccomend just treating namespaces like intellij treats packages, with some special rules for anon and other c++ things. Additionally some prebuilt cmake files that do all the modern requirements for exporting libraries as a find target and so on, integrated package management, and a bit more work on helping users with the build system would be great. 
Why not just base it on the file contents? e.g. the compiler hashes each file it sees with `#pragma once`, and if it's already been seen, skips it. As an optimization, it skips even opening a filename that exactly matches an existing one and perhaps does some more clever things (checking symlinks, etc), only ever skipping files it can guarantee to be identical.
A giant banner linking to the latest versions of the docs should basically be considered absolutely mandatory for anything that has versioned docs.
Using `?` for boolean predicates and `!` for things that mutate the object comes from Scheme (which based that on the Lisp practice of a `p` suffix for the same thing). I too wish it had spread further.
Better name: `std::hook`
As dodheim wrote, it is too low level for most applications (usually you want to work on tasks that are then put on a thread pool) on the other hand it doesn't offer any of the customization points one might be interested in when you actually have to go down to that level (e.g. Priority or could affinity). Of course something like std::thread needed and in many cases still usable, but a bit too minimalistic for my taste. 
Despite all the work to fix it D is still a GCed language at its core. For my work that just wont do.
Strongly agree with this. CMake has an OK *reference*, but it badly needs a comprehensive *user guide*.
&gt; You just need to have a 3rdparty or vendor folder and do all your FetchContent from there to isolate them from the rest of the project. This still doesn't prevent variables leaking from parent to child does it? As a real life example, in all of my projects I have a cmake cache variable `BUILD_TESTS` which is set to OFF by default, and I turn it ON by passing `-DBUILD_TESTS=ON` at the command line when I want to build the unit tests for the current project. If I'm using FetchContent, and I call `add_subdirectory(vendor)`, then `BUILD_TESTS` is now going to be passed down to my library, and the unit tests for the project *AND the library* are going to be built when I call `make` (not what I want), unless I explicitly override the variable, which is also not what I want (safety and scalability). When I use `ExternalProject`, I don't have this problem, as I have to explicitly pass `CMAKE ARGS -DBUILD_TESTS=ON` for it to get passed to the child. &gt; And in theory, you could have options from your project leak into the other projects and bad things happen, but in practice, I haven't seen that happen a lot. Personally I find it difficult to comprehend how anyone can be OK to rely on a build system that can sometimes fail due to an accidental variable naming conflict. In the example I give above it's just an annoyance, but 1 in 1000 times it's going to introduce an actual bug. &gt; To answer the other question, EXCLUDE_FROM_ALL will prevent installation of 3rd party libraries too, which is what you want usually. This is going to prevent the third party libraries from getting built at all, which defeats the whole purpose of adding them with FetchContent. I just want to prevent them from getting installed, not built. As @craskit mentioned, setting `CMAKE_INSTALL_DEFAULT_COMPONENT_NAME` should work, but it's not ideal.
Not sure if that would really have solved the problem. I've only skimmed the description, so correct me if I'm wrong, but this probably only works if there is an exact 1:1 correspondence between boost and STL type/library. That ignores new language features like lambdas, variadic templates, constexpr, r-value references etc. or situations, where there is significant difference between boost and standard library version. Imho, many boost libraries could use a full rebase onto c++17 (at least c++11), but I understand that very few people would be willing to spend that effort.
Meanwhile all of r/rustlang is probably already busy demonstrating how Rust does all of this better. They'll be here to tell everyone about it, briefly
I agree with this. Building the project and importing the targets from its install interface also solves the safety issue that I mentioned in a separate comment.
I have a hard time imagining a scenario where you don't want to build tests for your app but do want to build tests for your 3rd party dependencies. In any case, it's not rocket science to unset all variables that are. Too ambiguous at the top level of the CMake script that fetches and includes everything else if you are really concerned about it. In practice as I said, it doesn't happen a lot. And no, the build doesn't randomly fails. But as always, the unexpected can happen when you upgrade a library with newer code. And that will happen whether you use a custom CMake build script or not. Since you probably do CI builds for upgrading any library, then you find any failures early and fix them before merging. It's going to be stable from there. As for EXCLUDE_FROM_ALL, it doesn't prevent anything from building, you just have to be explicitly build those libraries or targets that depends on them. In my project, I will have all the Boost targets available and just use the ones I want in my app, and everything is built as it should, unused Boost targets are ignored. "all" is just convenience, you can't expect to magically mean different things in different contexts, that would be terrible practice. Then, for install, a lesser known feature is that you can control which subdirectory you build. If you start from the root, it will install everything that's in "all", if you start it from a subfolder, you'll only get a partial install. I tested it with make, and I believe there's a way with Ninja too. 
&gt; I answered a fairly similar question on the CMake mailing list where I provided some source code Thanks for the reply, but the question you linked me to is kind of fundamentally different from mine. My question is asking how to get transitive dependencies to work at arbitrary levels of recursion in chains of libraries connected via `find_package()`. The question you link me to is using `add_subdirectory()` in module 3, followed by an explicit installation step of the files. The targets to install are set explicitly in Module 3 so it doesn't really contain any "transitive" dependencies in the sense that module 3 shouldn't be required to know about the existence of module 1 (only transitively through module 2). ``` target_include_directories(Module3 INTERFACE $&lt;INSTALL_INTERFACE:include&gt;) install(FILES $&lt;TARGET_PROPERTY:Module1,INTERFACE_SOURCES&gt; $&lt;TARGET_PROPERTY:Module2,INTERFACE_SOURCES&gt; DESTINATION "./include") ``` In my post on stackoverflow, I note that the call to `find_dependency()` seems to be necessary and sufficient to get transitive dependencies working at arbitrary levels of recursion, and this is the point that I wanted to verify. The use of `configure_package_config_file()` in your code was interesting, though it doesn't seem to offer any benefit in my case...? Though I do see that if I had some variables with absolute paths then it would be useful.
&gt; I have a hard time imagining a scenario where you don't want to build tests for your app but do want to build tests for your 3rd party dependencies. Yes that would be silly, the situation I'm describing is the opposite case. The variable is to prevent the "3rd party" tests from building, which wastes build time. &gt; In any case, it's not rocket science to unset all variables that are Yeah I could do that, or use super build pattern, both of which are pretty unattractive. My goal is for adding dependencies to be super easy and super safe... that means no unnecessary verbosity and no changing the directory structure of my projects to get around the shortcomings of the build system. As noted in a separate comment of the main thread, it's possible to use `execute_process()` to build the dependencies after cloning with FetchContent instead of doing `add_subdirectory()`, and this is probably what I'll end up doing. It'd be nice if it were supported natively though.
There are efforts to make things GC-free in your code. See Andrei Keynote for DConf 2018.
If you use FetchContent and add_subdirectory, you have a super build. It's just not in one mono repository. Just use EXCLUDE_FROM_ALL, other tests won't build, if the project creates 10 variants of their library, you'll build only the ones you depend upon. The other targets will be available on demand, but won't build unless you ask for them. A reason for a super build is to be super safe actually when it comes to ABI and make it easy to port to new platform. In a multi build system, good luck rebuilding it all with ABI breaking flags or sanitizers. 
I agree with josefx, IMO D tried to be a mix of sorts between C#/.NET and C++, and bet on the GC; it's better now, but not so long ago there were language features (not library) that needed the GC behind the cover, and you wouldn't even know nor was it documented, it gives me shivers, and I think it's not all fixed; that is a huge enough for me that doesn't make D appealing to me; D has a lot of interesting features on the surface, but some interact weird or unexpectedly with each other when you actually try to use them or write something real
I am going to give it a serious try I think at some point. Because the problems you mention are on the roadmap: https://dconf.org/2018/talks/alexandrescu.pdf
I don't see the D development process better than C++, designed by committee vs not argument aside, if I understand it correctly not anyone can just go and contribute to D, while that's not the same for C++, you can write a paper and start from there
GC-free was in D's reach for a long time, yet, it only took 15 years? 
&gt; A customization point, as will be discussed in this document, is a function used by the Standard Library that can be overloaded on user-defined types **in the user’s namespace** This is too ugly and the reason this problem exists is the above part in bold. What's so wrong with overloaded functions existing only in the same namespace the prototype is defined? isn't that valid for template specializations? template specializations can only exist in the same namespace as the prototype. 
As an aside, do not let yourself think of C++ as "C with some extra things". That will cripple your thinking and lead you to write bad C++ code. The extra facilities C++ provides allows you to think differently about how to solve problems and write programs. You need to think in C++. To do this, you need to understand concepts and models that C++ facilitates; not just the syntax needed to implement them.
^This is very important
[lol](https//medium.com/leaningtech/even-better-source-maps-with-c-webassembly-and-cheerp-d872276b7d3cmedium.com/leaningtech/even-better-source-maps-with-c-webassembly-and-cheerp-d872276b7d3c)
thanks …
I guess we understand "super build" differently. In my understanding, it's when you introduce a separate cmake build that handles the building and installing of your dependencies. e.g. quoting from [here](https://blog.kitware.com/cmake-superbuilds-git-submodules/): &gt; in CMB’s superbuild there is a concept of a developer mode where it builds everything but the actual project. It may then write some config file or similar to help the developer’s build find the dependencies that were built for them.
wow, of course I did that wrong
One of the reasons is operator overloading. In which namespace should operator overloads exist?
&gt; Not sure if that would really have solved the problem. I've only skimmed the description, so correct me if I'm wrong, but this probably only works if there is an exact 1:1 correspondence between boost and STL type/library. This seems to be relaxed (apparently to a point), as the description states: "An implementation is a Boost library which has a C++ standard library equivalent whose public interfaces are **nearly** the same in both cases." Emphasis added. &gt; That ignores new language features like lambdas, variadic templates, constexpr, r-value references etc. or situations, where there is significant difference between boost and standard library version. If you would compile with C++17, those features would be enabled, iff the Boost implementation also supported/implemented them, i.e. those libraries, which don't have move-semantics are not gonna get auto-magically move-assignment operators and move-constructors. &gt; Imho, many boost libraries could use a full rebase onto c++17 (at least c++11), but I understand that very few people would be willing to spend that effort. The Boost crowd is very keen on maintaining the library to continue support for totally obsolete compilers. The users, many corporate, often come up with stuff like: "I just upgraded Boost from 1.47 to 1.52, and I have problem xxx", while the current Boost version might be 1.68. So yes there's a lot of garbage hanging around increasing the maintenance load, which results in some libraries staying in the dark ages (maintainers also disappear of course, then they go into CMT, i.e. minimal fixes, just to kick the can forward a bit). I used to use Boost quite a lot, the only thing I use nowadays is boost::container::deque (I'm on VC), as the std::deque sucks, which should be fixed with the next binary breaking upgrade (whenever that is).
&gt; EWG previously approved basic support for using dynamic allocation during constant evaluation, with the intention of allowing containers like std::vector to be used in a constexpr context (which is now happening). This is an extension to that, which allows storage that was dynamically allocated at compile time to survive to runtime This is awesome. ___ Reading the proposed syntax for concepts: // abbreviated syntax void sort(Sortable{}&amp; s); // Herb 1 void sort(Sortable{S}&amp; s); // Herb 2 template void sort(Sortable&amp; s); // Bjarne's minimal template &lt;Sortable S&gt; // S is a value (non-type template) void sort(S&amp; s); template &lt;Sortable{S}&gt; // S is a type void sort(S&amp; s); I don't quite get the push for abbreviated function templates. Replacing `typename` with a concept name is the simplest thing I can think of. Adding another unicorn `{}` into function argument doesn't appeal good to me and the minimal syntax proposed by Bjarne will likely lead to tons of ambiguities - if not for the compiler at least for the human. Can't we just continue with well-established `template &lt;...&gt;`? 
Apart from all the guides people linked, I recommend this thing: https://en.wikipedia.org/wiki/Compatibility_of_C_and_C%2B%2B There is a quite big chance you will not read about something because "it was already in C" and expect C++ to be exactly the same in this manner. You probably know than in C++ `sizeof('a') == sizeof(char)` but you may not know all rules regarding implicit convertions and internal/external linkage. It's good to re-read them as the problems you encounter without prior reading can be frustrating to debug.
I get the idea and I'll do what you all are telling me. Thanks for the help :)
Hmm, https only.
No, there is another implementation not using Cairo. Also the interface don't lool like a Cairo wrapper anymore. It was just a starting point, stop focusing on that and look at the actual interface to criticize it.
Those are run-time checkers, while the ones I was asking about are all static (compile-time) checkers (e.g. the CppCoreGuideline checker in MSVC).
Thanks, that make sense. I am not sure I follow why do `string_view` and `span` need to be updated for this. So IIUC `string_view` and `span` are just `Pointer`s that at some point must be constructed from an `Owner`, so if I follow you correctly, they should just be able to use a `T*` internally just fine.
*cough* ... dogfooding.. *cough*
`constexpr` actually means `on the fly at compile-time` in C++
Maybe some pseudo CMake will make what I'm trying to do clear cmake_minimum_required(VERSION 3.9) # This would result in one Visual Studio project that would build a shared library objs.DLL, and obviously a.obj and b.obj while doing so add_library( objs OBJECT+SHARED a.cpp b.cpp) # This would use the a.obj and b.obj object files built in the objs project add_library( foo foo.cpp) target_link_libraries( foo "objs as objects") # This would use the objs.DLL library build by objs, and so have a run time dependency that DLL add_library( foo bar.cpp) target_link_libraries( foo "objs as sharedlibrary") The crucial point is that the a.obj, b.obj and the objs.DLL are built in one Visual Studio project not two. If I have to create two projects to do this I end up with too many projects in Visual Studio for it to be usable. This is of course a Visual Studio bug/performance issue not CMake so I am not saying CMake is broken just that the VS stuff it generates is problematic. 
+1 for hunter, conan is nice but requires python in addition which really sucks for windows users
&gt; conan is nice but requires python in addition which really sucks for windows users or you know, install python?
You are correct it is different. My confusion is that `find_dependency` isn't really doing anything special. It really just forwards the `REQUIRED` and `QUIET` flags, but the ability to have nested and recursive find_packages is part of `find_package` where it caches if a package has already been found. One of the bigger challenges of managing multiple levels of find packages is tracking down misbehaving packages that presume a dependency has been found (without finding it themselves ), and using some variable like `&lt;NAME&gt;_LIBRARIES`. These issues are the reason why I posted that link, as moving to targets allows you to use targets that don't exist yet. 
That proposal is so wonderfully straightforward, teachable, developer friendly and extensible for those with more advanced needs. It'd be a real shame if it didn't make it in just because its convenience can sometimes come at the cost of absolute optimum performance. I agree that `&lt;random&gt;` is much to verbose and complex for the majority of uses and I've already seen it used via copying and pasting from the web without any understanding of what the parts do. `std::randint` does exactly one thing, but it's a dead end. Eventually someone is going to need a random double and so they are left with a choice of either trying calculate their own random double from a random integer which is not exactly trivial to get right, or bite the bullet and try to learn `&lt;random&gt;`. `std::randutils` makes it slightly more work to get a random int than `std::randint`, but it leaves the door open to easily do some much more.
I'd go with the most popular, which is Conan. No matter which one you use, you'll probably be pretty disappointed. The good news is there may be an official package manager on the way, and at that point hopefully everybody will jump on board to support it :-)
Official? From whom? Isocpp? You've got my interest piked
Python has issues on Windows, especially 2.7. you pretty much cant go with the releases on the website if we want to have numpy,etc. Pip also isn't quite friendly to windows. You have to go with anaconda/miniconda, but that adds another tool and another package manager (conda) into the mix.
Okay I understand now. I don't expect CMake to have support for libraries that are multiple types like that. I misunderstood your problem as being not the number of projects, but instead the cost of having to build the object files twice. I know that other projects have this workflow where they want to either build tons of dynamic libraries or a single large library with a single build option. The general solution in those cases are helpers/wrappers around every `add_library` call to handle this.
&gt; Python has issues on Windows, especially 2.7. you pretty much cant go with the releases on the website if we want to have numpy yeah you can, you just need to go [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/) Don't use Python 2 - it's EOL soon
vcpkg on Microsoft GitHub is interesting.
There were some internal rumblings a few months ago that some members were considering arguing for a package manager to be introduced for C++20. The community at large seems to be in agreement that the most difficult part of C++ is build tools and a lack of dependency management, and there was a survey done by isocpp in which people mentioned dependency management and compiling and package managers as items they wished were easier. https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02 Hopefully something will get done soon! I think Hans Klabbers was considering leading the charge.
Anaconda seems to work well enough nowadays if you just need your numpy, scipy, pandas, etc. Any other packages not in that "distribution", or any custom stuff that needs C or C++ compilation, is a nightmare. Official MS documentation says "just use wheels!" 
This is a really great piece of work, thanks for the link!
Omg... good thing I went through everything in this CMake thread to notice this - PLEASE!!! DO THIS!!! and advertise it! FastBuild is AMAZING! CMake can not just follow the trends (of what build systems are mostly used) but even set the trends - by adding support for it!
&gt; If you would compile with C++17, those features would be enabled, iff the Boost implementation also supported/implemented them, I think you misunderstood me. I was talking about dependencies that emulate native c++11 functionality with a lot of template and preprocessor machinery. E.g. boost mpl (on which apparently almost every library depends directly or indirectly) afaik doesn't know anything about variadic templates and thus has a separate template type of map (and others) for 1,2,3....50 parameters. &gt; i.e. those libraries, which don't have move-semantics are not gonna get auto-magically move-assignment operators and move-constructors. As I said, I don't think, that compatibility layer would have solved the problem (even though it might have solved part of the problem).
I have thought of one. I even started it, but quickly had to abandon it because of two issues 1. The minor one is that idiomatic CMake would be hard to impossible to generate, making the distribution of compiled CMakeLists files questionable. 2. The major one is that generator expressions simply cannot work (well at least not without basically re-implementing CMake from scratch, which kinda defeats the purpose) For posterity, the rudimentary example code of mine is here: [https://github.com/iboB/crake](https://github.com/iboB/crake)
What's wrong with just giving the customization point a different (but related) name? Why do we need overload resolution vor this? 
&gt; I think that a perfect programming language would have different syntax for these different notions. C++ smushes both notions together into the weird beast it calls a “constructor.” Why do you think this? Can you motivate this in terms of performance, safety, or some other practical concern, or is this just taste? Is this a "code readability" thing for you? For me, if I don't use constructors then I can't use initializer lists, which I think typically harms readability and performance. 
Right, let's stick to the c++ way of doing things: Solving a simple problem by using the most complicated syntax possible.
&gt; E.g. boost mpl (on which apparently almost every library depends directly or indirectly) afaik doesn't know anything about variadic templates and thus has a separate template type of map (and others) for 1,2,3....50 parameters. This exactly what I wanted to express, but that didn't work. &gt; As I said, I don't think, that compatibility layer would have solved the problem (even though it might have solved part of the problem). What I was saying, it's no silver bullet (... does not auto-magically ... ).
&gt;fastbuild I am currently trying to upgrade the work of inbilla / packadal / comicfans / JeremieA for the Fastbuild generator to the latest CMake release. I'm doing my work here [https://github.com/colin-guyon/CMake/commits/fastbuild-upstream-3.11.4](https://github.com/colin-guyon/CMake/commits/fastbuild-upstream-3.11.4) but I still have several things to commit before it could be testable by someone else. (locally I still have 37 tests failing out of 464)
Thanks. We have updated a couple of places in the README. Feel free to give us comments and suggestions.
I agree. For Windows, vcpkg is so easy to use and has many many libraries available. For Linux, the distribution's package manager already does a great job (even if vcpkg is available for Linux, it's not yet as complete as the Windows version). For macos, brew seems great but I have not tested it extensively. 
Thanks. In fact, we used TBB a lot before developing this library. One problem we found is the API. TBB's API is more complex but more powerful in determining detailed parallelism. The goal of Cpp-Taskflow is to offer a tiny and loadable library for most common use cases. I am not sure if I understand your nested loop. Are you talking about dynamic tasking? Currently Taskflow takes static graphs, where nodes and dependencies must be decided before use.
Thanks for the link! Do you know if wheels are compiler independent? I remember seeing a link with similar packages, but compiled using mingw. If in windows, I'd ideally like to use the native vc compiler
We use vcpkg on Windows it's really great. We start using it on Mac, to port a a project already using it on windows this week. We haven't yet enough return of experience to recommend it on mac but it works.
I write python extensions in C++ on windows with MSVC, and I use these distributions, and have had no compatibility problems
Question mark after the `s`
that's exactly the problem. The less things I ask my users to install in order to be able to build, the better they feel
no, it's a pain for non-technical users who want to compile the latest commits. Installing CMake is already hard enough, I don't want to have to add stuff.
Thanks!!!
There is no standard one. There are some package manager already, but each of them are not compatible with each other, and their script and usage are totally different. That is a headache I guess. Who knows which one will be abandoned in the future. 
&gt; official package manager SG15 have not even decided yet on the scope of their work: should it be TS? "Official" package manager? How should it integrate with external tools? It's too early to predict anything. 
&gt;may be True all that, I don't even know if it'll happen. I have a feeling it'll happen pretty soon though, since Titus has previously voiced his opinion on the abseil blog that there needs to be some dependency management solution to stop the "this is chaos." For me the question is will that happen before I can switch all my work over to Rust.
Yeah, the author is trying to fit the concept of a constructor within the frameworks of object factories and type conversions. But a ctor is neither. Instead it is a description of the memory-initialization arguments. A constructor's argument list is an implicit description of the state of the object after construction. It isn't an explicit statement of instantiation of anything (that's the class token itself, with either named or unnamed syntax) nor is it expressing a type conversion (the implied meaning behind a ctor argument may be unrelated to the type of the class entirely, like go look at socket p_socket and read the information there). Why flatten the abstractions? They require good design to be effective, but that's not a reason to change them.
It requires years of work. Optimistically one may think about some kind of TS in C++26, but still. Existing package/dependency managers are pretty young, got some popularity only recently, they yet to grow numbers of packages. I'm quite happy with current solutions such as Conan/Vcpkg, both follow slightly different philosophies, both get the work done, but in no way inferior to each other. Basically, "walled garden" solutions already work perfectly, for OSS part they need to grow a bit, it's all about the number of contributions, though one C++ expert can migrate his favourite projects to them. 
&gt; C++ &gt; non-technical users You're doing something wrong
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8stm7k/migrate_from_c_to_c/e140p5k/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
OK, I suspected as much, thanks for the feedback. I should have been clearer that the doubling of compile time was a result of Visual Studio not CMake. I just tried the new target_link_libraries object library support and it all worked first time. Will make some of our CMakeLists.txt files a lot simpler so great job!
Conan is my favorite out of the ones I've tried. Vcpkg: Pros: * Easy to get going (git clone then bootstrap it) * Easy to use existing packages (vcpkg install foo, plus the integration is nice for Visual Studio) * Non-intrusive CMake integration * Not too difficult to add new packages if you need them Cons: * Builds everything from source. If I only want Boost as a header only library, I still have to build it, which takes 30 mins to 1 hour * I haven't seen an easy way to request a particular version of a package Conan: Pros: * Integrates into every build system I've ever used * Recently got non-intrusive CMake integration * Gets pre-built packages by default, though you can request to build from source. Getting Boost took around 2 minutes, the shortest of any of the package managers I've tried Cons: * Requires Python, although this was a non issue for me * Non-intrusive CMake integration is still somewhat intrusive by default, due to it coming much later, so existing packages weren't aware of how to be easily non-intrusively consumable from Conan * Doesn't seem as easy to add new packages that you don't own, but doesn't seem that hard either Hunter: Pros: * Easy to use with CMake. The command to find dependencies mirrors find_package * Designed with CMake as its only target, which means that it adheres to good CMake practice with its generated targets Cons: * Only supports CMake * Intrusive CMake integration * Adds several seconds at a minimum to CMake's configure time * AFAICT, adding new packages requires submitting a PR * Builds from source. Boost again takes a long time * I couldn't figure out why it rebuilt my packages for every project. I ended up with 3 copies of Boost from using Hunter * AFAICT, packages can only be installed when you run the configure step. There's no way to install packages beforehand
&gt; Yes, I know this is basically Rust traits or C++0x concept maps It's almost as if... they chose those designs for a reason...
Thanks for picking up this work
Thanks a lot! This is exactly what I was looking for. The nicest thing was the boost comparision. Conan seems to be the best bet then :)
Artists in game development maybe?
Interesting article as always. &gt; Their presence is the customization. And since X can do new thing with them, I call them “skills”. &gt; Happy to discuss this naming point further in the comments section below. I think WTL calls them MixIns, unless I've gotten the concepts confused. I also *vaguely* think they might be called veneers in Imperfect C++, but I don't have the book to hand and it's been a long time since I read it.
So this issue is actually a lot simpler and can show up on simple parallel for loops. Imagine you have an object that has a lazily-evaluated cache variable. That cache variable is expensive to compute and requires parallelism and locks a mutex internal to the object when it is being computed. Now imagine that you are in a parallel graph (or just a parallel loop) that triggers the lazy evaluation. As expected, only one thread gets into the evaluation. However, with TBB, that thread can context switch out back to the outer tasks. Then, it can go back in and try to reacquire the mutex resulting in deadlock. With TBB, you can use task isolation or make sure that internal computation has its own "task arena", so it doesn't context-switch out doesn't happen.
And there never will be a standard one, just like there isn't a standard compiler or standard build system. You don't standardize tools and no tool will be around forever. The best you can expect is a defacto standard, must nothing will gain that status if people don't start using the available tools and decide what works best for them.
Upvote for actually giving credit to Modern C++ Design, one of the formative works exploring these exact sort of design techniques.
If you can live with Mac and Linux only nix is a great package manager. 
 vcpkg install boost-libname
they shouldn't have to deal with any of that directly, and if they do they will learn regardless
 Don't take the following as 100% truth, but what I remember without digging into CMake. &gt; Would it be possible to make a change where you make a per-project install target So for the makefile generator we don't have a global install target but we have an install target per directory. While for things like Ninja we just have a global target. I think for all generators to have a hierarchy of install targets that people can control will require real work redesigning how we generate some underlying build files. &gt; On an unrelated note, has any thought been given to adding some sort of automatic CMAKE_MODULE_PATH Nope. That is just one of those pain points that we have all gotten used to. It would be really cool if this information could also be exported seamlessly as that always trips people up to. 
FYI: on vcpkg you don't have to install full boost - every library has it's own package (I don't know if there is a meta-library that only gives you the header only bits though)
Depending on what your goal is, the main drawback of Linux system packet managers are the often outdated packages
You can request a specified version: https://github.com/ruslo/hunter/wiki/example.custom.config.id However, your dependencies versions are already locked when you pick a hunter version. Until you upgrade hunter for that project (manually), it will keep the same versions. The mechanism above is if you want to keep the same dep version even if you upgrade hunter. As for adding a package, sure, you need to open a PR if you want it in mainline hunter, but you're also free to use a forked version of hunter for custom packages, it's not even very hard.
Not sure why people keep saying C/C++ lack a good package manager. We have several great ones with thousands of well tested C/C++ libraries, for instance: apt-get, pacman, homebrew, dnf, urpmi, zypper, pkg, nix.
Apt-get install libbost-1.65-dev for example can't be done on older versions of Ubuntu. The idea is to not use system package managers which update their repository list once in a while, but to use an independent one that can update for frequently. Also, I would have to write a metal script to make it cross-platform. Further lines of code if I want to handle dependencies of the package manager. Something that's universal and well maintained would be appreciated :D
What about [a mixture](https://godbolt.org/g/EM1CX6) of the two approaches? struct with_size { int size; }; struct vector { explicit/*?*/ vector(with_size size); }; struct my_class { vector my_vec_; my_class() : my_vec_(with_size{5}) {} }; I'm not sure what *I* think of this; it just sorta occurred to me (though I know the "build an object instead of passing a dozen parameters" is a design pattern), but at first glance I think I like it. Obviously you'd want a better API here, e.g. make `with_size(5)` and not just `with_size{5}` work; the above is just sort of a "the least I could type" version. :-)
well, they're the ones who ask me to be able to compile the stuff. it's a DAW.
&gt; they shouldn't have to deal with any of that directly, they want to &gt; and if they do they will learn regardless from experience in 4 years, they don't. 
And distributions that split what should be a single package into 4-5 separate ones (cough Debian cough)
pkgsrc is good.
You can always choose a distro that keeps stuff more up to date. I agree that if you're on Debian Stable (for example) it can get pretty bad if you don't upgrade. But if you use a distro that keeps up to date it's usually not a problem.
Thanks for letting me know. I guess that makes sense. I didn't want to fork Hunter just to add a couple of missing dependencies so that I could have the dependencies for a particular public repository I was working on. It was also unclear to me how it would work with HunterGate if I did that.
I think there is a package for the header-only part. It's nicely modularized on vcpkg. In any case /u/Quincunx271 must have a bit an outdated CPU if it took 30-60 mins to build boost. It really shouldn't take more than 10-20, maybe 20-30 if you build *everything* and because vcpkg builds Release *and* Debug builds (doubling the compile time). 
I think the author over-generalizes a little. The size-constructor of vector is problematic, and we would have been better off without it, but that problem is pretty much unique to vector: it doesn't occur in the other containers, never mind in normal application-level classes. Trying for some kind of weird (and in my mind rather vague) classification scheme with associated rules is just not adding a great deal. I also disagree with the idea, stated somewhere in the article, that a default constructor is harmful: empty vectors are not something you can do without. Quite frankly, this kind of sweeping generalization is in itself harmful: yes, not everything needs a default constructor, but this should be decided on a case by case basis, not turned into some kind of twisted joke of a coding guideline. As happened with inheritance, member functions, exceptions, and for-loops, all of which are now regularly argued about in this forum and others by people who seem to be parroting one-line summaries of hour-long talks without ever bothering with the finer nuances. 
Ever looked at the version numbers of Boost, OpenCV or Eigen, in Ubuntu 16.04? Yea no, not usable, unfortunately.
Good to know. My reason to skip the idea of learning CMake DSL (wrongly looks like) is that I expected the border between language and system is blurry. If you define a language, I basically expect you can hardcode all special cases, so that language can't exist without build system and vice versa. And the same, one can't be understood without other. Of course, it's "bad design", but it takes more time to design things well, so it's very often the case with DSLs.
In theory maybe. In practice, there are a lot more factors that govern the OS selection Process.
Ha that seems to happen all the time. Even on experienced teams. It stinks that it's a manual step. That reminds me that I should write a git alias like `git update_all_the_things`.
I mean sure, they should be *capable* of doing this stuff, but it's still a hassle to do it and do it correctly. I'd rather the finite resource of brain power be used to deal with all the C++ intricacies and not build system details.
It requires a lot of code as boilerplate, and I don't promise it's very good code, however, it does end up being quite pretty in `main`. Sorting from multiple arrays: (it actually works with any random access iterables) https://gist.github.com/ubsan/b19c470028edd2d9c8bbcbb2f8fdce6e I couldn't give a compiler explorer link because it's too long, however you can copy-paste into compiler explorer if you wish.
Those are Linux-only. And it is my opinion that it is bad form to install development headers into a system header directory anyway: it's either part of the user data, or of the compiler installation, but the OS has nothing to do with it. They will not let you install older libraries (or just multiple different versions for different projects), and if it hasn't been blessed by the distribution there's nothing to install anyway. Maybe it is a good solution for system headers, but not, I think, for general libraries.
If you're curious about the original D code: void main() { import std.stdio : writefln; import std.algorithm.sorting : sort; import std.range : chain; int[] arr1 = [4, 9, 7]; int[] arr2 = [5, 2, 1, 10]; int[] arr3 = [6, 8, 3]; // @nogc functions are guaranteed by the compiler // to be without any GC allocation () @nogc { sort(chain(arr1, arr2, arr3)); }(); writefln("%s\n%s\n%s\n", arr1, arr2, arr3); } We, of course, get nogc by default :)
It's 5-6 years old, so maybe a bit outdated. Also has a HDD rather than a SSD, which I've heard can have a notable impact on compile times.
Excuse me, but it _is_ standard C++. A header name must consist of a sequence of characters from the source character set minus the terminators and newline. The listed name qualifies, and as such is proper C++. Of course correct interpretation relies on the underlying OS having a proper http filing system available. 
With designated initializers (C++20) it could look like: struct with_size { int size; }; struct vector { explicit/*?*/ vector(with_size size); }; struct my_class { vector my_vec_; my_class() : my_vec_({.size=5}) {} };
In this context, I actually rather prefer my solution for several reasons (actually I rather prefer making `with_size` a function that returns an object of unspecified type), but sure; I think to some extent that's bikeshedding, and your way solves the same fundamental problem with a solution with the same fundamentals. I think your way though works better when there are a variety of parameters and you may only want to specify some of them, or perhaps if you want to help with ambiguity in either order (e.g. is it `x1, x2, y1, y2` or `x1, y1, x2, y2`?) or meaning (is it `x1, y1, x2, y2` or `x1, y1, w, h`?).
I completely agree with you. In fact, I think the biggest challenge for parallel programming is not itself but the dependencies among tasks. This is exactly what I think needs to boost in existing frameworks. You may find this library [Cpp-Taskflow](https://github.com/cpp-taskflow/cpp-taskflow) interesting and helpful for building parallel programs with task dependencies.
Oh yes! My condolences then on having to work with a HDD. You should upgrade, like, you should've upgraded 5+ years ago. It will be like heaven, I promise. True, modularization came later. I'm not sure - maybe `boost-core` is the one? If I install it it only wants to pull in the following 4, so it looks really lightweight: boost-assert[core] boost-config[core] boost-core[core] boost-vcpkg-helpers[core]
Yeah, it mostly shines with multiple parameters. Especially for boolean flags. No more of this: open_file(true, false); // WAT open_file({.read=true, .write=false}); Also with a little less namespace pollution. struct vector { struct with_size { int size; }; explicit/*?*/ vector(with_size size); }; struct my_class { vector my_vec_; my_class() : my_vec_({.size=5}) {} };
The specialization approach does not work because you cannot limit it to be used for your own types, which introduces coherence issues (two libraries specialize it for the same type, and a third library tries to use both).
Fair point :)
[Cget](https://github.com/pfultz2/cget) is pretty easy to use, and it can install packages directly from cmake, boost, meson, and autotools. You can just point it to the source tarball url and install. There is also recipes available for installing commonly used libraries(like curl).
If we allow range-v3, the whole thing is pretty short: #include &lt;range/v3/view/concat.hpp&gt; #include &lt;range/v3/algorithm/sort.hpp&gt; #include &lt;array&gt; #include &lt;iostream&gt; int main() { auto arr1 = std::array{4, 9, 7}; auto arr2 = std::array{5, 2, 1, 10}; auto arr3 = std::array{6, 8, 3}; ranges::sort(ranges::view::concat(arr1, arr2, arr3)); ranges::copy( ranges::view::concat(arr1, arr2, arr3), ranges::ostream_iterator&lt;int&gt;(std::cout, "\n")); return 0; } Doesn't work on MSVC though :/
I agree with this. Perhaps worth noting is that I've never seen anybody using the "reference implementation" of the proposal as a standalone library now that it is available. That seems to suggest that there isn't a ton of pent up demand for exactly that, or I have to imagine we'd see adoption of it regardless of whether it was a standard, and then a vocal user community looking forward to no longer needing to use it as a standalone library...
I've looked at this before. Does it automatically detect the build system (cmake, meson, autotools, ...)?
I see. This sounds very similar to one thing we are trying to do now for a nested task parallelism. Inside a particular task, there can be another task arena which allows "child parallelism" without breaking "parent parallelism". As you can see, this might require another mutex or locking mechanism to work out. Still thinking what would be a good API to do this. Thanks for your great feedback!
Does the standard guarantee that the same `#include` resolves the same? In other words, if I `#include &lt;something&gt;` and then `&lt;something&gt;` also contains `#include &lt;something&gt;` -- am I guaranteed to get that same `&lt;something&gt;` back? :-)
An important case is when you are implementing an abstract interface with your CRTP mixin. Then their signature looks like: template&lt;class D&gt; struct Mixin0: Interface { D* self(){ststic_cast&lt;D*&gt;(this);} virtual void foo() override {} }; Now if we have N of these, we either have to `virtual Interface` inherit, or chaim them: template&lt;class D, class Base&gt; struct Mixin0: Base { // ... struct Derived: Mixin0&lt; Derived, Mixin1&lt; Derived, Interface &gt; &gt; which gets verbose. We can improve it with a helper: template&lt;class D, class B, template&lt;class...&gt;class... Ms&gt; struct Mixins_helper{ using type=// TODO; }; template&lt;class D, class B, template&lt;class...&gt;class... Ms&gt; using Mixins=typename Mixins_helper&lt;D,B,Ms...&gt;; Then: struct Derived: Mixins&lt; Derived, Interface, Mixin0, Mixin1, Mixin2, Mixin3 &gt;{}; and have `Mixins` chain them. template&lt;class D, class B&gt; struct Mixins_helper&lt;D,B&gt;{ using type=B; } template&lt;class D, class B, template&lt;class...&gt;class M0, template&lt;class...&gt;class... Ms&gt; struct Mixins_helper&lt;D,B,M0, Ms...&gt;{ using type=M0&lt; D, Mixins&lt;D,B,Ms...&gt;; }; 
&gt; Why do you think this? Can you motivate this in terms of performance, safety, or some other practical concern, or is this just taste? Is this a "code readability" thing for you? I think one benefit for thinking about this is determining if your constructor does too much and what should be decoupled, optional, etc. As one example, I sometimes see junior devs have their constructor read from a file to initialize the content out of convenience for their immediate need. This reduces testability because you now need a file on disk to test your class where even in tests where you might not care about the IO. In addition to testability, another reason is the code might evolve and you might want to directly instantiate the object without it loading from something else. It helps to decouple these concepts but it can be convenient to still provide those other "constructors". Rust is an example of a language where these concepts are separated by convention. Rust has no native support for constructors. There is the `Into` trait for conversion (plus some more specialized ones). People also write 1+ "named constructors" (member functions that initialize struct members)
The difference between type conversion and construction: struct Image { explicit Image(char const * filename); }; Image a("foo.jpg"); // construct a = (Image)"foo.jpg"; // wtf? convert a string to an image? https://godbolt.org/g/HDuHgU 
I think it's very useful if everyone who's working on a project can at any point build and run the game from the current commit without having to download the build from an external source. It's not very complicated if you have a batch file or something that they can just double click on to build everything. It just comes down to having all the build dependencies installed, so it's a pretty good thing if the procedure to install those dependencies is as simple as possible.
It also shows how C++ version of ctfe is a hack. D version seems to be using standard sort algorithm.
An option if you are just using the package manager for your own projects is to use cmake's new fetch project capability. [https://cmake.org/cmake/help/git-stage/module/FetchContent.html](https://cmake.org/cmake/help/git-stage/module/FetchContent.html) I used this to create a build system where all of my components (libs and binaries) pull and build their own deps (and so are independently usable).
Not yet, but that is coming soon. For now you have to explicitly say the build system if its not cmake(ie `cget install -X meson &lt;url&gt; `)
One area where this could be difficult is with inherited classes. If you inherit a class publicly, with the function customization points, it still works. With this way, you would have to specialize the template again.
&gt; Non-intrusive CMake integration I wouldn't list this as an advantage. It is IMO one of the biggest reasons against vcpkg. While it may be reasonable to not tie the build scripts to a particular package manager (though other languages use their package managers just fine), `find_package` or just having everything implicitly available is a maintenance and reliability nightmare.
What's so bad about this? I hate when things like Qt or Boost are packaged as one huge package. One rarely needs everything.
PR's are welcomed!
Unfortunately it is mostly relying on the fact that markdown accepts HTML. You can do tables in markdown, but it has to look like an ascii table - and that makes it really hard to edit code (ie having two functions side-by-side, changing a line of one function moves stuff on the same line of the other function). To do tables where the contents are not side-by-side, you need to just do html.
Seconding conan! I just started using it yesterday and i love it. The boost support is still iffy, and CMake integration will fail with it, since it's all modularized theres no boost root and scripts using find_package(Boost) fail. Recipes have to replace that from cmakelists.txt. I've tried hunter before but could never figure out how to use it, the documentation is quite lacking, and the packages don't seem to get updated much.
Never had any issues with wheels. and almost everything uses wheels these days so you don't need the unofficial ones anymore, including recent numpy. [Pypi has 2.7 wheel files too.](https://pypi.org/project/numpy/#files). What issues does `pip install numpy` give you? It should work fine, wheels are there. When did you last try it? I know it used to be it wouldnt work properly, but that was awhile ago.
"C++ smushes both notions together into the weird beast it calls a “constructor.” " The Haskell fan in me thinks: that beast is called "tell me how much memory you need", now please notice c±+ is not a declarative language and cut the crap (sorry for language, but it is necessary here)
Conan has installers that come with python interpreter bundled and transparently used. If no installer exists for the system, still an executable can be generated with the [pyinstaller.py](https://pyinstaller.py) file that is at the root of the conan repository, that will also embed python.
&gt; my_class::my_class() : my_vec_(decltype(my_vec_)::with_size(5)) {} Oh god please no (also, how is it not DRY?). It also turns out that readable syntax for this already exists: my_class::my_class() { my_vec_.resize(5); } 
Yup. Its probably the greatest obstacle for adoption from C++ crowd. Its unclear how good language would D be without GC though.
`nano::sort()` is [my version](https://github.com/tcbrindle/NanoRange) of what will become `std::ranges::sort()` in C++20.
I'm unsure what you are referring to. To use the non-intrusive CMake support properly, you'd write `find_package(PkgName)` (with appropriate other args). How is `find_package` a maintenance nightmare? It's the recommended modern way of declaring your dependencies.
Assuming you work for a someone that cares about money, it's trivially easy to show that the cost of an SSD (and even NVMe) pays itself off in the *very* short term. Build times are orders of magnitude better, so saved developer time quickly out-values the cost of the drive. I've measured/demonstrated this at multiple companies. Seriously, just get them to get you better machines.
Mixin is the proper name for such pattern.
You are suggesting conan while counting its problems. I’ve got news for you, I am using vcpkg (in both private and public fashion) and haven’t had single failure yet ;)
Honestly, I think vcpkg is the best, but it's clear to me that it's not yet mature on non-Windows platforms. That being said, if you don't require much, it may still be a good fit for testing. I have the most experience with this one, but I also like Hunter.
Based on the style, they *do* seem similar to the concept of mixins in other languages. 
A mixin gains functionality from the base; CRTP gains an interface from the base while the derived supplies the functionality.
I didnt say it was perfect. and that one specific problem is hardly counting it's problems? And it's really more an issue with boost/the boost recipe than conan. The maintainers for it decided to modularize. conan didnt do it. i could make my own boost recipe that downloads the whole thing just fine if i wanted.
I am starting to see. Initially, I just need it to produce the build deliverables: hex file, map file and a build log. &gt;How do you expect users to consumer your project? The user takes the HEX file and loads it onto his embedded board using a bootloader built into the target. &gt;In general you should't consider docker to be a build system, but a way to have a collection of consistent reproducible build environments It sounds like I need to use docker for getting the cross-compiler and its libraries set up, which is a royal pain, and difficult to get bit-for-bit identical across the different OS platform setups. Then use CMake to manage the different actual compile configurations that I need to support (like when I want to launch `make` with different -D options so that conditional code builds according to preferences.
Not that I know of. In practice it would be a bit of a mess if it didn't; there are a lot of common practices followed by compilers in this area that everyone relies on but aren't codified in the standard. Maybe we _should_ allow inclusion and linking of resources from the web. That would certainly put an end to all those package manager concerns: no need to download, build or install anything anymore, you just specify the resource in its original repository directly! (and yes - I'm aware of how horrible that would be. It's a joke, ok ;-) )
Some of the challenges are at the top of the post. People chose randomly so only hexdump from that list is there. But you can take a look I guess :)
&gt; that problem is pretty much unique to vector: it doesn't occur in the other containers It occurs in 4 standard containers, and those that want to be consistent with standard containers.
&gt;propose it as a Boost Library... Hmmm ... I'm not sure what "it" refers to. There are numerous proposals just in the responses here. * svg renderer * html canvas * some sort of primitive/backend implementation such as direct X * I don't see a widget set. Convincing something like wxWidget to be part of boost. I'm sure there are lot of other ideas. To me they're all so different they could all be boost libraries and there still wouldn't be a conflict. The beauty of Boost is that there doesn't have to be a consensus ahead of time. If anyone thinks they have something of value, all they have to do is to get enough of it working to convince reviewers that it will be useful, that it is finighable, and that the submitter has the wherewithal to actually finish it and maintain it. Then they just have to be responsible for following through. No need for huge discussions. Only down side is the possibility that one can't make a convincing effort and suffer the consequences of rejection. So - get to it! Robert Ramey
I think lifetime checker != bounds checker. The job of the lifetime checker should be to only check for: 1. use after free (accessing an object after its lifetime is over) 2. not leaking memory (an object's lifetime can no longer be terminated). 
&gt; Is your constructor an object-factory or a type-conversion? No, neither. Constructor is constructor, it's used to construct an object. You can use it to implement object factory or type conversion, but that doesn't mean constructor is either. There are other ways to implement object factor (singleton) and type conversion (type conversion operator). 
The difference between D, Rust and Nim for me personally boils down to: if you know C++, the easiest to pick up is D and D is more mature than the others at this point. I like Nim quite a bit also, it is just that the D style feels right at home for a person that has used C++ for a long time. Rust... gives me mixed feelings. Good language but overcomplicated for most tasks sometimes. But it is a good language if you need guaranteed safety without GC.
So, uh... what's a source map? I was expecting some "look, I made this code analysis thing, it works like this", but instead got drowned in Rust in JS implementation details of some... thing.
Git submodule
boost-core ist a boost library that is mainly for internal use. That won't help you much. But I tend to only install the boost libraries I need anyway.
Thanks for the feedback. At the beginning I point to the original post by Mozilla that explains the context more. I didn't want to do it twice, also because the article was long already, and my focus was in using Cheerp and not on the algorithms themselves. 
Conan gives you modular Boost too - in two forms. Either get the package from conan-center and disable unneeded modules using options or get separate packages from bincrafters. 
Sure, but with binary distribution that is probably less relevant anyway.
Do you really expect the committee that is already overload with bringing the c++ language forward to standardize another language for exprressing build scripts and/or packet formats? Certainly not anytime soon. BTW.: Such surveys have been made for quite some time.
Can you elaborate? 
Right, and Rake is pretty nice for Ruby.
Downloading the full binary distribution of all boost libraries only takes a few minutes/seconds, so why would you only want to install individual libraries. If you have to build them you can save significant time by only downloading and building the libraries you need.
Why do you think it's good to cater to mentally ill people? Why not go further and think if something else about that post can offend some people.
Definitely. But on the other hand, you should have a CI anyway, so people can just get builds from there.
Yeah absolutely. I think having both options available is the best.
We evaluated conan, buckaroo and hunter when we selected a system at plex. Ended up with conan because of the flexibility and haven’t regretted it. I recently did a talk about our (complicated) environment (we have a lot of deps building for a lot of targets) at SwampUp 2018. You can probably find the video on YouTube (don’t want to link it directly if since that might run afoul of rules).
special shootout to ngladitz, he helped me so many times on IRC
&gt; Not too difficult to add new packages if you need them I'd strongly argue against that. This is only true if the package you want to add already ships with CMake, otherwise you have to rewrite its build system in CMake. The same goes for Buckaroo.
I think it's fine to post a link to your own talk in this sub, but here's the link for ease: https://www.youtube.com/watch?v=jKG6cETLN3M&amp;feature=youtu.be
I build http://teapot.nz and I use it in several commercial projects. It's designed around immutable repeatable builds, from source, with versioned packages (including packages for build rules).
We need a cross platform build dep solution for our cross-platform language. Even if you're adept at building native packages from scratch and creating PPAs, it's still several times the effort compared to any one of these package managers. It wouldn't be so bad if it was one OR the other but invariably a mix of the two is deemed reasonable at the time and now you get to write your own bootstrap layer to integrate and verify the native versions and the ones provided by the c++ package manager.
&gt; at it's core its* ffs
i think his point is there's not even a de facto standard in C++. 
Thanks for the overview. Additional question: Can I have a local repo in addition, e.g. so I could place company internal libraries there? 
This isn't rocket surgery, I am specifically referring to the implementation for P0267 which is already finished: https://github.com/mikebmcl/P0267_RefImpl
To quote myself: &gt; but nothing will gain that status if people don't start using the available tools and decide what works best for them.
Fairynuff 
Conan and vcpkg let you do this. No idea if Hunter does
You could almost argue the opposite: this will allow you to selectively implement customisation points (or not) for derived classes: struct Base { ... }; template &lt;&gt; struct ::some_iface&lt;Base&gt; { // implementation }; struct Derived : Base { ... }; template &lt;&gt; struct ::some_iface&lt;Derived&gt; : some_iface&lt;Base&gt; {}; 
The evangelism on what is just another stylistic choice is getting tiresome. 
Never.
&gt; Python wrappers for SWIG Bless your soul, brave warrior. If you get a chance, maybe try pybind11 with a different auto-generator (maybe roll a quick one based on Clang?)
That pretty much only works for ints... `Widget const w;` Widget constant? Constant is a quantity, as you yourself showed.
West const all the way
If I could personally choose, I'd write them with boost::python. But alas, can't do that xd
If we are rewriting all the code to move `const` around then we should probably consider two other alternatives: * Both const (universal const) ``` const int const i; ``` * North const: ``` const int i; ```
Reppin' East Const till I die, yo. Throw up your span signs.
I hate (blog) posts without a date. Or am I blind? In any case, wasn't this posted already a couple weeks or months ago?
I have not, I upped us to 3.10 a few months ago - I'll give it a shot next week. Thanks for the tip. It's a mix of static libraries, shared libraries, interface targets, custom generated targets, and executables. It's a fairly large repo so the dependencies are heavy. We can't split into individual components for reasons....
If you think boost documentation &gt; cmake documentation I'm honestly speechless.
You mean `const West`.
South-East `const`: ``` int const* const p; ```
Heresy! Easy to const is the path to enlightenment. 
I thought this was tongue-in-cheek... 
I think part of the appeal is that the adjective goes first like in English. "const west" would be the east way to say it. ;) 
You've got it!
Conan has local repos in Artifactory Community Edition (CE) for C/C++, which is a totally free edition, you can host in your company servers to have your private conan packages there: [https://conan.io/downloads](https://conan.io/downloads)
Good point. I guess that one of the reasons for it not being widespread is that without P0665, it is very verbose to specialize the structs in namespace std. I like how this technique generalizes to both functions and aliases. 
There won't be "an official package manager". Not soon, probably not ever. The committee is not in the business of putting other companies out of business. If anything, like having multiple compilers, there will be some kind of standard dewscription (or even just recommendations) which will allow a number of *compatible* package managers and build tools. Some day. Not soon.
You know what's most irritating to me? It's when "the other side" considers it a stylistic choice when I consider it a logical conclusion. What am I supposed to do with that frustrating sentiment? I know some people are going to see this is arrogant but in my mind, it's as arrogant as claiming there is no choice to be made on whether the earth is flat or not. See what I'm saying? I didn't choose my side. It was self-evident.
You write some code in C++. You run it in the browser. Then you open the debugging tools and debug it. You get the WebAssembly in the debugger. A source map allows that to be translated back into the original C++. In the debugger. It's been used for almost a decade now by various compile to JS languages, to do the same thing when you want to debug.
why not defines types lisp-like (* (const (int)) x; ... oh wait, we can std::add_pointer_t&lt;std::add_const_t&lt;int&gt;&gt; x;
I feel like you are implying Conan does only binary distribution, but it does a source one too.
That wasn't my intention.
Absolutely serious answer: you better learn using google for such things if you want to learn c++ or any programming language. 
And even the stuff that *is* documented, maybe even well, is inconsistent. Some things list their properties and what they mean in one nice place, others spread it throughout several paragraphs and function signatures and make them hard to find. Some have a nice TOC at the top, like [add_library](https://cmake.org/cmake/help/v3.11/command/add_library.html), others like [add_executable](https://cmake.org/cmake/help/v3.11/command/add_executable.html) don't. Writing Find Modules is still undocumented AFAIK. Toolchain files are still lacking documentation on how to write.
Getting a Udemy course would probably be a good way to learn 
True that, but I first wanted some advice or tips from people who know what they're talking about before doing so! 
&gt; learn C++ during the next two months How much C++ do you want to learn as two months isn't very long to learn any language let alone C++ If you're on Windows then Visual Studio Community is my recommendation. For books pick up C++ Primer. 
&gt; Titus has previously voiced his opinion on the abseil blog that there needs to be some dependency management solution Titus has also said many times over publicly that everybody should just statically link everything always. Starting from that position is an uphill battle, because shared linking et al is a non-negotiable requirement for many. &gt; For me the question is will that happen before I can switch all my work over to Rust. Rust has the opposite problem (the same that NPM has). There are just too many damn packages because they refuse to commit to having a proper, rich standard library. Any non-trivial Rust program depends on tens of crates and pretty soon you discover that your transitive dependencies include ten different libraries for parsing command line arguments[1]. [1] Not an actual scientific number, may contain exaggeration.
&gt; What am I supposed to do with that frustrating sentiment? Think more about creating actual business value for your paying customers.
And this question was asked and answered like a few 10'000 times.
Oh, the evangelist strike force strikes again. \- [http://nosubstance.me/post/constant-bikeshedding/](http://nosubstance.me/post/constant-bikeshedding/)
Annoying for me is those one that have gone one step further and [turned style into logics](http://nosubstance.me/post/constant-bikeshedding/#conclusion).
I stick to west-const b/c the well known east-const "logical arguments" simply sound shallow, flawed and restricting to me. Zero benefit. Reason I've wasted time writing [a blog post](http://nosubstance.me/post/constant-bikeshedding/) on that.
&gt; Software that would have to be maintained. That would be ridiculous. It _is_ software already. You talk complexity, and of course i want my build system to be tailored to be doing builds... Language that you write it in wont change that. Primitives provided (or not) by the tool will. That was my gripe with scons, i _had to_ write, as you call it, software - this wasn't because it was exposed as python, it was because i didn't have enough tooling provided to solve my problem right away with primitives it gave me.
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8ryuee/in_case_you_want_to_migrate_from_constwest_to/e16k1ee/?context=3.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I've compiled many gut feelings into words [here](http://nosubstance.me/post/constant-bikeshedding/). If you agree with it, share it anytime an eastconst evangelist shows up! :D
Nailed it.
Why not make a git* project to show the usage instead of an insanely long post?
I heard the author of Nana (https://github.com/cnjinhao/nana) wanted to submit nana into boost. I think that would have been a wonderful thing since nana is a really great modern library.
Especially when they are a bunch of heretics going against the glorious Constodox Church.
I prefer infix const. `con int st`.
[I'm one of the heretics, I guess.](https://github.com/Ebenezer-group/onwards)
Doesn't the behavior of range-based for loops depend on using begin and end through ADL?
Is there a reason you would need to do that? Are you stuck on a really old compiler version? 
'std::burn&lt;std::stake&gt;`!
Yes and yes. I simply want to know if this is possible. Please answer with a "yes"/"no" response or suggest a source I can reference for help. Thank you dagmx.
LOL, thanks for this video dude. It's exactly that and from now on I'll also recall that.
Should be possible using a [clang source-to-source transformation](https://eli.thegreenplace.net/2012/06/08/basic-source-to-source-transformation-with-clang/), the same way [cpp insights](https://cppinsights.io/about.html) works. That site will actually do some of the necessary work, but not all. IMO generated code is usually not worth anything, though, and you're better off doing it by hand.
no. many of the improvements to c++ were things that were just not possible in c++ before. that's why they were added. you can probably still express just about any idea you want in older versions just probably not as well if they are actually using features of c++11. that being said a lot of code will work with compilers targeting c++11 and older versions so you could just try to compile it with an older language spec. then you look and see if it spits out any errors. if it does you go to that line and re-write that part with changes that meet the old spec. and hit compile again. repeat until it works. You can write new code in older language specs and just call c++11/14/17 code that's already compiled. no problem there either.
There used to be a C backend for LLVM (i.e. "compile" C++ to C), but this was removed in version 3.1 (though forks of this exist). Not sure about C++ though. see: [Using the LLVM linker to produce C code](https://stackoverflow.com/questions/31960290/using-the-llvm-linker-to-produce-c-code) and [C++ Weekly - Ep 52 - C++ To C Compilation](https://youtu.be/aMpCr8yXUaM)
I don't know of any automated tool to do this. A lot of what went into C++11 came from Boost. Some library things are mostly Boost things in a different namespace, some language features have a Boost utility that provides similar functionality. Your best bet is to convert the code to use Boost when possible. Maybe you can automate this using a code refactoring tool, though I'd advise doing it by hand if it's a one-time thing.
Your reply is a bit rude given I'm asking for context to what is a pretty odd question which might help me answer better. But yes it's possible , but nothing exists yet because its a very big undertaking with very little value. Clang has some tools to analyse the AST of the code and you can use that to convert back, but at that point you already need a compiler that does c++11. You'd have to essentially rewrite most of the code or be left with incredibly messy results. If you want any semblance of maintainable code you'd need to hunt down an old version of boost that works with your compiler version too to fill in the holes of what is missing. I find it dubious though that you're stuck on such an old compiler version. The only industries that would mandate such a thing would also not allow you to put in any code that was the result of a conversion system because it would be quite unmaintainable.
Thank you all for the suggestions. I will take a look into the suggestions you guys made.
&gt; things that were just not possible in c++ before That's definitely not true. Remember that any Turing-complete language can produce any equivalent program, it just may be excessively ugly code. (see [bf](https://en.wikipedia.org/wiki/Brainfuck), [whitespace](https://en.wikipedia.org/wiki/Whitespace_%28programming_language%29), etc.) As an extreme example, you can imagine that the output of such a converter is the x86 assembly language compiled for c++11 and is an x86 emulator written in c++99, which would, technically, achieve the goal. Most of the actual examples like variadic template arguments had already be done using c++99 but were inefficient, leading to very long compile times, or excessively ugly code (see Boost MPL, Hana, etc.) 
&gt; That's definitely not true. Technically correct doesn't mean it's practical. You could always write C++11 compiler in C++98 and get the support this way but that's definitely not the desirable solution. Some C++11 features can be mapped well to C++98, others would map correctly only in some specific simple cases (i.e. most of the language changes, like move semantics). 
Everything is possible in source-to-source translation. And I mean literally everything. So your question shouldn’t be about it being possible, but provide context so that those who answer could give you tools that would make most sense for your application. What CPU are you targeting? It may be less work to refresh a stale gcc or llvm port if such exists. 
That’s nonsense. Of course it’s totally possible to do this. Compiling is a process of lowering representations until you hit machine code. Starting at C++11, you need to stop at a level that is usable in C++98. Worst case, you can convert LLVM IR to C, but clang architecture provides plenty of opportunity to emit a simpler C++, after the modern C++ specifics are taken care of. For example, replacing custom literals with their expansions, new integer literals with old ones, new member initialization and constructor chaining with old syntax, auto with deduced types, expanding lambdas into class definitions, and even expanding coroutines into state captures and giros would be very simple to implement in clang. Some other features would be peskier. 
Move semantics are trivial to map to C++98 when you have a compiler that understands C++11. The compiler already knows whether it’s a copy or a move. It can emit tagged overloads of move constructors and regular methods for move assignment. The compiler can also emit tagged methods for methods that work on rvalues. In fact, when you choose a suitable set of rules, all of C++11 move semantics and rvalue references roundtrips to nice C++98 code that performs the same, and then back. You wouldn’t want to be writing that C++98 code manually, but here the compiler does all the housekeeping for you. `std::auto_ptr` didn’t have the advantage of a compiler that digs moves. 
A reliable 11 to 98 converter is totally technically feasible. The fact is that nobody has wasted man months writing and testing such a thing and nobody ever will. The real answer is to sit down and do drudgery fixing compiler errors. Maybe use some regex to find recurring problems as you see them, and fix them. It's a really weird question in the first place. I can't imagine any circumstance you'd need to backport from 11 to 98 on a big codebase. If it's some embedded thing being ported to a different vendor specific compiler I can't imagine it's loads of code and the fixes should be obvious and not take weeks,
Perhaps they wanted to upgrade to C++11 code while still targeting some obsolete platform.
I've also tried it, and works nicely.
Why did they hate it?
Like promoting west const does? This post makes no sense.
If you read the whole comment you see that I clarify with "you can probably still express just about any idea you want in older versions just probably not as well if they are actually using features of c++11"
it's not totally possible to do this for someone that has a question like that. Maybe in a few years, but I doubt that will do him much good for his current project.
It’s probably like a week’s time for the person with experience with that part of LLVM. It’d probably be a steal. 
sure and the person asking the question wouldnt need to ask that question if he had experience with that part of LLVM
&gt; North const LOL :-) I think to resolve this issue that now divides our community, we need a new standard. I propose _uniform const_: int *i = {const} nullptr; 
It is perfectly obvious to me that source should be indented using tabs. Moreover, to avoid gratuitous waste of space, it is perfectly obvious that tabs should be set to the equivalent width of four spaces. Tabs are the correct, intended tool for indentation, so why simulate them using spaces? Spaces fail in too many cases: when the character width is not fixed, when people accidentally type 3 or 5 of them, etc. It is perfectly obvious to me that the opening brace should be placed at the end of the line that marks the start of the block (i.e. after the if, for, etc.). It is just a single character, horizontal space is cheap in these days of 400+ character wide terminals, and sacrificing valueable vertical space just for one character is criminal. It is perfectly obvious to me that line width should not be limited to 80 columns. VT100 is dead, and so is the notion of printing source code; these days we all have decent screens to work on. Why limit yourself to a tiny part of the screen, when you can have so much more visibility if you make use of the available horizontal space? It allows use of longer identifiers and clearer structure. It is perfectly obvious to me that exceptions are the one true way to handle exceptional situations. Because this style is madness: int myfunc (int &amp;result) { const int errcode1 = funccall1 (); if (errcode1 != err_ok) return errcode1; const liberrcodetype errcode2 = funccall2 (result); if (errcode2 != liberr_ok) return errcode2; // I'm sure it will be fine } It blows up your line count by a factor of 4, making your entire program about error handling, instead of business logic. It forces the return value to always be an error value, and never a business value, thus interfering with a pleasantly clean-looking functional approach. It conflates error numbers from all sorts of sources. It bubbles up error numbers that will be mostly-meaningless by the time someone gets around to handling them, instead of a descriptive text. The only correct approach is using exceptions: int myfunc () { funccall1 (); return funccall2 (); } As for where the const goes, I really don't care one way or another. In part this is because the issue doesn't come up very much in C++ - assuming of course you use it properly: - for a const object or basic type, having const on the left is fine. - for a pointer to a const thing, having const on the left is fine. - since the introduction of references there hasn't been much call for const pointers. - having a vector of pointers is almost always the superior solution over having pointers to pointers. Basically, the rule should be this: "if you find yourself needing a 'const' that's not on the left, you should probably reconsider what you are doing since it is almost certainly not good C++." 
yeah Windows is the name of the problem. On linux you can really gets thing easily with integrated package managers. Worst thing, you'll have to recompile from source.
This is the problem I have with the "standard" package manager. On Linux everything is done to easily integrate libraries, installed in known directories, compile/installed with free standard tools. And most distribution now have their how package manager and repositories.
&gt; It’s probably like a week’s time for the person with experience with that part of LLVM. I was nodding my head until this point, but here I think you're wrong. There are simply too many new features between C++98 and C++11. I am pretty sure you're right that no specific feature poses a significant problem, but each new feature would need some sort of code for it, you'd want separate tests pretty well each feature and then you'd need a pretty serious integration test that covered all those features working together.
No one actually made suggestions. They quite rightfully said that it would be quite a lot of work for someone at a very advanced level, and a near-impossible amount of work for a beginner or intermediate C++ user. Also, you're getting downvoted because you didn't try to help us out at all. You told us really nothing about _why_ you would want to do such a thing - had you, we might have been able to help you with your actual goal. I'm really unaware of platforms where the most recent compiler is C++98, unless you're targeting some legacy hardware platform... but if you did have such a platform, why wouldn't you just write new C++98 code for it? If you're planning to backport some modern C++ code to a legacy platform, then it's quite possible that the legacy platform simply doesn't have enough memory or CPU to make it possible.
He's saying the waste of time is lamenting "there's an obviously correct choice and how can someone dare say it's just style."
And I'm saying participating in this discussion at all is a waste of time; avoiding the conversation because "there's obviously a correct choice" is a _better_ move for one's paying customers. And as far is it goes, opting for inconsistency is, frankly, stupid.
&gt; shared linking et al is a non-negotiable requirement for many. keep telling you that and then one day you find yourself in front of a codebase rewritten in Go and Rust because people there saw shared libraries as the madness that it is
&gt; The committee is not in the business of putting other companies out of business. yep, better let them be put out of business by other competing technologies
Toolchain files are one of the reasons I opted for hand writing makefiles for an embedded project.
Well, that'd be big visible rewrite. I.e. it's fine when you have C++11 code and want to convert it to C++98 once, but when you want to consume C++11 lib on C++98 compiler it won't really work. For example such renaming could easily create issues with templates. Most of compile-time stuff wouldn't map 1-to-1 so you wouldn't be able to use it. `constexpr` is another good example, though C++11 constexpr specifically shouldn't be hard to replace with TMP.
Obviously north const is the way to go. That is to say, follow every const with a line-feed. /s
As a French, were adjectives usually go after the name they qualify, I've always thought of English as backward ;)
Looks somewhat verbose. Wasn't there some proposal about natural (terse) `const` syntax?
For a large-ish codebase, it would be easier to add a new target back-end to LLVM for your obsolete platform. It would also be more reliable, give you the benefit of all future upgrades, and every other supported front-end language.
You have a very weird definition of subset. C is (mostly) a subset of C++, something that compiles in both languages. I don't see Rust compiling in C++, hell, not even doing the fundamental syntax similarly.
My personal favorite is `type const` or `template &lt;typename T&gt; using const = T const;` And then you get stuff like `ptr&lt;const&lt;char&gt;&gt; str = "hello";`
Even though I agree with \_some\_ of your "perfectly obvious" statements, stating that it's "perfectly obvious" and hence why they should be followed is just one thing: condescension. Even though I could show several counter arguments to the ones being used to back such assumption, it doesn't seem worth it. I can see that eastconst evangelists have one type of personality, they generally self entitle what's logical, what's right, and now, what's perfectly obvious.
joke's on you I'm unemployed
&gt; VT100 is dead, and so is the notion of printing source code; sorry what ? source code is printed all the time.
That title is interesting, can't guess if it's about bikeshedding that always happen or bikeshedding about const.
Dude, it was sarcasm. Check out the parent post. 
On an 80-column matrix printer, no doubt, using chain-fed paper with those cute alternating blue lines ;-)
Ah, haha, OK, s/condescension/sarcasm/. I didn't notice :D
I am sure your programming dogma seems logical to you. 
What's up with the inconsistent function names? Some functions are snake_case and some are PascalCase? Seems odd?
^^ This.
Looks like some names use std library conventions. The ones involving the file system use Windows API conventions.
&gt;std::reduce, seq: 4.07034 ms, res 3e+06 &gt; &gt;std::reduce, par: 3.22714 ms, res 3e+06 I make that a 20&amp;#37; speed up. Which is pretty small.
Maybe I missunderstood the concept of technical Dept in the context of a programming language, but imho c++ has more technical dept than java, swift &amp; rust combined. &gt; My current contract has me working on two million lines of 1994 era C++ codebase. Still compiles and works in C++ 17. That's very valuable to the big multinationals who mainly support WG21 work. I give extremely little experience with that, but is there any reason to believe that the same wouldn't work with java too?
I'm surprised to see that dynamic_cast and bit fields are considered unsafe. Does anyone know why?
I always prefer CamelCase because it saves me the effort of to move my fingers all the way to the underscore. Additionally it always felt old to me using underscores. Like I am writing in 1990 or some thing. I'm all for c++ hardware level convenience but some modern technology that isn't as much of a cluster fuck always would be good. 
In most other contexts that I'm aware of (mainly electronics) you standardize things long before any products are shipped. Sure, you want to demonstrate technical feasibility beforehand (and at least the revised modules proposal will probably not be able to do that prior to c++20), but usually, standardization precedes widespread implementation. There is however one fundamental disadvantage of c++ compared to e.g. let's say the USB standard: Versioning. Whenever the protocols in the current USB standard don't reflect the state of the art anymore, a new mode get's **added** and except for the initial handshake, the new one doesn't have to be compatible with the old one. In c++, this side-by-side isn't really possible with the current implementation model and imho that should be the first and foremost concern is the committee. On the language side modules would be a small but very important first step (getting away from the text inclusion model) and on the library side `stdX::` would imho have been the correct solution to solve that problem.
You should look at all the numbers really: &gt;std::accumulate: 6.14874 ms, res 3e+06 &gt;std::reduce, seq: 4.07034 ms, res 3e+06 &gt;std::reduce, par: 3.22714 ms, res 3e+06 &gt;std::reduce, par_unseq: 3.0495 ms, res 3e+06 So reduce is significantly faster (33%!) than accumulate (which is all we had before the parallelism extensions), which is interesting to know. And then `par_unseq` reduce is ~25% faster again! It's also worth noting that part of the reason the gain is less than you might expect is that modern CPUs are very good at single-threaded work - this particular CPU can boost a single core to ~40% over the base frequency. This will eat significantly into the gains of a parallel run. It's also quite a small dataset for a parallel algorithm - "only" six million doubles - about 48 MB of data - and it's just being summed (quite a simple operation). The parallel algorithms really shine on much larger datasets (that would take a noticeable amount of time to run) and with operations that boil down to more than a single pipelineable instruction...
And I prefer snake case especially since I work in telecoms which has many acronyms which are painful in camel case..
I've done the same and haven't been able to see any big improvements in the build time reduction for that particular project so I would say that it really depends though. Compiling is mainly CPU intensive task, and compared to it, I/O is really negligible in that context (e.g. many smaller files will be generated by the compiler but that's not a big amount of data and SSD will not make that process much quicker because OS filesystem will already kick in by caching the data). On the other hand, if linking is bigger part of your build (very often large unit test suites will be the biggest offender here, e.g. very large binaries), then SSD will obviously give you major improvements. 
For me it's the opposite, CamelCase feels like old-fashioned Java from late 90s. Ugh I don't miss those dark days
CamelCaseIsHardToRead.OptimizeForTheReaderNotForTheWriter
I indeed understand opposing opinions. I understand there are cases where it is indeed better to use snake case in a snake case environment and I'd rather set on consistency than a will to change something that has only a minor effect. And I think using CamelCase inside the stdlib is a bad thing.
Why do we want ADL calls again? what's wrong with calling object.size()?
&gt; This is my first time writing one so all feedbacks will be appreciated. Good job, book-marked.
You're down voted for sharing your preference? With valid reasoning? I don't get Reddit sometimes.
Are you a flat-earther? Zenith const and nadir const should at least be made optional. ;-)
&gt; You have a very weird definition of subset. A subset of the functionality, not the syntax. Very "weird". /s
For libraries it is not the case that standardisation precedes widespread implementation. All the libraries added went through many years of battle testing before standardisation, except perhaps for Ranges, but it was unusual, and nobody could claim that a v3 by the same authors as the v2 would not be safe. Executors is a library feature, and ought to have a similar high bar for acceptance. In my opinion, get it into Boost first, get a few years on it, come back. Don't innovate by committee (and that *certainly* is what the present Executors proposal is doing). Modules, or its better name Precompileds, suffers from chicken and egg. Compiler vendors won't expend the considerable resources on implementation until it goes into the standard without much use experience. And you can't get the use experience without it shipping. I have to hold up my hands here and opine that I think the least worst choice is to the ship the thing knowing full well it'll be defective, and do a breaking change v2 after.
If you have 4 cores, then ideally you'd be looking at a 400% improvement. So getting 20-33% is not as much as one might hope for. I agree it's partly because it is a small example. The overheads of parallelism seem to be such that you need a lot of data, and/or a lot of work per data item, for it to be worthwhile. 
Correction, when I said 33% faster I meant it takes 33% less time, which is not the same - I need to invert my numbers.
&gt; Maybe I missunderstood the concept of technical Dept in the context of a programming language, but imho c++ has more technical dept than java, swift &amp; rust combined. Java definitely has far more tech debt than C++. Lots of rash decisions were made early on, combined with multinationals making sure that Java 1.6 code still works on Java 9, and so on. Even though C++ is more than a decade older than Java, Java is just plain *huge*. Swift has performed multiple rounds of tech debt dumping by forking the language and libraries, rendering early Swift code into legacy. Rust used to simply break old code, but more recently as they've started to give guarantees, the tech debt is beginning to mount. You're right that C++ has more tech debt in absolute terms, due to it being much older than any of the others. But in terms of tech debt per age adjusted million lines of code, I think it's amongst the best. Especially if you never used the STL, as most of the tech debt lives in there without doubt. &gt; I give extremely little experience with that, but is there any reason to believe that the same wouldn't work with java too? It's not my forte, but certainly Java 1 era programs appear to work perfectly on Java 9 byte code interpreters. And in fact I've seen Java 9 bytecode often appear to work fine on old byte code interpreters too. I'd imagine most of any incompatibility will be at the source code and/or linking levels, where say mixing Java 1 libraries into modern Java probably would have some surprises. But not my forte I must disclaim.
It’d be code lowering so if you attempted to use a feature that the C++98 compiler doesn’t grok, it’d be that compiler barfing on it. The starting point is that the C++11 compiler emits un-lowered code. As long as you write C++98, the emitted code is OK. Then whenever you want to use a C++11 feature, you figure out a lowering transformation. That transformation is the input to all further stages of the C++11 compilation as well as to the code source dump that precedes the rest of the compilation after the last code-level lowering is done. Since the transformation must not change semantics, you have the entire gcc and llvm test suite to ensure that you didn’t mess it up. I don’t think that much in the way of extra tests would be needed, other than to verify that the transformations lower the code low enough, i.e. that the C++98 compiler won’t dislike it. That’s simple enough to add since there are no special cases, it’s “dump output to C++98, ensure it compiles”. Of course there may be tricky cases of emitting something that is UB in C++98 but not C++11, or where otherwise the semantics of identical code in C++98 and C++11 differ even though the code is valid in both. Can’t win it all I guess. 
Nice, thanks! 
You can also remove `return 0;`.
It's not vague. It's *implementation defined*. So your "implementation" just happens to support such paths
Got to the main site, looked at examples (indeed they look very terse), reading through [C++ to D](https://dlang.org/articles/cpptod.html) and what I dislike is that as usual, they compare D to C parts of C++ out something outdated. Why do they show `memcmp` as implementation of equality operator? Some statements are no longer true, unless they want to compare 2018 D with C++98/03.
It is a bit outdated indeed. It was just not updated for a long time that part.
Nice, thanks!
I'm not arguing about how things happened in c++ so far. I'm wondering if this (standardizing established practice) is desirable. And looked at other areas for guidance.
A very good point. Thanks for the correction!
We are relying more on implementation defined behaviour than usually aware. Eg the maximum length of hardcoded string literal, maximum nest level of control flow, maximum file length, maximum identifier length, maximum amount of parameters in functions, maximum template instantiation depth, etc - all these are implementation defined. All implementations are obviously far beyond what we need. AFAIK the nest level for most compilers is 256, but haven't seen ever more than 20.
hard drives are cheap
Because they don't think like I do, which I understand since it is a totally subjective thing. I'm quite young in comparison to some other people here and I do not have a large pool of experiences to validate my opinion. Just by all the things I have done yet I usually prefer CamelCase since it is more handy and I don't work on large group projects.
`unsafe{}` here is just an example. Maybe better named `advanced {}` or `yes_i_really_need_this_instead_of_interface`
Maybe because it was straying further off-topic, and into an area of pseudo-religious beliefs where heated yet futile arguments are common.
Time is precious. This is especially important if packages are always compiled. Additionally it is much easier when it comes to packaging the final application since you don't have to rummage through dozens of shared libraries and check whether they are needed or not. (Or maybe you really are content with massively bloating the final result ¯\\_(ツ)_/¯)
It doesn't matter if you have experience, even the people who claim they are experienced forget that they aren't God. Reddit is about logical discussions, and it doesn't matter if you're a koala, long as you make a logical point. Downvoting for disagreement is not cool.
&gt; I'm wondering if this (standardizing long established practice) is desirable I think it's the least worst available with the current configuration of things. If the multinationals supporting C++ threw a million a year into a pot dispersed by the Standard C++ Foundation to pay for full time folk to lead the ecosystem in a strategic direction, that would be a whole different situation. We could permanently solve the C++ build and packaging problem within two years if we had that, then followed by lots more low hanging fruit, none of which is hard except for the lack of full time developers to go implement it. 
&gt; Reddit is about logical discussions Is there a RES plugin to enable this feature or something?
So where's the write-only part? 
Nobody expects 400% performance from using 4 cores over 1.
Ya, but it is written in 90s Java, so it takes 20 minutes to load for each post.
Modify the Register class to contain only Set() method and/or an assignment operator. I also provided the read part to avoid the code duplication in the article.
Depends on the sub I guess
&gt; Note: _GLIBCXX_SANITIZE_VECTOR was added in the GCC 8 libstdc++. what's the difference with -D_GLIBCXX_DEBUG ? AFAIK it already added sanitization checks to &lt;vector&gt; and others
I know, but for testing I usually have several main's in the same cpp file. I just take them [the ones I don't need] out by changing main () in main233534534 () for the ones I don't want, this will throw an error without the return 0;, i.e. I put return 0;.
This "write-only variables in c++" article, offers no write-only implementation. Additionally (sorry, I'm nitpicking), if you were to ask me in an interview to discuss how to optimize the implementation of this code (without further context), I wouldn't want to work with you (and would fail your interview :P): this thinking is what leads to premature optimization. Optimization (at the level of generated assembly instructions) only makes sense when you have a concrete execution scenario ("we run with this and this data, in this and this environment") and a concrete optimization goal ("we get this timed runtime execution and we need to get _that_ instead"). 