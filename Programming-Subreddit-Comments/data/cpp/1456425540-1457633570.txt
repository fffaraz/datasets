Skylake and later Broadwells have TSX enabled. C++ should support useful cpu features much more aggressively than it did in the past (SIMD anyone?).
Why was `vector&lt;bool&gt;` made to be so weird in the first place?
&gt; Skylake and later Broadwells have TSX enabled. C++ should support useful cpu features much more aggressively than it did in the past (SIMD anyone?). Skylake had: https://www.reddit.com/r/hardware/comments/44k218/intel_disables_tsx_transactional_memory_again_in/ You can decide not upgrade to the firmware that disables TSX, but that doesn't change the fact that TSX hardware is so buggy it doesn't work reliably. You really don't want to _almost_ always avoid data races. Either you always avoid them, or you have UB even if it is due to a hardware bug. One has to give it to intel, TSX is hard, and they have made serious progress. It is just not there yet.
In case you haven't looked into it yet, the thread heap in windows seems to be very fast. Intel's TBB library has a thread local allocator (which requires a .dll) and I think Intel's C++ compiler's open mp implementation has a thread local heap as well. Poco and Boost both have one but you have to use large bloated compiled libs. I think Jemalloc uses VirtualAlloc on windows and mmap on linux, then gives out chunks to threads concurrently.
There's also a chance merging the Filesystem TS is voted down. It has a fair number issues (the primary being the further entrenchment of TOCTOU security bugs encouraged by POSIX).
Concepts (and their shortcomings) were discussed rather heavily by Andrei Alexandrescu at last year's D conference: https://www.youtube.com/watch?v=mCrVYYlFTrA. In short, they help alleviate a lot of pain points, but become difficult to use in problem domains with a large "vocabulary" that can be combined in `n*m` ways. D has shown that life can be much simpler with a bit of compile-time introspection - what's the latest news in the C++ world surrounding `static_if`? I know it was initially met with [open hostility](https://www.reddit.com/comments/2hsxe2), but I've heard rumblings that members of the standards committee have come around.
I think a lot of problems with the original `static_if` proposal was that it tried to solve several problems at once -- some problems that are better solved (theoretically) by concepts, for example. This approach might've made sense in D but less so in C++. The last `static_if` proposal I saw whittled its scope down to a much smaller set of problems and as a result, seems much more attractive (tho I'd have to go back and read the paper again for details).
What can be done (and we are considering) is using fixtures in googletest that enable the real communication library if hardware is detected or enable the mock if it's not detected when making tests against hardware. Easy to dev and test the test on our workstations instead of scheduling time on the actual hardware.
http://www.codeproject.com/search.aspx?q=win32&amp;doctypeid=1
&gt; IIRC this was supported by the first concept proposal. And IIRC this was quite hard to implement and work with. So it was voted out. Why are we discussing this again? If you're curious, this topic was recently discussed heavily on the [mailing list](https://groups.google.com/a/isocpp.org/forum/#!topic/concepts/CX3aUunonaI).
The static_if proposal was in conflict with concepts and introduced a non-scope with using {}. Yet, there are ways to achieve static_if in C++14 https://www.youtube.com/watch?v=hDwhfjBPKv8
Contracts are statements you evaluate at compile or runtime. If you want .cbegin, you do assert(type.begin || type.cbegin). All concepts implementations (for basic types) are often just doing a type check on (int) foo to see if its an integral argument (or a class, or negative, or supplies a function, or throws X or emits Y), and you can do those in contracts all the same.
Maybe unit testing isnt the right word then? I agree testing against the hardware is key, and our overall goal. We want to read item X, do bit operations, then write the value back. The test would ensure that the end result is whats expected.
Follow the link in the description to his github page, where you can see the implementation and other details. This is the blog article which this is based on: http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html
&gt; TOCTOU security bugs encouraged by POSIX). What makes Posix particularly vulnerable?
Bleh, the rabbit hole certainly goes deep: [The most in depth stack overflow post I could find](http://stackoverflow.com/a/11996970/1876138) I was remembering back to c89, where it was explicitly implementation defined. tl;dr: Definitely legal in c99. But C++11 is complicated enough that it seems like an lvalue to a non-active union member should be uninitialized without special circumstances. In either case, I'll stick to aliasing through char types, which has been disambiguated.
Permission checks using access(), for example. stat()ing a file and then assuming the information is still valid.
I actually have experience with this. So, according to [this comment from the OP](https://www.reddit.com/r/cpp/comments/47g2gn/unit_testing_for_functions_communicating_with/d0dqy7f), he/she's not necessarily looking for *Unit Testing*, but simply a way to test functions and systems. I work in the robotics and automation industry and have worked with many different types of hardware: from microcontrollers to PCI cards to custom-designed hardware running RTOSes inside. So PCIe cards will usually map the memory of the card into the address space of the host system. One thing you can do is to create a simulator of some sort that creates a memory map. (Ideally, the simulator will share as much of the device's source code as possible.) For your API, you can add a flag for whatever does the initial memory mapping to look at the simulator's memory map instead of the PCIe card's memory. This type of system works great for quick tests that can flush out issues prior to having to test with real hardware, which in my experience is often rather limited. Testing on real hardware is important though. Where I work, we have dedicated computers with the hardware hooked up to a continuous integration system. We include in our API some private functions to allow us to reset the system to a known initial state. Our tests (unit/component/functional/etc...) will all start *and end* with resetting the system to a known state. Then our tests can call whatever functions we need to manipulate memory. Then we use whatever functions we need to validate the results. This can often involve hidden functions to read memory directly. I've frequently had to test the physical parts of hardware: digital and analog I/O for example. Are voltage levels correct? Current? Can we read inputs correctly? So on and so forth.... The way we've verified that is to connect up our I/O to a USB I/O module (Advantech is a popular vendor). Then our tests will validate that our outputs generate the correct data in the USB I/O module. And we do the reverse for inputs. There may be other challenges too such as what do you do when the hardware can't reproduce exact results. Maybe there's noise; maybe there's a random number generator involved. With some thought and planning, most things can be tested. If you have some more specific questions, I'd be happy trying to answer them. It might also be nice to describe your hardware and what you need to test in a little more detail.
OMG SO EXCITED! No, it's the usual *talk-so-we-don't-work* conference. Not going. Ty.
The problem is we cant modify the API in any way, it is produced by an entirely different team in a different country to be used as a framework for many tools besides ours. Basically we need to be able to initialize our card through the API, it sets up the memory space for that particular port of the card and passes us back an adapter structure that we then pass everywhere to talk to that particular port on the card. Then we test our code that is in essence a large wrapper around the API's read/write functions to turn bits on/off that we need. We do have a continous regression Jenkins system running right now every time code is pushed to git. But these test's are all written in python and largely just check the printed output(our tool is console based), if they type Get Status and see value X:FALSE, they send "SET VAL X 1" and then check that it then says X:TRUE if they check status again. What im hoping to prevent is escapes where its returning X:TRUE when its not really, aka a programming error. We do a lot of bit shifting, masking and bitwise operations and obviously its easy to mess these up every once in awhile and no one would ever know currently in our regression testing because it reports out the "correct" value the regression test expects. Doing these "bit checking/logic" level tests in a continous regression enviornment seems incorrect, it seems like something a developer should be doing locally before committing code in the first place, which is why i lumped it into the "Unit Test" name which may be incorrect. Thanks for all the info it really does help! Any more insight is appreciated :)
I've read the pro and contra papers; for the record, I lean mildly toward the pro side (if I discount the issue I'm about to discuss). I suspect one major reason why so many people are eager to introduce concepts without delay, apparently to the surprise of you and Eric Niebler, is that many C++ users today are aware of the ISO committee's long term plans: a release every 3 years, alternating "major" and "minor" updates, with C++17 being a major one. This suggests that, if concepts don't get into C++17 (with its deadline getting worryingly close), a language change that big probably won't make it in until the next major update in C++23. I don't know how accurate that perception is, not being on the committee, but I'm pretty sure it's there. I'm still trying to decide how seriously I should worry about it myself. (In contrast, if concepts get into the language but not the library in C++17, adding them to the library in the smaller C++20 update sounds more plausible.) In short, I believe many people are urging for concepts now because they don't see it as a question of "how long should we delay before they're fully baked?", but rather as a stark binary choice between "concepts right now" vs "concepts in seven years".
This is probably [bug #22010](https://savannah.gnu.org/bugs/?22010) in GNU Make. Child processes inherit make's huge stack size limit. It seems to have been fixed in version 3.82.
Agreed, and I do that when I can. This is for a no-Internet situation unfortunately. Thanks for the recommendation.
Thanks. I never caught the 'Boost' in that subtitle. I've seen the cover of this one many times.
As it says on the right-hand side of this page, and when you submitted the above: &gt;**For C++ questions, answers, help and advice see /r/cpp_questions or StackOverflow.**
Aakash23, Your program is not skipping the "getline part." When you extract a string from a string like this: std::string current_status; std::cin &gt;&gt; current_status; You are not reading the white space after the extracted string. So if the user types a word followed by a carriage return the word will go into "current_status," but the carriage return will still be in the stream waiting to be read. If you then call getline() on that stream, then it will read an empty line into the string you provide and discard the carriage return. This is what is happening in your code. 
`a1` is a local variable inside the function. It's passed by value, so you're operating on a copy of it. Any changes made to it evaporate when the function returns. Any expression that begins with `a1 = ...` can have no effect on the caller's value. You get a segfault because `c` is never initialized, and dereferencing an uninitialized pointer is undefined behavior. To have the function make a modification to the caller's value, you have to dereference `a1` and modify the thing it points to, not the pointer itself. 
This is exactly right. I think [this](http://stackoverflow.com/questions/4426474/is-passing-pointer-argument-pass-by-value-in-c) stackoverflow answer gives some good additional info. However, this type of pointer usage / manipulation is frowned upon in idiomatic C++.
Ah, so when running it through Make, it allocated more stack space for each subprocess, and my recursive calls were considered new subprocesses? Which means each recursive call extended the stack space limit until it had no more stack space to give (all my memory). How interesting. I guess I might be misusing Makefiles to run my code, but I'm glad that it's a problem that has already been fixed. Thank you for answering! For reference, was this an appropriate subreddit for this sort of question, or is there one which you can point me to? It's be much appreciated by me (and others I hope).
To me, unit testing is: take a unit X (a class, a function or some other grouping thereof), replace anything it depends on (it's calls to other units) with a mocked implementation, test that X functions as expected. The *purpose* of unit tests is the ability to test the unit *in isolation*, quickly, also on a build server, without requiring other set-up. So yes, I would not call what you need "unit" testing. There's at least two units there, your code and the hardware. Yours is the case (IMO) where testing your code "unit" brings much less value than a synthetic test of the ensemble.
It's not about being "skeptical". Until one implements and uses a new language feature it is very hard to be sure that the feature is designed properly independently of how good one's brain is at compiling hypothetical C++ code. The whole point of the TSs is to move forward with features we are not sure about, _gain implementation and usage experience_, and tune them "outside" the C++ standard till we are sure about them (either because they are worth it and we want to standardize them or because we find a fundamental flaw and decide to go with something else). To be clear: a TS is not _finished_ when the draft gets approved. That is just the beginning. A TS is finished when it gets implemented, shipped, and used. So 3 TS: - Filesystem TS: this is basically Boost.FileSystem v4 (thats 4 redesigns of the library), 10 years of experience with it, shipping implementations available, lots of usage experience. This TS is finished, it works, minor complaints: Ok, let's talk about standardizing it. - Library Fundamentals TS: this is special because it is a mix of different libraries with different backgrounds. The major compilers have shipped only some parts of this. We have no implementation and usage experience with this as a whole. It makes no sense to talk about standardizing this as a whole. We have good implementations and lots of experience with some parts of it, so we can cherry pick things for standardization that are finished. But consider variant. Even though we have 10 years of experience with Boost.Variant, formalizing it for standardization has been more controversial than it should. We have a pretty good solution, but not shipping implementation of it, and zero usage experience in the real world. It makes no sense to talk about standardizing variant: it is not finished. - Concepts TS: we have a single "prototype" implementation. We have no shipping implementation, we have little to none usage experience (AFAIK only CMCSTL2), and this feature is both very important and huge. I can't even understand why people are discussing about standardizing this. It should be clear to everybody that this feature is not finished. 
Thank you!
Audio starts at [4 minutes in](https://www.youtube.com/watch?feature=player_detailpage&amp;v=drhIIbmrjzg#t=235).
I'm going to guess it's about when you pass a shared pointer by value it increments the reference count when you enter the function and then decrements it after you leave the function, which generally isn't necessary.
Languages where RC are built into the language are able ellide increments/decrements thanks to dataflow analysis. Maybe C++ compilers could be a bit more library aware, at least with their own one.
While I'm OK with compilers optimizing like this, it's still a practice that should be discouraged. Smart pointers make it very easy to reason about object lifetimes; passing them around willy-nilly breaks that. Too often people see RC as magical, the compiler will take care of everything just like GC, solution. It's not, and should not be treated that way.
With filesystems finally coming onboard to standard hopefully it won't be too long until networking does as well. There is a WIP TS for it.
I mean...this seems obvious to me? This isn't any different from using any other type. * Pass by value if you want to make a copy of the object (in this case, the pointer) * Pass by reference if you want to modify the object (but consider using a return value instead). * Pass by const reference otherwise. Edit: I mean, yeah I guess it's different from using a raw pointer, which you never pass by const reference, but that's typically just a rule for POD types, which std::shared_ptr is not.
So, it sounds like you are provided something similar to the following: * port_obj connect_pcie_card(const connect_info&amp;); * error_obj write_bits(port_obj&amp;, size_t bit_address, size_t bit_count, unsigned char memory_to_write[]); * error_obj read_bits(const port_obj&amp;, size_t bit_address, size_t bit_count, unsigned char read_buffer[]); Is this correct? If this is correct, then what are you trying to test? Are you trying to test that the PCIe board does things correctly? Or are you trying to test that an API build on those primitives work correctly?
Try /r/cpp_questions
If there's no ownership involved, I rather prefer passing by const A&amp; instead of const std::shared_ptr&lt;A&gt;&amp; i.e foo(*aptr) there's no point to work with **const shared_ptr&amp;** in that case (except to pollute the API), but for sure passing a shared_ptr by copy when you don't need it add quite a lot of overhead
I have definitely seen code where the majority of the time is spent incrementing/decrementing reference counts.
Concepts TS is a language change, not the library change. This kind of evolution is better, in my opinion, as a standard. Lambdas (IIRC) were implemented near C++11 standardization finale or even after it. constexpr's too (MSVC - I'm looking at you and C++14 relaxed support). Thing is - this change is impossible to use widespread without adopting it, so the library designers can actually rely on this functionality. You can rewrite File System code using #ifdef guards - but you can't do this with concepts.
Oh, and by the way, quite often there **is no** ownership involved! I have a codebase where previous people were quite refcounted-ptr-happy. There is just so much... Noise!
Thanks. Fixed
&gt; Defining = in terms of references was a mistake. What would he have defined = in terms of if not references?
&gt; there are subtleties about other operators, such as &lt;=; does its meaning involve &lt; and == or &lt; and !? ~~I assume that should be `&gt;` and `!`~~ **Edit:** I was thinking that `!(a &lt; b)` ≠ `a &lt;= b`, I was reminded that `!(b &lt; a)` does.
My interest has really been piqued by the multitude of classes being offered this year. Last year I just attended the five conference days, but I'm thinking this year I won't be able to resist going to the class sessions. My only complaint is that all the classes are two day events, which means I would have to choose one of the six for the whole weekend slot. :( Another concern is that unlike the majority of the conference talks themselves, there doesn't seem to be a record of the classes themselves for review (at least that I could find) - are there any plans to give those who purchase access to the two class days access to course materials for classes offered that year? $795 USD for two days of material on one subject isn't quite as good a deal compared to the early bird pricing of $845 USD for five ten+ hour days of technical conference talks... Also, does anyone have any ideas what the differences between the embedded and low-latency classes might be? Both seem quite appealing, but there also seems to be quite a bit of overlap...
IIRC, there is a paper (by Crowl?) that argues that of the two definitions for `a &lt;= b`, the one through `(a == b) || (a &lt; b)` is not slower than `!(b &lt; a)`. I must admit that I find that hard to believe for general tuple-like types. 
now i want to know what his list of "(mostly mythical) top-20 list of desirable improvements to C++" are.
OK, so I see two things: (1) You want to test the functionality of the hardware that you guys develop. (2) You want to test your high-level API (that uses a low-level API that you do not control) For 1, if at all possible, I strongly suggest creating a simulator that is built using the same sources as what gets sent to the PCIe card. Then you can test (unit or otherwise) parts of the software that runs on the PCIe card. You may have to fake I/O or special chip functionality, but it will allow you to catch many errors early. For 2: honestly, this isn't so different than testing other software. I mean, we build our C++ software using operator new, file I/O, etc.... We don't test that stuff works; we have to assume that the compiler vendors / standard library implementers did their job correctly. It's no different here. You have to assume that the PCIe port API and associated drivers is written correctly. If you've been able to use a simulator for (1), then you can assume that the card's software is running correctly. Then you are left with some sort of unit tests that you can test at a high level. By the way, *your* high level functions can certainly be forwarded to a simulator, mapped memory, or something else. You don't have to rely on the port driver to do that for you. If there is some performance overhead that prevents this, then so be it; test with real hardware. But testing without hardware will certainly allow you to do more testing and allow developers to speed up their testing. &gt; What we are NOT interested in testing is if the adapter actually does what its supposed to do. ... Those types of tests are handled by our regression testing each release. In my experience, it's best to have something automatically tested if it can be automatically tested. That way you catch errors early and not just before release with regression testing. Where I work, our regression tests are focused primarily on things that either can't be automated or are very challenging to get automated. When I joined my current project, our regression testing lasted over two months -- actually longer as we found so many problems that needed to be fixed that we had to go back and fix and re-test resulting in a final release testing phase that lasted *6 months* long. Now, our regression testing is only 2 weeks long -- and that includes time to fix any bugs found.
:-) This has been mentioned in Scott Meyers' Effective Modern C++ as well :-)
This might give some idea: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4492.pdf
This is the issue exactly, I don't want to use virtual calls for critical path code, but on the other hand I don't want it to be a hairy template mess that no one other than myself can deal with.
So, would the benefits of undefined signed overflow on signed integers outweigh the benefits of using unsigned integers in regards to certain operations (like divisions/modulus) potentially being faster?
Yeah, I'm stuck having to target the flash compiler and it's stuck, probably forever, in C++03 :( If you're targeting social platforms like facebook and mobile devices then you're stuck with the tools they provide. Sucks ass because even just updating to C++11 would make things way easier.
No problem, thanks for this answer.
In Section 2.10, a preference is indicated to treat mutable as special and remove it from compiler-generated comparison operators. I sympathize with the rationale here, however, it does seem like it could result in difficult to detect errors if the intent was to compare all. What if a mutable member variable suppressed implicit generation of the comparison operators, but you could still do the following (similar to copy constructors) to force it to compare all non-mutable members. bool operator== (const T&amp; rhs) const = default; If you attempt to compare without requesting it, the compiler could prompt you that you can use this syntax if you don't care about including mutable members.
current proposal [p0221r0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0221r0.html ) doesn't generate implicit comparison operators if there is any mutable direct member.
No. I used ExternalProject in a big project and it definitely is not a good solution. Too many issues, both with it's implementation and design. Too unflexible and too many problems with having several layers of build steps to get the final dependency available. ExternalProject works for trivial dependency needs though, but it's definitely not a general solution. Also, dependency management should definitely be independent from the (meta)build system.
Could attributes not be used to be able to explicitly exclude specific variables from the default comparison operator? class A { public: int a; Thing c; [[noncomparable]] mutable ThingData mCache; }
&gt; I started to re-design concepts from scratch. I think that is why people would like to wait a little longer since you just scratched the enormous amount of work that was done previously on concepts, and haven't gotten as a far as they did. &gt; Andrew Sutton quickly demonstrated Yea well "quickly" demonstrating doesn't really show how well it will work in real-world C++ code. I think it would be wise to gain some experience with Concepts before integrating into C++ so tweaks could be made if necessary. Because it's not only template definitions that are a concern, but future language features as well.
This is a pity as it's quite common IME to have classes with a lot of members that you can (and want to) compare trivially and a single mutable field and it seems like there is no way to get the default comparison in this case. It seems to me that it would have been much more useful to generate the default `operator==()` for the classes with mutable members ignoring them which could be easily suppressed with `= delete` rather than not generate it as it can't be easily requested (presumably, using `= default` would not work).
I can't say for certain if it's better. The Endian Template is small, and only provides a minimum of what's needed to handle automatic endian conversion. As I understand it Boost.Endian provides three different mechanisms for handling endian (one providing swapping functions), whereas my primary philosophy here is "don't use swapping functions." Just for trivia: I originally created this class in 2001 as a proof-of-concept, hoping I could use it in some emulation projects (never got around to those unfortunately.) This predates when Boost.Endian came out, which was around 2002 or 2003. That said, I've used boost (in general) before on a day-to-day basis for a client project. I'm not a boost "hater", but I'm not a regular user of it either.
Thanks-- I liked this, tested it out, and it works great, so I've changed the code to use this technique instead.
I'd be curious if my endian templates / typedefs could help you in this situation. Would you be willing to give it a shot?
Awesome, thanks
&gt; &gt;Andrew Sutton quickly demonstrated &gt; Yea well "quickly" demonstrating doesn't really show how well it will work in real-world C++ code. &gt; I think it would be wise to gain some experience with Concepts before integrating into C++ so tweaks could be made if necessary. Because it's not only template definitions that are a concern, but future language features as well. I can't agree more. The implementation was straightforward and quickly reached a state of adequate capability. But even at this early stage of implementation and experimentation, it becomes obvious how many open questions remain about definition checking in real-world C++ code. For example, I wonder how this would work with template definition checking. struct D : B { template&lt;typename T, typename... Args&gt; requires C&lt;T&gt; // Note: a constrained template S(T, Args&amp;&amp;... args) : B(std::forward&lt;Args&gt;(args)...) { } }; `std::forward` is an unconstrained template, so the 0x model would make this program ill-formed. Even if that function were constrained, what are they? And in the 0x model, how would you define a concept map for a template parameter pack? What does the archetype look like? Can you even generate an archetype of a parameter pack? I don't know. I suppose you could try to put this in a late-check block... except that you can't because you're expanding a pack into a function call, and not listing statements. Given the frequent use of such common calls as `std::forward` and `std::move`, we clearly need to find a definition checking model that doesn't penalize programmers for doing obvious and recommended things. I suspect that many people think that template definition checking was a problem completely solved by C++0x concepts. It wasn't. Simple and strict rules like, "don't call unconstrained templates" don't scale because there are obvious reasons for their use, and we mustn't penalize our users for it. Thanks for calling this out. It would definitely better to gain experience with template definition checking. That said, I see no reason to dismiss or postpone the current proposal. It suffers none of these kinds of questions. After more than a decade of faking it, we're pretty good at constraining interfaces. 
As you noted, this is not really a performance measurement. The number of relocations is somewhat correlated with startup time and not in my experience correlated in any way with steady-state runtime performance.
Yes, we did get frustrated and starting CopperSpice was not a decision made lightly. We spent about six months reviewing other projects but Qt was the best base to start from. As mentioned earlier in this thread, the Qt maintainers shot down an idea to support templated QObjects. If even this modest proposal was summarily executed, you can imagine how a complete rewrite of the entire signaling system would be received. One of the other problems we had with moc is that for toolchain compatibility reasons the Qt developers are forced to marshal signal parameters as an array of void *. This may be legal and standard compliant code, however there are a lot of reinterpret_casts. Given the constraints of the supported platforms and compilers, this is the best that can be done in Qt.
Values. That is, that = would not slice (it rarely makes sense to compare a Base and a Derived).
VeiledSpectre, Thanks for your interest in the classes. There are at least two significant advantages of classes over conference sessions. One is that classes have hands-on exercises and the other is that instructors can go really deep. Both of these advantages come from the fact that instructors have enough time. Exercises take up a lot of time and so does deep exploration. Offering one-day classes would reduce the potential depth pretty significantly. So we opted with the two day class format. I understand that it makes it hard to choose. (I probably shouldn't mention that providing content that forces attendees to make hard choices is what I consider the mark of good conference. ;) I'm not certain what you mean "there doesn't seem to be a record of the classes themselves for review." If you are asking if the classes will be recorded, the answer is no. And also, no, attendees of one class will not receive material from the other classes. Sorry. The best way to attend more than one of these classes is to work with your employer to have one (or more) of these instructors deliver the class to your team. The instructors are professional instructors and most are available for onsite training. I can see that there is some overlap in what the embedded and low-latency classes cover, but the focus of each is different and quite clear. Dan will focus on hardware control and minimalist environments and Patrice will focus on raw performance. One is low-level and the other is high-performance. Yes there is some overlap, because we talking about C++, so everyone is concerned about performance and lightweight, precise control, but the focus is different. Jon 
You are correct that there are people who want moc gone or improved. But the reality is that this is not going to happen any time soon. Getting reflection in the standard is most likely 7-10 years away. It will take a while for toolchains to catch up on all the platforms Qt needs to support. My rough educated guess is somewhere around 2025 you may see moc deprecated.
You raise another moc myth that needs debunking: "moc adds some handy capabilities that can't be added any in other way." These capabilities can be, and have been, implemented in standard C++11. The CopperSpice registration system supports all the functionality that moc provided in a fully templated manner with no type erasure.
Mocking with dependency injection is fine for anything that isn't directly talking to the hardware. His question &gt;My question is what is the proper protocol for testing functionality that requires information from a 3rd party? Is a perfect example of when to use DI and mocking (you "fake" the behavior of the API calls with what it should be as far as what's needed on your end, since you only care about testing your own logic and not the API's).
&gt; std::forward is an unconstrained template, so the 0x model would make this program ill-formed. In the original C++0x concepts it would've been ill-formed, but there was work from Dave Abarahams that solved this issue. &gt; I suspect that many people think that template definition checking was a problem completely solved by C++0x concepts. Yes but it was a lot further along than Concepts TS. Plus, there was a lot of solutions to the problems that came out post-C++11, including name lookup, generic lambdas, and unconstrained templates. Furthermore, there was a clear path forward with future features in the language. There isn't a clear path forward with Concepts TS if we want virtual concepts or semantic-based concepts. &gt; Simple and strict rules like, "don't call unconstrained templates" don't scale because there are obvious reasons for their use, and we mustn't penalize our users for it. Yes and there was work done to address these issues. &gt; That said, I see no reason to dismiss or postpone the current proposal. Beyond just the template definition, Concept TS is not ready for C++17. There is two different syntax style; one should be decided on. They return raw booleans, so they won't replace type traits. I think for C++17 it would be nice just to add a `requires bool {}` so we can replace `enable_if ` with it.
Your post was removed because it should be sent to /r/cpp_questions. I believe the answer you're looking for is that streams are single-pass - when you consume words or lines from them, you don't get to consume them again. If you want to perform multi-pass operations, you should use getline to fill a vector of strings.
I've always thought the `operator=` for an inheritance hierarchy was a complete mess, but put it in the too-hard basket , I'm very excited to see this proposal which would straighten it out somewhat.
inb4 *Every programmer who calls himself like that should not copypaste rather refactor bla bla C++17 bla bla and this kind of crap.* Judging from the amount of crappy software existing everyone does that.
Nice information. With the level of detail it goes into however, I'd think it worth mentioning what happens once templates get involved and perfect forwarding, since you can get surprised if you think &amp;&amp; always means rvalue reference. https://isocpp.org/blog/2012/11/universal-references-in-c11-scott-meyers
Yes, that looks better indeed. Yet I think the C++11 only approach is better, it allows also for checking multiple interfaces.
The above can also work with C++11, I was just using C++14 (and even `_v` from C++17) to shorten the example. Also worth noting that with the trailing return type technique you can not simply negate a condition and hence it can not be simply extended to exclude overloads when some interface is *not* available. 
Paul solution is not compiling on VS2015 at the moment (**member function already defined or declared**) which is too bad because it's elegant :) 
yeah, I see that as a disadvantage too. But as long as I don't have to negate interfaces, its much more clear and readable. And less boost::mpl is always good :)
Feel free to use whichever you feel works best for you. Going forwards QML and Components will be developed the most heavily though and in my brief experience can produce desktop application which look and work just as well. If you're happy with QWidgets though, stick with that.
There was a cppcon talk in 2014 where they talked about doing a similar thing with void_t. The example testing for copy assignment operator; template&lt; class T &gt; using void_t = void; template&lt; class T &gt; using copy_assign_t = decltype( std::declval&lt;T&amp;&gt;() = std::declval&lt;Tconst&amp;&gt;() ); template&lt; class T, class = void &gt; struct is_copy_assignable : std::false_type {}; template&lt; class T &gt; struct is_copy_assignable&lt;T, void_t&lt;copy_assign_t&gt; &gt; : std::integral_constant&lt; bool, std::is_same&lt;copy_assign_t, T&amp;&gt;::value &gt; {}; //Elsewhere template&lt;T, class = std::enable_if_t&lt; is_copy_assignable&lt;T&gt;::value &gt; &gt; void foo(); Using that sort of idiom, you can pretty much straight up implement concepts lite. However, from experience, I would be wary using this sort of logic in your code unless you ABSOLUTELY HAVE TO, as the compiler will give you very incomplete information to try and solve any function selection bug.(for example, if you start combining these concepts, the compiler can tell you that the enable if did not succeed but currently won't delve deeper than that, leaving you to figure out which aspect of the concept failed.) Of course when we call get concepts lite the point is moot.
I wonder if a better editor may help with functional programming. It may make sense to make lisp code be in a 3d topographic map - so you can see that the peaks are what's executed first.
This again…
He seems to be confounding declarative programming with functional.
so you wrote a short article to complain about functional languages being too abstract to be convenient? It's still "compiled" to imperative / procedural code so the cpu can understand it, not sure what you're getting at with this
Functional languages are a kind of declarative language, though
If you want to negate the interface, then you will need to use [conditional overloading](http://pfultz2.com/blog/2014/08/22/overloading-requirements/). For example, if you want to overload on streamable types and ranges and print them. Using C++14 with the [Fit](https://github.com/pfultz2/Fit) library you could write this: FIT_STATIC_LAMBDA_FUNCTION(print) = conditional( [](const auto&amp; x) -&gt; decltype(std::cout &lt;&lt; x, void()) { std::cout &lt;&lt; x &lt;&lt; std::endl; }, [](const auto&amp; range) -&gt; decltype(range.begin(), range.end(), void()) { for(const auto&amp; x:range) std::cout &lt;&lt; x &lt;&lt; std::endl; } ); So if the type is not streamable, then it will print the type as a range. For MSVC, you will have to write it using separate functions because their generic lambdas are buggy: template&lt;class T&gt; auto print_with_cout(const T&amp; x) -&gt; decltype(std::cout &lt;&lt; x, void()) { std::cout &lt;&lt; x &lt;&lt; std::endl; } template&lt;class T&gt; auto print_with_range(const T&amp; range) -&gt; decltype(range.begin(), range.end(), void()) { for(const auto&amp; x:range) std::cout &lt;&lt; x &lt;&lt; std::endl; } FIT_STATIC_LAMBDA_FUNCTION(print) = conditional( FIT_LIFT(print_with_cout), FIT_LIFT(print_with_range) ); Finally, you can do this in C++11 as well by using function objects instead of separate functions.
Yes. The already voted-in "is_detected" family of helpers is based on this. http://en.cppreference.com/w/cpp/experimental/is_detected
As always, the solution I feel lies somewhere in the middle and is part of why I feel functional is taking JS by storm (react helps too ).
Did you read the article? The author is saying that functional languages are too hard to write in. It has nothing to do with translation to machine code.
Hmm... To me, functional programming is more about looking at your data like it needs functions applied to it. I think of the requirements is that functions are first-class objects and with lambdas and functors in C++, you pretty much have that. Look at super cool stuff like std::transform and std::for_each. It's also relatively simple to write functions that take functions as objects and do whatever. Functional programming has definitely become a good part of the language since C++11.
Good example where this is usefull is json. In a language with a proper sum types support you can easily declare a type which can hold all possible json values. E.g. in Rust: enum Json { Int(i64), UInt(u64), Float(f64), String(String), Boolean(bool), Array(Vec&lt;Json&gt;), Object(BTreeMap&lt;String, Json&gt;), Null, } while all C++ json libraries that I've seen use a combination of enums/unions/structs/raw pointers to get the same result. And they also need to declare constructors/destructors manually to ensure that object is initialized to valid state and will be properly deallocated, something that you get automatically in a language with sum types.
Stateless REST servers have no state
I've seen something similar many times. Average C++ programmer using templates to solve a problem when it doesn't call for templates. They usually talk about why templates are bad , useless, etc. but this might be the first time I've ever heard one take it to the level that an entire category of programming is unfit for consumption without actually mentioning a functional language or specifics about said deficiencies.
Care to elaborate?
[Are you kidding?](http://learnyouahaskell.com/chapters). I hack C++ and GLSL e'ryday, man... but it's not hard to learn (and even [design](https://github.com/aubreyrjones/parasol)) a functional programming language if you approach it the same way you learned to program QBASIC|Python|C|whatever back in the day. You've got to play with it, follow along with the tutorials. Don't expect to jump in the middle and "write a simple webservice" or some nonsense. Ignore blogs and academic papers, practice humility, and start back at the beginning.
Why I don't care?
Are algorithms more verb-like or noun-like? Does the answer make one programming paradigm more suitable for algorithms over the other then?
In the above example it's not important but in some real, robust code it might be. But given that the template parameter is there to create a dependent template parameter to enable SFINAE, I'd rather not obscure it and instead move the `const ...&amp;` to where it belongs: template&lt;class T = Interface&gt; auto query_interface() const -&gt; decltype(std::declval&lt;const T&amp;&gt;().interface()) { return i.interface(); } // ...
The real world is comparatively simple actually. Just simplify it to inputs and outputs and keeping it functional is easy. Heck most silicon is designed in a functional way for that reason. Internal state is the hard part, especially since you want to minimize timing requirements if at all possible. Haskell is lazy to make re-adding those kinds of dependencies difficult. Example being when I calculate this increment an unrelated variable.
Did they fix all of the bugs that you filed already? I find their bug-fixing policy rather incomprehensible. GCC prioritizes fixing regressions (i.e. things that were broken compared to older versions) and they postpone fixing broken new features (constexpr, variable templates) to the next major release (6) instead of the point release (5.3 or whatever). The entire GCC 5 series could have had a much better quality had they fixed broken new features immediately. 
CPP hasn't died because there's still and likely always be areas of programming that you just can't do with the level of abstraction that functional programming requires. This subreddit isn't a good place for this article unless the point is just for us to talk about how great we are. 
While I admit that functional programming required me to change the way I think a bit, it isn’t as weird as claimed here. And it didn’t take long for me to get used to it. Now, when programming in Scheme, the Scheme-ish solution is usually the first one that comes to my mind.
An algorithm is a verb, and a data structure is a noun. We talk about algorithms by describing *what they do*, step by step. The high-level description for "Bresenham's line algorithm" is "it draws a line". The low-level is how it draws the line, laid out step by step, with what the algorithm does as each step. Verbs within verbs. But we talk about data structures by describing *how they are*, for example by attaching a bunch of adjectives or describing its attributes. A "binary tree", for example, "is composed of nodes, each with one or two children." That doesn't describe anything you can *do*, it merely describes what *is*. We can be more precise still without describing any operations whatsoever: a balanced binary tree **is** a binary tree where the difference between the heights of left and right subtrees of any node is 0 or 1. We still don't have any verbs in there.
Oleg here. It was my paper that prompted Bjarne to do all the heavy-lifting language work to bake the notion of equality into the language. The floating-point types are not *regular* (in Stepanov's sense) and, thus, defy the natural rules of copying/assignment/equality. Things work more or less well outside of the NaN value which just breaks every single comparison. The type is not totally ordered. More generally, the normal equality via == should not be used for such types. You have to do something like this instead: http://c-faq.com/fp/fpequal.html
Generally, one can certainly invent additional syntax to express the "non-comparable" semantics that you seek. You would have to weigh the language impact from several perspectives: compiling legacy code, teachability, etc. As for the attributes, today they are intended as "non-semantic hints" to the compiler. That is, an ISO C++ conforming implementation of the compiler is allowed to ignore these hints as they do not change the meaning of the code.
Oleg here: this very subject came up when I submitted the initial (minimal patch) proposal to the Committee. Fundamentally, there are two distinct uses of `mutable` members: 1. Thread-safe things such as `std::mutex`. The point here is to allow operations on `const` objects as these members are self-synchronized. 1. Data caches that do not contribute to the salient state of the object. These do not effect equality comparisons and, thus, can be safely ignored. Bjarne says that (2) was the initial usecase for `mutable, yet there is wide-spread (and reasonable) usage in line with (1). This is especially important today as the Standard has explicit threading and memory model specifications. There was no way to reconcile these legitimate views and so the compromise is to suppress equality generation when `mutable` members are present. This forces the type's author to implement equality manually, thus attempting to impose correctness. Yet the usability of the new feature is reduced.
Great answer. Most of my other response went left field, but yours explain the dilemma well between functional programming and algorithms.
For those who are stuck in c++11, note that while std::integer_sequence and related functions are left in c++14, the implementations of these are dead simple. I know from experience if you can get a copy of the relevant gcc header file, you can easily lift the implementation directly into your own project until you can upgrade. EDIT: Well that's actually probably a gpl violation. But you can probably look at the relevant header file and discover the implementation and easily reimplement it yourself.
I'm just happy(?) someone else also used the somewhat weird name "ContextScope" for a class. This makes me postpone renaming that class in my code until even later. ;) 
This is not really a genetic algorithm due to a lack of a fitness function. This is more like a self reinforcement password generator.
According to [this](http://meta.stackexchange.com/a/12528) answer posts on Stack Overflow are automatically under a creative commons license. IANAL but I think that it should be safe to use code from there for nearly everything. There are dozens of implementations on SO. E.g. [this one](http://stackoverflow.com/questions/17424477/implementation-c14-make-integer-sequence/17426611#17426611).
I can't see this working. There are no degrees of fitness for a password. It's either right or wrong with no way to measure if you are close. Since GA is a kind of hill climbing optimisation, there is no landscape for it to climb, everything is flat with one 100% spike at the right answer that you can't see until you test it.
Shame c++ had to go down its usual route of being insanely verbose compared to other languages. Compare &gt; f(x) = (x, x^2) &gt; a, b = f(5) With &gt; auto f(int x) { return make_tuple(x, x*x); } &gt; int a,b; &gt; tie(a,b) = f(5); I guess it's not so bad. I would have liked the comma operator to be used instead though.
This seems like a bit of a niche thing to include in the standard library. What rationale are they giving for adding it?
having support for UI in modern C++ would be nice. Actually I've been thinking if SG14 shouldn't start to work on a vulkan implementation for C++.
No modules? No Networking TS? WTF are you doing C++17 for?
Yes, that's what I meant but I was on mobile and the formatting was making me lose my mind.
Yet modules are already implemented in two compilers (different designs thought) - MSVC and Clang. Can't we just pick one already? If neither concepts nor modules will make it into C++17, it will be effectively a minor update. It will be, IMO, complete and utter failure too. Under "failure" I mean that these features are long overdue. I know it's hard to implement them. Still - the fact that its hard, doesn't make it easier to accept. And with "New Kids on the Block". Dunno...
Interesting concept. Unfortunately, the app they talk about (https://worklogassistant.com/) is not open source.
There is lots of other stuff moving into C++17, I hope that there is a solution for concepts. But currently I don't see Modules in the next standard, but we'll see. I'd like to see so big features first in TS. Getting concurrency and parallelism TS into C++17 the standard is IMHO much more important.
No compile-time reflection at all too ?
Also no uniform call syntax... No excellent Eric Niebler's Ranges.
So, NOTHING of major things what we all expected from C++17.
Agree completely. Also add the concurrency TS. Almost since it came out std::future is unusable for a lot of real workloads because it only has the blocking future.get. Future.then is absolutely needed for any useful version of future. I second that c++17 needs modules and networking TS. I would rather have C++18 than this crippled C++17. 
This would be great. 
&gt; Making exception specifications part of the type system. I was under the impression exception specifications were deprecated.
I imagine it wouldn't be easy to postpone things when we're so close to the deadline, but it does feel a bit underwhelming in comparison to C++11. Maybe C++20 could turn out to be the major update instead?
Vulkan != OpenGL, it won't replace OpenGL simply because these are very different approaches to 3D graphics. More realistically, if Vulkan will ever get as spread as OpenGL (Apple doesn't seem interested to provide a Vulkan implementation for now) I would expect to see the OpenGL API reimplemented in Vulkan, but I doubt OpenGL will be replaced in the foreseeable future. 
Homework questions are better suited to /r/cpp_questions.
My point was that as long as Vulkan is not fully cross platform, you can't really use it as base for the C++ standard. 
That sucks. I thought that GCC 5 was only getting really serious bugfixes, but that all the "new feature support fixes" that you filled were going into GCC 6 (GCC has kind of a weird way of semantic versioning where normal bugfixes go into the next version but really serious stuff can go into the previous one.. i really dont understand them but language support is not a major issue and has to go into the next version). Good luck with filling bug reports. I recently discovered `delta` and it did help a lot with some bug reports, but then I was getting some wrong warnings in clang and because the warnings did not really get the compiler to fail (not even with -Werror) i coudln't really use it that much.
I would really love to see modules... Oh well... at least I hope committee guys get it 100% right when they are included even if it takes some more time. I think C++ syntax and facilities is quite good at the moment especially with C++11 new features. I would rather focus right now in improving and extending the standard library. A networking library, extending std::string to include helper functions like split (please bring split!) and even a simple command line parameters parsing library (like the one in boost) would be great! Although I will be very happy if the Filesystem library gets included for C++17, it will simplify so much the writing of new programs which uses the filesystem.
Keep fighting the good fight (here and at MS)!
Coming from a UI programmer: this is a bad idea. Whoever thought of this isn't thinking things through.
Why concurrency TS is not in? Is future::then and those such a big change? std::future is incomplete right now as-is.
The rationale is that C++ gets a bit of a bad rep because it doesn't have a standard library the size of say .NET or Java. People often say "C# (or Java) is so much easier because I can do all of this cool stuff." The reality is most of that cool stuff is just default library functionality and has nothing to do with the language. If you strip away the default libraries, the languages are very similar, so there's been a big push recently to create a bigger standard library for C++. 
It's not really any better but if you don't use auto for the return type, you don't need to specify it in your code, making it look a little more like the first line of your python program: std::tuple&lt;int, int&gt; f(int x) { return {x, x*x}; } This also has the advantage (until modules) of being split header/source.
The release seems not to be as big as it was intended. But I do appreciate all the effort that the people who are working on the standard are putting into it. It is really a tremendous amount of work and things are *always* more complicated than they seem at first. Thank you! All the stuff listed in the link are making the language and libraries better and my life easier. Again, my thanks to all the people who are putting their time and effort in the advancement of the standard. Every bit counts. 
Wow, Bjarne's Thoughts about C++ 17 is definitely a joke. No ranges, no modules, no coroutines, no reflection, no ... But what about inline variables, structural bindings, constexpr_if ?! I really want them...
Why is std::tuple&lt;int, int&gt; foo(int); int a, b; std::tie(a, b) = foo(1); better than void foo(int x, int* a, int* b); int a, b; foo(1, &amp;a, &amp;b); ? I'm applying two principles here: [KISS](https://en.wikipedia.org/wiki/KISS_principle) and "when in Rome do as Romans."
&gt; The futures improvement are language changes but are already in well tested in Microsoft’s Visual C++ and C# compilers. What are the language changes here? Coroutines?
Agile is something more attributable to a codebase than to a developer. For my money, it's not how you do it that matters, it's how you leave it. Specifically, can the project's code, and more specifically your code, be easily understood by others, easily modified by others and safely used by others while still meeting the functional and performance criteria. That type of codebase agility matters far more than the process agility. C++ developers who are adept at that come see me.
It doesn't compile though. https://ideone.com/qECZSD
Ding ding ding. You win the prize for common sense when applied to development. Now go back and write 300 pages of documentation to appease the gods of process. 
If you use non-const references, the ambiguity just moves to the caller. It it not immediately clear what is happening when you have `int b, c; foo(a, b, c)`. With tuple unpacking, you have something like `auto {b, c} = foo(a)`. It's just easier to infer that `b and c` are the returned value and `a` is the input arg. Default construction does not happen. consider the function std::tuple&lt;std::string, std::string&gt; foo() { return std::make_tuple("hello", "world"); } The default constructor for string is never called. I just think that using tuples leads to more readable code. If you can use it, do it, especially in local code.
Thanks for that great feedback! Those are some really good points - I think I will need to write a follow-up blog post now =) *EDIT*: I've rewritten the Loguru implementation now using a linked list, and it's much better. I've described the new method in the article under the EDIT header.
What exactly is the use case for this? With no way to receive input it wouldn't be a great user interface. I guess you could have a thread reading from standard input and use that as input to the UI, but that isn't exactly normal. I'm also not too excited about a standard UI library in general. I'm not convinced UI is a "solved" enough problem to necessitate a spot in the standard.
what about input?
&gt; For a while I called Agile a cult, for a while I called it a fad, and for a while I called it something cooked up by MBAs. It is neither. &gt; Then it dawned on me, Agile is a route to transferable seniority. To the same degree that having a successful career is a route to retirement (if you as someone who has had a successful career, they will never say "I don't care about career, I just care about getting to retirement"). No, agile is not a "route to transferable seniority". &gt; But then I was talking with an Agile guy and I suggested something that didn't fit his obsessive compulsive world view so his response was, "You can start altering Agile after you have 5 or more years under your belt." This makes sense: the agile methodology has a lot of rules that seem weird at a first glance (especially if you have not seen them applied in practice for a while) but that add large speedups in development and greatly increase a team's ability to react to changing requirements. I have been in more than one team where they tried to practice agile, but many policies were applied in a lax manner and the benefits of the methodology were not there. Attempting to change the methodology without having implemented it first, is (in some cases) worse than not applying it at all. Your friend probably could have explained it better, but this answer doesn't (by itself) give him an obsessive-compulsive view of the world. Also, your examples are not problems with agile, but problems with _someone_, after a lot of things have gone wrong (management placing someone unqualified in a management position, someone using their experience for arguments from authority, ...). These are not "agile problems", they are "HR problems".
https://gcc.gnu.org/wiki/reload 
&gt; you got the normal amount of stack space which you quickly exceeded and ran into some other process's memory space Pretty sure no OS implements it this way.
Huh, you're right. My bad. Apparently this is being fixed in c++17
The author clearly hasn't used the Copy-Paste method in the article, which is why it turns out as "Copy-Pate" at the top. Irony aside, this isn't the first article that they've done on the "last line effect", which I find is actually a really interesting observation of programmer behaviour, and worth knowing about in general.
[llvm](http://llvm.org/) 
You can perhaps give these a go: * https://github.com/SFTtech/openage * https://github.com/vcmi/vcmi
I'm getting a dead link for that one.
You have to make an account on unrealengine.com and link it to your GH account to see it.
it appears that the last bug that I filed was fixed today in GCC 6. So C++14 constexpr should now be at least beta quality. Now I am waiting until a GCC 6 binary appears for Ubuntu 14.04 :)
Does it use C++11 and C++14 extensively?
Take a look at IncludeOs https://github.com/hioa-cs/IncludeOS It is a unikernel written in C++14 that runs on an x86 VM They are also using the ISO C++ Core Guidelines (except the parts which rely on the GSL)
Any chance it is online somewhere? I feel exactly the same. 
Not too extensively yet, but those features are starting to filter in. LLVM is very well designed, though.
I scrolled through vcmi code and was greeted by 7 levels of curly braces nesting and one-letter variable names... While good for learning software in general it's not where you learn best modern practices. I'll check openage tomorrow.
If you like games and want to see some cool net code check out the repo for /r/haloonline
[CopperSpice](http://www.copperspice.com/)
One can be writing "modern C++" without extensively using 11 and 14. The goal isn't to use language features for the sake of using language features; the goal is to avoid explicit new/delete and C-isms.
I don't know about gcc's code quality; but most GPL'd libraries are a non-starter for this question's purpose because they "infect" a developer reading that source code for months if they need to work on anything not-GPL'd.
But the features in 11 and 14 are what make C++ modern, e.g. lambdas, type deductions, move semantic, async, just to name a few important ones.
See my other answer [here](https://www.reddit.com/r/cpp/comments/4845x5/unpacking_tuples_in_c14/d0hukii) Most of my points I've defended there. You're right `std::experimental::apply` is an edge-case. I was just using it as an example. OP's article implements zip/transpose using tuples. You can also use tuples to replace local temporary structs that has nothing to do with your code's interface. The use of tuples shouldn't dominate your code either. Consider this nifty example: bool Vec3::operator==(const Vec3&amp; o) const { return std::tie(x, y, z) == std::tie(o.x, o.y, o.z); } A very elegant (and zero-overhead with optimization) abstraction. This can be extended to a generic `Vec&lt;N&gt;` using `std::tie`, `std::make_index_sequence`, `std::get` etc. (4) is a personal taste issue I guess. The reason I avoid using pointers when possible is because they are used for so many things. (input, output, arrays, dynamically allocated owned memory). I prefer to use (raw) pointers for dynamically allocated memory that isn't owned by the callee, but this lets the callee know that what they are dealing with is dynamically allocated somewhere.
&gt; By comparing issue density in Unreal Engine 4 to other codebases in the game development industry, **Unreal Engine 4′s code quality is in a league of its own, achieving issue density nearing large mission critical software projects**. I would guess that Unreal Engine 4′s code quality success is attributed to two factors: Static analysis by PVS-Studio and Unreal’s open source initiative. [Source](http://blog.klocwork.com/static-analysis/analysis-of-unreal-engine-4/) It may not be "modern" but it's definitely "good" 
Guidelines for humans are good, checks performed by machines are better! Are there any tools that provide you with more confidence that you are indeed using modern idioms where applicable?
Flinging naked pointers around everywhere is not good. 
I am sure that the bunch of nobodies at Epic didn't think really long and hard at the type of data structures they wanted to use. By the way, it's not because you see a naked pointer that it's not garbage collected for you. Ownership is very well defined
I'm hoping these don't go in. The function type system is already verbose and unintuitive. Adding throw specs to signatures will make it worse. The question remains whether the good outweighs the bad.
The risk is that someone looks at GPL'd code, then unintentionally copies more-or-less identical implementation(s) into GPL-unsafe. As a result, businesses often require a period between when someone looks at GPL'd code writing any code to be included in that business' product. At MS, for example, that "cool down period" is around 2 weeks. Maybe "months" was exaggerating.
I love your suggestion that the Agile Manifesto fits a single sheet of paper which is what makes it great and lives up to its platonic essence. Yet I could probably find you a Agile certification course where the study material goes past 1200 pages. It is the later Agile that so many are evangelizing. 
Oh, well then. I never realized this was an issue, thanks for explaining. 
Eugh God no. Please no. The C++ standard library shouldn't be about stuff like this. It is for containers and algorithms and other stuff that is hard to built. UI frameworks already exist.
I learned a fair bit of modern C++ from reading the [VideoCore](https://github.com/jgh-/VideoCore) source, which is a utility library for Apple's audio/video encoding frameworks, and RTMP. I modified it to provide RTP streaming on an internal fork.
Lol at missing &lt;cstdlib&gt; in headline due to lack of &amp;lt; and &amp;gt;. Rookie mistake!
I found it interesting that it was early in the article rather than at the end.
Run `gcc -dumpsecs` and look for `pthread` in the output.
&lt;sarcasm&gt; Everything will be fixed in the next version! Hooray! &lt;/sarcasm&gt; It's a weird world we've made for ourselves, but I keep coming back to work and enjoy it. I just don't know what I'm doing in Stockholm. 
I thought it was a mistake to hide all of the standard library behind the std namespace in the first standard, and it's nice to see confirmation it was a mistake. 18 years of people wearing out their colon keys and writing ugly code, all for a system that has reached such levels of cruft that the C++ standard now just shrugs and says "Well, fine, do whatever you want then." The only benefit to hiding everything in std is not breaking code when a new function gets added to the standard, but at the expense of writing ugly code, making it harder on new programmers, and breaking old code.
It is absolutely amazing how often, the answer to "why is this mess here", is "history". The only way to have a grasp on messes is to be aware of their history.
Create a structure where each level of the tree is a vector. Instead of pointers they will contain an index into the vector one level down.
"Why is more complicated than you might think" Here's someone looking for the real answers.
Yeah that makes sense, thanks for clarifying.
You are correct. C++ futures /are/ incomplete. As one of the HPX developers, this is an issue that is particularly close to my heart. I want to see the Concurrency TS adopted as soon as possible. There are serious performance implications from suspending an asynchronous task by calling .get, and .then and when_* allow you to write true continuation-style, wait-free code that lets you avoid suspensions (if someone is really interested in those performance implications, ping me with a reply). However, the Concurrency TS was just finalized last meeting, at Kona. It simply has not been out in the wild for very long. Are there a number of existing implementations? Yes, there are - but many of these implementations are from libraries such as HPX or folly which implement standard library compliant futures but are not full C++ standard libraries themselves. Do any fully-fledged, production C++ standard libraries currently implement the Concurrency TS? I am honestly not sure. Regrettably, it is 1:45 AM here in Jacksonville, and according to the agenda we're starting at 8:00 AM so I do not have time to check :). My guess is that there are few implementations where __cpp_lib_experimental_future_continuations &gt;= 201505 holds true. I believe the Concurrency TS should go into the standard, and I'll push for that to happen. Lets give vendors some time to implement the TS and get it out to users, so we can get implementation experience. And there are other things in the pipeline (synchronic comes to mind) which we may want to get into a future iteration of the Concurrency TS and then into the standard. Also, we need more than just .then and when_* to make futures truly usable. std::async is not going to be particularly usable on posix systems if it's backed by pthreads. We need to get executors right, get executors into a future Parallelism TS, get implementation experience with said future Parallelism TS. Coroutines are also pretty important, too, because they basically give us a more elegant way to write continuation-style wait-free code which avoids suspensions. **TL;DR - We just published the Concurrency TS in the last meeting in Kona; we need to give vendors time to adopt TSs and get implementation experience before putting TSs into the IS. Also, futures need a lot of other things to be "fixed".**
Another way to see this is that Json is an abstract base and its members are concrete derived classes. The correspondence to an open sum type is easier to spot under the hood where we can imagine the vtable pointer serving the purpose of a tag in a tagged union. Where someone would want to use a sum type in C++, the right solution is usually virtual inheritance. I don't think your assertion regarding resource allocation is correct. That's a separate issue from types.
Regarding the Parallelism TS, SG1 had strong consensus (15 strongly for, 2 for, 1 neutral, none against/strongly against) today to put it into C++17, and to send it along to LEWG. There is an issue (a concern about API/ABI issues relating to execution policies, a summary of which I will omit as I cannot do it justice at 2 AM). I see that it is the first agenda item tomorrow morning for LEWG is the Parallelism TS. I think the Parallelism TS has a pretty good shot of going into 17.
I am not aware of any current proposal to fast-track the Concurrency TS v1 for C++17. We just finalized the Concurrency TS last meeting. Parallelism TS was approved by SG1 today with strong consensus, it is going to LEWG tomorrow. I am hopeful it will go into 17.
SG7 met this evening and made forward progress on that. That is definitely not something we'll have for 17, though.
Funny, but better suited for /r/programmerhumor .
Shameless plug: https://github.com/mosra/magnum :) C++11, new features mainly in the internals to make it "just work", trying to keep the API intuitive without template overuse.
Dude, it's more complicated than you think.
Awesome! By far my favourite framework. And just in time too :P Good job team!
Maybe try looking for a contracting gig to start rather than a full time position. Companies are far more willing to give you a chance if they know they can end their relationship with you without any trouble/fanfare. Full time gigs are like marriage whereas hourly is like dating and I've seen plenty of companies use a contracting job as a sort of "extended interview" which ultimately did lead to a full time position. 
Going all-out SoA every time can be overkill. A good middle ground is having a not-shit short vector library that gives GLSL-like API. When you have a bottleneck then by all means engineer the shit out of the problem, however, most likely you will find it in the GPU these days, that is where the heavy lifting is going to take place. Having a nicer API to work with with complimentary performance benefits is not a bad thing. I am not commenting on the code in the article but about short vector code in general. It has it's moments.
This is about germany? I have studied geography with focus on geoinformatics. So not a CS degree only part of it. The door opener for me was that I worked several years during my study as Linux Sysadmin / Developer for my science department and later as a working student / student trainee 20 hours a week in a software company to develop software. Additionally I had a github repository with opensource projects that I could use to show off what I am up to (different things from web development to number crunching on GPUs with c++). The problem is, in germany you are first judged by your education, professional background. Additionally you are most like pretty young and today not only working experience but also life experience is highly valued. Looking at your c++ projects on your website I'd say you are absolutely talented but I'd strongly suggest to get a formal education. Studying CS is not only about software development. Many other things might come in handy for a job later like all the math, logic, designing and understanding algorithms/data structures, technical understanding of electronics and so on. There are also many different course of studies offered in germany that all have a different focus and might suit you more than a traditional CS study. Seeing you are also interested in photography maybe something like Medieninformatik (media informatics) could suit you well. Getting an internship or place as a working student is pretty easy then. A degree will also pay off at salary negotiations. Good luck! EDIT: Of course I forgot to mention many other important things like studies are actually a great time. You meet lots of new people. You could spend some time on a foreign university which is good for you and (if thats important for you) for your career. Take your time and learn something properly and get a decent degree. You will have to work for the rest of your life (or at least a big part of it) anyway and didn't harm anyone to have some fun first and try different things. 
auto modifies storage duration!? Where is this mentioned?
I'll play devil's advocate a bit here. If you want to be a professional software engineer, then I think you should reconsider quitting, and complete the programme to end up with a formal qualification in the field. I know it's something of a cliché about talented geniuses quitting college to go on to be majorly successful. But the reality is that only works for a tiny fraction of people, and they often have rich parents to help fund and sell their ventures. The rest would benefit more from sticking it out and getting the certificate given the risk and reduced career prospects. You might find a few years down the line that your career options are limited without it, and regret quitting. I think you might be somewhat premature in deciding that the course is not interesting based upon the first semester. It's the first semester. It's by design going to be fairly simple, aimed at bringing people with widely differing backgrounds and levels of experience and skill up to some basic level so they can then cope with the rest of the course. They have to cater for people who are already skilled, but also talented but inexperienced people who have never done any programming before. You would likely find the course becomes increasingly demanding and intellectually satisfying if you keep it up. You can likely already look at the course content right now, to see what's available. And hell, you're at a university; there's plenty of scope for doing stuff outside the course syllabus--go and proactively find people doing stuff that interests you, and get involved. That said, it is possible. One of the senior people in my group has no academic qualifications after high school, but spent years working in a major ISP and working his way up to become a skilled developer and now architect. He's the only one though. I'm a C++ developer but I didn't study formal CS; I studied molecular biology, worked as a programmer for a bit, and went back to do a PhD in biology involving a good chuck of computational image analysis. I picked up CS theory and practice on the side, though it's definitely not going to be as comprehensive as someone who studied formal CS; I have a lot of specific domain knowledge and other skills which are equally if not more valuable for some people. That said, I used to think I was a shit-hot programmer, and if there's one thing doing a PhD taught me it was perspective and humility--I now have some idea about just how little I really know. I'm a reasonable programmer, with a good bit of knowledge in certain domains, but I'm now well aware I'm not up there at genius level. Don't make the mistake of thinking that you know it all at square one when you might not even be aware of a whole lot of important stuff. The course might teach you a little bit of that. The problem you have right now is that most jobs have a minimum entry qualification, which you don't have. Your application will be automatically rejected. Quite a lot of employers also have "or equivalent experience" to cater for skilled people coming in via a different career path, but you need to get the experience for that as well. It's a tough situation, and I was in the same place 15 years back. What I would say is that having the qualification helps open the door for jobs where you can't demonstrate the experience (yet). Later on, the qualification becomes less important. I would say that you should think about the longer term as well. If you decide to change careers at a future point in time, your experience would be less useful than a degree certificate. You might not think this would happen, but you might get frustrated/bored/depressed and want a switch; or get RSI or some other medical problem, etc. Factor the possibility of that into your considerations as well.
Create arrays of shared pointers for each data type (possibly using custom allocator(s)). Each AST node is aggregate of shared pointers to its constituents.
For clarification, it's C++17 *library* feature complete. The language is still missing a few things right now, and they're a ton of work to implement due to the historical compiler architecture. I look forward to seeing this big project yield some cool results. MSVC's TS implementations are going very well so far, so once the language is done, it's going to be really nice.
&gt; I second that c++17 needs modules and networking TS. I would rather have C++18 than this crippled C++17. We all would, but the problem is then something else means that C++18 becomes C++19 and eventually C++1A and so on. We saw that happen with C++11 where it took 13 years to get the new standard out. Better to have a smaller C++17, and then the things we want in 2020, than to have nothing until 2020 or later. 
Yes, only C++11 constexpr, for now.
Don't know where you are, but here they have technical recruiters. These folks can find you contract gigs but they take some money off your hourly rate (the downside). But, they don't get paid unless you do so they're pretty motivated to find you work. I've also heard of folks finding contracting work off Craig's list but I have no personal experience with that at all. Seaching google for 'contract programming jobs' comes up with some decent seeming options as well: freelancer.com, indeed.com, guru.com, simplyhired.com, etc. 
C++17 library feature complete _so far_, as C++17 isn't finalized. 
Slowly getting to full dependent types, just in the most horrible way possible. It's workable I guess but just awful to read and must be a nightmare to maintain. Having 2 different languages for the types and the values is awful. 
Don't they also mean added the C++17 changes, since they're not even totally C++11 complete are they?
Do you have more info on that? Just curious.
Gosh 2016 just started. That means almost 2 more years to set c++17 in stone. There should still be time to push for more content - at least concurrency, and networking since they are implemented already and delivered as isolated library changes. What about to vote them in and get additional feedback through 2016? If there are really serious issues found which I doubt I understand there wouldnt be enough time to fix it. But in that case just strike it out of the standard draft. Revert the standard chapters to state before they were added. Be more aggressive!
And obviously `export`. . . But let's not go there :D
A codebase that has a low issue count from static analysis does not imply that it's of high quality. This does not factor in other elements such as architecture, performance, organization, readability, and ease-of-use. The author doesn't have enough experience with the engine to give a well-informed opinion.
My work laptop/brick is a Lenovo W520, IIRC.
Great machines though. My main machine is still a T420s. 
Whoa, what? What does MSVC use instead of an AST then?
AFAIK it just doesn't build a _complete_ AST. Why? I don't really know. Maybe because when they started implementing the compiler they didn't knew it was required for two-phase lookup, or maybe two-phase lookup wasn't even a (well understood) part of C++ back then. Without a complete AST the compiler is also likely to uses less memory and be faster. This might not be that much of an issue right now (unless you are building something huge), but could have been a huge problem 20 years ago. Probably all of these things combined plus more influenced the decision to avoid building up a complete AST. Some MSVC engineers and lead comment on reddit every now and then. Maybe /u/STL can ping them and they can point us to a better explanation of how MSVC works internally and why (this is computer science history). Last time they commented IIRC they mentioned it was challenging to implement two-phase look-up because they would need to re-design some fundamental parts of the compiler, but they are on it. It just probably takes a lot of time. In particular since they will likely need to offer both two-phase lookup "styles" for backwards compatibility. AFAIK clang on windows doesn't support MSVC "two-phase lookup style" yet, so MSVC would be the first compiler to support two "two-phase look up" styles.
I like templates! :-)
What are you referring to? What is getting us closer to dependent types?
Unfortunately, not using two phase lookup mean that templates in all versions of MSVC are broken. In most cases this means code that won't work in compliant compilers works fine in MSVC, but occasionally it's the other way around. They've fixed many of the former with checks, like noticing that you forgot `typename` in a scope it was needed. So they've gotten a lot better I guess, but the fundamental problem is still fundamental and I don't think they'll ever get templates right without implementing two phase lookup.
Not true if we are talking CUDA.
Solid series.
In case we don't know what to define.
I don't get your question. With `int a` you declaring variable and with `#define a` you defining macros which is empty. Those are 2 totally not related things. You may also want to edit your question since there are just 2 empty define's.
And the vendor for that compiler was one of the most vocal in getting it outright removed....
Start Googling things that interest you and building on your knowledge. Write stuff that solves problems that you see. Learning the language is only the tip of the iceberg that is programming.
I think the worst thing you can do is to not write code. I don't think there is a way to avoid write poor code at first. It's a learning process, write the code, read the code, improve the code. Read other code, identify the good and bad in other code, think about it, talk about it, and maybe adopt the better habits you see. The contradictory advice you got is typical - its a rich language, there a lot of ways to write it, and as many opinions on those approaches. There is no right answer. You also won't solve this quickly or easily either, it takes years. Enjoy the learning process, strive to improve all the time, keep reading and reviewing. Jump in. Do. Care. Repeat.
Get out there and make a couple mistakes. It won't all be "good". Some of it will be "good enough". If you try your best, you'll do really well, and you will always improve-- don't worry so much! :-)
Just do it. Don't worry about accidentally writing bad code when you don't know any better! What can really help is to ask for code reviews (although this is not the best subreddit - try /r/cpp_questions or perhaps /r/learnprogramming). Just remember that the internet (and in my experience C &amp; C++ programmers in particular) can be sort of acerbic, so don't take it personally if someone says "this code is terrible" or "that technique is evil".
Perhaps relevant: [Plan to Throw One Away](http://c2.com/cgi/wiki?PlanToThrowOneAway). And further: [the Second System Effect](http://c2.com/cgi/wiki?SecondSystemEffect). General idea is that you will get better with time. Don't be afraid to write bad code now; only experience will allow you to write good code (and separate the good from the bad advice).
Googling "pi gpio C" gave me [this](http://wiringpi.com/), which looks to be something you might be interested in. For reference: [GPIO](https://en.wikipedia.org/wiki/General-purpose_input/output) Quickedit: There are many other libraries for this -- search around and see what suits you.
You never learn c++. It is an oxymoron.
AFAIK, it was originally a C compiler, too. C++ plans didn't come until later.
Retire and pick up botany.
In addition to writing code I'd recommend going back to look at code you write a few weeks later to see how easy (or not) it is to understand without all the context you had when you were writing it and what you could have done differently to make it easier to follow.
&gt; it will simplify so much the writing of new programs which uses the filesystem. Because you don't have to use Boost which you're probably already using anyway? I don't understand when people get excited for things that are in Boost to get into the standard library. You're probably using Boost anyway.
One thing that helped me with this sort of thing was monitoring and participating in the C++ tag on Stackoverflow. People are quick to point out anything, however pedantic. 
One can dream but I hope this means native cmake support in visual Studio someday
&gt; I have a fairly solid understanding of c++ No you don't. /thread
Yeah, go ahead.
https://imgur.com/GY5nyPp
“Fear is the path to the dark side. Fear leads to anger. Anger leads to hate. Hate leads to suffering.”
Keep learning it! I thought I knew C++ pretty well. Then I started my data structures class. This [book](http://www.amazon.com/ADTs-Data-Structures-Problem-Solving/dp/0131409093/ref=sr_1_1?ie=UTF8&amp;qid=1456902160&amp;sr=8-1&amp;keywords=data+structures+and+algorithms+nyhoff) has greatly expanded my understanding of C++. I'm still not even close to done. But I'm working on other languages and learning how compilers work now as well. It never ends muwhahaha
For example: I wrote around 3000 lines of C++ code that compiles fine under clang and under gcc in C++11 mode, but I am now installing a virtualized Windows machine, because my college cannot get it compiled under MSVC nor under mingw. The first one says there are 620 compiler errors, the second one has an issue when compiling the `Python.h` header together with `#include &lt;cmath&gt;`.
What about X-macros: #define ENUM_LIST(X) \ X(alpha) \ X(bravo) \ X(charlie) #define AS_ENUM(e) e, enum class MyEnum { ENUM_LIST(AS_ENUM) }; const char *get_enum_string(MyEnum val) { switch(val) { #define AS_RETURN_STRING(e) case (MyEnum::##e): return #e; ENUM_LIST(AS_RETURN_STRING) } } std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const MyEnum&amp; val) { os &lt;&lt; get_enum_string(val); return os; } 
HackerRank is a fun site to get quick and easy programming challenges to solve. Work on building up your skill first. Then find something you're passionately interested in, and write code for it, either a new project or contribute to an open source project. I can't tell you what this should be, since every person has different interests.
Hi, you can check [this](https://github.com/krabicezpapundeklu/smart_enum), also [this](https://github.com/aantron/better-enums) is pretty interesting.
Then become an astronaut for a Mars mission. What could possibly go wrong?
My solution: http://stackoverflow.com/a/34683385/1405424 Converts strings to enums and vice-versa. Performance was not a priority (is slow). Also for some reason I don't remember, I couldn't use a global vector of strings so this method will create one at every function call. The main goal was to have a user input string converted to enum, so I could have my code 100% clean of hard-coded strings. Is a bit of overkill for the topic here, which is the opposite conversion. 
If they aren't C++11 and C++03 compliant, how can they say they are C++17 compliant? 
I think it should never be the goal to write "good code". You write a Program, a Function. If it does what you want it to do = success! Nobody really cares how it does it, no user is going to dig into the code and think, "ew.. I'm not using that program, the code smells". (Well a library might be a bit different) Of course that's only the first step, after a while, you realize, you learn how you could do better. Then you refactor. Start to see patterns and next time hopefully start off better. But really, you're never going to frame your code, hang it on the wall and think: "wow, that's beautiful code".
Yeah, that would cause issues.
&gt;after learning C++ Lol, that's cute.
Do you have a timeline? They went for C++98 but never for C99 so I wonder what happened. EDIT: https://blogs.msdn.microsoft.com/vcblog/2015/09/25/rejuvenating-the-microsoft-cc-compiler/
Suffer, read endlessly, learn to love the pain. i believe this is more towards C but check out [Handmade Hero](https://www.youtube.com/playlist?list=PLEMXAbCVnmY6RverunClc_DMLNDd3ASRp) , [2](https://www.youtube.com/user/handmadeheroarchive/videos) and as for books https://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list
The only way to improve your skills is to code. Read a good beginner book, apply what you learned in a personal project. Read a more advanced book (Effective C++ for instance), improve you project or start a new one to apply your newly acquired knowledge. Rinse and repeat for more and more advanced (or specific books). You can also watch videos from conference, there are a lot of them on Youtube and consorts. The most important point is that you apply what you learn. Just reading books is useless if you don't practice. 
Very helpful, thanks, also I've heard about reference books? I believe this means if you need confirmation or correction about something you consult a specific book? Would Stroustrup's books would be good for this? After all he created the language.
Thanks, I'm pretty sure that's the source I'm remembering. 
"I must not fear. Fear is the mind-killer. Fear is the little-death that brings total obliteration. I will face my fear. I will permit it to pass over me and through me. And when it has gone past I will turn the inner eye to see its path. Where the fear has gone there will be nothing. Only I will remain."
I'd say next logical step would be to touch the sky. Or levitation. Levitation's cool too.
Except we could have had many of the C++11 niceties already in 2008.
I used that compiler AND that feature and it was very handy at the time. :)
No, I think I got a mild case of food poisoning.
_creeps nearer_
Very good, I really enjoy lock-free programming. Concurrent algorithm design is way more harder/funnier when you don't want to lock anything.
I mostly prefer Cppreference or Devdocs.io for confirmation but if you want simple examples for something I recommend Professional C++ book. Its not a beginner book, but after Stroustrup's books you will be familiar with the more advanced technics. 
I didn't until a couple of seconds ago, but I'm in the UK and have never heard it used here.
Interesting. I never really thought about that, but it makes sense. I had a glance at C++98 and this appears to be defined in &amp;sect;5.7/8: &gt; If the value 0 is added to or subtracted from a pointer value, the result compares equal to the original pointer value. If two pointers point to the same object or function or both point one past the end of the same array or both are null, and the two pointers are subtracted, the result compares equal to the value 0 converted to the type `ptrdiff_t`. (Also, it sucks that Dr. Dobb's closed down because Andrew Koenig's articles there were fantastic.) 
This book is really good: http://www.stroustrup.com/Programming/ It's important to get the second edition. This book basically teaches C++ in the sensible way. Also things to avoid: 1. Any C++ book which introduces C-like things early on. If you get new/delete or pointers before std::vector, then you will find that the book is a better source of fuel than of C++ knowledge. 2. Any book not using C++11 and ideally C++14. C++11 is a 5 year old language standard, there's no need at all to learn starting with a 18 year old standard (C++98). 3. Old or buggy compilers with poor standards support. All the major, easy to get hold of compilers (GCC, clang, Visual Studio mostly) have versions with decent (if not complete for VS) C++14 support. .
That's not necessarily an MSVC++ deficiency; GCC implements many nonstandard extensions, e.g. variable length arrays in C++, which Clang implements to be compatible with GCC. (I'm not saying it's *NOT* an MSVC++ deficiency either; just that getting compiler errors when migrating from another compiler is not necessarily non-conformance of the target compiler)
But oh so satisfying.
Yeah UK here too. I guess it was more an American thing that has slowly worked its way over here. 
I realised there was a slightly simpler way to do it, which also takes care of the `enum class` problem: template &lt;typename EnumType&gt; void write_enum_value(std::ostream&amp; out, EnumType t, long first_value, const char* prefix, const char* names) { size_t index = long(t) - first_value; for (size_t i = 0; i &lt; index; ++i) { names = strchr(names, ','); if (names == nullptr) { out &lt;&lt; long(t); return; } names += strspn(names, " ,"); } out &lt;&lt; prefix; out.write(names, strcspn(names, " ,")); } #define DEFINE_ENUM_IMPLEMENTATION(EnumType, class_tag, name_prefix, first_value, first_name, ...) \ enum class_tag EnumType { first_name = first_value, __VA_ARGS__ }; \ inline std::ostream&amp; operator&lt;&lt;(std::ostream&amp; out, EnumType t) { \ write_enum_value(out, t, first_value, name_prefix, # first_name "," # __VA_ARGS__); \ return out; \ } #define DEFINE_ENUM(EnumType, first_value, first_name, ...) \ DEFINE_ENUM_IMPLEMENTATION(EnumType,, "", first_value, first_name, __VA_ARGS__) #define DEFINE_ENUM_CLASS(EnumType, first_value, first_name, ...) \ DEFINE_ENUM_IMPLEMENTATION(EnumType, class, # EnumType "::", \ first_value, first_name, __VA_ARGS__) 
I think the terminology of "pointer to member" is rather unfortunate and they should be called "unbound member" or something, both taxonomically (what do you call regular pointers? pointers to objects? but members are objects as well…) and because they are just fundamentally different—pointer arithmetic is senseless, nulls are different (as you point out), size is not directly tied to the address size of the architecture, they see significantly less use, etc. There's no reason to throw them into discussions about pointers, because they're a different concept sharing similar syntax.
I've created a list of some very common problems that I encounter when reading beginner-questions, because I got tired of writing down the same things over and over and over again: http://florianjw.de/en/modern_cpp.html Avoid the problems listed there and you should be on a good way. Most of the points listed should be pretty uncontroversial in the big picture (of course there will be competent people disagreeing with details, but that is always the case). Manage your resources implicitly (RAII), never use `new` and follow the stdlib-naming-conventions. Most of the remainder is either independent of C++ (make your functions short!) or not *that* important.
I think it'd be nice to have a standard audio library, but that's not going to happen any time soon. Audio processing and playback is far too varied and changes too quickly to justify standardizing it. On the other hand, vector graphics libraries are pretty stable, and they're all very similar. Off the top of my head: Cairo, GDI+, Skia, Qt, and OpenVG all have pretty much the same API (the main differences are how OO they are). Other languages also have very similar drawing libraries, some that are standard. This proposal is not just some random act of self-interest. It's an attempt to standardize something that already exists as a de facto standard. It's the same reason that we got the thread support library, regex, and the unordered associative containers. For a concrete example of why this is being proposed, consider that standard C++ cannot function as an app on phone or tablet devices. `cout` simply doesn't work there. This library would allow users to write working apps while staying within the standard, and it would still provide access to the native context just in case something platform-specific needs to be done. That's a massive benefit. 
&gt; As a new programmer to C++, how do I know if the code I'm writing is 'good'. As a new programmer, the code you write (regardless of the language) is not 'good'. You can become an old programmer and still write crappy code. To avoid this, you must write lots of code, and keep working on improving it). I have a lot of experience writing code (I've been using C++ professionally for 13+ years). Approximately two years ago, I saw a presentation by Alexander Stepanov, and the code I wrote changed drastically after that; I am now re-reading "Effective Modern C++", with the same effect (I keep changing the way I write code). Your first purpose should not be to write good code, but working code. On top of that, you should strive for maintainability; it's an extra; a bonus. Every time I find a good book (or online video / article etc) on the subject, I go into refactoring afterwards (these days I'm working on my ability to choose names for things, attempting to make the code obvious in purpose and readable like prose). &gt; how can I close the loop on writing good code so that I don't spends hours and hours on something, only to find out that it was poorly written? There are three steps: 1. practice 2. practice 3. practice Seriously speaking, there are many methods, steps and processes you can apply for this: - write test code first (even if you don't do strict TDD, writing some basic/simple test code first will change the way you design interfaces and improve/naturalize the way you implement dependency injection) - refactor for SOLID - refactor for better names &gt; I'm currently reading through some Scott Meyer books, and am having a conflict of determining 'when I'm ready' to actually work on a project. You are ready now.
But for the sake of C, blocks that perform pointer arithmetic rarely make importance for the entire logical address until at least after the fact. Unless you're writing drivers or a kernel, it's pretty safe to assume pointers are just simple offset in most cases.
No matter how good you are or become, much of your code from 6 months ago will seem like trash. It's more about knowing your improvements than anticipating problems you don't even know about yet.
Nope, it's best and safest to think of a pointer as indicating where something of that type starts, or pointing to where the next element in an array would have been had it been one element longer. Anything else, and you are skating on thin ice. Arrange your pointer math so that intermediate values happen to not be sensible, and you are skating on *melting* ice.
**Warning: The following probably wrong - if you read it, please also read the reply.** It's a variation on things that have already been said, but I'd say it's because the null pointer value is just one run-time value that a pointer can have. In general, you cannot determine whether a pointer is null statically, and for the compiled code to add extra run-time checks would be an added run-time cost. Also, it was really C that made this choice. C++ mostly inherits this from C, and must do the same things for compatibility with C code. There are exceptions to C compatibility, of course, but pointers are pervasive in C so fundamentally changing the semantics of pointers in C++ would pretty much make any claims of C compatibility completely worthless. In C++, you can add null-checks if you really want - define your own "smart pointer" class that wraps a raw pointer (or maybe a `std::unique_ptr` or `std::shared_ptr`) and overload `operator*` and `operator-&gt;`, including null-checks in those overloads. In principle, those null checks will be done too much - in cases where the compiler can prove a pointer is/isn't null anyway. In practice, small methods tend to inline, so the compiler can probably see the nullcheck at the point of use and optimize it away in most cases where that's possible anyway. If your priority is safety first in all contexts, it's pretty bad - you have to opt in to being safe and, since your null-checking pointer isn't standard, you're also opting out of convenient interop. with probably the biggest bug-avoiding strategy there is - using pre-existing, widely-used well-maintained code where possible. That's really just a variant on the trade-off C++ made by being C compatible. 
Ok interesting, I think really this just boils down to "the standard says so". I guess it's enough :-) I guess at this point I wonder why the compiler doesn't reject it outright. Perhaps some warn and you can fail on warn...
Perhaps the array is in the middle of a page the programmer manages? There is no way for an optimizer - focused on a very narrow context of code - can know what is or isn't really legal. Sure, the address may now point at an unmapped memory page and be illegal to do anything with or it could be a pointing at a footer of an allocation.
Odd, but correct. The action of adding 3 to arr is undefined behavior, not its result. The compiler can assume that the code is never reached and typically will use that assumption to optimize out every code path containing the statement. Check out other comments in this thread for the standardese.
Probably because the xlib API was hacked by the MIT students 30 years ago. And because you started with windows, I guess.
This can work in simple cases, but it's annoying to have to write by hand since you have to repeat each enum value three times: once in the enum list, once in the comparison, and once as a string. It's also easy to get out of sync with the actual enum list (the compiler won't catch it if you forget to update the function, you'll just get wrong values at runtime). Also, you want an inline function, not a #define, so that you don't evaluate `f` multiple times if it's something with side effects.
Sure, I guess we have to equate "might" with "will" though! I agree it's a thought exercise. In practice we would have another line dereferencing.
To be explicit, I haven't written code that does this. I've migrated plenty of legacy code away from this pattern and stored explicit pointers to the datablock. Some of that code is pretty challenging to migrate though as its the most efficient balance between data size in memory and cache miss avoidance. I really dislike the code as debugging it is hell but haven't found an alternate solution that works without hurting memory or performance. :\
&gt; Probably because the xlib API was hacked by the MIT students 30 years ago **THAT** might really be the root cause. I'm probably used to Windows, that's true. But the quoted point makes a **huge** impact (I've had internships under my direction, and I see no reason to doubt that code was written by some of them).
&gt; Have you ever heard of coding guidelines? Guess not. Of course I have, don't be a dolt. So do you propose that the thousands of open source developers, working completely independently of each other, should all standardize on a single set of coding guidelines? If not, then I have no idea why you'd bring it up, since just about every major project has their own set of guidelines, e.g.: * [GNU](http://www.gnu.org/prep/standards/standards.html) * [Linux Kernel](https://www.kernel.org/doc/Documentation/CodingStyle) * [Qt](http://wiki.qt.io/Qt_Coding_Style) &gt; the lack of decent software Your inability to understand things does not necessarily reflect on the quality of those things. And as others have noted, Xlib is nasty and old. Is there poorly written code? Sure. That's the case on literally every platform in the world and will be for all time.
Do you have any particular example of badly name linux functions ? Of course there are a lot in any project, including in win32, you just happen to be more familiar with those. Also, drawing a windows make no sense. You *obviously* **paint** a window. Except you don't actually repaint anything, you actually update a region. so it should be **update**, *duh*. Except the window is not updated right away so the function should be called **invalidate**. Except maybe the surface is 3d-accelerated in which case it should be **render**. And see,how we talked about windows when *technically* it was a **surface** all along ? Naming is hard. Naming is never perfect, and depending how your brain approches a problem an api may or may not make sense to you the first time you approche it.
Yeah MSVC is shit when it comes to installing, that's a known fact.
hi! I'm Steve Carroll, the dev manager for VC++. We are investing a ton in this space. We have a big event coming up at the end of the month at [//build](http://build.microsoft.com/) where relevant VS announcements will be discussed and I hope the community will forgive me for holding off on pre-disclosing. :) In the meantime, please /u/melak47 gives a useful answer for some people below. This might be a good use case for the RemindMeBot: https://www.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/ edit: I suck at markdown 
see my top level comment. we actively care, I promise. :-)
Is your surname really 'Bearman' ?
When you're new to a language, you'll always write "bad code", because you're still learning the language. After writing your code, and debugging for any errors, think "How can I make this better/more efficient?". Also, while debugging, searching the internet for help will also help with your coding habits, because you can see how other people code as well. At least I know that's how I've always done it. 
I agree that a complete graphics library as outlined in the paper does not belong in the STL, but a simple container type that represents a 2D matrix in contiguous memory would be a welcome addition IMO.
Can I set the install drive yet? All I use is inc/libs, I don't like shortening my SSDs life for something I don't want. Please consider providing a zip.
can I ask a followup question? Are you asking about the Build SKU? Main VS does allow you to set the install drive, iirc.
If you read down to Crazy Eddie's answer on that SO post, you'll see that the original reasoning for this to be UB was that on some processors, invalid pointers were generally trap representations.
I tried installing VS Community like two weeks ago, and the option for setting the install drive was grayed out for me. I did found a few comments online about people saying that it was available, but I couldn't make it work. I ended up going back to QtCreator + MinGW, since I made my C:\ of 40GB (just for the OS). The default VS C++ installation was around 11GB iirc, I can't afford that much SSD space for it. Nice to hear you're doing something about it.
Did you not see Google's 6 year study in SSDs? Hardware failures from age are much more serious that read/write count. You haven't had to worry about read/write for a few generations of SSDs.
I've had two just up and die, so I'll reduce what risks I can, cheers for the info, will have a look.
What's so wrong with xlib api? It follows coherent naming conventions and is very well documented and designed... maybe a bit overengineered. The elisp code I have seen is in general of good quality, well documented and formatted... although emacs itself is a gigantic project with a bit of an organic taste. I think you're the drunk monkey here.
Everyone say that writing more code is important, yes it is but not only. Also reading other code can help a lot, to write better code.
Why on earth would you be using Xlib to write applications? Was this post written in the 80s? 
Well, it's not like the XLib is regarded as a masterpiece of software engineering and code-writing by Linux developers. We realize X is not pleasant to work with, but display servers are **hard** and X has been our only choice until something better comes along -- the best contender, Wayland, has been in the works for about 8 years and still isn't ready, just to give you an idea of how hard it is. So, yeah, XLib is not that good, but you talk about its flaws and assume they also apply to all other Linux software, which is not true.
yeah.... this is basically what I was going to post coming into the thread. I'm sorry, but as a polyglot who actively develops for both operating systems, Windows is by __far__ the bigger offender when it comes to these sorts of issues. I'm not saying linux/unix is perfect, but I'm sure as shit saying I'd rather be in linux/unix land trying to figure stuff out than windows. Just today I discovered I have to load a DLL that's only available under certain configurations of windows in order to update IIS 6 SMTP bindings via Powershell because the property on the wmi object is literally a byte array that's a serialization of an internal object. The DLL has the correct types to enable .net to deserialize/serialize this object. The generally accepted way to set this value is to manually setup what you want via the GUI, extract the byte array and then blindly set that byte array to the property for subsequent windows installs. This is because most people don't want to go through the hassle of trying to identify if the interop DLL's are available, loading them, and so forth. Did I mention these DLL's are 32-bit only? This means if you're in a 64-bit remote session (for example) you end up calling the 32-bit version of powershell and pass it a string that's the powershell code you want it to execute. Do you know what I was trying to do? update the relay IP's for the IIS6 SMTP server. In linux/unix that shit is just some text in a configuration file. This argument that somehow windows is less complex than linux/unix blows me away, honestly. This is clearly a person who doesn't have much experience in both environments.
:) I love doing channel9. and I hear you on the mean lean c++ only install!
I don't know if there is one, or can meaningfully really be one. When you're working in a higher level language, you can just smooth over so many more edge cases and state more general principles. In C++, a lot more things will come down to nitty gritty specifics. That said, here's my best attempt (blatant stylistic rip-off incoming, not to be taken too seriously): - Not too ugly is better than really ugly. - Explicit is better than implicit, but bad defaults are worse than either. - A simple implementation is better than a complicated one. - A complicated implementation is better than a complex interface. - Contiguous is better than pointer chasing. - Pointer chasing is better than double pointer chasing. - Readability is important; so C++ being what it is, write lots of comments. - Special cases will require breaking the rules, but try to keep the rule breaking hidden. - That said, if you can't hide the rule breaking, a practical impure solution to a smaller problem is better than a pure impractical solution to the generic problem. - Don't use error codes. - Unless their checking is statically enforced. - In the face of ambiguity, don't compile. - There will be many ways to do everything; know their trade-offs and be an engineer and pick one! - Although the cleanest, most maintainable way will not be obvious at first unless you read books and watch talks from a lot of really smart people. - 2011 is better than never. - Though never can be better if you haven't thought it through. - If the interface easily causes undefined behavior, it's a bad idea. - If the implementation is safe, it may be a good idea. - Const is a honking great idea, use it everywhere you can! Suggestions for improvements welcome. 
In the standard's view, that is apples and oranges.
No worries, glad you know at least. Mind fielding something quasi related? Any idea when StackWalk64 will work with ARM/64? (P_CONTEXT registers and such) https://msdn.microsoft.com/en-us/library/windows/desktop/ms680650(v=vs.85).aspx Thanks
I think the closest thing to Zen of Python for C++ is the Philosophy section in the ISO C++ Core Guidlines https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#S-philosophy 1. Express ideas directly in code 2. Write in ISO Standard C++ 3. Express intent 4. Ideally, a program should be statically type safe 5. Prefer compile-time checking to run-time checking 6. What cannot be checked at compile time should be checkable at run time 7. Catch run-time errors early 8. Don't leak any resources 9. Don't waste time or space 10. Prefer immutable data to mutable data 
Apparently I missed a case lol
I see, great that was the kind of detail I was curious about. Many thanks. It's very c++ that this detail came from someone called "Crazy Eddie".
If you want to interact with hardware learn about the cpu, assembly, SIMD programming and intrinsic functions. I think it's a very interesting topic. Try your new skills and implement the 2D Mandelbrot set with scalar and SIMD instructions. I'm just going to leave this here: http://www.lighterra.com/papers/modernmicroprocessors/ 
That's a nice ideal, and one I share, but that's not how it actually _is_.
[He's no one special](https://www.reddit.com/r/programming/comments/2lctuz/crazy_chess_crazy_eddies_sane_c/cltkkuq), apparently not a terribly original name though. ;-P
Accurate bounds checking is actually a technological achievement worth respecting. The C standard (and C++ followed) was carefully written so that even raw pointers may implement bounds-checking, although I don't know of any major implementation which actually went through with that. GCC tried but I gather it didn't work very well. 
That was really underwhelming.
[Downvoting doesn't make you right](http://stackoverflow.com/q/15608366/636019). &amp;nbsp; ^^Derp...
Of course, asan does much of that.... :)
Logical addresses: platform specific and beyond scope of language spec Arithmetic to pointers pointing to anything but an array: undefined behavior and beyond the scope of the language spec (regarding what is being pointed to, as another user has pointed out it does mention pointers pointing past the end of an array, in my case for sake of example, your data could be something like the next header for an allocated block for a heap) No one was wrong, I was simply emphasizing context. Sorry about the downvotes, I just felt like your short responses didn't **contribute** as much as they needed. So I was simply following etiquette: &gt; If you think it does not contribute to the subreddit it is posted in or is off-topic in a particular community, downvote it. I apologize for following the rules :)
don't know... will see if I can get an answer.
I would be ok with the option of installing it wherever I want to. My setup is a 128GB SSD and a 2TB HDD, so I don't really mind if the *whole* IDE takes up 11GB, as long as I can put it in the space I designated for stuff like that. It is still a lot of space though. A less than 1GB option would be nice (or expected, I guess. My QtCreator folder is 250MB, for example). The standalone building tools would solve my problems. I have no other reason to install VS other than being able to see if my code builds ok using MSVC. I'm up for more questions. Thanks for caring.
Const is a great idea, use it where it makes sense. 1. Don't declare const by-value return. It's just rude. 2. Don't declare function locals const if you intend to return them or move them (const inhibits move). 3. Use const non-static data members judiciously, they disable assignment operators and (again) inhibit move.
All 3 are excellent points, my mimicking of the python style guide shouldn't be taken *too* seriously. 3 in particular is actually a pet peeve of mine; many people I've found will use reference member variables for classes where they can. While normally I'm all for references over pointers, I point out to them that if they want to make their class assignable they will need to change a million '.' to '-&gt;'.
If I had to pick a number, I'd say &lt; 1 GB for a minimal install that supports C++ should be doable. Ameller is always better, all other things being equal, but the practical difference between a few hundred megabytes and a gigabyte probably isn't worth fussing about. OTOH, whole gigabytes are still an issue. When I need VS X.Y and A.B for doing different versions Maya plugins, and VS Z for doing something else, and maybe also whatever is the latest version because I've become infatuated with some new feature, it adds up fast. I don't want half of my disk taken up with various Visual Studio versions! Especially if it turns out that I am paying for tools and SDK's that I've never heard of, let alone asked for. Have some sort of "VS package manager" functionality so if I need a .Net thingie for a project or whatever, it's easy to install it on demand, and the 1 GB target shouldn't be at all hard to hit. Qt Creator obviously manages to support C++ development with a &lt; 1 GB footprint.
"Care" is the correct pronunciation, as it's short for character.
So if we're always qualifying namespaces, why do namespaces exist?
If you know c++ enough to genuinely say you know c++, go get a job applying in that. There are very far and few people who are good at c++, and those who are make some serious money. 
I'm surprised at a few of what seem to me like obvious omissions from the answers so far. Three that almost jump out are: - You don't pay for what you don't use. - Don't leave a requirement for some other language providing a lower level of abstraction (more direct access to the hardware). - Penalties for abstraction should be minimized. A few more to at least consider: - Ugly syntax can be acceptable if it discourages special-purpose constructs (e.g., `reinterpet_cast`). - Solving a general problem is better than solving a specific one. - It's better to prevent problems than solve them. - Errors should be caught as early as possible. 
Looking forward to this eagerly. Keep up the good work.
It's a style choice. operator++ has higher precedence than operator/ and operator, so they're really the same thing.
Not with virtual memory, it ain't! :) And what's more even with an absolute address of zero, in protected (p-) mode, the IDT can be loaded anywhere. In fact what I think you're referring to is more specifically the Interrupt Vector Table and is used in *real mode*, and so is obsolete once transition is made to p-mode (commonly at the start of the second stage of the bootloader). The kernel may choose to keep an IVT at address 0 however if it ever requires re-entering real mode (to access BIOS functions for example, although *BIOS extensions* give p-mode access) or, on Intel processors, to enter *Virtual 8086* mode.
Python contradicts its own zen so many times. It's a total hypocrite, and I am looking to move all large python projects over to swift. C++ zen, I dunno: - keywords and symbols should be overloaded as many times as possible; confusing the programmer is better because legacy can of course never be touched - performance is better than correctness because cpu cycles!!@ - related to above: always make the defaults as unsafe as possible. Some languages say optimize the 1% of code that matters? fuck that. we optimize 100% by default. - stl library source code should be as hard to read as possible because parsing speed - stl must use as many abstractions and inheritances internally as possible because l33t job security? - exceptions should have zero overhead except for memory and code size and cpu cycles when you have constructed objects. and be very, very hard to implement on different os's. - programers love lawyereese language and stdlib documentation (e.g. draft standards), because ... uh, you got me on this one. oh yeah, it's because the authors make money selling books. - the simplest solution is never appropriate. always provide the union of all possible feature sets. - strive to have the world's shittiest standard string class. I think maybe it should be a byte buffer but someone renamed it as a joke. - always make implicit assumptions about what the programmer wants, and provide automatic coersions. because they will stumble over these features by happy accident some day and be enlightened, rather than having to consult documentation. - exception safe code? cross your fingers. and toes.
Thank you :)
Got it, thanks. But I have no prior experience in ANY C. I only know Javascript, and they are extremely different languages. All I know about C is integers, booleans etc.
I wonder how much disk space is wasted on .NET metadata, for C++ code that is compiled with `/clr` that doesn't need to be..? IOW, the easy solution to using .NET from your C++ codebase in VS-land is to compile your whole project with some `/clr` variant – the _proper_ solution is to compile only the TUs that actually need it with `/clr`. With the easy solution and a large codebase, the CLR metadata can accumulate and get out of hand quicker than one might expect. VS2005 had a large jump in disk space requirements; I wonder how much could be avoided from `/clr` partitioning...
&gt;Arithmetic to pointers pointing to anything but an array: undefined behavior and beyond the scope of the language spec Not quite so: int a = 42; int *p = &amp;a; ++p; `p` points past the end of `a`. Perfectly well defined.
/u/Dekken_ , I asked the windows guys who own this. they think the docs might just be wrong. Can you try it out? if it doesn't work PM me and I'll connect you with the right people.
&gt;It's a variation on things that have already been said, but I'd say it's because the null pointer value is just one run-time value that a pointer can have. In general, you cannot determine whether a pointer is null statically, and for the compiled code to add extra run-time checks would be an added run-time cost. Extra run-time checks are what this is about, although I think your understanding of when the checks are required is backwards ! Having cases be undefined means that the compiler does NOT need to do extra any run-time checks. It just generates code for the defined cases and does not care about the other cases. In C, adding `0` to a null pointer is undefined. C++ adds a definition for that case. That means that a C++ compiler has to do equal or more runtime checking than the C compiler, because there is an extra case. (The title article is explaining why C++ chooses to incur this potential runtime penalty). 
Also, let us have the compiler without the IDE. Most open source projects can be built from the command line, so if one's reasonable objective is to "just build the damn thing and use it", the IDE, documentation, etc. is unnecessary. Personally, I use other IDEs 90% of the time, for example, but I do use the compiler all the time from other IDEs.
In C and C++, pointer arithmetic only works for pointers to objects that exist. The null pointer is just one of trillions of pointer values that does not have defined arithmetic in C (but does in C++). Also, you are mixing up the null pointer with address 0. The null pointer is a special pointer value which indicates the pointer is not pointing anywhere. It would be possible (and, in fact, a good idea IMO) for the null pointer value to be something that does not correspond to any actual address, for example `0xDEADBEEF` on a target where that does not fall in any possible address space. Then you could access address 0 with no problems. However, for various reasons (mostly laziness I suspect) most compilers give the null pointer value the same value as a pointer to address 0 would have. This means that you actually cannot access address 0 whilst still complying with the C or C++ standard. 
In general, our recommendation is to use /clr for glue layers to connect native and managed code. There is definitely C++/CLI code in VS in places where we are doing that, but I'm pretty confident that isn't the dominant reason for the size issues. There's just a lot of stuff in VS.
we did this one already. https://blogs.msdn.microsoft.com/vcblog/2015/11/02/announcing-visual-c-build-tools-2015-standalone-c-tools-for-build-environments/ give it a try, let us know what you think. We are still improving it.
Yeah, I don't intend to learn any language other than C++ currently. Later I might learn C# then C. All the C languages seem extremely powerful, and all have a large amount of uses.
Can you provide any arguments?
Yay, great!
&gt; Why should it be defined? The only reason I can think of is to legitimize bad APIs that rely on this behavior. Take OpenGL's [glDrawElements](https://www.opengl.org/sdk/docs/man/html/glDrawElements.xhtml). If you have an index buffer selected, the pointer to indices is an offset into the buffer, starting from 0. The only way to use the API to start rendering at index 3 is to pass in something like `static_cast&lt;index_type&gt;(nullptr) + 3`. Of course, such APIs should just die and instead take an appropriately sized integral type.
You could say the same about 4 and 8, given the C compatibility and unsafe by default. But the language does provide the tooling for making it safe, if one cares about.
 &gt;Am currently studying cs in my first semester. After a couple of months I realized, that studying does not suit me. So far if something sparked my interest, I learned it autodidactic, programming being no difference. Making the cs courses intended for people completely new to programming, rather boring. This right here would disqualify you at most business I know about. Frankly this comes off as a bad attitude, even a bit arrogant. Plus if you aren't willing to learn the ins and outs of CS and the other stuff associated with a degree people will have very little interest in you. Even a two year degree would do much for your sale ability. &gt;So at the start of this year I decided to actively look for jobs and made a website to showcase some of my work: www.lukas-bergdoll.net Lots of luck! Seriously nothing is impossible but you are putting yourself in a very difficult position. &gt;Using stackoverflow careers and indeed, I manged to get some interviews. Most of them went well and some even led to a followup interview. Ultimately all declined, giving positive feedback stating talent, but they wanted someone with more experience. What they are basically saying is that you don't know as much as you think you do. &gt;Given the fact that software-engineering is my passion and what I want to do professionally, I am currently in the situation where I have to figure out what to do next. Go back to school!! I can't stress thIs enough. If you can't hack school you will never make it as a programmer. Think seriously about a two year degree which is better than nothing if it comes from a decent two year college. &gt;So here a couple of questions I´d like answered by some of you working in this field: &gt;Should I look for an internship to garner some experience, rather than directly as full-time employee? I wouldn't do either, instead get back in school. &gt;Are finished studies more important than skill? Yes! In the long run a formal education makes you more valuable as an employee. Consider this what if you get a job where you are required to deal with some advanced mathematics? Or maybe it requires understanding some physics. With out the formal education how would you handle the need for technical knowledge outside of code generation. &gt;What are some good of ways of finding jobs/ internships? Get a college degree! Otherwise you start at the bottom doing support. 
You can set the install drive, but you still end up with quite a bit of stuff on your system disk. 
Well, you could use Emacs as your editor with VC++ build tools, if you wanted, or any other text editor/IDE. How about VSCode? I think you need to more specific with regards to what you want, Visual Studio is not intended to be a "lean, small IDE".
One case I can think of is the compire realises that arr's memory is never actually used and doesn't allocate any space for it, or arr fits in a SIMD register and compiler never puts it memory. Then arr has no address (although in my thinking it should constating arr "type" to addressable array and thus arr should have an address for arr + _ to work. (Where as no such usage could mean that arr is allowed to not have an address)) 
I was quite impressed the amount of devs that stated not using any form of analysers at Herb's talk at CppCon. That very few care about it, is clear to me, based on my experience with other C++ devs I met at work during my C++ days, but I never expected a mere 1% among the crop that manages to attend CppCon.
Sigh, thanks I guess.
Apparently, [I'm fighting with this](http://stackoverflow.com/questions/9183993/msvc-variadic-macro-expansion)
As others have said, near the throw is often a place that can't reasonably deal with the error. For example of the user asks to save a file but partway through saving the file the disk runs out of space it doesn't make sense to catch this in any of the intermediate file io layers, but instead catch any and all file io errors at the GUI level so that you can pop up a dialog telling the user what went wrong.
I use Coverity at my actual company and I have to say is really good. The only problem I see with these tools is the prize, I don't know all the details but expect more than 10k$ or even more per year for bunch of licenses (in our codebase, more than 20M of lines of code splitted in more than 20-30 projects) The only free alternative as developers we have is cppcheck, and I don't want to say is bad, because it is not, I think is an amazing tool, but it is really far away from tools like PVS studio or Coverity.
Never used `boost` but good to know, thank you! Unfortunately we can't use this library in our project.
Ah, yeah; the preprocessor is one of those areas we only have partial support for C99 features.
While a part of me finds the snarky comments funny - because there is truth there - the rest of me finds them deeply saddening. This is the kind of thread that serves to reinforce the impression of programmers as cold, snooty, exclusionary, and derisive when a student asks a question. We've all been there - where we thought we knew everything there was to know about a particular subject, only to find out later that we didn't know how much we had yet to learn. It's not exactly encouraging when someone comes up and laughs in your face, then shoves a pile of unknowns at you. And to be fair - OP didn't even claim to have mastered the language - just to have "a fairly solid understanding of c++". To answer OP's question - I'd say push the boundaries of what you have learned. Take on bigger projects - find something that interests you and start working on it. Apply for some jobs - even just interviewing for some positions may show you interesting areas to explore. Try for Google - they're notoriously challenging in their technical interviews (and have lived up to that notoriety the times I've interviewed there). You may not get the job, but more than once I've been turned down for lack of experience, then tried again a year or two later and gotten the job because I studied what I failed the first time. Programming in any language is a life-long learning experience. Even the rarefied few who can truly claim to have mastered a language (far different from just a "fairly solid understanding", mind) spend time reading up on new techniques and algorithms, new concepts and different ways of looking at problems of computation. You specifically mention interacting with hardware - have you looked into the various micro-PCs out there, like Raspberry PI? There are all kinds of things you can do with that hardware and it's all well documented, much easier to approach than most commercial hardware.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [\[Xpost \/r\/cpp\] Seriously C++, WTF](https://np.reddit.com/r/programming/comments/48si6v/xpost_rcpp_seriously_c_wtf/) - [/r/python] [WOW! Python beat C++ in Speed](https://np.reddit.com/r/Python/comments/48wiq1/wow_python_beat_c_in_speed/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
How about googling "ifstream file size". Then you'll find out that there is a much easier solution than what you wrote. ifstream file( "example.txt", ios::binary | ios::ate); return file.tellg(); If you want to read the whole file fast and easy, use a stringstream std::ifstream t("file.txt"); std::stringstream buffer; buffer &lt;&lt; t.rdbuf(); all stolen from stackoverflow: http://stackoverflow.com/questions/2602013/read-whole-ascii-file-into-c-stdstring and http://stackoverflow.com/questions/2409504/using-c-filestreams-fstream-how-can-you-determine-the-size-of-a-file
Better but still much slower that python: $ time ./a.out 160090000 real 0m0.709s user 0m0.503s sys 0m0.193s
Didn't you write this : &gt; More broadly, this is another demonstration of "evil of premature optimization" and "good enough" principles. People tend to feel like they need a Facebook-like infrastructure for a business that is yet to come and that probably will have no more than 100 customers on a good a day. Seriously, with today computer power, you could start your business with a web server written in Q-Basic and it will work okay for a good amount of time. just a few days ago?
I did. And didnt' you write: &gt; There's a lot more kinds of rice in Indian food than just basmati. just few days ago?
iostreams are cancer confirmed.
Please, see the edit.
The standard library is just very poorly made and vastly over-engineered to try and handle every possible use case by everyone ever, sadly. In my library, nall: #include &lt;nall/nall.hpp&gt; using namespace nall; void main() { print(file::read("longfile.txt").size(), "\n"); //or if you prefer ... file::read returns vector&lt;uint8_t&gt;, which is convertible to string string text = file::read("longfile.txt"); print(text.size(), "\n"); //or if it has to be one line ... print(string{file::read("longfile.txt")}.size(), "\n"); //it's just a pointless extra move //or as you said you don't want to do for whatever reason ... but the fastest way: print(file::size("longfile.txt"), "\n"); } To me, the extra "\n" is very much preferable to languages with print functions that auto-add that. I can't give you a comparable benchmark since I don't have your PC, but it boils down to basically an fopen, fseek, malloc, rewind, fread, printf behind the scenes. Point is: blame the library, not the language.
mmap is not a fair comparison, as it doesn't actually read the file. You'll pay for reading the file as you access each page.
I think the point here is that even with all the different answers here and on SO, there's still not a single answer which runs as fast as Python's one-liner. Chances are, most people will be using any of those methods, and ultimately, they'll be loading files slower than what other languages will. This IMO is a huge flaw in C++, specially considering it's such a basic feature. I shouldn't have to study how to read from a file as fast as possible just to get decent performance.
What you have shown is not C, it is POSIX. 
The difference is that he's pointing out some interesting contradictions between your two opinions, while you're just kinda being a dick for no reason
But strlen() has to read the whole file if the null-byte is actually at the end, right? Also, why would a textfile end with a null-byte? Seems like a bug. Or is this how mmap() works?
We can easily improve C++ result by this: #include &lt;fstream&gt; #include &lt;iostream&gt; #include &lt;sstream&gt; int main() { std::ifstream f_in("file.bin", std::ios::binary); std::ostringstream ss; ss &lt;&lt; f_in.rdbuf(); std::cout &lt;&lt; ss.str().length() &lt;&lt; std::endl; return 0; } time on my machine: $ time python read.py 54945254 real 0m0.036s user 0m0.016s sys 0m0.020s Original python code $ time ./read 54945254 real 0m0.045s user 0m0.025s sys 0m0.020s This C++ code $ time ./bad_read 54945254 real 0m0.194s user 0m0.166s sys 0m0.028s Your C++ code
Hold your horses! Your python example is flawed because it does not read the contents of the file. It only checks the file size. Try XORing all the bytes in the file and print out the result. As you can see from these examples, python is horrifically slow! python test.py 80.94s user 0.61s system 99% cpu 1:22.02 total ./a.out 0.97s user 0.12s system 99% cpu 1.092 total ./a.out 3.77s user 0.47s system 99% cpu 4.271 total cat test.py chk = 0 for c in open('faust.txt').read(): chk = chk ^ ord(c) print chk cat main.cc #include &lt;array&gt; #include &lt;fstream&gt; #include &lt;iostream&gt; int main() { std::ifstream is("faust.txt", std::ios::binary); std::array&lt;char, 1024 * 1024 * 2&gt; buffer; int c = 0; do { is.read(buffer.data(), buffer.size()); std::streamsize size = is.gcount(); for (std::streamsize i = 0; i &lt; size; i++) { c ^= buffer[i]; } } while (is); std::cout &lt;&lt; c &lt;&lt; std::endl; } cat main.cc #include &lt;fstream&gt; #include &lt;iostream&gt; #include &lt;string&gt; int main() { std::ifstream t("faust.txt"); std::string str((std::istreambuf_iterator&lt;char&gt;(t)), std::istreambuf_iterator&lt;char&gt;()); int chk = 0; for (auto c : str) { chk ^= c; } std::cout &lt;&lt; chk &lt;&lt; std::endl; } EDIT: I forgot to mention the most obvious indicator for the difference between the OPs python example printing out the file size and the C++ example allocating memory for the whole file before printing out the size of the string: valgrind --tool=massif --trace-children=yes python test.py ms_print massif.out.* | less # 98.35 KB rm -f massif.out.* valgrind --tool=massif --trace-children=yes ./a.out ms_print massif.out.* | less # 576.0 MB 
Haha; I completely missed that. Yeah, this code is broken.
Python does not read the bytes at all. =)
It's easy to write slow code in C++; but it's quite unfortunate that the standard library doesn't have facilities to do this. It's a simple task that many programs want to do, and C++ currently makes this a headache. This is a library problem, not a language problem. It's one we can fix.
Where is the contradiction? I was just toying around with languagues, I'm not applying to my brand new and shiny startup saying "holy cow, I need to make this FAST". I think it's legitimate.
Doesn't really prove anything, other than Python is really slow at xoring a file byte by byte.
There is also https://github.com/KDE/clazy
How do you feel about immediately-evaluated lambdas as a way to manage to make things const? Not as bad as goto, but it's still kind of bizarre looking, and I'm not really sure it's worth it.
No, python doesn't work that way, check it with strace if you're not convinced.
Please, format the code properly. You can do it with four-space indentation. Example
Just a note about "syscalls": while the istreambuf_iterator has an interface that works on bytes, how the file is accessed is up to the implementation. The underlying streambuf is likely to use buffering. When testing the example with libstdc++, I can see that it reads from the file in 8kb chunks.
This is precisely the reason I suggested inspecting the memory usage e.g. with massif from valgrind. If you still think it does not prove anything, then please elaborate some more. EDIT: Oh, I forgot to mention that. I am very sorry for the misunderstanding. Still, the memory usage should be enough proof.
It definitely appeared that you were applying to your brand new and shiny experiment program saying "holy cow, this is SLOW" when the reason for that is because you're using a language built for close-to-metal machine manipulation like it's a high level language suitable for human expression. Computers aren't people. Approaching a language designed to fit machine paradigms like it's going to fit yours doesn't work. Kind of like how approaching a small business problem like it needs enterprise level scaling is silly. C and C++ are languages for the machine, not the human. Scripting languages are more for humans than machines.
&gt; One first change has made it into C++17: the range based for loop supports now different types for begin and end. I saw a paper in which this was proposed, but I cannot find when was this accepted. [The current working draft](https://github.com/cplusplus/draft/blob/master/source/statements.tex#L536) doesn't seem to have it =/ As soon as this gets implemented in clang and gcc range-v3 won't require the `RANGES_FOR` macro anymore so this is awesome news.
Please check it yourself and quote the results. I'm looking forward to it.
So what _does_ it do?
Python never reads the whole file into memory.
I don't know ... I ran these on a 266MB file. Python code: from datetime import datetime start = datetime.now() print len(open("c:\\temp\\logfile.log").read()) end = datetime.now() print end - start measured 640ms on my box (far from 0.1 second), whereas this C++ thing: auto start = std::chrono::high_resolution_clock::now(); std::ifstream file("C:\\temp\\logfile.log"); file.seekg (0, file.end); int length = file.tellg(); file.seekg (0, file.beg); char * buffer = new char [length]; file.read (buffer,length); auto stop = std::chrono::high_resolution_clock::now(); std::cout &lt;&lt; chrono::duration_cast&lt;chrono::milliseconds&gt;(stop - start).count(); measured about 480ms on my box. How did you manage to get Python to read disk with 1.5GB/s? That is quite impressive I think. Edit: formatting and additional details.
One thing to have in mind when using `add_subdirectory` is that it will add targets found in that subdirectory as targets for the main project. This is sometimes undesired, since many libraries have examples and include targets to build them. Using `add_subdirectory` may add SDL_build_examples target (just an example) to your project targets. Next time you run `make`, you will end up building your project, SDL, and SDL examples. There is an option to avoid this, `EXCLUDE_FROM_ALL`, that adds it to your targets but at least won't get called by default when you run `make`. In u/ojd5 example, you could change: add_subdirectory(SDL2) to add_subdirectory(SDL2 EXCLUDE_FROM_ALL) In any case, `find_package` is the "standard" way to go, and I haven't found a good way to make it work reliably on Windows. I have done things like this though: find_package(LIBX) if(NOT LIBX_FOUND) message("LIBX not found. Try adding LIBX_ROOT_DIR.") endif()
It's not a performance difference between languages, though, it's a performance difference between *programs*. Scripting language stdlibs don't do what the example C++ program did; when C++ is instructed to do what the scripting stdlib (which is C/C++) does, it has basically the same performance. C++ stream IO is just that -- a STREAM. The examples given are for working on a POOL of data instead, which requires different approaches, and which C++ happily provides to us. As stated in the post, though, for whatever reason OP wants to use stream operations on pool data and is getting mad about why that doesn't stack up to using pool operations on pool data from other languages which are, at the machine level, C/C++ as well
As you can see mmap isn't used to actually read the file, the read() system call is used. I think what's mmaped here is actually the buffer in which the content is copied.
we are collecting lists of 3rd party libraries that are not yet available for VS2015. if you are willing to share your list, I'd appreciate it.
So what's with valgrind reporting 98 kB memory usage, then?
Python has this optimized because reading whole files in memory is a bad habit of scripting people. It would be more fair to compare with some library that abstracts this kind of operation. I googled for an example with boost and got some code example working in less than 5 minutes (and I'm not a C++ expert by far): #include &lt;string&gt; #include &lt;sstream&gt; #include &lt;iostream&gt; #include &lt;boost/filesystem/fstream.hpp&gt; using namespace boost::filesystem; using namespace std; string load_data_in_str(string fname) { boost::filesystem::ifstream fIn; fIn.open(fname.c_str(),std::ios::in); if (!fIn) { cerr &lt;&lt; "Error reading the file: " &lt;&lt; fname &lt;&lt; endl; exit(0); } stringstream ss; ss &lt;&lt; fIn.rdbuf(); return ss.str(); } int main(int argc, char* argv[]) { string load_txt; load_txt=load_data_in_str(argv[1]); cout &lt;&lt; load_txt.size() &lt;&lt; endl; return (0); } $ g++ -Wall -O3 test_read.cpp -lboost_system $ time ./a.out some300mbtextfile.txt 368755866 real 0m0.315s user 0m0.101s sys 0m0.213s 
Presumably it would be voted into the working draft on Friday, if that is the ship vehicle for this. EDIT: I am trying to find out what the status of this is (read: I will shout vague things on IRC). P0184R0, unless I'm mistaken. EDIT #2: On the Core Working Group (CWG) wiki, it is indicated that P0184R0 is "Ready for vote on Friday.". CWG is the last working group that a paper like this needs to go through before going to plenary to be voted into a draft document. This means that during the Friday plenary session, there will be a motion to put P0184R0 into the working draft for C++17. I suspect this will be a non-contentious motion that gets wide consensus. I will certainly vote for it. EDIT: This has passed vote in plenary.
[This post](https://www.reddit.com/r/cpp/comments/48shrb/seriously_c_wtf/d0m9fqd) by /u/cym13 hints that python is using mmap for memory allocation instead of malloc. mmap allocations are not tracked by valgrind. http://stackoverflow.com/a/28579668/3547503
&gt; The goal in phase 1 is to eventually reach the point where no header file includes any other header file. This is a terrible idea. 
Take a look at my tiny-project - SLACC. I wrote it to be base module for in-game console. It allows you to call any previously binded function from string. It also deals by itself with command parsing so you can use it like: `cmd.execute("foo 42 3.14f \"my name\"");` and provides simple way to allow you customization for conversion between argument in string to argument of given type. Example + source on [GitHub repo](https://github.com/RippeR37/SLACC). BONUS: no dependencies! :) (i really hate when i have to link to boost just to get one tiny feature)
For completeness I ran the python version, and it runs faster: $ time python testread.py 368755866 real 0m0.084s user 0m0.012s sys 0m0.071s But again, I'm pretty sure there are optimizations in python's implementation. Another short readable C++ implementation without boost is also available: std::ifstream in(argv[1], std::ios::in | std::ios::binary); if (in) { std::ostringstream contents; contents &lt;&lt; in.rdbuf(); in.close(); cout &lt;&lt; contents.str().size() &lt;&lt; endl; } It's not at all different in performance with the boost version though.
The problem is that open sources static analyzers are not exactly extraordinary. I've found that most of the "XXX rules" they boast having are generally syntactic checks (if not style checks), and while keeping a codebase clean is certainly advisable, those checks rarely find bugs. There's generally a few gems inside (kudos for checking that the result of `std::remove` or `std::unique` is used!) but it's a lot of muck to sift through to get their unfortunately. I've heard a lot of good about Coverity.
Massif doesn't include memory allocated with mmap(). Guess how Python is getting a chunk of memory big enough to hold the file?
&gt; libstdc++ fstream Might want to specify `ios_base::in|ios_base::binary` in argument, lest it scans the file for line-endings to convert. Might also want to check the buffer size, and possibly bump it up (not sure if the buffer is used when using `read`). Have you tried with C directly? Unfortunately streams have a lot of baggage (facets, line-ending detection, virtual calls gallore), so it would be interesting how getting to C would look.
Yeah, nobody is saying you can't read stuff from that data, just that it's misleading to measure the performance of doing so compared to reading it all into memory. Just try to actually do something with the data you pulled in, and that "fast" program suddenly takes a massive hit.
In essence, I agree with you: in the [fourth part of the multithreading in C++11/14 series](http://www.loic-yvonnet.com/articles/multithreading-in-cpp14-part-4/), I wrote that *mutexes must never be locked manually. Lock guards and unique locks must be used instead*. In the [eight part](http://www.loic-yvonnet.com/articles/multithreading-in-cpp14-part-8/), I try to introduce the concept of atomics. Some people may never have heard about it. In my humble opinion, the best way to teach a new concept is a suitable comparison with something that is already known by the audience. In the case of atomics, the most suitable comparison that I found was the use of a mutex. Besides, I wanted to emphasize the fact that conceptually, an operation performed on an atomic variable is implicitly surrounded by a lock and an unlock. The idea is to show explicitly what happens implicitly conceptually. If I had used a lock_guard in this particular example, as you suggest, I would have been less explicit. Indeed, a lock_guard *implicitly* unlocks a mutex in its destructor. As my aim in this very particular setting is to be as *explicit* as possible (for pedagogical purpose), I am convinced that a lock_guard is exceptionally inappropriate. Yet, again, I agree that RAII (i.e. lock_guard and unique_lock) must be used instead of explicit lock/unlock in production code.
Actually, in could very well start as a `mmap`ed list and only copy in case of writes... but it seems a bit tricky for C-Python which generally privileges straightforward code over performance.
The Python version does read the whole file into memory.
I hope we can read a trip report very soon. So much excitement at once... lmao 
You can capture maximum [Resident Set](https://en.wikipedia.org/wiki/Resident_set_size) size in KB with `/usr/bin/time -f '%M' ./a.out` (the real `time`,not bash's built-in `time`). It shows Python does allocate enough memory to store the whole thing.
Still not perfect, e.g. reverse(reverse(range)) does not work... Do you have similar constructs in your code or libs? Any better solutions? Hints on what to improve?
&gt;That's one thing I didn't know, and it suggests that there is a valid case for doing pointer arithmetic starting with a null pointer. That is what the article is about that we are commenting on ! 
In fact, it was more an example of *how an atomic operation may be understood conceptually*. In the case of a simple integer, I agree with you that in most cases, we would use a std::atomic&lt;int&gt; instead of using an extra mutex.
For some reason, I thought there *wasn't* a link, and this was just a question post. Obviously I got here clicking directly on "comments". Oops! 
The name of the [C++ library I work on.](https://gitlab.com/higan/higan/tree/master/nall) I would call it a standard library, but it's kind of anything but. Same basic idea though, implement all the standard objects, containers, algorithms but with saner namings, simplified code, more useful functionality, etc. Plus a few things that are specific to my own projects which I reuse a lot. You can easily write code that looks as nice as Python in C++, it's all up to the library. Unfortunately, having the standard one be so terrible though results in xkcd/927, basically. nall is no exception to that.
Windows, Python 64bit, test file size: ~92mb PS C:\Python\fileload&gt; Measure-Command {python load.py} Days : 0 Hours : 0 Minutes : 0 Seconds : 0 Milliseconds : 278 Ticks : 2789174 TotalDays : 3.22821064814815E-06 TotalHours : 7.74770555555555E-05 TotalMinutes : 0.00464862333333333 TotalSeconds : 0.2789174 TotalMilliseconds : 278.9174 C++ code. Compiled with MSVC 2015 64 bit full optimizations. Source [Insane Coding - How to read in a file in C++](http://insanecoding.blogspot.ie/2011/11/how-to-read-in-file-in-c.html) std::ifstream in("file", std::ios::in | std::ios::binary); if (in) { std::string contents; in.seekg(0, std::ios::end); contents.resize(in.tellg()); in.seekg(0, std::ios::beg); in.read(&amp;contents[0], contents.size()); in.close(); std::cout &lt;&lt; contents.size() &lt;&lt; std::endl; } Results PS C:\Python\fileload&gt; Measure-Command { .\filelength.exe} Days : 0 Hours : 0 Minutes : 0 Seconds : 0 Milliseconds : 75 Ticks : 759854 TotalDays : 8.79460648148148E-07 TotalHours : 2.11070555555556E-05 TotalMinutes : 0.00126642333333333 TotalSeconds : 0.0759854 TotalMilliseconds : 75.9854 Not using `std::ios::binary` **doubled the time** taken. Those are the lowest timings for both from multiple runs. I guess the file was cached but relatively the results probably still stand? P.S. std::ifstream in("file", std::ios::in | std::ios::binary); if (in) { std::string contents((std::istreambuf_iterator&lt;char&gt;(in)), std::istreambuf_iterator&lt;char&gt;()); std::cout &lt;&lt; contents.size() &lt;&lt; std::endl; } Gave a rather poor PS C:\Python\fileload&gt; Measure-Command { .\filelength.exe} Days : 0 Hours : 0 Minutes : 0 Seconds : 0 Milliseconds : 789 Ticks : 7896209 TotalDays : 9.13913078703704E-06 TotalHours : 0.000219339138888889 TotalMinutes : 0.0131603483333333 TotalSeconds : 0.7896209 TotalMilliseconds : 789.6209
Cross platform variant: #include &lt;boost/filesystem.hpp&gt; // ... std::cout &lt;&lt; boost::filesystem::file_size("longfile.txt"); &lt;&lt; std::endl Actually, `boost::filesystem` will become `std::filesystem`
It seems to me that a reminder about [try_lock](http://en.cppreference.com/w/cpp/thread/mutex/try_lock) may help to explain the behavior. In fact, try_lock will: - Either lock the mutex (if it is not already locked) and return true; - Or return false (if the mutex is already locked). So, your question was: what happens if the consumer's call to try_lock happens before the producer's lock? Festival, in this case, the data.mutex is not already locked. Consequently, data.mutex is locked and the while loop ends. Now, we are in a critical section. Thus, if the producer thread is executed, it will stop on the data.mutex.lock() line. The next instruction in the consumer's critical section is the std::cout operation. This operation reads both data.id and data.val. None of these values have been written by the producer code. Therefore, it is the initial values of data.id and data.val that are flushed to the standard output. In the constructor of some_struct, we see that id and val are both default initialized with {}, which is respectively equivalent to id{0} and val{0.}. Hence, you will see something like "0:0.0000" on the standard output. Finally, the consumer unlocks data.mutex, which leaves the opportunity to the producer thread to resume. In conclusion, the execution of this code can yield any number of "waiting..." and 2 outputs for data.id and data.val: - Either "42:3.1400" if the producer's critical section is executed first; - Or "0:0.0000" if the consumer's critical section is executed first.
See my timings with Visual Studio 2015 : https://www.reddit.com/r/cpp/comments/48shrb/seriously_c_wtf/d0mft4d
The python version isn't mmap()ing the file it's reading. It's using read() on it. 
couldn't reverse( reverse(range) ) just check if it is calling a reverse instance and if so just call start() end()?
Can you please elaborate on the race condition? In what case can it happen?
No. From the strace output somebody posted, Python is a) finding the size of the file via lstat(), b) allocating enough space to hold it via an anonymous mmap(), c) copying the file into that memory with read(). Also, people quoting the allocation stat in that sample massif run that got posted don't seem to have read the line in the documentation that says that massif doesn't include memory allocated via mmap() in its figures, just that allocated through malloc() or new. 
It's an efficient way of requesting (And later releasing) a large chunk of memory from the kernel. Many malloc() implementations will do the same for big enough requests. 
Wouldn't it be in the case where the encoding is *not* utf8 since utf8 characters are one byte in width?
It's broken. When the range is a temporary, the temporary gets deleted *before* the loop is executed. This is due to the way range-based for loops are defined. IMHO the proper fix would be to fix the standard, treat the expression like a function argument and only delete the subsequent temporaries of the expression *after* the whole for loop was executed. Sadly, that was already a [DR 900](http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_closed.html#900) and it got rejected. That said, the rationale for the rejection is only correct for the top-level temporary, not subsequent temporaries in the expression. I'll ask them to reopen it, maybe there's hope :) 
In utf8 a character is 1–n codepoints (combined characters) where a codepoint is 1–4 bytes.
In our code base we solve this problem by overloading range wrapper constructor on r-value reference and moving underlying range into it. So instead of having Range &amp;r we have variant&lt;Range *, Range&gt; inside. Maybe that way it works a little slower but you can use it inside range-based for or return it from a function with a temporary objects safely.
Best practice for SFINAE is generally to put the enable_if in a default template argument, not to put it in the return type. You also don't really need sfinae at all, you can just have a template with no primary definition, and some specializations: template &lt;class T&gt; my_from_string(const std::string&amp; s); template &lt;&gt; my_from_string&lt;int&gt;(const std::string&amp; s) { ... } Template specializations can be dicey on occasion but I don't see any problem with them here. Also, I think this solution can be cut down considerably. You don't need to use inheritance, you can just use a map from string to function pointer. All you need aside from that is the meta-function that returns a lambda with a proper signature, returns a lambda that takes an array of strings (which will decay to a function pointer).
I haven't looked into his history at all, I'm going by his behavior in this thread. Here's a good translation of what OP may have meant: "hey I want to compare finding the size of an arbitrary data input in various languages; why is my C++ method so much slower than doing the same in Python/etc?" To which the answers would have been given as they have been -- namely, that in C++ he was reading the file like a stream, requiring more memory allocation calls, while behind the scenes Python and friends are reading it like a pool, requiring one memory allocation call. Instead, he started out from a position of 'why doesn't C++ act the way I think it should', explicitly stated he didn't want to use the methods C++ provides to do what he wants and what is being done in other languages, and concludes with the sentiment that C++ is bad for not having the same meanings as do other languages and having the audacity to yield different performance results when asked to do a completely different thing. Willfully ignoring stream mechanics to try and push an agenda is not a brilliant way to endear yourself to people, especially when it comes with a demonstration of knowing that another way exists but refusing to use it because it uses the 'wrong words'
Good point, thanks for the hint! I'll see if I can find a fix for that. Though in my daily work, temporary ranges did not occur so far... Do you have an example for usecase of a temporary range?
It can happen quite easily: std::vector&lt;int&gt; foo(); // some function returning a vector and later: for( auto i : reverse( foo() ) ) { std::cout &lt;&lt; i &lt;&lt; std::endl; } The above will fail as the temporary returned from `foo()` gets destroyed *before* the loop is executed.
Because I don't have push notifications on my phone like you because I do not really care what happens on reddit. I happen to have a bit of time now so you get a reply. Aren't I nice. I beg to differ on your reading comprehension though. I really cannot understand how you got that from my post.
Got it, thanks! I'm on it, but it doesn"t seem so easy to solve...
Nice, something like that is what I try to achieve now.
`char` streams operate as if "characters" are 1 byte, regardless of locale
The consumer relies on the fact that the mutex is locked and then unlocked to determine if the producer is done: while (! data.mutex.try_lock()) { However if the producer thread takes too long to start up then it won't yet have locked the mutex by the time the consumer starts and checks if the lock is available. When this happens the consumer's try_lock will succeed immediately, blocking the producer from taking the lock. If the consumer thread wins the race it will print out whatever the default initialized value of data.id and data.val (0 in this case for both) is because the producer has not yet changed it. If the producer thread wins it will behave 'as expected' and this will likely be the most common outcome because generally threads will take the same amount of time to spawn (lots of caveats on this statement ;)). You could force this path to be taken and see the 0:0 output if you put a long sleep (1 or 2 seconds should be more than enough) at the start of the producer. Something like this: auto producer = std::thread{[&amp;data]() { std::this_thread::sleep_for(std::chrono::seconds(1)); data.mutex.lock(); data.id = 42; data.val = 3.14; data.mutex.unlock(); }}; I haven't compiled this so the syntax/calls may not be exactly right :)
All utf8 chars (bytes) are of course one byte big. To represent every Unicode code-point many chars (bytes) are necessary for utf8 and urf16. So using characters in this context is very misleading. If one read text file that contain utf8 encoded text into std::string than this string should be the same size as the text file. But of course do not forget about utf8 BOM (0xEF,0xBB,0xBF) that some files may have on the beginning. To better work with such files you will need to remove this 3 chars from the begging of the string. And of course std::string::size() will always return number of bytes and Not Unicode code-points.
Sure; but so long as the standard facilities encourage code like this it's a fair criticism of the language. You don't install Python and then need to find some other external I/O library to get efficient I/O; the default thing comes with decent I/O.
Ah. I didn't see minutes for it.
IRTA "hurr durr I am a self-entitled little retard"
mmap maps whole pages at a time, so if the file size isn't divisible by the page size there will be some extra space at the end. This extra space is required to be zeroed, so this will happen to work in most cases, but can fail (either with segfaults or larger numbers) if the file size is a multiple of 4096. And obviously, it doesn't work at all if the file actually contains a zero byte.
&gt; The above will fail as the temporary returned from foo() gets destroyed before the loop is executed. Can we say for sure whether it fails or not? Isn't that undefined behaviour?
In phase 3, he relaxes that restriction, by defining a couple of "prelude" headers that everything includes, and some "module" ones that group together headers that are commonly included together.
why?
In the standard, prior to C++11. And it doesn't modify storage duration per se, since it's always the default in contexts where it's legal; it merely declares it.
I started working on this a few years ago, just tidied it up and added a bunch of tests recently. It's my first chunk of C++ code in almost a decade, mainly just so I could play with C++11 features. Feedback solicited :) ~~It doesn't have a sensible build system yet, because I couldn't decide which of the horrible choices available sucked less than a plain old Makefile.~~ /u/tcbrindle converted the project to cmake! ~~I ended up bikeshedding a unit test framework because most of the options available were insane in some way (bleeding fingers, tons of ceremony, or weird things like recommendation to not package the library so it's impossible to use without a submodule (gtest)). Would be keen to replace this with some single-header-only or packaged framework, but I couldn't find one that didn't suck~~ /u/tcbrindle strikes again.. this guy is a machine! Now uses Catch for tests
In case it wasn't clear from the above, I'm not intending for this to be the next high performance library that will beat out the libraries I listed out above. This is mostly for my benefit to learn the corners of C++ that weren't really taught at my university as well as to help with class projects for this semester. Thanks for the heads up though. After looking at the source for dlib and Eigen, I don't really have hope of supplanting libraries such as those.
Oh, sorry. It's dependent upon GCC or Clang right now. Microsoft cuts too many corners in the standards, and I care more about OSX/Linux/BSD support, so I don't really bother with VC++ anymore. I don't really encourage or intend other people to use nall. It was just a necessity for me because I was too unhappy with the STL. But at the time there were no other compiled languages of decent performance without GC that weren't glorified assembly wrappers like C. It's exactly like you said, though. You look at std::string, and it's basically a glorified struct { char* data; size_t size; }; wrapper. The functionality inside of it is completely useless. Nothing to really work with strings as types: no upper/lowercase, no trim, no token-replace (their replace is just a memcpy wrapper), no tokenize (split), no wildcard matching, no string evaluation ("3+4"=&gt;(int)7), and even regular expressions are a disgusting hack in a separate class that is brutal to use. It's not good at all that everyone are writing incompatible strings, and everything else. UFCS really would have done a lot to help fix this situation: people could have created new string functions and used std::strings like actual objects. But Bjarne and co caved to a few critics, so the status quo it is =(
You even don't need to make overloadings: template&lt; typename container &gt; struct reversed_container { container container_; constexpr auto begin() noexcept { return std::rbegin(container_); } constexpr auto end() noexcept { return std::rend(container_); } }; template&lt; typename container &gt; constexpr reversed_container&lt; container &gt; reverse(container &amp;&amp; _container) noexcept(std::is_lvalue_reference_v&lt; container &gt; || std::is_nothrow_move_constructible_v&lt; container &gt;) { return {std::forward&lt; container &gt;(_container)}; }
Technically you are correct, but that is nitpicking. It might *appear* to work, or should I say *appear to not fail*? It might appear to work when executed the first time and crash or do whatever else another time. It's not important, just make sure you don't have that kind of code in your program.
Good news, everyone! Core issue DR 900 will be reopened to consider the case of subsequent temporaries :) https://groups.google.com/a/isocpp.org/forum/#!topic/std-discussion/xW0ODKBEkxQ
Just open that as memory mapped file and enjoy *real* speed :) 
&gt;Would be keen to replace this with some single-header-only or packaged framework, but I couldn't find one that didn't suck Have you tried [Catch](https://github.com/philsquared/Catch)? It's single-header, C++11, and I've found it to work quite well. Also, as far as build systems go, the C++ world seems to have settled on CMake these days. It has its quirks of course, but it's as close to standard as you'll find so it's probably the way to go.
&gt; I ended up bikeshedding a unit test framework I kind of want to see something with (most of) the power of pytest, as far as test discovery and enumeration goes. But IMO the *most* important part of a C++ unit test framework is the ability to automatically run *only* the tests whose code has changed.
Nice idea, didn't think about this.
Lesson to take away here: benchmarking is hard.
cmake &gt;=3.2 is quite pleasant, just use the new modern style.
I really don't think mmap matters much, what's important IMHO is reading chunks as big as you can, that's where the bottleneck is. Once you've done that sure mmap allocation may win you a little but it will start to be difficultly comparable as you have to launch the full python runtime etc. So I don't really know past that. Glad you found a way to be faster in C++ though, and happy that python is just so good by default, there are things to be learned from that.
That is definitely better suited for /r/cpp_questions. But the fact that you point out old questions is just another point in my book, why we shouldn't answer them here and instead delete them more consequently.
There is a blog post which is quite related to this. In the post the writer tries to create word frequency lists of a large text in different languages, and at first Python was significantly faster than c++, but after modifying the c++ code, it became significantly faster than before. This is the post i'm referring to : http://juditacs.github.io/2015/11/26/wordcount.html
Yeah. It's true. Streams in C++ suck donkey's balls. I typically use FILE from C for file handling, or wrap it in my own file object.
Catch's `REQUIRE` macro does some magic so that in your test you can say REQUIRE(x.get_value() &lt; 10) and if that fails, it will print both sides of the inequality, something like: x.get_value() &lt; 10 FAILED [ 20 &lt; 10 ] It works for all the relational operators as you'd expect.
http://i.imgur.com/3FckcgF.png
You shouldn't be reading huge files in memory unless you absolutely need to though. You shouldn't even read hundreds of small files neither if you can avoid it. It's pretty nice of Python to optimize that the best way possible because it's a common practice, but I'm pretty sure that as long as people start manipulating the file contents, the performance goes down compared to C++.
Calling external programs is easy enough in C (and C++ as well of course). However, piping mechanisms are clunky and not platform independent as they are in python.
&gt;platform independant https://msdn.microsoft.com/en-us/library/aa246905(v=vs.60).aspx For Windows. So doesn't that cover all bases 'cause Mac is UNIX too...? Again, forgive me if I'm oversimplifying.
FTFY: "I post this since I'm a faggot loloxoxo"
The benefit is reducing dependencies. &gt;One of our header files, “alias.h” had been included in something like 500 source files. After all the reductions, it is now used in just 35 files. When you start chaining .h files together, you quickly wind up with a mess. The real problem is that touching some header file can cause a massive number of libraries to require recompiling because it's several levels up the \#include chain. As I understand it, your .cpp files would need to \#include headers for all the types they actually use, but not necessarily all of the types used in the headers they \#include. If you use the pImpl idiom, the .cpp files won't even be required to \#include all of the member types in the implementation classes. Header files shouldn't _require_ other headers, as you say, instead they should just contain forward declarations for most of the types they use. Then the .cpp files just have to include what they use. If you do it right, the ordering of your \#include files in your .cpp files won't matter, and you wouldn't have had to waste all those half-days. Forcing these dependencies on everybody who wants to use part of your class is the _actual_ clusterfuck, and it's what the OP is eliminating. There's a whole item about this ("Item 31: Minimize compilation dependencies between files") in Effective C++. It is probably the best^^TM way currently to organize your code, but it is very very hard to do after the fact and requires some diligence to keep it correct going forward.
What distro of Linux do you work exclusively in?
Are you idiot? Did you see the Python code? They used read() method and built-in len() function, it means that he read the file and get the length.
I'm not saying C++ is perfect; I'm saying this is a bad example of its problems. And yeah these are all doing the same semantic task of reading a file but they're doing very different things mechanically. And clearly if the standard library has means of doing something, those means are standard, just not common. I'll happily rag on C and C++, but this isn't a justifiable case of doing so. It's like asking why waiting via sleep() is fine but waiting via while(true) causes unresponsiveness Streams aren't good at immediately seeking the end of the stream. I'm not sure how that's a surprise to anyone, given that the entire concept of streams is to act as if the incoming data was of infinite length. And I meant naïveté in code, not the programmer's skills. "Append data to a buffer until it stops coming in" is naïve code.
learn to embrace the seg faults, let your soul be consumed by them
Others have given some good posts, but I'll add: https://github.com/Microsoft/GSL ...and also mention that what "best practices" are is going to change-- possibly drastically-- depending on the needs of your specific situation. That's part of what can make it difficult to give blanket recommendations for C++.
Various versions of Redhat/CentOS and SuSE
&gt; When you start chaining .h files together, you quickly wind up with a mess. The real problem is that touching some header file can cause a massive number of libraries to require recompiling because it's several levels up the #include chain. Why not just using pch ? 
No, result of expression inside of range-based for lives till the end of loop due to lifetime extension. Range-based for is equivalent to the expression `{auto &amp;&amp; __range = range_expression; for (...)}` and so lifetime of the *last* temporary expression is prologed. 
More like: for (auto &amp;&amp; item: reverse(range)) {/*...*/}
As the release notes say if you dig into them enough, this has: * Our C++17-so-far [feature complete STL](https://blogs.msdn.microsoft.com/vcblog/2016/01/22/vs-2015-update-2s-stl-is-c17-so-far-feature-complete/), for one more day (then lotsa stuff will be voted into C++17). * Major STL perf improvements. * Many STL correctness fixes (I will be writing up the perf and correctness fixes soon). * Compiler variable templates (C1XX only, not Intellisense yet). * Lotsa compiler fixes, including constexpr (but constexpr is still C++11). An extremely limited number of things will change between RC and RTW. The significant one for the STL is a fix to std::tuple that will allow it to work with types like Boost 1.60's optional again.
size_t was actually C89.
Comment from blog reproduced here for convenience: A few small points. First, RVO existed pre C++11 AFAIK. The difference now is that in C++03, you were incredibly dependent on RVO, because the alternative was copying a string which is very expensive. Making some small change that accidentally disabled RVO could be a silent, enormous performance regression. Now, however, if RVO fails then the fall back is move construction, which is still pretty cheap. Second, but when discussing getline, you write: "this style had serious performance advantages over the alternatives, since it can avoid extra copying". getline still has major performance advantages over the naive alternatives, even with copying elided. If you rewrite getline to return for instance a struct with a boolean and a string, then yes you are guaranteed not to have copying in C++11. But for each line, you are performing a new heap allocation. The current getline interface reuses the same heap space over and over. There are more elegant solutions to this using ranges, but they require a little more work to set up. Happy to post details if you're interested. Third, in C++14, tuple has a new get function, where instead of specifying an integer that tells you which element in the tuple you want, you can specify the type itself. This only works if the type only appears once in the tuple. So as long as your multiple returns all have different types (this is a lot easier to guarantee if you are like me and prefer to avoid primitive types at interface boundaries in lieu of strong typedefs), you can still use a tuple and gain most of the clarity advantages of a struct without the boilerplate.
In my highschool, i was taught turbo c++. In college i am not sure about the vrrsion, but using namespace std was there which was new for me
It's no secret that tuples suck (although they can be useful) and are much hated. I second this.
I saw the Channel 9 interview, quite interesting. Congratulations on your work!
I was assuming that size_t is something which is used professionally, something not taught in a college course maybe?
Do you know when are we likely to see C++14 constexpr (or other compiler-side features)? Is it likely to come in an update or are compiler features strictly major versions only?
I don't think your classes were very good. Most aren't.
Aha! So the issue is that the life of the container adapter object gets extended but not the life of the underlying container. The adapter is responsible for this. 
structs are my preferred way over tuples as I usually need to use the grouping in multiple places plus having named return values is a bonus.
Python is written in C so it can't be faster as a C++ code written by a programmer who knows what he does. Besides python uses the io operations on the lowest level (fopen, fread and fwrite). Try to read your file with fread() and check the difference.
I am currently finishing C++ Primer (Lippman). Next I intend Data Structures and Algorithms in C++, but after that I have no idea.
[**@ericniebler**](https://twitter.com/ericniebler): &gt;[2016-03-04 21:05:41 UTC](https://twitter.com/ericniebler/status/705861797342003201) &gt;We're creating a new work item for a Modules TS! [#cpp](https://twitter.com/search?q=%23cpp) [#cpp](https://twitter.com/search?q=%23cpp)17 ---- [^[Mistake?]](/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=/48zdk3%0A%0APlease leave above link unaltered.) [^[Suggestion]](/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/joealcorn/TweetPoster) [^[Issues]](https://github.com/joealcorn/TweetPoster/issues) 
Also can we also get a Concurrency TS implementation. Blocking future.get() is an embarrassment for C++ and seriously cripples the use of std::future.
We're planning to work on this. No promises or ETAs yet.
I've never heard a race condition called non-problematic before :P 
Yes, that's what I meant.
EDIT: Whoops. I was wrong. This IS deferred until Oulu, and was not voted on today. I was thinking of uniform call syntax, a related feature which was voted on today and failed to get consensus. 
Does that mean that the feature will never get into C++17? Or that a further decision has to be made in Oulu?
Great, I love watching it. Just lots of little tips and tricks vids would be great. Such a fun language. Look forward to the announcement! 
Because there are no ambiguity. a-&gt;b() is a function call that call b() on something returned by a. a.b() : can you tell if b is a member of a or a member of a thing returned by the result of calling the dot operator on a ?
more info after //build conference. :) - thx, Steve, VC Dev Mgr
[The beginning of the tweetstorm says why](https://twitter.com/AlisdairMered/status/705844175858765825). &gt; More features approved by evolution for #Cpp17 than we will manage to adopt #CppJAX, pick up the rest at #CppOulu in June There was a bunch of nice things accepted that he tweeted after the deferrals.
Also uses void main... I really really hope this isn't typical of Indian programming textbooks.
Curious about this: https://twitter.com/AlisdairMered/status/705877885274427392 Is Marshall stepping down? o_O
[**@AlisdairMered**](https://twitter.com/AlisdairMered/) &gt; [2016-03-04 22:09 UTC](https://twitter.com/AlisdairMered/status/705877885274427392) &gt; Worried that our library chair is wearing a red shirt... \#CppJAX ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
Not to my knowledge, no. Marshall made a joke that he was wearing a red shirt for a reason after someone made a sharp quip :)
Because a is not a pointer, which is easy enough to check at the declaration site.
&gt;When you start chaining .h files together, you quickly wind up with a **mess**. That's subjective. &gt; The real problem is that touching some header file can cause a massive number of libraries to require recompiling Well, it's my own personal experience so it has little weight but... I don't think I ever was in a situation where compilation time was a factor in my decision of doing the right thing for the codebase or not. &gt;If you use the pImpl idiom But the purpose of that idiom is not to reduce compilation time because of fewer `#include` lines. It's for separating implementation details from a public class' definition. It may be a welcome side effect but I'm not about to split all my 250 private classes in two for that sake. &gt; Header files shouldn't require other headers, as you say I don't think I said that explicitly. I also don't think it. Yes, use forward declarations when you can, but otherwise I don't find anything inherently wrong or incorrect with a header file including another. &gt; If you do it right, the ordering of your #include files in your .cpp files won't matter If you do it the "dumb" way, the ordering also doesn't matter. &gt; Forcing these dependencies on everybody who wants to use part of your class is the actual clusterfuck, and it's what the OP is eliminating. I don't see any "forcing" happening when doing it the "dumb" way. You need `A.h`? Then include `A.h`. `A.h` needs `B.h`? Don't worry about it because `A.h` includes `B.h`. Is that "forcing" you to do something? My understanding of the OP's purpose is to turn a tree of references into a shallower tree of references but I just didn't see the point of that. 
Hooray file system TS! I only get to use C++ in my passion project, not professionally, and maybe don't keep up with the standards process as much as I'd like. I was overjoyed last week to learn that the file system TS existed *and* the version I already had in VC++ was a perfect drop-in replacement (save for namespace qualification) for the boost one I've been using the last few years. The `path` class and its wonderful `/` operator are fantastic. I write C# for a living and can only wish the core library had something for dealing with paths that's half as good as what's in the file system TS. When will we know which parts of the library fundamentals were voted in?
Not true when there are libraries involved, or JIT.
Well - promise of a renaissance is over. Ah well - we knew it could't last forever. Jokes aside - that's extremely disappointing. The fact that committee was not able to reach consensus even on UCS (I am not talking about concepts, which will reach 10 year of discussion) and those features was talked to death (mail groups, conferences, presentations) and was almost promised to be there, is quite sad. Legacy software MUST be supported, but with this old problems, is getting really hard to convince new person to write new software using C++. I'm also worried about competition. 3 years ago, Go had it's first baby steps. Now it's replacing Python, Ruby and even Java\Scala in some shops. C++ is in no danger from Go, because of GC. But it has different competitors. Once again - I don't talk about legacy software, which will work in coming decades. And established frameworks (yay Qt). Oh well. Just a little sad time for me. TS's should bring this functionality eventually. At least i hope so.
Basically all of v1 except for the invocation traits and a little bit of allocator stuff. I can dredge up an exhaustive list later, but the post meeting mailing will have it.
The saddest thing is the wasted momentum. C++11 really made things move forward, and C++ was great again, people were involved and coincidentally Microsoft started to catching up. And even C++14 was great. It complete C++11 and was never oversold. It is was it was supposed to be. But now... It look like C++17 will be almost inconsequential. So at the earliest all the much awaited feature will land in 2020. 9 years after C++11. It's a sad long time. I'm afraid it won't help the image of C++ in the larger community. Did the Committee wanted to do too much so they end up doing too little ? The number of proposals is staggering. Yes, I now, multiple WG, etc, but still. I wish they had concentrated their efforts on getting concepts rights before working on UCS on stuffs. After all, concepts are about 10 years old and /u/bstroustrup has been speaking about that since... 1983 ?
I expected future.then and executors at the very least. We already have implementations of them shipping in boost. Blocking future.get is an embarrassment to C++ and it is a shame that it will be around for 9 years at the very least before it finally gets fixed in the standard. The problem with this is that it causes fragmentation. Because std::future is so limited everybody and their brother are implementing parts concurrency TS on their own Boost has future with future.then Boost.Fiber has future.then Facebook folly has future.then Microsoft cpprestsdk has task.then Basically because the standard future is unsuitable as a building block for an asynchronous interface we have every async library reinventing it. It harkens back to the old days when everybody wrote their own string class. Even if std::string has its warts, it is so still better to have a standardized string class than for every library to roll their own. In 2016, futures and executors are such a building block. And even though the concurrency TS was just recently voted, future.then and executors In some form have been in several versions of boost and in already shipping libraries and have been used by Microsoft, Google, and Facebook. For purely library extensions I feel the bar has been raised too much compared with previous versions of the standards. For example shared_ptr was placed into the standard because we had a widely used implementation. Now not only must we have widely used implementations, we basically have to wait also wait 3 years further for it first to get a TS and then get into the next standard. 
As much as I really, really, *really* want modules, this is the right way to do it. I have the same opinion on Concepts, a feature they've been working on for about 10 years and still can't seem to get it right. Big features like this need a trial period.
Structured bindings are not a candidate for C++17; we do not intend to add new features to the working draft after this meeting. Structured bindings was reviewed by EWG. There was a lot of feedback, and the authors were unable to incorporate all of it during the meeting. So, they will come back to EWG next meeting. A lot of things were worked out, though. One notable change is the switch from {} to [] for the syntax.
&gt;That's subjective. We're all being subjective. &gt;I don't think I ever was in a situation where compilation time was a factor in my decision of doing the right thing for the codebase or not. Our (large) codebase takes about 2 hours to build both debug and release configurations. We have a few header files where if you touch them it takes slightly longer to build than it does to just 'git clean -dfx' and build from scratch. We don't do any of this recommended stuff because it _would_ take a long time to implement and we don't have the funding and there's a non-negligible chance modules get here and fix most of the problem for us in the meantime. &gt;I'm not about to split all my 250 private classes in two for that sake. Me neither but it'd be nice to have if it could have been done already. &gt;I don't think I said that explicitly. From your example, "B.h requires A.h" and etc. &gt;I don't see any "forcing" happening when doing it the "dumb" way. You need A.h? Then include A.h. A.h needs B.h? Don't worry about it because A.h includes B.h. Is that "forcing" you to do something? The dependency you force is a compile-time one. A.h should never need B.h. There's no technical reason for it. Making it "convenient" for .cpp files to not need to \#include the things they use is just lazy and should imo be considered an anti-pattern. If I'm C.cpp I might only use 30% of the types in A.h and not need anything in B.h, so I would be annoyed if somebody changed B.h and I had to recompile for literally no reason. The objectively ideal C++ .h file _would_ have no \#include files. I can see making exceptions for system stl stuff, but only because it's really hard to forward declare those things correctly (not impossible, though). I don't disagree that that is often prohibitively hard to do with legacy code, but it's technically correct and the best/most ideal way to do it with standard C++ right now. That's the way the language works, and while it sucks and is a real problem that hopefully modules come along and address in the future, that doesn't mean the lazy way to do it should be a suggested way to do it. If you can look at the cost/benefit of doing it the right way and conclude that it's not worth it, that's completely fair, but doing it the wrong way is not really something you should be getting away with in new code if you know better.
Modules were never likely to be part of C++17. It is a relatively major feature set, and we just don't have that much deployment experience with it yet. We just opened a Modules TS, and hopefully we'll get a working draft into it soon. A lot of organizations who are involved with standardization care a great deal about modules, so I'm pretty confident that users are going to get a Modules TS and an implementation of that TS in a relatively fast time-frame.
I'm surprised there was no progress on the move-only `std::function` front. That's an unfortunate and increasingly embarrassing oversight these days :-\
&gt; Because it is only a TS, compiler and std lib vendors feel less pressure to actually implement it. In the discussion implementers stated that TSs make it _easier_ for them to implement new and upcoming features as they can tell users that it's not part of the standard and they may break it, thus there's little fear of eagerly implementing controversial features.
I have to agree with many of the comments specifically I understood that the new TS system would speed up the introduction of things into the language. Not slow it down! How about finishing all of these things and making a C++18?
&gt; Also, they all have some sort of consensus or they would not be a TS. In the past, it would have been good enough to make it into the standard. I don't think this is correct. You are assuming that consensus for a TS == consensus for an IS. These two things are not equivalent. There are MANY things that are acceptable in a TS, but unacceptable in an IS. For example, we're completely fine with shipping an interface that we may break later in a TS, because TSes are experimental.
Well, the TS system speeds up the introduction of things into the language by providing normative, optional specifications of experimental features. We are currently intending on sticking to our release schedule - e.g. C++17 will still be C++17. It looks like we will be moving to a 2 year release process. So, we may be doing C++19, not 20. 
Er... No type-erased allocator?
After all the things that were in C++11, this is kind of a letdown. Hopefully ranges, concepts and modules make it into the next standard.
The majority thought that it just wasn't ready in its current form. * Core strongly agreed that the current wording was not ready to be moved into the IS. * There have been zero implementations from the TS. * The TS has only been out since October. * Numerous committee members that have extensively used the GCC 6.0 implementation love concepts, but voted no because they feel it need more usage and implementation experience. * There are unanswered questions about the compile time complexity of partial order of overload sets.
Wasn't there a "concepts-lite"?
This is concepts-lite.
Wasn't fold expressions voted into the working draft in Urbana back in 2014? What's new?
&gt; Legacy software MUST be supported, but with this old problems, is getting really hard to convince new person to write new software using C++. I do not believe this statement is true at all. C++11/14 consistently ranks as one of the most popular programming languages in a variety of online polls. Obtaining good data on the quantity and diversity of C++ programmers is tricky, but the numbers I have seen indicate a resurgence in interest in C++ that continues today. The C++ developer base has been growing in recent years. I can try to dig up the data when I get home tomorrow, if you're interested. &gt; The fact that committee was not able to reach consensus even on UCS UCS greatly complicates overload resolution - it has complications and complexities on par with ADL. Just as importantly, UCS threatens to present serious library maintenance challenges - users can write member functions which overload free functions which might have unintended consequences. Additionally, UCS and the operator dot proposal have some convoluted interactions which I also believe will lead users to write code with unintended consequences. On paper, I love this feature. If it goes into a TS, or a reference implementation is developed and shipped in a compiler, we'd be able to get some deployment experience and learn how feasible this feature is. &gt; was almost promised to be there, is quite sad. This is the thing I find unfortunate - that users believed that features like UCS were guaranteed to get into C++17. About 1/3rd to 2/5ths of the committee voted "no" on the UCS proposal that was suggested. 1/3rd abstained. I talked to many committee members with serious concerns about UCS, both at Kona and at Jacksonville. I believe it was fairly clear that there was not going to be consensus on the addition of this feature without deployment experience and a better understanding of the implementation costs and overheads.
The defaults for unary folds of empty parameter packs were changed. Only &amp;&amp;, || and , have defaults now. The defaults are true, false and void respectively. 
This is an important point. Think of TSes as "betas" -- they give us a chance to field designs and gain experience, before casting them in stone when we can rarely if ever take a breaking change if we discover something needs to be fixed. Another reason TSes actually let us move faster is by publishing a specification (at least in beta form) for new C++ features that otherwise would be too risky to try out if we had to do it in the standard itself. Finally, having those pre-standard published specifications also means that when vendors implement the still-experimental features, they'll implement them the same way and avoid fragmenting the C++ community with incompatible and conflicting pre-standard versions of the features that each vendor comes up with on their own.
This is already a "problem" with static libraries, and modules will use the same solution as libs: 1) Compile it yourself from source -or- 2) Download a distribution which is compiled using the same compiler as you. I don't really see this as an issue. The benefit of modules is compile time speedup, plus the removal of headers.
&gt; constexpr lambdas Yes! What about decltype of lambda?
Seriously, at least any_of() for tasks or future.then() should have been fast-tracked. As is futures are almost useless (speaking from experience, tried to use them and it is just impossible, so much state to keep around, so quirky to be notified if you launch several tasks.
alrighty tidy
C++ Fastcgi-Container (C++11): https://github.com/lpre/Fastcgi-Container
&gt; Also, it looks like we will be moving to a 2 year release process. So, we may be doing C++19, not 20. This is great. Could the cycle be shorter even ? Like, a year ? There are already some minor features voted in c++17, including the ones that were voted in in mid-2015. Are there technical limitations preventing already voted-in feature to be part of the standard right away ? Or is solely due to the nature of the ISO process ? As for TS, I suppose they are great for implementors and pet project but it not something I feel conformable using in a production software 
Modules barely touches the language, it's more of a compiler and linker implementation detail.
What happened to Fold Expressions?
They are great for implementors, but I don't think I can rely on them in production software. When TR1 came along I don't remember using it once. I used boost instead, because boost actually had a stronger long-term viability. And I think lot of people started using and discuting about shared_ptr after C++11, not in 2005. Being part of the standard is a huge selling point, for any feature. 
I know - I am one of the implementors of a HPX, a C++ runtime that is *completely based on future&lt;&gt;s and .then()*. I assure you, once we have executors in the Concurrency and Parallelism TSes, and sufficient deployment experience, I will write a paper advocating that the future&lt;&gt;-related changes from the Concurrency TS be added to the IS.
The things Alisdair listed were meant to be voted on this meeting, but we did not have time to consider them. My understanding is that they will be considered for inclusion in the IS at Oulu.
They were applied to the working draft of the IS sometime in 2014, IIRC.
Wow, that was fast. Great and many thanks.
Do you mind if I ask what's wrong with futures?
Eric niebler is handsome.
I don't have numbers to back it up, just work experience. On my area of enterprise software, C++'s use has been steadily pushed down the stack for infrastructure work when JVM or .NET stacks aren't able to deliver for the respective use case. So just tiny pieces, not the whole application. Even GUIs have moved to WPF for Windows only, or a mix of Web/Swing/JavaFX/Eclipse/Netbeans for when portability is required. No Qt or wxWidgets in sight. Where I saw C++'s usage increase on our space was as business code for mobile applications thanks to its presence in all SDKs, but then again, its use is focused on infrastructure as the respective mobile OS APIs aren't C++ friendly. Android is the extreme example from all mobile OSes where the NDK APIs are mostly only relevant for games. However most business are likely to use Xamarin or something like Cordova if their performance requirements aren't that high. So maybe worldwide the numbers are increasing, but on my little area of the planet doesn't feel that way.
I agree with the others who say while it is good for vendors, TS is not good for the end user. Try explaining to your boss why your production code has experimental in it If we had TS at the time of C++98 we might have had * TS1 The Standard Template Library * TS2 Partial template specialization After all didn't the STL come very late to the game and we would not want to delay the standard and we did not have any experience with those things. The STL in my opinion revolutionized C++ programming. It probably would not have succeeded so well if people though, "maybe I should not use std::experimental::transform, it is experimental and a beta after all" C++17 feels like C++98 would have felt without the STL and partial template specialization. There are some interesting little features but nothing that is really going to change how C++ code is written. 
well, it introduces things like #import... The biggest problem seems to be on how to support Macros with modules, thats where currently the biggest differences are between the MS implementation (needs a macro header if you want to use macros from the modules outside) and clang, which in its implementation seems to be more backward compatible with things such as macros. I guess the TS will show what direction we're going.
I was confused lol thank you
Solved Thank you
I'd rather have a good proposal stuck in TS than a pre-mature one in the Standard. Bryce, thanks for the update. WG21, thanks for all the hard work.
So excited. I'll go and take a crap in the meantime.
I also think that shorter release cycle would benefit current TS system. Faster "idea -&gt; proposal -&gt; implementation -&gt; testing -&gt; adoption" cycle with all fixes. It's also make more sense for TS to evolve that way. Edit: But I'm not sure about compiler devs. TS's doesn't force them to implement things, but this can double their job. 
&gt; I can try to dig up the data when I get home tomorrow, if you're interested. Yes, that would be really interesting. &gt; users can write member functions which overload free functions which might have unintended consequences. We are talking about f(x,y) -&gt; x.f(y) proposal, right? If so than this consequences was known before, don't they? &gt; This is the thing I find unfortunate - that users believed that features like UCS were guaranteed to get into C++17. Well - this because committee members are so great in introducing new things and be enthusiastic about it. In transition it also make other people enthusiastic. And when they don't deliver this new "shiny" things in time - people become sad. We all like new toys =)
I'm not sure. We just closed the Array TS, so I don't believe it is there. It was mentioned at one point during plenary, but I don't remember the context.
People were using `auto` type deduction, constexpr functions, rvalue references, variadic templates and built-in lambdas for a decade before C++11? Really?
Bang on. In India, CBSE class 11th and 12th used turbo C++ in 2007 and I would strongly assume that it is still the case.
People may not be happy with the language progress, but as a library implementer i'm excited by all of the progress that has been made moving LFTS and Filesystem into the IS.
Thanks for the suggestion. isocpp.org/tour seems like a good way to refresh and learn the parts I have missed in a short-er time than investing time in a 1000 page book. I will certainly go through it and assess if I need to go through a text book regardless. After that Eff. Modern C++
I like C++ but I'm not wed to it. If managed toolchains get to a point where they can do what C++ can do, more power to them. Quite a lot of the "enterprise space" is solving problems I don't really think C++ is appropriate for. It wouldn't be my first choice for a web site, nor would it be my first choice for a workflow management system or a "big data" analysis system. All of those are better supported by "script-like" languages like Python. But I wouldn't try to write an operating system, an RDBMS, or similar in Python.
The TS is based on the GCC implementation. There is no implementation done purely from reading the TS. This is important for making sure the TS is worded correctly.
Same here (i.e., I mildly voted in favor of the proposal brought to the evolution group, whereas I strongly opposed the x.f(...) =&gt; f(x, ...) "equivalence"). However, the people bringing objections against even the reduced version made a good case (which is why it didn't make it). It was really the only true surprise of the straw polls. That said, I think opponents opened the door to an alternative approach.
 #include &lt;stdio.h&gt; #include &lt;functional&gt; void meow(std::function&lt;void ()&gt;) { puts("nope"); } void meow(std::function&lt;void (int)&gt;) { puts("You have std::function SFINAE."); } int main() { meow([](int){}); } 
I personally don't think that having multiple implementations is good. Let's look at C++. It has multiple impls, which are basically different dialects of the same language. Because each compiler implementor wants its own bells'n'whistles. Just look at the amount of PP magic in Boost and the like. More importantly, we need committee to add any new feature to language. Even more importantly, it's slow both in producing any proofs and making decisions. Next, C# which has its own ECMA standard. The problem is, it's narrow down to the level when it's a very small usable subset of the language. So .NET and Mono are basically becoming two dialects, with Mono lagging behind. So I would prefer one and only compiler frontend with multiple backends for different platforms. That's how CLang, Rustc and all other LLVM-based compilers are developed. That's where I'm sure I won't get 2 different syntaxes for attributes or 2 different sets of compiler intrinsics. EDIT: Java is fully controlled by Oracle. But we still have OpenJDK. And that's the problem for any big product. The difference is, open compilers can be forked.
+1. Although I argued for the bracket-instead-of-braces notation, the more material change, I think, is the reformulation in terms of something akin to "by reference init-capture". That makes the semantics quite clean, and I'm looking forward to being to use this in C++ code.
What happened?
Me neither. I left C++ professionally in 2006. Since then most of our customers are using JVM and .NET languages for the type of applications we used to use C++ for. I still follow C++ and use it occasionally for personal projects, but would need to relocate to find C++ related work. These languages and the newcomers in the native space are what I was speaking about. For me I don't see Python as nothing more than a scripting language. Having compiled LLVM a few times last week really made me wish for modules. It took around 4h in an humble core duo with 4GB netbook. 
Thank you, but I mean using something like macros _MSC_VER to disable code that requires new std::function.
I wrote a c++ image detection application and wrapped it in a node extension so that I can easily deploy it across multiple servers. I dont know any c++ web frameworks but this could be an option.
I should have been more explicit, but by corner cases I mostly mean template programming and its associated techniques. 
Correction -- we discussed it Saturday afternoon :)
Isn't node single threaded? I want to be able to make use of shared memory since I have load large models into memory.
Some guy wrote about here a month or two ago. He was using folly from Facebook 
I still find it strange that key features are left for individuals with full time paid jobs to work on in their spare time. Perhaps organisationally more people should be 'funded' to develop proposals and implementations. I think Eric Niebler got some support in developing ranges and no doubt others do, but there are so many basic areas not yet solved in c++. Somebody give Chris Kohlhoff and Jonathan Wakeley 200k each to have a year long sabbatical and get networking sorted. Same applies in many areas. Relying on volunteers just isnt working. I am extremely grateful to those volunteers but the speed of things when key people havent got time to progress their work significantly between 6 monthly meetups leaves the whole userbase frustrated. Several years ago key committee members highlighted the need for a much richer library ecosystem. I dont see much progress. I see lots of platitudes and promises that things are faster but the key problem I see, other than manpower, is the desire for perfection in what gets standardised because everyone is so afraid of being hamstrung by the spectre of getting it wrong and suffering due to maintaining compatibility. I want compatibility but I also want pragmatism and I understand that even the smart people sometimes don't think of everything up front. Revision 2 or 3 of a product or library is often simpler and more effective in implementing the basic feature set. Sometimes the temptation is to pile in new features in 2/3. Perhaps the standards for TS should be lowered further so we get earlier implementations and feedback so the time taken to get to rev 2 or 3 ready for ratification is lessened. I feel that design by committee is slowing the pace of what goes into TS. Maybe look at a TS as a boost library review plus a bit better documentation but not a fully blown specification. 
Nice. There's also Zeal: https://zealdocs.org/
you could use [poco](http://pocoproject.org/). [link](http://pocoproject.org/slides/200-Network.pdf) to pdf with small http server sample
I came across this webserver. https://github.com/eidheim/Simple-Web-Server. When I tested with a rest-ressource against a postgresql db on my local network using apache's ab and 50 concurrent queries 10000 times on FreeBSD 10.2 and the default clang 3.4 compiler I got some bus errors. Using clang 3.7 fared much better.
How about Ulib which used to win various of the Techempower tests. It is [here](https://github.com/stefanocasazza/ULib). It is supposed to be high performance enough to outperform or be equal to everything from Wt over cppcms to all sorts of specialized stuff from other languages.
I saw the term "Rustwin's Law" coined on another forum recently: &gt;As an online discussion about C++ grows longer, the probability of a comparison involving Rust approaches 1. Thank you for proving this to be true.
Oh yes, with crap interfaces like this: void hello_world_resource::render(const http_request&amp; req, http_response** res) https://github.com/etr/libhttpserver/blob/master/examples/hello_world.cpp#L34
Yes the interfaces are pretty ugly. I forgot how ugly they are, all I remember it was a better looking interface than boost ASIO. I have to withdraw my recommendation to libhttpserver and just second libmicrohttpd. 
I'm partial to nginx and libfcgi. 
Why is this a fork rather than a contribution to the original project? 
I second this. Proxygen seems to be one of the best choices I've experimented with as long as you only need linux compatibility.
Node is single threaded, but the ffi allows it to call functions that run threads 
Pay attention to the versions. It is available only in Win NT and Win XP. Microsoft used to ship a posix compatibility layer, but not any more. 
[crow](https://github.com/ipkn/crow) is pretty cool. It's similar to flask(python) or sinatra(ruby).
The Networking TS isn't even finished yet, so it wasn't considered for inclusion in C++17. There were some design questions remaining at the end of the Kona meeting last year, so it wasn't approved for publication then. It was not put forward for publication at this meeting either - I expect that that was due to LEWG being busy with C++17 features all week. Best case scenario is that a Networking PDTS is approved in Oulu, which would result in it getting published late 2016 or early 2017.
It's shelved for now. std::dynarray was part of the Arrays TS, but EWG was unhappy with it in its current form. There's been little progress on a replacement for it for a few meetings now, so the Arrays TS was closed this meeting.
&gt;well, it introduces things like #import... It better not use the preprocessor. Jesus christ it better not use the preprocessor. &gt;The biggest problem seems to be on how to support Macros with modules Just don't. We don't need C-style macros anymore.
&gt;Things that have NOT received consensus to go into the C++17 working draft at this meeting: Uniform call syntax. Thank god. &gt;Things that are heading towards a Technical Specifications (TSes) based on guidance from this meeting: Coroutines Coroutines should just come down to 'Do whatever Kohlhoff says to do'. Microsoft's proposal is so flawed.
Concepts were promised for fucking C++11, then they were promised for C++1z, now they're "some time in the future maybe". Jesus just give me some concepts already.
The analysis of concepts before lite was that they were absolutely excellent but the standards committee is like all committees: it meets far less often than it needs to and design-by-committee always always always sucks.
I haven't seen a single UFCS proposal that didn't make me want to vomit. 
Please, please do not. It's not a good proposal at all. It adds huge, huge complexity to the language with very minimal benefits. It makes an already hard to understand aspect of the language even harder to understand.
Can you rephrase this post without buzzword bullshit like 'Internet of Things'. 
I was thinking embedded, but hosted could work too.
SG1 has been looking for a path to integrate both proposals. We talked about that pretty extensively at Kona, and a paper from Chris on a solution that might work for everyone's use cases was discussed at Jacksonville. P.S. Personally, I value Chris and Wakeley at a bit more than that!
I've been working on my own HTTP/REST framework for the last year: [pistache](https://github.com/oktal/pistache). It provides an HTTP server and an experimental HTTP client (it's experimental because my multi-threaded scheme is still buggy). The server-side part of it is the most completed for now. I also implemented a simple router and a simple DSL that you can use to describe your API that can be plugged to things like [swagger](http://swagger.io/). The project is still missing documentation but it's on my TODO-list. In the meantime, you can check various examples I wrote: * [hello_server](https://github.com/oktal/pistache/blob/master/examples/hello_server.cc) * [rest_server](https://github.com/oktal/pistache/blob/master/examples/rest_server.cc) * [rest_description](https://github.com/oktal/pistache/blob/master/examples/rest_description.cc) Note that the concurrency model is threaded AND event-based (it's using epoll underneath for I/O notification) Also, from a pure machine learning perspective, my company recently released [MLDB](http://mldb.ai/), an open-source machine learning framework written in C++ that provides [REST API](http://mldb.ai/doc/#builtin/WorkingWithRest.md.html) as well as a [Python](http://mldb.ai/doc/#builtin/Notebooks.md.html) interface that you can use to play with it.
I personaly see no flaws. Pretty ordinary debate of stackful vs stackless coroutines. Former require per-platform hacks to swap execution stacks, but no compiler magic. Where latter are quite opposite - they require AST transformations to convert function into FSM, but almost no need for special runtime.
declspec dllimport vs attribute visibility template processing MS's extensions like *_s functions Tons of preprocessor flags
It is absolutely not a debate over stackful vs. stackless coroutines. Both proposals are for stackless coroutines. 
I'm not writing a paper to express that I think it's a bad design, and I'm not the only person to feel this way. The mere thought that the committee hasn't already rejected it is quite frightening in fact. Do you people not understand how hard it is to learn C++ already without confusing people even further about name resolution?
Aren't lenses just papering over not having mutable state? 
What does the class do ? The number of lines / files is of little consequences to a point. Identify what the class do, and what each if the method do. Put all utilities codes in separate classes, try to remove code duplication, etc. You can have multiple sources files for a single header file, this can be useful to kick-start your effort. I'd start to remove as many global variable as I can. Do you have tests ? Because there is a good chance you will break things in the process. 
The committee works by consensus. There was not a strong consensus for either of the two designs to be go into the IS. So, we will need to find some way to come up with a compromise solution that everyone finds agreeable.
&gt; C++ is moving on from an archaic process of just putting stuff on IS to take it back after bad results in the field. It's fairly rare for us to take stuff out of the IS. Usually we do so because it's deprecated.
(L)GPL is a big no-no.
LGPL, GPL, LGPL/Expensive dependency. Seriously?
Take a look at the implementation. It's not safe to use it except perhaps on the intranet.
Yah, agreed. The major con of goroutines is its viral nature. From my perspective, I'm in favor of the Microsoft proposal because it can compose with futures, which is important to my users. If Chris' proposal composed with futures in a similar fashion, I think it would likely be suitable for me. &gt; It's a proposal that's designed to make the syntax that they like work, rather than to give users and standard library implementers good building blocks with which to build strong low-or-zero-overhead abstractions. I think this is incorrect. Both parties feel that their users want the syntax and abstractions they're proposing, and both proposals have support within the community outside of their authors. 
&gt; I'm not writing a paper to express that I think it's a bad design, and I'm not the only person to feel this way. You suggested that we should not discuss UCS. It sounds like something you have strong feelings about, as I do. I was informing you how you could provide input to the committee, in case you wished to do so. &gt; Do you people not understand how hard it is to learn C++ already without confusing people even further about name resolution? I do understand. That's part of my job as my organizations representative to the committee. I actually agree with you, and as I noted in my post, UCS has not been added to C++17. We took two votes on adding UCS to the C++17 working draft. I voted "no" on the yes/no/abstain poll. I was one of the five people who voted "strongly against" in the five way poll.
We are likely going to be moving towards a faster release train - C++19 instead of C++20. However, don't consider this normative just yet - Herb will let the community know if the decision is made to accelerate releases of the IS.
Any idea when std::filesystem will be part of libc++? That's the only thing holding me back since VS2015 already ships with it.
I'd prefer the latter. And no ADL for that matter.
I don't think Rust is a competitor. If Rust catches on like wildfire, and grows a user base, it could become a major programming language. History indicates that process takes time - years, likely.
Was the vote about the `f(x, y) -&gt; x.f(y)` part? In that case, I'd agree because anyone can just add new non-member functions . It's the `x.f(y) -&gt; f(x,y)` that would really be enabling to writing pipeline-style compositional code. If the ADL interference is breaking too much legacy code, then at least a new proposal using a new (builtin or library) operator `@` (just a placeholder, `@` could be `.` or `|` or `|&gt;` for all I care) would be very nice: `x @ f(y) @ g(z)` etc. And it should work on builtins too: `2 @ sqrt()`. 
They're competitors in the sense that Rust targets the same niche - system programming, high-performance systems etc.
It does when you say it.
Could you please elaborate? What is your biggest security concern?
&gt; I do not believe this statement is true at all. C++11/14 consistently ranks as one of the most popular programming languages in a variety of online polls. Obtaining good data on the quantity and diversity of C++ programmers is tricky, but the numbers I have seen indicate a resurgence in interest in C++ that continues today. The C++ developer base has been growing in recent years. I can try to dig up the data when I get home tomorrow, if you're interested. This is because C++ caters needs other language don't. And because C++11 really pictured C++ as easier, innovative and forward moving. Others languages are trying to eat C++ launch ( rust mostly ). My own experience is that C++ has still a bad reputation, because of its complexity and ability to blow your foot of. Case in point I am the sole C++ developer in game studio with about 50 devs. Java/.net programmers would not touch C++ with ten foot pole. Partly because FUD. Partly because valid concerns ( that the Core guidelines attempt to address), and partly because C++ requires a bigger cognitive investment that most goal-oriented developers are unwilling to commit to. If the language stays stagnant too long, I am not confident the user base growth will continue. 
Take a look at cpprestsdk from Microsoft https://github.com/Microsoft/cpprestsdk It is available on multiple platforms. In fact, it is in the repos of recent Ubuntu distributions and libcpprest-dev http://packages.ubuntu.com/search?suite=default&amp;section=all&amp;arch=any&amp;keywords=cpprest&amp;searchon=names You can use the http_listener to write your web_server
[**@aras\_p**](https://twitter.com/aras_p/) &gt; [2016-03-03 11:56 UTC](https://twitter.com/aras_p/status/705361200826155008) &gt; Game development or, "inheritance is hard". &gt;[[Attached pic]](http://pbs.twimg.com/media/CcnyRipVIAAT3o7.jpg) [[Imgur rehost]](http://i.imgur.com/bKXwl69.jpg) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I think what we really need is extension method.
Maybe, I've heard that it works fairly well, but it may not scale terribly well 
I tired it out, since it seemed like a good fit for what I was looking for (the python app I am porting is in Flask). My first impression is not every good. In the default setting the app logs each request to stderr, but it does so in a non-thread safe way so that if you have concurrent requests the output is garbage. I when disabled logging and ran a load test with ab, it would just close the connection if there are too many concurrent connects.
I suspect you are going to want to place a lot of the new classes and functions into a new namespace and put them all in a new directory. Having directories with names that match the namespaces can be helpful.
1. If you don't have them already, write tests, lots of them. If you don't, you're stumbling around in the dark and odds are you will end up breaking so many things you have to abandon the refactoring completely. 2. Consider grabbing a copy of Michael Feathers' "Working Effectively with Legacy Code". It's about exactly stuff like this. 3. One small step at a time, and make sure all the tests pass between each change. 4. Commit frequently so you can undo changes when you go down a dead end. 5. Assuming you pull this all off, you probably won't end up with 61 500 line classes. Poorly organized code tends to also have a lot of duplication since the authors couldn't find the first implementation of some thing hiding in there. As you reorganize things, remove that duplication and you'll probably end up with something quite a bit smaller.
Thanks! I stay corrected. 
I really enjoy Kate Gregory's teaching style of "C++ for this century" and not "C with classes." You can find her beginner [video here](https://www.pluralsight.com/courses/learn-programming-cplusplus) and all her [other stuff here](https://www.pluralsight.com/authors/kate-gregory). 
I also find it to be massively useful for refactoring large, tightly-coupled codes. The seemingly simple act of pulling out smaller functions/classes that are simple enough they *can* be tested naturally leads to better code. There's more benefit than simply the testing-for-correctness. Code that's difficult to test is invariably bad code.
I suppose I wasn't quite clear there. Currently many people ship headers and a binary library. Modules doesn't change this.
Everybody agrees `co_await`... what? It's not the syntax that is bad, it's the whole idea.
So we do have development experience, then, and it should be in C++17.
We have implementation experience of two different module designs. We have no document formalizing these implementations so that third parties can implement them which is the whole point of an international standard.
It's fine. I really like the api. Maybe some day I am going to implement something similar.
Then get off your lazy butt and write one. How the hell is possibly the most popular programming language in the world so dependent on just a few individuals?
You shouldn't split up a class just because it is big and has too many LoC. You should if it increases readability. For example, if you have a part of code inside your class which executes something for every subset of a given list, that should be its own class or function, like a "SubsetIterator". On the other hand, it doesn't make sense to extract parts of an algorithm which depend on another. For example, a bad usage of refactoring would be to split up "some_algorithm" into "some_algorithm_step1" and "some_algorithm_step2" just to have functions with less than X LoC. If you use this approach, it should be clearer where to put the new files. Also, this is better for /r/cpp_questions.
Outside of prime number theory, what is the zeta function useful for?
("Agrees" on the name if we do need a keyword in the current proposal.) But if Chris's proposal does work out though maybe we won't need a keyword, which is what I said would be best of all I think if it could work. And so we're going to give more time to find out if that can work (and if it works we can pursue that too or instead). Taking time to see if Chris's could get the same efficiency and be a better alternative overall is what Microsoft supported and it's what I thought you were speaking in favor of, did I understand correctly?
&gt;But if Chris's proposal does work out though maybe we won't need a keyword, which is what I said would be best of all I think if it could work. And so we're going to give more time to find out if that can work (and if it works we can pursue that too or instead). That's good, then. Yeah. My concern (although perhaps I didn't express it in the.. uhh.. friendliest of ways, sorry) was that I haven't seen much discussion coming out of the committee (obviously I'm not on the committee) about Chris's proposal and it looked like it was being pretty much ignored wholesale in favour of Microsoft's proposal. As long as that isn't the case (and it sounds like I was mistaken) I'm happy.
The C++11 workaround is dispatch ( std::bind ( [] (unique_ptr&lt;int&gt; P) { cout &lt;&lt; *P &lt;&lt; endl; } , std::move (P) ) );
 constexpr int f(int x) { return [x] { return x+10; } } Is this supposed to be constexpr int f(int x) { return [x] { return x+10; }(); } or am I missing something here? The first two examples below that are also missing semicolons? Also, 'Construction rules for enum values' and 'Clamping values' are missing the header markup.
&gt; Then get off your lazy butt and write one. How the hell is possibly the most popular programming language in the world so dependent on just a few individuals? It does not depend on a few individuals. I do not care about modules. If you do care then do it yourself, or pay somebody to do it for you.
Is there consensus on Ranges though ? Everybody agrees that Ranges are fantastic, but I don't think it's clear how to integrate them in the STL. Both concepts and ranges have huge implication on the STL and there have been talks of STL2
There was something about C++19 rather than C++20 - a two-year release cycle.
I don't have any "inside" information but from what I've heard that's indeed the case that there is debate on how to integrate them. I would love to read an update on that status though! Hope there will be a trip report about what's happening to ranges soon :-)
Can someone expand or link to a paper on `hardware_{constructive,destructive}_interference_size`? The article alludes that they're related to cache line sizes, but doesn't really say more than that.
The committee definitely relies on individuals.
C++17 won't help you with that...
I heard it might, but then there's Eigen &amp; stuff that don't work with clang-cl yet, so... ;) Oh well, being an early adopter can be hard ;)
We are considering names for a new stl namespace. EDIT: To clarify, a new STL will likely go in a new namespace, because we will not just be dropping the old STL.
Looking at it like this: What's the point of even releasing this as a new standard? There is nothing in it that brings sufficient advantages to convince people to switch. I mean: There has to be *something*. C++14 was the minor release, C++17 was announced to be *major*. 
Couldn't that be resolved by just making insufficiently constrained code like that ill-formed, no diagnostic required? Then at some later date, if we want to support concept checking, now it's ill-formed.
Wolfram (http://www.stephenwolfram.com/publications/history-future-special-functions/, the usual Wolframism is really only in the first paragraph, at least until he starts talking cellular automata) suggests it's useful for QCD; wiki confirms: https://en.wikipedia.org/wiki/Zeta_function_regularization 
The range-based `for` change was voted into C++17. We should start seeing it in compilers Real Soon.
Clang has triples for MSVC 2015: amd64-pc-windows-msvc19.0.0 i386-pc-windows-msvc19.0.0
You mean "template definition checking". Most committee members concerns did not revolve around template definition checking. Most committee members felt that it would be appropriate to continue as TS while implementers gained experience and continued to refine the wording. 
Could you expand? It sounds like maybe you're thinking of the gamma function?
I meant code that used concepts, but insufficiently. Templates that aren't concept-constrained would be fine as-is.
No, I meant only for when you use concepts - but don't sufficiently constrain it. Current code wouldnt change. 
Well, apparently nobody will be using concepts within that gap.
There's a lot of stuff in 17. This "minor, major" idea is harmful.
It would be great for the first real library use of concepts to be used to complement the "special_functions" proposal. Specifically, regarding the treatment of complex arguments and return values. Wouldn't this be a good place to use concepts? Especially [considering this](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1542.pdf): &gt;Many of the proposed Special Functions have definitions over some or all of the complex plane as well as over some or all of the real numbers. Further, some of these functions can produce complex results, even over real-valued arguments. The present proposal restricts itself by considering only real-valued arguments and (correspondingly) real-valued results. Complex numbers are ubiquitous in science and engineering. They should be in the standard.
Well, I guess that is reasonable. The other comment I would make is that it seems to be unclear how to help out with this process. How can the average interested cpp developer help move the needle on these issues? Is there a reason why these ISO meetings aren't streamed online ? 
&gt; You shouldn't split up a class just because it is big and has too many LoC agree. It is more from a standpoint of making it readable and testable. Yeah, I saw the /r/cpp_questions after the post. I'll look to there from now on. Cheers
&gt; Do you have tests ? Need to start here I believe. Its quite the effort however due to the many external dependencies that need to be mocked up.
Thanks for the Scott Meyers link above. This helped to have a plan of attack for the pieces that come out.
Good idea. I'm thinking to create a sub-folder next to the file, e.g. /source/monster_class.cpp and /source/monster_class/&lt;new_classes_go_here&gt;
Looks fine to me. Are you reading with something different from the normal web page?
&gt; Is there a reason why these ISO meetings aren't streamed online ? An interesting question. For now, let's ignore the logistics of streaming ISO meetings online. I do not believe it is practical to do so solely on logistical grounds, but there are other reasons I am opposed to this. Streaming the ISO meetings would seriously hamper the committees ability to do work. It would likely prevent a large number of individuals from attending due to concerns from their organization about the public optics of what they would say at a committee meeting. Those of us who would still be able to attend would probably be overly cautious about how we express our opinions due to the public scrutiny. &gt; Well, I guess that is reasonable. The other comment I would make is that it seems to be unclear how to help out with this process. How can the average interested cpp developer help move the needle on these issues? The best way for an interested C++ developer to affect things is to get involved. It's very similar to getting involved in politics, actually: * **Correspond with an author** - If there's a paper you care about, write the author! Tell them why you're interested in their work. Would you use the feature in question? Would other developers working in an environment similar to yours use the feature? If you have suggestions for how the paper can be improved, let the author know about those too. This sort of information can be very useful to an author, especially when they're presenting the paper - it can help them justify the usefulness of the work. My involvement on array_ref&lt;&gt; began with an email to the original paper's author. * **Write a paper** - If you see a way to improve the C++ programming language, write a paper about it and submit it to the C++ committee. Right now, we don't really have resources explaining how to do this, but my understanding is we're working on that. However, there are many great examples in the mailings, which you can find [here](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/). You don't have to be a member of the committee to write a paper. It is not necessary to attend a committee meeting if you submit a paper, but it is strongly recommended. * **Come to a committee meeting** - This is the best way to get involved. If you have experience with the C++ programming language and you feel you'd be able to effectively contribute to the standardization process, it shouldn't be an issue for you to attend a committee meeting. If you're not a formal member, you'll be unable to vote on plenary polls, but on the C++ committee, **your voice is often as important as your vote**.
Think about how to treat z^(1/7). What would you return? https://en.m.wikipedia.org/wiki/Multivalued_function 
Yes sorry you are correct I was actually completely thinking of the gamma function, although the two functions aren't entirely unrelated
It's the Store, not Desktop, API surface area. (Warning: this is the end of my knowledge.)
Someone sent a list around on the reflector. I think there was talk of sticking it up somewhere that is publicly accessible.
Right, this will be distinguished by the compiler id. We already have `clang` (for vanilla clang) and `clang-apple` (for Apple/Xcode clang). So `clang-cl` will fit in nicely.
The question is where do we stop in trying to encode all the possible variations/incompatibilities? Note also that we have a similar problem on Linux with, for example, clang being able to choose between incompatible libstdc++ and libc++. These are not encoded in the triplet. So I am not sure if we should try to encode /M* into the triplet. Maybe we should, seeing that we already have the runtime version.
What about the concurrency TS?
Honestly, I was just as disappointed as everyone else in that Concepts didn't make it to the IS, but I have to say these are very good reasons for the decision.
While I accept that these are good reasons, it is sad that neither modules or concepts made it into C++ 17. Means is will probably be another 4 years before those are available. I'd have preferred to see a delay than this.
&gt; Concepts have been widely expected to produce better error messages than are currently produced when template instantiation fails. The theory goes, since Concepts enables rejecting code based on a constraint at the point of usage of a template, the compiler can simply report the constraint failure rather than an error in some expression in a potentially deeply nested template instantiation stack. Unfortunately, it turns out not to be so simple and use of concepts sometimes results in worse error messages. Constraint failures frequently manifest as overload resolution failures resulting in a potentially long list of candidates, each with its own list of reasons for rejection. Identifying the candidate that was intended for a given use and then figuring out why the constraint failure occurred, can be a worse experience than navigating a template instantiation stack. I don't know if GCC6 get this right, but current clang with a concept emulation layer does not. This is extremely important to get right. For example with clang, suppose I write my own iterator `MyIterator`, and I try it to use it with a function that requires `RandomAccessIterator`. When I try to do so, i get the error: `MyIterator` does not model `RandomAccessIterator`. Ok: I screwed up the implementation of MyIterator: surprise, did anybody implement an iterator right at the first try? Probably not, now what? Why doesn't it model `RandomAccessIterator`? No idea. So I have to write a dummy function that takes a `BidirectionalIterator`, try it, and maybe get the error: `MyIterator` does not model `BidirectionalIterator`. These types of errors tell me that my type failed the type predicate, but the type predicate _is huge_. `RandomAccessIterator` is part of a concept hierarchy of at least 10 concepts and probably 20 or more (edit: i think this is a very good thing). I have to look up the whole concept hierarchy and bisect by writing dummy functions to find exactly which concept doesn't my iterator fulfill so that I can repair it. The fun is not over, though, because when I do find the concept that fails, this concept might have multiple require clauses, so then I have to write my own concept, and start adding the require clauses of the failing concept till I find the exact culprit. This is just not acceptable. An implementation of concepts should not only tell me that a predicate fails, but also which exact concept failed in a hierarchy, and which exact expression within that concept my type does not fulfull, something like: &gt;`MyIterator` does not model `RandomAccessIterator` because: &gt; &gt; - It does not model `Regular` because: &gt; - ... &gt; - there is no `bool operator==(MyIterator const&amp;, MyIterator const&amp;)` &gt;- It does not model `SomeOtherConcept` because: &gt; &gt; - ... Now if I have an overload set, there will be multiple overloads involved. If all of them fail I am going to get at best such an error message for each overload. I guess this is what the OP comments. IMO this is fine and better than what we currently have. While you might get a huge error message, you can at least find the overload you wanted to get picked, and find out which concept failed and why. So IIUC OPs case, concepts lite does help a lot here! The problem is, what if you wanted one overload to be picked, but another overload is picked up instead, and that overload is _under/partially-constrained_ (that is, it is constrained, but it uses something that it is not required by the concepts it is constrained on)? Then that overload will get picked, but when the compiler tries to substitute the types, you will get a template instantiation error. This is bad, because template instantiation errors are huge (its the only errors we currently have in C++). In particular, look at this from a compiler point-of-view. Concept checking passed! What is the likelyhood that you wanted some other overload to get picked if concept checking for them failed? Probably none! So why make a huge error message "huger" by giving you all the overloads and concept checks that failed at every step during the template instantiation backtrace? No reason! But what if you really wanted to have some other overload to get picked (e.g. because your type is not right yet)? You then really have no idea what went wrong and where. We already have this problem today without concepts (e.g. with SFINAE for expressions), so it is not really a new problem. However, compilers might try to help you by using concept checking information, and that might make the problem worse (by omitting information that you actually needed to solve the problem). If you think "well compilers shouldn't do that" I think you are right, but e.g. clang devs take very seriously giving you the best error messages possible, and that means omitting non relevant information. The heuristics for what relevant/non-relevant information means for C++ templates are far from being clear. Right now, "the whole world is under-constrained", so yes, this will be an issue, in particular when mixing partially constrained templates with completely unconstrained ones, which is going to be the common case. Why? Because fully constrained templates are hard to get right. If the standard committee and the standard library writers get this wrong sometimes, I am going to get this wrong very often. So what can we do? Well we could check that templates are either "fully unconstrained" (like they are now), or "fully constrained" (they only use what the concepts promise that they do require). This "splits the world", but in a backwards compatible way, and is probably the best we can do anyways. This would provide a good error message in the "contrived" case I depicted above, since the unconstrained template is either only partially constrained (in which case you will get an error saying that the implementation of the function is wrong and why), or fully-unconstrained, in which case you will get all the information you need from the overload set (because no concept-checking for the function took place). However, if we add concepts without definition checking to the standard, and in the future decide to add definition checking, the best we will be able to do is have a three-way split: fully-constrained, partially-constrained, and fully-unconstrained templates. We might be able to make such a three-way split work out nicely, but I doubt it will be as nice as if we only had a two-way split. From all of this it might look like I am against of concepts without definition checking. I am not. Concepts lite doesn't make _all_ template instantiation errors better, but it does make most of these errors better. I just would like for _all_ of the template instantiation errors to be better in the future. And for that I need to be reassured that we are not making a hard problem harder to solve just because we want this feature now rather than later. 
I'm not sure I go along with the 2 year release cycle, if it just means all major features continually get pushed out to the next version. I understand why concepts and (especially) modules need more work, but without at least one of these is it really worth having a new version? At this rate I'll be retired before I get a chance to use any of this stuff professionally.
Yes, in retrospect it might seem better to just have a 3 year release cycle with whatever is ready. Sometimes major features, sometimes minor features. With major features having had run through a TS. What Herb commented about TS being "beta" standards sounds like the proper way to see the new release cycle. Maybe back then they didn't wanted to scare implementors with "3 year release cycle, maybe major features in every cycle" or something.
This is really interesting, thanks for the link.
&gt; This probably could be added later if need be Doing so would require subsetting the Concepts TS. There has been opposition to doing that.
You have asked a lot of questions (I will address them in random order): &gt; How do you break this stuff down? &gt; The bigger question is what to do with the resulting source code. &gt; For example, if we follow the rule of thumb that a class should be no more than 500 source lines (uncle bob?), my existing class of 30,500 becomes 61 classes in their own right. &gt; What do you do with all the new source files? &gt; How do you organize such an effort? Do you put them next to the original source file? Or in a separate folder? Do you place them in a Utilities::Monster namespace? ---------- &gt; How do you break this stuff down? Into iterations. For this, you must make sure you can test each iteration separately. This means add tests, if you don't have them. &gt; The bigger question is what to do with the resulting source code. I usually do the following steps: 1. set up a baseline / starting point: 1.1. branch SC, establish refactoring base line/backup point 1.2. setup tests (you can add code to log all inputs and outputs for a method, then run your app, then write an automated test that (for a method) goes through tuples of &lt;in, out&gt; values and runs the test for each data point). This part will be a bit of a pain, but well worth it, once you can execute something repeatably. 2. pick one functionality area of the monolith; I do this based on levels of abstraction, and repetition. 3. write a clean minimal implementation for the area you want to extract, directly into the .cpp file (header and rest of application should remain unchanged), then use it in the implementation of your class. 4. test 5. write a clean, official implementation for the class: move it outside the .cpp file of the original code (into it's own header); since you mention that the code only has utility within the class, move it to a new namespace and/or directory. 6. test 7. doc invariants for the new code, add tests 8. test 9. move to step 3. At the end of the process, your large class should be describable as "puts together all steps for the XYZ operation", or (at some point during the refactoring) you may break it's interface into two (or more) abstractions. &gt; The bigger question is what to do with the resulting source code. Move it into it's own files (header and implementation); this will improve reuse and testability efforts; If the extracted functionality only makes sense to your class, you can: - use a details namespace (similar to boost); even better, find a better name (if possible) - move all extracted files into their own subdirectory - both of the above (this is what I usually end up doing) &gt; For example, if we follow the rule of thumb that a class should be no more than 500 source lines (uncle bob?), my existing class of 30,500 becomes 61 classes in their own right. Don't. extract code by abstraction levels, instead of LOC. If you do that, your class will (probably) end up in much less code than a 1-1 conversion. 
After reading this I think that it was right decision not including Concepts into C++17. But again why nobody write something about Reflection for C++ ? There was great library (with some compiler magic) proposal for compiler time Reflections that was probably safely to include into C++17 and then later extend to more general form in the next standard. 
Please help..
Well, nothing is ready to go in of the big things it seems. But there is progress on all fronts, things aren't stalling, so C++17 will bring a few new language things and mostly library improvements. But in parallel you'll have available the TS, which give you the needed language support. Once major compilers have implementations, its going to be in the standard. Much better then having a standard that takes years to implement...
Thank You. This makes me more curious. Do you already have any specific syntax in mind on how to expand the current state of concepts? (How would one define a concept with definition checking).
&gt;Is very lowlevel. Good as a building block, but not for everyday use. He demonstrated well how awkward his generators are to implement - "return closure which yields values through another closure". Ugh. This is good! C++ language extensions shouldn't be about presenting nice syntax. They're about giving us fundamental building blocks with which to build strong abstractions on top of. Then we can think about adding new syntax in future that abbreviates some of the more wordy bits. We (and by 'we' I mean the C++ community in general) didn't start the design of C++ by saying "how do we make range-based for loop syntax work?" The design of C++'s iterators led naturally to the `for (auto it = begin(x); it != end(x); ++it) { ... }` syntax. That's a bit wordy, even with the `auto`, and so we have range-based for, which does a very well-defined thing with those iterators: auto __foo = *expr*; auto __it = begin(__foo); auto const __last = end(__foo); while (__it != __last) { *type* *name* = *__it; *code* ++__it; } Obviously it's not exactly that, but it's very close to being that. The fundamental design of iterators came first. It's a good, performant, extensible design. On top of that we have been adding low or zero-overhead abstractions and a few little syntax extensions to make life nice, but **the core concept came first**. &gt;Due to resumable's resume, as proposed, you cannot generate stream of values which don't have default constructor. Because you must create temporary storage somewhere outside and then pass down as reference, so values can be assigned to it. A relatively minor detail that can be sorted out (and unlikely to be a real issue really). &gt;resumable and async/await are basically different concepts. Resumable is a cut-down yield, with only final result, where async/await is a syntactic sugar for "suspend current thread until specified expression evaluates somewhere else". Please don't mix coroutines and asynchronous computations. You can very easily build `yield` etc. using resumable. That's the point of Chris's proposals: just like we didn't make the mistake that D made, which is basing everything on ranges, and instead we based everything on iterators which are **strictly** more expressive. &gt;Resumables might be less viral, but not completely viral. Because 'resumable auto'. First of all, I don't see why do we need 'resumable auto' expression. Why do we need another syntax to instantiate resumable object? What happens if we do 'auto foo = bar();' where 'bar()' is a resumable function? If it's ill-formed, then why do we need yet another keyword there? And more over, Chris proposes less virality basically through 'oh, make this one resumable implicitly'. Because it makes sense. Why do we need to write `constexpr` when it's 'clear' that something is a constant expression? Well because it makes the code clearer, for a start. And Chris's proposal uses a very elegant technique that works well with C++'s primary means of doing polymorphism (templates) to allow you to essentially abstract over the difference between resumable and non-resumable functions. That's huge! The Standard library should not have to reimplement every algorithm in `async` and normal versions! &gt;I don't see any reason for such a cut-down thing like 'break resumable'. Suspending coroutine basically makes sense when there's some notable side-effect or some intermediate result is required. 'yield' can handle both, where 'break resumable' cannot handle second one without square wheels. That's like saying that we don't need raw pointers because we can just bake every type of smart pointer into the language. How the heck are you going to implement those smart pointers then? Right, you have to bake them in. Come on, this isn't Rust. This is C++! We don't bake every potentially interesting pattern into the language's syntax. We provide the tools to **build** abstractions. `yield` is one thing you can do, but it's a pretty narrow usecase and it definitely shouldn't be confused with the full generality of Chris's proposal. Your response to me is like someone seeing raw pointers and saying 'let's just bake every possible type of smart pointer into the language instead'. It's way easier to change this stuff, like the implementation of yield, if it isn't part of the core language. We want simple powerful core language constructs. The language should be the mechanism. Leave policy for libraries (including the standard library). 
Right, thats why VS 2015 shipped with a module implementation, GCC 6 will have concepts and clang also has a different module implementation...
&gt;modules, concepts I'm disappointed about these two, but they are what they are: really massive changes to the language that shouldn't be taken lightly. **If modules are done wrong, C++ could very easily die.** It won't necessarily die if they're done wrong, but it could. If they're done in a way that seems appealing at first but has big issues down the road, that could definitely end C++. If they're done in a way that just doesn't really work, and doesn't get widely adopted, then they're a massive waste of time and yet another shitty feature for compilers to support, deprecate and remove in 15 years. Similar story with concepts - there's a very real possibility that if Concepts Lite is done wrong, we won't be able to get proper Concepts in the future! Imagine that! We'll be stuck with shitty Concepts Lite and some ad-hoc crap fitting in around the edges if they get it wrong. Better to wait. &gt;coroutines They can't decide between just doing what Microsoft tells them they should do and going with Chris Kohlhoff's proposal (which actually fits with C++'s language design philosophy - focusing on core language features that give a lot of efficient/low-or-zero-overhead expressive power to library builders). It's a bit too early to be having coroutines yet. &gt;uniform function call syntax This and `operator.` are completely braindead ideas that overcomplicate the language. I'm so, so relieved that they haven't made it in. C++11 we got decltype, auto, etc. They were all great changes, basically. The committee seems to think that it has to keep making changes like UFCS so that C++ seems 'fresh'. Well let me tell you now, if they add UFCS it will be one of those things people talk about in their "features of C++ you should avoid" talks. It'll be widely regarded as one of the most pointless mistakes in the history of the language. Mark my words.
&gt; We don’t just want shiny features in C++, we want features which will mature well instead of adding more gotchas.
Please no dynamic allocation.
"C++ Primer" fifth edition, for a gentle but effective introduction to vectors, arrays, storage containers, templates, and general programming techniques. There are introductory sections on variables and functions that may seem like you should bypass but there worth at least skimming to get a handle on const declarations and references.
They are waiting because there is not enough experience with those *implementations*. Regarding concepts, there is a detailed blog post why its not in C++17: http://honermann.net/blog/?p=3
That is an excellent, clear, thorough explanation!! You guys should broadcast this, not have it buried multiple comments deep in some reddit thread.
&gt; or am I missing something here? Yes. It's the lambda syntax with an [omitted parameter list](http://en.cppreference.com/w/cpp/language/lambda). Also, in your second code snippet, you misplaced the parentheses, it should be right after the capture list. 
Ideally IDEs should be able to mark in the code each function that failed to match the overload resolution. IDEs that use clang can do that quite easily (KDevelop does that IIRC). So when you get an error with a list of 152 failed overloads, you can simply navigate to the function that should have matched it, hover over it, and get the contextual information about why that overload failed.
&gt; Could both forms be referred to in an uniform fashioned at the site of use ? That has been discussed, but no papers have been submitted to propose such a change. Since Concepts are defined as either variables or functions, using a non-variable syntax to evaluate a function concept, or vice versa for a variable concept, seems a little strange. Consider: template&lt;typename T&gt; concept bool C() { return ...; } static_assert(C&lt;int&gt;); // Implicitly calls C&lt;int&gt;() ? Likewise for variable concepts: template&lt;typename T&gt; concept bool C = ...; static_assert(C&lt;int&gt;()); // Not actually a function call ? My preference is to introduce a single concept declaration that defines concept entities that are neither variables nor functions; they would be their own kind. I intend to explore this in an upcoming post.
You guys do know that GCC is an open source compiler. If you had wanted something different, you could have submitted a patch sometime in, say, the last 2 years. Sadly, few such patches were received. I can count on one hand the number of people who have worked to make the implementation better. I cannot count the number of people who have lined up to throw stones. And yes, this is how I am perceiving your comments. Be sure in your next blog post to mention you have failed to investigate the problem more deeply than simply showing up to complain.
If you're looking for a real-world guideline as to what constitutes "enough", [Boost's naming](http://www.boost.org/doc/libs/1_60_0/more/getting_started/windows.html#library-naming) is a good indicator. It covers more than you need, e.g. STLPort, but has served them well for a long time. I can speak from experience (designed/maintain the build system for a cross-platform team targeting multiple Windows tool chains) that the static/dynamic linkage of the runtime is very important to encode.
Another big thing your code is missing is const-corectness. Mark member functions which don't modify the object as const, and pass objects by const-ref if you don't modify them. `node::isSolved`, `node::isValid`, `node::getCandidates` `Node::getMostConstrainedCell` `Node::getNumPossibilitiesForCell` and `Node::getPossibilitiesForCell` and `Node::printNode` shall all be `const`. The constructor shall take `const int *p`. `getGrid` function be const too and shall return `const int *` (Also any function defined inside the class declaration is implicitly inline, no need to spell that explicitly. And you don't need a semicolon after function definition). It can look like this: const int* getGrid() const { return grid[0]; } `solveR` shall take node by const ref: bool solveR(Node const &amp;n)
Honestly, this is the core problem I see with Concepts. So, so many people are talking about the theory behind it. All these wonderful features, how it's going to make our lives better. But when it comes down to it, nobody (well, one person, I think you know who that is) seems willing to actually *do it*. Actually actually implement the idea to see if it works. Patch and refine the one implementation we do have. The committee seems to recognize this problem to some extent (it's one of the main reasons it's not in C++17). I don't know how to fix this. I'd love to work on the Clang implementation, but I'm not sure I'm up to that.
No need to be passive aggressive like that. &gt;this is how I am perceiving your comments. I perceived it as stating facts and nothing more, namely that gcc6 doesn't get it right. This much is quite relevant, given /u/gnzlbg 's claim that he didn't know this. He didn't say that gcc "sucks", or that it's "bad", nor compared it negatively with any other implementation. If anything, he stated that it's been better since Sutton's (you, I assume?) patch. Good job working on it, but there's no need to be bitter about it. &gt;I can count on one hand the number of people who have worked to make the implementation better That's about as many as I expect to be capable enough to do it in the first place, not to mention that many people already have other things to do too. /u/tahonermann stated he didn't know of any good solution, so what exactly do you expect him to contribute with? &gt;Be sure [...] to mention you have failed to investigate the problem more deeply than simply showing up to complain. You're the only one who's throwing stones here, as far as I can see. With an attitude like this, I don't think you're going to get many contributors in the future either.
I just posted the proposal on std-proposals: https://groups.google.com/a/isocpp.org/d/msg/std-proposals/XqD3BLeEDkw/nOLEJegABgAJ
&gt; my employer moved away from C++ to JVM and .NET platforms in 2006 The C++ committee was moving too slow back then. Considering the C++ committee is run by volunteers and not by a single organization, it's making pretty good progress. The committee has pointed out in the past that having more people join and participate will help move things along more quickly. C# and Java benefit a lot in that they are basically run by a single organization.
Andrew, I'm very sorry that my comments are coming across as criticism, that isn't what I intended at all. I have so much respect for the pioneering work that you have done and for the time you have committed to it. Without you, Concepts obviously wouldn't exist at all. My goal here is to try and help build consensus on a solution for the community and I believe that requires identifying and discussing the issues that appear to be contentious in a wider audience than has previously been discussed. I want concepts in C++. You are absolutely right that I have not contributed anything other than bug reports and concerns. I'm still new to using concepts, am still new to the standardization procedures, and am not yet familiar with the gcc code base. I would very much like to contribute more and will be trying to make time to do so soon.
&gt; You were promised more in multiple web publications as well as conferences such as CppCon. Or rather, the committee was overly optimistic when it publicly stated what could be in C++17, and the internet rightfully interpreted this as what would be in C++17. Sure reddit is angry. You know what? It always has a base of anger, and they have a right to be angry because they were promised something! I can't tell if the author is serious or not. I watched CppCon and other presentations. Nobody *promised* anything other than the committee had a goal of shipping a new standard in 2017. The committee may have been optimistic in their presentations, but do you really want them to be pessimistic? If anything, I think that reddit community and maybe some bloggers are more responsible to hyping expectations into perceived promises. That's my take anyway. I'm grateful to the committee for all their hard work, for putting up with all the criticism, and for fighting for what is right. I really like the idea of TSes and believe that is a great way to get experience with features so we don't end up with export again!
Type erasure requires dynamic allocation for arbitrarily-sized objects, but the small functor optimization is equally applicable to move-only functors.
A highly subjective statement, prone to disagreement.
Huh - thanks for the detailed explanation. So Concepts TS only check what kind of type has been passed inside method, not how it was used. I was wondering about _template definition checking_ and all talks about it. Thanks again.
Some good algorithmic insights here: http://norvig.com/sudoku.html
Concepts Lite isn't even in the standard. I think we are really lucky to already have a complete implementation that can compile the STL2! Whoever thinks that such a first implementation is going to be complete, fast, correct, and have awesome error messages should seriously review their expectations. As early adopters start using (and definetly abusing) concepts, they will break every single compiler in 1000 unexpected ways, and the quality of error messages will significantly improve. 
&gt; The Concepts TS was published 2015-11-15 following approval by the committee in between the Lenexa and Kona meetings. The TS has therefore only existed in a published form for less than four months. I don't see how anybody could have expected C++17 to get Concepts. The purpose of using a TS was to give a bit of time to get experience, and 4 months isn't *nearly* enough time for that. Not for a core language feature. &gt; The Concepts TS does not specify any concept definitions. Some committee members question the usefulness of concepts without the availability of a concept definition library such as that in the Ranges TS. Adopting the Concepts TS into C++17 without a corresponding concept definition library risks locking down the language without proof that it provides the features needed to implement a library as might be designed to conceptify the standard library. I'm not sure I buy this argument, especially since it's creating a chicken-and-egg scenario. It also adds yet another layer of complexity to the proposal: now it needs a concept definition library to be published at the same time, which adds more debate, more discussion, and a whole set of new technical problems to add to your list. We need to be simplifying the whole thing, not adding to it. The real point of the Concepts TS is so that we can gain experience building a concept definition library, just like the Ranges TS. *That library doesn't need to be published at the same time*, just used for experience. &gt; The Concepts TS includes new syntax to define function templates. An abbreviated function template declaration looks similar to a non-template function declaration except that at least one of its parameters is declared with a placeholder type specifier; either ‘auto’ or the name of a concept. This abbreviated definition might be my favorite feature about Concepts. It simplifies and unifies so much about the template syntax, making it more accessible to more people. Ok, so some people think it goes a little overboard on that, hiding the fact that it's a template. There are solutions to this (`template void f(X x); is my favorite. I've also seen `void f(auto X x);` suggested). But I also think the problem is overblown. It's the programmer's job to understand what X is. This has always been the case well before the idea of Concepts existed. Is it an abstract base class? What functions does it have? How are you expected to use this class? Adding in Concepts makes this problem a bit harder, to a degree (since there's one more possibility to consider), but at least it's a Concept with a defined interface and not a completely unconstrained template parameter. And a lot of these concerns apply to generic lambdas as well, which really aren't a problem in practice. It's an overall win in my book, even if there are some subtle downsides. &gt; The Concepts TS also includes a template-introduction syntax that allows omitting the verbose template declaration syntax that we’re all used to while simultaneously stating type constraints. I don't see any reason this feature needs to exist. As much as I hate the verbose declaration syntax, it exists and it works. For simple cases, use the abbreviated syntax, and for complex cases use the full syntax. Seriously, there are *four different ways* of stating constraints (last I counted). It can be introduced later as a separate proposal after Concepts is part of the IS. And so I'm fair about this, the same goes for the abbreviated syntax that I love. If it's getting in the way of Concepts as a whole, just take it out and put it back in later if possible. Trying to include everything and the kitchen sink is killing Concepts. &gt; There are two forms of concept definitions; function and variable. I am so split on this one. I hate the function-style definition they're verbose and just feel weird, so I would prefer the variable-style. But if that doesn't provide the capability that's needed, then it should absolutely be dropped. Having two syntaxes makes absolutely no sense. To make it a bit easier, would there be any benefit for allowing the abbreviated syntax? concept bool C(auto x) { return ... } &gt; A number of committee members are concerned about whether the current Concepts design suffices as a foundation on which full template definition checking can be implemented in the future. I look forward to a future post about what these concerns are. This is the most important and valid concern of all those you raised (most of the others are additional features that, worst case, can probably just be dropped for now). However, &gt; It seems unlikely that these concerns will be addressed other than through an implementation of definition checking. If this is so, then all hope is lost. We already tried that way and failed. The point of Concepts-lite was to remove the definition checking half of the equation entirely. Just get one side working ... *and we can't even get that side right*.
So what did the committee have to say about this proposal at the Jacksonville meeting?
&gt; They can't decide between just doing what Microsoft tells them they should do and going with Chris Kohlhoff's proposal (which actually fits with C++'s language design philosophy - focusing on core language features that give a lot of efficient/low-or-zero-overhead expressive power to library builders). I hadn't seen that proposal. It actually looks very clean and neat, just what C++ needs.
/u/andrewsutton I know you feel responsible for the current state of gcc's concept implementation, but IMO you should be proud of the TS, and you should be proud to have deliver the first implementation that is able to compile the STL2. GCC is a community project, and concepts can still be considered a pretty "niche"/"very early adopter" feature (GCC6 hasn't been released yet). Soon you will see more libraries using concepts, people reporting bugs, and more patches being submitted. Once clang and microsoft implement the TS and the feature becomes an early adopter feature, the number of contributions will definitely increase. Hell most new features are implemented as proof of concepts in branches and never get merged at first, the fact that GCC6 will ship a complete concepts implementation is awesome and going to speed concept usage in C++1z libraries significantly.
&gt; This makes me more curious. Do you already have any specific syntax in mind on how to expand the current state of concepts? (How would one define a concept with definition checking). No. This is what definition checked templates would look like -- just regular constrained templates. I don't think I need any new language features to do this.
This completely. Well, they did add the parallel stuff. Which probably will be the sole major change. I'm not counting the filesystem lib, because people could use boost. (It's an exciting feature nonetheless). I think the expected "major" nature of C++17 is the principal source of disappointment ( not anger, nobody is angry here !) And as I said elsewhere its is not the first time Concepts are not included in a standard, it's actually the third time. At some point after C++14 a huge number of proposals started to emerge while concept stayed in limbo. I think the fact that Microsoft implemented a form of coroutines (that do not please everybody) in VS long before concepts is telling. There certainly was a lack of directions / focus. Also over optimistic people, on both side of the fence. To be clear, I don't think people had real expectations about modules and ranges but even a few month ago, It felt like concepts were a given. On the "bright" side, C++ had never been much transparent in its development process, and our disappointment is probably related to the fact that the post-C++11 committee is much more sharing. ^(I'm still looking forward to the obligatory "How to improve your codebase with Concepts, today !!!" at Cppcon16) 
&gt; which cripples their usefullness for creative usecases I admit you made me laugh out loud, well played :D Does this mean that I can write: template&lt;typename F&gt; auto foo() -&gt; decltype([]() { return F{}; }); and use `foo` in an unevaluated context like: using a = decltype(foo&lt;int&gt;{}()); // calling the lambda here but I cannot write something like: template&lt;typename F&gt; auto foo() -&gt; decltype([]() { return F{}; }) { // I am not the lambda below return []() { return F{}; }; // I am not the lambda above } auto lambda = foo&lt;int&gt;(); // which lambda am I? since I would get a return type mismatch?
No, you can't write the above because you'd have the type of a lambda appearing a function signature, which would still be disallowed per [expr.prim.lambda]/2: &gt; A lambda-expression shall not appear in an ~~unevaluated operand~~ (Clause 5), in a template- argument, in an alias-declaration, in a typedef declaration, __or in the declaration of a function or function template outside its function body and default arguments__ (all emphasis mine) __Edit:__ However, writing template &lt;typename F&gt; auto foo() -&gt; decltype([]() { return F{}; }()); // &lt;- notice I call the lambda would be allowed.
Thank you, I will take a look.
Indeed. It is the same problem as many open source software have: people *expect* stuff, but are not willing to *participate* and/or *contribute* (money, time). Plus, standardize stuff that works always, everywhere, portable, implementable, etc. is *hard*. All I read lately is everybody complaining about unmet expectations, which should be met, now, for free. What a shame. 
I thought that maybe as long as you don't define the function, just declare it, that might be fine. One could probably find some "creative" uses for this in e.g. `has_member_...` kind of traits. Anyhow, cool thing. Thanks for writing the proposal.
&gt; On the "bright" side, C++ had never been much transparent in its development process, and our disappointment is probably related to the fact that the post-C++11 committee is much more sharing. I think this is pretty much entirely it. From the perspective of people not involved in the committee, C++0x spent most of its existence as some mythical vaporware that may or may not ever actually exist for real. C++14 was the first version to be really designed in the open and happened to go pretty smoothly, so a lot of people got unrealistic expectations.
&gt; which actually fits with C++'s language design philosophy - focusing on core language features that give a lot of efficient/low-or-zero-overhead expressive power to library builders Chris proposal has issues too - in requires non cross-platform implementations for each OS\Arch. It's also doesn't solve "blocking problem" and I'm not convinced about it somewhat _implicit_ nature. This are my main concerns. Also boost.Coroutine2(and 1) is stackfull. And boost.context \ boost.fibers too. Keeping this points aside - there is already implementation inside boost library, so I fail to see why you somewhat angry. I like boost coroutine (2) variant - but it can be already used. Committee too, stated that they want both options. I also think that FSM transformations can benefit in terms of speed in the long run, but we'll see. &gt; Well let me tell you now, if they add UFCS it will be one of those things people talk about in their "features of C++ you should avoid" talks. It'll be widely regarded as one of the most pointless mistakes in the history of the language. Mark my words. This sound a lot like FUD. When you make such accusations it's better to have some code that will back it up. 
&gt;What Bjarne says about the feasibility of this list &gt;&gt;I consider this complete list feasible, to be delivered in the standards text in 2017 and in compilers and libraries at the same time. I don't think Bjarne can meaningfully speak about delivering features he has no hand in developing. EDIT: Seriously... Feature development is largely done by volunteers like myself. 
E.g. there is no `std::path` constructor that takes a pair of `std::path::iterator`'s.
My main "angry" point was that this concerns and objects started to flow when C++ almost reached C++17. Where were _this_ guys in last two-five-seven-ten years(concepts) and why they weren't involved in conversations before? There is mail discussion group, why not use it?
 double getLength(); 
It didn't get in *over* anything. It got in because Beman Dawes did the work to make it ready to go into the standard. Coroutines are not even handled by the same group of people. What have you done to inform the committee you want these features? EDIT: Also Filesystem has at least 2 mature implementations. Boost has been shipping it forever...
I don't see the issue. Lots of what's currently available in std::experimental in gcc and clang are of higher quality than many widely used thirdparty libraries. The "experimental" tag might sound scary, but it's probably on par with Boost as far as (API) stability goes.
Plain `template` is an explicit instantiation. `template &lt;&gt;` is an explicit specialization.
Template specialization needs `&lt;&gt;`: template&lt;&gt; void f(int); So no conflict there, at least for this naive example. I can only find two places where `template` is not followed by a parameter list. One is dependent name lookup. This one shouldn't be a problem as it must follow `::`, `-&gt;`, or `.`, which should never be the case in this syntax. The second is explicit class template instantiation. This is where there could be a problem with parsing. It looks good to me, but there could be a tricky corner case that I haven't thought about. &gt; I suspect that syntax isn't viable, but I'm not sure. It would be inconsistent with existing parameter declarations for generic lambdas. Yeah, the generic lambda syntax would need to be modified for consistency here as well. After all, that's part of the point, to make things more consistent.
Right, thank you for the correction.
&gt; So... how do we get more money into C++ so that our best and brightest and actually work on what we want them to work on AND make a living? You can't - Nvidia, Intel, MS, AMD, IBM and others can, but then we have another problem - conflict of interests. Right now every proposal can be rejected and moved into reworking. With corporations actually PAYING for the standardization and feature inclusion process, they can start to implement things only they need. Design by corporations it will be (like C# and Java). Which may not be so bad if not for another reason - if NVIDIA want one feature and AMD want another, and they are conflicting - how do we resolve this situation? This can get ugly extremely fast.
To expand on /u/xyphanite, using "foo(void)" is a C thing. In C++ omit the "void" and it's the exact same thing.
Thank you guys. I have read all your answers. Thank you
I like to put void as a "Page intentionally left blank." reminder.
I think there were major concerns that Microsoft was pushing something into C++17 where a significant number of people disagreed with a lot of the choices. Or was that not coroutines? I think it was.
We are not able to repro this issue locally. Could you clear out the packages folder (and packages.config) and try it out. Could you provide us with more information? Are you trying to add it to the solution or project level? Or other nuget packages installing fine? 
Ya committee is bad model
Assuming setting the length has no side-effect (e.g. error-checking, automatically re-adjusting other members, etc., though if that's the case, you might be better served with a struct), here's another option at your disposal: double&amp; length() { return length_; } double length() const { return length_; } // ... foo.length() = 4.;
Sorry for my stupidity but I didn't really follow. Maybe you misunderstood me because of that question. I'll try again. How would a concepts definition look like with definition checking? I mean there must be a way in code to express this (the definition checking besides other constrains) and I don't mean a whole new syntax.
I can't find array_view even in TS
Ah, now I remember hearing about that. Still was a sizable chunk of work before that, though.
What about [cpp-netlib](http://cpp-netlib.org/)?
C++ was never a good language for 'enterprise application development'. And won't ever be. That's not its goal. It's a systems programming language. And it's still by a MASSIVE margin the best choice for that task.
You could argue that's what the TS' are. Features not (yet) part of the standard, but they will be if they prove they work in practice.
&gt; Sadly, there really isn't much choice in the system programming space. Rust? D? ATS? Heck, some people even use OCaml in that space (see Mirage UniKernel project)
The Visual C++ team was actually happy with the concept spec. Most proposals are usually implemented and spec'd by the same individuals. This new demand is quite unusual. I wonder what would happen if every implementer has to vote against a proposal because he or she hasn't personally implemented it yet. There is a risk associated with voting something in; there is also a risk associated with delaying something.
That's a great observation. Maybe the solution is that Bjarne, as the undisputed BDFL, should've said to the committee "C++17 is, first and foremost, about Concepts. Nothing gets in until we deal with Concepts." Herb formed all of these working groups with the grandiose idea that we could fill all of the feature gaps simultaneously, but I think all that it actually did was water down the talent pool and distract everyone from the main priorities. As important as Concepts are, it is ridiculous that there is only one guy working on it, volunteer or not. Ironically, the one guy actually drawing a legit C++ paycheck was working on a project that itself requires Concepts. In an ideal world, the powers that be would've recognized the priority inversion and said "Eric, you need to help Andrew with Concepts first." The simple fact is that most of the lowest-hanging fruits are gone. Every major improvement from here on out is going to be heavy lifting because life is complicated and shit is hard. It's clear that prioritization (even if politically unpopular) is going to be required to get anything done in future standards. This is how software gets made in the real world...
Neato! This project does sound interesting. Setting a cpp project up can take some time so anything that simplifies that is a good idea. However, I do question the point of demoing this with a video. I'd say that examples and well written documentation (and tutorials) are a lot more effective in getting people interested.
&gt; 1.2. setup tests (you can add code to log all inputs and outputs for a method, then run your app, then write an automated test that (for a method) goes through tuples of &lt;in, out&gt; values and runs the test for each data point). This part will be a bit of a pain, but well worth it, once you can execute something repeatably. This is along the lines of print statements of sorts. Good idea. Let me see if I'm understanding your proposal here... 1. The tests are system or application level tests. Understanding the level of coverage could be difficult since coverage tools are close to non-existent (native VS2012). 2. Then you effectively delete the method targeted for refactoring and implement with what you'd like it to be. 3. Then fill in the blanks to fulfill the many tests. Is that about right?
Agreed. But we're early in development and wanted to start getting feedback on what people think. To see if people would use it, like it, have suggestions/questions etc :) One of my favourite pieces of functionality is the ability to go "shimo --update-library boost" and in ~60 seconds I have the latest version from the repository. I've had to recompile every version of boost manually in the last 3 years... it takes a long time. 
How does the Khronos Group function? I would say that companies should be able to pay for a seat at the committee, and the committee should solicit contributions from the community. Even if there was an just optional donation option when people sign up for conferences, it should be enough to hire a few full time developers.
Why should it matter if people are qualified? If they're interested in C++, I'm happy to speak with them. JF is a cool guy, not sure what your issue with him is. Yes, he looks good in a hat.
Ah you're right. It was me who was missing something then!
We're focused primarily on indexing, but have had some other groups express interest in working on a front-end. Additionally, we place a strong emphasis on enabling cross-language indexing, particularly for things like Protocol Buffers and other generated code. It's definitely still early stages, but progressing rapidly.
It's still a giant flashing invitation to TOCTOU security vulnerabilities.
There's no developing going on here.
Concepts should be much higher priority than modules.
Yeah the split into working groups seems to have been a mistake. They're trying to do everything at once and not really thinking about how they work together. If we're not careful we'll have `std::ranges::vector`, `std::parallel::vector`, `std::concepts::vector`, etc. They're already starting to do stuff like duplicating algorithms for parallel use (should not be necessary, work out a way to avoid duplicating work, we've got templates, bloody use them) and now there's talk about duplicating algorithms **again** for MS's braindead coroutines.
Nothing useful.
Personally, I did not want to receive bug reports for a `.foo()` method of my class that *I did not write*. I feel strongly that the syntax you suggested is a maintenance nightmare.
What? How do you think proposals and their implementations come from? They are developed by somebody. 
Yay. Now I will have to reapply the dozens of fixes I made to get 1.6.0 to compile in g++/Windows ! 
Good response. By default Shimo will create /include and /lib folders within the root of your project (or where you instantiate shimo from). You only need to add the /include to your project's include folders and /lib to your library search path and you're good to go. Outside of this you can configure your project layout anyway you like. We'll also allow you to change the location of the /include and /lib paths if required. The aim of Shimo is to be as non-intrusive as possible. 
If Bjarne really wants something to happen, it will happen. Nobody else has that kind of political leverage.
OpenGL predates Khronos, so I'm not sure how much we should blame them for it. I was thinking more about OpenCL and Vulkan.
Great work, thanks for the effort! Good luck with your project. Some remarks: * Why do `lib/` and `include/` don't go into the `.shimo/` directory? * Can I install dependencies system-wide? If I have 10 projects, it's probably not a good idea to store 10 local copies of boost. * I see in your video that there are multiple entries for the same version of boost, e.g. two entries for 1.59.0 from two different people. Which one am I going to chose? Which one is the better one? This creates big problems, like with ppa's in Ubuntu. For example half the OpenCV ppa's don't work properly, but you need to try them all to figure it out...
Have you tried sending them upstream? The easiest way to avoid having to maintain your own patches against OSS is to get them incorporated into the project itself.
Most of them were changes to the build system, but I may have broken compilation for the MSVC targets (which appear to be the only thing that Poco officially supports in Windows). I haven't got the time to go and modify what I did so that MSVC builds still work. The main issue in the source itself is one where I'm not sure exactly what is going on. g++ expects that if a class is labelled `__declspec(dllimport)` , and a member function has an out-of-line definition marked `inline`, then the function declaration inside the class definition must also be marked `inline`. There were about 100 occurrences of this that I had to fix for g++, but IDK whether this is a g++ problem or a MSVC problem. `__declspec(dllimport)` is of course not covered by the C++ Standard. I posted on the forum asking what to do with my changes but nobody replied (not that I saw, anyway). 
1. lib/ and /include can go in the .shimo directory if you wish. They're currently in the project root folder so you can install your own libraries in the same location and they won't conflict. 2. We have no plans to make installations of libraries system-wide at this point. Although it could be an interesting piece of functionality and something we'll definitely consider. FWIW compiling and keeping a copy of Boost yourself will use about 1.5GB of space. Taking just the headers and libs and packaging them up uses ~75MBs. 3. By default it'll use your own copy. Otherwise the newest publicly available. You can specify the version and owner yourself to override this behaviour. (e.g shimo --add-library &lt;library_name&gt; &lt;version&gt; &lt;owner&gt;). You'll be able to quickly replace libraries if you have issues (and we'll likely implement a feedback/reporting system for bad libraries).
Uhm - how is it even possible? I mean - how do you propose to banish member functions without affecting existing code? There is visibility issues, Inheritance. Constructors and destructors which most of us use for RAII. Is this all must go away?
With UFCS friend free functions could replace member functions, but I'm not sure what the point would be.
i disagree?
Um I might be misunderstanding exactly what you're saying so if you could give a code example that would be nice but I'm not aware of Go supporting static methods (if that's what you're representing there) and I wouldn't expect UFCS to support static methods. 
But how is, from compiler view, static method different from free function if method first parameter is *x or x? I meant that you can't get rid of member functions, if type system already relying on it. I also showed you that Go too have a concepts of free functions and member functions and they are not interchangeable. That's it. Edit: change typos and language
I'm not a committee member, so I don't think you should ask me :). I have nothing against this feature, but also none points for it. This is exact situation where TS and compiler's flags would be nice to play and see how different code is behaving.
He didn't sound done yet, either. Following iterator improvements and algorithm improvements, there's the whole side of all the composable algorithms.
&gt; Maybe the solution is that Bjarne, as the undisputed BDFL, should've said to the committee "C++17 is, first and foremost, about Concepts. Bjarne is not a BDFL. &gt; In an ideal world, the powers that be would've recognized the priority inversion and said "Eric, you need to help Andrew with Concepts first." Huh? Eric doesn't work at the behest of the committee. There are no "powers that be".
Why not?
I think that argument makes it a great candidate for a TS if language features qualify. It'd be interesting to see often that is actually a problem. Otherwise it's sad to see progress so impeded essentially due to fear.
To be honest "design by committee" is part of C++'s success. The fact that it is slow to evolve is a feature, not a bug. It means people can rely on it and there's less chance of (new) dead weight being added to the language.
C++: the language of the Ents.
That's not development. 
$200 &gt; $0
Now it worked, and the output didn't mention a 'Miscellaneous Files' project. Strange. Thanks for looking into this!
Is there any alternative for someone who needs only sockets and database(sqlite or pg)?
While the standard is crucial to compiler implementors, it is also valuable to users, and the number of users by far exceeds the number of compiler implementors. Use of the language is being actively hindered this way. Access to the standard should be free. The IETF and W3C both somehow manage to publish free specifications. ISO is stuck in a pre-digital age.
You may want to check [Do you plan to support binary packages?](https://build2.org/faq.xhtml#binary-packages) `build2` FAQ entry for some potential problems with binary packages in the C++ context.
&gt; two Only two? Given the horror stories I hrad, Id expect 10+.
thanks for posting this
Let me reword that example: template&lt;Integral... Args&gt; // All Ts satisfy Integeral void f(Args... args) { (args + ... + Integral(0)); } I don't understand why checking the definition of that template is not decidable (I must be missing something). My logic tells me that all `args` are constrained, model `Integral`, and as a consequence they all provide `operator+(Integral, Integral)`, so type-checking is not only decidable, but it actually type checks fine. On the other hand your original example should not type check, for two reasons: - the`Integral` concept does not require an `operator+(Integral, int)`. So unless it requires it, it should not type-check because an user can construct a valid `Integral` with an `operator=(Integral,int) = delete` that would break at a call site. - `T` is unconstrained. As a consequence, `T` might have no constructors, destructors, ... This makes it potentially impossible to create a `T` at all, and as a consequence it is impossible to know if one could pass it as an argument to a function, or even instantiate a function that has it as an argument. Instantiating `f&lt;impossible_type&gt;(...);` would then again fail at a call site (`impossible_type:` a type with all constructors/destructors `delete`d, for example). A function that has been "template definition checked" never fails to instantiate (or at least this is my definition of what type-checking a template means, you might have a different one). This allows things like type-checking a template function once, but not having to type check it every time on monomorphization. &gt; I don't distinguish between unconstrained, partially constrained, and fully constrained templates. If you want to do definition checking, then the same rules should apply everywhere. AFAIK type-definition checking is only decidable and sound if all types _used_ by the function are constrained. The only case I can find in which it isn't necessary to have a type parameter constrained, is when that type parameter is not used at all by the function (e.g. something like a dummy parameter defaulted to `void`). I am pretty sure I can break any other example by just passing an `impossible_type` to whatever template you give me, but I would be happy to be proved wrong (by break I mean making a function that type checks fail at instantiation time). Maybe when we talk about "template definition checking" we are talking about different things. For example, if I would allow templates that have been type-checked to _sometimes_ fail to instantiate, then I guess I can make better sense of your points. This also would allow calling unconstrained functions for constrained ones, which is something you mentioned before. But while this might be useful or desired, this kills _all_ the advantages of type checking: - even if your program type-checks it still might not compile (and you will still get bad error messages sometimes), - template compilation speed will still be slow (since compilers must still type-check a function template multiple times), - template header-only libraries still cannot exist (because they can't be type-checked). - even if an user fully constrains a template, it can never rely on it being fully constrained, because he might inadvertently call an unconstrained function that still fails to instantiate. IMO if a type-checked template is allowed to fail to instantiate, we shouldn't complicate the standard with it. It would be useful as a warning is some compilers want to provide it, but it doesn't make the language better in any way in the sense that it doesn't properly solve any problems.
You probably should also consider the WinXP toolchains (e.g. v140_xp)
&gt; I'm in total agreement. I would rather have seen a delayed standard with (at least) Concepts Lite. Without that, or at least one of the "major" features listed, this is just a patch. The delayed standard is C++19/20, don't you think we are taking things too seriously? People talk here as if the next chance was on year 2035. 
If you really want to learn C++, then choose C++ Primer or Accelerated C++. But these books are sometimes treated as TL;DR.
I'd say the lack of people is also quite a problem. You can't really translate millions of programmers to lots of available free time to work on C++. Firstly, not everyone's code is of the same quality. Secondly, if you don't have the time to really get into the project, it doesn't really matter whether you have 5 minutes or 10 minutes available, you might as well have none at all, and it doesn't matter that there are millions of people, if none have the required availability.
You won't get such reports, it should be trivial to see where `foo` is defined. The first scheme works great in D and I don't remember anyone there having trouble you worry about.
Project page: [build2.org](https://build2.org)
I'm looking through the Library Fundamentals TS that was accepted and I noticed what I believe is an error in the definition for `apply`: template &lt;class F, class Tuple, size_t... I&gt; constexpr decltype(auto) apply_impl( // exposition only F&amp;&amp; f, Tuple&amp;&amp; t, index_sequence&lt;I...&gt;) { return std::forward&lt;F&gt;(f)(std::get&lt;I&gt;(std::forward&lt;Tuple&gt;(t))...); } IMO, it should be using `get&lt;I&gt;` here instead of `std::get&lt;I&gt;`, with `using std::get;` on the line above it: template &lt;class F, class Tuple, size_t... I&gt; constexpr decltype(auto) apply_impl( // exposition only F&amp;&amp; f, Tuple&amp;&amp; t, index_sequence&lt;I...&gt;) { using std::get; return std::forward&lt;F&gt;(f)(get&lt;I&gt;(std::forward&lt;Tuple&gt;(t))...); } This way people can write their own classes that match the tuple interface and it will work with `apply`.
Surely you understand that going from a committee model to a community model would 1) not be one person designing everything and 2) it would still be C++ going forward.
In the brave new world, there are no virtual functions so there is no overloading as it's done now. Rather, polymorphism is achieved through constrained template functions. As to visibility, well that's what modules are for. If you don't want a function to be visible, then don't export it.
With Coati (https://www.coati.io/) we went a step further and looked at the semantics of the source code as well. Our interface features a graph visualization that gives an overview of how a class or a function relates to the rest and thereby makes navigation and understanding even simpler.
I'm simply afraid that we'll be left with only this bare feature and no simple building blocks, like we have none for iterators in stdlib. Oh, and if we compare with pointers, I would prefer abstraction whose ears and tails won't be sticking out of every piece of code, and which can be nicely isolated without tons of wrappers.
&gt; Each complex number except zero has two square roots, three cube roots, and in general n nth roots. The nth root of 0 is 0. [Wikipedia](https://en.wikipedia.org/wiki/Multivalued_function)
Another issue that isn't mentioned here is the current model for developing proposals. I sometimes wish that C++ had a github repository with proposals, where people could send their proposals as pull-requests, where the proposals could evolve, dynamically, with contributions of the whole community, where people could comment, inline, on particular aspect of the proposals, and this intertwined discussions would help evolve the proposal. And maybe, once the authors deemed them ready, they could be merged, get a paper number, and discussed in a meeting. The google groups are actually a pretty bad place for evolving a proposal since issues cannot be addressed inline and it is hard to follow the proposal as it evolves (do you need to copy paste it again)? Also, _sometimes_ the community there feels a bit toxic (borderline passive-aggressive). I think there is no excuse for that.
"What's next?" "Windows/VC++ support"
Nice to see gtkmn and sigc++ are still being developed. 
&gt; D is what happens when you start with C++ and then have one person design everything, instead of a committee. I have used both languages for years and I will take the "one person" design any day of the week. It's also called "vision". &gt; If you try to change C++ then you just reduce the number of choices available to programmers. No. 
Yes, the last example in the Intro uses a cross-compiler.
Our primary focus at this stage is to develop the build system. Once it is reasonably complete, we will start working on populating the repository. Having said that, the other day I was very tempted to package a little `sha256` library. Maybe will still do it if find time... Also, I am going to try very hard to package all the dependencies of the toolchain (ODB, `libstudxml`) before my talk at C++Now 2016.
Actually, the Committee meets both in and outside the US. (I've attended the Bristol, UK and Rapperswil, Switzerland meetings in the last few years; I missed the Cologne, Germany mini-LWG meeting.) Also, purchasing the Standard is **NOT** a prerequisite, especially not for $200. In fact, the IS is (hilariously) not useful for Standardization work. You need the current Working Paper, always available for free.
I've been told that a lot of the work on this is done. Hopefully you'll be seeing it soon.
So C++ iterators weren't modelled asfter pointers? I'd say that types like std::vector show quite the opposite. And the set of required operator overloads to make an iterator. And they have that issue with undefined length sequences, where we need to figure out some "stub" value for end iterator, instead of just asking if the sequence is empty. And many more troubles.
If it doesn't work with CMake these days, it kinda stands no chance. So... does it?
The "open" was not the point of my post. The problem is the lack *participation* and *contribution*. And it's really not about the $200. The hard work are (mostly) the proposals and the implementations. The effort going into them are magnitudes more costly (time, money) than the $200, which you don't even have to spend (see [here](https://github.com/cplusplus/draft)). The hard parts will not automatically vanish, if people can *just* open a pull request. Pull request for what exactly? A change to a proposal? A change to an implementation? Something else? The evolution of C++ is hard work and I appreciate the effort all the people involved are putting into. Often on their own time and money. That put's me in a position, where I don't even have to think about *demanding* or *expecting* anything. If people see the current state as such a huge problem, please, do something, anything. It seems to me, people involved are quite bright and already putting huge effort into it. To tell them, "you are doing it wrong" won't solve *your* problem. Sorry, doesn't work that way. 
And now we need tons of boilerplate code to please iterator contract. With, as I said, no building blocks in standard library. And with no compile-time check.
Very cool. I use variadic templates to the excess because they are so powerful and reduce the amount of code in many situations.
&gt;I have used both language for years and I will take the "one person" design any day of the week. It's also called "vision". That's great. You seem to be agreeing with what I am saying then. You tried D and C++ and you prefer D. No problem. Some prefer D and some prefer C++. That's why it is good to have multiple choices that do not try to be like each other.
The meeting in person though has real advantages when it comes to resolving issues. In any event the cost of the travel should be borne by your company. If they aren't willing to do that then you are working for the wrong company. 
Could these experimental proposals be implemented somewhere like clang under some verbose syntax (hidden &amp; disabled by a compiler flag) so they're 'ready to go' after debate is settled. (just tweak the syntax and enable it)
How would 'community' work in practice... each compiler could have gated features, and the "standard" is simply the common subset of all compilers? each compiler team copies the best features from others, eventually arriving at a consensus? I can imagine syntactic avenues being difficult to approach this way, I wonder if you could also separate the language into syntax and AST. (once a feature is added to the AST &amp; semantics, you could even have differently gated syntaxes to access it)
Here: [https://github.com/llvm-mirror/clang](https://github.com/llvm-mirror/clang), give it a go!
The epoll() implementation seems to create and populate the entire epoll fdset, which might work for one or two sockets but doesn't seem like a good way to provide scalable I/O. At least, that's my understanding when I last looked, and based on https://github.com/pocoproject/poco/blob/develop/Net/src/Socket.cpp#L74 it hasn't changed - so that's enough to put me off the project.
if I implement 'full UFCS' , do you reckon they'll accept the pull requests..
Have you tried?
You mean the Primer by Lippman and Lajoie or the one by Prata?
After reading through the accepted parts of the proposals I think we can all agree that C++14 had as many major features as C++17, and that's just sad. The purpose of the Standard Committee isn't to adopt finished specs, its to lead with new and better features. Each compiler project/company will then implement them. So pick some cool features for the standard and let the implementer's start! As a programmer who has been using C++ for over 20 years I would gladly have volunteered work (or got my company to sponsor me) to work on Modules. BTW: Microsoft has a fairly nice implementation of Modules in their C++ compiler. 
"std lib ports and whatnot"? you don't need to use the GC if you don't want to, and even if you do the language generates much less garbage than java - so there's no need to spend millions of dollars as did java. of course I wouldn't dream of suggesting you move to D, because it sounds like you're happy with .net. but other people might be in different situations from you. it's so strange that people see things as a fight to the death, when Knuth himself welcomed the proliferation of languages - since language is an expression of thought, and people think in different ways. https://www.simple-talk.com/opinion/opinion-pieces/don-knuth-and-the-art-of-computer-programming-the-interview/ "I think of a programming language as a tool to convert a programmer's mental images into precise operations that a machine can perform. The main idea is to match the user's intuition as well as possible. There are many kinds of users, and many kinds of application areas, so we need many kinds of languages". eg EMSI containers use Andrei Alexandrescu's allocator and don't depend on the GC. not so tough to do without it. avoiding the ~ (concatenate) operator is hardly the biggest sacrifice in the world. https://github.com/economicmodeling/containers import containers.ttree; import containers.slist; import containers.unrolledlist; import std.experimental.allocator; import std.experimental.allocator.building_blocks.allocator_list; import std.experimental.allocator.building_blocks.region; import std.experimental.allocator.mallocator; import std.datetime; import std.stdio; // For fun: change this number and watch the effect it has on the execution time alias Allocator = AllocatorList!(a =&gt; Region!Mallocator(1024 * 16), Mallocator); enum NUMBER_OF_ITEMS = 500_000; void testEMSIContainer(alias Container, string ContainerName)() { Allocator allocator; auto c = Container!(int, typeof(&amp;allocator))(&amp;allocator); StopWatch sw = StopWatch(AutoStart.yes); foreach (i; 0 .. NUMBER_OF_ITEMS) c.insert(i); sw.stop(); writeln("Inserts for ", ContainerName, " finished in ", sw.peek().to!("msecs", float), " milliseconds."); } void main() { testEMSIContainer!(TTree, "TTree")(); testEMSIContainer!(containers.slist.SList, "EMSI SList")(); testEMSIContainer!(UnrolledList, "UnrolledList")(); }
Lippman and co
I'll stick with GNU make.
Lol fucking hilarious. Implementing iterators is easy. 
We'd have ranges coming in c++17 if the committee didn't have ADHD. 
&gt; We have two modules implementations and one is actually pretty old (~2011). So since 2011 one can write `import mylib.foo` ? I don't think so...
&gt; production-level This is way inferior to the level of correctness required in an ISO standard.
For sockets, use asio for sure. Something very much like asio will eventually be part of the C++ standard.
Modules weren't postponed in any way.
&gt; In any event the cost of the travel should be borne by your company. If you're submitting a proposal while representing a company, sure. If you're doing it on your own time (or if you do work by contract, self-employed, etc), not so much.
Swift is very object-oriented isn't it?
Uhm, weren't they moved to a TS? AFAIK that means that they will be marked as experimental and optional until at least the next standard, no?
I wonder why there are no signals proposals for c++? They're very useful and commonly used and there are tons of different implementations available. Maybe that is a problem - different people needs different things from them, like thread safety, speed, disconnect on desctruction, accumulators and so on? 
Every job has deadlines, the idea that I can just go and ask my employer one week off and $2000 of expenses in order to contribute this tiny thing to the greater good of the language 3 years down the road is just ridicolous. If you're someone that works on tools for sure, but any other programmer that doesn't directly work on C++ won't have that luxury :)
I'm perfectly calm, I just wanted to point that out :)
Rust has a nice RFC process, unrelated to these options.
&gt;storage for invocation result at each stage, to simulate proper pointer dereference That's not simulating anything. Iterators in C++ aren't consumed every time you get a value out. That's really useful, and something I find really annoying in other languages. In other languages, like Python, you have to write lots of annoying wrapper code to get 'peekable' iterators. In C++, all iterators can be `*deferenced` or `++incremented` separately.
Yes I agree. I feel like it's a bit like programs themselves, the ones made by a single person vs the ones made by teams. Though D isn't made by one person anymore, it has not been with a C++ level of scrutiny.
From the article: &gt; C++ variadic templates allow us to have one class or function where we previously had to generate multiple versions for 1 to 6 function parameters, sometimes with additional versions for const and non-const parameters or const and non-const (and volatile and non-volatile) member function pointers.
Loads of us don't want any form of UFCS whatsoever. And frankly, it's better to err on the side of not adding it.
And that is directly discriminating against open source, despite the fact that GCC and Clang are by a GIGANTIC margin the best C++ compilers and that open source has done wonderful wonderful things for C++. C++ would not be where it is today without open source libraries like Boost and open source compilers like G++.
Targeting them for a TS instead of C++17 is a delay in and of itself.
Not everyone uses some shitty IDE, and not everyone wants to be forced to use some shitty IDE. C has worked fine with `f(x)` being the only notation for years.
D is what happens when you listen to people like Alexandrescu.
ADHD?
https://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder
'peekable' iterator can be obtained from 'non-peekable'. And 'non-peekable' is easier to implement. And I'd say that 80% cases require non-peekable version. But that's another long story and another long debate which I wouldn't start here.
I disagree. The vast majority of the time I want to be able to deference an iterator without incrementing it. Every time you use a range-based for loop, you're taking advantage of the fact that iterators in C++ are 'peekable'. 
Nice presentation. I do a lot of CMake right now, mainly while holding my nose. When you get Windows/Linux/Mac support and you/we can start populating the Packages-Repo I will gladly look into moving over to it. 
My goal is to be able to read it and understand what I am reading essentially. 
benefits:- - it plays extremely well with 'dot-autocomplete' which is a godsend for discovery in large sourcebases and complex APIs (unavoidable, software is complex). Put your most significant operand (known from your locals) upfront to narrow the search for the right function. - It keeps functions closer to their operands, (EDIT, sorry typo) - reduces the nesting level, - It makes it easier to write readable function names, with a division between one operand and 'the rest'. e.g. you can have a trailing preposition in the name that clearly refers to the later arguments rather than the preceding one, clarifying how they are used. - the order in which you write is closer to the order in which operations are actually applied. e.g. f(g(x)) calls g, then f. so you could think that and write x.g().f() directly. Nice coherence between what you read &amp; write , and what the machine actually does. The popularity of infix is why Lisp didn't catch on (for it's use cases, it's extremely good, it just has that barrier, so languages like python &amp; javascript took it's place). Allowing the ```a.foo(b)``` syntax for everything gives you flexibility to refactor code to &amp; from vtables or encapsulated member functions (if you want them). demands change over time- you can't predict everything upfront. Having to change syntax as code evolves is a huge waste of time and leads to people going round in circles. Some C++ people have learned to hate ```a.foo(b)``` because of the coupling/dependancy hazards; now they irrationally fear this *syntax*, which is a case of throwing 'out the baby with the bathwater' (the real hazard is putting functions inside the class, the syntax itself is actually superior in every way).
I'm one of the maintainers of C++ Apache Thrift. You're basically out of luck here unless you make changes to Apache Thrift. In order to do this properly, you need a Transport that calls something like 'select' for every read, write, and connect. You need to pass in two socket handles: one for the thing you care about, and one that is only there for the asynchronous close case. Closing the socket handle while it is being used is very bad, and can cause all sorts of terrible things to happen.
Please note that /r/cpp_questions or /r/askprogramming would have been more appropriate. cpp_questions is even linked right on the side bar here. That said, what problem are you having? All I know is that you haven't had "much success." Have you had some success? Are you getting an error message? Don't keep secrets from people who might try to help you.
I think stackoverflow would be the best place for your question. Also I don't have problems with my CMake projects with OpenCV and Qt (not "QT" btw). I think your problem is not the combination of OpenCV and Qt, but you're using both of them in the wrong way with CMake. Try them individually first. You can find excellent tutorials on how to use Qt in a modern CMake way with CMake &gt;= 3.2, and OpenCV is as easy as `find_package` and pointing `OpenCV_DIR` to OpenCVConfig.cmake in your OpenCV install directory (if you're not using Linux with system-installed OpenCV). Also, use the Qt from the Qt-installer from their homepage, not a system-wide installed Qt, the latter one will probably not have the Qt*Config.cmake files.
Thanks for that example.
&gt; Arguments and operands are the same thing. sorry, I corrected that typo, i meant functions &gt; and have your IDE autocomplete that either. thats a possible workaround but not as nice because you no longer have coherence between what you read &amp; write &gt;For a tiny category of functions, there is one special variable and a bunch of others check out the vulkan api for example. ditches all the state fullness of GL and you pass an object to modify. it's a really common pattern. Same applies to GUI frameworks. &gt; Why should there be a special syntax for a.f(b, c, d) but not (a, b).f(c, d) Indeed, tuples would be nice (Rust can do exactly that). In C++ we'd have to write ```make_pair(a,b).f(c,d)``` unfortunately, you'd probably want some helper to make a pair of references i guess. &gt; I would much rather write (f ∘ g)(x) than x.g().f(). All those unnecessary (), yuck. (i) Maybe but we are where we are, there's no compose symbol. '.' is already there for field access. This syntax is hugely popular &amp; familiar; most new languages choose it. C#, Rust, Swift, Go, D. (and all those languages have some sort of extention method mechanism to facilitate using it more pervasively). (ii) now you say it, I'm not sure that makes sense for stateful functions, only pure-functions. interestingly clojure and F# have ways of writing operations sequentially too .. threading macro and ```|&gt;``` . &gt; Lisp didn't catch on because it was slow. Its syntax has nothing to do with it. In the internet age a clear market emerged for 'slow but productive' languages, there are so many now. And they originally considered using something scheme like for the browser, but changed it into what became javascript to make it more readable. All those languages (javascript, python, ruby ) basically re-invented the 'slow but productive' aspects of lisp, just in a more readable form. In terms of the underlying language 'engine', Lisp would have been perfect fit for all those use cases. &gt; Don't be so patronising. I said 'some' rather than you. I've had to deal with this so many times: explaining "NO, I'm not asking for OOP/vtables/classes, just the benefits of this syntax with my free-function code."; so I'm mentioning it preemptively before anyone else replies. &gt; incredible amount of complexity it adds to name resolution the complexity is already there for free functions with ADL &amp; overloading. it's just generalizing the idea that a class has its own namespace. As C++ is so popular there's decent IDE's that are aware of all this and you can use them to help navigate. accurate jump-to-definition. Software is supposed to make life easier, and that should equally apply to the tools we use to write software. The dot-autocomplete is the big one. Look at the complexity of an API like vulkan (which is unavoidable, before you blame the API - exposing all the controls needed to build command lists &amp; manage states efficiently, resulting in massive streamlining of the CPU overhead) - an IDE with dot autocomplete etc is a godsend when dealing with all that. No one can remember the whole API, you spend all your time digging through documentation, and having that on-the-fly argument driven search makes people so much more productive. 
I would recommend Programming Principles and Practice Using C++ by Stroustrup instead. It's much more modern and targets c++11 and 14
&gt;sorry, I corrected that typo, i meant functions Right but again that's not true. f(g(a, b, c), h(d, e, f)) a.g(b, c).f(d.h(e, f)) That order is just insane. It makes no sense. It introduces these `.`s that aren't useful. `.` means member access, and there's no member here. If I see `a.g` I expect to look a `decltype(a)` and see the member `g`. &gt;thats a possible workaround but not as nice because you no longer have coherence between what you read &amp; write Incorrect. It's not a workaround. auto f(int a, int b, int c) { return a + b + c; } int a = 1, b = 2, c = 3; std::cout &lt;&lt; a[tab] At this point, you are presented with the same interface you are presented with if you type `a.[tab]`. It's not hard. It's not a workaround. It's exactly the same thing. &gt;check out the vulkan api for example. ditches all the state fullness of GL and you pass an object to modify. it's a really common pattern. Same applies to GUI frameworks. And sometimes you have that pattern, but a lot of the time you don't have that pattern. Sometimes there are two such arguments. Sometimes there are none. Why do you want this special syntax for that one particular pattern? No other such pattern gets its own special syntax in C++. &gt;Indeed, tuples would be nice (Rust can do exactly that). In C++ we'd have to write make_pair(a,b).f(c,d) unfortunately, you'd probably want some helper to make a pair of references i guess. No, I don't mean tuples. `a.f(b, c)` doesn't involve tuples and I'm not saying that `(a, b).f(c)` should involve tuples. I'm saying that if writing the function name essentially at an arbitrary point along the argument list is useful, why should it be restricted only to the cases where it's either the first thing or the second? More particularly, being able to write the arguments first and the function name last could be useful - `(a, b, c).f()` rather than `f(a, b, c)`. Yet nobody ever proposes the former, but people always propose `a.f(b, c)` even though that's only useful in a couple of situations. &gt;Maybe but we are where we are, there's no compose symbol. 'We are where we are' is not a good argument. Obviously we are where we are. There's no reason we couldn't have a composition symbol - I don't think `@` is in use, for example - `(f @ g)(x)` could be a nice addition to the syntax, certainly far preferable to `x.f().g()`. &gt;this syntax is hugely popular &amp; familiar, most new languages choose it. C++ isn't most languages. The languages you mention have either had it forever, even if forever is a short time (like D and Rust) or they're highly object oriented (like C# and Swift). Go is different - you have to explicitly make something a method and write it differently in order to get `f.g()`. It's more like C++. Yes Rust and D have UFCS but they've had it for their entire life as languages, afaik. C++ has a lot of legacy code that could break with UFCS added and already has **EXTREMELY** complex name resolution rules that are really hard to learn already, and very difficult to implement correctly. Adding more complexity to name resolution in C++ doesn't seem like a good idea for an idea with as little use as this. &gt;now you say it, i'm not sure that makes sense for state-ful functions, only pure-functions. It would just be syntactic sugar for `[] (auto&amp;&amp; ...args) { return f(g(args...)); }`. &gt;In the internet age a clear market emerged for 'slow but productive' languages, there are so many. And they originally considered using something scheme like for the browser, but changed it into what became javascript to make it more readable. Managers decided they wanted something similar to Java because Java was hip and cool (at the time), so they called it 'Javascript' (even though it's nothing like Java semantically) and forced the author to give it Java-like syntax. The language actually has significant semantic similarities to the Lisp family. But not because 'Lisp syntax sucks'. Because Lisp was already long forgotten by mainstream software development. &gt;I said 'some' rather than you. I've had to deal with this so many times, explaining "NO, I'm not asking for OOP/vtables/classes, just the benefits of this syntax with my free-function code." . I'm mentioning it preemptively before anyone else replies. It felt like you implied it. &gt;the complexity is already there for free functions with ADL &amp; overloading. Which is why adding even more complexity is bad. &gt;it's just generalizing the idea that a class has its own namespace. As C++ is so popular there's decent IDE's that are aware of all this and you can use them to help navigate. Given that the main argument by far is that it helps IDEs, claiming that IDEs can already do this doesn't really help your case. &gt;Software is supposed to make life easier, and that should equally apply to the tools we use to write software. I don't think adding more complexity to the language for the purpose of making it very very slightly easier to write IDE autocomplete is worth it. &gt; Look at the complexity of an API like vulkan (which is unavoidable, before you blame the API - exposing all the controls needed to build command lists &amp; manage states efficiently, resulting in massive streamlining of the CPU overhead) - an IDE with dot autocomplete etc is a godsend when dealing with all that. No one can remember the whole API, you spend all your time digging through documentation, and having that on-the-fly argument driven search makes people so much more productive. Again, that has nothing to do with UFCS at all. 
 I think it was more to do with wrapping GTK signals (like `GtkButton::clicked`) in a type-safe way than anything to do with Qt.
Hi, Eric. The thrust of this thread is the perceived (or otherwise) lack of feature production in C++17 due to the shortage of people to work on implementing them. You, being the sole bearer of Ranges, are uniquely positioned to provide some insight into this process. Do you find that much of the C++17 work has been centered around one-man bands, as it were? Have you received outreach from people beyond the standards committee who are interested in helping you with Ranges? Have you heard of others building C++17 getting such gestures? In talking with folks in the other study groups, have you found them to be having similar experiences to your own? I could only imagine how busy you are, so I won't pressure you for a response, but it would be great to hear from someone on the inside. Thanks for all of your efforts!
&gt; I don't think adding more complexity to the language for the purpose of making it very very slightly easier to write IDE autocomplete is worth it. [1] complexity in the language simplifies use. [2] not to simplify writing IDE autocomplete - it's to simplify reading &amp; writing the code. coherence between how you write it , and how you read it. we already have this autocomplete, it's just to use it we have to stuff everything in the damn class (and that already happens, because this kind of autocomplete is SO useful) &gt; And sometimes you have that pattern, but a lot of the time you don't have that pattern. you have this pattern enough that it gained separate syntax. OOP is very popular, but produces the coupling hazard. Extention methods and UFCS fix that, giving you the best of both worlds &gt; ```f(g(a, b, c), h(d, e, f))``` &gt; ```a.g(b, c).f(d.h(e, f))``` yes, even this demonstrates what I meant. 'f' is applied to the result of 'g', and 'h'. being in the middle, you've now reduced the distance to it's arguments. your example is a little hard to read but in practice those function names would be verbs standing our from noun argument names it's quite similar to this:- ```a*b+c*d``` ```add(mul(a,b),mul(c,d))``` ```mul(a,b).add(mul(c,d))``` or even ```a.mul(b).add(c.mul(d))``` .. much closer to the way you'd write it with operators. ``` a * b + c * d ``` (obviously for 'add' and 'mul' we already have operators, but in so much other maths code , we don't. ) &gt; Sometimes there are two such arguments. Sometimes there are none. horses for courses. I'm not suggesting the f(a,b,c) format should be banned or anything. Usually there is one 'most significant' operand. a buffer.. a collection class.. an entity.. a window.. whatever. Many people like to use the receiver for 'the thing that get's modified', if there are side effects. (and of course you could clarify with the function name when it isn't used like that). &gt;&gt; It's not hard. It's not a workaround. It's exactly the same thing. I'm not saying it's a bad idea, just that it's no where near as good as having complete coherence between read &amp; write. smoother IO between source &amp; your brain (and other peoples later). You're going to have the expression you type being re-arranged on the fly, it's going to be messy. &gt; Again, that has nothing to do with UFCS at all. It's an example of the kind of case where dot-autocomplete is useful ; a case where the low level control of C++ is important but nonetheless you have a huge, fairly OOP-y styled interface. (again in these arguments I have to constantly deal with "go use Java!!" responses) There's loads of APIs where functions have the form SomeType_MethodName(SomeType*, args..) for C FFI. &gt; Given that the main argument by far is that it helps IDEs, claiming that IDEs can already do this doesn't really help your case. eh? I'm saying IDE's can tame the complexity, I don't get the criticism here. you're complaining that it increases the complexity. Ultimately it's to help *the user*. &gt;&gt; why should it be restricted only to the cases where it's either the first thing or the second? I've brought up rust, because the ability to tuple-up a load of arguments does actually seem nice. we don't propose it for C++ because you can't retrofit that kind of tuple formation to C++ syntax (unfortunately). &gt; C++ has a lot of legacy code that could break with UFCS added I would actually be happy if UFCS-able functions required that you write an explicitely named ```this``` pointer argument as an opt-in, then there'd be no surprises. (and this would help even more with refactoring code to &amp; from member-functions.. taking existing kitchen sink classes and breaking them up, or absorbing useful additions). &gt;&gt; " Because Lisp was already long forgotten by mainstream software development." er, when the guy was writing javascript, he originally wanted to make something scheme-like; he clearly hadn't forgotten. And many other languages clearly had features directly inspired. Lisp was remembered, still taught, just hated by most because of the syntax. &gt; . means member access, indeed refactoring from member access to accessor functions is another hugely common case. &gt; (f @ g)(x) could be a nice addition to the syntax, certainly far preferable to x.f().g(). why not just write yourself a little ```#define COMPOSE(a,b) [] (auto&amp;&amp; ...args) { return f(g(args...)); }``` seeing as you like prefix :) but honestly I don't see the appeal when you've got chaining already..
&gt; complexity in the language simplifies use. I disagree. Here it makes it harder to tell where something is defined. &gt;you have this pattern enough that it gained separate syntax. OOP is very popular, but produces the coupling hazard. I don't think it's enough to be worth the huge additional cost. &gt;yes, even this demonstrates what I meant. 'f' is applied to the result of 'g', and 'h'. being in the middle, it is now closer to all it's arguments Yet it's far far harder to understand and separates the arguments to before and after `f` even though there's no reason to do this. &gt;a*b+c*d add(mul(a,b),mul(c,d)) mul(a,b).add(mul(c,d)) or even a.mul(b).add(c.mul(d)) .. much closer to the way you'd write it with operators. And yet oddly asymmetrical. Why `.add(`? A dot to the left of the operator, an open bracket to the right... why? Why is that the syntax? Those are fundamentally symmetrical operations. &gt;Usually there is one 'most significant' operand. a buffer.. a collection class.. an entity.. a window.. whatever. Many people like to use the receiver for 'the thing that get's modified', if there are side effects. (and of course you could clarify with the function name when it isn't used like that). Yet often there are two or none or many. &gt;I'm not saying it's a bad idea, just that it's no where near as good as having complete coherence between read &amp; write. smoother IO between source &amp; your brain (and other peoples later). You're going to have the expression you type being re-arranged on the fly, it's going to be messy. You don't lose any 'coherence' whatsoever. I really have no idea why you think there would be. Nothing has to be rearranged. Either way you're writing the first argument and pressing tab. &gt;we don't propose it for C++ because you can't retrofit that kind of tuple formation to C++ syntax (unfortunately). sure you can. `{a, b, c}.f()`. &gt;I would actually be happy if UFCS-able functions required that you write an explicitely named this pointer argument, then there'd be no surprises. (this would help even more with refactoring code to &amp; from member-functions) If we could deprecate the `this` pointer and replace it with a `self` reference that would also be wonderful. &gt;er, when the guy was writing javascript, he originally wanted to make something scheme-like; he clearly hadn't forgotten. and many other languages clearly had features directly inspired. Lisp was remembered, still taught, just hated by most because of the syntax. Lisp was not hated at all for its syntax. That's a myth. &gt;indeed refactoring from member access to accessor functions is another hugely common case. I disagree entirely. I've literally never done this in any language nor code reviewed any code that's ever done this. &gt;why not just write yourself a little #define COMPOSE(a,b) [] (auto&amp;&amp; ...args) { return f(g(args...)); } seeing as you like prefix :) but honestly I don't see the appeal when you've got chaining already.. Because `@` is a single character and macros are disgusting. I love infix operators. Stupid and weird asymmetry like `a.foo(b)` is what I don't like. 
&gt; Why .add(? A dot to the left of the operator, an open bracket to the right... why? Why is that the syntax? as another option, objective C has its [a foo b] syntax, which you might say is more 'symmetrical', but it's harder to type, as soon as you pass results along. the a.foo(b) just 'writes' so easily in most situations. ```[[a foo b] bar c] ``` ```a.foo(c).bar(c)``` ```[[[a foo c] bar d] baz e] ``` ```a.foo(c).bar(d).baz(e)``` ... much nicer. 
Well you can do this for functions, but not classes like tuple.
CMake, Meson, build2, Shimo, Premake, autotools, Conan, BiiCode, Nugget, SCons, Waf... this looks like ISO C++ committee lately :D
Yes, it naturally supports all the things MSVC does not (yet) like expression SFINAE and proper two-phase lookup. Remember that while clang as a frontend is pretty well tested and feature complete, the combination is still highly experimental, so dont trust it with production code. But I had no problems compiling my modern C++ libraries with it.
GNU `make` works well for simple things. In fact, from all the other build systems out there, I actually think `make` is the sanest of them all.
Casey Carter has been working on Ranges. He also worked with Beman Dawes and Eric implementing the iterator facade proposal.
And yet, the syntax proposed appears to be perfectly fine in C# and is used by millions of people, none of whom seem to be complaining about having to test other people's extension methods. I think you are imagining a problem in a place where there isn't any problem.
As I understand, the API has very few changes, so it's not a problem to tune gtkmm for current new API. How compile time was increased? How hard to debug the library with so extensive usage of c++14 features, compared to old one? If we abstract from: --&gt; "Over the years, we have built up quite a large set of --&gt; regression tests"
system-wide installed Qt has cmake files.
Thanks. After reading this proposal I see that it is not final and need more work. Well TS with Reflection would be nice to have soon. 
You can also use this for VS 2013/2015. [Windows snapshot builds](http://llvm.org/builds/)
Creator here. I put up this website to make it easy to keep Doxygen documentation up to date. Similar to other GitHub integrations (Coverity, Travis-CI), it will rerun Doxygen every time you push changes to GitHub. The generated docs are then nicely hosted on the website. Consider this an early prototype, but I would like to get feedback. It currently only works with your public repositories and organizations. As an example, the official Doxygen code on GitHub: https://github.com/doxygen/doxygen Has started to use CodeDocs for its Doxygen documentation: https://codedocs.xyz/doxygen/doxygen/
What is the latest version of C++ supported by Clang 3.8? The linked page doesn't seem to say.
Currently no. Is there a particular permission you don't like, or all of them? I could add something to submit a new repo on the website, but you would have to add the GitHub hook yourself for it to automatically update. Let me look into this.
Write/Admin access are things I don't feel to good about. No offense. How about a downloadable zip post scan or whatever?
&gt; It writes easily because you're used to it. I don't use every C++ feature. e.g. I don't use multiple inheritance, so I'm not used to that. I'm used to this because it's *useful*. This is why this syntax has been picked up and copied by just about every mainstream language developed since. It's ubiquitous now. the attempt to make something more symmetrical (objC's brackets) was tried by the world and discarded; infact swift was motivated by them wanting to get *rid* of that 'strange' syntax.
&gt; It's just so obvious! With that, we could extend classes without modifying them. Even builtin types! Absolutely. but just read comments by 'holomorphological', and reply to him aswell. keep fighting the battle. It seems enough people out there don't get it, and as such they argue against it, and the committee supports their view. &gt; I thought about this feature before I've heard of it being considered by other people. Same. I've known of this problem for so long. I've seen entire sourcebases thrown away and re-written over these kind of stylistic changes.. it's so frustrating, such a waste of time. 
It needs to update your hooks so that github.com can notify CodeDocs every time you push. Otherwise CodeDocs won't know when to scan.
Here's another nice example, interpolating between 2 points.. ``` (x-x0)*(y1-y0)/(x1-x0) +y0``` compare: ``` x.sub(x0).mul(y1.sub(y0)).div(x1.sub(x0)).add(y0); ``` with ``` add(div(mul(sub(x,x0),sub(y1,y0)), sub(x1,x0)),y0 ); ``` thats 's much harder to write with prefix &amp; to see what it will do. *where the hell is the denominator in the divide?* *how many parameters does the first function take?* (if I'd had exposure to lisp I might have wanted overloaded n-ary add..). When you see the 'dot' , you know everything to the left has been evaluated. Did I get the brackets right? Whilst writing that, you have to go back and forward with your eyes &amp; arrow keys much more. Now lets write some helper functions.. ```float lerp(float f, float y0, float y1){return y0+(y1-y0)*f;}``` ```//move by a fraction 'f' between y0,y1``` ```float inv_lerp(float x, float x0, float x1){return (x-x0)/(x1-x0);}``` ```//what fraction is x between x0,x1``` **now we can write:** (if we had UFCS, or if we rolled a whole damn wrapper class for a float) ```x.inv_lerp(x0,x1).lerp(y0,y1)``` That does the same as the original formula, composed from 2 common, simple helper functions, each individually useful in other contexts. Look how nicely it flows as a pipeline: 'input =&gt; get it's fraction between the x's =&gt; move that far between the y's' ^ For the full case of lerping between 2 points, thats arguably clearer than a single call, ```lerp_between_points(x,x0,x1,y0,y1)``` because you know the grouping of x's &amp; y's ; when you see *that* called, is it ```(x,x0,x1,y0,y0)``` or ```(x,x0,y0,x1,y1)```, or ```(x0,y0,x1,y1,x)```, or what? I guess a haskeller would have wanted to put the 'x' at the end, as it would be more useful for currying. Someone might also have overloaded 'lerp' for 2 different functions , the 3 and 5 operand versions. (the 3 operand version is common in shader code, but some people think more of linear interpolation as the 5 operand form) Of course you could clarify it further by making point and line objects and so on, but it's nice to see how much you can do with functions alone, and thats' more verbose to setup. (I like currying but I wouldn't propose retrofitting it to C++ because it would be hell with overloads) 
C++ Primer also targets C++11 (not sure how much 14 though).
Yes, things are admittedly sparse on details, sorry. But I will try to answer your questions here. To use CodeDocs, once you sign in you will see a list of your public repos. Just toggle the repos you want documentation generated for, click 'Apply' and that is pretty much it. No Terms of Service at the moment, I should put something up, thanks. No usage limits currently, and no intention to add any for public repositories.
Universal memoization is a great practical example too. Check out the SO answer [here](http://stackoverflow.com/q/17805969).
You can use pacman -S mingw-w64-x86_64-clang pacman -S mingw-w64-i686-clang in the mingw shell to install clang. The first line is for 64bit, the second one for 32bit. As the name suggests, MinGW is meant to be a minimal GNU environment on Windows.. So you can just download all the things you need. Alternatively, there is also a package built directly from SVN: pacman -S mingw-w64-x86_64-clang-svn Note that this only applies to **mingw-w64/MSYS2**, not mingw32. These two are different projects.
Yea, highly experimental :) I am happy that I didn't encounter those kind of failures though. But I can imagine that there are at least some things that just don't work as of yet.
good features will reduce the amount of abuse needed. With the right features, the 'right' way to do things is more obvious, and it's easier to refactor code to correct mistakes. (I count UFCS as such a feature, it'll make refining libraries easier)
I see it that they're more like free functions, but it can then optionally collect them into a table of function pointers when you pass abstract types around ( which seems rather nice to me ).
i wish github had the ability to vote on posts.
I think OP meant that they were a PITA to get right. it probably involved a lot of tears and blood
On github and docker, everything has a namespace in front. So to access Hana, its boostorg/hana. This is important, because if I push a zlib package and then someone else push the another zlib package, who gets priority? On pypi, its a first-come basis, which can be problematic if someone makes it there before the official package. Also, supporting namespaces, can better help support forks of the same package as well. I hope that makes sense.
Nice work! Can CodeDocs generate document not only for master branch but also other branches? I think it would be a useful feature when the project would like to maintain multiple API document pages for multiple versions or to provide API document for the stable version.
Business-targeted integrations generally let you set up the hook manually to avoid having to grant excess access.
Currently, it just works on one branch. The branch you set on GitHub as the [default branch](https://help.github.com/articles/setting-the-default-branch/), which is usually 'master'. Being able to support other branches and keeping multiple versions is a good idea, and something I would like to add. Stay tuned.
Could you please link to an example of documentation that was generated using this tool?
So... How does it work? I am using doxygen already but the only thing I see there is a big "Sign up with github" button and I guess I would like to know a bit more. Does it just run doxygen on all/some/user specified part of my repo given a path to a Doxyfile.in? 
&gt;&gt; Again, that's because people are used to it from Java. There we go. I knew soon enough I'd hear Java. I've never actually used java; just BBC BASIC, 6502 assembly, 68000 assembly, C , x86 assembly, MIPS assembly, C++, (then tabled with a few other languages in passing but not settling on anything), and Rust. Most of my total time has been spent in C and C++. I'm a low level person (console gamedev), not an OOP person, and I find the syntax incredibly useful. Maybe because I like to think in sequences of operations, like assembly, working on some 'current value' in registers. And when things are working right there is indeed coherence between whats efficient in hardware, and software, and representation. e.g. imagine chaining sequences of multiply-accumulate instructions. The specific syntax seems perfectly natural for me because I've seen it in C++ for ~20 years. it's useful for the reasons I've been going over in this thread, but comes with the stupid hazard of having to put things into the class. Nonetheless it's *SO* useful that most programmers, myself included, have this perpetual draw to go ahead and put things into the class. Classes that have all the right functions (for you) in them are much more pleasant to use; so programmers constantly fight over what does and doesn't go in, and headers get bloated, dependancies explode. All we need is UFCS, and we get the best of both worlds: the superior syntax, with superior decoupling. R.e. **asymmetry**: as we've seen only a few functions are actually symmetrical (commutative). Most of the time, the operand order matters - and the asymmetry is actually useful to highlight the difference. 'less than'. 'divide by'. 'subtract'. 'transform' . 'interpolate between' etc. Note that matrix multiplication is not commutative, neither is cross product. Even in the commutative case: you can read it as taking a value, and doing something to it, then passing it on to the next stage. (or if it really offends you , go ahead and stick with prefix for that). Pipelines of operations are incredibly common. As such functional languages somtimes have something specific to express that ( |&gt; , threading macro, whatever). C++ member functions can do it, and Rust shows rather nice libraries built around the chaining behaviour. We seem to suffer painful momentum with C++. It took so long to get 'auto' (we had to suffer people rationalising the need to write out long iterator type signatures), and lambda functions (again people insisting that rolling objects to do things was enough). It's vector maths libraries in particular I'm thinking of where you want all sorts of helper functions like 'perpendicularize' 'look at' etc; the chaining is SO useful, but then incredible scope for disagreement over what should and shouldn't go in (all the permutes etc that you might want in vector code, do you want to make separate classes for colour values or just use 4 element vectors, etc etc). Other newer languages demonstrate decent solutions to this. we don't need a new 'composition' or a 'threading operator' for it; Rust (and C++ sourcebases where one has control over the classes) show how the existing syntax can be used perfectly well for 'pipeline style' code. It's 2016, can we just get this feature into C++ and move on to the next desperately needed fix. As I say, I'm perfectly happy for it to require an explicit 'this pointer' opt-in, so it wouldn't affect existing source bases at all. I'd even be happy with a completely separate way of writing a 'class extention' (although i'd prefer free functions), but the proposals allow it with the smallest change to the language. I know of the whole other world of functional language syntax with currying, composing and so on (the fact you're actually suggesting a 'compose operator' over chaining makes me suspect that's your preference?) but I don't think it retrofits as well into where C++ is now (i.e. overloads/default parameters break currying.). You'd be better off making a whole new language for that. (I think there's ATS? which is ML style syntax but in a systems language? but it's not for me. I'm happier with C++ overall). UFCS takes the syntax and features we currently have in C++ to their logical conclusion. I see C++ somewhere in the middle (like Rust) between functional and (java-esque)OOP extremes, and that's exactly where I like to be.
&gt; Anyone have an idea of how slow this would be? That you even ask that question means you know very little about what your product is doing. What are the access patterns to the data that you hitherto stored in RAM? For all I know, you could use a tape drive instead - it all depends on the access patterns. &gt; I'm just looking for some anecdotes to see if this has been tried Anyone admitting to this would be admitting to having a rather poor system design to start with... This question cannot be answered without knowing specifics about the data, its meaning, and what algorithms are using it.
I'm using CMake-Gui to cmake the opencv binaries (unclear why the tutorials do this) and received an error. I'm not even sure if I need to use CMake to do this, it's just what all the tutorials say to do. Why can't I just move all of the *.h files for OpenCV into a Qt directory and include them? That doesn't seem to work, though.
&gt; and last but not least, pay $200 to get a standard copy Goes to ISO for organizing the process, not to the actual developers working on it...
&gt; [Alignment](http://llvm.org/releases/3.8.0/tools/clang/docs/ReleaseNotes.html#alignment) Or, in C++11 and upward, `std::aligned_storage&lt;sizeof(char const*), alignof(char const*)&gt;::type`. No need for alignment attributes :)
My repos require a `cmake` run to generate the `Doxyfile` with all the needed static and generated paths in the source and build trees, and provide a custom `docs` target which makes some source generation happen before `doxygen` runs (or else it will not scan the full set of sources). Can this support a custom way to invoke `doxygen`? Also, I have all my interdependent projects use tag files for cross-references. Again, I use `cmake` variables to allow the installed location for needed tag files to be specified and set these in the generated `Doxyfile`. Is there any provision for tag files?
Yeah, your objects get sliced without even a compile warning, this: struct A { int v1 = -1; }; struct B : A { int v2 = -1; }; B bs[2]; gsl::span&lt;B&gt; bspan( bs ); gsl::span&lt;A&gt; aspan( bspan ); std::cout &lt;&lt; bspan.size_bytes() &lt;&lt; std::endl; std::cout &lt;&lt; aspan.size_bytes() &lt;&lt; std::endl; output: 16 8 edit: fix typo
 gsl::span&lt;A&gt; aspan( aspan ); Is "aspan( aspan )?" a typo?
For a language with a similar feature, see Rust and its [slice](https://doc.rust-lang.org/std/primitive.slice.html) type. This `gsl::span` appears to add compile-time constant lengths and multiple dimensions on top of it.
Ok, cool! I remember faintly that last time I tried it didn't work and/or I didn't find them. Any way, the ones in Ubuntu are usually horribly old so using the ones from qt.io is the better choice anyway.
Yeah, this one is also good for beginner. I forgot to mention it :( I think it depends on whether you have experiences on programming. If you know nothing about programming, than Programming Principles and Practice Using C++ would be good (less details, many examples). If you have experiences in other language, than I would recommend C++ Primer. After all, both are beginners to C++ in above cases.
Given the differences, is it that big a deal for an introduction to C++? (OK, make_unique could be nice)
I used them as a syscall wrapper. For example, instead of writing C-style code like this all the time: if (read(fd, buf, 200) &lt; 0) signal_error(); I can now write code like syscall(read, fd, buf, 200) and the wrapper will throw an exception if the syscall fails.
Thanks for clarifying this. I feel that allowing multiple competing packages is both duplication of effort on the packager's part and extra confusion on the user's part (so now I have two hana packages, both say they are the best, which one do I choose)? So I think we will first try to make the one package approach work. But if that doesn't work out, we can always consider introducing some kind of namespaces.
Isn't this consistent with container types? std::vector, std::array and C-style arrays will all slice subtypes. 
You can have a look at our [SDL2.cmake](https://bitbucket.org/tido/rainbow/src/4eab15f26e0f439330ca95bc99bd8e5698844f0b/build/cmake/SDL2.cmake?at=master). We use CMake to generate project files (e.g. VS, Xcode, Makefiles, Ninja…). What we're doing is download source and binaries from libsdl.org and put it in a predefined path, and include that path in our project. Currently, we use CMake to download source/binaries, and a separate script to clone the repo on Linux/Mac. But recent CMake can clone repos as well, so there is no need to separate it like we did. Edit: Forgot to mention that the `download_library` macro is defined in [Utilities.cmake](https://bitbucket.org/tido/rainbow/src/4eab15f26e0f439330ca95bc99bd8e5698844f0b/build/cmake/Utilities.cmake?at=master).
That's what I was thinking.
ah, yeah, fixed
So far this seems all "magic", can I set compiler flags? Defines? Default build configurations? This is a thing I see missing from many build systems, I ask for control, I get magic :(
&gt;unfortunately no IDE with dot-autocomplete Then write one. Be a productive member of the community. &gt;and the safety is overkill. No it isn't. The safety simply encodes in the type system exactly what you should be doing already. &gt;I genuinely think C++'s ad-hoc overloading is better for 3d maths code. I actually do agree with this - I like C++'s "dynamically typed templates". 
No, all of the source needs to be in the repository. Generating source through other tools like cmake isn't supported. BTW, I also use cmake to generate my doxyfile in many of my projects, but have been able to find decent static settings. Tags aren't supported, but it has been requested before. I'll look into it.
I can add the LAYOUT_FILE option. Please suggest any other options you need, and I'll add them. BTW, if you put a file named DoxygenLayout.xml in the root of the repository, it might work without the option. I haven't tested this, but it should work.
Well there is still chance that [constexpr if](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0128r1.html) will be included into C++17, that would be great. Now the question is what happens with [Operator Dot](http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2014/n4173.pdf) ? Was it rejected or is there still chance that it will be in C++17 ? Now lets wait for "SG14 - Games &amp; Low Latency" ... 
constexpr if: its forwarded to Oulu, its likely to go in from todays view. Not sure with operator dot, probably have to wait for the post meeting mailing, the unified call syntax was rejected. Also, with two more meetings this year, C++17 isn't finished yet. Maybe we'll get a few surprises.
&gt; I set compiler flags? Defines? Default build configurations? There are actually no defaults ;-). Whatever you pass with `config.*` values (or set in your `buildfile`'s) is what you will get. And you can see the actual command lines being executed by simply adding the `-v` option. And if you want to see absolutely every command line, including, say `-M*` runs (header dependency extraction), then pass `--verbose 3`.
My knee-jerk reaction to this is to disallow it. Slicing is insidious enough as it is and directly supporting/promoting it with GSL types (that are intended to promote best practices) seems like the wrong direction to go. I could be convinced otherwise but it would have to be a compelling argument... 
You're all over the place here, I'll try to clarify. LLVM offers windows installers of the releases [here](http://llvm.org/releases/download.html#3.8.0), and SVN snapshot builds [here](http://llvm.org/builds/). These contain clang and other LLVM tools, plus some LLVM headers and libs. They also include clang-cl, which is a compiler driver for clang that aims to be compatible with cl's command line flags. This is the part that integrates with Visual Studio. It will still use Microsoft's stdlib and linker, so you need to install the C++ tools (the stdlib, compiler, linker and other tools are all one package). In the VS2015 installer, you can find them under Languages &gt; Visual C++ &gt; Common Tools for Visual C++ 2015. Note that these releases of clang try to generate binaries that are compatible with ones built using Microsoft's toolchain. MinGW-W64/MSYS2 on the other hand provides complete GCC and LLVM/Clang toolchains (and others). By default Clang will use GCC's libstdc++ here, but libc++ is also available. These toolchains do not attempt binary compatibility with cl, and they do not integrate with Visual Studio (directly - you could use makefile projects and GDB extensions, though). Then there's also Microsoft's Clang/C2, which uses Clang as a front-end and Microsoft's codegen underneath. Like clang-cl, this also uses Microsoft's stdlib and linker and gives you binary compatibility with cl. Instead of translating cl's command line flags, this exposes most equivalent clang flags in Visual Studio's project properties GUI.
About reflection, what happened to [N4428](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4428.pdf)? It seems to be the most reasonable proposal to me.
Is std::optional there? Thanks.
Wow - that's both crazy and saddening. This fact makes me appreciate the web. Were my only learning resource a brick-and-mortar library, I could have easily gone down that path as well.
There was an evening session about reflection talking about the three proposals. There was lots of discussion and bunches of votes and P0194r0 seemed to be the most "directionally favorable" for reflection. [1] http://open-std.org/JTC1/SC22/WG21/docs/papers/2016/p0194r0.pdf