Hi. Yes it is! Splitting it would require other compilers to support the same tech - we rely on some compiler extensions, like __property for example for properties that read/write to methods or fields. The closest is MS's properties but they require only public methods, ie there's no encapsulation provided by properties. Plus we tie in several languages - we think you use the right language for the job, and (sorry) for UI controls C++ isn't always it. So we expose other language's frameworks or styles to the C++ language using those extensions. It's like our COM support - it uses __interface to give a native interface with GUID primitive that a class can implement. You can't do that without compiler support. So yes, it's closely tied to the compiler, that's how it gets its features, so unlikely to be a separate product. RTL: currently we support that in native controls but not the non-native.
I see you're a wxWidgets developer, so it's a bit awkward for me to argue that stuff with you, but here goes. Most of what you said is either "it got better" or "it's rarely used". I don't doubt it. But I used it from 2011 to 2013 and the experience was rather unpleasant. I found Qt to be much better. * Event tables were ([and still are](http://docs.wxwidgets.org/trunk/overview_events.html)) prominently featured in the doc: "There are two principal ways to handle events in wxWidgets. One of them uses event table macros [...] the other one uses wxEvtHandler::Bind&lt;&gt;()". Saying that they are "obsolete" is misleading at best.^1 * `wxGridBagSizer` was the best thing I found for complex, table-like layouts. I'm not sure why you're dismissing this as being "marginal". * For graphics, I think I was using `wxRendererNative`, which requires a `wxDC`. * As for `wxTheBrushList`, the [doc still says](http://docs.wxwidgets.org/trunk/classwx_brush.html): "Therefore an application may wish to get a pointer to a brush by using the global list of brushes `wxTheBrushList`". Not sure why you're saying it's not supposed to be used. It was also mostly an attempt at humour, it's not a real problem. &gt; wxWidgets does the best it can do, but it can't work miracles I understand that. It's part of what I learned. The important lesson for me wasn't "wxWidgets is bad" (which is a very personal thing), it's "research before use because you can get screwed". ^1 edit: Looks like this is a documentation issue. It goes on to say "this absolutely doesn't mean that using the event tables is the preferred way". Sounds like the paragraph was added at one point, but that nothing else around it was changed. May I suggest a rewrite of that part to put more emphasis on `Bind()` and relegating the event table stuff to the bottom?
`std::move` implies a "moved-from" state, regardless of how it is defined or whether it is a truly unique state. To do anything other than assign or destruct a "moved-from" object is UB. Do you check for all forms of potential UB at every entry point into your library? Why not? Why should this one be special? If you want a language that cannot be used incorrectly, C++ is not the language for you. Good library design implies the APIs are _awkward_ to use incorrectly, not _impossible_. `std::move` is awkward enough and should be a loud warning bell for any knowledgeable developer. *EDIT: I used the term 'UB' too loosely above -- what I was thinking was if your library API has specific preconditions on the object, then the caller cannot assume those preconditions hold for a moved-from object -- therefore, calling that API effectively invokes undefined behavior per your library API contract.*
Does it still have shit ton of bugs?
Will std::any make it as well? To complete the holy optional/variant/any trinity?
https://woboq.com/blog/verdigris-qt-without-moc.html
I tried lemon graph once. In my test it was both easier to use, for some parts more flexible and also faster than bgl. I looked at their code and did not like some parts, since not all class members were initialised iirc.
&gt; IIRC WPF was originally meant to completely replace native controls YARNC it is impossible to replace native controls, thousands of apps depend on them and MS is pretty anal about backward compatibility
https://woboq.com/blog/verdigris-qt-without-moc.html
I used it for my image manager project. Candidates were GTK, wxWidgets, QT and fltk. After spending a week with each, decided to use fltk. As you said its lean and mean. It also is the easiest to learn - at least for me it was. 
No promises at this time, but it's currently being implemented.
Oh my. I guess on tough days at work I can look at a post like this and realize things could be a lot worse!
Interesting indeed. But the ugly macros is what kills it, but I suppose if you can't generate the mocs for whatever reason then is a viable alternative.
I don't think that's fair. QtCreator is a tool that many people use every day -- the people on the project have done a great job IMHO.
&gt; Do you check for all forms of potential UB at every entry point into your library? Why not? Why should this one be special? No, of course not. Each class has a documented contract about how it can be used. Suppose I am passed a `std::unique_ptr`. I _never_ check to see that the pointer it contains is somehow corrupt, even though that is a potential form of UB - because if that happens, the contract of `std::unique_ptr` is already broken. But I _do_ check (once, at "the top") to see that that `std::unique_ptr` is not `nullptr` - even if I "know" that this "should" never happen. The reason is that the contract for `std::unique_ptr` allows nullability. Even though today I might "know" that this pointer "is never" null, perhaps tomorrow that isn't so. In the case of your library, `my_socket` now has an additional _legal_ state - "empty" - which you didn't plan for. Remember, `my_socket` is in a perfectly good state, one that completely fulfills its new contract, but you cause undefined behavior, because you are only aware of the old, now incomplete contract. Your library causes undefined behavior when passed an object in a good state. It is broken. &gt; If you want a language that cannot be used incorrectly, C++ is not the language for you. [...] std::move is awkward enough and should be a loud warning bell for any knowledgeable developer. You should be a little more polite. It's hard to see this as respectful of me. And as I also pointed out before, `std::move` is definitely not the only thing that causes a move to happen. I saw someone's bad code effect a move like this: `(x).method()`, where `x` was a variable name. They put the `()` because "it didn't work" before they did it, because `method()` was an rvalue method... :-D EDIT: yes, there were other traps in that class. I mainly put that in for entertainment. :-D 
How exactly is using deleted pointer in this regard so much different than using object after move, where the internal state is unspecified? Also, If I close the socket and then pass it around as valid, it can hardly be blamed on whoever implemented the close() method. But if I move from socket and get old instance in unspecified state and then pass it around, it's now somehow fault of whoever implemented the move semantics?
&gt; If I close the socket Actually, I just notice that you close it in destructor. Nevermind then. 
As with GDI and GDI+, I think the plan was to support legacy applications by emulating this part of Win32, but using WPF as a backend. In other words, existing applications use their conventional Win32 API but behind the scenes this gets dispatched to WPF, GDI+ and DirectX.
&gt; I'd definitely recommend using wxDataViewCtrl Looking back at my old bug reports, I seem to have decided against `wxDataViewCtrl` because it wasn't native (negating the main advantage of wxWidgets) and had at least [one showstopper bug](http://trac.wxwidgets.org/ticket/14479). That particular bug was fixed and things have probably changed since then. Thanks for the conversation, though. It was nice to meet you. Whatever my thoughts on wxWidgets, thank you for spending your own time on an open-source project. We all appreciate it.
Sure they could - at the cost of huge complexity and changing the language. You'd need yet another sort of type - "dlvalue" say, for "destroyed lvalue" - you'd then need to have the possibility of lvalue, rvalue _and_ dlvalue methods and parameters - and you'd need three different constructors and three different assignment operators! So there would be two overloads of this method: bool concurrent_queue&lt;T&gt;::try_enqueue(T&amp;&amp; value); bool concurrent_queue&lt;T&gt;::try_enqueue(T$&amp; value); where `$&amp;` designates is the mythical "destroyed lvalue argument". I think we're in agreement basically - it's a bad idea. Just because I can see some horrible path to accomplishing that bad idea doesn't make it good. :-D
&gt; "Unspecified" means something very different from "undefined" I don't disagree. However doing operations with possible preconditions on objects in unspecified state without checking first is undefined behavior. &gt; You have this idea that moved out of objects are somehow burned and dangerous. That's definitely not true. I just don't assume to be able to do anything with the object after move unless I can tell which methods have preconditions and which don't. For smart pointer it is quite obvious. For sockets? Not so much. I would not be surprised if most methods on moved-from socket threw an exception. And yes, the original example by adding the move constructor does introduce new precondition. My issue here is just that the blame goes to whoever introduces the move constructor instead to whoever *explicitely* gets the socket in unspecified state and passes it around to methods that expect certain state.
Yeah. If you didn't go out of your way to use std::move, that would work just fine. By unnecessarily forcing the std::move, you're manually introducing a bug.
&gt; I don't disagree. However doing operations with possible preconditions on objects in unspecified state without checking first is undefined behavior. Absolutely. We are in complete agreement here. &gt; My issue here is just that the blame goes to whoever introduces the move constructor We agree on that too! The person who introduced the move constructor rendered the library broken. To fix the now-broken library, the library writer has to now do extra work. That's the point of the article! So what is it that we disagree on? :-)
But adding move constructor is not breaking change. You can add it to your library and no existing code will break. None whatsoever. It will only break after someone puts socket in new state that you have introduced. And there's nothing wrong with that. If I took new version of your library and recompiled and somehow obtained socket in unspecified state, that would be bad. That would be breaking change indeed. But if I take new version of your library, deliberately move from socket and then pass it around as if nothing happened, that's not a breaking change, that's just me violating (implicit perhaps) contract of other methods requiring valid socket instance. The huge difference here is that it required explicit action from me, and it really doesn't get much more explicit than having to do std::move.
Yes, that person who writes such code fucked up. But before it wasn't possible for him to fuck up, the type system prevented that. Thus move semantics weakened something.
This was just a really short example, in reality it would happen accidentally by complex control flow.
&gt; All I have to say is that all relevant library APIs have a precondition that the objects involved must not be moved-from objects. And previously this was implictly true for all objects, giving every member a *wide* contract because the precondition was *always* fulfilled. With move this isn't anymore. That's what I want to say.
If you look at the json header, how much of it do you think this would apply to? Would you really want to inline entire json parsing loop every time you invoke it? Also, link time optimization may be slow, but you don't usually need to do it for debug builds so it's much less of a factor.
It's pretty sad that the instructions for building on Windows are so out of date that when I found another site that described how to do it, the first step was "Ignore the instructions on openssl.org because they are wrong." 
&gt; Just because you potentially gave someone a rope and he hangs himself with it doesn't make all ropes bad :) But before move constructurs were introduced, there was no rope. Someone should only shoot himself in the foot by using language level UB like use-after-free, but he couldn't hang himself using a moved-from object.
I used Roguewave's C++ database library back in the 90s and thought it was awful. I found the proliferation of classes and excessive and non-intuitive use of overriding operators to be ridiculous.
Those are both fair points, the inlining you want can be within the implementation, the call to the parser will be nearly nothing compared to the gains from inlining stuff within the parser.
nice. Also it comes with a good number of examples.
Best damn IDE on earth
Can't answer on training courses but C++ books I like/have been recommended by the majority are - * C++ Primer, 5th Edition by Lippman, Lajorie and Moo (not the Stephen Prata book) * Starting Out with C++ from Control Structures Through Objects, 8th Edition by Gaddis * Programming Principles and Practice using C++, 2nd Edition by Stroustrup
I'm not sure if they still ship boost with resiprocate, I was not sure back then if an update to our boost version was possible at all. So this code went into its own DLL, living only in a cpp file and hence being contained. My biggest problem was, that our client isn't a server, and the example is mostly server like. So no way by the docs to check that back then, only got it running through help on the mailing list. Plus points: + good support on the mailing list + project is maintained but its niche (VOIP) Not the most modern C++, but if you can put it into its own DLL its ok. Other implementations for stun/turn, well, this one was the best to pick back then. Also: you still need to encrypt your data, once stun/turn has no layer for this, and is usually UDP. So you need to implement some simple format with handshake/keyexchange to encrypt your data.
&gt; A bit old That's a bit of understatement. It's from 2008. 
You are so far out in hypothetical-land right now that I think you've lost sight of any practical argument... Type casts can have severe semantic implications, and using them should be a yellow flag (syntatically, C++ casts being ugly and easy to grep is quite intentional). `std::move` is **not** special here – despite its library facade, `std::move` is just a prettier and less verbose `static_cast`. To reiterate: `std::move` can have severe semantic implications, and using it should be a yellow flag. If no one uses `std::move`, no bug can be introduced by adding a move constructor. Stop putting blame on the person who wrote a sensible move constructor and start questioning the idiot who was moving objects when they _knew and relied on_ it having no effect..!
So? The rope wasn't there but now it is, because it was deemed useful. The fact that someone doesn't know how to use it and hangs on it doesn't make it any less useful. &gt; Someone should only shoot himself in the foot by using language level UB like use-after-free, but he couldn't hang himself using a moved-from object. That's really not how c++ works. You can shoot yourself in the foot in c++ every time you call method on any moved-from object without considering possible preconditions. Calling back() on moved-from string without checking first if it is empty is UB and can hardly be considered language level UB. And if all your method require valid socket and someone violates the contract by throwing moved-from sockets around, that's his fault. Yes, it would be nice to be able to enforce something like this compile-time, but even without it the action necessary to get object in this state is very explicit.
boost::asio. Not that it's a bad library by any means (it solves a number of very difficult problems in the best way it can while still being cross platform), but it makes a couple of core assumptions about your problem set that made it inflexible to my needs. Basically, I had a requirement that a medium-ish (&lt;1000) number of client threads be able to share a connection pool and run scatter gather style network rpc calls. And for workload reasons I was concerned about strict latency over high speed local networks. The issues I hit in no particular order: * objects can't be shared between io_service's, this is due to a windows iocp requirement. In practice, this means one io service if you want to do things like offer connection pooling * Poor scaling off one io_service. In part this has to do with some fundamental locks down at epoll (on linux). In other cases its just a consequence of needing a thread pool of workers to turn the epoll crank (there's no way to wait on only your requests to avoid needing a context switch to process io). * No ability to mix reactors in the same build. You're running select, epoll, etc. globally. This can be bad when you want to scatter gather &lt;10 sockets (because sometimes poll's cheaper than epoll). * Difficulty abstracting the TLS primitives. ASIO works with openssl and getting it to work off of native crypto on windows or osx isn't a trivial exercise. In retrospect, I think the rule is that: if you have very specific requirements around latency, and an existing threading model for clients of your networking library, you probably can't use anything off the shelf. Different OS' push you in very different directions and I got burned a bit by not respecting that.
well; yeah, but I'd rather have an old source than no source :p
&gt; My point is just that it is possible by - bad - code to put it in an invalid state. This isn't possible without move semantics. But that's really not about move semantics. It's about introducing another state and someone else not respecting the state. Maybe at some point for some corner case in future someone will need to close the socket before actually destroying it. So you'll add close method() and specify that it is not permitted to call any methods on closed socket other than destructor. And then someone will close it and pass it around anyway. Now you have same situation even without move semantics involved. Would you write blogpost about close methods just because someone used the close method wrong? Yes, over time code gets more complex and objects can gain states. As long as the state change is expected and perhaps explicit, there's nothing wrong with it. Fact of life is, you can not enforce all preconditions compile-time, you'd need completely different language for it. 
&gt; Somebody can call std::move and fuck up. I repeat: Don't use std::move unless you know what you're doing. Move semantics aren't to blame if you purposefully use them incorrectly. That's like walking off the end of a string and blaming the for loop even though you didn't null terminate.
STL
Misinformation is worse than no information. ;-]
Am I missing something, or is there really no IDE option to specify "/std:c++latest"? I'm OK with manually adding to the additional compilation options, but I'd like to do it "the right way", if I can. (Sorry, been away from C++ day-to-day for a while, but trying to get back up to speed with changes since C++11).
Do yourself a favor and reevaluate. At the least, look through the examples and not just the tutorial. [Spirit.X3](http://www.ciere.com/cppnow15/x3_docs/) compiles and parses (at runtime) _very_ quickly, you are absolutely doing yourself a disservice by not keeping it handy. \#notsellinganythingiswear
&gt; wxDataViewCtrl because it wasn't native But this cannot be native on Windows. What native control on Windows does what wxDataViewCtrl is capable of?
&gt; It's poorly documented Poco is poorly [documented] (http://pocoproject.org/docs/)? Personally I find it one of the best documented libraries around.
Not sure if I missed it before but :set relativenumber for fakevim is finally implemented :O
I've had a difficult time with some other Boost libraries (Accumulators' documentation is riddled with TODOs), and I agree that Spirit's documentation is confusing (I don't know what documentation is X3 and what isn't), but if you can figure it out it's really quite a remarkable tool. There are a couple of conference talks from the authors on YouTube that serve as a better introduction than any documentation you might find.
GMPXX. It's what C++ looks like when written by C programmers with no OOP experience. Boost multiprecision's gmp class is what GMPXX should have been like, but it's only partially implemented, so it can actually make your code dven uglier, if that even sounds possible.
IIRC, it looked somewhat like `12345foobarunique_id_of_the_lambda::name_of_local_struct`, so yeah the name is still optimized. Again, I only remember seeing this happen on MSVC, so you'd better check for yourself if you plan on doing that kind of thing on a different compiler!
&gt; You are so far out in hypothetical-land right now that I think you've lost sight of any practical argument... Not at all. He has a perfectly consistent viewpoint - several other people agree with him. There's no need to be rude and childish. 
&gt; See, that's the thing. I don't think the library is broken. You don't break library by introducing new state that even needs to be triggered explicitly. You have this strange idea that people "should just know" not to do things like "pass in a moved out variable to a library". Of course, in the real world, almost no classes, and certainly _no_ STL classes, actually behave like that. You can move in and out of structs containing strings, ints, floats and that sort of thing, and nothing ever goes wrong. Before the change, the library cannot trigger undefined behavior. After the change, it can. Your competitor's library checks to see if the socket has been moved out of, and fails gracefully. I select it over yours because my life is too short to deal even with the tiniest possibility of undefined behavior. Because of a change in another interface, through no fault of your own, your library has a new and undocumented way to create undefined behavior. The idea that "people should just know not to do this"/"it is obvious"/"we can assume that" is not a strategy that leads to reliable, robust code. You should either document this pre-condition, or check for it in the code, quite likely both. 
Why do you keep saying that it is not documented? There's absolutely no reason to assume that. Yes, client of libraries should know that the operations possible on socket after move are restricted and that there might be preconditions to check, that's always the case, even with STL, you can't be calling string.back() right after move, even if you could before. Yes, preconditions should be documented. If you add new moved-from state you better document it. And explicit is indeed better than implicit, in this case explicit means that you won't get into the new state on your own, you need to explicitly ask compiler for it by doing a cast through std::move.
Could you please comment on salary? Because when I hear "Berlin" it automatically reminds me about 60K ceiling... :)
The topic ends as "...and why?"
Oh god the bugs. After using it at my last job I vowed never to use it again.
Ping: /u/Ozwaldo, /u/airflow_matt, /u/dodheim. /u/mcmcc, /u/TomSwirly Okay, because this discussion is repeated at every branch of the comments, everything downvoted and I don't want to repeat myself all the time, I'll explain it here once. I wasn't quite happy how the blog post turns out but thought I just role with it. Apparently that was a mistake as I wasn't clear enough, my apologies for that. The fundamental point I was trying to make is the following: Suppose we have a `socket` class as described in the post. Every `socket` object is valid, the constructor ensures that. There is no way to create an invalid socket, it only becomes invalid in the destructor. Because of this the hypothetical `read()` function can be called on every `socket` object - it has no preconditions for the object, a wide contract. (I'm ignoring preconditions on other arguments here). So far, we can agree, right? Now let's suppose somebody adds an explicit `close()` and `open()`. Suddenly there *is* a way to create an invalid socket. Thus we cannot call `read()` on every object, it now has a precondition - namely, the socket must not be closed Before it had a *wide* contract, now it is *narrow*. Changing a *wide* to a *narrow* contract *is* a breaking change, thus adding the `close()` and `open()` function *was* a breaking change. (If you don't agree on that definition of "breaking change", just substitute it with a different word. That is *really* not what I want to discuss.) I hope we can still agree so far. Now let's suppose instead of the `close()`/`open()` somebody adds move operations. They are essentially the same deal: There is a way to create an invalid socket, we need a precondition for `read()`, change of contract, breaking change. It doesn't matter that actually creating the invalid state is "evil" and that you shouldn't do it. The point is that you now have to document a precondition and make it a narrow contract. My motivation isn't to bash move semantics, they're great. My motivation also isn't to encourage explicit `std::move()`, it's evil. I'm also not blaming move semantics for anything or saying that they're a problem. They're only responsible for the situation because they give someone the rope to hang himself with and now they have to say "don't hang yourself" or rely on the fact that everybody knows that you shouldn't hang yourself. If something bad happens, it is the users fault, not move semantics, obviously. But I just want to make you aware that if you introduce move semantics you a class you make it possible that someone - for *whatever* reasons - creates an invalid socket object. That's all I wanted to say. If you think I'm saying total bullshit, that's fine. If you think I'm just doing some very theoretical argument, that's also fine. Just downvote me to hell if that's makes you happy. I personally have learned a lesson from this discussion and I'm happy for that.
lol the message tables of mfc were (and are) a lot slower than vtables.
It's actually just 2071 lines of source code - the rest are comments.
Hmm... I'd always thought that things that are private/protected shouldn't make any difference to the class interface, but it looks like they actually do. Is this good language design? I don't think most programmers would expect that modifying no client-facing code could actually break client code.
I recently moved from qt creator to vim. I just found that it didn't have enough intelligence to justify the fact that it was slow and kept freezing and crashing (and I was on a machine with 32GB RAM and an 8 core processor). 
COM. For some reason its popular for device manufactures to exposure their API via COM. In typical implementations there are many permutations of a-&gt;foo=this, a-&gt;setFoo that can compile but only one does what you want. Also requiring to include a bunch of Microsoft headers. Also static analysis struggles to deduce types. Also some libraries put you into single threaded mode, while Windows features such as drag 'n drop require multithreaded. In a better world we would forgo this layer of indirection.
I think the takeaway here is that despite movement away from optional types in recent years, in practice people tend to discover edge cases where their types are optional. A classic example of this is modern C++'s preference of returning references (which aren't optional) rather thsn pointers as we did in the C days, and which are optionals. Reference return values work well enough until you hit a edge case where you have nothing to return, and you get stuck either returning a local reference (bad), allocating a new variable to be returned (bad), or crashing (bad). The STL apparently thinks crashing is the right behaviour here, incidentally. (Make a stack, call top(), see what happens.)
I've used Boost.Program_Options with great success. If you are using Qt you can have a look at QCommandLineParser which is great too. In my opinion c++ std library should have a command line parser library, maybe they can grab the Boost.Program_Options?
And why is the author not using std::unique after sorting?
In sortReplace() * Probably no need for `res.reserve()`, as push_back has linear amortized runtime * Why a binary search? If the elements are sorted, shouldn't the duplicate (if any) be the last element in `res`?
http://gafferongames.com/
Took me a while to realize that the post started down there at the bottom after the **HUGE** header info.
If everyone knew that, this article would be useless.
Thanks, good call.
It is pretty well hidden, you can right-click a document, then Git -&gt; Show annotation. Or search for it in the configure shortcuts dialog. When the annotation bar is active, you can then right-click the individual commits and click "show diff". Yes, the Adapt Signature workflow is not great at the moment. We are aware and improvements are planned. You're right about the ":linenumber" shortcut, I didn't notice since I always use Ctrl+G ;) We should change that, I guess. For the "show annotate", make sure your project is using the git vcs plugin (you can select that in the open/import wizard).
In this article: Use the standard library but screw its established naming conventions.
Since you are going to copy the vector anyway, you can also capture it by value (with the benefit that the caller can then move the vector, if he no longer needs a copy).
Do you mean a precompiled header in your project builds? I didn't notice any clang code model performance gains from that.
It seems to work with views and composition, but when I tried to output a range it doesn't compile: #include &lt;iostream&gt; #include &lt;range\v3\all.hpp&gt; using namespace ranges; int main() { std::cout &lt;&lt; view::iota(1, 10) &lt;&lt; '\n'; return 0; }
I wish I can upvote you more. Xerces was one of the first lib I used as a c++ programmer, and **it let a scare in my mind**.
When I read the challenge problem I interpreted it as asking for something that could operate on any container, not just std::vector. There's also an assumption that the type parameter T is already comparable.
Thanks! Do not hesitate to try our project and make feedback :-)
I just don't get the why anyone would want to use visual c++ under linux.
I haven't try it yet, but looks like remotely editing and building files rather than running Visual Studio under Linux. 
I wonder how that will work out if you want to use the Ubuntu installation that the Windows 10 Anniversary update provides.
Timings on Debug builds? Sheesh
Hmm, that is too bad, if clang read it (and the precompiled header was actually used), it should have helped a bit. That is assuming that the precompiled header improved compile time with clang. I'll look at it when I get home. 
This is a cherry picked example though. There are many ways to measure perf. You took someone's bad implementation (very bad, you're comparing sort algorithms here) and replaced it with a very well designed STL implementation. If your problem doesn't fit neatly into STL containers (especially if it's a linked list or a hash map) STL perf is really bad. Sets for instance are painfully slow. There are lots of valid reasons to not use STL: Back compat with older compilers. Maximize cache coherency. Cannot use throws (aka legacy code). Requiring guaranteed perf. Requiring lockless data structures. Requiring communication across processes, VMs, or PCs. Etc... STL is a good tool to know and I really like a lot of it (at work I'm driving STL usage) but this example is terrible because you're comparing quicksort to insertion sort and you're cherry picking for a cookie cutter STL solution that fits. 
The first section of the [range-v3 documentation](https://ericniebler.github.io/range-v3/) (Why use ranges?) explains what actions are (eager algorithms). The `operator|` is to be interpreted as a "pipe operator" (like a unix pipe, you pipe something in, and get something out, and you can chain input and outputs via "pipes").
Can you fill an issue for the -O0 case including the code to reproduce and your measurements? While I expected that in release builds the performance would be equal, 10x slower independently of input in debug builds seems suspicious. In particular since most of the time should be spent in a loop and that loop should be the same whether one is using actions or just calling the algorithms directly. Range-v3 does some extra checking in debug mode, but not 10x slowdown worth of checking.
Well, that's the "blame the user" attitude that already caused billions of damage. But since it is technically possible to prevent compiling such code, one could also blame the language or blame the compiler. Edit: I'd also like to add that it's not necessary illegal to use objects after moving. It perfectly fine for, e.g., a unique_ptr.
That you keep taking offense to every little thing people say does not mean they're being rude.
std::thread will usually have extra heap allocations and deallocations, beyond what is done underneath pthread_create. Depending on your use case, you may be able to avoid the heap allocation using pthread_create manually, but you generally can't avoid it using std::thread. The heap allocations performed by std::thread are to support passing arbitrary parameters to your thread main, and to support things like notify_all_at_thread_exit. Unfortunately, it's a case where you (partially) pay for a feature whether you use it or not.
To be fair: comparing std::map and your own *flat* map is totally not fair because of a completely different implementation.
/r/cpp_questions is for learning C++ and /r/learnprogramming is for learning programming in general. Be sure to read the sidebar in both subs for additional information.
Will it do NRVO if possible, otherwise move, or will the move prevent NRVO all together like OP suggests? (The performance difference between RVO and move is probably negligible, but I'm still curious.) 
What I mean is time is takes for thread initialization running, processes running on threads and thread cleanup, with emphasis on processes running on threads. I am not sure on the whole API and most of them are only important if you are sharing common resources. I might want to explore them later but right now I care more about the workload on running a thread. For example, I want to sum the number from 1 to 1'000'000 (C++ 14 trick) on one std::thread and one pthread which thread would take longer to run. I know pthread could by more buggy then std::thread, but I would suspect pthread might be faster or just as fast as long as you avoid casting and type checking
If you don't _know_ you have a reason to avoid standard facilities (and it appears you don't), then use standard facilities. Focusing on performance to make a choice when you don't have any performance requirements is misguided at best.
Performance is not at all relevant for the -O0 case. No should be using this case for anything other than debugging/testing. Generally, libraries contain lots and lots of code used only for checking/trapping bugs and are not suitable for any shipping application. Don't benchmark code with -O0 !!! It's misleading and much less than helpful. 
that's one way of looking at it. But the other way is that programmers fail to investigate the STL for better algorithms than they think they can easily implement themselves. The object of this article is clear: If you're coding simple algorithms from scratch, you're missing out on faster, smaller, more correct code and wasting a lot of your valuable and expensive time time to do it. If your not doing this, then the article gives you ammunition to convince your colleagues to stop wasting their (and everyone else's time)
actually what is being compared is the manner in which one programmer approaches a problem with how another one does. It should be pretty clear that the first programmer is wrong. This is a very typical example of what often happens. The world will be a better place when programmers upgrade their skills and knowledge of the STL 
For some applications, excessive slowness in debug configurations is a huge deal. It can greatly complicate attempts at tracking down or reproducing issues. This is specifically true when comparing alternate implementations in debug. Excessive slowness - in execution, compilation, etc - can be a barrier to adoption.
are you looking for math function like pow, sin, cos, abs,..? is so then you need #include &lt;cmath&gt; for C++ or #include &lt;math.h&gt; for C both of these are standard library headers and should be found in the the header files of the standard library implementation of the C/C++ that you are using. PS remember if you are using C++ cmath all of the functions are found in the std namespace
The whole premise is flawed – `std::thread` is an interface, not an implementation. Until you start talking about the latter, you can't make such comparisons to begin with.
A lot of these are edge cases, and I think what the author is trying to point out is that the default mode of operation should be to look at the standard library for a solution to a problem first, and then look elsewhere when an actual -measured- problem can be identified. The industry is still riddled with decades old misconceptions about C++ and STL performance. What was true 20, 15, even 5 years ago may not be true now.
very cool! I'm excited to try this out...
Precondition violations are, by definition, always bugs in user code. Exceptions were **not** designed to expose or handle bugs in user code: - If the exception is caught, it can be ignored instead of fixing the bug that caused it to be thrown in the first place, and on top of that the stack will be unwound making it harder to _find_ said bug to fix it. - If the exception is _not_ caught, the program crashes and stack may _still_ be unwound (implemention-defined). The best way to keep the stack intact so the bug can be properly identified and fixed is to call `std::abort`; not coincidentally, this is what most `assert`s do.
Lol, what? It's not the "attitude" that caused "billions" of damage, it's hiring crappy programmers that don't know what they're doing. Blaming the language is trying to fix the symptom, not the cause. Also, I'm not sure what you're trying to say about using a move semantic with a unique_ptr. The purpose of a unique_ptr is to have sole ownership of an object, so moving it is... questionable. Could you elaborate?
The article changed so I can't see the version you were commenting on, but calling reserve() when you know how many elements you'll be inserting is generally a good idea. For large arrays the performance differences can be significant, and even for relatively small arrays the inefficiencies can add up if you do it a lot. Vector's growth factor is 2X so you can get that 'linear amortized runtime'. Which means that if you're inserting millions of elements it will have to allocate new storage and copy the existing array 20+ times (2^20 ~= 1 million), but with reserve() it will only need to allocate once and never copy. 
I've been using this every day at work on a Raspberry Pi project (we are otherwise Windows-only shop). It's not Visual C++ on Linux, but rather the ability to work on Linux projects, through Visual Studio on Windows via Remote building. Not even a new concept, but it's nice to have this option under VS. Now, if we were mostly on Linux I'd much rather do everything in Linux natively. But I spend so much time integrating this Linux project with a much larger C# project that its just not worth it. I like to keep everything in VS just as a consistency thing. That said, my favorite C++ IDE lately has been CLion. It's not my choice at work to decide what tools we use, but I'd use that if I could.
It sounds like you're a proponent of defensive programming whereas other people are not (&amp; it's a sliding scale so it's hard to pin-point where exactly anyone is). Checking nullptr of a unique_ptr pro-actively in every single function is counter-productive IMO. It litters the code in a way that makes it difficult for a reader to actually trace whether or not a nullptr is currently possible. Additionally, if you change semantics so that it's now nullptr *is* a valid state, what are the odds that you correctly guessed how to respond to that correctly before it was a valid state? In other words, trying to protect against hypothetical future states is a code smell; there are an infinite number of such states, it's impossible to predict how to correctly handle them, and it misdirects other readers (or you in 6 months) about how the code is expected to behave at runtime. It's better to try to structure your code in a way that violated assumptions will either cause the code to fail to compile or to crash explicitly at the earliest possible point; the former ensures you'll catch it during development &amp; the latter ensures that you'll catch it during your testing cycle; programming defensively ensures that you won't catch it at either point unless you're lucky &amp; you'll only discover it after ship.
Andrew Sutton is the Boost.Graph maintainer. His email address is in BOOST_ROOT/libs/maintainers.txt.
The problem is you present it as a performance issue rather than a simplicity issue. I can write an implementation that blows STL out of the water. STL isn't performant, it's really easy to use though, and you can get good enough perf 95% of the time. You're not selling STLs actual gains - it's not perf, it's the fact that you get good enough perf on top of all the other goodies.
I too implemented a [flat map](http://www.lukas-bergdoll.net/blog/2016/1/31/big-o-pitfalls), on top of std::vector and it outperformed the boost and eastl flatmap, implementations in a quick test.
PHP or Perl guy?
&gt;Do you want to write your own operating system kernel, your own high-performance database engine, or other low-level system tools? C++ is the way to go. Why are you ignoring C?
Thanks for the info. Let's try again. :)
Some may argue it's not even C :)
vector's growth factor is *unspecified*. VC uses 1.5x, libstdc++ (GCC) and libc++ (Clang) use 2x.
The main problem with this is that Visual C++ itself runs on Windows.
Because old habits die hard. It's frustrating to realize that you need to relearn a language you think you already know. C++ has changed so much in the last decade and most people don't get to work with the same language over such a long period of time, allowing them to pick up and integrate those changes into their working knowledge. That's not supposed to be an excuse, just an explanation. I agree you should take the time to catch up with the language if you intend to use it, it can just be irritating to constantly have to do so :p
&gt; I know pthread could by more buggy then std::thread what leads you to this opinion? &gt; pthread might be faster or just as fast if std::thread is based on pthread (usually is), the performance of code executed inside the thread should be equivalent.
No, `size()` returns an unsigned type (or should), so that can never be an infinite loop.
&gt; The real question is.. should it be the case? Seems comfortable. I kinda like it. while it might be convenient, I don't like it that much... it enforces using files on the disk. What if I receive pre-trained NN over network socket? Do I have to first save it to the file and then call NeuralNet("file")? I myself would prefer something like static NeuralNet NeuralNet::from_file(...) static NeuralNet NeuralNet::from_string(...) 
I am doing the same and I have a primarily C# background. So many libraries use CMake to build as well it makes library integration easy (and intellisense).
Would pthreads also be an interface and not an implementation at least on non-Unix-based systems? I agree that there could be unique std::thread implementations that do not use pthreads in any form and might be faster and more efficient by puting more compile-time checks than pthreads but that is not the case now
I'm not talking about create/destroy cost here, just saying that I don't see any technical reason why code itself inside should run any slower. But hey, I might be wrong :) you should benchmark it
I did my Master's Thesis about Task Scheduling. I've also been benchmarking a lot, reading glibc and kernel source code etc. As a hobby project, I'm also developing my own non-posix-compliant linux-only thread implementation in modern C++. Here are my 5 cents: In one of my benchmarks I measured how fast I could start/tear down threads using pthread_create/pthread_join vs std::thread::thread/std::thread::~thread. pthread won by 4-5x. /u/ben_craig explained quite well what's the difference. Passing the arguments to the new thread is actually very expensive, and in my benchmark I wasn't passing any arguments so the pthread implementation didn't suffer this slowdown. That being said, I would highly recommend using std::thread. I can't recall that I would've ever done a threading pattern in C where I wouldn't have done it as follows: first the "parent" thread creates the new thread giving it a pointer to the data for the new thread. Then the new thread acquires the data and signals the parent back. Now the parent can free the data. Sooo, exactly the same thing that std::thread provides and which makes std::thread slow compared to pthread in my aforementioned benchmark. By the way, in my opinion there's surprisingly little stuff going on both in userspace and in the kernel when creating a new thread. Unless allocating stack space (done using mmap()) for the new thread is slow, a modern x86 machine should be able to create 100k threads per second, or at least close to it. Disclaimer: Everything I just said is valid for Linux, glibc, gcc and x86. For example, CPU's like POWER have weak memory ordering, so there it's mostly likely a completely different ball-game. Also if you change to musl-libc, clang, etc analysis and benchmarks would have to be started over.
You will see, it looks interesting so it's possible I would say. One more sugestion after seeing this Matrix B(2,2); B(0,0) = 1; B(0,1) = 2; B(1,0) = 3; B(1,1) = 4; Have you considered also supporting API like this Matrix B(2,2) (1, 2) (3, 4) ; ? 
This cant happen. The condition ensures that i is always smaller than the max value.
I liked `boost::asio`, it has a lot of examples and it worked pretty well in my experience. Being able to use as header-only is great. I also had success using `websockets++` with `asio` standalone backend, that is a pretty nice lib also in my experience and opinion.
Most definitely on my list: implementing Matrix constructors that can take `array` and `std::vector` for initialization. Also, a Matrix constructor for `std::std::initializer_list`. I would love to give my Matrix class a more MATLAB like syntax, so things like: `Matrix A(4,4) = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16];` would be possible. Alas, I do not know how to do such things yet.
I have no idea how to do, dunno if even possible [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16] on the other hand, something like Matrix A(4,4) = ( 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12, 13,14,15,16 ); could be probably achieved by overloading operator `,`. Not as pretty as matlab's thing, but if formatted like this I think it works OK. 
Sweet, I will look into it! thanks :)
Sure. That's the approach newer classes take. But std::stack was written before exceptions, if memory serves, and I believe this is why it still doesn't throw an exception. And it still leaves your program in an unusable state - the return value in the calling scope is still invalid. So if you're going to try to continue (and not abort) then you need an optional value anyway.
Thanks, you should send this to the spacemacs people, they've been looking to write a layer for r tags. 
Having a consistent naming scheme is pretty key, makes jumping into a new project that much easier. 
There are times when it can produce very readable code. Matrix and vector (not `std::vector`) math is one. This is another. 
There's already the basics of one available in [issue 2327](https://github.com/syl20bnr/spacemacs/issues/2327#issuecomment-153283156). I've copypasta'd this into a private rtags layer and it works like a champ.
Totally. I'm there everyday, all day!
this article Is very good .it is totaly corect . The language is used in this article is very grammatical. To be fair, I was no better than the author of this article (both in understanding of c++ classes and in English skills) when I first started to learn c++ classes a year or two ago.
Gdb has such a feature
&gt; Qt Creator gains experimental support for the Nim programming language. Downloading now. Despite the flaws, I really do love writing Nim code. The only reason I don't do more with it is because of the limited tooling and (unfortunately) small community.
Do not dislike my submission without offering a counterargument.
Well they have to choose between teaching how to use a von Neumann machine and teaching an abstract programming (which doesn't exist in practice, because every language eventually leaks the underlying details).
Do not post obvious flamebait.
&gt; Or you can do the right thing and throw an exception. This is exactly what they were designed for. You can, but it's not always ideal. Maybe you don't want to perform bounds check every time you access vector element. I'd say that assert in debug mode instead of exception would be better idea.
Well, that's my point. C style cast is deprecated because it silently promotes all the way to reinterpret_cast making it too easy to convert between unrelated types. 
Why browsers are Awful: The HTML on [this page](http://www.radford.edu/ibarland/Manifestoes/whyC++isBad.shtml) is broken, yet browsers still render it, without any hint of a problem. 
Sorry for the extremely late reply - had a very busy week. I checked out your changes and I really like what you did with `zip_fold_and` and `zip_fold_or` - they are very nice short-circuiting abstractions that, as you demonstrated, allow a clean and intuitive implementation of subset/superset checking. I'll try it out in my [ecst](https://github.com/SuperV1234/ecst) project as soon as I can - looking forward to run some benchmarks against `std::bitset` as well.
Let me play devils advocate. A variant of that story... &gt; Imagine you are a spanner, and your boss tells you to connect the gas pipe in the basement to the street's gas main. You do nothing because you're an inanimate object. KWABOOM! When the dust settles from the explosion, you're not guilty of criminal negligence because you're an inanimate object and incapable of being responsible, but your boss is carted off to a padded cell. The point being that C and C++ - the languages and the compilers - are not the equivalent of construction workers. They're tools. Responsibility lies with the person using the tool. &gt; Imagine you are a chainsaw, and your boss decides to juggle you while you're running. You cut your bosses hands off. You're not guilty of criminal negligence because blah blah blah... Yes, C and C++ are metaphorically sharp tools. That's why you're supposed to be qualified and competent, and to behave responsibly while using them. &gt; `house [-1]` C pointers are doing the job of iterators/cursors as defined in the 60s at latest for databases, just for arrays rather than some on-disk data structure. That's why adding 1 to a pointer advances to the next item - it doesn't simply add one byte. And it's why the C++ abstract generalization of a pointer is called an iterator. Yes, C isn't much newer than that cursor/iterator concept, so pointer-as-array-cursor is a leaky abstraction. People still care about backward compatibility, though. If `house` is a pointer to a particular house, `house [-1]` is referring to an adjacent house. That's not some bizarre concept. Even non-programmers clearly understand terms like "next door" or "four doors down" etc. Of course there's no bounds checking, and even the array itself allows this, so you can easily end up invoking undefined behaviour by trying to look at houses that don't exist. It's nice when compilers prevent you making mistakes like that, that's why I like strong static typing, but ultimately, the compiler simply cannot catch all programmer errors anyway. If the programmer instructs the computer to use things that don't exist, that's clearly an error in the programming logic. All that's different in C is the particular symptoms that result from that logic error. BTW - in high-level idiomatic C++, it's mostly quite difficult to *accidentally* make that kind of mistake. Of course it's easy to make the mistake deliberately, or to make it in low-level data structure manipulations (which should therefore be encapsulated and thoroughly tested) but "here's the half-open lower and upper bounds - now go and do your stuff, nice standard algorithm" isn't tricky. You just need to be familiar with the relevant idioms - ie to be a competent, responsible programmer who's qualified to use those sharp tools. Of course there's bad programmers who can write bad code. That's true for any language, but the consequences in languages that are more cluttered with sharp tools can obviously be much more severe. Just as you can't replace a competent surgeon with a Rube Goldberg scalpel and expect a good outcome (yet), and that doesn't mean we ban surgeons from using scalpels, the same applies to sharp programming tools. But part of being a responsible, competent programmer is that you only use the sharp tools when you have a good reason to. That *could* mean not using C or C++ at all, or it could mean you use those languages, but don't give in to the temptation of Baldrick-style "cunning plans". 
Reminds me of my college professor who installed Turbo C in the new computer lab. In 2010.
Wow, that's actually a really nice way of separating string storage and string encoding.
Since that guy seems to be teaching only, there is no surprise. Neither is his ignorance of actual and modern industry practices.
&gt; STL isn't performant This really really depends on your definition. I've optimized a lot of code in my day by doing pretty much what this article does; converting raw loop code the using standard library algorithms. Sure, if I thought about the problem long and hard, and hand coded the same algorithms making some assumptions about the data, I might get a few more percent out of it. But, that's time I wouldn't be spending optimizing some other piece of code, and my time is a resource the same as CPU time. Of course, I think this is what you are saying with 'good enough performance', I just think we need to be careful with the term good enough because in my experience, I'm unable to beat the standard algorithms without serious thought. 
I was wondering about that when I read the release notes. I've never heard of [Nim](http://nim-lang.org/) before. What is the motivation for including support for it in QtCreator? 
Really cool. Seems designed by the rare person who understands both Unicode and C++17. Congrats on the good work. 
&gt; Indeed, any professional programmer who uses C++ will tell you that they use a disciplined subset of it C++ offers programmers plenty of choices about what subset to use.
How does this contrast with [Ogonek](https://github.com/rmartinho/ogonek) (documentation currently offline)? The obvious difference is that one is merely a *view* on the text but how do they differ in practical usage? I’m asking since it seems that two people duplicated a lot of effort here. I guess it would have been nice to build one on top of the other.
What follows is a pedantic and mostly useless correction of the parent post: Actually C-style casts lets you do one thing you can't do with new-style casts namely: class B1 { int a; }; class B2 { int b; }; class D:B1,B2 {}; D *f(B2 *p){ return (D*)p; }//this compiles and adjusts the pointer correctly Also new-style casts aren't a part of the library (STL), but a part of the core language.
With regards to the missing features, [utf8rewind](https://bitbucket.org/knight666/utf8rewind) could be a good fit. It provides C functions for case mapping, normalization and seeking in UTF-8 encoded strings.
&gt; warning: extern sequence-point is only valid in C89 or C++98 treating flushed property as floating-point character &gt; warning: initializer pragma left in a ambiguous state &gt; error: no mutable class at end of accessor Cute! However, I imagine we all know that *real* C++ compiler messages look like this: In file included from /usr/include/c++/5/vector:63:0, from cacheddata.cpp:1: /usr/include/c++/5/bits/stl_uninitialized.h: In instantiation of ‘static _ForwardIterator std::__uninitialized_default_n_1&lt;true&gt;::__uninit_default_n(_ForwardIterator, _Size) [with _ForwardIterator = CachedData&lt;int&gt;*; _Size = long unsigned int]’: /usr/include/c++/5/bits/stl_uninitialized.h:575:20: required from ‘_ForwardIterator std::__uninitialized_default_n(_ForwardIterator, _Size) [with _ForwardIterator = CachedData&lt;int&gt;*; _Size = long unsigned int]’ /usr/include/c++/5/bits/stl_uninitialized.h:637:44: required from ‘_ForwardIterator std::__uninitialized_default_n_a(_ForwardIterator, _Size, std::allocator&lt;_Tp&gt;&amp;) [with _ForwardIterator = CachedData&lt;int&gt;*; _Size = long unsigned int; _Tp = CachedData&lt;int&gt;]’ /usr/include/c++/5/bits/stl_vector.h:1311:36: required from ‘void std::vector&lt;_Tp, _Alloc&gt;::_M_default_initialize(std::vector&lt;_Tp, _Alloc&gt;::size_type) [with _Tp = CachedData&lt;int&gt;; _Alloc = std::allocator&lt;CachedData&lt;int&gt; &gt;; std::vector&lt;_Tp, _Alloc&gt;::size_type = long unsigned int]’ /usr/include/c++/5/bits/stl_vector.h:279:30: required from ‘std::vector&lt;_Tp, _Alloc&gt;::vector(std::vector&lt;_Tp, _Alloc&gt;::size_type, const allocator_type&amp;) [with _Tp = CachedData&lt;int&gt;; _Alloc = std::allocator&lt;CachedData&lt;int&gt; &gt;; std::vector&lt;_Tp, _Alloc&gt;::size_type = long unsigned int; std::vector&lt;_Tp, _Alloc&gt;::allocator_type = std::allocator&lt;CachedData&lt;int&gt; &gt;]’ cacheddata.cpp:15:34: required from here /usr/include/c++/5/bits/stl_uninitialized.h:540:22: error: use of deleted function ‘CachedData&lt;T&gt;::CachedData() [with T = int]’ return std::fill_n(__first, __n, _ValueType()); 
Yeah I was thinking of adding some template based error messages at some point but haven't had the time yet. That error you posted might be a good start though!
What's the problem with that?
Can you explain how the workflow with an existing CMake project would work in this case? What would it allow me to do?
I was considering implementing a text editor and one of the major hurdles was support for different encodings. Could this be used by an application that needs to edit the text and not just view it?
[Here you go.](https://en.wikipedia.org/wiki/One_instruction_set_computer)
Ok, you win ... :)
Of course, if I actually tried to *use* any of those languages, I would quickly go nuts. :-)
This might be slightly off topic and too vague to understand, but... I remember in one talk by Scott Meyers, he discusses how in one situation you might need to use a certain type of expression a certain way, so if you surround it with double parenthesis (I think?) it suddenly means something else. This is done purely out of convenience because otherwise without that syntax you either couldn't access it the way you wanted, or would have to jump through hurdles to do it... It's been bugging me because I can't remember what that syntax was or what purpose it served, and don't know enough to search for it. Could anyone help me out? I think it might be a bit obscure... 
That's great to know. That might have been it, but I think it may have been something else... I just remember that it was a syntax specifically designed to solve a totally obscure corner case in which a developer might want to access something a certain way... so they said "If you want to access it this way just stick another set of parens around it." I remember thinking it was just a totally arbitrary way to solve a crazy narrow edge case... Maybe I'm just crazy :O But thanks for that info, it's useful and that might have actually been it, just presented in a different way by Scott.
&gt; In C a[i] is syntactically equivalent to *(a + i) which allows us to write this: int a[5]; a[2] = 1; It's to my understanding that if ````a```` is a pointer to an ````int````, then ````a[i]```` should be equivalent to ````*(a + (sizeof(decltype(a)) * i))````. Because if ````a```` is at address 0x1000, then ````*(a + i)```` would point to 0x1001, which would be incorrect because the next element in the array should be at 0x1004 assuming that ````int```` is 4 bytes.
So, does this mean that somewhere (on a very low level in the language) the `+` operator is overloaded like that for pointers?
I think I remember that, it was decltype and specifically about getting the const when viewing a struct member through a const object.
The formal proposal paper, for those who enjoy following such things: [P0244R1](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0244r1.html)
Don't use an array. Using a vector, you could do something like this: std::istringstream in("20 20 20 20"); std::vector&lt;int&gt; data { std::istream_iterator&lt;int&gt;(in), {} }; If you're really set on that being one line...it's a bad idea, but if you insist on deleting some new-lines, I probably can't stop you.
I probably thought otherwise because I've done pointer arithmetic on 1 byte types before
http://i.imgur.com/T1qPcJc.gifv
Damn. But in the second case, what do the "()" parse as? Shouldnt the function declaration as described in 2. be &gt; TimeKeeper time_keeper(Timer); ?
My one parameter constructor is now `explicit`. Thanks for your feedback :)
Simpler scenario - after grabbing `node` on line 1, it could be deleted by a `garbageCollect()` on another thread, such that `node` is now invalid memory, and `node-&gt;next` is reading nothingness. Basically, `garbageCollect()` is not safe to be called at arbitrary times. Or I'm misreading. 
We use it for both for config files, and also an encoding format for messages via ZMQ (which by the way is one of the better libs).
Yes. I agree. This half assed solution is temporary. Just wanted to see if the thing works without spending too much time on exception handling. I will add exceptions on my next push to the repo!
Post titles that will definitely get downvoted for 100 Alex.
My understanding is that people are working on Clang's support, but I don't know how far along that work is. I suspect improving consensus around the Concepts design (see http://honermann.net/blog/?p=3) would help to accelerate investment in implementation for additional compilers. I wonder if some vendors are holding off investing more effort until consensus improves, but I have no idea if that is actually true or not. I've been planning to port Text_view to ranges-v3 so that it would be usable in C++11/14 compliant compilers, but time has been short. I would be interested in hearing from anyone who would find such a port useful (and especially from anyone who would be able to contribute to making it happen!)
That might be a useful way of conceptualizing it, but no. Although you could implement it like that, its one of those things that the compiler just does because that's how it's defined by the language. This can actually let the compiler perform some neat optimization tricks it wouldn't be able to do otherwise. 
will invariably lead to the cancer of the semi colons. i will show myself out.
I cant really even give and example of a possible example because of how poorly I understand neural networks. Clearly data goes in and data comes out. If you could give data input/output that has tangible meaning, then I could alter the input and observe the change in the output. Maybe something like, your input is a green grape and the resulting output is a green apple. Then I, working off of your example try a purple grape. Then that either results in a purple apple coming out and me thinking "well, I guess that makes sense, but that's not even a real fruit"(it might be, but I've never seen one) or it results in something like an eggplant and I'm left to wonder "well, it's purple, but that's not even a fruit!"
Might also have been some behavior of compilers when they see assignments in if statements. Per standard if(a=b) is an assignment followed by test. But depending on compiler settings, you might get a warning or an error, because the compiler thinks you might have meant a comparison like if(a==b). When you change it to if((a=b)), you make it more explicit, and the compiler stops complaining.
Stackoverflow Q&amp;A: [When do extra parentheses have an effect, other than on operator precedence?](http://stackoverflow.com/q/24116817/819272) 
There are two main ways of supporting different encodings. The model chosen by C (already present in principle in C89, confirmed strongly by the 95's amendment) is to avoid conversions and write the program in such a way that it works whatever the encoding is. The locale system of C and C++ provides some support for that. The model which I'd prefer (and I think what would be chosen if the choice had to be remade) would be to convert everything to a fixed encoding (the only which make sense nowadays are probably UTF-8 and UTF-16) and do conversion to it on IO. It could be meaningful to use different encoding of Unicode in different part of the program, but I'd tend to present that as an opaque memory optimization (i.e. the interface of a component would show the encoding chosen for the program, the implementation would use whatever is significant). The locale part of the standard library offers little support (the only support I remember is that there are some conversion between UTF formats). Other libraries like Qt or ICU offer more. What most programs seem to do is to work with the locale system and assume that the locale used use the charset they desire. That may be more or less guaranteed by the implementation (if I'm not mistaken, glibc for instance is using UCS-4 as the wide encoding for all locale, AIX and Windows are using UCS-2 for the same purpose; I don't think that's the model assumed by the original designers of the locale and encoding system of C and C++, but it stay within the spec).
I use the STL except for std::vector. Wrote my own implementation. The debug version is agonizingly slow and painful to step through with the debugger. And the standards committee members who are responsible for std::vector&amp;lt;bool&gt; need to be taken out back and beaten with a clue bat. 
`dynamic_cast` requires polymorphic types, i.e. it needs a vtable. The types in the comment you're replying to are not polymorphic.
Well spotted - no *run-time* polymorphism, certainly, because no `virtual` methods etc. 
Sorry Tony, but this doesn't convince me at all. IMHO, an entity with a natural non-negative value domain should be modeled like that - consistently. The index of an element in a container or the amount of elements in a container are such entities.
Gah! Just had a flashback to when I was using Microsoft's Debug Interface Access API. That was COM hell. So many poorly documented functions that don't do anything with the object you have at hand so it takes forever to try them all to find the ones that do something and hopefully what you need them to do. I'm going to cry myself to sleep now. 
It is also worth noting that even if the types were runtime polymorphic, `dynamic_cast` would still not be the same as a C-style cast in this case because it adds a run-time check.
Well it probably wasn't the doom people in that thread were claiming it to be, because 8 years later the company is still using JUCE to build their product[1]. They've rewritten their app from scratch once due to similar issues, so if JUCE was really so bad, they'd have already switched to something else by now. [1] https://www.juce.com/case-studies/max
That's not correct. They still offer LGPL for everything. They only switched from LGPLv2 to LGPLv3. You can still link dynamically without any GPL obligations.
This might be useful for people who are about to create many shared libraries and a way for me to get an opinion on this black macro magic.
I'm quite sure I have seen GPLv3 in their recent announcements. Might have been for some specific modules though, like charts and such.
If you want to give debugging hints to people based on the nature of the problem, can't you just write some documentation?
The catch as always is who will maintain and debug the debugger ;) I recommend adding checks in the C++ code itself in places where things commonly go wrong which do some sanity checks and print detailed explanations and suggestions when checks fail.
Sadly it is not: std::size_t("foobar"); Is completely equivalent to (std::size_t)"foobar"; but is completely ignored by that warning.
Yes, I want the debugger to "learn" and then integrate it with Jira. It's pretty wild right now :) but just thinking of giving it a try. Thanks for your reply.
If it was only 1 byte jumps then it won't be called pointer arithmetic :)
I think very good starting point would be to add (debug only?) checks and extensive logs (with some levels so you don't always get tons of spam). Then you could easily just run application, save log, find "error", "exception", "fail" etc. in it and mostly it would point you in good direction and from there you could simply check what failed before to get you to this place. I think people underestimate how much this can help in very big codebases... 
You're probably thinking about the tools like Creator and moc: &gt; All tools (Qt Creator, moc, etc.) will in the future be available under commercial license terms and GPLv3. The GPL license comes with two exceptions that ensure that there are no license restrictions on generated code, and that bridging to 3rd party code is still possible.
I don't think relying on a third-party header for this is really useful, but ymmv. Most of the complexity seems to stem from allowing _multiple_ sets of defines based on the token given in the `E_ABI` macro. While this sounds useful, I struggle to think of a situation where I would use it. Why would you have two sets of symbols that are exported differently within the same project? It also adds a lot of typing since it requires a "library name" for every declaration and definition. I guess you could maybe have a "namespace" equivalent? There's also no way to distinguish between a typo in a library name and the "import symbols" setting. ~~Finally, you have a bug in your compiler detection on line 80: it should be `#ifndef __GNUC__`. Right now, `EYENSEO_ABI_COMPILER_GWIN` is defined on Visual C++.~~
Remote debugging is a very limited scenario. Much nicer to develop natively on Linux which can be done using Qt Creator and CLion.
Or even take a page from the web folks and send the logging over to Kibana and to Graphite to make it searchable and graphable With a little more work to make it reflectable^[1], you can start getting deep introspection on things. --- ^[1] Man I really wish C++ had a decent generic reflection library. 
what class of issues are you finding?
Do you have any code review process? You can incorporate keeping the documentation up-to-date as part of the process so that people are *forced* to do it.
&gt; Remote debugging is a very limited scenario. Maybe. Maybe not. Maybe Microsoft is trying to change that? That's why I was asking. &gt; Much nicer to develop natively on Linux which can be done using Qt Creator and CLion. No, it's not. This is just your own personal preference. Of course it "can be done" on Linux, but in what way. I prefer VS.
You should also consider extending your standard debugger through its extension API. gdb, for instance, has a very nice extension API in Python.
Seems like a good idea, can talk to my team on this. Thank you. But I will still give a try to my "debugger".
Useful? Not so sure
I think you might be better served learning an easier language like Python or Java, and then C++. Sort of like learning automatic transmission before manual.
&gt; I'm not sure what you mean by namespace equivalent Instead of void E_ABI(lib) f(); void E_ABI(lib) g(); void E_ABI(lib2) x(); void E_ABI(lib2) y(); you could have #define E_ABI_LIB lib void E_ABI f(); void E_ABI g(); #undef E_ABI_LIB #define E_ABI_LIB lib2 void E_ABI x(); void E_ABI y(); #undef E_ABI_LIB This would also mitigate the typo problem. But again, I can't think of a situation where I would need something like that.
Oops - fixed :)
Aren't they called "unit tests"? If your code base is well designed, loosely coupled and modular, it should be easy enough to retrofit it for unit tests. And you should be able to keep your build times low by farming out individual libraries to individual build servers. Hah who am I kidding. No one builds software like that.
Fixed - thanks!
http://preshing.com has some excellent posts, code and benchmarks for hash tables that can be used concurrently. Much faster than std unordeted map etc. Well worth a look. Thanks for sharing a good article
Using unsigned does make sense logically, but I've always found that it just caused more headaches than it was worth, so signed wins in practice. I should just mention that the youtube link isn't what convinced me. It was based on my own years of experience. I find using unsigned for numbers just causes more compiler warnings, and more bugs. (And less performance.) And the comments from the link isn't meant to be just blind "argument from authority", but a "consider what others have learned from (lots of) experience". Part of the problem is that as soon as you get a number, you think of it as a "number" not a "natural non-negative value". You immediately assume that you can do subtraction, etc, with the number, and you don't think of it as subtraction mod 2^n.
In ImageJ (a Java program) there is a well known problem where you can't load an image larger than 2 giga-pixels. I can't help but think this problem could be avoided if Java had signed integer types.
Also unit tests (although this may already be implied by "more assertions").
CGAL - In theory - a geometric processing library with a multitude of powerful built-in functionality. In reality - a huge mess of templates upon templates upon templates that no intellisense can follow and that breaks up miserably for even the most trivial problem with your input. The vanilla "Mesh" class literally has no less than 10 inheritance levels, most of which are [CRTP](https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern). ACE - Just a classic case of code rot. The style of code in this library is so ancient it is barely recognizable as C++ anymore. Moved to the boost alternatives as soon as was possible. 
Nice article, but your benchmark is biased. Your implementation of HashSet doesn't fare well with bigger hash set without a better collision resolution. Example with this [benchmark](http://pastebin.com/thK49V0n). std::unordered_set: 3.59102s emilib::HashSet: 243.754s
[VirtualBox](https://www.virtualbox.org/wiki/Downloads) used to be my go-to hypervisor. Nowadays, I use [VMware](https://my.vmware.com/en/web/vmware/free#desktop_end_user_computing/vmware_workstation_player/12_0). Some people think VMware is easier to use, but I think they're about the same. Just use VMware. This looks interesting, although I haven't checked it out yet: https://blogs.msdn.microsoft.com/vcblog/2016/03/30/visual-c-for-linux-development/
This is a good and interesting article. As usual with me, I find a few parts to be iffy. My thoughts: ---- (1) In "Chaining or Open addressing", you decide on open addressing, because "it is faster". You give a justification for this earlier, when you mention that the specs for the C++ unordered containers requires that all items be referenced by pointers. But we don't have to follow that spec. In particular, what about a chaining-based hash table in which the table itself is an array of structs, each containing a table item and a pointer to a linked list holding the rest of the bucket. Since a bucket will typically contain 0 or 1 items, we would be doing mostly array look-ups, and they are nice and cache-friendy. And regardless, the fact is that chaining is much more popular than open addressing. I'm still rather hazy on why. But, for example, Python seems to be the only major programming language whose standard hash table implementation uses open addressing. I imagine there is a good reason for this huge difference in popularity. (?) (2) In "Linear probing or quadratic probing", you choose linear. The justifications for this are, first, that it is more cache friendly -- which is obvious. Second, its disadvantages only matter much when a poor hash function is used. But I must take issue with that second claim. The primary problem with linear probing is this. Suppose a table index i occurs in the probe sequence for some hashcode, and the next table index in that probe sequence is j. Then if i occurs in the probe sequence for some other hashcode, then the next index in that probe sequence is also j. This is easy to see in the case of linear probing: j = (i+1)%t, where t is the number of locations in the table. The result is that, any time multiple probes are required for some hashcode, multiple probes will end up being required for other hashcodes as well; this is "clustering". It is as if distinct buckets can fill each other up. Nothing in the previous paragraph depends on the hash function. With a high-quality hash function, clustering is still an issue when linear probing is used. Quadratic probing, and various other methods of generating the probe sequence, do not have this problem. Now, if you want to claim that the cache-friendliness of linear probing will, in practice, outweigh the disadvantages caused by clustering, then go ahead -- as long as you can back up that claim. But let's not say that clustering is avoided simply by choosing a high-quality hash function. (3) Near the beginning of the article, you say, "Hash tables are a wonderful inventions. They allow you to insert, erase and look up things in amortized O(1) operations ...." Bless you for not claiming that hash tables have constant-time operations. That particular meme very much needs to die. But you're still not quite right. True, doing hash-table insert in O(1) operations doesn't work, due to the periodic rehash. And so we stick an "amortized" in front of that "O(1)". But it's still not amortized O(1) operations in the worst case, as it is *possible* for all inserted keys to have the same hashcode. The correct statement is that hash-table insert uses amortized O(1) operations in the average case. For hash-table retrieve and delete, there is no rehashing to worry about, so we do not need the "amortized". But we still need the "average", for the same reason. So these two use O(1) operations in the average case. (It is *correct* to say that they use amortized O(1) operations in the average case; but we can remove the "amortized" without making the statement false.) (4) Continuing from above: "... (O(√N) time)." Your point about operations vs. wall clock time is a good one. I read your posts on "The Myth of RAM", and I find them worthwhile and informative. However, let's understand that, when we say "O(...) *time*", without further qualification, the convention is that we are counting operations. Strictly speaking, we need to specify what operations we are counting. But conventionally, when we deal with programs in a high-level language, the operations we count are usually C operators and client-code-provided functions (including operations on a fixed number of items of the value type of a container). When we work in the theory of computational complexity, we usually count transitions of some machine. Now, there is nothing wrong at all with counting processor cycles or ticks of a wall clock. You can count anything you want, and those two would give results that are particularly useful. But that does not mean that I'm wrong when I say that hash-table insert is amortized O(1) *time* in the average case, since I'm measuring "time" in calls to C operators and client-provided functions. And I, too, am allowed to count anything I want. ---- As I said, a nice article. I look forward to your future posts.
I can confirm those numbers. * std::unordered_set: 2.79603s * emilib::HashSet: 203.888s clang version 4.0.0 (trunk 278907), O3, Xeon E5-2687W 3GHz Also based on a cursory review of the code, it seems that hashset might not be providing the same guarentees that std::unordered_set gives with regards to iterators 
I'm surprised that nobody mentioned that `std::mutex` on Windows performs worse then almost any other implementation, for example the one in Qt. https://www.reddit.com/r/cpp/comments/4hoyzr/msvc_mutex_is_slower_than_you_might_expect/ Really, this article is advocating for you to use someone else's code, which is fine - but the best one might not be in STL.
With open addressing you need to have lower load factors. Performance degrades faster at high load factors. Also try my open addressing hash table: https://github.com/rigtorp/HashMap. I'm working on support for std::string_view lookups in std::string to T maps.
I love how the author starts with a strong position, staked out in their title, and then proceeds to *undersell it by not knowing the subject matter well enough*. &gt; For instance, it's not obvious that in i = v[i++], the final value of i is undefined No, if your C or C++ program ever intends to execute that, the entire program has undefined behavior. Neither i, nor v, nor any other variable, has any particular value at all. &gt; In Mathematica, two billion plus two billion is four billion. In Java, it's defined to always be -293 million (approx). In C/C++, it's defined to be whatever answer gets returned, and will vary from machine to machine. Nope, again. Signed integer overflow is UB. Perhaps it would have behooved the author to actually read the series of blog posts by Prof Regehr that the article links to. And for all that, the point that C and C++ are poor first choices for a teaching language is entirely right. This article, and its posting in the present, serve that argument poorly. It's not quite a strawman, but it could certainly be held up as one.
&gt; Also based on a cursory review of the code, it seems that hashset might not be providing the same guarentees that std::unordered_set gives with regards to iterators This is even called out in the post. It doesn't give any guarantees on inserts (because those can cause a rehash), saying that if you want them, you should insert pointers to objects instead.
What about some tooling like chrome dev tools blackbox where you can blackbox a library and debugger will never pause inside library. Does any such facilities exist for gdb? 
The first one is a pretty good idea and something that others brought up (with different terminology), but I like this better. However, the second part is not possible to implement because the msgpack-rpc protocol dictates the message format. If one breaks protocol, it's no longer a msgpack-rpc server, and external clients can't talk to your server (and vice versa). I'm planning to refactor the library so that the server logic is separated from the transport and the protocol. (Same with the client), and that will make this possible.
I didn't know that Qt offered a solution - thank you for the info. For the situation at hand it wasn't a solution since only a few libraries were using Qt.
Perhaps you can have a compile-time option for the 2nd requirement? or encode the message id either as a string or a combination of a special character not used in strings and an id? so as that the server either receives message name or special character + id. 
What happens if you're editing a header file?
That's a viable option, I think. I'm on a holiday right now, so I can't explore the idea, but thanks for the suggestion, I will think about it.
Only 2 makes sense - and is indeed a good idea - but only if the cpp file is in the same directory, otherwise the matching can become easily a bit too heuristic, because just having the same name won't fly for big projects. 1 can't work, because you don't have the necessary flags to build the cpp file, there's no entry for it in the database.
Well done. Thanks! I'm pretty sure that you are wrong about exceptions though. At least in wasm (asm.js should hopefully die soon anyway). I'm also a bit surprised, how much hatred C++ in the browser topics receive here…
And the worst part is that this kind of people want to teach to future professional programmers how to ride bicycles with training wheels so then they can go out to the Olympics
Idk if you have tested it but it's not compiling under windows, specifically the loguru dependency. (do you really need this?) VS 2015, looks very interesting though.
That dude's page layout sucks. I hate it when I have to scroll text sideways. Just sayin'...
Most constructors should be explicit. Implicit should be used when - the source and dest types represent the same "thing" (ie string and char * both represent a sequence of characters, int and long both represent numbers...) - the construction is cheap - the construction is noexcept - information is not lost - no danger is introduced (ie dangling references, ie imagine char * from string)
Cool, then don't write code in C and C++. I still will. But you don't have to.
I don't really know what to say. He's suggesting to change the language to solve his own bad code design.
I highly suspect this was written a decade or more ago. Back then, some of his criticisms were more true.
Couldn't you just write void foo(sqlconnection &amp;sql) { auto rows = sql.execute("SELECT * FROM table WHERE id=1"); if (!rows.empty()) { std::cout &lt;&lt; "found: " &lt;&lt; rows[0]; } } or something similar? And the second example could be easily rewritten with C++17 if with init statement.
Does your browser have a Reader View? It improves modern web page design immensely by removing it.
I suspect this was written at least a decade ago.
Hm, I don't have a horizontal scroll on this page no matter how I stretch the window. But I still don't like the layout since the text is off-centered, and there is a big useless block on the left taking too much space.
This article seems crafted solely with the purpose of being flame bait. If it wasn't meant to be so, why would you post it to /r/cpp? That's like telling a person to their face, "Your baby is ugly and should have been aborted to save the world from its hideousness!" Instead of hating (fallaciously might I add) try advocating the language you do like. 
I know about find_if. Your code does not work (where does row come from?). With my extension, you could write: template&lt;typename PredicateT&gt; void foo(sqlconnection &amp;sql, PredicateT predicate) { auto rows = sql.execute("SELECT * from table WHERE id=1"); if (auto &amp;row : std::find_if(rows.begin(), rows.end(), predicate)) { std::cout &lt;&lt; "found: " &lt;&lt; row; } else { std::cout &lt;&lt; "not found"; } } Which is much nicer than saving the result of find_if somewhere.
&gt; std::cout &lt;&lt; "found: " &lt;&lt; *row; Note that you have to manually derefence the optional here (which basically makes the std::optional similar to a raw pointer with all its problems). With my extension, you would write if (auto row : find_if(rows.begin(), rows.end(), predicate)) { std::cout &lt;&lt; "found: " &lt;&lt; row; } else { std::cout &lt;&lt; "not found"; } Which does not allow for any UB. (You cannot access row in the else branch with my extension)
&gt; (where does row come from?) Sorry I missed that. Correction: template&lt;typename PredicateT&gt; void foo(sqlconnection &amp;sql, PredicateT predicate) { auto rows = sql.execute("SELECT * from table WHERE id=1"); auto I = std::find_if(rows.begin(), rows.end(), predicate); if (I != rows.end()) { std::cout &lt;&lt; "found: " &lt;&lt; *I; } else { std::cout &lt;&lt; "not found"; } }
Well, of course I can write something similar :) I'm thinking about this extension since I want something nicer. Something where the syntax does not allow you to, e.g., access row[0] if the rows are empty.
Yeah, this is what currently have to write. But don't you think it looks nicer with my extension while at the same time being less error prone? I.e, did you never mix up the comparison in (I != rows.end())? It is the same as the range-based for loop. I could also write this by hand, but it's much nicer with the syntactic sugar.
Thank you, will be noting down all these points and see how they can impact what I am trying to improve.
So much for differences in personal experiences. When I started more than 35 years ago, everything was int and double. But since 25 years or so I pretty much hardly ever use ints: the problem domain that I am working in deals mostly with naked or wrapped natural numbers (in the mathematical sense, i.e. unsigneds), or integers mod 2^(n). And in particular the latter is a PITA because of undefined behaviour all over the place, whereas the former works like a breeze if used consistently (i.e. naturally). If you like to experience what's happening if you mix both and abandon consistency, run Boost's unit test with compiler warnings enabled at production level (e.g. msvc /W4) - but I guess I'm preaching to the choir.
I think this is similar to what was proposed by [N4127](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4127.html), which was rejected because "[EWG found the motivation for the proposal insufficient. ](http://wg21.link/ewg140)"
&gt; I could, but you just added a whole lot of extra boilerplate just for that sugar. If you are going to use `std::find_if` only once in a codebase, I think that you are better not using it at all and just doing a for-loop for that one case. Principle of least surprise, all that... Besides, it is already available (and has been for a long time) in [boost](http://www.boost.org/doc/libs/1_61_0/libs/range/doc/html/range/reference/algorithms/introduction.html)
indeed, that would be nice :)
Thanks! This is indeed a bit similar. My proposal is a bit more general but of course quite immature at the moment :) 
We needed it so we wouldn't break the builds of people who were using notelemetry.obj in update2. It should have no effect when running against update 3
You can use a macros to imitate it: #define RANGE_IF(element, range) \ if(!(range).empty()) \ if(element(*(range).begin()); false) \ { \ } \ else RANGE_IF(const auto&amp; row, rows) std::cout &lt;&lt; „found: “ &lt;&lt; row; else std::cout &lt;&lt; „not found“; Or even easier and without C++17 if you can live with mandatory braces and semicolon after }.
Where is the implication that there is no understanding of basic computer science?
Failing to write an O(N log N) implementation isn't an assertion that O(N^2 ) is the best there is. I don't know why you would call into question someone's computer science knowledge simply because they didn't write the asymptotically faster version. The implementer is simply saying this is "good enough". The article is saying "why settle for good enough if you can get something much better with less effort".
N^2 sorting is not "good enough", it's the wrong solution to the problem. If you think N^2 is good enough for anything but the smallest N, you don't understand basic computer science.
That's a great blog post, very very interesting! Thank you very much for posting this, nice! (I don't agree with your opinion about C++ exceptions though, try to be a bit more open-minded there ;-) )
&gt; it's the wrong solution to the problem Funny enough, it seems to work correctly and satisfies the requirements of the problem. Who'd have thought.
True, but the same could be said about the range-based for :) And I think it is quite hard to get the macros right. In your example, e.g., the range parameter is evaluated twice. (And empty() does not work if we only have begin() and end() but these are just minor details)
Prove it.
&gt; finding and using only the first occurrence of something matching in a container is not as common as iterating over a container using a for loop Sure, that's why I also showed the examples with unique_ptr and optional that fit nicely into the scheme. &gt; The other problem I have is that sometimes you want to use std::find_if(), sometimes std::find(), sometimes you want to determine if std::optional has a member, etc... I'm not sure what you mean by the latter part. You mean a value? See below. &gt; The range-based-for is pretty simple syntactic sugar; there's no need for the compiler to determine how to translate it; there's only one way. Given appropriate definitions of std::begin() and std::end(), there would also be more or less only one way for the compiler to translate my range-based if. (There are BTW also multiple subtly different way to translate a range-based for. This is why they had to change the semantics slightly in the for upcoming C++17). My proposal would boil down to something like this (modulo bugs, figuring out where to put const, etc): template &lt;typename R, typename F, typename G&gt; void if_non_empty( const R &amp; r, F&amp;&amp; f, G&amp;&amp; g ) { const auto &amp; b = std::begin( r ); const auto &amp; e = std::end( r ); if ( b != e ) { f( *b ); } else { g(); } } [This is the lambda way, but I'd like to have the syntactic sugar] Given the following, it would directly work with unique_ptr namespace std { template &lt;typename T&gt; T* begin(const unique_ptr&lt;T&gt;&amp; p) { return p.get( ); } template &lt;typename T&gt; T* end(const unique_ptr&lt;T&gt;&amp; p) { return nullptr; } } Demo: auto p0 = std::make_unique&lt;int&gt;( 1 ); if_non_empty( p0, [](auto i){ printf("%d found\n", i); }, [](){ printf("not found\n"); } ); 
That still sounds to me like you want the bool operator, you just want it on the iterator you get back, rather than on the collection it is iterating over. A lot of STL operators have this by returning back an npos or end operator.
cvec&lt;T, width&gt; const a0(cread&lt;width, aligned&gt;(in + 0)); cvec&lt;T, width&gt; const w3(diff02 - diff13); of course i'm still a c++03 hacker as I haven't totally gotten used to braces initialization. 
Fortunately that syntax doesn't work for pointer types, and those are the only ones where C++ casts are actually safer and not just easier to spot/grep/etc.
To add to this, the reason that `void()` is used (and not `0` or `nullptr` or something else) is because you can't declare a function with an argument of type `void` (`void*` yes, but not simply `void`) so you can be sure that you are getting the built-in comma operator and not any user-defined one, even if there is an unconstrained templated `operator,`.
How is it not backwards compatible? It wasn't legal syntax, so it cannot be a breaking change to make it legal..
While this might be interesting, I any not sure if range if would work unless if translate to something like: container c; std::copy_if(rows.begin(), rows.end(), std::back_inserter(c), predicate); if (c.size() &gt; 0) { for(auto row : c ) { std::cout &lt;&lt; "found: " &lt;&lt; row &lt;&lt; '\n'; } } else { std::cout &lt;&lt; "not found\n"; }
It would be great if clang-rename could work without saving the buffers first. I guess that will get implemented at some point. Thanks and keep up the good work.
Yes, but at the moment I'm putting more effort into clang-refactor, for which there would be a proper integration. clang-rename is just a prototype, even though it's quite old. It has been practically dead until I began my internship two months ago.
Sadly this is not correct: * The example I provided is IMO quite dangerous and does not work with a `static_cast` * If there is any alias involved in the pointer it still works perfectly: using iptr_t=int*; double* dptr = nullptr; auto iptr = iptr_t(dptr);
I stand corrected, I hadn't thought of that. :-]
Can clang-include-fixer remove includes that are no longer needed due to refactorings?
I'm not sure if it can or not... but include-what-you-use ( http://include-what-you-use.org/ ) can... I think.
Sorting in N^2 is too slow for almost everything, period. I don't think you realize how slow N^2 is.
Weird article. I don't think the criticism sticks. &gt; This logical understanding does not however match with the implementation. People who think that should look more closely at the implementation then. If they think "a method has to have `virtual` to behave polymorphically," then there is an education opportunity. &gt; The use of the _impl suffix also makes me suspicious... Me too. Do use a better name if you can. Unfortunately, naming things is hard. I've worked on teams that use a `do_` prefix, and it works well enough, though I agree that it feels kludgy. But it became a convention that was immediately recognized and so helped communicate what was going on, contra to his first point. &gt; One-level only Not really. The goal here is for the base class to maintain its invariants by insuring methods cannot be skipped or called at the wrong time: e.g: class base { public: void do_something(...) { pre_do_something(...); do_something_impl(...); post_do_something(...); } private: virtual do_something_impl(...) = 0; }; Well, there is no reason nvi couldn't apply to pre and post conditions as well: class base { public: void do_something(...) { pre_do_something(...); do_something_impl(...); post_do_something(...); } private: void pre_do_something(...) { // important setup pre_do_something_impl(...); // more important setup } void post_do_something(...) { // important cleanup post_do_something_impl(...); // more important cleanup } virtual do_something_impl(...) = 0; virtual pre_do_something_impl(...) {} virtual post_do_something_impl(...) {} }; Mind, there is obviously more complication here, so you'd really want to consider whether this truly models your problem before implementing it, but it is available. It seems that this approach would have handled his counterexample. Moreover, it avoids the potential LSP violation that his solution has. And I think the first commenter nailed it when they suggested avoiding concrete inheritance in the first place. I've come to the conclusion that if a class can be instantiated, you shouldn't inherit from it, you should extend it via composition. Whenever I feel like I needed to inherit from a concrete class, it was a sign that the class hierarchy was badly designed. 
What you want is essentially some syntactic sugar for the maybe monad. Instead of sugar for specific monads (async/await included) I'd prefer to have a syntactic sugar for all monads in C++.
Interesting, might have to persuade my boss to let me include these in our CI builds...
Yup on all accounts. At that point, I start asking myself, "man this hierarchy is unwieldy, is there a better way to do this?" I notice that there is a pattern forming: `pre_do()`, `do()`, `post_do()` and realize that what I am trying to do looks a lot like a resource. Often I find I can refactor things into simple value objects that "pre_do" and "post_do" whatever they need to do in the constructor and destructor and use simple composition to build reusable components that are much more readable, maintainable and easier to work with. Even when inheritance still seems like the right solution, I haven't seen the need to go beyond two, or rarely three, layers. And that right there is the elephant in the room, he seems to be trying to solve problems created by having a deep UI hierarchy rather than finding a better design for a UI library. 
I figured out why: the numbers you insert does not have good entropy. As mentioned in the article, `emilib::HashSet` requires good entropy in the input, or it will produce bad results. Try using `emilib::HashSet&lt;int, IntHasher&gt;` where `IntHasher` is http://pastebin.com/d4B6sLPC For that I get: ``` std::unordered_set: 7.847s emilib::HashSet + IntHasher: 1.280s ```
clang-refactor will be a nice addition. On the subject, for instance in the case of *Extract Method* refactoring, it will be nice to: * be able to specify how data are exchanged * *in*: as in-parameter (copy/const reference/pointer), attribute * *out*: as out-parameter (reference, pointer), attribute, or returned (tuple may be required) * *sink*: `unique_ptr`, or by value depending on data nature. * the syntax for returning something may be the new C++11 syntax, or even the improved C++14 one * be able to specify naming policy (in case we are using attributes, we may want to prefix with `my`, `m_`, ...; some even have naming policy for parameters) * be able to extract a generic template function, even when types are known
I'm a casual qt-creator user... afaik there used to be knob to ignore all break-points while debugging and then it vanished at some version? Has this functionality made it back to qt-creator?
Awesome. I always thought that hopscotch hashing was one of the most elegant of the hashing strategies. I am glad to have a C++ implementation in a header-only library. Just a quick question about the overflow list? My understanding is that it is used if the H buckets around the hash position are all full. Is it really needed. From page 2 of the paper - &gt;Finally, notice that if more than a constant number of items are hashed by h into a given bucket, the table needs to be resized. Luckily, as we show, for a universal hash function h, the probability of this type of resize happening given H = 32 is 1/32!. 32! is basically 2^117. One thing you might want to think of is not using an overflow list and just resizing in the (extremely rare) instances where the bucket array is not full but H buckets are full. Just my thoughts, I am interested in what you say seeing you actually implemented it.
&gt; Boost Serialization Library Sure but it doesn't provide a "give me a type and I'll write the serialization code myself" feature AFAIK
I suppose you could create a new reserved word e.g. for { } forelse {} Still breaking, but probably a more tolerable break. I'd personally love this behaviour, if I believed such a proposal had any chance at all, I'd write one up.
&gt; Sometimes, when some issues are reported, we spend a lot of time debugging the issues eventually landing on some common area. If you have done this enough times that you know what those areas can be, those are the areas where you must add monitoring code. Either add extensive logs allowing you to monitor data traffic and state when execution reaches there, or add these logs in a way that they are disabled by default, and by a runtime switch (that is, you shouldn't have to recompile to do this, preferably you shouldn't have to restart the application either). &gt; The problem is particularly acute when new members join the team and has to spend considerable time debugging issues. You should only need to tell the new guys "watch the log for entries with this particular text token/label/location/???; they are only generated in one place"). &gt; I was thinking of developing some kind of intelligent debugger, something which can give some hints where to debug based on the nature of the problem, thus reducing turnaround time for bug fixing. I think the solution that would fit best would be a log analizer, looking for patterns in the application event log. &gt; I know it would be complicated, so just wondering if it is worth the effort. Log file line-by-line reader, followed by regular expression-based rules finding relevant lines, extracting information and compiling reports based on that ("no inconsistencies in event processing; no inconsistencies in SQL access; all config files loaded correctly; could not find the Foo and Quux module log entries; etc). Your tool should also not suggest anything, only provide reports. If it suggests anything, new programmers (especially new guys) may waste a lot of time looking in a wrong place because the tool "suggested it". &gt; Has anybody worked on something similar and has some prior experience? Do not make a tool that attempts to draw conclusions for you; it is difficult to get right (most smart tools are not _that_ smart) and most times not worth the effort. Instead make tools that make your current process easier (better data visualization, automate your repetitive steps, keep things that should be kept in mind clearly visible etc).
Love this article overall - one quibble: &gt; clang-format is extremely flexible! Now, I keep hearing that, but I'm on my third project in a row where we were unable to reproduce the coding style of the team with clang-format - and yes, each time someone smart has said, "Oh, clang-format is very flexible," and almost immediately gone to, "Of course it doesn't do that!" And none of these format requests were particularly unreasonable. For example, one serious sticking point when I personally was the one trying to set up clang-format was enumerated types - the project had quite a lot of enums with fairly long names, and wanted one per line, so we could easily see them. Unfortunately, clang-format always tries to fit as many enum names into a line as it can, resulting in a table with one, two or occasionally three enums per line - and even in my early "all humans must use clang-format" fervor, I understood that this was both hard-to-read and ugly. I personally would be extremely happy if I could just run a tool on my code and always get perfect formatting, but clang-format isn't that tool, and my belief is that it will _never_ be there for many projects. clang-format is aiming to create a canonical format, which means that all of the input formatting will always be thrown away (unless you protect regions from clang-format with a directive, but that's too horrible for anything other than intermittent use, and it's "all or nothing" - you can't say, "suppress just this one rule here"). For many teams with existing code, the dramatic reformat is not an option. 
Another unfortunate thing I just discovered is that, if you're on OS/X, there's no homebrew install formula for Clang extra tools. (For non-OS/X users, homebrew is similar to apt-get for Debian et. al.) If some enterprising person read this and had the spare time to whip one up, it'd be a service to both communities. It's a shame it's not there, because OS/X uses the clang compiler by default, (gcc is aliased to clang!) which probably makes OS/X users a plurality of the entire clang user community.
Doesn't AfterEnum inside BraceWrapping do exactly what you need? (one enum value per line)
Also, couple more things on clang-format. You can suppress it on portions of code using // clang-format off and // clang-format on. I found this especially helpful with nested macros, which clang-format doesn't indent. Some of the options, such as initialization list style, are not directly configurable but tied to BasedOnStyle (LLVM, Google, Webkit, GNU, etc). So it takes a bit of experimenting on which of the styles is matches closest and then overriding what can be overridden. We managed to get pretty close approximation, and while there are always some style tradeoffs and some getting used to, in my experience the benefits outweigh those by huge margin. Especially with code that's heavy on lambdas.
As I recall, the IWYU project ran into problems that couldn't be fully solved with just the textual inclusion header system and that was one of the motivations that went into clang's modules. You can get full 'include what you use' behavior with properly defined modules. Unfortunately the module definitions being distributed seem more interested in ensuring compatibility. For this reason I think it would be good to add a 'legacy export' feature to clang modules, so that we can distribute module definitions that allow both cases, and compiler users can just select whether they want include-what-you-use behavior.
Re: enum, http://stackoverflow.com/questions/23072223/clang-format-style-options-for-enums
I currently don't. There's more stuff to come soon, I'm about to publish design docs this week.
That's true. However, there is clang-format formula.
No, it can't. clang-include-fixer only purpose is to add missing includes. IWYU is intended to be a temporary solution for such things.
Great point! That's exactly what the current state of IWYU from what I know of is.
I can see your point. That's true, if your code style is really different from what the pre-defined are it's not possible to make it work. I updated the post. Thank you for the note!
Thank you for the feedback! That's a great point, I will definitely consider these options while implementing Extract Method!
It _does_ control one-enum-per-line. It's kind of strange that this is tied to the enum brace formatting.
Great! I sent an email on Sunday, though haven't heard back. Sadly my company won't send me this year so I would go myself if I could get there as a volunteer.
Sorry, typing from phone, please pardon me if it's not relevant or repeated. Personal favorite : Bo Qian's Video Tutorials https://www.youtube.com/user/BoQianTheProgrammer https://www.ntu.edu.sg/home/ehchua/programming/index.html#Cpp www.cppquiz.org www.interqiew.com And of course, the very nice www.cppreference.com
forelse is then a new keyword, which also has not much chance. One could just use "for else" or reuse an existing one. For example "default". Or maybe "else default".
Nice introduction, thanks!
yeah its too bad that I didn't add much on third party dependencies. But that would be something I would want extend my talk on :) 
What are requirements on the key and value types? Default constructible, copy constructible, copy/move assignable? It would be also good to include some corner-case types (non-copyable, for example, like `std::unique_ptr`) to your test suite. Update: and what are exception guarantees?
I would love to see something like that. I've tried cutting my teeth on ExternalProject_Add but never really got it to work with my project's dependencies.
Yes it does. We can't have a meaningful discussion about algorithms without agreeing on this.
I kind of got it to work with github by specifying the git hash of the version I want to get from which repository, which kind of works if I want to track their master branch closely. Obviously, this is not a dependency management system. If two libraries depend on two different git hashes you are on your own :/
&gt; Unfortunately, clang-format always tries to fit as many enum names into a line as it can &gt; unless you protect regions from clang-format with a directive, but that's too horrible for anything other than intermittent use, and it's "all or nothing" - you can't say, "suppress just this one rule here") I don't find this "too horrible": // clang-format off enum class triangle { one, two, three, four, five six, seven, eight, nine }; // clang-format on In particular, since clang-format cannot be perfect for things like this, because it would need to know more about your code than what is actually there. &gt; Unfortunately, clang-format always tries to fit as many enum names into a line as it can, resulting in a table with one, two or occasionally three enums per line Do you have a link to the bug / feature request report? &gt; clang-format is aiming to create a canonical format, I moved my projects to clang-format 2 years ago, and at the beginning I filled a ton of bugs/requests (10? 50?, I even implemented one). Their response time till merged commit in clang trunk was of about ~2 days ! After 2 months (and clang format was brand new back then) it could already match my style! Since my style has nothing to do with google or llvm, and since clang-format does have a lot of configuration options (albeit maybe not the one you want, yet), for me your statement is false. &gt; Now, I keep hearing that, but I'm on my third project in a row where we were unable to reproduce the coding style of the team with clang-format Complaining on reddit only brings you that far. Clang format is developed by _people_, with their own interests, typically for their own projects, and sometimes in their free time. Fill good bug reports, and if you get lucky somebody might fix it for you (for me they did fix a ton of them very quickly). If they don't fix them quick enough for your, a lot of people have committed to clang-format (even one off commits). Getting started isn't that hard, the code base is well documented and easy to follow. So if a particular option is very important for you, implementing it will take you less than a day.
In modern C++, std::string("hello")[-1] produces an index out of range error (at runtime) in this situation, which is pretty well the behaviours I'd expect. I also found it amusing, that an article expounding the benefits of high-level languages, presumably written with one of those languages has, in Chrome at least, "[an error occurred while processing this directive]" as the last of the article's text. Guess those high-level languages aren't so capable after all.
My coding style is completely different from all the predefined ones, and I still could make it work :/
But would they have been on a 20MHz 80386? 
One more thing: What do the `-E` and `-P` options stand for? Give me the mnemonics, please; otherwise, I won't remember. Spell them out in their long forms (if they exist) on the slides.
I think we are talking past each other. My suggestion is for `std::unordered_map` to be internally implemented as `std::vector&lt;std::unique_ptr&lt;Node&gt;&gt;` (+ extra inline data) so as to regain memory stability but still be on top of an Open-Addressed implementation. The idea is that the Open-Addressed implementation would inline the pre-computed hash (for example) to allow fast (in L1 cache) checking of whether the hash matches before following the pointer indirection to see if the key itself matches, thus hopefully having less indirections. It might also save up some of space in the node itself... but that's node clear (for iterator stability, I expect you will need a back-pointer to the hash-map + the index of where you currently are, which 2 pointers worth of data). (I've also thought of, but never implemented, stable sorting hashes within a bucket, so as to be able to stop look-ups earlier in a given bucket; it's unclear how much this would gain in practice)
cmake is an opaque monstrosity. I wrote an MIT License tool, zmake, that does the same thing as cmake*, but with only 5200 LoC, 100x faster, and more precise compiler options. If someone wants to use it, lemme know since I've been pretty lazy about uses other than my own. *currently only supports Visual Studio and Makefiles, since I've haven't needed xcode or other IDEs for the 6 platforms I'm currently targeting, but there's no reason it couldn't.
There is no long form, `-E` doesn't stand for anything as far as I know, and `-P` stands for something completely unhelpful for remembering what it does. --- `-E` (for "Execute" maybe?) is just how you access the command line tool replacements provided by CMake for scripting. CMake provides these so that you can write scripts without depending on the shell environment. For example if you want to copy a file, instead of using POSIX's `cp`, or Windows' `copy` commands, you use `cmake -E copy`. Say you need a custom build step that involves expanding a tar archive. `add_custom_command()` takes arbitrary shell commands to be executed during the build. You can use `cmake -E tar` instead of POSIX tar in order to achieve the 'cross-platform' aspect of CMake. add_custom_command(OUTPUT ${LIBFOO_TAR_HEADERS} COMMAND ${CMAKE_COMMAND} -E tar xzf "${CMAKE_CURRENT_SOURCE_DIR}/libfoo/foo.tar" COMMAND ${CMAKE_COMMAND} -E touch ${LIBFOO_TAR_HEADERS} WORKING_DIRECTORY "${CMAKE_CURRENT_BINARY_DIR}/include/foo" DEPENDS "${CMAKE_CURRENT_SOURCE_DIR}/libfoo/foo.tar" COMMENT "Unpacking foo.tar" VERBATIM ) `-P` (for "Process") runs CMake scripts, without having anything to do with the usual config/generate steps. For example: $ cat &gt; hello.cmake message("Hello, World") $ cmake -P hello.cmake Hello, World
nicely done, thank you
very interesting thanks! I've always preferred SCons over CMake due to its simplicity and the fact I already know Python. I never thought CMake would be so much more popular then SCons, theres also more hits on stackoverflow (although that could be since its more complicated). The hardest part for me is setting up all the dependencies for my projects, I think this is the best CMake guide I've seen so I will give it a try.
I think the only question that should be asked is: As a C++ programmer would you be willing to support and develop a framework that leads to the following bit of code needing to exist? FieldListReadIterator itr; itr.start(field_list); for ( ; !itr.off(); itr.forth()) { ..... } If so then you're HIRED! 
&gt; In modern C++, std::string("hello")[-1] produces an index out of range error (at runtime) in this situation Maybe you get an assert in debug builds for this, but `std::string::operator[]` does _not_ throw – the standard requires it. If you want an out of range error you must use `std::string::at()`.
I didn't say it was irrelevant for debugging, I said it was irrelevant for performance testing. Times measured with -O0 compiles are not at all indicative of the times your user is going to experience.
Of course. A hell of a lot slower, in fact.
Nice presentation. On my github repo (madduci/CppDevLibs) I have an example of ExternalProject used to build Boost, OpenSSL and Google Test from sources
The problem I have with c make and many other build systems is they know too much about when I am building, when I want to do something outside of the normal it's a configuration nightmare.
&gt;Development Status: &gt;This code is fairly stable, well-tested, and suitable for casual use, although currently lacking documentation. No promise is made about support or long-term stability. This code will evolve without regard to backwards compatibility.
You are correct. I designed a solution for this some years ago: http://public.kitware.com/pipermail/cmake-developers/2013-July/019337.html and I explored what CMake generally is missing to address cross-compiling needs: http://cmake-developers.cmake.narkive.com/9GPy2H5Z/modern-cross-platform-buildsystem-design-requirements if you want to be constructive, and you can write C++, then help make it happen. 
Thanks! I guess "Third Party" is a bit misleading for the modules included in CMake. I changed it to "Modules (CMake Extensions)" 
Hi Thanks for your critique. I will update the slides with more links to documentation soon and also update the sample repository to show the single steps more explicitly (with links from the slide). Your other suggestions are great as well :D Seems like I still have a ways to go ! 
Ah very cool :) 
Hard for anyone to tell if they want it without links and source.
It is beautiful to listen to him talk. What a brilliant understanding and mastery of both theory and practice. He is a master.
Got it! :)
&gt; I've tried cutting my teeth on ExternalProject_Add but never really got it to work with my project's dependencies. ExternalProject is great, but the IDE integration for that feature seems to be lacking (at least I never got it working well with Clion or Qtcreator).
The more important reason is that the properties get attached to the target, and moreover the target can be exported with this attached information such that users of your package can import a target that "knows" what include directories, compiler flags, and libraries are needed to link to it.
Ah ha, so is this the same mechanism that `find_package` uses to export things like include directories etc?
Ctrl-F "Why use ranges?". 
Awesome response!
Technically, cmake and zmake have an intersecting set of platforms, but neither supports all the platforms of the other. Last I checked cmake only supports Win32 and x64 in Visual Studio. I use zmake for two additional platforms with Visual Studio slns. In terms of compiler features, zmake supports more than cmake, because zmake gives direct access to the compiler configs, whereas cmake obfuscates it with its own pseudo language, and is more error prone because if you assume some option enables a compiler option on all your platforms, you would be wrong. Also zmake's execution time is 100x faster, making it not a noticeable impact to the compile times, unlike cmake which does noticeably affect compile times.
Time to make a breaking fork so we can go scorched earth on the synax?
Indeed. I'm not sure Dv1 had much life in it either though (sadly).
What would you replace it with, if you had the opportunity to modernize a project written back in the 90s? Boost does not seem to be a good fit, because of its use of templates, which would mean a lot of rewriting. 
cmake is hundreds of megabytes. I doubt it's makers could say they fully understand it. That makes cmake dauntingly large for an individual, which is literally the definition of monstrous. Also, if having to ask for the source is too much of an inconvenience, then it's unlikely that you would put it to good use as an early adopter. As such, I stand by my original statement: cmake is an opaque monstrosity; if someone wants to use a better alternative, let me know and i will make that happen.
So if I push it to github, you'll check it out?
Sure
STL, though I don't have conviction. Current plan is to rewrite it in rust.
Very cool. I've never seen some of these before. Great share!
you've probably seen these by now, and at least one of them builds on ncurses anyway, but: * https://github.com/nsf/termbox * http://tapio.github.com/rlutil/ * http://cwidget.alioth.debian.org/ some day there will be C++ wrappers + widgets for http://www.leonerd.org.uk/code/libtickit/, but until then ncurses is probably the most complete option.
I'm happy with it and have been using qt-creator for several years now. My only problem is that the Clang code model, as much as I'd like to use it, doesn't highlight my company's code correctly.
&gt; Also Visual Studio 2015 and 15 I completely understand VS's versioning scheme and I'm hardly ever confused by it, but this made me laugh nonetheless, it's so ridiculous. None of my colleagues has any clue about VS versioning and all of them are always horribly confused.
My objection is to this: &gt; it says cross platform, but its impossible to setup a project which uses this complicated syntax for generators to build components for use in the later build when working on a cross platform build. You've written this as if there's in inconsistency between being 'cross platform' and supporting this particular kind of cross-compilation build step. But one can only interpret this as a contradiction if they misunderstand what 'cross platform' means. If you're not intentionally implying that there's a contradiction between CMake being called a 'cross platform' build system and its lack of support for this kind of cross-compliation build step, then good. I just want to make it clear to everyone that there's no contradiction.
Unrelated but visually witnessing both the `bind` and the lambda get *completely* optimized away was pretty awesome.
On the one hand there can be a lot of cache misses if the types you're searching are small. On the other hand the perf is predictable, and can't cause denial of service due to an attacker figuring out your hash function and making a hashtable linear.
&gt; which features you would like A few years ago I wrote a tree where each node had an integer field that I named weight. I wrote it myself because I couldn't find such a thing available anywhere else. (I know, there's got to be one *somewhere*, but I had no luck in the search.) The weight of a node is unsurprisingly calculated as: 1 + weight (left subtree) + weight (right subtree) Of course, an empty subtree has a weight of zero. Weights do have to be updated on insertions, erasures, and balancings, so the overhead incurred won't be to everybody's taste. I balanced according to weight rather than height, since the numbers were sitting right there. With all that, it was simple to write an `at` function allowing the tree to be indexed in a method that (in the user's view) works like `std::vector::at`. Lookup time is O (log n), where n is the number of nodes.
I am an electrical engineering major at a large American state university and I need help after our school swapped our programming courses from Java to C++ and I took a gap year. Any tools you can give me in addition to the above site and the reference on the sidebar would be appreciated greatly!
Sidebar: &gt; For C++ questions, answers, help, and advice see r/cpp_questions
https://en.wikipedia.org/wiki/Weight-balanced_tree
The Core Guidelines Checkers are built on top of the C++ Code Analysis that's in Visual Studio. More on the code analysis here: https://blogs.msdn.microsoft.com/vcblog/2015/02/24/cc-code-analysis-in-vs2015/ 
Was the list of books on SO not sufficient?
No. 
Could you provide an example how class template deduction can "extract" the return type and argument types of a lambda? I can't think of a way of doing this without using decltype + a function.
Repost to /r/programmerhumor pls
I wish the answer was `twiceThePrevailingWagesForMyPosition`, unfortunately I can relate too much to `notEnough`.
Your post has been automatically removed because it appears to be help/homework related. If this has been in error please message the moderators. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Good article, thanks for sharing!
For me, it's `const auto &amp;` whenever possible and `auto &amp;` when I want to change the item. I see no point in not using a reference, really (the `auto &amp;&amp;` tip was interesting, though). I've seen somewhere someone propose this refinement to the `for` syntax: // Equivalent to for (auto &amp;&amp;x : container) for (x : container) do_stuff(x); Does anyone know what I'm talking about? Did anyone put this forward?
I expected to see some pre-analyzed Clang code-base to play with for trial though...
I just wanted to say that I bought Coati, and it works pretty much as advertised. I use it with 60k-loc project. It's nicely animated, fast, and pleasant to use. The indexing takes some time though, which stops me from using the tool all the time. It should be very useful though, if I ever need to read someone else's code. Btw, you guys should do a web based version of Coati with git integration. See: https://www.reddit.com/r/cpp/comments/50sl50/is_there_a_web_based_code_browser_that_allows/
I know MS is aware of it and I actually expected them to clean up that mess a bit or at least don't continue with it / make it worse, so I was expecting them to do it differently for "VS 15" - but the mess keeps going on ;-) Explaining it (over and over again xD) certainly helps but I'd say 90% of my confused colleagues you'll certainly never reach because they're neither on /r/cpp nor reading the VS Blog. 
There is https://code.woboq.org/ for example. Supposedly, google had a powerful clang-based indexer that they planned to open source. They never did afaik
Thank you, glad to hear that! We will look into speeding up the indexing with out next release. The web based code browser is a good idea. We already think about doing the indexing server-side and maybe we might also add a web-based front-end later.
Mozilla's DXR looks very promising. I've been wanting to try it, but sadly didn't have the time to do that so far.
That's a great suggestion, thank you. I opened a feature request: https://github.com/CoatiSoftware/CoatiBugTracker/issues/175
Doxygen will generate html pages for your code so you can navigate files, classes, variables, etc. That might be what you remember.
Reported this as spam...
I think that's one of the best articles I've read on this subject. Very thorough. Thanks!
Great article. A couple of points: &gt; As a side note, the const before std::string is necessary because keys in maps are immutable. Without the const, the code would fail to compile. It could be worth mentioning that this is only due to the variable being an lvalue reference. If one were to use `const std::pair&lt;std::string, int&gt;&amp;`, then it would compile and it would make tons of unnecessary copies. &gt; Nevertheless, as of 2016-08-17, there is no compiler supporting this feature. While not released, Clang [does have this implemented](http://melpon.org/wandbox/permlink/qTXKXFFbX92FYR8R).
Thanks, I have updated the article to mention that the development version of Clang partially supports structured bindings. 
Casting an unsigned possibly 64 bit value to a signed possibly 32 bit value, what could go wrong? &gt; 1:02:50 for where they just give the short answer: “sorry” Since I have to deal with ~3GB data sets and don't care for people using -1 as uninitialized index value, I can only say "Thank you".
The if is only mandatory if you create move operations that result in that state. If you don't want to do that, then don't do it. It is only possible to have an res if you allow it. Of course, if you do allow it, then you must support it in your destructor. I don't think the move destructor's behavior is obvious from its name. Are you suggesting that the compiler uses it when destroying an object in the moved-from state? We don't want to/can't require the compiler to know if an object is in the moved from state.
OpenGrok
&gt; For me, it's `const auto &amp;` whenever possible I changed my tune on this - I only now use `const` for auto variables when it is essential to the meaning of the code, not just because I _can_ put it in - which means that I rarely use it at all (on local variables - I'm big on const correctness in general). So if I'm iterating over a `const` collection, I _never_ use `const` in my auto variables - because it gives me no information when I read it and it doesn't change correctness. For years I believed the compiler was able to do a better job with local const variables. Someone smarter than I took me to task on this - got me to measure this and look at the code. It is just not the case. This strategy gets rid of a little cruft in my code and it means I have to think a little less about unimportant details when I write. It's marginal, but it's a tiny improvement. 
It really isn't that bad, to be frank. You spend a few minutes thinking about it at the start, and then never again. But it really helps to be all "modern C++" in your thinking...
Yes, and the argument that "it's confusing" really doesn't wash if you can always do it that way and it'll just work. It seems like _less_ confusion if "there's only one way to do it".
It's not a matter of helping the compiler. Assuming the variables are immutable by default and only making them mutable when necessary is safer than assuming they're mutable by default and it's less prone to errors. Rust does just this and I think it is the right approach. Edit: in fact, it's actually more a matter of letting the compiler help _you_ than the other way around.
Yes there is. It's called Emacs. :-)
Yeah, I love how amazing the optimizer is. Maybe 2017 is the year of the c++ revival^tm ;)
&gt; Therefore, I see no reason for using const auto. Use const auto&amp; instead. If you're working with primitive types, they are smaller than a reference anyhow, and it's better to have a fresh copy as it makes compiler optimizations easier.
For me, it's the obscure syntax (auto&amp; vs auto&amp;&amp; vs auto, etc) with the added caveats mentioned in the article - proxy objects, copyable objects, etc. It feels bolted on. I'm sure i could get used to it. i use vi and am fluent in perl so I'm used to obscurity and unexpected behaviour. At this point, the clarity of languages like ada are even more appealing and I'm seeing why languages like go, D, nim, and rust are gaining traction.
But the `const` argument doesn't make sense here because it's a comparison of `auto &amp;` and `auto &amp;&amp;`, neither of which is `const`.
Is there any chance we see structured bindings in VS "15" /u/spongo2?
That's a strange thing to say, because those languages don't have the features that this syntax handles. So I don't think it's a very meaningful comparison.
Cloud9 https://c9.io - I've never used it, but it used to come installed on the stock OS on Beagle Bone Black embedded development boards as a web service. Apparently it can also work in a more cloud/Internet way, instead of just as a dedicated web service. They were setup for Python and doing embedded stuff with python seemed weird and limited to me so I only poked around on it once or twice.
&gt; The if is only mandatory if you create move operations that result in that state. That is the natural move behaviour of pointer based resources. You take ptr value and set the source to null. Maybe it is useful to go down to basic semantics. There are two different kinds of object setups possible. 1. "Classical" owner without move semantics. The internal pointer to a resource is initialised on construction and freed on destruction. It is _impossible_ to create an object that points to invalid memory or null. This object can not be moved. 2. Same as above but with move semantics. Now there is a way to create an object that does not point to a valid memory location (almost always pointing to `null`). Using an object in an invalid state is of course UB and must never be done. However there is the *possibility* of bugs because developers are not perfect. We can not achieve both of these at the same time. An object can't be both movable and always guaranteed to be fully valid. We also can't require the compiler to know when an object has been moved from. However if we *do* know that the object will be destroyed then we can define semantics to move it instead. Suppose we had type `Foo` which is *not* movable but is "move destructible". A hypothetical (and awful) move-from-object-to-be-destroyed-constructor would look like this: Foo(Foo &amp;&amp;&amp;to_be_destroyed) { ptr_to_smth = to_be_destroyed.ptr_to_smth; // No need to set to_be_destroyed.ptr_to_smth = nullptr because its destructor // will not be run. } Then we could do this: Foo func() { Foo x; return x; } Here we know that `x` will be destroyed so we could move its contents elsewhere and deallocate `x`'s memory. This way creating a `Foo` object whose contents are not valid would again be impossible (because `Foo` does not have C++11 move semantics). This has severe downsides, the main one being that you can't pass `Foo` as an argument: Foo x; do_something(std::move(x)); // FAILS do_something_else(); Because `x`'s life time is not guaranteed to end on the function call (and the function might not even take `x`'s resource). All of this is of course academical and unlikely to go anywhere but it would be really nice to have both move semantics *and* objects that are always guaranteed to be valid in a single package. However the complexity from syntax changes alone make this pretty much a no go.
`auto&amp;&amp;` is const **correct**. It says, "same constness as the container", which is exactly what you get when you say `auto it = v.begin()` followed by `*it`, or when you say `v[i]`. Nobody ever looks at `v[i]` and says "that's const incorrect!". It doesn't add constness, but it doesn't remove constness either.
&gt; My point is simply that that argument does not hold for primitives. I answer that [here](https://www.reddit.com/r/cpp/comments/50rgbe/auto_type_deduction_in_rangebased_for_loops/d7783y5) (tl;dr `const auto &amp;` everywhere is more consistent and allows for easier refactoring). Aside from that, thanks for the explanation. I always assumed the cost of a reference vs a primitive was only the cost of a pointer.
Thanks!
&gt; this sounds horribly complicated with nonintuitive corner cases. Welcome to C++, you can pick up your complementary `vector&lt;bool&gt;` and overloaded comma operator at the door.
Ok, thank you very much for the reply!
In the macro expansion, define a dummy member function with an auto-deduced return of `std::declval&lt;std::decay_t&lt;decltype(*this)&gt;&gt;()` or similar, then decltype an invocation of that member function. There is more than one way to do that but you get the picture.
[This StackOverflow question](http://stackoverflow.com/questions/21143835/can-i-implement-an-autonomous-self-member-type-in-c) asks the same question and the consensus is that it's not possible. There is one answer (by hvd) that appears to work but it's invalid and only happens to work under gcc. (According to the comment, at one time it also worked with clang but in my testing clang now rejects it.) 
http://stackoverflow.com/a/21148117/541208
On code formatting, I was getting tired of pull requests that had a significant amount of formatting changes which obscure the functional changes. This makes it difficult to track down bugs in the history. In my last two projects I now require all changes to be formatted before they get merged. I've been using [astyle](http://astyle.sourceforge.net/).
clang should use libstdc++ by default. What OS are you using?
It's a dynamic compact bit array. That's extremely useful in an age where CPU cache is very important. I use it rather frequently. I've had to make special cases for it when doing TMP. It's a bit ugly, but it's not such a huge deal. The problem is that it simply should have been a separate container altogether.
Using auto everywhere seems like a bad idea. Especially when used with user-defined literals. They can easily clash among different libraries or lead to bugs if you mean one variable type instead of another. Otherwise, sure you get a compiled JavaScript but that just makes it easier to shoot yourself in the foot.
 I use it all the time and example function that uses it is [here](https://github.com/mhogomchungu/sirikali/blob/b125d6bfd723bf38bdad1f13d265f2726d515de0/src/siritask.cpp#L81)
honest question -- what does auto x = MyThing{}; do? As opposed to MyThing x{}; ?? Does it construct a MyThing, then copy it and give it to x? Does it create a unique pointer to a MyThing? I've used auto a great deal for assigning smart pointers, arrays, etc, but never considered using it as an alternative to the way stack-based objects are normally constructed.
 why do you leave so many blank lines in your C++? You seem to throw spaces in at random where they do no good at all, and then remove them elsewhere, like `}else{`. It's like you're trying to be as different from everyone else's code as possible...
You should probably edit your original post to have the fixed link.
Blank lines makes my eyes work less when going through code. When going through other people's code,i sometimes start spacing them out before i "dive in" to please my eyes. I suspect i need glasses. github makes the spaces stick out much more than my editor and i would probably not have them if my editor display text the same way github does.
For me this used to be a symptom of using a too small font. Remember, you don't fit any more text on the screen with a small and compact font if you feel the need to space everything out to read it comfortably.
If you don't mind using VS, you can go for the Clang compiler that's integrated with it. It's all ready to go at least. Otherwise, using Clang on Windows is not a fun task, and even less so if you try to get libc++ into the mix. You will need to change the include search paths (via `-Ipath\to\libstdc++\headers`). These headers will be wherever you installed MinGW or whatever. There are like 4 or more paths as well, which is really annoying. Honestly, I've been having much better luck running Clang in a Linux VM. Speaking of which, since the Windows 10 anniversary update, another option is running Clang from a bash shell. It's a simple `apt-get install` to install Clang and it should run without problems from a bash shell, which can access your Windows files from `/mnt/&lt;drive letter&gt;/...`. You'll have to enable the Linux Subsystem for Windows feature before you can run bash from the command line. For whatever reason, building Clang from source in the WSL bash shell was also extremely fast for me. It's really slow on a Linux VM.
Yeah I do. I find it easier to manage Cywin + Clang then MinGW + clang. That being said, Cygwin itself isn't the easiest to setup by itself. And I've ran into weird build dependancy issues before unrelated to clang that were due to Cygwin idiosyncrasies.
Is there a reason for the heavy use of lambda functions (e.g. inside _args)? +1 for using Qt!
That makes sense. Hmm, that might in fact be impossible. I've been on mobile all night so I'ven't a chance to test it yet.
How could i declare variables with "auto" otherwise? :-) On a serious note,its primarily just a style i like to use and to minimize pollution of local namespace as a practical matter.Most local variables i use are single character and "e" is the variable name that i use the most and these lambda helps in allowing me to reuse these single character variables multiple times in the same function without collision issues. 
You might be better off writing a full-fledged code generator that reads a file with specs. This has the advantage of doing exactly what you need without being hampered by the limitations of the c preprocessor. This is what Google protobuf does, and this allows them to use the same input spec file to serialize data to a variety of languages. As a nice bonus, the generated C++ classes have reflection.
Right, because when I review your code it is awesome for me to have to switch back and forth through the code to deduce "what the heck does 'e' mean at _this_ point in time?" That is just awesome. 
I think that the first example could be more confusing to newcomers. I've seen a lot of code with std::string = "Hello"; So suddenly changing it to auto s = "Hello"; could cause people to mistakingly have the wrong type.
The author of the article is either a troll or completely clueless (despite alleged 20 years of experience). His "points" are elaborate, but without meaning though he seems to have no idea what the embedded systems industry is doing. The article is bullshit. Advice: save your time, ignore it. 
The compiler interprets the intention, so it's as if you wrote MyThing in place of auto. If there is not enough information, the compiler will let you know, and you'll have to declare the type.
It is indeed, didn't realise Qt started upgrading their codebase like this
That's not upstream. That's my Qt clone on Github to validate a tool. See: https://steveire.wordpress.com/2016/03/19/aaargh-aaa-right-good-and-hygenic/
There is no such things as "C/C++", they're two different languages with hugely different concepts. For example: &gt; While C/C++ is slow to write, error prone, and frequently unreadable This may be true for C, but it's very wrong when talking about C++ in the age of 2016.
Good points. The article strikes me as an dumbing-down/quantity-over-quality type or article. There needs an article to be published, so let's do one. How else do people/the faculty recognize me/my work. I mean, clearly there went not too much thought into the article. Once a professor (working at a certain university, both shall remain nameless) told me about the faculty required him to publish a certain amount of articles per year to keep the level of salary... what can possibly go wrong!? Or maybe just a python fanboy/C++ hater? I don't know. 
Well, for one, the author totally crushes the whole tab/spaces battle by eliminating both. Pro move!
Sometimes you don't need to know what the type is, and making the type explicit by writing it is, actually, as error prone as using auto in your examples. For example: for (const std::pair&lt;std::string, int&gt; p&amp; : produce_map()) { ... } ~~This code won't compile~~. This won't work as you expect. Do you know why? `std::map`defines `const T` for the key type of `std::pair`. You see that we wrote `std::string` for the first template parameter, which results in a ~~compile error~~ temporary copy, because you lose the cv-qualifier passing a `const&amp;` to `&amp;`. Using `auto`, and letting the compiler deduce the correct type for the expression solves the problem, and makes it more consistent. for (const auto&amp; p : produce_map()) { :) } Thanks u/tcanens for the correction. `auto` can also prevent accesses to uninitialized variables. Once `auto` needs an initializer to deduce its own type, there's no room for variables without a value to be declared. Another important thing is `decltype(auto)`, which is really useful in templates. Basically, it's `auto` with `decltype` rules applied to it. That is, there's no cv-qualifier loss, because `decltype` preserves the cv-qualifier of an expression, whereas `auto` doesn't. So, why is it useful? This is great for writing function signatures, where the return type depends on the expression being returned. If it's an lvalue of type `int&amp;`, then the return type will be `int&amp;`. If it's an rvalue of type `const T`, the return type will be `const T`. `auto` would just strip off its constness and/or volatileness for anything. A ~~contrived~~ example for this case would be a function returning an element of a container and giving you read/write access to it: template&lt;typename Container&gt; constexpr decltype(auto) container_element(Container&amp;&amp; c, size_t i) { return std::forward&lt;Container&gt;(c)[i]; } std::vector&lt;int&gt; v { 1, 2, 3, 4 }; // decltype(auto) = int&amp; container_element(v, 0) = 5; 
You don't need to. You know what it's about by reading what the code is doing. It's the same idea of concepts: generalizing stuff so you don't need to be aware of every useless detail. It's a common thing on new languages, specially in Rust, where you only need to write the variable type when the deduction can't find an answer for an ambiguous expression. And still, you only write what the compiler can't deduce, the rest is deduced as normal. e.g. let there: Be&lt;_&gt; = light(); Where `Be` is a generic struct, and light my have implementations for types other than `Be`. the `_` part is left for the compiler to deduce. What he's doing with lambdas is also common in Rust. let three_evens = { let c = Container::from(something_else_outside); let c_evens = c.filter(|elem| elem % 2 == 0); c_evens.take(3) }; `{ }` is parsed as an expression. The result of `three_evens` is the value of `c_evens.take(3)`.
Yeah it's useful every time you're programming to a concept, not a type, like with iterators, when the type is long to .. uh .. type and/or is clear from the expression (and in many other cases). What I don't get is why it should be used for primitive types when it's more wordy and less clear. The "uninitialized variable" argument is a good point, but the compiler can warn here.
[The Definitive C++ Book Guide and List](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) epic List by stackoverflow users 
Good but a bit rough. I use it more as a reference, rereading the relevant chapter when I face the problem in my code
Tour of C++ is a better title for that purpose.
Everything that /u/redditsoaddicting said. If you can use the [Clang/C2 inside of Visual Studio](https://blogs.msdn.microsoft.com/vcblog/tag/clang/) (it uses the Microsoft code generator with the Clang parser), use that. If you can't use Clang/C2, use the LLVM-VS integration package: http://llvm.org/docs/GettingStartedVS.html. If you don't want to use Visual Studio, use Clang inside of the [Windows 10 bash shell](http://www.hanselman.com/blog/DevelopersCanRunBashShellAndUsermodeUbuntuLinuxBinariesOnWindows10.aspx). And let me know why you don't want to use Visual Studio (seriously--I'm really quite interested in why developers choose not to use VS when they are on Windows. Please contact me!) Edit: using Clang in the Windows bash shell produces Linux executables. If you want to produce Windows programs, that's not the way to go.
Thank you for the response.
Thank you very much.
I just started reading this last week! I am on Item 39 out of 42, and I can say I've already in the last week flipped back to multiple Items while working on my own code, and made changes based on them. I can't recommend it highly enough!
You shouldn't write `int`, for example, because its size might surprise you. Nor should you write float literals without the `f` suffix (because good practice and consistency). So things like that become redundant. Everyone knows `3.14f` is a float. But of course, `auto` isn't an absolute rule. You may still use `size_t` and `ptrdiff_t` in contexts they suit. &gt; The "uninitialized variable" argument is a good point, but the compiler can warn here. Getting warnings from compiler won't save the programmers' life, because it will still compile (if `-Werror` is not set). On the other hand, there's no way to avoid it when using `auto`.
&gt; You shouldn't write int, for example, because its size might surprise you. That sure isn't an argument for having auto deduce int from an integer literal, though. &gt; Getting warnings from compiler won't save the programmers' life, because it will still compile If the programmer lacks the discipline to heed compiler warnings, they're equally likely to lack the discipline to follow the always-auto style. You shouldn't write floating point literals without the `.f` (more to the point, you shouldn't write int literals in place of float literals), but that's a mistake that can happen accidentally (just as the missing initializer).
Your post has been automatically removed because it appears to be help/homework related. If this has been in error please message the moderators. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I found it very useful. At work we are still C++98 at present, though I hope to upgrade sometime in the near future, so I haven't used it in anger yet other than for some smaller personal projects. But like his older books, the practical guidance of when and how to use the new features is invaluable, and I'll definitely be referring to it a lot once I start using C++11/14 properly.
In my opinion EMC++ is much better than Tour of C++.
What's your take on `auto` return types? I understand that in public APIs you might want to advertise the return type, but perhaps a sufficiently smart future version of a Clang-powered Doxygen tool can generate the explicit return type in the docs. At least for private helpers, I like to use `auto` return types everywhere, with the occasional `auto&amp;` or `decltype(auto)` for tricky reference returning helpers. 
Yes, I ported Qt to aaa in order to give people with your opinion, and people with the opposite opinion code to point at to support their position :). I don't agree with you, but I'm glad the Qt result gives you concrete things to point at instead of an abstract disinclination toward aaa.
It sounds like you're not running it from an appropriately configured build environment. You likely don't have the INCLUDE, LIB, LIBPATH, etc. variables set up correctly. Like /u/bames53 says, you need to run the appropriate vcvarsall.bat to set things up. That's most easily done by just opening the appropriate VS Tools shortcut from the Start menu, e.g. `VS2015 x86 Native Tools Command Prompt` or another suitable one.
_A Tour of C++_ is a selection of chapters from _The C++ Programming Language_ intended for developers previously familiar with C++ to get up to speed. The "what" is short for "what is new", the "how/why" for _EMC++_ is short for "how/why do I effectively use the new stuff". Like mentioned elsewhere in the thread, it's a great resource for avoiding finicky edge cases with 11/14 additions. I own both (well, all three) but if you could only get one to start with and are a decent C++ dev, then _Tour_ will get you back up to speed fastest as a mini reference.
I can't recommend this book enough. I got it when it came out and I've read it three times now. I didn't find it rough - it's actually a good read. Not quite as rollicking as "Essential C++" but still a page turner...
Clang/C2 compiles code from other platforms on Windows, ideally without modification. The idea is to help in sharing code cross-platform while MSVC finishes standards-compliance work. Because it uses the MSVC code generator, Clang/C2 code can be linked directly with MSVC code. Clang-Cl maintains a set of MSVC-compatibility tweaks. The idea is to let you compile code written on Windows with Clang without having to adjust your code for MSVC-specific issues. 
I thought the coverage of move semantics and L-value, R-values in chapter 5 was really comprehensive. Where would you go for a better explanation? (Just curious) 
Yeah, I'm not sure how well this would work on huge codebases such as chromium, where at best it takes hours to index the code, at worst the IDE simply chokes on it. You don't always have the luxury of being able to hover to see type, or jump to declaration. On saner codebases it's probably much less of an issue, but you're still losing some glance-ability. On the other hand getting rid of redundant information from the code does feel nice in a way.
&gt; Therefore it's an excuse to ignore warnings? I'm sorry, but that's the opposite of what I said.
I'm not sure how true that is yet - I only just got my copy a week ago, and I also got Elements of Programming (Stepanov, McJones) which isn't really a C++ book, using a pre-C++11 style of not really C++, but which is still interesting and is taking up a fair bit of time to understand its point (an extremely math-focussed view of generic programming which takes costs, memory and partial functions seriously, dry, terse and difficult - Stepanovs own words IIRC - and really explains where concepts *still* haven't come from yet). Anyway, for the superficial (and not so superficial) stuff, there's lots of video talks, especially [cppcon](https://www.youtube.com/channel/UCMlGfpWw-RUdWX_JbLCukXg) ones. The cppcon2014 videos are in there, just a bit hidden, and particularly useful for C++11/14 changes. Particularly... * Leor Zolman " An Overview of C++11/14 [Part 1](https://www.youtube.com/watch?v=Gycxew-hztI), [Part 2](https://www.youtube.com/watch?v=pBI0tS2yfjw). * [Herb Sutter - Back to the Basics! Essentials of Modern C++ Style](https://www.youtube.com/watch?v=xnqTKD8uD64) * [Bjarne Stroustrup - Make Simple Tasks Simple](https://www.youtube.com/watch?v=nesCaocNjtQ) * [Alisdair Meredith - What's New In The C++14 Library](https://www.youtube.com/watch?v=fBU1R7jp_TE) I've rewatched the Leor Zolman ones a few times - very useful. The others I admittedly don't remember exactly what's in there, but I vaguely remember thinking they were very worthwhile at the time, except the Alisdair Meredith one (I'm not sure I watched it, based on the title it should be an obvious one to watch). cppcon 2016 is very soon - I'm hoping we'll get some good introductions to what's coming in C++17. 
I think we need to check research geometry, see if there's a word for something beyond mere higher-dimensional non-Euclidean corner cases.
Agreed, the section on move semantics is particularly good. This coupled with a few extra bits and pieces online filled in the gaps in my brain.
c++ has almost no meta-programming/reflection and you'd definitely be better off just preprocessing the files
I don't see Elements of Programming mentioned enough. It fundamentally changed how I approach programming.
Yeah I was confused by his comment. I'm using 3.9 as well and the only issues are some libraries (like boost and libsodium) make the assumption that you're not linking to VS objects on Windows. Hopefully MS will do the smart thing and place a full CodeView generator in the LLVM build tree.
Tour is packed full of bad habits too, like using new/delete when it should be teaching use of shared_ptr and unique_ptr. (it teaches about uniform initializers, too, so don't tell me that it's not written against modern C++)
I can only assume you threw it in the trash after seeing the first new/delete. &gt; The technique of acquiring resources in a constructor and releasing them in a destructor, known as Resource Acquisition Is Initialization or RAII, allows us to elminitate "naked new operations," that is, to avoid allocations in general code and keep them buried inside the implementation of well-behaved abstractions. Avoiding naked new and naked delete makes code far less error-prone and far easier to keep free of resource leakes (11.2). [...] &gt; In very much the same way as new and delete disappear from application code, we can make pointers disappear into resource handles.
Yep. "My pacemaker skips a beat from time to time but at least the dev team was able to write it quickly!"
your rvalue addPeriod doesn't move anything, mutates the argument for no reason and ends up copying anyway; binding an lvalue to an rvalue reference doesn't create a copy, it's just ill-formed; and I don't even know why you're mentioning RVO this is the way to make a mutating addPeriod: std::string&amp; addPeriod(std::string&amp; s) { return s += '.'; } this is the way to make a non-mutating version (`addPeriod(std::move(lvalue))` still doesn't perform an unnecessary copy): std::string addPeriod(std::string s) { return std::move(s) + '.'; }
google
This makes it seem like high level, interpreted languages will ever be able to compete with the performance and control of systems programming languages and while we can appreciate the simplicity of Python in terms of raw power it's not even in the same league as C++. 
Thanks for your replies. That makes sense and I'm excited about the work you guys are doing. C++ development on Windows has improved immensely in the last few years.
Is this a wish list for guests?
OTOH, I've seen a lot of code where all that was needed was a `const char*`, but it was put on the heap anyway, presumably because "`std::string` is the string type".
The point of `std::move` is to change the value category of something to an rvalue-reference. This way, you trigger the move constructor/assignment right when you make those operations with this new rvalue-ref. That is, the type matching to select the correct function will choose based on the value category. And, as rvalue-references can bind to both lvalues and rvalues (here the rvalue will be kept alive until the reference is gone), it's a good idea to make it the category for move semantics, since you are able to modify whatever the reference is binded to. `std::move` is pretty much a `static_cast` to `T&amp;&amp;`. struct quxx { int i; quxx(int i_) : i{i_} {} quxx(quxx&amp;&amp;) = default; quxx(const quxx&amp;) = default; }; quxx x {42}; // initializes x with 42 quxx y {x}; // calls quxx(const quxx&amp;) quxx z {std::move(y)}; // calls quxx(quxx&amp;&amp;) Now, forwarding references only make sense in template context. `std::forward` will change the value category of something to what it was before being passed on to the function. For that, let's examine why we would want such a thing. void foo(int&amp; i) { i += 1; } void foo(int i) { std::cout &lt;&lt; i &lt;&lt; '\n'; } template&lt;typename T&gt; void bar(T t) { foo(t); } What happens if I call `foo(42)`? Of course, the overload resolution will select `foo(int)` and print 42. However, whenever I pass on lvalues to `foo`, the compiler will always select the `foo(int&amp;)` overload, because a pass-by-reference is a better match over pass-by-value. Let's test with `bar` now on. We want the same behaviors we see in `bar` to happen in `foo` as well. Calling `bar` with an lvalue will result in: int i = 42; bar(i); // T = int&amp;, bar(int&amp;) `T` is deduced to `int&amp;`, as we expected. The function looks like this now: bar(int&amp; t) { foo(t); } Which will happily call `foo(int&amp;)` consequentially. Aight, `bar` is working with lvalues and matching the expected overloads. Now, what happens if I call `bar` with an rvalue? Simple: bar(42); // T = int; bar(int) This should print 42, right? void bar(int t) { foo(t); } Eh, not really. What happened here is that we passed on an lvalue to `foo`, which, as we know, will only trigger the `foo(int&amp;)` overload. That's bad, because it wasn't the expected behavior. For that to be solved, we have two options: 1. Write two definitions for bar, one with rvalues, another with lvalues. Just like `foo`; or 2. give `foo` the correct value category. If your choice is (1), you can stop reading now and get yourself a C compiler. If you choose (2), then let's figure out how we're gonna do it. Remember that `std::move` casts a value, so its category becomes an rvalue? Well, if we need to pass on the correct value category to our functions, surely casting them back to their original categories is how we solve this. But... how do we know the original state? One thing we know well is the value type. And this is great, because our type has the exact value category of that parameter. Why, you ask? template&lt;typename T&gt; void f(T&amp;&amp;) {} f(42); // T = int, f(int &amp;&amp;) int x = 1; f(x); // T = int&amp;, f(int&amp; &amp;&amp;) It's clear that's the same behavior of the previous code. The only thing differing here is the use of `&amp;&amp;`. What is that shit? We saw some `&amp;&amp;` uses earlier as well, if you remember. That thing is called an rvalue-reference. That is, it can bind to anything, from rvalues, to rvalues, or xvalues, or prvalues... anything. A good name for this is universal-reference, at least for this specific case where we only make use of the reference itself, like: int&amp;&amp; x = 44; // the temporary 44 will live until x dies. In template context, `&amp;&amp;` is called forwarding-reference. This is because this kind of reference behaves differently in templates. The language doesn't allow a type like `int&amp; &amp;&amp;`, as we've seen, so the type deduction converts it to a simple `int&amp;`. The rule for this is quite simple too: &amp; &amp; = &amp; &amp; &amp;&amp; = &amp; &amp;&amp; &amp; = &amp; &amp;&amp; &amp;&amp; = &amp;&amp; Looks like an AND (pun intended). The type will be a universal-reference only if T is deduced to `something&amp;&amp;`. Okay, now, how does this solve our previous problem? Simple, take a look at [std::forward](http://en.cppreference.com/w/cpp/utility/forward), then come back when you're done. I know, it didn't help much. The crucial point of `std::forward` (which only makes sense in template context) is to pass on the parameters of a function to its inside function calls, in the exactly same way they were passed on to the outer function. That is: template&lt;typename T&gt; void something(T&amp;&amp; t) { something_else(std::forward&lt;T&gt;(t)); } // T = int // T&amp;&amp; becomes only int (no normal reference, no need to check stuff) something(1); // something_else will see t as if it was passed an 1 int var = 2; // T = int&amp; // T&amp;&amp; → int&amp; &amp;&amp; → int&amp; something(var); // something_else will see t as if it was passed var `std::forward` says: Are you seeing this parameter here? Please, call this function here and make it believe this parameter looks identical to how it was passed on earlier to us. That's about it. Examine how our function `something` had its template parameters deduced out: // for something(1); void something(int t) { something_else(std::forward&lt;int&gt;(t)); } // for something(var) void something(int&amp; t) { something_else(std::forward&lt;int&amp;&gt;(t)); } The first case is clear: we make sure `something_else` triggers the overload for `int`s, and not references to `int`s. The second case triggers the overload for a reference. There's one more thing to note: If we pass on a xvalue (the category `std::move` produces), we're going to get our object moved by the forwarding as well. struct S { int i; }; S s {42}; // T = S&amp;&amp; // T&amp;&amp; → S&amp;&amp; &amp;&amp; → S&amp;&amp; something(std::move(s)); Will create: void something(S&amp;&amp; t) { something_else(std::forward&lt;S&amp;&amp;&gt;(t)); } `std::forward` is a casting anyway, just like `std::move` (but `std::forward has some different behaviors). That forwarding pretty much acted like a move, just how we I wanted. Wow, that was a lot of writing.
An elaborated type specifier allows you to simultaneously use *and* forward-declare a type that isn't previously mentioned in the code. Crazy, but (as shown by Boost.Spirit X3) occasionally useful. I'd imagine that it has many scope/name-lookup related corner cases that give compiler vendors headaches.
This right here is the big "gotcha" with AAA in everyday C++ code. I'm a big fan of AAA, but it's hard to deny that: `auto x = std::string{"lorem epsom salt"};` is harder on the eyes than: `std::string x = "lorem epsom salt";` Additionally, with the AAA version, sometimes you might need to explain the concept of copy-elision to your over-optimizing, under-informed coworkers. (I've never had to explain that to anyone, but I can see it happening) 
Wow, this is brilliant. Even though I already follow you on GitHub, I had no idea this existed.
`++featuresThatHorrifyBill`
Very nice. On one hand, I kind of hate to tack on more features to an already extremely complicated grammar. On the other hand, this is C++, so why the hell not.
What does AAA stand for here? 
Yes
&gt; Especially when used with user-defined literals. User-defined literals are also bad, static const whatever being preferable. Always reach for the original sin ;-).
&gt; compiler vendors headaches Somebody should do an AMA with these guys.
"almost always `auto`"
This would be great to crosspost on r/ProgrammerTIL/.
You can't do well with Go on this sub, go is *way* too simple and under-powered for the target demographic. :-)
This post has some more info. http://bitfunnel.org/debugging-nativejit/ https://twitter.com/danluu/status/771622870132809729
I use this all the time when creating optional indices with [`compact_optional`](https://github.com/akrzemi1/compact_optional), which is an optimization over `std::optional` for those situations in which the "empty" state can be stored within the object itself. In a nutshell the class takes two type parameters: - a policy that contains the type to be stored, and how to know if an object of that type contains the "empty" value, - a tag, which allows making two otherwise identical compact optionals be actually distinct (preventing implicit conversions). For creating optional indices I have a small alias like this: template &lt;typename T, typename Tag, T value = std::numeric_limits&lt;T&gt;::max()&gt; using optional_idx = compact_optional&lt;empty_scalar_value&lt;T, value&gt;, Tag&gt;; That can be used to create indices like this: row_idx_t = optional_idx&lt;unsigned long, struct row_idx_tag&gt;; col_idx_t = optional_idx&lt;unsigned long, struct col_idx_tag&gt;; // ^^^^^^^^^^^^^^^^^ Given the following: row_idx_t foo(); col_idx_t bar(); baz(row_idx_t, col_idx_t); The tag is what prevents users from passing the wrong indices to your functions: baz(foo(), bar()); // compiles baz(bar(), foo()); // compilation error: row_idx_t != col_idx_t The important thing is that to make this work the only thing needed are distinct tag types. The tag is in general never needed for anything else afterwards (sometimes I've used it for pattern matching but that is rare), which makes this feature a perfect fit for this use case.
C-like is a very vague and broad term, when I look at C, if I see a language that: * has compilers for any platform imaginable * has very powerful compilers with decades of effort in optimization and compile-time efficiency * has explicit memory management * has excellent open-source tooling (IDEs, debuggers, profilers, static analyzers etc.) on most desktop OSes * has an incredible library ecosystem * is very easy to find employment in Then D is very unC-like.
yes... Is there an 'official' rule to name such resource list? or it has to be titled by someone else?
It's a really handy feature when you want decouple code from each other, particularly when you design libraries. For example, implement a facade pattern for a library API: `api.h` - public header class APIFoo { public: using ImplT = std::unique_ptr&lt;struct Foo, void(*)(struct Foo*)&gt;; APIFoo(ImplT&amp;&amp; newImpl); void doSomething() const; private: ImplT impl; }; `api.cpp` - private implementation detail in the library //Full definition of Foo struct Foo { int something = 42; void doSomething() const { std::cout &lt;&lt; something &lt;&lt; '\n'; } }; //APIFoo implemention detail APIFoo::APIFoo(ImplT&amp;&amp; newImpl) : impl(std::move(newImpl)) {} void APIFoo::doSomething() const { impl-&gt;doSomething(); } //Helper function that wraps Foo into APIFoo auto WrapFoo(const Foo&amp; foo) -&gt; APIFoo { auto deleter = [](Foo* p) { std::default_delete&lt;Foo&gt;{}(p); }; return { APIFoo::ImplT(new Foo(foo), deleter) }; } Something like that.
&gt;I don't like how you named it "Awesome" yourself =&gt; Downvote How will OP recover from this.
+1 for codexl/xperf for debugging gpu performance problems. They're like, super awesome I'm only using them for opencl so codexl hasn't got the full range of shenanigans you might want, but its still very helpful
I was kind of surprised when I saw what it really does. Way cooler than yet-another C++ JIT I think.
I would add linux `perf` tool and `gperf`. 
ok, I've added those tools, should be visible in the repo
I am sorry, I actually meant `gprof`, that was a typo. But incidentally, such a thing actually exists and is somewhat relevant.
On that note, it would be nice to co-opt the `unittest` keyword from D.
&gt; You might be better off writing a full-fledged code generator that reads a file with specs. But then people will surely whine that "this isn't C++!!!" just like they do for Qt's moc (although not for Google's protobuf, for some strange-but-surely-logical reason).
OP is obviously referencing these very popular lists: https://github.com/fffaraz/awesome-cpp https://github.com/rigtorp/awesome-modern-cpp
OG framework for inline tests is [Catch](https://github.com/philsquared/Catch).
That's pretty spot on. D is modern c++ Go is modern C.
C-like as in C-like syntax all of things you mentioned describe any mature language, D isn't mature yet. 
Yet another? There are barely any C++ JITs...
Ok, I can see how the g++ behaviour would be useful, except that there is still a bug, or at least misleading diagnostic. When issuing error diagnostics, it positions caret according to the error's position on the offending line, see this: ~/scratch$ cat test.cpp #line 1 "test2.cpp" static_assert( 2 + 2 == 5, "oops"); ~/scratch$ g++ test.cpp test2.cpp:1:14: error: expected constructor, destructor, or type conversion before ‘(’ token // Can you see me? ^ The carret would point at the opening brace of static_assert, because I have a g++ that doesn't do C++11 by default. However, because of the `#line` pragma, it now points to some nonsensical part of the target line. Basically I think it should drop the caret, because there is no way its useful if you are using `#line`. 
It depends on your skill set and goals. Don't try to learn it only because you think it's fun to learn a lot of stuff. Don't waste your time. It's kinda hard to find decent online courses on cpp. At least I haven't seen good ones. It's not because of lack of trainers, but cause of difficulty of the language. I would say so - keep focus on what you really want to achieve. Build your knowledge around it. It would be more interesting and motivating for you. I'm a cxx developer with 5 years of experience. Hope it will give you an idea.
I'm not a compiler vendor, but I do work on a professional C++ parser. I remember seeing parsing rules for elaborated type specifiers in the source, but I'll have to try out this particular example tomorrow at work to see if our parser handles it correctly.
Yea, C in embedded systems is, honestly, dangerous unless you can throw huge amounts of time into it or you can afford to pay for someone who has done C in embedded systems forever (and even then it is no guarantee). C++ is much safer, but you run into problems with the STL/STD in bare metal systems, and even in real-time linux. These can be mitigated, but the reluctance is often deeply embedded (hur hur) in the field (to the point where it is often hard to find platforms that have toolchains that can compile C++11/14).
Doesn't Go's GC pause the entire environment for unknown lengths of time? Goodbye deterministic behaviour!
There is a logical reason. Protobuf is essentially used in one specific use case, as a transport encapsulation/decapsulation for RPC messages (or other inter-service messaging). MOC is much more varied and deeply embedded into the design philosophy of Qt. 
&gt; MOC is much more varied and deeply embedded into the design philosophy of Qt. So your "logical reason" for decrying Qt's use of a pre-processor in an area where a pre-processor is technically a great idea is "design philosophy"... and that despite Qt having to face a similar situation to protobuf (i.e. problem set where the language still has significant limitations). Never mind that protobuf's "design philosophy" is *also* focused against its problem domain instead of simply limiting to what the language easily permits. That's actually great though, stuff like this kind of makes my point for me, much better than I've ever been able to.
well, I mainly program on top coder and code forces, so vim is good for me. VS is too slow to start up in a crappy win7 machine that I have
wait so open native tools cmd and type vcvarsall? that doesn't seem to work
 #Talks should be #talks in the index.html l:26 :p
In cases like this you should be able to leave out the name and just say `struct` ;)
You should have a shortcut that has *exactly* the name I put in my last comment. There should also be one for x86. Those shortcuts run vcvarsall.bat for you with the appropriate arguments. You'll know things are working if you can run cl.exe from the command prompt without using a full path. If you don't have those shortcuts, then you didn't actually install the native build tool chains.
So does D if you want to use anything from the standard library :p
I wouldn't use either. 
Seeing as this is /r/cpp, understandable :p
There's no reason not to just forward declare `struct Foo` instead of hiding it in the bowels of some other type.
Note that C89 is what our compiler implements in C mode; I have a "dead tree" copy of C89 on my desk for occasions where I need to touch C stuff; there are a surprising number of files in msvcpXXX.dll which compile as C.
Would still prefer to see a plain forward decl.
Why no dynamic allocation? You think you don't want the dynamic allocation for performance reasons I assume, but you really do want it otherwise the implementation is forced to use two mutexes (one for the promise and one for the future) and is prone to deadlocks because it's not possible to always obtain the mutexes in the same order. 
&gt; Name one language that's as new as D that has this? I'm sorry but D isn't that new anymore. &gt; It has compilers for all three of the major OSs and all of the major CPU architectures This set is extremely small compared to the set "any platform imaginable" See, the point here has nothing to do with the age of D; C (and less so for C++) is THE language of choice for interfacing between obscure hardware designers and programmers. Regardless of how old a language, unless it's C (and C++, a little bit), it's not blessed with this state of affairs. &gt; D produces faster binaries than C++ and Rust as of now. I haven't been following D closely on this front, but what kind of code does it produce faster binaries with? It's been a long time since I stopped writing classic OOP, nowadays all I write is Haskellesque C++ with a lot of templates. Does it produce faster binaries with this kind of code as well? More importantly, can it at all handle compiling a codebase where each line expands to 25 levels deep template instantiations? &gt; The Code::Blocks IDE comes with all of this built in for D So, if I have a heavily template-metaprogrammed codebase, does the IDE still support content assist (jump to definition, completion etc.)? &gt; It can use C and C++ libraries as if they were it's own This is not entirely true, is it? The subset of C++ that D can use as its own was very narrow the last time I checked. Unless the D compiler contains a fully-fledged C++ compiler, how could it make sense of function and class templates? In my experience, interfacing between languages is never as smooth as people like to think. You even occasionally run into issues interfacing C with C++. 
Don't get me wrong, I respect D a lot. There was a time when I had set my eyes on D, evaluating whether I should migrate most of my programming there. The trouble though is that, IMHO, D has set out to be "a better C++", but I think that's not a very attractive position today. The point is that, the features D supports might arguably earn it "a better C++" title on the grounds of the language alone, but it's definitely not "a better C++" TODAY when you also consider all the other aspects that I've mentioned in my other comment. So, then spending time on D is a good investment only if you believe in its future. But coming back to the "better C++" aspect, C++ wasn't in a good place to start with. IMHO, if you take the projection of the language we'll be using in 2500 AD to the space spanned by the languages we use today, it'd probably fall somewhere between Haskell and Idris. That's why I've stopped searching for nicer languages in the vicinity of C++, and just love and use it for my engineering tasks TODAY.
&gt;IMHO, if you take the projection of the language we'll be using in 2500 AD to the space spanned by the languages we use today, it'd probably fall somewhere between Haskell and Idris. You don't know that at all. 20 years ago people would've said that the language of the future would be Java since OOP was getting popular again then and Java was the new kid on the block. I think if anything by 2500 there will probably be thousands if not tens of thousands of popular languages as the programmer population continues to grow. 
*This is a reference then, oh.
There are plenty of reasons, particularly if you want to create a framework API. The most compelling reason is to keep every aspect of `struct Foo` implementation detail completely private. Constants, private helper functions, and typedefs that might be in Foo scope won't be spilled into the client code via the public header. Also,if you want to keep the public API stable for your framework, a facade or decorator (or even an adapter) pattern like this comes really handy. I recommend the book, *API Design for C++* by Martin Reddy for some good tips on the subject.
You can do that too, the code for declaring the indices then just reads: struct row_idx_tag {}; row_idx_t = optional_idx&lt;unsigned long, row_idx_tag&gt;; struct col_idx_tag {}; col_idx_t = optional_idx&lt;unsigned long, col_idx_tag&gt;; Once you have 10s of different types of indices and it is clear to everybody what is going on, I prefer the inline form. It makes it more clear that the type is not supposed to be used for anything else.
I think you're overcomplicating matters for a student
`this` is a pointer to the class from which this method is created. In this case, `this`'s type is a pointer to `Vect` When you dereference a pointer, you just get the underlying type underneath the pointer, in this case we will get a `Vect` In C++, if you mark the return type of a function with a &amp;, you are signifying that the returned value will be a reference to that type. Weirdly (imo), we do not have to specify that we want to return a reference when we execute the line `return *this`. Instead, the compiler (and you the author) can see that the returned type is a reference to `Vect` and the way we do that is just return ANY `Vect` instance. For example, I can write a silly example which does not use the `this` pointer. Vect myVector; Vect&amp; GetVector() { return myVector; } Better examples would probably be returning some data member of your class by reference. In this case, callers of this function would receive a reference. This can be really beneficial, as we are not creating copies each time we call `GetVector()`. Of course, in some cases you would prefer to return-by-value, and receive a copy of the object. Also, in the future /r/cpp_questions/ is a more appropriate place for these type of questions. 
Perhaps I'm not very good at explaining the intent here. Hiding forward declaration is not the aim here. It's about hiding its full class definition while still being able to interface with it. Say you are designing a frawework. In your API you want to make use of what `struct Foo` has to offer, that is, call its methods and manipulate it somehow. Perhaps it maintains some kind of resource. But you don't want the full class definition of `struct Foo` to spill into the public header, because it might have dependencies that pulls in a whole lot of boilerplate code which has no business in client code. Perhaps the implementation detail of `struct Foo` is platform specific. Perhaps its development is unstable and keeps changing. The point is, you wrap it up with an interface and you communicate with `struct Foo` indirectly with it. 
And day before that, I hide lead sheets inside binding, so it would be less of hyperbole.
I still don't understand why you don't use struct Foo; // declares, but does not define Foo struct APIFoo { // definition of APIFoo }; Admittedly `struct Foo;` is still using elaborated type specifier: it's a declaration that consists solely of an elaborated type specifier. But I prefer to think of it as a different feature, since there are special rules that make it behave differently from other uses of elaborated type specifier.
https://www.reddit.com/r/cpp/search?q=cppcast&amp;restrict_sr=on
We use clang libTooling for our indexing, so Coati can index everything that is standard C or C++. I'm not familiar with CUDA myself unfortunately, so I don't know. If you want to try it, I can give you test license. Just write to support@coati.io with subject "test license"
I prefer int *a; over int* a; because int* a, b; don't state clearly that b is not a pointer, and you probably wanted to write int *a, *b; instead. But I accept int * a; as a trade-off ;) EDIT: note that I'm not advocating for initializing two variables on a single line or on two separate lines, but for where to attach the pointer/star character depending on how the language/compiler works.
What, no White Smiths? 
&gt;! Rotten Bananas! &gt; &gt; There was an issue getting your responses. In the meantime, please visit our Help Desk for more information on analyzing results." I guess it didn't like my responses?
I prefer writing each variable on separate line, and only when I have something to initialize it with: int* a = nullptr; int* b = a;
&gt; Class and struct names I'm missing `Little_Screaming_Snake`
Also, Bjarne makes a case for not declaring multiple variables on one line. So while I get the reasoning of DummySphere, it's based on an unclear or possibly confusing declaration to begin with.
`std::tie` is dead, long live structured bindings: int* [a, b] = callSomeFunc();
Fair, maybe I'm an "atypical C++ programmer'' who writes ``int *p;`` and explains it "(((p) is a pointer) to an int)'' emphasizing both type and syntax. Indeed the type of p is int *. Indeed, the * binds to the name p in the grammar. &gt; Whenever something can be done in two ways, someone will be confused. Whenever something is a matter of taste, discussions can drag on forever. It's maybe why I ended up to write at work: int * a;
I prefer int *a; int* b;
I dream of the day I will use it at work :D Another example without `std::tie`: int *a; int *b; if(...) { a = ...; b = ...; } else { a = ...; b = ...; } But I agree here again we (will) can do: int * [a, b] = callSomeFunc();
Seconded. The survey is rigged!
Haha, at my last job (med. software), I had trouble convincing people (both coworkers and mgmt) to start using C++11, because when they started, ~5 years ago, the Android NDK didn't support the whole c++11 standard library. That wasn't even a year ago, so I don't see C++17 happening there like, ever. 
Can you close the survey now?
Snake case is trying to import styles from other languages? It's the style used in the standard library for crying out loud. It doesn't get much closer to C++ than that.
 int* a, b, c; differs from int* a; decltype(a) b,c; I found that not immediately obvious, even though I know it is. 
Decomposition declarations must have a declared type of `auto` or a reference to `auto`.
I prefer not using raw pointers. :P
[The smartest dumb pointer is coming!](http://en.cppreference.com/w/cpp/experimental/observer_ptr)
People often use them for header guard defines, presumably because they looked in the header guards for the system headers to see what to do. I've had a conflict from this once (not due to me, I hasten to add! - my header guards are GUIDs with a prefix of `H`), and it took longer to figure out than you might think. Though I could just be thick.
That is not the important questions, this is: const int x; or int const x; The last one is of course the correct one and the first is the exception. :-)
Only if someone breaks the convention... which... with all spaces if someone tabs... or with all tabs if someone spaces?
So I guess `auto* [a, b] = ...` works? Not being able to specify the types sucks. After overusing `auto` for a couple of years I've switched back to writing the types whenever I can. Types are both documentation and a contract (although a pretty strict one), but they make the code significantly easier to understand, and compilation to fail as early as possible when contracts are not met. Whenever the Concepts TS gets into the standard I will recommend to Almost Never Use Auto. That AAA is still a thing surprises me.
The theory is that you pick one style for all the code. If you align code then spaces all the way, otherwise your choice. Mixing tabs and spaces doesn't work for alignment unless everyone uses the same spaces per tab, which kills the purpose of tabs. 
Or you could, you know, just use #pragma once :) that is unless you have to care about some obscure preprocessor that doesn't support it
Well since you put it that way COBOL is actually still used more than Rust, Haskell, or Idris today so I don't know what that says to you but it speaks volumes about not fixing what isn't broken to me. 
This is why automated code formatters like ```clang-format``` are a god-send, fixing people's terrible formatting in a couple clicks.
Oh, of course. I see tons of people using incorrect include guard names. I was referring only to data members.
Editors have largely removed any other drawback to spaces only. Then the argument solely rests on preference, and IMHO if you have a strong preference for 2 space tabs vs 4 vs whatever, you're far more particular about formatting than necessary in practice. I care about consistency far more than how wide your tabs are. Adjusting between them is a very quick process for me. I just dont want to do it multiple times in the same file/project.
For decomposition declarations, the declared type must be `auto` and can only be qualified with *cv-qualifiers* and references (`&amp;`/`&amp;&amp;`). [Reading the proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0144r2.pdf), I'm disappointed that recursive destructuring didn't make the initial implementation.
and this is a problem regardless of what you choose to enforce, unless you run everything through a formatter prior to all submissions
&gt; Lie to me Ok, I'll give it a try, but only if you insist: *you are the best c++ programmer in existence and you write the most beautiful code.* ;-) 
Thank you, very nice try. But knowing the bugs I'm guilty of, I don't believe you! :-D
Don't tell that to a former co-worker, who used `#define private public` before including a header I wrote, than modifying a private member variable and later trying to blame *me* for the resulting bug :-) 
Unfortunately Stroustrup is wrong when he says: &gt; The critical confusion comes **(only)** when people try to declare several pointers with a single declaration: Emphasis added. He's correct that C++ emphasizes types instead of expressions like C. However, there are a number of ways the inconsistency between how declarations actually work and how we might want them to work manifest, and the one he points out here is _not_ the only one. The next error caused by inconsistency between how the syntax really works and how people want to pretend it works, is people using `const int* x` thinking that this meant "x is a constant pointer to an int", when in reality it means "x is a pointer to a constant int". I've seen this error in real code. The issue here is the same: You either try to pretend that the syntax is _type focused_, in which case it makes perfect sense that `const T` works the same whether `T` is a simple type or something more complex like `int*`, or you deal with the reality that the syntax isn't defined that way. And of course the spacing certainly makes it look like the `*` binds more tightly with the `int` than the `const` does in `const int*`. I don't think that stubbornly pretending that the declaration syntax isn't what it is, is a helpful approach. It obfuscates the truth, prevents people new to the language from learning how the declaration syntax really works, and persistently causes confusion and errors. If you want a C++ "type based" syntax rather than a C "expression based" syntax, then you can do that using something like: template&lt;typename T&gt; using pointer = T*; template&lt;typename T&gt; using constant = const T; constant&lt;pointer&lt;int&gt;&gt; x; Now the reality of the syntax matches up with what you were previously only pretending, the inconsistency is gone, and the errors that come from that inconsistency won't occur. (Of course the above is not complete. You'll need several more aliases to cover all the various type decorations.) There are other downsides, though, such as this not being idiomatic or even widely used, not being supported by the standard, etc.
In stead of accepting `int * a` *adopt* it! The `*` really is kind of important. It is nice to see it stand out a bit.
No, mixing tabs and spaces can successfully allow one to configure the tab width and maintain alignment for any tab width. if (a || b || // one tab for indentation c || d) { // one tab for indentation, followed by four spaces for alignment In the above, no matter what width tabstops are set at, the lines will remain perfectly aligned. The thing that makes mixing tabs and spaces impractical is that some people can't even use just spaces consistently, so obviously consistent use of tabs for indentation and spaces for alignment is far beyond them.
Would you mind explaining why "open source" is a requirement? (i.e. did you perhaps rather mean "costs no money.")
&gt; It's declaring the type of the hidden, intermediate object. That makes it make sense, thanks. TIL. I thought that `auto&amp; [a, b]` was doing something like `[auto&amp; a, auto&amp; b]`. Since for a normal variable `auto*` works, I extrapolated that it would work there as well (i.e. that `auto* [a, b]` would do something like `[auto* a, auto* b]`).
Nifty! Any reason not to use std::is_same rather than the size trick?
What do you actually want do you want to learn the features of the language or do you want to lean how its used in a professional context or do you have no idea?
Yes, exactly! It's consistent to use `string const&amp;`, not `const string&amp;`. Compare: const int * const * and int const* const* Which one is more intuitive, and less confusing? **Edit:** Can people please stop spamming my inbox with their weird `const string&amp;` preference? I get it, you don't care about consistency. Do what you want. Just please don't think you're going to convince me that `const int * const *` is intuitive.
[nah fam](http://stackoverflow.com/questions/228783/what-are-the-rules-about-using-an-underscore-in-a-c-identifier) * All identifiers that begin with an underscore and either an uppercase letter or another underscore are always reserved for any use. * All identifiers that begin with an underscore are always reserved for use as identifiers with file scope in both the ordinary and tag name spaces. Identifiers that start with consecutive underscores or an underscore then an upper-case letter are reserved everywhere (including data member names).
No reason, didn't think of this much more obvious way :-) I'll update the article accordingly, thank you.
Ah. Misread what you said. I've never seen a convention that followed underscores with a upper case character.
You are an unsung hero amongst the 'low level C devs that could use C++, but can't because peer pressure' group.
I am a `int *a` kinda person, but even I have to admit that functions returning pointers (or references) really throw a wrench in that style. int *function(int *p); // ew, but matches int* function(int *p); // clear! but now it doesn't match the params `int * a` is just a weak compromise between the two. Double gross.
god ... pet peeve of mine, i think ive seen this used incorrectly more than correctly. Of course, nothing really ever happens due to that, but it just makes me a little sad that pragma once never got any attention form std ...
&gt; is prone to deadlocks because it's not possible to always obtain the mutexes in the same order. Although locking mutexes in a consistent order is one way to avoid deadlocks, it's not actually necessary, or even the a particularly efficient way. See [_Dining Philosophers Rebooted_][1]. [1]: http://howardhinnant.github.io/dining_philosophers.html 
Amusing, but this is not a humor subreddit.
&gt; Which one is more intuitive, and less confusing? Trick question because idiomatic C++ code should rarely use nested declarators like that.
Clearly it did bother you, since you had a conflict for using it.
You still have to erase 4 spaces every time you want to lower the indentation by one code. I guess now that I think about it you could do shift tab... but I'm just not used to it.
If somebody would give me $26, I can publish the results of all 750 respondents ... :) 
Let see, I know we have to read those from right to left... const int * const * a pointer to a constant pointer to an integer that is constant int const* const* a pointer to a constant pointer to a constant integer So it's down to a "constant integer" vs an "integer that is constant". I have my own preferences of course, but they are equally intuitive. Good, one less thing to worry about! 
So many people answered wrong. =(
Instead of trying to remember anything from that discussion, [here it is](https://groups.google.com/a/isocpp.org/forum/#!msg/std-proposals/EYK9YeOozHk/OG2vXRKnKO0J). I have nothing valuable to add on top of what's already there. At least modules will help somewhat, especially as they parallel macros and become more and more common.
This is actually pretty simple compared to everything else your parser has to deal with.
Or better yet Point(int x, int y) : x{x}, y{y} {}
Oh, derp. :) Sorry.
thank for the spelling correction
&gt;With the second option, someone who doesn't know the rules would have to look it up. That's a good thing; understanding of this rule is required to understand any more complicated declarators involving multiple levels of `const`.
&gt; people using `const int* x` thinking that this meant "x is a constant pointer to an int", when in reality it means "x is a pointer to a constant int" That has nothing to do with the white space around the `*` ; it is to do with whether the word `const` is on the left or the right of the star. 
google docs has free surveys with unlimited responses
&gt; I prefer int *a; This is C++, not C. In C++ the language is built around types and so ```int* p``` says "p is of type int*". ```int *p``` just serves to confuse anyone who doesn't have a C background and doesn't add any real semantic benefit. 
Yep, turns out it just worked.
`const string&amp;` is completely unconfusing even to a beginner. It looks like a `string` reference that's const, which is exactly what it is. `string const&amp;` - what's that? a string with a reference that's const? Obviously not of course, but it's not intuitive at all. Now, for multiple levels of const'ness, yes, the latter ordering absolutely is more consistent **if** you know how to read it correctly - which, if you do, it really doesn't matter. So I would posit that it really only matters when you add multiple levels of const and even then, unless you're going to be the sole developer for all eternity or you feel vindicated to write a lengthy preamble explaining your syntactic standard, you're fighting a losing battle.
Could you please post the survey result.
&gt; but older companies probably would not use C++ 11 as they want to keep their code base Or they're fine with using c++11 features in newer parts only
Yeah well I can do that with just T* right now. However, observer_ptr is supposed to aid readability. If everyone rolls their own alias for it that kind of defeats the purpose.
I have not used build2 yet, but I do have a design question for you, as well as anyone else that has designed a new build system. Why does your build system encourage the usage of globals? Build systems are often complicated. The build system for a large project often rivals the complexity of the "real" code for a small project. Yet build systems regularly encourage, or even require the maintainers to ignore good software engineering practices. If I were to start a new C++ project and immediately add five globals, code reviewers would (rightly) be horrified. If I do the same in a build system, that's considered a normal practice. Build system globals regularly cause me problems. I often need to do a mix of compiling for my host system and cross compiling for multiple systems. For example, I'll build a code generator for the host x86 machine, then invoke the code generator to produce code that I will compile for ARM (32-bit and 64-bit). Something like "cxx.poptions =+ -I$src_root" rarely makes sense in those kinds of builds, because it is important to specify which of many environments I'm targeting.
I think that falsehood is pretty loaded, personally, since the dictionary definition is exactly doing something quite unethical that I'm clearly not doing. I think just "plainly incorrect otherwise" would be fine. Naturally my "Understood" had some attitude, but by that point I was fairly annoyed. If you agree with my definition of CC, then using `auto&amp;&amp;` in *all* circumstances in for loops, is not CC. I don't really understand your dichotomy at all. Presumably every programmer sometimes writes loops where they don't intend to change the container contents. If they are following "always use `auto&amp;&amp;`" then they will use `auto&amp;&amp;` in that circumstance, which would violate my definition of CC. I am also not the one that is insisting on one definition of CC; others in this thread have told me that my definition of CC is plain incorrect, and nobody has offered a source. I understand the value of different definitions. I think ignoring all the definitions etc, the important thing here is that I think you should make your loop variable `const` if possible; that's the actual substantive point. I would have thought that this would be universally agreed, but maybe not.
You might want to look at http://cplusplus.github.io/LWG/lwg-active.html#2543 #include &lt;type_traits&gt; #include &lt;functional&gt; class S{}; // No hash specialization template&lt;class T&gt; auto f(int) -&gt; decltype(std::hash&lt;T&gt;(), std::true_type()); template&lt;class T&gt; auto f(...) -&gt; decltype(std::false_type()); static_assert(!decltype(f&lt;S&gt;(0))::value, ""); 
&gt; assuming the existence of a reasonably good heap allocator That's actually a big assumption in many cases :-) &gt; the future is going to use a minimum of 6 atomic ops on each call Really, six!? I suck then. It should be less than that. But I agree, in general, that atomics don't always make things faster. More than a couple of atomic-ops means you should be thinking about just using a lock (depending on ...). For just reading the future, you should just need to do an Acquire on the promise-ptr - if it is null, you have a value, else you don't. If it is null, you don't need any additional atomic ops to read the value (as the promise is gone, and you did an acquire already.) Of course this assumes that the promise-ptr is atomic (and had a release when set to 0). I don't think I made that at all clear in the video. "Exercise left for the reader" and/or I suck. 
This is up to you to decide. The conference offers two keynotes and 5 Tracks with 7 Talks each.
Not currently but assuming it tries to mimic MSVC it will be pretty easy to support (and if it doesn't mimic VC it will be a bit harder but not impossible; nothing is impossible after you managed to support VC ;-)). So the hardest part will probably be getting hold of the distribution. We got one for Linux and it was a pain in the butt with all the license servers, etc., and the thing still phones home...
In a nutshell, the problem is there are all these different platform/compiler-specific build system out there. The CMake's approach is "hey, let's come up with a single project description that we can translate to all those underlying build systems". In contrast, `build2`'s approach is "hey, let's come up with a single, uniform build system that we can use on all the platforms and with all the compilers". The nice thing about the CMake's approach is that you can reuse all the existing build system. Plus people can continue using, for example, Visual Studio since they get its project files (but they have to regenerate them and reload if they add/remove any files). The bad thing about the CMake's approach is that you are essentially restricting yourself to the lowest common denominator. If you want to support multiple platforms, you can only use build system features that are available on all of them. This gets really hairy if you want to do automatic code generation and especially header generation; most of the existing build systems (especially those in the IDE's) simply cannot handle this properly. BTW, I think this is largely the reason source code generation is the F-word of the C++ community ;-). But I digress. Let's now see what the `build2` approach gives us: 1. Now you have a uniform build system interface that works across all the platforms (Linux, Mac OS, Windows, FreeBSD, etc) and compilers (GCC, Clang, MSVC, Intel icc, etc). 2. We can handle source code generation properly, again, on all the platforms. 3. We can do cross-compilation, properly. In `build2` we just don't do anything that is not "cross-compile"-clean. For example, `pkg-config`'s library search model -- we had to fix that by search for the right `.pc` file ourselves. The result? We can cross-compile from Linux to Windows with MSVC (no, I kid you not). 4. `build2` provides a set of operations that you would expect from a modern build system, again, uniformly and across all the platforms/compilers: `configure`, `update`/`clean`, `test`, `install`/`uninstall`, and `dist` (prepare a distribution archives). 
How does build2 work with IDEs and/or debugging? One of the biggest advantages of CMake is that developers can keep their dev environment of choice. 
The directory scoping you are describing sounds half-way between a global and a function argument. You're handing copies of all of the variables from parent to child, whether the child wants / needs them or not. But since they are copies, there is less "spooky action at a distance". The expected interface between parent and child seems to be left as a documentation task though. One day, I'd really like a build system that has a "compile()" function, where the inputs are provided as arguments instead of globals. &gt;&gt; Something like cxx.poptions =+ -I$src_root rarely makes sense in those kinds of builds [...] &gt; I think you are misunderstanding what this does. This sets a header search path for source (i.e., non-generated) headers, which will be the **same for all targets.** The "same for all targets" aspect is the root of my complaints. Most build systems encourage setting things for all targets. It is often difficult to later add a target that doesn't want the global behavior. My builds don't involve one compiler for a single invocation of "make" / "ninja" / "bpkg build", they involve multiple compilers targeting multiple environments. cxx.coptions is ambiguous because it doesn't say which compiler or which target those options belong to. However, you also say that lots of things are cross compile clean, so maybe I'm misunderstanding a feature in here somewhere. Also, apologies if I come across as hostile. I don't intend to do so. I just get a bit ranty when it comes to build systems :). I'm hoping to one day find a system that I consider "good", and not just "the least bad one available". Maybe my ranting can help build2 be that system.
`ucm_add_linker_flags()` is the big prize here for me. With modern CMake, there are simple enough ways to cleanly handle compile flags, compile definitions, include directories, and link libraries. Link flags? Not so much.
note that I'm planning on making versions of these macros that are for specific targets and not global like now so it will be even better 
In my particular case, it's the global ones that I'm most interested in. I manage the build for a largish project with several dozen targets. Mostly, I just want them to all link the same way.
cool, thanks :) &gt; The only downside to ucm_add_target() is that it is intrusive This is one of the biggest drawbacks of CMake in my opinion, it's very hard to generalize / abstract stuff *without* resorting to intrusive command wrappers. 
It's also notable that the fact C++ is growing its standard library (the most notable addition was threading, but network and I/O are coming too) might also be a deterrent in the embedded world.
Sure, but in this case we already have expectations for constants: log N instantiations vs one pretty similar instantiation + one make_index_sequence&lt;N&gt;, which should be fast with recent help of compiler magic.
&gt; Isn't this how CMake works, but functional instead of object-oriented (and with the build deferred) ? Maybe in theory, but the globals you mention really spoil everything. Suppose you want to build release and debug with one invocation of "make". There doesn't seem to be a way to do that with cmake. Similarly, suppose I want to build32-bit and 64-bit x86 binaries with one invocation of "make". The choice of compiler and build_type are global by convention, and are pretty difficult to isolate. http://stackoverflow.com/questions/5204180/how-to-build-several-configurations-at-once-with-cmake
&gt;If you agree with my definition of CC, then using auto&amp;&amp; in all circumstances in for loops, is not CC. Yes, I agree such *practice* isn't CC and I find your recommendation beneficial. But I disagree that `auto&amp;&amp;` itself, *as an individual construct*, is not CC either, which is the thing you said that I disagree with. What I'm trying to point out is that the above are two different things (practice vs. language construct) which warrant different criteria. &gt;I don't really understand your dichotomy at all. Suppose you have a function. You can ask two things of it: 1) "As it is written, does it use const wherever it can?" Your def of CC answers this, and I call it "human" because it's most useful to ask it as a matter of best (human) practice, as you write/edit the function's body. You can't use this criterion to evaluate the CCness of an abstract construct like `auto&amp;&amp;`, but you *can* use it to evaluate a code snippet like `for(auto&amp;&amp; x : xs) cout &lt;&lt; x;`. 2) "If someone calls this function, may they obtain mutable access to what would otherwise be (at most) only accessible as a const?" STL's def answers this, and I call it "technical" because you can answer it objectively by evaluating it at a technical level and make a concrete assessment of the function. This one's useful when designing libraries. This criterion can not be used for code snippets, but *can* be used for reusable constructs, like functions, macros, `auto&amp;&amp;` etc. &gt;others in this thread have told me that my definition of CC is plain incorrect, and nobody has offered a source. I'm not one of them, though I'd argue STL himself counts as an authoritative source. He maintains Microsoft's VC++ std lib, and he's also an arguably active community member. &gt;I would have thought that [using `const` in `for`s] would be universally agreed, but maybe not. No one here disagreed with you on this one. STL didn't address this matter and dsqdsq merely addressed terminology. I've seen C devs hate const altogether though, for some reason, but I haven't seen any in this thread.
I've also heard SFML mentioned often as a good C++ alternative to SDL, but I don't know how much their features overlap.
SFML is quite nice, as well. It has a C++ API, where as SDL2 is a C library.
Maybe then the root of the misunderstanding. I never claimed that `auto&amp;&amp;` on its own as an individual construct is not CC, only that blanket usage of it is not, and I think that point was clear and well understood by everyone else who responded to me. This distinction you're trying to make between 1) and 2) is a distinction between violating the const contract, and being maximally const. I require both for CC, others on the thread require only 2. It's not a "human/technical" distinction. You can make 1) technical by simply adding const and seeing if things compile. And 2) rapidly becomes human when you start dealing with logical vs bitwise constness.
Good alternative to SDL is GLFW for input &amp; windowing. I don't recommend using SDL's software renderer anyway... I think u/nnevatie mistook GLFW with GLEW... u/nnevatie It's hard to call them essentials... ;) Pretty much you could be fine without any of them
Link flags go to the same place than link libraries target_link_libraries(mylib PRIVATE libfoo -Wl,-zdefs) 
Oops, you're absolutely right - meant GLEW instead of GLFW. I shall not edit to preserve proof of my brain fart.
It seems to me that `return i &lt; 1 ? 0 : i &lt; 2 ? 1 : 8;` is wrong; should be `return i &lt; 1 ? 0 : i &lt; 8 ? 1 : 8;`.
All I would like is for someone to point me into the direction of some good online resources and possible examples of c++ programming in a professional environment. 
I like that one. Unfortunately some compilers will issue a warning for this.
Yeah a lot of switches are the same as msvc but there are quite a few icc specific ones. The trial is free though for testing if you need it but I agree, licensing for it is a pain although worth it. 😃 
Have you looked at the (non-official) [Boost.Outcome](https://ned14.github.io/boost.outcome/) library? &gt; This is the proposed Boost.Outcome library, a Boost C++14 library providing Configurable lightweight simple monadic value transport with the same semantics and API as a `future`, a factory and family of policy driven lightweight monads with the specialisations of `outcome&lt;T&gt;`, `result&lt;T&gt;` and `option&lt;T&gt;`. &gt; Features: &gt; - Very lightweight on build times and run times up to the point of zero execution cost and a one to eight byte space overhead. See below for benchmarks. Requires min clang 3.7, GCC 5.0 or VS2015 Update 2. &gt; - Just enough monad, nothing more, nothing fancy. Replicates the `future` API, so if you know how to use a `future` you already know how to use this. &gt; - Enables convenient and easy all-`noexcept` coding and design, giving you powerful error handling facilities with automatic exception safety. &gt; - ... I haven't tried it myself but its author has been quite evangelical about efficiency-of-implementation on the Boost-dev ML.
Dash works fine &amp;ndash; from [the VC++ compiler docs](https://msdn.microsoft.com/en-us/library/610ecb4h.aspx): &gt; Options are specified by either a forward slash (`/`) or a dash (`–`). And [linker docs](https://msdn.microsoft.com/en-us/library/hx5b050y.aspx): &gt; On the command line, an option consists of an option specifier, either a dash (`–`) or a forward slash (`/`), followed by the name of the option.
I would have sworn I tried that yesterday and it didn't work, but I must have been mistaken. Thanks! I still don't like that the two, very distinct properties (libs and flags) are set via the same commands, but I guess I could just write my own `target_link_flags()` command that just forwards to `target_link_libraries()` if I really cared that much.
Still no mention of the "indexing" loop. Or lack of parsing and refactoring for templates. Clion has been my daily driver since the early EAP phase. But it is frustrating to see release after release add features without fixing some of these fundamental problems that have been there since launch.
Is that when the cpu melts and the OS comes to a crawl during the indexing period? they finally added CMAKE output. Though their workflow is a little weird for the cmake output
Yeah, I run into an issue a few times a day where CLion starts indexing, and just never returns. On OSX this usually means 99% cpu usage and I have to kill the process. In linux it doesn't eat up the cpu, but it hangs CLion making it basically useless until the process is killed and restarted.
&gt; Note that C89 is what our compiler implements in C mode Plus [a few cherry-picked C99 features](https://blogs.msdn.microsoft.com/vcblog/2013/06/28/c1114-stl-features-fixes-and-breaking-changes-in-vs-2013/), right? &gt; - C99 `_Bool` &gt; - C99 compound literals &gt; - C99 designated initializers &gt; - C99 variable declarations Are there more in addition to that list?
Why I need to learn the new things that CMake forced? Why not leverage I'd had learned things to do the job? All platforms have shell and make, if I can do the job with shell and make why I need more new terrible things? I'd read this reddit many times and decide to try a different way, just use shell and make to do the job and keep **configure**,**make**, **make install** workflow which everyone known how to play with it. The GitHub repo at: https://github.com/junjiemars/nore
huh? 
&gt; but then the linker is faulty The linker is hardly faulty; you need to setup your environment and/or linker flags correctly.
Yes, a `std::string` _does_ have to be allocated dynamically; `std::basic_string&lt;CharT, AllocT&gt;` doesn't, but how many public interfaces have you seen implemented in terms of a template taking `std::basic_string&lt;&gt;` vs a function taking `std::string`? Very few, IME!
I've been using CLion for a while now and overall I love it. The git changes are pretty interesting as well. Overall looks like a good update. 
RAII, references
It's amazing how people insist on writing C++ parsers in this day and age.
It's left as an exercise to the reader to figure out how these things are monads, which category they operate on and how they satisfy the monad laws.
&gt; until we get string_view. [We already have it](http://www.boost.org/boost/utility/string_view.hpp).
&gt; After a couple of months exploring Rust (which I've decided is a train wreck), I switched back to C++ because of templates and destructors. Curious since many of the Rust folks tend to proselytize to the C++ community, what specifically about Rust turned you away.
I sense a like-minded soul here. I wish OOP and exceptions never made it into C++, and instead we had better support for the parametric polymorphism that we get from templates today (not that they're bad today). If it weren't for exceptions, it'd have been much simpler to implement algebraic data types in C++, then we would need exceptions even less. I also gave Rust a chance, hoping that it would be the language I described above, but I think Rust's model of parametric polymorphism, though more disciplined, is too restricted to be a replacement for C++'s.
You can use C++14 just by changing std to c++14 in CMakeFiles.txt
Oh :-) that was easy:-) 
Haha, happens to a lot of us. ;) Also, it doesn't help that there is a lot of libraries with "GL" prefix like GLEW, GLFW, GLU, GLUL (which is mine :D), GLAD, GL3W and so on, and so on... ;)
... and we have RVO too :)
&gt;I meant in the standard library. [We already have it](http://libcxx.llvm.org/ts1z_status.html)
Yeah, I would like algebraic data types too. Instead of tearing out the bad parts of C++, I think of it as adding a few more good parts to C. The way Haxe, Swift, and Rust generalize enums seems nice.
I have to restrict it in some way. I don't have the resources to "background check" if a hundred+ people are students or not.
We've started delivering some C++14 support within 2016.3, however there are still some problems that we are going to address later, check subtasks: https://youtrack.jetbrains.com/issue/CPP-1263
Have you reported this "indexing" loop to our tracker? CPU snapshot will be useful. We are working on performance issues, investigating every snapshot shared by our users. But without this information sometimes it's difficult to guess what's going on there. Templates are improving as well. Maybe you can point to some issues in particular, so that we pay more attention to it?
That's really a great program. Especially interested in Hans Boehm's talk. That's a pity I can't join in person this year.
Program is available here: http://cppcon.org/2016program/ and it's a great
Just for duplication, the repository certificate subject and SHA256 fingerprint are as follows: CN=cppget.org/O=Code Synthesis/admin@cppget.org 86:BA:D4:DE:2C:87:1A:EE:38:C7:F1:64:7F:65:77:02:15:79:F3:C4:83:C0:AB:5A:EA:F4:F7:8C:1D:63:30:C6 
There are plenty of languages in which you can write object oriented code. For me, C++ is about expressiveness. All the major paradigms are supported, and it's expressive enough to enable simulation of them. And, where it's not expressive enough to simulate them, someone in the Standards committee will be pushing a feature to make it so (c.f. Lambdas).
std::copy_if and std::generate are probably my two favourites, so powerful. 
I don't get to use `std::generate` as often as I'd like, but it is good for writing tests and debugging stuff.
I tried to give up C++ repeatedly. You know, stumbled upon a ugly code or a rant in a blog and nodding I say "enough, it's done". Then, when I'm diving in other languages I ask myself "Is this freedom? Why lacking features or losing control?". And then, soon or later, I always end up coming back home. 
Is this new? i.e. the notion of a cpp package repo? And/or using bpkg as a cpp package manager?
Dunno if Dave's free, but some of the others are probably possible.
C++1z is called as such because it isn't out yet...
RAII and exceptions. I hate error code return values.
I'm disappointed that the ticket for c++14 still hasn't been assigned or split up into tasks and assigned. Without c++14 support, this IDE is useless to me and everyone I work with. It makes clion seem like it's on life support. 
Thanks for the background. I now read the [Compilation Database](http://clang.llvm.org/docs/JSONCompilationDatabase.html) spec and, yes, should be pretty straightforward to add and it will work for all the platforms and compilers. In fact we kind of have something similar for detecting changes in compile options, etc. Except that this database is per object file and in many cases includes hashes of options, not actual values. So, yeah, if someone comes and says they are serious about wanting to use this feature, I will implement it.
That will let you compile c++14 but the IDE still won't be aware of the vast majority of c++14 features.
&gt; I've got more complaints, but I'll save it [...] I recommend trying Rust [...] If you're like me, it might give you a new appreciation of C++ :-) For me it's the opposite. Granted, the coherence rules are too strict. And Rust's lack of non-type parameters for generics is a bit of a pain in the butt, sometimes. But in many respects, Rust feels like the cleaner and simpler language where C++ just feels like a mess of patchwork with bad defaults and too much need for boiler plate. My Rust experience changed how I look at C++. But not for the better. (Maybe my opinion of C++ was higher than yours before I started playing with Rust) ;-) I don't really believe you when you say "lifetimes are bizzare". For a C or C++ programmer the concept of lifetimes is very important, too. The only difference is that in Rust lifetimes are part of signatures (etc) which the compiler can check whereas in C and C++ you're (still) left with documenting these things via comments that express lifetime constraints. Unfortunately, comments are ignored by compilers and too many users. Yup, it may take a while to grok how lifetimes work in the type system. But I found that to be the most interesting/exciting part about learning Rust as a C++ programmer. I like the ability to express these things in order to help the compiler stop the user from making mistakes. As for the data structures: I don't know what you expected. For things like hash tables and heaps you should be able to build on `Vec` without the need for unsafe code. And implementing something like a vector in C++ is no safer nor simpler than in Rust. There are differences, pros and cons on both sides. For example, in Rust you don't need to worry about failing moves if you implement the growing of capacity part. As for overloading: This is not a feature I miss anymore. In the cases where you would overload on the type you'd simply make the function generic in Rust with the help of traits like `AsRef` or `Into`. That tends to end up a clean solution in my experience. Speaking of sane defaults: Rust does not need a "rule-of-three" or "rule-of-five" or a syntax to delete copy cosntructors or assignment operators. And I like the fact that invoking a possibly expensive clone/copy constructor is more explicit in Rust. Moving by default in Rust is a good thing. It avoids potentially expensive clones. And moves are not error prone in Rust. You can't accidentally move something in Rust without the compiler stopping you from trying to access something that's already gone. So, "move by default" for nontrivial types seems to fit well in there.
Competition is good. Since clang entered the scene, gcc got so much better. 
&gt;I have not used build2 yet, but I do have a design question for you, as well as anyone else that has designed a new build system. &gt; &gt;Why does your build system encourage the usage of globals? I am the author of [the Meson build system](http://mesonbuild.com) which does not encourage global state (though we support it because it is sometimes useful). Instead we encourage putting state explicitly in target definitions. All data is also immutable, which makes things easier. &gt; For example, I'll build a code generator for the host x86 machine, then invoke the code generator to produce code that I will compile for ARM (32-bit and 64-bit). Something like "cxx.poptions =+ -I$src_root" rarely makes sense in those kinds of builds, because it is important to specify which of many environments I'm targeting. For this particular case Meson supports compiling both with the "native" and cross compilers at the same time, but the outputs and settings of each are strongly separated. 
I do like optional, as I come from a Haskell background and love the Maybe monad. I still like exceptions too though for exceptional things. Between optional for the everyday scenarios where Nothing is a valid return value, and exceptions for the truly "this shouldn't happen" times, I'm glad I never have to return -1 again.
They have some good reasons (reasons which they have specified about earlier ) for using their own parser, and them not using libclang is NOT a good reason to complain unless they is missing a certain functionality in which case "bug tracker".
I'm a big advocate of the profiling tools developed by hardware vendors. You've got Intel VTune listed (which is good, because it's awesome and very well designed). Here are some other proprietary profilers provided by hardware vendors: * [The Intel Vectorization and Threading Advisor (the Vectorization Adviser is new; it is certifiably awesome)](https://software.intel.com/en-us/intel-advisor-xe) * [The NVIDIA Visual Profiler](https://developer.nvidia.com/nvidia-visual-profiler) * [AMD CodeAnalyst (haven't used it in a long time, I believe this is now discontinued)](http://developer.amd.com/tools-and-sdks/archive/compute/amd-codeanalyst-performance-analyzer/) * [AMD CodeXL (haven't used it, open source)](http://developer.amd.com/tools-and-sdks/archive/compute/amd-codeanalyst-performance-analyzer/) * [Qualcomm Snapdragon Profiler (haven't used it, but I wanted to include an example of something from the ARM world; looks legit)](https://developer.qualcomm.com/software/snapdragon-profiler) * [Oracle Solaris Studio Performance Analyzer (haven't used it, but I've heard its good for SPARC)](http://www.oracle.com/technetwork/server-storage/solarisstudio/features/performance-analyzer-2292312.html) I am also a big advocate of hardware vendor manuals. If you truly want to know how your software runs on an Intel x86-64 processor, at some point you will need to consult the bible: * [Intel 64 and IA-32 Architectures Optimization Reference Manual](http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html) Quite literally a case of RTM! Basically, my advise is, figure out who makes the hardware platforms that your software is designed to run on and find their profiling software and manuals. Some other software I can recommend: * [oprofile is an open source sampling profiler for Linux which I've used before](http://oprofile.sourceforge.net/news/) * [I've made extensive use of the PAPI framework on Linux systems](http://icl.cs.utk.edu/papi/) * [I've heard good things about dtrace, a Sun sampling profiler which is apparently open source](http://dtrace.org/blogs/) Plugging my own research division's product here a bit, but: * [The Roofline Model](https://crd.lbl.gov/departments/computer-science/PAR/research/roofline/) Anyways, glad to see people are still enjoying my talk :). 
&gt; Also, CMake don't do that properly as of now. Really?
Cling seems to use clang for a similar purpose. They've had to modify in ways that clang has yet to accept upstream, but I have to imagine those changes are easier than trying to write a brand new parser that conforms and keeps up with the standard. https://rawgit.com/vgvassilev/cling/master/www/index.html 
ah, right. If your non-std `final` had a nested typedef `final&lt;T&gt;::Im_not_std_final` the syntax could be used, but you are not saving as much as I thought. (And, technically, std::final _could_ have that nested typedef and still be conformant, if some std lib author wanted to mess with you).
It has nothing to do with IDE parsers. http://lakhin.com/blog/15.11.2013-handy-incremental-parser/
Here's one that's both performance and fundamental parsing issue that needs some love: https://youtrack.jetbrains.com/issue/CPP-1644
IIRC their reason(s) is something like common backend for all their IDEs and that all their code is in Java. You're not wrong, but you can't deny that it is a fact that in practice, their parser will _always_ lack behind current C++ standard/features. A couple of people (and I doubt they even have a couple people full-time on the parser) will never be able to keep up. And if anything, the evolution speed of C++ is increasing, and not decreasing. Oh and then there's the business/community aspect of it: If I know an IDE is using clang completion, I'm like "ooh yea, awesome, I'm gonna try it out - I know the C++ parsing will be **awesome**!". And I would try the product because I know it will be future proof in that respect, so when C++17 or whatever comes out, it'll work in a reasonable timeframe.
All of the type safety stuff that it has over C is huge. That involves stuff like more explicit casting, but also stronger enums, derived types, RTTI, and a bunch of other stuff. The ability to say "this is an audio sample" rather than "this is a float*" takes a bunch of cognitive load off the developer compared to C, so you can focus on solving application domain problems, rather than 'pure' CS domain problems like, "what is the length of this array?"
&gt; I don't really believe you when you say "lifetimes are bizzare". Well, you're right on time. I don't really believe you dropped the "to me" part and think it doesn't change the scope of what I said. I'm not interested in arguing with you. 
I suspect it's because they want to stick to Java and not have to interface with non JVM libraries.
That bug is just full of people requesting a feature. No one is trying to get the feature in. If you or anyone else want that feature in CMake, then start here: http://public.kitware.com/pipermail/cmake-developers/2016-June/028809.html
Actually is there any documentation for using the Intel compiler? I've been unable to find anything. 
When this is the wrong place for feature requests for cmake, where is this place? IMHO, posting something in the mailing should be not the place.
I'm open to suggestions - what to remove, if a new category should be created, if stuff should be sorted, should there be an articles section at all, etc. I basically searched 'cmake' in github and sorted the results by stars in descending order and went through the first ~30 pages - everything with more than 4 stars was inspected. [here](https://news.ycombinator.com/item?id=12456094) is the hackernews thread
Posting on the mailing list is for volunteering to do something. Requesting features is fine.
&gt; I'm not interested in arguing with you. That's fine with me.
That depends heavily on what you're doing, and the scale of code you're writing. For small programs, the (much) more substantial standard library can make a huge difference--to the point that programs that can be written in 10 or 15 minutes in C++ would often take hours or days to write in C. For large programs, the contribution of the standard library (as a percentage) tends to drop. Here the emphasis is more on things like namespaces, which can (and do) keep things manageable until you reach a much larger scale (i.e., you're solving much larger problems). Now, it's certainly true that *some* of those tools (e.g., inheritance) are object oriented, or at least can be put to use in object oriented code. Quite a few others (e.g., namespaces, templates, lambdas, RAII) have essentially nothing to do with object orientation at all. From a high-level perspective, it comes down to this: C++ reduces risk and improves your ability to concentrate on what's important to the code you're writing. When code needs to be as small and fast as possible, C++gives you all the same capabilities as C to do that (plus a few, so it can produce code that's smaller and faster). When you care more about implementing some set of features quickly, C++ gives you much higher level abstractions to let you do that--with little or no speed penalty. I do feel obliged to add that the situation isn't *always* completely rosy. For example, if your primary concern is with the size of executable file you're producing, C++ often does carry a significant penalty compared to C. Although you're not (technically) using C++ any more when you do so, most compilers will let you disable exception handling. When you do so, code size is often quite comparable to C's, while still retaining *most* C++ features.
The same reason you code in C rather than in assembly - higher level semantics.
Subcategory perhaps? 
Writing that your own project is awesome is dumb.
In "Articles", add the link to the slides from Daniel Pfeiffer's presentation about modern CMake... one of the most important resources on the topic (sorry don't have the link handy). Also there are a couple other very important links that came up in the last few cmake threads on this subreddit, have a browse if you feel like it :-) PS: In my opinion, the list is getting a bit too long and very hard to navigate so a better system would be nice, and don't add too much stuff.
Rather than 10 thousand different ways to do the same thing with cmake, wouldn't it be better to have may be two or three examples of common source code layouts and how cmake can be used to build projects using those layouts?
Not sure why it's making Celsius a whole number
I think you forgot to #include &lt;iomanip&gt; Also the way I learned it was something along the lines of cout &lt;&lt; fixed &lt;&lt; precision();
It's pretty much out. Just some ISO guys needs to pass the final judgement (if it hasn't happened already), but the standard itself is complete. Not all C++17 features have fully landed in a compilers though, but that's another story.
A common project layout simplifies things so, so much. It also lets one put their boilerplate into macros turning the project specific `CMakeLists.txt` into a tiny and simple affair.
&gt; Ok, here comes the tricky part. That `foo.hpp` is your normal C++ headers, it just has some extra `#pragma`'s in it for ODB. Which means it `#include`'s other headers which in turn include more headers and so on. I think you can guess where I am going: if we modify one of those headers deep down in the include hierarchy, we would expect the ODB files to be regenerated since the change might affect the database mapping. Okay, the problem you're talking about is with detection of implicit dependencies rather than specifically with code generation. There's no build system that has built in support for detecting all possible implicit dependencies. However CMake actually does support implicit dependencies in the situation you're talking about: CMake allows for custom commands, such as you'd used to generate those files, to specify which input files have implicit dependencies, so that some kind of language specific scanning can be done to detect those implicit dependencies. In your case the files are C++ files which is one of the languages for which this feature is supported. CMake's documentation indicates that the above feature isn't supported in the Visual Studio generators, so you may be correct that Visual Studio can't handle this. However, this also demonstrates that you're wrong about CMake limiting you to only 'lowest common denominator' features. Features that are not supported in all the build systems for which CMake provides generators can be and sometimes are still supported when generating build files for build system which do support those features. &gt; &gt; I'm not sure it's fair to you to compare your new build system to CMake &gt; If I give a factual, technical comparison, why wouldn't it be? Is CMake somehow holy? I am not being sarcastic, I really would like to understand. Just to clarify what I said: It's not fair _to build2_ to criticize it for anything it lacks vs. something far more mature and well supported by the ecosystem. Otherwise I'd simply dismiss build2 as not mature enough for wide usage and not having enough support from the rest of the ecosystem. I figure I should at least let build2 get out of alpha. Hopefully my clarification makes it obvious why this prefaced my comments on build2 not supporting IDEs.
[I somehow get it working](http://ideone.com/2dvl1z) What you need is :`std::cout.setf( std::ios::fixed, std:: ios::floatfield );` [reference] (http://www.cplusplus.com/reference/ios/ios_base/precision/) 
They are missing the vast majority of c++14 features. The reason why people complain about them writing their own parser is because they also aren't assigning nearly enough people to have feature parity with libclang.
If you look at Linux distros such as Ubuntu, you'll see that 90 percent or more of the OS code is written in C. 
That's not very accurate -- and, even if it were, so what?
What
If you want to use things in iostream you need to do either a "using namespace std;" after it or prepend everything with "std::".
Would you mind airbnb'ing your front lawn to me? i can byo tent and also perhaps drive me in every day to the conf? :D
That's probably why the analysis in CLion after inserting a space in a place where it does not make a difference takes about 20 times what make takes to decide what needs to be done to update the corresponding library and do it (using g++, the c++ compiler well known for its speed).
Ok :). You're the only person I've seen to report that it doesn't work. Seems to be fine for everyone else. BTW, I just ran the unit test on linux and it passed!
What does cross-compiling have to do with IDE? Unless there is deeper integration with other architectures/systems (say remote debugging of android applications) there is little need for IDE doing cross-compiling. I use IDE to edit code and build on my workstation, and then i use shellscript to build and deploy to android device. Hitting "compile" button in IDE to make android build would give me little to no benefit. And yet it is still possible because what cmake can do - clion can do.
thanks for listing cmakepp. here are some articles describing it :) * download any type of package from anywhere (http,local,git,hg,svn,...) http://thetoeb.de/2015/02/04/heterogeneous-package-search-retrieval-cmake/ * use processes parallely with cmake (long downloads, long pre compilations) http://thetoeb.de/2014/12/16/parallel-processes-cmake/ * use cmake as a template generation tool http://thetoeb.de/2015/02/11/cmakepp-template-generation-cmake/
This is largely because the existing code was C at the time development started. C++ was not a viable option for the kernel at the time. Some of the userland utilities date as far back as 1970. It is a fool's errand trying to rewrite. So what you say is hardly relevant at all to using C++ today, or to using C++ outside the kernel.
There is Meeting C++ in Berlin: http://meetingcpp.com/index.php/schedule16.html In Spain there is this year also again the (local) using std::cpp conference. For Audio/C++ devs there is the Audio Developers Conference in London. And earlier in the year you can visit ADC++* in Bavaria, ACCU in Bristol, NDC Oslo. So, plenty of choices in Europe. *Advanced Developers Conference for C++, not related to the new Audio Developer Conference in London.
First you need to understand namespaces. They provide a way of grouping related functions and classes together with a prefix. This stops them clashing with functions that might have the same name in different library. So if I was writing a little library of my favourite utilities, I might want everything prefixed with my initials. Then if I wrote my own cool function for finding the smallest of something, my `cds::min` won't clash with `std::min`. C++ provides the Standard Library. Common utilities that are useful to pretty much everyone. `&lt;iostream&gt;` (actually just called `iostream`, but usually shown with the `&lt;&gt;`) is part of that. In particular, the part providing textual input and output. There is also, for instance, `&lt;vector&gt;`, providing a container of objects. Everything in the Standard Library gets put in namespace `std`. So to access any of it, you have to prefix with `std::`. So `std::cout`, `std::vector`, and so on. For a really great reference on the Standard Library, see http://en.cppreference.com/w/cpp. The only "official" reference documentation is The Standard itself, and trust me, don't try to read that (yet), it's cryptic.
Seems you didn't come here to ask a question, rather to try to make a point.
1. templates. 2. destructors. 3. lambdas. 4. stl.
[removed]
Read the side bar please: &gt; For C++ questions, answers, help, and advice see r/cpp_questions or StackOverflow.
I'd like to add [cpp-dependencies](https://github.com/tomtom-international/cpp-dependencies) which can regenerate nearly all of your cmakelists from nothingness (or completely empty cmakelists) other than flagging what should become an executable. The only things that you need to add are the parts that are not in your source tree - externally imported libraries, dependencies on system components, iOS frameworks, CMake policies, that kind of stuff. I have a few things running on it with about 9 lines of handwritten cmake (to pull in OpenGL and to set the cmake version).
Compare with my weekly blogroll: http://meetingcpp.com/index.php/br/items/meeting-c-blogroll-9th-september.html Also I find it a dangerous wording, that you feature "not so popular/popular" libraries. boost.test is widely used, most of boost uses it. Also its mature I think, so not many changes will happen. And I guess you use some kind of measurement from your platform + activity like commits. At least you should give people a hint how you decide whats popular and whats not.
perhaps [this](https://github.com/sindresorhus/awesome) is what you're looking for? the C and C++ lists are already there - I'm planning on adding the CMake one there as well.
Thanks @meetingcpp. I agree. You are most probably right and the might be better wording.
I followed the [awesome manifesto](https://github.com/sindresorhus/awesome/blob/master/awesome.md)
Since this is /r/cpp, I find it strange to promote "Awesome C++" libraries, then show a section "Featured and popular libraries" with the first entry being `ZXing`, a *Java* library. Is this because of monetizing? Thanks, but not thanks. Personally, I don't think this *awesome*. Edit: typo
The flair just says "awesome" though.
I would add it to the [awesome-cpp](https://github.com/fffaraz/awesome-cpp#build-systems) list.
Well, ZXing has a port in Cpp. However, I think your advice is in place. Most probably we should not feature libs in non cpp langs. Thanks!
Last I checked, C didn't have any of those things either.
Ah, thanks for listing `cmake-unit` and friends. I haven't done as much promotion of that module as I should have - do you know if there is anyone outside of the `polysquare` (e.g., my own) project that is currently using it?
&gt; But it doesn’t really bring anything new to the language. I think partial ordering of templates based on constraint subsumption is something new to the language.
Flair? I'm using Alien Blue. I guess I'm missing out. Still, the flair is only secondary and could apply to the list or its contents. 
Not really. I looked over all the polysquare modules and these caught my attention more than the rest. I'm guilty of not testing them, but they seem of high quality. Sometimes a list like mine has to not only include what is trendy but also set the trends.
right. templates are turing complete, all we need is templates...
But both require the ability to change the primary template AFAIK. For example, if there is `template&lt;class T&gt; void f(T);` and I want `f` to do something special if `T` satisfies `Pred`, I can't just add another function template `template&lt;class T&gt; enable_if_t&lt;Pred&lt;T&gt;&gt; f(T);` because it would result in an ambiguous call. I heard that currently I have to change the primary function template.
Yes, you would. Good point.
 _______________\ | Turing Tarpit &gt; ---------------/ || || ||
Fair enough about C, but I still can't consider the STL a selling point for C++. The little bit of benefit from the worthwhile parts is more than offset by the cost of all the rest of the junk in there. The examples in this thread (accumulate, transform, generate) are all cleaner (and probably faster) with a for loop, and yet your coworkers are going to use that crap and kluge up the baseline.
Link: http://www.slideshare.net/DanielPfeifer1/cmake-48475415
There is also conference in Poland - code::dive, which is free.
Got it. All of you guys thanks! 
Do you have proof for your extraordinary claims?
thanks a lot! This is really well explained :)
I guess we can't use it, I swear this professor is a nut case.
But it's part of iostream, how come you can't use it? Well, time for printf().
Maybe you think it looks like a cleaner arrangement of characters when you write it as a for loop, but when I look and see "accumulate", I know from that one word what exactly is going on with the container. And all i have to do is look at `a * b` to see what operation is being applied. for loops could do anything so you actually have to follow the logic to understand what's going on. The accumulate example is fairly trivial, but once you get into transform etc, the two methods diverge when it comes to ease of understanding.
Which claim did you find extraordinary?
I'm always happy to see functional programming in C++. Thank God. Haskell is just nonsense.
Works with Clang 3.7 not with MSVC 2015 update 3.
Yes, MSVC and expression SFINAE aren't best friends.
There is already the [Tick](https://github.com/pfultz2/Tick) library which provides emulation which is closer to the Concepts TS. It uses a macro for the requires clause because it help improves the error messages and portability with MSVC. Compilers don't have the infrastructure to trace parameters to template aliases, unfortunately. Also, areas where compiler messages can improve error reporting: 1) Merging patch [D8309](https://reviews.llvm.org/D8309) into clang, which improve the stack trace reporting from overload resolution failures. 2) Have the compiler evaluate the boolean expressions in both `enable_if` and `static_assert` so it can know which part of the boolean expression failed and for which values or types it failed for. For example, writing `static_assert(foo&lt;T&gt;() and bar&lt;U&gt;())`, the compiler should report if `foo&lt;T&gt;()` or `bar&lt;U&gt;()` is false, and what `T` and `U` are. The problem with using `static_assert` is that it leaves your functions unconstrained. This can bring surprises especially when overloading based on concepts. Alternatively, a string is valid in a `std::enable_if` parameter, so if a message is desired for an overload, it could be written something like: template&lt;class T, class=typename std::enable_if&lt;( is_foo&lt;T&gt;() and "This overload is not valid" )&gt;::type&gt; void foo(T); 
&gt; constexpr if where appropriate. If only `constexpr if` could be used at function scope. We could easily overload based on constraints and no need to use `enable_if` ugliness.
boost::variant is a completely different beast from a different time of C++ :)
You're being sarcastic, right? If so, I think this is a bit unfair to C++. Value-level functional programming isn't that ugly in C++, most of the ugliness here is due to type-level programming. Though in all fairness, I think type-level programming is nicer in C++ than it is in Haskell. On the other hand, many things that require type-level programming in C++ can be expressed with regular elegant value-level Haskell.
https://securelist.com/blog/research/75990/the-missing-piece-sophisticated-os-x-backdoor-discovered/ You look for something like this?... :P
Yes, it sucks that these licenses don't qualify for maintenance renewal. VA keeps running, you just don't get any updates anymore, which means you can most likely run it until a major compiler or IDE update, then you'll have to pay again. And you'll miss out on bugfixes and new features.
I'm looking forward to this conference. The program looks rather strong this year. The only gripe I have with meeting c++ compared to CppCon is the length. I would love to get more C++. :)
This was already posted last week in the sub: https://www.reddit.com/r/cpp/comments/50rgbe/auto_type_deduction_in_rangebased_for_loops/
Ok, sorry I've missed that. I'll remove this one here.
The license also expires after one year. If you need to reinstall the license can't be used and you have to renew it anyway. In the other hand VA is really worth it.
Np. I just started reading it and I was like... this seems oddly familiar... Then I remembered that article from last week.
Working on that ;)
You can use any version released during your license period, you just can't install the newest build. I've installed older VA builds before for this exact reason. Don't know if this still works the same way though.
I would suggest not calling that function compose, as function composition is very different from overloading. Can be confusing to some.
Is this for professional use? If yes, 80$/year is peanuts. Otherwise, think of something else you pay 80$ for, and compare the value, to you, of that to 1year of VA. I would not ask internet about something like this. 
Check out [meson](https://github.com/mesonbuild/meson). It's definitely my preferred build system. It's expressive enough for complicated builds and it's syntax is beautiful. 
I'm using boost::variant and MPL, but I'd love to switch to something more modern. Is there any backport of the C++17 std::variant for use with C++11/14 compilers, which can be used as a fallback? Related to that, is there any MPL replacement which can use variadic templates which could be used to replace complex variant type list construction?
CMake tends to be the one that I've always gone for, but I always hope that there's something better come along. Especially in regards to project structure, and determining which files are in which builds, and finding includes for other modules. I have considered writing my own build system that actually does everything that I want it to do, but i never actually get the motivation to do so...
One helpful feature from 3.1 and later is [`CMAKE_CXX_STANDARD`](https://cmake.org/cmake/help/v3.1/variable/CMAKE_CXX_STANDARD.html), which lets you declare that you're using C++11 without manually tweaking compiler flags.
Really depends on what you count as "work". In c++ metaprogramming it's more about how many types you create and how complex they are. I assume you are referring to the the "map trick" for 'at'. In that case making your map will be O(N) and indexing will be O(1) where the constant is super small. 99% of the work is creating the map in the first place. Therefore if you want many indexes from a list making a map will eventually be faster because the expensive creation process is memoized. 'at' may not have been a good algorithm to start with as there are trade offs, I started with it as it is one of the simpler algorithms. I would be surprised however if anyone has come up with a faster 'take' for example.
Autotools, i.e. [autoconf](http://www.gnu.org/software/autoconf/autoconf.html), [automake](https://www.gnu.org/software/automake/), and [libtool](https://www.gnu.org/software/libtool/). Many people seem to hate it, but after reading the [book](https://www.nostarch.com/autotools.htm) from John Calcote I had no problems with it. Although it is definitely not perfect, I like them because they do a lot of things "right". Autotools make use of shell and make programming, so it is quite easy to invoke external tools. The remaining items: - It should work on any UNIX or UNIX-like OS - C++14 support: [`AX_CXX_COMPILE_STDCXX_14`](https://www.gnu.org/software/autoconf-archive/ax_cxx_compile_stdcxx_14.html#ax_cxx_compile_stdcxx_14) - Multiple libraries/executables: Declare them in `_PROGRAMS` variables for programs and [`_LTLIBRARIES`](https://www.gnu.org/software/automake/manual/html_node/Libtool-Libraries.html) variables for libraries. Define their sources in the [`_SOURCES`](https://www.gnu.org/software/automake/manual/html_node/Program-Sources.html#Program-Sources) variables. Use [`AM_DEFAULT_SOURCE_EXT`](https://www.gnu.org/software/automake/manual/html_node/Default-_005fSOURCES.html#Default-_005fSOURCES) to enable automake to find some sources itself. - External dependencies: Use [`AC_CHECK_HEADERS`](https://www.gnu.org/software/autoconf/manual/autoconf-2.64/html_node/Generic-Headers.html#Generic-Headers) to check for headers and [`AC_SEARCH_LIBS`](https://www.gnu.org/software/autoconf/manual/autoconf-2.66/html_node/Libraries.html) to find libraries with autoconf - Support for building and executing tests: Use `check_PROGRAMS` and `TESTS` automake variables - Support for generating documentation: Use e.g. `doxygen` and write a doxygen make target. It is explained in the book I linked above and in this [blog post](http://chris-miceli.blogspot.de/2011/01/integrating-doxygen-with-autotools.html) 
Oh but i am on windows. 
1. Install TortoiseGit. 2. ??? 3. Profit!
I'd say target-based syntax, i.e. no global state anymore. Everything (like compiler flags) is bound to a specific target. This makes it so much cleaner and easier to reason about. And easier to integrate one project into another, etc. Also definitely better support for header-only libraries, and selecting/detecting compilers and C++11/14 standard/features. I'm sure I forgot a lot here (I haven't used "old" CMake forever), you can go through Daniel Pfeifer's slides to get an idea of modern CMake. 
Then you just need to set CMAKE_CXX_EXTENSIONS to off 
What additional complexity?
I use CMake at work quite extensively and it definitely does the job. However, I recently started playing around with blaze and I would definitely use it for new projects. It's both simpler for easy use cases and more flexible for complicated ones. In addition to that, it also solves the "missing package manager" problem by taking source code from various sources and including it in your build.
There are 2 kinds of build systems: the ones no one uses and the ones everyone complains about.
Ah, thanks, I'll use this :-) I do wish CMake's documentation was good enough that I didn't have to find things like this out via Reddit, though...
If you require C++14 support, what purpose does autoconf / configure still have?
Reading the docs, it looks like it has direct support for constructing a boost::variant. It definitely looks like it might be useful. Now I just need a C++11 variant implementation compatible with std::variant and I'll be happy!
[This](https://i.imgur.com/RJiLq.png).
Yes, that's the usual solution, and as long as you remember that you need to manually trigger CMake it works alright. But it's not the recommended way of using CMake.
In practice it is not as complicated as this diagram would suggest. Most of the time you only have to deal with `configure.ac` and `Makefile.am` and `config.log` if a configuration fails. The fact that internally there is a dependency of tools should not bother you.
&gt;1. Install Linux &amp; git. 2. ??? 3. Profit! FTFY
It's just disappointing this isn't a solved problem yet or even close to it.
Same here. I'm used to maven from the Java world, where you add a source file to src/main and it just works, and likewise you add a file to src/test and the tests just get run, and fail the build if they don't work.
If you require C++14 then autoconf tries to figure out which compiler flag to use to enable it (depends on the compiler, and whether you want to use GNU extensions for example) and it checks if the compiler actually supports C++14 (and not some limited subset of the standard).
&gt; That straight forward for loop is much clearer and shorter than: Sorry, no. If you've got reasonable familiarity with algorithms, `auto prod = accumulate(begin(vec), end(vec), init, multiplies&lt;&gt;());` is much more readable, more explicit, and less prone to bugs. A `for` loop is much more flexible, which means there's more possible other things it might be doing, and so you have do more work to figure out which of that larger number of possibilities it really is. Using `accumulate` is also shorter in this case, but I disagree with you that that's a selling point as opposed to being readable. auto prod = init; for (long ii = 0; ii&lt;vec.size(); ii++) prod *= vec[ii]; auto prod = accumulate(begin(vec), end(vec), init, multiplies&lt;&gt;());
Its for my personal use for side projects so 80$/year is a number.
Do you know if I can renew just on new VS resales or do I need to keep it continuous?
export/import of targets. Finally, your library can install a file that describes what directories to include and how to link to it, as well as to its dependencies. With linkage between targets, we get a proper dependency graph of packages.
Checking for any libraries or os features that aren't part of a standard?
I really don't get the autoconf hate. Nothing I've looked at is as easy to use or works as well.
Well to be fair the OP doesn't need to support Windows, so autotools definitely do the job. Plus, cygwin is also there with MinGW support.
No team. Just me, and it's just a hobby thing, nothing commercial or serious...
I'm definitely biased, but I love using Blaze at work, but Bazel less so. It's moving in the right direction, but definitely has room for improvement. In particular, it's quite focused on hermetic builds, which can make using platform libraries or dynamic executables a challenge. But if you're just using it for a personal C++ project, it mostly just works.
This. After reading the book autotools totally fit the bill here, at least fulfill my daily work. Be sure to checkout autoconf-archive for the huge collection of autoconf macros, which contains compiler feature checks, library discovery, etc.
CMake is probably the most widely used build system for C++. Personally I enjoy FastBuild more: http://fastbuild.org/docs/home.html It's more modern, flexible and currently still in active development. Has some nice additional features.
Am I the only one who is totally fine with simple handmade Makefiles using gnu make. Building works on cygwin, Debian or redhat without any hiccups. 
Freestanding implementations were _never_ required to implement the standard library in its entirety. Now that C++17 has [`__has_include`](http://en.cppreference.com/w/cpp/preprocessor/include), it should be easier than ever to progressively add new library features (either from new standards or from TSs).
Or call system() and invoke shell scripts.
Even if it's not Dave, anyone who can fill in the details on what happened with C++Next and BoostPro, and various key figures' simultaneous disappearance from the Boost ML, would have my undivided attention... ;-]
I second, we use it at work and gives a really great improvement on MSBuild (default visual studio project handler).
I prefer premake to cmake any day of the week.
I love my handwritten makefile, with perfect incremental parallel builds.
You can keep using older versions of VX that were released during the 1 year you had a license. When a new version of VS comes out, you will probably need to purchase another VX license to update.
I renewed recently after a 2 year lapse with no problem. 
If you like writing a little Python, I'd recommend SCons. My C++ hobby project uses it and handles multiple host/target platforms, makes the binaries depend on successful execution of unit tests, gives you a lot of control over how you link in external dependencies, and has facilities to crawl all files in a directory. Definitely slower (though not by too much) than CMake, but much more extensible. 
Curious, what are the complaints with CMake? At first glance it looks more inviting than autotools.
I don't see how not being a fan of an unusual^1, poorly documented, easy-to-misuse^2 system like CMake says anything about someone being able to handle complexity. CMake may be the best the overall community agrees to use, but that doesn't make it a joy to use. ^1 e.g. if is a function ^2 e.g. "if (release)" works for single pass generators but not multi pass generators
[removed]
In my experience tup is great on Linux, but definitely lacking on Windows. When I was using it on Windows it didn't work well with clang and kept rebuilding my project because it couldn't get the filestamps right. I have since switched to waf and it's great, gives me more control over my build process than tup did, and it's just as fast in my experience.
If my job didn't pay for it I wouldn't pay for it personally. I like it, but I don't like it _that_ much. Especially as newer versions of VS have been making some of its features seem redundant.
While the general consensus seems to be CMake, I'm going to go with [Waf](https://waf.io/). Waf does something fundamentally different than most build systems, or perhaps build generators is the better word, and that's leaving everything up to the programmer. The default flags it uses are the absolute minimum to get your compiler to output a program. So let's start from the beginning, Waf is actually a python library, which honestly is great. Waf being a python library means you can do whatever you like, since you're not limited by some custom language that the build system uses. This also means that your OS in theory only needs to support python, I say in theory since incompatibilities and other fun. &gt;Easy to support C++14, preferably without needing to do per-platform/per-compiler configuration This is as easy or as complicated as you want it to be. You can go all out a la autotools style and check for every single c++14 feature, or a do a version check, or just add `-std=c++14` to the compiler flags. &gt;Easy support for multiple libraries/executables as one project, and dependencies between libraries/executables in the project - especially regarding finding include files if the different modules are in different areas of the source tree. With waf you generally make a wscript (python file executed by the waf executable) per project and recurse in every one of them, this is not required however, but I personally find it nice. Dependencies are done by putting the recursion(s) in the order that you need, e.g. ctx.recurse('Common') ctx.recurse('DependsOnCommon') ctx.recurse('DependsOnBoth') When it comes to finding include files it's as complex as you set it up, you can either search for includes, or just add them to the includes list directly, again, by default Waf does nothing. &gt;Decent support for external dependencies. I'm ok with needing to have installed the dependency libraries first though Again, this is as complex as you want it to be. If you require the libs to be installed, you could just add the library name to the list of libraries, or you can set something up so you can pass the folder and library name via the command line or something, or load the information from a file. Waf supports pkgconfig, so you could also use that. &gt;Support for dynamically finding source files if possible. (I'm used in Java, and most of the Java build tools just use every single file in the source directory for a given module) `ctx.path.ant_glob('**/*.cpp')` &gt;Support for building and executing tests From my knowledge waf does support building tests, but it doesn't execute them. But of course you can have your test command also execute your outputted tests. I have a command called devrun, so whenever I type `./waf devrun`, it builds it, and executes my program with the proper command line parameters so it finds the files it needs. &gt;Support for static checks You mean like checking if something exists at configuration stage? That it does. You can check if files exists, you have certain libraries, compiler features, not as out of the box as autotools, but you can certainly do all the tests autotools does. The checkcxx function allows you to build a piece of code and do a bunch of stuff of with the build result. You can either just bail out if it fails (default), just continue, or output the result to a config header. &gt;Support for generating documentation, and generally running other tools as part of the build Waf supports executing custom build tools, so doing something like this is pretty simple. &gt;Ideally, support for being able to execute tooling before and after test execution - to be able to start up externally required services such as databases. Waf allows you to execute files, which I said at the part where I answered your requirement about tests. To start up externally required services such as databases would probably require running was as root, but I don't think Waf has any problems running as root. Anyway, I hope my incoherent rambling was at least a bit informative, I encourage you to try it out or look up some samples, that should give a bit context to the stuff I said.
Thanks! Still I have two questions already.... &gt; template &lt;typename... lambda_ts&gt; struct composer_t; where does this bring in an operator()? What would that operator do? Generally I do not understand or see 1. where the operator() is called. 2. where the dispatching to / the actual call of the lambdas takes place
Sure about Blizzard? I remember watching a GDC talk about how they build their own build system.
Also work on Windows without Cygwin, simply calling CL.EXE instead of clang or gcc. 
Bloated project structure. Also, this kind of build systems end up being misused by writing an impenetrable custom python/lua layer on top of them.
I love premake, but [I use cmake](http://szelei.me/cmake-is-not-great/)
Multiple libraries in Maven/Java are trivial. The concept of "static libraries" doesn't exist though - everything is just a JAR that can be treated as a Shared library. What it doesn't do is mix the source trees of different libraries together. Instead you have a filesystem like: | pom.xml | /module1 | pom.xml | /src | /main | /java | /test | /java | /module2 | pom.xml | /src | /main | /java | /test | /java pom.xml is the build file that Maven uses to describe the module. Each module is entirely self contained, and can also contain other sub-modules if so desired, and each pom.xml file describes the dependencies for this module - which can be external or can be other internal modules, and Maven just works out the correct order to build everything.
I hate the idea of domain specific languages. Waf is a build system based on Python. Its documentation is dense, but at least there’s a whole book of it. Being Python, you can make it do anything. All you have to include in the project is a single Python script named wscript (though I like leveraging a couple json files, as well).
Hey, I find The Waf Book nearly impenetrable. Do you have a good learning source for Waf? Or perhaps the url of a couple projects that have very clear wscript files? I’ve got it working for my needs, but I’m sure I’m doing a couple things the ‘wrong’ way (keeping my own local 3rd-library installation location index in an untracked json file (with a template json file for others to fill in if they want to compile the code), and I can’t get waf to display the output from my unit tests when it runs them. Also, suggesting one compiler over another for it to configure to. And getting which compiler it’s been configured to.)
It's strange that nobody mentioned gyp. It works great, is easy to set up and generates projects for MSVC, Xcode, CMake and make.
IIRC, arrays are supported, just stupid - when parsed, they turn into a mapping from string integer keys ("0", "1", etc) to values.
The second one. It does not detect 100% yet, but you can use addon files to add on to what it finds. It also only replaces cmakefiles that you marked as generateable (for safety) but it will generate 100% of those files from your source. See also the example in the source tree.
Thanks! I had some misconceptions about std::visit it seems. &gt; Think of each composer_t as a Russian doll that has the operator() of a lambda, plus the operator() of all inherited-from composer_t. This is probably a stupid question .. but how can that thing have two operator()s?
&gt; This is probably a stupid question .. but how can that thing have two operator()s? Just regular function overloading. The same way you can have `foo(int)`, `foo(double)`, and so on. `operator()` is no different.
Yeah, the Waf book is definitely long. It's however a good source of documentation when you combine it with the API docs. For me personally I looked for anything I thought would be relevant in the Waf book and I also looked at the examples that can be found on their github. There is the [playground](https://github.com/waf-project/waf/tree/master/playground) and the [demos](https://github.com/waf-project/waf/tree/master/demos), both provide numerous samples on how things are generally done using Waf. If you have more questions on how a certain thing works you could either try the mailing list or ask on IRC, server:freenode channel:#waf. If you ask on IRC you can mention me if nobody answers for a while, my nickname on freenode is Serus.
Is there a reason why globbing is not recommended? It's the first thing I look for in a build tool and CMake is actually one of the few that have a simple solution.
Something like this? (The trick for writing makefiles is trying to work from the end product to the sources, not from the sources to the end products, default rules are also targetting to build in the directory where make is invoked, so you have to work without them.) PROGNAME=foo OBJECTS=foo.o bar.o qux.o BINDIR=bin OBJDIR=build SRCDIR=src $(BINDIR)/$(PROGNAME): $(OBJECTS:%=$(OBJDIR)/%) $(CXX) $(LDFLAGS) -o $@ $^ $(LDLIBS) $(OBJDIR)/%.o: $(SRCDIR)/%.c $(CC) $(CPPFLAGS) $(CFLAGS) -c -o $@ $&lt; $(OBJDIR)/%.o: $(SRCDIR)/%.cpp $(CXX) $(CPPFLAGS) $(CXXFLAGS) -c -o $@ $&lt; This lacks the automatic generation of dependencies. Setting a correct value for CPPFLAGS and using -include $(OBJDIR)/*.d should work with gcc and other compilers able to generate them as a by-product of the compilation (here I assume they are named something.d and generated in the object directory).
Well look at that, I have never seen the `$(OBJECTS:%=$(OBJDIR)/%)` syntax before. I assume it modifies the output of $(OBJECTS) without overwriting the content of OBJECTS? I always tried to generate OBJECTS at the beginning via something like `OBJ=$(patsubst %.cpp,build/%.o,$(SRC))` and got into trouble trying to strip directory prefixes from the paths in later steps. Adding build units manually to OBJECTS like in your example is a bit bothersome but I guess I could get creative and use git hooks or write some vim macro to generate that string and place it in the makefile at opportune times.
Most (if not all) normal sessions would, but not lunch-sessions, birds-of-a-feathers, meetings (like SG14 game-dev meeting if there is one again), etc. This stuff is A) harder to record since it's organised more spontaneously or more discussion-like, and often more informal, and B) some content the organizers want to keep exclusively for people physically attending (fair enough imho! since all the videos are **free**!)
why does everyone hate on cmake? I'm not saying it's perfect, but it's big improvement over anything else...
As with C++, there's a much cleaner language within struggling to get out. One thing I recommend everyone to stop using is "file(glob...)": this makes it guesswork what CMake will build, and requires the manual re-run to pick up new source files. Save that manual-work slot for adding the file to the CMakeLists. Another thing is its auto-detection for various environment parameters. Cool if it works, sucks if it doesn't. Better give it explicit parameters ("today, I want to build for Cygwin; tomorrow I want to build for Win32"). OK, and another thing that makes CMake suck is its abuse of Makefiles in the Makefile generator. "Recursive make considered harmful".
Go minimal: scripts, batch files Build system is a black hole of wasted time, just isolate all process from each other and build step by step. We use Anthill but it sucks, always broken... Seriously, spend no time on it because you'll still try to find the best way to do it in 5 years with different tools. If you use a modular, step by step approach, you'll be able to easily switch a step for another one (like changing test engine easily)... A good, scalable and stable solution doesn't exist... 
Why is it better than gyp?
Yea, as best as I know that means regenerating everything on every build. But you should be able to do that without blowing it all away first I think. I've not tried in quite a long time though tbh.
If you reconfigure it without throwing it all away, new targets gets added, but old non-existing targets remain there, so if you rename or remove a file, you really need to delete your build directory and start over.
Everything is written as a function. Don't see any problem with that.
&gt; Also, if I got a person on an interview complaining about cmake, I would start thinking about how is such person able to cope with C++ language and its complexity. If he ONLY complains and can't see good sides of CMake, then OK. But why shouldn't he complain at all? It's good to be critical if you're a developer. I hope you didn't mean it this way. I complain about CMake, not because I can't handle it. I can. It's because its language, API and documentation are so unnecesarly bad...
Unfortunately, it is impenetrable to me as well. I sometimes resort to source diving. I have had good luck asking on the mailing list. As for getting the output of tests, you can run [custom commands] (https://waf.io/book/#_custom_commands) as part of the build. I have not done it myself, so I do not know how painful it is. 
You do not have to delete your cache and cmakefiles. 
all the core principles are here : https://cmake.org/cmake/help/v3.6/manual/cmake-buildsystem.7.html
Calling system() is a pain when supporting multiple operating systems. One of the nice things about CMake is the abstraction layer over basic system commands, such as copying files and executing programs. I don't have to worry about the differences in path separators, escaping spaces, etc.
I'm objecting to use "file(glob...)" as the means to gather up source code just because it seems convenient during development. I'm not objecting to using "file(glob...)" to locate plugins. Locate a list of subdirectories in folder `plugins/`, pass them to `add_subdirectory` - fine. Actually, our CMake-based build system works totally like that. But that's about the only place where "file(glob...)" makes sense. My criticism with CMake is that it has few if any conventions, and people happily start using its internals wherever they see fit. My CMakeLists consist of an `add_executable` and maybe some `add_definitions`. That's easy to reproduce or to port. Others poke within internals using target and file properties, dozens of if's with all sorts of semi-undocumented variable names, etc. That's definitively not a joy to maintain if no two CMakeLists look alike.
Note that there is a library in the boost library incubator (www.blincubator.com) by Paul Fultz (called Tick) which implements these ideas. I believe that it includes support for overloading based on concepts. It's hard to tell from the poor documentation though. In spite of this, I've used some of it's facilities and it seems work as advertised. I believe it was inspired/derived from Eric Niebler's implementation which is part of his ranges library.
One reason I can think of is that then making a change is intrusive -- it requires changing the enum. By using distinct types the system is extendable from different libraries, etc, without having to access the common enum.
Although VS is getting better and faster, VAssist is a necessity on our project. We have a large (multi million LOC) codebase. VS still chokes on it, while VAssist handles it without a hitch. It's improving, but still has a way to go.
So if you have 10 lines, you will delete 10 times?! (With tabs). Point being: if I want to change the indentation, probably the best command is "change indentation", not deletion or insertion.
I was looking forward to presenting my C++ HTTP/WebSocket library Beast in a 15 minute lightning talk (https://github.com/vinniefalco/Beast) but they changed the format at the last minute so now I'm in limbo.
I think /u/gnzlbg is referring to the feature described in [P0051R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0051r0.pdf). The proposed `overload` is similar to the `lambda_util::compose` described in this blog post, while `first_overload` chooses the function object based on order.
I use this and I'm quite happy with it. It's not exactly std::variant (it follows Boost's design) but it doesn't use the MPL. https://github.com/mapbox/variant
Yes thanks, that paper is what I was referring to.
You need to include `&lt;functional&gt;` for `std::ref()` in `thread_simple.cpp`. This doesn't explain how `std::thread` decays its arguments, and the reasons for that design, and why that makes `std::ref()` necessary. `reference_wrapper` itself is something that few programmers are familiar with. `promise_simple.cpp` has simultaneous calls to `cout` on different threads; while the Standard promises that this is not a data race, you can get interleaved character output if the library is feeling sufficiently uncharitable to you. "In this execution something called data race is shown. That can be seen because the execution of the same program can generate different outputs (even if the result value is always the same)." is both wrong (not a data race) and confusing (doesn't explain why the output can differ). There is no discussion in `promise_simple.cpp` of alternatives (the more basic one being a mutex to synchronize access to a shared variable). It is much easier to understand things when they're presented alongside alternatives, so you can see how they differ. I am not a fan of this article. It doesn't talk about data races, or when it's appropriate versus inappropriate to use dedicated threads, which are critically important to understand. You can't just say at the end, "didn't talk about data races" and make it okay. Writing *correct* multithreaded code needs to be the paramount concern from the first sentence. "Moreover, the method get synchronizes the future." This is a terrible non-explanation of how futures work. "Future is gotten from an async class." No, `std::async()` is a function (template), not a class. "again if people is interested in this topic" should say "are".
maybe a stupid question but is std::variant just better union?
I am not a native English speaker. I will fix that mistake tomorrow after work.
I strongly disagree, globbing your source directory is correct, and manually specifying the files is a tedious and needless waste of time. It's not "guesswork" -- it's going to build everything in the directory. What is ambiguous about that. And even if you manually curate the list, you are going to have to rerun cmake anyways whenever you update it, and you will get "confused" if you don't. I fail to see any logic in your argument.
Agreed. I've seen horrible, horrible bugs shipped as a result of people putting globs in a build system, then not discovering components have been shipping broken for months in some unrelated part of the system because inputs were moved around, and nobody noticed because the build "succeeded". Globs make figuring out what depends on what in large build trees impossible.
No, because when you add the file to `CMakeLists.txt` the build system sees that it is out of date and runs the correct bits for you automatically. You just type `ninja` or `make` and you get correct incremental behavior.
Well, if I look at my cmake build files they are still full of if()/endif(). Our projects still need to invoke custom commands and it's quite cumbersome with cmake, so in the end I find it much cleaner to just invoke python script and deal with it there. Also, I'm not a big fan of how cmake handles custom build steps. I.e. on xcode project you get make invoked for certain things, while other are done as part of xcode build, feels very messy.
Anthony Williams wrote a book. I haven't read it, but he knows what he's talking about.
It's illegal I think. You're not allowed to modify a value twice in one expression. I don't know the standardese on it though.
Now that's confidence!. ;)
You can mutate a value twice in one *expression*; `x++ &amp;&amp; x++` is fine. You can't mutate without an intervening sequence point / happens-before relationship.
but they're both constant right? I suppose I don't understand the difference.
Git can also touch the cmake file when pulling or switching branches, so other peoples machines will update when I add a new source file. 
I like meson the best. If you need windows with visual studio, then cmake. But I favor meson much more.
And it is not overengineered like waf. I tried once I liked it the most. But its lack of support for testing without writing a bunch of code and other "overabstractions" made me drop it. I do not mean it is not powerful, I just mean that things ended up looking like plain programming almost. I would go for meson or cmake.
Isn't it relatively easy to sponsor work permit/blue card in EU? So European companies who can't do this don't quite make a good impression.
So? Autotools support all POSIX-based systems (with Windows system support through Cygwin, though non-native).
This idea is usually called "phantom types".
That's such an incredible thing to say that I don't know where to start with a response. I can't help you because your misunderstandings are so great and steadfast. I just hope you don't cause confusion for others.
How is this article related to C++?
&gt; gn Does it support macOS as well as windows?
Yes, that's the entire point of them. But: http://honermann.net/blog/2016/07/18/refining-concepts-improving-error-messages/
I didn't really say that he can't complain at all, but you are right, it has to be seen in the whole context of a discussion. I don't think that documentation is bad, it is just painfully lacking a complete tutorial, with setting up external dependencies, using subdirectories etc ...
ah yes that is essentially what brigand is doing now https://github.com/edouarda/brigand/blob/272414075719e0e9060874226a2337595aa80686/brigand/sequences/at.hpp. The thing is that creating the list of void* (although hana seems to be doing it better than brigand) is not constant (although compilers are getting better) and you can't just add a pointer to any given type (because there are no pointers to references) so you are creating intermediate types of varying complexity. Therefore I would not hink of this as O(1). There is an O(1) https://gist.github.com/porkybrain/6059d0b51d41e0b97e3b408eba13b94a#file-at_bench-cpp-L232 here is the map trick I was referring to. When indexing into an existing map it truly is constant time and super fast, making the map is slow however, we are currently experimenting on how to build a map in a faster way.
I'm not entirely sure that this is a fundamental issue with concepts and not just a "QOI" as the article uses the term.
Yes, it is just a QOI, but I don't expect much improvement over normal templates, simply because both have to list the call stack and stuff.
The build system is pretty much platform agnostic, but you do need toolchain definition for every platform/compiler in the build folder. So you need toolchain definition for clang on OSX.
Wow, that looks great. It could be the one I'm searching for a long time (coz beginner game dev). Thanks. :)
[removed]
I've used it for 6 years now. You can get the personal license and use the latest version of that license expiry. I personally only renew the license after 2-3 years because the updates bring very minor improvements or bugfixes which almost always don't affect me or I can live with. I refresh my license every 2-3 years purely because they add proper support for Visual Studio 2010-2013-2015 etc. My last renewal was when they released official support for 2015, I haven't renewed it since and it works just fine.
Is [this](https://hownot2code.com/2016/09/09/operation-priorities-in-cc/) a mirror of [this page](http://www.viva64.com/en/t/0064/)?
Use [KDevelop](http://kfunk.org/2016/08/23/whats-new-in-kdevelop-5-0/). It's clang based C/C++ language support is always 100% correct with respect to code navigation and code completion. Nitpick: It's for Linux only right now. A Windows Version will be available soon, though, so stay tuned :-)
The conventional solution to this problem is to have a set of named coordinate frames, and a naming convention so that it's unambiguous which frame each variable or argument is defined in. So a point defined in layout coordinate would be `pL`, a point in scrolled coordinates would be `pS`. Transforms between frames will include both frame names: pS = dcmLS * pL; Here `dcmLS` is a direction cosine matrix which transforms a point in the L (layout) frame to a point in the S (scrolled) frame. With the proposed system I'd be worried about multiple identical copies of functions being generated. Say I've got an algorithm to find the distance between two points. Using the old system, we could call the same code irrespective of the coordinate frame of the points, as long as they both have the same frame. With the new system, points in different frames have different types, so new code will get generated each time a new frame is used. We'd get the same problem with transforms. Say we want to convert a direction cosine matrix into a quaternion; we now need to encode both frames in the type, and so we'll generate new code for each combination of frames. I agree it's nice to have the compiler enforce correctness, but I'm not convinced the complexity is worth it.
Imported targets. Allows export and re-import of transitive dependencies. I.e. automatic generation of library configuration, so find_package(xxx) works transparently.
So? You replied to this comment by STL: &gt; autotools is the devil made flesh. Source: I have to deal with it when building my MinGW distro. Windows isn't a Unix, and when things go wrong (as they often do), autotools introduces so much additional complexity. 
I think the improvements for users of a template are significant since it keeps the error messages focused on their (presumably incorrect) code rather than (assuming that overload resolution would have succeeded) the template author's (presumably correct) code. Support for separate template checking would then answer the question of whose code was actually incorrect.
so.. on the windows implementation, calling reserve() bypasses the small string optimization even if the reserved size would fit in the size of the string object? That's a bit unexpected..
gn can generate hybird Xcode, MSVC, etc builds (so you get code indexed by IDE, debugging support, etc, but the actual build is performed by ninja). Regarding gyp being abandoned - not sure how else to call [this](https://groups.google.com/forum/#!topic/nodejs/0QiIr7jr2iY).
Does the new search ( Ctrl+',' ) still makes VS chokes? I find that it is now the fastest way to find code. The main issues are with the previous ways to get infos about the code.
Bingo, a typesafe union.
I just checked, and our implementation **doesn't** dynamically allocate memory in this situation. We end up executing a lot of unnecessary code, but ultimately the Small String Optimization remains active and there are zero allocations/deallocations. I've filed VSO#262848 "&lt;string&gt;: reserve() does too much work".
Sure! Game industry on a 4 or 5 million LOC codebase (engine + tools)
It remains awesome to me that you're so involved with the reddit community and help out on so many small issues brought up here =) 
All right, it is not used to build Chrome any more, but it's being maintained and actively developed. Thanks for the info about gn being able to generate project files, I'll check out how well Xcode and MSVC work together with Ninja. But experience with CMake makes me believe it will not be as comfortable as working with native project files that use the IDE's build system.
&gt; you can get interleaved character output Can you sneak a change into the ms implementation, so it won't break the lines? :) (to be more like unix)
We're just happy to have people reporting issues like this. One of our biggest performance problems in the library is that we don't really know where our performance is a problem unless someone can come to us with something that's slow, and we can get it in the profiler, since library perf is a function of what people do with the library. For example, the `string::push_back` work we did making that thing 3x faster in 2015 Update 2 and another 40% faster on top of that in Update 3 is the direct result of a picojson example someone submitted (and we submitted minor perf tweaks for that [upstream](https://github.com/kazuho/picojson/commit/f0edfa6d73b3f4d4c1e49df2d946410861554bb2)).
If I could jumble up the characters more aggressively, I would. This is not behavior that is worth strengthening.
It's horrible. :(
Littered with Google and legacy code specific desicions. At least it seems to get better every other year. 
Edit: Sorry. Miss-understanding on my part.
It's deleted now … What was it?
Reworking the SSO implementation is on our list of things to do, although it needs to happen in a new major version of the libraries. It's tracked by VSO#154236 in our internal database.
A link to the current version of the [Google C++ Style] (https://google.github.io/styleguide/cppguide.html). 
Really? I think the paradigms used in his library are the state of the art of TMP in C++14. I also think Louis is a great speaker - all of his talks regarding Hana and the used implementaiton techniques have been enlightening. I would love to see a book that teaches TMP starting from type-value encoding *(a.k.a. dependent typing)* and immediately uses modern C++14/17 techniques such as generic lambdas and 'if constexpr'. I'm very confident that Louis would be able to make it happen :)
Throwing huge amounts of caching to solve a problem that shouldn't exist doesn't sound like sensible use of technology. That aside, the CMake projects I work with sometimes take minutes just to generate the Makefiles, and often take hours to build from scratch. "In many projects running cmake takes no time" just says you have not seen huge projects yet.
Well, IIUC, your previous quote is a better reply to STL.
What would be really nice is that compilers would somehow "see through" the combination of `reserve(n)` followed by loop over an `n`-element range with `std::back_inserter` and eliminate the `n` `push_back` calls altogether in favor of direct assignments.
There is a difference between &gt;OP doesn't need to support Windows and &gt;supporting at the very least OSX and Linux
would a Range-v3 overload of `vector::assign(rng)` help in that respect? 
Sorry. I'm an idiot. I don't usually post on reddit r/cpp. Entirely misunderstood the meaning of '/u'.
That kind of analysis is *really really really* hard for the backend because push_back has a bunch of aliasing checks, conditional memory allocation, and because it writes into the string block itself through `char *` (when the string is in "small" mode) which is allowed to alias with everything (so the BE has to prove that the write through the destination iterator can't flip the string from small mode into big mode or vice versa). It's also hard for us to do in the library because the user can do something like: inline std::string to_upper_v4(std::string const &amp; text) { std::string result; std::transform(std::begin(text), std::end(text), std::back_inserter(result), [] { if (rand() % 2) { throw something; // !!! !!! !!! } return blah; }); return result; } so if we do any kind of speculative resize special case for `back_inserter` then we also need backout logic built into the algorithm.
To clarify: `assign(range)` does something like: _Destroy_all_the_things(); _Initialize_from_range(range); So the inputs for your transform iterators would be dead when trying to _Initialize_from_range.
The threading support in C++ is essentially unchanged from C++11.
It was kinda apparent that it's only "for now" and not really desirable. So suggesting a "broken" tool which is not really a cross-platform... (I remember trying to fix some bugs in llvm build to make it compile correctly with correct artifacts on OS X. Thank god they've got rid of it in favor of cmake).
You can get 23 bytes out of a 24 bytes `string` by storing the size as `(23 - size)` so it goes to 0 when the string is full, serving as a null terminator. Shame that clang doesn't go the extra byte.
Can I upvote this 1000x?
&gt;The *Enforcement* sections seem to be written thinking of an automated linter, is there a linter around that enforces these guidelines? Yeah. Microsoft has one, I believe. And... I can't recall who else. Bjarne talked about it at CppCon last year if you want to learn more. The talk is on Youtube. &gt;These guidelines are basically what we enforce by hand through reviews alread, so covering them with a tool would be a huge time saver. Yep! The whole point of the guidelines is to write code like "owner", which does absolutely nothing but allows a static analysis tool to check to see if you're doing something bad with pointer ownership. Check out the guideline support library. 
Pretty decent, but goes into woo in some places. Like rejecting CamelCase without any rational argument. Imho unprofessional.
&gt; more inviting than autotools. That's largely also true of a sign saying "Trespassers will be shot, also beware of angry dog."
&gt; Like rejecting CamelCase without any rational argument. That part is the least important section in the entire guidelines. 
No kidding. I actually found one style that does this: `ACRO_Finder`, `My_OTA_UpdateBuilder`, etc. So using underscores around abbreviated parts while keeping them all caps. I typically end up doing `AcroFinder` and `MyOtaUpdateBuilder`, but I thought that was pretty neat.
Psh it's easy, I use llvm guidelines because they fit what I want [Do not use RTTI or Exceptions](http://llvm.org/docs/CodingStandards.html#do-not-use-rtti-or-exceptions) Now all I need to do is get most people to agree with me. Most people disagree with me but you got to start somewhere.
It's called Microsoft GSL. Here is the Github repo: https://github.com/Microsoft/GSL
Yeah, clickbait post title.
Surely `call(f)` is more natural syntax to write though. 
Oh man, that's clever. 
Your post has been automatically removed because it appears to be help/homework related. If this has been in error please message the moderators. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Howard is on record here saying that using the extra byte is a tradeoff for zero-init: http://stackoverflow.com/a/21710033/82320
IMO it's less MSVC++ lagging behind and more Howard's implementation being extremely clever. libstdc++'s implementation looks like ours; we're playing standard library string representation leapfrog :) There's still a lot of performance value on the table for our string implementation that doesn't require a bincompat break. Please report bugs!
By the way, I'm not sure if you're new to reddit or not, but if you reply to your own post instead of editing it, the person you were replying to doesn't see your addition.
Thanks for the info. :)
That's OK. futures are allowed (in fact, typically do) outlive their promises. ~~As far as I know this code should work.~~ Actually, it's broken. See here: https://www.reddit.com/r/cpp/comments/52j3fk/weird_problem_with_stdpromise_and_boost_asio/d7m9y91
Don't forget the crazy amount of legacy C code.
Oh, you are right. I wonder how more efficient zero initialisation is. Does anyone have any metrics on that ?
&gt; ignorant up voters Seems to be a reddit-wide thing. I only upvote something if I've read the article and verified that it is not fake, false, misleading, or otherwise rubbish , it seems I'm a minority.
It could be that fut.get() may have thrown an exception in which case your function would return before you would expect. Perhaps you check that. 
Only on little-endian, though.
That's a lot of tickets.
[Here's](https://github.com/elliotgoodrich/SSO-23/blob/master/README.md) another good article about SSO.
Modern C++ Design by Andre Alexandrescu?
&gt; Perhaps try this: [this,promise=std::move(promise)] That won't work as ASIO [requires handlers to be CopyConstructable](http://www.boost.org/doc/libs/1_61_0/doc/html/boost_asio/reference/Handler.html). Capturing a promise like that would create a move-only lambda.
I like to commend you on being a very approachable and friendly person! Compared to MSVC from 2012 and before, knowing you exist makes me quite happy.
So we need to learn m4 and write some macros?
In CamelCase shops, I've always insisted that acronyms should be considered words (XmlReader, not XMLReader) for this reason. I still prefer snake_case.
[Discovering Modern C++ ?](https://www.amazon.com/Discovering-Modern-Scientists-Programmers-Depth/dp/0134383583) by Peter Gottschling. 1st edition: December 27, 2015 chap 5 is on meta-prrogramming technique: 1. Let the Compiler Compute 2. Providing and Using Type Information 3. Expression Templates 4. Meta-Tuning: Write Your Own Compiler Optimization 5. Exercises 
&gt; The only way to ensure this is to have the last thing the background thread does is to unlock the mutex. With Boost.asio that's not possible though, as the `io_service` thread may outlive the object for decades. Joining that thread is obviously the easiest and always correct way. &gt; No, it isn't fine. Putting things at scope exit does not affect data races. In this case "scope exit" is the closest we can get to "last thing in background thread". The data race on `_counted_down` is obviously resolved by extending the lock. Then again, this implies that `~lock_guard` does not yield… Unless boost.asio supplies a way to wait for termination on a specific handler (and not the `use_future` overloads, those just use an `on_completion = promise.set_value` as the OP does), this is not really solvable as far as I understand, and first line being `on_scope_exit { locked { notify(); } }` is the closest we can get.
Well why does it matter that some call does a type "conversion" on the fly? If the library is well-designed it clearly should not. What danger so you see? 
The GSL is one part, providing library things like owner, but you still need an actual tool that you run to use the library for static analysis and tell you about the violations. 
Honest question: why C++? I know that this is a C++ subreddit but there are languages that are a lot easier to start with and that aren't any less powerful depending on what you want to do.
&gt; Gradle I recommend Gradle as a build automation platform for C/C++ because it solves a much bigger problem than simply compiling C/C++ code for multiple platforms. We used CMake for embedded C/C++ projects, but also have Java tools to build and various other languages (CEU, Python, Lua, etc.) We found our build system business logic, which is complex, was being distributed among various tools (CMake, Jenkins, Vagrant, Docker, etc) and was becoming difficult to maintain and impossible for developers to contribute. The biggest benefit we have found with Gradle, besides being polyglot and very extensible, is the fact that we can pull all our build system business logic back into the source repository. It allows us to automate the full development, build, versioning, test, packaging, release processes for all our languages, factoring common logic into reusable plugins. It encourages transparency and developer collaboration on the build system because the build logic is code and right in the source with the applications. We have had developers submit pull requests against the build system (instead of just complaining about it) to make improvements or add functionality which is the ideal situation. Gradle is not a magic bullet for native builds. While it is easy to get simple native C/C++ code to compile, there is a pretty steep learning curve to be able to extend it. Groovy/Java experience is extremely helpful, but many C/C++ developers visibly twitch if you even say 'Java.' Gradle is also under very active development, including the underlying model. We have chosen to invest the time and energy to learn it because it solves a much bigger problem for us than just compiling C/C++ code.