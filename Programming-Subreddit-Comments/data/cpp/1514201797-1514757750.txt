I wish I could use it, I am not quite sure it would be too ergonomic yet. For the main application I am working with, the first issue I have to face is the massive existing C++ codebase with heavy reliance on templates. Replacing it wholesale is just impossible, replacing some parts seems complicated due to the foundations making heavy use of C++ advanced functionalities (like variadic templates). It is not clear to me how easy or difficult it would be... even if my colleagues thought it was a good idea. Even imagining it could be done, however, there are still some uses of C++ generics which Rust cannot translate 1-to-1 which would make porting a bit awkward. The lack of (1) specialization (it's coming), (2) const generics (it's coming) and (3) generic associated types (it's coming) are quite crippling. It's unclear if the lack of (4) variadics, which is not being worked on, would be annoying. I think that in 6 months/1 year, Rust will definitely be an outstanding contender for replacing C++ in the application I am working on; though the "upgrade" path is not clear. Also, and to be honest, there are quite a few instances of "fuzzy" ownership in the codebase today which *probably* work, and cleaning them up would likely require quite a re-engineering.
 "relative" / "/absolute" ts: "relative/absolute" final: "/absolute"
You always should remember the low level representation of everything. In example, interfaces are implemented via base classes and *must have virtual destructor*.
Can you give me an example?
Hmm.. this might lead to bias. For example, if a file is sufficiently abstract with well named types, variables, and functions, then it will most likely get marked as pleasant. However, looking at the same project and looking under the hood might be more likely to be marked as unpleasant. I feel like this is trying to take a number of attributes and merge them into hot or not, which is going to lack too much resolution to gleam any light on anything.
But what if I don't want to modify my system just for the sake of contributing three lines of code to project X?
What do you mean "modify your system"? 
Standarizing member initializers or argument names is not a breaking change. No one has used them before. Also, why would we use signed integers for size type?
For the time being. I've no doubt we'll see something being added. Also, Rust seems to run as fast as C++. Do we really need TMP?
Yes. Or some other way for doing things compile-time. As far as I know there's not even `static_assert`, let alone `constexpr`.
I think in the scenario described, if the constructor `T(T&amp;&amp;)` is a better match than `T(const T&amp;)`, then `T(T&amp;&amp;)` will be selected even if it is marked as `= delete`. So to be able to use the copy constructor with e.g. a temporary, you have to cast the temporary to `const T&amp;` to have the compiler select the copy constructor instead of the move constructor. I don't have a compiler right now, but I think this should force the use of the copy constructor: DeletedMove foo(static_cast&lt;const DeletedMove&amp;&gt;(DeletedMove()));
Do you need static_assert?
Conan, build2, cmake, vcpkg, plus whatever your Linux distribution ships with. You don't need one build system, you unfortunately need _all_ the build systems... :-( 
Interestingly enough there are language-aware diff tools for C++, but I've not been able to get any of them to work with Git. Has anyone else had any luck? I also noticed quite a few of them you have to pay for I find the default diff programs pretty terrible. For instance they don't "understand" when you've added a new function and will do funky things with neighboring function definitions
Can we make everything automatically inline (in this sense)? Is there any benefit to it not being the default? It sure seems to work fine for member functions, why not just all functions? Same for constexpr, actually - on functions(!) it is just a suggestion now. So why not make it the default? Let constexpr variables just call whatever they want, the compiler will issue a warning if it fails (just like now). _Clarification: `inline` already means two things: one is a suggestion to the compiler that the function should be eligible for inlining. The other is that the function may be declared multiple times without it being an ODR violation. I refer here strictly to the second meaning; the first meaning can stay as it is._ _Several people seem to believe that `inline` has an additional meaning, something like "don't remove a function during optimisation if it isn't marked as inline". It may seem to work that way right now, but that is strictly a non-standard use._
&gt;I strictly write header only code and my compile times have increased substantially compared to having each header file have its own translation unit. So uhh, that's not really a benefit of header-only then, is it? In fact it sounds like an excellent reason to stay away from that kind of source organisation...
I adore the wxWidgets string class. You can convert it any time: wxString str; str.wc_str(); str.c_str(); str.utf8_str(); etc. http://docs.wxwidgets.org/3.0/classwx_string.html Why can't it be that easy in the STL?
What about Fastbuild? Anyone using it extensively?
We use it to build Factorio on Windows. I don't know if that qualifies as extensively. It works well. The unity-compilation and the auto-generated solution files are the 2 main things we use it for. That being said, removing the Boost library from our code base gave about the same compilation speed gain as switching from standard precompiled headers + VS compilation to the precompiled headers + unity compilation Fastbuilid offered.
Of course, I use it a lot for checking types and non-type parameters such as various buffer sizes, divisors, etc
Not with `build2` -- we have no plans "integrating" (read contaminating) with anything except perhaps system package manager (dpkg, rpm, etc). Yes, that means a long road ahead converting useful libraries to `build2`. What may make a difference is C++ modules: if the only/easiest way to take advantage of modules is by switching to `build2` (and 3x build speedup is not something to sneaze at), then things could unravel a lot faster.
Yes, my thoughts exactly: Conan uses whatever build system the project happens to use which means whatever problems it might have are now your problems.
&gt; Can we make everything automatically inline (in this sense)? well, no because then you couldn't have for instance dynamic libraries since all functions would end up being optimized out for being inline and unused (since there's no calling code). foo.cpp: inline int foo() { return 0; } bar.cpp: inline int foo(); int main() { foo(); } build with: $ clang++ -O3 -shared foo.cpp -o libfoo.so $ clang++ bar.cpp libfoo.so Undefined symbols for architecture x86_64: "foo()", referenced from: _main in bar-7788e2.o
Rust has an `if let` statement with pattern matching which allows for something like this. It's as general as it gets and I'd love if C++ had something similar. if let Some(val) = map.get("key") { // val is set to the value in "key" } else { // val is not declared }
Given that exported functions must already be marked as being intended for export, I don't see how that could become an issue. 
If you have to do anything like this, it's because you did something wrong! Lol
well, at least in current compilers the two concepts are unrelated. eg adding ` __attribute__ ((visibility ("default"))) ` to `foo()` in my example doesn't make it exist in any way in the resulting shared library. It has to exist in a given .o before being exported and when inline and not used, the function just isn't present at all.
yes, of course, the whole standard library, boost, etc... are all wrong with their use of inline functions. Do you ever write struct foo { int bar() { /* do stuff */ } }; ? because that's exactly the same except you don't have to write `inline`.
Yes, the compiler plays a big role in the performance of the language. But it's also tied to the language and is not associable to another language. C++ is fast because you are in control. You can do what you want but you also need to know what you're doing! Which goes back to the seniority of the user.
No, the language is not perfect, yes, there are a lot of better language for different contexts. But c++ is the best for a lot of use cases and have to be used by somebody who knows how to use it correctly. Which is why the users level of knowledge is relevant to quality of the code. Other languages trade off usability for performance and c++ doesn't. It's not defective. It's purposely kept like this. I don't mind the downvotes, I still think that being a good programmer is mandatory to using c++
It's useless because using it will not change the output of the compiler. Like you said, it's just a suggestion.
Std were written before the inline keyword became irrelevant and they didn't change the Api after. That's it! The code you wrote might be inlined or not, the compiler will decide. If you optimize for code size, it will not inline a lot, for performance, yes...
FWIW, in `build2` we believe C++ modules (we've observed 3x build speedup) and distributed compilation (with CPUs like Threadripper) is the future. Everything else (precompiled headers, unity builds) are just dirty hacks that only make the build more brittle.
I meant in Rust.
For linking lld is even faster then gnu gold. From https://lld.llvm.org/ : &gt;Program Output size GNU ld GNU gold w/o threads GNU gold w/threads lld w/o threads lld w/threads &gt;ffmpeg dbg 92 MiB 1.72s 1.16s 1.01s 0.60s 0.35s &gt;mysqld dbg 154 MiB 8.50s 2.96s 2.68s 1.06s 0.68s &gt;clang dbg 1.67 GiB 104.03s 34.18s 23.49s 14.82s 5.28s &gt;chromium dbg 1.14 GiB 209.05s [1] 64.70s 60.82s 27.60s 16.70s
&gt; But what if I don't want to modify my system just for the sake of contributing three lines of code to project X? Then you create a chroot with all your dependencies. Or maybe use lxc or systemd-nspawn. 
Conan depends on python to build my C++ code. No thanks.
&gt; The code you wrote might be inlined or not, the compiler will decide. you seem to believe that this is what the `inline`keyword does. That's not the case. The `inline`keyword is about bypassing the one-definition-rule. Without `inline`semantics you wouldn't be able to include &lt;vector&gt;, &lt;string&gt; or &lt;regex&gt; in two different files because it would break ODR.
My university textbooks
The are many flaws, but the one that impacts my day to day the most is probably the total lack of reflection. Logging and parsing of business logic enums, serialization of data, are such essential use cases that not having reflection is really rough. That said, I can work around this well enough use macros. It's not ideal but in practice it gets the job done. I guess that will be a pattern; it's a flawed language (like all languages) but at the same time it's relatively easy to work around the flaws. Another issue is the whole move vs default constructor dichotomy. You almost always want your class to be movable, but that basically necessitates some kind of default state. For a lot of business logic type classes you definitely don't want a default constructor. 
Yes, but using inline to have multiple copies of the inlined code makes the code bigger. And not having it inlined requires a jmp to another function so yeah, inlining code is all about code size vs performance. But those are compilers directives so using the keyword inline and asking the compiler to favor code size is just stupid. In the end, the compiler will decide what is more appropriate to do.
I meant to say compile times decreased, I will edit that.
I second the removal of Boost for a speed boost. I once got a 20x compile time improvement (30 seconds, down to 1.5 seconds) from hacking Boost up to provide specialized/minimal support for a small project's dev builds. We also chose to be pretty limited in use of templates, and to try to stick to a few common types. FWIW, that was with Visual C++.
You are pardoned, and you are wrong. Idiomatic C++ is faster than idiomatic C, this has been beaten to death here already. Templates allow removal of indirection and improve performance. Gcc is great but other c++ compilers reach similar performance; Java though fast does not. Language choices have a big impact on performance. 
Usually mixins are a tool for the writer of the class, not the user of the class, so it's pretty common for it to be intrusive: https://en.m.wikipedia.org/wiki/Mixin, see e.g. the ruby example. Also note that in the python example, mixins are implemented by inheriting from the mixin, not vice versa. Basically, crtp fits the definition and style of mixins much better than what you call a mixin. What you call a mixin, is to me, just templated implementation inheritance.
Modules won't give you what you're asking for. You will still have to build the module the same way you build your project, and integration will be exactly as hard. Modules will control name visibility, and keep you from having to recompile that header only library N times. 
Personally, I think the authors advice about making code dependent on declarations (as opposed do definitions) is all one needs to eliminate compile/link time as an issue for all projects. In other words, his advice about using forward declarations, pimpl etc. is sound. In addition to this I find it helpful to factor the project around one or more static library subprojects. This diminishes by a large amount the amount of compilation required when anything changes. It also helps get the most benefit from parallel build systems. Also, it often happens in the constant churn of a project that some functions/modules can totally disappear without being eliminated. This may mean that dead code is added to the applications. If one uses the static library approach referred to above, this problem never occurs. Finally, it's common that one is building not one application, but several related ones which share code. Factoring ones code as a family of static library subprojects is even more beneficial in such cases.
Compile times are my biggest bane. I know I can disable optimizations to get a faster compile, but it's still a pain in the ass to wait full seconds to see the result of a source code change.
Name something you can't make with C++.
True, it depends if modules are just some files, or an addition of headers and binaries.
Instead of ccache &amp; co, you could also use sccache from Mozilla, which can save object files in a distributed cache, works with GCC, Clang and MSVC.
In my experience lld is much faster with make -j1, I assume due to better multithreading, but as soon as you do multiprocess builds the advantage goes away.
I do take this approach, but I don’t seem to have quite got it right with regards to templated classes. I have a bunch if classes that should ‘live’ in the static library, but due to being templated any minor changes triggers almost the whole project recompiling. Any advice on how to get round that one is very welcome.
The current proposal of C++ modules is nothing but syntax candy wrapping wishful thinking. Good luck getting lower times than just using single compilation units or even just precompiled headers. Nobody will use it in anything big if it is not faster, nothing to win, too much to lose. 
That's what is being proposed. An additional file that will control what is exported. Processing that will produce a file that can be imported by the compiler with little to no additional work. It's more than just compiled headers, as it also involves a new kind if linkage, but not a lot more. 
Aah those one return path idiom functions, it cannot smell more to its C ancestry
Personally I found switching to a i9-7900X helped compilation times far more than any code dependency/forward deceleration cleanup ever did. Compilation times are one of the few things where I can throw money at it and it actually helps. At least for our code base. Your results may vary of course.
Congratulations, and good luck in your learning; however, this is off-topic for the cpp subreddit and has been removed by the AutoModerator.
But he mentioned he saw 3x speedup?
May I ask what boost components you were depending on and what did you replace it with?
I participate in a Discord server for learning programming. Just a few days ago somebody asked this: &gt; `{int temp; temp = a, a = b; b = temp}` This was an example function body given for the purpose of just swapping two values. The comma is a typo, right? We also often have confusion where people use `,` in a for loops to do a whole bunch of extra work or where a comma sneaks in where they wanted a `;`, and now the code just does something strange.
Don't overdo it on the templates. I have a few classes that could use a template value, but I choose to cast a few things to int (things that will fit, obviously) to avoid getting the template instantiation penalty. I suppose I could try to type erase it, although I'm not really sure how to go about that. And if you can afford it, control when and where you instantiate. I have a series of template classes that together adds up to around 30,000 lines of code. I only instantiate it for two data types, so instead of instantiating all that in every translation unit that touches it, I put an `extern template` in the header, and instantiate the templates explicitly in a translation unit of my own choice. It helps significantly with compile time, and it triggers far fewer recompiles as well. Of course this requires that you know the complete set of possible instantiations in advance. 
What?! The TS version seems much more logical.
In the order find-all reported them in VS: * chrono | std::chrono * circular_buffer | self-written * flat_map | self-written * small_vector | self-written * crc | self-written * filesystem | std::experimental::filesystem * intrusive list | self-written * intrusive set | self-written * mpl for_each | self-written * optional | std::optional * voronoi | https://github.com/JCash/voronoi * program_options | cxxopts * property_tree | self-written - this one was what started it all: replacing this gave us a *massive* performance boost everywhere it was used (on the order of 10-20x depending on what it was being used for). * regex | std::regex + self-written * thread | std::thread/future * locks | std::locks * shared_mutex | std::shared_mutex * variant | self-built macro-based union (a macro isn't strictly required but it makes adding/changing the members of the class far easier). We tested using std::variant but the macro-built union compiled 60 seconds faster than std::variant.
&gt; I do take this approach, but I don’t seem to have quite got it right with regards to templated classes. I have a bunch if classes that should ‘live’ in the static library, but due to being templated any minor changes triggers almost the whole project recompiling. Look into explicit instantiation. Leave the template declarations in the headers. Put the template definitions in other headers (I use *.imp suffix). Then make a file which looks like: // put file declaration in header my_class.hpp template&lt;class T&gt; struct my_class { // member variables T m_t; // member functions int f() const; // note: don't include definition !! }; // make file for implementation my_class.imp #include "my_class.hpp" template&lt;class T&gt; int my_class::f() const { // implement function return sizeof(T); } // make a module to be added to the library to instantiate // my class for all types you expect to use // include implementation #include "my_class.imp" // explicit instantiations template &lt;int&gt;my_class; template &lt;long&gt;my_class; 
What would you say in to compare Ninja to Fastbuild? 
I've never used Ninja so I can't give any comments on it.
It seems C++ now finds itself in a place where because one tool (the compiler) uses UB as an excuse to be very clever and generate fast code for an incorrect program, that we now need a second one (a static analyser) that flags up UB as an issue that needs to be resolved. He even admits that clang's analyser couldn't find issue with the invalid areay index access, yet recommends that we write code in a way that helps these analysers. We use Coverity, it's great but $$$. The situation is madness. Garbage In does not necessarily mean Garbage Out, it can also mean Bitcoins Stolen or Harddrive Encrypted. Young devs will watch these videos and believe that they represent an industry standard. They'll be in for a shock when they start in industry.
I’ve got dual Xeon processors, and 32GB ram, still takes &gt;30 minutes to compile. We’re not really able to throw any more money at it, we’ve got to look at solutions like the article (reducing coupling being the big one). 
No, it's not. If you append an absolute path after another one, the only two reasonable things to do are throw an error, or just discard what comes before. As an example, pythons equivalent (path.join) does the same thing as the final version. This behaviour lets you e.g. take a user specified directory that is implicitly relative to something (e.g PWD) and do the right thing. In fact, you'll see this behaviour is basically going on behind the scenes all the time, anytime you try to open an ifstream it appends the argument to the process working directory in exactly the same fashion. Edit: another "reasonable" approach, is to actually make relative and absolute paths different types, and then simply only accept relative paths to the append method (both relative and absolute paths would have an append method, but they would both only accept relative paths). I actually think this is a good example of design trade-offs. Some people are ultra gung-ho about strong types and would probably prefer this without even thinking twice, but I don't think this is the right direction here (based on thinking about it for all of 10 seconds).
It's merely using Python scripts to manage the entire build process of your libraries while keeping the proper dependencies. Python doesn't build your code directly. _Even_ then, what's wrong with Python building your code?
What Xeons though? Because in testing the i9 was still faster than a dual-xeon build one of our linux guys was using by a wide amount.
Depends on whether you model `path`s as fancy strings or actual paths.
That's true but _is there a better way_? In my day to day workflow I don't have to deal with anything other than CMake and Conan, and obviously the toolchains (GCC on Linux and MSVC on Windows). Using Conan I can enforce what versions of those toolchains people should use, this is invaluable to me as I had no way of forcing people to update their toolchains. So the combination of Conan and CMake hides the entire toolchain and the end user doesn't have to really know how their binaries are being built. They just have to update their toolchains which is not that hard. It took me a lot of effort to do that but I am really happy with what we have. On the other hand we have some third party codes/libs with different toolchains/build systems, some require a cross compiler. Using Conan and our CI I have setup the pipelines such that I don't really have to deal with how those third party codes/libs are being built and what their build system is. I just package it with Conan and release it, it's totally transparent for the end users. This has removed __a lot__ of headache because we used to setup, configure and update and provide support for those toolchains on more than 200 machines. With Conan it's just a built binary that people get and use. Obviously someone has to do the initial setup and configuration but once it is done, it becomes very pleasant for the end users.
that's also the speed-up you can get from PCHs (sometimes more, sometimes less) in my experience. And unity builds can be a down-right 30x speedup for the overall build time (but they are not very useful for incremental builds imho, it's much more effective in CI for instance)
Fastbuild does distribution and caching, a la distcc and ccache, so it's not an apples-to-apples comparison. I'll use Ninja over msbuild or nmake every single time, though.
Clearly we have different problems. My nr. 1 problem is getting libraries to compile in the first place, and then keeping it up to date with newer releases. And boy, are some of them painful to compile for yourself... I would strongly appreciate a standardized way to download, compile, and integrate libraries. Package managers do this to some extent, but I would prefer a solution where library authors would simply conform to a standardized mechanism rather than relying on a 3rd-party maintainer who may or may not care for specific libraries. Once a library is in a useable state I'll stick it in SVN, which works fine for internal distribution. I'm a little puzzled by your phrase "end user". That's programmers, right? 
Santa Parent delivers https://www.reddit.com/r/cpp/comments/7l3la3/sean_parent_secret_lightning_talks_meeting_c_2017/dro3tzz/ :)
Weakest: Small language struggling to get out(C++20 is more complex than needed due to the age of the language). Most irritation: build times and using libraries with headers and .so an .dll ... idiotic that in year 2017 it is not automagic in IDEs.
I'm a guy and I was attending the conference and it did not register at all for me when you said "you guys". But I was also aware of its gender neutral meaning. These kinds of misunderstandings are inevitable when you put people from so many different cultures and with so many different native languages. Letting the speaker know (ideally in a non accusatory manner) and the speaker apologizing and clarifying what they meant (public and/or in private) is really all you can do about it and it seems like every party handled it in a friendly manner to clarify the miscommunications which I would say is ideal. 
&gt; pimpl I don't tink that doing PIMPL for the sole purpose of reducing compilation time is a good idea.
Because you avoid a lot of sneaky promotions that break things
How is signed to unsigned a promotion?
This is just stupid... On the other hand ` std::mutex μ;` is proper engineering... :P BTW there is a classic SO Q by (ex?) MSFT eng: [Why is this program erroneously rejected by three C++ compilers?](https://stackoverflow.com/questions/5508110/why-is-this-program-erroneously-rejected-by-three-c-compilers)
I've personally found that [zapcc](https://www.zapcc.com) can really help with heavily templated code. (The speed gains are far smaller on code less reliant on templates and pretty much nonexistent with C code.)
I would make my build more brittle for 20x speedup, but not for 20% speedup. So the problem with your comment is that you ignore how much unity builds are faster. People have reported crazy speedups, that make modules 3x look pathetic. I mean 3x is good, but for a change so intrusive I would have hoped for more... I think Bjarne said 10x couple of years ago... So I am hoping for this in 4-5 years if compilers improve...
Unsure, I’m off for Christmas so can’t check either. They’re about 4 years old at this point too so due an upgrade. I’d imagine we’ll look at i9s too. 
So you keep the libs in SVN? That worked a while for us, but it sort of breaks down if you're cross compiling or trying to build for even different versions of the same OS.
If he used the correct 'integral conversion' would that make your argument more substantial?
Yes. Now it makes sense.
I'm sure you've looked at it already, but I'll mention mapbox's variant class as another alternative. Can't speak for it's compilation times though.
This "people mad at compiler optimizations" things is just a weird and wrong thing that gets the same responses each time, but I guess I'll be the one to respond this time so that young devs don't get the wrong idea from you. Many of the optimizations can occur in perfectly reasonably code when things are inlined into one another, or other optimizations like constant propagations occur. It's very easy to show a naive example: void foo(bar* b) { auto&amp; r = *b; if (!b) throw std::runtime_error("oops!"); // use r } And say: why does the compiler optimize out this if check, that's just mean. But the optimization pass can't magically infer intent. The same structures can emerge from code that is written exactly as above, or after the code has been transformed by other optimizations. void blub(bar * b) { if (!b) throw std::runtime_error(""); } void blubber(bar* b) { if (!b) throw std::runtime_error(""); // do stuff blub(b); // impossible for b to null here } If blub gets inlined into blubber, the branch inside blub will get totally removed. It's not obvious to an optimizer that's performing many, many optimizations, that the first case is subverting the developer's intent and the second is a useful optimization. You could of course add tooling onto the optimizer to try to figure this out, but why not have a separate tool instead that tells you that foo is really badly written? This informs the programmer and lets them diagnose the problem and figure out what they meant, instead of the optimizer making some silent decision. It would also be a huge burden to add to the optimizer which is already the most costly part of building. UB is important and useful, and the reason why (primarily) is that it enables better performance. This isn't even a language specific thing; without UB you wouldn't have binary search. It's not an "excuse". Clearly, your industry is different from mine; in my industry we care about the performance this enables and I expect young developers to be open to learning and understanding UB, how it is leveraged by compilers, and how to properly write code in consideration of it.
I like to express the literal as `0x1` so it gives some indication that it is a bit mask.
Look, it's really very simple. C++ compilation of large projects needs to be fast, and even if we could build all the bells and whistles into the compiler, it would be a bad idea because it would be slow. You can't run all the sanity checks and validation and unit tests every time you build. There is ALWAYS going to be some aspect of "I invoked the compiler to make a build, but by some other tool or some other means, I later learned that there is a problem" in any practical build environment. So what exactly is the problem with external static analysis tools? I think the truth is that static analysis and sanitizer tools are one of the best developments in C++ in recent memory, and they represent not only a big advance, but the promise of even greater and more rapid advance, precisely because they are decouples from the compiler and not tied to crazy long release cycles.
&gt; that's also the speed-up you can get from PCHs (sometimes more, sometimes less) in my experience. Yes. But PCH is "just dirty hacks that only make the build more brittle.", while modules, having the same benefit to compile times among other things "is the future". That's the point.
Since you are in game development I recommend you take a look at [FASTBuild](http://fastbuild.org). It does not do caching but does distributed building really well. If you have money to throw at workstations you can do the same for a build cluster. We are in a similar spot and distributed builds let us scale ~5x.
That is super cool, do you know how much non-Mozilla use it's seen? It looks fairly young, heavy dev last ~6-8 months, and it's README mentions it has limited compilation flag support.
Indeed, I forgot that unsigned to signed is not a promotion if it's the same size.
I wonder how Matt Godbolt would feel about his name becoming a verb at 53 minutes in.
That tends to be the biggest benefit, the other being that you can more easily change the implementation without breaking the API you had.
Are all cores running 100%? It's common that projects get so many inter-dependencies that you can't split the compilation so much. I know most of my code wouldn't get any speed-up with more than 4-5 threads (but thankfully it takes less than one minute for a full build).
Only for Windows. On Linux we rely on the libraries already being in the package manager. 
Processors dont matter that much if you aren't on an ssd. Ssds will speed up compilation by enough that every dev shop should default to one
I almost asked if you meant decreased. Glad you caught that. I've been moving to header-only over the past year and have had a similar experience to yours.
There is also [cget](https://github.com/pfultz2/cget).
Of course, more ways to do static checking of constraints is always better. 
Being faster than log10, not approximating.
That's not the problem. The problem is the lack of this message: warning: function foo assumes b!=nullptr, but still contains redundant test for b. Also, I'm stunned at the lack of historical perspective in your message. Please try to keep your downvoting in check; I'm only going to explain why we have UB and why it is phrased the way it is. Ready? Originally, UB was introduced, not _in order to allow optimisation_, but rather _because actually checking for the behaviour was just too expensive_. Those are different things: 'optimisation' means _making code go faster_. 'Not checking' means _not making code go slower_. Take pointer validation: the compiler cannot prove statically that every pointer is always valid, but code to test for pointer validity (basically, storing all pointers in a large table and looking them up at runtime) would be horrendously slow, so we don't do that. This is not an optimisation; it is simply an acknowledgement that we don't have the resources for testing properly and prefer the faster and more dangerous solution. Of course, accessing memory through an invalid pointer (especially if it's a function pointer, i.e. we execute whatever is on the other end) can lead to pretty much _any_ result, and that's what the text in the standard means: _we have no way of predicting the end result_. Anything can happen. If your OS has a function, somewhere in memory, for formatting harddisks, and you call an invalid function pointer, you might accidentally call that function, and you will get your harddisk formatted. There is a subtle, but very significant difference between the original meaning of UB (where the program will do what you asked, but the result is not what you expect) and the new meaning of UB (where the program is already rigged not to do what you asked from the get go, and doesn't even emit a warning for it). The original UB was triggered because a dumb, mechanical translation of the instructions you provided had an unexpected result. The new UB gets triggered because an extremely smart piece of software has figured out that your instructions may, under certain circumstances, result in an unexpected result, and therefore it can safely ignore them. In the original UB, an expression like `void foo (int a) { if (a+1&lt;a) ...}` would be translated into something like move r0, r1 add 1, r1 ; this may overflow, but the compiler may ignore that. ; it doesn't have to emit a test here, and even if there were ; a test there is no meaningful way to handle the error anyway. ; The behaviour, if that happens, is undefined, at least from a C++ ; language perspective. cmp r0, r1 jge .end ... ret In other words, it would be a mechanical translation that still represented everything the programmer wrote, and it would, on every known platform on the planet, work as expected. The new UB disagrees though: ret After all, signed overflow is UB. According to the standard, the compiler may ignore UB. Without UB the test is always false, so the whole block may be eliminated as being dead code. Without a warning, of course, because the standard doesn't require that either. How could it? UB exists precisely for situations that cannot be detected statically in the first place, and where we don't want to spend time checking for it at runtime. The problem is, of course, that we transitioned from old-style UB to new-style UB without any discussion, and pretty much without warning. Code that worked perfectly fine for decades (like the signed overflow example) all of a sudden resulted in mysterious bugs. Is it even possible to write software now that checks for signed overflow? Or can we look forward to a future in which _every_ check of that nature will always be found and eliminated? Isn't it strange that we cannot express such checks anymore? There is a second problem: a lot of different conditions have been thrown together in the UB bin, ranging from fairly harmless and predictable to supremely dangerous. What should we expect from compilers: will all of these eventually be reclassified as "optimisation opportunity"? Or will some of these remain bad, but generally not too problematic practices? Let's give an example: namespace std { bool TestOdd (int a) { return a % 2 == 1; } } Injecting into namespace std is UB, so will compilers eventually just erase such functions? Or is this a 'different' type of UB that warrants a different treatment? What we desperately need to do now is this: - We (the C++ community and the standards commission) need to have a discussion where we want to take UB. UB-as-an-opportunity-for-optimisation is a relatively new development. We have no idea how much it really contributes to performance, nor how much it contributes to broken programs. Both effects must be studied and discussed. - The definition of UB in the standard needs significant improvement, making the distinction between old-style and new-style UB clear, and requiring a warning for new-style UB (if you statically detect a UB condition and make different choices for the generated code as a result, you can also emit a warning for that. Yes, I'm aware it's hard. If you can't handle it, don't do the optimisation either. At any rate, don't just dump the problem in the programmer's lap!). - Given that there are different kinds of UB that get different treatment from the compiler, we need to revisit all of the hundreds of possible causes of UB and specify whether we want to keep them as UB (either old-style or new-style), reclassify them as something else, or remove them entirely. This is a really important subject, and one that needs a lot more serious discussion than it is getting. 
So, how will you figure out _why_ code is pleasant or unpleasant to review? Not that it matters: reviewing code in a barely readable courier font in a tiny window is always, automatically unpleasant...
It adds another dependency. A dependency that I do not want. I dont see a feature in conan that needs python. Why didn't they write it in c++?
Why the downvotes? The US government has used this to test it's Air Force Fighter jets for decades. I know - I have done it.
There are pros and cons. If you are using languages besides C++, they offer something I don't. My approach uses on-line code generation. Much of the hemming and hawing here about packages and deployment -- "My nr. 1 problem is getting libraries to compile in the first place, and then keeping it up to date with newer releases. And boy, are some of them painful to compile for yourself" , is addressed by an approach that minimizes the amount of code you need to download/build/maintain. Protocolbuffers and Flatbuffers aren't on-line code generators so you have to download/build/maintain a lot of code for either of those.
1. Evenly merge all your compilation units into x number of compilation units, where x is the number of cores available for your processor. 2. Compile with -j x (or for windows, enable multicore compilation and set it to the number of cores available) 3. If you are using GCC, switch to clang. It is _much_ faster, especially with multicore compilation.
&gt;I would make my build more brittle for 20x speedup, but not for 20% speedup. I still woudn't. I would take modules and then buy a couple of extra build machines to get the same 20x speedup. But this all comes down to values, what is more important to you. To us reliability is pretty high up. &amp;nbsp; &gt;I mean 3x is good, but for a change so intrusive I would have hoped for more Yes, I fully expect this to be more as implementations mature. I am just reporting what we have actually observer with one compiler's (Clang) incomplete implementation.
or just return a pointer, so you have to do *val ++;
Are you by any chance coming from a language like Java or c#? I honestly never encountered that problem. But generally speaking, compilers can warn about wires to variables that you don't use later. That would at least catch some of the cases.
I would prefer to return a reference to convey that the data can't be `nullptr`.
Nope, C++ is my main gig. Are you saying you just never forget the `&amp;`? I think it's a pretty easy mistake to make. &gt; generally speaking, compilers can warn about wires to variables that you don't use later Haven't seen any warnings for this with the latest MSVC, sadly.
Ah - I have my include chain setup differently, with the template .h file including the .hpp at the end, which I’ve seem elsewhere. Thanks! If I do it your way, every time I need an instantiation with different template parameters, I either have to add it to the explicitly instantiation file in the library, or include the implementation header in client code, right?
Somehow I’ve only managed to discover `extern template` recently, despite programming C++ for a long time. Thanks!
Use Gyp with a ninja backend instead of CMake or autotools.
 template&lt;class T&gt; struct ref_only{ T&amp; t; ref_only(T&amp; in):t(in){} ref_only(std::decay_t&lt;T&gt;&amp;&amp; in)=delete; template&lt;class U&gt; operator U()const&amp;&amp;=delete; template&lt;class U&gt; operator U&amp;()const&amp;&amp;{return t;} }; 
You aren't familiar with D because it isn't backward compatible with C++.
Values have types, and converting from an expression returning a value to one returning a type is easy. 
Have you tried almost always auto? I find it much easier to remember `auto&amp;`, as opposed to `int&amp;`. Now, instead of having to think about types, you just have to think about whether you want this binding to be a reference, or not.
Use Haskell
Well, first of all, I very rarely need to create a local noon-const reference variable in the first place and of course I have probably forgotten a reference somewhere at some time (everyone makes typos), but I can't remember ever having to debug a situation like that. What warning level are you using?
Yeah, fair enough. This most recent time it was because I had a PIMPL object that stored an `std::unordered_map` of IDs to vectors of data relevant to each ID, and upon giving the object an ID, you'd get back an `std::vector&lt;Foo&gt;&amp;`. By forgetting the `&amp;`, I wasn't ever modifying the original instance. I suppose you could avoid it with more explicit operations that perform the exact operations I was performing on said referenced vector. I'm using `/W4`.
To be honest it looks smelly to me. Why do you access a class's field like this? If there's some logic that involves incrementing simply create a class that have a getter and some increment method. Of course sometimes you have a need for some data object (POJO for java guys) but if you need some logic associated with a field it's not a simple data object anymore. And as others suggested if you really need a getter like this just return a pointer. 
Being nasty is unacceptable. Regardless of the point you’re trying to make, a nasty response gives the C++ community a reputation for being nasty, which harms C++’s growth. Please contribute to the glory of C++ by responding in a professional manner, especially when you must criticize. Criticism is fine; nasty criticism is what’s counterproductive.
I've had this issue before with a custom class, so just prohibited implicit copies. There might be something in [https://github.com/foonathan/type_safe](https://github.com/foonathan/type_safe) that helps.
Agreed. Things that return reference-to-guts are highly suspect because they easily break encapsulation and allow class invariants to be violated. Note that the STL almost never does this, especially with modifiable references (const references to guts are less bad but still create lifetime dependencies). Things like vector returning references to elements work that way because a vector doesn’t care about its elements being modified. Similarly, pair and tuple expose their guts because they have no invariants.
I'm using fastbuild (fastbuild.org) for distribute compilation for gcc and clang compilers, indeed it could be used as resource compiler, it's free and very effective
I agree with both you and /u/STL in terms of this not being 'great', but it was most recently just direct access to a PIMPL object in the .cpp that housed it, and much more convenient than a bunch of functions for modifying it. I appreciate your points though.
Is getval just a getter? Why would it return non-const? In my experience it's a bad idea to expose object internals like this - a getter should have a signature like const int&amp; foo::getVal() const { return value;}. If you want to be able to increment the data member of an object, why not jusy write a function member to do that?
That's a good point - I'd say I've made this mistake far less when using `auto`, but find that it messes with my Intellisense/Visual Assist somewhat - I can't seem to get as reliable function suggestions from `auto` as from `Foo`.
Basically: try not do this. If you are modifying inside the class just do it directly without a getter and i suspect your outside usage is either a read or write to your hash table in this case which you could simply implement as two methods, preventing these type of issues. There are very few cases where i would ever store something by non const reference outside a class, its just tricky and leaves no guarantees for the class you are using since an outsider can do whatever he wants.
Much more convenient in a short term - but will you be able to work with this code after a year or two? Is your current code testable? Tests are a great way to check if your classes aren't coupled in some weird, mind-boggling ways. 
You can look into zapcc they offer nice improvements in subsequent recompilation speedups (have to produce cache first)
Really? I haven't experienced that personally, but I can imagine that totally being a thing. That sucks!
GetVal is just an example function to demonstrate the problem. No need to analyze Foo's code elegance.
I just assumed if you're talking about speed related anything on a computer you're already using an SSD or an M2.
I agree with the other guys about this being an anti pattern. You should encapsulate your logic in a member function or something similar. Looking narrowly at your issue however, one idea to enforce the rule you want is to make the object being returned non copyable and non assignable? That way the line where you forget the reference won't compile.
Thank you - I feel like some interesting discussion was missed by way of my example, which is frustrating but a lesson I had clearly forgotten.
Ehhh, I think I'd rather have it obvious that 'I' don't own the object and pointer syntax does that very well. If you want to convey that it can't be nullptr, easy enough to stick an "if nullptr error" chunk underneath (or add a comment).
Exactly! Lifetime dependencies are a topic that many new developers miss. It's important even when writing a single threaded app but when you have to switch to a multi-threaded one you really start to appreciate code with a clean and responsible approach to objects' lifetime. 
For me, it's obvious I don't own it if I'm not receiving a `std::unique_ptr`. Everything else is a non-owning pointer/reference.
The support is not straightforward. You need to use side projects like beathe or doxyrest. Still I have never ever seen nice Sphinx appearance for C++. In some sence Doxy interface is not worse then Sphinx.
There was this mantra in some book that went like Make interfaces easy to use correctly and hard to use incorrectly. If you want to modify object internals by directly accessing them there's no easy way to do this. So I'd suggest either returning a pointer - at least when you see a pointer you'll remember, hmm I have to be careful here. Or just make the data member public - after all direct access is what you want, and I don't agree that 100% should comply with all the best practices in the world.
I agree re. making it public if you're gonna make it public. This recent case was one in which the 'getter' performed a look-up and then returned a reference to what it found in a collection, rather than exposing the collection directly and making every caller perform that themselves via code duplication. I think it falls in a grey area, but I get that nobody can know that without seeing the real code. Still, an interesting tangent for us all to go down.
One useful example is this: // in .h std::map&lt;int, some_struct&gt; m_map; // in .cpp auto &amp;my_struct = m_map [some_id]; ...do stuff with my_struct... It avoids having to do the lookup for each access, and it is much easier on the eyes than the corresponding iterator-based solution: auto it = m_map.find (some_id); if (it == m_map.end ()) it = m_map.insert ({some_id, {}}).first; ...do stuff with it-&gt;second... &gt; And as others suggested if you really need a getter like this just return a pointer. That would only be good advise if there is the possibility of a nullptr being returned. Otherwise you are much better off with a reference: it has more pleasing syntax and conveys the intention more clearly. Besides, it is not like pointers are entirely free from pitfalls: int *Foo::getVal () { return &amp;m_someVal; } // later... void someFunc () { Foo foo; auto val = foo.getVal (); val++; // _still_ does nothing useful } 
They are all being awefully hard on you. Don't let it get you down ;-) 
IMO, the biggest benefit of pimpl is not polluting namespace with headers that are needed for the implementation, but are not part of the interface. 
As STL also wrote, collections are one of the few exceptions to that "don't return references" rule. And responding to your example - returning a reference wouldn't be any better, auto in that case would create a copy of that int, while expected behaviour would be incrementing that int to which reference was returned, no? 
Yeah that's almost exactly what I was doing - as I've explained elsewhere, this was in a class' .cpp file, getting references to internals of its PIMPL class. I've since changed it to work via explicit functions as per the advice of this thread to see how it feels, and it means I have to perform the look-up a second time...
All tools have limited compilation flag support anyway, so I wouldn't worry about that one. If anything is missing, just talk to them on Github and they should be pretty responsive. I'm using it for my projects and it works fine. I know some companies are using it and it improved build times tremendously. They had tried clcache before and always had issues with faulty caching on Windows, not anymore with this one!
Almost all the time yeah. We’ve got a unity build for the entire project. With the exception of the final link step it’s all 100% usage
I have a collection of metrics I am going to run the software through. Traditional things like Halstead, Cyclomatic Complexity, Lines of code, etc. Then cosmetic things like K&amp;R braces, etc. 30 or so different rules. So there won't be just one magic thing. I will post again as I make progress. I also appreciate the comments on the 'user experience' for the review process. I will definitely make improvements if I run something like this again. 
&gt; But PCH is "just dirty hacks that only make the build more brittle.", maybe it's bad from a philosophical point of view, but in practice you add a command or set a flag in your build system and you're good to go. How much time will have to be spent migrating to modules, waiting for bugs to be fixed in compilers and toolchains (eg it's nice to have module support but it's not very useful if debuggers, static analyzers, valgrind, etc... don't all support them) ? I mean, for instance lld, llvm's linker was out two or three years ago, it's mostly a (good) reimplementation of well-known linking features, and it's still quite buggy. Here this entirely changes the compilation model, not everyone still agrees on the module's scope, etc. 
VS 2015 is old and buggy. Please upgrade to VS 2017 15.5, which contains many constexpr fixes after the original implementation appeared. (This advice stands regardless of whether this particular bug has been fixed; I am on vacation so I haven’t checked.)
The bug is fixed in 14.12.25907 (from VS 15.6.0 Preview 1.1): 00007FF77C62A380, 6 00007FF77C62A380
My project takes around 7 minutes to build in release mode and clang 5 builds it at a almost the same speed as gcc 7. Clang can be faster, but it's not guaranteed, and the out of date website exaggerates this and every other benefit (e.g gcc's error reporting has largely caught up, too). Clang also produces a binary which is about 20% slower. Try both. Use the one which is best for your use case. 
What is the advantage over using cmake to generate a ninja build, in terms of build time? 
I was using boost::flyweight in a couple of places, and managed to move it out of headers completely using a thin wrapper. It improved build time by 25%. Oh, and I did the same thing with property_tree, the runtime performance of that library is abysmal. 
Oh, there isn't one, to my knowledge, if you are using CMake with a ninja backend. Usually people just use make as the backend -- I should have probably written this more clearly. My point was more about using ninja, period. I just find Gyp to be the simplest way to use ninja, personally.
Fair enough. Using it with cmake is just one flag on the cmake call, though. 
I agree this is a serious issue, but systematically using the `explicit` keyword on all one element constructors mitigates the issue almost entirely. Yes, life would be better if we had had `implicit` instead, but once you learn to `explicit` everything it never gets in your way.
I've literally made studies on the fact. Of course your lineage may vary, but in a big majority of situations, GCC has again and again been proven inferior, both in terms of compilation speed, as well as the quality of the resulting executable. I'd post it, but I've been doxxed before and live in a very politically oppressive country, where my job safety, physical freedom and custody of my children is at stake. I hope you understand.
I'm aware -- and it's definitely one that more organizations should use. I don't like CMake, but if I had to use CMake, I would certainly use the ninja backend.
I've seen lots of studies and comparisons, the results are always mixed. A "majority" of situations doesn't equate to "clang is much faster than gcc". As I said, you have to try both before you know which is right for you. 
You can refer to [Hackr.io](https://hackr.io/tutorials/learn-c-plus-plus ). It is having all the preeminent online tutorials list for C++. The most intriguing factor is that those tutorials are upvoted by the user community. This makes the results more credible and genuine. There are many diverse tutorial options, so it's a hassle-free experience for you there.
&gt; How much time will have to be spent migrating to modules, waiting for bugs to be fixed in compilers and toolchains That's the purpose of the "is the future" part. Bugs will be fixed (much faster than with lld, I presume). Just like they were for PCH at some point. And in the end you'll get a better standardized alternative with added perks (modules are not just glorified PCH at this point). There is always a cost of migration and for some projects it might be to high to warrant the move but it doesn't mean that it shouldn't or won't happen for everyone else. 
How does the recommendation to write code in a way that forces a static analysis run help with compilation times exactly? The problem with static analysers... a) the free ones aren't that good, and b) the commercial ones are pricey.
When you click the "Unpleasant" or "Pleasant" buttons, it doesn't scroll back to the top of the file, so it sometimes seems like nothing is happening and I just pressed the button multiple times "by accident" because I didn't think it was going to the next code example. Sorry but this study seems completely useless in its current form. You are getting very, very biased results with a lot of effects you're not thinking about.
No it doesn't. You don't have to install Python. You can just get the installer from their website and install it like any ither application. I really don't understand why it matters if they had developed it in C++. Could you elaborate what difference it would have made?
This guy cpp’s
&gt;Feel free to pretend my example isn't returning a reference to a member if that's easier. The problem still stands thanks to what seems like no compiler warnings in this situation. What you need to think is **why are you returning a mutable reference** in the first place. There are only two "correct" ways to do it: 1. You're returning a reference to some internal object of the class instance, or 2. You're returning a reference to an object (or some internal object from some object) that you received as a parameter; Any other thing is probably a memory leak or a future `SEGFAULT`. Note that if you were talking about **pointers** that would be a different story (mostly because pointers can be `delete`'d, so there is no `SEGFAULT` nor memory leak). Now notice that both "correct" ways of returning a mutable reference *are code smells*, in the first one you're breaking the object abstraction and in the second one, you're returning something the caller already have access to. So instead of trying to fight with the language, change *your design*.
&gt; But imagine the progress if we let pre-c++11 exist as a separate language, almost like c. #if __cplusplus &gt; 202000L // C++ 2.0 extern "C++" {
Hexadecimal seems a good middle ground: easy for computers to generate, easy for humans to read.
I thought I was replying to a personal message and the tone was pedantic so, I'm sorry. If you look at the others answers I've provided, you'll find more appropriate explanations. 
&gt; No it doesn't. You don't have to install Python. It is implemented in python (look here: https://github.com/conan-io/conan), so it needs python, one way or another. C and C++ are available everywhere, no matter the platform, and that's not the case for python. Don't get me wrong, I also develop system administration and network-centric programs in python, and I'm loving it there. But imho, using it to create a C++ package manager, forcing C++ users to integrate it into their workflows is not the way to go. Not everyone is using the latest linux distro, the latest macOS, or Windows®™© you know? 
About clang vs gcc, these days there is very little difference in compilation speed. Clang used to be faster back when the binaries it produced were less optimized. It has now caught up to gcc in optimization (and also compilation time)
Sorry when did you do these tests? It used to be true a few years ago but not anymore
bazel needs JVM to run its BUILD file which is written using (a subset of) Python, so conan is much better circumstance than bazel. Honestly, using python-like syntax is better than creating another DSL. 
You seem to misunderstand what [deleted functions][1] are supposed to do. What you described isn't "such bullshit"; it's the whole point of `= delete`. Deleted functions provide a match in overload resolution that causes an error if selected. That is their only purpose. They behaves exactly the same for special member functions as they do for any other functions and the fact that it stops the compiler-generated member functions is unrelated to the `= delete` syntax. What you really want is an `= nodefault` that interact specifically with the special member functions and is more closely related to `= default`. [1]:http://en.cppreference.com/w/cpp/language/function#Deleted_functions
Good read. I need to process double CRTP though.
&gt; Those are different things: 'optimisation' means making code go faster. 'Not checking' means not making code go slower. I think this is a distinction without a difference. &gt; The problem is, of course, that we transitioned from old-style UB to new-style UB without any discussion, and pretty much without warning. You're criticizing me for lack of historical perspective, but your very long post doesn't really get into this at anything but the most shallow level. Did it really happen all at once? What were the opinions of the compiler devs? Did any particular compiler get aggressive about this before others? What was their reasoning? If we want historical perspective, lets trace the actual lineage of when, where, and why this started happening. &gt; Code that worked perfectly fine for decades (like the signed overflow example) all of a sudden resulted in mysterious bugs. Yes, people wrote code that was then-idiomatic but was essentially sloppy. The future happened, and they got bit. Now that the future is here, let's avoid UB. &gt; Is it even possible to write software now that checks for signed overflow? Yes. You just have to write the check correctly (there's a number of ways to do this). &gt; What we desperately need to do now is this: Educate programmers so they correctly understand UB. Really, the overwhelmingly common cases the compiler optimizes that everyone brings up are null pointer checks and signed arithmetic. Make sure you understand how to do this correctly (and just avoid pointers as much as possible). &gt; The definition of UB in the standard needs significant improvement, making the distinction between old-style and new-style UB clear, and requiring a warning for new-style UB (if you statically detect a UB condition and make different choices for the generated code as a result, you can also emit a warning for that. Yes, I'm aware it's hard. If you can't handle it, don't do the optimisation either. At any rate, don't just dump the problem in the programmer's lap!). I've never heard of the standard requiring a warning for anything. Throwing away optimization opportunities for this reason would be totally unacceptable to a large fraction of the C++ community (like mine) and people would just stop using that compiler. It really doesn't need serious discussion. If it did people would be seriously discussing it. If you are writing code with a large attack surface (which is not the majority of the C++ community, as far as I can tell), and this is a desperate concern for you, there are already so many things you can be doing: - compiler flags that force signed wraparound. - compiler flags that prevent null pointer check elimination. The two biggest examples of what you complain about already have flags to do what you want! (in gcc, at least). - ubsan. - and most importantly, fuzz testing. Because UB is almost just one very specific kind of bug, and one very specific type of attack.
you should use std::reference_wrapper-like class, that cannot be converted to a value type, as your internal object.
Yes, it is indeed implemented in Python but they provide pre-built binaries that you can install on popular operating systems, see [here](https://conan.io/downloads) and you don't need any Python interpreter installed as these are compiled binaries and it just works out of the box. Everyone, 200+ engineers, in our company has been using these binaries, instead of installing Python/pip, for almost 9 months without any issues. 
About a year ago.
Sorry I don’t understand your point
Most experienced people never encounter this problem because most never do this in the first place. If you want to move a data structure, move it, if you want to copy it, copy it. If you want to return a reference, use shared_ptr, but think about why you would need to automatically keep track of ownership instead of structuring your program around it. 
This is currently done with C to compile code differently. If such breaking change happends, we might also end up `extern`ing old C++
Can't stay away even on vacation, huh? ;) 
I don't understand what the big deal is about taking a reference to some internal data in a constrained way (e.g. a class taking a reference to some internals of its own PIMPL object), where doing so allows you to cache something instead of performing another look-up or copying around data. I'm finding the assumptions that this is 100% a horrible thing to do a bit much considering ya'll have 0 context? Even just the humble language of &gt; I don't know why you're doing this, but here are some ways to avoid the bug you mentioned. Having said that, have you considered that there are better ways to do things? E.g. _____ and _____. would be nice.
Correct. If you just use "include" you generate code "on demand" and there is a demand every time you use it - even though it hasn't changed. If you do it my way, it doesn't redo the work on demand, it does it only when requested by the explicit template instantiation. The downside is you have to specify in advance all the types you might want to use the template for. If you overlook one, you'll get an undefined symbol error at link time. So now you have a choice. Think ahead about what types you'll need and eliminate redundant compiles or generate code every time it is demanded and never have to worry about which types you need to generate code for. Take your pick. Actually, as you can imagine the choice is pretty straight forward. You create a template class and start using it. When it starts to seem that it's slowing down the build, to a slight refactor to prespecify the types which it's to be generated for. And of course if you combine all this with pre-compiled headers - that can also help, albeit at the cost of some more advanced planing.
Should've expected it, good god. 'Twas a simple question about best ways to take a reference. I didn't even give a real example or enough context to make critiquing the pattern actually helpful to anyone...
&gt; It's hard to make an HTTP library that everyone is satisfied with. The reason is also a momentary inexplicable I use vertx a lot in the Java world, and it's native support of http2 and http1/websockets is amaze-balls. One stop shop. I'd love to have something similar for C++. Nghttp2 does http2, but not http1, so you need a proxy in front of it, and yes, cpp-netlib is somewhat stagnant, but it's not beyond forking and doing something new. I want Vertx-C++. I don't see how that's not flexible enough for everyone? 
oh okay, sorry for the trouble mod
&gt; I was surprised that returning a string_view and then copying it was a lot more performant than returning a string after taking in a string_view via the parameter list. well, returning a string_view does not allocate, while returning a string does
I think the problematic example you showed at the beginning should be cought by analysis tools. Until they do, returning a std::string is imho the correct default. That aside, I would call the function `trimmed`. From a function called `trim` I'd expect it to modify the string passed as a parameter.
&gt; it's not clear to the developer that this essentially returning a reference rather than a copy. I guess, than it's better to mutate the argument: `void trim(string_view&amp; s)`
Sorry, I wasn't clear, I'll update my post. What I was doing was the following: std::string s = ""; while(looping) { s = trim(other_string); } There's still an allocation it's just on the other side of the function. I would have expected the performance numbers to be fairly similar and was surprised by the difference.
I actually have both versions. std::string s = " test "; trim(&amp;s); std::string s2 = trim(s); You don't think the usage itself would be enough to differentiate the behaviors?
I believe it depends on the usage of `std::string_view` throughout the entire API. Does every single function that requires a string use `std::string_view`, or `const std::string&amp;`? Do other string manipulation functions (lets say split) return `std::string_view`, or an `std::string`? If your entire API is build around `std::string_view`, it would make sense that this function also returns `std::string_view`. But if this is in an older API that still uses `std::string` everywhere, then it may be confusing. So my advice is to stick to whatever is expected from the API in general.
that's a good point. Something else I was considering is to just explicitly create a different version. trim_sv, for example. That way the developer is explicitly opting into the behavior.
I prefer it when functions do the least expensive thing. If I really want to fully allocate the result I can do that myself. Sure, in this case, if you are not used to a function returning `string_view`s you can end up shooting yourself in the foot, but I personally don't think it's a library's responsibility to "protect" people who might use a function in the wrong way, if it ends up hurting the users who would use the functions the right way. If you force your users to always allocate the result, you will just make people write their own `trim` function that doesn't do the hand-holding. It feels like this is something that happens pretty often with the standard library. After all, the reason `string_view` was introduced was because people didn't like that their code could result in allocations that weren't visible at the callsite if a function took a `const string&amp;` and you passed it a `const char*`. One option, that imo gives the best of both worlds, is to provide e.g. both `trim` and `trim_copy`. `trim_copy` is both a longer name, and contains the word "copy", which signals that this is probably the more expensive version. Another option is to make the expensive but safe `trim` the default, and the \~dangerous\~ version be called `trim_view` or something. In general, though, I've found it rare to manage to accidentally have a `string_view` outlive its source `string`. Storing `string_view`s (in a `vector`, for example) is almost always the wrong thing to do, and you can't accidentally pass a `string_view` where a `string` was expected, since `string` is not implicitly constructible from `string_view`.
no, you're still getting an allocation every time. The mere act of calling `trim` allocates if it returns a std::string (bigger than SSO). The code you posted is what I'd call worst-case for std::string :p https://godbolt.org/g/xnyjrr
I feel like there's a miscommunication here. I'm aware that my current version allocates. That's the intended behavior and is part of the reason I created this post. However, it sounds to me like you're saying the following would not allocate if trim returned a string_view. std::string s = ""; while(looping) { s = trim(other_string); } can you explain to me how the above code could possibly get away without allocating? I would expect this to call the new std::string constructor for string_view that's available in C++17, which will allocate.
why wouldn't you just access a data member directly? What does this gain and how is it worth the fragility? 
&gt; I prefer it when functions do the least expensive thing. If I really want to fully allocate the result I can do that myself. Sure, in this case, if you are not used to a function returning string_views you can end up shooting yourself in the foot, but I personally don't think it's a library's responsibility to "protect" people who might use a function in the wrong way, if it ends up hurting the users who would use the functions the right way. It's not about protecting users as much as it is about making dangerous things obviously dangerous. If something is falling apart I want to be able to read the code and see where the inputs may be problematic so I can test assumptions. my concern was about a problem over in some corner of the codebase that's caused by code in a completely different location that's not even obviously dangerous. I could see someone passing over the function many times before thinking to test their assumption that it's returning a copy. It takes time to track those sorts of issues down. I always ask myself "if this fails, WHERE will it manifest itself and how obvious will it be that the failure came from here"? I guess what I'm saying is that it's not that I disagree with your sentiment, but there needs to be thought put into it. OTOH, you've convinced me that my concerns are a bit overblown. thinking about it, string_view is basically readonly and as long as its explicit in the code that it's returning a string_view it's perfectly ok. so I'll end up with 3 versions. util::string::trim(&amp;s); // in-place util::string::trim(s) // copy util::string::sv::trim(s) // view I think using the namespace sv to indicate things that deal solely with string_view's is probably pretty reasonable. 
It will allocate only if the size of the string_view is greater then the size of the previous string. Otherwise it will only copy the characters and adjust the new end of the string.
SBO, I understand that. I suppose I should have used a better example, but my tests involved a string with a couple hundred characters in them. it feels like people are getting caught in the trees and not seeing the forest here.
That's not construction, it's assignment. If you create the string inside of trim it will always allocate but if you return `string_view` it can reuse the buffer in `s`. That is, provided it's big enough which will almost always be the case in the loop you show.
&gt; It's not about protecting users as much as it is about making dangerous things obviously dangerous. I agree fully, but I'd argue in this case that C++ the language and not the library is defective. Some day in the future the language will catch up to modern idioms and allow some kind of user-defined reference lifetime extension or checking. Until then, the fact that some C++ types look like values but act like references and have no safety mechanism is just A Thing You Need To Know And Be Really Careful About(tm). Remember that this is all going to get a _lot_ worse with ranges and the new STL. Of course, this viewpoint is dependent on goal. For my domain, excess copies are performance bugs that soak up a ton of my time to find and fix, while dangling references are relatively easy to handle with static analysis. I'd further argue that the clarity problem in your case is the use of `auto`. This is exactly the kind of "it hides toes unnecessarily" stuff that opponents of almost-always-auto are referring to. :) A policy of using `auto` where it really helps (... templates and iterators) and nowhere else works really well on larger teams with varying levels of C++ expertise.
Thanks very much, that’s really helpful.
If `trim` is not a case for NRVO, than if it returns `std::string` the allocation always happens inside it, and then copy constructor is invoked. If it just return another string_view, than only the constructor is called.
Do not use boost classes in header files. If necessary, measure the compile time Impact. Especially avoid boost::ptime and boost::optional. In our Project, one of the Base classes includes ptime and time_duration and it cost for each compile unit about 1.5 seconds more to compile. We have about 2500 compile Units which are affected. :( 
sure, auto was the problem, but I'd rather design with that in mind than to blame the developer for using it badly. and I'm not even sure I would blame them in that case, one would expect that s = trim(other_s) copies. the code itself screams copy here. at best you would question the assumption right off the bat, but I don't think most people would and I think that's reasonable. You'd have to be familiar with the specific trim function to know off-hand if it were copying or not. which isn't hard, it only takes a second to look up the documentation or look at the code itself, but my main concern is how long it takes before someone thinks to do that. I honestly think trim_sv or sv::trim or something similar is a reasonable tradeoff to help protect against misunderstandings. 
`string_view` is a glorified pointer, treat it like it. In the situations where you would be comfortable using a non-owning raw pointer, you can use a `string_view`. Would you design a public api taking a Foo* ? 
Oh gawd: this is a C api! Nopenopenope. C++ has references for a reason. You’re using the pointer type as a “workaround” for not having a sensible function name. Fix the function name and get rid of this awfulness. `trimmed` should take and return a view. `trim` should return `void` and trim in-place. Perhaps you can also have `trimmed_str` that returns a string. The default I think should be to return a view where possible as the default API and always offer a `_str` for those who like to use `auto` everywhere.
I disagree with that, if you're trying to give a rule of thumb I would say use it in places where you would normally use a const std::string &amp;.
I'm actually pretty tempted to work on `if constexpr` this week, although I should be playing games or rereading Mistborn.
Why not include type checking, template instantiation, `constexpr` evaluation, etc? I.e. everything that's not product-specific analysis, optimization, and code generation. All of those are necessary to build all but the shallowest of tooling around C++ code.
An Intermediate Representation (IR) is not an Abstract Syntax Tree (AST). To really make that concrete, some given input source code could be parsed to identical ASTs as both C and C++ code. Syntax doesn't encode semantics that aren't superficially explicit. Type checking and semantic processing could give different meanings to that code, resulting in different IR forms depending on which language it's interpreted as.
You could avoid requiring an overload with two lambdas using `std::optional` or just a pointer as the lambda argument: map.if_exists("key", [](auto val) { // val is type map::mapped_type* if (val) { val-&gt;foo(); } else { } }); This seems only slightly worse, to me.
This may help: http://en.cppreference.com/w/cpp/compiler_support Unfortunately, Apple doesn't very well document what upstream Clang source they're working with. We may actually need a separate 'Apple XCode Clang' column in that table. Want to help?
&gt; Concerning the compiler optimization, you are right concerning CLang and GCC, but not Visual Studio, where the (x &amp; 1) is more optimized. Did you change the optimization flag for Visual Studio? Visual Studio doesn't have an O3 flag, so it's defaulting to no optimization on your test. Switch it to O2 or Ox and both versions generate the same code: [https://godbolt.org/g/JQaPFf](https://godbolt.org/g/JQaPFf)
That's the one big thing about C++17 I'm hyped for. Instead, I'm stuck trying to get libc++ working properly for embedded targets.
Oh what games out of interest?
FWIW, [according to the docs](https://docs.microsoft.com/en-us/cpp/build/reference/ox-full-optimization) /Ox is not actually full optimization: &gt; In some versions of the Visual Studio IDE and the compiler help message, this is called _full optimization_, but the `/Ox` compiler option enables only a subset of the speed optimization options enabled by `/O2`.
Thank you! I have received a lot of helpful comments from this forum. Don't be surprised if a fresh version does not show up with new software sometime soon. 
I don't know what exactly this means. If you never make a mistake then there's a lot of things you could just skip: - Unit tests - Debug builds - Sanitizers - Static analysis - Compiler warnings If you're god, you can skip all that. For the rest of us mortals all that is pretty helpful. There's also lots of times that I build and I don't use all of that, for instance when I make a very small change in my feature branch, and then I want to do a fast, incremental compilation. It's unlikely that I'll actually run the unit tests for instance every time I build, I generally only do that when I commit.
TIL. Thanks for the info!
The problem with hexadecimal is characters are not consecutive with ASCII. You're better off using only letters so you need a single subtraction.
Link can take most of the compilation time. It's the part that gets worse and worse even if you try to organize your code well.
That was part of my "not breaking the API" part. Like you can ship new dlls without requiring people to recompile their projects.
C++17: if (auto iter = map.find("key"); iter!=map.end()) { auto&amp; value = *iter; } else { // iter is declared but it's map.end() } 
Even LLVM's developers admit that it's getting slower and has become not faster than GCC. See for example http://lists.llvm.org/pipermail/llvm-dev/2016-March/096488.html This was my experience as well. Compile times are very similar (provided same flags), memory usage, however, was significantly lower for Clang. 
You might look at the string_view utility functions shipped with Abseil -- https://github.com/abseil/abseil-cpp/tree/master/absl/strings The issue you raise about lifetime extension is a real concern. Neverthless, widely used APIs within Google take and return string_view substrings. E.g. https://github.com/abseil/abseil-cpp/blob/master/absl/strings/ascii.h I think the rationale is that avoiding unnecessary copies is worth the risk. Bugs that might occur tend to surface in obvious ways in well tested code in practice. So, at least at Google descent unit tests are common and various memory sanitizers are commonly used, the risk is considered acceptable. 
&gt; and I'm not even sure I would blame them in that case, one would expect that s = trim(other_s) copies. the code itself screams copy here We definitely disagree there. :)
I'm not suggesting that static analysis is unhelpful, I'm disagreeing with the notion that we should write code in a way such that issues will only be picked up by a sufficiently advanced static analyser.
&gt; You’re using the pointer type as a “workaround” for not having a sensible function name. I'm using a pointer to indicate an out parameter, as is typical in C++. literally the only way that gets used incorrectly is if an opaque type is being used and that IS a C idiom, but not one that gets used for string types. I understand the argument for an approach being better, but the idea that there's something actively wrong with using pointers as out parameters is off-base imo. And for what it's worth, I think the name trimmed vs trim isn't any better than using a pointer for the out parameter. The developer would already need to be aware of the naming convention. It isn't obvious to me that trimmed implies a newly instantiated object. There's nothing wrong with it, I just don't think it's any better, just different. &gt; Perhaps you can also have trimmed_str that returns a string. The default I think should be to return a view where possible as the default API and always offer a _str for those who like to use auto everywhere. I ended up going with the idea that the api does 1 of 3 things. it either works in-place or via string_view or if the endpoint is allocating memory it appends _copy to the name. below are various function declarations. void trim(std::string *str); std::string_view trim(std::string_view str); std::string trim_copy(std::string_view str); If nothing else, the existence of trim_copy will cause developers who are unfamiliar to pause and look further to make sure they understand the semantic difference between the different versions. If I find there are misunderstandings I'll rename the string_view version 'trim_sv', but I don't expect that to be a problem.
It really depends on why you're rewriting in C++. If it's to learn the language then write a module for Apache. It won't scale at all though due to using a thread per connection and the inability to anything asynchronous in the handler function. It's unlikely to perform better than node.js' process-per-connection model. Then there's nginx which is/seems complex enough, so you'll probably spend more time reading about nginx than writing C++. If it's for performance reasons (the ability to handle 100's of thousands connection simultaneously) then you're pretty much out of luck. There are plenty of boost asio examples out there but they skim over the intricacies of HTTP so you'll need at least a parser and in-depth knowledge of the HTTP spec. The joyent HTTP parser code is opensource and is quite good, but this is what node.js uses! Not using node.js saves you from a process-per-connection but it will require a lot of work on your part. There is the hiredis client library and it's quite solid. For talking to MySQL we use libzdb which is ok but inexplicably uses C-style exceptions so you'll need to write a C++ wrapper for that. Are you sure you want to do this?
I recently did a very simple backend in C++ and used Crow library for the HTTP endpoints as well as for web sockets. I would not recommend doing it in C++ unless you really have to. It’s certainly a good exercise though
Can't I use something like this? https://github.com/eidheim/Simple-Web-Server https://github.com/civetweb/civetweb
I recomend you to have a look at uWebSockets, Qt WebSockets and Boost.Asio.
Possibly, but are they production quality? When we embarked on a journey similar to yours (getting rid of apache) these things didn't exist.
well, perhaps that's not the case anymore. I'm just now diving back into C++ after years of not doing more than keeping an eye on the new standards. So perhaps with the advent of std::string_view in C++17 that expectation is changing, but I think in the past it was a perfectly reasonable assumption about that code.
I'd look at [Beast](https://github.com/boostorg/beast). Recently added to boost, web server with ssl and websocket support, running on asio for portable high performance.
You can still write a simple c++ (local) webservice and use it behind a reverse proxy like nginx. It's a common pattern. It doesn't scale thou (nginx has multiple workers but your c++ process is one...) . 
It looks like this is the one what I'm looking for. Thanks!
Yes, it's the 'worker' pattern of apache and nginx that I detest. It will force you into using shared memory if these need to communicate, which is awful for performance and boost::interprocess is a bit of a head-f**k
Mutable references more strongly indicate out parameters, but maybe that's just me?
I've been doing a prototype system with [served](https://github.com/datasift/served), but it seems like a fairly young project. Perhaps interestingly, served seems to play well running in a thread in a boost::python library. So you could write some fast back-end code in C++ and build some hybrid python objects to interact with your system using Python. You're pretty much defining your objects in 2 or 3 languages at that point, but it's pretty flexible to use. I make http requests to an image classification system that currently uses OpenCV image recognition and Tesseract OCR. I can create requests for image processing in a Javascript front end to the C++ backend, and then process the requests with some python automation. The whole thing is centered around the python automation, which I'll be using to build automated video processing tests once the system is complete.
I don't know whether this makes a difference or not, but I'm the only user. Currently there are 23 http and 9 websocket APIs I'm working with, but websockets and the http server will only communicate with frontend when necessary.
Ooooooo. Man I wish I didn't need to use Java right now
With that approach you can't tell the difference between a const and a non-const reference without referring to documentation or looking at the source of the function. and yes, I'm aware that the core guidelines recommend const/non-const references for in and in/out parameters. I don't care. I'd rather see mutation via my_func(&amp;my_var); than my_func(my_var); my_func2(my_var); and only 1 of them mutates via the parameter list.
You've just shifted the problem. Can the pointer you're passing be null? Better consult the documentation. Besides, any modern editor will give you inline access to docs, be it via autocomplete or otherwise. 
Is your existing site well organised as a REST site? If yes, you may be able to translate the backend without much change to the frontend using CPP REST SDK. It used to be known as Casa Blanca, so don't let the repository age fool you. This is one of the most battle hardened web platforms out there. It is also in my experience better documented than most :-)
Java is not bad, but I agree with you!
I'm not a fan of the PIMPL idiom, although I understand it solves the problem it addresses. But it seems like a case where sketchy design choice (again, JMHO!) forces you into these sub-optimal situations. My solution to this might be some kind of wrapper for int where you can delete the problematic constructor. Others here have suggested similar. Others here have suggested the same in different words. 
&gt; So perhaps with the advent of std::string_view in C++17 that expectation is changing The concept isn't new, though. `string_view` and a few other C++17 addition are the first _standard library_ types to expose some of this behavior, but these are just standardizations of old and very common idioms. One of the reasons that `string_view` even got into the standard is because just about every large and major C++ project out there had its own incompatible analog. :) The problem isn't these new view types; the problem is the missing language facilities to make them safer than the old C code they replace. :)
Yeah with ltcg a full link step is about 4 minutes and it’s serial. We disable it for day to day development, and it’s maybe 30-45 seconds 
you're trying to back into reasoning to support your preference, which is always a mistake (as well as being transparent). It's ok to have a preference, and it's ok to disagree with my opinion, but it's not reasonable to attack the downsides of my preference as if the alternative doesn't have downsides of their own. You'll never have a reasonable discussion doing that as every approach has downsides. The question is whether or not the downsides warrant the upsides in general or in specific cases. Not to mention, your points are off the mark. null is dealt with the way it's always dealt with: gsl::not_null, static asserts, asserts, if checks, and so forth. All of these solutions are on the function writer and it's **unreasonable** to imply that someone who is calling a function expecting it to mutate one of the parameters is going to be unsure as to whether or not that parameter can be null. and I've never seen an IDE that can examine code written by a coworker and tell you what it does, so that point is moot. either way, I'm done with this conversation. As far as I'm concerned, your thought process is severely flawed and it's not worth my time or effort to deal with it.
I didn't say the concept was new, this is about what assumptions are reasonable while looking at a piece of code. move semantics isn't new either, but that didn't make it unreasonable for a developer to look at a pass-by-value argument or return a value and assume a copy was happening (even in the face of copy elision). obviously the context of the project matters when speaking about code, but on the whole without that context you would expect s = trim(s) to copy. Atleast in the past, I was making allowances for that changing due to the standardization of string_view. &gt; The problem isn't these new view types; the problem is the missing language facilities to make them safer than the old C code they replace. :) I have no idea what problem you're referring to, I thought we were talking about developer expectations when looking at a piece of code.
One thing you should do (that the standard library doesn't, typically for corner case performance reasons) is try to be aggressive about deleting out rvalue overloads. E.g. string foo(); string_view trim(const string&amp;); auto s = trim(foo()); // shit, s is a view into a non-existent string You can prevent this at compile time with: string_view trim(string&amp;&amp;) = delete; Or, as an alternative (I like this less) provide an overload that returns an actual string: string trim(string&amp;&amp;); Obviously, implement one in terms of the other. You have to be very careful with reference-like types like string_view, array_view/span. They behave more like references because they implicitly convert from types, you use them more similarly to a const T&amp;. But unlike references, they don't provide lifetime extension, and unlike references, they can be reassigned. Both of these characteristics are more pointer like, and make dangles more likely: { string x; const auto&amp; r = x; string_view s = x; { string y; // can't reseat r s = y; } // using s here is UB } const auto&amp; p = returns_a_string(); // safe, lifetime extension string_view p2 = returns_a_string(); // using p2 is UB
C++ programmers will probably disagree – you may consider to rewrite your backend in go. Go comes with a solid and efficient server implementation and is widely used. The basic of go is fairly easy to learn to C++ programmers. BTW, you can find more discussions in [this older reddit thread](https://www.reddit.com/r/cpp/comments/4o5zdm/a_good_c_or_c_honestly_so_long_as_it_works_http/).
thank you for pointing that out, I hadn't considered the r-value binding. I'm still trying to get up to speed subtleties of the new stuff.
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7m8g5l/debating_on_use_of_string_view_would_like_others/drsuj3r/?context=3.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
yes, you can use asio to write your async services.
&gt; Skipping '//tensorflow/cc/models:model': no such package 'tensorflow/cc/models': BUILD file not found on package path
you should not be returning a string_view, unless the "object" returning it owns it, if it does not, you are putting the burden of dealing with the memory on the caller, and it's not better than returning a pointer; don't do it, that's not what string_view is for, it makes no sense to return it
Ugh. Fun fact for when you escape embedded hell, implementations make `if constexpr` available in C++14 mode too, so you can use it as long as your compilers are sufficiently new and you locally suppress the warnings about using future technology.
I suppose one more off-topic reply won’t hurt: some combination of Race for the Galaxy’s Brink of War expansion, Bridge Constructor Portal, Hearthstone’s Dungeon Run, Elder Scrolls Legends, and Opus Magnum (which I haven’t yet started).
Your reply sounds like that bug was fixed in 15.6.
Yes. I don't have VS 15.5 on hand, so I have no idea, if it's fixed there.
Actually it's only an issue on Windows' systems. On *nix ones processes and IPC are very cheap. Moreover IPC is not usually needed, worker process should be stand alone. 
Escape? Hah, I wish I could, (well, sometimes) but that's our market. At least most ARMs are sane, but others can be... unique. Using libc++ with 16-bit size_t and CHAR_BIT? Good luck! :) It'll be fun to see what embedded customers come up with in the field as they continue the uptake of the language. It's a relatively recent development.
It's not specific to C++. It's an error from [Bazel](https://bazel.build/) I think. Bazel supports many languages and operating systems.
These are all basic questions that could be answered by reading the documentation. 1. No 2. Yes, though an argument can be made that it is slower than a highly specific customized implementation for the task at hand. 3. Theoretically yes. I have used ASIO (standalone, not the boost version) on Linux, Windows, OS X, including on embedded Linux with no need to change anything.
Slow compile times and unstable ABI.
Bookmarked. I never managed to get C++ version of Tensorflow working, but I was building it with contributed CMake project, because I read that Bazel wasn't working well on Windows, so maybe that was the problem, at least partially.
The last paragraph sounds wrong. Running the model multiple times gives wildly varying outputs?
&gt;I'd post it, but I've been doxxed before and live in a very politically oppressive country, where my job safety, physical freedom and custody of my children is at stake. I hope you understand. And yet, you chose username "PutinBitchBot". If I lived in such a place I'd stay miles away from such an obvious political statement. 
The fact that you think that my username is a serious political statement just goes to show how ridiculous this whole muh-russia thing is. Like, really? I can't belive I have to spell it out for you, but it's called **satire**.
What the other guy said. I'd suggest using the stand-alone version, as it uses more `std::` facilities, and will probably be marginally easier to translate into the standard's own networking library, which is based off of asio. It also doesn't come with all the baggage of boost.
in addition, this applies to not only Boost.Asio but really to the whole of boost and many other C++ libraries. 
the modulo expression cannot give negative numbers, unless there is an integer overflow. `-13` is an odd number `(-13 % 2) == 1 // (1) == 1`, therefore `true` `-20` is an even number `(-20 % 2) == 1 (0) == 1`, therefore `false` a good example of modulo arithmetic is a clock; subtracting 20mins from the hour, is the equivlent of adding 40mins to the previous hour. `-20 % 60 == 40`.
&gt;I think this is a distinction without a difference. I'm sorry, I cannot explain this more clearly. Read my example on pointer validation again and try to follow the reasoning. &gt;You're criticizing me for lack of historical perspective, but your very long post doesn't really get into this at anything but the most shallow level. On the contrary, it captures the most salient points: - UB was never intended as an opportunity for optimisation. It was only an excuse to leave out some very expensive checks. Those are not the same thing at all. - Code that was perfectly safe for decades, all of a sudden isn't. Anytime we recompile something that has been around for a while, we risk triggering new, and never seen before bugs. - There are still plenty of UBs left that currently do not trigger any problems, but given this particular trend, we should be worried about those as well. - The reasoning for not requiring a warning, being that compilers are unable to detect UB, is no longer valid. They do detect (some, not all) UB, and they should be required to provide a warning if that happens. Why do you suppose the discussion keeps coming up again and again? People are worried about this. And just telling them "well, don't do that then" is simply not good enough: we all make mistakes. &gt;Yes. You just have to write the check correctly (there's a number of ways to do this). Such as? Because the very concept is UB, so anything you can come up with here should be detectable by a sufficiently advanced compiler. &gt;Educate programmers so they correctly understand UB. Really, the overwhelmingly common cases the compiler optimizes that everyone brings up are null pointer checks and signed arithmetic. Make sure you understand how to do this correctly (and just avoid pointers as much as possible). Ask any programmer what's bad about C and they'll tell you "pointers". Why pointers? Because most of them get bitten by some kind of pointer-related UB situation. I've been programming for 30 years, and I still get hit with them. That's not because I'm uneducated or unaware of UB or its consequences, it is because I'm not perfect and make the occasional mistake. But nobody ever complained that addition and subtraction are so difficult in C. Why? Because despite them formally being UB in some situations, nobody was actually ever bitten by that. Until now, that is. All of this new super-smart analysis changes the simple act of addition into a minefield. Sure, it may look like a harmless piece of code, but maybe the compiler, after analyzing a million lines of context, can _prove_ the existence of UB, and get rid of some if-statement where you least expect it. Would you be able to foresee that as a programmer? Of course not. And again, that has nothing to do with education or understanding how to do this correctly. Accidents happen. This kind of super-optimisation multiplies an accident into a disaster. &gt;I've never heard of the standard requiring a warning for anything. 1.4:1: "The set of diagnosable rules consists of all syntactic and semantic rules in this International Standard except for those rules containing an explicit notation that “no diagnostic is required” or which are described as resulting in “undefined behavior.”" &gt; Throwing away optimization opportunities for this reason would be totally unacceptable to a large fraction of the C++ community (like mine) and people would just stop using that compiler. Those are pretty strong words, considering that right now we don't have a clue how much this type of optimisation actually contributes. I'd like to see some numbers, please. I also have a nagging suspicion that some of these optimisations (such as a+1&gt;a always being true) _never_ contribute any speedup; that each time it is triggered, it is actually a bug in the program that must be fixed, and that after fixing it, the performance benefit is gone. &gt;It really doesn't need serious discussion. If it did people would be seriously discussing it. SG12? And right here on reddit? You yourself wrote "This "people mad at compiler optimizations" things is just a weird and wrong thing that gets the same responses each time" And since we're here, I've some more comments on your original message as well: &gt;You could of course add tooling onto the optimizer to try to figure this out, but why not have a separate tool instead that tells you that foo is really badly written? Because the optimizer has already done the work of figuring it out, because quite frankly I'd rather deal with one tool than a dozen, and because some of those tools are only available on specific platforms that people may or may not want to compile on. &gt;It would also be a huge burden to add to the optimizer which is already the most costly part of building. Yes, because of all that analysis going on. Why repeat it just to get some extra warnings? &gt;This isn't even a language specific thing; without UB you wouldn't have binary search. Ok, that one you are going to have to explain. How does the concept of binary search rely on UB? How would a language that does not even have the notion of UB still rely on UB to do binary search? &gt; It's not an "excuse". Clearly, your industry is different from mine; in my industry we care about the performance this enables and I expect young developers to be open to learning and understanding UB, how it is leveraged by compilers, and how to properly write code in consideration of it. Do you show your young developers actual numbers for how much of a contribution UB makes to optimisation, or is it all handwaving? 
I used civetweb for a project, it was painful and full of bugs, we ended up rewriting. Not recommended.
you can look at Cutelyst: https://cutelyst.org/ it has a nice API and performs miles and miles faster than node since it's based on the great Qt event loop. https://www.techempower.com/benchmarks/#section=data-r14&amp;hw=ph&amp;test=fortune
We’ve used boost::asio for an embedded system (semi real time, 24/7) and it has worked very well for us. The other engineers hate boost (they’re still old school engineers, and probably hate what they don’t understand), but I integrated it in our code stack and it has given us a 100% reliable cross platform network stack (albeit with a significant increase in project compule times, so use a PIMPL pattern to address the long compile times). The docs could use a better set of tutorials, and the overzelous use of shared_ptr (in small tutorials which are available) is a turn-off for me personally (I’m old school and enjoy traditional pointers). Also, the tutorials (and boost in general) do seem like too much intellectual masturbation (it screams look how clever I am), I would prefer simpler tutorials without the additional noise. boost::asio will eventually be part of c++2@, so it will most likely become simpler to use. 
In my opinion it really depends on the context. If you are working in a large team and you use a lot of auto then avoid string_view without being explicit. It is not worth the risks. The danger of undefined behavior is too high. In any case have multiple versions. One trim(_inplace) which works inplace and another trim_copy that returns a copy. If you really want to use the string_view then call the function trim_view and delete the overload that takes an rvalue. This would help readers when auto is used extensively. Of course you still can do optimizations if an rvalue is handed to the trim_copy function. Yes that looks like old C++ but just being new for the sake of it can be dangerous. In my experience that pays off in the long run as it makes your code easier to read and reason about. Both are essential aspects when working in teams and on a product to be around for many years.
yeah, I've gotten conflicting thoughts on that :) I agree with the sentiment, but I think there's a place for having string manipulation functions that work solely on views as long as it's obvious. I was just going to have void trim(std::string *s); std::string_view trim(std::string_view sv); std::string trim_copy(std::string_view sv); but now that I've slept on it I think the following is probably better void trim(std::string *s); std::string_view trim_view(std::string_view sv); std::string trim_copy(std::string_view sv); There's never any question about which does it in place, and which allocates. And deleting the rvalue version is something I hadn't even considered. You're the second one to mention that so thank you. I'm just now getting back into C++ after years of doing nothing more than keeping my ear to the ground, so that advice is super helpful.
&gt; you're trying to back into reasoning to support your preference, which is always a mistake (as well as being transparent). Aren't you doing the same here? Additionally, you allow yourself to attack the person, which is way worse. &gt;It's ok to have a preference, and it's ok to disagree with my opinion, but it's not reasonable to attack the downsides of my preference as if the alternative doesn't have downsides of their own. &gt; You'll never have a reasonable discussion doing that as every approach has downsides. The question is whether or not the downsides warrant the upsides in general or in specific cases. These paragraphs contradict each other. This is absolutely reasonable to argue against one solution on the grounds that an alternative, while having its own downsides, has fewer of them. At least Qt creator highlights ref and const ref arguments differently. I am sure other IDEs should be able too. Clang-tidy warns if the function takes a parameter as mutable ref but uses as a const ref only. And I am deeply disappointed by your last paragraph. Remember, if someone resorts to 'ad hominem' arguments, they are seen as they've run out of logical ones. Do you want that?
Wow, once you go down the Clean Code route, it’s hard to go back. Almost all the code I just reviewed there was awful. Huge functions, complicated if statements, loads of parameters... Yuck!
&gt; Yes that looks like old C++ but just being new for the sake of it can be dangerous. I agree with that 100%. I think the following is what I've landed on: void trim(std::string *s); std::string_view trim_view(std::string_view sv); std::string trim_copy(std::string_view sv); 
test
The implementations are of course different. Only windows IOCP can be simply mapped to ASIO's proactor pattern. On other platforms, ASIO has to simulate the similar behavior. The performance difference should be same for most cases, while on linux, the cpu usage could be higher for lots of connections.
As I already said, I'm done with this conversation.
A reference is a glorified pointer. Would you design a public API taking a Foo const&amp; ? Yes, yes I would.
I'm going to refrain from nuking this, because having the answer to number 1.) on something indexed by Google seems important.
C
Lots of people hate boost!
We use it for sending thousands of HTTP messages/sec, 24/7 without issue. You will have issues if your commercial software is distributed in shared library form rather than a standalone executable, but this is both the fault of C++ and boost.
What issues exactly?
Is beast not just a wrapper over asio? How much HTTP functionality does it give you?
This *is* wrong imho, an ann is just a function like any other, same input should give the same output. This sounds like uninitialized memory somewhere (i.e. the random results).
Lovely things like this https://stackoverflow.com/questions/39948997/use-of-boost-log-and-boost-asio-causes-a-crash/39958285#39958285
/u/philocto this is how a reasonable person consumes and responds to a message. I gather by the rest of the thread you have a difficult time navigating interpersonal matters, so maybe you can use this as a learning experience. I'm not holding my breath, though.
Thanks, I hoped I wasn’t the only person that smelled suspicious to.
This goes back to a [blog post I wrote seven years ago](https://lanzkron.wordpress.com/2011/02/21/inferring-too-much/) about the dangers of `auto`. At the time [Herb Sutter said](https://herbsutter.com/2012/04/03/reader-qa/) that they're considering an `operator auto` but that seems to have been abandoned. If we had `operator auto` then the `string_view` class would be safe to use in such cases since it would detect type deduction going on and construct a new `string` object. std::string std::string_view::operator auto() const { return *this; }
honestly, I'd rather be able to disallow auto assignment from a method or function. std::string_view trim(std::string_view s) noauto { }; and let auto x = trim("mah strang"); fail to compile with a clear message that auto initialization has been disabled for that particular function/method. or perhaps do it on the type itself although I think that's probably a little too large as a hammer. 
If we had `operator auto` you could `= delete` it and get the behaviour you want. In my opinion this should be part of the type not the function. A type that isn't safe to be captured by `auto` is never safe, this doesn't depend on the function. I don't think this is _too large a hammer_, it's a tool that makes sense and currently C++ has a nasty gotcha that it could easily get rid of. 
You're not stuck with the name `trim`. You can have a function `trimmed_view` that returns a view.
&gt; A type that isn't safe to be captured by auto is never safe, this doesn't depend on the function. I disagree with this. To me the important point is being able to understand the assumptions in the code when reading said code. It's the unknown assumptions that tend to bite you hard. For example: auto s = trim_view(my_string); I can read that code and understand that it's returning an std::string_view, therefore when I'm reasoning about the code I can take that into account. I would even say if this sort of naming scheme, or something else to identify it, was recommended in the core guidelines, you could even call this perfectly safe and acceptable. My view is that the goal isn't to make dangerous things impossible, but to make dangerous things obvious. OTOH, I haven't put too much thought into it either, it was mostly an off-the-cuff idea. I think you'd really have to consider how it interacts with template expansion before you could say for sure what a good approach is. And I'm not wholly against disabling it for the type, I just don't agree with the absolute given in the above quote.
yeah, that's what I ended up doing. void trim(std::string *s); std::string_view trim_view(std::string_view sv); std::string trim_copy(std::string_view sv); I feel like you can determine the behavior at the call site in every variation.
No, I don't want a warning every time the compiler assumes `int*` and `double*` do not alias each other. No, I don't want a warning when the compiler sees a `for(;;)` loop that contains `&lt;` instead of `!=` on an `int` that is increased by 1 each loop, and can generate faster code with `!=`. Your position seems to be "anything that could change the semantics away from what you, personally, understand as unoptimized code behaviour -- what assembly you would write if you translated the C++ by hand -- should emit a warning". Maybe I don't understand your position. 
&gt; UB was never intended as an opportunity for optimisation. It was only an excuse to leave out some very expensive checks. Those are not the same thing at all. I think you're just doing gymnastics at this point. Leaving out an expensive check is a performance improvement. The reason to have UB (primarily) is to improve performance. That's all there is to it. &gt; And again, that has nothing to do with education or understanding how to do this correctly. Accidents happen. This kind of super-optimisation multiplies an accident into a disaster. I've been bitten by UB like every C++ developer, but I've never been bitten by these optimizations in particular. Maybe it's because I simply don't dereference before null pointer checks, and I correctly check for signed overflow? Do you have examples where this wouldn't be sufficient? &gt; Those are pretty strong words, considering that right now we don't have a clue how much this type of optimisation actually contributes. I'd like to see some numbers, please. I don't. But then, you don't have any numbers for anything, either. Neither performance, nor percentage man hours wasted. &gt; that each time it is triggered, it is actually a bug in the program that must be fixed, and that after fixing it, the performance benefit is gone. As I already showed, even if programmers are unlikely to ever literally write `if (a+1 &gt; a)`, does not mean that it never shows up. Other optimizations (especially inlining) can collapse code together. I don't have a good example offhand for signed comparison, but I'm pretty sure you can see that with null pointer checks, they are going to be optimized out *a lot* if the function gets inlined (as my example shows). &gt; Because the optimizer has already done the work of figuring it out, because quite frankly I'd rather deal with one tool than a dozen, and because some of those tools are only available on specific platforms that people may or may not want to compile on. The optimizer in a compiler like clang, which has a separate frontend and backend, AFAIK, is not even aware of the original source code. It works entirely with IR and assembly. On top of this, the optimizer is making many passes; sometimes things get optimized out only after other things have already occurred. Is enough information being kept around to reconstruct exactly what a certain optimization corresponds to in source? I'm not so sure. Seems extremely non-trivial and expensive to me. I think having a separate tool makes sense. The people working on clang, esp at google, are highly incentivized to make the most logical choice here. Why did they put so much effort into a separate tool, clang-tidy? &gt; Ok, that one you are going to have to explain. How does the concept of binary search rely on UB? How would a language that does not even have the notion of UB still rely on UB to do binary search? C++, the language, has UB. That means that there are programs for which a conforming compiler can output whatever it wants. Python, the language, does not. But a binary search function (written in python) has the precondition that its input is sorted. That function makes no guarantees as to what would happen if you feed it unsorted input. It could enter an infinite loop for example. At the Python language level, this is not UB, but at the level of the function itself, it is UB. UB is not about any particular language spec, it's part of the contract of a function/program, and it happens whenever a precondition is violated. Many functions have preconditions. Compilers and interpreters are functions too, which mostly don't have UB (though C and C++ do). &gt; Do you show your young developers actual numbers for how much of a contribution UB makes to optimisation, or is it all handwaving? Well, I teach them to dereference their pointers after a null check, and I teach them how to check for overflow correctly. Haven't had any issues.
Const reference to guts also constrains implementation, e.g. if `vector::size`/`capacity` returned const ref, you would actually have to implement `vector` using a pointer and two integers instead of three pointers.
There are literally dozens of us!
You can check out socket.io. https://socket.io https://github.com/socketio/socket.io-client-cpp
Agreed 100%, the docs are really terrible. They are very useful as reference, albeit only after one has already learnt how to use asio.
Check out [poco](https://pocoproject.org/docs/Poco.Net.Socket.html)
&gt; I still think that being a good programmer is mandatory to using c++ There's certainly a steep learning curve. I've been a dev for more than 15 years, and I've been learning C++ for a few years, I still feel there's a lot for me to learn. :P
I would strongly recommend to use [asio](https://github.com/chriskohlhoff/asio/) or its [boost version](http://www.boost.org/doc/libs/1_66_0/doc/html/boost_asio.html). It is on its way to standard, so I consider it to be a choice by default, and one should have strong reasons to use something else for network stuff.
Have you tried the VS option /debug:fastlink? It halved the link time for us. It's not perfect (some debug symbols don't work) but the link time savings more than made up for that (for us).
&gt; It is on its way to standard, so I consider it to be a choice by default, Asio is indeed the default choice but I wish we had a better alternative for standardization. By all accounts the library is feature rich and performant but I find it over-engineered and and not intuitive to use at all.
How to make an entry with newline as its content?
Exactly, its overengineered, which makes it very less intuitive
I've always wondered if a clever use of `clz` would indeed help. I mean, `clz` + `cmov` give you a log10, but you don't care (really) about log10, so using `clz` as the input into a `switch` should allow you in a single multi-target jump to get "about log10". For example, using `clz` to get the number of digits: int nb_digits_ifless(std::uint64_t n) { if (n == 0) { return 1; } switch (__builtin_clzll(n)) { case 0: return 20; case 1: return 19 + (n &gt;= 10'000'000'000'000'000'000ULL); case 2: case 3: case 4: return 19; case 5: return 18 + (n &gt;= 1'000'000'000'000'000'000ULL); case 6: case 7: return 18; case 8: return 17 + (n &gt;= 100'000'000'000'000'000ULL); case 9: case 10: return 17; case 11: return 16 + (n &gt;= 10'000'000'000'000'000ULL); case 12: case 13: case 14: return 16; case 15: return 15 + (n &gt;= 1'000'000'000'000'000ULL); case 16: case 17: return 15; case 18: return 14 + (n &gt;= 100'000'000'000'000ULL); case 19: case 20: return 14; case 21: return 13 + (n &gt;= 10'000'000'000'000ULL); case 22: case 23: case 24: return 13; case 25: return 12 + (n &gt;= 1'000'000'000'000ULL); case 26: case 27: return 12; case 28: return 11 + (n &gt;= 100'000'000'000ULL); case 29: case 30: return 11; case 31: return 10 + (n &gt;= 10'000'000'000ULL); case 32: case 33: case 34: return 10; case 35: return 9 + (n &gt;= 1'000'000'000ULL); case 36: case 37: return 9; case 38: return 8 + (n &gt;= 100'000'000ULL); case 39: case 40: return 8; case 41: return 7 + (n &gt;= 10'000'000ULL); case 42: case 43: case 44: return 7; case 45: return 6 + (n &gt;= 1'000'000ULL); case 46: case 47: return 6; case 48: return 5 + (n &gt;= 100'000ULL); case 49: case 50: case 51: return 5; case 52: return 4 + (n &gt;= 10'000ULL); case 53: case 54: return 4; case 55: return 3 + (n &gt;= 1'000ULL); case 56: case 57: return 3; case 58: return 2 + (n &gt;= 100ULL); case 59: case 60: return 2; case 61: return 1 + (n &gt;= 10ULL); case 62: case 63: case 64: return 1; } __builtin_unreachable(); } And a single-branch formatting: void write_ifless_depless(int nb, std::uint64_t n, char* buffer) { switch (nb) { case 20: buffer[nb - 20] = '0' + (n ) / 10'000'000'000'000'000'000ULL; case 19: buffer[nb - 19] = '0' + (n % 10'000'000'000'000'000'000ULL) / 1'000'000'000'000'000'000ULL; case 18: buffer[nb - 18] = '0' + (n % 1'000'000'000'000'000'000ULL) / 100'000'000'000'000'000ULL; case 17: buffer[nb - 17] = '0' + (n % 100'000'000'000'000'000ULL) / 10'000'000'000'000'000ULL; case 16: buffer[nb - 16] = '0' + (n % 10'000'000'000'000'000ULL) / 1'000'000'000'000'000ULL; case 15: buffer[nb - 15] = '0' + (n % 1'000'000'000'000'000ULL) / 100'000'000'000'000ULL; case 14: buffer[nb - 14] = '0' + (n % 100'000'000'000'000ULL) / 10'000'000'000'000ULL; case 13: buffer[nb - 13] = '0' + (n % 10'000'000'000'000ULL) / 1'000'000'000'000ULL; case 12: buffer[nb - 12] = '0' + (n % 1'000'000'000'000ULL) / 100'000'000'000ULL; case 11: buffer[nb - 11] = '0' + (n % 100'000'000'000ULL) / 10'000'000'000ULL; case 10: buffer[nb - 10] = '0' + (n % 10'000'000'000ULL) / 1'000'000'000ULL; case 9: buffer[nb - 9] = '0' + (n % 1'000'000'000ULL) / 100'000'000ULL; case 8: buffer[nb - 8] = '0' + (n % 100'000'000ULL) / 10'000'000ULL; case 7: buffer[nb - 7] = '0' + (n % 10'000'000ULL) / 1'000'000ULL; case 6: buffer[nb - 6] = '0' + (n % 1'000'000ULL) / 100'000ULL; case 5: buffer[nb - 5] = '0' + (n % 100'000ULL) / 10'000ULL; case 4: buffer[nb - 4] = '0' + (n % 10'000ULL) / 1'000ULL; case 3: buffer[nb - 3] = '0' + (n % 1'000ULL) / 100ULL; case 2: buffer[nb - 2] = '0' + (n % 100ULL) / 10ULL; case 1: buffer[nb - 1] = '0' + (n % 10ULL) / 1ULL; return; } __builtin_unreachable(); } Now, this is unoptimized (written for clarity and demonstration purposes); still: - only 2 multi-target jumps are necessary, so branch prediction should never get too bad, - there is no dependency between the various cases of the second switch, allowing parallel execution, - only constants are used, thus I expect the compiler to be able to optimize away the divisions/multiplications with strength reduction tricks. I wonder if this approach could benefit of some of the tricks mentioned above (radix-100 formatting, BCD encoding, etc...) to the point of actually beating the current solutions. What do you think /u/Veedrac?
Indeed - I use it for day to day development but it’s unusable for giving builds to QA. All trade offs, of course. The real issue is the 2.5million lines of code, anything else is just masking the problem. 
The "Deco" approach is to make a multi-line string a set, with each entry representing a single line: string: this is a multi-line string :
Depends on whether they use any probabilistic modelling internally. I dont yet know the details of TF, but its not uncommon to use things like that for deep learning. 
THANKS a huge amount! Over my break learning some ML was one of my goals, and this will move me along that path. 
Course, in the JVM ecosystem that means you can use my favorite, **Scala** (which is just a bit above **C++** for me personally). 
I encourage you to read: http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html (and its 2 follow-ups) by Chris Lattner, who created LLVM and the Swift language.
&gt; bool IsOdd(int x) { return ( x % 2 ) == 1; } The != 0 also compiles really nicely IsOdd(int): mov eax, edi and eax, 1 ret Where == 1 isn't as compilable IsOdd(int): mov eax, edi shr eax, 31 add edi, eax and edi, 1 sub edi, eax cmp edi, 1 sete al ret
http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html The problem with your "warning" proposal is that: 1. There are many legitimate cases of the compiler taking advantage of UB which arise after various optimizations passes or as a result of translating a high-level language (C++) into a lower-level language (the compiler IR), you're going to drown in warnings, 2. Actually reversing the suite of translations/optimizations to produce a helpful warning is a titanic work. In short, it's not done because of dubious usefulness and massive work required. --- That being said, I do agree with you that C++ has way too much Undefined Behavior (and Implementation Defined Behavior) for my liking. Some of it has very strong motivations (double-free/dangling-pointers are really hard to diagnose statically and costly to diagnose at run-time), but others seem to be there just to be mean or have rather dubious usefulness in today's computing landscape.
Look at [that generated assembly](https://godbolt.org/g/cFERpk) though. This might be something that works if you carefully handwrite the ASM, though it would be damn hard and you're severely limited by those branch mispredictions (you barely have time for one), but I wouldn't dream of trying to trick a C compiler into generating something that wasn't atrocious.
&gt; Linux has it much simpler, given the package management and things like makefiles. No. Not really. I don't *want* to use the Linux package managers to compile code: - I don't want to install libraries/executables on my build agents, they're not going to use the compilation result but only build it, - I do want to be able to cross-compile, - I do want to be able to build programs depending on different versions of a dependency, - ... Linux package managers are the wrong solution here because they solve a fundamentally different problem to start with.
Agreed. Actually, at this point, I'm not quite sure that backward compatibility is such a boon. There is an oft forgotten cost to backward compatibility: increased language complexity. Each new feature of the language interacts with the other existing features (in general), whether at the syntactic or semantic level. This means that when adding a new feature does not linearly increase the language complexity, it quadratically increases it. That is, if the current language has complexity N, adding one new feature results in a language with complexity N x N. Removing features, in exchange, drastically simplify the language (`sqrt`). --- At the very least, though, I wish the committee would avoid adding technical debt from the get go. C++11 uniform initialization syntax was a resounding disappointment to me: - you can use `X{...}` for constructors to avoid syntactic ambiguities, - unless the type has a constructor from `std::initializer_list&lt;...&gt;`. This is horrible. The Uniform Initialization Syntax is actually not uniform, sometimes you have to use `X(...)` and worse of all adding a constructor from `std::initializer_list&lt;...&gt;` is API-breaking. :(
How does it handle "unprintable" characters/other chars that usually require escaping?
Interesting but adds nothing. Did you watch the presentation?
Coul you elaborate a little? 
&gt; I don't want to use the Linux package managers to compile code: In like 9/10 package managers, you just simply download prebuilt binaries. Given this fact, you base your whole comment on an incorrect presumption.¨ &gt; I don't want to install libraries/executables on my build agents, they're not going to use the compilation result but only build it, Your build agents will need the dependencies one way or another though. &gt; I do want to be able to cross-compile, When is this an issue? You are not making very much sense. Package managers usually come with great tools for cross compilation, including prebuilt dependencies for other architectures. &gt; I do want to be able to build programs depending on different versions of a dependency, So install different versions of the dependency. Package managers usually come with functionality to manage different versions of the same package. &gt; ... I know that your other arguments seem to lack any sort of substance, but you simply cannot replace substance with quantity here. Especially not with 3 dots. &gt; Linux package managers are the wrong solution here because they solve a fundamentally different problem to start with. I dont think you understand how package managers work, or how they very much help setting up a build environment and adding dependency libraries. It's difficult to not offend you here, but this is like the worst comment I've had the privilege to respond to this year. Congratulations, you win most-out-of-ass comment 2017. ---- # And in an attempt to be a nice person: Package managers are awesome! Here's why: Under windows, you probably know the process. Browse the different sites for each dependency, and their dependencies, and so fort. Download the dependencies from whatever sources they come from. Download cmake and any other build tools. Painstakingly generate project files and manually build everything, making sure to make a few mistakes here and there to add a few hours to the process. Under linux, open a terminal, type `sudo pacman -Syu glm-dev glfw-dev anyotherdep-dev` and hit enter. All of the dependencies now exist in your system, ready to be used! Then simply use `pkg-config` to retreive the compile and linker flags you need to use in your makefile or whatever. 
`static_assert` is only useful for duck-typed generic code (C++, D); Rust uses nominal generic code, so the constraints you'd state in `static_assert` are instead already stated by requiring that the type implements a particular `trait`. The equivalent of `constexpr` is `const` (which means compile-time in Rust, mostly...). Rust supports a limited amount of compile-time computations at the moment, although since MIRI^1 landed the rustc compiler is theoretically capable of interpreting *more* code than C++ compiler: you can use memory allocations in MIRI, so you should be able to use `Vec` and `String` in `const` functions! ^1 MIR Interpreter, where MIR is Middle-level IR, where IR is Intermediate Representation.
My scope was printable characters since the goal is human readable/writable format. There's no fundamental reason it wouldn't work with "unprintable" characters since it doesn't have any special logic distinguishing between printable and unprintable. If you want you can give me a case example and we'll think about it. 
&gt; The ideas brought forward by Rust are the future. Yes. I think it's important to disentangle ideas and language here. Rust proved that some static checks were *pragmatic enough* to be implemented, which I hope will spur further investigations in the area, however there's no reason that only Rust could use those ideas! &gt; C++ is implementing them. Might be, but I am not that hopeful. The core ideas of Rust (ownership/borrowing) are such a tremendous break from current C++ code that they cannot be integrated wholesale without throwing backward-compatibility out the window. Many current C++ APIs would have to be completely overhauled to integrate those concepts and they are the cornerstones of Rust's safety. And it's not clear to me that C++ should launch itself headlong into this right now. While it's been proven that Rust programs can work, the idioms/guidelines on how to create good APIs within the constraints of borrowing will take years to emerge.
What can be simpler? #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;boost/asio.hpp&gt; namespace ip = boost::asio::ip; int main() { ip::tcp::iostream s("www.google.com", "http"); s &lt;&lt; "GET / HTTP/1.0\r\n" &lt;&lt; "Host: www.google.com\r\n" &lt;&lt; "Accept: */*\r\n" &lt;&lt; "Connection: close\r\n\r\n"; for(std::string l; std::getline(s, l); ) std::cout &lt;&lt; l &lt;&lt; '\n'; } live demo: http://rextester.com/CETCT51338 
What can be simpler? #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;boost/asio.hpp&gt; namespace ip = boost::asio::ip; int main() { ip::tcp::iostream s("www.google.com", "http"); s &lt;&lt; "GET / HTTP/1.0\r\n" &lt;&lt; "Host: www.google.com\r\n" &lt;&lt; "Accept: */*\r\n" &lt;&lt; "Connection: close\r\n\r\n"; for(std::string l; std::getline(s, l); ) std::cout &lt;&lt; l &lt;&lt; '\n'; } live demo: http://rextester.com/CETCT51338 
Can I *enforce* compile-time evaluation? Also, I use static_assert for a lot of non-type parameters. Think of things like clock dividers and DMA buffer sizes.
On my quick glance through I didn’t see mention of probabilistic. Even if it’s there the author justified his answer by claiming that modelling from three inputs with only a couple of layers was unstable. I don’t do NN research, but this intuitively seems wrong. Simple multi-linear regression should do an alright job on the dataset he was using, so what would be the point of using a much more complicated and computationally intensive method if the result was worse? I am hoping the other explanation (uninitialised memory) is the correct one.
Unfortunately, I just don't think you can get away from it. At least, not with the C++ we have today. I think the biggest problem is simply that feature removal will almost certainly break larger applications. The more dependencies you have the higher the likelihood that one of them relies on that removed feature. You might be able to get around this with an ABI or something along those lines, but we don't have that today. Right now, there is a strong possibility that you can't link things using different versions of the same compiler. Which means that your compiler needs to still support all those past features and have flags to compile the old code. Further, the build tools around the old code need to know how to signal the compiler "Hey, built this using X version of C++". You end up with the python 2 vs python 3 problem. A breaking changed ends up bifurcating the community.
Why not use prefixed lengths like a human-readable version of BER and thereby avoid delimiters altogether? That encoding also has datatype rules, which might be nice in some cases.
Compile-time/Meta-programming features in Rust. Technically unlocked: - `const` functions (`constexpr` functions): MIRI^1 landed in rustc, and is more capable^2 than existing C++ compilers, so it's just a matter of settling the policy before rustc overtakes C++ compilers. - `const` variables (`constexpr` constants): relatively limited right now; policy could quickly allow a lot more, and I see no technical issue (only work) in allowing types containing heap-allocated variables (such as `String`, `Vec`, etc...) since MIRI can handle them. Short-term (6 months/1 year horizon): - non-type generic parameters: a conservative implementation is in the works, and should unlock a good amount of use cases, notably allowing implementing traits on arrays *once* (instead of once for every cardinality). Further refinements may come later, on an as-needed basis. - generic associated types (close to Higher-Kinded Types): actively being worked on, and initial results look good. It's going to stir up a hornets' nest though, since many `std` APIs could be made much better with GAT but maybe not in a backward compatible way; I expect lively discussions and hope good solutions will be found then. Unclear: - specialization: a buggy implementation exists, and the bugs are being smoothed out one at a time, however it's unclear whether some could actually *block* the feature AFAIK. - variadics: its native support for tuples and function-like types covers a lot of use cases already, many others are adequately covered by its macros, ... so it's unclear what is really *needed* and the community is loathe about adding features just to tick a box. ^1 *Essentially, an interpreter for the internal representation of Rust code used in the compiler.* ^2 *MIRI handles memory allocations, and de-allocations, so it can use types such as `String`, `Vec`, `HashMap`, ... It does not however support I/O or multi-threading, and even if it did the functionality may not be fully exposed at compile-time in order to guarantee deterministic builds.*
Prefix an entry with its string-length? If so that should be hard to write and maintain by humans.
For me, I think the weakest part is that the names or identities of things in the standard, for many reasons, don't always match those commonly used by other languages. For instance, transform and accumulate, which arrived early on before other languages got map and reduce or left fold, but for someone coming over might not be obvious.
This is like arguing C++ is easy by showing someone HelloWord
I disagree, it may look over engineered because you can't whip up a quick example, but the truth is that for anything non-trivial, you would end up redesigning and re-implementing pretty much the same thing by the time you are done.
Ckeck out the code to write a simple hello world webserver in python, and compare it with what is needed to be written using asio, asio is easy for seasoned programmers, but not for a layman. The simple approach should be, i declare a socket object, initialize it using some ip and port, bind listening port and then setup is done
Kind of like the code we write using winsock, but a toned downed version and more simple version of the code, remove htonl, filling all the other parameters, turning in the API, etc
How much Unicode are we talking about? --- &gt; Walking on water and developing software from a specification are easy if both are frozen. &gt; -- Edward V. Berard. My main issue with Unicode is that it's an evolving standard. This poses some challenges as to integration in languages/standard libraries. For example, the [character properties](https://en.wikipedia.org/wiki/Unicode_character_property) of code points have changed between v9 and v10 (see UAX #29); thus any algorithm based on those properties exhibits a different behavior depending on the version of Unicode. This is not an isolated example. It's common for the Unicode committee to refine collation, normalization, etc... if you use normalization before a look-up in the database though, you very much **don't** want the algorithm logic to change under your feet! Oh, and in some environments, you may need to handle *multiple* versions of Unicode in a single executable. --- As a result, you really *don't* want a language/standard library to pick the version of Unicode for you. This leaves few solutions, unfortunately: - no support: easy for implementers, not great for users... - barebone support: don't support Unicode, support Code Points and the various Unicode encodings (UTF-8/16/32); this is what Rust do, - full support: support each version of Unicode, with a distinct namespace? Maybe? Then there's also the issue of *bug fixes*. If your implementation of Unicode v10 is incompatible with another because it's buggy, you would ideally wish to update it. Or conversely, if upgrading your implementation of Unicode v10 causes look-up issues in the DB because the normalization algorithm is different (another name for "fixed"), then you may want to NOT upgrade. If the update is tied to a language version bump; you lose the flexibility to get one without the other. Personally, I think that barebone support in the language/standard library with full support via external libraries is the best solution from a flexibility point of view.
&gt; but I really miss Lisp macros whenever I use C++ It's not clear to me that macros would help here; in TMP you typically use *semantics* whereas most macros are purely *syntactic*.
* Poco * boost asio Boost asio has a stiffer learning curve than poco if you are not familiar with boost ecosystem. Boost has better documents, more examples, a lot of 2ndard libraries above boost. Another advantage of boost asio, a small part of this is going to be part of c++20. So, I would recommend you to use boost library. For Boost, the documentation are * http://www.boost.org/doc/libs/master/doc/html/boost_asio.html * https://www.amazon.com/Boost-Asio-Network-Programming-John-Torjo/dp/1782163263 Good luck !! 
&gt; Overloaded functions are just too useful when doing generic programming. There are other ways to define overloads. For example, you could define an open-ended *named* "overload-set" when you define a function; then in your generic function you'd specify which "overload-set" to pick the function from. Frankly, the name look-up/overload resolution of C++ is very ad-hoc and you regularly wonder why a particular overload gets (or doesn't get) picked. I would favor a more principled approach here.
Yeah cause it feels like I'm going in circles
For a desktop app, I'd recommend taking a look at [Poco](pocoproject.org) or [Qt](https://doc.qt.io/qt-5.10/qtnetwork-index.html) Both have fairly decent documentation and easy to comprehend. Poco has this nice [walkthrough here](https://pocoproject.org/slides/200-Network.pdf) of pulling up a simple socket client or server.
I wish we could at least get rid of implicit *narrowing* conversions. I had a junior developer writing this very simple code: double total = std::accumulate(begin(double_vec), end(double_vec), 0); and honestly I saw nothing wrong with. Its Unit-Tests looked good too. It's only in regression testing that we noticed some strange behavior. How much time does it take you to figure it out?
So it's not just about human readability but also ease of human writability.
There is actually an easy work-around for this issue. First, you want a couple base classes, such as: class NoMove { protected: NoMove() = default; ~NoMove() = default; NoMove(NoMove&amp;&amp;) = delete; NoMove&amp; operator=(NoMove&amp;&amp;) = delete; NoMove(NoMove const&amp;) = default; NoMove&amp; operator=(NoMove const&amp;) = default; }; Then when you inherit from this base class, it specifically disables the default move constructor and move assignment operators. Annoying, perhaps, but at least it works.
Take a look at zeromq as well. It helps a lot when you have to manage multiple connections. A "socket" in zeromq is actually a higher-level abstraction that can connect to multiple endpoints, store messages in a queue, reconnect automatically, etc.
I easily agree with trim, split and tokenize. I also really like `starts_with` and `ends_with`. However case sensitivity is a wildly complicated topic: it depends on the language, and sometimes the *usecase* (I seem to remember dictionaries and phonebooks behaved differently in German...).
Uninitialized memory seems weird to me. Usually your answers are *NAN* or *+INF* and things like that. Things in the relative domain of your problem set are incredibly unlikely. I havent seen any comments of probabilistic functions used in the base of TF or anything like that, so yeah, thats weird too. But Im just starting my ML adventure, so I barely know anything yet. 
It's actually very different. How do you translate: map.if_exists(auto&amp; val : "key") { if (val.done) { break; } // other... } else { if (first) { continue; } // other... }
Yes, in spirit of C++, it makes simple things simple.
I so agree. I find the reluctance to integrate *sum types* and *tuples* as first-class language features extremely annoying. The wonky library versions are just not as nice to use, and cause non-trivial compilation-time and binary-size bloat.
That's because Asio doesn't embed a webserver. Try to implement a webserver with python asyncio to compare apples with apples. However I hope that c++ coroutines will simplify the callback hopping we have now. 
I would note that the latter is "fixed" by distributing code and letting others compile your code with their toolchain, their set of options, etc...
I would note that in many companies, this online code generation approach would be rejected outright, unless the service itself can be hosted on the company premises. And even then, I for one very much prefers checking in the generator and sources rather than the generated files. The sources are generally much clearer to read.
I don't see how having default implementations is an issue. State is a much different beast, and `interface` prevents it.
&gt; I guess that will be a pattern; it's a flawed language (like all languages) but at the same time it's relatively easy to work around the flaws. Having built an introspection framework based on macros/template meta-programming, I disagree that it's "easy to work around". The one developer who suggested it was reasonably experienced (close to 10y) and managed the macro-part quite well (which is definitely non-trivial) but was foiled by the TMP side of it. I had to step in, and it took the two of us quite a few days to get something going. The end result worked, but the error messages when it failed were horrible, and adding in new features was NOT a smooth experience. I fully expect the whole thing was ditched within a couple years of us two leaving the team; it was just an unmaintenable mess.
I agree. I sometimes wonder if there is a false-economy fallacy in the industry though: - the cost of migrating away from a deprecated feature is immediately felt, - the cost of weird interactions between a "mostly unused" feature and new features is hard to quantity. Therefore the industry values backward compatibility very high, but it's unclear to me that it's objectively better.
Declaring a variable `const` enforces compile-time evaluation in Rust, yes, that's the whole point. Regarding `static_assert` on non-type parameters, I expect that this will be available in `where` clauses since this is where constraints are placed today for type parameters. &gt; also, memory allocation in constexpr functions is either in TS or C++20/17 Oh, do you know if this means people solved the issue of having a `constexpr std::vector&lt;int&gt;` or is memory allocation only usable internally?
Would you mind unpacking "nonprwf" / "prwf"? I'm assuming it's a value category thing, but I'm not familiar with this shortening.
I'm curious, what features did it have that it was so complicated? I built a reasonably featured reflection framework in 100 lines of code (using boost PP but no other third party library). On top of that, it took me 50 lines of code to create macros for structs that auto serialize/de-serialize into/from json using nlohmann/json. If you really need something that featureful, could just use Hana, it's pretty thorough. I probably would have except we didn't meet the compiler version requirement.
&gt; &gt; I don't want to use the Linux package managers to compile code: &gt; In like 9/10 package managers, you just simply download prebuilt binaries. Given this fact, you base your whole comment on an incorrect presumption. I think there's a misunderstanding here. I meant that I didn't want to use Linux package managers to install the dependencies when all I wanted was use the headers/libraries of those dependencies for compiling *my* application. &gt; &gt; I don't want to install libraries/executables on my build agents, they're not going to use the compilation result but only build it, &gt; Your build agents will need the dependencies one way or another though. Sure. But there's a big difference between installing the dependency on the box for everyone to use and using a copy of the headers/libraries locally and temporarily (though ideally cached) during the compilation process. Just because I want `libcurl` doesn't mean I want the box (or user) to get `curl`, for example. A number of packages require privileged access for installation, or may alter the environment, etc... I don't want any of THAT. &gt; &gt; I do want to be able to build programs depending on different versions of a dependency, &gt; So install different versions of the dependency. Package managers usually come with functionality to manage different versions of the same package. I've been bitten more than once by a package manager breaking some of the installed executables by updating a shared library when installing another executable. Also, just because two versions are binary-compatible doesn't mean I *want* the upgrade. I prefer pinned dependencies to get a reproducible build. This means that I don't want to depend on libxx or libxx-1.2; I want to depend on libxx-1.2.4-rc3 if I need to. I have yet to find a package manager for Linux which allows multiple versions of libxx-1.2.? to exist on the same system.
Whatever be it, i am not a very advanced c++ guy, but i created a very rudimentary chat server, i copied the initial setup code for winsock and then implemented the easy bind, listen, send, recv functions myself, that was what i was talking about
These asio guys should provide a easy, expert mode
&gt; Did you watch the presentation? No. And I don't plan to. &gt; Interesting but adds nothing. It's a reply to your comment: &gt; It seems C++ now finds itself in a place where because one tool (the compiler) uses UB as an excuse to be very clever and generate fast code for an incorrect program, that we now need a second one (a static analyser) that flags up UB as an issue that needs to be resolved. There seems to be an assumption that C++ compilers are out to get you^1 ; Chris explains the compiler writer POV which I find very valuable. ^1 *At least, that's what the wording "uses UB as an excuse" means to me.*
This is just perfect,thanks
&gt; The simple approach should be, i declare a socket object, initialize it using some ip and port, bind listening port and then setup is done There you go: #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;boost/asio.hpp&gt; using boost::asio::ip::tcp; int main() { boost::asio::io_service io; tcp::iostream s; tcp::endpoint p(tcp::v4(), 3333); tcp::acceptor{io, p}.accept(*s.rdbuf()); // s.socket() in newer version for(std::string line; getline(s, line); ) std::cout &lt;&lt; "received line: " &lt;&lt; line &lt;&lt; '\n'; } loop if you want it to listen again after the connection is closed.
Well, then what's the reason that most people use windows, as it is user friendly, and also has linux like advanced functionality to use command line, but a regular user will not use cmd, but will use gui for everything, similarly most of the asio features are intuitive for hotshots but not for beginners 
Thats why experienced programmers are more comfortable with command line, and beginners are comfortable with gui
This is true, but you're sort of asking all middleware developers to always be open source. That's not being very realistic.
Use beast if you want an asio web server.
&gt; Declaring a variable const enforces compile-time evaluation in Rust, yes, that's the whole point. That's good to know. &gt; I expect that this will be available in where clauses since this is where constraints are placed today for type parameters. It would also be good to have it in arbitrary places — like check some const variables even if they aren't a parameter. &gt;Oh, do you know if this means people solved the issue of having a constexpr std::vector&lt;int&gt; or is memory allocation only usable internally? I tried looking for it now and couldn't find anything which leads me to thinking I confused it with some other language. Possibly rust :)
Equivalent boost.asio is simpler: https://www.reddit.com/r/cpp/comments/7mflm5/most_suitable_c_networking_library/drtnsi8/
You make a fair point about NaN and Inf. It’s very weird!
The point of designing a standard library is for it to be actually used, not to introduce people to the language. If the library is built in a way that's useful for beginners, but everyone ends up ignoring it to use their own implementations when building actual production systems, then it has failed in its purpose.
&gt; one could almost write pretty much any program without ever worrying about memory management I disagree strongly with this. In the real world, you still need to think carefully about stuff like ownership even when using smart pointers and modern idioms. Doing stuff like graphs of nodes on a neural network, you can make a spaghetti of shared pointers pointing back and forth between each other than mean when the Graph object goes out of scope, it doesn't actually delete any of the nodes in the graph because they still have references to each other. Obviously, it's possible to avoid that situation, but the point is that you do need to think about ownership and lifetimes and memory nonsense. (And then if you need to interact with a GPU or use a useful library with older idioms, you pretty much have to deal with raw pointers no matter how clean your own code is.)
&gt; I think there's a misunderstanding here. I dont think there is. &gt; I meant that I didn't want to use Linux package managers to install the dependencies when all I wanted was use the headers/libraries of those dependencies for compiling my application. Is there a specific reason that you do not like to use the best tools for the job? &gt; Sure. But there's a big difference between installing the dependency on the box for everyone to use and using a copy of the headers/libraries locally and temporarily (though ideally cached) during the compilation process. Just because I want libcurl doesn't mean I want the box (or user) to get curl, for example. A number of packages require privileged access for installation, or may alter the environment, etc... I don't want any of THAT. Again, most package managers have options to install packages in /usr/local, or even your homedir if you'd like. Learn the tools before you dismiss them. Feels like you're arguing about this simply to argue. I wont be responding any further, you are full of crap, and it doesnt seem like you actually want to have a decent discussion, it seems like you just want to be a pain in the ass. Have a good one.
`void` `void` is a very strange type. Sometimes it can have a value, sometimes it cannot, and this really hurts in **generic programming**. It would be much easier if `void` was, instead, a regular type with a single value.
&gt; It would also be good to have it in arbitrary places — like check some const variables even if they aren't a parameter. Possibly, though lots of time this can be covered by creating a `#[test]`. Feedback is delayed a little, of course, but it's simple enough. Of course, it's quite possible that this will come later to the language. I've seen some attempts to write such checks using array sizes to produce an error (a size of `-1` is not valid).
But that is not c++, winsock(2) are C functions and if you're familiar with them it might seems easier. But once you learn c++ and are comfortable with lambda and type safety asio became gold. The only api I dread about Asio are the buffer ones. I believe we could do really better while retaining the same performance. 
&gt; That's not being very realistic. To be honest, I wonder what is the major impetus in NOT providing the sources. First of all, note that I am NOT talking about being fully open-source. A library which you need to buy or get a license for could only provide its sources to its clients. It could even provide a slightly different copy of the sources to each client (using watermarking) to identify leaks if it wishes. Also, I am not talking at all about allowing external contributions, or anything like that. Secondly, there is a real advantage for clients in having the sources: - you get to pick the compilation options, - you get a much nicer debugging experience, - you get the possibility to audit the code, - ... as a result, it seems to me that providing the sources could be sold as a competitive advantage, or even charged for. I assure you that having had my share of crashes/buggy behaviors in Oracle's client library, and having to delve into their obfuscated binaries to understand whether the fault was mine or theirs, I would have been willing to argue with my boss paying *extra* to get unobfuscated binaries with debug symbols. Thirdly, despite the best attempt of obfuscation and debug symbol stripping, programs still get reverse-engineered anyway. Even sometimes as black-box. This raises the bar, but if you have a genuine break-through in there and someone is unscrupulous enough, it will be done. I'll remind everyone here about how often video games are cracked *before* getting released. So, to be honest, I see very little benefit in NOT providing the source code with the software, and a lot of downsides for the client. Actually, I see one benefit for the vendor: it allows to throw in crap code. Yet another reason for clients to require open-ness.
Maybe i will use asio, but i need to do some thorough research and read books
It can be hosted at a company's location, but in that case it isn't free. It's free for those who use the implementation hosted by Ebenezer. 
What's the TLS support for asio, poco and other recommended libraries?
From memory: - you could iterate over all bases and all data-members of a type, - you could access both names and types, - you could get O(1) access to data-member by index and O(log N) access by name, - you could "visit" a type, with an interface relatively similar to a `static_visitor` from Boost, in two ways (just getting the name and type, or getting the name and pointer to value), - there was 0 run-time overhead when NOT using introspection. And all of this in C++03, which probably accounts for a lot of the ugliness. We also had, on top, a mini-ORM for sqlite and a serialization framework with multiple backends (json and an in-house binary protocol), which may actually have had most of the complexity. Notably because we wanted to be able to deserialize non-default constructible types.
Seems fairly reasonable!
You know, if we could generate the same binary object model for multiple languages, we could just squirt our data around in binary and not have to incur serialization/deserialization overhead on both sides. That would be pretty awesome.
How many layers down in Python would ASIO be? You could build a web service library on top of asio, and there are a number of projects that do so on github. It's just that there's not one in boost and there's not really a clear leader among those projects so far. [Served](https://github.com/datasift/served) has worked reasonably well for me so far, but I'm just prototyping something at the moment. I also had to do my json serializing/deserializing myself in my objects (using boost ptrees), which was a bit of a bother. It'd be nice if boost or someone would do request and response objects and a standard URI, so I don't have to re-implement those things every time I change libraries. In any event, you're still going too have to understand how the http protocol (or whatever you're working in) works to get the most out of whatever you're using, whether it's C++ or Python or Ruby. Any trivial demo that claims you don't is going to be entirely too trivial to actually be useful.
&gt; Of course, it's quite possible that this will come later to the language. I've seen some attempts to write such checks using array sizes to produce an error (a size of -1 is not valid). That's how it was done in C before it was a standard
Sure binary representation is way faster than textual representation, but it isn't a text format so it can't be viewed and edited using a text editor, so it's a solution for different kind of problems. And debating if the pros and cons of binary files vs text files is kinda off-topic.
The best WebSockets impl I know is Qt WebSockets. Every other impl is strong verbosity, hard to use.
Definitely a possibility. Honestly I think a lot of the problem stems from management more than anywhere else. At least the places I've worked, management wants you to be working on new things, new features, or bugs. They don't really want to give you the time to circle back and re-architect or fix something that was done poorly. So when they see "version y won't work with version x" then they tend to say "Well, don't go to version y then!". And that is where you see IBM arguing to keep freaking trigraphs in C++ in 2015. Because some poor soul won't be able to move to C++17 because their source was written with trigraphs in EBCDIC.
Yeah it was way harder to work around in 03, and just in general the language was way way worse. Not sure if this is really a fair example. All of the bullet points you mention are applicable to my 100 line framework except for bases (annoying to implement, not very useful), and access by name (could easily be changed by using the gcc/clang user-defined-string-literal-type extension). The framework just let any macro-fied struct return a tuple of pair&lt;const char*, T&amp;&gt;, which gives you almost everything you listed for free. Of course, tuple wasn't in 03. So in 11, I stand by my statement that it's pretty easy to work around.
had a quick look through the code -- here is what popped out for me: - get rid of boost dependency - get rid temporary std::strings - your parsing logic seem to be broken -- you can't rely on assumption that for any Iterator type `Iterator()` is the end of any range - `skip_whitespace` seem to be excessively complicated for something that simply checks for ' ' and '\t'. It will also crash if I pass a string that doesn't end with \0 - `escape_content` needs some love -- some of allocations can be avoided I like terse'ness and simplicity of the format, though... But implementation needs more work
That's ridiculous! Why do I need specify the URI times? Where's the abstraction? Where's C++'s Power?
&gt; without that context you would expect s = trim(s) to copy Well, again, I disagree. :) &gt; thought we were talking about developer expectations when looking at a piece of code. The original complaint of this whole topic: that `string_view` looks like a value, acts like a reference, and the language doesn't support lifetime extension for such types. :)
The extent of standard libraries. Java and .NET have been so successful in part due to the large number of standard libraries. C++ needs standard libraries like unicode conversion, HTTP libraries, XML parsing, serialization, thread pools, time zones, decimal/currency types, etc.... I am grateful for the expansion of the C++ standard libraries in C++11,14,17, but there needs to be so much more to make it a simpler process to put together large programs.
A lot of assumptions here... I've seen few examples that would not fit your pattern
I would say boost.asio. It isn't that easy to get into but it is very well documented and has a lot of flexibility on how you want to handle stuff.
&gt; Where's the abstraction? Where's C++'s Power? Because if you need it you can write it over this interface (see Beast). This is C++ power. You won't have to throw it away if you need yo use different/non-standard protocol.
I just wanted the Banana. The boost people want I to have the entire Banana Tree.
well, it's a TCP client demo. For web, boost has beast. 
&gt; It's just that there's not one in boost But there is one, boost beast is a thing,
a lot of people are arguing that python is easy because of `python -m SimpleHTTPServer`
realistically you only need to read the [examples](http://www.boost.org/doc/libs/1_66_0/doc/html/boost_asio/examples/cpp11_examples.html). There's a chat example for instance. 
Still verbosity. 10 lines of code for something that can be 2: request_manager rm; rm.get("http://www.google.com", [] (reply &amp;r) {})
Good
ASIO (the boost version of it, at least) has full TLS support.
What you see as a simple API, I see as unnecessary extra API surface. That API is well and nice for a simple example, but I would throw it out the door as soon as I'm trying to do anything non-trivial. So at the end of the day, the time I spent learning the simple API will have been wasted because I will have ended up learning the "real" full API in the end anyways,
I wonder if tools to migrate source code automatically could work. For example, it seems easy enough to port EBCDIC to ASCII, replacing tri-graphs by the actual characters as you go. I seem to remember Go chose this approach and proposed a gofix tool for this; in C++ we already have the clang-modernize tool for example. Of course, this will limit deprecation to only "small" features, but if you have most of it automatically migrated, then it's easier to argue for 1 or 2 manual ones.
Try adding a `.gitattributes` file with: *.[ch] diff=cpp *.[ch]pp diff=cpp 
No, I'm saying to them provide 2 version a simple and a complex. There are people that just a Cross platform solution to do just a http request or something like this. Look at Qt Network, I never saw someone crying to have only a complex version of that API.
&gt; ... It will also crash if I pass a string that doesn't end with \0 That.. isn't a string if it doesn't end with \0, that's just an array at that point. At least, as far as I've seen anywhere in C++ that handles strings they *must* have a null terminator or they aren't strings.
It's because 0 is an int not a double, right? I wouldn't have caught that unless you said there was something wrong with it. 
Thanks, hope you enjoy it!
Yep! My clue to recognize what realizing that all our totals had no fractional parts. You should have seen the eyes my junior made when I pointed out the issue :x 
Its worth noting that std::strings are not null terminated (or at least are not required to be). You need to call std::string::c_str() to get a null terminated char* I haven't checked OPs code to see if this distinction matters
Using a newline as the only delimiter seems iffy; an array of integers is going to become ugly very quickly. I suppose you would argue that you should use a recursive format, like CSV-in-Deco, but at that point you're rolling an ad-hoc format which is kind of a shame.
You'rr talking about C strings, not C++ strings.
I looked at it. It's too low-level. I'd spend most of my development time implementing a server on top of it. There's a cost incurred in bringing in a new dependency, but it's lower than it would be to build a solution with boost beast.
Thank you! It looks great. I think that sometimes it's important to know exactly what is your direction and your advice would help. Maintaining the code is important and it's achievable. It can happen by using programs that helps, such as checkmarx and it's also happen by planning the code the best way you can. Good luck!
Your parse function takes a (begin, end) range. There is nothing in it about nul-termination
I think that one of them is high complexity which leads to a lot of bugs and errors among the code that are hard to be detected later and it can become a problem. There are programs that helps with it, such as checkmarx, but working on catching those vulnerable places is very important in order to maintain your code working.
since C++11 they are nul-terminated. Otherwise you can't have `noexcept` on `c_str()`
actually using CSV with integers is fine since integer content won't have delimiter collision with commas. If you have such additional information you could specialize. Deco's scope is only the clean solution for delimiter collision, it doesn't dictate how types should be serialized. It's possible to standardize type serialization on top of Deco, and that's a good idea to make sure different implementations "speak the same language".
&gt; The core ideas of Rust (ownership/borrowing) are such a tremendous break from current C++ code that they cannot be integrated wholesale without throwing backward-compatibility out the window. Compiler switches. Someone on the C++ twittersphere already said there's work going on to make the compiler check for UB and other things. This alone should make programs orders of magnitude safer.
Thanks for the feedback! - The boost dependency is the result of benchmarking several formatting libraries - Boost.Spirit turned out to be the fastest overall. IIRC C++17 suppose to introduce `std::to_chars` and `std::from_chars` and IIRC I benchmarked them and they're really fast but don't have widespread compiler support, so can't be reliably used yet. - Temporary std::strings - To be a bit more organized, maybe open an issue on GitHub detailing which temporaries can be avoided? - `skip_whitespace`'s Lookup Table implementation is faster than the naive approach. It doesn't expect strings to end with `\0`, only with a non-whitespace character. - `escape_content` - detailed GitHub issue too? Yeah the library is newly developed so there's room for improvement. The implementation is complicated with template metaprogramming but that's for the sake of making it as simple as possible for the user without runtime cost.
&gt; To be a bit more organized, maybe open an issue I am not THAT bored... lol &gt; which temporaries can be avoided? Pretty sure you can avoid (almost?) all allocations if you redesign your public API in terms of writing to (reading from) user-provided "streams". Not sure if C++ gives you `to_string()` overloads with similar logic. &gt; The implementation is complicated with template metaprogramming Yep, it certainly would've given Linus a heart attack :D
This is a sensible argument for *leaf* types, like integers or dates, but the whole point of a format like Deco is to give you a target for semantically encoding structure. If you have an array of T you *really want* to encode this to a Deco set where each T is Deco encoded. If you didn't, you'll end up with silly things like a byte string holding comma separated Deco encoded strings, or other such nonsense. 
For most stuff I provide both `std::string` and `std::string_view` overloads. Usually the ones that allocate `std::string` are used for serializing into a `std::string` anyway so there shouldn't be an overhead. I did keep in mind avoiding allocations when writing the library, so most `std::string`s are there because they're needed. TMP isn't for the faint hearted ;). No, I really mean it, debugging can be a nightmare.
You're right. BTW In the library implementation I provides some ways to specialize such cases for more concise formatting. For example see: https://github.com/Enhex/Deco/blob/master/documentation.md#decois_single_entry For example in the case of serializing things like integers into CSV, a more specialized type trait than `deco::is_single_entry` can be used that also means the type's content doesn't contain commas, and have a CSV container type that automatically will check at compile time if the type got that trait and choose how to format the elements accordingly.
a good alternative would be exposing internals of the object model with 2 paradigms: one for instantiated definitions and one for templated/generic definitions, Then being able to manipulate these definitions in a manner analogous to plain old code. this one is a little bit far fetched and hacky, but even exposing type identifiers as a part of the object model in a way that would deprecate preprocessor macros by allowing you to manipulate said idenitifiers.
I think that is a great idea. It would be interesting if you could do something like babel for javascript but in reverse. That is, take old C++ and turn it into new C++ using a transpiler or something similar. It might be interesting to see if you could do something like auto replace news/deletes with unique_ptr when the analysis determines that is safe.
But can you use the distributed build feature of FASTBuild without changing the underlying build system of the game? Otherwise, you would have to consider Incredibuild or SN-DBS (if a PS4 dev) for distributed builds.
SDL net is pretty minimal and one of their demos is a chat application.
Show me a C++ string that isn't null terminated because the only one(s) that I know of are null terminated.
What makes PCHs and unity builds dirty hacks? They produce decent results.
The progress being made on RESTinio is very encouraging. Keep up the great work! 
 const auto id = restinio::cast_as&lt; unsigned int &gt;(params["user_id"]); it should be like this: const unsigned int id = params["user_id"];
having to use the prefix std:: everywhere is annoying and makes the code less legible, but there is no real alternative. “using namespace std” would import unwanted symbols into the namespace, especially in headers/template function definitions. it would be good to have a single character prefix that stands for “std::”, like “$sort($begin(arr), $end(arr), cmp)”. the naming convention used by the standard library is to have both functions and class names lowercase, but then you cannot have house::door() if there is a class named “door” there is no way to templatize a member function for const-ness of “this”, so often it is necessary to write two variants there is no easy way to define strong typedefs for integer types, for example the std::max, std::min functions are hard to use in practice because the arguments have to be the same type for it to compile the iostreams library can be annoying because its syntax is more verbose than printf, is not designed to work with binary streams, and fstream cannot be constructed from a std::FILE* 
So the `:` character is used to separate key-value pair, and it's used to escape the list of values of a key?
`:` is the structure delimiter which isused to start and end a group of entries. The single line name value pair isn't part of the Deco spec, it's just a concise short way to write NVPs and it can have delimiter collisions. (for more details: https://github.com/Enhex/Deco/blob/master/documentation.md#name-value-pair)
Networking != web server Those 2 things are galaxies away. Like everyone in this thread said: if you want networking use asio. If you want web server use a library that uses asio that provides that (beast for example).
If you're using libstdc++, it's slow because it uses a hash table instead of an array. If you look in the `functional` header, the searcher is chosen like so: conditional_t&lt;sizeof(_Val) == 1 &amp;&amp; is_integral&lt;_Val&gt;::value &amp;&amp; __is_std_equal_to&lt;_Pred&gt;::value, __boyer_moore_array_base&lt;_Diff, 256, _Pred&gt;, __boyer_moore_map_base&lt;_Val, _Diff, _Hash, _Pred&gt;&gt;; Here, `_Val` is `std::byte`, which has a size of 1, but is not integral. This causes it to fail the condition, causing the searcher to use the map base. If you use `unsigned char` instead `std::byte`, it becomes actually faster than `memmem`: memmem : 30 ms std::default_searcher : 537 ms std::boyer_moore_searcher : 23 ms std::boyer_moore_horspool_searcher : 23 ms 
glibc memmem use some hybrid Two-Way and Boyer-Moore algorithm, and is highly optimized. std::searcher should use memmem if possible. You may check the actual code path being taken. As c++17 features, they are still frequently patched.
That's very interesting. Thanks for the info! I didn't even think that `std::byte` might cause issues. I just switched to using it too.
Should a bug report be filed for this? 
I have limited experience working with large files, so I'm curious as to what I'm misunderstanding about your situation: Why go through the trouble of loading the file in 8Mb increments, and having to deal with edge overlaps when you could simply mmap() the file and call it a day?
That's a great question. In my particular case, I have an abstract `File` type that could represent a file on disk, some byte array in memory, a nested file in some container format, or some other source that supports random access. I needed something that could work for all of those cases.
Ok, yeah. That'll definitely do it.
I would say that it is not overengineered. It is just low-level. But it has all the pieces that can be put together for doing things such as cpp-netlib or Beast. I think it is the kind of stuff that should go into the standard at first. Other layers can be added later on top of it.
If you want a high-level library, use something else. This is the equivalent of algorithms over iterators for networking. And I agree it should be the first step. If you want something that hides all the complexity, it should be built on top of this.
how would you control in your API allocation? When we have stackless and stackful coroutines, how would you add them into a high-level API? You would be just locked down if you choose a high level wrapper. Even if I understand that people want higher level components, the standard networking that is being standardized is a low-level, composable and extensible API. People complain because they cannot do http::get("www.something.com/here/you/haveit"). It is just absurd. This is a foundation library, not a high level API, which has its place too, I can agree on that.
As a person who has written a client and a server with boost.asio + coroutines I can say that the library is in fact low level (you do reads and writes one by one) but with coroutines the code looks sequential. It is not that complicated, just a bit low-level. I migrated from callback-based asio: that one was indeed a hell to code :). BTW, shameless self-promotion here, I wrote this some time ago an API on top of asio for simple requests but I did not check it for a while. Basically it is one single API call: https://github.com/germandiagogomez/simple-requests-library
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/motivelang] [What users want](https://www.reddit.com/r/motivelang/comments/7mko2t/what_users_want/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I live in Qt land and i have a simple [NetworkAccessManager](https://github.com/mhogomchungu/NetworkAccessManager) header only library for simple use cases of the network.
Suggestion: the file class could expose an iterator that gives a pointer and size to read from. Local files just give a single iterator with the whole size. Network resources could block until more is available
Is it fair too say that it is delimeter collision free with the limitation that newlines can't be directly represented? In the same way any other file format that uses escaping for delimeters is delimeter collision free as long as you don't use the delimeter character(s) in the content. There are 2 ways to absolutely prevent delimeter collisions without reducing the charset: 1. bencode or pascal string style (i.e. length+content), basically sacrificing at least partially the human readability. 2. C++ raw string or MIME multipart style, basically allowing the user to define their own arbritrary length delimeter.
I think there's a reason why C++ isn't popular for web servers. It's a real pain in the ass.
I'd like to see a comparison with [YAML](http://yaml.org) since that's what Deco looks like it is competing with.
I had no idea about this feature :) Thank you. For anyone else curious, read up here: https://git-scm.com/book/en/v2/Customizing-Git-Git-Attributes and https://www.git-scm.com/docs/gitattributes git has a scary amount of features
curious, for what reason would you want to sometimes page into a buffer little by little, and other times mmap the whole size, regardless of where it is from? I don't deal with this low-level stuff too often, so I'm curious why the situation differs depending on the source of data (besides something obvious like read speeds).
Nice find, so this particular implementation is suboptimal, at least in this case. However, I’m curious to know how much time is spent in creating the searcher object and how much doing the actual search. Although re-using searchers is not the main scenario here, the STL approach allows that, while monolithic functions like `memmem` don’t. 
What about values that contain (or end with) a `:`?
For a search set of any non negligible case, tone of construction and destruction is negligible. `memmem()` would have to do something similar but without RAII anyways.
Of course, I expect the cost of the actual search to dominate the cost of the searcher setup, but I won’t say it is negligible without actual measures.
Why is this so much faster than std::function? I suspect some constraints on std::function cause this, but which? 
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7mflm5/most_suitable_c_networking_library/druvv6k/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I’m not able to scroll the tiny code box on my iPad. 
Strongly-typed paths sound interesting. I wonder if that was considered and if so, why the idea was rejected. 
thanks for linking
The trouble with Boost is its massiveness. I've used Boost successfully for many projects in the past, but they were all enterprise-level things. I wanted to use it for a much smaller project, but it was so huge that I never even bothered to formally present it to my teammates after we played with it - we'd need extra build stages, and the Boost download was around one hundred times all our other source and dependencies put together. There's a way to extract a subset of Boost, but that still resulted in a huge download, and now, one we had to maintain ourselves. More, adding even the one Boost header in increased our compilation times by ~~40%. We would have gotten a lot out of it, but suddenly our application would have changed from "fast download, build and install" to "sludgy mass" - particularly on Raspberry Pi, which was a big target for us. For me, the ultimate C++ library is [Catch](https://github.com/catchorg/Catch2) because I can drop a single .h file in my project and I am done. The more dependencies a library has, the more stuff I have to do in my build, the slower it makes my compilation, the more reluctant I am to include it. 
Lots of comments, small and big. Let's just start with a few formal things, probably less important. You are targeting C++17, so don't use include guards, particularly since you have inconsistent naming for them - just use `#pragma once`. Your files are also inconsistently named - not a big deal, but it grates. The `types/` directory is daunting - a large quantity of tiny little undocumented files doing "container things". Very unclear. If your primary target is casual readers on Github, I suggest using spaces instead of tabs. By default, Github uses eight spaces for tabs, so for your average reader, a lot of your code is scrolled off the right side. You also don't need to indent for namespaces - they aren't a scope, and indenting namespaces doesn't add to readability if all the code in any file is always in a namespace, which is true for you. That's more "to taste", and I'm getting into the small stuff. Now, let's take a look at some of the higher level stuff. First question is - does the world need yet another format? Probably not :-D but we can't really know without trying. It's hard to get traction here - Yaml, a direct competitor, has the advantage that it's backward compatible with JSON. Anyway, we learn by trying, so let's go to the next question - does it do what it say? Does it really avoid the delimiter collision problem? Well, it can't, can it? If there's a delimiter, then if that appears in my target text, there will be a collision - but there has to be some sort of delimited _unless_ you include a count of the number of characters, which is horrible! Look at the meaning of `:` and `\n` in the code. There are some combination of those in a name or piece of text that will cause you to misunderstand it. So it doesn't deliver on this promise, as that is impossible, so that needs to be resolved. The next is that I spent some time reading this, and I had trouble figuring out how I would actually use it. Indeed, my very first question is this - do I get back called back periodically when the parser recognizes a new text element, or does the parser build some big structure that it then passed back to me? Do remember in the real world that these files might be big - might be very very big, gigabytes - and often, you are only interested in extracting information out of the file, not keeping it all. The callback mechanism might be a lot better if you are interested in high-performance code. If I were doing this, I'd do "both" - I'd have it based on callbacks for potentially high-performance but I'd then have some helper function that called inserters on the user's containers to return a large container with everything in it. I'm running out of time here, but let me address the "temporary strings" issue someone else brought up. There isn't one place that you can fix this: it's possible to do this sort of parse _without ever creating any temporary strings at all_ (or perhaps "very few" for some edge cases) - but you can't just bolt this on piece-at-a-time, you have to design for this from the start. The idea is to use [`std::string_view`](http://en.cppreference.com/w/cpp/header/string_view) or some similar reference class. `std::string_view` doesn't create new strings - it just returns a range within an existing one. If you did this, you would never return new strings, but simply these ranges, and there would be one large fixed string that contained the original input. Properly done, this sort of thing that increase throughput dramatically. This is particularly true if your parser is a callback-style parser and not one that creates a structure and returns it.
What is wrong with u/blelbach, this is genuine question and please dont put away users by behaving like SO guys. Please, delete the request 
I have not asked for help with coding or homework,carrer advice or book/tutorial/blog suggestions, my topic is the best network library, it is a discussion of C++ language library, please clearly read the question before taking such steep steps
Copy paste compatibility with C code. The culture among some C++ devs, mostly C expats, to avoid any benefits of writing safe code and basically use C++ compilers as if they were C compilers. The same culture that has lead C++ to loose to Java and C# due to lack of adoption of good frameworks like OWL, VCL and Qt, because POSIX is good enough. Thankfully many on the ANSI C++ working groups see it differently and we are finally getting the nice libs that should have been there since C++98.
Yea. Have `operator[]` return a proxy object that allows casting in `operator=`.
On the one hand, I like the idea of a deleting overload for safety, on the other I am annoyed by it. Imagine that I have a function `void bar(std::string_view)`, I'd really like to be able to do: bar(trim(foo())); but with the deleting overload I cannot even though it's safe :(
Actually, they could be lazily NUL-terminated so long as an extra character was reserved (but possibly uninitialized). I doubt any does that...
Stuff like this is so... idk, disappointing? I mean, it's amazing that standard library implementations can and do have optimized overloads for types that fulfill certain requirements, but there's no way to know which overload is selected (or even that optimized overloads even exist) without reading the source of the standard library, and sometimes it's hard to know *why* it was selected, even after reading the source. I suppose concepts would help a little, but you still wouldn't be able to explicitly say you want a certain overload, since it's still an implementation detail.
&gt; Jackass. If you're going to break a bunch of code with an optimization, figure out how to make a good warning. No, jackass would be resorting to name-calling when we happen to disagree :-P &gt; `refresh(&amp;frameCounts[i]);` breaks the function, but `refresh(frameCounts+i);` would be fine. Once again, no :) The error appears only because `frameCounts[i]` produces a reference that can actually be null, while a reference is a data type that _cannot be null_ (UB). Also, `refresh(frameCounts+i);` would require an `operator+(const refarray&amp; x, int y)` which doesn't make sense for the `refarray` class.
`std::function` is value-like, whereas this `delegate` is pointer-like: it does not take ownership of the object, just a pointer to it. As a result, special methods (copy, move, destruct) are easy for `delegate` (pointer copy) whereas in `std::function` they can be arbitrarily complex (invoke user-defined copy/move/destruct logic). This in turn means that a different implementation logic is used: - `std::function` needs *multiple* customized functions, so uses a virtual table, - `delegate` needs a *single* customized function (the call), so uses a function pointer. Thus, `std::function` has some overhead in fetching the function pointer from the virtual table (data dependency). Also, whereas `delegate` always has a payload of 2 pointers (data pointer and function pointer), `std::function` has a variable payload. The simplest solution to handle a variable payload is to allocate on the heap, however this is wasteful in the case of a single pointer worth of data (a function pointer). As a result, `std::function` implementations will generally have a way to "cheat" for small payloads, and put them inline. This, in turns, requires at call time to check whether the data is heap-allocated or inline; this may induce some overhead though I would expect the branch predictor to completely hide this issue in micro-benchmarks (since the same branch is always taken for a given instance).
Lesson: When launching rockets - use safe numerics library http://blincubator.com/bi_library/safe-numerics/?gform_post_id=426 
std::function can be made as fast as this. But it will require a interface like std::variant to retain the type of callable object.
These are just styling choices I rather not get into it since it's quite derailing into irrelevant endless arguments. It isn't yet another format - it solves problems that other formats don't. YAML doesn't solve delimiter collision(ex: you need to escape a single quote inside a single quoted string), it just mitigates it. Also YAML is very slow (because it's super complicated), about ~2,000% slower than JSON, which roughly means ~40,000% slower than Deco. Deco does really avoid the delimiter collision problem. Read the [specification](https://github.com/Enhex/Deco/blob/master/delimiter%20collision%20free%20format.txt) for an explanation how. If you're not convinced I challenge you to find a case with delimiter collision ;) There's [documentation](https://github.com/Enhex/Deco/blob/master/documentation.md) and [examples](https://github.com/Enhex/Deco/tree/master/tests) for how to use the Deco library. In my implementation there's no intermediate representation of the Deco document, entries are directly parsed into the serialized variables. I assume this is qualifies to what you mean by callback based. The library is new, didn't try to tackle big files yet. Currently it parses one entry at a time from a stream, so it's possible to just read some of the file, parse it, then read some more, and so on. If you can give me more specific use cases I can provide some API for them. Deco parsing doesn't create temporary strings, it returns `std::string_view` to each entry's content (i.e. after skipping whitespace and removing delimiters). `std::string` is only used when it actually needs to create a new string. I hope I clarified some of your questions!
std::string::c_str is also const, so it has no business lazily changing anything. 
I'm using Conan package manager so having more dependencies doesn't come with the cost of setting them up, it's all handled automatically. I'm using Boost.Spirit which is header only, but the Conan recipe I'm using for it is a bit primitive so it has to download and build all of Boost. I might want to switch to [Bincrafters](https://github.com/bincrafters)' Boost.Spirit Conan packaging since it's more modular and will probably speed up the installation time by a lot.
Boost.Asio seems over-engineered and over-complicated, you can see this in code examples, even for simple use cases. Any simpler alternatives?
&gt; Running the model several times will give different results, sometimes really different like 8000€ vs. 17000€. Maybe he means "training the model several times". But once it's trained, given a model, using this model for inference should always result in exactly the same output for the same given input. Otherwise there's something very fishy going on.
I don't see the contradiction; `const` is a logical statement, and in the case of the `std` implies some multi-threaded guarantees, but that's about it.
You need to delimit the entry's content end using content end delimiter (`'`). It's explained near the end of the [set entry documentation](https://github.com/Enhex/Deco/blob/master/deco_tutorial.md#set-entry).
What would you like to see compared? Few things I'm certain of is that Deco is much faster, much simpler, and doesn't have delimiter collisions (YAML does).
I should note that Andrej is an essential contributor to the the safe numerics library. Without his input, it would never have been built. It's been accepted into Boost, but I'm still working on the changes required by the review.
So it's not delimiter collision free as advertised.
You can represent newline, you just don't need to use hacks to do it. You're welcome to run the [multiline_string](https://github.com/Enhex/Deco/blob/master/tests/multiline_string.cpp) test and see for yourself. 1. Writing an entry's length isn't practical by humans. 2. That requires the human to do extra work and validate the content doesn't contain the delimiter he chooses.
&gt;skip_whitespace's Jump Table implementation is faster than the naive approach. I rather doubt that a memory-intensive solution like this is faster than simply: if (char=' '||char='\t') It introduces more cache pressure and thus possibly additional memory accesses, which are way more expensive than the if. I'm also worried about the way you derefence the table. If `*iterator` is signed, for example because someone calls this with some `const char *`s as iterators (which seems like an obvious thing to do, really) you're headed for UB territory if you encounter a non-ASCII character. 
How are the content begin/end delimiters differ from string sorrounded by `"`? How come this is not delimiter collision: "a string:" While this is a delimiter collision: 'a string:' ? Which delimiters are colliding?
If you allow escaping and/or quoting, then every format is collision free. So when you claim that your format is "collision free" I read "collision free without escaping/quoting". It seems that you need some form of quoting to avoid some collisions, so it's not "collision free" according to me.
the difference is, taking common usage of `"` quoting, this string contains delimiter collision: "delimiter " collision" You'll need to escape the middle `"` to deal with the collision like so: "delimiter \" collision" While this Deco string doesn't contain delimiter collision: 'delimiter ' collision' You don't need to escape the middle `'` because it doesn't collide.
I see. That's clever. But you are forgetting that : is the delimiter here, not the quote.
I benchmarked Jump Table vs naive and Jump Table was significantly faster (overall benchmark performance improved), at least on my machine. Could be related to things like branch prediction. And I think the table is likely to stay in cache all the time since it's frequently accessed. That could be a real type mismatch, perhaps the jump table should be templated to use the character type the input stream uses.
You're right. From the Deco specification standpoint an entry ends with a "delimiter sequence": https://github.com/Enhex/Deco/blob/master/delimiter%20collision%20free%20format.txt#L53 That is, the behavior is specific to the sequence, delimiters don't have independent behavior from each other. If you don't view the entry end as a sequence you may consider it escaping individual delimiters to avoid collision.
Well, std::filesystem is very closed based on boost::filesystem, so maybe in the boost docs there's justification. One issue is that there are going to be many functions that just want to take a path, and don't care if it's relative or absolute. So now you either have to template everything, or you have to create a base class called `path` with derived classes `absolute_path` and `relative_path`. It's a lot of complexity for IMHO dubious benefit, especially when there is already a well worn understanding of what it means to append an absolute path to something.
So what would you say: UB or not? std::string foo ("hello, world"); char *fooptr = &amp;foo [0]; for (int x=0; fooptr [x]; x++) ...do something with fooptr [x]... If we accept that foo is only null-terminated after calling c_str(), data(), or operator[foo.size()] (the only ones I'm aware of that mention the null-terminator), this is UB. On the other hand, if we accept the theory that there is really no conforming implementation possible without the null-terminator already being there, it is perfectly fine. 
That's fair. What do you think of the overload that returns an actual string? I'm pretty sure it would work for your example. For reasons it's hard for me to articulate, it makes me a bit nervous to have rvalue/lvalue overloads return totally different types, but that could be misplaced fear.
The problem with that is that a Boyer-Moore search requires a random iterator, and figuring out when we are done with a chunk of memory in the file interface wold be needlessly complicated. Treating all files like streams and handling the paging by hand at consumption makes a lot more sense here.
Can't speak for OP, but for me. sometimes, the thread that reads the data cannot afford to wait on a page fault. The simplest example would be when sending render resources to a GPU. It's better to load the data in memory in a thread, and then hand it off the the rendering thread. 
I'm not convinced having a base class is overly complex but I can see where it makes sense to follow existing convention if doing so allows for a simpler design.
Thank you for feedback. Yes, it less verbose. Will consider this for the next version.
If you can remove the rudeness in your comment I can reapprove it.
Well, not just code complexity, but cost. All those functions now are doing virtual dispatch. You might end up with a branch in `filesystem::path` as it stands now in some case, but not in many others. Also, polymorphic objects take up extra space due to the vtable, etc. Maybe seems irrelevant for 99% of users since filesystem operations are slow anyhow, but standard library code has to be fairly uptight in this regard.
&gt;In conclusion, at the time of this writing, you need to inspect the generated code of your compiler, if you want to be sure that something is really calculated at compile time. I don't get this. Why? Doesn't making a variable constexpr ensure it is a compile time value, or it will fail to compile?
For the use case of a single streaming read through a file, mmap is quite a bit slower than classic streaming I/O.
You’re absolutely right. I hope an expert can jump in and better explain it, but there are serous caveats to its use and the author appears oblivious (there is no “caveats” section) For certain it is not a drop in replacement for std::function: 1. Does not manage the lifetime of the stored callable. 2. Mutable lambdas are only ever stored by reference, so even if you deal with the lifetime issue the code will behave differently as if you had used a std::function. My examples suck cause I’m on mobile and I forget how to format code. 😐 delegate&lt;int ()&gt; d1; std::function&lt;int ()&gt; f1; { auto lambda = [i=0]() mutable { return ++i; }; d1 = lambda; //wrong syntax f1 = lambda; //makes a copy of i auto d2 = d1; //still references same i as original lambda auto f2 = f1; //copies i again lambda();//1 f1(); //1 f2();//1 d1(); //2 d2();//3 lambda();//4 } f1();//ok, returns 2 d1();//crap, UB (pointer to lambda that was destroyed as it went out of scope)
Oh, crap. Did I offend somebody? That was very inconsiderate of me. So easy to offend someone these days... Such a convenient tool -- just have someone take an offense and you have grounds to remove it. Nah, I don't give a crap about fate of my comments on this shitty website. Have at it. 
I also think this is a good question. I would be very hesitant in relying on a feature as fragile as this seems.
Beats me. If a constexpr isn’t enough to make an expression a compile time constant then there is something wrong with the compiler.
I still don’t understand why the result of training would be different given the same input data without some kind of probabilistic inference going on?
nice try :)
&gt; If you compile the above code with GCC 7.2 or Clang 5, the compiler will replace the call to factorial(5) with 120 at compile time This is just common function inlining + constant folding, you'll get the same result with a normal (non-constexpr) function.
The author is wrong. They came to that conclusion when dealing with the recursive Fibonacci impl. But they did not assign to a constexpr variable. 1) no guarantees: std::cout &lt;&lt; Fibonacci (20); 2) guaranteed: int i= Fibonacci (20); std::cout &lt;&lt; i;
See my reply, it is not nearly as fragile as it was made out to be. If you want compile time computation, then use it in a context where compile time evaluation is required.
Essentially if the value isn't used in a constant expression, it might not be a compile time value: https://stackoverflow.com/questions/14248235/when-does-a-constexpr-function-get-evaluated-at-compile-time/14248310#14248310 And [cppreferences](http://en.cppreference.com/w/cpp/language/constexpr) first line (emphasis mine): "The constexpr specifier declares that it is _possible_ to evaluate the value of the function or variable at compile time. " 
Yes. This needs to be standard practice. Just as the systems engineer uses automotive grade ICs for cars, the software engineer should use libraries that provide similar safeguards when writing code for for such environments. 
English is expressive enough to let you devise function names where it’s obvious as to whether it’s a function that modified in place vs. one that returns a new value. That’s my experience at least and thus far. 
I'm already not a huge fan of having both `const` and `constexpr` and it's pretty silly how a `constexpr` function doesn't guarantee to be a compile time constant every time. Are we going to need PHP level `real_constexpr` functions next? But the C++ commite is afraid of new keywords so we might end up with `co_real_constexpr` but at least we would have GUARANTEED compile time elavuation.
I would rather not be using a function called trim_whitespace_from_both_sides_and_then_return_a_string_view_object too much typing when I can instead just use a pointer to express intent.
Painting homosexuality as a negative trait, coupled with this toys-out-of-the-pram response, is very telling. There's no place for you on this, actually, excellent website. 
A few more than that
I still wonder how `constexpr` as a keyword made it through the C++ committee.
Yeah my thinking is for filesystem operations a virtual dispatch has a negligible impact on performance, but that's a fair point.
What would you use instead?
Something else entirely, don't you think having both `constexpr` and `const` is at least a bit ambiguous?
"A `constexpr` specifier used in an object declaration implies `const`." For me, the name is OK. Give me a name that would better inform about behaviour of constant expression.
I would also stick to `lexical_cast` as the one from Boost if it has the same functionality.
Almost all Linux distribution package managers install the packages into /usr/blah/blah/blah. I.e., they install system-wide. When you build something yourself, you can generally install it wherever you want, on a project-basis, e.g. /home/me/proj/foo/3rdparty/thingy.
hopefully it continues though, we do space systems at work and a lot of the newer `constexpr` and TMP features in C++14/17 *really* benefit our particular kind of work by making it safer, more robust, and offloading stuff to compile-time
&gt; For reasons it's hard for me to articulate, it makes me a bit nervous to have rvalue/lvalue overloads return totally different types, but that could be misplaced fear. I've had several instances in the past few months of `std::unordered_map&lt;std::string, ...&gt;::get(key)` creating a `std::string` from thin air when passed a `const char*` in a hot loop. It has not made me keen on APIs which spring up memory allocation on their unsuspecting users. So, in this case, I would just go ahead with `std::string_view trim(std::string_view)`. There are risks, yes. There are also ways to spot the issue (ASan/Valgrind) for tested paths.
No problem. From the [C++17 March Draft](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4659.pdf): &gt; **24.3.2.5** `basic_string` element access [string.access] &gt; const_reference operator[](size_type pos) const; &gt; reference operator[](size_type pos); &gt; 1. Requires: `pos &lt;= size()`. &gt; 2. Returns: `*(begin() + pos)` if `pos &lt; size()`. Otherwise, returns a reference to an object of type `charT` with value `charT()`, where modifying the object to any value other than `charT()` leads to undefined behavior. &gt; 3. Throws: Nothing. &gt; 4. Complexity: Constant time. Note how careful the standard is to ensure that you get a reference to a NUL character without actually guaranteeing that it's *contiguously* allocated :)
The standard offers the following rather bleak statement in section 5.20: _"Constant expressions can be evaluated during translation"_ So constexpr is really just another hinting keyword, like inline, and like inline it means something completely different from what most programmers would intuitively believe (it creates a literal value, but it says nothing about compile time evaluation other than a general hint that it is not forbidden). I'd be curious to learn why a stronger phrasing wasn't chosen, at least wrt. constexpr variables (constexpr functions I understand, given that they may also be called at runtime). Also, I'm totally looking forward to the arguments raging twenty years from now, when "delaying computation until runtime" will be seen as an amazing optimisation opportunity, and that we should "be educated" and "fix our programs" if we somehow relied on constexpr being evaluated at compile time... No, I'm not bitter at all about the UB discussion ;-) 
Homosexuality is a negative trait, a disorder, regardless how much your masters try to paint it as normal. You can go back and pat yourself the back, your virtue signalling is probably noticed. Or not... It is hard to be noticed when there is so much competition. Whether there is a place for me on this website or not -- is not for you to decide, poser.
No, but when training the network, all weights are usually initialised randomly (according to a specific random distribution - Gauss, Xavier-initialisation, ...). Now of course it depends on whether their seed is random or not... Not sure how the RNGs in TF are seeded. Another thing is that you usually shuffle training data indices, so the batches will be different in each run, leading to different results. Of course this all depends on how the RNGs are seeded... if they're all seeded with a static value, then, you might be right and it should learn the exact same model, unless I'm overlooking anything.
As far as I can tell it isn't _required_ anywhere. The only requirement is that the resulting expression must be considered a literal type by the compiler. However, that is also an abstract notion: it doesn't mean there is a fixed, stored pattern of bits somewhere in memory representing the value, it only means that the value can be used in certain places in the language (array sizes, template parameters, case statements, etc.). There is nothing in there that disallows, for example, calculating all cases of a switch statement at runtime, every time before the switch is executed. It's an interesting situation: intuitively we all believe constexpr to mean one thing, but the standard really defines it to be something completely different, and leaves it to the compiler to come up with an efficient solution. 
I am crying a little inside :(
What is the difference between translation and execution, if the C++ implementation is an interactive interpreter (e.g. [cling](https://github.com/vgvassilev/cling))? Since a constant expression can be used in a template argument, a compiler must effectively evaluate it at compile-time, unless the compiler wants to generate functions at run-time.
 ``fuckyea`` would work for me.
Yeah, I really dislike this part of the feature. It just doesn't make any sense to me at all. There should be a way to force a constant expression to resolve.
Well, at least Rust got it right... :(
I didn't realize they had a constexpr equivalent. What are the limitations? Did they get variadic templates yet? 
&gt; I didn't realize they had a constexpr equivalent. What are the limitations? https://github.com/rust-lang/rust/issues/44580 --- &gt; Did they get variadic templates yet? Nope.
No, just the opposite! The problem was not the overflow. The problem was that the overflow was being checked, and triggered an abort. An unchecked overflow had not caused any harm, since the value was not actually being used for anything anymore. Also, it wasn't an overflow but a range check that failed, not that it really matters for the discussion at hand. 
There's also escape characters, as a way to avoid collisions. Like how strings are encoded in C++...
Thanks, random initial weights makes sense. Still worrying that it can train such different models on such a simple dataset, but I am not an expert in NN.
Just making sure that the kind of person you are is advertised a little more publicly. Thanks for playing. 
Thanks for the insight. It sounds to me that an implementation would have to go out of its way to cause a problem. As a comparison this is not quite equivalent to the quality of implementation issues sometimes discussed on this sub. I mean, it is, but your example is taken a bit to the extreme, no? 
As the name from boost `lexical_cast` has a certain semantics, in particular its not only a string2type but also a type2string. `restinio::cast_to` is limited to a fixed source type (string_view) and so it has a different name for not to be associated with lexical_cast.
For example, what if someone uses `\r\n` instead on `\n`?
makes sense
But isn't it now a "structure collision": there is no way to distinguish between a set of strings and a multiline string?
I'm not sure what you mean. Are you asking where the name came from? It's like "the next value of C" or "the thing that comes after C" ... so it's the postfix increment operator applied to C. Are you saying: "What are the guiding principles of C++?" I'm sure that's debatable, but I suspect it will have something to do with not paying for what you don't use and maybe higher-level abstraction without additional runtime cost?
That depends on the type you're serializing. Deco itself doesn't standardize ways to serialize specific types. I argue that your program must know the type its trying to parse anyway so there's no need to create "rituals" around it. Note that it is beneficial to build on top of Deco a standard for type serialization to guarantee proper interoperability between different implementations, while Deco's scope is limited to creating structure without delimiter collisions so it's "lower level" in a sense.
I'm not 100% sure since I haven't tested it, `\r` isn't a delimiter nor text editors display it as a line break, so I'm guessing it will just be part of the entry's content. It could potentially cause trouble with opening sets, so I'll try testing it.
Everyone seems so quick to dive into language lawyer discussions. Is my other comment wrong? Seems simple to me, if you want compile time evaluation then force compile time evaluation by assigning to a constexpr variable.
I don't know the reason for the design decision, but I would extract the loops in a separate function and use a return statement. In my opinion, that would be more readable than a break(n) or a goto. 
Format code by starting your lines with 4 spaces.
There's actually no guarantee that a constexpr expression is evaluated at compile time. Eg with msvc in debug mode it happens at runtime.
[FastBuild lists PS4 as one of it’s supported platforms.](http://fastbuild.org/docs/features.html)
From my reading of [rust/rfcs#911 (const fn)](https://github.com/rust-lang/rfcs/blob/master/text/0911-const-fn.md) and my experiment on https://rust.godbolt.org, `const fn` in rust doesn't seem much different from C++ constexpr functions. It's possible to call them with non-const values. But it seems that [I can't use `if ... else` in a `const fn`](https://godbolt.org/g/vU4W4q), so I failed to produce a more complex scenery. (That's not even as powerful as C++11 constexpr!) Or maybe you are refering to a different feature that I'm not aware of?
They've been banned. Thanks for the level-headed responses.
Agreed with /u/doom_Oo7 -- that still doesn't use `i` somewhere a constant expression is required (e.g. a template argument) so it still has no guarantees.
&gt; As far as I can tell it isn't required anywhere. Template arguments are an easy option for integral types, e.g. just instantiating a `std::integral_constant` with the value in question.
you can do [](){ for (int i = 0; i &lt; N; ++i) { for (int j = 0; j &lt; M; ++j) { if (cond[i][j]) return; else do_stuff(); } } }(); but I think it's not idiomatic enough to be used as a replacement 
Could you explain what's happening with the pointer stuff at the end of your message? I have a get function just like that, should it not return the address of a member variable? is auto not a pointer to it?
A constexpr variable must be initialized with a constant expression. However, I am not enough of a compiler expert to state definitively that this will guarantee that the computation will run at compile-time when the result isn't "really" needed at compile time. (By the as-if rule, there is no way to detect the difference.) My expectation for C1XX is that it delays constexpr evaluation as much as possible, but that constexpr variable initialization will trigger it. Still, if I were paranoid, I'd run it through `integral_constant&lt;MEOW&gt;::value" to be sure.
They don't need design decisions for not putting in arbitrary features.
Lack of labelled loops to use with break and continue is a pain, yes. Every alternative to goto that people try to come up with to make up for it is uglier and harder to understand than just using goto, too.
I would characterize `constexpr` as more than a hint like `inline`, but short of an ironclad command. The difference is that if you have a `constexpr` variable, you really can use it as an array bound or template argument.
Sometimes it may be unavoidable to double iterate (like iterating over a matrix) and it's somewhat inconvenient to create a separate function for every nested loop (and you are not guaranteed that the compiler will inline the functions)
A C++ implementation could defer everything till run-time, and then it is called a interpreter. :/
`val` has type `int *`. The problem is that it is the _pointer_ that gets incremented here, while the original goal was to increment the variable it points to. 
Yo usually insert into both for loops a boolean or whatever you want and check it every turn, it is an elegant solution, at the if you change the variable making the next evaluation false and it will stop for itself, no need to call goto or anything else
There were some bugs with this in MSVC, but with VS2017 15.5.2 the #2 case above is producing a constant in the generated code even with /Od. That having been said, the only actual guarantee I can find in the standard is that a constexpr variable is guaranteed to be constant initialized instead of dynamically initialized -- there are no guarantees about evaluation at translation time or complexity. :( 
I've gotten burnt by a change from: const char *const STRING_ARRAY[] = ... to: constexpr char *const STRING_ARRAY[] = ... 
The unsatisfying answer is that the switch statement only supports primitive types, and std::string is not a primitive type, but a class that comes part of the standard library. No user defined types can be used in switch statements. 
No, you don't need a separate function for every nested loop. You need only one function enclosing all the loops. You may even make it a lambda as /u/Sopel97 [shows](https://www.reddit.com/r/cpp/comments/7mp2dn/why_doesnt_cc_have_a_multibreak_statement/drvn91h/). 
Probably because the implementation for a switch table only works on integers. If you want to switch over strings, you'd need a whole different implementation of switch. It might come at some point, if there's enough demand, but I don't think there is. string switch is much slower for starters.
Well, `enum`s and strong `enum`s are supported. But those are themselves just syntactical sugar around primitive types, so your original statement is broadly true.
That's actually a really cool idea, if a bit confusing. 
&gt;Everyone seems so quick to dive into language lawyer discussions. Is my other comment wrong? Because given how complex C++ is the only way to understand anything about the language is to get involved in language lawyer discussions. C++ is basically a language where you have to consume a great deal of your time and resources thinking about exceptions (and I don't just mean try/catch exceptions, although you have to worry about those too). There is no guarantee that assignment to a `constexpr` variable results in compile time evaluation. The only way to really force compile time evaluation is to use the result of the expression in a situation that must get resolved at compile time and to the best of my knowledge this can only be **forced** in a template. constexpr int value = some_constexpr(); my_template&lt;value&gt; v; // value is forced to be evaluated at compile time.
Even if we're being pedantic this isn't true. Even an interpreter has a compilation phase. The question is how can we force the evaluation of an expression in the compilation phase.
If the clutter is necessary, then it's just code.
I see. Thank you. Are primitive types universally primitive, or different for every language?
Thanks. Why are switch statements so much slower than other alternatives like if-statements, when you might be using less code?
That doesn’t address my issue. For example, UE4 uses a custom build tool (UBT). If you wanted to use fastbuild’s distributed compilation, could you use it conjunction with UBT?
A quick google for "Fastbuild Unreal" shows several projects intended to make building Unreal via Fastbuild work. No idea how solid they are.
In other words, switch statements on strings are just syntactic sugar on an if/else block and no one has championed adding that to the standard.
C++ inherited `switch` from C, and understanding that will help understand why you can't use strings. C doesn't really have a "string" type at all, actually. There's string literals (`"for example"`), but these are just arrays of `char`, which most often decay to pointers to `char`. So that's the first difficulty. You aren't switching on a string, you're switching on a `char*`. The second is when you consider what switching on a `char*` really means. How does the compiler know if two strings are equal? Well, it has to use a function like `strcmp` under the hood -- and that's really, really going to bother certain C developers. Calling hidden functions like that to enable syntactical sugar isn't going to be a popular idea with them. Since such a `switch` is (mostly) functionally equivalent to an if/else tree with `strcmp`, there isn't a driving need for the feature. Often this tree will get replaced with a table and a loop anyway. I'll also point out that `switch` doesn't work with `float` or `double` ... and for good reason. You almost never want equality comparisons with floating point types. Maybe if `switch` got a range feature, but it doesn't. So `switch` is limited to integral types: `int`, `char`, `short`, `unsigned`, and `enum` (since it's backed by an integral type). What about C++? For now it's just uses C's rules and calls it good. However, extending `switch` to work with other types [has been proposed](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3627.html), but that hasn't been adopted. Personally, I think someone could come along and write a good proposal to extend the `switch` semantics to work with any user-defined types (and, IMO, ranges) and it has a chance of being adopted.
You should abstract 2D iteration into a single loop. This is simple with the [range-v3](https://github.com/ericniebler/range-v3) library: auto loop2D(int w, int h) { return view::cartesian_product( view::indices(0, w), view::indices(0, h) ); } int main() { for(auto [x, y] : loop2D(2, 3)) { std::cout &lt;&lt; "x: " &lt;&lt; x &lt;&lt; ", y: " &lt;&lt; y &lt;&lt; "\n"; } } Output: x: 0, y: 0 x: 0, y: 1 x: 0, y: 2 x: 1, y: 0 x: 1, y: 1 x: 1, y: 2
The full blown `const` evaluator was merged into nightly a couple weeks ago. It allows for full evaluation of any function including those with heap allocation that does not perform io. However rust‘s `const` cannot yet be used in generics, that should come in 2018. 
For a REPL, parsing is interleaved with execution. I don't think the two phases are always distinguishable in this case.
It can only become idiomatic if people use it. The code within the lambda is identical to splitting the loops out into their own function. The only differences are this function isn't named and local variables can be captured.
There is a proposal of non-terminal deduction of template parameters that would allow pretty decent syntax for thise but I'm not sure of it's status.
They aren't, you misunderstood the comment: switch statements on strings are slower than those on integral types. But the same is true for if statements. In general, compilers can produce *more efficient* code for switch statements than for chained if.
Right, but one is guaranteed behaviour while the other is hopeful and depends on the level of optimization.
This the right right answer, but it's not quite the end of the story. C is meant to be basically directly translatable to assembly code, and switch is in C to support one very specific usage pattern: the jump table. If you think about it, that's also why switch has a default follow-through behavior. So from the perspective of C, if you can't make a jump table out of it, then it doesn't make sense to make a switch out of it, which is why only integral types are supported.
and this is why you use east-const ;)
I am not aware of other programming languages that have such a multi-break statement, does anybody know some? That could bring some ideas to the discussion.
I believe they're implemented as a look up table
PHP has numbered break. Java has labelled break.
How old are you talking about? It's not like boost libraries are new. 
In addition to what the other people have said I'd like to add some ways of getting around this restriction. [This page on stack overflow](https://stackoverflow.com/questions/650162/why-switch-statement-cannot-be-applied-on-strings) discusses a lot of options. My personal favorite which I think is conceptionally easiest is just to convert strings to ints using a hash function. The only caveat is that if you want to use the hash function in the case statements as well, then the hash function must be constexpr. Unfortunately, std:: hash is not constexpr, so you're stuck defining your own. Taken from the stack overflow page you then get this solution: constexpr unsigned int hash(const char *s, int off = 0) { return !s[off] ? 5381 : (hash(s, off+1)*33) ^ s[off]; } switch( hash(str) ) { case hash("one") : // do something case hash("two") : // do something }
For learning, yes. But if you are going to keep your application running, why change it now? In future you will find yourself increasingly isolated and having to hand craft libraries on your own if you port your backend service to C++ from a more specialized framework. 
Different everywhere in principle, but most languages are pretty similar :) The only thing that makes a type primitive in this case is that it is not defined in terms of other types in the language - it just de facto exists. Strings are primitive in JavaScript, for example.
[removed]
I'd use a macro to add it a little more readable, but I guess it's still ugly. NEST_START for (int i = 0; i &lt; N; ++i) { for (int j = 0; j &lt; M; ++j) { if (cond[i][j]) return; else do_stuff(); } } NEST_END
I think what cpp_learner is saying is that the results OP is seeing are the results of inlining and constant folding---which are really outside the purview of the C++ language. For example, I've seen GCC reduce `std::find` to a constant even though it's never been declared `constexpr`. But this has nothing to do with `constexpr`. `constexpr` does not guarantee that a function is evaluated at compile time. It means that, when used in contexts that *require* constants, it will be evaluated. Also, `constexpr` does not guarantee that a variable is a "compile-time" variable. You can take the address of a constexpr variable, which is admittedly a bit weird. It really just guarantees that the initializer is computed at compile time. This turns out to make the variable usable as a template argument (you can also use `const` qualified variables as template arguments as long as the initializer is a constant). FWIW, I think the addition of a `constexpr` keyword was a mistake. 
That's not a good characterization. It's not a hint, it's a permission. Specifically, `constexpr` permits a compiler to (try to) call a function in the various contexts where constant expressions are allowed, like template arguments and array bounds. `constexpr` variables are bit odd. They're still variables, so they have addresses, but their initializers are guaranteed to be constants. Don't ask me to explain the ODR implications of returning references to members of `constexpr` variables in lambda expressions. I'm still trying to figure that out :)
The keyword was invented by the committee. The original proposal did not include any keywords. http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1521.pdf The better question is, "why did the committee require a keyword?" 
The keyword `constexpr` was demanded by the Compiler Writer Guild back in 2003. It is was a surprise move, at least for me - usually they would rally against new keywords (those were the times). I initially fought it, but in the end I made peace with it. There is at least one situation where it is both necessary and useful: for declaring a constexpr object - `const` proved to be both too nebulous and entrenched at the time to improve.
Either guaranteed compile-time computation or static initialization (think references or pointers). This is the one case where `constexpr` is meaningful as a keyword -- and surprisingly a very useful one.
&gt; disappointing You can manually do memmem, but the point is that hopefully C++ STL is used so much that as times go to infinity probability of searchers being slower than memmem goes to 0. :) In other words if you want to be sure you need to measure and compare to "old school" style, but in general case you can usually just use STL and most of the time it will do the right thing. It is all about time investment. Productivity vs "guarantee" of performance.
Because the Compiler Writer Guild demand it? :-) My initial proposal for `constexpr` functions didn't have the keyword -- I still believe it isn't needed for functions, but that ship has sailed. It is however both necessary and useful for declaring constexpr object. 
Not necessarily, in retrospect. I tried to reform `const`, but it proved (and still is) too nebulous and its uses too entrenched to be amenable to the kind of certainty, clarity, and rigor we enjoy from `constexpr`.
As far as I can tell a const variable and constexpr variable differ only in the way they're linked (constexpr vars are inline --- which didn't exist in &lt; '17). My understanding is that constexpr evaluation doesn't care about the qualifier, only the quality of the initializer. Is my understanding wrong?
It is hard to live with history. Initially, `constexpr` was fought hard with claims that it was impossible or unsound to implement. People were told to be scared because `constexpr` "required a VM" (yep, those were arguments heard in 2003-2007). Consequently, I needed a model that was simple but useful enough: so we have the evaluation by substitution model in C++11. It was simple (and predictable). But (and this is good) soon enough the community saw the value of constexpr in practice, so WG21 had to finally embrace it, removing unneeded restrictions. I believe the C++14 expansion is a welcome generalization; but without the more restricted C++11 version it would not have been possible -- WG21 just wasn't prepared for it yet.
No, it isn't not a "hinting keyword". It carries a full force, enforced by the compiler at translation time. This is not a case where a textual interpretation without context is useful.
You can't call a non-constexpr function from a constexpr function.
Actually, `constexpr` on variables IS good. See the array table examples in [General Constant Expressions for System Programming Languages](http://www.axiomatics.org/~gdr/constants/constexpr-sac10.pdf)
Note that `constexpr` variables are quite very useful for placing objects in ROM; this is useful for embedded systems.
/u/sakarri is correct. There is an notion of 'phase distinction' in C and C++ that is easily lost until you start doing hardcore system programming, or you are writing a C++ compiler :-) compilation, linking, loading, and running. At each phase, the notion of "constant" is slightly different.
Initialization. If you have a `const` variable, if the compiler cannot guarantee a static initialization, it would just silently fall back to dynamic initialization -- that behavior is absent from C. Which trips lot of people. For a `constexpr` variable, if the compiler cannot guarantee static initialization, you get an ill-formed program. That restore the C behavior, get go ways beyond -- "constexpr"-induced static initialization is a strict superset of C static initialization. This is useful when building systems. I've seen real-world deployed systems (to billions of users) where changes from `const` to `constexpr` have dramatically improved startup times in addition to *fixing* bugs.
The Module TS spec is clarifying that.
Thanks for posting. This is an interesting topic for me. I've written my own game-focused library [Jinx](http://www.jinx-lang.org/), an embeddable scripting language, so it's always interesting to see other approaches to solving common issues. As you indicated, part of being game-developer friendly means things like overridable allocators. In my case, I also offer memory tracking, memory and performance APIs, etc. And like with Hummingbird, in case the user doesn't have an engine-specific allocator, I provide a built-in allocator that provides better performance and cache-locality than the default allocator. In my case, it's a custom block-type allocator, trading a bit of extra memory for faster allocation, a decision you decided against. Jinx uses very little memory anyhow and tends to use it in fairly predictable chunks, so this seemed like a reasonable tradeoff. A decade or two ago, I would have balked at wasting *any* memory, but these days, unless you're targeting very small devices, I think most games can afford a few wasted kilobytes here and there for the sake of speed. This is also mitigated by allowing users to change the size of the allocation blocks to some degree, so they can sacrifice some performance for less wasted memory if they choose, or of course, just use their own allocator. You can even use both in a hybrid mode, where custom allocator callbacks are only used for allocating and freeing the larger internal blocks. Having a custom allocator also lets you do some nice things, like adding additional opt-in features like leak detection, memory tracking, and guard bands to detect memory corruption (although this is thankfully more rare when writing modern C++). And I've found it's valuable to have a quick and easy way to switch between a custom allocator and default malloc, not only to be able to check performance, but to verify correctness as well (is it my allocator or a different bug?). 
&gt;The full blown `const` evaluator was merged into nightly a couple weeks ago. I am unable to find any information on that. Could you help me?
The disappointing thing is exactly that it will only do the right thing most of the time, and not every time. It's the reason we have threads like this.
Ah. That makes a lot of sense. Thanks. I suppose that means that a constexpr variable is statically initialized also impacts other aspects of its use (like returning a reference to a member of a constexpr variable).
Yeah, I think it's hard from a design perspective. I was looking at [the implementation of `memmem`](https://github.com/bminor/glibc/blob/73dfd088936b9237599e4ab737c7ae2ea7d710e1/string/memmem.c) and it uses a different algorithm depending on whether the search sequence is longer or shorter than 32 bytes. In this particular case, using a 31 byte search sequence is 8x as fast as a 33 byte search sequence, while there's no speed difference there for `std::boyer_moore_searcher` since it always uses the same search algorithm. Should we also standardize `std::two_way_short_needle_searcher` and `std::two_way_long_needle_searcher`, since I know the short needle searcher will be faster in my case even if my needle is 33 bytes? Should we have a `std::i_dont_care_searcher` for if I just want whatever the implementation thinks should work best? Then you look at the bug, where the point was made that we can add a special case for `std::byte` but not enum types in general, since you can overload equality for your enums. Do we add `std::i_promise_i_didnt_overload_equality_for_this_type_searcher`? Maybe have some sort of detection facility so you can do `conditional&lt;has_boyer_moore_array_searcher, __boyer_moore_array_searcher, boyer_moore_searcher&gt;`? Do we throw the specializations in a third-party library so they're always available but duplicate code that's already in the standard library but not portably available? I don't know, and it *is* disappointing that we don't have a good answer.
As far as I understand, it tends to depend on the contents of the switch statement. It could be if/else, one or two level jump tables, or binary searches (required for large numbers of non-contiguous values). There's a [very interesting analysis found here](https://www.codeproject.com/Articles/100473/Something-You-May-Not-Know-About-the-Switch-Statem). It takes a bit of time to slog through, especially if you're not all that familiar with assembly, but it's fascinating at how involved such a simple language feature ends up.
I am aware of [\[lex.phases\]](http://eel.is/c++draft/lex.phases). But the phases does not have to be implemented as distinct steps AFAIK (e.g. phase 2 might start before the file is completely scanned in phase 1.) All that means a REPL can be conforming to the standard. (I'm not sure whether [cling](https://github.com/vgvassilev/cling) is conforming since I've never used it, but at least I think it can be.)
Right, there is a monadic aspect to partial evaluation (which is what `constexpr` provides) that the C++ design ensures didn't end up in the type system -- which would have been terrible. That is emphases in the sections "Related Work" and "Conclusion" of the [constexpr paper](http://www.axiomatics.org/~gdr/constants/constexpr-sac10.pdf). The evaluation context needs to the threaded in.
What I mean by phase distinction has little to do with Clause 5. What I mean is that in languages like C or C++, the compiler is long gone by the time the (static) linker starts its work, which is long gone before the loaded starts its work, which is long gone, before the runtime. Consequently, things that the linker would consider "constants" aren't available to the compiler. Similarly, things that the loaded would consider "constants" aren't available to the static linker. 
That might be true for compilers (the overwhelmingly common kind of implementation), but I thought we are talking about uncommon implementation permitted by the standard.
I see that monads are now unavoidable in all contexts :)
Agreed, but AFAIK it also needs to have static storage duration to qualify for constant initialization, so at function scope mere `constexpr` alone wouldn't be sufficient, no?
I'm down to take this pedantry all the way to the end but there's no escaping the fact that you can not defer every aspect of C++ until run time, even in an interpreter. A programming language, like any formal language, is nothing more than a set of strings. Given a language `L`, a recognizer for `L` is an algorithm that takes a string `S` as input, and returns `ACCEPT` if and only if `S` is an element of `L`. An interpreter for a language takes as input a string `P`, uses a recognizer to determine if `P` is an element of the language `C++`, and if the recognizer returns `ACCEPT` the interpreter returns an output `P` which it computes based on some "semantic" rules it has about arithmetic, memory, so on so forth... Now to the crux of the issue here. Since C++ is a statically typed language, then in order to determine whether a string `S` is an element of the language `C++` we must perform the process of type checking. One mechanism in C++ to construct types is through the use of templates which may take as parameters values. The mechanism for producing such values is `constexpr`. In other words it's irrelevant whether you're running an interpreter, a compiler or even carrying this out by hand with pen and paper. If you are given a string of characters `S` as input and you want to determine whether `S` is an element of C++ then you have no choice but to evaluate `constexpr`'s before you ever get a chance to take the string `S` and produce an output `P`.
That's atrocious if it doesn't get inlined, you've just built a (potentially huge) temporary just to hold a matrix of index pairs, ew.
it's lazily evaluated, bc why not.
AFAIK, the range-v3 views are light weight (like std::string_view). In this particular case, each instance created by view::indices holds two integers (and bool flag for done) and the instance created by view::cartesian_product just holds these pairs of indices in tuple or something like that. The size of the result returned by loop2D function should be equal to the size of 4 integers (+ 2 bool flags + padding).
In theory correct. In practice, local statics are far more common than we would like to believe, or just we would like to see hem. And, in the context where `constexpr` was designed, that is exactly the use cases we wanted to cover. See the [constexpr paper](http://www.axiomatics.org/~gdr/constants/constexpr-sac10.pdf).
And look at the terrible effects it has on design when you require them in the type system ;-)
And how does that change the notion of phase distinction as embedded in the core language design?
&gt; the idiomatic method of breaking multiple loops is with a goto, but this forces you to stop thinking in a highly controlled way. I don't know what this means, but labeled break seems clearly better than a PHP-style depth count, and the language already has labeled break in the form of goto, so I don't understand the issue
perl.
The temporary is a constant size, as others said. I tested with `clang -O3`, the range-v3 functions are inlined, but still have overhead compared to native nested loops. I suspect `loop2D` could be implemented for better optimization, potentially equivalent to native loops. For now, range-v3 is good enough for me.
I'm quickly realizing that the main issue I have with writing the goto is that I just don't want to write goto. For all intents and purposes the labeled break is the same thing (and imo also the same as the depth count break) but writing goto to break loops just feels wrong. I probably just need to get over it. 
Yeah. I wasn't trying to exhaustively list the differences, so what I wrote was probably more confusing than helpful. The subtlety that I had in mind is that a function template marked with the `constexpr` keyword can call a function that happens to be non-constexpr for particular template arguments, and this is perfectly valid (the function template is marked with the `constexpr` keyword, but the specialization happens to be a non-constexpr function). The clearest example is `std::plus`'s function call operator, which is constexpr for `int` and non-constexpr for `string`. As implementers, we know how this works, but users see this sort of thing and think that `constexpr` is merely advisory. It's a lot more than that, but the template thing is why I referred to it as "short of an ironclad command".
I once benchmarked this approach vs normal string comparison (if_elseif_else) and hashing the string everytime is a performance hit even in the average case. 
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
AAA is still pretty tight on memory usage, but your average indie game could waste MBs without issue.
Golang has a labelled break: https://golang.org/ref/spec#Break_statements
Delighted to hear it. Is this context within the standard itself, or in external documents? 
Won't vectorize.
I would hope that `static_assert` would work too. 
Ok. It still bothers me that there is really only one implementation possible, because of the noexcept clause on c_str() and data() (you can't store the characters as an array of pointers to chunks, for example, since you could never guarantee that enough space would be available when c_str() or data() is called). And I'd still argue that despite it being a 'merely logical const', that const clause does imply that whatever internal state changes happen, are not (ever) visible to the outside world - also not for a piece of code like my example above. However, at the same time we are apparently supposed to program as if UB is ready to jump us if we ever take a pointer to one of the characters in the string and pretend it is an array. Worse: I very much doubt that any implementation will ever have strings that are not permanently null-terminated, but perhaps a clever compiler will detect the potential UB and start eliminating code... Once again I'm struck by the notion that C++ would be a much nicer language if only the standard bothered codifying things that are actually already true in every practical sense anyway... 
To elaborate, it's not that it's slower that `if`, in fact, as /u/guepier says, it can be faster. If I remember Java's implementation correctly, it involves hashing the string, jumping to a branch and then doing string comparison, since multiple strings can have the same hashes. So, best case would be a single hash and string comparison, which would be much better than `if` once you exceed a couple of cases. What I wanted to say is that this is still a slow, somewhat high level feature that does not translate to assembly very well. I think C++ development is more geared towards improving performance and this does not interest most advanced (high requirements) users. Another reason is that strings are icky on C++. There's `char*`, `wchar*`, `std::string`, `std::wstring`. That further complicates building language features on top of strings. Though `case` could only contain string literals, `switch` still would need to consume an arbitrary string to appease everyone. And, of course, the implementation is different for different compilers.
That's what I expected as well. It's probably best not to use this in performance critical sections of code. In my use case this wasn't a big problem though. The string indicated a type of method to use, and I wanted the type of expressiveness of switch statements to choose between different methods. In my case, the switch statement was much faster than the methods themselves took. 
For functions we might as well drop it again (which, at this stage, would mean 'make it optional' for backward compatible reasons). I don't see what difference it makes at this stage: // constexpr allowed, despite non-constexpr path: constexpr void foo (bool b) { if (b) return 42; else throw 42; } constexpr auto a = foo (true); // correct. constexpr auto b = foo (false); // ill-formed. So why not also allow: void bar () { return 42; } constexpr auto c = bar (); 
That are good points.
Burnt in what way?
I have just myself created a delegate thing which is extremely lightweight but guaranteed to not allocate memory or throw exceptions. It is intended to be usable for embedded and real-time systems. Here: https://github.com/ambrop72/aipstack/blob/master/src/aipstack/misc/Function.h The Function object can be initialized from any Callable object (such as lambda) whose size does not exceed the statically defined maximum FunctionStorageSize (default: sizeof(void *)) and which has a trivial copy constructor and destructor. This means that there is no overhead (e.g. function pointers) when copying or destroying a Function object (copy all bytes and do nothing respectively). In practice this means that Function can be initialized from lambdas that capture at most one reference or pointer or a set of trivial types that fit in the storage. This is sufficient for all my use cases because: - For async event-driven callbacks, I really only need to capture the "this" pointer since anything else can be stored in the class to which the callback goes. - For local callbacks, if Function cannot be used directly due to its restrictions, the lambda can be wrapped into std::reference_wrapper (with RefFunction() helper). I see that it does not work std::bind(&amp;Class::memberFunc, this, placeholders...), which is because std::bind stores the function pointer inside the bind object. However it is possible to create a custom "bind" that does (just stores "this" in the object and encodes the function to call in object code of operator()). I will probably do this soon.
As an AAA dev, I can say we can normally spare kilobytes. Megabytes take more justifying, but a lot of AAA games are a lot looser with memory than they used to be - both UE4 and Unity are garbage collected engines, which was an inconceivable waste of memory back in the day.
Could you elaborate on that a little bit more? How would it look like if it affected the type system?
I should've specified AAA on console, which is a bit tighter. Kilobytes are sparable, but I've also watched my team spend weeks reducing GC allocs in Unity. Probably varies a lot from studio to studio depending on what the biggest constraints are for their titles and platforms.
It would be so cool if an IDE could highlight variables/functions that are actually evaluated at compile time according to the compiler in use. Bump for VS devs!
Isn't that the same thing, in that static_assert itself has to be compile time? Any time I've used constexpr I've used static_assert liberally in order to clarify that something is indeed compile time.
Thanks, interesting, I hadn't use perl, golang or php, but I did java, and wasn't aware of this. Then, imo it might make sense that C++ had labeled breaks (not break(n) as the OP suggested), sounds like an interesting language feature proposal. All the above seem a bit like a workaround, and having this will get the goto advantages without some of its disadvantages.
Wow, that's really interesting! So, it can't do IO, but can it make system calls? How about threading? Can you deadlock the compiler?
This is a great topic for a blog post that I should write one day. Remind me 4 months from now, if you don't see anything from me.
The C++ community has a strong and persistent track record of: (a) asking for new syntax (usually heavy) for new functionalities especially where not needed; (b) after a few years of usage, expressing outrage against said syntax (usually felt as unnecessary). constexpr, concepts, modules, etc.
There are specific contexts in the language specification where a constant expression is required, therefore compile-time evaluation required. Examples include array bound, the "operand" of `case` labels, non-type template arguments, initializers of variables declared `constexpr`, `static_assert`, etc.
Yeah, I'd support this. I think the reasoning behind not switching on (say) a string is that it would be less efficient than a switch, because it would have to be a list of `if()... else if()... ` and so on. I say that we're going to have to write that anyway, so we may as well get a `switch` that does it. There might be some oddness where the order of items in the switch statement really matters (especially if comparison is slow, or has side effects for some reason) but I think it's something we'll be able to deal with. 
just make a `[&amp;]` lambda instead
Right. I still don't know how I feel about that exceptional "failed-constexpr-but-is-declared-constexpr" thing. It was necessary to complete the CWG review process, but I don't know how much it improves on programming itself ;-)
If you call a constexpr function with constant arguments in constexpr contexts, you are guaranteed to get a constant back -- assuming evaluation goes well. We need to distinguish between compiler bugs, standard behavior, and just simple optimizations that certain compiler refuse to perform.
The GC unity uses is very slow, so the concern rather is to reduce GC allocs than save memory.
I would say that it can't perform any system calls, the heap access is most likely replaced with calls into a memory manager inside of the compiler.
The plan is to move to more powerful pattern matching: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0095r1.html
Were there other suggested names for the keyword?
Well, it's a language in development, and you yourself described how constexpr could only become a feature by passing through a growing stage first. That we arrive at new wisdom is a consequence of that growth. Anyway, I was wrong: constexpr on functions could still serve a purpose, as a compiler hint that it might be worth trying a compile-time evaluation (I mean, it is allowed, and it would in some cases be a clear performance win). I wouldn't expect compilers to necessarily try this for all functions (you'd know better than me), given that it would carry a considerable compile time cost, so having a hint to that effect would definitely still accomplish something. 
Woah that feels good. I don't get the `@` thing in `=&gt;` expression but naming/aliasing memers of `std::pair` is a big improvement. I haven't used it much but `.first/.second` is easy to forget it's purpose.
switch is unsalvageable IMO. I'd rather see it obsoleted by real pattern matching. switch (1) { case 1: int x = 42; break; case 2:; // error: jump bypasses variable initialization }
&gt; If you call a constexpr function with constant arguments in constexpr contexts, you are guaranteed to get a constant back -- assuming evaluation goes well. I think the issue is many people don't understand the constexpr context part. They think, "I've written a constexpr function, and I'm assigning it to a constexpr variable, that whole statement must be constexpr, as the compiler complains if it's not possible". That's the sort of "intuitive" thought that I think many folks have when they first hear about and start using the feature, I certainly thought that initially. Unfortunately that's not the case, and I think it'd be good if we added a way to make _that_ guarantee as well as the ones already provided.
&gt; I think the reasoning behind not switching on (say) a string is that it would be less efficient than a switch, because it would have to be a list of `if()... else if()...` and so on. This isn’t true though. Other languages implement switching on strings efficiently by transforming it to a hash table lookup. Of course this puts other constraints on the type of the switch expression (needs to be hashable) but that’s entirely reasonable.
This was proposed as part of [N3879](https://wg21.link/n3879) and rejected back in 2014. [EWG voted 3/8/4/9/3](https://wg21.link/ewg85) on labeled break and continue.
I would happily have less efficient but more readable, understandable code. Bottlenecks in performance can then be easily refactored as needed. 
Just encapsulate it in a function and use return.
Me too! 
Ah! Very sorry. lol leave my comment so that other people learn from your corrections. :-)
I was referring to console too. Interesting to here about the Unity side, I mostly work with UE4. Our biggest memory hog these days is content rather than anything used by code.
&gt; Once again I'm struck by the notion that C++ would be a much nicer language if only the standard bothered codifying things that are actually already true in every practical sense anyway.. In this case, I fully agree with you. I see no practical benefits in not enforcing that `std::string` always be NUL-terminated. I think this is merely a historical accident, it didn't *need* to be prior to C++11, and indeed implementations using a reference-counted buffer could then propose a O(1) `substr` operation back then. I suppose that nobody realized it didn't make sense any longer, since now `c_str` must be O(1), and therefore things were not cleaned up. On the other hand, there are cases where the specifications are *so* implementation-specific that in practice a single implementation is available: `std::map` is always implemented as a Red-Black Tree as far as I know. I think this stems from the fact that the standard body derived the specifications from existing implementations a-posteriori and didn't consider that some constraints would have been better relaxed. This leaves us, nowadays, with a `std::map` which *guarantees* memory stability of elements, key and value being stored next to each other in a `pair`, etc... when much more efficient implementations could be offered when relaxing either constraint. Did you know that Rust's BTreeMap performed as well as its hash-map in most circumstances? That Eytzinger trees were wonderfully cache-friendly? Unfortunately, neither is usable for `std::map`. 
Besides some of the weird syntax what I really dislike is the weird choice of keywords :/ &gt;`lvariant` is used instead of `enum union` based on feedback in Kona. But why? If `enum union` isn't good enough (though I'd prefer `union class` for symmetry with `enum class`), why not just `variant`? Besides that `inspect` feels like it's named inspect just for the sake of not being the same as `match` in other languages.
No I/O, No systems calls, No threading. There are two constraints at play for the const evaluator of Rust: 1. Cross-compiling: the evaluator must behave like the *target* platform, not the *host* one, to allow compiling 16-bits binaries on your typical 64-bits Windows/Linux computer. 2. Determinism: the output of the evaluator must be deterministic to allow its use in generics (size of arrays, etc...). I think the I/O restriction could be *slightly* relaxed in the future, to allow reading resources files and to allow logging, but since it's already possible to use `include_str!` and `include_bytes!` macros to inline a file's content in a value, it's definitely not a priority. *Note: the interpreter itself, MIRI, could definitely be enhanced to support I/O, system calls and threading, especially when host = target, but even then this would likely not be turned on during const evaluation.*
As someone else mentioned, the main reason for reducing GC allocs is performance of the GC, but reducing memory usage is an additional motivator. Both contribute to more consistently hitting the title FPS target. 
Sorry for the confusion. There are two features here: - `const fn`: functions that may be called at either run-time or compile-time, - `const` values: values that are guaranteed to be evaluated at compile-time. Therefore, when you write: `const X: i64 = some_computation(4, "42");` you are guaranteed that `X` is computed at a compile-time, regardless of whether it is used in a constant expression or not. Indeed, if this is the sole invocation of `some_computation` and the function is not exposed outside the library, then the code of `some_computation` will not even be included in the resulting binary.
&gt; Did they get variadic templates yet? Not yet. Rust has punted on those by supplying: - built-in tuples (`(i64, String)` is a 2-tuple) and function traits (`Fn(i64, String) -&gt; String` is a function trait), - some intrinsics (such as that used by `println!`, which performs type-checking much like C compilers do it with `printf`), - macros, which are variadic, allowing `vec!(1, 2, 3, 4, 5, 6)` to build a `Vec&lt;i32&gt;` containing the 6 elements, - procedural macros, which are also variadic, and invoke arbitrary user code, though they are only available on nightly. This does not cover all use-cases, but a sufficient number of them that this is not a pressing matter. --- &gt; I didn't realize they had a constexpr equivalent. What are the limitations? Rust features `const fn` (functions which can be called at a compile-time) and `const` values (values which are guaranteed to be available at a compile-time, and can be used in constant expressions such as array length). For now, `const fn` are rather limited, however MIRI, a full-blown interpreter, was just merged into the nightly compiler. MIRI allows arbitrary Rust code as long as it avoids I/O and system calls (including threading/time), so it's expected that soon all of this will be available for `const fn`. As for `const` values, they are for now limited to values with no memory allocations. I hope that this limitation is relaxed at some point, but this is not pressing. Building upon those, `const` generics are being developed, so that some time next year it will become possible to parameterize generic functions and types with values on top of types.
The name variant is already used in the standard library and in other projects, choosing that name would break a lot of existing code. I don't mind `union class`, but you could argue it's not really similar to `enum class`, `enum class` is a more type safe enum, `lvariant` is not *just* a more type safe union. Personally, I like the name `enum union` a lot more than `lvariant`, it's a mix of a `union`, and a discriminator (`enum`), it also has the bonus of no possible collisions with existing code. I don't care that much though, whatever the keyword(s) end up being, I'd just like to have this feature in C++. &amp;nbsp; :-)
And NEST_START does what? '[&amp;]() {' ? This is so much more obscure.
&gt; and it can be tricky to spot that the return statement is actually inside the lambda I nearly used such a scheme a few times, but each time balked precisely because of this reason :(
Thanks for the information, that's really fascinating!
Rust [example](https://play.rust-lang.org/?gist=0d63302055f6731fad6474f347e42a99&amp;version=stable): fn main() { 'outer: for i in 0..5 { for j in 0..5 { println!("i, j: {}, {}", i, j); if (i, j) == (3, 3) { break 'outer; } } } }
Those kind of errors can be detected early when you develop correctly and look for mistakes within your code. You have a lot of tutorials on the web to help you do that. It also looks like you can use some programs to help you do that. I got an advise once to use Checkmarx to detect vulnerabilities in the code, you can try it out. Good luck
in your first example, do you mean switch (x)?
Since you have been playing with multiple memory allocators, I have a few questions for you: - `malloc` is well known *not* to allow requesting a specific alignment, will your allocator allows requesting both *size* and *alignment*? - `free` requires neither *size* nor *alignment*, while generally the user should know it, have you considered requesting them to save on meta-data book-keeping? - do you use `realloc`? - have you considered having `malloc` returning NOT the size/alignment that was asked for, but the size/alignment which is *available*? The latter is a particular pet-peeve of mine. I find it silly that when I request N bytes, I actually get N+X, but the X are not usable. This causes some friction between users and allocators, for example in the traditional case of trying to achieve amortized constant complexity on `std::vector::push_back` you'd really like more feedback from the underlying allocator; rather than picking a growth strategy out of thin air (x2? x1.5?) why not let the underlying allocator expose what it can best serve?
I might be talking out of my butt right now, but I think that'd be hard to implement, since I think the decision on whether to evaluate something at compile time is made in some optimization pass. I think the reason constexpr doesn't really force or guarantee anything is because generally the compiler knows best whether to evaluate something at compile time or not. It's a trade-off between compilation times, binary size, and run time efficiency, which I'd guess are hard to balance.
I really wish they held back on `std::variant` because if this were to be added it might as well be considered deprecated. Fair enough though, haven't considered it.
I agree that having a compiler hint is useful, but that could (should?) be an attribute rather than a keyword, imo, like most other compiler hints are.
Yes, you're right. It would be very difficult for sure. Especially with things like link-time optimisation. Perhaps even doing it after a full compilation and link would be somewhat useful. 
Useful as heck, but probably hard as heck as well! Would be really nice to be sure that my big tables are really compiled into the the binary directly.
I would really like to see match on a range. Something like: int x; switch(x) { case 1, 10: // matches 1 &lt;= x &lt;= 10 
 std::monostate fire_missile; // Fire a missile yikes... I suppose language would not support void fire_missile?
That's regular void: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0146r1.html However as with most C++ things, it's surprisingly tricky.
Do you mean the clangd? It doesn't matter where exactly to implement it actually. There are pros and cons for both options, but nothing really critical on both sides.
The only point of switch is to potentially enable branch selection with less than O(n) complexity, such as binary search or jump table. If you are ok with O(n) then there is no point in using it, just use the if - else if method. Unlike switch it gives you proper block scopes and does not suffer from the fallthrough problem.
That's mostly driven by console requirements though isn't it?
I think **var** is a better name.
My experience has been that most switch statements I’ve seen in the wild have been harder to read &amp; understand than a sequence of if/else statements.
But `x` is not used in the second case. It is not even expected to be usable there. Sure there's reason behind this design, but the design was made half a century ago, and is inconvenient now.
`var` makes me think of type interference because C# and a couple other languages use that.
GCC has supported this as an extension for years: switch(x) { case 1 ... 10: /* ... */ case 11 ... 20: /* ... */ } It'd be nice to see this ntroduced to the language proper.
`variant` is not deprecated by `lvariant` any more than `tuple` is deprecated by having structs!
Really? I have never seen it...
I'm not sure why you think, based on the previous two paragraphs at least, that `constexpr` was a mistake. It's often misunderstood to have more to do with optimization than it really does, sure. But it's very useful for transforming values that need to be fed into templates, for example. If you have some kind of template function and you want to be able to compute how large of a `std::array` you need for something. Or if you want to perform a computation on `index_sequence` style integers during unpacking. etc.
Isn't that a compiler feature rather than a library feature? Or are you saying you want to update the standard library to use the new feature?
If you don't need to use `x` elsewhere, then do this: switch (1) { case 1: { int x = 42; break; } case 2:; // We can't use x, but we don't care. } 
I think that the addition of the *keyword* was a mistake, not the concept. The original proposal, for example, did not include a keyword. http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1521.pdf Gaby does give a good reason for it's inclusion, but I don't believe we should be required to write `constexpr` on functions in order to have them evaluated at compile-time. It's misleading (a `constexpr` function may be evaluated at compile time, not must be), frustrating (why doesn't this work? oh yeah... forgot the `constexpr`... again), and it's showing up on *everything*. Make it optional. 
&gt; If @ is used, the pattern is matched only if the nested pattern matches the value being matched.
Tuples and structs have pretty different use cases I'd say. `std::variant` is just a bootleg attempt of mimicking sum types. I can't think of any reason to use `std::variant` over `lvariant` unless there's some next level template magic stuff you could do with it.
GCC supports the `optimize` attribute to change optimization flags on a per-function bases: https://godbolt.org/g/bcyxgQ
&gt; specialization: a buggy implementation exists, and the bugs are being smoothed out one at a time, however it's unclear whether some could actually block the feature AFAIK. The lack of specialization has been bugging me a lot lately because of the default implementation of `impl From&lt;T&gt; for T`. - The new `failure` crate can't have its `Error` struct implement the `Fail` trait because it implements `impl From&lt;Fail&gt; for Error`. This has impacted the design in several ways - I was contributing to a fluent way of defining environment variables and we wanted the type to both implement `IntoIterator` and `From&lt;IntoIterator&gt;` but again ran into `From&lt;T&gt; for T` conflicts.
Yeah, the exact same lines. I dunno, if you use it regularly it seems fine. You explain in the docs somewhere what it's for so it becomes part of the style. 
Doesn't GCC warn you when you declare a variable inside a case? I could have sworn it warned me last time. The solution is rather simple: add a scope to your switch statements.
Well I can use std::variant today, and then in 3 years I'll switch over the lvariant. No harm done. Can't just always wait for the next thing around the corner.
Ah, I misunderstood, sorry. What about variables though? Same thing, allow variables to be "implicitly" constexpr? If you use `i` in `std::array&lt;double, i&gt;` then the compiler just tries to figure out whether `i`can be computed at compile time? Same thing for static variable, try to figure out if it can be computed at compile time, and if so, initialize statically rather than dynamically?
Yeah it's been in there a long time
Well yes adding a scope improves the user experience. It doesn't make `switch` salvageable, though.
Yeah, generally people gaming on PCs have higher specs than a console. If a game hits perf targets and is within console resource limits the odds are good you're most of the way to shippable on PC too.
I'm going to keep it running until I complete its C++ equivalent. Currently my app serves only one stock market and it can handle things thanks to Node.js for this moment. I'm planning to expand into US markets and most of the reliable data providers only gives C++, .NET or Java API. C++ is the most common one. I can do C++ &gt; Node &gt; Frontend but Node will become irrelevant and it may create technical debt in the long term. I can access web APIs with C++, but I can't access C++ APIs using Node easily (it's possible, but it looked very ugly to me, so I'll pass.) I thought, I'm going to do this at some point anyway, so why not learn now? https://www.reddit.com/r/cpp/comments/3d1vjq/is_there_a_c_package_manager_if_not_how_do_you/ Reading the post above made me chuckle so I'm going to give it a try this weekend. Thank you for your comment!
For what it's worth, a break/continue of a nested loop is recognized as the only good use of goto by the core guidelines. (Rule ES.76)
ach, ok.
Can't you implement switch for strings in `O(longest string in case length)` though? Converting it to something like DFA.
I think the expression you were looking for is "well I never"
I don’t believe it supports threading, however you can emulate them because at the end of evaluation the threads would necessarily be “gone”. Most of the non-io syscalls calls can be emulated in similar ways, ie use atomics instead of futex waits etc. 
Interesting, but it seems useless in small template based code; I end up with non-inlined methods, which is far worse than the optim I wanted. Plus, as far as TBAA is concerned, this will often be an attribute applicable to a type that is needed (may_alias is good in that regard, but it just has the wrong polarity.)
https://www.reddit.com/r/rust/comments/7ju75d/miri_was_merged_into_rustc/
It is the same so long as the code doesn't change. Code will change. 
What is prwf? A tpyo. Fixed.
`compilexpr` or something =)
You could do pattern matching on a type implementing operator== if switch would accept non-PODs.
Or for any type that can be ordered, a binary search could be performed.
How does that not make switch unsalvageable? It literally solves the very issue you outlined. Surely if you have a personal reference that's fine, but you would need to make a case to objectively call it unsalvageable.
`lvariant` seem like a great candidate for metaclasses.
Oh, lol, thank you! 
`union class` is weird, since a union is already a class.
Kind of. Adding `constexpr` to a variable enforces properties on its initialization, which makes it meaningful in that context (See Gaby's responses). In contrast, adding `constexpr` to a function is meaningful when that function is called in a constexpr context. But the language already does try to figure out if a variable is a constant. For example: #include &lt;array&gt; int main(int argc, const char* argv[]) { const int n1 = 42; std::array&lt;int, n1&gt; a1; // OK, n1 is a constant const int n2 = argc; // Ok, but dynamic init std::array&lt;int, n2&gt; a2; // error: n2 not a constant } But, if you make `n2` constexpr, would would get this: constexpr int n2 = argc; // error: argc is not a constant Because it forces static initialization.
C++ doesn't really have a language concept of "string". They are just pointers with a non-enforced convention of zero termination. So if a switch statement takes a `char *` it doesn't even know how whether this pointer is valid and how many bytes are allocated. This can easily segfault or be a security vulnerability.
Pattern matching does way more than `operator==` can accomplish. For instance, pattern matching can dispatch based on (erased) type. This is in fact the whole point of the proposal: to allow *more* than what could be accomplished merely by `operator ==`.
Your link does not (or no longer) works.
&gt; I think the issue is many people don't understand the constexpr context part. This is why i always thought they would allow one to force constexpr context on stuff, instead of tagging everything with a keyword. Eg. no: constexpr int foo(int) { ... } But instead: constexpr(foo(5)); And: constexpr(foo(bar)); //error, maybe Unfortunately, committee went for a way that is 'intuitive' only for compiler writers and template wizards. I can sort of see why, but still, seems like a shame.
How would it be pronounced?
Yes, it's a bit annoying at the moment. On the other hand, C++ settled for wild wild west specialization and we've been suffering from it for 34 years (and counting!) now so I'd rather the developers take the time to figure it out and make sure it is sound rather than slapping something wonky together just to get it out of the door.
I did a similar some research in this topic. I did the work based on IPR by u/GabrielDosReis Here is my constexpr AST header https://github.com/hun-nemethpeter/cpp-reflector-mini/blob/master/constexpr-ast/ast.h And here is the intented use-case #include "ast.h" struct Test { bool member1; char member2; }; int main() { static_assert(typeid&lt;Test&gt;.name() == "Test"); static_assert(typeid&lt;Test&gt;.members().size() == 2); static_assert(typeid&lt;Test&gt;.members()[0].name() == "member1"); static_assert(typeid&lt;Test&gt;.members()[0].type().name() == "bool"); static_assert(typeid&lt;Test&gt;.members()[1].name() == "member2"); static_assert(typeid&lt;Test&gt;.members()[1].type().name() == "char"); static_assert(typeid&lt;Test&gt;.bases().size() == 0); return 0; } And here is the clang patchset for compiling it: https://github.com/hun-nemethpeter/clang/commits/typid_ast
While I'd never... You are fright! I'll blame it on autocorrect,though fairly certain I typed it on a PC. 
like compile-expr. Maybe shorten it to just compexpr.
Well, the part in my comment about 1st-class strings is true for most (if not all) "interesting" (i.e. more efficient than just a few ifs) implementations of switch.
A single switch statement is a lot clearer than a long list of ifs, though. As for performance, there are plenty situations already in which switch devolves into a series of tests which runs at O(n). 
Or just not require opt-in for optimizations that are perfectly safe when used on standard-compliant code. Like we do now. I'm fairly certain strict aliasing has been in the standard from the start, and it certainly isn't new. We shouldn't be going out of our way to fix things for code that doesn't conform to the standard at the expense of forcing all devs writing standard-compliant code to remember to opt into something they should get for free.
Yes, that works fine. In fact it was my experience with doing this that leads me to wanting to further generalize string: the resulting code with a single switch statement over a set of compile-time hashed strings is just so much more readable than the long sequence of ifs that was there previously. All you need is a constexpr hashing function for strings, and maybe a user-defined literal so you can turn strings into hashes with minimal effort. 
Yeah, optimizations should always be opt-out. If you can't fix your code then go ahead and flick some switches and sacrifice optimization. Strict aliasing has been in both C and C++ for ages. People really need to stop blaming the compiler (devs) when their broken code doesn't work. Sure, maybe strict aliasing is dumb according to someone, but it's in the standard and has always been there. People really need to get over it.
To be fair, your example with integers is a special case shepherded in AFAIK. This will not work for example: constexpr int foo(double d) { return d + 1; } const double x = 0; std::array&lt;int, foo(x)&gt;; So maybe you are suggesting that the above snippet should work for doubles, the way it also works for integers? And then, constexpr would only be really significant for forcing static initialization (which, as we've seen, it is not even guaranteed to do for local variables).
&gt;Yeah, optimizations should always be opt-out. To clarify a bit, *safe* optimizations for standard-compliant code should always be opt-out. I'm perfectly fine with something like gcc's -funsafe-math-optimizations being opt-in. But as mentioned above, strict aliasing is not one of those cases, and such optimizations will always be safe for standard-compliant code. Thus there is no reason for them to be opt-out.
I could say the same thing about tuples and structs. Why use a tuple? The answer is that sometimes you may want to combine things into a tuple without giving things names, either the combination or the fields. All `tuple&lt;int, double&gt;` are equivalent, not every pair of structs with those fields are equivalent. If you have a function that either returns a `Foo` or a `Bar` you may just prefer to return a `variant&lt;Foo, Bar&gt;` then to create a new type that the user has to be aware of. You can of course argue that you should always introduce an `lvariant` in this example, but some people will also argue that you should just always define a `struct` and return that instead of a tuple.
That would be too similar to "comparison expression"
[You sure there's any reasoning behind the design of switch?](http://blog.robertelder.org/switch-statements-statement-expressions/)
Solve this: int total_bytes = ...; int n = (total_bytes + 3) / 4; switch (total_bytes % 4) { case 0: do { *to = *from++; case 3: *to = *from++; case 2: *to = *from++; case 1: *to = *from++; } while (--n &gt; 0); }
If you're claiming that these make the switch statement unsalvageable, I ask one question. What practical value do any of these examples have in consideration of how to improve the programming experience? They're ugly, but rarely does anyone attempt do anything like these in the wild even accidentally. Never mind code reviews. Maybe Duff's device was used more coming at one point, but not so much today. Maybe it would be nice to make those illegal from the perspective of the language standard, but I don't see that solving any real problems except for the disdain of a few language puritans.
I don't have time to argue to people who live in a vacuum. Please show me a "standard-compliant code" in a non trivial project: hint; even compilers are not. (And libraries written by expert who basically dedicate their whole life to it do not count -- there are some people who are actually working on useful thing, and can't learn a standard of 1700 pages by heart) Will you fix all the legacy (or not) code-bases that have been written by non language lawyers and have worked correctly according to the intent of their authors for more than ten years or even 20? I'm not saying they don't have bugs. I'm stating the obvious; that we shall live with them! My proposal is concrete and actionable, and would permit to simultaneously profit from some optimizations in a controlled way without unreasonably endangering the whole population (you don't know my application, but it is semi-critical -- however please understand that dangerous (as found in practice) in default optim level will be used by all kind of projects including some people are relying upon for their safety in a way or another) You, wanting to apply dangerous optims even when I precise that it is not possible on certain code bases, have a dangerous state of mind. You are making some laughable hypotheses (that the source code is perfect on some sujects) even when you know that they are false. What even is the purpose of generating code that is fast to crash? I know the source is not strictly conforming, so what? Give me a tool to automatically prove it is in the first place. I hope you do and will continue to stay away from compilers. I hope the industry move away from that kind of madness, by regulation if needed. Sadly, catastrophes are typically needed before the society come to that. 
Were there other keywords you considered?
The compiler is perfectly capable of transforming a series of `if`-`else` statements into the exact same machine code as a `switch` and vice versa. There is zero reason to prefer `switch` for performance reasons. There are different statements because of differing _semantics_, e.g. how scope works inside a `switch` vs an `if`.
While this looks nice, am I incorrect in thinking that the static `lambda_stub` method should be using `std::forward` when returning the result of `p-&gt;operator()`, i.e `return (p-&gt;operator())(std::forward&lt;PARAMS&gt;(arg...));`? Then `PARAMS...arg` would be passed as `PARAMS&amp;&amp;...arg`? The proper usage of stuff like this is still something I'm trying to muddle my way through.
&gt;I hope you do and will continue to stay away from compilers. &gt; &gt;I hope the industry move away from that kind of madness, by regulation if needed. Sadly, catastrophes are typically needed before the society come to that. What a load of hyperbolic nonsense. Are you serious? You want the government to regulate what compilers can and can't optimize automatically, and on top of that, you want those regulations to not match what the official ISO standard says it's correct? Christ... I don't even know what to say to that. &gt;I don't have time to argue to people who live in a vacuum. Please show me a "standard-compliant code" in a non trivial project: hint; even compilers are not. (And libraries written by experts who basically dedicate their whole life to it do not count -- there are some people who are actually working on useful thing, and can't learn a standard of 1700 pages by heart) Sure, people make mistakes, or more often, people ignore or turn off explicit warnings from their compiler. None of that is a reason to not only not strive for writing standard-compliant code, but to actively make it easier for people to not do so. &gt;Will you fix all the legacy (or not) code-bases that have been written by non language lawyers and have worked correctly according to the intent of their authors for more than ten years or even 20? I'm not saying they don't have bugs. I'm stating the obvious; that we have to live with them! Of course I'm not going to fix everyone's code. I'm one person, that's an impossible thing to ask. I wouldn't call myself a language lawyer, buy I have done my fair share of cleanup work to improve standard-compliance of our legacy code at work. Also just because something appears to work, even for a long time, does not make it correct. Take for example a rare race condition. The code may appear to work most, even approaching all, of the time. Hell, maybe it even worked 100% of the time until the standard library stored sped up some function the code in question was calling, and this aggravated the latent race condition and caused the bug to start manifesting itself in a noticeable way. Sure, the code only broke in a noticeable way with the new standard library version, but that still doesn't make the code correct if there's a lurking synchronization bug in there. And it isn't the compiler, standard library, or anyone else's job to fix that code other than the person or team that owns it. A bug is a bug whether it caused noticeable issues previously or not, and failure to adhere to the standard is no exception. It's called undefined behavior for a reason: it might appear to work fine, it might set your hard drive on fire. &gt;My proposal is concrete and actionable, and would permit to simultaneously profit from some optimizations in a controlled way without unreasonably endangering the whole population (you don't know my application, but it is semi-critical -- however please understand that dangerous (as found in practice) in default optim level will be used by all kind of projects including some people are relying upon for their safety in a way or another) We're back to the hyperbolic rhetoric again. The solution is to fix the broken code or to use the optional flag to disable the optimizations in question, not hobble the compiler for everyone else writing correct code. Consider yourself lucky that there's an optional flag to make your broken code work at all, don't expect that to be the default. &gt;You, wanting to apply dangerous optims even when I precise that it is not possible on certain code bases, have a dangerous state of mind. You are making some laughable hypotheses (that the source code is perfect on some sujects) even when you know beforehand that they are false. What even is the purpose of generating code that is fast to crash? I know the source is not strictly conforming, so what? Give me a tool to automatically prove it is in the first place. First of all, without even seeing your code, it is possible to fix, it may just be a lot of work and you don't want to do it. That's fine. Use the flag to disable the optimization and be done with it. Don't expect that to be the default, because everyone what writing correct code benefits from those optimizations. If you do wish to fix your code, turn on -Wstrict-aliasing and -Werror or the equivalents on your compiler of choice. That will cause the compiler to generate warnings for violations of this area of the standard, and treat your warnings as errors so computation compilation of broken code will fail. No (or at least minimal) language lawyering required, as the compiler can run checks for you :) Anyways, have a nice day. I'm not going to respond further in this thread.
I guessed so, but what irritates me in this case is this "scrap parts" language/library design. Ah we have decltype and auto keywords laying around in the garage ==&gt;decltype(auto) Ah in the attic we have this std::monostate that serves as placeholder for std::variant... ==&gt; lets use it in lvariant... :/ S in SOLID means something. :) But then again do you want to block lvariant on standard void... ? :/ 
Latter, yes.
 template&lt;class T&gt; constexpr bool always_false = false; template&lt;class T=void&gt; constexpr unsigned constexpr_fn() { static_assert(always_false&lt;T&gt;); return 0; } template&lt;class T=void&gt; unsigned nonconstexpr_fn() { static_assert(always_false&lt;T&gt;); return 0; } decltype(size_t{nonconstexpr_fn()}) x; // OK, does not instantiate nonconstexpr_fn decltype(size_t{constexpr_fn()}) y; // instantiates constexpr_fn, triggers static_assert So `constexpr` makes a semantic difference.
Related: https://www.youtube.com/watch?v=EVGenON6p9g :)
Thank you. &gt; * This does **not** cause any code to compile that didn't compile before. That will also come in the future So I still can't use branching in a `const fn` yet, even though the new evaluator is capable of handling it :(
It's particularly annoying that there was already a perfectly good monostate in the library spelled std::tuple&lt;&gt; (tuple of no types). 
In what instances, other than a pointer, can void have a value?
The question is why we permit that kind of code in the first place. Some compilers will even crash given a wild enough snippet.
Got ya, thanks!
The reason is obviously C compatibility. I agree with you that ideally we would not allow such things. But I feel like it just doesn't matter in a practical sense.
I feel like misusing the switch statement for non-integer types is top-5 C/C++ mistakes new programmers make. I hate giant `if` `else if` `else if` blocks, so I'd certainly welcome some convenient shorthand. 
Sometime they are, sometimes they are not. I recall refactoring some pretty ugly code, heaps of similarly structured stuff that boiled down to function returning (ptr to base class of) its nth parameter. At that time (granted, it was a few years now) no amount of variadic template magic could outdo a switch (and visual still needed poking and prodding to not emit brain dead code). Eventually i ended up covering the switch with template magic and get machine code i needed got generated. IOW, check the output, if you care for it.
&gt; It's particularly annoying that there was already a perfectly good monostate in the library spelled std::tuple&lt;&gt; (tuple of no types). I actually prefer std::monostate... kind of clearer than std::tuple&lt;&gt; (much easier to google :) )
Ah damn missed that detail. 
so... what would be the problem with `using std::monostate = std::tuple&lt;&gt;` ? 
It would be great if that Gist had links to any LLVM Phabricator tasks related to those steps that aren't yet completed but have the former. (Looks from [here](https://llvm.org/docs/Proposals/GitHubMove.html#what-this-proposal-is-not-about) like the LLVM projects will still be using Phabricator after they [move to GitHub](https://llvm.org/docs/Proposals/GitHubMove.html).) 
Or ` using monostate_t = std:: tuple&lt;&gt;; inline monostate_t monostate; `
That's why we need another construct for OP's needs. Just piling more functionality over `switch` isn't sounding.
Can you test it against raw rapidjson? I don't believe it's 20x time difference, since rapidjson is said to be not much slower than walking a string.
Why should we pander to people who write faulty code, and make life more annoying for people who follow the standard? Sure, it's a hard language, but just bending the rules whenever someone does something wrong will only make it even more of a mess. If you don't like it then don't use it.
That's actually pretty cool imo
In C#, switching on a string translates to either an if-tree if there are fewer than 5 (IIRC) cases, otherwise, it uses a hashmap and indexes to the proper case.
Do you know what's the state of it?
I was talking to world without std::monostate, IDK if it matters if it is a "typedef" or real type. 
&gt; std::function can be made as fast as this. But it will require a interface like std::variant to retain the type of callable object. well, then it wouldn't be std::function at all anymore. 
1) shirt, camera and lighting do not agree with eachother :) 2) maybe I missed it, but it does not mention that set_bla actually works on multisets also(aka sorted elements that may contain duplicates), there is example on [cpp reference](http://en.cppreference.com/w/cpp/algorithm/set_difference). 3) this is not a topic for introduction video but there is no nice way to do inplace_set_difference using set_difference. Other than that nice video...(although I prefer blogs I appreciate any c++ content that is good). Other than that nice video. 
I didnt know how much i needed this.
&gt; I think the reasoning behind not switching on (say) a string is that it would be less efficient than a switch, because it would have to be a list of if()... else if()... and so on. But isnt a switch equivalent to that in the worst case anyway? At worst performance will be the same, but look better(especially where fallthrough can be used), and at best be optimized better and *still* look better.
&gt; I hate giant if else if else if blocks, so I'd certainly welcome some convenient shorthand. how about a `switch`?
Are you trolling, or being serious right now?
I am not arguing with that at all.
Your original reply doesn't necessarily read the way you mean it. 
Those kind of language constructs will not be found because switch-case instruction is used when efficiency can be achieved. For other cases, in case where complex matching is needed, but there are lots of matching probably modular code design through sub-procedures or object methods will be used. **So switch-case is an instruction used not for structuring the code but for performance.** 
You could always use the preprocessor to alias something to `goto`. That way you can still easily distinguish the very specific use case you want.
I'm not sure how else it can be read - what do you mean?
It would require a comparator for non-trivial types, so you cna have a case-insensitive or unicode-aware comparison. But yeah, it's one of the non-intuitive corners of C++ that needs to be fixed. 
Took me a while to figure out you meant "*trying to use* the switch statement for non-integer types" - and now I can't unsee it. The first read had a strong undertone of *"using switch for non-ints is a misuse"* - at which point I wondered if *you* are trolling.
It's nice to switch over a list of enums, then you can enable error if an enum isn't used. 
Man, I really wish this supported inline struct declarations instead of having to declare it separately. Seems like that could really clean up using this for ASTs.
Yes to all of this.
UWP apps can be distributed outside that store.
I have started learning some more systems level programming languages (mainly C++ and Rust) and found this talk (as well as the [guidelines](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md)) to be a big help in writing cleaner, more performant code. Full disclosure - I made the site this video is linked on ([Talkery](https://talkery.io)) 
&gt; https://github.com/hun-nemethpeter/clang/commits/typid_ast That is great! Keep it up! 
&gt; fwiw i think there a lot of communication among modules implementors. Indeed. Now and then, I would read theories on The Internet that things don't progress on modules because modules implementors don't talk to each other. Nothing can be further from the truth.
&gt; I think the issue is many people don't understand the constexpr context part. Probably. One thing I would like to understand, as a tool builder, is when NOT in a constexpr context why does it matter the call is also evaluated as if in constexpr context? Note: I am not excusing poor compilers that do not take advantage of constexpr-ness of a function. I am trying to understand when and why it is an issue to raise as a language defect. What programming problem does it involve? 
Consider changing the new line escape sequence so that users can manually edit Windows file paths!
It wouldn't have fit the attribute framework: if attributes are erased from a program, the resulting program should be valid and should have one of the allowed semantics of the original program. `constexpr` functions were introduced because you couldn't use functional abstractions to create constant expressions. We are back to the original claim that `constexpr` was a hint: It is not.
&gt; That we arrive at new wisdom is a consequence of that growth. Sure. It is a concern when the general process is a pattern :-)
No, no other keyword was considered - as far as I can recall. Keyword bikeshedding was a luxury you could afford (at the time) when there was a shared agreement on the semantics of what was being proposed. Plus, the spelling wasn't sufficiently atrocious enough to distract from the task of getting compiler writers to agree to the notion of compile-time evaluation :-)
Yeah, what I meant is *if* it was a hint. I know it's more than that.
Most variables with automatic storage are initialized at runtime -- as they should be. Enforcing static initialization - either for non-local variables or static local variables - is something quite often misunderstood and regrettably under-estimated. :-/
Well, current gen consoles have considerable amount of (V)RAM anyway. Considerably more, than the previous generation. at least
This sort of thing is standard in Javascript. https://en.wikipedia.org/wiki/Immediately-invoked_function_expression
Sure you can, you just cannot call that non-constexpr function in a code path that is executed in a constexpr context.
It's hard to make a solid recommendation without knowing what you really intend to accomplish. That said my first choices for the task at hand would probably be RESTinio and Crow. Both provide relatively high-level routing, so you can set up the end-points you want and what each should do, and from there tell it to run. Both are fairly fast, support websockets, etc. Choosing between the two is harder. I find Crow a little easier to use at least for relatively simple work, but RESTinio seems to me a little easier to deal with as the task becomes more complex and (for example) more of your handlers end up being asynchronous. As far as the rest (no pun intended) go: Redis: I've used Redox quite successfully. MySQL: I've avoided MySQL quite successfully. In my experience, for any situation in which MySQL might be acceptable, PostgreSQL will be at least as good and usually better. If you don't need SQL/relationalism, there are lots of other choices that may be superior (depending on what you need). Just for example, I've used lmdb quite successfully, but it's a *much* smaller, simpler sort of thing than almost anything that does SQL (though, there is a version of SQLite using lmdb for its storage engine, which gains quite a bit of speed over the normal SQLite, at least for some kinds of tasks). 
actually no. Not sure about gemv but if it's true, it's either you configured blaze that way or it's a bug(not yet implemented). Blaze's interface should work at least as good as Eigen. Though lack of Tensors is a true minus
WTF are you talking about? 
Fucking clueless C++ coders, half the shit posted here is fucking retarded. 
IMO it's a shame that there is no standard~ish delegate around anywhere, although you can find dozens of half-baked ones on Github. A delegate has an advantage of not ever throwing or allocating, and generally behaving deterministically, which makes it ultimately more suitable for embedded and realtime code. And no, one doesn't need to do everything that std::function does, but delegate on its own is a very useful construct. Definitely beats passing around raw function pointers as most any embedded API does. 
It's an application that trades stocks. That's why it needed to be over engineered in a positive way since I don't want surprises. I'm very proud of it performance wise. I've built it piece by piece, but as I said, it's become boring and new APIs I'm going to use aren't web APIs, so it turned out I will have to move into C++ anyway. Btw, I'm the only user, so there are lots of possibilities. Thanks!
Just a complaint: I bookmarked the video, and the page title (and so the bookmark title) is "Talkery", not the title of the video. That's not helpful. Good luck with your site!
Hey thanks for the feedback! Literally noticed this today when I posted the link, I’ll fix that one up shortly. 
&gt; As it stands, the idiomatic method of breaking multiple loops is with a goto, but this forces you to stop thinking in a highly controlled way. Lambdas can be quite costly in term of performance if you use a weak optimization flag. You have better guarantees with goto at this point. 
I really like your blog, and the video is a great addition. I like that you take the time to explain the concepts and that you include diagrams. Will the next video take the discussion into the applied space? That would be great. ( For what it's worth, I was awfully surprised to hear you have an English accent. I expected you to sound Italian. )
I don't know much about compilers and optimizations but I'd say that the tight loop is in the lambda, and the lambda isn't in the tight loop. To our knowledge, if the lambda doesn't get inlined, any overhead should be a one time thing, and I would assume that to be minimal compared to the work going on inside this lambda.
You can always fork clang/gcc and write this yourself :)
Sorry, I didn't express myself properly. It's not that it "can" have a value, more that it "behaves" like a value. template &lt;typename R, typename... Args&gt; R forward(R(*f)(Args...), Args... args) { return f(args...); } void foo(int) {} int main() { forward(foo, 1); } Works, as if `void` was a regular value type like `int`, but: template &lt;typename R, typename... Args&gt; R forward(R(*f)(Args...), Args... args) { auto result = f(args...); return result; } fails because `void` is not a regular value type. This is really annoying.
Ctrl+F [youtube link](https://www.youtube.com/watch?v=XkDEzfpdcSg)
I would call it `std::function_ref` to stress the parallel with `std::function`. Other than that, I agree it could be a nice addition. Now we just need someone willing to spend time on making a standard proposal and defending it...
[older thread with link to youtube](https://www.reddit.com/r/cpp/comments/772hc4/cppcon_2017_kate_gregory_10_core_guidelines_you/) [direct youtube link](https://www.youtube.com/watch?v=XkDEzfpdcSg)
Sorry, I'm a little confused by your question, perhaps I'm not understanding something about the feature? Apologies for the length. My understanding is as such (using examples from above): Given a constexpr Fibonacci function, this: constexpr int i= Fibonacci (20); std::cout &lt;&lt; i; The variable i is not guaranteed by the standard to be initialized to the result of that Fibonacci call at compile time. The Fibonacci call _might_ happen at run time. Is this correct? The standard has requirements for that declaration of i but provides no guarantees beyond that were i later used in a constexpr context, it would be fully evaluated (as in Fibonacci computed) at compile time. Is this correct? If so, given this: constexpr int i= Fibonacci (20); static_assert(i); std::cout &lt;&lt; i; The variable i should be initialized to the result of this Fibonacci at compile time. Fibonacci will _not_ be called at run time here. Is this also correct? Given this understanding, I'm interpreting your question ("is when NOT in a constexpr context why does it matter the call is also evaluated as if in constexpr context?") as such: Why does it matter that the standard make requirements on a constexpr variable even if that variable is not used in a constexpr context? Why is this causing users confusion, making them think the expression _will_ be computed at compile time? If this interpretation is correct, well I don't feel like it's a _bad_ thing that the standard has requirements here, it's great that my compiler tells me when I've violated one of the conditions of a constexpr variable. I think the only bad thing is that the compiler doesn't _require_ these expressions to be computed at compile time. You mention that you don't excuse compilers that don't take advantage of constexpr-ness of a function, but if they're not required to in some cases (by my understanding) how can we blame them? If I misinterpreted, I apologize, I just want to make sure I'm on the same page.
Damn I searched the URL before I posted but it didn’t come up, guess I searched the wrong one! Thanks for the links though, I’ll add them to my comment as well
I don't like switch in the first place. It is an unstructured construct and switches look ugly most of the them. I never know whether to indent the cases or not (not -&gt; looks ugly, yes -&gt; wastes space). If I want to enclose all cases in blocks (on principle) it looks even worse: switch (x) { case A: { // code } break; // do I add an empty line here?? case B: { // code } break; } I've gotten used to just writing "if, else if, else if" even when most people would use switch. If I need to do the same for multiple cases, I use a helper that allows this: if (x == OneOf(Foo, Bar)) (https://github.com/ambrop72/aipstack/blob/master/src/aipstack/misc/OneOf.h).
Typically there is something between 8 and 16 bytes header allocated by your runtime + the memory block is usually machine-word-aligned (that's another 0..7 bytes added to align it properly). So when allocating 1 byte i can safely say that you allocate between 16 and 24 bytes of system memory. In debug build there can be additional small paddings before and after your block, anywhere from 4-8 (Visual Studio) up to 32 bytes each side of the block and on some systems is configurable to be even higher (Android). Is called No-man's land here http://www.nobugs.org/developer/win32/debug_crt_heap.html#table and is used to catch off-by-1 errors.
GSL provides so-called not_null pointer. How is it different from a regular reference, from usage point of view?
It could even be a metaclass with unspecified implementation, like how vector's implementation is unspecified (ie in theory could be magical)
* There are versions of the malloc functions withing the library that support arbitrary alignment. The default 16-byte alignment however serves 99% of the requirements we have in our code. * The user might have the "free" data required, but just the size and alignment are seldom enough bookkeeping data and imposing such requirements on a generic allocator seems too much to me. Specific allocation strategies for other memory is emplyed that saves bookkeeping when necessary - linear, slab allocators etc. * Some third-party libraries use it, which is somewhat annoying, so it is supported in the coherent allocator * I haven't considered it. The vector being aware of the allocator or vice-versa is a very interesting concept that I'm exploring as well. Facebook's folly library has such a vector implementation that is aware of their jemalloc allocator: https://github.com/facebook/folly/blob/master/folly/docs/FBVector.md 
There used to be nice way to ask Win how much memory you can get withoout realloc, but somebody had "nice" idea to kill it... https://blogs.msdn.microsoft.com/oldnewthing/20120316-00/?p=8083
What does "l" in "lvariant" mean?
I wasn't aware of the Core Guidelines - so needless to say, I'm looking it up and enjoying Kate Gregory's talk. It's nice to see that many of the core guidelines are things I've actively encouraged myself to do and to have written into coding guidelines. Almost 20 years of professional experience can be a harsh teacher :)
reference does not have a not in the name... :P other than that I assume it is used for APIs where rest of the code expects pointers so you can add it more easily than a reference. I personally dislike it but I it is better than raw ptr.
See [GSL views](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#SS-views) in the Guidelines. Basically, a `not_null&lt;T&gt;` tells analysis tools that T can never be be null. Compilers don't enforce this yet, nor does the type system, but the GSL is (slowly) making its way into the standard library. 
Because of `auto`, that advice could become problematic.
On its own, Asio is pretty low-level to use. I'm happy it exists because it's one of the few well-supported cross-platform networking libs in C++. I'm more HTTP-focused so the addition of Boost.Beast has been amazing for me.
Rust does it really well with Cargo.
I thought I recognised the name, Kate Gregory did another good video a couple of years ago called."Stop Teaching C". She has a good teaching style
I have a WIP of a sort of framework to help with this: [Nbdl](https://github.com/ricejasonf/nbdl). It has websockets and a means of organizing data in stores. It can even do front end rendering.
Seems there already is a [paper](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0792r0.html)
Great!
Right, I forgot we are on reddit. ;-)
There are so many layers entangled here. Popping back to the toplevel, the question I would ask is: "Are you asking from practical programming perspective, or from language lawyer perspective?" If asking from language lawyering perspective, one can invoke the "as if" rule to make up just about any devious implementation strategy imaginable. And we would never be done. If asking from programming perspective, then I fear there are a few disconnects here. &gt; The variable i is not guaranteed by the standard to be initialized to the result of that Fibonacci call at compile time. The Fibonacci call might happen at run time. Is this correct? That is not correct, from programming perspective. The variable is declared `constexpr`, initialized with a constant expression, and will be initialized by a constant (no call at runtime) for all practical purposes. It is correct from language lawyering point of view. &gt; The standard has requirements for that declaration of i but provides no guarantees beyond that were i later used in a constexpr context, it would be fully evaluated (as in Fibonacci computed) at compile time. Is this correct? From programming perspective, that is not correct. From language lawyering perspective that is correct. &gt; The variable i should be initialized to the result of this Fibonacci at compile time. Fibonacci will not be called at run time here. Is this also correct? From programming perspective, that is incorrect; from language lawyering perspective, that is correct. &gt; Why is this causing users confusion, making them think the expression will be computed at compile time? No. From programming perspective, the expression will be computed at compile-time (baring a bug in the compiler). From language lawyering perspective, I can understand why they would be confused but it is confusion they would bring upon themselves. &gt; I think the only bad thing is that the compiler doesn't require these expressions to be computed at compile time. The standards text has strong requirements actually. For programming, that matters and those requirements are observed. For language lawyering, it doesn't matter - determined language lawyers will find something to argue about, using the "as if" rule as the ultimate justification. &gt; You mention that you don't excuse compilers that don't take advantage of constexpr-ness of a function, but if they're not required to in some cases (by my understanding) how can we blame them? I am not blaming them - I just don't excuse them. Eventually, it is up to the C++ community at large to nudge compiler writers about what they care most about. Optimization is, by definition, a program transformation that preserves the observable behavior of the program. You can't meaningfully legislate it in the standards text because of the "as if" rule. Consequently, one has to leave it up to "the market" to put pressure on compiler vendors.
&gt; From programming perspective, that is not correct. From language lawyering perspective that is correct. -- &gt; From programming perspective, that is incorrect; from language lawyering perspective, that is correct. -- &gt; From programming perspective, the expression will be computed at compile-time (baring a bug in the compiler). From language lawyering perspective, I can understand why they would be confused but it is confusion they would bring upon themselves. This strikes me as condescending rhetoric. I am a programmer who wants to understand and ultimately know the language I'm working with – why are there two different narratives with different answers? Which one applies to me? Neither, it sounds like; my trying to understand the standard isn't bringing confusion upon myself, but reading this thread certainly is... ;-]
Well it's implementation-defined, which is a nice way of saying "none of your business." The runtime implementation can do what it wants, using pools or whatever, so the allocation call doesn't necessarily hit the Windows APIs. On Windows, the actual allocation depends on the options set for the system and for the particular executable, of which there are more than a few: [https://imgur.com/9AFLnn6.png](https://imgur.com/9AFLnn6.png) 
http://www.gotw.ca/publications/mill14.htm Scroll to the heading "I'll Take 'Operator New' For 200 Bytes, Alex".
Yes, this subreddit is for pedantry, not beginner's guides. &gt;_&gt;
In any modern(ish) OS, memory is managed by the OS in units of one "page". Page sizes are a property of the hardware, but on all common architectures they're 4KB. That means that an application's heap will always be a multiple of 4KB. Exactly how malloc/new works depends on the details of the runtime library, but in general, it will check to see if there is space in the current heap before requesting more memory from the OS, so such a call can result in the heap being expanded by anything from zero bytes (there's a large enough unused block in the current heap) to the number of bytes requested, the alignment padding, plus a few bytes for the runtime's accounting rounded up to a multiple of the page size. As for padding, the C standard says (for malloc-and-friends): &gt; The pointer returned if the allocation succeeds is suitably aligned so that it may be assigned to a pointer to any type of object with a fundamental alignment requirement and then used to access such an object or an array of such objects in the space allocated (until the space is explicitly deallocated). In practice, this generally means that the pointer is aligned to at least CPU's word size. For C++, "new" knows the type being allocated so can (in theory) allocate to the exact alignment requirements of that type. In practice, "new" is often just a thin wrapper around "malloc" so will allocate to the same alignment regardless of the type.
This particular part of constexpr is of particular interest to me. There's a couple of latent bugs in gcc where it refuses to compile a constexpr function because it does not "see" that there is a valid code path. But I've also really enjoyed reading the history of constexpr here. I did not know that you were behind the original proposal.
OK, you just answered the question you asked elsewhere.
I guess it stands for "languages" as in "language variant" opposed to `std::variant`. I don't like the name either but don't think it matters too much.
`constexpr` evolved from a very simple idea that every programmer could understand to an encrypted text that I am not sure I understand anymore ;-) Consequently, my advice - from programming perspective - has been to refer to the constexpr paper along with Richard's relaxation. While it is historical, it still remains simpler to understand from programming perspective. I fully appreciate it does not help much the implementer who must absorb all the complexity of the standards text. We are no longer at a place where we can understand a feature by reverse engineering a textual interpretation of the standards text.
If you feel 'condescending', that feeling is misplaced. If you feel 'rhetoric', that feeling is misplaced. The overall message was simple: how to effectively program with a C++ feature is no longer attainable by textual interpretation of the standards text. That is not rhetoric, that is not condescendence. If you are a programmer who wants to understand how `constexpr` works, I suspect the [constexpr paper](http://www.axiomatics.org/~gdr/constants/constexpr-sac10.pdf), along with [Richard's relaxation paper](http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2013/n3652.html) is all you need. The overriding principle in understanding the relaxed model is that "you can initiate a computation from a constexpr context, that computation can do any form of mutation it wants, as long as it stays in that context, and when it comes back it shall yield a value plus potentially a temporary that does not over-lives the contexpr context". The intent is to give you a way to ask the compiler to evaluate an expression at compile-time in certain contexts. When you go over that fence and go into the land of the messages I previously answered, I am not sure we are still in the land of programming. If you can only accept that as condescendence, then I am sorry you misunderstood what my message is about.
Looks like my implementation also is vulnerable to this :(
While this is a good breakdown of what the standard library will use the operating system itself will hand out 1 page (4KB on most systems) https://en.wikipedia.org/wiki/Page_(computer_memory)
&gt;Any possible update on an ETA? Not at this time; will be sometime after finishing '17. &gt;/u/STL also mentioned releasing new dlls alongside current ones, might that occur for this? New DLLs will allow us to effectively "add exports" but won't solve packaged_task's issue; as the layout of that comes from the header and is compiled into everything built with the current toolset. You have no idea how much I want to ship the redone concurrency support I spent months working on as much as you do. :/
Great talk. I must say that in general I find the examples a bit "class-heavy", I would prefer intro talks/examples to throw around more lambdas and light-weight data structs and ease up on the classes with state. I don't agree with the section about casting away const-ness. The moment you have mutable variables the signature of a const function alone is not sufficient to reason about what might happen. I've been bitten by this big-time when parallelizing a suite of algorithms, one of the algorithms had internal mutable state like this which I failed to spot, *boom* in rare cases we got different results when using multiple threads. I prefer requiring the cache to be passed in as a function argument, either mutable or even better by-value and return the mutated cache. I don't like this particular example as a whole I must say, the first question I would ask myself about this class `Stuff` is if it really needs to be mutable because mutable state should always come with an exclamation mark.
I thought it'd be a fun project to compile clang to wasm, since its wasm backend is well on its way. Write, compile, and run limited C++ all on a single page, with no server support.
That would be horribly inefficient to do for each request. The run-time will create pools of memory (generally at least 2, one for small allocations, and one for large) and give out addresses within those pools. The run-time will ask the OS for more memory only when it runs out of space in a pool. E.g. [see this](https://social.msdn.microsoft.com/Forums/vstudio/en-US/b64242c9-c140-44b5-8ae0-0674535d866a/new-and-virtualalloc?forum=vclanguage) &gt;haven't looked into the specifics there, but it certainly isn't as simple as "HeapAlloc calls VirtualAlloc". As I said, VirtualAlloc will allocate multiples of 4 kB blocks, which would be horribly inefficient for most allocations. If anything, HeapCreate uses VirtualAlloc to create a pool of memory, but I wouldn't jump to any conclusions there either. And in either case, it really doesn't matter. What you wanted to know is whether or not "new" uses virtual memory, and the answer is Yes, all memory allocations on the Windows platform (at least win95+) are made to virtual memory -- not straight to RAM. What the API is called makes no difference to that.
You're some kind of crazy madman and it's awesome!
FWIW I dislike class member initializers... It spreads the code too much... like I have constructor that initializes 3 members, other 2 are initialized where they are defined... Barf
So it's "language variant" as opposed to "library variant ". You can see that "l" is not a good mnemonic. 
This is quite awesome. Great work!
Hierarchical contexts also allow you to keep your logging code short and to the point. You pass in relevant state info once when you construct a context instead of passing it every time you make a log call.
You absolute madman!
What's a "weak optimization flag"? An immediately evaluated temporary lambda is literally the most brainless inlining decision a compiler can ever make. It will not get inlined at -O0 and I suspect that's it.
`switch` statement operating on value other than scalar is right there next to non-type template-argument of non-scalar literal types. I've read theories of impossibility of something, but I don't what that is. Additionally, you need to a good constexpr hash-function that the compiler would use at compile-time to avoid degeneration into linear `if-else` staircase of death. It is not impossible; it is not lack of love; I floated the idea a few times in the C++0x era; like everything C++: slack of time and need to prioritize.
I actually use this all the time, most of my coworkers do, so I think it's well on its way and it should 100% be an idiom. Often called Immediately Evaluated Function Expressions (IEFE), they also allow making variables const that otherwise wouldn't be, and avoiding pointless initialization. E.g.: int x; try { x = foo(); } catch (const std::exception&amp; e) { x = -1; } Versus: const int x = [] { try { return foo(); } catch ( const std::exception&amp; e) { return -1; } } (); If you think it's not clear you can always write a trivial wrapper like template &lt;class F&gt; auto evaluate(F f) { return f(); } const int x = evaluate([] { ... });
Is it possible to pass command line arguments in any way? Specifically, I'm trying to compile with -O3.
Prior to 11 I would have agreed with you, but 11 it's nicer to just use a lambda and return. It's easier to read and more predictable and easier to maintain. goto's have their uses but it's very rare; mostly only for ultra high performance code.
Try dlib. It’s specifically meant to be used from c++: http://blog.dlib.net/2016/06/a-clean-c11-deep-learning-api.html
Wonderful, thank you!
Not yet. I bypassed the command line parser to stop it from linking in too many dependencies; the clang frontend supports much more than compiling.
Nice post and video. To avoid confusion, it might be worth stating that `std::merge` differs from `std::union` in that `std::merge` keeps all elements from both sets - duplicates included (the video doesn't explicitly point out their differences).
If you're new-ish to C++ or not super experienced with it, learning Asio and Beast might be pretty hairy. There might be better libs out there that have lower learning curves.
Yeah, glibc (on *nix) has several different pools. It has the primary pool that is extended with calls to ```sbrk```, and there are other arenas created with ```mmap``` when collisions between threads start to grow. I also believe it will just use ```mmap``` directly for large allocations rather than going to a pool.
It doesn't give a warning. If initialization is skipped, it will print an _error_. If the variable is declared but not initialized, then nothing is printed.
&gt; The problem was that the overflow was being checked, and triggered an abort. I think that Rober's library includes the possibility of guaranteeing no overflow ever happens, although obviously it is impossible to do this for all use cases. Not sure how this works, but you know more about using types than me... :) https://github.com/robertramey/safe_numerics/blob/master/example/example7.cpp
Hey, I heard you like compilers
Others have already pointed out traditional header costs, but it's also possible for allocations to have nearly zero size overhead. This is because the only information a malloc() style allocator needs about an allocation is its size. A trick that some allocators do is to reserve blocks of memory for arrays of slots that are all the same size; any allocation whose address falls within one of these blocks is implicitly of that size. Therefore, all that is needed to track them is an address lookup structure for the blocks. This means you can't even expect one byte of header overhead per allocation; it's possible for the overhead to be well below one byte on average. Note that even HeapAlloc() can vary, because there can be at least three different main heap implementations backing it: the standard heap algorithm, the low fragmentation heap, and the fault-tolerant heap. These are selected based on process and heap settings. Thus, the behavior won't be consistent even with a specific version of Windows. It really is unspecified, implementation-dependent behavior. 
Props for using AWS spot instances for the build process. That wouldn't have crossed my mind.
Are you sure you had optimizations on? Compiling with clang and `-O2`, it looked like all the functions compiled to the same assembly. Similarly with MSVC and `/Ox`, but the assembly was harder for me to read, so I wasn't entirely sure if they were the same
Code was complied with -O2 in clang++ and g++. But I am using Cygwin.
Can moderator unban my thread related Set Algorithm? I believe it is shadow-banned. https://www.reddit.com/r/cpp/comments/7n1kfo/pythonic_operators_on_stl_set_algorithms/
All functions are accessing uninitialised data.
The functions are simplified for you to copy to Godbolt. If you check out the benchmark code, they are initialized, else they will not compute the same sum.
Personally, this looks like over-engineering to me. In practice, embedding labels in the log content, and just using grep to filter accomplishes the same thing, while keeping the logging API simple. 
There could be another drawback: void do_something(unsigned int id, std::string tag) {...} ... do_something(params["tag"], params["id"]); // It is a mistake This mistake will be detected only at run-time (because an attempt to convert string value of "tag" into unsigned int will fail). But in such case: do_something(restinio::cast_to&lt;std::string&gt;(params["tag"]), restinio::cast_to&lt;unsigned int&gt;(params["id"])); that mistake will be caught at the compile time.
These are all good points and perhaps it is doable, but there would be a non-negligible amount of wrinkles to iron along the way. Suppose you have a class that implements a comparison operator that accepts (say) int, I don't see how a jump table *or* C# like hashes can be used. Suppose further ClassA implements comparison operator to ClassB, and ClassB implements a cast operator to ClassA. Maybe even ClassA implements cast to ClassB. To keep it from being boring, let's pretend that some overloads (const/non const, ref/non ref) are implemented. How would switches be resolved? In general, I fear that this would be a potential source for many gotchas of the sort that C++ is infamous for. Personally I would vote to keep it simple. 
goto's are absolutely fine and anybody saying otherwise have never read Djikstra. 
Basically, because you don't want to keep an old compiler that you know compiles your code without exploiting UB, and you don't want to invest the work in opting out, you want EVERYONE ELSE to invest their work for their correct code. Do you really not see why this is laughable?
It even works alright on my phone with a Snapdragon 810. Incredible.
I imagine that would work the same way as equality comparison gets selected today. Fundamentally it can be translated into the equivalent series of if/else if/else statements. Using hash-functions, when available, is an excellent idea, and one that had not even occurred to me. Given that people are working on constexpr containers I imagine it could work, and give compilers the option to generate a hash lookup instead of a series of if statements if appropriate. 
why -O2 and not -O3 ? the time where -O3 routinely produced buggy code was.. like.. two decades ago
I cannot immediately figure out what this proposal will let you do. It seems more like a type-based matching system, but can it also do variable-based matching? One use case I run into quite a lot is this: struct base { bool operator== (const base &amp;other); }; struct derived_T1: base; struct derived_T2: base; (this is part of a typed column identification system for a rather advanced grid control, and in reality it has more code to it) constexpr derived_T1 Col1; constexpr derived_T2 Col2; constexpr derived_T2 Col3; // note: same type as Col2 void value_changed (const base &amp;Col) // I'd love to be able to do this: switch (Col) case Col1: ... case Col2: ... case Col3: ... } } This is currently not possible because neither Col nor Col1..3 are integral or enum types (even though they are really just wrappers around int, and support equality comparison, so there is nothing fundamental that stops it from happening). Would the new proposal be able to do this? Also, what are the odds of it actually making it into the standard? It looks intriguing, but I see a lot of new (and compared to the rest of C++, strongly deviating) syntax... 
Just tried -O3 with g++, it produced vectorized code and result is on par with clang++.
Why are you using such old versions of clang and gcc?
why not `std::function_view` to be consistent with `string_view` ?
There should be a link on the sidebar to message the mods.
you are thinking of switch as a series of if/else if statements and syntactic sugar. it isn't. it's a jumptable or goto/label structure. it's like the word ”semitic”, as in "anti-semitic”, does not refer to or mean ”jewish”. it has a very specific meaning involving a language family or people that speak languages of that family, including both hebrew and arabic. of the actual, scientific definition of the word "theory": it has a meaning that a lot of people don't get. switch does what it's designed to do. just because you *can* do ugly things with it doesn't mean it's wrong. c/c++ is an *expert* language; trying to *sanitize* it is wrong. it was never meant to prevent idiots from idioting, or clever optimizations.
If someone has a list of all the C++ conferences (/u/meetingcpp maybe?), I'd love to have one.
It's one of the alternative names proposed. Sounds weird to me, but to be honest I just don't care that much about the name.
This... This... This is madness! Cue Escher's artwork; there's bound to be one with some infinite recursion...
Security-wise... I really like the idea from the "compiler-as-a-service" point of view. Websites like godbolt have to really tighten their security, to avoid malicious users from taking over the server by exploiting the compiler. They are also subject to DOS attacks, or simply enormous success and the accompanying bills. By delegating compilation to the malicious user's machine, you niftly bypass any such issue; that's genius. I really like the idea of the code never leaving the user's browser. It means it's possible to copy/paste sensitive code or data and try it out without first obfuscating it. It's a niche requirement, but nice. On the other hand, I am afraid that allowing "sharing" code via this kind of technology could open up a whole host of nastiness for the user. Webassembly code can do a lot on the user computer, and therefore a malicious snippet could be used to "infect" them, or gather information, etc... turning the snippet into malware/spyware/...
We need `std::is_bitwise_equality` or somesuch.
Just use it to compile clang yourself.
Now we can compile clang in the browser!
Can we put all language elitism aside for a second? C++ is great, but certainly not designed for large-scale server applications. That’s just how it is. And unless you have some super tight real-time requirements you may want to consider using a more suitable language like - for instance - Golang.
https://isocpp.org/wiki/faq/conferences-worldwide/
your for loop with index is using unsigned integer, which is a slower version compared to signed. You should better use int for your comparison. Reference: *When a loop uses a signed integer index, the compiler can do much better when it doesn’t have to consider the possibility that the index will overflow.* https://www.airs.com/blog/archives/120
Yeah, but being nasty for someone is not the same for another one, especially when you're writing in your second or 3rd language. I don't think the c++ community is a bad one but there sure are a lot of programmers who see's c++ as "just another language" but it is not. You actually need to know what you're doing in order to do it correctly. I've come across so many C++ wannabe that rated themselves as senior programmers who did some very shady, buggy and ugly c++ code.
We should just write a wasm interpreter. Then compile that to wasm and run the code in that instead.
AFAIK the previous block pointer is a debug mode thing only.
It's usually aligned to greater than word size so that doubles have natural alignment and so instructions like cmpxchg16b can work.
Bringing us one step closer to [the death of JavaScript](https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript).
I don't think this is true when the size of the data you iterate over is known at compile time
I don't like getting snow on me, so you propose spraypainting the snow yellow and pretending it is sand.
And the code is inline with the rest of the dunction logic, not possibly pages away.
Could you add an option to show compiled WASM as text?
Algebraic data types do this: const int x = expected_eval( [&amp;]{ return foo(); } ).on_error( [](auto&amp;&amp;)return -1;} ); or something similar.
A quick experiment on godbolt shows there is no difference in generated assembly for signed or unsigned loop variables. If you stop and think about this for a second, this makes perfect sense: the instruction that may overflow is the addition on the loop counter. If the loop counter is signed, this overflow is UB, so the compiler doesn't have to care about the result. And if the loop counter is unsigned, this overflow is defined to _exactly do what the processor will do in case of overflow anyway_, so it doesn't have to take any particular precautions either. This is why there is no performance benefit to leaving signed overflow as UB: processors do have a behaviour associated with it, and this behaviour is exactly identical on every CPU made in the last four decades. C++ can simply adopt it, and it would change nothing in terms of optimisation opportunities. 
I should probably add that. For now you can use the save button, then load it into https://cdn.rawgit.com/WebAssembly/wabt/aae5a4b7/demo/wasm2wat/
Yeah usually ADTs have a `value_or` method, or something similar, so you would just do `foo().value_or(-1);` which is obviously much nicer. Just an example though; another example is e.g. populating a vector and sorting it, but then having it unchanged for the rest of the scope. You can populate &amp; sort inside the lambda and return it, allowing the outer vector to be const.
Is there a benefit to using std::bind vs a lambda?
I think the problem is that it is such a small/simple dataset. If it was significantly larger it might not have gotten stuck in a local minimum/maximum. quality and extensive training is key. If all you do is teach it to tell if a dog is dead or alive, you may be surprised it tells you a cat is both alive and dead at the same time.
A WebAssembly instance can’t call out to any function that wasn’t provided as an import or is in its Table. So you could choose to have a very strict policy where the instance has no imported functions. The worst that the instance can do then is go into an infinite loop. Realistically you’ll want to provide more functionality to an instance, but even then it will be limited by what is possible to do in the web platform (which is quite a bit, but still fairly restricted.)
Would be interesting to compare this to the built-in libraries in macOS (dispatch) and Windows (concurrency).
I think you just outlined the reason why `switch` is awful (or, to put it objectively, unintuitive) and yet cannot be amended.
i outlined why it's not a series of if/else and why teaching it this way is incorrect. think of it as a switchboard, hence the name. it's much more accurately emulated by an array of function pointers and calling the *n-th* function. i would heartily support a ”multi if” or ”pattern matching if” syntactic sugar, as it would make life a hell of a lot easier...but it's ultimately *not* switch. just because it was unintuitive to *you* doesn't mean it's unintuitive to everyone else, and basing a language construct completely on intuition is a terrible idea, as it's a piss poor metric and relies heavily upon the background and education of a ever-changing userbase. if we relied on this type of thinking, every tool would be a hammer, and everything would be written for the least common denominator. programming, like life, has nuance, and to eliminate it because it's not immediately understood is a thing of greatest stupidity. 
Thanks for the summary post. Some issues: * It's almost 2018 - your website should use HTTPS. * "things that happen" should say "happened", twice. * "Before we dive into newest stuff" should say either "the newest stuff", or "newer stuff". I don't know why English works like this (I think it's because "newest stuff" sounds specific so it needs an article, but "newer stuff" is less specific so it doesn't need an article; "the newer stuff" would also be correct). * "we knew it’s full form" should say "its". * "You can also download a free version of the last draft: N4700, 2017-10-16, PDF." That's a C++20 draft, containing things like C++20 [endian](https://wg21.link/p0463r1). [N4659](https://wg21.link/n4659) is the draft corresponding to C++17 (modulo editorial changes, and defect reports that were voted in later, a few of which are significant). See [N4661](https://wg21.link/n4661) for the editor's report confirming this. (In my own work I don't actually care about C++17; I refer to the current WP with knowledge of what's in 17 versus 20, so I don't need to worry about missing issue resolutions.) * "Full version, and up to date can be found @cppreference" - I guess you've copied these table cells from them, but somebody should go fix it. Specifically, referring to MSVC as "19.0", "19.1", and "19.5" is incorrect (and yeah, our versioning story is a dumpster fire orbiting a supernova). Either IDE versions (e.g. 2015 RTM, 2015.2, 2017 RTM, 2017 15.3, 2017 15.5) or compiler versions should be used (which have the advantage of being more uniform, although less helpful for the old 2015 series). The mapping is: 2015 (and all updates) was compiler 19.0, 2017 RTM was 19.10, 2017 15.3 was 19.11, 2017 15.5 is 19.12, and 2017 15.6 will be 19.13. * "Microsoft team made 5 releases of VS 2017!" - off by one error: counting RTM (15.0) through 15.5 is 6 releases. Only 3 were significant toolset updates, though (15.0, 15.3, 15.5; the other releases contained IDE updates and the occasional compiler bugfix). * "CLion - it’s current version" should say "its" * "but there’s a good progress with such tools" shouldn't say "a". * "Read that article/discussion for more information: Does C++ need a universal package manager?" should say "this". Here, "that" sounds like it's referring to something previously mentioned, when you're actually mentioning it at the end of the sentence. * "so I feel that grown and vibrancy" should say "growth". * "It looks like being C++ developer" needs "a".
If whatever vtable_ptr is is a pointer, it's going to be 64 bits on a typical 64 bit system, and 32 bits if you're still using 32 bit.