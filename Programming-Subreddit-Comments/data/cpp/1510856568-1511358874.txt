First we do disagree about the definition of weak and strong and that's fine. I also disagree about the relevance of that contract to this discussion. All that's being asked is why can't a compiler take a function like this: int f(P1 p1, P2 p2, ...) { // body of f } Transform that code into the following: constexpr int f(P1 p1, P2 p2, ...) { // same body as above } See if it compiles and if it does then automatically infer that `f` is `constexpr`. Now perhaps that isn't worth doing and perhaps the C++ committee doesn't see much utility in adding that, but regardless of the answer one thing is for sure, it has nothing to do with optimizations or expensive analysis or potentially breaking a compiler due to upgrades. Any compiler that supports `constexpr` can also support taking a function, adding a `constexpr` modifier to it, seeing if that compiles and then making the inference based on that. There could be some minutia about why that may have unintended consequences, such as rules about template instantiation, and those consequences would be worth understanding but one thing is for sure is that it will have nothing to do with the reasons you claim.
&gt; The full contract is that a constexpr function given constexpr inputs can be used in a constexpr context. Are you sure about this? constexpr int func( int i){ if (i==47) return rand(); else return i*2; } int main() { // kaboom static_assert(func(47)); static_assert(func(46)); } I mean 47 is known at compile time it is just that function does bad bad stuff if that is the argument. 
That's incorrect. A constexpr-qualified function is not required to be constexpr for all constexpr inputs. From [cppreference.com](http://en.cppreference.com/w/cpp/language/constexpr): &gt; there exists **at least one set of argument values** such that an invocation of the function could be an evaluated subexpression of a core constant expression For example: // valid function, sometimes constexpr constexpr int foo(int i) { if(i &lt; 0) { std::cout &lt;&lt; "foo was called with negative!\n"; } return i / 2; } // valid. foo(BAR) is constexpr constexpr int BAR = 2; constexpr int foobar = foo(BAR); static_assert(foobar == 1); // compile error. foo(BAZ) is not constexpr constexpr int BAZ = -2; constexpr int foobaz = foo(BAZ); 
I think you are right. Only real problem I can think of is that normal functions do not need to be in the header while constexpr ones must be if you want to use them from another translation unit. But still my feeling is that they should have just added pure atribute that is useful for compile time and runtime :) and avoided consexpr keyword. 
How much OpenGL knowledge do you require to consider someone viable for a position like that? I'm curious because I'd like to get into the industry doing that, but the barrier to entry seems huge for graphics devs. Here's some of my work, for reference: https://github.com/velocic/opengl-tutorial-solutions I've essentially been working through the [ogldev](http://ogldev.atspace.co.uk) tutorials and doing my own implementations without referencing the author's source code much, using OpenGL 3.3 core profile. Highlights so far is I've built a camera, ambient/diffuse/specular Phong lighting shaders, and I'm working on mesh importing. Haven't done shadows/animations/deferred rendering yet. Basically, I'm wondering what you consider the threshold of "hireable" is. Anyway, thanks for taking the time to share your perspective.
I've had my C++ job since before C++98 was a thing. And I got it by responding to a two-line ad in a free sunday newspaper ;-) Domain knowledge... We've just hired someone, but we had very few applicants. This is for a job writing software to control spacecraft on, and from the ground, so you'd think at least a few technically-minded people would be interested. However, we demanded years of experience in the space industry, as well as real-time knowledge, so we were fishing from a rather small pool, and I fear we probably missed out on good candidates that were simply scared off by the demands we made. From an insider point of view, the real-time stuff is pretty limited (a few interface components), so it shouldn't be a deal breaker. And years of experience in the space industry... Come on, what precisely is it that makes a user interface for a spacecraft different from, say, a user interface for an invoicing application? Of course I don't know how this works in other companies, but certainly with smaller companies it may be worth sending in that CV anyway. 
&gt; Adding constexpr enforces the compiler to evaluate the expression or throw an error if something is missing. No, that is a common misconception. Unless the result is e.g. used to initialize a constexpr variable or used as a non-type template parameter the compiler is in no way required to execute a constexpr function at compiletime.
This is about the 15.5 preview 4 update. https://www.visualstudio.com/en-us/news/releasenotes/vs2017-Preview-relnotes
It sounds like Conan would actually give you the best of both worlds in this situation. I have a project that use 3rdparty libraries that take about 90 minutes to build (both debug and release on windows), so building all 3rdparty deps is impractical in my situation. Conan fixes that in the sent that we can build the deps once, upload them to the remote, and any Dev can retrieve the precompiled ones for a given combination of platform/architecture/compiler/compiler version/buildtime/runtime Conan packages can have arbitrary options as well, so you can expose the typical offenders (like, c++ 11/14/17 flags etc) if required. Or you can have arbitrary build configurations : "debug" "release" "debug with address sanitizer" to match your requirements. Then when retrieving the packages, you get two things: the Conan recipe (a python build script that should be written to ensure that that any machine can retrieve the source and build the binary) , and a binary package that match the required build (with regards ro build options). For the latter, you can use preset conan "profiles" . So you can have a profile that actually forces all cmake-buildable packages to use a certain toolchain file). If you pass options for which no binary build yet exists, conan will ask you if you want to build it instead. So if someone for any reason requires specific compiler flags or build settings, they can do it too. 
Oh yes, sure! But for every good experience there's also a bad one :-) Boost is very slow in adopting new Visual Studio versions, 2017 support was missing for ages, they had months or even around a year if I remember correctly while the Beta/Preview/RC (whatever it was called back then) was out, but nobody cared, VS2017 final was out, but some Boost libraries not even compiling. And these were not uncommon ones. The same with the `/std:c++17`, I had a project that needed this flag but Boost doesn't compile under it. (maybe now it does... didn't check again). Also quite a number of bugs, even in common libraries like serialisation or program-options, just don't get fixed, because nobody maintains it anymore or nobody cares.
Ehm, that's kinda the point (i.e. I was replying to show a valid code example where using `0` or `NULL` for pointers would be a problem). And the proper (better) solution would be to not use `NULL` (and especially `0`) and only use `nullptr`. Though only complete removal of `0` -&gt; null pointer conversion would solve this (`NULL` can stay) and I can't see that happening since it'll break a lot of valid (though smelly) code. And most importantly it'd break `C` compat.
I was able to get this working with the folder view without checking system headers. The issue is where to set the environment variables. Setting them in the cmake file as you've done won't work, because those environment settings are only present at cmake generation time. Therefore, when an actual build is running, those environment variables aren't being set and thus caexcludepath is empty. The solution is to make use of the CMakeSettings.json and set these environment variables using the "environments" config as outlined in [Customizing your Environment with Visual C++ and Open Folder](https://blogs.msdn.microsoft.com/vcblog/2017/11/02/customizing-your-environment-with-visual-c-and-open-folder/). One caveat, the %include% variable isn't available in the CMakeSettings.json. Therefore, I had to explicitly set caexcludepath with the paths I wanted ignored (just copy past the results of echoing %include% from the developer command prompt). Hope that helps!
clang and gcc user here, I never had nor heard of Boost not compiling on those. But I see how much standard compliant code is not working on msvc. Maybe the problem is msvc and not Boost? I think you are right, though, that some libraries are not as well maintained as they should be.
The one argument from another thread I found somewhat convincing is that the ABSENCE of constexpr is a way to explicitly document that your user should not use that function to initialize a compile time constant, because even if it might be usable in a constexpr context now you might want to change the implementation in the future in a way that doesn't allow this anymore
I was able to get this working with the folder view without checking system headers. The issue is where to set the environment variables. Setting them in the cmake file won't work, because those environment settings are only present at cmake generation time. Therefore, when an actual build is running, those environment variables aren't being set and thus caexcludepath is empty. The solution is to make use of the CMakeSettings.json and set these environment variables using the "environments" config as outlined in Customizing your Environment with Visual C++ and Open Folder. One caveat, the %include% variable isn't available in the CMakeSettings.json. Therefore, I had to explicitly set caexcludepath with the paths I wanted ignored (just copy past the results of echoing %include% from the developer command prompt). Hope that helps anyone that might be interested in trying this on their projects using folder view. 
Glad to hear it's been improved upon. Either way I think it's a great release. Really happy to finally see fold expressions in MSVC.
It's actually the reverse. A *non*-`constexpr` function is _guaranteed_ unusable in a `constexpr` context. Functions marked `constexpr` may still be unusable (depending on arguments or types used)
Yep, just verified with the dev who fixed it. We had an issue with aliases in general. Good memory! 
That's a good idea, I just looked up Kahan. In principle, you can already use your own type if you use the `array_storage` policy.
Glad you got it working! Paging /u/segfalt_wa regardless.
Thanks. Still, feel free to cast nasty glares and ambiguous aspersions over towards /u/caseycarter. He claims to be working as hard as possible, and he's certainly at work much longer than I am, but I don't yet see a VS2017 update to Range-v3!
I think /u/berium has it right, below. 
Thanks, will look at that one as well. 
That actually makes a lot of sense. Thanks !
Ironically I just got contacted by a recruiter on LinkedIn as well. However I am really worried that the market might be shrinking at an exponential rate, and whether I should look into other areas. I really hope I'd have enjoyed web development but that realm is just so...typical. And crowded.
My thirdhand understanding is that you don't need an active security clearance to apply to those sorts of jobs, just a reasonable expectation that you'd pass one: even when you switch jobs between contractors they still have to go through the clearance process, it's just having one already is a pretty good indicator that you'd pass it the second time.
I'm told that this is fixed in the 15.5 previews. We're running dlib in our real-world code testing suites so we're pretty sure that the "infinite" loop is fixed. (It's not really an infinite loop, it's just very very very very slow.) 
Thank you! Whenever I browse SO job listings there is like 1 or 2 intermediate role for every 10 senior ones.
Are you suggesting that C++ is a dying skill to have?
Yes, this is blocked first on some compiler issues, then on the library. Sorry. 
It might be a naivete on my side as I touched Jave 4 years ago for like 40 hours max, but what makes Java so attractive to employers than C++? I thought they were considered the same.
/u/gabrieldosreis gets credit for this one, I believe.
Nah. Looking at measurements like the TIOBE index, it doesn't really look like it's shrinking, or at least not much. But the area of use clearly changes.
You can install multiple copies of VS on the same machine now. Here's a link to the preview: https://www.visualstudio.com/vs/preview/
Thank you!
my biggest concern is that maybe I should invest in another language entirely
Do you mind sharing your portfolio?
Ah nice! I always want to do some math-heavy development but my current job is just building apps that don't really need that much math.
Casey is currently busy implementing `&lt;memory_resource&gt;`, one of the 6 STL features remaining for C++17 STL completeness.
&gt; And how do you test your inner functions when they grow out of proportion? Because you can't call them without the containing function, so you can't really unit test them, from what I understand. You test them the same way you test private methods of a class. You verify the public interface, keeping in mind the various code paths that are possible.
Actually, they aren't *quite* the same. Although they enable the same features right now, `/std:c++17` and `/std:c++latest` produce a detectable difference in the value of `_MSVC_LANG`.
No problem, thanks to the dev team for a useful tool!
Not blocked on the library anymore! :-) I've implemented constexpr char_traits in the STL for the third toolset update (the one after 15.5). It's enabled for Clang and EDG right now. When C1XX (the MSVC front-end) gains support for the compiler hooks, I'll enable it there and mark it complete instead of partial.
While we use SFINAE extensively, it doesn't typically compete with `if constexpr`. You're thinking of tag dispatch, which is almost completely superseded by `if constexpr` (the only place it isn't, is constructors). `if constexpr` dramatically improves debug codegen size - I experimented with `find()`. I expect compiler memory consumption to also improve somewhat, and wall clock throughput to improve by some nonzero amount (not sure if it will be noticeable). Overhauling the STL to use `if constexpr` is near the top of my todo list.
Ah, thanks! I'd missed that reading through the TS.
If you're coming with just general dev knowledge plus working through a tutorial, that's not really enough. You don't have knowledge of advanced graphics techniques, and you don't have knowledge of advanced hardware usage. It might be enough for entry-level at a bigger game studio, since they'll already have people who can mentor you in learning modern graphics. But, again, domain expertise trumps all! For instance, I'd hire somebody with a couple years' Java distributed systems experience plus a C++ OpenGL tutorial. 
That is partially true. An initial investigation for a security clearance is going to cost more than a renewal. And if you already have an active security clearance, then it is just a matter of transfer of sponsorship which may not cost the company anything.
Oh, sure, blame us because it's our fault! /s
I never doubted that he was working hard. I never even doubted that he was working hard on more important things. I just know I haven't gotten what I want yet!
This just cries for a user defined litteral 
I've chosen to use Java for GUI tools and non-graphics modules (e.g. DSL compilers) because I can go from nothing to a complete, portable, maintainable, deployable program much faster. Especially for GUI stuff. Java's standard library has portable utilities for huge numbers of high-level problems: URI parsing, networking, GUI, JavaScript execution, etc. In C++, you have to amalgamate the different libraries. This isn't so hard if you just want to pre-compile libraries and make something work as a toy on your own computer... but if it's something you have to maintain for 20 years, you have to have a system in place for building the whole world yourself. And that becomes exceedingly complicated in C++, with different incompatible compilers across multiple platforms, and inherently incompatible binary representations. Sometimes the cost of setting all that up for C++ makes sense: you need access to hardware, or you need extreme performance. Sometimes you just want to build a tool without also having to rebuild the universe. EDIT: Also, enterprise-grade tooling for Java that still isn't really available for C++. You can do all sorts of magic with the JVM that you can't do with "normal" system-specific binaries.
School taught mainly C++, so it was my strongest language and listed first on my resume. Went to the engineering career fair, got a few interviews and offers, chose the one that best fit what I was interested in and where I wanted to move. Having some C++ knowledge is likely at least partially why the company I'm at was interested in me, the vast majority of our code is in C++.
What it's doing in this subreddit? It has nothing to do with cpp. It's a general programming question. It even sounds like a homework. Anyway, the fastest method on `x64` would probably just to use [mulx](http://www.felixcloutier.com/x86/MULX.html) instruction (there should be an intrinsic like `mulx_u64` in your compiler).
Great! :) Also IDK tag dispatch is not SFINAE, I thought it is subset of SFINAE, but now I think about it for SFINAE you need some substitution to fail, while overloading on true/false_type is not that. :)
I have an old C API, I have to work with. You pass a function pointer to a visit function, to collect your arguments. Usually those functions are really short and passing a non capturing lambda works well. But in some cases you have tree like structures and to collect deeper levels, you have to call visit again with a function pointer. Recursive lambdas would really help in that case, although you can use a reference to a later initialized function pointer.
Sir, please before writing the comment think what you'er saying. It's not homework, and I'm developing the solution using C++ and usually the best solution turns out to be using intristics.
Depends on the field of work. Enterprise software, distributed programming, GUIs,.. the ship has sailed and C++ is usually last resource when nothing else helps, or used at the lowest architecture layer. ML, AI, HPC, Fintech, games, embedded, audio and graphics codecs that is where C++ rules.
Cool, thanks for the insight. I really appreciate it. Might be worth looking into entry level studio positions, but I get the impression that they look for a lot of gameplay programmers.
I'm getting some warnings which I highly suspect are bogus in 15.5: TileCorrectionMapGenerationTask.hpp(49): error C4596: 'borderSizeInChunks': illegal qualified name in member declaration The changes it wants me to do to "fix" the warning: https://i.imgur.com/JJvFjqG.png The "warning" makes sense in other contexts (function definitions, member variable definitions) but this is a constexpr definition and neither Clang or GCC complain about it which is what makes me think something's fishy about it.
&gt; but I get the impression that they look for a lot of gameplay programmers. Yep. You can definitely go from gameplay to special effects to graphics core, though. If you want into graphics, it's your best path. Most of the people we hire for graphics come *from* games (myself included), not the other way around.
I worked in Java before. I had some C++ experience from the university. I did some development on Clang for a year and joined the local C++ user group. Then out of the blue, a company contacted me to come work for them. They needed C++ developers.
That would be best
I've also noticed a few changes to the /Za flag as well lately, are these intentional, or side effects? For example, xtgmath.h has received some 'alternative token' errors lately (with &lt;::std ...). Is /Za being maintained at all, or is this just a bug :) ?
It's still in the preview ([preview 4 to be exact](https://www.visualstudio.com/en-us/news/releasenotes/vs2017-Preview-relnotes)) with the final version expected to be released later this year.
to make C++ even more complicated
&gt; what precisely is it that makes a user interface for a spacecraft &gt; different from, say, a user interface for an invoicing application? Getting Hal's voice right?
Development on Linux is great from my limited experience, but I think Windows development has gotten a lot better with Visual Studio.
I had C++ courses at the university and made an internship at a company developing GIS software. There I also came in contact with Python and lots of C#. After I finished university they offered me to stay there and so I did. After that I did some embedded stuff for about 1.5 years and now I am developing logistics software in C++. My latest job was easy to get. They hired me immediately. Had some other jobs to choose from. We are still searching for C++ or C# programmers. 
My then-future employer has an open source project. I contributed to that enough times that everybody already knew me when I decided to apply and work on it full time. My interview became an excuse for the team to expense a nice meal and hang out. They already knew what I could do and what features I wanted to add, so there wasn't any whiteboarding or general probing. We still did a few of the critical thinking puzzles, but only well into beers, and I think it was strictly for fun.
I have encountered the same problem you are having but in a different domain. I work in the defense industry as an embedded software engineer. I work on systems that have safety critical requirements and commercial real time OSes that control on board sensors and equipment. Knowing how to program in C++ is a fraction of the job. I also have to maintain a steady knowledge of the domain I work in, digital signal processing. I'm finding it very difficult to move out of this industry. Most employers are looking for C++ developers that have extensive knowledge in the domain they are in and it typically requires years to build up that knowledge. I also don’t like giant companies who don’t hire you for a specific position and just throw you in a pool of random candidates.
Because question like "what is the fastest way to multiply to long/64bit integers modulo another long/64 bit integer" has very little to do with cpp and can be asked for any language that has 64-bit integers. Also, Sir, please write this subreddit's description: &gt;Discussions, articles, and news about the C++ programming language or programming in C++. &gt;For C++ questions, answers, help, and advice see r/cpp_questions or StackOverflow. That's why I said that, IMHO, this is not the place for this kind of topic. 
I'm a fresh grad (couple months out of b.s.), and landed a mid level position at a robotics company because of my internship experience (low level kernel Dev in C and asm), and my senior design project (Controls project). I mostly got lucky as the company was looking for young talent, and it just so happened that my experience tickled their fancy. During the interview they were mostly interested in seeing whether I understood the C++ memory model, basic threading concepts (show a deadlock, and memory layout of a thread), OO concepts, and very basic DS (implement Linked List and walking a binary tree). I'm also not in a tech hub, so competition may be less stiff. 
Maybe the main reason we still need to declare constexpr for now is just that they started as very limited in C++11. Now that most limitations (except allocation and side effects; but even there if I understand correctly people are working to allow some of that) have been removed, more and more people start to wonder what is the point of requiring that declaration. But to avoid mistakes, maybe at least having !constexpr available will remain a good idea. For now, this is the default.
I recently had a very good reason (IMO) to like that 0 can be recognized as a null pointer (well, at least in some contexts): I needed to migrate a type from a primitive to a class, and disallow *most* of the constructions of said type, except from centralized code, and except for the initializer 0 for the application code. For that last part, a constructor with a param of type std::nullptr_t did the trick.
Good empolyers will have employees that can create things which will, as an end goal, generate revenue. A programmer looking for a job that uses C++ is like an architect looking for a job that involves using concrete or a painter looking for a project that involves using synthetic brushes or a musician looking for a band that uses gretsch guitars. Look for jobs where what you can and want to do will help and you. If you really want to use Cpp learn to do things which can be done well using it, but that's sort of putting the horse before the cart.
IIRC, the calendar example will compile with VS2017 and `/permissive-`. (It compiled with VS2015U3 and `/d1permissive-`, the undocumented predecessor to `/permissive-`, and needed only 22GB of memory to do so!)
I got the most recent C++ job by previously working on a shitty C++ job for about half a year after graduation and because of the pre-shitty-job experience I got doing stuff in spare time (which also enabled me to enter the first one). I didn't apply by ad, but called directly at companies' HR (these were not giant companies, just some old local ones), in most cases it didn't work though.
Previous emploer closed development site in our country so I called my old colleague when I can start and he replied on the phone "as soon as possible". That was the job interview and the end of this story.
Not so much to actually alter the semantics of programs^1 as to remind C++ programmers of the advanced constant-folding and inlining capabilities of optimizing compilers, such that you can do your metaprogramming with plain C++ instead of unnecessarily baroque template and/or macro tricks. It also lets you use plain C++ functions and variables in template contexts instead of needing to resort to those tricks. That could as you say be achieved by evaluating all expressions for constant-ness, but that potentially leads to breakage when the upstream implementation changes such that it's no longer statically evaluable. `constexpr` makes this feature an explicit part of an API contract. --- ^(1. Compilers will virtually always generate the same code for `constexpr` and non-`constexpr` (or, technically, the same code for `constexpr`and)^`inline`)
Like DoD security clearance? Go take the CompTIA Security+ exam and you will have baseline DoD security clearance!
Who are you with? SpaceX, Boeing, Lockheed Martin, NASA???
As we mess with /Zc I wouldn't be surprised about /Za changes. Anything specific?
LOL People have been saying C++ has been "dying" ever since Java became mainstream in the late 90s. Fact is it isn't going anywhere in our lifetime. I'd even say that since C++11 interest has only grown. 
`constexpr std::string_view NodeType = "constant"sv;` works fine, as do custom UDLs (notable in case you're not working with `std::string_view` exactly, e.g. `boost::string_view`). EDIT: N.b. `constexpr auto NodeType = "constant"sv;` works, too, if you're into the whole brevity thing..
I got fired from sweeping floors. My current employer hired me 12 years ago to be the third IS guy because I had experience coordinating data center moves and had been hacking C and Unix since the mid-1990s. I fixed PCs, maintained servers, wrote job-queueing and network monitoring software, and coordinated a move of a couple thousand machines from a smattering of ersatz data closets to a proper data center in the ground floor of an office building a few miles away. And then they ran out of work for me. My job became to show up in the afternoon and leave late at night (to provide evening helldesk support), to sweep the datacenter floor, perform server maintenance, to rack new servers, and to write VFS modules for FreeBSD to address some of our specialized storage needs. NOC monkey and kernel-hacking, no in-between. :) My boss and I never got along, but my boring work tasks left me time to happily write tiny bits of code for other parts of the company. You can only clean up so much trash from the floor of a datacenter every day, and word got around that I was competent at making problems go away. As luck would have it, the director of one part of the company needed an embedded systems guy on exact same week that $boss and I got on each other's last nerves. $director hired me away before $boss could fire me, and now I write semiconductor test software in C++. I haven't professionally held a broom since. Publish your work, cultivate a love for solving people's problems, and you'll do fine. If your employment contract permits it, contribute to open source software so that you can have a portfolio to include with your résumé.
I think you stretched the analogy too much. Its more like a painter wanting to use Fresco instead of Oil painting. Or a musician wanting to play punk instead of metal. Programming languages are not just brand names, or undifferentiated substitutes. They represent particular communities and are actually ways of thinking, and good programmers care a lot about that. I could not stand to be a in JS, hot frontend, startup, culture. That is just not the kind of problems or tools I am interested in. 
I don't think breaking C compatibility would be so bad, especially since `NULL` is often defined as `(void*) 0` in C anyway.
We're continuously testing our STL with /Za (despite how icky it is), and I can't repro that issue with our current build. We intentionally removed spaces there after being informed that the compiler had implemented C++11 digraph disambiguation. C:\Temp&gt;type meow.cpp #include &lt;cstdint&gt; #include &lt;iostream&gt; #include &lt;vector&gt; int main() { ::std::vector&lt;::std::int32_t&gt; v = { 11, 22, 33 }; for (const auto&amp; e : v) { ::std::cout &lt;&lt; e &lt;&lt; "\n"; } } C:\Temp&gt;cl /EHsc /nologo /W4 meow.cpp &amp;&amp; meow meow.cpp 11 22 33 C:\Temp&gt;cl /EHsc /nologo /W4 /Za meow.cpp &amp;&amp; meow meow.cpp 11 22 33
Sounds like a bug in the refactored parser, can you distill this to a small self-contained repro?
Just the one I mentioned. Looks like STL thinks its no longer an issue in 15.5.
Domain knowledge and knowing people, here. There was an opening, someone in the group I knew mentioned my name to the lead, and they got in touch with me.
You might want to have a look at https://graphics.stanford.edu/~seander/bithacks.html
One problem I see is debuggability: Given that constexpr contexts are evaluated at compile time, there is no way a runtime debugger can be used to step through for example a constexpr function. This has already been a problem with templates, where you only get SFINAE error messages. With more readable C++17 compile time programming allowing for complexer programs, debuggability is already becoming a problem. For example the constexpr JSON parser (from the constexpr all the things talk) required *removing* constexpr to be able to step through some functions. This gets worse for compile time only functions using [the lambda trick](https://mpark.github.io/programming/2017/05/26/constexpr-function-parameters/) or user type value template parameters (the proposed literal type non-type template parameters). The problem with implicit constexpr takes the possibility of moving to runtime away. Until we get something like compile time debugging, I do not think we should make compile time the default. This would get easier if we had a real interpreter in the compiler for constexpr expressions. Rust has been pushing into that with [MIRI](https://github.com/solson/miri), which at some point would be used for const evaluation. It would be a heavy blow to see Rust have better compile time programming than C++.
Sure thing: class Example { static constexpr bool a = false; static constexpr bool b = Example::a; }; Is enough to reproduce it.
That would work if `sv` wasn't defined in a namespace. It requires `using namespace std::literals;` to use, and these string_view constants are defined in header files. So to make it work I would have to do a `using namespace` inside a header, which is one of those things I never want to do.
You can use namespace `std::string_view_literals` if you want to limit the symbols brought into scope, or even just `std::string_view_literals::operator""sv` if one were paranoid. I understand your sentiment, but IMO the usual caveats to `using` stdlib symbols/namespaces in header files do not apply to literal operators because their names cannot clash with user code: they never start with underscores while UDLs _must_ start with underscores.
It would be breaking both C and C++ compatibility. In a somewhat common (anti-)pattern. `NULL` has nothing to do with it, since nothing stops C++ form treating `NULL` specially (and many compilers do). I'm not against it, and I think it can be easily fixed/solved by tooling (changing 0 to NULL/`nullptr` where appropriate is a pretty straightforward thing to do), but it'll probably won't be enough for committee to break the status quo.
The problem is that you have to manually point to which Conan package profile you want for your project, that's error prone, especially when you support many (50+) configurations. You also need to have build scripts for packages supporting building with uncommon flags. If you have long build times, what you need is really just a caching layer that is possibly shared and good parallelism. For that last matter, msbuild is quite bad at it and easily overcommits the CPU, and Ninja is the recommended solution. For caching, you should use sccache, which is what they use on Firefox. It's basically ccache with MSVC support and support for a shared cache on S3-like storage and other databases. Allow your machines to fetch from it, only allow CI to write to it and you should have fast builds. Also, ensure you don't have to do clean builds on dev machines and incremental always work. You can also use a build farm with something like distcc and you could use incredibuild or some other solutions as well. Building a long project is fine, and manual caching of the intermediate artifacts has to be done VERY carefully.
No repro with my current dev build. C:\Temp&gt;type meow.cpp class Example { static constexpr bool a = false; static constexpr bool b = Example::a; }; C:\Temp&gt;cl /EHsc /nologo /W4 /c meow.cpp meow.cpp C:\Temp&gt;cl /EHsc /nologo /W4 /c /permissive- meow.cpp meow.cpp C:\Temp&gt;cl /EHsc /nologo /W4 /c /std:c++17 meow.cpp meow.cpp C:\Temp&gt;cl /EHsc /nologo /W4 /c /std:c++17 /permissive- meow.cpp meow.cpp C:\Temp&gt; Also no repro with a real install of 15.5.0 Preview 4 (I didn't try all those switches though). Can you provide a command line, and depict the whole compilation as I've done?
Headers still shouldn't have using-directives for UDLs at global or public namespace scope, because that contaminates users. Headers can have arbitrary using-directives at function, class, or details namespace scope.
&gt; Maybe the problem is msvc and not Boost? No, the problem is that C++17 removed a lot of previously-deprecated symbols, notably `auto_ptr`, `unary_function`, and `binary_function`. MSVC correctly removes these in C++17-mode unless you define a macro beforehand – it's doing everything by the book. Some old boost code simply _needed_ to be updated to use modern alternatives (it already "needed" to be updated since using deprecated symbols for the past few years wasn't really acceptable, either, but obviously compiler warnings rather than errors aren't nearly as motivating). N.b. this affects more than just MSVC; bleeding-edge libc++ similarly requires that the user opt-in for these symbols in C++17+ modes, it's just that MSVC made these changes first and has a far larger marketshare so you don't hear about libc++ being an issue as often.
Sure thing: C:\temp&gt;type meow.cpp class Example { static constexpr bool a = false; static constexpr bool b = Example::a; }; C:\temp&gt;cl /EHsc /nologo /Wall /WX meow.cpp meow.cpp meow.cpp(4): error C4596: 'a': illegal qualified name in member declaration C:\temp&gt;
&gt; Headers can have arbitrary using-directives at function, class, or details namespace scope. 'Arbitrary', I agree; but this isn't exactly arbitrary since you know exactly one overload set is being brought into scope (EDIT: and again, it's one that cannot clash with user code since it has a reserved name regardless of namespace). ;-] But again, I understand the sentiment.
On a somewhat related topic: I also get this warning (promoted to an error) from the winbase.h file when compiling with /Wall /WX: 1&gt;C:\Program Files (x86)\Windows Kits\8.1\Include\um\winbase.h(7150): error C2220: warning treated as error - no 'object' file generated 1&gt;C:\Program Files (x86)\Windows Kits\8.1\Include\um\winbase.h(7150): warning C5039: 'TpSetCallbackCleanupGroup': pointer or reference to potentially throwing function passed to extern C function under -EHc. Undefined behavior may occur if this function throws an exception. 1&gt;C:\Program Files (x86)\Windows Kits\8.1\Include\um\winbase.h(7150): note: to simplify migration, consider the temporary use of /Wv:18 flag with the version of the compiler with which you used to build without warnings 1&gt;C:\Program Files (x86)\Microsoft Visual Studio\Preview\Community\VC\Tools\MSVC\14.12.25827\include\thr/xthread(50): warning C5039: '_Thrd_start': pointer or reference to potentially throwing function passed to extern C function under -EHc. Undefined behavior may occur if this function throws an exception. 1&gt;C:\Program Files (x86)\Microsoft Visual Studio\Preview\Community\VC\Tools\MSVC\14.12.25827\include\thr/xthread(50): note: to simplify migration, consider the temporary use of /Wv:18 flag with the version of the compiler with which you used to build without warnings But that one I just ignore for now since it's not our code.
a) Programming languages are not undifferentiated but the difference are nothing short of irrelevant when it comes to the final product. From a technical point of view all popular modern languages boil down to more or less the same concepts. That's why the large majority of popular language (and I mean that as in top 100 popular language, as in, any language that still has greenfield projects generating money being written in it, even if only a couple) will compile to GIMPLE, LLVM or JVM-Bytecode. The first two being not all that different and the former relying heavily on a powerful C/C++ runtime, so the concepts available to you and the performance you can obtain are still tightly tied to a couple of llvm and gc backends. (And I haven't seen any impressive optimizations come out of the Java JITC yet, but feel free to make that argument if you want) The language which don't fit the above (e.g. Node, Python, Forth, PHP... etc) are almost exclusively relying on a C/C++ runtime, which, once again, means they are technically limited by the capabilities and compiler chains used by C/C++. If a language or hardware architecture came along with a ground shattering concept (e.g. a language + GPU that are more powerful due to using ternary logic or having a different take on task scheduling), I'd be all for trying to use that language for the sake of using that language. In the current environment language choice usually boils down to one thing, preference, support for the platforms you use and portability. b) I will totally agree with you on the second statement, that programming languages have come to represent communities, ways of thinking and other entities and concepts that are intertwined with that language. But then I'd argue that, in that case, you should look for a culture you like, not a language you like. To give you a personal anecdote: I've recently started working with a "JS, hot frontend, startup, culture" type company (shinny front end, dependency chains that involve pulling hundreds of package of npm, a mix of 3 dialects of js... etc) and it's the most amazing thing that's happened to me as far as career fun goes. I've spent my time working on a macro processor for a database, revamping infrastructure, making code efficient by applying common sense in the usage of data structures, refactoring critical components to add safety (const correctness, type safety... etc). Which is shit that I truly enjoy doing as someone who is totally down with "C++ culture", and I get to do it all in code that I can compile with +2a (ok, I do actually use 17, but in theory nobody would stop be from doing the former), that I can revamp at will. To add to that I don't get the stink eye from a "senior" developer/ops for "breaking company practices" in my effort to optimize, I get thanks from guys that are completely mystified and delighted because their charts are rendering at 20 times the speed and their machine learning algorithms are now taking minutes rather than hours to run and I get to work on a "fuck all" schedule on a salary much higher than I could get with a "C++ team". I can do this stuff not because I'm some guru (I'm shite at best when compared to the kind of C++ developers that write the books I read, prepare the talks I watch and write the code I reuse), but simply because I work with people who are better in other areas than myself and never cared to learn what I know (just as I've never cared to learn angular/react or browser/device specific quirks when using CSS animations or how to talk to a customer and offer him tech support or give him a sales pitch or understands how the *** he wants the UI to Look). My point here being that you can thrive in a team which doesn't use your language or OS or framework or coding style, as long as what you bring to the table is valuable to them. Indeed, you might even fare better in such a team and enjoy it more, because it offers you more freedom and it's full of people that will love doing the stuff that you find annoying, boring or difficult. /rant
I'm curious what you're doing that warrants using `/EHa` rather than `/EHsc`... Tricky WinAPI interaction?
did you mean "very... slow"? ;)
We're using /EHsc.
No, I mean very
slow.
&gt; an expression being constexpr 'compatible' is something the compiler should be able to figure out by itself In general it is not possible to decide whether an expression can be computed at compile time for the same reason it is not possible to decide the halting problem.
I'm more looking forward to being able to compile windows.h without MSVC language extensions wink wink nudge nudge :)
Doing c++ for my ph.d. There's not too many languages available when you want to write software for artists and shows and latency requirements are &lt; 1 millisecond.
The warning is specifically for compiling under `-EHc`, which is equivalent to `/EHa`. &gt;_&gt;
Jees... Finally! :D
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7dbeln/lruc_headeronly_modern_c_lru_inmemory_cache/dpxqy11/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Sorry to be negative here, but is this really worth posting? It's a default naive LRU cache implementation using std::list and std::unordered_map. This exists as a practice task on leetcode. 
&gt; what makes Java so attractive to employers than C++? Money spent per unit of developed functionality. Most of areas don't require extra 'performance' edge of C++ over Java.
I made my current place a C++ shop. When I joined as a junior, all of our code was written in Delphi (except for web stuff), and everyone just kind of put up it because it's just what they always used. I didn't rock the boat, but occasionally would show the senior software engineer (we're a small shop, 4 full time developers, and some hardware engineers) how expressive C++ solutions could be, and how efficient the resulting executables were. About three years in, the senior started playing with C++ for some experimental projects on the side. He would occasionally come and ask me questions about how to do X, or Y, so I took the opportunity to really push a modern C++ style on him. I got word earlier this year that we are now investing heavily in C++, and we will be using it for all CPU-intensive projects going forward. I didn't mean for this to happen, but I'm really happy it did.
I don't know what to say. We explicitly set /EHsc and that's the error I get.
If you are in US -- yes, quite likely. As C++ dev you won't be able to compete with outsourcing (to Russia/Phillipines) and even indentured servants brought here on H1-B are more preferable to companies than you. Not sure what happens in government sector, though -- I heard defense is using quite a bit of C++. Or (if you really love C++ development) you could take a risk and try to outweigh negatives with knowledge and productivity.
Aha, /Wall. Can you submit exactly that to Report A Problem in the IDE? (User bug reports are treated with higher priority than bugs I file directly in our internal database.)
The article is over a year old though, for some reason OP is doing some digging.
Just putting this here: https://careers-phishme.icims.com/jobs/1547/senior-software-engineer%2c-windows./job
And this: https://careers-phishme.icims.com/jobs/1532/application-security-engineer/job
just for you and because I'm bored &lt;3 std::unique_ptr&lt;MyClass&gt; instance(new MyClass()); std::unique_ptr&lt;MyClass&gt; instance; instance.reset(new MyClass()); &gt;what happens if MyClass throws for any reason ever? class Outer { public: Outer() : impl(new Inner()) {}; ~Outer() { delete impl; } private: Inner* impl; } &gt;owned resource of a class is managed but not ref counted That code is horribly broken no matter the version of c++ and would function entirely different from the std::unique_ptr version in so much as the latter actually functions as intended. Any copies that are alive longer than block scope causes the prior implementation to shit itself. -&gt; double deletes to memory regions; deleting data that's actually being used. but no besides wanting to ree over the calls to raw new's there's nothing that screams broken; or me being early pedantic about things the guy is probably attempting to imply but doesn't specify. 
Do we really need yet another article about smart pointers? A feature that was almost 6 years old when this article was written?
C++ will outlive Java even, IMHO.
I wrote an enterprise app, for a field I was vaguely familiar with, and sold it to some large companies. I bootstrapped a company on credit card debt to support it, and 10+ years later it’s doing nicely. I’d done professional development in C and Perl before, but it was my first C++ project (chose to use Qt, and that meant C++). This isn’t an approach that works for everyone, but the takeaway is that even if your goal is laser focused on being a C++ developer there are a lot of other skills that are at least as important as being able to grind out C++ code.
He uses C++14 features in it, so it was only 2 years old at the time. It's too bad he didn't use custom deleters as well though.
For everyone on this thread looking for C++ Jobs checkout foghorn.io . We are actively hiring C++ developers for some really cutting edge work. 
/u/AndrewPardoe constexpr lambdas should work in 15.5, right? I'm getting an internal compiler error: https://developercommunity.visualstudio.com/content/problem/152045/internal-compiler-error-with-constexpr-lambdas-use.html I do use /std:c++17. As you see here, clang and gcc both compile the code correctly: https://godbolt.org/g/aCpQij
We certainly don't find enough good C++ developers (it seems students only learn Java or some other language). But C++ development is not the defining feature of what we do.
You can't compile &lt;pre&gt;/EHc&lt;/pre&gt;, the front end ignores it. The warning pertains to the 'c' option, which tells the compiler to assume that all C functions are nothrow. Asynchronous/synchronous doesn't matter, you can compile &lt;pre&gt;/EHac&lt;/pre&gt; and get the same warning.
 D:\test&gt;type exc.cpp &amp;&amp; cl /c /WX /Wall /EHsc exc.cpp extern "C" void foo(void (*)()) { } void bar() { foo(nullptr); } Microsoft (R) C/C++ Optimizing Compiler Version 19.12.25828.2 for x86 Copyright (C) Microsoft Corporation. All rights reserved. exc.cpp exc.cpp(4): error C2220: warning treated as error - no 'object' file generated exc.cpp(4): warning C5039: 'foo': pointer or reference to potentially throwing function passed to extern C function under -EHc. Undefined behavior may occur if this function throws an exception. exc.cpp(4): note: to simplify migration, consider the temporary use of /Wv:18 flag with the version of the compiler with which you used to build without warnings Looks like a broken check in the compiler.
a) I think your focusing too much on implementations rather than expressiveness and grammar. We certainly are seeing more similar VM and ASM, since there is no reason for those to be different. I agree that the big general purpose languages (C#, Swift, Kotlin etc) are essentially converging. But programming languages still define and constrain the way you think. The C++ generic style of programming is completely different than anything else. You regularly express things that aren't possible in other languages. Similar for Lisp or Python. If you haven't read Paul Graham's thoughts on this I recommend them http://wiki.c2.com/?BlubParadox I agree with your statements in b). A good programmer has the skills to thrive anywhere. But I definitely know programmers (myself included) who enjoy certain kinds of programming to others. I am not making any claims about business or making money.
Seems like pretty much every other language that isn't C or C++ these days is implemented in some way in C or C++.
I got a real C++ job at a manufacturing company. Granted, it was Win32, but I was writing C++ every day. A few years later, finance companies were willing to grant me interviews based on that experience. I had other Linux experience, so these banks and prop shops were willing to let me combine the two (Linux and C++ experience) and interview for a C++ dev role.
I work here: https://www.terma.com/ I'm located in the Netherlands, but the company has offices in various other countries as well. Our offices makes ground control systems (i.e. testing of spacecraft on the ground and control in orbit), as well as software for thermal testing (my department) and emulation of onboard computers (this is largely LLVM based now). 
Go, Lisp, Scheme, OCaml, Haskell, Ada, FreePascal compilers are bootstraped. D guys converted the frontend to D and concurrently in the process of porting the backend as well. Rust depends on LLVM for the backend, but nothing prevents them from replacing it, other than losing the optimization work done there. All .NET compilers are bootstrapped, and the .NET team already stated in a InfoQ interview that they plan to incrementally port the runtime to C# as well. Hence the new low level features in C# 7.x. Oracle started Project Metropolis to rewrite OpenJDK in Java. Yes there are many languages written in C and C++, but it is usually an historical accident, not wanting to bootstrap a compiler or just some preference of the language designer.
Apparently you cannot read. I clearly mentioned the areas where you will find C++ jobs, like ML, Fintech and games. Now try to find a C++ job on a typical enterprise, not the SV loved ones, that isn't related to maintenance.
Allocations and side effects are allowed in constexpr functions since c++14.
I don't think so, because an implementation is e.g.not required to support arbitrary recursion depth.
You can give a non-completion constant as an input to a constexpr function you you have guaranteed runt-time evaluation.
I'm curious to hear why the committee thought we needed this kind of behaviour. Surely it makes much more sense to have constexpr provide a guarantee? How about constexpr variables: could those also end up being computed at runtime? How about anonymous constexpr expressions? Let's say I have an expression somewhere that could be assigned to a constexpr variable (i.e. it is compile-time computable) but isn't: is the compiler required to compute it at compile time? 
My first couple of jobs were as VB/Java developer but I always had a passion for C++ and gamedev so I used to leave office asap to get home to work on my hobby gamedev stuff in C++, often even while in the office I was coding games when nobody was watching. Eventually I had enough code and little things to show to make up for missing formal education and, after a lot of attempts, landed my first job as C++ dev, interestingly enough it was not in the game industry but in the video editing software industry. Now as the head of a game studio I look for people with the right attitude towards development and highly regard and prefer people that can show personal projects done in their own time more than pure C++ chops.
Why does the standard define an ordering on `std::optional`? The chosen semantics where `none` is smaller than all proper values seem quite arbitrary. 
Before my current job, I mostly had experience with Matlab and Python for data analysis. But I wanted a change of career, so I applied to some jobs where my background in physics might be usefull. So now I'm working in the field of advanced drivers assistance systems, and our language of choice is C++ (real-time requirements, performance reasons). We also use C++ for non-embedded programs and some GUI applications. That being said, we're hiring. Domain knowledge would be nice, but is not a fixed requirement. As long as you're a good programmer, we can teach you everything else (worked for me). We're also hiring juniors/new grads. The only potential downside: some knowledge of german would really increase your chances of being hired (all our internal communications is in german) - and you would have to relocate to Berlin. If anybody is interested, send me a message.
I was just about to post something similar. Generic programmers are ten-a-penny (especially as they can often be outsourced). Programmers with additional skills/knowledge/experience in one or more non-programming domains are a much smaller pool, and can be hard for employers to find. A good background in some branch of science, engineering, maths, electronics, medicine, etc + even average programming skills can help you to get a well paid niche programming job. If you’re a “one trick pony” with just programming skills then you’re not particularly unique or useful - get some additional skills to make yourself more valuable.
Ah, I see. auto_ptr is still used in Boost.Python, so that's one case.
That has nothing to do with it. The compiler can speculatively begin execution of any function in the hopes that it will turn out to be an acceptable constexpr function. Such functions are guaranteed to halt: the compiler is only required to execute them until a certain number of full expressions have been computed, or until a certain number of constexpr functions have been called. This also puts us on the trail of why it might be best not to automatically make everything constexpr: it would be bloody expensive at compile time to have to try this for each and every function. So, here's my proposal for why we cannot have automatic constexpr on all functions: the compile time cost would be unacceptably high.
I'm looking forward to being able to `import windows` and not have to pay the huge price for including half a million lines of code in each of my &lt;1000 line translation units... Will be interesting to see how they will deal with all those macros though ;-) 
Hey, if you need something fixed you can always ask us. We'd be happy to send in some 'user' bug reports for you ;-) 
In all honesty, though, msvc was not supporting the full set of C++11 and C++14 features for a long while, while gcc and clang had them adopted before the standard came out. It took until Visual Studio 2015 for constexpr to be supported. https://msdn.microsoft.com/en-us/library/hh567368.aspx
I'm quite late to the party, but in my experience it's actually hard to find good C++ developers! I work in the medical industry and 90% of our codebase if C++ (only the frontend is C#,WPF / HTML5). My advice is that even if you don't fit all the requirements of a specific job, it's just as important to have a good CV and even more important a job specific cover letter. If you only have some, but not all of the requirements, try to concentrate on your strengths, there is only so much you can write in a cover letter anyways. For quite a few positions you can often make up for the lack of domain specific knowledge. HR filters the bad applications already, but they probably don't understand lots of the domain specific requirements anyways. So if you get past HR and there are no 100% perfect candidates, you might have a good chance.
Would you mind sharing what are you working on? Graphics programmer side of me is intrigued.
I'm in embedded systems. There's lots of C &amp; C++ jobs in that field. We hired 2-3 developers straight from college in the last year. If you're interested in embedded you'll need some microcontroller projects on your resume. Find some electronics meet-ups and figure out if there it a TI Tech Day near you, or similar vendor hosted meetup. Ability to read schematics and understanding of interrupts/DMA/ADC is also highly desired.
Is that due to a more extensive standard library for high level things? Or memory management stuff? Would finally using smart pointers not clear up most of that? 
I don't get the error. Even if `f&lt;int&gt;` is instantiated, would not `h(decltype(int{B ? f&lt;T&gt;() : 0}))` be SFINAE'd because `decltype` expression is ill-formed?
I choose C++ because of it's nature (all the type features and complex syntax, performance, openness (no company behind it), templates and general feeling of expert-friendly complicated language) and ... just send CV to local outsourcing. I got the job ~2 months later and now it looks like I will be continuously working on various linux-based C++ projects in Eclipse. I very like it.
Looks like you know some hidden compiler-specific magics. Are there any other fancy `__` things?
IIUC the error happens inside the body of `f`, so it's not subject to SFINAE.
Are you sure? Should `decltype` evaluate expression here? I though it only checks the return type of `f()` here.
I've tried it now with VS2017 and ranges from vcpkg (/permissive-, std:c++latest). It gave me the following error: https://gist.github.com/Predelnik/f7168bc09ae62b5fecb922ea039ea783 I don't have 22 gb of ram even nearly though so maybe I shouldn't try this :)
What is a good GUI alternative? Cross-platform mind. I can only think of java and find C++/Qt superior.
but is it only because nobody deemed it possible though ? for instance, languages like F# are able to use run-time information (eg read from a file on your hard disk or whatever) as part of their compile-time type-system, so why couldn't, say, fopen and fread be able to do the same WRT c++ compilers ? 
I thought it said versus. Don't know if I should be relieved or disappointed. 
So you can insert it into sets and maps?
&gt; The C++ generic style of programming is completely different than anything else. You regularly express things that aren't possible in other languages. I'd correct that to *You regularly express things that aren't possible in other languages without runtime overhead* In that sense I do believe that the future of the C++ generic system (if concepts and metaclasses) make it in the language, is something that will make it stand out from all other languages (well, languages with similar concepts exist, but they are too unpopular to consider for most things), but I'd say that is an exception much like how the Node async model of coding is an exception Lisp's macro system is an exception. I will not touch on the Blub Paradox, personally I've come to stipulate the above whilst being someone who really enjoys trying new languages now and then and enjoys always having projects ongoing in a few languages. If you've got any language recommendation that could change my mind I'm all ears and always will be, but in the end most code that "matters" (read: that has to be decently fast, maintainable and predictable or test-able) ends up converging to the same concepts because of the aforementioned similarities in the lower levels which the code compiles to.
Took all the C++ related classes I could find at my university. Got a Job through a friend recommendation at a small/medium sized ATC software company, got so much experience there it was nuts. Applied to my current job because I wanted to move to the area, got the job after a series of interview rounds.
Depends on what the customers want. We only do desktop development for Windows with WPF/UWP. Occasionally some JavaFX, if there is a requirement to run on UNIX systems, but if no OS native features are required, they tend to be done as web applications. On mobile OSes, Xamarin or a MVVM approach like Dropbox and MS are doing (Swift/Objective-C++ and Java/Kotlin for the UI, with C++ for common code). But the majority of enterprise customers have been moving into Web GUIs, regardless how those of us that only enjoy native think about it. Interesting that you mention Qt, even they have been moving away from pure C++, many of the new controls are available only in QML specially for mobile deployments.
If I remember my CS lectures on orders correctly, `none` (in my lectures represented as ⊥) is often added to a (maybe partially) ordered set as a unique infimum. I don't remember the maths or the details from the top of my head but I remember it made perfect sense to have it that way.
Oh wow - you have absolutely no idea how perfect this is. Ill take a look at integrating it with emacs lsp-mode.
You don't need to parse the whole utf8 string to check if a byte is part of a multibyte character. You just have to check the first bit. If it is one, you have to parse backwards until you get a byte with the second bit set. That sounds considerably easier to me than surrogates and I think I'm not the only one: [https://m.youtube.com/watch?v=ysh2B6ZgNXk](https://m.youtube.com/watch?v=ysh2B6ZgNXk) Also you can never guess the size of the visual representation from the length of a unicode string. Not even in UTF-32. Don't do it, it's wrong. Also UTF-16 sometimes makes peole think they could do that or that they don't have to care about surrogates. UTF-8 at least makes that a bit more obvious. I'm sorry, while UTF-16 may be fine, I've seen so much code that does it wrong (splits surrogates for example, because the string would be too long for the text box). UTF-8 seems to be better off in that regard.
I have to admit, that I had to explain a lot more, when I used remove_if to sort my vector...
There is a world of difference between returning from `main` and exiting with `exit(x);`. In most cases (probably all code most of us have ever written) the use of `exit` and `std::terminate` is unjustified. One should only use `exit` to mean "something is terribly wrong; quit the program right now", not "main has finished execution and this should be the application's status code".
Yes there is, the sub.misc.unsafe class allowqs Java developers to rip any JVM security and Java language guarantees apart.
If I get this to work with Unreal Engine I'm gonna buy you a beer!
I did not know PACXX before, thank you. Perhaps you should add a section to the README to explain the differences to Kokkos. But well. The API is ... it has room for improvement. `Executor` has a method `allocate` (humm). This returns a representation of memory that provides method `upload`. You see what I don't like about that?
Can this also work for the `operator -&gt;` in `std::unique_ptr`? When I call a method through that operator, I really don't want to jump into `std::unique_ptr` internals, I just want to jump into the method right away.
Arrh, sorry, of course I know it. Nevermind the comparison with Kokkos.
LOL butthurt much?
Compile times comes to mind
Ok but lets think about the pros and cons of this. The cons are that now your optional may be accidentally comparable in surprising ways. There was an example on andrzej's blog. std::optional&lt;int&gt; aircraft_weight = ...; if (aircraft_weight &lt; limit) { proceed(); } else { signal_error (); } The optional may be used to signal absence of a value or failure to compute something, but because of implicit conversions and comparison operators, the program will think an empty optional is less than the weight limit. And the supposed benefit, e.g. `std::set &lt;std::optional &lt;int&gt;&gt;` actually has some drawbacks, compared to just `std::set &lt;int&gt;` and a `bool` flag on the side for empty object. When I search for an `optional&lt;int&gt;` in the former, it is going to check whether I have None at every step of the search, because that is how comparison is implemented: first check none types, then fall back to `std::less &lt;int&gt;`. With a normal set and bool flag on the side I can avoid those unnecessary extra checks and only do that check once.
Fog Computing and Edge Computing are both up and coming emerging technologies. I'm writing a paper that surveys the current state of the art of Mobile Edge Computing. I'd be interested in the type of work being done in this field from a software perspective. 
That's why I said I was skeptical about comparing `optional&lt;T&gt;` to a raw `U`. But that problem doesn't exist for comparing two optionals.
I skimmed this and have no idea what it is trying to say or what point it is trying to make. 
Looking forward to it!
&gt; Visual Studio version 15.5 Preview 4 we’ve annotated our standard library code with hints to the debugger that enable this behavior. How is it done? Some kind of attribute?
I see exactly where you're coming from and you make a fair argument I just happen to disagree :). Because there is a non-zero chance that there will be a well supported target that does not have `CHAR_BIT = 8` that will have a C/C++ compiler through either gcc or clang (more likely). I think these shouldn't be thrown away when it has been a goal of C++ to never pessimize a platform through assuming that things work in a way that they don't on the platform. If this was not one of the ISO Committee's goals, I would agree, but because it is, I think it is more important for them to stay consistent.
You really need to work on your English reading skills. It is dying on the enterprise space, not on the other areas. &gt; ML, AI, HPC, Fintech, games, embedded, audio and graphics codecs that is where C++ rules. Do you happen to understand what **C++ rules** actually means?! &gt; There's even a resurgence in desktop development in C++. Where? Microsoft is one of the companies pushing Electron apps, and although UWP is written in C++, everyone is using it via C#. Almost no one is picking up C++/CX and C++/WinRT will only reach feature parity next year. Andrew Pardoe mentioned already that the C++/CX uptake wasn't as expected. Google is pushing ChromeOS (Web) and Android (Java). Android has the NDK, but it quite constrained, with the goal of implementing Java native methods, 3D rendering or real time audio. Apple is pushing macOS and iOS (Objective-C and Swift), they even took offline the Objective-C++ documentation. C++ is used on drivers (IO Kit) and Metal 2 shaders. Even Qt nowadays is pushing their JavaScript dialect QML, on top of C++. C++ is there and isn't going away, it has become a language to write OS infrastructure code, not what average Joe/Jane use as their daily **enterprise** language.
You'd also have to require that the assignment be to an auto variable, since assignment to a `std::function` or function pointer wouldn't be applicable without an explicit capture. It would just feel weirdly circular to have the validity and semantics of an expression be dependent on what it is assigned to, and very much unlike the rest of C++. Personally, I really want to have recursive lambdas, but feel sort of "meh" about having to give them names. If the proposal is adopted, I'm pretty sure 95% of my lambdas will be unnamed and the remaining 5% will all be named `recurse`.
Hell yes
Yeah, how do we turn it on for our code? How do we turn it off for STL code (when you *do* want to step into these functions)?
&gt;Yeah, how do we turn it on for our code? How do we turn it off for STL code (when you *do* want to step into these functions)? The sentence after the quote already supplied. &gt;We know that this feature could be useful for other libraries as well and are working on a way to generalize this feature and make it available to developers.
Sure I would be happy to share more. As most applications; software is where a lot of the real use cases and business value for edge computing really lies. I am writing a 4 part blog to precisely cover this question from a software perspective. Here is Part 1 of 4: https://www.foghorn.io/building-edge-intelligence-platform-iiot-part-1-4/
Would love to see this for RxCpp.
This might seem like a great goal, but many of the MSVCisms were created for Windows. If you're including Windows.h you're almost certainly targeting Windows. I'd like to see this happen too, but it's not the biggest priority. I'd rather see MSVC developers be able to use all other code that's out there like Range-v3 and Hana. 
&gt; but is it only because nobody deemed it possible though ? Sorry, I don't follow. Deemed what possible?
/u/agcpp, are you the extension author? I've been using cquery on Win10 with Clang trunk for about two months and I've never been sure whether the issues I encounter are Windows-specific or not. I had to make some small changes to get the code to play nice on Windows, and most things work great, but some things like parameter tooltip for function objects are notably absent and it makes me wonder what I might have missed...
First code block has unnecessary semicolon on line 18.
The thing is this: The compiler would only have to check if a function is constexpr, when you execute it in a compile-time context (e.g. to initialize a constexpr variable) but in those cases, the compiler has to do all those checks anyway.
Thanks for reporting that through the IDE. There are benefits to that on our side. (Also, we're pushing to make it so that you can go directly to [the bug tracking tool online](https://aka.ms/vs-rap) and report the bug without opening the IDE.) We're currently moving our lambda parsing to the new parse trees. It's always good to have new test cases for this! I'll let you know what I find out. 
I don't get the problem here. Why do you want to remove virtual inheritance here if it's cleaner solution than both macro and template? What's the relevance of `dynamic_cast` here? It has a runtime overhead but why would you do so if you have virtual functions? Also, if you are sure about given object you can safely downcast with `static_cast`.
Should I disable the C++ extension to use this one correctly?
&gt; This would get easier if we had a real interpreter in the compiler for constexpr expressions. sounds like something that would have its place in [metashell](https://github.com/metashell/metashell)
&gt; Deemed what possible? this: &gt; you might want to change the implementation in the future in a way that doesn't allow this anymore what if the compiler allowed for complete "run-time" capabilities at compile time. eg not only pure computations but also I/O, etc. Hence, there wouldn't really be any "non-constexpr-evaluable" functions
Also the recent Connect video on VS Code debugging: https://channel9.msdn.com/Events/Connect/2017/T221
&gt; UWP Eww. But seriously, it's *worse* than WinAPI. Making complex (and high-performance) GUIs seems to be literally impossible with it. Furthermore, it doesn't support many APIs essential to many programs. Only reason I can see anyone using it is if you care about Windows Phone (that's dead now) or Windows Tablets (very small market). For desktop WinAPI or Qt are both much better choices. Additionally, it also doesn't even support Vulkan. Not to mention that the Microsoft Store is incredibly crappy, if you actually try to use it ([here's a quick cap](https://i.imgur.com/LIiDu8x.png) from the front page of the store). I also often wonder why the UWP calculator included with Windows [use **15**MB of RAM](https://i.imgur.com/e7q2eff.png)?
Hey all, I'm the author of cquery, let me know if you have any questions! I wasn't quite ready for this to be posted - eventually cquery will be available through the vscode marketplace.
So, it is allow to use `static_cast` with virtual inheritance? That would be superb, I didn't know about that. Can you point me, where it said, this is allowed? https://isocpp.org/wiki/faq/multiple-inheritance#virtual-inheritance-casts
Those who own money don't care about these details. But they notice that if you hire 10 devs and ask them to write an app that does what they need -- in most cases C++ devs end up going to deliver (if!) much later than Java devs. There are billions reasons for that and zero f--ks those (who do financing) give about them. 
I'd be very happy if you uploaded PRs or filed issues for those changes :) Parameter tooltips should work on function calls.
Yes indeed, thank you.
Yep! The C/C++ extension will make semantic operations much slower, especially on a large project. There are alternative extensions for debugging (native code debug) and formatting (clang-format) which work well.
 libclang crashed for I continue to have this crash on all project files (MacOS). Moreover, I need to manually set the include folders, in case I don't use the compile_commands.json? THanks :)
That sounds like a bad project configuration. What build system are you using? Can you share the arguments you're passing to cquery? Feel free to file an issue on github if you prefer to discuss there. Yep, if not using compile_commands.json include directories need to be setup. cquery uses clang under the hood which essentially builds (but does not code-gen) the project.
Of course, and I'm in complete agreement with that. Especially since, as far as I know, there is nothing stopping me from writing my own minimal module for Windows, exporting just the few symbols that I actually need. 
Well, for 1 thing, because then you would be programming in Common Lisp, not C++. nudge nudge, wink *WINK*
You can't downcast with `static_cast` if using virtual inheritance, except for downcasting to the exact type of the object. If you're not _certain_ you're going to the final type of the object, it's not safe. Consider the standard virtual inheritance triangle example: struct A { int ia; }; struct B : virtual A { int ib; }; struct C : virtual A { int ic; }; struct D : B, C { int id; }; If you have an `A*` (a virtual base), a downcast to a C* can't be done statically, even if the actual type behind the pointer is guaranteed to be derived from C. In C, A might be placed at the start of C such that the offset between the two types is 0. In D, the A used could be placed at the start of _B_ instead - so the offset between A and C is non-zero. Theoretically this could be allowed if the class being cast _to_ is marked "final", preventing any further children, as then the exact offset would be able to be known statically, but I believe the standard hasn't caught up to that yet.
It's about the concurrent interaction of different objects.
Managed to get a graduate job though a bunch of applying and through agencies. Then using that experience i gained ( worked for a bout 6 months ) managed to find another grad role within a games company, currently still here.
This is a simplified answer. `static_cast` can be used anywhere when you can guarantee that the resulting type is valid. For built-in types it is always valid bevause they have pre-defined convertions. But for inheritance example: class Vehicle { /* interfce functions */ std::size_t getTyreCount() const = 0; }; class Car : public Vehicle { /* ... */ }; class Truck : public Vehicle { /* ... */ }; std::unique_ptr&lt;Vehicle&gt; p = ... ; // normall approach Car* ptr = dynamic_cast&lt;Car*&gt;(p.get()); if (ptr != nullptr) // do something car-specific // corner-case approach - you are sure that the object has correct derived type if (p-&gt;getTyreCount() == 4) { Car* ptr = static_cast&lt;Car*&gt;(p.get()); // insane UB if p does not point to object of type Car // do something car-specific } The problem is that you need to guarantee that the object is valid for such convertion. If it's not, you will end in UB (with probably very insane bugs due to how vtable is implemented). `static_cast` for upcasting is always safe. For downcasting (and even sidecasting!) it's safe only if you can guarantee that the object is valid. Compiler will trust you and do not create any run-time checks.
Again, why would you have an interface that takes `void*`? I can't really think of any example where you have virtual inheritance and need to downcast something. Virtual functions exist so that you don't have to downcast.
&gt; But that problem doesn't exist for comparing two optionals. Well it does if you allow `optional&lt;T&gt;` to be constructed implicitly from `T`, because then when the compiler sees `optional&lt;T&gt; &lt; T`, it can promote the `T` to `optional&lt;T&gt;` and then use the comparison of `optional&lt;T&gt;` with `optional&lt;T&gt;`. In `boost::optional` they did allow implicit construction of `optional&lt;T&gt;` from `T`. Because you want it to be ergonomic to use this type -- it's not quite the same as other containers, it's a "langauge-level" construct like `variant` and such. IIRC in `std::optional` they didn't allow these implicit conversions but I don't remember for sure. IMHO the best way is to allow implicit conversion of `T` to `optional&lt;T&gt;`, but disallow comparison of `optional&lt;T&gt;` with `optional&lt;T&gt;`. No one is going to make a map or set of these anyways.
This would be a really interesting competitor to rtags.
Doesn't matter if `optional&lt;T&gt;` is constructible from `T`, template deduction doesn't allow conversions. And the optional-optional comparison is a template.
Why do you need virtual inheritance at all? If the only thing you have multiple bases of is a pure abstract class, you shouldn't need virtual inheritance at all. My solution to these problems is the same: don't use virtual inheritance. If you are just using inheritance for polymorphism it doesn't come up. Using inheritance for code reuse can be ok, but only sparingly. Needing virtual inheritance means you are probably over-using inheritance.
You can't use `static_cast` for ANY downcast to types with virtual inheritance. It doesn't matter exact or not exact type. It is not question of safety. You simply will have compiler error. See this: http://coliru.stacked-crooked.com/a/1929c5fca8ddcc06 And this: http://coliru.stacked-crooked.com/a/7f05ea8565a9ecd8
Ok, how you do this? struct IView{ virtual void setOnClick() = 0; }; struct ITextView : virtual IView { virtual void setText() = 0; }; // implementation struct View : virtual IView { virtual void setOnClick() override { std::cout &lt;&lt; "setting OnClick!" &lt;&lt; std::endl; } }; struct TextView : virtual ITextView, virtual View { virtual void setText() override { std::cout &lt;&lt; "setting text!" &lt;&lt; std::endl; } }; How you would implement `TextView`? Where, `TextView` is the same as View + setText method. 
Ok, let's find andrzej's blog post which documented this exact issue, and see if I remembered wrong / what the truth is: https://akrzemi1.wordpress.com/2014/12/02/a-gotcha-with-optional/ &gt; Another thing that is not obvious and surprises some people is that optional&lt;T&gt; is LessThanComparable (whenever T is LessThanComparable). In this comparison, the value of boost::none is considered as a unique value less than any other value of T. This is a sort of lexicographical order. &gt; &gt; These two features combined together render a “mixed” comparison a valid operation. I think you are mistaken when you write this: &gt; Doesn't matter if optional&lt;T&gt; is constructible from T, template deduction doesn't allow conversions. And the optional-optional comparison is a template. We aren't doing template deduction at the time that we compare `optional&lt;int&gt;` with `int`. We are doing overload resolution. If `optional&lt;int&gt;` defines an `operator &lt; (const optional&lt;int&gt; &amp;)` in its class body, that is usually not a template function, it is just a regular member function of a class template.
Thanks for this! How does cquery compare to clangd?
If they are not executed.
&gt; If `optional&lt;int&gt;` defines an `operator &lt;` ... in its class body [It doesn't](http://eel.is/c++draft/optional). And it shouldn't.
It is hardly comforting that they have instead made `optional&lt;T&gt;` comparable not only with its own type but all other `optional&lt;U&gt;`... yet another reason not to use `std::optional`.
&gt; Theoretically this could be allowed if the class being cast to is marked "final", preventing any further children ... Well, honestly, I think that wouldn't do much. It would be rather be more helpful, to have something for cases single inheritance + multiple interfaces implementation (I think in this cases we could prove that we have stable vtable's). Or just make forward, like using `MyBaseClass::*virtual` :)), this is simpler...
Originally `optional&lt;T&gt;{} &lt; T{}` worked because `T` implicitly converted to `optional&lt;T&gt;`. And the implicit constructor seemed important. So it was either allow that conversion, or do it directly by defining `optional&lt;T&gt;{} &lt; T{}` or purposely disabling it. It wasn't disabled for a few reasons: - this is how boost::optional works. The standard is suppose to mostly standardize existing behaviour, and it is good to rely on things that have lots of experience, ie things from boost. (Of course we changed things from boost::optional, but much of that was due to boost not having lots of C++11 experience.) - `optional&lt;T&gt;` is meant to "work like" `T` (Or "`T` with an extra value"). This is the design of optional. It is not "container of 1" or struct with maybe T, etc. It is "works like T in as much as syntax allows". (Should optional have an operator dot if that ever becomes a thing?) You can argue whether "works like T" is a good design, but if you are wondering about any piece of the design, that is part of the answer. So this also answers why optional have a lots of operations that take a `U`. If `T` can work with `U`, optional should to. This starts with `operator=()`. Assignment calls T's assignment (unless T isn't constructed). In general, optional::operator calls T's operator.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7dn7sp/c_use_in_android_for_the_removal_or_prevention_of/dpyztvc/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
At the moment, clangd is very limited compared to cquery because clangd does not do cross-translation-unit indexing (ie, it only supports completion, goto *declaration*, fixits, etc), whereas cquery supports references, derived types, callers, etc. cquery is designed to support very large projects, so it makes very specific design decisions w.r.t. the data model, indexing pipeline, and multithreading model. I hope when clangd adds indexing they can match the performance - but so far every project I've seen simply does not run nearly fast enough on a code-base the size of Chrome/ChromeOS.
You can just separate out the text and view interfaces, for starters. Of course, you can then come back with: but now let's say I have a class that wants to take a reference to something that has both interfaces. To which I would say: well, this already smells bad if one function needs access to both. Inheritance is fine when it's composing orthogonal things, whether they are interfaces or even implementation. This means flat hierarchies basically (2 level, or 3 level is common: pure interface, CRTP, derived). Once you start expressing lots of is-a relationships using inheritance you're in for a bad time. If your needs in terms of classifications of interfaces (interfaces that are supersets of each other, and other such relationships) are really that complex and you have to use dynamic rather than static dispatch, this is probably where I'd start looking at Sean Parent-esque techniques for decoupling the interfaces and implementations completely.
Assuming clang can compile unreal engine cquery should be fine, it easily scales to million+ line code bases. More CPUs = faster indexing, I use around 50.
You can attack this here instead: struct View : virtual IView { virtual void setOnClick() override { std::cout &lt;&lt; "setting OnClick!" &lt;&lt; std::endl; } }; change that to template&lt;class B&gt; struct TView : B { virtual void setOnClick() override { std::cout &lt;&lt; "setting OnClick!" &lt;&lt; std::endl; } }; using View=TView&lt;IView&gt;; and now `TextView` becomes: template&lt;class Base&gt; struct TTextView : TView&lt;Base&gt; { virtual void setText() override { std::cout &lt;&lt; "setting text!" &lt;&lt; std::endl; }; }; using TextView=TTextView&lt;ITextView&gt;; this keeps inheritance linear. Which highlights an issue with your design. IView IView View ITextView IView::Forward&lt;View, ITextView&gt; TextView your design has *two* copies of `IView` in its class heirarchy. Casting up from `TextView` to `IView` is ambiguous. With the `TView` solution we get: IView ITextView TView&lt;ITextView&gt; TextView a linear inheritance and no virtual inheritance. 
&gt;Additionally, it also doesn't even support Vulkan. Well iOS and Mac OS X doesn't support it either.
We could add a subsystem to your C++ program that is a poorly specified buggy version of Common Lisp. I'm sure nobody has ever done that before.
* Paper: "Reducing Calling Convention Overhead in Object-Oriented Programming on Embedded ARM Thumb-2 Platforms", https://conf.researchr.org/event/gpce-2017/gpce-2017-gpce-2017-reducing-calling-convention-overhead-in-object-oriented-programming-on-embedded-arm-thumb-2-platforms / https://dl.acm.org/citation.cfm?doid=3136040.3136057 * Master Thesis: "A Study of Calling Convention Overhead on ARM Thumb-2 Platforms", https://www.csg.ci.i.u-tokyo.ac.jp/paper/caldwell-master2017.pdf 
Mind blowing... A little bit hard to read, though, but... In my defense I would say that while casting from `TextView` to `IView` is ambiguous indeed, it is still possible: `TextView` -&gt; `ITextView` -&gt; `IView`.
&gt; Well iOS and Mac OS X doesn't support it either. And due to that I don't support neither macOS nor iOS.
I wonder, what if `Base` replace with `Bases...` in your design?
You can do that for your own code or extra STL functions with this: https://msdn.microsoft.com/en-us/library/dn457346(v=vs.120).aspx#BKMK_C___Just_My_Code
You can do that yourself for any function you want. See here how: https://msdn.microsoft.com/en-us/library/dn457346(v=vs.120).aspx#BKMK_C___Just_My_Code
What features are still missing or bugs still there that keep you from adding the project to visual studio marketplace 
Is cquery something like rtags? If so, what are the differences?
There are various user-polish things, making it just work, etc. There is the odd indexing issue which requires nuking the cache directory at times. cquery is much better now though than it was even a month ago - it is getting very close.
Similar, but cquery scales better w.r.t. indexing time and semantic operation performance. cquery also uses the language server protocol.
&gt; I use around 50 CPUs?
All right very cool. Actually I have a more in dept question for you. I have been working on a visual studio code plugin that allows you to autogenerate doxygen comments above functions when typing a configurated character sequence: https://github.com/christophschlosser/doxdocgen I expended way to much effort to be able to parse C++ function definitions and right now it supports 99% of C++ but not everything. So would it be possible to interface with cquery to get that information in an abstract form and then generate the comment stup from it? 
Yep, my machine at work has 56 cores, and I run 50 indexer threads concurrently.
Yea, this should be relatively easy to accomplish. The trickiest part would be determining the function that the comment should target, but even that will only be a couple lines of code. The data is available after indexing is complete, ie, https://github.com/jacobdufault/cquery/blob/master/src/indexer.h#L333. If you're interested in migrating to cquery I'd be happy to provide assistance on gitter / PRs, since this would be an awesome feature to have. Though fwiw it may work more effectively using regexes like what you have now, since then it won't depend on an indexer.
Thanks!
Thanks. I'm currently using rtags in my workflow. Does cquery have the concept of multiple projects, and automatically picking up the right index depending on my cwd? Also, any idea of how difficult it's to integrate into vim (via some plugin)?
Yea, cquery is per-project and not global like rtags. Each project would run a separate instance of cquery. It uses the language server protocol, so vim integration depends on that.
Lol. Where do you work ?? ;)
But doesn't the [optional.comp_with_t] section provide comparisons with the underlying type?
I work on Chrome OS at Google :)
This sounds the most familiar to me. Expertise in modern c++ was just assumed / expected. What I got hired for was architecture design, mostly around platform abstraction, networking, and rendering. The best thing you can do is get real application experience. Come up with a significant enough problem, design a solution, and build it. Dabble in the common but less exciting areas of application development: networking, promises, databases, error handling, unicode strings. Flesh out each into a powerful, useful module. Handle all your corner cases cleanly. Don't be afraid to alter interface design for the sake of error handling.
Should have figured that out when you mentioned ChromeOS...
I think that if you want to be able to insert things into sets and maps, but don't want people to actually be able to write `x &lt; y`, you can specialize (or in this case, partially specialize) `std::less`.
That's a pretty good choice. Did you find it helpful?
&gt; (Also, we're pushing to make it so that you can go directly to the bug tracking tool online and report the bug without opening the IDE.) Thank god! I don't use the IDE, so i really appreciate this.
How does this compare the KDevelop's DUChain based (hope I got the terminology right) clang indexer?
Good to know.
Haha.. starting a comment off with insults is usually a sign of butthurt. Sorry pal!
Well, read better want everyone else is writing on this thread, including outsourcing outside US.
Your first response to me started with a slight. You're obviously easily offended, which is generally a sign of insecurity. I'm sorry I hurt your feelings!
Are there any previous analysis of ABI overhead for x86 or amd64 programs? I suspect this overhead is still significant.
This is one of those things that is really important to understand. Compilers are built to optimize common code. So don't try to outsmart the compile (see /u/stl 's talk [Don't help the compiler](https://channel9.msdn.com/Events/GoingNative/2013/Don-t-Help-the-Compiler)). In the best case, you do nothing but make your code harder to read and maintain, in the worst case, you may make things slower because the compiler can't safely untangle your nonsense. Performance is won by finding slow spots, understanding what they are trying to solve, and making a better solution. They aren't won by making a faster comparison function. (I've yet to see the case where a faster algorithm isn't the answer over trying to use bit shifts and xors to get something.)
Disappointing there were no benchmarks, if u/STL is reading this he can ping http://quick-bench.com/ to Raymond over internal mail for future blog posts. :) Still a great post, although I do not pretend I read the asm I got the idea behind the post. :)
&gt; I've yet to see the case where a faster algorithm isn't the answer over trying to use bit shifts and xors to get something. Look at cppcon talk about Google hash map. They use shifting for performance, not like to speed up division or something, but they pack information in 7 bits of char IIRC.
The current version of said documentation: https://docs.microsoft.com/en-us/visualstudio/debugger/just-my-code
The current version of said documentation: https://docs.microsoft.com/en-us/visualstudio/debugger/just-my-code
As much as it pains me to suggest it, could you please take a moment to enter a on the user voice suggesting this? https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/category/30937-languages-c
I would like some elaborate graphics that may be to elaborate for such mall icon: For me maybe &lt;&gt; for up * for down But it may be confusing due to &lt;&gt; meaning ^ Another alternative would be swiss knife with 7 tools in open position open for up swiss knife with 7 tools in open position (5 of them same as up) for down or katana with small blade handle for up katana with large handle blade for down
That's not even remotely the same thing.
Ah, didn't have in mind that you can recognize multibyte chars in utf8 that way. Good to know. I don't use utf-8 much ;) I'm aware you need to pretty much actually render the text to get the correct size -- but in many cases, good approximation is sufficient, for example when setting the default width of a column in a table view. With utf-16 I can probably get quite good results by just multiplying the average character width with the amount of code units (which is constant time), while with utf-8 for basically every language which isn't English it will be completely off. My experience is that when you count characters, nobody cares whether you count the "pile of poo" symbol as two characters, but if you count "ä" as two it's weird. Similarily, in a lot of cases my whole text will not contain a single high/low surrogate pair, which you can take advantage of. All this put aside, I don't think Qt can switch QString to utf-8, it will break everything. I also don't feel like it's worth it.
&gt; Similarily, in a lot of cases my whole text will not contain a single high/low surrogate pair, which you can take advantage of. How do you take advantage of this aside from simply ignoring the issue?
++ and --
but C, Java, C# also have those... :/
Noob question: how did you build cquery? I guess building it in wsl won't cut it...
I'd force every downvote to be substantiated with a message and every upvote to that message would decrease counter on original post. Same logic should apply to that message, recursively. I.e. if you dislike smth you either provide a reason or agree with someone else's reasoning. Another idea is voting "weights" -- 1 vote per account is not ideal. ("Democracy: where any two idiots outvote a genius" :)) Or at least don't give voting rights until given account earns it in some way. 
They can have them too!
That fixes one of the two main problems with std::function by hiding the root cause of both problems. So, just how much does a program bloat by using std::function extensively? Because those 31 F11s are still in your binary unless they get optimized out.
With Clang trunk on top of VS2017 – no WSL (or Linux of any kind) involved. The build script in /u/sient's repo won't work OOTB. I'll post a followup here once I have my fork laid out sufficiently to file an issue, then you can use my attempt at the script if you like.
Prefix implied rather than postfix, naturally.
[`#pragma inline_depth(255)`](https://docs.microsoft.com/en-us/cpp/preprocessor/inline-depth), friend. ;-]
Maybe it's a second secret message that the wrong way of benchmarking is to look at assembly and declare the shorter solution better without testing it. 
Im confused. How can it be possible for the feature to be 7 years old currently, the article to have been written one year ago, and for the feature to have been 2 years old when the article came out.
I still think something like CsStrings size_codePoints would be a more proper approach. Also limiting a field to a certain amount of bytes can be really bad if you don't handle truncated multibyte characters. Afair C# had a bug, where it produced incorrect UTF-16 strings in langth limited text fields. Also not handling the length of a single ä is probably also not relevant, as german does use mostly ASCII characters. I think the difference would only matter if someone typed the whole line with special characters. It would probably matter more, if you are not using a monospace font and someone only enters i's. Also asian characters would break that again, as they tend to be wider than ASCII, so maybe UTF-8 would be a better fit. Not to say that I didn't have my own headaches with fitting UTF-8 strings in tables. At the time I calculated the bounding box and then rendered again with the text split into chunks with a certain amount of codepoints, because the built-in text wrapping wouldn't work. Ugly and it still wasn't perfect. There should really be a proper way to calculate the proper boundaries and I'm glad that in most cases wrapping at the size of the text box works properly and I know the proportions, I want, beforehand.
Exactly. so what needs to change is not what is allowed inside a constexpr function, but what is allowed inside a core constant expression.
&gt; Should decltype evaluate expression here? Yes, that's the resolution of CWG 1581. To quote, "Constant evaluation may be necessary to determine whether a narrowing conversion is performed."
I haven't taken a super close look at DUCHain, but kdevelop seems to do a nice job - it just crashes too much on Chrome.
FWIW I actually develop cquery primarily on Windows, but have not spent the time getting a portable build system work up - I just have a Visual Studio solution. I'm very interested in the PR! :)
About user-polish things, may I provide another suggestion? The c/c++ extension on marketplace has 1 great feature that helps me prototype quickly with cpp, it generates a default configuration file for each project which resolves system headers automatically and looks for header files in current directory. So when I've to just test some new thing or write small function for which I don't want to write a cmake file and make proper directory structure and generate compile_commands.json and set everything, everything just works. Every other plugin asks me to setup tons of things which I don't want to when writing a bunch of cpp files that include only system headers or custom headers present in current directory(recursive) and its frustrating af. YCM solves this problem by asking to provide a default global config file which loads up when no config is present in cwd, please provide an option for cquery too (or better make it automatic like c/c++ extension does ;-) )
I have uploaded [`cquery-git`](https://aur.archlinux.org/packages/cquery-git/) to AUR so Arch users can enjoy it now.
But, does it work?
Microoptimizations have their place, but it's the last resort. And frankly, it's pretty rare these days.
Downvote arrow: a teeny little Turbo C++ screenshot.
Use the one by Bjarne Stroustrup: Programming Practices and Principles Using C++ (Second Edition). His whole understanding is complete and apparent. I cannot personally attest to the quality of any other books in like genre, but I can to this. I was writing code within the first hour and understanding it - the most crucial part, knowing why you're doing what you're doing. With apprx. five to ten hours of reading, I was tinkering with the in-built tutorial program. I was using code and Machiavellianly manipulating the computer. I thoroughly recommend this book. 
Smart pointers were introduced in C++11, but `std::make_unique` was only introduced in C++14. The later is quite interesting because it makes `new` disappear from your code so you can more easily grep for code smell.
The problem with passing nullptr has been fixed for a compiler update after update 5: https://developercommunity.visualstudio.com/content/problem/130244/c-warning-c5039-reported-for-nullptr-argument.html
I uploaded its PKGBUILD into AUR at [cquery-git](https://aur.archlinux.org/packages/cquery-git), so Arch users can enjoy it now. P.S. the absolute `rpath` at `wscript` makes it difficult to package so I added additional patches to change it to `lib/`. Hope in the future a `./waf install` will make this easier.
You are the one being insecure about C++ not being available everywhere, not even being able to provide an example where exactly `There's even a resurgence in desktop development in C++.` is taking place, after I deconstructed what major companies are providing for their OS SDKs, including what Qt is doing with QML. I also forgot to mention that Windows UI team, only does UWP live demos in C#. Another easy fact to validate on their blog, code samples and presentations available on Channel 9. A fact that is corroborated by fellow C++ programmers posting here, which are also using other language stacks for their UIs, just keeping the application logic in C++. Something easier to find to anyone bothering to read all the answers here. It was easier to play the offend game than provide facts, it seems. Don't worry, your C++ job is safe.
Why would you think the implementations are "incorrect"? The return type of `duration.count()` is `duration_t::rep`, which happens to be `long long` on MSVC and `long`. There's no "problem" here, as the standard says these are implementation-defined; the thing that needs correcting is your `0ll` literal: auto result = std::max(duration.count(), std::chrono::seconds::rep{}); // or auto result = std::max(duration.count(), decltype(duration){});
I just naively plugged in Eigen for the 2D matrix multiplication in my im2col implementation. The results are very impressive! The following table shows the improvement (measured using `GCC -O3`, run on a single core of an Intel Core i5-6600 CPU @ 3.30GHz). Model K+TF fdeep fdeep+Eigen InceptionV3 1.10 s 1.68 s 0.82 s ResNet50 0.98 s 1.16 s 0.66 s VGG16 1.32 s 4.41 s 2.67 s VGG19 1.47 s 5.45 s 3.17 s Xception 1.83 s 2.76 s 1.59 s In some cases it is even faster than Keras (2.1.1) with TensorFlow (1.4.0). I do not know why, but perhaps there is some overhead in Keras/Tensorflow when forward passing just one single input image and not a whole batch. I'll now check how I can handle the dependency to Eigen cleanly and then commit. Perhaps it makes sense to replace my tensor2 implementation completely with `Eigen::MatrixXf` in the future. All in all, thanks again a lot for your suggestion. It lead to a big improvement.
&gt; They aren't won by making a faster comparison function. So, erm, I got wins in strcmp by doing these kind of tricks. Essentially the function ended with: ``` if (difference &lt; 0) { return -1; } if (difference &gt; 0} { return 1; } return 0; ``` Even if the compiler had generated cmov here, that's more expensive than it needs to be. This version was faster: ``` return (-difference &lt; 0) - (difference &lt; 0); ``` which now becomes totally branchless: ``` mov eax, r9d shr r9d, 31 ; grab the sign bit neg eax ; negate shr eax, 31 ; grab the negated sign bit sub eax, r9d ; subtract sign bits ret 0 ``` The advantage in the article is assuming that your comparison function is small enough to inline and have constant prop applied. That's not true for a lot of functions. Of course you have to measure to see if such things are actually worth doing, but they can be.
When implementations differ it tends to make me think one might be more correct than another, but I know that is not always true. I could not find adequate type information for rep. If you mean that the lack of specification of an exact type on rep in the standard means there is leeway for compiler vendors to choose their own implementation I can see that argument, but if there is more exact wording that states it I'd like to dive in more.
504 Gateway Timeout from nginx :-(
Being a dev in the std lib for a major compiler you have a privileged position. In 99% of cases /u/cogman10 's advice is reasonable, hopefully people will use profilers and other forms of real measurement to know when they are in that 1% as you are. Your case was specific to one compiler so you didn't need to consider what GCC, Intel or Clang would do (or worse one of those freaky embedded compilers), twiddling one line of code to manipulate the asm made sense. Because it was part of the std lib it will be used by a huge number of programmers so it is justifiable to spend huge time looking for such small optimizations. Most libs are less popular and used by fewer programmers than the std lib. A similar case is even less likely to be the bottleneck in application code which often has to wait on random stuff from other parts of the system. Even caching to prevent a single file access is likely to provide many times the gains of your style of optimization.
From [\[time.syn\]](http://eel.is/c++draft/time.syn): &gt; using seconds = duration&lt;signed integer type of at least 35 bits&gt;; Note that on x64 Linux `long` is 64-bits while on x64 Windows it is 32-bits.
I'm a bit surprised about your performance issues with `rtags`, because I'm running it on a ~4 million lines project and it works quite nice.
He isn't a magical unicorn and his solution isn't compiler specific. It's very common to have a tight loop that takes a big chunk of your run time - and taking the time to look at the generated assembly and making C/C++ hacks to force the compiler to generate what you want is just par for the course (.. b/c fundamentally C is a bad "cross platform assembly") After all people are often using C++ because they need to shave off these extra cycles Unfortunately to me it seems C and C++ lack a good number of features that would make life easier when it comes to these optimizations
First of all, different duration types can have different representations, as this is a template parameter. So there is certainly no more correct type for `std::chrono::duration::count`. As far as the pre defined types in the standard library are concerned, the duration type has to be a signed integer type with enough bits, such that the duration type can represent at least +-292 years. For second and shorter that means 64 bits and my assumption is that the standard library developers used std::int64_t, which would then map to long on your gcc toolchain and long long with the MSVC one (keep in mind that long on msvc has only 32 bits). Reference: http://en.cppreference.com/w/cpp/chrono/duration
Off-topic, but fascinating that on MS blog GCC was used instead of "x86-64 MSVC 19 2017 RTW". :)
"Apparently you cannot read" = "let me start my post off with an insult to really show this guy how smart and in control I am!" Reasonable people would have just presented their argument. You felt the need to slight someone because you're insecure. And now you're trying to use my own argument against me. :-/ I'm sorry I hurt your feelings pal. :(
Yes, that's what I wanted to say. Now I understand there was a mismatch between the transition I talked about (C++11 -&gt; C++14), which allowed more function to be constexpr, but did not let all of the execution of the new one to be constexpr, and the changes I was thinking of: to allow constexpr execution of more code.
You're not capturing anything, so why not make that a normal (named) function? 
You are the one with hurt feelings, because you are unable to present facts. A behaviour well known to psychologists, when someone fails to present a case, turns to play as victim to attract support and pity of others. Still waiting for **facts** about `There's even a resurgence in desktop development in C++`. You know things like online graphs, statistics and reports from well known sources proving adoption of C++ GUIs in the enterprise space. 
upvote: `&gt;&gt;` downvote: `&lt;&lt;`
&gt; You're not capturing anything, so why not make that a normal (named) function? well, because now you have to scroll back-and-forth for something that could very well be inline.
MS does cross compiler code testing to ensure their libraries and compilers work properly. This guy is a MS employee that maintains that Visual Studio's C++ Standard Library. He has posted on Reddit in the past. * https://nuwen.net/stl.html I think MS uses this special GCC build for it's tests (or at least he does). * https://nuwen.net/mingw.html 
And, by the way, comparing `count` to anything is very dangerous and should be avoided. By using `count`, you lose your units of measurement, which can lead to surprising results later. You should always prefer comparing duration to duration. Howard Hinnat explains everything about `&lt;chrono&gt;` in this [CppCon 2016 tutorial](https://www.youtube.com/watch?v=P32hvk8b13M) ([slides here](http://schd.ws/hosted_files/cppcon2016/d8/%20chrono%20Tutorial%20-%20Howard%20Hinnant%20-%20CppCon%202016.pdf))
This is an assumption though. The thing is, cmov could be implemented totally branchless in say Intel coffeelake and above. In which case the extra assembly to avoid branches negatively impacts the instruction cache. Or maybe it is special cased in the branch predictor or the oo evaluator. This is exactly the sort of trade-off a compiler writer would make. I'm not trying to say you should never do this, I am saying that you should only do this for individual apps in a hot loop after you've identified that this is exactly your problem. Even then, that should be after evaluating the algorithm that lead to this being a hot spot and seeing if it could be avoided. Doing it preemptively is folly.
Upvote: `std::vote&lt;std::plus&lt;reddit::vote_type&gt;,reddit::single_vote_traits&gt;()` Downvote: `std::vote&lt;std::minus&lt;reddit::vote_type&gt;,reddit::single_vote_traits&gt;()` 
Use [boost](http://www.boost.org/doc/libs/1_63_0/libs/tokenizer/introduc.htm)
Spoiler: nothing to do with Excel. It's just CSV.
&gt; Lambdas probably cannot be optimized the same way a declared function is optimizable - because they end up as complex instances. This is incorrect. Lambdas are actually easier to optimize than functions in many situations. A captureless lambda is at least as fast as a function in any situation I can think of (where you can actually use the lambda).
I'm sure you measured it, but I'm still having a hard time believing that a simple `return difference;` wouldn't have been faster... 
It works fine if you're only doing a semantic operation every second or two, but I use cquery to power code lens, which may require 100+ separate reference/call/etc requests very quickly. cquery can do this within 10ish ms, even on large projects.
Thanks! I'd be happy to accept a PR fixing the issue :)
Good idea! I've filed https://github.com/jacobdufault/cquery/issues/28.
Unfortunately I recently came across a case where we absolutely had to help to compiler. For reasons that are too long to discuss here we are using packed bitfields for some data structures in our code. Something along the lines of #pragma pack struct foo { uint32_t a:22; uint32_t b:22; uint32_t c:20}; On GCC accessing b turned out to be extremely slow, when in theory I would gave expected it to simply take the whole bitfield and apply a shift and bitwise and, but the asm code was much less efficient. Clang doesn't seem to suffer from the same issue. Didn't test other compilers.
I have a fairly complicated love/hate relationship with CInt and Cling (https://root.cern.ch/cling) , but I have actually once or twice used it to debug some template instantiation thing that was pretty opaque otherwise
This really misses the point. If you are doing so many small but different allocations, I would say to first figure out how much total memory you will need, then allocate it. The CS101 style of making a loop and doing everything to one unit of information instead of doing one thing to every unit of information was great in the 80s when memory was scarce I'm sure, but these days it means heap allocations, which means locking and caches misses. This tool proclaims to be well multi-thread too, though I can't imagine it scales all that well if they are doing so much heap allocation.
/permissive- by default for new projects sounds great! While you're at it, could you maybe also change the default warning level to 4 and change to the 64-bit hosted toolset by default? :-)
Why does RTTI get all the hate?
Thank you for the vote of support! With regards to your second two suggestions, I'd personally love to. But the high-level bit for Microsoft is compatibility. We seriously don't want to break people's code, build systems, arms, or anything for that matter. "Changing defaults" will be a theme in planning the next release. Another one you forgot is [caret diagnostics](https://docs.microsoft.com/en-us/cpp/build/reference/diagnostics-compiler-diagnostic-options), by the way. As always, I'm going to suggest you start a [User Voice](https://aka.ms/uv-c) suggestion. We actively review these things and get hounded by management on what we're doing about them. [Proof in this blog post from, not coincidentally, a manager](https://blogs.msdn.microsoft.com/vcblog/2017/11/08/completed-uservoice-suggestions-in-visual-studio-for-c-developers/).
There are always better alternatives man, I will take fmtlib over boost.format any day.
I like the voting weights part, plus reason for dislike or like.
My understanding is that it *should* work. I used TSan on a mid-size codebase with basic (parallel for with various reductions) but widespread use of OpenMP with no false positives in the last few weeks.
Wouldn't the same goal be achieve, with far less code, by using a stateful allocator: - Create a `SmallAllocator&lt;T, N&gt;`, which contains space for `N` instances of `T`, - Create a `CompositeAllocator&lt;A, B&gt;` which first tries to allocate with the first allocator, then uses the second if the first fails. Compose: template &lt;typename T, std::size_t N = 0, typename Alloc = std::allocator&lt;T&gt;&gt; using dyn_array = std::vector&lt;T, CompositeAllocator&lt;SmallAllocator&lt;T, N&gt;, Alloc&gt;&gt;; *Note: a small flaw in the above plan might be that unfortunately the `Allocator` concept does not feature a `reallocate` method; it can be circumvented by making `dyn_array` a proper class which reserves `N` instances in its constructor... but `N` will have to be carefully chosen to avoid `vector` asking for more.*
Thanks, done for the 64-bit hosted toolset which I consider the most important one: https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/32308945-make-64-bit-hosted-c-toolset-the-default-for-new You're right, I forgot /diagnostics:caret! Why is this even still a project-level option :-(
(coming back to this a month later to flesh this out a bit more) With a pure virtual interface interface, you have no way of treating the object as a value type. You * have to use factory functions rather than constructors, which means you can't use the copy constructor (and get value semantics), but instead must use e.g. a `.clone()` method * always have to access the object by pointer or reference, which is merely syntactic, but nonetheless annoying
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7dspqv/does_tsan_with_openmp/dq0mkej/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Now that you mention it a lot cooler feature would be allowing cquery to parse the comments above a class/function/enum etc and show this documentation while providing intellisense the same way MS does in visual studio for C#. Is such a thing possible? I have never worked with clang libraries before but this is a feature that will go perfectly with documentated functions. If it is possible I am very interested on working on it.
Comments are shown for code completion already, but not yet for hover. I believe there is an issue on GitHub for this specifically.
Comments are shown for code completion already, but not yet for hover. I believe there is an issue on GitHub for this specifically.
cool but I meant something more like this: https://i.stack.imgur.com/3MKER.png So it has knowledge you are currently putting in whcih parameter and specifically wlll mark that documentation for you. This would be really helpfull with function with lots of parameters.
OK, article but 2 things: small_vector is not a dropin replacement for std::vector. IDK the exact requirements but I think std::vector swap guarantees for iterator invalidation mean it can not have internal buffer. Ubisoft uses profiling for this kind of things(if you are bored you can go through cpp con videos from this or last year and find a video where they talk about this). So they do not have to manually guess the size of internal buffer, they run the code on real scenarios and they have the statistics.
But they are not called C#`++` or Java`++`?
I couldn't find any tutorials / examples of WebGl and Cheerp working together. Did I miss any? Also I assume that unlike Emscripten Cheerp doesn't convert OGL to WebGL?
Thank you!
At least if memory serves, there are also some problems with exception safety (e.g., `vector::swap` is `noexcept`).
&gt; At least if memory serves, there are also some problems with exception safety (e.g., vector::swap is noexcept). I guess, there is so much cr*p in C++ that most of the times I just remember true/false and do not have the interest/brain to remember why. :)
There's many problems and memory management just isn't that simple. A good case in point: reports of Unreal Engine 4 struggling on 16gb of RAM and wanting more, when more intelligent engines like Cryengine seem to be fine on much less. There's a lot to be said about memory management but in all practicality the general advised solution is just to throw more hardware at the problem. If you have 2-100x the memory you need then you can save mental work as you're programming. Or better yet - use a higher level language. Most of what you're dealing with anyway is simple "container" based structures and programming. Ironically programming is supposed to get away from all that, in theory, rather than simply tie you down to an existing interface each time and emphasize it as a dogma. But that's where C++ is in practice, and it's where it will continue to be for some time (due in part to memory management styles). A lot of programming is based around simpler ideas and simpler moves away from primitive APIs. You end up with programmers getting bored and burned out because to interact with a sytem you have to get to grips with the nitty gritty and its core detail. Whereas in other languages even, this isn't a concern anymore at all (C# with no headers, no circular references and no DLL hell). The constraints you are chained to are all about how you approach the problem of software engineering.
I guess I can't blame you for that. 
On the topic of documentation.. Whats up with it? Why are their so many different links and places for the same documentation, all at different URLs, and whys it all so hard to find, among other things? And with VS2017 it got worse, with articles being moved to yet another new URL and only *some* of the old ones having links to the current version. Though the new one seems to have more sensible URLs and design, so improvement? but how will it handle different versions, VS2018? And then the documentation for the Windows API is all over the place, and who knows where, and i've never figured out how to find anything from the Microsoft site(which one for my docs??), only google can find everything. And then some of the docs are outright wrong, outdated, or describing features with bugs that stop them from working(and apparently are "working as intended"), and sometimes formatting is horribly broken. Such as Profile Guided Optimization, a feature that requires obscure hacks that i can't remember right now, found on some obscure post somewhere on some other docs for an old version, to even work, or the plugin for it that hasnt even existed for any version beyond VS2015? [Apparently the plugin has semi recent comments mentioning that very issue, from august, saying there are no plans to bring it to VS2015 or VS2017 and the docs will be updated.](https://docs.microsoft.com/en-us/cpp/build/reference/profile-guided-optimization-in-the-performance-and-diagnostics-hub) They don't seem to have been updated to reflect that.
Chromium's [stack_container.h](https://github.com/adobe/chromium/blob/cfe5bf0b51b1f6b9fe239c2a3c2f2364da9967d7/base/stack_container.h) basically does all that.
No problem. I've sent this to my documentation manager who can answer it more thoroughly. I actually find that the VS 2017 docs are way better than the old docs. One huge benefit is that you can now submit PRs to fix the docs. The change in URL is difficult, but one advantage is that you can just scope your Google searches to that domain, e.g., [profile guided optimization site:docs.microsoft.com](https://www.bing.com/search?q=profile+guided+optimization+site%3Adocs.microsoft.com&amp;qs=n&amp;form=CHRDEF&amp;pc=U470&amp;sp=-1&amp;pq=profile+guided+optimization+site%3Adocs&amp;sc=0-37&amp;sk=&amp;cvid=E4F231B8651C472AB3A7E00E11A838E6)
What's your company?
Is this intended for writing new/simple code in c++ or is it viable to attempt to port a large codebase to run within the browser? 
Wow! This is impressive indeed! Very nice :-))) Thanks for reporting back the results!
Btw, I'm wondering, how large are these models in the .json format? I think models like ResNet50, VGG or V3 are easily like 100MB and larger, in h5 format. That must result in like 500MB and much larger .json files? Apart from the huge space requirement, is that not awfully slow to parse? (even though I'm sure nlohmann/json does an awesome job)
&gt; eventually cquery will be available through the vscode marketplace. Do you know when this will happen?
&gt; Creating a std::string has a cost. It contains a small memory overhead... The solution is of course `std::string_view`, which has its corresponding `sv` literal.
An attempt to get access to all the source you run through it :0
I think u/STL once told me VC++ will break on allocators that return memory from inside themselves... Could be I remembered it wrong, it was 5+y ago...
thank you!!!!
i had to install libtinfo5 to get it to build correctly [pic](https://imgur.com/a/mobyT)
You can implement the Dllmain() which gets invoked when the DLL gets loaded, unloaded or a new thread starts in a process and add the logic to test the condition there.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
What could be the reason behind astonishing performance boost in clang 6.0!
That would break the API contract (on the other side of a DLL boundary).
&gt; the extra assembly to avoid branches negatively impacts the instruction cache There are more instructions in the branching version.
&gt; In 99% of cases /u/cogman10 's advice is reasonable, hopefully people will use profilers and other forms of real measurement to know when they are in that 1% as you are. Before I was a standard library dev I was a plain ol' application dev and routinely got quite a lot of things to be ~4X faster by washing them through the profiler. To the point where people started reporting bugs that they thought the new version of the application was "broken" because it seemed to fast to be doing anything. :) The point is, you need to measure. A sweeping generalization like "don't ever do X" isn't useful; a profiler tells you when you need to do X. If the profiler tells you 90% of the exclusive time of your app is in a function, that function is integral to your application, it's instruction peeping time. Before you have that data, it's not instruction peeping time. &gt; you didn't need to consider what GCC, Intel or Clang would do I consider what they do and file bugs against my friendly optimizer friends all the time.
I think installing `ncurses5-compat-libs` rather than `libtinfo5` is a more popular choice.
If you want to call web api from wasm, it's still extremely inefficient. The only solution is implementing gc inside wasm, which may be in next standard.
Fixed.
A little old-school, but maybe "new" and"delete."
I'm just installing it from AUR on ArchLinux. It downloads and builds from git repo. I noticed it started to download version 4.0.0 of clang. Any plans to upgrade it soon? Clang 5.0.0 has more full C++17 support, 4.0.0 is already outdated by now for this application.
I didn't mean to say "don't ever do x" I just meant to indicate your position is very different than most. Even just being in a position where one can spend real time in a profiler is not normal in my experience. The more I work on libraries the more time I can spend on optimization, but even there once it works fast enough we must move on. Application devs spending enough time to get a 4x speedup just isn't common. Too many people demanding results too quickly. Results usually mean new features that can be sold and performance is hard to do that in many products. I am glad you do look at GCC and clang and seek to emulate them when they are winning, but I think you know that is not what I meant. I meant that if your optimizating change has a slower result on those compilers but faster on yours then you don't really care. Any dev building for multiple compilers cannot generally justify such tradeoffs. Either the code must be made more complex with ifdefs or the tradeoff is not made. It seems unlikely for some one working for a major compiler vendor to need to write code that is faster on some other compiler as part of a daily routine. Thank you for taking the time to respond.
According to the documentation of that function (MSDN: https://msdn.microsoft.com/en-us/library/e0z9k731.aspx) that API is defined as returning either less than 0, 0, or greater than zero - not just -1, 0, or 1. Considering that you thought it worthwhile to optimize this, I'm surprised to see that it was somehow seen as necessary to spend additional cycles to unnecessarily restrict the range. 
Is this performance boost reproducible on Intel chips?
Yea, I'd like to upgrade, but on my first attempt it caused indexing to fail on Chrome. Chrome is still on C++14 so I haven't spent the time investigating fixing it yet.
I'm aiming for December/January.
If you change a function which previously returned -1/0/1 to returning negative/0/positive you're basically guaranteed to break something regardless of what the documentation says, and it's the sort of breakage that can very easily be missed for a long time.
200s for me.
Yes, in C++ an allocator is a *handle* to the memory, it cannot contain the memory itself. But there is a trick: the container itself can contain the buffer thus avoiding having to allocate it explicitly as a separate object. See `build2` [`small_vector`](https://git.build2.org/cgit/libbutl/tree/libbutl/small-vector.mxx) for example.
We might need a "COM to C++17" article
The weights in the json files are stored Base64 encoded. So yes, there is a size gain, but not a huge one. VGG19 for example is 575 MB in h5 format and 776 MB in my json format. Loading/parsing and constructing VGG19.json with fdeep takes 11.2 s on my PC. So yeah, this probably could be faster, but I guess in most cases one loads a model one time at program start and then uses it many times. Another thing, that bothers me more is the RAM consumption during loading. I would like to reduce it in the future.
Yeah. Still, this is the best "bottom-up" (as the author himself says) article that explains concepts and workings that I've found.
The copy semantics thing is definitely true. There is a good amount of boilerplate necessary (or at least some what inelegant CRTP weirdness) to make copying work correctly. In general though, I find it’s very rare you want pimpl or pure virtual interfaces for value types. Maybe it’s me, but I tend to do very little mixing and matching; my objects either have identity and encapsulate details, or they are plain, transparent and structlike. There might be some exceptions though, like RAII style classes and such.
Why is the redis test without optimization? This makes it pretty useless
But the point of the proposal is that the body is only a single return statement that being pre-parsed in an unevaluated context while creating the method definition, the assignment expression received is used to produce a noexcept and late-return-type clause, the tokens are then rolled-back and late re-parsed, this type evaluated, when filling the content of the body. [&amp;](auto&amp;&amp; x) =&gt; f(y, x); Produces exactly this the same as: [&amp;](auto&amp;&amp; x) noexcept(noexcept(f(y,x))) -&gt; decltype((f(y, x)) { return f(y, x); } In that sense I don't understand the critic as this proposal is a sortof macro: #define abbreviated(...) noexcept(noexcept(__VA_ARGS__)) -&gt; decltype((__VA_ARGS__)) { return __VA_ARGS__; } As noexcept &amp; decltype take unevaluated expressions, 'y' and 'f' in that context are not the captured members of the lambda but the original 'y' and 'f'.
&gt; Also I assume that unlike Emscripten Cheerp doesn't convert OGL to WebGL? It's just a transpiler, it doesn't know anything. It doesn't have the same use case as emscripten, which is, for example, to take an already written game and port it to a browser with little to no effort. &gt; And does it support C++17 features like constexpr? It uses clang as the frontend. Cheerp is mostly on the codegen side. The current version seems to be 2.0RC1 and it has clang 3.7. &gt; Also the documentation is not working? The doc is a bit sparse, it's mostly in the [wiki](https://github.com/leaningtech/cheerp-meta/wiki).
So what's the intention behind creating Cheerp. What problem it solves? What can't i directly code in JavaScript for web when it gives me more flexibility than C++
Bjarne and Linus
Eigen is now [integrated cleanly](https://github.com/Dobiasd/frugally-deep/commit/f2c5b4723b56ac9728325b5f709495fb9078a894). This allowed me to remove some copying when converting to `Eigen::Matrix`. Now the performance is even better. :-) | Model | Keras + TensorFlow | frugally-deep | |-------------|--------------------|---------------| | InceptionV3 | 1.10 s | 0.78 s | | ResNet50 | 0.98 s | 0.68 s | | VGG16 | 1.32 s | 1.57 s | | VGG19 | 1.47 s | 1.98 s | | Xception | 1.83 s | 1.34 s |
I looked at both emscripten and cheerp and ended up writing a small-ish project with cheerp. Cheerp is pretty awesome for a web application. It usually doesn't include anything you don't use and, without optimizations, maps your code pretty much 1 to 1 from C++ to JavaScript. Since it's based on llvm, it can have a rather optimized codegen. It also knows about standard JavaScript objects, so your code ends up using `Array` and `String`. Emscripten, on the other hand, is basically its own VM. It allocates an array as a "heap" and has its own "malloc" that puts stuff in that array. Your objects all become "pointers", that is, offsets in the "heap". It's great for taking an already written codebase in C++ and just transpiling it to JavaScript, but it kinda sucks for a new project. It's also mostly undebuggable since all your variables are basically just integer offsets in the heap array. The main advantage for me was being able to share code between the backend and frontend. No more PHP, no more Javascript. My backend is in C++ and runs as a service/daemon. The front end is in C++ and shares a lot of common code with the backend. JSON is the glue for serialization in AJAX requests. Honestly, I'll always go for transpiling from now on when I can. I don't want to write JavaScript anymore. For me, it's become the assembly of the web. Nobody wants to write in assembly. We use modern languages that compile into it. I do the same with JavaScript now and I'm much happier.
It isn't &gt; All tests were built under each compiler while the system CFLAGS/CXXFLAGS were set to "-O3 -march=znver1" 
If those redis results are real then AWS need to upgrade their redis elasticache offerings, pronto!
But why does every test show -O3 -march=znver1 in the compiler options, except the redis test?
That the gcc builds compiled redis in debug mode but clang did it in release mode.
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7ddd8u/how_did_you_get_your_job_as_a_c_developer/dq1gxge/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I don't think so, unless you did that testing and are now telling me the fact how you configured it. See clang 5 has comparable performance to gcc too, so it doesn't seems clang was in release mode.
For anyone interested, there is now an Emacs client with completion, diagnostics, fixits and semantic highlighting, with more to come! 
And the redis test shows only gcc options, no clang options: ``` 1. (CC) gcc options: -ggdb -rdynamic -lm -pthread ```
I was thinking the same.
Can't compilers optimize `operator=` if they know the size of the character literal?
That is indeed interesting. From the forum: &gt; [Michael Larabee] The compile CFLAGS/CXXFLAGS flags were obviously maintained the same during testing. Will do some checking to see if Redis is injecting anything odd based upon compiler name/version but it was just a clean stock build each time. 
In C++03, this would certainly be the case. In C++11, since allocators are now stateful, it may depend on the specifics. One such specific would be that such an allocator should not be moved while memory is allocated from within its internal buffer, for example. There may be others but I cannot think of any of them easily.
You could try to do some profiling on rtags to see what it is spending time on.
Where?
Why not use a tablet or (small) laptop instead? I wouldn't see how you can be productive on a phone.
Yeah the thing is i don’t carry my laptop around with me all of the time as I do my phone. I do write fixes on my phone and committing it to the server through the git-client Working Copy but I can’t know before the commit if the code even compiles or passes the tests. Am I the only one who works like this?
&gt; I found HPX, but it seems to be somewhat unconcerned about performance with every task launch taking a mutex lock. What kind of workload do you expect to launch through `define_task_block`? HPX spawns a lightweight thread for each of those tasks. Last time I looked this incurred an overhead in the range of about a microsecond. This overhead might be acceptable depending on the amount of work you are planning to run concurrently in a task block. In our use cases these overheads are usually not a problem as once you are going for a fork-join parallelization construct (like `define_task_block`) the anticipated synchronization overhead for the join operation -- when using conventional systems (like openmp) -- will be in about the same ball park anyways.
A few thoughts. * use an ssh client to do your command line. I've only done this connecting to Unix flavors, I assume it is doable to windows * don't worry about the many commits in git. Work in a feature branch that you can to a squash merge or interactive rebase. * at least in Android, there are keyboards intended for coding. I understand that they make things like &amp; more easily typed 
HPX doesn't acquire a lock every time it launches a task.
The weird thing is that apparently, boost is only used in cpprestsdk for Boost.ASIO which can be replaced with the header-only non-boost ASIO (websocketpp supports this). So normally, the exact same result could be achieved without having to build a single library.
Don't write titles like this please. Put some information in there, not nonsense. This sub can at least set the bar a little higher.
The rtags cpu consumption is very low, rdm &lt;%5. Are you suggesting I should kick off a gdb on rdm and see what it's doing? 
My misunderstanding then. I'm looking at void run(F &amp;&amp; f, Ts &amp;&amp;... ts) and it uses typedef hpx::lcos::local::spinlock mutex_type; std::lock_guard&lt;mutex_type&gt; l(mtx_); tasks_.push_back(std::move(result)); Looking at spinlock its highly optimized, so maybe no more expensive than a lockless approach anyway. 
It's high time u hit [Github](https://github.com/search?l=cpp&amp;q=REST&amp;type=Repositories&amp;utf8=%E2%9C%93)
I need to get off Cilk eventually since it is abandoned. TBB is possible, although I wouldn't call it elegant. define_task_block seems very familiar, but if the extra TBB complexity gives better space/time bounds then I don't care that much about define_task_block's elegance. I wasn't sure if define_task_block was intended to be as fast as TBB, or if it was more slanted towards ease of use.
You make a really good point about the cost of joins being similar anyway. Do you have an observation about the current or potential space bounds (stack pressure)? TBB advocates a continuation style that somewhat emulates a Cilk-style cactus stack since the execute function returns before the continuation is executed. I wasn't sure if this was possible with the define_task_block structure. 
Emulating `small_vector` with a `std::vector` and a stack allocator doesn't quite cut it for a couple of reasons: * Even with stateful allocators in C++11+, you cannot store data within the allocator itself, due to the requirement that copies of allocators must compare equal. This means that the allocator needs to hold a pointer to some arena defined elsewhere, which is quite inconvenient. * Even if we fixed this, the design of `std::vector` means that the vector's "large" representation (usually three pointers) cannot overlap with the "small" representation (which would be contained within the allocator) effectively meaning at least 24 bytes of wasted space on a 64-bit system. It will also be much less cache-friendly in the case where you something like `vector&lt;small_vector&lt;T&gt;&gt;`. * Iterator invalidation rules will be slightly different for `small_vector` and `vector` (just as they are for `std::string` with SSO and `vector`). This isn't a major issue, but it means we can't just add a "small buffer" to the existing `vector` template.
Occam's razor applys here. Based on those numbers it's much more was a systematic error with the testing process rather than some exception new speedups in clang 6.
Or clang 6.0 has a pretty serious bug.
What a hot mess UScript was. Impossible to debug and from what I remember the idea of a type was pretty vague.
Cool, really nice! Btw, maybe you don't even need to convert/copy data. Have a look at `Eigen::Map`, you can map around data in memory and then use most Eigen functionality directly. Maybe that's just my personal opinion but I find it a bit confusing to find a type like `eigen_mat` which is actually an `Eigen::Matrix&lt;...&gt;` - in such cases I often stick to the convention of the used library to make it obvious that their type is used, e.g. do something like `using RowMajorMatrixXf = Eigen::Matrix&lt;...&gt;` etc. I also think you're "building" Eigen on travis, may I ask if there's a particular reason for it? Why don't you just do the `hg clone` and then add that directory directly to the include path? Maybe there's a good reason for it, I just can't see it yet :-)
As hard as it is to believe today, COM still works, and it works the same way it worked in the 1990s.
I like the idea very much. Has anyone tried this out?
C++ doesn't require you to know any more math than C or C#. How much math you want to know just depends on what you want to program. 
Yes. The more I think about it, the more I realized why in the various policy-based designs I've seen: - the whole storage is abstracted over: allowing both more optimized storage and moves, - the allocator used is a raw allocator, a separate concept from `std::allocator`, with a different interface and set of requirements. I suppose it's inevitable, that not having been designed for this, `vector` would not fit right in; but I must admit I find slightly disheartening that so much effort has to go to reimplement pretty much every method when "only" the storage changes.
I also looked at `Eigen::Map`. However, the [one huge matrix I always construct](https://github.com/Dobiasd/frugally-deep/blob/7919edb1207df051e92cd810056492d83fb76156/include/fdeep/convolution.hpp#L130) is filled with pixel values in an order specific to the ongoing convolution parameters, so this can not reuse memory from my `tensor3` with reshaping. But I will check further with the profiler, where the time is consumed. --- Good suggestion with the naming convention. I just [renamed `eigen_mat` to `RowMajorMatrixXf`](https://github.com/Dobiasd/frugally-deep/commit/7919edb1207df051e92cd810056492d83fb76156). --- `make` for Eigen is a basically a no-op, and `make install` just copies the headers to the systems include directory. So it is not relevant for the time a build on travis takes, but I personally find `make install` cleaner than adding an include directory to the compiler flags. It also fits better to the recommended installation process of FunctionalPlus. ;-)
You might to read the relevant page on some [differences between C# and C++](https://isocpp.org/wiki/faq/csharp-java).
&gt; My biggest fear is that i cant work with c++ because of math. There is a long-standing perception that you have to know lots of math to be a computer programmer. One reason is that nearly all the early research (years 1930-1965) into theoretical computer science was done by mathematicians. Another reason is that much early programming very frequently involved mathematical calculations; witness the rise of the FORTRAN language. Too, there is a substantial overlap between people who are good at computers, and people who are good at math. &lt;voice of authority&gt; I have a college degree in a combination of math and computer science, and I've done C++ for twenty years. To program C++ I don't need much math beyond arithmetic (sometimes in binary) unless I'm doing a math project. &lt;/voice of authority&gt;. So relax. C++ has no more mathematical requirements than any other language I've used. &gt; how much aprox. time would learning c++ take? C++, with its Standard Library, has become a *huge* language. Nobody knows it all. Schools vary from country to country, but in my estimation, a four-semester course (as would be presented in a typical American university) could give a good introduction to all the major areas of the language, with enough programming practice for the student to develop some proficiency.
It'll potentially be faster than hand written JavaScript
Thank you for help, im a little bit more confident!
Thanks, great info for me to start.
I use Linux Deploy on an android device that has LineageOS. Linux Deploy requires loop devices and root among other things. However, in unison with Termux, I can have a full Linux environment. 
Probably because everyone works on a PC or laptop. Also, I too don't get how one can code safely without a 'proper' development environment. The suggestion is basically buy a smaller and lighter laptop.
Surely you were given some indication during the interview as to what you'd be doing!? Something about this doesn't add up.
or it could be some bug in the compiler or a neat optimization that triggered UB and removed significant code to change the numbers
Looking for tips, advice, code review and whatever other comments!
&gt; I personally find make install cleaner than adding an include directory to the compiler flags I see! `make install` is a good idea but I personally think that installing user libraries into the system is not a good idea - they should stay in some local user directory, and isolated :-) (of course you could configure `CMAKE_INSTALL_PREFIX`, which would make `make install` install into a custom directory). But anyway on travis this couldn't matter less! :-) What I meant though is not adding a compiler flag, but using CMake's `target_include_directory(...)` directly on the Eigen directory from `hg clone`.
Occam's razor is about likelihood. Yes, there are plenty of competing explanations, but which is most likely given the data?
I think aaptel meant that use a profiler, such as GNU perf. You can tell it to record a process to give you an idea of where the process is spending time on. It does this by sampling. That is, it will check what is the current callstack/function running in that process. It will do this over and over, and tell you how many of those samples were in each place in the code. If you see a lot of calls to functions you recognize (`read`, for example) or a function with a descriptive name (`wait_for_timeout`) then you might get an idea of what's causing it to be so slow. And armed with that, you could attack the problem and speed it up.
Yes, and it's a basis for the new windows APIs in modern C++. I like the idea. It's kinda hard to get started with writing components; in the end it doesn't seem a lot of code but there are many bits to piece together. It's a bit funny how I started looking into COM. I'm developing a system on Windows, and I implemented a custom IPC protocol across named pipes. Cumbersome. I started to write another component, which would also be based on custom IPC, and said - NO. It's cumbersome and fragile. So I went back and studied COM and will replace "manual" IPC with COM. It's just saner. Back story: I work with a proprietary driver which doesn't let multiple programs open the device. No built-in multiplexing. So I'll package it into a COM singleton in own process and let other programs talk to the COM server. Yay. --- Actually, I also like the idea. Using an IDL to describe component interfaces is the "right" way to go with component-based programming, IMO. The irony is that COM is based on DCE RPC, which is an open standard, but Linux went the route of Sun RPC, and later dbus :( There's an open-source DCE RPC implementation for Linux/OSX and if I get the time in the near future, I'll try to make cross-machine RPC calls between Linux and Windows. Will be cool if it works :)
I enjoyed this talk: https://youtu.be/NH1Tta7purM It'll give you an idea of the problem space.
I was told the team does low latency stuff - and stuff around that not a whole lot more. Wasnt told which project i'd be put on. They said i'd be put with a more senior engineer and work with him on one of the projects. I will ask more but seems like they are not sure about it right now.
thanks! my thing is since i'll be a junior, i am not sure if i will get to work on all the cool stuff. dont wanna get stuck with doing C things.
From what I've seen personally, the weakest C++ developers have been at least as good as the average in the larger tech companies. A lot of it may be size/variation in ability or competition to join low-latency roles (job openings are way more vast in general tech than HFT shops). Either way, I'm pretty confident you'll learn plenty from your team. Pros in HFT: - Less variability in talent (generally on the higher end of the spectrum) - Deeply learn about each component that drives low latency (i.e. Linux kernel, network drivers, routing rules/network devices, CPU cache) - Generally driven teams; I find that *much* less people feel simply content with their roles, and everyone is pushing hard to get better - Control over product; generally, you are devops, and you can control how the product is reviewed/built/configured. You might not touch this much as a junior, but your team will certainly be the ones deciding this. Cons in HFT: - Critical components of the code base at times may seem hacky. Sometimes to cut off latency, you need to rely less on your compiler. This can result in code that may be less readable. Fortunately, C++14/C++17 is really helping out with the introduction of constexpr. Less and less code require much complex TMP and macroing. - Hours may be longer. This goes along with the pro above. When you're with extremely dedicated teams, you may see people expecting you to work longer hours or on weekends. It honestly doesn't feel like much though if you enjoy it. Office time is generally no more than 50-60 hours max (still more than the Big4 normally) - Instable compared to below. While developers have it best, and they're the least likely to be laid off, it's significantly easier to suddenly be out of a job here than in the big 4. - Varying infrastructure. You might not have a devops team. This means that you'll be responsible from keeping your code base clean and easy to build. To goes along with the above "control over product," but if a predecessor has set up a weak infrastructure already, you might need to spend time out of coding to fix it up. Pros of big 4: - With variation, you get some of the best C++ developers out there. I don't need to name any since you've probably heard of a dozen from Google and Facebook alone. - Work flexibility; for the most part, the job can be really cushy. Generally hours are extremely flexible and are built by your team and their meetings. 9 AM is often considered early, and it's not uncommon for places to let you come in from 10-11 AM (of course, you'd stay later). - Job stability; it is practically impossible to be fired without violating an HR policy. I'm not sure about the other two, but Google and Facebook implement a PIP (performance improvement plan) policy. Basically, to fire you off for performance, they need to try least try and improve you. This will at least give you time to find a new job. In some HFT shops, if they think you're underperforming, you'll be greeted by security at your desk one day walking you out. - Education benefits; most of the big 4 allocate a large amount of time for you to take training and time off to go to conferences. It's not nearly as standard for HFT shops to provide that. You'll need to do it on your own. - Mature infrastructure; generally, there are guidelines already established by each team, and only reviewed/tested code will show up in your codebase. Cons of big 4: - Very little control over your work. This is significantly better than many other large companies since they generally hire more competent support staff, but everything needs to be audited. Your work will be very much centralized towards a particular department and product. You absolutely have nearly no control over your database servers or network switches (some larger HFT shops might not either though). Often, to run queries, you will even need to send the ticket out to another team. You likely will only have a say in your current product while in your HFT role, your managers will almost certainly take your feedback seriously and deviate from the original course of work if the feedback is reasonable/backed up. - Again with the variation of technical skills, you will almost, without fail, always meet coworkers that are straight up incompetent. This is likely due to the third pro from above in that it's usually easier to transfer an underperformer to another team rather than fire them. Fortunately for you, the infrastructure will often prevent the incompetents from pushing our junk into the codebase. In terms of compensation, they're actually usually pretty similar. Junior HFT developers are often paid a lot less than first year big 4 developers, but it ramps up fast. After a couple years in HFT, you can jump to the $130-180k cash range (so no RSUs to worry about). After around four, you can easily find places looking to offer $200-300k cash. My friends that have worked in the big 4 generally have not seen $200-300k offers after four years. I'm sure many are in that range, but from my observation, I haven't seen as much.
The way I see it, he who needs IPC and/or RPC on a windows domain is best (by a long far) served by COM and component services. BTW... your driver and the other code : it's the correct way to do it. I had the same situation some 10 or so tears ago, and we used handle duplication and synchronization (named mutexes) to achieve the same results. Blergh, what we did.
Many people on Reddit wonder why anbyody would ever use Eclipse for C++. That's why. Eclipse CDT is the fastest one by a magnitude, it's nothing new.
Thank you so so much for this detailed post. I really appreciate all the input. What kind of work do you think the junior guys (1-2 years of experience) usually do at HFT shops. I really want to join the role since it is niche and I am already in finance. People there seem smart and hopefully I can learn a lot from them. What kind of questions can I ask them to get a better idea of the role? Again, thank you so much for this :)
Without any clear documentation, I just poked around the codebase randomly to get a feel for how it's operating. Here's I proceeded and what went through my mind: I started at BuildTool::Run(), since that's the entry point, and the first thing I saw is your StartTimer() EndTimer() monitoring code peppered through and thought: "Wow, this would be so much better if it used RAII. So I went to look at Timer.h to see how that's setup and see how I'd structure a recommendation around that, and saw a bunch of global non-static variables at the top of the header file (which is just asking for ODR violations). At that point, to be honest, I just turned away. I think this project needs a fair bit more time in the cooker before being reviewed by people not involved with it. It's basically impossible to suggest improvements that don't involve reorganizing most of the codebase. Sorry. Please don't let my comments discourage you! I really like the idea of what you are trying to do here.
Thanks for taking the time to write this! I actually just moved it from C to C++ this morning and I definitely left out some good practices... I think you are right and will actually remove this post for now.
It seems to be more general than that, as the original code in the Windows headers is simply passing a function pointer argument through. I'm guessing it's flagging any function pointer it can't statically determine safe, which makes this a conservative (and somewhat spammy) warning.
https://stackoverflow.com/questions/11703643/can-i-assume-allocators-dont-hold-their-memory-pool-directly-and-can-therefore
I know it works, just some tutorials are still teaching unsafe C++, with lots of new and delete
* Upvote: `make_unique()` * Downvote: `malloc()` 
All cool! Just to make sure you are aware: the "global variable in a header" thing is not a matter of best practices, it's fundamentally broken code in 99% of cases.
Yep, here are some additions (not very important to OP, since he is a junior): - there is a good chance HFT shop will sue you to prevent you from working for a competitor - if you work for HFT shop it is better not to read your contract too closely or you may decide that your dignity worth more than these money ;) - Big4 companies have backroom agreements about poaching from each other -- totally illegal (but who cares?). This helps them to avoid paying you what you really worth and impedes talent price discovery process. 
Thanks for the tips!
Yes, I've suppressed the warning for all external code and made the relevant function pointer types in my own code noexcept. Works fine so far.
Thanks! Don’t have a Android but there are terminal emulators for iOS
I guess I didn't read your post properly. Sorry about that. Terminal Emulators might not accomplish what you're looking to do. SSH to a VPS would likely be your best option. DigitalOcean has a $5/month server that would serve your needs. 
Be aware that C++ classes usually don't even begin to touch on the language. Many times using anything in the standard language beyond I/O routines is forbidden, for example, and they teach a lot of bad habits that later have to be unlearned to get up to speed with best practices.
I wouldn't say so. When I interviewed with firms that did HFT I had very generic dev questions throughout. Other than the algorithm whiteboarding questions I just had questions/tests involving the best way to store specific data, designing database setups with a variety of constraints, designing caching schemes, etc. These are questions I can see being asked in any dev interview.
Oh I'm aware. The program compiles but I fully consider it broken code. I've already fixed the globals.
Without wanting to sound callous or anything, why are you asking for reviews and recommendation for code that you know to be broken?
I started programming with C, then C with classes. Never really got C++ until later, much later - and 6 years ago I moved to C#. In C# I learned, for real, what programming with a sane API and with elegance looks like. I got back to C++ almost by mistake - definitely I started watching what C++11 means in the C++ world, and I re-learned the language. Now I realize I'm not an expert in it (although I considered myself an expert before programming in C#), and I have way too much to learn. How much it will take C++ to learn? Years. How much it will take to be proficient? I guess not that much. Modern C++ shares a lot with higher level languages like Java and C#, and C++11 onwards looks very C#ish in a sense. You don't need to learn math. For most projects you just need a clear understanding of how you want to build your solution and what tools you have at your disposal (STL, other frameworks/toolkits). You need to understand how to design a solution. How you want to make things work. What C++ will offer you is a way to perform any task a computer can perform. As I said, it will take years to learn C++. But you can finish projects successfully without knowing everything about C++ and without using all the features the language offers. So be focused on the tasks you want to perform, on the problems you want to solve, on the new things you want to create. C++ is a tool, you'll learn to use it more proficiently as you go, you'll make tons of mistake and you'll learn from them. The advantage being that if you learn C++ most other imperative languages will be quite easy to master.
It's not broken in the sense it doesnt work/compile, but rather in that it is not polished and has bugs(one that you found already). I'm also looking for reviews on how the program works(how it finds source, modules, parses data and such) and if there are better ways to interact with compilers and all that jazz. Its really a simple tool that automates clang for me and works for my needs at the moment. I should probably have held off on asking for code reviews while there are still some obvious bugs in the code... Since it works for me, I have been spending most of my free time on writing my next project, using this build tool and I'm not sure how much I'll play with it until it misbehaves or I need to boost performance.
Yeah UWP is a pointless platform IMHO. If I were to be building a new "app" that didn't require the performance offered by C++ I would just go with C# or maybe even a Progressive Web App with JavaScript (shocking I know). If making a desktop application that needs real native performance then C++ with Qt would be my preferred choice also. I don't see why anyone would want to limit themselves to UWP. 
One of my first jobs out of college was in HFT. My first few projects were to improve the build process with CMake and set up a CI system, so it wasn't quite programming at first to get familiar with the code base. I think setting up the build system was a good way to start off cause it let me explore the environment/learn about the dependencies and systems. The next project (about 2-4 weeks later) were to fix up a number of bugs in the backlog and develop a new market feed handler for a prospective client in the simulation system. This particular system is latency sensitive, but it's not bound by any of our SLAs, so a guy just entering the industry can't screw things up (too bad at least). The work beyond the third month involved bug fixes and more towards their execution engines (which ran similarly to the simulation, but every line of code was thoroughly vetted). The thing about HFT shops is that the logic is generally extremely simple (it must be to reduce latency), so there's not a whole lot of "strategy" that you hear about all the time. The majority of it is keeping things simple, know about the exchanges and their available order types and figure out when and where to use them (without violating any rules). Generally, junior roles aren't as well-defined (they don't expect you to architect a new system or anything), but I would ask the following two questions would be good for clarity (this is all subjective of course): - What resources should I read up on to be prepared for the start date? I like this question because it answers two questions in one: what you will likely be working on and have them them provide helpful resources that you don't know about. If they don't make either clear, feel free to ask directly. - What is your typical day like? I'm sure this is a common question, but the goal of it is to see how much time they (and you) will be in the office. In a more junior role, I found that more office time is good in some senses and bad in others. I supplemented my learning at work with a lot of time off-hours reading about C++ and the industry (trust me - there's a LOT to read that you won't have time to at work). You'll need to be familiar with most aspects of C++ (back when I started, TMP was almost a requirement since constexpr wasn't as widely available), and you'll also want to be familiar with modern regulations and exchange nuances. You don't need to memorize things like that whole RegNMS document, but you'll want to read through that type of stuff so you can understand your colleagues when they talk about the subjects. I usually switch things up, but the two above are generally my two go-tos for new jobs. 
Writing a pile of code that "just works for my needs" is totally fine, and there's nothing wrong with that in general as long as you don't need anyone else to read that code. Taking your example of how one would help you improving integration with other compilers. Deciphering what your code is doing through the hacks, bizareness and blocks of code that work only in the specific way they are currently being used is just not something anyone wants to do in their free time. You'll have much better luck getting assistance from other programmers if looking at your code doesn't turn them away immediately.
As for time, it depends. C++ is a vast language, but to be productive, or even proficient, only requires a small subset. The amount you need to know depends on your problem domain. Realtime, high-throughput, large-scale programming obviously requires a fairly in-depth understanding of how the language can help you, but for most mundane tasks you'll probably be able to get up and running in no time at all.
wow thank you for such a detailed response again, I will definitely make sure I ask them those questions. I try to read a lot of C++ whenever I can. I have realized that I want to do C++ development. Maybe because its the first language I was taught but I really enjoy C++. The only thing that I am not sure about HFT shops is that I want to make sure I have a chance to learn C++11/14. Looks like they dont do a whole lot of that where I am interviewing, they said they dont want to use something just for the sake of it which although is fair, makes me fear that I wont have the opportunity to learn C++11/14/17. I can always learn those outside or work like I try to do right now but would you consider that a red flag? I dont want to learn outdated c++. maybe my fears are completely wrong but this is a big thing holding me back from being excited about the role.
I guess the expectation is that you have installed vcpkg with boost anyway
His reply: &gt; Thanks for the feedback on the documentation. We’re always looking for actionable feedback. We are looking into the POGO plugin issue you mentioned. In general, we think the move to the new documentation site, docs.microsoft.com, has brought improvements, but some content hasn’t yet been moved, so until all the content is moved, there are some known problems with site search. The Windows team is working hard to migrate all their content to docs.microsoft.com to help solve that problem. &gt; &gt; If you see issues such as poor formatting, incorrect or out-of-date information on a docs.microsoft.com article, and you know how to correct it, you can just submit a pull request through GitHub with your fixes. Just click on the Edit button on any page, edit the page, and submit your changes. If you see an issue, but aren’t sure what to change or don’t want to fix it yourself, you can enter a GitHub issue in the repo that comes up when you click that Edit button. &gt; &gt; Detailed feedback really helps us make improvements that matter to people. There’s so many ways we could spend our time, and knowing what people are noticing and seeing helps us improve the content more efficiently. We appreciate your comments. &gt; 
did you end up taking the role? how was work like if you did indeed accept
thank you! have you worked at an HFT firm?
also sorry for spamming, any resources YOU recommend for HFT-esque C++ dev?
Didn't pass unfortunately :[
algorithms and program design are most important for high performance programming. The only time i really am forced to screw around at the lower levels is when I have to deal with possible memory blowout issues. For example when dealing with 10s of thousands of 100 megapixel 4 channel 16 bit images it can get VERY ugly...
are you still working with C++?
Well, you need some math base to be able to properly understand float's behavior and things like algorithmic complexity. But, yeah, nothing major. Most important thing is to ability to recognize and keep all these execution paths in your head, and be accurate enough to ensure each one it taken care of.
I'm still in school, last semester :[
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7e287j/c_and_hft/dq2aq9p/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7e1rnl/from_c_to_c/dq2ask0/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Big financial institution, but I have few friends working in HFT shops. Primary difference: hft is a shark tank -- you make them money, they'll share some with you, but don't be on their bad side or else; big companies -- they make money on tapping into markets normally not accessible to small players and their main concern is to comply with regulations (or they'll lose access privileges) -- they want you to be replaceable and dependable, for them you are the means to unlock access to money-making schemes, a cog. But because (unlike little hft shops) these guys end up making huge money -- whatever scraps they give you could still be rather big in absolute terms. 
What exactly from the code as of this moment do you find bizarre? Sure functions could be named better, and commenting would go along way but other than that I don't see the difficulty here. Actually I just made some changes that allows msvc and gcc to be used it took me all of 2 minutes. I think the functions are easy enough to understand and they can certainly be extended / complemented. I believe most of the functions are quite modular. Anyone should be able to figure this out within an hour. Can you suggest a faster / easier way to get started using modules on Visual Studio that can target multiple platforms such as Android, iOS, Windows... Here is a quick start. Engine::Run gets called Getprojfiles reads hashes from previous compilation Getargs parses the args.txt file, and fills variables used in the build process. Getfiles/getfilesvs reads the files included in the project and generates new hash for each file CompileNodes goes though each file and compares hashes then compiles if needed, it works recursively on a Node. A Node contains the file location, source, hash and flags to let CompileNodes know if it needs compilation. Link is pretty self explanatory Dopackage just runs any command in the package: {} var from args.txr It literally took me longer to write this post than the code, it is just a proof of concept using Modules. I dont care if anyone ever uses it. When did code have to be 100% perfect before been shared? Take a chill pill.
&gt; it's faster than other IDEs and separate indexers I've never had Qt Creator's Clang-based indexing stall on me. I can't say the same about Visual Studio or CLion, though, they run like trash from my experience.
Woah, woah. You are the one who asked for tips and advices. And my main tip to you is: write code that people want to read if you want good quality advice, that's all. I didn't mean that in a judgmental way or anything, there's no need to be defensive about it. 
I was thinking more of the memory sequencing features, threading and atomicity. But agree that algorithms are very important. At a certain point, though, one has to understand how the language works at a *very* detailed level. Of course, this depends on the problem domain. Hard real-time systems, where somebody's life is at stake or where huge financial cost is at risk is one such example. 
Sorry if I got defensive. Thanks for the tips.
&gt; What exactly from the code as of this moment do you find bizarre? reimplementing a tree by hand: https://github.com/warvstar/buildtool/blob/master/BuilldTool/Node.h debug macros intertwined with normal code that try to load a test project: https://github.com/warvstar/buildtool/blob/master/BuilldTool/BuildTool.cpp#L691 &gt; `if (useVS)` what will you do when you want to port it to Xcode ? or CLion ? or QtCreator ? or whatever *IDE du jour* ? this is exactly why virtual functions were invented: separate the general control flow from the implementation specific details. &gt; https://github.com/warvstar/buildtool/blob/master/BuilldTool/BuildTool.cpp#L600 the number of string copy is through-the-roof. there are `std::cout` calls everywhere. One day someone maybe you will want to log your stuff to a file given as a command line argument, or maybe to a network port. Also when it's not cout it's printf, which is even worse. string hash = std::to_string(XXH64(linkArgs.c_str(), linkArgs.size(), 0)); if (linkerSettingsHash == hash &amp;&amp; objects.size() &lt; 1) { now this seems downright scary. comparing hashes is *never never never* enough. There will *always* be hash collisions. &gt; https://github.com/warvstar/buildtool/blob/master/BuilldTool/BuildTool.cpp#L485 FFS use polymorphism ! it would be so much clearer. class Node { public: void Compile() { needsCompilation = false; auto ret = Compile(*this, forceRecompile); if (ret) { SetNeedsCompilation(); // put this in Node too } return ret; } protected: virtual auto CompileImpl() = 0; // replace auto by whatever your type is } class CNode final : public Node { auto CompileImpl() override { ... } } class CppNode final : public Node { auto CompileImpl() override { ... } } class CppmNode final : public Node { auto CompileImpl() override { ... } } and your big switch becomes return node.needsCompilation ? node.Compile() : false;
Actually, I'd say c++ is one of the very few languages, where you can learn "all of it". Not to the level of knowing the standard and any function signature by heart, but still having pretty complete knowledge about what language features and functions / classes exist in the standard library is certainly possible. Compared to most other languages I'm familiar with, the standard library of c++ ist tiny. The real "problem" is to learn all the edge-cases and strange interactions of different language features. 99.99% of the time, you don't need to know them of course (which is why you can write decent and correct c++ programs after a few months) but sometimes you hit a subtle bug, performance bug or compile time error which is totally non-obvious to a beginner.
The post has generated a variety of thoughtful responses from a goodly number of redditors. Perhaps it should be restored.
Why did you reply to this specific comment? Are those things really bizarre? What's wrong with implementing a tree by hand? I've done this more times than I can count, on two hands, with different types of trees mostly for games. The debug macro is a quick hack yes. I haven't had time to implement a logger, the codes so simple I figured cout would be fine for now. Can you tell me the probability of a 64bit hash collision? I suppose i could add a line of code to compare the file size or whatever. Yes you are right about the virtual and good idea just something I havent gotten to yet. I wouldn't call any of that bizarre though, just unfinished with a couple quick hacks to get it out the door. Thanks for checking out the code and commenting though!
@Pand9, thanks for confirm this!!! Nobody around me have experiences around these two indexers. 
If you are the op, shouldn't &gt; Thus, Computed&lt;bool&gt;::type would become bool, while Computed&lt;bool&gt;::type would (specially) be double Be &gt; while Computed&lt;int&gt;::type ....
You could try something at home on your spare time. It may be hard (I also want to do some things unrelated to my current job and even when I have time I don't feel like coding) but who knows, it can be good.
It really depends. Modern C++ actually offers a lot to further improve performance, so there's a lot of incentive to stay near the bleeding edge. In a larger bank where there are more regulations, you might be more restricted due to the bureaucracy. But for the most part, they stay near bleeding edge. Some of the benefits like emplace, std::move, constexpr, and variadic templates (for static, heterogeneous containers) simply can not be skipped. Just looking at my LinkedIn advertisements, I see that roughly 25% of them explicit say they're looking for people that know C++11/14. From what I see honestly, HFT firms stay *more* current than mature tech companies. I would need to speak to a C++ developer in Google to confirm, but I can bet a majority of their core code base have just recently converted to C++11 (while new projects are using it) while my first company started adopting/moving to C++ back in 2013 as soon as gcc 4.8 was released. This obviously varies from firm to firm, but do keep in mind that larger companies generally have much more complex systems that include proprietary tooling and vast amounts of infrastructure/people to handle change. Even if the tools were C++11+ compatible, they need to to make sure all downstream code will continue to function, and all of the new coding guidelines have been finalized and in place. All the complexity will contribute to significantly delayed adaptation compared to the shop that's less than 1,000 people and having a team lead say "let's start moving library X to C++ 11 and begin rolling it out in the next couple weeks".
When I started, the most memorable three resources I know I had were the following: - [Bjarne's book](https://www.amazon.com/C-Programming-Language-4th/dp/0321563840) - I don't think any C++ developer can truly call themselves even intermediate until they have absorbed at least half the content in the book. I started off with his 3rd edition, which is way less intimidating and shorter, but I subsequently ran through this entire book after it came out. There are no shortcuts on here - you need to read it. - [Effective C++ 3rd Edition](https://www.amazon.com/Effective-Specific-Improve-Programs-Designs/dp/0321334876) - I would almost require this one as it prevents any new C++ developer from getting caught in C++ gotchas. You should ideally follow this book up with his [4th edition](https://www.amazon.com/Effective-Modern-Specific-Ways-Improve/dp/1491903996) afterwords. The reason why I recommended the 3rd first is because that book is much more newbie friendly. The 4th edition is targeted towards experienced C++ developers that already have had their feet wet with C++11/14 and want to figure out some best practices. After you're done with the two above required books, here are some useful readings: - [What Every Programmer Should Know About Memory](https://www.akkadia.org/drepper/cpumemory.pdf) - This is an absolutely essential reading whether or not you've taken any systems courses. It's the foundation of what you will be programming towards (optimizing CPU cache usage). - [1024cores](http://www.1024cores.net/) - I believe this guy works/worked at Google at one point, but his site is essential to understanding multi-threaded programming (which is essential in the field). Browse through his site and learn what you can. - [Linux Kernel Development](https://www.amazon.com/Linux-Kernel-Development-Robert-Love/dp/0672329468) - Robert Love (who also works at Google) has probably written the most concise and understandable book on the Linux kernel I've ever read, and I've run through the Daniel Bovet's book and [Michael Kirrisk's](https://www.amazon.com/Linux-Programming-Interface-System-Handbook/dp/1593272200). For one thing, they're 1,000 and 1,500+ pages, respectively. Secondly, all I've found in those two books that I didn't find in Robert Love's is the implementation details in some areas as well as the details on the scheduler. Robert Love's incredible descriptions on the bottom-half/tasklets were already more than effective for normal understanding. I believe the latter books were more detailed in the networking areas as well, but in that case, you're better off with [Understanding Linux Network Internals](https://www.amazon.com/Understanding-Linux-Network-Internals-Networking/dp/0596002556). The above readings will probably be a solid 6-12 months to read and absorb assuming you spend a couple hours a day, but I think it'll be well worth it in the long run since this type of stuff sticks with you for a long time. I read the above around 2013, and I can still talk about the CFS/other schedulers, software interrupts, and how the CPU cache works. It'll also make converting to other languages even more of a breeze because you'll know how everything works underneath the hood.
Thanks for pointing that out! I've corrected it.
Going on a quick tangent here, I know that I'm not really addressing your original question. &gt; so maybe no more expensive than a lockless approach anyway There's a number of common misconceptions about "blocking" constructs *vs* lock-free constructs *vs* wait-free constructs, and this is one of them. It is most certainly not the case that: wait-freedom _is strictly better than_ lock-freedom _is strictly better than_ potentially blocking. Neither lock-freedom nor wait-freedom is a silver bullet, and there are and always will be cases where that order is reversed. The things to remember are: (1) the relative performance of potentially blocking, lock-free, and wait-free implementations of a given algorithm depend highly on the implementation quality, target architecture, *percentage of otherwise synchronized code* (i.e., Amdahl's Law), and typical workload; and (2) to understand which approach is best *for you and your system* you must must must benchmark!
I am sorry but I heard this words "potentially" multiple times for multiple libraries without telling "How". I don't mean to discredit anything but somehow one should also explains why shouldn't I go with native JavaScript. Inter programming language wrappers generally tends to complicate things than making it easier My 2 cents Thanks
This is not as useful as it looks because you cannot compile code conditionally using this approach.
But that's kind of the point. Conditionally compiled code cannot be checked by the compiler when it's not included. This can become a massive pain when a product has multiple SKUs. It's just too easy for someone to make a change and only realize they broke the build once it hits CI because they didn't check every single #define permutation before submitting. Conditionally compiled code should be kept to an absolute minimum, and just let the optimizer strip unused parts as much as possible. constexpr if helps a ton with that.
I already have DLLmain, I just don't know what to put in it to make it not inject after a month
&gt; Conditionally compiled code should be kept to an absolute minimum Even if you do it just once, you need to support both build parameter variants, constexpr something and #define something. The most obvious scenario where #if is unavoidable is platform-specific code, e.g. #if defined(OS_WINDOWS) #include "some/windows/specific/header.h" #endif #if defined(OS_WINDOWS) someWindowsSpecificFunctionFromHeaderIncludedAbove(); #endif &gt; only realize they broke the build once it hits CI That's the point of CI, you don't have to do that work locally. I work on a project that needs ~12 hours to build from scratch on my notebook for a single platform. I definitely don't want code for other platforms to add to that time.
That is a valid point, though as mentioned previously it is also a feature. I cannot count the number of times I break #ifdef code at work. The if constexpr block "can odr-use variables that are not defined". For simpler things, you can provide a declaration and you will be OK. I understand that may not be possible for big projects. Though I'm guessing you have *some* options which could use this technique. There is no reason not to mix both, it is just another tool to your arsenal ;) The block does need to be "well-formed".
Did you look at the date of the article?
'perf record', filtering by command, would tell you quite accurately where your program spends most of the time. I've worked a lot with profilers and can tell that the Linux one does a good job. GDB is not meant for this at all. To see the results use 'perf report'. Everything in Linux command line. There's a nice talk about perf in YouTube that is worth if you have never used Linux perf counters.
Take care with names! Perf, the kernel profiler is not known as GNU perf (even though the kernel itself is GNU Linux) and could confuse with gperf, the hash function generator. People usually calls it Linux perf counters or simply perf.
But they kinda do similar things, right? From what I remember perf is a lot more powerful but trickier to use - and last time I tried the default Debian kernel doesn't have it enabled
But they kinda do similar things, right? From what I remember perf is a lot more powerful but trickier to use - and last time I tried the default Debian kernel doesn't have it enabled
Not at all! gperf generates a perfect hash function (non mutable key to value container). That's why I said to take care about the names! Perf has a tricky learning curve but you can do simple things with 'perf stat'. It takes only a bit to do more intricate things like a real profiling because it is so flexible and allows system profiling as well, but once you found the options you need to use is an awesome tool to find hot spots in your codes and in the kernel. Check Brendan Gregg's blog. He explains it quite nice and gives you some good examples you can start use as a first step to find your ideal configuration.
I seriously wish that people would stop calling the removal of significant, but badly specified code an 'optimisation'. An optimisation is where you make things go faster _without_ losing required functionality. This is something else. Once people stop thinking of UB as a source of optimisation, perhaps we can then begin to work on eliminating it as much as possible. 
oh you're absolutely right! Got gprof and gperf confused :) I've just used valgrind in the past. But if your kernel can't be easily recompiled with perf (had this issue on a Chromebook) and you need something faster than valgrind, then I feel like it could make sense I need to learn to compile and update kernels one of these days :S Just not sure where to start
When I initially released [FunctionalPlus](https://github.com/Dobiasd/FunctionalPlus) I recommended adding the include directory like you now suggest for Eigen. However people convinced me, that this is very unclean, and I should remove this advice from my `README.md`. Instead I should only show using `make install` to install into `/usr/local/include`, since this should be the exact purpose of that directory. One contributer even helped me out in creating the CMake configuration. Perhaps in the end it is also a matter of personal taste, but I guess I will stick with the current approach for now. ;-)
So much this. It's just essentially a vendor lock in disguised under the banner of easy to use, which is complete bullshit. I don't get what's wrong with Win32/Winapi and any of the million decent wrappers for it if you enjoy fancy object orientation. What problem is exactly being solved here?
 &gt;even though the kernel itself is GNU Linux No it's not. The kernel is Linux, the userland is GNU, so the two together are GNU/Linux. You can also have, e.g. busybox/uclibc/Linux, or GNU/kFreeBSD.
I use fedora and it comes enabled by default.
You are completely right. Thanks for the correction.
虽然根本看不懂，但是给虎哥打call！
Modern effective c++
How big is your codebase, i.e. how many LoC? While I am not using RTags, jumping to the symbol or finding all symbol references should take no more than 5s. Also, you should really report this issue on [RTags issue tracker](https://github.com/Andersbakken/rtags/issues).
I've had a look at it briefly before. I think it suffers from the same problems as the other attempts at a "unified" C++ package manager: lack of packages used in the real World.
I'd say Effective first. Modern is a nice follow up, but you first need to learn the fundamentals. 
I tried Qt Creator with a big project that I'm editing in Eclipse CDT. Qt Creator had a very noticeable delay for "jump to definition", while Eclipse CDT navigation was instant. On the other hand, I allowed Eclipse to use up to 8Gb of RAM.
Any package manager is doomed to fail since it won't include that once library you need. I stated this numerous times in r/cpp and downvoted but I'll state again. Package management is an OS problem that we have solve in OS-level.
My team is using conan on our internal server. We use it to create libraries, push to Conan internally and then when we need to build projects just pull in the required version. Seems to work well although I have to say, we are not using majority of its features. We just scripted its use via makefiles. Versioning works well, uploading and downloading of libraries is very useful.
Read the Effective C++ series in order (EC++, MEC++, ESTL, MEC++), but make notes how the basic guidelines are modernized. E.g. EC++ has an item "know which functions a compiler silently generates". In modern C++ this is considerably more involved (move operators) and future C++ will be more involved still (default comparison operators). MEC++ has good back-references to the earlier books, but it is still a good idea to read the basics first IMO.
Deployment packages are not the same thing as development packages. At development time you might require multiple versions of the same library in your system, and OS packaging is just not appropriate for this.
Some time ago I saw a job offer in Apple Maps Team where they requested Conan: https://www.linkedin.com/jobs/view/215496969/
I think OS package managers are for distributing pre-built binaries for your OS whereas language package managers should distribute source code .
Our company tried to integrate it, and decided that we better off without it. It's pretty good if you are working on a project with a single target platform without a lot of configurable parameters, otherwise it becomes pretty cumbersome to support both CMake lists and conan files. The only real advantage is caching of binaries. But then again if you have a complex build system it becomes a nightmare. Documentation is pretty good, but most of the tools have some sort of "hidden" logic in them which aren't obvious and hard to work around. I wouldn't expect it to be widely adopted by big companies.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7e7jlq/should_i_get_modern_effective_c_or_effective_c/dq33sz8/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; Any package manager is doomed to fail since it won't include that once library you need. How is not that the case for OS-level package managers? 
I was surprised by "Round 3. Shifts vs. no shifts". How can a memory lookup be slower than a direct shift? Unless the compiler is unrolling that lookup or something else perhaps?
The original `define_task_block` does not allow for using it in a continuation style context as it is strictly fork-join. In HPX however we have extended it to (optionally) being usable in an asynchronous way, see here for an example: http://stellar-group.org/2015/06/hpx-and-cpp-task-blocks/.
Absolutely no one. conan.io and similiar package managing for C++ never been successful because due to being an outdated language, C++ suffers from having most of the widespread libraries being stuck in the 20th century, with old webpages and obscure downloads without any centralized standard and the ISO comitee is too busy adding irrelevant features to `std::` to fix this... These hard to manage dependencies this why I recommended against C++ in my company.
Cheerp is and has been used in very large commercial projects. For an example, have a look at https://home.by.me/en/.
Man. I really hope build2 and bpkg catches on. C++20 could be the opening. Cynical me says libraries like Qt will not comply. 
https://github.com/AnthonyCalandra/modern-cpp-features/blob/master/README.md
A quick Github search shows inaccuracies in your statements.
Why would someone publish source code on mediafire when github, gitlab, bitbucket, etc. are available for free in this day and age?
I'm not 100% sure without looking at assembler, but the lookup table variant can be parallellized by the compiler. XOR is commutative, meaning that you can divide up the loop into a series of parallel XORs (through SIMD instructions) until you reduce the number of operations to just the answer.
&gt;Cynical me says libraries like Qt will not comply. Yes, no matter how many more build systems are created, big libraries will NEVER standardize and pick one, leaving a split and divided mess for library consumers. I dont have much hope in the situation improving in C++20 or the like, considering it seems to be focusing on features like metaclass to further complexify the language.
I am currently studying computer science and my main experience/expertise is in game (engine) development. At the moment I am delving into embedded development. Specifically, writing a led panel driver for the Esp8266. What are the chances to get a junior position in the embedded industry? Most job listings Look for senior devs with years of experience. 
He may or may not be wrong but I can see where he's coming from. The lack of package+build management plagues C++ in a way that may not change at all.
Bincrafters repo is pretty good in that way: they have plenty of real-world packages
Yes, do tell!
Nicely written. Expected platitudes, got actual reasoning. Grammar quibbles: "a generic algorithms" (get rid of "a"); "feels like there was no other choices but doing so" (rather, "felt like there was no other choice").
&gt; Those helpful wall of text template errors If you're using GCC or Clang, just look for the "Required from here" line. In my experience, it points to the problematic line in your code more than 90% of the time.
This article makes some good points about avoiding the use of back pointers. However, I think it also strawmans the use case for back pointers. I would imagine that the most common use case for a back pointer is asynchronous programming. If there are several non-trivial connection objects that manage a socket and occasionally need to pass information up to their owner, how can you do that without back pointers? A function object will just obfuscate the issue and waste memory (`std::function` is much larger than a single pointer, and if there is more than one callback, then you are better off using one back pointer instead of raw function pointers). If the parent class is managing other resources as well as the connections, it doesn't make much sense to try to refactor to make the connections the parents. Short of abandoning the threading model in favor of something like `select` / `epoll` / `kqueue` from the parent (and dispatching to the connection synchronously), I don't see how you can avoid the back pointers.
I'd rather use some cmake based package manager like cget.
No sane company will want to use a public binary package repository. conan is not primarily an open source package repository but a technology that allows making one (or making your own in-house repo).
&gt; It's pretty good if you are working on a project with a single target platform without a lot of configurable parameters, otherwise it becomes pretty cumbersome to support both CMake lists and conan files. Profiles solve that in a manageable way.
&gt; Compile times still suck [Modules](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4681.pdf). &gt; Those helpful wall of text template errors [Concepts](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0734r0.pdf). These are not yet completed but it is anticipated that C++20 will add support for them. 
&gt; What can't i directly code in JavaScript for web when it gives me more flexibility than C++ When I read that, I think of "Why not code in C?" or "Why not code in assembly?" The lower you go, the more flexibility you get, at the cost of readability, scaling, maintenance, etc. In my experience, large codebases in weakly typed languages are hard to maintain. I find that JavaScript also isn't a particularly well designed language. I prefer something like C++. There are JavaScript transpilers for most popular languages, not only C++. I chose Cheerp for a particular project because my backend was already in C++. It allowed me to eliminate a crapload of code and gave me the safety of a statically typed language. YMMV, but I think it's important to remember that you do have a choice of languages, even for client-side. If you like JavaScript, then there's nothing wrong with that. But you do have a choice.
Gosh, I did terribly. What a great exercise!
We (plex.tv) use it for many of our projects.
we are using Conan to solve the ancient problem with externals dependencies and multi platform build. Until last year we used only subversion and all external dependencies were built in the same repository -- I know, totally ugly. So, we migrated to use Conan + CMake + Gitlab, as result we removed all externals from our internal repo, and the build process is more homogeneous now, we can use Conan to invoke CMake or any other build tool. Also, Gitlab helped to integrate all things, but conan has a great helper that provides travis, appveyor and gitlab template files. Finally, now we run an internal Conan instance, as server and some packages we consume from Bintray.
A big thank you for the grammar issues. I will correct them at once.
The biggest change is that you won't need to sweat memory management nearly as much. Don't allocate to raw pointers ( in fact don't allocate to smart pointers, make data structures that can be moved ). 
We are working on integrating it in our workflow right now.
It's a long way until these become usable in production code (at least if it's expected to be portable).
Conan is being used at German company Rheinmetall Defence Electronics http://blog.conan.io/2016/10/24/conan-at-work.html
Talk to you Career Services department. Show up to the career fair days. That's how smaller companies hire junior Devs. My boss would go to 2-3 career fairs a year and generally find enough candidates that they wouldn't pay to post junior positions on internet job boards. Also make friends with the guys on the year ahead of you. Referrals are gold. My last company hired 2-3 intern-to-ft positions just on word-of-mouth. Finally, if you can meet and get to know the local FAE (field application engineers) for Avnet and Arrow (and ST/TI/NXP) they usually know what smaller companies are looking in the area. Getting on their radar is a bit harder. I lucked out and guys I used to intern with are now FAEs. Best way to get to know them is to get into some of their free or cheap courses. Usually once you get on their mailer they'll start sending info about free courses on new stuff they're trying to sell.
While you may have a point wrt. the problem you are attempting to solve, I disagree (completely) with your generalisation thereof. There is nothing fundamentally wrong with the concept of pointers to parent objects. Reading through your rant, a few things jump out at me: - you haven't properly encapsulated, so a change to flow requires 'changes all over the code'. Encapsulation is not some religious 'chant', as you call it. It's a tool to help you write good code. In particular, it stops you from having to deal with changes all over the place, if changes are necessary. - you haven't given enough thought to ownership. Flows seem to be owned by trades, by various STL objects, and at one time you are unclear on whether flows can exist outside of trades at all. Not knowing that sort of thing is a clear sign of insufficient design. - You also seem unclear on what the invariant of flow is. The invariant is not typically a property that changes depending on where in the program you actually are. If it is, you haven't modelled your problem correctly. These seem much worse problems than a developer adding a single function in what is probably the wrong place. What would the business equivalent of a flow without a trade be? What would the business equivalent of moving a flow from one trade to another be? For the rest, feel free to use words like 'aliasing', 'invariant', 'declarative', 'cyclic dependency', etc. to mean whatever you want them to mean, just so long as you are aware that the rest of the world uses different definitions. 
This is one of those "I managed to solve a problem without doing X; therefore nobody should ever do X" articles.
Please don't fall into the 'use auto everywhere' trap. Future-self, and current reviewers, will thank you
[AAA Style (Almost Always Auto)](https://herbsutter.com/2013/08/12/gotw-94-solution-aaa-style-almost-always-auto/)
Nice! I didn't catch the significance of that when I first read that post. I hope the standards committee adopts your proposal. HPX is amazing. Sorry my original post sounds like I was throwing shade at HPX. HPX turns out to be what I was hoping to find after all.
&gt; I hope it's not career suicide. 
yep. there are always exceptions to the rule, but in my opinion parent objects should always be passed as references to child functions and never be stored. 
Thanks!
Just a few remarks as I think that we mostly agree, reading regarding the content of your answer. Encapsulation is not something 'wrong' (whatever that might be). It is very useful in producing decoupled code, that hides important decision from the user. But it might also be used incorrectly (or say very awkwardly) to hide the very data we are supposed to transform and maintain (the purpose of the program). The encapsulation 'chant' I am referring to is what lead to "generate setter / getter" which do not bring much value, or some bad habit of putting yet another responsibility in a class for the sake of following it, violating separation of concern. I also agree on 'invariant' being something that does not change. I used it wrongly. I mostly meant that the developer X tried to enforce an 'invariant' that is not really one. Hence the rigidity of the system. I will correct this.
I had a good experience coding on an 8" Windows tablet using Vim and some extra macros to better 'suit' the on-screen keyboard. Would that sort of form-factor work? If you absolutely have to work on your phone - the best experience is probably going to be through SSH to a 'real' machine.
This is way too specific. The only generalization that can be made, I think, is "minimize coupling". &gt; copying a Trade requires to rewire the Flow instance of the new Trade If the Flow objects are children of the Trade, copying a Trade or a Flow is not a simple operation and must be defined in any case (deep, shallow, etc.), regardless of back pointers. &gt; Flows became special the moment we added the back-pointer in them. They are not like other values anymore No, Flows became special the moment they became part of a hierarchy. &gt; In general, we cannot use the STL directly as our type is special Stop using the term "special". The correct term is a "value type", where _object identity_ is irrelevant. Two `point` objects are equal if they have the same coordinates and therefore are value types. A `Flow` is not a value type as its _identity_ is important because it is part of a hierarchy. If an object does not have value semantics, it is usually described as having _reference semantics_. These kind of objects cannot usually be simply copied or moved. This has nothing to do with back pointers, which only adds coupling to objects that are _already coupled_. &gt; testing becomes harder for our Flow class [...] Instantiating a valid Flow now requires a Trade to be instantiated as well If a Flow is part of a Trade and requires it to function properly, the addition of a back pointer doesn't change any of that. &gt; Various performance hits Don't mix a discussion about design with runtime performance, these are at two completely different levels. If you find that copying Flow objects is your bottleneck, I doubt that removing back pointers will eliminate it. &gt; Back-pointers are very often the indicators of a misconceived problem No, they're a solution to a specific problem. Whether it's the _right_ solution depends on the problem. But you cannot derive a rule that says that back pointers are "very often" incorrect.
I think you still should check all compile-time branches, because `constexpr if` strips unnecessary branch, doesn't ever try to compile that, IIRC.
&gt; Go, Lisp, Scheme, OCaml, Haskell, Ada, FreePascal compilers are bootstraped. Unused languages... But I agree with the remainder of your post :)
What about sorting the answers in the std array and then doing a binary_search? 
Tried this software, ultimately found it to take a huge amount of time to build databases for projects where it'd be useful (extremely large projects, see https://github.com/Mockingj4y/Source-Engine-2007/tree/master/src_main). Seems more reasonable to just use "go to definition" on functions you are curious about, rather than try to statically build a database for everything. 
Amazing comment! Thank you :) I really like "Flows became special the moment they became part of a hierarchy". Indeed, adding the back-pointer adds this relationship, and this is what brings the whole value-type (also, I used special in order to avoid falling into explanation of value types and in particular regular types).
But the memory lookup was faster than a direct shift.
&gt; Indeed, adding the back-pointer adds this relationship, and this is what brings the whole value-type down. If Flows are children of a Trade, _this relationship already exists_. That's the whole point. Adding a back pointer may increase the coupling, but it doesn't create it. If it's even possible to add a back pointer, it means that Flows already have reference semantics and that the identity of a Flow object is what defines it, not whatever values it might contain.
Conan makes it very easy to create a package for that one library you need and isn't in repositories yet. 
Interestingly I was just re-reading C++ Primer and one of the example projects they talk about is a pair of classes (Email &amp; Folder) that have pointers to each other. I assumed it was merely for demonstration purposes, but I felt the class design was less than ideal.
 uint64_t magic_number = *(reinterpret_cast&lt;const uint32_t*&gt; (sq.data())); is super undefined
But one big benefit it has is that you can package up third party code without rewriting its build system. This is I think the defining innovation needed to migrate people to a unified C++ package manager.
We (Pix4D) have been using conan for almost 1 year. It was a good decision to move from our customised way from managing 3rd party dependencies to Conan :)
I wish I could use it, but last time I tried to use it as a MinGW user, it was lacking too many libraries for the project I was trying to build unfortunately. Now it might just be a matter of time, and eventually the most commonly used libraries will build correctly on every main platform.
But I am talking about constexpr if, not `constexpr`. I just used the actual in-code statement: `if constexpr`.
Me too. `constexpr if (x) { y } else { z }` means that either `y` or `z` won't be compiled. I'm not sure how deep compiler checks unused compile-time branches, but I'm sure that build can still fail on compile-time. Imagine code like this: `int main() { int a; constexpr if(SOME_MACRO) { a++; } else { b++; }`. Build will fail if macro evaluates to false.
Why? As far as I can see this is a perfectly okay use of reinterpret_cast... As long as the string is long enough that is...
Do you mean OSS packages? Because there is an answer for that: https://www.reddit.com/r/cpp/comments/7e7jbt/who_is_using_conanio/dq36qu2/ Or your own propriatery packages? Beacuse here you go: https://www.reddit.com/r/cpp/comments/7e7jbt/who_is_using_conanio/dq3dcfw/
You may want to double check that assumption: https://godbolt.org/g/QcusfV
I am not sure to fully understand. I agree that the relationship exists from the trade point of view: it owns the flows. But the relationship does not exist in the other direction until we add the back-pointer. This is in fact the point of the article, avoiding bidirectionnal relationships.
Actually, this will fail to compile regardless of wether SOME_MACRO is set to `true` or `false`, which is my point. When using `if constexpr`, the validation is decoupled from the selection of what's being compiled.
Could explain why RDE stocks' underperforms for a while...
For your leftshift where did you get the magic constant 49? I'd rather start building the algorithm for a string of integers, for initial clarity. It's not often that you need to work so quickly with user-supplied strings, which are generally short by modern standards.
&gt; This is in fact the point of the article, avoiding bidirectionnal relationships. This doesn't make sense to me: relationships are inherently bidirectional (insert joke here). If a Trade owns a Flow, then the Flow is owned by a Trade. Do you mean to avoid bidirectional _coupling_? This I do agree with, but like I said, back pointers are only one of the many ways of coupling objects, which is why I said it was too specific.
Violates the [strict aliasing rules](https://stackoverflow.com/a/7005988/502399).
&gt; relationships are inherently bidirectional Hahaha, love it :) As you say, it somehow depends on how we define things, and what relationships we are referring to. If we are referring to the "knows about" relationship, then the trade necessarily knows about the flow (if it owns it, it must) but the flow does not necessarily have to know about the trade which contains it (unless we add a back-pointer in it). Or is relationship the wrong term here? &gt; Do you mean to avoid bidirectional coupling? Yes, we could define it like this. And I fully agree with you: back-pointers are just one way to couple objects, but this is one that is often overlooked (at least in my working experience: I saw plenty of them being added without much thoughts given to the issues it would cause - hence the article).
You are right, I was mistaken. https://godbolt.org/g/FKrBh7 But you can't always define things independently from compile flags. You have to design it this way from the start. Think buffer sizes in database implementations. Some caching, etc. In good design, all this would be abstracted and switchable on runtime.
This task requires some `constexpr` magic
I introduced it to my company recently. We develop a desktop application that runs on Windows and Linux, and we now use Conan to manage our third party libs (about 50 libs). Most are C or C++, some are Fortran, and some are Java libs. Before Conan, all our third parties were stored on a shared directory and developers had to copy files around each time a third party was updated. Also, we did not have a way to keep track of how the lib was compiled, and where the source code was coming from exactly. Today, we have a SCM repo for each Conan recipe (we recompile all our third parties, we don't use the libs published on conan.io at all). The recipe formally documents where the source code comes from, how it is compiled, and its build and runtime dependencies. The binary packages are stored on our internal Conan server. We integrated Conan with our build tool (SCons), so that all the required third parties are retrieved and setup using one command. We did not have any blocking issue with Conan. The developers were very reactive when we submitted issues.
I wonder if the results will be at least same-sided on a different platform, e.g. ARM.
But doesn't `sq.data()` return a `char*`, so aliasing is allowed?
Does it though? From that link: "If a program attempts to access the stored value of an object through a glvalue of other than one of the following types the behavior is undefined ..." The only glvalue that I see used there is `sq.data()`; the cast as used there does not become an lvalue. The following, however, would violate the rule: ``` const uint32_t* ptr = reinterpret_cast&lt;const uint32_t*&gt;(sq.data()); uint64_t magic_number = *ptr; ```
this isnt npm. there is a reason other package managers "suffer" from this and thats because most companies are not going to touch a public binary period.
Amazing. I've seen the codegen on https://godbolt.org/ for the two approaches and they are in fact totally different, the "no shifts" version requiring a lot less instructions. As for the memory lookup, it is probably cheaper than expected as soon as all data is in the cache.
It really sounds like you dont understand the problem domain.
Isn't that effectively what happened in round 9? std:array&lt;uint64_t&gt; vs std::set&lt;uint64_t&gt;
&gt; For your leftshift how did you determine the magic constant 0x1FFull &lt;&lt; 49? Remember that the magic square values are in ascii at this point, so: '1' == 49 
std::string::data() returns char*
It does, it is allowed.
This. Saves your bacon more often that you would think .
I think it is allowed to access a uint32_t through a char* but not vice versa. It can for example have an incorrect alignment.
++ and --
I see...
Had exported templates been viable, the original aim was for the C++ standard library to be take it or leave it, so its bloat didn't matter as only those bits needed were linked in. Bloat like iostreams is thus unimportant, it's a link time only issue. But bloat like `std::variant` is very important, every single thing which includes or Modules imports that pushes cost onto every single compiland. I very much doubt anything can be done about it, however.
I use a similar but not identical rule of thumb (which I think has been attributed to P.J. Plauger): The standard library should provide things that are hard for users to write themselves, or things that everybody needs to reinvent. The article gives more precise categories, with nice rationale, but I think it agrees with the above rule in spirit.
I agree with you, Jonathan, that I don't see a big value for a 2D library. * Would anybody build on top of it a UI toolkit? * Would a commercial product use it as a core component in a product with graphic visualization? I have very strong doubts.
I find it to be an essential tool - without it managing dependencies is very tedious and takes a lot of work, which leads to compromises. Other solutions before it didn't do a good job, for example: - git submodule creates a duplicate for every project that uses it, and doesn't handle building - git subtree is just copying and manually merging - some other package managers are built on top of specific tools like CMake (as much as CMake is popular and gets the job done it is quite awful and there are better alternatives) - other package managers which aren't cross platform conan.io is the first proper solution I encountered. (I actually started planning a similar tool when I saw there were no good solutions before I found conan)
Round 1: I guessed the LHS would be ~25% faster, since the first `if` condition there is sq[0] + sq[1] + sq[2] != c15 which is fairly predictable (&gt;90% false), whereas the alternative uses a near-even, ergo likely to mispredict, condition. It's surprising to me that this isn't the case, though the difference is small. Round 2: I thought the RHS would be ~50% faster because the 90% predictable condition seems like it removes roughly that much from the critical path. Round 3: I said the two should time the same because I didn't notice the changes to the only part that should take nontrivial time: the `if`. The removal of the bit shifts seems like a red herring, since it runs an absolutely trivial number of times. The simplification of the `if` might account for 25%, especially given it affects the second condition, though I'd have only predicted a ~10% difference. Round 4: I said the LHS would be 20x the speed; the average LHS quits on the first test, whereas the average RHS quits after 8 `std::string` comparisons (eww). Round 5: I said the LHS would be 10x the speed; a `std::set&lt;std::string&gt;` is a terrible data structure... but I assumed the comparison was against the normal one, not the Round 4 version. I should learn to read better :P. The 2x speed improvement was a bit of a surprise (I thought it'd be more like 25-50%), but I wouldn't read too much into it. Round 6: I said the LHS would be 50% faster. Hashing, random lookups and string equality are all more expensive than traversing a shallow tree. Round 7: RHS is 20x the speed. It should be obvious why I say that. Round 8: The question seemed really like whether the improvement from `sq[4] != '5'` compensates for the comparative slowness of the 8 tests afterwards. I suspected it's pretty much a wash, so I guessed equal. I did forgot that the first `!= c15` test passing means the later ones are more likely, which significantly worsens mispredict penalties, so the 50% improvement is not actually that surprising. Round 9: Set is obviously not going to be faster. I guessed a 25% difference because it only matters 10% of the time. Round 9: I thought an unordered set might beat set, since hashing and lookup aren't that bad with integers, but not by much, so I guessed 25% again. I don't really get why it ended up doing so badly, though I'm not surprised because it is a terrible implementation. I scored "606 pixels of error", which is pretty meaningless but somehow interesting nonetheless. If you correct for Round 5 where I misread the question, we're looking at "351 pixels of error", which goes to show just how much guidance reasoning can give. All in all, I think this is actually a fairly good test of how well you understand performance and what contributes to it, whilst gently reminding people to back up their analyses with observation. I'm pretty sure I can beat that Round 8 implementation, though :P. Just need to set some time aside...
Conventionally, *undefined behavior* in C++ has been taken to mean that the compiler can do anything it likes because whatever it does is just as undefined as some arbitrary (but perhaps in your particular use case more sane) action. This means that the compiler can just not emit anything and be just as well within the bounds of UB as doing anything else. I didn't downvote you, but that's my $0.02.
I wonder what those developers do at Microsoft, Google, Facebook, Locked, Boing, European Train Network control systems, London City, Citrix, Dropbox,... with such unused languages then.
To expand on my previous explanation: Imagine you wanted to test that all integers 1-9 are present. One way is to have a bit set in `ideal_map` for each integer (bits 1-9, so it would initially be `0x1FF&lt;&lt;1` because 0 isn't allowed in a magic square), then for each value you could use `(1&lt;&lt;value) XOR ideal_map` to unset the appropriate bit. Now, since we're getting the values as ascii instead of integers the range is bits 49-57 instead of 1-9.
O how you will miss the class libraries, you will know pain!
[removed]
`std::variant` should have just been a proper sum type introduced into the language. As it stands now, it's so ugly and cumbersome I'd argue it's not even worth using.
C++ has great tools for representing the abstraction in your solution without compromising performance. Your detailed explanation is appreciated. On x64 I expect working in ASCII with this 64-bit integer is the fastest. However the author's simple algorithm is cluttered with optimization. 
Cross language package management is the future. Most places mix a few languages. Google's Bazel is another. TBH everyone's view of the scope of the build and release problem is different. Systems - Deployable solution configured for a customer. Solution management - releasing a versioned solution brings together multiple versioned assets with hardware, documentation, guides and reports. Asset management - Meta products, products and packages. Inter Asset Dependency management Single Asset building and testing - traditional scope of a build based package management system Compile time, runtime and local test/tooling dependencies Environment(s) - compiler, host, target Build types Package storage, archival (auto) based on release type? (Types infer whether artefact is stored permanently, LIFO auto cleanup) Artefact caching etc. Etc. Etc. Lots of other things could be considered part of the process of resolving dependencies, building an asset, releasing a solution based on relations between multiple assets and deploying a system based on a versioned solution. OS level package managers manage asset relationships &amp; Runtime dependencies Conan and alike are single asset management systems focussing on build time dependencies. 
Not quite. std::set involves more memory (4x) and poor caching (data isn't contiguous, in addition to being larger). ~~I'm certain that a linear search of std::array will still be faster for only 9 values~~. Linear search for small vectors is typically faster than binary search. But perhaps given that the array is size is known at compile, the compiler can optimize the binary search to be faster even for a small array (9 values). But binary_search on a sorted array will very likely be faster than searching an std::set.
&gt; Cmake is too easy Oh man ... as a C++ newbie, I hate Cmake with a passion because it is overly complicated (coming from Rust...).
Tossing in a bitset instead of the uint64_t map generates pretty similar assembly, so I don't know about fastest but you're absolutely correct about abstraction without compromising performance (although clangs assembly is pretty abysmal at first glance for bitset::flip!): https://godbolt.org/g/8DbSyy
A 2D graphics library for sure ^^^/s
I think the last paragraph is the most important one. C++ doesn't need a standardized GUI library but it should provide much more vocabulary types, like 2 and 3d vectors.
`*x` is an lvalue when it's the built-in indirection operator, including `*reinterpret_cast&lt;T*&gt;(foo)`.
`0x1FFull &lt;&lt; '1'` would be clearer
Hmm! What about writing simple python bindings for your library with pybind11 that exposes your c++ model class and a c++ save function that saves the model to disk in binary format using cereal? cereal is pretty awesome in that respect and works on all platforms, and creating bindings with pybind11 is also super easy, and both libraries are header-only.
This appears to be a pure flaming opinion piece, raging against numerous things C++. Should be reported for being off-topic. 
What's a proper sum type? Any implementations (preferably header-only) out there for C++ to have a look at and how it's used in comparison with `std::variant`?
Sure..my original comment was hoping in an updated version, based on 2017 knowledge
The rationale for adding a 2D library is (as far as understood it) primarily to allow new C++ developers to learn to program with a GUI framework instead of just console output. A secondary reason is that the Cairo library (which is what the proposal is based off of) is quite capable and is actively used in many projects -- including web browsers -- so it isn't really just a "toy" library even though there will still be other graphics libraries. And [here's a good video where Herb Sutter explains why he's pushing for this](https://channel9.msdn.com/Events/GoingNative/2013/Keynote-Herb-Sutter-One-Cpp). For reference, some of the projects that use the Cairo library are: * Since GTK+ version 3, all GTK+ rendering is done using Cairo. * The Mono Project, has been using Cairo since very early in conception to power the backends of its GDI+ and System.Drawing namespaces. * The Mozilla project has made use of Cairo in its Gecko layout engine, used for rendering the graphical output of Mozilla products. Gecko 1.8, the layout engine for Mozilla Firefox 2.0 and SeaMonkey 1.0, used Cairo to render SVG and &lt;canvas&gt; content. Gecko 1.9, the release of Gecko that serves as the basis of Firefox 3, uses Cairo as the graphics backend for rendering both web page content and the user interface (or "chrome"). * The WebKit framework uses Cairo for all rendering in the GTK+ and EFL ports. Support has also been added for SVG and &lt;canvas&gt; content using Cairo. * The Poppler library uses Cairo to render PDF documents. Cairo enables the drawing of antialiased vector graphics and transparent objects. * The vector graphics application Inkscape uses the Cairo library for its outline mode display, as well as for PDF and PostScript export since release 0.46. * FontForge enabled Cairo by default for rendering in mid-October 2008. * R can output plots in PDF, PostScript and SVG formats using Cairo if available. * Gnuplot 4.4 now uses Cairo for rendering PDF and PNG output. * The Konfabulator/Yahoo widget engine uses Cairo for identical output to both Win32 and Quartz on Mac os/x. 
I think I do not yet correctly understand your suggestion. Currently in my mind the proposed chain looks something like this, which probably is not what you mean. ;-) generate_and_save_keras_model.py -&gt; model.h5 convert_model.py model.h5 -&gt; model.json fdeep::load_json_model("model.json") -&gt; fdeep::model fdeep::save_cereal_model("model.cereal") -&gt; model.cereal fdeep::load_cereal_model("model.cereal") -&gt; fdeep::model model.predict(...) Right now we have: generate_and_save_keras_model.py -&gt; model.h5 convert_model.py model.h5 -&gt; model.some_format fdeep::load_model("model.some_format") -&gt; fdeep::model model.predict(...) It's just that `some_format` currently is `json` and `fdeep::load_model` uses too much RAM. In what way would python bindings help?
Not really, because this is the kind of thing that really needs to be implemented at language level, with support in `switch` statements for pattern matching instead of having to write an abomination of `std::visit(&lt;20 lambdas here (and the lambda syntax isn't gorgeous either)/&gt;)` If you want an idea of how it could look like, look up Rust enums and how they're used with `match`.
Just look on the clang output: https://godbolt.org/g/cXtdbB The compiler internally sorted the values and generated binary search. That blows my mind !!! 
It is not. There is no guarantee that `sq.data()` is properly aligned. In practice, code like that will work on x86 but will crash as soon as you try to run it on architectures like ARM. The standard compliant way to do this would be uint32_t value; memcpy(&amp;value, sq.data(), 4); which will probably be optimized out by all major compilers. 
I tried to use conan.io but even for my test case it had to build part of boost from source and it failed so I dropped it.
Lambdas and std support for asynchronous code (futures, threads, synchronisation, etc). A good reference would be to pick up Bjarne Stroustrup’s The C++ Programming Language (albeit C++11) and a copy of Effective Modern C++ (C++14).
When we switched from boost::variant to std::variant it took our debug full-rebuild compilation time from 2 minutes and 30 seconds down to 1 minute and 30 seconds. When we replaced std::variant with a macro-built union (the macro part wasn't necessary but it simplifies adding/changing union members) it took our compilation time down to 35 seconds. std::variant for-sure should have been a language construct and not this template-based "thing" we have now.
https://docs.microsoft.com/en-us/cpp/cpp-conformance-improvements-2017 has been updated. Thanks Alastair!
Yes, that's what's surprising to me. For example on some architectures, some types of shifts are almost zero cost since they can be coalesced into register read/write or other operation. https://en.wikipedia.org/wiki/Barrel_shifter
Both of those operations can be parallelised the same way.
Yes, you're correct. I should look at the assembly some time to see what's really going on.
Change that 4 to `sizeof(value)` and no other surprises shall appear.
If you're writing code on an embedded system or some other mid to low level type of work, that would never display any output or require any sort of user interaction, forcing vendors to support a graphics library like that would be a waste of time and effort. 
Let's say sorting is O(n log n), binary searching is O(log n), and simple linear searching is O(n). If you place your array data in a way that's sorted already (i.e. without doing the work on runtime), you'll only need to do O(log n) work. Sorting it on runtime plus searching would give you O(n log n), which grows faster than O(n) for linear searching. In practice, however, CPUs favor linear search for small data, because it can be parallelized (taking micro operations into account as well). In the end, measuring it is the right answer, but I'd bet linear search wins (it has fewer branches, so branch prediction works in favor).
You can use UBSan to detect these issues. Using `memcpy` and `memcmp` will be lowered by the compiler where possible. e.g. https://github.com/glennrp/libpng/pull/188
How does alignment enter into this? sq.data() should return a pointer to a block of continuous memory...
I also wonder how the results would look on a processor without a cache. As an embedded systems guy, some of them (specifically \#3) struck me as different from what I would expect, but I usually expect memory to be pretty far away.
&gt; sq.data() should return a pointer to a block of continuous memory... ... that may not be aligned to an `alignof(uint32_t)` boundary.
They will simply not support those parts of the standard. Just like lots of then don't support many modern C++ features
Yes, I know that. I'm not saying compilers aren't allowed to do that; as it is, they are. But that still does not make it a source of legitimate optimisation. Whatever performance increase occurs as a result is, in every example I've ever seen, always been obtained at the cost of the intended functionality. For example, this: bool test_overflow (int i) { return i+1 &lt; i; } The _intention_ is to test if an overflow occurs. It is specified incorrectly, of course, but saying the program is "more efficient" or "better optimized" because the compiler replaced it by bool test_overflow (int i) { return false; } is just not a legitimate position. An optimisation is anything that makes it go faster _without_ a reduction in functionality. Here we are seeing a clear reduction in functionality. I can make _any_ algorithm finish in 0.0s if I'm allowed to remove its functionality. Nobody would count that as 'optimizing it' either. So to reiterate my point: instead of thinking of UB as this great opportunity at optimisation we should really be focusing on how we can reduce the amount of UB in the language. I'm not saying we can completely eliminate it, because the elimination of some UB would come at very great cost, but there is low-hanging fruit out there as well that we could painlessly eliminate. 
Do you know of any work done on introducing sum types at the language level?
I don’t understand why people hate on auto so much. Type deduction is a common feature present in almost every major programming language. Did you guys really enjoy writing „std::vector&lt;T&gt;::const_iterator...“ so much?
I would love to have some more basic math types ((up to 4d) named vectors, matrices + math operations on those) in the standard library. That's something missing in my day 2 day life...
I'd rather advise "A Tour of C++" which is really thin &amp; quick to read. It gives a good overview to what c++ nowadays looks like. Its target audience is already fully fledge developers and/or former c++ (98) developers that want to get updated quickly.
Yes, I agree in general. However "hard for users" is a little bit more permissive than I think: just because some algorithm is hard doesn't necessarily mean it should be in the standard library. Just if it is only hard for people who can't influence the compiler.
Instead of using a for loop to compare with the eight uint64_ts, what about just making eight if statements? Or does the compiler convert the loop into eight if statements anyway? Alternatively, what about using the strings but having if statements to branch between the different solutions? (For instance, notice how all the correct solutions start with one of {'2','4','6','8'}, then '8' is always followed by {'1','3'}, '81' *must* be followed by 6, etc.) Seems like that way you could avoid the inefficiencies of both the for loop and the set data structure.
&gt; But that still does not make it a source of legitimate optimisation. That's a rather subjective judgement. As I said, the compiler choosing to do nothing is just as valid as choosing to do anything else. &gt;An optimisation is anything that makes it go faster without a reduction in functionality. I think you are treating UB as a special case. What if we consider thread safety in the same way? Does that mean the compiler isn't allowed to optimize memory accesses because we are using multiple threads that may cause incorrect program behavior? I'm not saying that thread safety and UB are identical, but the possible presence of either is no reason to restrict the compiler. In this situation, thread safety is much more well-specified in general (although by no means completely) and in C++11 in particular. &gt;It is specified incorrectly, of course, but saying the program is "more efficient" or "better optimized" because the compiler replaced it by I agree with you here. Flubbing UB and proclaiming that the new code is "better" is of no use. However, that doesn't mean that the compiler isn't allowed to do it. It would be better if the compiler did it *and* told you it did it. Though, this could lead to crazy amounts of compiler messages. &gt;we should really be focusing on how we can reduce the amount of UB in the language The standards committee agrees. That's why they added a section on UB. The UBSanitizer has also been developed for this reason.
Wow, just wow. That is really mind blowing
I did try and it was slower, but looking at the answer above, clang (which is what I used) internally sorted it and did binary search on its own. So I am now not so sure anymore on what I compared with what :) Since the data is compile time, I naturally sorted it at edit time, so I was aiming for the O(log n) case. Alexandrescus 3 optimization tips talk came into my mind when trying this
If the compile times aren't enough, I really have a bad feeling regarding the code optimization I'm going to get out of `std::variant` considering the heavy use of lambdas, etc. I remember there being a case of gcc or clang, not sure which, failing to optimize `std::array` under some weird criteria. If even something simple as that can have bugs, I'm definitely not going to put my trust in the mess that is `std::variant`.
But, do you use it? Do you know any companies that are using it? These were the original questions, and they're good and specific questions. Several people I know wonder the same thing. It didn't seem to be asking for personal opinions. 
I've always really wanted standard library (modern, not using existing C API) socket / network APIs. It's something a lot of people have to rewrite over and over again. The requirement for specific network requirements is a concern but if we get a really lightweight and minimal socket primitives to write on top of I don't think it'd be an issue. 
No, my company does not use it, nor do I know any companies doing so.
Search up premake5
I think being able to open a window and choose between software rendering or a hardware API would be worthwhile. 
Yes, sadly not much has been added to the standard library. Basic stuff you take for granted like SQL, GUI, networking etc.
I've just read that book. I agree it's a good quick read. 
Auto was never intended as a syntactic sugar but rather to increase type safety.
P0095R1: [Evolution] Pattern Matching and Language Variants (by David Sankel) (2016-05-29) https://wg21.link/p0095r1 
I think there a lot of aspects of normal computing that have become standard enough for libraries. Networking in general, with tcpip and udp specifically and sound. Other huge blind spots in my opinion are shared memory, flat data structures, lock free data structures, interprocess communication in general, and basic shared library support. Other than that, doing basic operations on arrays using SIMD would be very enabling for a lot of people.
Modern graphics APIs shouldn't permit software rendering. They should be designed to assume the presence of hardware rendering, and everything in them should guide applications to use that hardware efficiently. (I should really work on my 2D engine so I can point people at an actual API that does this.)
&gt; If there are several non-trivial connection objects that manage a socket and occasionally need to pass information up to their owner, how can you do that without back pointers? &gt; &gt; A function object will just obfuscate the issue and waste memory (std::function is much larger than a single pointer, and if there is more than one callback, then you are better off using one back pointer instead of raw function pointers). To pass a std::tuple of std::function objects is the fastest way to write that kind of asynchronous code. All declarations are in-place, unit tests don't need separate mocks. They'll rearrange multiple times before the performance tuning phase comes.
So how do you use it on a platform without GPU?
But some of them do. There is nothing stopping them from supporting newer standards, they just need to update their toolchains. But badly designed UI lib can't be supported in principle on platforms that may want it. And if it's hard to support/implement and no one really using it it'll just become a dead weight in STL.
The "macro-built union" we use now also reduce the amount of code generated during the compilation process by almost half compared to the std::variant implementation that's shipping with the major compilers today. It's what should have been done at the language level except we have to do it now through an ugly system of macros or copy-paste the same thing 5 times and hope you don't make any mistakes anywhere.
In practice the code will work perfectly fine on ARM as well because you have to work rather hard to get an un-aligned .data(). If it's a heap-backed std::string then it's going to be alignof(uint32_t) on most (all?) platforms as that's the guarantee of malloc or any other generic allocator. So you'd need to swap the allocator with a specialied un-aligned char allocator to have a non-aligned heap-allocated .data(). For a small-size optimized std::string it is going to be pretty tricky since alignof(std::string) is likely going to be greater than alignof(uint32_t), so not sure how you'd pull this off for small-size optimized strings at all.
You don't.
Is that really CMake's fault though? You still need to deal with things like compiler flags, conditional preprocessor definitions, dependency searching, and cross-platform quirks, etc. for any large C++ project. At least CMake tries to put it all in one place and do some of the work for you.
The required functionality isn't lost. The *likely intended* functionality is what's arguably lost, but what's *required* is not. UB is a massive source of real optimization, but contrary to some other claims it also doesn't mean the compiler can do whatever it wants. Simple example is bit shifting. The behavior is undefined for a variety of inputs simply because CPU hardware varies on the behavior in those circumstances. So by making shifts in those circumstances UB the compiler can simply do the CPUs native shift and call it a day. Maximum performance, zero undefined behavior, cross-architecture - pick 2. You can't have all 3, that's simply not possible.
Well they're not forced to implement it. The big ones probably will but the smaller ones won't. I'm fine with that situation.
How about Unicode support? This time, one that works please.
&gt; Would anybody build on top of it a UI toolkit? Like [GTK+](https://en.wikipedia.org/wiki/Cairo_(graphics)#Toolkit_bindings)?
See the Networking TS. 
Metaclasses should cover this to some extent. The missing part is mainly pattern matching. At least Michael Park's library for that is pretty good, even though a language solution would be ideal (or something as general as metaclasses that would work for making your own match expression). 
The standard makes a distinction between hosted and non-hosted environments. Not supporting a 2D library on a non-hosted environment would probably be completely fine.
I struggle with the way std:array was implemented in the standard. Since the size is defined over a template argument you can't hand around arrays of arbitrary sizes. I know it is because they wanted to have no overhead, but I think that was a bad decision. Most applications don't have performance requirements and if you do have a performance requirement you always can use C style arrays.
I think "hard for users" is supposed to be interpreted as "hard for any user no matter how brilliant". As in, you need access to compiler internals or need to know specifics about the platform to be able to implement it.
I don't agree with this. The way the `std::array` was designed determines what it's appropriate for. It wasn't a bad decision to design a tool for particular situations, because there are _other_ tools for other situations. It's true we don't yet have a standard library `array_view`, `span`, or simple collection type that is nothing more than a run-time sized paired with an array reference that would be the 'zero overhead' replacement for C-style array parameters, but you're already saying that you don't need zero-overhead so you can just use `vector`. &gt; and if you do have a performance requirement you always can use C style arrays. `std::array` is to fix the problems with decaying C arrays and C arrays not behaving as regular value types. That's what I use them for and I can't use C arrays to do that. I can as easily say that if you want to pass arbitrarily sized arrays arrays as parameters you can just keep using C arrays/pointers.
When why is this needed when? Who'll need std API that's unusable on a large chunk of devices C++ wants to target, while there are better already existing alternatives that have no such problem. Seems like a short-sighted decision that greatly reduces the flexibility of some 2D/UI lib. Also, if we are talking about HW rendering when there can exist multiple ways of doing so (on different devices using different APIs, some might not even exist).. And one might expect a graphics lib to provide abstraction over that. And if there is abstraction, I don't see what prevent adding sw rendering functionality.
Don't templates help with that?
I was taught programming with Ocaml, and really like the language, but let's face it, like all the other languages listed it is statistically insignificant. Only Go is not *that* insignificant, but even *it* is orders of magnitudes less used that most popular languages. "Unused" was more tongue in cheek that anything else ;)
Just felt like saying this because as someone who writes a lot of lua I constantly have to deal with lua's lack of a robust standard library: standard libraries are super useful for setting a standard when it comes to writing code. In lua we have 15 different implementation of the same basic tools.
The only way you can guarantee it will work is if you allocated a `uint32_t` first, casted to to `char*`, put the string in, and cast it back. Obviously the result is implementation-defined (endianess and shit), but it is not UB because no strict aliasing violation.
Just pull in [GLM](https://glm.g-truc.net/0.9.8/index.html). Header-only, no dependencies, and fucking awesome.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7eeavm/learning_c_help/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
It's hard for simples users
If you apply probabilities correctly, you can optimize performance. In this case, you can also take advantage of the branch predictor because you are testing in a specific order. Which means testing on the first elements of the string is more efficient because the same path will be consistently taken. A deeply nested approach like you suggest is likely to work well, though I'd probably use a `uint64_t` cast without caring about the middle 5 and you can easily add additional testing for the last one the few times it's actually needed. You avoid the UB because if you ensure correct alignment, it will be fine while before you were always misaligned for one of the casts. So I suggest one test with an `and` and one with an `or` to test even/odd of every number (except one), then if both tests pass use the list of numbers, then if this passes too use the last digit test (that will be called in succession and will fuck up branch prediction if not put in last). Note: on some architectures, casting to 32 bits might be faster, and there's the added bonus that the first 4 digits are stable and will do great with the branch predictor.
&gt; Or does the compiler convert the loop into eight if statements anyway? I suspect most compilers are able to determine whether unrolling the loop is beneficial.
&gt; and sound Ehh... the APIs and design patterns for pro-grade audio are non-trivial to understand, and they drive the entire design of the application. And non-pro-grade audio would be ignored by everybody but a newbie or hobbyist. Including things useful only to a newbie in the stdlib seems wrong.
CMake is better than Make, but that doesn't make it perfect obviously.
I didn't choose my words well, but I didn't mean software rendering as in openGL, I meant access to a pixel buffer to be able to draw GUIs like qt, gtk, Juce etc. 
What devices, exactly, need graphics rendering but don't have a GPU? Note that for the most power-constrained devices (e.g. watches), GPUs are the most important. Yes, such a choice reduces flexibility. It also reduces complexity and encourages performance.
What does "access to a pixel buffer" mean? With a GPU, you can display textures (i.e. arbitrary images), render polygons, and run shaders. If your mental model is "I want to set some pixels on the screen at the following coordinates", then your mental model doesn't map to what GPUs can do efficiently. That is exactly what I think a constrained API should do - force users to think in terms of what GPUs can do efficiently.
I'm not talking about only a GPU, I'm talking about the ability to draw into a pixel buffer for the window with software or use a hardware graphics API. 
It's fine to have back-pointers, but you need to be careful with them. If class `A` uses elements of class `B` that it owns, there is no issue with `B` having a backpointer to `A`, as long as it's a weakpointer (I'd suggest raw pointers since `B` are destroyed with `A`).
Such an ability increases API complexity and doesn't encourage efficient code. (In fact, one of the problems with OpenGL itself is that there are too many ways to do things, most of which are slow.) Here's an attempt to explain by analogy. Consider `std::vector`. Computers have the physical ability to insert elements at the front. Users might "want" to insert elements at the front, and they can technically so so (via `insert()` at `begin()`). But wanting to insert elements at the front of a vector is *wrong*. It's wrong because vectors are insanely fast at `push_back()` and they would be inherently slow at `push_front()`. To reduce interface complexity and encourage efficient code (by discouraging inefficient code), it is better for `vector()` to omit `push_front()`. When it comes to graphics, the most important thing to realize is that GPUs are insanely powerful. I've dabbled with graphics only a little, and it's just astonishing how fast they are. Doing anything graphical with the CPU only is so slow, relatively speaking, that you shouldn't do it at all - like `vector::push_front()` would be, except a thousand times worse. And not only are GPUs fast today, they are *everywhere*, all the way from desktops to laptops to tablets to phones to watches. The smaller and more power-constrained the device is, the more important it is to rely on the GPU. Drawing into a pixel buffer with software is *wrong* to want, in the exact same way that `vector::push_front()` is wrong to want. A modern C++ API for modern graphics, built for the next 20 years, should strongly discourage such thinking.
&gt; EDIT - If you downvote me, please respond and explain why. Otherwise I must assume the worst. Not me &gt; I am suspecting That Mr O'Neal or one of his fans Fans? *Bill runs screaming away* 
https://github.com/bkaradzic/GENie
Cmake is only complicated if you make it complicated. Make files by hand on the other hand have always been a pain. Especially cross platform ones.
C# is a really neat language, and it helps you build nice OOP skills while not damaging too much your access to other ways of thinking (LINQ being almost functional). And you'll find C++ quite similar in many ways - the only thing you'll really miss is having sane APIs, and no matter what people say the STL is not a sane API. If you have a choice (although you might not, being new in a team) stay away from Boost. I know I'll get downvoted for this, but Boost is a bad idea, as bad as it always was. Saner APIs, inspired somewhat from .NET, exist, but are not easy to find. For example I found POCO quite fun to use. Qt is a very nice and quite round framework, I'd use that too for something more complex. Thing is that you shouldn't fear your productivity drop. There is an initial productivity drop, but after you'll solve the basics you'll see that you'll get almost as productive as with C#. The thing you'll miss the most is good Intellisense. It's something that is way harder to do for C++ because a lot of things are not available as methods on a data type, but instead they are functions in the std namespace, and they are named in surprising ways. Things are badly named in C++ (vector, for example, method names are just awful), but you'll get used to it fast, and you'll protest against anyone that reminds you that things are named badly because you just got used to them. And, of course, watch a lot of conference content, cppcon and meetingcpp for starters. Start with Herb Sutter's and Bjarne Stroustrup's presentations, they help a lot.
The problem with this question is that I change my mind on this every week. For a while I would love a graphics library. But the paradigms on graphical display change so often that it would be sad to be stuck with one. Thinking about things that people seem to reinvent over and over leads me to: * maybe better thread communications would be nice. * something to help serialization. * more for the GPU * more threading models like green/hybrid/mn threading out of the box. I would love to say things like networking, but again those models change over and over so locking on to one would be bad in the long term. 
Think about what devices need UI rendering but don't have a GPU (or have something really basic). All kind of embedded devices/controllers that need a user-facing GUI, but don't have really have any significant power constraints (i.e. don't run on battery). &gt;Yes, such a choice reduces flexibility. It also reduces complexity and encourages performance. How so? I.e. it's obvious that having no choice =&gt; less complexity, but to me it seems (maybe wrongly) that the choice would still exist (or will have to exist), since there is not (and probably can't) any unified graphics API even on single platform. In fact, software mode is the only thing that is pretty much guaranteed to work everywhere and give similar results. And it should also work across different HW setups. I don't see how will you get rid of at least configurable "backend" abstraction if you want a solution that would work on more than a dozen of devices.
&gt;&gt; When I read that, I think of "Why not code in C?" or "Why not code in assembly?" The lower you go, the more flexibility you get, at the cost of readability, scaling, maintenance, etc. This seems to me the classic case of asking someone to write "Assembly" which will generate "C++" code. not the other way aroud. As a third person doing both C++ and JavaScript development, I just asked why should i move to it? Unfortunately as of now I didn't found anything convincing. Anyway, everyone is free to choose their own way of doing things Thanks.
Nix does all of that in an incredibly elegant way, alas only on Linux. There's some MacOS support claim and even less Windows support, but they're clearly second class citizens. If nix supported Windows and Mac as it did Linux, nobody would be talking about package managers anymore.
I don't know why you're so stuck on only openGL. Was Herb Sutter's proposal only GPU accelerated? Does it not include the ability to set pixels in software? How do Qt and Juce work? Do they not draw to pixel buffers? I can't tell if your are really not understanding what I'm saying or if you just want to focus myopically on something to win an argument that I'm not making. 
So MSVC is non-conforming here because it's three bits too small. I'm curious what the rationale of those minimum integer sizes is.
It's a low-level library. There's room for something on top, like Beast. Still, the simple uses seemed straightforward enough from what I saw in presentations.
MSVC is the only compiler to use `long long`, which is of course 64-bits even on Windows. :-]
I've tried using it for a few of my projects. It'll have 50% - 70% of the libraries I need, then I still have to download the rest myself Example: One project used HTTP requests to talk to a server. Simple, right? Super common in this day and age. I did some research and seemed like most people were using Microsoft's CppRestSDK, so I checked conan.io... nope, gonna have to download that manually. Well, CppRestSDK uses Boost, and surely conan has Boost? It did! So I added Boost to my Conanfile, then `conan install .`... Oh, there isn't one pre-built for MinGW-W64, and it'll have to be built from source. _And I have to issue the command to build from source._ Okay, `conan install . --build Boost`... Oh, Boost depends on zlib, and there isn't a build of zlib for MinGW-W64. _I have to issue the command to build zlib from source._ `conan install . --build zlib`... Now that's done, time for Boost. `conan install . --build boost`... oh, there's an error in the Boost build script cause it can't find `b2` cause Boost is too much of a special snowflake to use `CMake` like the rest of us. I spent about an hour trying to debug that but eventually decided I didn't really need to work on that project They used to host everything on `conan.io`. That was pretty good. You went to the website, downloaded the package manager, and immediately could search from packages. Awesome. Then they moved to JFrog, which was probably a good idea for a lot of reasons but suddenly I go to `conan.io` to search for a package and the searchbar was gone. It wans't obvious how to get to JFrog, and once I finally got there they had like ten packages (apparently they put everything in a separate repo until they personally vetted it, which had the net effect of making it look like they lost most of their packages). The JFrog website was much harder to use and really wasn't useful Conan (and similar package managers I guess) are a wonderful idea, but there hasn't been a popular C++ package manager for so long that it's going to take a while for it to get off the ground. Until then, GitHub has a pretty nice search, `git add submodule` is very simple, and most things provide CMake build files Maybe I'll check back in a year
To answer the original question: much less! The C++ language specification should be completely separated from the standard library. The language should define the necessary built-in types and constructs required for all of its features, with no "library magic" like std::initializer_list.
Compile times can be decent if your project is built in a way that can be easily parallelized (using make -j). Using the pImpl idiom helps as well. As long as you're not setting up lots of crazy recursive templated stuff, I definitely think compile times have improved drastically in the past few years. It's usually the linker that's a killer for me on large projects (my last job took like 30 minutes to link). Then again, it was an absurd static beast. Coming from C++98, you'll be happy to hear that memory management has been made extremely easy now with little accounting needed. TMP has been made both *significantly* easier with all the built-in type support and variadic templates, and many cases of TMP aren't even needed anymore with constexpr. Honestly, I've been loving the road C++ has been going on since C++98. Hope you enjoy it as well coming back!
&gt; Think about what devices need UI rendering but don't have a GPU They're somehow surviving without a standard graphics library. They can continue to survive. &gt; I don't see how will you get rid of at least configurable "backend" abstraction if you want a solution that would work on more than a dozen of devices. You could abstract over DX12/Vulkan/Metal, if not OpenGL/OpenGL ES.
I think there's mutual incomprehension here, and that I don't know any other ways to say "setting pixels via software is wrong; APIs that encourage setting individual pixels are badly designed; GPUs render polygons that you can apply textures and shaders to, so a high-level API should be oriented around what's efficient on GPUs".
What the hell is this incoherent drivel? &gt; "Universal Windows Program has been developed based on the Windows Runtime (WinRT) technology." &gt; "Language projections has encapsulated complete implementation of COM concept and it provides a more natural programming experience to the call WinRT APIs." &gt; "In C++ developer, you can’t directly use the WinRT API, you have to use C++/CX language extensions (i.e. this means, you can’t directly use the UWP (WinRT) into the C++ language) and is a little bit complicated Microsoft has brought UWP features directly into C++ and First RTM build has been released in Windows 10 Anniversary Update SDK."
I am relatively new to professional cpp? How do you do that? 
If you call being the code that runs DropBox TCP/IP stack on Windows and macOS, or Facebook's messenger platform insignificant, then yeah.
I don't believe that thread safety can be retrofitted into C++, but that shouldn't stop us from thinking about it. Same for UB: I don't think we will ever have pointer validation, but we can at least consider our options in areas such as the standard library (does `tolower((signed char) 0x80)` really need to be UB?), signed integer overflow (are there really CPUs out there that do not just roll over to INT_MIN?), etc. &gt;However, that doesn't mean that the compiler isn't allowed to do it. It would be better if the compiler did it and told you it did it. Though, this could lead to crazy amounts of compiler messages. 100% agreed. And I'd love to see those error messages. If my software has a crazy amount of UB, I should probably know about it. &gt;The standards committee agrees. That's why they added a section on UB. That's great news. Uhm, the section was told that the goal was to focus on _reducing_ UB, right? ;-) 
"Required" in the sense that the programmer wanted it to be there. For the rest I strongly agree: the compiler should simply emit the corresponding native instruction and be done with it. I believe that this was the original intent of the standard: "you get whatever the CPU does in cases like this". Taking the CPU out of the loop completely because the wording of the standard accidentally allows it is, in my mind, a step too far. UB, if it occurs, should occur at runtime, not at compile time. Also, and I seem to be repeating this endlessly but here goes again: I'm not arguing for _zero_ UB. I'm categorically not asking for thread safety, or pointer validation, or array bound checking. How do you recon UB is a massive source of real optimisation? Do you mean that in the sense of "it allows compilers to let potential UB go undetected"? (i.e. it's faster because it is not generating code for avoiding UB?)
Good point!
&gt; UB is a massive source of real optimization, There's some dispute about that. E.g. [_What every compiler writer should know about programmers_ or _“Optimization” based on undefined behaviour hurts performance_][1]. Although the testing was done on C, and I'd like to see what happens to in C++, since IME C++ depends much more on optimization to achieve that 'zero overhead' goal for their abstractions. [1]: http://www.complang.tuwien.ac.at/kps2015/proceedings/KPS_2015_submission_29.pdf
Surely though in 10 years the efficient primitives in (by then) modern GPUs will be different (e.g. even higher level) to those if today. Hence even a forward thinking API made now will begin to hold things back in the future.
Definition and explanation of sum types: https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/sum-types
That is possible, and is an argument against standardizing anything at this time.
All it does is save a programmer time. Nothing else. It reinforces the common misconception that if the code compiles then it's correct. It hides subtle bugs like unintended copies. It makes pull requests harder to review
I just want the expressiveness of Fortran multidimensional arrays. No C++ linear algebra library comes close. 
Why would you expect ` i+1 &lt; i` to be anything else but false? Nowhere in the standard does it say that i+1 needs to be translated into an instruction that performs two's complement arithmetic. And in fact, it doesn't require this to be translated into a runtime instruction at all if the compiler can generate the same result otherwise e.g. because it can merge or simplify a chain of complex instructions or replace them with a sequence of more efficient instructions (think about division). 
Nor really. There are plenty of areas where the requirements are relatively stable and have been so for a long time. Binary file I/O for example.
Maybe an easier way to embed a scripting language in applications? I tried to load a Python program and let it call back to my application's classes and functions, but it was not simple.
Yeah pretty much. I know those libraries exist, but it would be a great addition to the stl none the less.
Why not use dynamic programming for something like this? I should think a parser in something like Spirit that matches the 8 possible solutions would generate a DFA that would scream through the benchmark pretty efficiently.
I think you greatly underestimate how many day to day optimizations in your program depend on assumptions that certain things don't happen (like integer overflow is just one of them). Also don't forget the fact that an optimizer pass often doesn't have the exact information about where in the code a certain instruction was coming from originally (most passes don't operate on the AST but on an intermediate representation e.g. llvm bytecode after lots of inlining) You'd be literally swamped with warnings about perfectly fine code. If you are afraid of integer overflow, compile with wrapv` our ubsan.
I totally disagree. I would go as far as saying that 'back pointers' are almost a design pattern of sorts. For example, back pointers can be great for performance, such as to avoid having to re-traverse a graph just to find a parent object. In fact, the process of walking parent objects to the root of a graph is also basically required when dealing with graphs, and it's made utterly trivial by back pointers. Anyway, I simply can't agree with a blanket prohibition on something that is incredibly useful when used properly.
Then I agree.
„One common concern is that “writing auto to declare a variable is primarily about saving typing.” However, this is just a misunderstanding of auto. As we saw in GotW #92 and #93 and will see again below, the main reasons to declare variables using auto are for correctness, performance, maintainability, and robustness—and, yes, convenience, but that’s in last place on the list.“ Read the article linked above.
&gt; It reinforces the common misconception that if the code compiles then it's correct. Explicit types do, too, and also allow for implicit conversions. &gt; It hides subtle bugs like unintended copies.
&gt; Binary file I/O for example. The proper paths to high-performance async file I/O has changed **dramatically** over the last 10 years. Bad example, or proving STL's point? ;-]
&lt;&lt; But it might also be used incorrectly (or say very awkwardly) to hide the very data we are supposed to transform and maintain (the purpose of the program). Okay, but the fact that encapsulation ( or other C++ features ) can be used incorrectly isn't a good justification to avoid using them. It's also worth mentioning that not every element of every problem space maps neatly onto C++ ( or any language ). Some problems are just messy, and the systems that arise from these problems are sometimes full of messy details. C++ has a lot of great language features ( such as the ability to write getters/setters or other class methods ) that help smooth out some of these rough edges in a design. It's really important to keep in mind that incorrect use of a language feature does not mean that the language feature should be avoided. If you write enough C++ code, you'll eventually bend a language feature to suit some business problem you have, and everything will still be just fine!
Bidirectional relationships between objects are not something to avoid. Bidrectional relationships have numerous valid uses cases for solving lots and lots and lots of business problems. You probably don't want to have a child object own a parent object, but the mere presence of a back pointer doesn't imply ownership. I also think it's pretty tenuous to say that objects that have pointers to each other are tightly coupled. The bidirectional relationship you're discussing isn't really a matter of coupling. Such a relationship might be a natural, efficient, intuitive way of connecting objects.
Maybe for modules 'cos it changes a lot, but Concepts is easy to integrate Concepts makes life easier, hence the name
And that's exactly what I implied with the second part of my statement. I am fully aware of it but I think it is still valuable information for someone who is interested in C++ but not completely up-to-date with the recent changes.
&gt; I would imagine that the most common use case for a back pointer is asynchronous programming. If there are several non-trivial connection objects that manage a socket and occasionally need to pass information up to their owner, how can you do that without back pointers? Yes, this is one of my use cases for back pointers. There are ways to do what you describe, but they require using 'shadow data structures' that somehow have to been synchronized with the real data structures. I prefer back pointers because they offer better performance and far less book keeping. Overall, the back pointer makes the whole system more robust IMHO.
Well... I don't know about u guys but I became my own package manager over the years, I don't even know what Package management is anymore All in all, introduction of Modules should change that hopefully.
Regarding compile times, I want to say two things. First of all, C++ compilation in MSVC has become very incremental, so that only the initial compilation is costly; subsequent recompiles where you make minor changes are very cheap. Second, IncrediBuild (or FastBuild, though I couldn't get it to work). Seriously, if you *do* need full rebuilds (for example, for Continuous Integration), distributed compilation tools totally save the day. IncrediBuild costs money, but has zero-effort integration into VS and 'just works'.
Well... I don't know about u guys but I became my own package manager over the years, I don't even know what Package management is anymore All in all, introduction of Modules should change that hopefully.
Then what does it add, other than to save typing, if the programmer still needs to know which auto to use?
Your intuition should tell you that there is something very wrong in global "variables" (a variable is something that may change in value) like this ones: auto c15 = '5' * 3; uint_fast64_t ideal_char_map = static_cast&lt;uint_fast64_t&gt;(0x1FF) &lt;&lt; 49; uint_fast64_t char_map_one = 1u; 
I read the article, watched the youtube presentation, and still get presented with nonsense such as auto x= static_cast&lt;int&gt;(0);
&gt; off-topic. It isn't though? I do agree that this is flame mongering. 
Yeah that’s an example of code written by someone who obviously has no clue what he is doing. But why blame the „auto“ feature? 
I'm not blaming auto, I'm blaming Herb Sutter.
Just passing by to say a huge thank you for this amazing piece of work. I work on heavily templated code and none of tools available on vscode were good on it. Everything was slow and not very accurate. cquery is really saving my life. Moreover the linter actually found bug in some deep templates that were not instantiated. I'm very grateful to you for your work.
Well if auto would default to auto&amp; we would see even more subtle bugs than unintended copies.
Take a look at Bincrafters: https://bintray.com/bincrafters/public-conan
each field has a value 1..8 so its 3bits required. lets store tem in an int32. there are exactly 8 possible magic squares in an 3*3 field. ok so we need to compare 1 int32 with 8 ints32. we'll we could do that with an avx instruction. and i really don't think running that code once will yield correct measurements: cpu with dynamic clock rate, interrupts, ....
Could you provide some references for me to read up on this?
I have only anecdotal evidence offhand; but I'm sure that /u/14ned has concrete resources, as this is his specialty.
As for the `conan install . --build Boost` problem, there's a much better solution. `conan isntall . --build-missing` will install all of the dependencies listed in your conanfile and will build all of the ones that don't supply prebuilt binaries.
Yeah basic file i/o at the hardware level is undergoing a truly dramatic revolution right now, as seismic as when DMA was first introduced over PIO. Basically all future storage will look to the CPU as if it were RAM at the hardware, just with a bit more latency. This profoundly changes how you ought to implement your filesystem algorithms because there is no longer a RAM cache layer in between you and the device. When you write a byte, it goes straight to the storage hardware (though it may not be committed until much later, as with now) Some additional reading on the kernel support: https://lwn.net/Articles/610174/. Note Windows has implemented the same. Basically page cache is disabled for these devices, they are used directly. And C++ will be, for once, far ahead of the other programming languages in natively supporting this if WG21 accepts AFIO as the File I/O TS next summer. https://ned14.github.io/afio/. This is pure luck of timing in fact and was not intended, STL2 is being actively worked upon, and my argument would be that all STL2 containers and algorithms need to be File I/O aware so this TS is on the critical path for the rest of STL2, but we'll see if WG21 buys that argument. 
if you don't care CI, vcpkg would be much simpler and also has more official packages.
Damn you! You made me miss resharper and stylecop! 
CLion tries to get all the good stuff from resharper, but it still feels heavy AF (at least it felt like that when I last tried it).
Yes, having a back-pointer to the object that owns the resource/object is never a problem. All you need is an invariant that maintain a 1-1 correspondence between "owns" and "owned by". Pointers to unrelated objects is a problem… but in a graph you have worse problems, such as loops of references. Back-pointers is a relatively small issue with proper encapsulation and use of the type-system. 
I think it first unrolled the loop, then detected that it's a if-then-else chain, and found that it can be trivially converted into a switch-case, and then synthesized the switch-case as a binary-searching if-tree.
By that measure, shouldn't 90% of the current standard library be thrown out ? For instance none of the containers use any special magic AFAIK. Likewise for streams, regexes, iterators, utilities like variant / optional / pair / tuple....
you can get a whole tcp server in less than ten lines of code. Maybe some people can try to offer some abstractions on top of this but I don't really see the point apart from doing trivial examples.
why don't you just use them as packages ? what does having them in the stdlib gives you ?
Sorry, the example was trying to be too clever. The problem occurs when your char type is signed, and its value is negative (or "high ASCII", as most would call that situation) but not EOF. In that case the conversion to int will pass a value that is not representable as unsigned char, and tolower has undefined behaviour. Same for toupper, isalpha, isdigit, isxdigit, isnum, isalnum, etc. Considering the purpose of the function (to convert characters, which would typically come from an outside, and thus untrusted source since if it is already in your program you might as well just type lower case yourself), why not make it at least safe to use for all possible input values? 
There is an error in that test - the "1" isn't a constant so the compiler is reading it from memory every time the function is called. Ditto for "ideal_char_map". In fact that goes for every solution - those globals need to be const, or ideally constexpr.
Yes, and I totally understand that, so no offense from my side against CMake. My statement was only that, coming from Rust where we have Cargo and things are _really_ easy, CMake is a monster to me and I fear it. That does not mean that I don't understand that the things CMake does are required for its domain! :-)
The standard seems ok with defining how unsigned overflow works. Why not define behaviour for signed overflow as well? Perhaps this contrasting behaviour made sense 30 years ago. There is no harm in revisiting such choices from time to time. 
they are switching from Cairo to a scenegraph because it's too slow.
A big chunk of hardware devices actually.
Thas why there is the second part: &gt; [..] things that everybody needs to reinvent. Of course it's a point of discussion which things are actual *relevant*
&gt; That is exactly what I think a constrained API should do - force users to think in terms of what GPUs can do efficiently. I really, really, really disagree. There are immense amounts of artistic software (see [OpenFrameworks](www.openframeworks.cc), [Processing](www.processing.org), etc.). It's a model that works just fine.
&gt; Since GTK+ version 3, all GTK+ rendering is done using Cairo. They are phasing it out. &gt; Gecko 1.9, the release of Gecko that serves as the basis of Firefox 3, uses Cairo as the graphics backend for rendering both web page content and the user interface (or "chrome"). They've since switched to Skia, google's rendering library.
did you check pybind11 ? It makes it very easy to do
From the library developer's perspective, vcpkg imposes requirements on you, and vets your submissions. Conan does the same for its central repo, but it's more friendly to distributed package sources where the library dev can set up their own source, and be free of others dictating requirements onto them. I'll be honest in saying that neither is ideal. I'd like something like homebrew which uses all of github as its sources with very little extra configuration. That's the right design for both devs and end users, though horribly reliant on github.
Portability between different libraries mostly.
I don't see what "things everybody needs to reinvent" should be in the standard library and not in just... "normal" libraries ? 
Can you guarantee that's SSO string's internal buffer starts at offset `0` (or multiple of `alignof(uint32_t)`) in every implementation?
Nope, we are Conan shop here.
Yes, it makes it easy to create a C++ function in a module and call it from Python and vice versa. What I find difficult is to let C++ load a Python instance which is then able to call back into my (running) C++ program. I imagine it would be really useful for testing and for scripting.
It just depends on whether you're planning to walk up or down the hierarchy. Some examples of where it might make sense to store parent references are IK calculations on a skeletal mesh or constant propagation on an AST.
vcpkg is simpler. This comes at the cost of some very, very strong assumptions, like targeting Windows only with the MSVC compiler, libraries building in a certain way etc. Conan, on the other hand makes much less assumptions and pushes the abstraction into the conanfiles, i.e. the package manager gets a unified interface for building packages because conanfiles implement build steps. In my opinion this is the correct approach. vcpkg often repackages libraries by adding their own cmake scripts (replacing the original build systems). I don't think this is a correct approach since in that case you are not packaging library Foo, you are packaging a fork of library Foo with your own build system.
I have built my own UI toolkit on top of Cairo. And I do use it for graphic visualisation as well. I would consider replacing that by a standard-based solution if it wasn't too much work (it shouldn't be, considering the shared origin), and if they haven't removed the parts I need, like output to printers and to PDF. 
sure, when you have algorithms which require cyclic references there is no other way. But as a rule of thump for class design I stick with my rule 
The purpose of the standard library is - and should be - to provide solutions for *commonly-known* problems. If you outsource them to "normal" libraries, then there will be a huge variation of different implementations with their distinct vocabulary. You have non-uniform warranties for quality features like performance, compatibility, portability and consistency. 
&gt; make data structures that can be moved Do you mind expanding a bit on that comment?
&gt; They're somehow surviving without a standard graphics library. They can continue to survive. So 99% if not 100% of other C++ code. Right now C++ community is surviving without standard low-level graphics lib just fine (by using non-std ones, that usually have even better support than STL running on many devices). &gt;You could abstract over DX12/Vulkan/Metal, if not OpenGL/OpenGL ES. So there still be an abstraction. I'm sure you are not proposing to structure std low-level graphic API around some specific currently existing API, since there isn't any uniformly supported by all platforms, let alone HW (thanks Apple &gt;_&lt;), so the std API would need to abstract other all "backend" APIs it'd need to support on different devices. It'll also most likely need to be configured at runtime, since one device may support some APIs from the list but not the others (or do it very poorly). Using the lowest common denominator is a no-go (since it'll be SW in the most common case, or something really old, slow and outdated if only one platform/OS is considered). So in the end, ideally there would a be a configuration point somewhere that chooses abstraction over "DX12/Vulkan/Metal, if not OpenGL/OpenGL ES." and if it exists, I don't see how adding "SW" to this list will bring any more complexity. If anything, it'll be the only viable fallback in case others don't work.
I have a lot of experience with GPUs, including desktop and mobile. Mobile GPUs consume a ton of battery power when run hard. I don't disagree with a lot of what you wrote, but I feel like it's important to understand that even the most current mobile GPUs have pretty serious limitations.
Sweet, more Fortran support! 
&gt; though horribly reliant on github Why not rely on generic git instead? Then it becomes the packager author's and package user's decision whether to rely on GitHub. I.e., I as a package author may choose to host my own git repository and I as a package user may choose not trust a package hosted on GitHub.
At Bitprim we build cryptocurrency software in C++. We use Conan in more than 15 projects. OSs: FreeBSD, Windows, macOS and Linux. Packages are optimized for several CPU microarchitectures. Also, we have APIs in other programming languages: Python, Javascript, C#, Eiffel and Go (At the moment). We use Conan in each recipe (pip, npm, nuget, go get, etc...) Conan saved our lives, ;)
Users get confused by git submodules. Hell, devs do. I don't, so I agree with you, but I've seen very highly intelligent and experienced devs on Boost become utterly perplexed by the simplest git setups. So I don't think it can work in practice unfortunately.
Not sure where git submodules come from. All I am suggesting is instead of building a package manager that requires GitHub build a package manager that requires git.
&gt; The standard streams are a beautiful way to do I/O — if it is 1998 I was there. It wasn't beautiful at the time either.
&gt; Including things useful only to a newbie in the stdlib seems wrong That's exactly what is happening with the Graphics API proposal.
I'm pretty sure I comprehend what I'm saying since I've done it so many times. How would you write something like an emulator? 
I don't think that is true from an API standpoint. Directly reading a file and memory mapping a file work even better with the current devices. Is there performance being left on the table with those two methods?
In many cases you draw not on the screen, but into the image. Like a web service rendering some png to serve to a user, or (my use case) a scientific application running on big cluster. 
For what? We have much nicer Conan :-)
vcpkg for us
std::make_shared&lt;&gt;, std::make_unique&lt;&gt; is a good start. but op is right, having no (raw) pointers and using value semantics is usually best. If you have to use the new keyword, you may doing it wrong.
Try to make the same, but a HTTP server with Req, Res control
I prefer Qbs
Ah okay, let me explain: generate_and_save_keras_model.py: reads keras model.h5, and saves fdeep::model as bin using cereal (through python bindings to fdeep) fdeep::load_model("model.bin") (using cereal) =&gt; win :-) So basically you write a `fdeep::save_model` function in C++ that saves a model to disk using cereal (just 4 lines of code), and then you expose that function to Python using pybind11, and you expose the `fdeep::model` to Python as well with a constructor that takes all the weights &amp; stuff from `model.h5` (I guess most will be in numpy format, which plays very well with pybind11). Basically this makes the conversion easier (only 1 step) and you directly save the model as binary which `fdeep` can then read in a fast way. I hope this is clearer :-D
I'd certainly be using vcpkg if it weren't for a horrible requirement for XP support. It's just for one machine, that's running one application... 
Wait, there's a ```protobuf_generate_cpp()``` function? I've been spinning my own version of that for years...
Congrats to the CMake team on finally moving to C++11. I'm sure this must be a welcome change for them. 
GLM is pretty portable itself, actually. I've used it everywhere from latest Intel server chips down to ARM Cortex M0+. Can't say it's super performant down on that M0, but it did compile and work.
Take for example strings before std::string came around. Everyone had their own incompatible string class, so you had to either only use one library per program or you had to constantly convert between different string classes.
Ok but you could equally well add an element that is greater than everything
OK, I understand. And that way the loading would read the weights directly from disc into the `fdeep::model` structure without taking up all that intermediate RAM. Thanks, I [will check]( https://github.com/Dobiasd/frugally-deep/commit/2539ef7cd3a1c154ffb81b0c1cafdec330987c7d) what I can do.
I would be curious as to why you think the STL and BOOST APIs are "insane". Extrapolating from the libraries you do like (Qt, POCO) it seems to me what you really don't like is C++-style interfaces. I think Qt has very un-C++-y API.
&gt; vcpkg imposes requirements on you, and vets your submissions. If you want you can use local port files (files that stores information about obtaining, building and installing packages), not files from central repository.
From the paragraph regarding the `[[uninitialized]]` attribute: &gt; There was also a desire to address the broader use case of allocating dynamic memory that is purposely uninitialized (`new char[N]` currently zero-initializes). Why would that zero-initialize? Or does this just mean in practice, as opposed to being specified that way?
http requests library.
Servers. That plot scientific data, web server visit statistics, tiles for online maps, etc. 
The only problem is that you can't tell people reluctant on upgrading CMake that they could just build it on any platform if they don't support C++11. But at the same time, if they don't have C++11, maybe 3.9 is just enough for them...
I think he intended: std::vector&lt;char&gt; buffer(N); That currently value-initializes the buffer, and there's no easy way for me to specify that I want to default-initialize it. I'd have to either write a new allocator or write a wrapper type for `char`.
It looks like [GTK+ Scene Graph Kit](https://en.wikipedia.org/wiki/GTK%2B#/media/File:GDK_software_architecture.svg) sits on top of Cairo.
What happens is that Boost and STL has the most unexpected APIs - they don't behave and don't work consistently. The STL : the erase-remove idiom which is just crazy. Naming things is random at best, APIs are split between member functions and namespace level functions making intellisense next to impossible. The string cannot tell you how many characters (unicode) you have there. char_traits is a headache. The random library is amazing, yet it's quite cumbersome to have a simple: „start random series, give me next”. And I'll stop here. I'm looking forward to the ranges library and the concepts to be added to the language, and then we can ditch STL completely. Boost is just horrendous. Everything compiles in 100x time, almost every Boost API is complicated and quirky. I don't want to have a PhD in Boost, I want to write my application. We can't have a simple HTTP get, we have to have suicide-inducing APIs like ASIO. Boost is arguably helping you to write code faster, but that's write-only code that should not be touched, because if you make a single mistake you have to delete everything and start over again. Boost and STL are painfully anti-user, and there's no reason to be like that. STL is improving slightly, but there is no way to go through the disorderly API that is spread all over the place. Don't get me wrong, I think STL is valuable, but it's painful on the first contact, and there's no good way to find out if there's an STL function that does what you need to do. Qt has by far the best documentation out there. It's one of the most consistent APIs I ever worked with, everything works as expected, with no or minimal impact on performance. I don't know why you say it's very un-C++-y. Why, because it doesn't take 10 years of study to write code using QString?
`new char[N]` uses default initialisation (i.e. it does nothing for `char`). You have to use `new char[N]()` to value initialise (i.e. zero it out).
Yes exactly! :-) It has been proven an awesome workflow for me, and once you've used pybind11 and seen how simple and awesome it is, you'll never want to miss it anymore. It's so awesome to use your C++ libraries from Python scripts, and so simple, and it "feels" also very natural (you can bind classes, properties, numpy works in an awesome way out-of-the-box, so does Eigen, etc.).
I completely agree. Especially in a language like C++, magic types makes stuff so complicated. Like I'd argue initializer list is a language (mis)feature, but you can't use it without including some magic header. Imagine if lambdas were created with some magic macro and a `std::lambda` type or something. The same goes for `std::variant`. You can implement it in a library, but the amount of code needed is insane, and using it isn't very ergonomic at all.
Isn't Fortran a dead/dying language? Why add more support for it? Or is Fortran gaining momentum for some reason?
Yeah, that's my idea of Fortran as well, but there might be use cases I don't know. Should've put a /s on there. :) 
What he probably means is to have a move constructor and/or move assignment defined in your class. In many cases the compiler auto generates them. With these if a function wants to make an object and return it, rather than returning a raw pointer or even a unique_ptr, you just return the object as a value. The compiler is then smart enough to use the move operators to create the object for the caller - rather than making a copy - because it knows for sure the object is disappearing in one place and appearing in another.
There is still plenty of Fortran code around that needs to be supported wether you like it or not. In particular there are tons of scientific computing libraries in Fortran 
&gt; are there really CPUs out there that do not just roll over to INT_MIN? DSP and GPUs do this.
Qt's API is easy but in plenty of cases it means that you won't be able to reach max performance. For instance : QString::split(). Sure, that's fine and funny and nice to use BUT it does not give any flexibility. You can't use preallocated buffers, custom allocators, custom splitting heuristics, etc. Want to split and put in a std::vector&lt;std::string&gt; ? Enjoy your allocation-fest.
You are not wrong in your criticism, boost and stl are hard. But many times there is good reason for things being the way they and its usually because of performance goals. When performance is not a primary focus it is much easier to build nice clean interfaces.
No, when I said "it made perfect sense to have it that way" it implied "it made a lot more sense to have is less than everything rather than greater than everything". So, not arbitrary. Conceptually, `none` represents unterminated computation (which may or _may not_ terminate at some point) and is sort of a "ground state" or "sink" which "propagates" to computation that depends on it. IIRC it makes the order on `optional&lt;T&gt;` a semilattice which has useful properties (what properties I don't recall).
&gt; I agree. C++ language should not be coupled to libraries. It should be possible to have a standard library implementation which would compile on any conforming C++ compiler. As far as I know, this is not currently possible. This is bad for the industry. It means that a market entrant has to provide two products (compiler and library) instead of one. I think that the committee should diminish it's focus on libraries and replace it with a focus on providing standards for "acceptable" or "conforming" libraries. A "conforming" library would: * depend on and only on the C++ language standard * be required to present documentation meeting standard defined requirements for content and formatting * be required to provide comprehensive tests I don't think it's a great idea to design libraries by committee. 
I think you're somewhat overestimating the abilities of the compiler. It's basically a theorem prover. For things like loops, it will attempt to determine the range that variable can take based on the rules of the language. If it finds useful subsets of values, it will attempt to substitute in more efficient code (for example eliminating redundant tests) based on whether it can prove the substitution is identical given the language. What it sounds like you want is for the compiler to attempt to guess when the user was intentionally violating the rules, then not do the optimizations in that case. That's very very difficult because the compiler has no real understanding of the rules and the high level meaning. It just blindly attempts to prove things and then prune out stuff. 
Not precisely - it's maintaining momentum for a few reasons. Namely, lots of key scientific libraries use it. Secondly, scientists aren't exactly the kind of people who want to learn more programming languages (or even want to switch, they can be a bit stubborn ;p): they may have picked up Fortran over the years or may have learned it in school, but for whatever reason it's what they know and use. I'm happy to see the support: I'm a developer working with astrodynamics and trajectory plotting algorithms that are occasionally written in Fortran
Man, a year into this programming C++ thing and CMake is my best thing. It works so well, for so much! Getting my renderer - in Vulkan, mind - to compile and work across all 3 major platforms was hard enough. Having CMake take care of setting the correct environment and link library settings on Win+Linux, or getting all the right frameworks set up on OSX, is awesome! 
And in general, you can get precompiled headers to work (somewhat) on all three platforms with some clever CMake work. That and making sure to be careful with your includes (and using forward declarations) can help loads
Does this happen for user-defined types as well? If I write a class with a char member and an empty default constructor, will that also zero-initialize on new?
It's not that easy unfortunately. We still have CentOS 6 machines. On these machines, only GCC 4.4.7 is available. GCC 4.4.7 doesn't have proper C++11 support. Now of course we could install another compiler - however, this means that this compiler will produce executables that link against this compilers libstdc++ and libc. That means that every single program (being cmake) compiled by it requires the LD_LIBRARY_PATH to be modified. Now of course we could just upgrade to a newer operating system, and if it were my desktop machine, I immediately would. But we're in a company environment, and that's not always easily possible. Getting rid of legacy compilers and using newer tools and newer standards sounds fun at first, but when you actually try to do it on an old system it quickly turns into a nightmare. (Ever heard of a GCC configuration flag called `--with-default-libstdcxx-abi=gcc4-compatible`? No? Well I wish I hadn't either.)
You may also find https://github.com/SuperV1234/camomilla useful.
Quite serendipitously, I started watching a new set of CppCon 2017 videos yesterday and most of them have been about UB. There is a lot of interest in this going forward and for C++20 in particular.
The CMake official Linux binaries are built statically on an older machine as a way to avoid requiring systems to have a newer libstdc++ and libc installed. A quick look with ldd and I believe that the official binaries require a libc of 2.2.5+
That paper doesn't actually dispute what I said fwiw. It confirms that letting the compiler do platform-specific behavior is good (which requires the platform-agnostic language to define things as UB), but then calls into doubt the extreme cases that can be derived from that UB with *extremely* little supporting data for that doubt (specifically they only looked at 3 benchmarks in total) But the paper isn't questioning letting 'int a = b + 1' compile to a simple 'add' instruction. C has to call that UB because ones compliment machines do exist, and that's fine. The paper isn't doubting the merit of that. It is, however, then doubting when the compiler takes that to the extreme, optimizing away entire chunks of code instead of just letting it overflow. The question isn't "should int overflow be UB?", the question is "how far should the compiler take the fact that int overflow is UB?"
On CentOS, you can install the devtoolset software collections to compile. They use linker scripts to use the system libstdcxx first and only resolve symbols that aren't provided in a static version, leading to binaries that work with the base system libraries.
The FindMPI and InstallRequiredSystemLibraries changes improve Fortran support as a side effect, they're also very useful for C++ apps. The Flang compiler changes needed were small. That being said, I implemented both changes motivated by the need of better Fortran support for our commercial project at work, which is a Fortran codebase ranging back into the 80s. :P
https://www.llnl.gov/news/nnsa-national-labs-team-nvidia-develop-open-source-fortran-compiler-technology
&gt; How do you recon UB is a massive source of real optimisation? Do you mean that in the sense of "it allows compilers to let potential UB go undetected"? (i.e. it's faster because it is not generating code for avoiding UB?) I gave a specific example of this, what was confusing about it? Or for your particular concern of int overflow, how do you define int overflow behavior in a way that results in a simple 'add' instruction without being 'undefined behavior'? Different architectures overflow in different ways, so you can't define specific overflow behavior without sacrificing performance across the board on architectures that didn't happen to have the behavior you wanted.
I wonder if plans to support module are being thought about.
Quick look and ... why does it test n &lt; N if the precondition is n &gt;= N? assert (n &gt;= N); // We should never be asked for less than N. if (n &lt;= N) { buf_-&gt;free_ = false; return reinterpret_cast&lt;T*&gt; (buf_-&gt;data_); }
A standard date &amp; time library will be so much welcome.
Instead of using cmake machinery to detect c++ version, you can directly test __cplusplus macro in your code (e.g. 201402L for c++14).
I believe that last time I tried, it would require the libstdcxx of the devtoolset compiler. Maybe i did something else wrong though, so I might give it a shot.
I believe that last time I tried, it would require the libstdcxx of the devtoolset compiler. Maybe i did something else wrong though, so I might give it a shot.
It's ~~turtles~~ Cairo all the way down
&gt; They are phasing it out. To the [scenegraph?](https://en.wikipedia.org/wiki/GTK%2B#/media/File:GDK_software_architecture.svg) That uses Cairo too.
Containers, regex, variant, optional, pair, and tuple are incredibly difficult to implement at the Standard's level of attention to detail.
They only distribute CMake for common architectures. People on exotic ones have to build it themselves, and then it depends on the availability of a C++11 compiler.
That's a good point which I completely agree with.
Exactly what I'm saying, some people don't have an easy access to a C++11 compiler on some platforms. As for you, you can always link a modern standard lib statically and dynamically link against whatever libc your system is using, that's all fine when you are using any popular architecture.
https://cmake.org/pipermail/cmake/2017-November/066611.html
Ah! Nice to see that's it's their agenda!
Letting `+` compile to an add instruction doesn't require overflow to be undefined behavior. Defining the behavior as resulting in an unspecified value, for example, would also allow direct implementation via different hardware add instructions without granting to compilers the full freedom of undefined behavior. So the fact that it gets compiled to an add instruction can't be attributed as an important real optimization due to UB. A better example of an optimization that can fairly be attributed to UB is given in [_What Every C Programmer Should Know About Undefined Behavior_][1]. &gt; *Signed integer overflow:* [...] &gt; for (i = 0; i &lt;= N; ++i) { ... } &gt; In this loop, the compiler can assume that the loop will iterate exactly N+1 times if "i" is undefined on overflow, which allows a broad range of loop optimizations to kick in. On the other hand, if the variable is defined to wrap around on overflow, then the compiler must assume that the loop is possibly infinite (which happens if N is INT_MAX) - which then disables these important loop optimizations. This particularly affects 64-bit platforms since so much code uses "int" as induction variables. Having overflow simply produce an unspecified or implementation defined value that's valid for the `int` type would be insufficient to allow this kind of deduction, so whatever performance gains are made possible can be attributed to UB. The analysis in [_What every compiler writer should know..._][2] does use the `-fwrapv` flag which does disable this kind of deduction. The paper concludes: &gt; “Optimizations” based on assuming that undefined behaviour does not happen buy _[sic]_ little performance even for SPECint benchmarks (1.7% speedup with Clang-3.1, 1.1% with GCC-4.7), [...] Although I notice that they tested 32-bit builds, whereas _What every C programmer should know..._ specifically states that the effect they described "particularly affects 64-bit platforms [...]." So I don't think the question is settled as to how much benefit we're getting out of what compiler's are doing with UB and I would like to see further analysis. [1]: http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html [2]: http://www.complang.tuwien.ac.at/kps2015/proceedings/KPS_2015_submission_29.pdf
&gt; People on exotic ones have to build it themselves, are people really doing *software development* on anything other than x86 &amp; friends ?
People are certainly doing software development _for_ other platforms, and that often means the build is happening on those platforms, because they don't have cross compilation set up, or even available.
&gt; they don't have cross compilation set up, or even available. ... what kind of platform cannot be cross-compiled to ? If anything I'm pretty sure that there's more platforms that can't be selfhosted. 
The platform libraries and things may not be packaged to be available on other systems, though perhaps manually extracting them and moving them to another system is possible. The toolchain binaries might not be available for anything except the target platform itself. Or they might exist as a separately licensed product but someone has decided that the license for just the non-cross-toolchain is sufficient. &gt; If anything I'm pretty sure that there's more platforms that can't be selfhosted. Sure, there's tons of embedded systems where cross-compilation is the expected mode of operation. But we're not talking about what's common, just what weird, exotic edge cases still exist.
&gt; It's true we don't yet have a standard library array_view, span, or simple collection type that is nothing more than a run-time size paired with a pointer True, but you could use [mine](https://github.com/crusader-mike/parray) -- I don't mind :)
Good tip. I used CMake because I wanted to show how to inquire for compiler features. It is a bit of a CMake tutorial as well as C++. [Full feature list here.](https://cmake.org/cmake/help/latest/prop_gbl/CMAKE_CXX_KNOWN_FEATURES.html#prop_gbl:CMAKE_CXX_KNOWN_FEATURES)
This is not the case, at least in my experience. I build all our code with devtoolset-4 and it runs on the host without it. It's the same on both CentOS 6 and 7. I use devtoolset-4 to be able to use C++11 with pybind11 to generate python bindings; it works fine without devtoolset-4 being active, and none of the shared libraries have DT_NEEDED references to things not on the base platform or any unresolvable symbol versions (I've checked for all my libraries). It might be possible to use it in a way which makes it require additional libraries, but this does not appear to be the default state of things. This must presumably mean there are some missing bits for non-header-only parts of libstdc++, but I haven't encountered any breakage in practice to date.
Not me. I looked at it, and chose to stick with our own superbuild. It might be fine if you only support Windows, but it's not at all cross-platform. It even hardcoded specific Visual Studio versions last time I looked; it wasn't possible to use the v141 platform toolset with the packages I looked at, only v140 built with VS2015.
[removed]
I have my sincere doubts that Boost performance is as amazing as we tend to think it is. STL suffers a lot from legacy support. I know that a lot of things happen for performance reasons, but there's no reason not to be able to do correct UNICODE processing in 2017. The standard library (plus the language) has no support for basic stuff like serialization. We keep saying that C++ is not for web development, but I look at stuff like node.js or python and I see no reason why we couldn't have a three lines HTTP server or client. While we fight with awkward stuff like ASIO some kid writes three lines in C# or python and has a webserver running. There's no reason why those three lines couldn't offer maximum of performance that C++ has to offer. We just don't offer those modern day APIs, do we? Why can't we offer a friendly default that can be tuned for maximum performance?
Absolutely, things could be improved. Qt has amazing documentation and is consistent. It's easy for anyone to understand what split does. I could bet (without looking at the documentation) that the first parameter can be a character it will use for separators. And there's no reason not to have a std::string::split() that returns a vector of string_view. But that's incorrect, isn't it, because split might happen on a temporary. So what can you do, not have a string split at all? Write all the code that does performance stuff and absolutely shy away from splitting strings? I only need to split strings once when I read a config file, but I have to drop to Python for that? That's what Qt does. It offers you nice APIs and you can use whatever C++ feature you want. They try to make it as inter-operable with STL as possible. Just having a less efficient function doesn't make the whole API evil. Performance is an important issue, but it's not the only one.
Right. Sorry, completely forgot about that. And I agree, that is a case where it beeing UB just seems unnecessary. 
&gt; We still have CentOS 6 machines. On these machines, only GCC 4.4.7 is available. Same problem but with RedHat 6. We're finally getting momentum in keeping up with newer libraries and OSes but even RHEL7 barely supports C++14, and we'll likely be stuck with it until after 2020.
Unsigned overflow on all hardware wraps back to 0. Signed overflow on all hardware does not result in INT_MIN. You end up with some other weirdness if you defined int overflow, particularly in 64-bit platforms due to the frequent conversions between 32-bit &amp; 64-bit numbers. Example of this can be found in bzip: [https://youtu.be/yG1OZ69H_-o?t=2358](https://youtu.be/yG1OZ69H_-o?t=2358)
It shouldn't, there isn't even any `libstdc++.so` being provided. The `libstdc++.so` file shipped by `devtoolset-7` has the following contents: $ cat /opt/rh/devtoolset-7/root/lib/gcc/x86_64-redhat-linux/7/libstdc++.so /* GNU ld script Use the shared library, but some functions are only in the static library, so try that secondarily. */ OUTPUT_FORMAT(elf64-x86-64) INPUT ( /usr/lib64/libstdc++.so.6 -lstdc++_nonshared ) And the provided `libstdc++_nonshared.a` is a static library. That's pretty much the trick they use; the versioning of `libstdc++` symbols will ensure that only those symbols not found in the system `libstdc++` will be linked in statically. Note that because of this behavior, a `devtoolset` built binary might work differently than one built with a vanilla GCC 7. That being said, `libc` has a versioning system built in and code built against a given version will work with all later versions but by design refuse to run on a `libc` even one minor version behind the one it was built on. Alas, if you want to build a binary that works on multiple Linux systems, you should choose the oldest one of them for the compilation.
Not the kind of portability I'm talking about. Library A uses a 2D vector from Eigen My code uses a 2D vector from GLM Library B uses a 2D vector from it's own implementation So, I've got to convert between the different vectors if I want everything to work together. If the standard library had a 2D vector then we could all use the same type and no conversion would be necessary. Even GLM and Eigen could provide code to either work with or automatically convert the STD implementation.
There is plenty of slow, baroque stuff in boost, for sure, but specifically ASIO is the way it is for very clear performance reasons and its not obvious to me how you could improve upon it without giving some of that up. You could build a high-performance web-server on top of ASIO and that is the niche its aiming for.
I'm not necessarily saying signed integer overflow shouldn't be made defined behavior (I don't really have an overview over all the pros and cons that would entail. [This talk by chandler](https://www.youtube.com/watch?v=yG1OZ69H_-o) was pretty interesting though). In any case, that is a standardization issue not a compiler issue. Blaiming the compiler for optimizing this function is a bit like blaming the compiler for generating code for `1/2` that produces `0` instead of `0.5`. Thats just how the language is specified and if the code relies on semantics that are different from those of c++, the error is in the code, not in how the optimizer works. 
Welcome back. C++ had gotten better but it still suck if you compare it with c#. You'll quickly feel the Verbosity of the code, the compile times, the cryptic templated errors, the lack of apis in the stdlibs... You'll like auto, range based loops, lambdas, threading support (but much hairy than c#). You'll still hate header files (DRY anyone?) and cross platform development with c++
sphere991 is right, the comment that was actually made concerned `std::vector`, not `new char[N]`. I corrected this in t he post. It turns out that `new char[N]` doesn't initialize, while `new char[N]{}` does, so there the caller already has the desired control.
I actually think that the part starting around minute 38 is much more important, because he explains how the narrow contract of signed integer arithmatic is used to find bugs and security vulnerailities in google's and particular android's code base.
Let's all take this moment of silence and pray for all those who, after all these years, still have to program in a standard that is about 19 years old by now.
I really hope the 2G graphics TS will make some progress before C++20, because there's one feature I really want: `text_extents()` without OS API calls that are all platform dependent. The rest I don't care as much but this has been quite a pain to make shit portable when you want to make your text align correctly.
The only thing I dislike about ASIO is that it remains a complex low-level API without a strong high level counterpart. 
I mean, this is this is neat and all, and a sneaky bug to hit, but it's got absolutely nothing to do with C++, other than that's the language you tickled it with.
Compared to all the code in C, C++, Java, Python, JS, PHP... definitely. by orders of magnitude.
Thanks :)
It's about a bug in a common C++ compiler. 
You know, as i walked in on my girlfriend banging the mailman, i couldn’t help but notice the C++ stl reference book was still open on my desk from last night. Thanks a lot bjarne!!
It's not a bug when the compiler thinks its dependencies have changed and recompiles things. That's literally its job. This problem manifests in all sorts of systems that rely on timestamps proceeding forward. Good old-fashioned `make` has this problem. It's not a C++ issue, it's a "computers and time" issue.
&gt;using arbitrary niches, not just 0, to represent a data-less variant, e.g.:Option&lt;bool&gt;, Option&lt;Option&lt;bool&gt;&gt;, Option&lt;Ordering&gt; are all 1 byteOption&lt;char&gt; is 4 bytes
The standard is very clear on this: "If the mtime of a source file indicates that the file has time traveled from the future, the program has undefined behavior, no diagnostic required. Unless of course, future standards documents have time travelled along with it, in which case they shall be treated as normative references."
Yes, but the heuristic "if time(dll) &gt; time(this obj) then dll has changed" has a false positive. Call it a bug or not, I don't mind.
Still it is good to be aware of it. Some people understand how compilers work some don't. Those who don't may classify something weird as bug and those who understand should teach and show those who do not understand why they are wrong, otherwise the collective can't grow. It is alright if someone doesn't understand a concept. But by attacking someone for not knowing we damage the whole collective. Sorry for the rant, and thanks for explaining how compilers work. 
eigenbom, don't let rusty naysayers tell you what's interesting and what isn't. For them everything has already been invented and nothing is good enough to tickle their enthusiasm. The important part is that you observed a problem in a complicated system and solved it. Cheers.
As /u/render787 points out, this is, in fact, [very clearly defined undefined behavior, according to the standard.](https://www.reddit.com/r/cpp/comments/7en9i2/a_curious_bug_i_encountered_today/dq6d2mf/)
Nobody said it wasn't interesting. I said it had nothing to do with C++. This would be a fine post over in /r/programming or /r/coding or some other non-language-specific subreddit.
https://github.com/akrzemi1/markable
I'm unclear why system DLLs should influence precompiled header generation. They are binaries; they shouldn't be involved with headers (precompiled or otherwise) in the first place. 
Thanks, it's all good - I just don't have the time to debate semantics. Enjoy the bug!
Finally, someone who understands how to think fourth dimensionally!
Good point, fixed. Thanks for the report!
Afaict precompiled headers can be a serialised binary representation of the compiler state, which could depend on system libraries etc. 
Interesting, I always though the STL is a consistent and nice API and I still do. Intellisense is not a concern to me, I code in vim and I hate auto-complete but I'm not dismissing your argument, good IDE support is important for a lot of people. I find cppreference, especially in offline mode, used through some dedicated documentation-browser (QtAssistant or Devhelp) to be of tremendous use. I can always find what I need in the matter of seconds or minutes at worst. I guess the reason for some of the things you criticize (erase-remove, random library) is that STL has to fit a lot of different use-cases. They made it very general and sometimes this is a nuisance. The best example is that all functions doing something with a container take a pair of iterators. This is very annoying if you always want to pass a whole container but is a godsend when you want to slice one. Ranges should help here. Some stuff is simply old but I agree that it's a shame we still don't have a proper UNICODE string. I love and hate boost. It's nice that it follows STL style with its API so closely, it's very easy to combine it with STL code. But it severely lacks in readable documentation and it has a lot of legacy solutions from the pre C++11 age that it forces you to use (e.g. boost::smart_ptr). Some boost libraries just copy-pasted their headers into the documentation but at least they have dense language-lawyer style design documents that I as a user am totally uninterested of. I also love Qt. Its my favourite UI and desktop application framework. Its documentation is awesome. QString is awesome. What I meant by un-C++-y API is that sometimes they differ from STL style interfaces in subtle but annoying ways. For example you cannot use ranged-for over a QMap because it's iterator dereferences to only the value. What if you need the key? This also disqualifies it for use in any std::algorithm. Or that most of their containers lack cbegin() and cend(). Or that most of Qt's types are not move-friendly. They still use COW extensively and C++ as a whole moved away from that, towards move-semantics and views. Qt didn't follow although to be fair even the STL is not quite there yet in this regard. It's small things like that that make you feel things work in a slightly different way than you are used to. 
Nice and detailed summary of the state of Modules TS. Thanks!
Is there an explanation of this optimization for non-Rust people? What and how was optimized? Looking through the linked comments and the GitHub link, I have no clue.
enums in Rust have a tag (or discriminant) that keeps track which variant is selected. Various optimizations can eliminate the tag and its overhead. This PR adds an optimization for Option&lt;T&gt; (an enum) that eliminates its tag when T has bit-patterns that are invalid: - bools must be 0 or 1, so any other value is invalid. - chars have many values that are not valid unicode codepoints Previously this optimization was valid mostly for NonZero types (like references)
After you have the experience of working with .NET framework you'll see that STL feels a lot less consistent. And then everything crumbles - you'll tend to appreciate more things like Qt, even if they are not performance workhorses, just because they allow you to do your job properly. That's what's bothering me, the constant defense for an archaic way to do things, use-cases that bother everyone but are used by .001%, the inability to answer to modern needs (like unicode, serialization). I'm not happy with the basic offering, although I know it's as fast as it can be. But it doesn't make me happy, it feels half complete, and in serious need of revamping. The sad part being that the STL is designed by the standards committee, and they insist in keeping the cumbersome part in because someone used them in 1987.
I never worked with .NET so I cannot compare but I never felt I cannot do my job properly with STL or even with boost (annoyances aside). I'm not defending the archaic bits but there is a reason they exists. And while I agree that some of them suck bad, it's part of why C++ is so successful. No one would take a language seriously if it broke backward compatibility in every other release. And believe me there are companies that still code C++ *exactly* like in the '90s. I work for a company that moves to the most recent standard as soon as it's supported but most people are not that fortunate. Yes there are areas where C++ is lacking but there is progress. It there weren't it would bother me a lot more.
Yepp. The same happened to me. Fall Creator Update. It also breaks Visual Assist X.
How do you store an invalid value in an Option? And how is detecting such things an optimization? I'd consider it a warning/analysis improvement...
Rust enums carry a payload, and thus have two parts: first a C/C++-style enum (called the "discriminant" in Rust), followed by a C/C++-style union of all possible payloads. Previously the discriminant was simply placed before the payload in memory, but now it has the ability to store it *inside* the payload, if the payload types permit this. This is typically the case if the payload is a `bool`, `char`, reference (non-zero pointer), or another enum. 
It doesn't add it for `Option&lt;T&gt;`, it adds it for *every* enum type. `Option&lt;T&gt;` is not magical in any way in Rust. 
But wasn't that introduced in C++14? What if OP still has to use an older version?
It also breaks itself; won't boot with NVMe SSD. Microsoft apparently forgot to add the NVMe driver in the OS update.
Much clearer explanation than the other. Thanks
The reason I'm not using it : https://github.com/Microsoft/GSL/issues/559 One does not simply replace std::insert_your_container by gsl::span
What did go wrong when you modified the toolset file?
I'm not unreasonable. I do know what legacy means, but also people must understand that what I'm saying comes from a different experience than their own. My answer keeps getting downvoted not because I'm not right, since I expressed my opinion and point of view based on my experience, but because people are hurt to read that their system is not the most perfect in the world. Instead of trying to understand where my opinion comes from, they assume I'm unreasonable and get upset by what I write. They forget that this post is about a .NET developer who worked for 15 years with that technology stack, and the message I wrote was an answer for him (or her).
You should never be compiling and programming on your phone. Just vnc to a VPS nerd. Jesus christ.
This is why you don't using namespace std;
For the record I didn't downvote you. I respect your opinion and for me it was interesting to read about the experience of somebody coming from another language ecosystem. Indeed C++ has many pain-points that many younger languages solved/did better at. Don't even get me started on dependency management or build-systems in C++. :))
It seems the PCH file is basically a memory dump of the compiler at the end of the stdafx.cpp file. No wonder it's so sensitive to the used dll files version.
Still no updates :(
No experience of Conan but we use vcpkg. Our nightly builds are MSBuild based. Using vcpkg has helped simplify library adoption. Yet to fully understand managing libs with specific build configs and adopting newer releases into release cycles but overall, it's been pretty seamless. Certainly better than integrating libs manually or NuGet, which was never a good fit for cpp.
What if you used uint64_t?
Because of returned references in http://en.cppreference.com/w/cpp/utility/optional/operator* they could only do such optimizations if they do abominations like they do (and regret to do so) with std::vector&lt;bool&gt;.
No, I wasn't complaining about you :) But thanks for the thought. I'm just saying that people misunderstand my point, I don't really care about the downvotes &amp; all.
How does it break VAX?
I am old enough to remember when C only run on very expensive UNIX workstations, whereas the peasants had dialects like Small-C on their systems. Or when PHP was nothing more than a Perl wannabe replacement for doing web sites. Adoption takes time.
Think Solaris or AIX for example.
proxy references are not an abomination. getting either proxy or real references because of a template instantiation is an abomination.
I think of standard library as a standard **interface** library. It would be great to have common interface hierarchy inside, so library developers could inherit their classes from them and be compatible with other parts of the language. Range -&gt; String -&gt; (UnicodeString -&gt; (std::u32string, std::u16string, std::u8string), ByteString -&gt; std::string), and so on.
The type you use in the bitset only affects the return value that is given when you access the variable. It has no effect on the code that is generated to extract the value from the bitfield. Sorry I wrote the above comment from a mobile phone, should have written uint64_t. Looking at the following snippet in gcc.godbolt.org I get vastly different asm in GCC and Clang: #include &lt;cstdint&gt; #pragma pack(push, 1) struct Foo { uint64_t a:22; uint64_t b:22; uint64_t c:20;}; #pragma pack(pop) uint32_t getB(const Foo&amp; f) { return f.b; } GCC: getB(Foo const&amp;): movzx eax, BYTE PTR [rdi+2] shr al, 6 movzx edx, al movzx eax, BYTE PTR [rdi+3] sal rax, 2 or rax, rdx movzx edx, BYTE PTR [rdi+4] sal rdx, 10 or rdx, rax movzx eax, BYTE PTR [rdi+5] and eax, 15 sal rax, 18 or rax, rdx ret Clang: getB(Foo const&amp;): # @getB(Foo const&amp;) mov rax, qword ptr [rdi] shr rax, 22 and eax, 4194303 ret
Exactly this! We actually use an inhouse math library and having those tools in the STL would be really awesome when trying to use code that is dependent on other math libraries.
vcpkg is available on AppVeyor 
CMake can still be built on Solaris and AIX, and is on a nightly basis. https://open.cdash.org/index.php?project=CMake 
You could also just name the getter `x()`. Of course that's still 2 characters more than `x`, but `a * b.x()` looks very reasonable to me and similar to `a * b.x`. I don't see any reason to choose a name like `getX()` for something like this. Now what I am wondering about, does glm have exactly that problem and compiles to really inefficient assembly? They seem to use the union approach: https://github.com/g-truc/glm/blob/master/glm/detail/type_vec4.hpp#L18-L80
Great article !!! Awaiting for part 2
Someone needs to bring this up at the next Grill the Committee panel at CppCon. It is a serious design flaw that I've fallen victim to at least 3 times in the last year.
You do need several mock *functions* in your scenario, just not a mock *class*. If I am concerned about testing (and not too concerned with performance), I can have my parent class inherit from an abstract base that declares all the valid callbacks. That will allow just one parameter to be passed in to the constructor, instead of a tuple of N (which in the declaration looks like one parameter, but when it's actually called it will be as verbose as the original N). It also supports unit tests equally well as your scenario.
It would be nice if we had get/set like C# does so this wouldn't even be an issue but since we don't have those in C++ I was forced to hack an alternative to make the syntax more compatible with GLSL. For me it is more natural to write vector code with this syntax. I haven't seen this approach used so figured might be worth writing a few words about it. At any rate, I am my own best customer so I mainly try to please myself. :) The technique should be conforming and not invoke UB or anything suspicious like that. The C++ standard has a few words about unions of same types and I am hoping I am not bending the rules too much by hiding the type as outlined in the code. I can't really compare this to glm as it does not do SIMD. It might benefit from not having to do things "the safe way" as the union members are assumed to be discrete objects if their storage overlaps. The standard has some wording about it being legal only to read from object in union that was written in prior so it might be UB to do otherwise but I would have to read the specs to make any certain statements about that. In practise every compiler I ever used treat the objects as discrete instead of aliasing. Don't quote me on this as I did NOT read the specs prior to writing this paragraph. I am most likely wrong so if anyone has more accurate information please do step in. :) Long story short, I am certain that the union of same type works correctly so the above isn't even an issue. Just wrote that as a response to the glm question. I was poorly prepared. Sorry. 
I have been watching SG13 discussions since a long time now. My own conclusion is that a 2D library should not be in the standard. The fact that this SG changed its name so many times is an indication that the problem is not well defined. So the answer (the current proposal) is totally useless for anyone because it does not solve an actual problem. 
Thanks, glad you enjoyed it. Part 2 will be online next week.
Will modules speed up build time? I thought they were primarily an encapsulation/interface thing.
IMO the most significant change since move semantics. Huge modernizing step forward. I can't wait to NEVER write a .h again.
Seems very interesting. Looking forward for next part. 