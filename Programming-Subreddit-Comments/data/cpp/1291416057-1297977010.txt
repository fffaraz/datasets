Does RAII handle lower-level exceptions such as SIGTERM and stack overflows? Those are the types of exceptions I'm unsure about.
While your points are valid, CORBA is *never* the way to go.
For whatever it’s worth and speaking only for myself, if I were still reviewing CVs and covering letters, I would be interested in meeting someone in your position for a first interview for almost any entry-level programming job. You seem genuinely enthusiastic, and I appreciate that you’re willing to work hard to get into the field. I suppose that means my first advice is to make sure your covering letter gets this enthusiasm across concisely but clearly. That puts you ahead of the field before most people have even looked at your CV. The other thing that is always attention-grabbing IMHO is some sort of worthwhile demo. CVs are, basically, boring, particularly for entry-level positions. When you send a CV, most people are only going to scan it for a few seconds, and it’s going to be tough for you to get noticed in a competitive field given your inherent disadvantages. On the other hand, if you make an interesting demo, put the code on a web site somewhere, and then stick a prominent link to it on your CV, it is practically guaranteed that any geek who sees the CV *will* go and take a look. Again, at that point, you’re already ahead of 90+% of the field, regardless of their academic credentials. NB: It doesn’t much matter what the demo is, as long as it does something interesting, it is clearly your own work, and the source code is available. I slightly disagree with others here about contributing to Open Source projects: by all means do, but unless it’s *immediately* obvious what your contribution was and what your code looks like, it’s not going to have the same impact as a personal project, at least not if I’m the guy you’re trying to convince. If you’re interested in the mathematical side of programming, you might try building a basic ray tracer and rendering a simple scene. No-one’s expecting POV-Ray, but it will make the points that you are enthusiastic and that you have at least basic programming and mathematical skills. Another idea would be to implement a basic AI player for a particular game. Again, you don’t have to beat the world chess champion, but if you can show an awareness of how to research and apply the basic algorithms from the field, it speaks volumes for your aptitude. If this sort of idea appeals to you, I suspect you’ll do just fine getting interviews at some good places. If both of these ideas sound unrealistic and too far beyond your current skill and understanding, then perhaps you’re not (yet) ready to go into programming as a career. Some other thoughts, in terms of expectations... You *are* going to be behind the CS grads on theory, but most of us aren’t nearly as good as we like to think we are straight out of a degree course. Industrial practice requires much more than just knowing why you can’t sort more efficiently than O(*n*log*n*), except when you can. Everyone going for an entry-level programming job is starting the climb up a steep learning curve, and you’re not so much further down. IME, you are much more likely to benefit from everything above if you are applying to a smaller company, for the simple reason that larger companies tend to filter candidates via HR (who are notoriously bad at judging technical merit, particularly for those outside the norm) while smaller companies tend to give CVs to team leaders and other senior developers and often don’t have as many candidates to screen. Remember, show the geeks your enthusiasm and a demo that captures their attention for even *one minute*, and you’re already ahead of most applicants who sent in a cookie-cutter CV with their cookie-cutter course projects and token OSS contribution because someone told them it would look good. Please understand that you have a long way to go before you are going to achieve the kind of job you seem to really want. So would any new graduate, CS-trained or otherwise. But for you personally, as you are mostly an autodidact, there are going to be blind spots where you don’t even know what you don’t know for quite a while, so perhaps try to find good discussion forums on-line on related subjects and lurk. It’s surprising how often you’ll come across new ideas or “tricks of the trade” this way and broaden your horizons. Also, whatever the Internet-famous programmers and consultants say, most of them aren’t super-human, and there are people just as smart and knowledgeable in most software companies who don’t choose to pursue their field in public to the same extent. Figure out who those people are every time you start a new job, and learn by watching how they get things done — not just the coding, but how they think about a problem, what sorts of tools they use, what sort of process they follow as they develop. Everyone is different, but you’ll start to see recurring themes as you watch more good people at work, and the sort of insight you’ll build up into effective professional practices just can’t be taught in any degree course. Best of luck...
Here's another C++/Qt web framework worth looking at: [Wt](http://www.webtoolkit.eu/wt).
He mentioned Wt (as Witty), but he didn't even mention http://tntnet.org/.
I wish at least one of the downvoters would leave a constructive comment letting us know why this is getting so many downvotes.
Generally it's for one of three reasons: * It doesn't belong in this r/ * It is offensive, misleading or from a bad source * People aren't interested Or there's always bots. Allegedly (who really knows?) bots are forever voting up and down posts to give profile to certain material. In case it's not clear, I don't think there's anything wrong with this post.
Are there any websites that currently use any of the mentioned frameworks? I'm just curious to know.
Even in C, **break**, **continue**, and **return** send control out of a block and can bypass resource-releasing code. That's still manageable when you first write a function, but with multiple authors it's hard to maintain. Your last sentence brings up a very important point about RAII. With RAII, *all cleanup uses the exact same mechanism*, whether the scope exited due to control reaching the **}**, **break**, **continue**, early **return**, a **throw** in the current function, or exception propagation from 20 stack frames down.
These are not widely adopted yet. But there exist some sites that use them, mostly web _applications_.
There's also [QtWui](http://qtwui.sf.net) (based on Qt too), [Creole](http://svn.geiseri.com/svn/projects/creole) and [Qt Web Client](http://labs.qt.nokia.com/2009/09/18/qt-in-the-cloud-with-qwebclient/). I wrote a comment in that blog but it's not shown now :-?
Rust design goals are much more verbose I have to say. 
well, it does or it doesn't. those type of errors come in as signals which are up to the application to handle. If they are not handled, the process immediately punts. If you are concerned about RAII, you could handle the signal, set a flag, check that flag in a controlling process and throw an exception. of course, in scenario of stack overflow, etc...you have to be prepared for the worst. note that signals etc are not a language feature -- they are an operating system feature that the language works around. C/C++ do not have 'sigterm' -- they are exposed via libc etc. 
I just found out about this project. It is really easy to use, so I thought I'd spread the word.
I think when it comes to ease of use, nothing beats [CLI](http://www.codesynthesis.com/projects/cli/). Compare TCLAP: TCLAP::CmdLine cmd("Command description message", ' ', "0.9"); TCLAP::ValueArg&lt;std::string&gt; nameArg("n","name","Name to print",true,"homer","string"); cmd.add( nameArg ); cmd.parse( argc, argv ); std::string name = nameArg.getValue(); To CLI: class options { std::string --name|-n = "example" {"Name to print"}; }; options o (argc, argv); std::string name = o.name (); Plus, CLI, besides the usage information, can also generate command line documentation in the man and (x)html formats.
Seems remarkably similar to [Boost.ProgramOptions](http://www.boost.org/doc/libs/1_45_0/doc/html/program_options.html)
My biggest pet peeve with VS 2010 is the lack of IntelliSense for C++/CLI. I am sticking with VS 2008 until this is remedied. If it's never remedied, I guess I'll never upgrade; I don't feel that I *need* to. https://connect.microsoft.com/VisualStudio/feedback/details/501921/c-cli-intellisense
It bothers me that ever since VC6, the releases have been getting more and more bloated. I can understand the overall system becoming larger as more APIs and functionality are included, but somehow the IDE itself is getting slower. Startup times used to be nearly instant, even when hardware was slow. Now, the interface hangs, not just on startup, but even if I hit Ctrl-F to open the find dialog. I timed this at no less than 3 seconds. I would imagine that the developers at MS would try to make an IDE they themselves would want to use, and that they have all the knowhow for making things snappy.
But the latter is valid C++ already, as long as foo is only needed in the current scope.
Wow, context sensitive keywords I never thought I'd see the day.
One huge problem is how the interface hangs when you press F1. In the age of multiple CPUs, couldn't they launch the help process in parallel? Another huge problem is MFC: it's a real shame Microsoft chose not to promote C++ as its language of choice by providing a really awful and painful framework like MFC. 
i'm pretty sure that the developers at MS use Visual Studio ... 
VS2010 support parallel compilation, yet yield longer compile time. It cannot break from compilation immediately, I have to wait for 5-10 seconds. I bet it is still the very sample compiler in the underlying except a prettier and slower interface. 
MFC simply still has users who will riot if MS kills it completely. I don't think MS even wants anyone to use it anymore, they aren't really developing for it, they just bought a couple of new UI gizmo libraries to roll into it to appease the MFC die-hards. You are welcome to use whatever UI library you want in VS and ignore MFC.
In VC6 days the compiler was significantly diverged from the standard. (in part , but only in part, because it was released before the 2003 standard) But since then they have shown a much stronger commitment to the standards, and hold their own against GNU in compliance. Honestly, I think VC users are being a bunch of whiners, and those that yearn for simpler VC6 days are nuts. Though, VC7 was crap.
Boost is very nice, but sometimes it is a lot to pull in for small projects. 
WPF will likely always be a bit slower, but it's where they are and likely where they'll be going forward.
CLI is interesting and has a lot of great features, but it is not C++ itself. You need to run the CLI compiler to generate the C++ code. This can be a blocking point for some projects.
&gt; Though, VC7 was crap But VC 7.1 was great. IMHO, the best VC++ release.
Fair enough, but ProgramOptions has a fairly minimal set of dependencies. My main thought was that TCLAP didn't seem like news.
What? Maybe I'm just sore about f4 not working 'right'. I would go back to 6 before using 7 again.
Those comments make me rage so hard. You peoples are complaining about things that are NOT "Microsoft C++ 2010". "Microsoft C++ 2010" is a C++ compiler, plus an implementation of the C++ standard libraries, plus a C++ runtime. Its not an IDE, editor or some GUI library or framework! Seriously, reading these comments makes me want to punch someone in the face, hard. So, for my coworkers sake, next time you feel like whining about an IDE, library or framework dont blame the compiler for it. Thats just ignornace and bad manners. For what its worth, "Microsoft C++ 2010" kicks ass with its C++0x features.
Why those that will riot if MFC is removed (all three of them) don't use whatever UI library is there other than MFC then? 
You have never worked on a big legacy code base, have you? And you'd be surprised how much MFC is still around. And you miss my point. Their is little cost to you that MFC exists there in visual studio. You don't have to use it, you don't have to use CLI, you don't have to use C#. So quit whining.
Actually, MFC is a crime against humanity. (I have to work on legacy MFC-based software, 140 kloc, all by myself. The code base is 12 years old). 
Do your own homework. 
Okay, then. If MFC is so bad, then why haven't you converted the entire 140kloc application to something else yet? Oh, yeah, because the returns don't justify the costs. Imagine if Microsoft flipped some imaginary kill switch and disabled MFC entirely. Everybody would suddenly be forced to upgrade their ancient legacy applications even if they worked fine previously. People will riot.
Can't believe Incredibuild still doesn't support 2010... wtf are they waiting for?
I think they support it in both 2008 and 2010. The only problem is parallel build doesnt work with incredibuild. 
VS Integration only works in 2008 and earlier. Was told by a coworker who tested it that remote buildagents don't work either.
I'll have to see how easily I can extract ProgramOptions. TCLAP may not be news to you, but it was to me. ;-) EDIT: And thanks by the way for pointing out boost.ProgramOptions.
The stand alone implementation of TCLAP was definitely news to me. I just got the impression the whole approach was news, which it shouldn't be. Boost may be a a big pull, and I've heard more than a few people expression your sentiments, but it has a number of very high quality libraries in it, often without requiring linking (though not the case here) as they are all in headers. As a consequence, I actually find it *ideal* for small projects, where compile times tend to be less of an issue, and the compiler tends to throw out all the stuff you aren't using during code generation (and even if it doesn't, these days on anything bigger than a small embedded system, the extra space isn't really missed). Definitely look at using [bcp](http://www.boost.org/doc/libs/1_45_0/tools/bcp/doc/html/index.html) if you are interested in extracting out a specific subset of boost.
I've been aware of other libraries -- though none as nice as TCLAP or boost.ProgramOptions. I've browsed boost many times -- even using it on some projects -- but never noticed ProgramOptions before. Oh well! Thanks for the tip on bcp! 
&gt; Oh, yeah, because the returns don't justify the costs. Not true. The maintenance cost is much greater than the cost of rewriting the application. &gt; Imagine if Microsoft flipped some imaginary kill switch and disabled MFC entirely. What a marvelous situation...!!!!
&gt; Not true. The maintenance cost is much greater than the cost of rewriting the application. If that were true, then it would have already been rewritten a long time ago! Legacy applications exist *precisely* because it's almost never cheaper to rewrite an application from scratch. Every business is looking to maximise returns, and no sane business would spend the time, money, and resources maintaining legacy applications otherwise.
It is true. They just never realize it. 
Seconded. I think the comments show that there are really a lot of different C++ communities. Personally, I download the Express versions and the SDK for free and use them with (X)Emacs, bash, and boost build so I have a similar environment on Linux and Win32/64. The only IDE component I use is the Visual Studio debugger, which is a very fine piece of software. My main concern for the future would be about a vendor with big market share implementing the rest of the C++00x standard in some timely manner so that it eventually does become a standard. I can't imagine every using the stuff people are complaining about in the article under any circumstances. 
Recently I started using VS 2008 quite often. There are two things that both me: Accidentally pressing F1, and the IntelliSense that almost always seems to get confused and not ever give me any suggestions. At least when I'm in VIM and use ctags, the completion isn't perfect but it works all the time. 
The terms are not used consistently as you describe them, even by Microsoft. If you download the free version, Microsoft calls the whole package "Visual C++ 2010 Express Edition". So in that case, the Visual Studio 2010 IDE is only part of the Visual C++ 2010 Express Edition package, so it would be appropriate to complain about IDE features as part of "Visual C++".
You are maintaining MFC-based software. You have realized it. Go rewrite it, and stop complaining that Visual Studio still supports the frameworks your software relies on.
I have already done that in my spare time, but the management won't accept it. 
Sample code?
I'm bored. What should I do? What does an atheist do when he drives up behind a car with a "Honk if you love Jesus" bumper sticker and that car doesn't move when the traffic light turns green?
I think your formatter should be %2d rather than just %d.
What you are looking for is &lt;iomanip&gt; from the C++ Standard Library. It provides functionality like std::setw() and so on. Here is an example of your program using setw(): #include &lt;iostream&gt; #include &lt;iomanip&gt; int main () { for (int i = 1; i &lt; 13; i++) { std::cout &lt;&lt; "Enter the rainfall for month " &lt;&lt; std::setw(2) &lt;&lt; i &lt;&lt; " in inches: " &lt;&lt; std::endl; } } And as you expect: Enter the rainfall for month 1 in inches: Enter the rainfall for month 2 in inches: Enter the rainfall for month 3 in inches: Enter the rainfall for month 4 in inches: Enter the rainfall for month 5 in inches: Enter the rainfall for month 6 in inches: Enter the rainfall for month 7 in inches: Enter the rainfall for month 8 in inches: Enter the rainfall for month 9 in inches: Enter the rainfall for month 10 in inches: Enter the rainfall for month 11 in inches: Enter the rainfall for month 12 in inches: Just a quick note on manipulators in general. std::setw does not have to be cleared or reset because it is volatile, i.e. setw(n) only works for a single subsequent variable. In general though you shouldn't **ever** really manipulate the standard output streams. I would suggest using std::stringstream when you are dealing with output in a manner that requires io manipulators. But, in this case there's really no worries since you are using it in a very concise context. Anyhow, in case you venture more into manipulators (which you should if you want plan on using iomanip): When you do use io manipulators, then you probably want to clean up after yourself. Here is an example of that: #include &lt;iostream&gt; #include &lt;iomanip&gt; int main () { std::cout &lt;&lt; std::hex &lt;&lt; std::setiosflags (std::ios_base::showbase); std::cout &lt;&lt; 100 &lt;&lt; std::endl; std::cout &lt;&lt; std::resetiosflags (std::ios_base::showbase) &lt;&lt; 100 &lt;&lt; std::endl; } The output: 0x64 64 Like usual, you should probably actually read about the actual functionality and purpose of iomanip before using it blindly. And, much like the rest of the standard library, it's easy to use it incorrectly and then just develop bad habits without ever knowing it. Best of luck. 
uh, topcoder.com? careercup.com? careercup.com is probably smaller-scale, more thinking problems (job interview Qs). 
You should definitely take a look at Herb Sutter's "Exceptional C++" book series.. contains many purely C++ specific puzzles that require different levels if expertise...
Try solving the Project Euler problems using only C++. It's definitely not fun and you'll gain a new appreciation for higher-level languages, but it sounds exactly like what you want.
C++ definitely is a pain in the ass, but I feel like those high level languages have set me back a little as a programmer. I've been going back to the basics to try to get a better job.
Yes, this is a great idea and a great site. I always find that having a problem to solve when learning (or remembering) a language is much better than just randomly learning things. Having a problem to work out makes you really concentrate on certain aspects of the code that you may pass over to 'come back and look at later' and makes you figure the tricky bits out.
While I can appreciate Project Euler, in my opinion it won't help one brush their C++ skills. Problems there are mostly from number theory and hence only require understanding of basics like functions, integral datatypes, maybe arrays and a bit of STL. At this level it's not *too* different from other languages. More involved concepts like RAII, virtual destructors, exception safety, and all the template shenanigans are not touched.
Do you think C++ will help you get a better job? ;) Disclaimer: I'm a C++ programmer.
&gt; More involved concepts like RAII, virtual destructors, exception safety, and all the template shenanigans are not touched. I cannot think of too many problems one could solve as an exercise that absolutely require any those features in order to find the solution (with the exception of templates). Those would rather be meant to hone one's technique than to test their skill.
I second topcoder.com, problems range from slightly challenging to tear your hair out tough.
For real, learn c# or java if you want to get in with most employers. Disclaimer: I'm also a C++ programmer.
Go to boost.org, choose a particular module from the latest distro, read the docs about what that module does, think about how you would write that code, compare with how the library implements it, repeat. 
write any program that runs quickly and doesn't crash. :-D 
If you're going to read library code to brush up on C++, Boost probably isn't the place to start. You need a large amount of (sometimes obscure) C++ knowledge just to follow the source. And so much of the source is laden with cross-compiler portability hacks it can really distract from the meat of the implementation. Boost is a great set of libraries but they're not exactly a great or enjoyable read. Learning how to use Boost is probably a great idea though. Just start playing with it. You'll be learning advanced C++ techniques in no time. 
Write a video game. I don't mean something particularly ambitious, but games force you to get into object oriented programming and I/O, and you have to think about optimization. Top-down space shooters are a good practice problem.
I didn't suggest reading every line of the source. I suggested thinking about how to implement the same thing first, so one has a good idea of what is involved, and then comparing the gist of your own approach to how boost implemented it. The differences will probably highlight more "modern" C++ idioms. Of course one also gains familiarity with potentially useful libraries at the same time. I disagree with the suggestion to just use the libraries without attempting any understanding of how they work. 
Agreed. Learning to use Boost, one or two libraries at a time is a good way to learn Boost (*the* library for C++) as well as *modern* C++. The #1 mistake I see aspiring and intermediate C++ programmers make [fn1] is messing around with the lowest level features and treating libraries like Boost and the standard library as something to avoid or outgrow. I've got lots of C++ experience, shipped many products, and know lots of trivia but when I need a dynamically-allocated array I use **std::vector**. That's not because I don't know how to dick around with new[], delete[], placement new, explicit dtor invocation, etc... but because I need something uniform that works now. Get a decent book like Koenig &amp; Moo's Accelerated C++, read it and do the exercises. Grab Boost, and use it in implementing a project. A simple, single-thread, blocking, static-page http server is a decent learning project to play with boost::asio and boost::filesystem. Expand it out to have useful command-line options (boost::program_options), make it multi-threaded, make it non-blocking, etc... and you'll have a reasonable learning project to play with. [fn1] Ignoring the C++ haters who will insist that using C++ at all is a mistake.
I know. **I'm saying that even getting the gist of Boost implementations without serious C++ knowledge is quite difficult**. Even reading the simpler libraries like Boost.Variant and Boost.Optional is incredibly challenging if you don't already know what you're looking for. I wasn't suggesting using the libraries without understanding them. I was suggesting that using and playing with the libraries would naturally lead to an understanding of them more quickly and more easily than reading the source code directly. I've been using Boost for years and consider myself a fairly competent C++ programmer, and still consider slogging through most portions of the Boost source to be a chore. 
Sure, develop a port scanner that uses raw sockets
I don't quite understand where you are coming from on either point - quickly getting the gist of various boost libraries from their source is not something I've found difficult, whereas getting a good understanding how they actually work without looking at the source - i.e. only relying on the docs, would be very difficult for me. Understanding template specialization is an obvious prerequisite - but that's a major language feature - arguably the most important one added over the last 10-15 years. I imagine that it would be very difficult to run a software project where programmers were significantly using boost libraries, but also not understanding how to write template specializations. For starters, they wouldn't know how to make sense of many error messages from their compiler. 
Yes. At first it will be fun. then they'll start asking things that don't fit in an unsigned 64bit int. For the first few problems you'll kind of hack something together until you realize: you need to create something that supports addition and multiplication with, what I call, stitching. This is the not fun part: the fucking overflows and stitching. Once you get that down, you can get serious again and mind-fucked by the problems. good luck 
I work with c++ but free-time with c# because I like it so much
&gt; Do you think C++ will help you get a better job? I do. Real software shops (MS, Adobe, etc) heavily use C++. Sure, for internal IT positions C++ may not be important, but I don't consider that a good job.
[Irrlicht](http://irrlicht.sourceforge.net/) is fun to play with and will get your C++ mojo going. Check out the [tutorials](http://irrlicht.sourceforge.net/tutorials.html). 
http://flightgear.org - lots of stuff in there
I think better knowledge of it will. My skills are mostly in java/javascript/php. I want to know lower level languages better. I'm also dusting off the assembly language and os books. I want to get into computer security.
Intermediate. N threads publishing arbitrary-length structured data to 1 queue, at arbitrary fast time intervals. M threads consuming from that queue in order to do some output. 
When I think "intermediate problem," I immediately consider library code for lambda operators, smart pointers, and Asio.
I was doing reasonably well on topcoder.com until some problem came along that involved finding minimum paths around a fractal digraph or some other nonsense. 
Stroustrup's "The C++ Programming language" has exercises in it.
There is a big list of modules from which to pick something - http://www.boost.org/doc/libs/1_45_0/ Obviously, some are more suitable choices for a given student. You're being sarcastic, but smart pointers might often be one good choice as an exercise. The point is not to get to the same solution. 
If you want to see six figures, C++ is the best way to get there. 
The 99 prolog problems are fun.
Going straight C might be a better idea. I would heavily recommend reading the book Computer Systems: A Programmer's Perspective by Bryant and O'Hallaron. It is what Carnegie Mellon uses for it's Systems Programming class and has a lot of interesting topics to introduce you to core of programming and computers. Assembly is also lightly touched in the book.
It's not just understanding template specialization. It's understanding **all** the compile-time dispatch rules in C++. You have to know when it's dispatching based on template arguments vs ADL vs overloading, etc. And not just that, if you want to follow some compile-time dispatch you have to have some idea of where it's conceivably dispatching to (i.e. exactly what definitions are in scope at the time). And that's not even including some of the dark magic going on in the pre-processor. Effectively reading Boost source requires that you have a reasonably capable C++ compiler in your head. Sure that's true of all C++ libraries to some extent, but Boost really pushes your internal compiler to the limit. Hell, it pushes actual compilers to their limits. The Boost source basically uses every C++ trick in the book (and some that aren't in the book) and manages to pull of some pretty amazing things. I think it's just too much to ask of someone who's just starting out in the language. There are better ways to spend one's time that will give a better return on investment. It's true that it you'll have to delve into the Boost source some when learning to use the library, and that's good. But it's a gentler introduction than just diving straight into the source code.
I remember a problem similar to that. It was finding at path around any of the fractals edges (I.E. the outside of any layer) without going into another layer. Is that the problem you were talking about?
 int main(){ int i = 1337; return 0; } Do I win something?
Id suggest a physics class.
Find an open source project in the area of physics you're interested in, an analysis suite or some such. Join the developer mailing list, create an account, etc. Go over bug reports. When you reach a description that sounds interesting, try to fix it. Submit a patch. The benefits you will receive are: * Learning how a group collaborates to solve problems * Reading other people's code * Receiving critique of your code from experienced, skilled programmers in your field of interest Continue along that path, try a few projects till you find a good fit personality and interest wise. You'll find as you learn the project, you may have your own ideas for features or improvements. Do it. Submit a patch. Add it to your resume as experience. *edit* Oh, and read [The Pragmatic Programmer: From Journeyman to Master](http://www.pragprog.com/the-pragmatic-programmer). It's not a technical book and is an easy read. I'm about 1/2 way through and I don't think I've seen a line of code. It's about how to *be* a programmer. It's basically an entire book that answers the question: &gt; Now what?
Yes, I've completed those requisites too.
As a potential interviewer, I don't care what classes you've taken. What have you done? You need to be able to demonstrate your knowledge. As several people have mentioned, contributing to projects is a good place to start. Interviewers want to see ability, not promises.
You gotta do some algorithms, dude. And then, after you get some basic algorithms down (sorting, searching, trees, graphs, etc.) you gotta pick up a graphics api (either opengl or directx). Then, if your username is in reference to astronomy, you can create simulations of solar systems, etc. Sweet! AmIRight?
http://projecteuler.net/ is always good. Btw, since when was computer science a pre-req for physics? I double majored in both and the only overlap was math. 
I would expect C++ to increment C by one whole something, such as a whole-step; this would imply C &lt; C# &lt; C++. But who knows, ++ could have been overloaded to do something different. :-)
&gt;&gt; [citation needed]
Maybe spend some time with a physics API - Bullet is the hot one right now: http://bulletphysics.org/ Bullet and APIs like it are of course more about things looking convincing and than being perfectly accurate, but I expect you'll find your dynamics coursework is directly applicable. Look at combining that with a visually-oriented API like OGRE:http://www.ogre3d.org/ or Cinder:http://libcinder.org 
&lt;--chick. But yes, that is the goal eventually. And yes - sweet.
I think since programming is a fundamental tool for theoretical physics. Some school's merely recommend it; others require it. It was the first time that I really understood learning a computer language was equivalent to controlling a brain outside my own head. The prospect is very exciting, but I'm so relatively new to the process that I don't have a sense of how, when, or why to use it. The same could be said for physics and math. I've learned a lot of theory, but have had very little experience with application outside the labs associated with the classes. In the labs, everything is pre-planned and there is very little time to explore because there is a time crunch to finish the procedure. Opportunity for creativity has never once been a part of my curriculum in the sciences. I'm so used to showing up and being told what to do, that I am struggling to change the way I think and use what I've learned because I want to, and not because someone is telling me to. But, well, I don't have much experience with this, and I don't personally know anyone with this sort of experience either. 
Thank you, I will check these out. 
Thank you for this recommendation.
Ah, forgive me my dear. The internet, as wondrous a utility of idea-germination as it upon first glance appears also doth remove from us those so humanizing traits, of which I of course refer to the experiential wonder of one's gender, that become so requisite to the interaction of that so recently referred to humanity. Please accept my most humbly actualized apologies and let me extend to you my deepest and most heartfelt welcome. For I, as so many of us have before me, do know how unwelcome this engine of thought-transference we colloquially refer to as "a series of tub-girls" has proven to be to the most mysterious and compelling sect of humanity known as "females." For to brave such waters as you have clearly done must show what strength of heart, what mettle, what shear audacity in the face of danger it so clearly has been for those of your kind in days of yonder that I must step down in abeyance of my former demeanor to allow you the respect you most certainly deserve. Allow me then to bequeath to you the title of "Female!," a mantel you must wear with pride and honor. For the composite parts of the word above so clearly show you to be what you most certainly are: "Fe" from the ferrous, meaning iron, combined with that descriptor which we most here belong to, "Male", doth make you "Iron Man!" And Well Earned, I might add! For she that doth wade deep in the waters of anonymity do subject themselves to the waters and sewage of humanities most horrific and terrifying of societies adolescent male base urges, which heretofore have always been swept away by society, but with the resolution of mind that so easily comes by the powers of the internet we find that we are also subjected to its mental feedback, and thusly unable to drain away between societies cracks and crannies has become clogged and now must wash over us all. Nevertheless, I welcome thee! Know thee that my home throws wide its doors in offer of hospitality. Welcome, Female, Welcome IronMan, Welcome "Chick," as you have so thusly self-titled yourself, and know that you are most certainly welcome, and well come.
Well met, male.
And you too, female. Also, here's a few links to get you started: [Nehe's tutorials are pretty good](http://nehe.gamedev.net/) [CProgramming has great stuff on it](http://www.cprogramming.com/tutorial/computersciencetheory/sorting1.html) [GameDev is a favorite haunt of mine](http://www.gamedev.net/) [And the same goes for CodeGuru](http://www.codeguru.com/) [C++ faq lite is awesome bathroom reading material](http://www.parashift.com/c++-faq-lite/) As always, read the faq's. Happy Coding! - Goishin
Why can you not use a member class? That is how the STL handles iterators for containers. The member class will have access to the parent class's template parameters. Something like this: [gist](https://gist.github.com/737749)
This is the idiomatic (as far as STL goes) thing to do. And probably the best solution.
I think with a typedef you can do what you want. Not sure if the standard allows that.
First, let me try to come up with a design that does what you want. How about something like this: template&lt;typename TContainer&gt; class TIter{ public: TIter(const TContainer&amp; c) :C(c){}; TIter&amp; operator++(){ C.PrivateUpdateIterator(index); return *this; }; bool operator!=(const TIter&amp; rhs) const{ return false; }; private: int index; const TContainer&amp; C; }; template&lt;typename T&gt; class TContainer{ friend class TIter&lt;TContainer&lt;T&gt; &gt;; typedef TIter&lt;TContainer&lt;T&gt; &gt; iterator; public: iterator begin(){ return iterator(*this); }; private: void PrivateUpdateIterator(int&amp; privateIteratorMember) const{ ++privateIteratorMember; }; }; Obiously I just hacked that together, but you get the idea. The container knows its iterator. It makes the iterator a friend, then the iterator calls the private PrivateUpdateIterator function with references to the things that the container needs access to. It's hard to tell what you're trying to do, so I have no idea if that's appropriate. But I am concerned that you want the container to have access to the iterator. Shouldn't the iterator know how to increment itself and dereference itself without needing privilaged access to the container? Or maybe I misunderstood your question? I took a quick look at how list implements iterators, and they require no such hackery. Anyway, I sincerly hope I'm not comming off as a jerk. It just looks a little strange to me. Hope this helps. 
That's what I had originally. But I was using Borland and it has problems with nested classes within a template in some versions of their compiler. It doesn't work with partial specialization of containers when you need to use iterator_traits on the nested class. So I made my iterators external to the containers and everything started compiling a lot better. Until I ended up with the above problem. Also, I have several containers. I was using macros and could go back to that I suppose. I'm not really a big fan of nested templates. They are awkward to work with. If I wanted the iterator and const_iterator to be each other's friend, I have a difficult time getting all the compilers to accept the source code. If they are external to the container, it's no problem. Just frustrating that all the compilers act differently. There are countless other problems, gcc having its own problems with template methods vs. overloaded methods when an argument type is a template class parameter. I'll go back to the nested class version and see if I can't make that work somehow. edit: Actually, no. I'm going to make it public if there's no better solution. It's just too messy the other way. 
Your setup is very close to what I want. Yes, the iterator knows how to update itself, but it needs to be told when to do so, hence the problem. I only want the container to be able to tell the iterator to do so. Also, the container does some very advanced node manipulation in some cases and can't possibly delegate such responsibility to a mere iterator. But those iterators do need to be updated with the new state (new index) in some cases. So how do you tell the iterator that its new index is 5 without allowing external classes to be able to do the same when friend functionality is not available? In TIter, the following is illegal: friend class TContainer; Oddly enough, Borland will compile this if you omit the word 'class' and VC++ will compile the above just fine. But it's not legal C++. Even gcc has a way around this, again not legal. edit: Right now, I have two choices. Make the internals public (and I have no qualms of doing this since it would work on all compilers straight up). Or I follow the advice of authorblues and AlternativeHistorian and go back to what I had originally with nested iterator classes. Partial specialization of containers won't compile in Borland, but that's only for two containers. I'd have to find an alternate solution, possibly conditional compilation to solve said problem. Like I said, 'public' is looking REALLY good right now. edit: I've been looking at it and there aren't that many places where iterators are kept updated. One place I would have liked for this to happen is insert(where, value); Right now, 'where' is updated so that its index is incremented, but not its node. That way, you can insert several values before the same iterator without that iterator becoming invalidated. With STL, no iterators are ever updated like this. I could go that route as well, but it would render a lot of things near unusable. 
Before you make the members public, how about a public function on iterator that is called from container and updates the state. Call it "PleaseDoNotUseThisIfYouAreNotAContainer" or somesuch. At least you can grep for it if you come up with a better design and it documents that you don't want clients to call the function better than just making the members public does. Edit: I almost wish I had the real code in front of me. It's hard to figure out the right thing without looking at the actual code. *Almost* wish. I have enough work to do :-)
&gt; Call it "PleaseDoNotUseThisIfYouAreNotAContainer" or somesuch. Yeah, that's where I'm headed. No need for a method. A couple public members work just fine. The state isn't complicated enough to worry about future changes. My biggest worry is that anyone can update it. A method that can alter the internals has the same issue, so is not worth it. I'm perfectly fine with 'public' if it solves the problem with the drawback that people may abuse it if they disregard the comments. Sometimes 'public' is the right thing to do. Yeah, I said it. ;) &gt; Edit: I almost wish I had the real code in front of me. It's hard to figure out the right thing without looking at the actual code. Almost wish. I have enough work to do :-) It's quite lengthy unfortunately (and I have two versions of it, neither of which work properly on all compilers). But your code is very close to what I was trying to do. In a few places, I want the container to update the iterator state. That's it. I thought this would be a REALLY simple issue. But it isn't. Not by a long shot. I'm caught between a lot of different compilers that all act differently. There are more issues than this as well. I'm really surprised at how badly templates are supported overall. 
Marshall Cline from the [C++ FAQ](http://www.parashift.com/c++-faq-lite/index.html) has a pretty thorough look at [const correctness](http://www.parashift.com/c++-faq-lite/const-correctness.html). Take a look and try a few examples. If you're still confused I am sure we can help. It also sounds like you're a little bit confused on the distinction between structs and classes. `struct` is default public, while a `class` is default private. There is also no difference between `struct` and `class` in terms of memory used. That's honestly all. Everything else is left up to interpretation in application. It's also good to note not to confuse the difference of a C `struct` vs a C++ `struct`. You can apply inheritance, create constructors and destructors, and even use private/public keywords with a C++ `struct`. Like I said, the only difference is that a `struct` is default public. See 11.2.2 and 11.2 of the C++ standard. The [C++ FAQ link (7.9)](http://www.parashift.com/c++-faq-lite/classes-and-objects.html#faq-7.9) also answers the question as well.
I understand the difference between a struct and a class, but sometimes I am getting compilation errors where I was supposed to have const after a member of the class/struct. For example, Node(const Nod&amp;n) { //Copy constructor name = new char[strlen(n.name) + 1].....etc Why is that const? I really don't understand.
You're passing by const reference there. That's because 1) It's a copy constructor 2) It's const because you shouldn't modify the object if it's being copied 3) It's a reference because you are not paying the cost of object construction.
To simplify a lot. - When passing a method an in-parameter you can do it by value or by reference. Simple built in types (like int or float) you should pass in as *non-const* value (as copying them is cheap and the caller does not care if the copies are modified or not). Other types (like std::string or YouOwnType) should be passed in as *const reference* (as copying them is more expensive, and the caller cares if you modify it or not). You almost never want to pass in a *non-const reference* to a method. - When returning a result from a method you should do it by value. The optimizer will usually make it cheap (Named return value optimization). You should never return by *const value*, as you don't know if your user wants to change the return value or not. You can also return results by writing to *non-const out parameters* but you generally should not.. unless you really know you are smarter than the optimizing compiler. - Local variables or member variables should be const whenever you don't intend to change them. If you don't know these things it hints that you would improve your C++ skills a huge amount by reading the Effective C++ books by Scott Meyers (and if you want to improve some more the Exceptional C++ books by Herb Sutter).
Cool article, thanks for reminding I really need to be using template classes more! I guess one can never solve all the corner cases, maybe a pretty heavy test suite provided in the spec would help. Also, happy Reddit birthday! :)
I didn't think anyone still used Borland C++. I did not check the standard but my money is on GCC being correct ([Comeau C++ online](http://www.comeaucomputing.com/tryitout/) agrees with the GCC begaviour, and I suspect MSVC would also). If you want to overload a method of a class level template you should do it, like you are supposed too, by template specialization. Like so: template&lt;class T&gt; struct Bar { Bar(T val); Bar(int val); //overriding like this is bad }; Bar&lt;int&gt; b(0); // Error. template&lt;class T&gt; struct Foo { Foo(T val); }; template&lt;&gt; struct Foo&lt;int&gt; { Foo(int val); //overriding like this is good }; Foo&lt;int&gt; f(0); // All is good 
It's nice to see the underrated expression 'my code sucks donkey balls' used on a corporate blog.
if you want to lessen confusion i strongly sduggest you put const to the right of whatever you are making const. This rule works in all cases whereas the popular convention of putting const first is really inappropriate.
straight 'c' can supplement other higher level languages but c++ can be used as both. Regardless I would suggest using a c++ compiler even with straight 'c'. The extra strictness makes it easier. As usual, always use -Wall (or its equivalent).
For the record My Opera is a public blog service, and in all likelihood he's not actually working for Opera.
I can find only one fault in what was otherwise the best comment I've seen so far in this thread: &gt; Simple built in types (like int or float) you should pass in as non-const value (as copying them is cheap and the caller does not care if the copies are modified or not). I would disagree with this for two reasons: 1) The OP was complaining about problems debugging. Knowing for a fact that an argument to the function he's in is immutable is one less headache he has to worry about. Granted, there's no performance benefit for taking an int&amp; over an int, but for const int vs int the impossibility of programmer mistakes (for example, accidentally ++'ing the wrong variable) is a worthwhile timesaver. 2) For normal functions, you're right. For function templates, where your current uses of the function are all passing in POD types but future uses (or uses by someone else on your team) are a mystery, it pays off not to make any assumptions about which types will be passed in and take const&amp; template arguments wherever possible. For the POD types it's harmless, for complex types it will save you the trouble of having to make an unnecessary copy (or having to specialize on the types to avoid the copy).
On number 2) Fair enough. In generic programming it is often cleaner to just do the same thing for all types (built in or not). The reason that built in types are usually not passed by const reference is that it is usually more expensive than by value. A reference is probably going to be the same size as a pointer. A lot of built in types is smaller than that. A reference like a pointer is also probably going to have the same cost of one indirection as a pointer (I think). On number 1) I think you are missing one crucial point. The fact that a parameter is passed by value means that the function is working with a copy, not the original value. The caller does not care if the copy is const or not, as the original value will not get changed either way. It only makes a difference to the callee. And that is a type of implementation detail that should not really be advertised/exposed in the interface. I guess the biggest reason though is that there is a potential performance penalty. If every argument passed by value was passed as const, then the callee would have to create another copy of the parameter if he wanted to update it. Its an easier rule of thumb, to say that you should either pass by non-const value, or by const reference. I didn't make these guidlines up.. most of the "gurus" say the same. I think they are in the "C++ Coding Standards: 101 Rules, Guidelines, and Best Practices" book. 
Yeah, it's a personal blog (nothing to do with opera) I have where I let myself say pretty much anything under a pseudonym. It lets me look at issues, even if I get it wrong the first time, where I can learn and find out more that otherwise would be left to ridicule. 
 template&lt;class T&gt; struct Bar { Bar(T val); Bar(int val); //overriding like this is bad }; What's the official rule on this? If T is int, is there a collision? Borland treats Bar(T val); as a template method. So it'll pick Bar(int val);. But gcc complains. With one argument, it's not an issue (the body is the same). It's when there are two arguments and one of them is T and where one of the methods is a template method. Here's what I really wanted. IndexedSkipList(size_type count, const T&amp; val); template&lt;class InIt&gt; IndexedSkipList(InIt first, InIt last); These are two different methods with different bodies. Since there are templates in both cases, Borland picks the second one when you use IndexedSkipList(unsigned int,unsigned int); So I added a method that had explicit exact arguments. The specs are clear that exact methods is to be used. But once you specify the class template parameter as unsigned int, is this method also considered an exact match? IndexedSkipList(size_type count, const T&amp; val); Borland doesn't. gcc does treat it as an exact match (and causes a collision with the other explicit method). I agree that gcc is likely right. If so, then I'll just put a conditional compilation section for specific Borland compilers. I like this as well, template&lt;&gt; struct Foo&lt;int&gt; { Foo(int val); //overriding like this is good }; Too bad it's unnecessary for gcc and creates more problems with Borland because some of its compilers have problems with iterator_traits of nested classes inside partially specialized template classes. I'll just use conditional compilation on the older Borland compilers that have these problems. Thanks for the response. I wish I could upvote you more. Very helpful!!! BTW, for anyone that wants to know, it's not just Borland. There are lots of C++ compilers out there that have problems with templates. Borland is actually one of the better ones and the new company that owns the compiler has fixed all of these issues. Older compilers are still out there unfortunately. edit: I will add Comeau C++ to my tests. I've already applied to a gcc compile farm. Any other C++ compilers I should test on? This is for my skiplist container library. edit2: Comeau C++ isn't free. I'll perhaps add it later if need be. 
Strike 'corporate', but the rest still stands :)
All right, non-corporate. I nevertheless applaud your choice of phrase, even if not applicable since your code didn't. I'm also partial to 'sucks golf balls through garden hoses'.
&gt; IndexedSkipList(size_type count, const T&amp; val); template&lt;class InIt&gt; IndexedSkipList(InIt first, InIt last); These are two different methods with different bodies. Since there are templates in both cases, Borland picks the second one when you use IndexedSkipList(unsigned int,unsigned int); That Borland picks up the second one sounds wrong to me. The first one is declared with a class level template type so should take priority. The second one is "only" declared based on a function template type. It sound a little bit like you want to have your cake and eat it. For the same signature (in the case of unsigned int) to declare different functions. IndexedSkipList(unsigned int count, T val); IndexedSkipList(InIt first, InIt last); When T is not unsigned int, the second function will be used when the two arguments are the same type (because the first function that has priority does not match). But when T is unsigned int, the first function that has priority will be used because it matches. There is no way to make it not match except for removing it. It sounds from "InIt first" and "InIt last" arguments names that they are used as somekind of iterator. If you made InIt somekind of templated iterator type you could distinguish between the two functions even when type is unsigned int. Something like: template&lt;class T&gt; class InIter&lt;T&gt; { typedef T Type; }; template&lt;class T&gt; class IndexedSkipList { IndexedSkipList(size_type count, T val); IndexedSkipList(InIter&lt;T&gt; first, InIter&lt;T&gt; last); }; Then again I'm not sure what exactly you want to do and im no expert. If you want advice you should consider asking the C++ gurus and language lawyers on http://groups.google.com/group/comp.lang.c++.moderated. Be prepared for them asking exactly what you want to accomplish. They can be a bit grumpy. Good luck. Edit: I find it useful to test snippets of code on the free Comeau online C++ compiler page. It seems to be very standard compliant. 
&gt; That Borland picks up the second one sounds wrong to me. Yes, now that I look at it, I have to agree. Once the class template argument is specified, the method with T as an argument type cannot take on any other value. So it is an explicit method definition at that point. &gt; When T is not unsigned int, the second function will be used when the two arguments are the same type (because the first function that has priority does not match). Yes, but this method is only supposed to be used with input iterators. &gt; But when T is unsigned int, the first function that has priority will be used because it matches. Correct. It will create 'count' elements with a value of 'val' in the container. What you describe is EXACTLY what I want. &gt; But when T is unsigned int, the first function that has priority will be used because it matches. I believe gcc is correct as it does what you say and the above two constructors are all I need. It's again Borland that messes things up by choosing the wrong method. Borland incorrectly chooses the second one when T is unsigned int. Consider this problem solved. I will use the two constructors as I had originally, and remove IndexedSkipList(unsigned int, unsigned int). For Borland, I will use conditional compilation where needed. &gt; Edit: I find it useful to test snippets of code on the free Comeau online C++ compiler page. It seems to be very standard compliant. Good idea. I will use both gcc and the online Comeau C++ page. If they both agree, I'll go with that. 
&gt; On number 1) I think you are missing one crucial point. The fact that a parameter is passed by value means that the function is working with a copy, not the original value. His point isn't about the caller but the function itself. When your debugging having as few mutable objects as possible means you have less that you need to keep track of in your head.
Thanks that helps! Dumb question, is it similar to passing by const reference in regards to classes? I'm trying to master how to actually think in c++.
Oh, the joys of C++ template programming.
The declaration: void foo( const std::string &amp; bar ); and void foo( std::string const &amp; bar ); are equivalent. I believe that **bnolsen** is recommending you prefer the second one, I recommend using the second one as well. My reasoning is based on the [right-left rule](http://www.codeproject.com/KB/cpp/complex_declarations.aspx#right_left_rule) of interpreting C++ declarations. Note that the author of that piece recommends putting the **const** before the typename, which is the opposite of my preference. Reading the first aloud, from right to left, you **const std::string &amp; bar** becomes "**bar is a reference to a std::string constant**". The second is read as "**bar is a reference to a constant std::string**". Also note that it's more complicated with pointers. You must distinguish between whether the **pointer** is const or the **thing-pointed-to** is const. The following are **not** equivalent: int const * baz; vs. int * const qux; The first is "**foo is a pointer to a constant int**", the latter is "**foo is a constant pointer to an int**". baz = NULL; //legal *qux = 0; //legal qux = NULL; //not allowed *baz = 0; //not allowed
If I remember correctly, foo(const int) emits the same signature as foo(int). In other words, you could have foo(int) in your header file, and foo(const int) in your .CPP file, and everything would compile and link just fine. This is because from the caller's perspective, there's no difference in how one actually calls the function. The only difference is in the function definition.
VisualAssist supports C++/CLI apparently.
Can you use the debugger without starting the IDE? Or do you mean CDB/WinDbg?
How about allowing access between the containers and the iterators using template public base classes that are friends to each other?
In this specific case, I think Borland is incorrect to choose the second overload when constructing with IndexedSkipList(unsigned int, unsigned int). But keep in mind the very similar: IndexedSkipList(int, int), which will select the second overload, even though you want the semantics of the first overload. See http://gcc.gnu.org/onlinedocs/libstdc++/ext/lwg-defects.html#438 for a description of the problem and solutions. 
gcc will select the first actually if T is int. When you use IndexedSkipList(10,1);, gcc will use IndexedSkipList(size_type,const T&amp;); which will end up being IndexedSkipList(unsigned int, int); depending on size_type, but the first argument will be unsigned. Borland OTOH will use the second. Anyhow, I've used conditional compilation and it works great. I just need to know what versions of Borland act incorrectly and add them as special cases as I get bug reports. I'm going to test on as many compilers as I can. 
When you pass by value, you can only modify the copy on the stack and not the original. So const offers little in such a case. But there is no need to copy the object. You can pass it by reference. Now the original CAN be modified. So you put const in front to prevent any changes from happening. IOW, you have an equivalent situation to pass by value without having to copy the object. What this also implies is that you can only call methods on your node that are themselves const to ensure that no changes are made to the node. The above can cause compilation headaches (when you want to invoke a method that is not const). Not only that, but if you use the object again in another function external to the class, the argument must be defined as const as well unless it is passed by value. All in all, your choice of methods and functions are drastically reduced when using const arguments. This can be frustrating when starting to use const. Const has another feature of temporary creation. If you perform an operation using operators on multiple objects like n.func(a+b); then a temporary object holding the result of a+b will be created and passed as the argument. This is only allowed when the parameter is defined as const. Makes sense since const arguments cannot be modified. When I was learning how to use const in my own classes, the biggest problem I had was defining things as const that I shouldn't. I now pretty much restrict const to a few cases only. 1) Copy constructors [const argument], 2) const methods that only return state information (these can have a separate non-const version for returning the value by reference to allow updating said value), 3) operator overloading [const method] and 4) const arguments when you really, really want to avoid making a copy and passing by value just won't do. #3 almost always has a const argument. It is itself a const method unless it is a +=, -=, *=, etc operator. Also remember that const methods are methods that you can call on a const argument. That's how const methods and const arguments are connected. 
I mean the debugger in the IDE, but that's all I normally use the IDE for. I call the IDE app from the command line when I want to use it, so I have the same environment variables/path stuff set. Then I just load the executable I want as a one-off "VC solution (sln)". If I worked with the same executable before, then it is already there in the menu of recent projects. 
Or use boost::signals2? 
Not lightweight, in fact very very heavyweight.
I use boost::signals as well, I'm actually using a bunch of boost libs in my project, to me it's worth whatever weight it may have.
I think the benefits of using boost out-weigh the "heavy-ness". I'd be interested to know why someone would choose this lightweight implementation over boost (other than you've been banned from using boost, for some weird reason).
Because in small projects, having Boost as a dependency is a huge deterrent in its use by other people.
boost::function all the way (actually, std::tr1::function all the way) for me.
Boost is huge, but it is very highly compartmentalized. Most of it is header-only. You only pay for what you use. 
And std::tr1::bind. Oh it's so delicious. A bit annoying that you can't directly bind lambdas, but it's whatever. I don't see why people get all bent out of shape about something that's not "lightweight". A callback system like the one I implemented and use is really not that hard nor expensive. A std::map or hash_map of a key type template associating a list of std::function callbacks. Simple, usable, not as heavy as boost::signal, not as lightweight as this.
True in theory, but in practice those header dependencies are quite hairy. I am convinced that including boost::function will include a ton of other stuff.
In the spirit of discussion regarding C++ callback implementations, I've included the complete implementation of a generic callback dispatcher (aka, events) that I wrote and use in my latest personal project. As a disclaimer, the naming is a bit exotic so I'd imagine that it will be anathema to a particular number of people. It's just to fit the feel of project I'm currently working on. Anyways, with that out of the way. The goal of this implementation is to be simple, easily extensible, and also convenient to use. I derive from a policy template in order to provide a base event type, as I plan on using a custom reflection system later that will support its own any-type. The idea is that it would provide immediate and delayed dispatch, and also be easily expanded to a concurrent architecture (which would be done through tbb::concurrent_queue, for instance, instead of a std::list of events). The nicest thing about it, as far as my taste goes, is that it automatically generates the up-cast for you. For instance, if you have an event type that has several derivatives, and you want to cast to a particular derived type, the system will create a simple lambda to cast it up to the requested type. Example: std::function&lt;void (Causal::Events::FileDrop const &amp;)&gt; fdf = std::bind(&amp;DrawTest::LoadMeshOnFileDrop, this, _1); Fabric::Is()-&gt;Causality-&gt;Relate("file-drop", fdf); // ... void DrawTest::LoadMeshOnFileDrop(Causal::Events::FileDrop const &amp; fd) { //... } or another use CausalRelations_ += Causality-&gt;Relate("creation-make", std::bind(&amp;Harmony::Make, Harmonics)); As it is, there's what I use. Hopefully there's nothing horribly broken with it. I've done a moderate amount of testing, but I could easily have forgotten something. Enjoy.
Could you please post the exact error message? I'm not familiar with OCCI, but the real message can be useful anyway.
 Undefined First Referenced Symbol In Line typeinfo for oracle::occi::SQLException/var/tmp//cc0H7Fxr.o ld: fatal: symbol referencing errors. No output written to /my/path/to/file.o collect2: ld returned 1 exit status
There is also [the list of issues fixed in GCC 4.5.2](http://gcc.gnu.org/bugzilla/buglist.cgi?bug_status=RESOLVED&amp;resolution=FIXED&amp;target_milestone=4.5.2).
Lacks copyright and license statements. If you are releasing this in the public domain, you still need to state that clearly. Otherwise the code can't be used anywhere other than for contemplation.
Learn C# or Java if you want to get in with most employers. Learn C# if you want to get in with most employers and you don't want to learn a terrible, dying language.
No typeinfo for the class? Are you sure RTTI is enabled? Also since it's C++ and not C, you'll have to make sure that the GCC version used to build OCCI is ABI compatible with the version you're using. Oracle almost certainly tells you this somewhere in the docs or the download page. PS: If all else fails, use ODBC.
Looks like RTTI wasn't enabled when compiling that object file
I'll go ahead and fix that, thanks for letting me know. Just ripped out the header comment which has personal information, etc etc, as well as the copyright stuff.
An even better approach is to make method calls asynchronous, i.e. each method call is put in a queue, and the object's thread simply extracts method calls from the queue and executes them. 
If anyone else sees your code please keep it's members private. I've seen more than one project where someone (or multiple someones) insist that members can be manipulated directly from the outside (with "that's not a class-it's a struct" or "it's a good idea just this once" kind of arguments) and within a year or two you get to the point where some of a class'es logic and/or validation is all over the place. This teaches newcomers to the project that it's OK to do so ("yeah, but see? The container does the same"), and the codebase becomes large with changes like that. It's a slippery slope to bad design and "fixes to the patches of the workarounds" type of code; It is also the reason why the "data is private" rule was put in the first place. TL;DR: The "just this once it's OK to have public data" argument fails in the face of people who see the code but didn't hear the "just this once" argument. **Edit**: As a possible answer to your question, have you considered callbacks? template&lt;omitted stuff&gt; class container { // to be called from iterator constructor void AddIndexChangedCallback(/*callback pointer and other data*/); // to be called from iterator destructor void RemoveIndexChangedCallback(/*callback pointer and other data*/); }; Iterators should implement the callback. This way, even if Joe Coder decides to use your callback to see weird things into your container, you don't care and your data integrity is still OK.
The two are not exclusive. You can apply your idea on top of his class (I've seen this done and done it a few times myself).
Indeed. I've used this approach in a heavily threaded complex real time app and it worked like a charm. Generally, the no 1 rule in threading is 'do not share data, unless you really have to'. So, I made all my thread-related data Active Objects (the name of this pattern is Active Object, I believe), and forgot I ever had threads.
Troll? Let's consider that you are not, and answer. &gt; why am I subscribed to this subreddit you might ask So, why are you subscribed to this subreddit? &gt; I've always felt that C is a better language than C++ Better for what? There are areas where C may be more suited and areas where C++ may be more suited. &gt; that you if you are forced to use a C++ environment, just program in C. You mean that if you are forced you're not actually forced? Methinks you don't actually know enough C++ to know where it offers advantages over C to make blanked statements like that. &gt; Maybe its because I had a heavy dose of the horrors of C++ when it was taught to me: I had to build a BigNum implementation that used operators, templates and friends. What the teacher did not explain to us is how to deal with the ridiculous compiler errors that g++ produced. Sounds like you're rejecting (hating?) C++ because you had a bad teacher. &gt; ISO C++ forbids declaration of 'operator' with no type HAHAHA! It means that if you declare an operator, the declaration requires that you specify the return type. To put it in other words, the C++ language (aka ISO C++) forbids a declaration of an operator (aka doesn't accept it as valid) if you don't specify a return type. That doesn't sound like a C++ failure, but like an English comprehension failure (sorry). Is English your native language? If not, were you speaking it well while learning C++? By the way, this behavior is not true in C (maybe it's the same in C99 though - haven't checked) where if you don't specify the type of something, "int" is assumed. &gt; I was one of the few in my class who succeeded. Yeah ... bad teacher. &gt; I would like C++ a lot better if compilers could accept definitions in any order Not sure what that means. &gt; or if they had reasonable and sensible error messages that's true - and one of my pet peeves with C++ :( &gt; or that the language specification was clear enough to be able to know why something is failing that's also true. &gt; Also, when I asked on IRC, what does "ISO C++ forbids declaration of 'operator' with no type" mean, people said "exactly what it sounds like". My point exactly.
Can you elaborate? Heavily threaded *real time*? I have a hard time getting real-time programmers not to write their own scheduler (when they have an RTOS), much less use active objects.
I actually advocate against the use of private whenever possible. It's shoddy programming. Yes, this goes against conventional wisdom. But my experience has taught me that private causes too much harm. Too often, you get a third party package that has a bug and you have to dump it because it breaks your software and you can't update the state of your third party objects. Sometimes, it's just trying to maintain the correct state of your objects while performing an action and it can't be done. Again, the library gets ditched and so does countless hours of work. Now, this may sound somewhat ironic since I'm actually trying to find a way to make the members of my iterators private. The reason here is that the code is free and the iterators should never be derived from. Also, since I want other people to use this library, I want to conform to convention even if I disagree with its premise. If you're relying on the protection mechanism for how programmers use your code, then you need to rethink your strategy. There are times I read docs and it says, "DO NOT USE THESE METHODS!!! They are for internal use only and will change in the future.", I don't use those methods. This actually works. Most times, I don't even know about them unless I look at the header file. The docs don't mention them at all. With OSS, you can recompile anything you like anyways (where they can change private to public). Callbacks are ok. But C++ doesn't support closures yet (unlike Borland). And in Borland and MSVC++, you have real properties. These are made to remove the encapsulation flaws in standard C++ that comes with members. When you have properties as part of the language, you can make members public as part of the API just like you make methods public without any downside at all. If you want to change what happens behind the scenes, you can still change the getter and setter code. Private isn't what it used to be and neither is public. There are new and better ways to program in C++. The next standard will go a long way to making this easier and you'll see what I'm saying here come to light. Unfortunately, Stroustrup is still against properties and is holding C++ back. edit: Just in case people want to see it, I released my library here. http://csskiplist.sourceforge.net/ 
What do you mean by this? Personally I've been a fan of just writing my methods to be single-threaded, and if I want to call them asynchronously I use a Future which runs that method in a thread/task. Something like: Future&lt;int&gt; result = AsyncCall(&amp;Class::Method, object, p1, p2, ...); And then I can wait for result to complete, or have it invoke a callback when it's done. You can still have synchronization in the class with a mutex, but the class itself doesn't create any threads or handle any scheduling.
Not bad. My solution goes like this: class Actor { public: void putMessage(Method, params); void doSomething(args) { put(&amp;_doSomething, args); } private: void _doSomething(args); }; All public method calls put the relevant method into a queue, along with the arguments...and then the actor's thread pops a method from the queue and executes it. 
Soft real time, not hard real time. 
http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list/4476094
If you want to learn C read [K&amp;R](http://en.wikipedia.org/wiki/The_C_Programming_Language_\(book\)). Don't worry about up to date, this is the best book to read to learn the language. C99 only changed a handful of things and they don't really matter for a beginner. For C++ there I haven't read a good introductory book that I'd be comfortable recommending to people. It looks like somebody has already posted a link to a list of books on stackoverflow that may be useful. I'd highly recommend reading Effective C++ once you start to get the hang of things though.
if you know the basics of programming (what's a function, what is Object orient, what are classes, pointer, strings, ints...) then you might start by going to http://cplusplus.com/doc/tutorial/ Then pick up Effective C++
Some awesome work has gone into the [standard template library](http://en.wikipedia.org/wiki/Standard_Template_Library), which is provided with most c++ implementations. That is where you will find things like vectors and maps (similar to Java's ArrayList and HashMap).
C and C++ are two different languages, and should be treated as such. For C, K&amp;R's "The C Programming Language" is a good reference.
and don't forget effective stl as well, or anything by meyers
While I doubt that Mr (or Mrs) Eeeeaaii intended something so insightful, the number of implementations can have an impact. It's called branch target prediction. If your virtual calls dispatch to different implementations, it can take measurably longer to dispatch the calls as the processor is unable to pre-fetch instructions effectively, compared to the use case where there is only one implementation of the class (or compared to the case where most of the calls dispatch to one class). That said, it's still darned fast. We're talking a few 10's of nanoseconds on modern processors in the worst cases. EDIT: eh, i take it back. 2.6 ns for virtual dispatch seems suspect, too fast. Even the latest i7's take 2ns longer for a predicted virtual call over an inlinable calls. There'd be a measurable difference. I'm guessing the JIT engine did the analysis and determined it could statically dispatch the calls. The author needs a more realistic scenario -- eg, a large array with instances of more than one class (eg, 999 instances of class A, 1 of class B) to force the JIT to use dynamic dispatch.
That is C# not C++. Please post in the correct categories. 
Bjarne Stroustrups new C++ book (came out in 2009) is directed at beginners and is very good, it covers the relevant parts of C++ as well as how to write programs which contain error handling e.g. This book is also easy to read (dare I say fun). http://www.stroustrup.com/Programming/
First figure out if you want to learn C or C++. I would go with C for the basics but C++ will be more familiar with your java background.
`std::map` is nothing like `java.util.HashMap`: one is a tree and the other is a hash.
My intent was to direct the OP to a few of the replacements for his/her handy Java toolbox, something that programmers learning a new language always find useful. I've no experience with std::hash_map so I'm unable to recommend it.
Actually, it will be std::unordered_map&lt;&gt; (that will come with the next standard)
read http://www.parashift.com/c++-faq-lite/
Forget dates, just worry about quality. As others have said, C and C++ are two different languages, but there are high-quality starting places for both: C (this is canonical): K&amp;R 2e. C++: Koenig / Moo _Accelerated C++_.
http://abstrusegoose.com/249
For C: K&amp;R's The C Programming Language. Still as relevant today as it was 20 years ago. For C++: C++ Primer by Lippman is a good place to start. I keep a copy of Stroustrup's The C++ Language (K&amp;R for C++) handy. More advanced topics which show off some of C++'s power (and danger) can be found in Alexandresciu's Modern C++ Design. That's where you go if you really want to learn to use templates correctly.
Hi, I was facing a similar problem. This implementation has opened my eyes about the RelationShip object. I didnt use an extra object do clarify the relationship which made my code barely readable / extendable. There is a problem I'm facing with this (also in my own implementation). Suppose you have an event fired. Now the receiver of that event wants to add some data to it which is specific to the receiver type. In your case this is done better already because the event type caught is dependent on what kind of event is thrown, which is alot better then having a single event type for all kind of events. But the type of event created and fired is still just dependent on the thrower, not on the receiver. Would there be a way to have some receiver specific data in the event, which can be read by other instances of receivers of the same type? I ran into this problem in the next case: There are alot of client apps that watch the state of a single phone line. Those apps are connected to a server which notifies them about the state of that line. Now an event occurs like, incoming call. An incoming call event is fired. The first receiver processes it, creates an XML object to send to its app. Now the second one receives it processes it and needs to create the exact same XML object. This is done alot of times. It would be alot quicker to resend the same XML object to the other apps, instead of recreating it over and over again. So in my case, this generated XML object needs to be stored somehow and travel along with the event. Receivers other then for this app dont need to know about this XML object, not even its type. So the sender shouldn't know either. The only thing I can think of is like how boost::exeptions carry some extra data to some catch clauses. But is that a good design in this case? Hmm... lots of text. When you got here, thnx for reading! Greets Johan
If this is the kind of thing you want to do, you're better off using Boost.Bind.
STL defines mem_fun[_ref] for member functions that take 0 or 1 arguments. But not 2 or more arguments. see http://stdcxx.apache.org/doc/stdlibref/mem-fun.html For more, you need to use boost::bind (needs heap apparently :-/), or C++ 0x lambdas :P
I don't believe it's possible with the current STL's bind options, but it is possible with Boost::Bind. I try to avoid the STL bind1st/bind2nd whenever possible, as Boost::Bind is just *so* much better. Quick example: #include &lt;iostream&gt; #include &lt;algorithm&gt; #include &lt;list&gt; #include "boost/bind.hpp" class X { public: X(int v) : val(v) {}; void func(int b) { std::cout &lt;&lt; val &lt;&lt; " * " &lt;&lt; b &lt;&lt; " = " &lt;&lt; val * b &lt;&lt; std::endl; }; int val; }; int main(int argc, const char* argv[]) { std::list&lt;X&gt; x; x.push_back(1); x.push_back(2); x.push_back(3); std::for_each(x.begin(), x.end(), boost::bind(&amp;X::func, _1, 2)); return 0; } 
bind() is in TR1 so your compiler may already include it.
Indeed. For gcc, `#include &lt;tr1/functional&gt;`. For Visual Studio, `#include &lt;functional&gt;`
Primarily, what I'd do to make this flexible is use something quite like boost::bind and bind in extra data, like a shared pointer. Also, if you could modify the base event class, you could easily add a null XML pointer, which gets constructed by the first receiver. Each receiver would then later construct that, though I don't really like either of these, but they're doable. If you must not modify the base class or have bind listeners at the same point, perhaps you could have a shared table keyed by pointer-address or some unique ID for the event, that each receiver checks. If the event isn't yet inserted, create the needed information and store it in the table. Another idea, would be to hook different event keys. For instance, an event is generated by your library, then a newer event is created based on this earlier one with the extra information, which is then fired again. The receivers that require the new data get it, along with a packaged event with the old data. The listeners that just need the old-style event, could simply hook that original key and ignore the new one. It would seem that your case is currently latent tolerant, so this perhaps might be an issue. Does this give you some ideas on the solution?
[http://blog.emptycrate.com](http://blog.emptycrate.com) has some nice C++ articles
Lambda the Ultimate is one of your favorite c++ blogs??? The guys over LtU hate C++!!! (see thread 'why do they still use C++?' for more details).
You're right about that... but it's good (IMO) to get a smattering of viewpoints. Personally I love C++ (yes, I escaped from a mental institution) but I find it helpful to understand others' criticisms of / issues with the language.
It's nice to know that I have good rss channels ;)
You mean you have the same ones, or one of them is actually yours?
pretty much the same. Lately DrDobbs is a bit dry (C++ wise).
Yes, not much on there about C++ anymore. The exception is Herb Sutter's brilliant Effective Concurrency articles.
The one I check manually is: http://www2.research.att.com/~bs/papers.html and http://www2.research.att.com/~bs/interviews.html Anyone know how to get notified when a webpage updates? Google got something for that, ammirite? 
It's been waiting in my reader for too long :(
Their criticism is biased that to the point of being useless. 
I've very few to add: Jim Beveridge: http://qualapps.blogspot.com/feeds/posts/default?alt=rss Ofek's VC++ stuff (Ofek Shilon): http://thetweaker.wordpress.com/feed/ Harder, Better, Faster (Steven Pigeon): http://hbfs.wordpress.com/feed/ 
Wow, I've been missing out on a lot! Thanks! (I have nothing to add :( )
Thanks, the Ofek blog looks promising.
This indeed gives me some ideas, especially the last one I think. Repost the event with the extra data. I'll consider it. Thanx!
I wanna add C++ Soup! - http://cplusplus-soup.com/
A Sense of Design (http://codesynthesis.com/~boris/blog) has quite a few articles on C++.
http://wordaligned.org/ has some good stuff, too.
I guess its time to seriously visit co-routines. I use threading very heavily for some HPC and get amazing scalability using basic thread pools &amp; good design but making the threading easier for use to guarantee correctness is definitely something worth exploring.
I've seen it argued that therefore C is a better place to start, it'll be more of a shock, and will teach more of /the difference/, at once. 
Why aren't those bozos in the habit of including your nice, relevant query in their release notes? Instead their 'changes' links, even between last-dot versions, all go to the generic GCC 4.5 features page. ^%$^@#%$
I don't have enough points, but someone really needs to ask Konrad Rudolph why he randomly italicizes words. I mean to ridicious levels. Maybe some of those were added for emphasis, but this guy used more italics in a single post then I've seen in the last year.
Could someone elaborate on something I once read. I forget the exact details, but it was something like method implementations in the header files (in the class definition) were treated as if they had the inline keyword. So it's not so much that you're using templates, but that the code is all in the header file. Is this wrong? Anyone have info on this? 
&gt;When I'm going to performance, the first and last choice is C. *Sigh*. The average [difference is less than 0.03%.](http://shootout.alioth.debian.org/u64/benchmark.php?test=all&amp;lang=all&amp;lang2=gpp) &gt;When "performance" isn't the main issue, programming languages like perl and python would be good choices. Perl is a good choice if you are processing text files. Python is a good choice if your shared code base has thousands of developers and you need all of the code to be consistent. &gt;Almost all of open source applications I know in this area has been written in C, perl and python, bash script, awk and even php, but no one goes to use C++. Then you don't know many open source applications. &gt;I'm not discussing about some other areas like GUI or Web application, I'm just talking about Linux and about CLI and daemons. Linus's argument is valid but it's only to do with the kernel, not application development. With kernel code, you need to be able to look at a few lines of source out of context, say: y = m.P; a = y + b * x; and you need to know within reason what the machine language output will be. With C, you can do this. With C++, the statement 'a = b;' could do all kinds of things under the hood. Linus's argument is a valid reason for kernel development, but not much else. The whole point is, the best language depends on the problem at hand. There is no one language that works best. 
I don't have enough points to vote on replies there, but the top-voted one has it pretty-well covered I think.
It's quite possible. I've noticed that in some compilers if a simple method is implemented in the header (in the class def) it doesn't need an export out of the DLL/shared library. That would seem to imply that other libs have full access to it and aren't linking against an externally-defined function. Which makes sense if you think about it. What object file should the methods be added to if they aren't inlined? You can't put them in every object file that includes the header or else you'd get multiply defined symbols when linking them together. I'm not sure if this is always done or only for easy-to-inline methods, though. I tend to avoid putting anything but declarations in header files on principle.
1. Implementing a function inside the class definition is an implicit `inline` 2. All `inline`s (explicit or implicit) are just hints to compiler, and compiler has the right to avoid inlining any of them at will
I've run into this a couple of times. My usual hacky solution is to define operator&lt; on the value type to only compare on the key field(s), and do lookups by constructing a bogus value object with only the key field(s) filled out.
It seems like defining any comparison operator that's not consistent with operator== is an incredibly dangerous idea.
*Your* post needs *more* italics. It's *too* boring.
Absolutely correct. The compiler even has the right *to* inline a function that isn't declared `inline`. The as-if rule is extraordinarily permissive. Also: TIL that reddit supports `inline monospace text`.
[Boost.MultiIndex](http://www.boost.org/doc/libs/1\_45\_0/libs/multi\_index/doc/tutorial/key\_extraction.html) ETA: Clarification #include &lt;boost/multi_index_container.hpp&gt; #include &lt;boost/multi_index/composite_key.hpp&gt; #include &lt;boost/multi_index/ordered_index.hpp&gt; #include &lt;boost/multi_index/member.hpp&gt; struct Pos { int ID; double x, y; }; using namespace boost::multi_index; //declare our container-type typedef multi_index_container&lt; Pos, indexed_by&lt; ordered_unique&lt; member&lt;Pos,int,&amp;Pos::ID&gt; &gt; &gt; &gt; Pos_set; int main() { Pos_set s; Pos p[] = { { 1, 1.0, 2.0 }, { 2, 2.0, 3.0 }, }; s.insert( p[0] ); s.insert( p[1] ); assert( s.find( 1 ) != s.end() ); //lookup by id assert( s.find( 3 ) == s.end() ); //ditto, but for not-found } 
Use a std::set and define the &lt; operator on your own. The only reason there's a map is to make a clear way to join values together. The std::map&lt;&gt;::value_type is just std::pair&lt;key_type,data_type&gt;.
As was mentioned above and as madmoose mentioned, you'd need to build a bogus object every time you want to search for a key. Do you consider this an acceptable tradeoff? Also, the &lt; operator may not be accurate as it may only be correct as far as the key is concerned and not the entire object. A custom comparison predicate gets around this problem though. 
If you don't want to duplicate the key... then you have to accept a mutable key inside a const framework. If you are okay with duplicating the key, then you can offer "just the key" and nothing else. Of course... I don't see why you would want to offer "only the key" to search for if you DO need an entire object anyway. The key makes the object unique and therefore if it IS a key.. you don't need more. The operator &lt; is all you need in that case. If you want to use more than the &lt; operator on a non-duplicated key then its a matter for needing a whole 'bogus' object anyway. Remember that containers must have explicit memory bounds and cannot derived and larger types. If so.. your "bogus object" must have all the relevant parts anyway to even do a search.
&gt; There is no one language that works best. But on the other hand, there are *many* that you should probably never ever use.
&gt; If you don't want to duplicate the key... then you have to accept a mutable key inside a const framework. That issue happens with sets anyhow. &gt; Of course... I don't see why you would want to offer "only the key" to search for if you DO need an entire object anyway. I only need the key to search. But I need the entire object upon completion of the search. Take the Pos object I've described above. I only need the ID to search. But I want the whole object upon completion. If I use a set, I need to construct a bogus object where most of it is ignored by the search algorithm. Currently, each of the two ways to go about this have drawbacks. I'm wondering if people are ok with said drawbacks and if they would use a container that allowed nested keys. 
The setup is brutal, but the usage is fantastic. 
You could do something like the following (you avoid making a copy of the key, but you pay the price of the reference wrapper--so it really depends on the size and complexity of the object you're constructing; e.g. for a simple int this probably makes no sense _not_ to duplicate the key): #include &lt;map&gt; #include &lt;boost/ref.hpp&gt; #include &lt;cassert&gt; struct Pos { int ID; double x, y; }; int main() { std::map&lt;boost::reference_wrapper&lt;int&gt;,Pos&gt; posmap; Pos p1 = { 1, 2.0, 3.0 }; Pos p2 = { 2, 4.0, 5.0 }; Pos p3 = { 3, 6.0, 7.0 }; Pos p2_2 = { 2, 99.0, 87.0 }; posmap[boost::ref(p1.ID)] = p1; posmap[boost::ref(p3.ID)] = p3; posmap[boost::ref(p2.ID)] = p2; posmap[boost::ref(p2_2.ID)] = p2_2; assert(posmap.size()==3); // not 4! assert(posmap.find(boost::ref(p1.ID)) != posmap.end()); assert(posmap.find(boost::ref(p2.ID)) != posmap.end()); assert(abs(posmap.find(boost::ref(p2.ID))-&gt;second.x - p2.x) &gt; 94.9 ); } 
&gt; You can't put them in every object file that includes the header or else you'd get multiply defined symbols when linking them together. Consider two compilation units that independently define and use a `std::vector&lt;int&gt;`. Not only is the linker capable of linking in the presence of multiply defined symbols, it *must* do so.
If I wanted to avoid incredibly dangerous things, I'd hardly be using C++ now would I?
Typically, what I do in this case is maintain a sorted std::vector&lt;T&gt;. use std::sort with a functor that knows how to compare by your member function, and then use a functor (typically the same functor, just overloaded) that knows how to compare T with a key value to std::lower_bound, std::upper_bound and/or std::equal_range.
Oh, I agree. MultiIndex is one of the few boost libraries that *always* sends me back to the examples &amp; tutorial. If `std::set`'s `find()` was defined as: template&lt;typename T&gt; iterator find( T const &amp; t ); That is, if it could accept any type that was comparable-to its `value_type`, then it could support that usage too. (Obviously this would apply to `equal_range`, `lower_bound`, and `upper_bound` too.) The `set` and `map` interfaces could use a refresh: I'd also like to see the `template`d equivalent of: `mapped_type const &amp; get( key_type const &amp;, mapped_type const &amp; default )` and `optional&lt;mapped_type get_optional( key_type const &amp; )` Some of the boost containers (I'm thinking of MultiIndex and `property_tree::basic_ptree`) support these, but it would be good to have them standard.
I should also note, if you're inserting into an already sorted container, you can use lower_bound to find the optimal insertion point. Admittedly, this approach is more finely tuned where you've a small dataset and few inserts, but frequent lookups (which is often my case). even for a marginally large dataset, if you can do the sorting up front and make infrequent updates, this pattern will make more optimal usage of your CPU cache than if you used std::map or as sorted std::list or std::deque.
Two different types of dangerous. I assume you're talking about low level memory and pointer manipulation, whereas I'm talking about good design and (related) potentially undefined behavior.
What's the big deal with duplicating the key? 
Sometimes, it's a significantly large object that affects both runtime and memory usage. 
I have my own implementation of "set"-like and "map"-like that was, by luck, able to handle this type of thing. I got there via the following route: 1) I wanted to fix a few negative issues related to the std containers - namely, binary bloat, slower compilation, and difficulty using them as members of classes inside Win32/64 dll's. 2) To solve 1) I created a non-template implementation of RBTree that uses binary blobs, custom allocators, and function pointers; the amount of dynamic allocation is the same as with the STL - the tree stores an offset to the memory aligned contained object within each tree node. This non-template is safely wrapped by a thin template layer of inlined functions for each actual Set&lt;T&gt; and Map&lt;K,V&gt;. For efficiency, the form of the functions used for the comparison in my implementation implement a ternary logic comparison analogous to memcmp() and strcmp() rather than just a partial order binary comparison. 3) Because my underlying tree implementation was passing around function pointers anyway, it was easy to add a template member wrapper that is parameterized by a special comparison between a different type and the regular key. My typical use case for this is for when the container contains smart pointers to some dynamically allocated, possibly large, object and all I want is to check whether some matching part of the object is help by the container and/or to retrieve the object without constructing a new one. 
It's okay as long as you understand the semantics of a strict weak ordering, that two keys `a` and `b` are considered equivalent if they are incomparable (i.e., neither `a &lt; b` nor `b &lt; a`). In practice, you're usually dealing with totally ordered keys anyway.
There are a bunch of cases where this won't work; laptop in sleep mode, variable clock rates, etc... etc... etc... Doing a high resolution timer that works in all cases is way harder than it should be. 
Wow, for all the shit people give Java for being slow, its actually one of the faster ones.
What you're looking for is n2882, "Adding heterogeneous comparison lookup to associative containers": http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2009/n2882.pdf Unfortunately, it appears it's aimed at TR2 rather than C++0x.
shared_ptr that mother?
...in a single post _than_ I've seen...
...less _than_...
Yeah, I was surprised to see it beat out C# Mono by so much. Still, C and C++ both run these benchmarks significantly faster -- I think most people who complain about Java being slow would prefer C++ for performance-critical tasks.
I don't understand why the person who asked the question would assume C is faster than C++. Wouldn't exactly the same C code also compile as C++ (perhaps with a few pointer casts thrown in)? Is the C++ compiler going to be worse at compiling C? Besides, I'm pretty sure the C++ STL is going to be better optimized for complex data relationships than anything he'd write himself in C. Not to mention he's got a much higher chance of leaks since he'll probably need to do a lot more manual malloc/free calls. I'm not hating on C though. We're actually using it for server-side web applications and it's a decent language for that, once you have some frameworks for easier Apache communication in place. Plus memory leaks aren't really an issue when the program only runs long enough to serve up one page. I'd probably prefer C++ for some of the more complicated applications, but we get by with C just fine.
Apparently it measures wall clock time, not CPU time. This can reduce benchmark accuracy a lot if you have other programs running.
Thanks for letting me know, I'll look into it. Shouldn't you only have minimal other programs running while benchmarking anyway?
Fixed. Thanks!
If it's sufficiently large it could lead to cache thrashing.
I'd call Java **fastest of the slow**.
&gt; Shouldn't you only have minimal other programs running while benchmarking anyway? Sure, but at least on Windows you usually have at least some IDE running. I once looked at benchmarking in CPU time, Windows has GetThreadTimes function and Linux has times function for that. Of course, other processes can still affect the result, since system calls may take longer if the call must wait for some resources, and scheduling more often causes more cache misses. (And probably for a number of reasons I can't think of)
I've been reading up on the difference. I'm thinking about recording both and letting the output formatter choose. I think that may be the best way to handle it because I could see a case for wanting wall clock time (if a benchmark spawns processes, and you want the time for all processes to complete). I'll work on updating the Stopwatch class to handle both.
Some kind of reference would work. But it still takes up unnecessary memory. And the cache would now be reduced even with small objects. It may not seem like much extra memory for the reference, but it adds up. These can be acceptable tradeoffs. But sometimes they are not. 
[Indeed](http://catb.org/esr/writings/unix-koans/shell-tools.html).
From what I understand, it requires two extra predicates that can compare key against element and element against key on top of element vs. element. That would certainly work and I understand why it would be done that way. I think adding an accessor method or predicate would have been better. Instead of the concept of value_compare, you would only use key_compare and use the key_accessor predicate on elements before passing them to the key_compare predicate. I'm not familiar with the committee review process. What is TR2? 
[Sometimes carefully tailored C code can be faster than the STL.](http://unthought.net/c++/c_vs_c++.html) But as the article points out, the carefully tailored C code can often be improved and made more generic if it's rewritten in C++. 
&gt; Besides, I'm pretty sure the C++ STL is going to be better optimized for complex data relationships than anything he'd write himself in C. People still assume they can out optimize a compiler. Inexperienced programmers will listen to the programmers of yore tell tales of how poorly the compiler did some operation or another. These stories get passed on to other programmers and are often embellished along the way to the point that the compiler used 96 instructions to add a pair of numbers. These inexperienced programmers then assume that all compilers suck and they too can write better code than the compiler. They never realize that they have no understanding of the underlying architecture, they have never written assembly (much less machine) code, they have only a basic understanding of the algorithms, and the complexity of processors has increased dramatically since these early stories. Instead of focusing their attention on choosing better data structures and algorithms, they nitpick over the 13 extra clock cycles being consumed in a non-critical path of code. Never mind the fact that it took the other programmer weeks of work to eliminate those 13 clock cycles and his code is now an incomprehensible mess. Yes, there are a few exceptional programmers out there that can write better code than the compiler, but they are few and far between. Not to mention that the applications that require that sort of speed are even rarer. 
I love how any post/question about C++ everywhere on the internet, ALWAYS ends with boost::whatever_is_relevant.
With good reason.
I think this [article](http://msdn.microsoft.com/en-us/magazine/cc163996.aspx) is a more accurate run down of the difficulties of developing an accurate timing mechanism on Windows. For the Windows platform, the best you can do in accuracy is going to be about 10ms at best. One of the problems with the WinAPI is the low level cpu query methods provide no guarantee of accuracy or consistency across machines. The QueryPerformance* methods can, without warning or notification, fall back to the system clock queries that have a resolution of 15ms. The picture on *nix operating systems is somewhat better and reliable accuracy of approximately 1ms is attainable by polling the cpu freq. counters by using calls like [mach_absolute_time(\)](http://developer.apple.com/library/mac/#qa/qa2004/qa1398.html) on OSX or [clock_gettime\(\)](http://pubs.opengroup.org/onlinepubs/007908799/xsh/clock_gettime.html) on Linux. On *nix systems, the situation can be further improved by using methods such as polling the beam raster rendering of the display device for sub millisecond accuracy. Unfortunately, at least last I checked, Windows does not support this in their video drivers. Of course, once you get down to this level of accuracy, you need to start asking yourself why you might need it. For example, in neuroscience accurate timings are needed to determine correlation between stimulus onsets (often visual) and BOLD responses. I've run into a lot of neuroscientists who are obsessed over accurate timings, even though for this purpose the default limitations of even windows (~15ms) are more than sufficient. Besides inherent latency limitations of the MRI scanners themselves of around 50ms for the image acquisition pipeline if the stimulus is visual in nature it's virtually guaranteed the subject is presented that information on a monitor with a 60Hz refresh for a latency of around 17ms. So, it's very important to have a firm grasp of context before drilling into the more advanced methods for obtaining accurate timings. Now, one might point out that if you have two independent latency events of 15ms which you're trying to synchronize you could potentially produce an unacceptable delta of ~30ms if the they're on opposite sides of they're latency distribution. By employing methods like poll jittering and drift adjustments the effect is normalized (NTP employs some of these). TLDR; Due to inherent platform limitations and variability across systems and trials, there is no such thing as a high-resolution timer on Windows if we define "high-resolution" as sub 15ms accuracy.
I am a java developer, therefore I do not know enough to comment on this.
As a c++ developer at Intel, I would beg to differ with him. Intel wouldn't even be able to make chips without c++ as many critical steps in the design and manufacturing flows use it.
I wonder if he's talking about *new* development, though. Of course C++ is useful for legacy code; you don't exactly have a choice. Ditto for interfacing with C++ libraries. But if I were starting a green-field project, I'd do all my prototyping in Python. And then, as I discovered performance bottlenecks, I'd push them down into C (not C++). C++ often gets you the worst of all worlds: the productivity of C and (if you're not careful) the performance of Java or worse.
Spoken like a true Java developer.
That's lovely. Java is such a cute language. The successor to PASCAL at schools everywhere. If it creates an app that pays the bills, then great. As for me, my apps in creaky old C and C++ will continue to pay mine.
IMO C# is so much more powerful than Java. For me, the "unsafe" keyword is a godsend (pointer arithmetic as well as garbage collection, fuck yeah :) I want SIMD instructions, though. Mono has them...! But yeah, I hate C++, yet feel myself compelled to use it :)
Yes methods declared in the header are inlined, and yes template must be declared in the header (because they too must be inlined), but I don't believe it's correct to claim inlining is the source of template speed, since the two forms of inlining mean quite different things. The former means: struct A { int a; void foo() { a += 1; } }; A a; a.foo(); the latter gets transformed to if the compiler is able to inline the method foo() for a performance gain: A a; a.a += 1; But with templates, we have: template&lt;class T&gt; struct A { T a; void foo() { a += 1; } }; A&lt;int&gt; a; a.foo(); with inlining means the compiler will (use a mangling scheme to) generate: struct A_withInt { int a; void foo() { a += 1; } }; A_withInt a; a.A_withInt::foo(); which it again may or may not also transform the latter into A_withInt a; a.a += 1; it's probably more correct to say that the speed of templates is because the compiler pre-calculates all the type information at compile time, and avoids any run-time type-checking.
C/C++ is still going to give you the best performance, hands down, in numerical calculations. If it's viewed that the speed gain is marginal compared to ease of development, this is true in most cases (except when it's not). However, in those same cases one could claim that Java and C# has little use anymore compared to the ease of development in other languages like Python. Particularly combined with the ability to call C/C++ modules. Quite frankly, this is a complex and highly situational topic and anyone that provides a blanket answer like &gt; C++ has little to no use anymore As if it pertains to all situations and all uses is making an idiotic claim. For example, say I'm running physics simulations that take 14+ days to complete on a 50 node cluster and a particular frequently called C routine provides a .03 execution advantage and I run this simulation 100 times a year that saves over a month of CPU time if written in C. That's an example that might fall into the "little" category. But there are other, more common discrepancies that could impact application's efficiency depending on what kind of routines they're using. There is only one correct answer to the question: &gt; Is the greater difficulty/ease of X worth the extra gain/performance of Y? Where X is some language and Y is some feature. And that answer is: &gt; It depends, could you tell me more about the problem you're trying to solve and the constraints you're dealing with? Anything else is myopic and/or dishonest.
Surely your trite hating of Java is as information-free as this fool's assessment? Personally, I think Java's a useful and **small** language that you can keep in your head, like C. I like C++ too, but it's gargantuan feature set and side-effect-y-ness can make for some real craziness ... and the corner-case bugs ... oy oy oy. Machoness shouldn't really enter into it ...
Wow, an Intel employee is wrong. Call the presses. (rolls eyes)
Actually, Fortran tends to do math optimizations better than C++. All the novices here are likely to assume I'm trolling or making a joke about age, but go try it out.
Java is small? Bahahahahhaha. I agree with the rest though.
That's nice. Call us when you've made something noteworthy.
&gt; I wonder if he's talking about new development It's a shame the internet is too small for him to make himself clear.
I don't know why this is getting downvoted. It's perfectly reasonable for a blub developer to say "I don't have the context to criticize another blub developer talking about blargh." I really wish people would get a grip on the downvote button.
If the requirements include both great complexity and critical high performance, then C and C++ are often the right choice. That means video games, science apps, video editors, stuff like that. The biggest issue with C++ specifically IMHO is developers skilled enough to do it right. 
Honestly, it's amazing to me even that C++ haters can trick themselves into thinking the language is useless just because they don't use it. I mean, I don't tell myself a jackhammer is useless, even though I rarely need anything bigger than an awl. Thank you for reverse navel gazing. Upboated. I'd flotilla if I could.
I'm not talking about libraries, for example. Just the language itself ... No operators, single inheritance. What's the big deal? It's not a language I need a [FAQ](http://www.parashift.com/c++-faq-lite/) for ... Most of this looks straightforward: http://java.sun.com/docs/books/jls/third_edition/html/j3TOC.html
True, but I was also trying to make the point that I only learned Java for my 4 years in college and I don't know enough about anything to comment on this point, but if I had learned C+ I could comment on nearly every language out there as you really need to know what is happening with your program to develop with C++. With Java, it's all kinda done in the background. That's one very, **VERY** important use of C++.
Well, according to the reddiquette, down votes should be reserved for comments that don't contribute to the conversation. I'd say a post declaring "I cannot contribute anything to this conversation" is exactly what the down votes are supposed to be used for.
[deleted some pointless drivel] 
Yeah, Java can be described as small by observing that it has no operators (incorrect, even if you meant operator overloading) and single inheritance (which isn't actually going to make the language any simpler.) That's a legitimate way to observe the size of a language. Go learn Forth, please. Welcome to an actual small language.
Note that I was agreeing with you.
If that's what it said, you'd be right. However, it actually says "the reason I cannot contribute is X," and what he was reminding us was actually an important lesson about the limitations of a blub programmer. I can't choose what you do with your votes, but I disagree all the same. Happy holidays.
Tautologically all compiler optimizations are compiler dependant. However, fortran - "formula translator" - was designed for efficient math as its only goal, back in the 50s when it really really mattered. C++ on the other hand was designed for indirection delegation, which prefers flexibility and expressivity to speed. These are fundamental characteristics of either language. Every Fortran compiler is superior to every C++ compiler, provided a basic level of corporate competence, in terms of math optimization. I'm sure if you scrape hard enough you can find rare counter-examples, but then you'd miss the bigger picture. Sometimes it's in excess of 20%, when both compilers are extremely mature and profitable compilers from the same major vendor; in Intel's case, it's greater than 30%.
Ah, sorry, I didn't realise you were a snide cunt. I apologise for trying to have an open and honest conversation.
It's unfortunate that you react that way to being offered information. One wonders if you've realized how much worse your behavior is than what you're complaining about. Nobody here was un-open, and nobody here was dishonest. You just want to feel like a victim. That's spelled "thank you for teaching me that a production level programming language can be fully specified in six paragraphs of english text, which is less than it takes to define Java's boxed integer." Don't worry; I won't bother you with experience or the opportunities to learn something again. I did not mock you, and I was not derogatory. Please calm down.
If C could have typeclasses, partial application of functions, and a link format which carried more information with it (e.g. number of arguments to a function) then I would never use C++ again.
I'm with you here. I don't understand why you got the downvotes (oh right, this is cppit) but I agree. I wouldn't go so far as to say that C++ is useless, but its whole model of OO is very difficult to use. Templates and metaprogramming however are fucking amazing. But you are right, Python has a truly majestic way of doing OO, and any bottlenecks can be shoved down to C with almost not effort (I'm thinking Cython here) and ridiculous performance gains. Actually after thinking about it while typing this comment, I think C++ has a lot to offer (boost Python is great too), actually I think Java is the one that is becoming rapidly obsolete. When mush simpler languages can compete on performance, why would you bother anymore?
Come on dude, you don't have to be an asshole about it. I actually think he's got a point, perhaps hes taken it too far, but seriously why would you go to the extra effort when you can get almost the same performance in a high level language. The tools to do that are getting pretty impressive these days. C++ has its place of course, its still probably the second most important language, second to C.
&gt; Come on dude, you don't have to be an asshole about it. Do you really think what you're responding to is less aggressive than this quote? &gt; but seriously why would you go to the extra effort when you can get almost the same performance in a high level language. 1) C++ is a *much* higher level language than Python. 2) Python rarely gets within an order of magnitude of C++ performance.
Ok I do scientifc computing, and yes C++ is widely used, but most scientists I know would rather be using high level tools like R, Matlab, and often Python. Since bottlenecks in scientific computing are typically long arduous loops over ridiculously large arrays, C is the obvious choice for performance (and increasingly CUDA/OpenCL). And this works perfectly in tandem with high level languages such as R, Matlab and Python. [Check out this guys work](http://mathema.tician.de/software/pycuda), most of his code is C++, but the intention is to expose CUDA and a lot of excellent low level tools to Python. This sort of thing is the future of scientific computing, imo.
Excellent point. I actually think that as tools to wrap C/C++ in high level languages (such as Python) improve, Java and C# are the ones becoming obsolete, rapidly. Have you seen [Cython](http://cython.org/)?? Absolutely amazing little tool.
* Mockery: Bahahahahhaha * Insulting: Go learn Forth, please. Welcome to an actual small language. * Sarcasm: That's a legitimate way to observe the size of a language. * Derogatory: You just want to feel like a victim. These are not the signs of an open, honest person.
&gt; Have you seen Cython?? Absolutely amazing little tool. When I mentioned c modules, this was exactly the tool I had in mind. Yes. It's amazing.
What ever happened to reddiquette? I see no valid reason for this to be downvoted.
Oh I know, its just I didn't know whether I made my point clear or not. So you could have been agreeing with something I wasn't saying. 
&gt;Do you really think what you're responding to is less aggressive than this quote? Yes, but possibly because you misread my words, I'll try again: &lt;calm, friendly tone&gt;come one dude, you don't have to be an asshole&lt;/tone&gt;. Its just a conversation, I see you (in another comment) lamenting people who don't know how to use the downvote button but I see the GP had a downvote, wouldn't have been you using it as the 'disagree' button would it? Again, not being rude, just legitimately prodding you to perhaps not be so abrasive. (you can be abrasive without using the swears, just like I can swear and still be gentle in my tone). 1) I'm not sure we are using the same definition of high level. What I mean is things like dynamic typing, interactive interpreter, complete automatic memory management. In this sense, I don't think I've ever met a programmer who would call C++ "higher level than Python". But hey, I could be wrong, so I've clarified my wording. 2) I wasn't talking about Python, I was talking about *C modules accessible to Python*. The latter can be *close* to the speed of C/C++, thats right, very little overhead.
I think I'm in love, personally. 
Laughing isn't mockery. Asking someone to learn a language as an example of what they're talking about isn't an insult. You have a point with sarcasm. Your example of derogatory comes after you claimed I was being derogatory. Time doesn't work that way. Find someone else to complain at, please. You're welcome for the information.
&gt; Yes, but possibly because you misread my words, I'll try again: &lt;calm, friendly tone&gt;come one dude, you don't have to be an asshole&lt;/tone&gt; I hope you realize how easy that misread is. &gt; wouldn't have been you using it as the 'disagree' button would it? No. I realize it's an easy thing to expect. Since you don't know me, I don't take offense. &gt; just legitimately prodding you to perhaps not be so abrasive. I confess, even the warm tone as intended, I think you might have a bit of wrong sense of ratio here. An amateur got up on a post about how the world's second most common language is useless. I can either head-pat him, or I can do him the justice of shooting him down in a sufficiently brief form that he might actually realize he's got something to learn about. Politeness or learning opportunity. It's not abrasion for the sake of abrasion; it's abrasion because that's how you reach a Dunning-Krueger. If it offends you that I'm trying to help him in a way you don't enjoy, that's fine. That said, if you're going to warm-tone call someone an asshole, maybe you should also consider that abrasion is a useful tool, and a point of a judgement call which you yourself seem entirely willing to make. One does not decry graffiti with a spray can in hand, Senator. Saying calling someone an asshole is intended to be gentle doesn't mean it is so, nor does that mean it is suddenly less a frustrating public behavior than reminding someone that their opinion is weight bounded by their public accomplishments. The germane point here is that we are both attempting to deliver unpleasant news that we see as teaching messages. All the same, I had an attachable criterion, a germane point and did not curse; you called someone an asshole, offered no criterion, cursed, and then pretended, in the vernacular, that your shit doesn't stink. I bear you no ill will for trying to grab someone by the shoulders and shake; indeed, in the not-a-public-tantrum way that you did it, it's a useful and generally productive thing. All the same, when you pretend you aren't being as nasty or nastier than the person to whom you're speaking, you directly undermine the value of your point and your credibility. This means that your point will, generally, go unheard. If you need to be a jerk, be a jerk. But don't pretend it's not what you're doing. If there is a miscommunication in tone, it is the fault of the speaker, not of the listener. Remember Voltaire's first admonition: "it is the duty of every author to write such that the reader not may but rather must understand." You cannot expect someone being called an asshole to infer that you're being Mr. Rogers about it. To say retroactively that you were being gentle when you present no seeming theretowards simply makes the reader frustrated, as you seem to expect strangers to expect the best of you when you're cussing at them. Seriously, who ever does that? &gt; I'm not sure we are using the same definition of high level. A high level language provides facilities not found in the machine. C++ provides many more such facilities than does python. C++ is not low level simply because it is machine-near. &gt; What I mean is things like dynamic typing, interactive interpreter, complete automatic memory management. C++ has had 1 and 3 since before Python existed. My opinion is that an interpreter console has nothing to do with the level of the underlying language; it is a useful tool, but Python as a language would not change if the console were missing. Unfortunately, many Python programmers do not understand that merely because C++ does not enable memory management by default, it is unavailable. Indeed memory management in C++ is stronger than in almost any other language. The only language I know with stronger voluntary memory management is the fairly esoteric Fortress. &gt; 2) I wasn't talking about Python, I was talking about C modules accessible to Python. The latter can be close to the speed of C/C++, thats right, very little overhead. Well, then that suddenly doesn't make sense at all. C++ isn't necessary because you can get almost as fast with ... Python calling C++? C'mon, man, think that one through for a minute. Tell me with a straight face that you stand by that, or explain what I'm getting wrong. Triple dare.
Sorry to say this, but maybe you just don't get C++? I always loved it because it was the faster path between two points: 1. Underlying syscalls, h/w, gfx routines, other low-level components with C api's or processor intensive components and 2. Scaling complexity that can grow to reach anything. Honestly once you get comfortable with it (and building a stable set of libraries is critical) you find that C++ doesn't take more time than any other OOL. The learning curve is a bit steep, but it's the only swiss army knife of programming I know.
Hah, went to google for a systems programming and video codec job. Tried to explain to the recruiter I programmed in c++ and he kept trying to say that was like java right? Ended up with 5 java programmers. Tried to show some mmx/sse graphics code to each of them, they all glazed over, then I had to explain to them why a hashtable was the greatest invention in human history because it's O(1). Would laugh about that more if it weren't so damn sad...
&gt;No operators, single inheritance. Then don't use them... Single inheritance is part of the language, but so are goto statements... virtual interfaces ala Java work beautifully in C++. The problem is not a small language, but a small mind...
It's known as trolling, passive aggressive trolling. Just part of the Internet. He might mix in some tidbits of 'info' here and there, that's what some of the better ones do to give themselves an excuse to troll, but the bulk of the content will be trash and other snide remarks. Then when he gets a rise out of you and you call him out he'll reply with the usual: Problem? (hehehe) Down vote it and move on, no need to let it get to you. Also note how he's done it to other people in this very thread.
I love C++, but find myself compelled not to use it... Also, most C++ programmers are COMPLETELY WORTHLESS WTF? It's like giving a sawzall to a 6-yr old and coming back to find your car in 95 pieces in the driveway... The real problem with C++ is it's far too powerful, only a few people have the discipline not to abuse it.
I'm not sure how you can claim it takes no longer than other OO languages when you have to code up so much boilerplate for anything you do, memory management, etc. I do understand C++, its a ridiculously full featured tool, but sometimes just a bit too full featured. Plenty of rope to hang yourself so to speak. If you re-read my post, I wasn't damning C++, I actually said its more relevant than Java in my opinion. I just said that there are so many easier to use tools now, ones which can *also* access C's underlying syscalls with very little overhead, that C++ is possibly not the first tool to reach for for many things.
I'd like to add the real problem with C++ is that it's so damn powerful people who don't know what they're doing are overwhelmed by the features and feel compelled to use them. I think half the features Stroustrup put in should be deprecated with warning flags by gcc (non-virtual multiple inheritance, a few overloading mechanisms...). Strip it down slightly and you have a much more deterministic language that still does everything from C/asm up to high level languages. Just because it has massive functionality doesn't mean you always have to use it.
[Google](http://www.google.com/) count?
A: Yes, far too much rope to hang yourself, but then again goto is a pretty nice tool to have too isn't it? Just don't use those parts, and teach other people not to. B: Boilerplate is a problem (and every dev should keep his own toolkit of classes/templates for it), but the memory management is C++'s weakest link. Honestly I think C++ should have an automatic gc mechanism as an option (I think C++0x has one), because this is intolerable for a language this day and age. C: I agree, for small tasks C++ probably doesn't belong, but for larger tasks there is no other tool I'd likely reach for. Anything without complex h/w deps or performance requirements sure throw python in there. C++ is for the things that matter.
&gt;One does not decry graffiti with a spray can in hand, Senator. One can certainly make art with a spray can. I did not say you are an asshole, I merely said you were *acting* like an asshole. If your intent was to correct a less experienced programmer, you most likely succeeded in simply turning him off. Sure, your lengthy tirade justifying your actions could be considered, well justified, in some sense. But I certainly doubt you achieved what you set out to achieve, most likely just got his nose out of joint and further ingrained his opinion as a result. &gt;C++ has had 1 and 3 since before Python existed. I'd like to see you craft an effective class without resorting to manual memory management in anything but the most recent versions of C++. You can argue semantics all you want, Python is inarguably a high level language. C++ is everything and the kitchen sink (which is why its kind of a pain in the ass). You can't really say its a high level language (HIGHER level than Python??) just because it has *aspects* which are abstracted. It also has *aspects* which are machine level. Python does not. This is a silly, silly argument, but you're good at those, aren't you StoneCypher. An elder statesman, but an abrasive one who likes to nit pick the smallest, most irrelevant of points.
The thing that really bugs me about C++ is the way Classes are implemented, that you simply have no choice but to resort to new and delete, and simply must provide an overloaded copy constructor *in every class you build*, I just don't understand. Its nearly the same chunk of boilerplate every time, and Stoustrup never thought, "hey maybe we can automate this?" is beyond me. I have fairly limited knowledge of C++ though, so take what I say with due grain of salt.
Are you suggesting that you made Google?
&gt; If your intent was to correct a less experienced programmer, you most likely succeeded in simply turning him off. I find that quite rarely to be the case. &gt; I'd like to see you craft an effective class without resorting to manual memory management in anything but the most recent versions of C++. Define "effective class." I've had no trouble in writing C++ for ages. &gt; You can't really say its a high level language I can, and do. &gt; (HIGHER level than Python??) Absolutely. C++ offers, as I've already said, many many facilities that Python does not. The converse is barely true at all. &gt; just because it has aspects which are abstracted. These are not my words. (However, of course, just for the sake of the pun, C++ was powerful enough to implement aspects; Python is not.) &gt; It also has aspects which are machine level. Python does not. So what? &gt; This is a silly, silly argument, but you're good at those, aren't you StoneCypher. It's unfortunate that you're getting to this tone. &gt; An elder statesman, but an abrasive one who likes to nit pick the smallest, most irrelevant of points. A point is not irrelevant simply because you do not understand it.
Don't classes in C++ have to be put on the heap? and hence require the use of new/delete? I thought that was literally the implementation of classes in C++. I could be wrong, I am by no means a guru. I don't know how that fact that C++ has a *greater number* of abstractions has anything to do with how we would classify the language? Is the entire language made up of abstractions? No it is not, it includes both high level abstractions and low level machine interaction. With Python, the *entire language* is abstracted, I'm not talking about the *number* of tools, I'm just talking about the *characteristics of the language*. I think most people, if polled, would agree with me. It is, after all, semantics. The only reason we might classify these things is simply to aid communication, and when most people view something a certain way, you tend to go with the flow to improve communication. I don't really care if you disagree, I provided my definition: Python is high level because the whole language is an abstraction, the same cannot be said for C++. &gt;It's unfortunate that you're getting to this tone. Maybe you don't remember, but we've butted horns before. I was merely alluding to that, and suitable prodding you some more. You are sensitive, aren't you? :) &gt;A point is not irrelevant simply because you do not understand it. As I understand it, you are saying that C++ is high level because it has a greater number of abstractions available to it than Python. So despite the fact that you can directly manipulate memory, you have pointers and so on, you claim none of this is relevant because its libraries are bigger. More libraries == high level, go it (but disagree).
Cool :) There certainly are use cases for both values, and maybe even for user / kernel times separated (they might help interpret the results.) I'll try your framework next time I'm benchmarking something. *EDIT:* Realized that waiting for a resource (file, mutex, network) does not count as CPU time, so faster method may consume more CPU time than a slower one. Maybe wall clock time should be the default.
&gt; Don't classes in C++ have to be put on the heap? No, of course not. &gt; and hence require the use of new/delete? Classes don't require new/delete, and new/delete can act on more than just the heap besides. &gt; I thought that was literally the implementation of classes in C++. The only difference between class and struct in C++ is that class is default private whereas struct is default public. This differentiation was made to ease keeping compatability between C and C++ while making clear that default private was preferred for code moving forward. &gt; I don't know how that fact that C++ has a greater number of abstractions Please stop suggesting that I am speaking only of the relatively limited group of functionality that falls under the abstraction banner. There is more to heaven and earth than is dreamt of in that philosophy, even in Python. I very clearly was not speaking of abstractions. Again, the same quote you seem to keep wanting to read as saying something different than what it actually says: &gt; &gt; A high level language provides facilities not found in the machine. This says literally nothing of abstraction. &gt; Is the entire language made up of abstractions? No it is not, it includes both high level abstractions and low level machine interaction. With Python, the entire language is abstracted You are way too fascinated with abstraction, which is a minor topic. Python is also significantly less abstract than C++. &gt; I'm just talking about the characteristics of the language. Then why is it that C++ can implement lambdas, parsers and aspects within its type system, but Python cannot? Oh, right: because Python isn't actually very abstract at all. &gt; I think most people, if polled, would agree with me. Ad populam is not particularly compelling. &gt; It is, after all, semantics. Language facilities are not an issue of semantics. &gt; The only reason we might classify these things is simply to aid communication This is not correct. We might classify these things to learn what is and is not possible within a given language without implementing other languages in that context (the generic "turing equivalent" out.) &gt; I don't really care if you disagree, I provided my definition Given that this has been well defined since the 1960s, why you believe you have the privilege of defining this is beyond me. &gt; Python is high level because the whole language is an abstraction, the same cannot be said for C++. Given that you don't even know the basics of C++'s memory model, I think it's fair to suggest that you don't actually know much about C++, and therefore cannot be in a position to make such a criticism. Your handwaving around Python's being abstract are vague. C++'s grammar is strong enough to express new language constructs. Python's is not. C++ can express compile time polymorphism across new language constructs. Python cannot. C++ has a modular, closed type system. Python does not. C++ can be built to express domain specific languages at compile time. Python cannot. C++ has the ability to express abstraction at the TU level, meaning that its abstractions are available to code from other languages and vice versa; Python cannot and is not. On what technical basis do you make the claim that "the whole Python language is abstraction," or that "Python is more abstract than C++?" Notably, the whole Ook language is mathematics, yet C++, which is not completely mathematics, expresses quite a bit more math than Ook does. The presumption that 100% of small is more than N% of large is a false one. &gt; Maybe you don't remember, but we've butted horns before. No, I do not remember. However, given your increasing willingness to rely on unstated value judgements, and your rising tone of anger, I am not terribly surprised. &gt; You are sensitive, aren't you? :) Not hardly. If I were, I wouldn't have replied to you, read what you had to say, taken you seriously, discussed this at length, and mentioned only once in passing that I am disappointed by one of your choices. Please do not attempt to inflate your ego by inventing sensitivity to look down on. It's boring and uninteresting, and fools nobody but you yourself. &gt; &gt; A point is not irrelevant simply because you do not understand it. &gt; As I understand it You do not understand it, is the problem. &gt; you are saying that C++ is high level because it has a greater number of abstractions You are repeating this after I have explicitly told you that it is wrong. This suggests either that you are preferring your guesswork to what you're told, that you're not reading or that you have a low reading comprehension. I said nothing of abstractions, and find your insisting on discussing the topic frankly silly. You might as well insist I'm trying to tubthump that C++ can use non-IEEE maths. It's just dumb. &gt; So despite the fact that you can directly manipulate memory, you have pointers and so on, you claim none of this is relevant because its libraries are bigger. I didn't say anything about libraries. Where do you get this stuff? Are you writing fanfic here? &gt; More libraries == high level, go it (but disagree). I said nothing of the sort, and find the suggestion jaw droppingly stupid. Providing a library to a language does not make the language higher level. Please stop pretending I've said things that do not appear in my actual text. Moving forward, to help you cope with your reading level, please consider quoting where you think I'm saying a thing, so that when you go back to find the quote, and it isn't there, you can go "oh right, that's because I was arguing with something he never said."
Two things I'd meant to say but forgotten: 1) Using placement on the heap as an example of anything important is silly; that barely batters. 2) There is no implementation of classes. C++ compilers are free to implement classes however they want, subject to functionality and performance guarantees, and various compilers do things various different, interesting ways.
Many of the parts that you see every time you do a search, yes. And some of the parts you don't see. Everything at Google is a team effort, of course. If you didn't assume that, I'd wonder just who's the amateur here. I work with C++ every day. And Python, JavaScript, Sawzall, and several proprietary languages on most days. And even occasionally Flash. I'm hardly a C++ newbie, and before Google I've worked in companies in every stage of the development cycle, from green-field startups I founded myself to other people's startups that are just trying to get to market to startups that have customers and are iterating on their feedback to mid-size consulting companies. I would not use C++ for a green-field project. I would start with Python (or Ruby, or some other dynamic language) to get a sense of the problem domain. Only once a piece of code has proven to be a bottleneck would I rewrite it, and then in C, using ctypes to interface with it. Only once a piece of code has proven to be a bottleneck *and* grown to sufficient complexity that C is painful would I think about rewriting it in C++. I'm far from the only developer that works like this. I've noticed you rag on a couple of them (Firaxis etc.) in past comments. Ever considered that they may know something that you don't know?
&gt; Many of the parts that you see every time you do a search, yes. And some of the parts you don't see. Everything at Google is a team effort, of course. If you didn't assume that, I'd wonder just who's the amateur here. How nice, we're jockeying. The clear implication of your phrasing was something other than "I'm a cog within Google." How should I know whether your work at Google matters? Most of what Google does does not matter in a profound way. Google is a search engine, an ad network, a mail client, a shareware 1992 quality usenet news client, and an enormous pile of useless pet projects and privacy intrusions. If you remove AdSense, Google generates less money per staff member than the average McDonalds'. You're window dressing, largely, because they're rolling the dice at large scale that one of you eventually will come up with something that drives revenue. At Google's scale of revenue, one hit in a thousand is good enough, and that's about what they get. &gt; I work with C++ every day. And Python, JavaScript, Sawzall, and several proprietary languages on most days That's nice. I wonder whether if I responded with my list of daily languages, you would interpret me as bragging about an unimportant subject. Being a dialect dilettante is neat and all, but Pike's team at Google implementing Sawzall is just rediscovering what's been well known since the 1950s by people Pike's never even heard of. I still find it amazing that Google believes it invented map/reduce. How you guys can have that many supposedly amazing programmers and not have one single person going "hey guys, that's kind of been known about since two hundred years before Ada Lovelace was alive" is beyond me. Oooooh, you gave a new name to something that was considered a fundamental language feature in ADA. Bravo. &gt; I'm hardly a C++ newbie Have you ever met a C++ noob who didn't think they weren't a noob? &gt; and before Google I've worked in companies in every stage of the development cycle Yay for you. Are you trying to have a job interview here? Are we going through your personal history for a reason? What you could actually do is say what noteworthy thing you've built, but given how much effort you're going to to not name anything, I kind of get the impression you've been job-hopping and are trying to ride on the coattails of the people around you. Since you didn't build google, your response "does google count" isn't an actual legitimate answer. After all, I've never even worked for Google, and they run some of my code. &gt; I would not use C++ for a green-field project. Yay! Opinions from a guy who talks about how much he's done but won't name anything. &gt; Only once a piece of code has proven to be a bottleneck would I rewrite it, and then in C, using ctypes to interface with it. Sounds like a plan for high quality software delivered quickly. No, wait, that other thing. A catastrophe made by a glue fascinated cowboy. &gt; Only once a piece of code has proven to be a bottleneck and grown to sufficient complexity that C is painful would I think about rewriting it in C++. And yet you won't name anything you've grown to complexity. It never ceases to amaze me how people fail to understand why their undefended opinion-driven assertions, backed up by a list of specified accomplishments which a LISP programmer would call a cons cell, aren't actually going to get taken seriously just because of the name on their paycheck. I know a lot of bad programmers at Google who think they aren't bad programmers, and hold up ridiculous accomplishments because they don't have any real ones (eg #2 on topcoder, or being the "release engineer" for a gba toolchain's mod player.) All I see here are a bunch of bland assertions about complexity that could be cut and pasted from half a dozen mediocrity loci such as StackOverflow or BlogSpot. Maybe you could next enthrall us with stories about how you value the end customer perspective in design, or how use cases drive your UML pattern driven object methodology to deploy architecturally unified interface paradigms which lower the thrash factor and demarginalize range customers. I mean seriously, it's like listening to dilbert for programmers. "Only once a piece of code has proven to be a bottleneck" ho hum. Because clearly language choices and engineering decisions should be based on bland anecdotal rules of thumb with no data defense, rather than a sound argument based on the job at hand. I've seen mad libs that get closer to a real understanding of design. &gt; I'm far from the only developer that works like this. Yes, it's expected for a mediocre programmer to be far from the only developer which works mediocrely. That's what mediocre means. &gt; I've noticed you rag on a couple of them (Firaxis etc.) in past comments. Yes, Firaxis being the company which worked on a game for five years under the motto "we'll release when it's done," and deployed a game whose multiplayer mode didn't have game saving or unit animation. Clearly, they're the people to hold up as banner cases of good engineers. &gt; Ever considered that they may know something that you don't know? Yes. And then when Soren tried to hire me, and I got a look at their codebase, I declined. Ever consider that guesswork about hidden wisdom in a company which is famous for deploying fantastically low quality implementations of great ideas might not be smart? I get bored of these hypotheticals. Show me your work, or stop acting like some kind of sage guru. Google's less engineering talented per head than Microsoft is these days. Hell, you're not even keeping up with Apple anymore. That's meant as an insult.
By the by, since you mention Firaxis and since we're talking about people who fap to Python with no apparent technical justification, Civ4 was implemented in Python, using ctypes to get at high efficiency stuff. If you've ever played Civ4, you know how badly that went. The players are locked to 125 cities for performance reasons, when cities are less complicated than cities from Empire Deluxe 2, which ran on the 8086. If you look at Civ5, you see that the lead architect from the Python era doesn't work there anymore, and the whole engine has been replaced. Sadly, the new one isn't much better. It took them two patches and two months to get "save game" working. But hey, if those are the people you can't understand criticism of, and if they just happen to share your viewpoint on architecture? ... well, yeah, that seems to fit.
&gt; But you are right, Python has a truly majestic way of doing OO, and any bottlenecks can be shoved down to C with almost not effort (I'm thinking Cython here) and ridiculous performance gains. Thereby moving from "appallingly bad" to merely "very bad." &gt; actually I think Java is the one that is becoming rapidly obsolete. Other than the failure to use past tense, you are correct here. &gt; When mush simpler languages can compete on performance Except Python can't, no matter how often you pretend that it can. The typical range is 15x to 300x, and spikes well over 1000x; there is not one test where python drops below double speed, and that one is just leveraging python's internal libraries, implemented in C *without* the overhead of ctypes. http://shootout.alioth.debian.org/u32q/benchmark.php?test=all&amp;lang=python3&amp;lang2=gpp You can say Python competes until you're blue in the face; it doesn't. (There are simple languages, like Forth, Factor and Lua, which do.) That you aren't trying to do it on language quality pretty much says it all.
One tends to have radically less boilerplate in C++ than in other languages for jobs of significant complexity, because of the extensive algorithm and container libraries. Yes, Python has less boilerplate for toy stuff, but languages like Erlang and APL make Python look positively verbose, and you don't seem to be switching to those, even though they're both more expressive and more efficient than Python. When you get up into difficult work, where it matters, you can't find a better friend than the C++ standard library.
&gt; The thing that really bugs me about C++ is the way Classes are implemented You already made this mistake earlier in the thread. There is no "the way C++ classes are implemented." You might as well complain about the way your car is designed to fly. The implementation of classes is a compiler vendor choice. &gt; that you simply have no choice but to resort to new and delete I've already told you that this is not correct. &gt; and simply must provide an overloaded copy constructor in every class you build This is also not correct. C++ will supply one for you. &gt; I just don't understand. This is correct. &gt; Its nearly the same chunk of boilerplate every time, and Stoustrup never thought, "hey maybe we can automate this?" is beyond me. Then maybe you should learn the language. The guarantee that all three forms of copy constructor will be provided to you is in early chapter 2 of TC++PL. It's not clear why you keep criticizing a language for wrong things, which have already been pointed out to you tonight alone as wrong. Please stop criticizing C++. You haven't actually said anything correct about it so far. It should be obvious even to you that the language you're criticizing lives largely in your imagination. class Ex { public: int A; void setA(int setTo) { A = setTo; } }; int main() { Ex First; First.setA(15); Ex Second = First; std::cout &lt;&lt; "Second: " &lt;&lt; Second.A &lt;&lt; "\n"; } Notice the complete lack of copy constructors, and that copy construction works. Seriously, get it through your head: you don't know what you're talking about.
C++ is typically significantly higher performance than C, because C++'s stricter delineation of information allows the compiler to safely make far more aggressive optimizations than the C compiler can. C++ is significantly higher level than R, Matlab or Python. Your last definition was "python is higher level because it's completely abstracted," although that's a nonsense sentence. Are you going to pretend that the same is true of R and Matlab now? &gt; Check out this guys work, most of his code is C++, but the intention is to expose CUDA and a lot of excellent low level tools to Python. The reason most of his work is C++ is that Python cannot correctly implement much of it, and cannot efficiently implement the remainder. The first is a blow to its high-level-ness, and the second is a blow to your consistant false claims that Python approaches low-level languages in speed. If you're doing scientific computing in Python, you're in trouble. Python has dangerous vaguaries in its implementation of IEEE floats and doubles. There's a reason nobody does scientific computing in Python.
I am going on about abstraction because that seemed to be the definition you provided me. First, your claim: &gt;1) C++ is a much higher level language than Python. My immediate response: &gt;I'm not sure we are using the same definition of high level. To which you reply: &gt;A high level language provides facilities not found in the machine. C++ provides many more such facilities than does python. C++ is not low level simply because it is machine-near. Ok, I took this as meaning *abstraction*. You can call it whatever you like, when I say abstraction, I was referring to your claim, right here. But herein lies the crux of your argument, correct? That C++ provides **many more such facilities than does python** (emphasis mine, quote is yours). And I ask, how does the *number* of these things that are not in the machine (but also not abstractions) have anything to do with it? C++ DOES provide "facilities found in the machine" along side its "facilities that are not found in the machine". Python ONLY provides "facilities not found in the machine". In any case, Wikipedia says: &gt;Python is an interpreted, general-purpose high-level programming language... And it also says: &gt;C++... ...is regarded as a "middle-level" language, as it comprises a combination of both high-level and low-level language features. So take it up with them if you like. Every single programmer I know, friend and colleague would agree with me, all that I care about is that I communicate with them in a way that they understand. Thats all. I don't really care what you think and what your definitions are, I'll just stick with what Wikipeadia says and what my peers say. Amazing that you provided me with 5 responses, most of which seem to be correcting me (after I said I was probably wrong in the first place, and that I lack experience in C++), but its good to see you are happy to use your knowledge as a bludgeon to beat your internet opponents with. I'll leave it at that, I'm going with the definition provided by Wikipedia, the Python website, and all of my peers, thanks.
&gt;Using placement on the heap as an example of anything important is silly; that barely batters. The point was merely this: As far as my experience, and poor memory, serve, there is always some direct manual memory management involved in a C++ class. Hence some low level coding. Thats all, I could be mistaken (copy constructor?) but thats all. You don't have to come down on everyone like a ton of bricks constantly proving how much more you know. Its ok little guy, we all think you are smart.
Inherit from a base class template that takes care of most of this, build upwards. I agree that it's a critical failing, but once you know it's there C++ provides the tools to work around it. No, it's not easy by any means, and it does mean common-subclassing all your classes, but then Java does that anyway and no-one cries about it. We could agree on a standard, a boost-like extension that meant that all C++ base classes had sane copy constructors, a way to extend object member transfer (property list kind of thing), and even a new/delete operator redesign, ala class.destroy(), classobject.allocate()/construct(). Symbian was one of the few C++ based OS's out there, and had all this functionality and more. While personally the OS was far too out in right field, it was proof of concept that C++ could be used to extend the language itself, and change the underlying character of the language without losing the fine-grained control of C/C++.
Hmm, seems I got a downvote for what I said... wonder if it was my friend StoneCypher...
&gt; I am going on about abstraction because that seemed to be the definition you provided me. Even after twice I had clearly stated that that was not what I said, and even though I did not use the word abstraction, nor did I discuss abstractions of any kind. &gt; Ok, I took this as meaning abstraction. Yes, you keep saying that. It's still wrong. It's like you don't know what abstractions are. I notice you completely failed to answer my question regarding your claim that Python is completely abstracted. &gt; Python ONLY provides "facilities not found in the machine". This is of course 100% incorrect. Python, for example, can add. The machine can add. Are you even thinking about what you're saying before you're saying it? &gt; In any case, Wikipedia says: Yeah. &gt; So take it up with them if you like. I also don't argue with creationists about biology. &gt; Every single programmer I know, friend and colleague would agree with me One, I very much doubt that. Two, that's ad populam *and* ad verecundiam *and* biassed sample *and* flat out probably wrong. &gt; I don't really care what you think and what your definitions are These definitions are not mine. &gt; I'll just stick with what Wikipeadia says I confess I find this not terribly surprising. &gt; Amazing that you provided me with 5 responses, most of which seem to be correcting me Not very amazing. It's also not amazing that people in Phoenix Arizona wear sunscreen five days in a row. If you get five posts telling you stupid things, you're going to make five correcting responses. I'd tell you to check it with a Viterbi decoder, but that'd most likely be a huge whoosh. &gt; after I said I was probably wrong in the first place And then continued to repeat that wrong as correct. Including in other threads to other people. And then you can't figure out why people keep correcting you. "I know, I'm probably wrong, I don't know a lot about math, but I don't understand a system where 2+2 is 5." "It isn't." "Okay. But given that 2+2 is 5," "It isn't 5." "Right, so going from the 5-ness of 2+2" "That 5-ness doesn't exist though." "You shouldn't have kept correcting me, I admitted I was probably wrong." You can't actually see it, can you? &gt; I'm going with the definition provided by Wikipedia, the Python website, and all of my peers "I'm going with a webpage written by strangers, the blub website and a bunch of blub programmers when they say blub is a phrase I interpret to mean good." Compelling stuff. Especially given that in another thread, you made the claim that R and Matlab were high level languages, and they do not fit what you call your definition of high level. Funny how the landscape changes so frequently to suit your needs. Ever wonder why creationists never make big science studies? It's because when someone points out their pseudoscience is broken, they say "I'm going to go with what I was told in this creationist book and by my creation science peers." Have fun with your 6,000 year old Earth, kid. I'm giving up.
&gt; As far as my experience, and poor memory, serve, there is always some direct manual memory management involved in a C++ class. Yes, you keep saying that. It's still wrong. I've told you that quite a few times already. This is very literally a chapter 1 topic. You are not correctly interpreting the eighth page of the book. When do you accept that you don't know enough to make these criticisms, and stop trying? &gt; Its ok little guy, we all think you are smart. Attempts to be insulting from someone who just doesn't get the very most basics of the language they're trying to criticize aren't actually very effective. Besides, I'm tall and fat.
Is it possible to make a copy constructor without using a pointer?
Not only is it possible, it is required. Edit: it occurs to me that you might think you're asking a trick question, because several low-quality sources of interview question ask this, and say that the answer is "no." That's because they're cut and pasting from an old Usenet FAQ from when people were coping with C++ code from pre-standard era, where the code itself existed before these rules were written. Generally, with C++, nearly everything done properly using value indirection is done with references. Pointers are for cross-language interaction, legacy code support, and address math. All three are rare in discussions of typical high quality C++. End edit. I wish you would answer the question I asked you about what makes python 100% "abstracted." I've asked it four times now.
Wow... First I had to check if I'd written this post and forgotten about it... This... more this... Yes, Microsoft's bright idea was trying to tie complex components together with visual basic (didn't work), Google's bright idea is trying to tie data together with primitives and domain specific languages. Both are fine, but don't actually solve major problems. The fact that google has had extreme commercial success from advertising isn't to be celebrated, it's to be pitied. They took over a hopelessly clueless market from a class of people who didn't understand it, and replaced it with the simplest set of rules and metrics. It's like trying to compare greek priests with a good neurologist when discussing mental illness. OTOH they did grasp the pathetically obvious fact that correlative data is worth money, which was somehow new to the universe... In the end, all software companies are trying to invent the mystic language that will someday free them from having to depend on actual skilled programmers. MS tried COM/ActiveX (which made things worse imho), Google is trying half a dozen different tricks, and the game industry is hopping from engine to engine, hoping Intel/ARM can pull more magic out of its ass. Thus far nobody seems to have had an original idea in software since 2005 when MS decided calling J++ C# would suddenly make everyone forget what it was in the first place (yes they changed a bit). Business as usual I'd say.
&gt;"I know, I'm probably wrong, I don't know a lot about math, but I don't understand a system where 2+2 is 5." I don't think this is a fair comparison, I'm talking about semantics, you are using a concrete mathematical definition. Actually what if I said to you, lets take the ring: (R, {+}, {x}) where we define x {+} y := x + y + 1, then indeed 2 + 2 = 5. See what I mean about definitions? I also highly doubt Wikipedia pages about programming are written by anyone who doesn't know anything about programming. Note also I'm not asserting that C++ isn't a high level language (or that it doesn't contain high level aspects), I am merely asserting that saying C++ is *higher level than Python* is absurdity. If you can find me one example of a credible source claiming C++ makes Python seem like a low level language I'll eat my words.
Google can't even invent new language *names* .
There is no machine level memory management in Python. There is no machine level interaction at all in Python. Any interaction with the machine must go through a layer written in C, I'm specifically not talking about PyPy here either. Python is literally an abstraction of C away from the machine level. Every single line of Python code runs through the interpreter, written in C.
&gt; I don't think this is a fair comparison Neither would the 2+2=5 guy, largely on grounds that you both operate from the false presumption that you are at least fractionally or conceptually correct. &gt; I'm talking about semantics No you aren't. You're making random vague assertions about constructor keywords, asking stupid questions about pointers, and yakking about the heap. This has all the insight into quality of language design as bikeshedding over whether the names of the fundamental types ought to start with a capital letter. &gt; Actually what if I said to you, lets take the ring: (R, {+}, {x}) where we define x {+} y := x + y + 1, then indeed 2 + 2 = 5. See what I mean about definitions? Yes. You mean nothing. Absolutely nothing. I'm reminded of a guy on IRC named "makk" who would start arguing with being shot down on basic physics by saying "let's begin with a hyperspace bubble where physical constants are different." &gt; I also highly doubt Wikipedia pages about programming are written by anyone who doesn't know anything about programming. Of course you don't. &gt; I am merely asserting that saying C++ is higher level than Python is absurdity. And you have provided no legitimate basis for that claim. Also you have provided a definition of high level which is not only incorrect, but invalidated by your later claims that R and Matlab are high level. You have also ignored the definition of high level as presented verbatim from Knuth. &gt; If you can find me one example of a credible source claiming C++ makes Python seem like a low level language I'll eat my words. I myself have provided such credibility. The burden is on you to show why it's absurd. Repeating that you think it is absurd in italics is not a successful technique. It's time for you to begin to cope with that uneducated opinion driven by wikipedia pages just won't win an argument where the other side has technical arguments, specific examples and Knuth on their side. You'll never eat your words. You could stand under the sky, claim it's red at noon, and you'd never admit you were wrong. Sure, you say "I'm probably wrong," but then you go on to argue, and say "if you can disprove me I'll eat my words." Paying lip service to being probably wrong doesn't mean you have the emotional maturity to actually accept it. Please focus on the question you've now been asked five times, to justify your assertion that Python is "100% abstracted," and to explain what you think abstracted means.
&gt;And you have provided no legitimate basis for that claim. Better double check your replies. Also, I think you claimed I would match you post for post, seems one needs to brush up on their counting. 
&gt; Every single line of Python code runs through the interpreter, written in C. So your assertion is that Python is a better language than C++ because it's 100% abstracted, and your definition of abstracted is being a thin wrapper for C? That's what you're going with? Really? Do you not know what C++ started as?
&gt; &gt; And you have provided no legitimate basis for that claim. &gt; Better double check your replies. I have. I've quoted them to you. You're simply sticking your fingers in your ears. There is no point at which pointing out that you have failed to justify your claims is adequately responded to by you pointing to someone else's text. &gt; Also, I think you claimed I would match you post for post The hell I did.
The fact that there is another programmer out there who understands C++ doesn't actually mean we're all sock-puppets...
Wut?
WTF? Where did *better* come into this? Seriously when did this conversation become about what is **better??** No I do not think Python is a *better* language than C++. Thats like saying a vacuum cleaner is a better appliance than a toaster. You are a strange, strange person. Since when was Python a *thin* wrapper either? It holds you hand a lot, does a lot of stuff behind the scenes. Yes, of course I know that C++ began as an extension of C, but this does not mean it lacks access to machine level functionality. It has that. Python doesn't have that. I'm using your definition, please.
&gt; I'm using your definition, please. No matter how many times you say this, it does not become true.
I'll just let StackOverflow finish my argument for me. I've posted this question to them, but of course its already been asked a dozen times before. [Heres one:](http://stackoverflow.com/questions/3468068/low-mid-high-level-language-whats-the-difference) And I'll quote the top response just to make sure: &gt;Yes, they're just general terms. It's to do with abstraction, and how close you are to what the computer's actually doing. &gt;Here's a list of programming languages ranging from very low to very high level: &gt;Assembly language is at the level of telling the processor what to do; you can't get much more low level than that. &gt;C is a step up from assembler, because you get to specify what you want to do in slightly more abstract terms, but you're still fairly close to the metal. &gt;C++ does everything that C can do but adds the capability to abstract things away into classes. &gt;Java/C# do similar things to C++ in a way, but without the opportunity to do everything you can do in C (like pointer manipulation in Java's case [thanks Joe!]). They have garbage collection though, which you have to do manually in C++. &gt;Python/Ruby are even higher level, and let you forget about a lot of the details that you would need to specify in something like Java or C#. &gt;SQL is even higher level (it's declarative). Just say "Give me all the items in the table sorted by age" and it will work out the most efficient way to carry this out for you.
[I've gotten a few responses over at StackOverflow already, haven't found one which agrees with you yet](http://stackoverflow.com/questions/4524046/can-c-be-considered-a-higher-level-programming-language-than-python) Now contrary to the way in which you want to characterise me, if I am wrong I really do want to know why I'm wrong and make sure I'm correct. But you haven't really given me anything concrete here and all my searching for answers has just shown that I'm, well, right?
&gt; I'll just let StackOverflow finish my argument for me. None of that is correct, of course, and fails to even attempt to answer the question you were asked. Whoever wrote that just likes the word abstract and making lists to feel smart about. Hilariously, three of the errors you keep repeating are right there in that text, yet you still want to use it as an answer. "This one's even higher level, because you get to vague handwave." Sigh. You really have no idea how little about this you actually get correct answers to by learning from blogs, do you? SQL isn't even declarative, for fuck's sake.
Let it go, guy. If you're not going to answer the questions you're being asked, can't stand up for your own assertions, and resort to websites full of amateurs for copy pasta answers to the wrong question, you're better off just finding someone else to talk to. It's unfortunate that people like you cannot cope with the proposition that you've got no idea what you're talking about.
From Wikipedia, again (of course you'll simply dismiss it entirely but for the record) &gt;A high-level programming language is a programming language with strong abstraction from the details of the computer. [Sauce](http://en.wikipedia.org/wiki/High_level_programming_language)
Enumerate the questions I've been asked (and failed to answer), because I've largely been ignoring your tangential ravings and sticking to my singular point: Your claim that C++ is a higher level programming language than Python.
[Feel free to provide any rebuttal you like.](http://stackoverflow.com/questions/4524046/can-c-be-considered-a-higher-level-programming-language-than-python)
I agree with you - subsetting the language until you get your sea legs - but be prepared for a lot of people to become all indignant and holier than thou when you say that. "Then you might as well use C", "When do the training wheels come off?" etc... Now that I'm pretty strong in C++, I try not to be the dick who goes around looking down on people who aren't using virtual inheritance &amp; template metaprogramming. (In fact, I've only used virtual inheritance once EVER...)
I'm glad you spoke up &amp; represented Intel. This guy (in the imgur capture) sounds like a jackass. You sound like the opposite. (Funny aside - when I interviewed w/ Intel right out of school (microprocessor design), one of the main guys I talked to was a total asshat/self-important windbag. There wasn't enough space in the room for his ego and myself at the same time. The other guy was smart, cool, easy to talk to &amp; interesting. When the offer came &amp; it was from the douche, you can imagine my reply. Dude probably couldn't get anyone to accept an offer! The cool guy wasn't named Matt, so now I know there's at least 2 of you.)
I have no interest in watching you tell me I'm wrong, then watching you point me to a web page written by amateur strangers when I ask you to defend your own assertions. Complete failure.
&gt; Enumerate the questions I've been asked I'm bored of this. You have no idea what you're talking about, and you go to any length to avoid admitting it.
No. You have claimed that C++ is a "higher level" language than Python. I called out your bullshit, we've run circles around each other with pointless disputes ever since. Except I provided a number of sources which support my claim, you provided me with none. At this point I say its game, set and match... unless you want to come up with some evidence? Surely if my evidence is so weak, you will have no trouble finding a counter-example to it. 
&gt;I have no interest in watching you tell me I'm wrong You think I am interested in watching YOU tell ME I'm wrong? Hey at least I've provided some links which back up my claims. You have merely stated that I am wrong, and that all the Wikipedia editors are wrong, and all the people who responded at StackOverflow are wrong. So I say to you, if you are so right then it should dead simple for you to provide some links which show people backing your claims. Really at this point you have nothing. 
I'll just add that you are an amateur stranger on the internet for all I know and care. You opinion is no more meaningful to me than those on SO, so on count I make it: Your (undefended) claim, VS Everyone else I've spoken to on the topic. Strangers and colleagues alike. You are out on a limb, and I dare say you've quite comprehensively lost this argument. 
&gt; I called out your bullshit No, you didn't. I justified myself, then asked you to justify yourself. You made excuses, so I gave up. Now you're acting like you're chasing down phantoms. &gt; Except I provided a number of sources which support my claim And I explained the problems with each. &gt; unless you want to come up with some evidence? I have. Learn to read. &gt; you will have no trouble finding a counter-example to it. I gave counter-examples to the things concrete enough to. Most of it is just vague "no i'm right" opinion hand-waving. There's no such thing as a counter-example to you making a list of programming languages and masturbating out an order of which ones you think are "higher level." I gave you almost two dozen cases of concrete errors that you simply refuse to acknowledge, and went on to repeat other places. Now you're saying you called bullshit. You have not identified one single thing I've said as incorrect except, you imagine, that C++ is significantly higher level than Python. Nevermind that I gave a concrete reason for that statement, nevermind that it's ratified by knuth, and nevermind that your failed attempt to call me out on it involved you pretending that you get to give definitions for well defined terms, and then misusing the term again later in a way that negates "your definition." I keep pointing this out, and you keep sticking your fingers in your ears and pretending. R and Matlab, you said, were high level languages, yet you said your definition of high level languages was that they were abstracted, and you claimed that the definition of abstraction was that python gets things from C underneath (which is so hilariously wrong that I don't even begin to know where to start, and completely does not apply at all to R or Matlab.) I get it. You're too stupid to see how wrong you are. That doesn't mean I want to listen to you tubthump how you won. I'm not interested in watching you demand I start giving counterexamples when I've already given quite a few. I'm not interested in you demanding I give evidence that I've already given. I really, really don't want to hear how someone caught in 18 novice errors about a language he's desperate to criticize has "called me out on my bullshit." You don't know what the fuck you're talking about. Stop messaging me. 
&gt; You think I am interested in watching YOU tell ME I'm wrong? Obviously you are. I keep asking you to stop bugging me, and you keep bugging me with more wrong. Stop bugging me. &gt; So I say to you, if you are so right then it should dead simple for you to provide some links which show people backing your claims. I did. Learn to read. &gt; Really at this point you have nothing. Whatever you have to tell yourself, princess. Go put some objects on the heap, or whatever it is you think C++ does. You can't even define abstraction, yet it's the basis of your whole unproven bullshit stain.
&gt; I'll just add that you are an amateur stranger on the internet for all I know and care. I cited Knuth. Go home, amateur. &gt; and I dare say you've quite comprehensively lost this argument. Says the man who's been caught in 18 provable factual errors and keeps screeching about a difference of opinion as "calling one out on one's bullshit." Go back to StackOverflow where you belong.
I knew you couldn't resist. I've never claimed to be anything more than an amateur anyway, I'm a mathematician. Note all I am claiming is that your statement is false, I am not trying to lay claim to the inverse, nor am I trying to disprove anything about C++. Really all it comes down to is you are a fucking idiot if you think Python can be considered low level in any sense. Also note that in every case that I've been wrong, I never made an assertion, I constantly qualified my statements with "I am by no means an expert and could be wrong" etc. I even posed them in the form of questions to actually *ask* you if this were the case. So strong is your bloodlust that you seem to have completely missed this. My only claim, one singular claim is that you are wrong to assert that C++ is "higher level" than Python (oh and you acted like an asshole). Quote Knuth all you like, bring that one out in a room full of programmers and be prepared for an argument, few, if any would come to your defense. This is over, you are so full of shit.
You cited Knuth, and this makes you not an amateur somehow? Shit I'll just cite some Knuth then, that will make me the expert!
&gt;And I explained the problems with each. All you did was claim they were "amateurs". That doesn't hold as an argument where I come from, this is called argument from authority and is a logical fallacy, as you probably know.
You made the claim (that C++ is more high level than Python), you should be defending the claim. I'm am into pure maths, and you think I don't understand abstraction?
&gt;...keeps screeching about a **difference of opinion** My original claim to you was that this is a **semantic argument** and that opinion on the matter will vary (go back through the thread, I fucking dare you). This is what I originally said to you. You had none of it, you insisted this was a clean cut definition. I simply said, given this is a loose definition, I'm going to go with what my friends and colleagues would say about the issue. If you've decided now that its a difference of opinion, then thank you sir, you just admitted defeat. Now see that, hours of you being a complete asshole for what, to concede defeat?? Maybe fucking think a little before you go off on another of your epic tirades next time.
:-D Trolls trolling trolls trolling trolls.
&gt; The problem is not a small language, but **a small mind**... PS3 is way better than XBOX 360! And your moms is fat! Oh, wait. :-) Listen I don't have anything against C++. I'm subscribed to this freaking sub-reddit ... ... I'm just saying ... I like that you can keep the breadth of Java in your head -- eg after a few years programming you shouldn't have too many surprises; whereas C++ ... every day can be a new adventure. I don't think the attitude, Java is for boys, C++ is for men is intellectually strong ... it's just posture and macho bullshit. I'm exaggerating, but hopefully you can see where I'm going. eg maintaining some nutter's Java is going to be a lot easier than maintaining some nutter's C++ They have advantages and disadvantages ... and one of the advantages Java has over C++ is that it's simpler. I should probably move away from the word "smaller" 'cos it's confusing the issue ... You know ... it really would've been nice to have a discussion about the merits and demerits of C++ and Java. But I might as well be in /r/gaming or /r/politics having a pointless X is the best and Y is a piece of shit; NO! X is worthless, you must be a fucking idiot if you think Y is amazing.
Yeah, I know. I needed to call him on it though. It looks like it hit home. Don't get me wrong, I can be an internet jerk as well -- eg if I'm in a bad mood and I need that (empty) warm glow of making something angry on the internet to fill my stony heart -- but these days I reserve it for bigots, or if the topic of conversation is trivial. But ... this guy ... what a waste of time. I tried to engage, but there's nothing there. It's like those abberrant programs that people write to pass the Turing test. It looks and act like a human ... but not a human you would ever want to hang out with.
Fine, but to the points of the language you don't like? DON'T USE THEM! To make an PS3 reference, don't play games that suck. Yes Java is simpler, but being simple in design and being simple in implementation are not identical. C++ requires responsible programming, while Java, well, try not to program after too much LSD. IMHO the sheer flexibility makes it worthwhile, but that's me, YMMV. Saying a more powerful language is worse because it is less "simple" is criticizing it for it's successes (which it obviously has).
Cool, let's distill. I think Java and C++ are both useful. It's important that people program idiomatically in C++. Less so in Java, which is more of a straight-jacket. C++ has a large and powerful and wonderful feature set that allows you to do some amazing things. (I really miss the stack in Java.) There isn't any part of the language I don't like, just bits I'm unfamiliar with/never use/never see in production code, eg [multiple inheritance](http://www.parashift.com/c++-faq-lite/multiple-inheritance.html#faq-25.2). Java is a simpler language -- which has its own intrinsic benefits. eg Maintenance and refactoring are arguably easier. You don't have to look suspiciously at every character wondering, what are you really doing? At the end of the day, I reckon it's okay to like them both equally.
&gt; My original claim to you was that this is a semantic argument No it wasn't, and that's not what that word means besides. All I want for christmas is for butthurt novices to stop screaming about how they won when they're too stupid to see their own mistakes.
You're half right.
I know. |-)
Firstly: &gt;Semantics (from Greek sēmantiká, neuter plural of sēmantikós)[1][2] is the study of meaning. If we are arguing over a difference of opinion then it is a difference of opinion about the meaning of a phrase. Now, the argument, you said this: &gt;1) C++ is a much higher level language than Python. To which I replied: &gt;1) I'm not sure we are using the same definition of high level. What I mean is things like dynamic typing, interactive interpreter, complete automatic memory management. In this sense, I don't think I've ever met a programmer who would call C++ "higher level than Python". But hey, I could be wrong, so I've clarified my wording. And you replied: &gt;A high level language provides facilities not found in the machine. C++ provides many more such facilities than does python. C++ is not low level simply because it is machine-near. After a few stupid comments, you drop this gem (in regards to my definition of "high level", the same definition found on Wikipedia incidentally): &gt;Given that this has been well defined since the 1960s, why you believe you have the privilege of defining this is beyond me. So in other words you claim it is clear cut and inarguable, you deride me for thinking I can come up with my own definition (even though as I said, I used the definition from Wikipedia, and no doubt most CS textbooks). Wow, I only just realised you tried to tell me R and Matlab are not high level languages either. Ok I'm not even going to go there but its pretty fucking clear you do not know what the fuck you are talking about. You have obviously backed off and its so hilarious. I'm not screeching at you I'm laughing my ass off right now. First it was clear cut, rock solid definition "from the 60s" and now its just a difference of opinion huh? Was it just a little more than you could take, seeing so many comments from programmers who think you are out of your mind? Is being wrong so fucking hard to admit for you? I'm laughing my ass off now watching you squirm.
How do I make boost de-fragment the heap space? Oh right, there's a boost::defragment_the_heap_space().
Seems ugly to me...
I agree that the syntax of C++ leaves a lot to be desired, however, any attempt to change the syntax should endeavor to minimize the number of changes and ideally keep the C compatibility. That said, I love the variable definitions, ^ [3] int seems rather more readable than int *foo[3] However, randomly changing pointer from * to ^, comparison from == to = and assignment from = to := seems like random changes to make it look more pascal-like (and sorry, my C brain considers Pascal quite ugly). Also, making type definitions look alike seems like a good idea, but putting all the stuff after the function name and an ugly colon and adding lot of random keywords like func doesn't seem like the right way to do it. Anyway, just my 2c :)
First you need to know what language you are using and what the language guarantees. Since you mentioned "volatile", I'll start with C (**edit:** D'oh! self.cpp. You'll need to dig up a copy of ISO/IEC 14882, though most of the issues here will be similar.) There is a lot of bad code out there which will behave differently both at different optimization levels as well as with different compilers. Unfortunately a lot of developers test in one configuration and assume it will always work under all conditions. You'll want to get a copy of the [standard](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1124.pdf). Some key phrases you'll want to search for include "sequence point", "implementation defined behavior," "unspecified behavior" and "undefined behavior." In addition, you'll want to spend time with your processor architecture documents to understand the memory model (volatile may post read or write transactions which the processor may reorder and may not guarantee immediate visibility in other threads on other cores. Volatile also does not guarantee the order of writes between sequence points.) 
In C++, volatile doesn't have anything to do with atomicity. It's completely useless in that regard. If you're not using atomics, and you're using locks correctly, you have nothing to worry about when compiling in release mode. This, of course, assumes your code is correct. If you're using atomics, and you're using the proper instructions, then it really depends on the architecture and your use of read/write barriers.
Also, this may be useful to you (if you use GCC or Clang): http://gcc.gnu.org/onlinedocs/gcc/Atomic-Builtins.html
Provide: 1. A faster compiler using this syntax 2. Converters to go back and forth between this and the traditional syntax and I'm in.
Using ^ for pointer is fine, but changing == and = is an obstacle... (me going from +1 to neutral. Is there a good reason for it?
Just make a source converter. Then use it as a middleman when you compile. Feed errors back through and transpose line numbers of errors as necessary. :)
Just a note - in MSVC++, volatile does convey atomicity, as a vendor specific extension. Your best bet would be C++0x atomics. Now you only need a time machine and grab them from the future.
Yeah, swapping equality with assignment seems dangerous. Every line of code would have to be converted and you couldn't count on the compiler to catch problems.
&gt; you need to know within reason what the machine language output will be. With C, you can do this. With C++, the statement 'a = b;' could do all kinds of things under the hood. In C++ all operators are customization points. Thats does not make C++ bad for kernel development. On the contrary. You could overload your operators to produce the most efficient machine code for each type, instead of cluttering your code with ugly C style type switches everywhere. You can still look at the generated assembly if you don't dont know what you are doing or dont trust the compiler. When it comes to software development, whether kernel or applications, the high level algorithmic time complexity matters orders of magnitude more than the cost of any individual step. With C++ its easier to separate high level code from low level code, and change one of them without having to change the other. With C its all detail, and the high level is lost between the lines. You cant as easily change the high level without changing the low. What is that saying about not seeing the forrest for the trees. Well thats C (and assembler) for you. The real reason C++ is not used much in linux/unix development is cultural. C++ did not exist when its foundations where laid. Its a shame Linus and co are stuck in their old C ways, they are missing out. 
Saw Damian Conway's name attached, immediately stopped reading. Are we really taking syntax suggestions from Perl folks?
Head asplodes. 
This is ugly
Should say "for Windows" in the title :)
Only part I actually agreed with were the "type x : class { y }" declarations. Rest seemed like it came from someone who had never actually understood C/C++, and the theory behind its syntax.
It's just a troll post... An Intel employee says... who the fuck is that employee? he could be a janitor (SOAP developer)
I've seen the proposal to change the * into a ^ a long time ago and AFAIK it will only benefit compilers (something a bout easier parsing - don't take my word) But still, I rather have it * and not ^
I do the following: struct Id { int id; }; struct Pos : Id { double x, y; bool operator &lt; (const Pos &amp;p) const { return id &lt; p.id; } }; std::set&lt;Pos&gt; positions; Id id = {1}; positions.find(static_cast&lt;Pos &amp;&gt;(id)); 
You can't do this. Id isn't a Pos. static_cast should generate a compiler error. Also, your &lt; operator isn't consistent with Pos, but with Id. Sure, in this example, it may be ok, but in general, such an operator would be inconsistent with the nature of the object. There are other problems with this, but they are more related to coding style. I don't want this kind of code anywhere near my codebase. Sorry. 
Yeah, it is a hack, but it works. And no, static_cast will not generate a compiler error. I don't want it either in my code base, but until TR2, we don't have much chance for anything else. 
a boost::signal instance is 192 bytes, last time I checked...
&gt; Clay has a special generic type Static[x] without a clear analog in C++. When used as a runtime value, a Clay function or type name x manifests itself as a value of the type Static[x]; for example, Int is of type Static[Int] and main is of type Static[main]. Integers and floats can also be introduced as static values using the static keyword: static 0 gives a value of the type Static[0]. A value of any of these types is empty and indistinguishable from any other value of the same type. Static values with their unique types allow compile-time constructs to be available as runtime values, and allow functions to be overloaded and specialized on numeric parameters using the unique types. Did this make sense to anyone?
I read through to the end - it means the value is propagated at compile time.
Seems like they just let you associate some value with a type. I do a similar thing in C++ with a template and a static member function that returns an unsigned int. When called the first time, an id is computed for the type. Every subsequent call to the id function of the template yields the same id.
Anyone have any experience using the tools (GCCXML / genreflex and CINT)? Are they robust? If that works really well then I'm all for it, but if it doesn't count me out. Also interested in some of the implementation details (performance characteristics of their instance creation calls, etc) but I can figure that out for myself and update my post.
taken back.
TL;DW: It's about type_traits and elementary meta-programming.
You can get to the content with the menus at the top of the page.
I was able to spend more time there after lunch. There actually is a lot of content there, and the explanations were very clear. The site design could use a lot of work, and I did read some articles that appeared to be halfway done. Overall, this looks like a quality resource. Changed my vote from down to up, and recanted my first comment. 
Its a bit annoying to read a paragraph at a time through some of the articles. But there is some good info, especially in the [Parallel Computing] articles, which are basically several case studies in writing a scalable program to do X.
Final video in his Intro to STL series. He says he'll be creating more in an advanced series :)
After seeing [Go's syntax](http://golang.org/doc/go_for_cpp_programmers.html) I find almost every other programming language's syntax below par.
Do I misunderstand or do you want an std::remove which takes a single iterator instead of a value, like this? #include &lt;algorithm&gt; #include &lt;vector&gt; #include &lt;iostream&gt; #include &lt;iterator&gt; template&lt;typename BiIt&gt; BiIt remove( BiIt where, BiIt e ) { *where = *--e; return e; } int main() { int a[10] = {1,2,3,4,5,6,7,8,9,10}; std::vector&lt;int&gt; v(a,a+10); v.erase( remove( v.begin()+5, v.end() ) ); // alternatively I suppose "simply": // v.erase( (v[5] = v.back(), --v.end() )); // or properly simply: // v[5] = v.back(); v.pop_back(); std::copy( v.begin(), v.end(), std::ostream_iterator&lt;int&gt;( std::cout, " " ) ); } Output: 1 2 3 4 5 10 7 8 9 
If you will look into the STL implementation the answer will be revealed (I looked at vs2010). The operation of the insert goes as followed (considering you have enough memory already allocated!) STL push_back your value to the end of the vector (emplace_back) and than "slides" (they call rotate) the value back to the desired place. And that's why it's O(N) you need to move all the values after the desired location to make room for your new value. The O(n) is a worst case, if you wanted to insert it into the last position in the vector than you will get a O(1). Think of it how can you push a value into a place that is already occupied by a different value? If you still want a insert of O(1) in a contiguous memory container then use list.
There is an ~~unordered~~ unsorted O(1) insert/remove container. Its called list. It does not invalidate any iterators on insert/remove (of elements even in the middle of the container). ~~Contiguous~~ ordered memory containers on the other hand cant have O(1) insert/remove by definition. If you remove an element in the middle you need to fill the gap by shifting down elements, otherwise the container would no longer be ~~contiguous~~ ordered. Think about it for a bit. If you changed the implementation of a vector to not do this shifting.. you would end up with "holes" in your container. You would have to have pointers to bridge the gaps. So you could no longer do random access lookup, as in iter+2. You would have to do ((iter++)++). As in you just turned indexed lookup from O(1) into O(N). And then you pretty much would have implemented a list. You cant have your cake and eat it. You have to choose. Do you want O(1) indexed lookup, or O(1) insert/remove? Actually you can have your cake and eat it with hash tables (in c++0x or boost). It has (amortized) O(1) lookup and insert/remove. Pretty neat. Edit: Fixed some unfortunate choice of words. Thanks rabid cow and sztomi and all. So if you want an unordered collection you could use hash tables (aka std::unordered_set). If you wanted a contiguous memory container like vector to also be an unordered container, well that's a bit wild. You could no longer use indexed lookup (or random access iterators), because unordered collections don't have (ordered) indexes. Say you had a vector and always did inserts to the back, and whenever you removed any element you would first swap it with the last element and then do a pop. This would mean that it would become practically impossible in general to still use indexes to identify a elements. As in elements that happen to be the last element in the container would move around in the container and have different indexes/positions at different times. This could happen many a times, if the collection grew and shrunk often. It would be hard for the programmer to keep track of the changing indexes of the same element. Lookup would practically be a linear search instead, as in O(N). And then you are better off with hash tables.
&gt; "As in you just turned indexed lookup from O(1) into O(N)" And you would have to allocate 2 times? 10 times? more memory than requested... Listen to IAskYouQuestions... he knows the answers :)
You can do O(1) erase with a vector/contiguous memory if order doesn't matter. I keep a whole bunch of utility functions in a header file and the one I use for Erase is as follows: //! Erases an element in a vector. /*! \param container The vector to erase the element from. \param location An iterator to the location of the element to delete. */ template&lt;typename T&gt; typename std::vector&lt;T&gt;::iterator Erase(std::vector&lt;T&gt;&amp; container, typename std::vector&lt;T&gt;::iterator location) { typename std::vector&lt;T&gt;::difference_type index = location - container.begin(); if(location != container.end() - 1) { std::swap(*location, container.back()); } container.pop_back(); return container.begin() + index; } Basically you just swap the element you want to erase with the last element and then do a pop_back.
Why the down vote?
&gt; It has O(1) lookup and insert/remove. You are confusing the big-O-notation (which is used to denote worst case complexity) with **average** complexity. Actually that constant is around 1.3-2.5 for a &lt; 80% load of the table, depending on the choosen implementation. The worst-case complexity of hash table operations is O(n) or O(n/M). This is not a problem in most business use-cases because it doesn't matter if some items are processed longer than others, what matters it the average time of the operations. For this, nothing beats hash tables. But it does matter in real-time applications where you need to be sure about the time of the operations. In that case, binary trees or heaps might be a better choice because they offer better worst-case complexity than hash-tables.
Yes, I could have written amortized constant time instead of O(1) for the hash table lookup. But wanted to keep the answer simple and to the point.
Thank you. I've launched the site basically a week ago. I agree that a lot can be improved, I am working on it. And much more materials are coming.
Usually I just implement this on my own as an unordered, unrolled linked list.
Thanks, thats quite useful (if the order does not matter, and one is aware that any iterators to the last element will still be invalidated). 
That's ok, I hope I didn't sound offensive (that was not my intention at all).
No worries. Your distinction is correct, and might be interesting to others.
IAskYouQuestions pretty much covered it... it's not clear that you actually want/need your data stored in contiguous memory, though if you do then I must admit I'm puzzled about what you could possibly have written that fits your requirements: &gt; Anyone know why the STL does not provide a dynamic array with these performance characteristics? I've obviously written my own, [...] A deque provides O(1) insert and removal, but *only* at the beginning or end of the container (push_front, push_back, pop_front, pop_back). And it's not guaranteed to be contiguous. An [unordered_set or unordered_multiset](http://www.boost.org/doc/libs/1_45_0/doc/html/unordered.html) (though only available in the C++0x STL, or [TR1](http://en.wikipedia.org/wiki/C%2B%2B_Technical_Report_1#Hash_tables), or Boost) would provide what you request in the headline (an unordered container with O(1) insert/erase, as well as O(1) lookup), but it wouldn't be contiguous. And I've now realised that you didn't mention any requirements re: lookup... and in fact this bit: &gt; It is very often more desirable to take the thing at the end of the list and put it in the spot of the removal. ...seems to suggest that lookup *can't* be important to you, because if you're meeting your needs by implementing a vector where "removing" an item in the middle of the container essentially involves transplanting the last item into the location of the removed element and decrementing the container size, that means your only option for lookup will be a linear search, ie. O(N). But that seems to suggest your *effective* removal cost would also have to be O(N), as you'd first have to find the element you want to remove! And of course you'd only be able to do O(1) insert at the end of the container (ie. with something exactly equivalent to vector's push_back operation, assuming enough space has been reserved). Or have I completely misunderstood what you're trying to achieve (and how)? :-) Would you mind giving us some detail on the problem that you're trying to solve? It'd probably help a lot to understand why you want the kind of performance characteristics you're describing.
If you only want to keep values and the order is not relevant... [unorderer_set](http://publib.boulder.ibm.com/infocenter/comphelp/v9v111/index.jsp?topic=/com.ibm.xlcpp9.aix.doc/standlib/stl_unordered_set.htm) or unorderer_multiset &gt;The template class describes an object that controls a varying-length sequence of elements of type const Key. The sequence is unordered. Each element serves as both a sort key and a value. If you have an optimal hash function, the number of operations performed during lookup, insertion, and removal of an arbitrary element does not depend on the number of elements in the sequence. Moreover, inserting an element invalidates no iterators, and removing an element invalidates only those iterators which point at the removed element. 
While I agree with your metrics, I disagree with your view of hash tables. Once the worst case is hit, hash tables stay degraded, while many or most other data structures have means of recovery. While there are double hash for this reason, IMHO O(log n) all the time is often preferable to O(1) some of the time, but to me determinism is important. Btw you work at google don't you? Interviewed there for a systems programming job, had 5 java ppl ask me about my how I would design a data structure for a certain purpose. And to please state my answer in the form of a hash. Funny
Edit: alien blue doesn't let you delete, WTF?
&gt; You can do O(1) erase with a vector/contiguous memory if order doesn't matter. Exactly. You can do this with an existing vector. That's why there isn't a separate structure.
&gt; makes the vector class terribly inappropriate for dynamic arrays of large-ish objects in large numbers. That's correct, if you need a large collection of large-ish objects you're probably better off with with `std::vector&lt;std::tr1::shared_ptr&lt;T&gt; &gt;` or a [`boost::ptr_vector&lt;T&gt;`](http://www.boost.org/doc/libs/1_45_0/libs/ptr_container/doc/reference.html) than with `std::vector&lt;T&gt;`. Neither of those provides for contiguous memory, of course, so if that's a deal-breaker then you'll need a different solution. &gt; I've obviously written my own, but the STL's implementations are more robust than mine. And I hate that vector is so useless. I hope you've written your own in terms of `std::vector`. It still provides useful services for managing your chunk of memory (it can perform construction, destruction, allocation, reallocation, and deallocation). It's not clear what access operations you'd require -- if you want to avoid performing an O(N) find operation for each element access, then an auxiliary indexing structure would do the trick (at the cost of either O(log N) access and update or O(1) access and O(N) update -- though those update costs are in terms of `std::vector&lt;T&gt;::size_type` rather than `T` itself.
&gt; Btw you work at google don't you? I wish :). This kind of determinism can be important for many applications, but I don't think it's always necessary. If you don't even have a chance to reach degradation and it's okay to perform well only on average (not for each item), hash tables are clearly better than any O(log n) data structure.
Vector is a *sequence* container - relative order of elements matters. It is absolutely trivial to do this yourself: *iter = v.back(); v.pop_back(); 
Big-O isn't inherently about worst-case complexity. The distinction between Big-O, Big-Theta, and Big-Omega is about the tightness and direction of the bounding function -- it has nothing to do with what the function you're talking about is modeling. Big-O is defined completely as: f(x) is in O(g(x)) iff there exists a c and k such that f(x) &lt;= c*g(x) for all x &gt;= k. That's it. Whether f(x) denotes the number of operations needed for some algorithm to complete on average, in the worst case, in the best case, or something completely different like the number of grains of sand you can hold in X cubic meters has nothing to do with the definition of O(n) notation.
Let's see your implementation, unless your employer owns it
1) std::swap(vector.back(),vector[delete_this_one]); vector.pop_back(); 2) vector.push_back(add_this_one);
&gt; You are confusing the big-O-notation (which is used to denote worst case complexity) with average complexity. uh... "big-O" notation is an *asymptotic* notation, it just means "as n gets large, the influence of details (constant multiples or additions) disappear" -- totally orthogonal to whether the structure of the inputs to a computer program are structured in the best, worst, or average cases with respect to a given algorithm. you could apply asymptotic notation any number of things not algorithm (and not algorithm-specific input structure) related. I think the idea you are after is [amortized complexity](http://en.wikipedia.org/wiki/Amortized_analysis).
You're trading O(1) look up (by in-order index) for O(1) insert. That's fine, but neither is that a vector[1] nor a particularly common requirement. It's also unclear why you need contiguous memory and can't use list, or why you can't use a vector of pointers. Either you're using vector wrong, or you've got a fairly specialized application with would probably benefit very well from a optimized custom container. [1] a vector is an in-order list elements with implied dimensionality -- think about it, it doesn't make a lot of sense of X axis could be arbitrarily swapped with Z, or the it didn't matter where R was in relation to G or B.
You can try http://rosettacode.com
Haha, later the day I figured someone will notice that - as two of you did. Yes, I know the definition very well, but thanks :). Anyway, this became a pretty informative discussion, this is why I love reddit.
Yes it is, thanks for correcting me. I'm not a native English speaker, and the statement used for this in my language mirror translates to "average complexity".
That's exactly what I do all the time, Kranar.. except I don't use a vector. I think that I'll experiment with wrapping a vector (probably by value) in a new class that has a more limited interface than vector and implements erase in this way. The explanation that it doesn't exist in the STL because it can easily be added to vector is a reasonable one, thanks!
Ugh, I've written a reply and accidently deleted it twice... here goes, attempt number 3. See Kranar's post for the implementation that I'm looking for. Does that clear up why the "insert" and erase operation would be constant time? The O(N) lookup is common to vector, after erase all pointers to objects beyond the point of erasure are invalid and need to be "fixed up". The solution to this problem (if you need to cache pointers to things in a contiguous memory container) is a handle system. When an object is moved it updates an intermediate pointer that points to the moved object (formerly its old location, now its new location). This is something common to any contiguous memory container, as the container will reorder its elements when certain operations are invoked. As for how I use it, here's some pseudocode: FastInsertContigousMemoryContainer container foreach thing i in container i.DoSomething() if(i.DidSomething()) container.erase(i) By using a contiguous memory container cache performance is made ideal. Even the order that we're using elements is ideal, if we want to get really fancy it's possible to force a cache miss on the next element while doing something with a given element and limit the chance of a cache miss further. Hope that all made sense.
Lookup is the same as vector, just as in vector some elements move after erase or insert.
Sorry for the confusion, but I would not want to keep a key value pair for the things in my container, just a value. If things need to access things in the container they will cache a handle to that object.
Yeah, that looks like what I'm doing. I don't usually use iterators in that way.
Yeah, I want the contained objects to remain in a contiguous block of memory for fast iteration or easy DMA. And yeah, I usually implement a handle system (one more pointer in the contained objects, a few more write operations on copy/destruction) to allow things to cache references to objects in the container.
If you think you've invented one of these, you've misunderstood the performance of your own container. The problem is deletion. Excepting the case of deleting from the end of the container, you either have to remove the deletion then (causes shift time,) or keep track of deletions (causes scan time on insert.) The STL doesn't have the container you want because it isn't possible. The closest you can get is to trade tail with the item you want to remove, then remove new tail. That isn't o(1) either, of course; it's amort. o(lg n) insert, because you have to scan to find the thing to be removed.
You have clearly misunderstood the original poster's request. He didn't say "I need a container where I can remove the last element cheaply."
Then the correct approach is a map of pointers into a block allocated with placement new. Please ask the question, instead of what's wrong with your answer.
set and multiset do not keep key value pairs - those are map and multimap. Your protest makes no sense. You're clearly not ready for making containers. You've been making garish o(foo) errors all over this article, don't know what the existing basic containers do, and have missed that the existing basic containers working with placement new do everything you actually need.
&gt; See Kranar's post for the implementation that I'm looking for. Does that clear up why the "insert" and erase operation would be constant time? Kranar's post only thinks those are constant time because he forgets to measure most of the work. Try benchmarking his strategy, and watch in awe as clear, obvious log growths emerge. &gt; By using a contiguous memory container cache performance is made ideal. This is, of course, complete crap. Go look up splay tree caching.
And this is why you will never write acceptable performance software.
Yeah, because why would you ever consider the costs outside this function? Clearly we know from where to delete without looking in an unordered container, so a swap doesn't need an iteration, and is therefore a meaningful o(1) erasure mechanism. *cough*
&gt; The explanation that it doesn't exist in the STL because it can easily be added to vector is a reasonable one It's also incorrect. When you benchmark this thing you're so happy about, you're going to see log growths in it. The explanation is "because you can't make work disappear like that."
Well ... that's what they're for. Maybe stop replacing the language until you understand it.
Sorry can you explain what you mean? Try not to use sarcasm, I know it makes you sound snarky and witty but as someone genuinely trying to understand your criticism (if that's what it is), I can't make sense of it. If there is a flaw in my implementation I would be more than happy to know what it is for my own good, rather than just for the sake of a pissing contest on the Internet.
&gt; Try not to use sarcasm This is why I'm not explaining. If you'd like help without telling the person donating their time to you for free how to behave, let me know. The correct spelling is "thank you."
Alrighty.
So basically, you'd rather be wrong and helpless than say "thank you?" Classy. Really.
Ah yes, downvote the person explaining to you why the advice you're getting is wrong, after asking them for help. Truly, you're a decent man.
&gt; There is an unordered O(1) insert/remove container. Its called list. List is ordered. You can use it in place of an unordered container, but that doesn't make it unordered. &gt; Contiguous memory containers on the other hand cant have O(1) insert/remove by definition. They can if they're unordered: "take the thing at the end of the list and put it in the spot of the removal." (vector already has O(1) insertion if you don't care about order.) &gt; you would end up with "holes" in your container. No: "take the thing at the end of the list and put it in the spot of the removal." &gt; As in you just turned indexed lookup from O(1) into O(N). Unordered (non-associative) containers don't have indexed lookup at all. Think about how you'd define an index. &gt; Actually you can have your cake and eat it with hash tables (in c++0x or boost). It has O(1) lookup and insert/remove. with worse constant factors than necessary.
You sound so calm and patient. Is it because you're thinking "next time, ask on StackOverflow"?
I think he's assuming you would be doing Erase(vector, std::find(vector.begin(), vector.end(), value)); instead of iter = Erase(vector, iter); Where in the first case it would still be O(n) because you have to search for it but saying that would also imply that erasing from a list is O(n) also.
This removes the value at iter. It does that by inserting the back value into the value to be erased and then removing the back.
&gt; It's also unclear why you need contiguous memory and can't use list Contiguous memory can give you much faster access times while iterating over the vector. Most of the cases where I use a vector I could also use a list just as well.
std::unordered_set is usually not contiguous.
when he mentions cache here he's discussing CPU cache, not some look-up cache. The reason it's faster is he isn't talking about randomly accessing the container, he's talking about iterating over it.
I'm aware of that. Iter had to be gotten somewhere. We've all seen that code snippet before. It's in every decent beginner's c++ book, and on every c++ irc quote bot. Yet again: benchmark it in active use. Pushing the work out of the two things you're narrowly measuring doesn't eliminate the work, only the correct measurement thereof.
And? All CPU caches are modified splay trees. Go look it up, I already said. &gt; The reason it's faster is he isn't talking about randomly accessing the container, he's talking about iterating over it. Splay trees are just as iteratable as vectors. You can keep saying that iterating is free, but that won't make it true. Maybe you should just benchmark it and watch the lg(n) time in the code you're certain must be linear. Then you can come back and ask why, and instead of arguing, you can lightbulb instead.
&gt; List is ordered. Fair enough. I meant to say list is an unsorted container (sorry if anyone got confused). As in the *values* of the container elements are not in order. I did not mean unordered in the sense that there is no order among the elements (implicit) *key*'s (its position/index whatever). &gt; They [contiguous memory containers] can [have O(1)] if they're unordered You are right about that. The special case of unordered/unstable remove. Inserts are still O(N) and ordered/stable remove is still O(N) though. &gt; Unordered (non-associative) containers don't have indexed lookup at all. Again I meant to say unsorted, see above. And unsorted list could have indexed lookup and that would be O(N). &gt; [hash table lookup have] worse constant factors than necessary. I could nitpick your choice of the word *necessary* there. But I assume you meant it is amortized constant time, not worst case constant time. 
&gt;Go look up splay tree caching Splay trees do not exibit the behavior the OP asked for. Insert is best case O(ln(n)) and worse case O(n) &gt;You can keep saying that iterating is free, but that won't make it true. In most (all?) cases where I call the erase function on a container I'm iterating over something like this: for(auto i = container.begin(); i != contianer.end(); ++i){ // Do something with *i if(i-&gt;IsDead()){ i = container.erase(); } } so iteration is happening whether I erase or not.
Ah okay. Yeah for sure it's O(n) in that case, but you still gain a constant speedup. In general I try to design in such a way that I don't have to do that kind of a lookup. Usually I'll be iterating through the vector anyways, and I'm trying to filter it or perform an operation on every element or possibly removing it. That's why I have the Erase function return an iterator to the next element. But anyhow, thanks for clearing that up.
I agree, 99% of the time I call erase it's like for(i = container.begin(); i != container.end(); ++i){ i-&gt;DoSometing(); if(i-&gt;IsDead()){ i = container.erase(i); } }
Correct, except for one subtle bug which you probably know about, but I remember getting stung by it. You do not want to increment i if you do an erase, because you will be skipping over an element. It looks uglier, but the correct way would be: i = container.begin(); while(i != container.end()) { i-&gt;DoSomething(); if(i-&gt;IsDead()) { i = container.erase(i); } else { ++i; } } 
I disagree, simply because log(n) is actually bound mathematically (compared to the domain experienced in computers). The key to hash tables is having an algorithm that is effectively guaranteed to minimize collisions. The ones I have seen have not been that impressive, coupled with the fact that an underused hash-table is a large overhead (memory), and an over-used hash-table is a collision-fest. So it has effectively more complexity outside of those bounds. A good log(n) can outperform it at the low-end, while still scaling well to the high end (unless custom designed for the application, most hash-tables can't handle extremely high datasets efficiently). In comparision, log(10000) is still only 4 times the complexity of log(10), while in a well designed algorithm, the constant costs can be amortized. Of course, in a very large dataset, I think both are inappropriate and a more complex system that segments the data (hashes into sub-datasets) works best. I just can't see the value of a hash-table by itself, at the low end, log(n) is rather efficient, at the high end, more exotic structures are needed. There is a small band in between, which, if you can guarantee your structures will never exceed that amount, a hash-table is useful. OTOH, a complex system of parallel hash tables each working in parallel seems cool to me, any time one comes close to being full, you make another, but this effectively requires parallelism that is only catching on now (and only had processing power available for lately).
yep, been stung by that a few times.
&gt; &gt; Go look up splay tree caching &gt; Splay trees do not exibit the behavior the OP asked for. I didn't say they did. Indeed I said the behavior OP asked for, when you account for the cost getting pushed around, isn't possible. I brought splay trees up as a counter to the assertion that caches should be linear for speed, when in fact they're nearly always splay trees, for speed, because the thing you two want to do - here's where it gets fun - *isn't* *possible*. If it was, all CPUs would be built that way. &gt; In most (all?) cases where I call the erase function on a container I'm iterating over something like this: Then you've identified a way to improve.
&gt; I did not mean unordered in the sense that there is no order among the elements (implicit) key's (its position/index whatever). Ok, but that *is* what "unordered" means in this context. &gt; Inserts are still O(N) and ordered remove is still O(N) though. Sure, but the whole point of this submission is the unordered case. And in case you missed it, always inserting at the end is perfectly sensible for an unordered collection, and that happens to be O(1). &gt; &gt; Unordered (non-associative) containers don't have indexed lookup at all. &gt; &gt; Yeah, thats kindof the point. A List in theory could have indexed lookup but it would be O(N). That's not what I mean. There is a sensible definition of indexed lookup on lists, because they are ordered. There is a defined *first* element, *second* element, etc. This is not true for an unordered collection. There is no indexed lookup at any complexity. Indexing is based on order. But I suppose that's too strong... You *can* define indexing on an unordered data structure, but the index is meaningless, so it isn't terribly useful. &gt; &gt; [hash table lookup have] worse constant factors than necessary. Sorry, not lookups. Mainly insert and iteration. &gt; I could nitpick your choice of the word necessary there. But I assume you mean it is amortized constant time, not worst case constant time. Not at all. It gets tedious to go around inserting "amortized" everywhere. I mean "necessary" in the sense of what OP wants to do. Clearly O(1) set membership isn't needed, so any cost to achieve it isn't necessary. Oh, and if you want to mutate elements in-place, hash tables are out.
Another thing this article should touch on is with boost::fusion you can even iterate over a tuple.
You shouldn't be down-voted. It's very true statement, but there's some reasoning behind it. You're blunt, but bring up a great point. There is a very small percentage of people that utilize the STL correctly. What people don't get is that some of the most brilliant minds constructed the STL and a lot of less brilliant people judge it. Iterator patterns seem to be very neglected initial topic and it's sad because they're (arguably the largest) foundation of the STL. 
What makes you assume it was Kranar downvoting you? There *are* other people reading this discussion - and it's quite clear that you weren't being helpful, you were just being a sarcastic prat. Kranar was actually quite polite in asking you to clarify what you meant before giving up and getting actual help elsewhere. (I'm not downvoting you, but mainly because it's not really necessary.) 
&gt; You shouldn't be down-voted. Welcome to Proggit, where data driven assertions are downvoted by angry fanboys who want to be right, according to whatever fanboy yells the loudest. I wouldn't care, except that it periodically interferes with my ability to post.
&gt; What makes you assume it was Kranar downvoting you? Timing. Several posts in a row. I could be wrong; people accuse me of downvoting when I haven't. This is part of the pox that is Reddit's downvote system, as well as Proggit's inability to cope with criticism. &gt; and it's quite clear that you weren't being helpful, you were just being a sarcastic prat. And this is where I stopped reading. It's not my fault he didn't understand what I wrote right in front of him. In the meantime, the help he's gone off and gotten elsewhere is still wrong. He may feel justified, sanctimonious, correct and like a victim, but instead of admitting humbly that he was in error and thanking the "sarcastic prat" for teaching him something nobody else here could, he chose mediocrity and comfort over improvement and humility. I get the strong impression you'd do the same. Ever wonder why meritocracies are always so sarcastic?
&gt; I could be wrong; people accuse me of downvoting when I haven't. I'm not surprised. It's a common accusation on Reddit. &gt; This is part of the pox that is Reddit's downvote system, [...] Would you prefer Reddit up/down-voting not be anonymous? It's actually an interesting idea... I wonder how it'd affect the environment if users had to actually put their identity behind their vote. &gt; as well as Proggit's inability to cope with criticism. &lt;shrug&gt; I don't think Proggit's any worse than Reddit itself in that respect, or humanity in general. &gt; And this is where I stopped reading. It's not my fault he didn't understand what I wrote right in front of him. Actually, it kind of is. It's a matter of communication. Sure, *you* knew exactly what you meant... it may well seem excruciatingly obvious to *you*. But that's not necessarily the case for anyone reading what you wrote, which is why it's worth making the effort to write clearly and unambiguously (at least when you're trying to explain something). Kranar was quite clear that he/she/it *wanted* to understand you, but couldn't make sense of your words. This is completely unsurprising - it happens all the time in human communication, especially on complicated technical issues. And I actually agree with him - your [original post](http://www.reddit.com/r/cpp/comments/et0i6/why_doesnt_the_stl_have_an_unordered_alternative/c1aqypw) *was* difficult to comprehend. Especially the sarcastic bit "Clearly we know from where to delete without looking in an unordered container [...]" - as it turns out that that's actually **true** in Kranar's (and beguiledfoil's) case! Kranar *didn't* need to do a lookup of an element by value, as the use case in question was the same as the OP's - iterating through an entire container and executing something on every element, followed by an erasure-test (as per the OP's [followup here](http://www.reddit.com/r/cpp/comments/et0i6/why_doesnt_the_stl_have_an_unordered_alternative/c1aqsqk)). Sarcasm just gets in the way. It may be clever, it may even be funny. But it gets in the way and it makes comprehension more difficult - especially when it turns out the sarcastic person has misunderstood something. &gt; In the meantime, the help he's gone off and gotten elsewhere is still wrong. Really? Can you be specific? &gt; [...] but instead of admitting humbly that he was in error and thanking the "sarcastic prat" He didn't *know* he was in error! This is the whole point, he gathered that there was something he may have misunderstood, but when he expressed a genuine interest in finding out what, you refused to help! &gt; [...] for teaching him something nobody else here could, Phew. What a relief that *you're* not suffering from any self-esteem issues here. :) 
&gt; Would you prefer Reddit up/down-voting not be anonymous? No. I would prefer the abolishment of the up/down system in favor of a reason flag like Slashdot figured out ten years ago. That way people could express disagreement without affecting score, which is appropriate, and people could set preferences how to handle each kind of reaction. Unfortunately Reddit's broken wilson confidence interval floor won't allow for this, and Reddit's staff can't handle current IO, let alone anything even slightly complicated, so that's not going to happen. &gt; &lt;shrug&gt; I don't think Proggit's any worse than Reddit itself in that respect We disagree. &gt; Actually, it kind of is. It's a matter of communication. If a man doesn't know what a foot is, you have no hope of explaining a shoe to him, no matter your level of clarity. &gt; your original post was difficult to comprehend. Then he could have reacted politely and requested clarification. Since he didn't, but instead raged and demanded an explanation with a very suspiciously timed possibly coincidental downvote, I said "if you want to know, be less of a dick." That response was also coincidentally timed with a downvote, so I waited a few minutes and poked the beehive again, which netted still another coincidentally timed downvote. Satisfied that I had a clear picture of what was happening, I moved on. I don't particularly care if he gets it. &gt; Especially the sarcastic bit "Clearly we know from where to delete without looking in an unordered container [...]" - as it turns out that that's actually true in Kranar's (and beguiledfoil's) case! No, it isn't. They're log amortizing an iteration. I explained this. &gt; Sarcasm just gets in the way. Did I somehow accidentally give you the opinion that I was interested in a lecture on personal behavior from a stranger? &gt; &gt; In the meantime, the help he's gone off and gotten elsewhere is still wrong. &gt; Really? Can you be specific? I suppose I could, but that would involve my going and looking, and as you know, when I feel that someone is being rude to me and asking me for information, my inclination is to tell them to change their tone first. &gt; He didn't know he was in error! He did by the end of my response, which preceded his retort. &gt; but when he expressed a genuine interest in finding out what, you refused to help! No, I didn't. I told him first he'd have to change his tone. He wasn't willing to do that. &gt; What a relief that you're not suffering from any self-esteem issues here. :) Facts are facts. Nobody else at that point had. Could doesn't mean possible. It means "from among the set of attempts already made, succeeded." To point out that any given gold medallist runner ran faster than anyone else could doesn't mean that for the rest of forever nobody else ever will. When I wrote that, as now, there were no other correct understandings of the growth complexity of what was being asked. I note that you're just as sarcastic as the person you're complaining about the sarcasm of. Guess the rules don't apply to you.
You're being contentious. Automatic prefetching only works on contiguous and sequential access. This is a speed boost above and beyond regular cache use that other memory arrangements cannot achieve. Also, he's not improving anything. He's showing you an example where he's using a linear algorithm where erase is invoked and is constant time. 
&gt; That's not what I mean ... Yeah, I realized I miss-read you just after I responded. So I went back an edited my response, but seems I was not quick enough. On the main question I guess I see what you are saying. I read and wrote unordered as if it meant unsorted, while the OP used unordered in the correct sense. So that potentially makes my first answer to the OP useless. Oh well. I'll add some edits.
Why the check for the case you are erasing the last one? If your objects are heavy enough that swapping them with themselves takes a significant amount of time then std::vector likely wasn't the right data structure for them to begin with. And for all other types it might very well be that the cost of all that pointer arithmetic to check if it happens to be the last one will have a larger impact on your runtime. Especially because that check happens in every call of your Erase function.
Why can't you search the set with just the id. For instance, using std::find_if you would just define a predicate: struct PosIdPredicate { PosIdPredicate( int id ):id_(id){} bool operator ()( Pos const &amp; val ) const { return ( id_ == val.ID ); } const unsigned int id_; }; Also if all your objects shared the convention of the ID being an int named ID you could simply template the predicate and it would work for all of them.
Who said anything about automatic prefetch? 
indeed -- that point is kind of obvious. so it the truism that "optimizing" before profiling is a fool's errand. unless you're relying on the memory layout for some other part of the code (possibly a dangerous thing to do), implement it as a set or list or vector of pointers. come back when the functionality is complete, profile, and if your "unordered contiguous memory set" is in a hotspot, implement one of the many optimizations mentioned here. either way it's an implementation detail, and it's no wonder the STL hasn't seen fit to provide for the OP's specific need.
This function is O(1), normal STL erase is O(N). I do not understand what your point is.
I mentioned it in the post you replied to earlier in this thread. See the bit about "forcing a cache miss"..
If I allocate all of the objects in a block how do I fix up that block after an object is deleted? The operations required would be exactly those of a vector of my objects.
the iterators after the removed element are in-validated, but the relative order of the elements remains the same. for example, if I have a vector of R, G, B, A; then alpha always proceeds after blue after green after red. If I project out blue, then the vector is mutated, but that relation remains stable -- I can always find all elements in column A in constant time (as long as I haven't removed it). if you do what you're suggesting, A can only be found by searching, and even then you'd have to have a specific value to A to know when you've got it. basically you've assumed in your mind a vector is a *set* of things laid out contiguously in memory, and you won't be dissuaded. best of luck with that. the fact is a (std::)**set** is a set of things, and if you need them laid out contiguously as an implementation detail, why don't you implement your data type and not blame the STL for your lack of understanding or experience?
there's only one or two posts of yours where you sound like a dick -- yet each and every one of them is *right*. at reddit I thought we have manners, but being right and a dick is better than being mild-mannered but wrong ... I don't know why you're being downvoted uniformly by probably the exact same 3 accounts.
The designer of the STL has written a whole book on the reasoning behind it: http://www.elementsofprogramming.com/
You're assuming that having things next to each other in memory trumps any other concerns, without being critical of the algorithm you're using -- your speed problems are with the above algorithm **not** with vector! The algorithm you presented looks very dangerous performance wise because the semantics are you are "destroying" objects (and somewhere else "creating" them) ... that's *way* more expensive than one pointer dereference! If you're creating a vector&lt;Object&gt;, then you're creating, copying, and deleting Objects left/right/center! If you *require* that objects must be created elsewhere and destroyed in a tight loop, what you *need* for cache locality is *not* that your objects exist in a vector, but that they're allocated together on the the same cache line. What you want, as StoneCypher mentioned, is a custom uniformly sized object **allocator** which preallocated your objects in blocks roughly the size of a cache line. This functions **exactly** as you require: when an object is "allocated", the next free block of sizeof(Object) is returned, and vice versa for deallocation, and blamo, there they are -- all right next to each other in memory! Custom allocators and std::vector is how many professional real-time applications do this -- including my workplace. This is fairly common and well-known optimization, please spend more time learning about your field and less complaining.
that algorithm is a fairly straight-forward implementation of how a human would think about the problem, but performance-wise it's problematic; ie. you can have "easy to understand", and you can have "runs really fast", but you can't have both. if you want that code to run quickly you need to be a little fancier. unfortunately the designers of the STL cannot make magic data-types that solve all our problems -- we will have to do a bit more thinking on our own. if you want cache locality, you want a custom block allocator that ensures the objects lie in an easily cachable block. if you want quick accesses to objects in those blocks, you want an (unordered_)set. I have also in the past used two vector&lt;Object*&gt;s: one for a list of live objects, and one for dead. I iterate over the live list, and transfer to dead when dead. when I exit the loop I erase the entire dead list to return the objects' memory. batching like this is also good for cache performance.
&gt; a complex system of parallel hash tables indeed -- but then you're adding in a lg complexity factor to your algorithm, as any given table may have a number of sub-tables... :D
I have heard arguments about forcing checked exceptions in Java, fixing (well, adding) exception hierarchies in C++, never using exceptions in favor of return codes, and so on. Through all this, I have only become more convinced that Python got it as close as possible to "right:" a dozen exception classes in the language, and derive from the most logical one for library exceptions. That this article mentions the debate between throwing std::runtime_exception or something else is just a terrible symptom of how inadequate C++ exceptions are, and that the people in such arguments don't understand how to use exceptions effectively.
Man, this is a really simple question and the answer that Kranar provided was satisfactory. The problem with a "normal" object pool that does not reorder elements on remove is that on delete I get gaps in the set of objects and I have to maintain an extra list, the problem with vector::erase is that it is O(N) to maintain order that I do not need. The solution is Kranar's (and several others) post. Yes, the cost of moving the object could make this solution a bad one in some cases, but in other cases it is efficient. It is always more efficient than vector::erase, so if you don't care about the order of elements in your vector then this is a more appropriate function to call. Also, I'm not complaining. I was asking a question. You have an extremely combative attitude and I don't know why.
Ah, my mistake. I was confused by the interface which takes key_types, which are actually the same as value. unordered_set is apparently a hashtable that uses object values for hashing. My question was not about hashtables. I do not have anything to add. A hashtable is a nice container, but it is not useful for what I am doing...
&gt; basically you've assumed in your mind a vector is a set of things laid out contiguously in memory Yes, that is what I use vector for. I see that that is only part of what a vector is, and that is why erase is implemented in the way that it is. Very often I do not care about the order of the elements, but I do like how flexible vector is. As discussed above I can wrap vector and get the behavior that I desire. Obviously it would be inappropriate to describe the new wrapped class as a vector. I'm not "blaming" the STL for some slight. I'm trying to understand why the default implementation of erase does so much work. I understand now that it is because of the ordering requirements that are occasionally useful.
It is not always appropriate to wait to optimize, sometimes it is painfully obvious that something is going to be a bottleneck. This is especially true when working with machines that use SPUs or vector processors, those machines expect you to write special code to achieve the expected performance. It turns out that I am working on a problem that will need to have the working data set DMA'd to SPUs. The working data set is easily described as a large-ish set of relatively small, long living objects with nonetheless different lifetimes. That's the reason that I was looking at std::vector, and that's the reason that I was wondering why the ::erase function was implemented in that way.
&gt; I'm not complaining. I was asking a question. "[...] I hate that vector is so useless." &gt; You have an extremely combative attitude and I don't know why. People have already explained to you where you've gone wrong, but you won't listen or they are downvoted. For a guy asking advice you're not interested in most of the replies and I don't know why. What you want is to either not use the algorithm above, or to have a custom allocator + placement new et al. that makes sure your objects reside next to each other in memory.
You do not understand what I want. Instead of shifting all things down on ::erase, I want to take the last thing and put into the newly free slot. That is the only difference. I was wondering why the STL did not do it this way, and it is because their goal was to provide a contiguous memory container that could be ordered by the user. I understand that now. Thanks anyway.
I'm sorry that line rubbed you the wrong way, but it's still true in my experience. Very rarely do I need a contiguous memory collection of objects that are in a particular order, even after objects have been removed from the collection. Occasionally I need to keep a set of objects in an order, and occasionally I need objects to be in a contiguous block of memory. I cannot recall ever wanting both. As for "[people explaining] where I've gone wrong" I got what I wanted out of the thread long ago. Someone explained why the stl vector implements erase this way (because of its order guarantees) and several people showed ways to implement what I wanted using the public interface of stl vector. Now I can wrap vector, get the functionality out of it that I want, discard the ordering guarantees, and make erase faster. All done! Most of the other posts did not understand what I was looking for. Considering their number this kind of suggests that I was not sufficiently clear in my OP. For that I blame the late hour and the lack of time. An example would have made the question much clearer. I didn't downvote anyone, by the way.. as much as possible I tried to clarify the fog that I had accidentally cast over the field of discussion.
He means that you should look how Erase() above is called. How did you get location? If you know your element is in the ith position finding location is O(1), giving a total complexity for finding and removing an element to be O(n). However, if you have to find the element first, because you can't rely on it being anywhere around i (since you swap elements all over the place), then you still remain stuck with O(n) to find and remove an element. However in the algorithm you're using, you always iterate over every element any way, so actually the standard erase on every iteration would give you O(n^2 ). Instead of hacking up your own container to get a cheaper erase, how about you just *not* erase during every iteration, instead mark them as dead, and when the loop is complete run v.erase(remove_if(v.begin(), v.end(), IfIsDead()), v.end()); So much simpler. The best optimization is always to simply avoid unnecessary work...
As I've tried to say, I don't think you want to do what you are going to do any way (Kranar's Way(tm))... so there's frustration there -- I'm trying to make your app better, but you prefer your first instinct... But I'm glad you got the answer that makes you happy, and I seriously recommend you take a deeper look at the fundamentals of STL.
Our games run on PS3 (Cell/SPU), Xbox360, and PC. It uses std::vector with a custom memory allocator to get good spatial locality.
That is great, I want to use std::vector for the same reason.
Ah, so by using algorithms we can replace this type of for loop with another O(n) construct. Still, I have a container with an erase method that I never really want to call, that is maintaining an invariant that I did not mean to maintain. It still seems worth using a class that is actually conveying the interface and invariants that I want.
Welcome to proggit.
Yes, but that complexity is already there and worse. By ignoring the error term in your complexity equation (hash tables are O(1) + (z)(O(collision handler)), where z is the frequency of collisions), you miss the actual performance of the hash, vs. the theoretic/average performance. Error terms matter.
Quite. Have an upboat!
^ is already commandeered for bitwise xor don't forget so you're swapping one overloaded operator for another.
Hungarian notation? Seriously?
The same topic was covered pretty thoroughly in a lecture (rather series of lectures) by Jerry Cain at Stanford quite a long time ago. The material was pretty introductory stuff, but if you're coming straight out of C++ and don't know much about how C handles generic algorithms the lecture isn't that bad (I sat through much worse explanations of it in the 90s). I'm too lazy to find it but it's on both Youtube and iTunes U.
&gt; Note also the new "stack" section that is the first C++ to beat OCaml, albeit cheating by exploiting the fact that this implementation of this benchmark always happens to allocate and free in FIFO order. So this approach is problem-specific and cannot be used as a general-purpose allocator. This is *precisely* the benefit of making garbage collection optional. Stack discipline applies in a great many cases like this, with significantly lower overhead than garbage collection.
Your observation might be correct: &gt; Stack discipline applies in a great many cases like this, with significantly lower overhead than garbage collection. But your conclusion does not follow from it: &gt; This is precisely the benefit of making garbage collection optional. To conclude that, you would need evidence that having a GC would degrade the performance of the stack-based solution but there is no evidence of that nor any reason to expect that result. 
Maybe I'm not a C programmer, But AFAIK usually templates are implemented with macros and not void * (in C). Doesn't void* is the normal way to have generic algorithms in C?
He's talking about good performance with the cache. A contiguous memory block enables automatic prefetching. No one needs to bring it up. It's a feature that happens if you know anything about the cache. 
That's linear, I do believe. 
Seemed to be okay. I checked out though on the array vals[max_thread_count] and seeing InterlockedIncrement[vals[thread_id]] Overkill...++vals[thread_id] is superior here One thing I hold to...threading is to be done at the engineering/design level, not at instruction level. That being said these articles show some reasonable things to look out for. I've been afraid to do anything beyond very basic locking due to keeping things cross platform and cross os. The libraries referenced are probably worth looking into.
one bad thing about this method is that if I'm removing stuff I'm generally searching for stuff to remove. The above forces any searches over to O(n). At that point of course I won't use a std::vector but a std::set
&gt; As I've tried to say, I don't think you want to do what you are going to do any way I'm afraid beguiledfoil is correct and that you are wrong. &gt; The algorithm you presented looks very dangerous performance wise because the semantics are you are "destroying" objects (and somewhere else "creating" them) ... that's way more expensive than one pointer dereference! For erase, he's copying one object. There is no destruction. Also, a pointer dereference is MUCH more expensive than sequential access because of automatic prefetching. &gt; What you want, as StoneCypher mentioned It was already explained why StoneCypher was mistaken. &gt; This functions exactly as you require: when an object is "allocated", the next free block of sizeof(Object) is returned And what about the holes? It's not just about allocations, but also about deallocations. This will create fragmentation no matter what you try to do. That's why copying the last element to the newly create hole will keep the memory use contiguous. &gt; This is fairly common and well-known optimization, please spend more time learning about your field and less complaining. Funny considering how you're wrong. And contentious. &gt; I seriously recommend you take a deeper look at the fundamentals of STL. I would suggest you take your own advice. 
Unfortunately, StoneCypher is wrong every single time. 
&gt; he's copying one object. There is no destruction ... &gt; a pointer dereference is MUCH more expensive than sequential access because of automatic prefetching. ... &gt; And what about the holes? ... OMG... whatever credibility you started out with was entirely lost. This is strictly grade-school stuff you're getting wrong. Here, let me do an impression of you... "you're wrong. I'm right." Get lost. 
Templates in C++ are the exact opposite of this. With templates, you should not need to cast "stuff" from a generic concrete type to the real concrete type. BTW, it should be possible to use C++ template with C syntax without using classes etc. (and without C linkage) 
How about some of your replies: &gt; Here, let me do an impression of you... "you're wrong. I'm right." &gt; Get lost. &gt; I seriously recommend you take a deeper look at the fundamentals of STL. &gt; People have already explained to you where you've gone wrong, but you won't listen or they are downvoted. &gt; This is fairly common and well-known optimization, please spend more time learning about your field and less complaining. The funny part is the irony of it all. 
Or you could use 'enum class' from C++0x.
Or you could use the type safe enum idiom, which does this even better as a side-effect: http://en.wikibooks.org/wiki/More_C%2B%2B_Idioms/Type_Safe_Enum
I'd hardly call that stupid, or a trick. But yes, that's good standard practice for enum-s. The real benefit of C++0x in this area is in the added type safety and in specifying the underlying type.
I like that! thanks, this is indeed even better
enum class doesn't give you any namespacing it just specifies the type of the data.
Er, wrong. Give n2347 a read. "Like a class, the new enum type introduces its own scope. The names of enumerators are in the enum's scope, and are not injected into the enclosing scope."
oh spiffy.
I've seen your source code -- color me unimpressed with your internet tough-guy routine. I have nothing to prove to the likes of *you*.
Because it was fun for me to show you what you're missing, I translated the problem description that the OP *eventually* gave up [here](http://www.reddit.com/r/cpp/comments/et0i6/why_doesnt_the_stl_have_an_unordered_alternative/c1aqsqk), into a simple test driver: http://pastebin.com/P2VqFx9Q * count the number of deallocations (as well as the implicit copies) in the first method * observe the cache friendly locality of reference in both methods * observe the triviality with external fragmentation is solved by uniform block allocation, the ease in which compaction for DMA transfer is implemented in the second method Sleep well!
&gt; internet tough-guy routine You may want to look in the mirror. You're clearly projecting. 
Sorry, but you didn't solve fragmentation at all. Also, you're erasing every single element. You can use clear() instead if you're doing that. In the "silly way", you don't even deallocate properly. Holy shit Batman. Talk about a waste of time. 
Oh my god you're dumb! * an array of uniform block cannot fragment * I'm "erasing" a pointer -- and erase was a requirement of the OP! * It's not my job to make a full-on STL-compatible allocator for you -- do try to use your intellect a smidge Talking to you is a supreme waste of time -- you have to be one of the stupidest commenters here. You shouldn't really bother to say anything next time.
wow -- "I know you are but what am I?" -- big comeback...
i'll just leave [this](http://en.wikipedia.org/wiki/C_preprocessor#X-Macros) here.
I do like your original suggestion though because it has a good chance of being used by a team if it was introduced to them - it's lightweight and easy to understand.
If you add something unique as a prefix of the enum, Visual Assist will show you all possible values anyway. For example, if you'd change your enum to something like: enum MyEnum { k_MyEnum_One, k_MyEnum_Second, k_MyEnum_Third, k_MyEnum_Count }; then as soon as you type k_MyEnum_ (or more likely before then) VA will list all possible matches for the remainder of the string. Also, I'm 95% sure that Microsoft (I mention them because you talk about Visual Assist/Intellisense, so you don't seem too concerned with portability) has supported scoping enums by their name (e.g. MyEnum::kFirst) since at least VC6. The only difference is now the compiler warns you that it's a nonstandard extension, whereas in old versions it just silently compiled without telling you.
&gt; an array of uniform block cannot fragment Of course it can if you don't use them in the order they appear in the array. Suppose you allocate 5 blocks, release the 3rd and then allocate one new block. You will end up with blocks {0,1,3,4,2} in that order and you now have fragmentation. That's why it was suggested to swap the last element to the missing spot when deallocating. So instead of having {0,1,3,4} still allocated after deallocating the third object, you would have {0,1,2,3}. This is true regardless if you use pointers or not. With {0,1,3,4,2}, yes they are all contiguous in memory, but you won't be accessing them that way with your allocator works. You will access them in the order listed and you will not have the speedup that comes with automatic prefetching. &gt; I'm "erasing" a pointer -- and erase was a requirement of the OP! Yes, but you're erasing every single one. Also, you don't even do it correctly in the first section (or in the second for that matter). In the second section, you lose any speedup you could have gotten from the cache if you do anything non-trivial. &gt; It's not my job to make a full-on STL-compatible allocator for you -- do try to use your intellect a smidge I'm trying to tell you that an allocator alone will only help to improve locality somewhat during initial use, but it won't solve the fragmentation issue when it cames to deallocations. &gt; Talking to you is a supreme waste of time -- you have to be one of the stupidest commenters here. Why are you giving out insults? Do you believe that this helps anything? 
&gt; Suppose you allocate 5 blocks, release the 3rd and then allocate one new block. You will end up with blocks {0,1,3,4,2} in that order no you don't, you have {0, 1, 5, 3, 4} assuming each job is identified by a monotonically increasing integer -- the memory itself never changes... &gt; and you now have fragmentation that word doesn't mean what you think it means: http://en.wikipedia.org/wiki/Fragmentation_(computer) *this* is why you earn contempt -- you walk around downvoting and insulting instead of listening and learning -- you're not even using the terminology correctly! &gt; you would have {0,1,2,3} no you would have {0, 1, 4, 3} ! &gt; Yes, but you're erasing every single one only because I can't be bothered to write a full simulation of what the OP intended from his pseudocode -- I'm missing your point entirely... also, how is the first section incorrect? it's what the OP specifically wants to do &gt; it won't solve the fragmentation issue when it cames to deallocations that doesn't make any sense -- the memory itself is allocated once, and deallocated once. everything else is just games with pointers -- which is the whole point! the objects stay in the same cache line the whole time! &gt; Do you believe that this helps anything? well you've now deigned to explain yourself...
&gt; no you don't, you have {0, 1, 5, 3, 4} If there are only 5 blocks pre-allocated by the allocator, then block #5 is not possible. In any case, it would appear at the end if you use push_back() as you do for inserting elements. &gt;&gt; and you now have fragmentation &gt;that word doesn't mean what you think it means: http://en.wikipedia.org/wiki/Fragmentation_(computer) Actually, it means exactly what I think it does. The ENTIRE list should not be fragmented. Every element in the list should immediately follow another element in the list. &gt;&gt; you would have {0,1,2,3} &gt; no you would have {0, 1, 4, 3} ! What you're mentioning is the element order from before. Element #4 is copied to element #2. So you have the following element order with the following block number used by their respective elements. Elements: {0,1,4,3} Blocks: {0,1,2,3} The new element order is now renumbered {0,1,2,3}. Note how both the memory and elements are still contiguous? Your allocator doesn't achieve this required invariant in the second part of your code. Your code in the first section does keep it contiguous, but it's O(n) instead of O(1). So it's inadequate to the task in both sections. &gt;&gt;Yes, but you're erasing every single one &gt;only because I can't be bothered to write a full simulation of what the OP intended from his pseudocode -- I'm missing your point entirely... It's just that clear() would work better. Moving the end element every time when you're just gonna delete them all is pointless. Only erasing every now and then is more what the op was talking about. &gt; also, how is the first section incorrect? it's what the OP specifically wants to do No, he wants to ~~swap~~ **copy** the end element ~~with~~ to the element being erased and then pop the last element off. You're simply using the default vector erase. &gt;&gt; it won't solve the fragmentation issue when it cames to deallocations &gt; that doesn't make any sense -- the memory itself is allocated once, and deallocated once. everything else is just games with pointers -- which is the whole point! the objects stay in the same cache line the whole time! A cache line is 16 to 256 bytes. At 4 bytes minimum for an int, you can have 4 to 64 objects within the same cache line. What do you do when you go beyond that? What if your objects are much larger? What if you can only store one object per cache line? Not just that, but if the allocation order gets mixed around enough after you do some deallocations, then you'll be thrashing the cache. This would happen if the element order is nothing like the pre-allocated block order. If the block order remains contiguous and the elements are scanned as they appear in the pre-allocated blocks, then automatic prefetching can take place. In your scenario (both of them), this extra speedup cannot be achieved. 
Pretty good, but, IMO they made a big mistake with optional semicolons, just like Javascript.
Garbage collectors are known for stopping all threads of execution, collecting at non-deterministic times, etc.... While these issues are not critical for desktop and server applications, they are huge roadblocks for real-time embedded software. It is true that a couple garbage collectors have tackled some of these issues, but I am not aware of a garbage collector that has solved all the issues -- i.e. that would work well for all real-time embedded projects. As long as there are projects where a GC would be an impediment, then having a GC can't be mandatory.
The typesafe enum is also lightweight. Note that the template only needs to be written once, not for each enum.
ok last go... &gt; then block #5 is not possible That is why you a) make a reserve pool larger than your expected usage b) make the storage growable (just like vector!) somewhere depending on your usage -- I can't be expected to design and write the OP's program for him just to prove to you that the techniques in rampant use in real-time applications actually work. &gt; you use push_back() as you do for inserting elements push_back is used on the linked list of pointers. pointers exist so they can be de-referenced to point other memory. it has no affect on the layout of actual memory (which is in the vector) -- only the way that memory is accessed. it is also mentioned explicitly that if you want to change the algorithmic complexity of your deallocate, you *can* insert the point in-order so as to have in-order memory access. but the *smart* thing to do would be to implement the simplest algorithm, then **profile** to find out what's right. calls not to make special algorithms and instead profile were also downvoted by the collective intelligence here. &gt; Every element in the list should immediately follow another element in the list. No, your definition either means that elements must be accessed in sequence (that all elements with lower addresses must be accessed before elements with higher addresses), or there cannot be any *unaccessed* address between *accessed* addresses. What the definition means is that data is wasted because of the way the elements are laid out in free space. You appear to have come to the belief the latter means the same as the former because you somehow believe that having "holes" in your access patter is wasting space. It is *not*. This is because at any moment that hole can and will be easily filled with another allocation. The "holes" are merely *temporary* artifacts of the optimization that we do not ever create, destroy, or move memory -- we just change pointers. This allows the actual memory to *never* fragment, and remain together in cache. &gt; Your allocator doesn't achieve this required invariant in the second part of your code. Of course it does! Prove to me it doesn't! Read the code! **arghhhh** &gt; No, he wants to swap copy You can simulate the same effect by erasing the end element. Do use some imagination. &gt; What do you do when you go beyond that? The hardware puts them on multiple lines! Exactly the same as a vector! &gt; allocation order gets mixed around enough after you do some deallocations, then you'll be thrashing the cache Nonsense -- the "allocation order", what ever that is that you keep talking about (since the actual memory is allocated once at start-up), stays the same, since the allocated memory stays the same. The **only** thing that changes is the way in which memory is **accessed** (thanks to the linked list). Thankfully, computers with RA(Cache)M can access memory in any (random) order. The **only** thing that is important for a cache-friendly data structure is **Locality of Reference**, which pre-allocating achieves. The funniest part of this whole discussion is you somehow think everything I'm saying is somehow my "opinion", when in fact it's what real-time software industry has been readily publishing research on for probably decades. It's how real-time systems are written, including the AAA game titles we make at work every single day -- so you're not arguing with me, you're arguing with people with decades of experience making things vastly more complex and performance sensitive than a skiplist... edit: there is one and only one way in which the OP's method is superior -- because the addresses are accessed in strict sequential order lowest to highest within the range (A,B), it is very likely to trigger the hardware prefetcher. Accessing the same memory over the same range (A,B) in arbitrary order may well be enough to defeat the hardware. this however would only make a difference on the first access, or if the entire cache is under extreme pressure, and the cache lines are being pushed out because some less stable part of the code keeps trashing. if the latter is the case, then that's the problem that needs fixing, not the pre-allocation. moreover, if you're confident that prefetching is what you need, there are other means besides the hardware prefetcher.
What specifically do you mean "properly"? You mean it used less strict aliasing and thus did literally what you asked, or you mean it used strict aliasing and converted your code to one assembly instruction? http://en.wikipedia.org/wiki/Aliasing_(computing)#Conflicts_with_optimization If it's in the standard, it's an issue with the language. Even if MSVC either assumes too much, or is just wickedly smart, it's still just one compiler. 
C++0x is not yet finalized and is not expected to be published until the end of 2011. As such not everyone is in a position to use it. Even when it is available it will take quite a long time before people and organizations begin adopting it.
Apparently there's another post on the matter which is a little dated, but may very well be more thought out: http://redd.it/evlhy
Which write-up? I think you accidentally the link.
&gt; I am not aware of a garbage collector that has solved all the issues The concurrent, parallel, real-time [Staccato GC](http://domino.research.ibm.com/comm/research_people.nsf/pages/dgrove.rc24504.html) solved all of those problems and parallelism as well. IBM commercialized it and it is used for real-time embedded programming. &gt; As long as there are projects where a GC would be an impediment, then having a GC can't be mandatory. I agree. Many such situations require static allocation. 
I've learned another tdd toolchain rake and igloo. Igloo seems to be very natural language. neat idea. 
&gt; That is why you a) make a reserve pool larger than your expected usage I addressed this. I said it would show up at the end. The reason I said that there would be a limit is to show point blank your mistake. Had nothing to do with reserving a larger pool. It's simple things like this that fly right over your head that makes this difficult to get anything through to you. &gt; push_back is used on the linked list of pointers. pointers exist so they can be de-referenced to point other memory. it has no affect on the layout of actual memory (which is in the vector) -- **only the way that memory is accessed.** The part I've bolded is the problem with what you're doing. But there's more. Your pointers also take up memory. Those are not allocated using your allocator. They are allocated with the vector. So you run into the same problems even if you think you solved the other problems, which you did not. The order you access them is important for automatic prefetching. And you're wrong on the layout too. It does change it because of holes created. &gt; it is also mentioned explicitly that if you want to change the algorithmic complexity of your deallocate, you can insert the point in-order so as to have in-order memory access. You had this at O(n) if I recall correctly. There's an easier way at O(1) which is the point of this discussion. &gt; but the smart thing to do would be to implement the simplest algorithm, then profile to find out what's right. Not if you want performance guarantees out of the implementation in the same way STL provides performance guarantees. &gt; No, your definition either means that elements must be accessed in sequence (that all elements with lower addresses must be accessed before elements with higher addresses) YES!!! &gt; or there cannot be any unaccessed address between accessed addresses. YES!!! BOTH!!! OMG, you've got it! This is what the op's been talking about. &gt; What the definition means is that data is wasted because of the way the elements are laid out in free space. There is no wasted data. Why would you say that? &gt; You appear to have come to the belief the latter means the same as the former because you somehow believe that having "holes" in your access patter is wasting space. It is not. This is because at any moment that hole can and will be easily filled with another allocation. But while those holes exist, automatic prefetching cannot happen. You lose that speed boost. And no, I don't think they are the same thing. You need BOTH! &gt; The "holes" are merely temporary artifacts of the optimization that we do not ever create, destroy, or move memory -- we just change pointers. This allows the actual memory to never fragment, and remain together in cache. But they DON'T remain in cache. This is what I've been trying to tell you. The cache is usually 32K to 256K or something like that for L1. If you've been doing something else, then the list is no longer there. When you decide to iterate over all the elements, your way introduces many cache misses. But if they are sequential and have no holes, then automatic prefetching comes into effect where you don't have any cache misses except for the initial access (unavoidable). Also, your pointers themselves cause cache misses completely losing locality or any chance of effectively using the cache. These are braindead simple things. Yet, you miss the completely obvious. There's more things you get wrong. It just never ends really. Your pointers change the order that memory is accessed. So even if you had no holes at all, memory access would jump around back and forth. Again, this would cause automatic prefetching to not work losing this speed boost. So your "method" loses out time and again no matter what. &gt;&gt;Your allocator doesn't achieve this required invariant in the second part of your code. &gt; Of course it does! Prove to me it doesn't! Read the code! arghhhh I've explained it with the order of the elements. At this point, you're choosing to ignore it, or just don't understand anything. If the memory isn't accessed in sequential order and without gaps, then automatic prefetching won't work. I don't know how much simpler I can explain it than that. By using pointers, this means you cannot guarantee the invariant that memory accesses will be in sequential order (by address) and without gaps. &gt;&gt; No, he wants to copy &gt;You can simulate the same effect by erasing the end element. Do use some imagination. And if the end element is not the element you're erasing? &gt;&gt; What do you do when you go beyond that? &gt;The hardware puts them on multiple lines! Exactly the same as a vector! Yes, but in a vector, automatic prefetching kicks in. Not so with your custom allocator. Sheesh. &gt;&gt; allocation order gets mixed around enough after you do some deallocations, then you'll be thrashing the cache &gt;Nonsense -- the "allocation order", what ever that is that you keep talking about (since the actual memory is allocated once at start-up), stays the same, No, the allocation by the unordered_list. True that you pre-allocate memory. But the list then makes requests for those memory blocks and uses them out of order because of the deallocations. At the very least, you end up with gaps. &gt; The only thing that changes is the way in which memory is accessed (thanks to the linked list). YES!!! Damn it, YES!!! That's what I'm getting at. &gt; Thankfully, computers with RA(Cache)M can access memory in any (random) order. YES, but automatic prefetching doesn't work with random access, only sequential access. Fuck, you don't know what automatic prefetching is, do you? You don't have a damn clue. Ah shit. You wasted my time. Nice trolling buddy. &gt; The only thing that is important for a cache-friendly data structure is Locality of Reference, which pre-allocating achieves. Well, that proves it. Look up automatic prefetching and come back. Can't believe I wasted my time like this and you don't even know what automatic prefetching. Heck, I don't think you know what manual prefetching is or how it's done. &gt; The funniest part of this whole discussion is you somehow think everything I'm saying is somehow my "opinion" You have no clue what automatic prefetching is and you talk like you're king shit. &gt; because the addresses are accessed in strict sequential order lowest to highest within the range (A,B), it is very likely to trigger the hardware prefetcher. Ah, the edit saves the day. He went back and looked at what I was talking about. &gt; Accessing the same memory over the same range (A,B) in arbitrary order may well be enough to defeat the hardware. May well be enough? Holy shit batman. &gt; this however would only make a difference on the first access Sorry, try again. &gt; or if the entire cache is under extreme pressure Nope. try again. &gt; and the cache lines are being pushed out because some less stable part of the code keeps trashing. Nope. try again. You're talking about general use of random access. Not sequential access. &gt; moreover, if you're confident that prefetching is what you need, there are other means besides the hardware prefetcher. It's not about confidence of what is needed. It's a simple braindead optimization that comes with a simple copy of the last element. Accessing the last element will affect the cache, but this is why I said erasing every single element would defeat this optimization. It's also why the op said he could force a cache miss to begin automatic prefetching since the memory layout was contiguous. Anyways, nice to see you've finally seen what I was talking about. I hope next time, you won't just assume that others are making shit up or don't understand. edit: &gt; The "holes" are merely temporary artifacts of the optimization that we do not ever create, destroy, or move memory -- we just change pointers. This allows the actual memory to never fragment, and remain together in cache. I probably should leave this alone, but I'm now curious as to what exactly you thought your custom allocator achieved. You mention "remain together in cache". You don't need memory that is close together to achieve this. Any allocator will do. The only requirement is if you are using less memory than the size of your cache. And don't have too many items that are away from each other by exactly the size of the cache. In any case, any random access, if the memory is already in the cache, will be as fast as possible. Your allocator does nothing to help nor hinder this. The the pointers do take one operation more and use up more of the cache leaving less cache for the actual elements. So what exactly DID your allocator achieve? What do you believe this **locality of reference** you mentioned does? You mentioned the pre-allocation achieves this and is a "cache-friendly data structure". Do you really believe that? How exactly is it cache-friendly in your view? I'm really interested. Is it about a single cache line holding several elements? That's not generic and only works for very small items and the pointers will lose any advantage sharing a cache line may have had. My apologies if I'm unnecessarily driving the point further than needed, but after everything you've said and how adamant you were, I'm now also questioning if you understand that memory doesn't need to be contiguous to be cache friendly. It simply needs to be re-used or total fewer memory than the cache size. This is leaving automatic prefetching and cache misses aside, of course. So what was your goal with that allocator? You do make it possible for pointers to elements to remain valid. But for an unordered list, that's a moot point, especially when considering that you were talking about performance issues. Was it just the sharing of cache lines? 
lawl
&gt; Those are not allocated using your allocator. They are allocated with the vector. Wait, so you're saying vector&lt;Object&gt; is fast because of prefetching (it is, if you ignore all calls to copy constructor or assignment operator), but vector&lt;Object *&gt; (which has no such overhead) isn't somehow? Make up your mind skippy! &gt; The cache is usually 32K to 256K or something like that for L1. Oh shit. You just magically changed the subject from L2 to L1 to suit your argument... If you can **actually** find any literature demonstrably showing how to take advantage of L1 cache, I'm all eyes... &gt; then automatic prefetching comes into effect where you don't have any cache misses except for the initial access (unavoidable). Actually having the memory in cache *before* it's needed, on the *first* access is exactly **the entire purpose** of prefetching! It's **only** useful for the first access! &gt; [ long winded repeating of yourself ] So basically you wagered your entire time on hardware prefetching as if it was the be-all and end-all of cache performance, ignoring how caches are supposed to work (hint: they cache frequently accessed data). Congratulations. I just modified the little test code to do about 1 million iterations, and O2 my code is 3x faster than your/OP's code. Just like I write code every day that is performance sensitive, and you jerk off on reddit for kicks. How on earth can that be possible if prefetching is the only thing that's important and *I'm* the dense one?? I can show it to you if you actually intend to read it and show me where I went wrong, instead of making unsubstantiated (incorrect) claims without proof. I think your skiplist needs a bit more work there skippy, because you've got nothing to teach anyone, not without hitting a lot of books there. &gt; Anyways, nice to see you've finally seen what I was talking about. I hope next time, you won't just assume that others are making shit up or don't understand. Oh I always known what a hardware prefetcher is, but didn't believe until now you were basing your entire performance claims on such a simplistic measure, since a prefetcher just puts stuff in cache, it's the cache hits thereafter than cause improve performance... &gt; What do you believe this locality of reference you mentioned does? It is how caches work. You need to hit the books harder: * http://en.wikipedia.org/wiki/Locality_of_reference * http://www.memorymanagement.org/glossary/l.html * http://hillside.net/plop/plop97/Proceedings/bhatt.pdf * http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.6621&amp;rep=rep1&amp;type=pdf Let's draw a picture a simple cache (two lines of two cells): RAM: | a | b | c | d | e | f | .... Access pattern 1: a, b, b, c, d, c, ... Access pattern 2: f, e, a, f, a, f, a, ... #1 has good locality of reference. Access: a -&gt; miss Cache: | a | b | | | Access: b -&gt; hit Access: b -&gt; hit Access: c -&gt; miss Cache: | a | b | c | d | Access: d -&gt; hit Access: c -&gt; hit #2 has poor locality of reference. Cache: | a | b | c | d | Access: f -&gt; miss Cache: | e | f | c | d | Access: e -&gt; hit Access: a -&gt; miss Cache: | a | b | c | d | Access: f -&gt; miss Cache: | e | f | c | d | Access: a -&gt; miss Cache: | a | b | c | d | Access: f -&gt; miss Cache: | e | f | c | d | ... In this case, the purpose of a block allocator (aside from costly context switches to the kernel through new/malloc) is to make memory accesses look like #1 and not like #2, without having to perform memory gymnastics in your main logic.
What a mess. You understand nothing. Nice trolling. At no point did you ever use the op's algorithm. And your example about the cache at the end can't be more wrong. But this was by far my favourite part. &gt; Actually having the memory in cache before it's needed, on the first access is exactly the entire purpose of prefetching! It's only useful for the first access! HAHAHAHAHAHA HOLY SHIT!!! 
HAHAHAHAHAHAHA -- wait -- what are we laughing at? &gt; At no point did you ever use the op's algorithm You haven't seen the code. You barely read the last version! How can *I* be the troll if I'm the only one explaining anything? Where's your source? Where's your diagrams? Nowhere -- along with your point. &gt;&gt; Actually having the memory in cache before it's needed, on the first access is exactly the entire purpose of prefetching! It's only useful for the first access! Later dude. In the mean time, you need to do some reading: http://www.ee.ryerson.ca/~courses/ee8207/prefetchprj3.pdf *"The idea behind Data Prefetching is that instead of waiting for a cache miss to perform memory fetch, design an algorithm (or a method) to predict future data request by microprocessor."* 
You only need to do a cache miss to kickstart the automatic prefetching mechanism just like the op said. BTW, do you know that caches will load cache lines in a few different locations in the event of collisions? So your second diagram will only come into effect if you have a lot of elements that are located at the same location some multiple of the cache size. Besides, your custom allocator doesn't even solve the issue even if that issue were real. And while you do have pointers in your vector, erasing is still O(n). And accessing the elements pointed to by those pointers will not be as efficient as they could be. My skiplists use forward pointers to the next nodes. Sequential access is nowhere what it is with a vector. And neither is your use of pointers. Sorry, but you really don't know shit. You didn't even know what automatic prefetching is. You STILL don't know how it works. This is obvious by what you write. Too late to showboat now. 
&gt; You only need to do a cache miss to kickstart the automatic prefetching mechanism just like the op said. *"The idea behind Data Prefetching is that instead of waiting for a cache miss to perform memory fetch, design an algorithm (or a method) to predict future data request by microprocessor."* If you get a cache miss, it's **not** prefetch (hint: "pre" means "before"). You can tell me what I don't know until you're blue in the face, but the fact is you're arguing with the Electrical Engineering Department of Ryerson University here. Maybe you can get them on reddit to tell them what prefetching is (since it appears you're the only one that knows)? I double-dog **dare** you to find references for all the things you're spouting. &gt; do you know that caches... Yes, and do you know that simplified examples tend to leave out details (usually left to the reader to understand) to make a point? It's called explanation -- something you seem incapable of. To my count, I've give much expository, two test programs, one diagram, and more than one appeal to published authority. You have given "prefetch, prefetch, prefetch!" -- I know what the score is by now. :D &gt; And while you do have pointers in your vector, erasing is still O(n) The list traversal itself is O(n)! If you read the code, you'd see it do the remove *outside* the loop, and thus O(n) + O(n) = O(n). As the other guy pointed out, simply repeating complexity guarantees without context is meaningless. Very elementary algorithm analysis! How about you wake me when you've got something concrete to say? How about you put your skiplist down long enough to write and benchmark your own test app?
&gt; You only need to do a cache miss to kickstart the automatic prefetching mechanism just like the op said. *"The idea behind Data Prefetching is that instead of waiting for a cache miss to perform memory fetch, design an algorithm (or a method) to predict future data request by microprocessor."* If you get a cache miss, it's not prefetch (hint: "pre" means "before"). You can tell me what I don't know until you're blue in the face, but the fact is you're arguing with the Electrical Engineering Department of Ryerson University here. Maybe you can get them on reddit to tell them what prefetching is (since it appears you're the only one that knows)? I double-dog *dare* you to find references for all the things you're spouting. &gt; do you know that caches... Yes, and do you know that simplified examples tend to leave out details (usually left to the reader to understand) to make a point? It's called explanation -- something you seem incapable of. To my count, I've give much expository, two test programs, one diagram, and more than one appeal to published authority. You have given "prefetch, prefetch, prefetch!" -- I know what the score is by now. :D &gt; And while you do have pointers in your vector, erasing is still O(n) The list traversal itself is O(n)! If you read the code, you'd see it do the remove outside the loop, and thus O(n) + O(n) = O(n). As the other guy pointed out, simply repeating complexity guarantees without context is meaningless. Very elementary algorithm analysis! How about you wake me when you've got something concrete to say? How about you put your skiplist down long enough to write and benchmark your own test app? 
&gt; You only need to do a cache miss to kickstart the automatic prefetching mechanism just like the op said. *"The idea behind Data Prefetching is that instead of waiting for a cache miss to perform memory fetch, design an algorithm (or a method) to predict future data request by microprocessor."* If you get a cache miss, it's not prefetch (hint: "pre" means "before"). You can tell me what I don't know until you're blue in the face, but the fact is you're arguing with the Electrical Engineering Department of Ryerson University here. Maybe you can get them on reddit to tell them what prefetching is (since it appears you're the only one that knows)? I double-dog *dare* you to find references for all the things you're spouting. &gt; do you know that caches... Yes, and do you know that simplified examples tend to leave out details (usually left to the reader to understand) to make a point? It's called explanation -- something you seem incapable of. To my count, I've give much expository, two test programs, one diagram, and more than one appeal to published authority. You have given "prefetch, prefetch, prefetch!" -- I know what the score is by now. :D &gt; And while you do have pointers in your vector, erasing is still O(n) The list traversal itself is O(n)! If you read the code, you'd see it do the remove outside the loop, and thus O(n) + O(n) = O(n). As the other guy pointed out, simply repeating complexity guarantees without context is meaningless. Very elementary algorithm analysis! How about you wake me when you've got something concrete to say? How about you put your skiplist down long enough to write and benchmark your own test app? 
&gt; You only need to do a cache miss to kickstart the automatic prefetching mechanism just like the op said. *"The idea behind Data Prefetching is that instead of waiting for a cache miss to perform memory fetch, design an algorithm (or a method) to predict future data request by microprocessor."* If you get a cache miss, it's not prefetch (hint: "pre" means "before"). You can tell me what I don't know until you're blue in the face, but the fact is you're arguing with the Electrical Engineering Department of Ryerson University here. Maybe you can get them on reddit to tell them what prefetching is (since it appears you're the only one that knows)? I double-dog *dare* you to find references for all the things you're spouting. &gt; do you know that caches... Yes, and do you know that simplified examples tend to leave out details (usually left to the reader to understand) to make a point? It's called explanation -- something you seem incapable of. To my count, I've give much expository, two test programs, one diagram, and more than one appeal to published authority. You have given "prefetch, prefetch, prefetch!" -- I know what the score is by now. :D &gt; And while you do have pointers in your vector, erasing is still O(n) The list traversal itself is O(n)! If you read the code, you'd see it do the remove outside the loop, and thus O(n) + O(n) = O(n). As the other guy pointed out, simply repeating complexity guarantees without context is meaningless. Very elementary algorithm analysis! How about you wake me when you've got something concrete to say? How about you put your skiplist down long enough to write and benchmark your own test app? 
&gt;&gt;You only need to do a cache miss to kickstart the automatic prefetching mechanism just like the op said. &gt;"The idea behind Data Prefetching is that instead of waiting for a cache miss to perform memory fetch, design an algorithm (or a method) to predict **future** data request by microprocessor." &gt;If you get a cache miss, it's not prefetch (hint: "pre" means "before"). Just look it up. It's getting embarrassing for you. I highlighted the key word. By inducing a cache miss in the **present** and then accessing sequential memory, the processor can predict what you need in the **future** and prefetch that data before you actually do access it. Look, it's all too clear you're way over your head at this point. Please just read up on it. You've already shown your hand here. There's just no denying anymore that you haven't the slightest clue how or what automatic prefetching is. &gt;&gt; do you know that caches... &gt; Yes, and do you know that simplified examples tend to leave out details (usually left to the reader to understand) to make a point? It's called explanation -- something you seem incapable of. You misunderstand the point. Your algorithm is pointless. It accomplishes nothing that isn't already available. &gt; To my count, I've give much expository, two test programs, one diagram, and more than one appeal to published authority. You have given "prefetch, prefetch, prefetch!" -- I know what the score is by now. :D You don't understand your own references. That's not my fault. And automatic prefetching is a simple optimization that you get for free using the Kranar's solution. Unfortunately, you're getting all defensive because you don't get it. This has nothing to do with me. You're still fighting against accepting that you went on this long without understanding any of this stuff and you're amazed and incredulous that it's possible. &gt; The list traversal itself is O(n)! If you read the code, you'd see it do the remove outside the loop Let's take a look at that code then. while (work2.size()) { cout &lt;&lt; "do: " &lt;&lt; work2.front()-&gt;v &lt;&lt; endl; work2.erase (work2.begin()); } erase() looks oddly INSIDE the loop. Also, it doesn't tell the allocator that the memory pointed to by the pointer that used to be in the sequence is no longer in use. So the total complexity is O(n*n) for your algorithm, but O(n) for the op's. &gt; How about you wake me when you've got something concrete to say? How about you put your skiplist down long enough to write and benchmark your own test app? Put my skiplist library down? Why does my skiplist library annoy you so much? I needed it for something else and thought it'd be nice to let others use it as well. I give freely of my code from time to time. Anyways, read up on automatic prefetching. It's too late to showboat now. I understand it's not easy to admit when you're wrong, but I really don't care about that. You're obviously trying to save face by trying to find something against me. So be it. Perhaps one day you'll actually take a step back and take a real look at what I'm saying instead of just being defensive. edit: I benchmarked your version compared to Kranar and just as expected, your code couldn't even handle 100,000 elements with arbitrary push_backs and erases. This is because you're using a vector with your pointers and erasing an arbitrary pointer takes O(n). This is true even when iterating the whole list where the total runtime takes O(n*n). http://pastebin.com/dKYbg4uF See if you can even get your version to complete this century. edit2: Forgot to add that your version also takes 5 times the space compared to the op's. 
Here is a new high quality one from ms: The Visual C++ Weekly: http://paper.li/visualc/news
Dude needs a haircut ... badly.
Its part of his [image](http://e-bergi.com/2007/Kasim/resim/bs2.jpg) now. 
The link is no longer working. [Here is a cached version](http://docs.google.com/viewer?a=v&amp;q=cache:iBjSTckDu_0J:dbp-consulting.com/StrictAliasing.pdf+http://dbp-consulting.com/StrictAliasing.pdf&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESgMAjye894iGFY9lQbe3dECik7HniBhGKCLdFkoAwBR-GDx5aGnTKEM8C_hieSbPn-7gIHHLSaJiknOdNpm4yMNEtKQR5zi3dM-vHwSce5RQ4XXHl2O-lxiBHEBKeHe0JUUqRf7&amp;sig=AHIEtbRUOso9-u2WNgSaUU3X1mfvPj-Waw). (Thanks Google!)
&gt; Please just read up on it. How can I? I'm the one who's delivered all the references! You just sit there and wave your hands about. Quit being so bloody lazy! As far as I'm concerned, until you provide your own references, write your own code, or draw your own diagrams, you've admitted your ignorance. &gt; You've already shown your hand here. This is exactly your problem, you're playing poker instead of being intellectually honest. I'd be perfectly willing to accept every thing you said if you could make any kind of argument that any intelligent person could accept. Instead you just wave your hands to dismiss direct quotes that contradict what you're saying. &gt; erase() looks oddly INSIDE the loop Wrong version. That's was what the OP wanted to do, not me. Oh, wait you weren't interested in the newer one. Let me know when you want to read that code, or better yet, write your own! &gt; Why does my skiplist library annoy you so much? Because that's the only thing you've ever don! It's the only piece of code you have for public scrutiny. Man up and make your own example for scrutiny. Skippy. Quit hiding behind vague hand-waving. &gt; take a step back and take a real look at what I'm saying What on earth **are** you saying?? * erasing objects from a vector doesn't call the destructor * you don't know what pre-allocation does * you don't know locality of reference means * fragmentation means non-sequential access * prefetch is the only cache optimization * prefetch doesn't mean predicting when data is needed and prefetching it, it means causing a cache miss and waiting for it to be fetched That's just black and white third party demonstrably wrong, and I've got the references for it! You are word for word contradicting quotes I've given you! The cognitive dissonance is astonishing! &gt; I benchmarked your version compared to Kranar and just as expected *WEAK*. Congratulations, you can compile a C++ program from the internet... I told you that's not my version for performance. That version was to show you that calling erase in the middle of a loop is stupid, and that it causes unnecessary constructor/destructor calls. For a guy who does a lot of claiming how right he is with no references, you are a bit slow on the up-take. Look, as far as I'm concerned, you've made no references, no explanations, no examples, no code, and done nothing but wave your hands. As far as I'm concern that means you've admitted you have no argument, and will continue to wave your hands until time immemorial. Cheers! It was definitely a study in something! :D
Just realized you *did* decide to man up and take a stab at it -- my bad. But that intellectually disingenuous, horrible experiment of yours -- your bad... It is premised **entirely** on * O( n^2 ) erase I said was stupid from [the beginning](http://www.reddit.com/r/cpp/comments/et0i6/why_doesnt_the_stl_have_an_unordered_alternative/c1awfvh) * cynically engineering an application to leverage that phantom deficiency The fact is you paid *no* attention to what a real application *might* look like. Your test is completely meaningless. However, after taking a moment to [fix your example](http://pastebin.com/ap1cYFxJ) -- to make it look like it might actually be a vector of work 1000000 items of which 33% are dispatched per round -- I still came out approximately **twice** as fast as you. Keep in mind, to falsify your uber-prefetch theory, all I need to do it *match* your performance without accessing sequentially. Later skippy...
Old thread, but... It's probably more "traditional" to put an identity-test into `swap` for heavy objects anyway. It won't always be a performance win because you'll test every time you swap objects of that type, even when you *know* they're not the same. Still, the code in things like this unordered vector will be simplified.
Not fast enough. int main() {return 0;}
Thankfully in C++0x we can pass lambdas into `std::remove_if`, so you can just std::remove_if(container.begin(), container.end(), [](auto i) { i.doSomething(); return i.isDead(); } ); Really, I don't know how anyone used `&lt;algorithm&gt;` before this stuff.
So you change the op's requirements. Nice. Your version is STILL so slow that it can't even finish this century when used in arbitrary insertions and erases. You just changed from general usage to a very specific usage that will likely never happen. Anyways, I'm done here. I especially like how you removed the iteration over the items after deletions are done. That was the whole point of demonstrating why your version doesn't do what you think it does. Nice try though. But your failure to admit when you're wrong is just to embarrassing. &gt; O( n2 ) erase I said was stupid from the beginning No one's ever suggested this. Only you. edit: HAHAAHAHAHAHAHAH Did you even RUN your program??????? HAHAHAHAHAHAHAHAH edit2: HTAHAHAHAHAHAHAHAH!!H@!@!!!@!one And also, HAHAHAHAHAHAHAHAHA and let's not forget... HAHAHAHAHAHAHAHAAH edit3: Oh, you said I have no other code online. Why does this matter to you anyways? I have MMX code I've submitted to the x264 codec used by videolan and tons of other software. I also have a few other tools and libraries here: http://csregex.sourceforge.net/ http://csparser.sourceforge.net/ But in case I forgot: HAHAHAHAHAHAHAHAH 
&gt; You just changed from general usage to a very specific usage that will likely never happen. No I didn't that's what he asked for here! http://www.reddit.com/r/cpp/comments/et0i6/why_doesnt_the_stl_have_an_unordered_alternative/c1aqsqk Since when is a work queue something that "never happens"? Holy shit you're dense! What is broken in your head?? &gt; Your version is STILL so slow that it can't even finish this century when used in arbitrary insertions and erases. That doesn't even make sense! The version I fixed after you made a mockery of it runs *faster* than yours! Your prefetch theory is objectively falsified! What is wrong with you?? Here is the one fact that's true: everything I link is directly contradictory to what you say, and that makes you several colors of fucking crazy.
Did you even run it? Your version is slower. I have access to a compile farm. Your version ran slower on every machine I tried it on. HAHAHAHAHAHAHAHAHAHAHAHAA edit: In case you didn't get it, HAHAHAHAHAHAHAHAHAHAHAHA 
HAHAHAHAHHAHAHAHA You're so full of shit. Somehow you'll forgive me if I don't believe a word of what you say. edit: My algorithm is O(1/logn)! HAHAHAHHAHAHAHAHAHAHAHAH
HAHAHAHAHAHAAHAH edit: ROFLMAO 
you're fucking crazy
HAHHAHAHAHAHAH Here's just one example, but they're all the same. 2x 2x2.0 GHz Opteron 2212 / 4GB RAM / Dell SC1345 / Debian x86-64 &gt;Linux gcc11 2.6.26-2-amd64 #1 SMP Thu Nov 5 02:23:12 UTC 2009 x86_64 &gt; &gt;The programs included with the Debian GNU/Linux system are free software; &gt;the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. &gt; &gt;Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent &gt;permitted by applicable law. &gt; &gt;gcc11:~$ vim test.cpp &gt; &gt;gcc11:~$ gcc -lstdc++ test.cpp -O3 &gt; &gt;gcc11:~$ ./a.out &gt; &gt;method1 &gt; &gt;Work Done: 33000601 &gt; &gt;Total: 16s &gt; &gt;method2 &gt; &gt;Work Done: 32994481 &gt; &gt;Total: 20s &gt; &gt;gcc11:~$ Every machine has similar results. But your version is always several seconds slower. I ran it several times with -O2 and -O3 and even -O0 HAHAHAHAHAHAHAAHHA Sorry man. You've been a riot. Full of shit. Big talk. But the data don't lie. Your version is consistently slower. Not only that, but your version uses tons of extra memory because of your allocator. 
HAHAHAAHAHAHAHAHA I never said it was 1/logn. It was either O(1) or O(logn). That's in the docs. I wrote it that way to save space. Not even part of the discussion. OH NOES!!! Pffftt... hehehe HAHAHAHAHAHAHAAH 
No, but apparently you lie. Intel Core 2 2.33GHz x4 $ uname -a Linux CALIBAN 2.6.35.10-74.fc14.i686 #1 SMP Thu Dec 23 16:17:40 UTC 2010 i686 i6 $ g++ -O3 memtest1.cpp $ ./a.out method1 Work Done: 32993284 Total: 25s method2 Work Done: 32991140 Total: 14s You're a loonie
Sorry, but I've tested it on many machines. HAHAHAHAHAHAHA Oh, and make sure to tell others to intentionally slow down other versions that you don't like. For example, using swap when a copy would do. I've mentioned a copy, oh I don't know, EVERY SINGLE TIME!!! Nice one. I really enjoyed that. Inducing a write when none was needed. HAHAHAHAAHAHHAHAHAHA Oh man, my stomach hurts. Really, try it on your machine. Use a copy instead of a swap. C'mon! Let's see it. Oh, and in case you forgot, HAHAHAHAAHAHA You know nothing about this subject. Why do you even pretend? That's all I want to know. Not only that, but did you even LOOK at your code? Did you see how ridiculous it looks and how much memory it wastes? ROTFLMAO You are seriously an endless supply of laughter. I thank you for that. 
&gt; Use a copy instead of a swap. Objects that define their own swap operator, those capable of move semantics, can be swapped more efficiently than being copied. It's best practise to prefer swap. You're confusing simulation with reality again. You're really good for my self-esteem you realize. :D
HAHAHAHAHAHAAHAHA You're inducing a write when none is needed. You really don't know shit about how the cache works. &gt; You're confusing simulation with reality again. Oh, so now the code doesn't mean anything? No, because that would burst your bubble that you live in. I really wish I knew why you continue this charade. It can only hurt you. You know that, right? HAHAHAHAHAHAHAHAHA A swap is more efficient than a copy? Now I've heard everything. I guess those assignment operators are there for show. AHAHAHAHAHAHAHAHAHAH At least you admit that my version is faster with a copy. 
&gt; Oh, so now the code doesn't mean anything? No, it's a simulation. It's one better than anything you've done in terms of making the discussion objective. But in the end, confusing a simulation for reality is dangerous. &gt; I really wish I knew why you continue this charade. I think you're the only actor playing charades... &gt; A swap is more efficient than a copy? You're not that bright are you? Imagine you have a vector that uses a pointer internal to the list. If you swap it, you can exchange the pointer in O(1) (actually one pointer copy). If it's assignment operator, you have no idea where the right hand side object came from, so you can't make such a destructive operation. C++0x will take it further to introduce move semantics which would make such an efficient assignment possible. Cheers!
If the element are Assignable, then it's fine to use assignment. That's what it's there for. The move semantics are so you get more efficient memory usage with temporaries. Learn to read for once. You're SO sad, it's unbelievable. I have a hard time believing that someone like you actually exists. I wish I could help you somehow. 
I'm on my way to designing a revolutionary language that stops all this bother...
Seems down. Anyone got a mirror?
There is always [google cache](http://webcache.googleusercontent.com/search?q=cache:E21j0w7BttMJ:www.daniweb.com/interviews/interview313564.html)
It might only have negligible differences, but that doesn't make up for the fact that iostreams are so frigging gross to use. If only printf could be made type safe. :/
&gt; If only printf could be made type safe. :/ with modern compilers it effectively is since any improper use of printf raises a compiler warning. using c++ variadic templates, it should be theoretically possible to make a printf style function w/o format string that can beat printf because of no format string processing, with emphasis on 'theoretically' :-).
The example he presents is about parallelism, not concurrency. 
With gcc the first two cases aren't actually testing printf, they're testing puts, due to gcc's [printf/puts optimization](http://www.acsu.buffalo.edu/~charngda/gcc_printf.html).
[Boost.Format](http://www.boost.org/doc/libs/1_45_0/libs/format/example/sample_formats.cpp)
Isn't it a well known fact that reference counting is slower than tracing garbage collection? The point of smart pointers is deterministic destruction, not performance. 
Boost has a type safe printf that will bring joy and unicorns to you and your family.
Ever see the South Park episode "Simpsons Already Did It"? Boost already did it.
Indeed. Using `shared_ptr` for large numbers of allocations is also not its use case. That's why it's not the only "smart pointer" in the library.
More like Wesley Crusher. 
got any handy links? i don't do c++ professionally any more and don't keep up with all the latest stuff. did they work it out so that if you pass literal arguments it compiles out the function completely? edit: found [this](http://www.cs.brown.edu/~jwicks/boost/libs/format/doc/choices.html)
If you are outputing a struct, [Boost.spirit](http://boost-spirit.com/home/) is even faster.
Try my c++0x flavor of print/sprint... Although a bit slower than printf/sprintf it's type safe and from 3 to 5 times faster than boost::format... http://code.google.com/p/nicola-bonelli-repo/source/browse/trunk/codes.cpp0x/print.hpp :-)
Boost format is pretty nice, but with c++0x variadic templates you can actually write a printf function that functions the exact same as printf while still being typesafe. It's actually one one of the examples on the [wiki page for c++ox](http://en.wikipedia.org/wiki/C%2B%2B0x#Variadic_templates).
Fair enough. Isnt that a bit of splitting hairs though? The example code could quite easily be tweaked to be about concurrency. Say if instead of calling async on find_all methods. You would call async on say an display_to_window method, where each chunk of data was logically independent and written to its own window. Say if each chunk represented text from different windows in an text editor. That would be concurrency, right?
&gt; improper use of printf raises a compiler warning. Unless the format string is constructed at runtime.
this also raises a warning (optionally via -Wformat-nonliteral). it's considered very bad style -- not type safe and a security risk.
http://stackoverflow.com/questions/257418/do-while-0-what-is-it-good-for
TLDR: It syntactically turns a a bunch of statements in to something that can be treated syntactically as a single statement, Which is perfect for macros so that they behave correctly if someone does if(statement ) FOO_MACRO; Outside of macro definitions, I can't see why you would do it.
Yep, it looks that way.
That was...remarkably succinct. Thanks Keveman. I had figured this would be a little esoteric, but it wasn't.
 do { if(...) break; if(....) break; do stuff... } while(0); Basically, it's a goto statement without a label. 
Since this is a C++ question in a C++ subreddit, it should be emphasized that that idiom is a C legacy, and it is a huge hack, the kind that looks very ugly in a well-structured modern C++ code. There is no reason to use this in C++ today, instead, you should use inline functions, templates or exceptions, whatever is more appropriate for the situation.
You could get the same effect with some polymorphic jiggery-pokery, or even a switch statement.
It's an old C trick, so you can't do polymorphism. A switch would do something different.
Just because you use Visual Studio doesn't mean that you don't care about portability :-)
Exactly right. If I saw something like this in my codebase I'd replace it with a `goto` immediately - less code, less indentation, more readable and more versatile.
I disagree. A goto label is far worse than a do { } while loop. At least with a while loop you have a set of curly braces to clearly define the scope, whereas a goto you could be jumping pretty much anywhere. Do not use goto... like, ever.
I disagree; gotos are useful and correct in a few circumstances. The big problems are when they are used for complex control flow. That being said, additional variable scoping is useful. For non-trivial functions that need a coalesced cleanup region, do { } while (0); can be a good way to accomplish it. As with almost any feature, there's a correct use, and that's not most of them.
You raise an interesting point - there is a semantic difference between using a `goto` there and using the `do`/`while(0)` trick - the scope changes, and objects on the stack will get destroyed. For most purposes that won't matter, of course, and when it does you could always put the braces in yourself: { if(something) goto end_of_scope; //... } end_of_scope: Of course, the label isn't "tied" to the scope like the `while(0)`, so you arguably *could* cause a mess if you were being silly. Really, though, the benefits of the `goto` show themselves when you get more complicated things going on: { for(auto i = vec.begin(); i != vec.end(); ++i) if(i-&gt;is_broken()) goto end_of_scope; //... } end_of_scope: is much more readable than do { bool broken = false; for(auto i = vec.begin(); i != vec.end() &amp;&amp; !broken; ++i) if(i-&gt;is_broken()) broken = true; if(broken) break; //... } while(0); There are a few ways to write it without a `goto`, of course, but I'm not sure any are as clear as writing it with one. Moreover, I imagine they'd rely more heavily on compiler optimisations to generate decent code.
 do { for(auto i = vec.begin(); i != vec.end() &amp;&amp; !broken; ++i) if(i-&gt;is_broken()) break; } while(0); looks much nicer, and probably optimizes much better as well. Using goto can sometimes destroy lots of nice compiler optimisations, which is another bad mark against it. I have only been developing in C++ professionally for about 3 years now, but I have never used a goto statement. In fact, I've never even wanted to use a goto statement. As an aside, if you are developing on 64bit architecture, there is little or no overhead to setting up exception handlers, (if you throw one, there is still a performance penalty, though its not that large), so if the handler is for a fringe case or rare branch then I would say to use exceptions instead of goto.
I have yet to put myself in a situation where I need a goto statement, though this may be just because I haven't been coding C++ long enough...
&gt; do { &gt; for(auto i = vec.begin(); i != vec.end(); ++i) &gt; if(i-&gt;is_broken()) break; &gt; //... &gt; } while(0); The `break` in this code jumps out of the `for` loop, not the `do`/`while(0)`. It doesn't make use of the `do` construct at all, and any code in the "`//...`" section will still be executed even if `is_broken` is true for an element of `vec`. In Java we could use a [named loop](http://www.informit.com/library/content.aspx?b=STY_Java2_24hours&amp;seqNum=96) to get the desired behaviour. In C and in C++ we don't have named loops. Instead, we can make the loop logic more complicated, we can [fake it with macros](http://en.wikibooks.org/wiki/More_C%2B%2B_Idioms/Named_Loop) (that use `goto`...) or we can man up and do it "the right way".
This is why I mentioned exceptions
Not at all. You can write code so that you never **need** a goto. The question to ask yourself is "Would this code be easier to understand/maintain if I used one instead?"
Many simplistic garbage collectors also have no way of specifying weak references (for example, an object A has a boost::shared\_ptr to a child object B and B has a boost::weak\_ptr to the parent A). So you end up either doing research to ensure that your language's collector can detect and break cycles, or doing a lot of complex cleanup to ensure both objects get freed--which defeats the whole point of automatic memory management. I'm curious if the results for shared\_ptr's performance are with thread-safety disabled. In recent versions it's turned on by default, and using atomic instructions for increment/decrement when they aren't necessary is kind of stupid. I'm also curious what the performance would be like if he had used shared\_ptr with a Boost object pool and a custom deleter that returns the ptr to the pool. All those tiny heap allocs and deallocs don't sound very optimized. Edit: using make\_shared might also have some benefit. I didn't look at his code.
When I read the title of that post I heard it in the tune of 'War' as sung by Edwin Starr
&gt; Many simplistic garbage collectors also have no way of specifying weak references Weak references are not really required in most garbage collection systems. In Java, ghost/phantom/weak references are used for cache timeouts mostly. &gt; So you end up either doing research to ensure that your language's collector can detect and break cycles, or doing a lot of complex cleanup to ensure both objects get freed--which defeats the whole point of automatic memory management. Tracing collection algorithms have been well understood since many years ago. There is no research in the area you mention. Most gc research is about optimization/real time guarantees. &gt; I'm curious if the results for shared_ptr's performance are with thread-safety disabled. Improved, but still many times slower than real gc. &gt; I'm also curious what the performance would be like if he had used shared_ptr with a Boost object pool and a custom deleter that returns the ptr to the pool. All those tiny heap allocs and deallocs don't sound very optimized. Indeed. Malloc/free has a large overhead. Still, real gc is faster. I've benchmarked it against Java; you don't have any idea how fast gc is. It's the fastest method to manage memory. The only problem with real gc (vs reference counting) is determinism. 
&gt;Using goto can sometimes destroy lots of nice compiler optimisations I believe the main ones it would kill are, loop strength reduction, loop jump minimization, and branch reversal. and induction variable elimination. Although, I think most compilers build the data flow graph and find loop-like behavior so some would probably not be completely messed up if you did something fairly reasonable. Strength reduction seems like the one that would be the hardest to get right. On the other hand, by using do;while in the place of a switch when one is appropriate though you might break some switch optimizations (most switches are broken into a jump block, so you have 2 jumps instead of a bunch, with non-sparse switches).
a switch statement is not a loop. This construct loops continually until some condition is satisfied. For example, it might look for an escape key being pressed and check for that, or a mouse click, etc. 
I managed to use C++ and for well over 10 years without using a goto, and C for several years before that. I would feel very awkward using a goto; I might even have to look up the syntax to remember exactly how they are used.
&gt; There is no research in the area you mention. Research as in reading about the language's implementation, not doing original research on garbage collection theory. &gt; I've benchmarked it against Java; you don't have any idea how fast gc is. It's the fastest method to manage memory. Depends on the situation. In most cases, yeah, the modern JVM is fast enough. I can't go into specifics without outing the project I worked on, but there definitely are applications of memory management (talking about gigabytes of objects which are maybe 32-64 bytes in size each), where garbage collection in Java was doing so poorly that we had to give up and rewrite the whole project in C to bring the runtime down from ~30 minutes to 45 seconds.
I think you need some more experience or something. Changing any code "immediately" just to suit your tastes is a bad thing in most environments. It adds unnecessary changes to be looked at for source code control, it should be fully tested for any side effects or errors introduced, etc. Not only that, but replacing anything with a goto would indicate to most experienced developers that you are an inexperienced hacker.
Absolutely nuthin'! Say it again, y'all. do while 0, good God! What is it good for? Absolutely nuthin'! Listen to me ... 
/agree I would like to add, if you can out optimize a compiler than you are not a beginning programmer. 
papers.html: http://page2rss.com/rss/f3a5661e71b236859b9ec911da5a1085 interviews.html: http://page2rss.com/rss/d9a1834e2449c3a591422809c77d164a And here is one for http://www.open-std.org/jtc1/sc22/wg21/ http://page2rss.com/rss/3a8a90409644d0cf8cf50930321a4f07 
The audio is quite bad until 11 minutes in (when the mic turns on), so just skip there or use the lecture notes (which is a good idea anyway). This is a very good lecture well worth the download. Lots of c++0x stuff and c++ history. Its also got some juicy sound bytes like: *"I was unwilling to use MS or GNU [C++ compilers] during the 90s [because of the very poor standards compliance]."* and *"you cant do things reasonably in C [because of the poor type system]."*
I recognize Bjarne Stroustrup, Andrew Koenig (Accelerated C++), and the stapler guy from office space. Photo is from 1st ANSI X3J16 technical meeting (Somerset, NJ) Mar 1990. Edit: I found the [meeting journal](http://webcache.googleusercontent.com/search?q=cache:yY5KHoDC_wQJ:ftp://std.dkuug.dk/mirror/www.maths.warwick.ac.uk/cpp/iso/minutes/90-0028r.txt) with attendee list. Steve Dewhurst (C++ Gotchas), Pete Becker (The C++ Standard Library Extensions), Greg Comeau (Comeau compiler) and Bruce Eckel (Thinking in C++) are in the photo somewhere also. I think the long haired dude in the middle is Pete Becker. 
And theres the issue of the getlocale() having an internal lock that is held on every single streambuf thats constructed (even if its for binary use only!). This does drag on multithreaded programs that try to use lots of temporary streams. output doesn't bother me as much...reading text files/streams is really bad.
See the [FQA](http://www.yosefk.com/c++fqa/const.html). &gt;You have to read pointer declarations right-to-left. &gt;Fred const* p means "p points to a constant Fred": the Fred object can't be changed via p. &gt;Fred* const p means "p is a const pointer to a Fred": you can't change the pointer p, but you can change the Fred object via p. &gt;Fred const* const p means "p is a constant pointer to a constant Fred": you can't change the pointer p itself, nor can you change the Fred object via p. One thing I know for sure is you should never, ever cast const away. 
The FQA was actually the first place I looked as well as the [C++ FAQ Lite](http://www.parashift.com/c++-faq-lite/const-correctness.html). I believe that your last line is incorrect, no? const_iterator myClass::find(NEntry const * dest) const Should be the above, correct? *dest is what I don't want changed. But I mentioned how I cannot remove the &amp; because the generic container can still use non-pointer elements. Perhaps there is a way to partially specialize for pointers so that the &amp; can be removed? edit: You changed your comment. I understand perfectly how const works and how to read it. That's not my question at all. It's that with a non-pointer type, 'const TYPE &amp;' gets converted to 'TYPE const &amp;' which is the same thing. But with a pointer type, 'const TYPE * &amp;' gets converted to 'TYPE * const &amp;' which is different than what I want as 'TYPE const * &amp; '. The prototype for the wrapper function will not change. There's no need. Its prototype is exactly what is required. I need a way for the correct prototype to show up in the generic container, or some other way to make this work. edit2: Oh, I see. You want me to change my wrapper method to have the same argument. That would work, but then the pointer would be const, not what it points to. I'm looking for a way to make what it points to be const. I may have no choice in the end, but I'd like to see if anyone knows of a way before I give up. 
did that, seems to not appreciably change anything: results here with core i7 860 cout with &lt;space&gt;endl 1136.803199 ms cout with ' \n' 227.650442 ms printf with ' \n' 220.983406 ms cout with string constant and endl 1388.838893 ms cout with string constant and '\n' 222.329302 ms printf with string constant and '\n' 328.033423 ms cout with double and endl 4450.712287 ms cout with double and '\n' 3245.503995 ms printf with double and '\n' 2128.181017 ms seems std::endl shouldn't be used so freely, just explicit calls to "flush" only. I do lots of multi threaded work so I explicitly flush in those critical areas as needed. Also another good test would be to print to memory buffer only but that's lost with printf. I find myself printing large text files to memory buffer first then block dumping those to disk. That definitely makes a difference especially in windows when writing files to a network share.
&gt;*dest is what I don't want changed. If I follow the FAQ correctly (it's been a decade since I've touched C++, sorry :) ), it would be: const_iterator myClass::find(NEntry* const dest) const You cannot change the pointer (like a reference) but you can change dest via the pointer, as in const_iterator myClass::find(NEntry* const dest) const { dest++; // error, dest cannot be modified dest-&gt;foo(); // foo is non const and changes dest's state so OK ... } &gt;edit2: Oh, I see. You want me to change my wrapper method to have the same argument. That would work, but then the pointer would be const, not what it points to. I'm looking for a way to make what it points to be const. I may have no choice in the end, but I'd like to see if anyone knows of a way before I give up. In that case you already have it ;) const_iterator myClass::find(NEntry const * dest) const That looks right to me. This makes dest point to an immutable NEntry. edit1: And sorry for changing my comment. I realized I was completely wrong and didn't want to point you in the wrong direction. 
I think you want a reference to the const pointer to the const TYPE, no?
Thanks for the reply, but I've mentioned several times that 'NEntry const * ' is what I want. Here is what I wrote in my original post: &gt; Is there a way to convert 'NEntry * ' to 'NEntry const * ' when NEntry * is defined within a typedef? Thanks. edit: I found a solution though it's not exactly what I was looking for. Since the key is actually 'NEntry* ' and not what is pointed to it, then the generic container is actually correct. So I've redefined the key to be 'NEntry const * ' and that ends up with the following prototype in my generic container: const_iterator find(NEntry const * const &amp; k) const I don't care about the pointer itself being const, but since this work, I'm happy. Still like to know if there's a way to convert 'NEntry * ' to 'NEntry const * ' when 'NEntry * ' is defined in a typedef. It may not be possible. 
Yup, just found it. The object isn't the key, it's the pointer. So the container was actually correct making the pointer immutable instead of the object. That I don't want the object to change is my own personal requirement. Changed the key to 'NEntry const * ' and it works. That I didn't care about the pointer changing is what was throwing me off. It doesn't matter to the wrapper, but it does to the container since it requires an immutable key. 
&gt;Is there a way to convert 'NEntry * ' to 'NEntry const * ' when NEntry * is defined within a typedef? One way would be to alias it into another pointer. class NEntry { int foo() const; int bar(); } NEntry *dest = ...; NEntry const *immutableDest = dest; dest-&gt;bar(); //ok immutableDest-&gt;bar(); //error, bar is not const 
What type is `key_type` for your container (of for the instantiation in question)? The reasonable interpretations from the code you've posted are that either `key_type` is `NEntry *` or `key_type` is `NEntry const *`. I'd guess that it's the former (`NEntry *`) rather than the latter, because otherwise standard conversions would dtrt for you. If that's the case, then the problem is obvious -- your first `find()` function expects `NEntry *` but you're trying to give it `NEntry const *`. Those types aren't convertible. Assuming the above, change `key_type` to `NEntry const *` or change const_iterator myClass::find( const NEntry *dest) const to const_iterator myClass::find( NEntry *dest) const
Yes, you identified the problem exactly. I have a key type of 'NEntry * ' when I should have had 'NEntry const * ' as my key type. It's strange though because without the &amp;, the type would have automatically converted to 'NEntry const * ' when using pass by value. &gt; Assuming the above, change key_type to NEntry const * Exactly! Thanks for the confirmation. 
What threw me off is that I'm used to 'const TYPE * ' being the same as 'TYPE const * '. This is true in general, but when you have a typedef (call it mytype) of 'TYPE * ', the following have different effects. void myfunc(const mytype val); void myfunc(const mytype &amp;val); These will convert to the following respectively: void myfunc(TYPE const * val); void myfunc(TYPE * const &amp;val); Those are very different types and is what threw me off. 
[I found that guy from Home Alone](http://i.imgur.com/K524D.jpg)
Lol! Thats quite good. Thats [Andrew Koenig](http://www.google.co.uk/images?sa=1&amp;q=andrew+koenig+c%2B%2B) by the way.
Heh, go figure. I have that book; Accelerated C++ is excellent.
&gt; One thing I know for sure is you should never, ever cast const away. Not really. Casting away const'ness is a common way to implement getter methods: struct Bar { Foo* GetFoo() { return const_cast&lt;Foo*&gt;(static_cast&lt;const Bar*&gt;(this)-&gt;GetFoo()); } const Foo* GetFoo() const { return m_foo; } }
I have it tooooooo woooooooooo haven't gotten past like page 30. I have to get back into that, damn..
The real question is "what in the fucking hell is that thing in the back row on the left?" The thing in the tight white long-sleeved turtleneck. I honestly keep switching back &amp; forth. Just when I think I have it figured out, I switch my guess. Ultimately, I think it's a male with small man-breasts... Also, on a completely unrelated note... Dan Saks is also in the picture. Anyone who works in embedded systems knows his name. 
You've also got the short guy in the middle of the frontmost standing row, hitting the trifecta: pocket protector (with about 70 pens), beard, and glasses.
Greg Comeau is looking over the left shoulder of the guy in the vertically striped shirt in the back row. Sam Druker is the tallest guy in the back row, 4th from the right. Dan Saks has the dark curly hair and full beard kneeling in front, Bjarne is behind him. That could be Bruce Eckel standing behind Bjarne. Of course, that's Andrew Koening in front with the curly hair and beard. I recognize several other faces, but am not sure of their names.
I think I recognize 2 faces.. The tall one on the front row to the left is Jim Roskind. At netscape, he always had awesome stories to tell about pretty much anything. He'd often get a crowd of interns listening to him. The one behind and to the right of the front middle guy reminds of another netscapee named Chuck. Can't remember his last name for the life of me though. 
Talking of juicy sound bites, this one (at 00:55:21) had me in stitches: &gt; […] Facebook is converting from these flexible scripting languages that are supposed to be good for you, by translating into C++ to keep their heating cost down... basically.
&gt; Isn't it a well known fact that reference counting is slower than tracing garbage collection? I have seen several threads of discussion among C++ programmers on the web where people implied or stated that `shared_ptr` and `intrusive_ptr` are efficient forms of garbage collection. A C++ developer in London asked me what the relative performance was compared with GC'd languages. I believed that reference counted smart pointers would be significantly worse but I had no data to quantify it so I did this comparison. I think we were all surprised how large the performance gap is. Furthermore, many people seem to believe that these reference counted smart pointers are slow because they use `malloc` and `free` when, in fact, far more time is spent manipulating the reference counts than in `malloc` and `free`. 
&gt; Indeed. Using shared_ptr for large numbers of allocations is also not its use case. That's why it's not the only "smart pointer" in the library. [I also benchmarked Boost's thread *unsafe* `intrusive_ptr`](http://flyingfrogblog.blogspot.com/2011/01/sweeping-700-faster.html) and found that it is not much better in this case. Matching the performance of a generational GC from C++ on these kinds of problems is *very* hard. 
&gt; So you end up either doing research to ensure that your language's collector can detect and break cycles That may be true of some GCs but it is certainly not true of any of the GCs I have ever used and has been a solved problem since the invention of mark-sweep in 1960. &gt; I'm curious if the results for shared_ptr's performance are with thread-safety disabled. In recent versions it's turned on by default, and using atomic instructions for increment/decrement when they aren't necessary is kind of stupid. Disabling thread safety improved performance significantly but it was still 6&amp;#215; times slower than OCaml's generational GC and 9&amp;#215; slower than stack allocation. &gt; I'm also curious what the performance would be like if he had used shared_ptr with a Boost object pool and a custom deleter that returns the ptr to the pool. All those tiny heap allocs and deallocs don't sound very optimized. &gt; Edit: using make_shared might also have some benefit. I didn't look at his code. Neither of those changes affected the performance significantly. The main optimizations to my original were: * Disable thread safety: 40% faster. * Pass by reference as much as possible in order to minimize the number of reference count bumps: another 20% faster. * Use `intrusive_ptr` to move the reference count into the object itself: another 25% faster. The most optimized version so far is still 4&amp;#215; slower than OCaml's generational GC and 5.7&amp;#215; slower than stack allocation. 
&gt; Indeed. Malloc/free has a large overhead. Still, real gc is faster. I've benchmarked it against Java; you don't have any idea how fast gc is. It's the fastest method to manage memory. My results bring two of those points into question. Firstly, the `malloc` and `free` solution was only 2&amp;#215; slower than OCaml. Is that really a "large overhead"? Secondly, the fastest solution to date exploits the FIFO allocation-deallocation order of this benchmark to use a stack so GC is not always "the fastest method to manage memory". I agree that modern GCs are very fast and hard to beat though, and I grant you that using a stack is not a general-purpose solution. 
&gt; Depends on the situation. In most cases, yeah, the modern JVM is fast enough. I can't go into specifics without outing the project I worked on, but there definitely are applications of memory management (talking about gigabytes of objects which are maybe 32-64 bytes in size each), where garbage collection in Java was doing so poorly that we had to give up and rewrite the whole project in C to bring the runtime down from ~30 minutes to 45 seconds. You could say the same thing about my prototype garbage collectors in C++. Just because I wrote them in C++ doesn't mean they aren't GCs. How were you managing memory in your C version? 
A 3rd? (after D2 and Rust) - cool! 
Also Matt Wilson's Fast Format: http://www.fastformat.org/performance.html
If anyone interested you may find this article on: http://dbp-consulting.com/StrictAliasing.pdf
I was going to say something about the stack not being a general-purpose solution, but you beat me to it. apples and oranges are not comparable, are they? 
Explain to them that the problem is the bus lock: acquire/release semantics means that the main memory has to be updated synchronously with the caches, so as that all CPUs see the same value. If you lock/unlock the bus continuously, you can easily kill any application.
That is because Java does not have value types. Granted, C/C++ can take the lead in algorithms that can use value types allocated locally. In algorithms that data must be allocated on the heap though, Java is faster overall. 
That doesn't explain why `intrusive_ptr` with no locking is also really slow. At the end of the day, the real problem is the touching of the reference counts whether it is locked or not. 
&gt; apples and oranges are not comparable, are they? That does not justify your statement that "[Garbage collection] is the fastest method to manage memory". Using a stack is still a counter example to your statement. 
As much as I like any kind of libraries in C++. TBB needs a better love in documentations on programming models.
But you can't use the stack for garbage collection. Stack = oranges, gc = apples. They cannot be compared. 
Yes, the touching of the reference counts cause cache changes. It's that and the bus locking/memory synchronization. 
I truly wonder if c++ smart pointers (or boost smart pointers) are actually used in any big projects. 
&gt;One thing I know for sure is you should never, ever cast const away. I disagree with this, but I also tend to disagree with "never ever" rules anyways. One use of const_cast is to create a barrier/firewall between a producer and a consumer or server/client. The const is used as a way to give read-only permission to the consumer, but the producer has read-write permissions. Thus the consumer will only ever get a const instance of an object, but when they pass that object to the producer, the producer uses a const_cast to perform any changes as needed.
That will result in infinite recursion. In the "GetFoo() const" method, the type of the this pointer is "const Bar*", meaning that this-&gt;GetFoo() cannot call the non-const version.
Compiled vs interpreted doesn't necessarily imply anything about backwards compatibility. For one thing note that the most popular version of haskell is compiled(ghc), also that languages can have both compiled and interpreted implementations.
If I asked someone to do a roman numeral program in a technical interview and their first thought was to jump to that templated monstrosity, it would be "thank you for your interest; we'll keep your application on file." Christ almighty.
If you were hiring people to maintain your own templated monstrosities, maybe?
So, basically, you write C.
No, you can still use templates, objects and all. Just not exceptions or rtti.
I'm curious as to why exceptions wouldn't work
Why would I write templated monstrosities?
It sounds weird... try and catch are supposed to be part of the language core and not the standard.
I searched and didn't find any justification, understandably you won't be able to use std::exception but nothing about the try and catch...
In gcc (g++) stack unwinding uses support functions that are provided by libstdc++ (and others from libgcc_s). I don't know the details, but at runtime the program needs to locate `auto` storage duration objects that require destruction and needs to locate exception handlers. Nothing in the C++ standard requires this design. A different compiler could generate it as compiler-provided boilerplate and another platform could treat it as service provided by the OS. Different implementations will have different strengths and weaknesses. If he'd implemented the necessary functions himself (as he did for `operator new` &amp; `operator delete`) then he probably could have gotten exceptions working too (at the cost of more work, confusion, complexity, and likely ABI incompatibility. Edited to add: [Here's some standards information for the Linux AMD64 exception handling ABI](http://refspecs.freestandards.org/LSB_2.0.0/LSB-Core-AMD64/LSB-Core-AMD64/libgcc-sman.html). Edited to add (2): **tita75** points out that these functions are provided by libgcc_s. Additionally, they are language-independent, not C++-specific. There are C++-specific exception-handling functions which are provided by libstdcxx (by default) or alternatively by [**libsupcxx**](http://wiki.osdev.org/Libsupcxx), which is smaller. [Here's some info on the Itanium C++ exception-handling ABI](http://www.codesourcery.com/public/cxx-abi/abi-eh.html#cxx-abi) for anybody curious. Linking **libsupcxx** instead of **libstdcxx** would be an intermediate step that would be less severe than what the article describes with less loss of functionality.
You can use stacks for garbage collection. In fact, region inference GCs like the one in MLKit do exactly this. Note that this was never about using "the" stack but, rather, the stack data structure. 
Doesn't it means that not all the implementations of C++ will act this way? So how does "STL port" \ Boost \ Qt \ loki \ etc manages to exist? If I'm developing using STL port, do I still need to link to libstdc++? 
&gt; If I'm developing using STL port, do I still need to link to libstdc++? Short answer: yes, you still need to link to libstdc++ if you're building C++ apps with gcc. Detailed answer: you could substitute another library that implements the required interface if you were so inclined, but I don't know of (read: haven't looked for) an existing project that does so. STLPort implements the algorithms and containers libraries specified in the C++ standard (commonly known as the STL). It does not implement the full C++ library and doesn't provide the libstdc++ interface required by gcc's c++ compiler. Boost, Qt, and loki are separate libraries. They add to an existing C++ implementation and depend on language and library features but they are not substitutes for libstdc++. &gt; Doesn't it means that not all the implementations of C++ will act this way? So how does "STL port" \ Boost \ Qt \ loki \ etc manages to exist? Those libraries (and C++ programs in general) depend on the language implementation providing certain features. They don't depend on the *specific mechanism for implementing* those features. That's the difference between [A**P**I](http://en.wikipedia.org/wiki/Application_programming_interface) and [A**B**I](http://en.wikipedia.org/wiki/Application_binary_interface). If you build from source, Boost, loki, STLPort, and Qt depend on certain A**P**Is. Linking object files or making shared objects interoperate is more tricky and requires A**B**I compatibility.
Ok then. Please ignore my comment.
Why else are you in /r/cpp? ;p You might have judiciously chosen an acceptable subset of C++ for your projects, but if you're a company, you're likely to have some over-designed eldritch behemoth that you have to support. 
Just you wait and see! I think you've named it for me! A3^rd
&gt; Short answer: yes, you still need to link to libstdc++ if you're building &gt; C++ apps with gcc. So (at least in GCC) the C++ core and the STL implementation are coupled? what is the status in other compilers? (MS and clang for example) I gave Boost, Qt, Loki as examples for libraries that has an implementation for wide variety of cpp vendors and STL implementations.
I used to think something like this would be good. I then realized that if you're not going to use the whole fucking language including RTTI, exceptions, and the standard library, you should probably be using a different language. Which brings me to the stock Android SDK which removes RTTI and exceptions for some unknown goddamn reason. I would love to kick the guys' who came up with that idea in the fucking balls. I might as well be writing code in C, except that I can't because I have four hundred fucking thousand lines of C++ code, which guess what... use fucking RTTI and exceptions. 
In Visual C++, throwing an exception is implemented as a call to [`_CxxThrowException`](http://msdn.microsoft.com/en-us/library/ff795730.aspx), which is part of Microsoft's C++ runtime. I don't know what clang does, though it's probably similar. &gt; So (at least in GCC) the C++ core and the STL implementation are coupled? This is not 100% correct, I'll break down the reasons: First, the STL isn't equivalent to the C++ libraries. The STL is a subset of the C++ libraries and gcc's exception-processing machinery is not tied to the STL at all. Additionally, it is possible to replace that subset of the C++ libraries with an alternative implementation, like STLPort or Dinkumware's implementation. In that case, you'd be using libstdc++ to implement exception delivery and STLPort for `std::vector`, `std::adjacent_find`, etc... Second, gcc's exception-processing machinery *does* depend on libstdc++ and libstdc++ does implement parts of the C++ library. But they don't *need* to be packaged together -- that is an implementation detail.
Wait I think that maybe I'm missing a small thing... the libstdc++ is the implementation of stl? or is it the equivalent to the ms runtime library?
&gt; that templated monstrosity What "templated monstrosity"? He just uses templates to make the algorithm work for both wide and "narrow" char types. How would you do it without templates. With macros? 
&gt; A different compiler could generate it as compiler-provided boilerplate and another platform could treat it as service provided by the OS. Different implementations will have different strengths and weaknesses. FWIIW, Microsoft's VC++ also requires the C++ std library to be linked to use exceptions.
The user of that function doesn't have to care about template machinery, so why does it matter?
&gt; I also benchmarked Boost's thread unsafe intrusive_ptr Good for you. I see lots of numbers, but no source code. Synthetic benchmarks are nigh useless without reproducible, representative samples. Your benchmarks are badly misusing `shared_ptr` anyway, so of course you're not getting the same kinds of performance. I'd even go so far as to say that you're putting the wrong kinds of expectations on smart pointers; they're not GC, and treating them like an all-purpose GC analog is asking for trouble. In your `shared_ptr` example, you're scattering ownership of the pointer about (by copying `shared_ptr`s), and doing so in all of the places where you should be taking a `weak_ptr`. This is having a severely negative impact on your benchmark, far more than you likely ever considered. Moreover, I'd contend that this particular style of code might be better served by holding and manipulating a reference to a `std::list&lt;scoped_ptr&lt;T&gt; &gt;`. C++ isn't a very good functional language and treating it as one _can't_ result in good performance. `shared_ptr` and `intrusive_ptr` are not the only smart pointers in the library, and this is so for a reason.
I've always thought that if you need RTTI, then you're doing it wrong. I would make exceptions in rare cases (e.g. I'd prefer to add a 100-line class and use RTTI instead of refactoring 10,000 lines of existing code to not use RTTI), but in general, I've gotten by without using RTTI. I've found that the requirement for RTTI is when classes are designed badly, and that this usually results from forcing the use of classes where they are not necessary.
I'll warn you that gcc and related projects aren't my specialty, so the deeper your questions go the greater the chance that I'll get something wrong. That said, here's my understanding of it: I'm not 100% sure whether exception support functionality is properly part of glibc or libstdc++. If you're really interested and have a linux system, you should be able to get a definitive answer by dumping symbols. From the official libstdc++ project page: &gt; [This is an ongoing project to implement the ISO 14882 Standard C++ Library as described in chapters 17 through 27 and Annex D. Participation is welcome!](http://gcc.gnu.org/libstdc++/) In reductionist terms, it's a bunch of header files, a static library file (libstdc++.a), a shared object (libstdc++.so), and documentation. The C++ standard library is a big thing, defined over 11 chapters (356 pages) of the 2003 C++ standard. Going chapter by chapter, the C++ standard library includes: 1. Language support library 2. Diagnostics library 3. General utilities library 4. Strings library 5. Localization library 6. Containers library 7. Iterators library 8. Algorithms library 9. Numerics library' 10. Input/output library Out of those 10 items, 3 or 4 of them are also known as the STL: Containers library, Iterators library, Algorithms library, and sometimes parts of the General utilities library. The other parts, like Input/output library are not considered part of the STL, but they are still part of the C++ standard library.
If you use a component model that works by implementing interfaces (think COM, XPCOM, etc.), you can roll your own RTTI-like framework (e.g. QueryInterface), but if you know you're going to be in a homogeneous C++ environment, you can skip all that and let the compiler do that work for you. I had a QueryInterface model for a while, but after doing some performance tests, native RTTI and dynamic_cast was faster than I could make QueryInterface work. Sometimes I kick myself for that, but at the end of the day, it makes the code a lot cleaner. 
I think that the latest NDK is bringing back full C++ (exceptions and the STL at least.. uncertain about RTTI).. it's been all the buzz today. But yeah, that was the first thing I thought when I first started looking at the NDK. No exceptions? No STL? Why the fuck am I even bothering? I want to use C++, not some impotent offspring.
This article is confused at best, and probably misleading for most people. The author is confusing incomplete types with incomplete template instantiation...
But the website theme is Absolutely F-A-B-U-L-O-U-S !
&gt; I see lots of numbers, but no source code. Synthetic benchmarks are nigh useless without reproducible, representative samples. That's why I posted the source code in the comments two weeks ago. &gt; Your benchmarks are badly misusing shared_ptr anyway, so of course you're not getting the same kinds of performance. In other words, the "correct" way to use `shared_ptr` is to avoid it. You seem to have misunderstood the objective of this exercise, which was to measure the performance of `shared_ptr`. I was asked to quantify the relative performance of Boost's `shared_ptr` vs a traditional garbage collector so I measured it and presented my results. There is obviously no merit in following your advice to avoid `shared_ptr` in the context of a benchmark that has the sole purpose of measuring the performance of `shared_ptr`. &gt; I'd even go so far as to say that you're putting the wrong kinds of expectations on smart pointers; they're not GC, Smart pointers are used as a rudimentary form of GC that has a variety of problems, awful performance being one of them. More broadly, reference counting is used as a form of garbage collection and these results reflect the performance capabilities of straightforward reference counting. &gt; and treating them like an all-purpose GC analog is asking for trouble. In your shared_ptr example, you're scattering ownership of the pointer about (by copying shared_ptrs), and doing so in all of the places where you should be taking a weak_ptr. This is having a severely negative impact on your benchmark, far more than you likely ever considered. Not only did I already explain that in the article, I even quantified it. 
In your day to day work it is hard to find cases where you have to use RTTI, but I think that for library\framework developers those are must have tools. 
OK, if you don't want to link in the IOSTREAMS and STL, that's one thing. But you don't have to give up the rest of the runtime support code for RTTI and EH. Just link against libsupc++ explicitly instead of libstdc++. Building your own static archive that contains modules extracted from libstdc++ is just plain ridiculous.
Regarding the problem with multi-line conditions, here is my preferred solution for C: if ( cond1 == "val1" &amp;&amp; cond2 == "val2" &amp;&amp; cond3 == "val3" &amp;&amp; cond4 == "val4" ) { do_something() } The brackets are still in K&amp;R style and the condition is clearly separated from the body. It takes up two more lines, however.
Am I the only one who doesn't have any trouble whatsoever reading either brace style? I think people give this issue a little too much undue attention. Code consistency is important but there are so many more important readability issues to worry about than where to put your curly braces. Just my opinion.
In My-Own-Style®, ALL line continuations always get at least two indents, while common blocks get one indent. x = sqrt(pow(a, 2) + → → pow(b, 2)); if (pow(a, n) + pow(b, n) == → → pow(c, n)) { → code... } This style makes line continuations very visible, even if they don't align precisely. Perhaps not good for someone with OCD, but it is fine to me.
I break the expression just _prior_ to the binary operator: if ("val1" == cond1 &amp;&amp; "val2" == cond2 &amp;&amp; "val3" == cond3 &amp;&amp; "val4" == cond4) { do_something(); } else { do_something_else(); } The first line looks unfinished because it doesn't end in a brace or semicolon. The second line looks like a continuation because it starts with a binary operator and is missing the left operand. Lately, however, I've been experimenting with not splitting lines of code. For the most part it's not an issue—long lines of code are relatively rare and it's unlikely I'm interested in reading them in their entirety anyway. Occasionally, I'll find myself with a method that has a large number of arguments. But then I find that this is typically a good excuse to refactor some code.
But can a single RTTI implementation be appropriate for every library or framework? In the case of C++ this is a hard problem.
Fuck yes, K&amp;R. I can't stand ANSI. I also add a lot of extra space to my code. while(true){ thing.doShit(); } generally becomes: while( true ) { thing.doShit( ); } It just feels less jumbled together that way.
Eh, I think I prefer: if( cond1 == "val1" &amp;&amp; cond2 == "val2" &amp;&amp; cond3 == "val3" &amp;&amp; cond4 == "val4" ) { do_something( ); } 
Hm, in practice I find consistency in naming quite important (because inconsistency makes it hard to remember or guess names), but consistency regarding braces doesn't really make much difference (as long as the code is properly indented)...
Yay, I knew it was going to be K&amp;R! The one true brace style :D 
&gt; there are so many more important readability issues to worry about Such as whether to use spaces or tabs!
I also had the idea for adding extra space and newlines to my code to make it more legible. For example, I was using 3 empty lines to separate class definitions and I always added an empty line after "{" and before "}". After a few months, however, I began to hate it. The window could only contain so little actual code now. :/ Now whenever I need to change some of the old code written in this fashion, I start by deleting those empty lines first. Good thing I'm working solo. :P
Seeing as it's only one line in the if body, you don't need any braces at all.
Yes, you could omit the braces in this case, but you have to be careful not to end up doing this (which I often do when I'm tired): Before (working code): if (...) doSomething(); After (not-working-as-expected code): if (...) doSomething(); doSomethingElse();
I also prefer putting the operator on the next line. It reduces the amount of time my eyes have to spend jumping from one line to the end of the previous if the expressions are nested and/or I forget which one was used. Of course, moving all these expressions into const local bools with meaningful but brief names, and then comparing those instead both reduces the amount of space you need for the actual if () condition (usually it then fits onto one line) and increases the readability dramatically.
I have all of my class definitions in different headers.
TIL I've been mixing Whitesmith and K&amp;R since '99.
Unless do_something() is a function-style macro and contains a terminating semicolon?
FWIF, officially TOTBS is an extension of K&amp;R, not *exactly* the same
Dare I say that people who are nazi about indentation style or notation are probably not good programmers. Its superficial and does not matter. What matters for readability (and safety) is learning to use C++ idioms properly. Like [these](http://www.amazon.com/Coding-Standards-Rules-Guidelines-Practices/dp/0321113586/ref=ntt_at_ep_dpt_1). Idiomatic C++ is what people should be a all nazi about. (Similarly, People who are Only comparing languages by syntax are also missing the point. Just like how code is not the notation or the indentation, the language is not the syntax. But that's another post) Its like girls who choose a car only by its color, "*I like pink cars*", instead of how it drives. 
This video is a rare gem. Lots of language philosophy. Dare I say, next to maybe Stroustrup himself, Sutter has the largest influence on the C++ community. Its always a joy to watch him explain complex ideas so very clearly. 
Can you clarify your comment? I couldn't fully understand your thoughts. 
I agree that over stating the point of indentation and brace style is silly. However, when working in an existing code base it is important to keep with whatever the existing convention is. It can really slow down a programmers ability to mentally parse code if it keeps switching between styles every few lines depending on who fixed what bug or implemented bits of functionality. 
I think it's harder to be flexible when you're just starting out - you've got so many things to keep track of, so much is still 'wet' in your mind. Once you've been forced to work a while and seen both, well, it's less of a deal. I've found the same thing with editor commands - somehow, now, I can keep track of 'Brief', CUA, or native Emacs styles without thinking, where early on I'd trip up a lot. Of course you have to waste valuable brain space for this flexible, 'context-aware' skill, but such is life. A tiny bit like learning another human language probably. 
http://accu.org/index.php/articles?theme=rss
Required Silverlight so I passed.
There are download links along the side...
I'm on linux so I downloaded the mp4 file. No problem.
Type information for everything in C++ is hard. Do you include information about template parameters? How do you encode multiple inheritance? If a class has a vtable should RTTI have a cost beyond another vtable entry? For many applications the answer to all of those questions is probably yes, but for some applications the answer will be no. And if the answer is no for some people then many libraries will also have to do without the standard RTTI implementation.
I follow ANSI, but I indent parens once more than curly braces and I tend to put items on separate lines if there are more than a few. if ( cond1 == "val1" &amp;&amp; cond2 == "val2" &amp;&amp; cond3 == "val3" &amp;&amp; cond4 == "val4" ) { do_something(); } If I have to indent more than one level in such a block, I inline the groups in separate functions. I do the same with anything that is a function or looks like a function: int result = someFunction ( itemA , itemB , itemC , itemD ); I will group them by multiple columns if it makes sense to do so. If I'm working on someone else's code, though, I don't really care - I tend to try to follow their style (so don't be worried that I'm going to "pollute" your beautiful code with my ugly style). Gets really annoying when you have a bunch of coders, all with different styles, though :-/ 
yay, I'm not the only person on earth who thinks like this... is this really a common belief? All I've heard from the people I talk with is that C# is a bastardized copy of Java, which I really don't think is all that fair considering that it's much nicer to use.
They do seem to talk past each other on occasion, though - Erik has a number of technical, academic questions on the nature of the language itself that some of Sutter's answers dismiss in favour of "folksy," high-level discussions of neat features and intended usage. In the end they do always seem to get onto the same page, but I think the interests of both speakers would have been served better by different counterparts.
I'm watching the video now, but just wanted to comment on the quote. I've seen others say the same in the past, but I think that programming languages are different than spoken language. I CAN think in two different natural languages if I want to. But most of the time, I think visually. I then use language to describe what I'm thinking. This allows me to describe what I'm thinking in either French or English right away. I do NOT translate. I SUCK at translating on the fly. If I were to think in ONE particular language, I would not be able to describe it in the other language as quickly as I do. So something else must be going on. With programming languages, I KNOW I don't think in ANY language. I think about what it is I want to do and then work it out on paper (the overall design). Yes, I'm old school that way. I also eventually work it around what language I'm using. I then go and implement specific functionality. But actual "thinking" in any particular language doesn't really happen. For example, suppose I want to visit every item in a list. That last sentence is programming agnostic. But you can implement it in a variety of different ways, using loops, recursion, dataflow, etc. in many different languages. So did I THINK in a particular language when I wanted to visit every item? I don't think a pure yes or no answer is adequate. 
Could you give an example? I thought Herb came across as a person who had talked about each topic many times before, and Erik was talking about these topics for the first time. Erik was asking questions, and Herb was explaining. For instance: Erik suggested that actors have all the same problems as threads. Herb said, actors are much better than low threads in that they solve low level synchronization. So that the user only has to worry about high level synchronizaton. For instance read an [article](http://herbsutter.com/2010/07/12/effective-concurrency-prefer-using-active-objects-instead-of-naked-threads/ ) about it he wrote. Erik thought it would be good for async or blocking to be invisible part of an methods interface. Herb said that's a bad idea in general. Things that matter should be visible. There are few things that matter as much as if an interface blocks or not. (On an unrelated note that's the reason I don't like CORBA. A thing that matters, whether a function call is remote or local, has been made invisible in the interface.) 
Erik seemed like he had to ask a few questions twice before Herb figured out what he was after, before answering, "It's like that existing feature in C++": Erik's questions on how `auto`'s type deduction was one example - "Do you allow full unification, or do you do left-to-right...", and Herb's answer, "Just like regular old templates." Erik's questions about the semantics of capture-by-value for lambdas, and Herb's answer, "Just like regular a copy." Erik's questions on non-local returns from lambda functions, and Herb answered that lambdas were just sugar over old function objects. (Unfortunate, perhaps - it could be interesting to see how being able to exit early from a `std::for_each` might be both in the language and on hardware.) Herb essentially said he thought template metaprogramming was an unfortunate side effect of the language definition, and laughed off ideas about JIT-compiling template programs. He was quick to steer the conversation to what templates were *meant* to accomplish. Overall I think a lot of that boiled down to a lot of *what* C++0x is giving us, but not a lot of *why* things are going the way they are, mostly because the two guys wanted to talk about different reasons for things - Erik seemed more interested in the philosophy of the language and the "language theoretic" choices made in its design, and Herb talked mostly about practical ramifications and interaction with C++ as it is today. I think Herb would have been more interesting had he been interviewed by someone doing high-performance computing work in industry, and I think some Erik's interests would have been better served by a (mythical) C++0x committee member who hangs out on LtU. The discussion on futures was one of the few times they really did get into the meat of an issue, and it was a real highlight.
What's wrong with silverlight?
I can't install Silverlight on my Linux computer. But it doesn't matter in this case, because the link has downloads in various other formats, just to the right of the embedded video.
http://channel9.msdn.com/Shows/Going+Deep/E2E-Herb-Sutter-and-Erik-Meijer-Perspectives-on-C
Moonlight doesn't work?
Some people won't install Moonlight. I like it. It's also nice to program for. My only complaint is it doesn't have webcam controls for this version. Perhaps the upcoming versions will work that in.
Some things are easier to say in certain spoken languages than other. Like the urban legend goes, Eskimos have a hundred different words for snow. And some things are more difficult, like in Finnish where the word for he and she are the same. How do you say "My email got caught in the spam filter" in old English? You just dont. Same goes for programming languages. In managed languages when everything is a reference, you cant say copy by value. Or how do you say compile time evaluation, when everything is evaluated at at runtime? Again you don't. Same goes for Deterministic destruction, automatic ad-hoc type polymorphism. C++ is nice that it does not limit you to one way of thinking. It has values and references, it has compiletime and runtime evaluation et. Although when you speak C++. You learn to prefer the concept of value semantics and consider references rather brittle. You think runtime reflection, is as ugly as casting void pointers. When you are forced to speak like this in C# or Java you feel a bit dirty. Look what you made me do! While I don't think in C++ syntax. I do think in the concepts behind the language. 
Haven't tried it recently, or on this particular website. Did try using for some other website several months ago, but couldn't get it to work. I've not found any websites requiring it that I've found sufficiently compelling to try again.
Given more time Herb would have been able to step through the history of each C++0x feature and why the final form is the best of all alternatives. The history can be found on http://www.open-std.org/jtc1/sc22/wg21/docs/papers/. Either Herb or Stroustrup is probably going to write a "Design and Evolution of C++0x" book. His goal here was to give the viewers not familiar with C++0x and overview of the finished language, rather than to take up time explaining dead end alternatives that never made it. The way I saw it was Eric pondered "I wonder if this kind of language feature is even theoretically possible". And Herb was like "we don't have to ponder hypotheticals here like we in the C++ community did a decade ago, today we have a fully implemented language that proves it is possible. Here is how." 
Fair enough. Did it seem like the install was broken or was it more specific to that particular website?
Think it may have been just that website. I was trying to watch the Feynman lectures on Project Tuva.
Is this quote in the title supposed to be clever? Sounds like something Captain Obvious would say.
Dang! That should say "C++" not merely "C"... :/
You should submit to /r/linux, I think you would have more success there.
thank you!
Doesn't anyone on the windows platforms want to have DTrace/SystemTap for windows? Or maybe such a tool already exists?
I use Boost Date Time extensively so this proved to be incredibly useful for me.
Any screenshots of how this looks? I remember having to write a ton of autoexp.dat modifications for VC98. It was pretty hit-or-miss.
Stephan Lavavej did a presentation on visualisers at BoostCon '10. Slides and video are [here](http://www.boostcon.com/community/wiki/show/private/2010/). (second link under 'Tuesday Presentations')
PHP programmers upvoting a comment about how C++ - exceptions - RTTI = C. Interesting place Proggit has become.
I'd take a screenshot but it's really quite simple. The values show up in Auto/Locals like this (for Boost Date Time): Name | Value | Type :--------------|:-------------------------|:------------------------ m_earliestDate | 2011-1-21 0h0m0s (0us) | boost::posix_time::ptime 
Already tried it this morning when I went in. But thanks anyway!
Awesome, Bjarne refers to the Poco project for Unicode support: &gt; Obviously, we ought to have Unicode streams and other much extended Unicode support in the standard library. The committee knew that but didn't have anyone with the skills and time to do the work, so unfortunately, this is one of the many areas where you have to look for "third party" support. There are libraries "out there" with good support for Unicode. For example, the Poco libraries "for building network- and internet-based application" (http://pocoproject.org/index.html) is available for download under the boost open-source license. Being a big Poco fan, this makes me very happy :) 
It does not provide safety against illegal values at run-time though. 
you are going to loose punk! your are going to loose horribly!!! ;) What type of competition? will it be C++ questions or C++ is just the language you selected and the questions are going to be logical\mathematical?
Isn't this 2011?
It will be a contest of who's gonna code the fastest (code should be right ofc).
probably streams too ... windows or linux? maybe some important system apis? for handling sockets?
&gt; Plus, no references allowed Can you use pointers, or does everything have to be passed by value? =]
Actually, I'm talking about library documentations. I didn't saw that coming, lol.
tee hee hee
Hah, there is an article in there titled, "[A Library Based Approach to Threading for Performance](http://software.intel.com/en-us/articles/a-library-based-approach-to-threading-for-performance/)". That title runs a little counter to that of Hans Boehm's paper called "[Threads Cannot be Implemented as a Library](http://www.hpl.hp.com/techreports/2004/HPL-2004-209.html)".
Yeah, I don's see how a lock based library could be safe when a threading ignorant compiler could just come in and reorder the locks. 
 If I start at template metaprogramming will I better get my head around this impressive hackery?
If you haven't read the previous articles in this series, you might start there.
Another excellent article. Thanks Eric!
 Touché
Nice article about template expression. This is a form of lazy evaluation that can be used in c++. However please don't row your own template linear algebra library. [Let the professionals do it with SSE and shit](http://eigen.tuxfamily.org/index.php?title=Main_Page)
educational example, meets wingsit. wingsit, meet educational example.
Just trying to tell people not to roll their own home-back expression template code in the production source. It is painful to maintain. Maybe 1 out of 100 C++ developers are keen enough to write this code. Then you need a twice as smart developer to debug and maintain it. :( BTW I also have high hope on your NT2 :)
Definitely an interesting read! I would be very disappointed if a developer had actually been using (A+B)[2] rather than A[2]+B[2], but I understand that this is just a an example picked for its simplicity.
And I prefer x = sqrt ( pow(a, 2) + pow(b, 2) ); Putting an operator on the following line makes it very explicitly clear that it is definitely a contiuation line. Similarly returnval = somefunc ( arg1 , arg2 and some other stuff //!&lt; extra long doxygen comment , anotherarg //!&lt; inline doxygen comment ); The above allows for simple per arg commenting, very trivial argument reordering and it's easy to see that it all belongs to the same statement without extra whitespacing. Funny enough both ruby and python barf all over this syntax.
I've edited code that looked like this. To me it is very difficult to accept some things that breaking the function name from the following open parentheses, or having the comma alone in the line below. It goes in the opposite way that we are used to think and write (in natural languages and math).
Agreed, I'd fire anybody who wrote (A+B)[2], but as you say, it's just a simple example. Expression optimization becomes /very/ important in less-trivial scenarios (regular expressions, SQL queries, etc), and that's exactly where a higher-level API for doing expression manipulation really shines.
This series of articles won't teach you meta programming, they will teach you how to play with boost and expressive (which is ton of fun ... (fml) so you should read them). If you really want to learn MP: http://stackoverflow.com/questions/112277/best-introduction-to-c-template-metaprogramming the book Modern C++ Design is really great.
There is a template linear algebra library based on Proto in development that is already much better than Eigen on quite a few aspects. It is not developed by Eric though, and is completely unrelated to this example. This example simply aimed at showing how to perform simple yet meaningful expression transformations.
NT2 you mean? I track its development and Joel can tell you more. The doc is still lacking for now compared to Eigen and there are huge community around the Eigen library. After all it is kinda back by KDE team. Few other ET linear algebra libraries are pretty much dead like blitz++, old NT2, MTL. Sure uBLAS in boost is not so dead but the performance is no where near Eigen. I am looking forward for NT2's submatrix capability but until it is mature Eigen is probably the right choice for me and most people. I am not saying Eric should not do template magic. In fact he is probably few of the skillful template magician in the C++ community who can do all these. I personally enjoy the article and his boost libraries. I am just telling others junior C++ developer NOT TO DO THIS AT WORK. 
I couldn't write a linear algebra library and wouldn't even try. I don't have the domain knowledge. I wrote Proto so that others with the domain knowledge (like Joel F.) could write one, and I'm really pleased that he did. Now if only he'd finish it. :-)
How old is this?
Very.
2011 - 1997 = 14 years
I'd be more interested in investigating this method of unit testing if the intro mentioned its advantages over other C++ unit testing frameworks. Without such information, I get the impression that the author just reinvented the wheel. That scenario may be of value to the author, but not so much to the reader.
Author here: I did reinvent the wheel. My testing needs are not particularly complicated and so I needed a simple solution. Being my first time actually doing something like TDD (but not really TDD, I usually write tests after the fact to check things), all I wanted was a way to chain together a bunch of functions that ran tests. To that end, if you need a unit testing system that has no external dependencies except for &lt;cstdio&gt; (for printf) and is only two files (header and implementation) then this one just works. To be honest, I haven't actually tried any other unit testing frameworks so I can't give an honest opinion about them. If/When my system fails to meet my future needs, then I will explore outside systems. On another note, I think that my way of chaining together statically allocated objects at runtime using templates to parameterize some form of computation is nifty. I used it to similar effect to guarantee the order of destructor calling in statically allocated templates as found [here](http://projects.ioreader.com/cftl/fltl/include/helper/StorageChain.hpp).
I think it's more accurate to say these are C++/CLI quizzes.
Yeah, I was wondering what was going on when I saw that "override" keyword.
I think you're better off learning a good unit testing framework. If ease of installation is important, try Unittest++. At the other extreme, there is the commercial Uquonitest.
if you knew :o 
nt2 is on the verge to be alive again. Lots of stuff happened and the project was dragging along. I fully agree on your comment afterward. We shoudl have some WWE style warnign on top of eric article: "kids, dont do it at home" :p
Are you sure? I always thought that exception handling was done using libgcc_s. Edit: Here is some more information regarding exception handling in GCC: http://www.airs.com/blog/archives/166
This is more about windows than C++.
Thanks for pointing that out. The functions in the first documentation I linked do come from libgcc_s. If I had been paying attention to the page title I would have noticed that. There are wrappers for the libgcc_s functions in libstdcxx or the smaller C++ language-support library libsupcxx. Those functions include `__cxa_throw`, `__cxa_rethrow`, `__cxa_end_ctch`, etc... It looks like these are probably implemented in terms of libgcc_s functions.
I still don't get it.
How about factoring the creation of the stream away from the parsing of the stream? Then none of this wierd over engineered monkey business.
I came to the opposite consclusion. Having started in python, I much prefer giving the braces their own line. if (condition) { doFoo(); } else { doBar(); } I hate seeing the curly braces next to any other notation, as I can't quickly discern which block they are enveloping. Also having an IDE to collapse blocks makes the spacing not that big a deal. 
This is really sweet. At first I was a bit leery of the on-the-fly kernel module implementation, but after reading through the documentation, it does make sense. By the way, it's also available as a .deb package for Ubuntu (and possibly other Debian-based distributions.) Thanks for for posting it!
I am a java programmer working in finance and I'm learning C++ as fast as my ass can carry me. Why? Because my company has finally worked out that in certain fields you need great performance and even though java is easier to write it can't deliver these apps in the 95th percentile.
Any program that uses virtual address space (IE almost everything post-DOS and everything on an NT-based system) any pointer is going to be within the programs address space so checking for NULL is really the only way to validate a pointer without keeping track of it for it's entire life.
So how old is AfxIsValidAddress?
I don't know, Lots of MSDN articles include a list of minimum version of Windows for a function to be supported but the one for AfxIsValidAddress doesn't. According to wikipedia MFC was first released in 1992 so likely sometime between then and win95 (which I believe is the first MS OS that supports using the virtual address mapping) being released.
Funnily enough, there are methods in some MFC classes that expect a NULL *this* and don't consider it an error. As long as they don't access any member variables or the vtable, the pointer is still "valid."
 int main(int argc, char* argv[]) { std::vector&lt;std::string&gt; params(argv, argv+argc); // Now all the command-line parameters are in the 'params' vector } Taken from http://stackoverflow.com/questions/1596139/hidden-features-and-dark-corners-of-stl 
Nice article, but it sounds like the author hasn't used/heard of [eigen](http://eigen.tuxfamily.org) which implements many of these ideas and has very competitive performance compared to traditional BLAS libraries. 
I want to show you in this post that static analysis is useful regardless of programmers' skill and level of a solution being developed. The idea "you must employ experts and write code without errors right away" does not work. Even highly skilled developers cannot be secure from all the errors and misprints while writing code. Errors in samples for IPP show this very well. AACStatus sbrencResampler_v2_32f(Ipp32f* pSrc, Ipp32f* pDst) { ... k = nCoef-1; k = nCoef; ... } int ec_fb_GetSubbandNum(void *stat) { _fbECState *state=(_fbECState *)stat; return (state-&gt;freq-state-&gt;freq); } vm_file* vm_file_fopen(...) { ... mds[3] = FILE_ATTRIBUTE_NORMAL | (islog == 0) ? 0 : FILE_FLAG_NO_BUFFERING; ... } ... and more ...
The AfxIsValidAddress documentation refers to the program 'memory space', which I think is different than the 'address space'. An address can (as you say, must...) be within the address space, but can be outside the addresses of pages committed to the process. IIUC, this is what the VirtualQuery 'bonus' in the post checks .
Its been done. You can create library types that lazily evaluate expressions build trees at compile time with boost proto. How to use it for optimizing numerical computations, is explained here: http://cpp-next.com/archive/2011/01/expressive-c-expression-optimization/
The "Conversion between Unicode UTF-8 and UTF-16 with STL strings" sample project is great. 
Eigen smoke almost all BLAS lv and lv 3 functions alone. If you have longer expression, it definitely beats blas and lapack equivalent. 
It depends whether you use dense or sparse matrices in your computations. It is believed that BLAS implementations (GotoBLAS in particular) are superior when it comes to dense matrix computation but for sparse matrices, template libraries are superior. If you are interested in problems involving sparse matrices, read the following article: http://grh.mur.at/sites/default/files/SparseLibraryBenchmark_0.pdf
&gt; I want to show you in this post that static analysis is useful regardless of programmers' skill and level of a solution being developed ...OBTW, I'm not just showing you this to add to the community, I'm also promoting my closed source static analysis tool.
An alternative is parsing command lines using Boost: http://www.boost.org/doc/libs/1_45_0/doc/html/program_options.html
AObjectServer, doing it in C++ for about 10 years. http://aobjectserver.codeplex.com/ The only caveat is that it's for Win32/Win64.
I'm suprised there's no web framework for C or C++. Good article though. 
http://www.reddit.com/r/programming/comments/fgafz/a_quick_experiment_with_websites_in_c/ recomment here for great justice and helpfulness? (Who reads /r/cpp?)
Sure, there are [wt](http://www.webtoolkit.eu/wt) and [cppcms](http://cppcms.sourceforge.net/wikipp/en/page/main)
In Visual Studio there is a setting so that the runtime will stop the moment, and at the line, that a exception is thrown. I find it very handy. I think its under "Debug-&gt;Exceptions-&gt;c++" ...
In Visual Studio there is a setting so that the runtime will stop the moment, and at the line, that causes an exception. I find it very handy. I think its under "Debug-&gt;Exceptions-&gt;c++" ...
I'd disagree, you may need to know if a accessing a pointer will throw an access violation exception, this only occurs for memory pages that aren't committed. Accessing reserved or free pages will raise that exception.
You could switch on `argv[0]` and create symlinks to provide multiple commands from one binary. One suggestion for an option parser: [Boost `program_options`](http://www.boost.org/doc/libs/1_37_0/doc/html/program_options.html).
FWIW, [the GNU coding standards advise against this](http://www.gnu.org/software/automake/manual/standards.html#User-Interfaces); better to use a command line argument. For parsing the command line arguments, GNU getopt is good and you will get a consistent command line interface like most other Linux tools (parsing the command line well is a surprisingly intricate process which is easy to get wrong). Of course, it's only portable to Unix-like systems (though MinGW does support it for Windows; Visual Studio obviously doesn't), so if that's a concern the Boost library may be a better option.
I'd say for the most part (except in the case of API bugs like the example in the article) if you have to check if you have read or write access to a page before accessing a pointer you probably don't know enough about the pointer to safely use it in the first place.
That's a fair point, although if you were writing your own heap corruption checking code it might be useful.
Forgot to mention that the project is placed under BSD license.
**GStreamer** has a nice tool to test pipelines at command line, some examples [here!](http://linux.about.com/library/cmd/blcmdl1_gst-launch.htm) Also the old `expr` command has a syntax to enter infix expressions at command line.
Looks interesting, too bad there is no sparse support
Look into Qt Creator, from nokia. It is available as a standalone or part of the Qt SDK. I recommend getting the SDK as it comes with a full C++ compiler, and qmake is a very handy and easy build system. I've been able to get a lot of sci/math libs working with it, And very easy to switch between compilers, as I use intel, Microsoft, cygwin, and mingw
If you don't have anything against MS compilers, just grab the Visual Studio 2010 express edition for C++. IMO, there is nothing on any platform that beats Visual Studio as a C++ IDE. http://www.microsoft.com/express/Downloads/#2010-Visual-CPP You'll still be able to write native C/C++, but the environment is hard to beat.
If you're interested in a matrix / linalg library in C++, check out Eigen (eigen.tuxfamily.org). It supports highly-optimized fixed-sized matrices (e.g., for OpenGL) as well as dynamic matrices. It's faster than ATLAS/BLAS/LAPACK on many operations as well.
I second that.
Visual C++ 2010 Express Edition Code::Blocks Qt Creator
What a terrible title and overbloated article. The tl;dr point he makes is that strpbrk (a "find the first of any of these characters in the given string" C-function) is faster than find_first_of when you are looking for individual bytes in a byte array (i.e., a string). No shit, sherlock, that's what std::string.find_first_of() is for. Could have saved a lot of text by just saying: "Hey, find_first_of is implemented as a double for loop, which isn't the fastest way to search for bytes within a byte array."
Eigen is the way to go, but upvoted for the release names :) 
and because NICTA is next door :P
If you're a student with a .edu email address, you can get the whole shebang for free https://www.dreamspark.com/Products/Product.aspx?ProductId=25
After having used both Armadillo and Eigen, Armadillo is the pick for dense matrices. It's API is more intuitive. It also allows linking with high performance versions of LAPACK, eg. MKL (http://software.intel.com/en-us/intel-mkl/) and ACML (http://www.amd.com/acml). MKL and ACML are highly optimised and can use multi-threading. 
forgot to mention GotoBLAS: http://www.tacc.utexas.edu/tacc-projects/gotoblas2/ 
find_first_of should come with specialization for searching for chars in strings. Question: is it possible to make specialization with binary search for objects that have &lt; operator?
If you are referring to a template specialization than no. Are you asking it in relevance to subject or just wondering? (Well you can but you need to inherit from a "Comparable" interface and specialize the function with enable_if but that's not the way we roll in c++ :) ) 
Oh, here we go again....
I want to upvote. But that thread was... painful. As much as I like C++, Linus does have a point. STL and Boost are not portable. Even on the same machine, you'll get problems if different parts use different STL distributions. This is actually a well known problem. Try having published lists or data structures and you can't use STL at all. You need wrappers or write your own lists. DLL's are especially notorious. And the abstraction model through the use of objects simply doesn't fit a vast spectrum of situations. Designs that seems good at first can quickly become horrible as the project grows. There are ways to get good design, but I fear that C++ has 'features' that make this difficult. I personally don't like the 'private' keyword for classes that can be derived. And 'protected' doesn't work because object A::B can't access object A::C's protected parts in A because A is a different object. Also, there are many textbook practices that are just not well thought out. I see people who believe in one level of encapsulation. When behind that 'barrier', anything goes and all rules of proper programming go out the window. C++ should be done in a hierarchical manner where each level interface is the next higher level's implementation. Then there is the notion of an object doing all the work itself vs. passing object A to object B and asking it (B) to perform an action on object A. The difference is like starting a car vs. using scissors to cut paper (as opposed to asking paper to cut itself). I've found that an awful lot of situations work like paper. In fact, it goes to the root of how things combine. Who performs the action and who receives the other object? Then there is the is-a and has-a relationship. I've found this to be inadequate for certain kinds of objects where we must use enums and things that really don't fit well in the language. They are tacked on. Seriously, ask yourself what is an integer? No, not the type. The number 4. It's not a has-a relationship. And it's not an is-a relationship even though we say 4 is an integer in regular programming conversations. 4 is NOT derived from integer. It's the other way around. The INTEGER type defines the number 4. So the true relationship is that 4 is a member of a set. 4 'is-one-of' all integers. Then you find lots of situations like this, especially when dealing with containers and hashes and key/value pairs, etc. that really give a better POV. Programmatically, nothing may change much. But how you look at it is forever changed (and you start to see how the type system breaks down for primitive types and sets). STL is still clever for how it includes pointers as iterators. I'm not sure it would come out the same if the language were originally designed today. Anyhoo.... I could go on and on. But C++ really does rock. Just because I'm ranting doesn't mean I think there's something else superior. For what I use it for, there's nothing better. I'll leave dataflow for another day. Hopefully, more of those concepts will find themselves in today's languages. I guess I could say one thing about common mistakes. 1. copy constructors, assignment operators, deep or shallow copies, creation and copying of objects when using polymorphism, etc. 2. const correctness. 3. initializer lists avoids copies (or prior init) from assignment in the constructor. Initializer list is also how you use custom constructors for members. 4. No template arguments as friends. GRRRRRR!!!!! FUUUUUUU!!!!! 5. use of 'typename' on dependent types in templates. 6. diamond problem. 7. no way for unique nested class to find base of outer class without hacks. 8. callbacks to methods. 9. controlling allocations when objects need to appear in multiple lists. 10. Doing REAL message passing (like socket or IPC) as opposed to C++ method invocation. Note I haven't said anything about pointers. There really are great tools in C++ in this regard. 
I was just wondering. Inheritance from a class doesn't make it general enough for what I was thinking about to be an optimal solution.
STL and boost are portable, at the source code level. I guess what you are talking about is that C++ lacks an ABI (application binary interface). So you need to compile all libraries you link with, with the same compiler version and with the same switches, or else limit yourself to the C ABI at the dll boundraries. I also find that annyoing.. but unless you are making 3rd party libraries, you should be able to take control over your build environment. I find restricting myself to the C ABI is not worth the trouble. The convenience of being able to pass strings, vectors, and smart pointers from one dll to another is worth the trouble of having to rebuilding all libraries when upgrading compilers or changing compiler switches. The golden rules I follow are: -*Inherit to reuse interface, not implementation* -*Prefer aggregation over inheritance* -*Prefer where possible free functions to member methods* -*Always use the RAII idiom* -*Use smart pointers, avoid naked (unowned) pointers* -*Prefer value semantics to reference semantics* -*Don't reinvent the wheel, use stl and boost* -*Use the Pimpl idiom to hide private and/or to provide a compiler firewall*
The WTF level of your comment is very high, over... no, forget that. STL and boost are portable. What you see is an ABI problem, and it is no different than C. If you define different structs for the same information, those structs are not interchangeable. You have to ensure that you use a single definition of the struct, and that both parts that are communicating agree on the implementation (this is a compiler/operating system problem, not a problem with C++). The language cannot define the ABI because it depends too much on the OS. I simply don't understand your problem with protected and A::B accessing members from A::C. Why would it have to access such members? Your car X paper+scissors analogy is an OO concept you have to grasp. There is NOTHING with C++ here. Your is-a and has-a confusion is ridiculous, sorry. You really have serious problems with basic OO concepts. Of course 4 is not derived from Integer, because 4 is an **instantiation** of the Integer **type**. You are confusing classes and objects here, the very first concept of OO. * Integer *is-a* Real (class x class) * Complex *has-an* Imaginary (class x class) * 4 *is-an-instance-of* Integer (object x class)
Its good to see C++ getting some love. MS got some good people working there. 
&gt; It supports highly-optimized fixed-sized matrices I'd note that Armadillo also supports fixed-size matrices. I'm not sure how "tuned" they are, though. A one-minute look at the source didn't turn up anything that looks like SIMD, so I wouldn't be surprised if Eigen was quicker for some use-cases.
have to tell as many lies as you can to get those "developers developers developers" back eh?
the [Armadillo](http://arma.sourceforge.net) c++ library also does a very good job for expression templates. Can use MKL for matrix decompositions etc. 
I cross compile my code for different platforms and different (desktop) architectures and I have yet to run into porting problems except for locales which is a mingw limitation that I wasn't aware of. I think Boost and STL are quite a bit more portable than you make them out to be.
&gt; STL and boost are portable They are as portable as the HIV.
Hmm.. what to make of that closing comment &gt; "I don't want to give anything away.. but don't be surprised if visual studio C++ gets some new designer tools first." Today it was announced that MS and Nokia will jointly develop smart phones. &gt;"Nokia’s Maps product will become a core part of Microsoft’s services, while **Microsoft’s development tools will create applications for Nokia Windows phones.**" This is good news, as this means Visual Studio C++ will become a big player in the smart phones scene. Id much rather use full blown C++ developing on smart phones.. than use the crippled Objective C, ala iphone. Or slow Java ala Android. I think modern C++(STL and boost) is a perfect match for smart phones. 
&gt; What articles have you found interesting? the first two.
That's not what I meant. Compile one module with one compiler using one distributor of STL and then SHARE those lists with another module compiled with another distributor of STL (you can use the same compiler if you'd like). It won't necessarily work. Also, try publishing STL lists. You're in for a world of hurt. STL is NOT portable. 
&gt; If you define different structs for the same information, those structs are not interchangeable. But you're not defining different structs. You're using the same list with the same elements. This issue does NOT happen with C. That was Linus' point. Sure, you can set different parameters for padding, but in general, it's not an issue. With STL, you can have different implementations which make it non-portable. &gt; I simply don't understand your problem with protected and A::B accessing members from A::C. Why would it have to access such members? It's breaks consistency. You can access A sometimes, but not all the time. If I want to share a base class with ALL derived classes, I can't do it. &gt; Your car X paper+scissors analogy is an OO concept you have to grasp. There is NOTHING with C++ here. No, nothing. Unless C++ can do OO. Oops! &gt; because 4 is an instantiation of the Integer type No, it isn't. You can change the number 4. You cannot change an instance of an object, only it's reference (not C references, but pointers and such). &gt; You are confusing classes and objects here, the very first concept of OO. Nope. You just don't get what makes a primitive type. 
No, not ABI. I'm talking about sharing the SAME list with different compilers that have the same ABI. In C, you have ZERO problems. But with STL, you get into problems at the source code level. 
To all those who have responded to my comment, seriously guys... doesn't anyone know how to program in C++ other than simple programs? C'mon! I was hoping for some intelligent responses. I like to learn about different ideas and techniques. All I got were pure nonsense and lack of understanding about C++. I expected better from this group. 
Does it really matter? When do you provide non-compatible binary files anyway? Just compile it all from source with the same chain and you won't run into any trouble. Name mangling is a problem with incompatible binary stuff.
I should have been more specific. What *other* articles have you found interesting? :)
It looks like a great library!
STL is source portable, either the STL implementation provides the interface and behaviour specified in the standard or it does not. Popular STL implementations are very standard conformant these days. Bugs like that are rare. The fact that you cant link C++ libraries built with different compilers, is a C++ ABI issue. The way C++ 3rd party libraries are designed is that at the dll boundaries only C types are used. Either that or the 3rd party builds a different binary for every compiler of his customers. Or he ships the codes and lets his users run a build script (like boost). The reason you can do this cross linking with C types an not worry about it, is because the C types are simple enough to have a de-facto ABI. C++ supports this basic C ABI aswell. All the built in types, and simple POD's. But for more complicated C++ types (those that use vtables, inheritance, etc) there is no de facto ABI. If a full C++ ABI was set in stone, then compiler writers would not be able to tweak their C++ language implementations, or even their standard library implementations. There are many different implementations of say std::string, even within the same compiler version the debug one might be different from the release one, and there might be a thread safe one, and a write on copy one, all possibly being binary incompatible with each other. So if your dlls where built with different string implementations, and you try passing a string from one to another things will fail horribly. All that is required by the standard is that they have the same interface. The easiest is just to recompile all dlls, if you can, with the same compiler and the same switches. Java and other managed languages get around this by doing the comping at runtime (interpreted compiling) or at startup (just-in-time compiling). And C gets around, as I said, by only supporting simple types that map straightforwardly to native processor types.
Good reads. Herb Sutter got a lot of good articles. I guess the biggest change in the way I program.. was from reading books nor articles, the best one is probably [this one](http://www.amazon.co.uk/Coding-Standards-Rules-Guidelines-Practices/dp/0321113586). I guess *"always use RAII"*, *"inherit to reuse interface not implementation"*, *"prefer free functions to members"* where the big 3 ones for me (in order of importance). Now I find C style reference semantics ugly, deep OO hierachies ugly, and classes with lots of methods ugly.
A recent spate of them on dependency injection, for unit testing (not so useful without the links, I know). But I expect I'll change how I code, based on them.
I thought we already knew that GCC's LTO doesn't really work on Windows
Silverlight kept bugging out for me in chrome, if youre using vlc, use on the links to the right of the video. Excellent content.
What do you mean by this?
Not talking about name mangling. Feel free to use the same compiler even. 
&gt; The fact that you cant link C++ libraries built with different compilers, is a C++ ABI issue. Use the same compiler. Can't be an ABI issue in such a case, can it? &gt; The way C++ 3rd party libraries are designed is that at the dll boundaries only C types are used. Oh oh. You're agreeing with Linus. &gt; The reason you can do this cross linking with C types an not worry about it, is because the C types are simple enough to have a de-facto ABI. C++ supports this basic C ABI aswell. All the built in types, and simple POD's. But for more complicated C++ types (those that use vtables, inheritance, etc) there is no de facto ABI. If a full C++ ABI was set in stone, then compiler writers would not be able to tweak their C++ language implementations, or even their standard library implementations. Not talking about ABI. Use the same compiler. How can it be an ABI issue in such a case? &gt; The easiest is just to recompile all dlls, if you can, with the same compiler and the same switches. Now you're doing Linus' job. If he read this, he'd be sitting back applauding you for your irony. Seriously, WTF is it with those that have responded so far? You all make Linus look like a genius. Take a look at this gem. &gt; And C gets around, as I said, by only supporting simple types that map straightforwardly to native processor types. WOW!!! Just WOW! 
I'm not here to argue with you for the sake of it, or drive some agenda. I am merely correcting misunderstandings where is see them. You claim the STL is somehow fundamentally importable without giving examples. You could prove everyone wrong by simply providing an example project that shows the issue. But you cant. The only way I can imagine you having issues with the portability of STL types, is if you where not aware that c plus plus does not have an ABI like I explained. Can I make a suggestion. Good argumentation is attacking the core issue at hand, clearly and honestly. When you are simply making ad hominem attacks, or argument from authority, you no longer have anything valid to say. Same could be said for Linus rant.
Two implementations of STL by different module, same STL interface, same compiler. The code will crash if any lists are shared between modules. It means that you can't use STL to share lists in dll's or shared memory, or even within the same compiler. Note that this has NOTHING to do with ABI. It's a simple issue that is well known. &gt; When you are simply making ad hominem attacks you no longer have anything valid to say. I think making strawman arguments doesn't help either. 
If you use two different STL implementations in the same program for some strange reason (say MsSTL in one module and STLport in another) they are obviously going to be binary incompatible. And it wont work for ABI reasons as I explained. Basically the types will have different sizes and internal structure, so you cant just copy one into the other, its not enough that the public interface is the same. You'd probably get a linker error if you tried. No amount of recompiling would make the binary interfaces compatible. If you used one implementation and claimed that failed between modules... well Billions of lines of live code would disagree. Mine included. I share all manner of STL collections and smart pointers between modules. And that is perfectly safe, as I have ensured that the modules are binary compatible with each other, by compiling with the same compiler version, the same standard libraries (and any other libraries), and the same compiler switches. This is standard practice. If c plus plus was broken fundamentally like this, you would not be the only one that had noticed. Now if you could provide such an example project, you would become famous overnight. I promise you that. This will be my final post here. Its no longer constructive arguing about this. 
You should move this to Google Docs or something, the site keeps going down. Oh, and nice work so far and cool idea. Community-made guides are always cool.
Come on man, If they re-posted it it doesn't mean you need to re-post it. You already posted it 2 days ago.
Great idea but it's too cluttered.
I am not the creator. What I also forgot to mention is that it is game development related, so not everything is 100% generic.
Compromise sucks, the best thing to do is always to either 1 Do the unequivocally better thing or 2 find a "third way" that transcends the limitations of both sides.
They already made that language, it's called C#
Awesome. I really liked his earlier series on STL. 
Yes, that's why C++ code is valid C# code, one just has to find a compiler turning error messages into executables.
There are a few at the bottom of [this page]( http://www.informit.com/podcasts/channel.aspx?c=dadf92ca-3bdc-484e-9cd8-cbfe0cfc0de6)
&gt; Inherit to reuse interface, not implementation Easy to say, harder to follow. Imagine a class with 10 methods. Not unreasonable as an API. Now you need to special case this class - 1 or 2 methods need to behave differently. The remaining 8 or 9 methods are identical. Do you: a) inherit from the base class, overriding the methods you need? b) inherit from a purely abstract class, implementing all the methods anew? c) inherit from a purely abstract class, wrapping an instance of the real class for 8 or 9 methods, and implementing the two you need? C sounds great but it's PAINFUL to do. Any tweak to the abstract base requires tweaks to your class, even if they are unrelated. B is just dumb. That leaves ....
I don't think that word means what you think it means. You are defining for two classes, one in each module (from different STL implementations). They just happen to have the same name. When you link them together, you are asking that the C++ compiler somehow magically divine your intentions and make them the same as each other. If you defined them EXACTLY identically and compiled them with the EXACT SAME flags, then it might just work. Or it might not. But it doesn't matter because YOU VIOLATED THE RULES. Your code is ill-formed and non-compliant. Read up on the One Definition Rule. It doesn't work in C either.
In general you should prefer, c). Sometimes, as you say working with old code its not so easy though. &gt; C sounds great but it's PAINFUL to do. Any tweak to the abstract base requires tweaks to your class, even if they are unrelated. The pure abstract class you inherit from is not unrelated. It is what specifies your interface. If you publicly inherit from unrelated types just to reuse some implementation.. you are also inheriting their public interface that is not relevant to you.. and so you get these massive monster interfaces. Writing a few forwarding methods to an aggregate object, is not so much work. As you should not have that many methods anyways. Like the saying goes, you are what you eat. Public inheritance is what makes you fat. If you have to inherit to reuse implementation, then do private inheritance. That still bloats your private interface. But I guess that one only makes you fat on the inside. 
So... Never heard of RAII. Awesome.
Link dead.. here's the cached page: http://webcache.googleusercontent.com/search?q=cache:http://alblue.bandlem.com/2011/02/reflections-on-objective-c.html&amp;ie=utf-8&amp;oe=utf-8
I'm sorry I didn't come back to this a month ago, but... &gt; That's why I posted the source code in the comments two weeks ago. For your other article, yes. But not the one you linked to in reply to my post. You know, the one where you tested `intrusive_ptr`. The use case for `shared_ptr` is where you're *holding* a few copies of a few pointers, and handing out access to them with `weak_ptr`. Your code doesn't do this, and instead creates a large hierarchy of objects holding smart pointers, and performs manipulations on multitudes of unneeded `shared_ptr` copies. The overhead of doing this is murdering your benchmark because it's not meant to be used this way. `shared_ptr` should only be copied when ownership is explicitly shared between two "parent" objects, and `weak_ptr` should be taken whenever a copy of the pointer is needed but ownership not shared. So, yes, the correct way to use `shared_ptr` is to use it as little as possible. It's not GC in and of itself, it's merely a cleanup tool to handle proper ownership and destruction of an object. Remember, C++ is not a declarative language, so the optimization opportunities afforded by using one are not available when writing this kind of code. C++ does not reward you for misuse of concepts.
&gt; Isn't it a well known fact that reference counting is slower than tracing garbage collection? Yes but how much slower? I didn't know so I quantified the difference and it really is big (4-10&amp;#215; slower). Folklore also states that `malloc`/`free` is much slower than generational GC but these results show that it is only 2&amp;#215; slower. 
&gt; For your other article, yes. But not the one you linked to in reply to my post. You know, the one where you tested intrusive_ptr. I can post that code as well if you like. &gt; C++ does not reward you for misuse of concepts. Again, you're only claiming this a "misuse" because you don't like the performance results. 
Just as a first guess, I don't think such a book exists, here's why: (bear in mind that I know exactly NOTHING about the subject) 1. Systems books historically trend toward the C-in-a-.cpp-file style - that's what the guys writing the books learned, so that's what they teach. (i infer this from your question - you're looking for a property !P, which implies that most books have the property P) 2. Systems programmers tend to frown upon C++'s features - templates cause a new instance of the function to be created for every type it is instantiated on, operator overloading tends to hide complexities of code, which is a bad idea when you're so low-level. (I remember Linus Torvald ranting about something like this some time ago) 3. The idea of systems programming (implementing the lowest level of abstraction) and the ideas behind the advanced language features (hiding abstraction from the user). If you're writing low level stuff, you don't want the compiler hiding details from you. (This looks a lot like #2, but THIS LIST NEEDED MORE ITEMS, DAMMIT!) 4. I would enjoy reading such a book (as would 7 others) and would have noted the book if I had seen it before. Neither I, nor anyone else who has upvoted this thread seem to know of this book (if someone who voted on this article knew the answer, or had a strong opinion, he would have posted it already. Others, like myself will upvote because they want to know the answer, and want to bring it to the attention of more knowledgeable redditors) Taking all these details, and without actually googling for an answer, I have heuristically determined that there is no such book. Then again, anyone who can prove me wrong will receive my ~~eternal~~ momentary gratitude.
More like 11 other people. Also, it doesn't really matter if abstraction is hidden or not, IMO. If it does exactly what I expect in a way that is often faster than c, then it doesn't really matter.
I second this request with a bit more focus. Anyone know a good C++ book with emphasis on embedded systems? It would be awesome if there was a good book out there that detailed the memory and performance requirements for all of the common C++ features and libraries. I am a very experienced embedded C programmer and am very comfortable in pure assembly. As soon as I start using C++ I feel like too much of the memory allocation becomes dynamic and resources are used excessively.
Ah, you're not thinking like a Systems Engineer. Let me enlighten you on the sort of man you are dealing with: [They will stop at nothing to squeeze every last computing cycle from their machine, and are not content until they are done](http://www.pbm.com/~lindahl/mel.html) Not every Systems guy is Mel, but every systems guy wants to *be* Mel. C++ will only get in the way of that low-level bit-twiddling.
Are you writing your own OS? Even if not, the emphasis should be on good design rather than using all the features of language XYZ.
But aren't c++'s data structures more efficient? It's always beating c at benchmarks.
This is more about being able to do things without using 10 fucking lines of code to copy strings. Then again, I'm kind of a noob, since this is kind of about an operating systems course and how I'm tired of using c to do what should be basic.
V-tables, man. When you call a method on an object, you don't just call that function, *you have to look it up first*.\* \* This isn't actually true. As long as you don't use `virtual`, you never have do deal with v-tables, but their mere existence offends some people It's not that C++ isn't faster than C, it's that Systems Guys don't accept that it is so, and they're the audience, and the authors of the books.
Oh, yeah. I forgot to mention that all low-level APIs *that matter* are written in C. It's not that they *couldn't* be written in C++. it's just that they *aren't* also, if you read /r/programming you might have noticed the article on why C++'s `new` is way more complicated than you think.
Here's Linus's rant: &gt;On Wed, 5 Sep 2007, Dmitry Kakurin wrote: &gt;&gt; &gt;&gt; When I first looked at Git source code two things struck me as odd: &gt;&gt; 1. Pure C as opposed to C++. No idea why. Please don't talk about portability, &gt;&gt; it's BS. &gt;*YOU* are full of bullshit. &gt;C++ is a horrible language. It's made more horrible by the fact that a lot &gt;of substandard programmers use it, to the point where it's much much &gt;easier to generate total and utter crap with it. Quite frankly, even if t&gt;he choice of C were to do *nothing* but keep the C++ programmers out, &gt;that in itself would be a huge reason to use C. &gt;In other words: the choice of C is the only sane choice. I know Miles &gt;Bader jokingly said "to piss you off", but it's actually true. I've come &gt;to the conclusion that any programmer that would prefer the project to be &gt;in C++ over C is likely a programmer that I really *would* prefer to piss &gt;off, so that he doesn't come and screw up any project I'm involved with. &gt;C++ leads to really really bad design choices. You invariably start using &gt;the "nice" library features of the language like STL and Boost and other &gt;total and utter crap, that may "help" you program, but causes: &gt; - infinite amounts of pain when they don't work (and anybody who tells me &gt; that STL and especially Boost are stable and portable is just so full &gt; of BS that it's not even funny) &gt; - inefficient abstracted programming models where two years down the road you notice that some abstraction wasn't very efficient, but now all your code depends on all the nice object models around it, and you cannot fix it without rewriting your app. &gt;In other words, the only way to do good, efficient, and system-level and &gt;portable C++ ends up to limit yourself to all the things that are basically available in C. And limiting your project to C means that people don't screw that up, and also means that you get a lot of programmers that do actually understand low-level issues and don't screw things up with any idiotic "object model" crap. &gt;So I'm sorry, but for something like git, where efficiency was a primary objective, the "advantages" of C++ is just a huge mistake. The fact that we also piss off people who cannot see that is just a big additional advantage. &gt;If you want a VCS that is written in C++, go play with Monotone. Really. They use a "real database". They use "nice object-oriented libraries". They use "nice C++ abstractions". And quite frankly, as a result of all these design decisions that sound so appealing to some CS people, the end result is a horrible and unmaintainable mess. &gt;But I'm sure you'd like it more than git. &gt; Linus
&gt; Oh, yeah. I forgot to mention that all low-level APIs are written in C. It's not that they couldn't be written in C++. it's just that they aren't. Unless you are still running BeOS for some reason. ;-)
Oops, you're right. Edited ;-}
One reason low level API:s are in C is that it is usually much easer to bind to other languages (no name mangling etc). On the other hand, if you want to look at C++ API:s, look at Be/Haiku. It might not be "modern" C++, but it's C++ if that's what you are interested in.
C doesn't really have any data structures so there is nothing to compare against. When doing low level systems, you usually don't need general structures. Sure you can use them, but they will almost always be slower than a specific structure that solves the problem. Context is everything.
Maybe you can copy strings with a macro...?
That's very shameful and does not have any content, just ranting.
Of course, the `_ops` structures in the Linux kernel are basically vtables.
He's done quite a bit of ranting (against C++, Emacs, Gnome). He's not even particularly consistent, because he's pro-KDE but anti-C++.
I'm so sorry that it takes TEN FUCKING LINES of code to flip transistors on an arbitrary number of memory cells at very specific locations, causing their capacitors to fill or drain, sending impulses through ALUs, registers, and much more, all so that the transistors elsewhere may have some flips as well. It should be much more simple. 
This! I am a little fuzzy on the details here but as far as I know there's no C++ [ABI](http://en.wikipedia.org/wiki/Application_binary_interface) officially recognized by the C++ standard, meaning that things like calling conventions and name mangling schemes can vary from compiler to compiler. This makes it difficult to produce code that can call into a C++ library. A common work around is to wrap C++ code with a C-style interface, but is more nuanced than it sounds in practice. Further, there's nothing standardized to force binary compatibility between the structures of the standard template library. For instance, passing a reference to an STL container (like vector) from one library to another at run time isn't guaranteed to work because the underlying structs involved are dependent on a particular STL implementation. What's promised by the standard consists of interface specification and complexity constraints. tl;dr The C++ standard is too high level for this to be practical. The residual degrees of freedom at the low level introduce compiler dependency which makes guaranteeing binary compatibility very difficult. edit: clarifying what is meant by "no standardized C++ ABI"
C++ Footprint and Performance Optimization By R. Alexander, G. Bensley Memory Management Algorithms and Implementation in C/C++ by Bill Blunden (That's a bit C code wrapped in C++) TR on C++ Performance 2006 Look into the standard lib documentation, the data-structures have O() annotations. google for dinkumware, sgi stl If memory is your issue acquaint yourself with allocators / boost::array If your using *nixes "The GNU C++ Library" documentation is a great resource. 
Everything you can do in C++ can be done in C, it just might take some more work. So no, not really. It's best to think of C as a form of high level assembly. It's used in systems programming, not only because all of the common APIs are in C, but also because when you make a statement in C you know the gist of what it will compile into. With C++ a lot of that stuff can be hidden, especially if the types get crazy with overloading. And, as others have mentioned, ABI compatibility is kind of a mess. Especially with the GNU libraries.
This sounds like a hash table, not a linear array.
Dan Saks at embedded.com is the man to write such a book. Here is enough articles to fill a book. http://v2.embedded.com/columns/archive/?content_type=pp He has plenty of articles on why developers should prefer C++ to C for embedded programming. The only reason I see for C++ not having caught on in embedded programming is for cultural and historical reasons, just like it is for Linux (Linus anti-intellectual rant does not help to change this). There is this commonly repeated point that the abstraction features in C++ somehow means you loose performance or control of the details. This is a miss-understanding that at best is supported by anecdotal evidence. The "abstraction penalty" might be true for other languages but not for C++ (That's what makes it so special). To be honest, it makes me rage a bit every time I hear someone repeat it. If it was a good idea to use C++ instead of C for developing the constrained Mars Rover system, then the same should apply for developing any old washing machine. Lastly I want to say, if abstraction is a good idea in mathematics, its a good enough idea for software development. Bonus points to the anyone who can point Dan Saks out in [this photo](http://www.softwarepreservation.org/projects/c_plus_plus/Initial%20meeting%20group%20portrait%201.jpg) of the very first C++ committee.
[This](http://www.cplusplus.com/reference/stl/) is a fantastic reference and I use it all the time. It gives guaranteed Big O's for common operations on each of the container classes of the STL.
I think you're the first person in this thread to actually answer OP's question. I've been bullshitting my answers until you showed up.
&gt; all low-level APIs that matter are written in C C++ also doesn't have a stable ABI. I used to deal with C++ libraries (plugins for VFX software, mostly) and we always had to be careful to compile and link using the same compiler used on the library. Sometimes, that means you end up with 3 or 4 instances of the same library in different places, compiled with different compilers. OpenEXR is the worst offender, off the top of my head.
Thank you.
Looks like undefined behavior to me (so could cause a reddit downtime for example). Why do you want to do that anyhow?
Every *public* STL data structure I can think of intentionally doesn't have a virtual destructor to discourage you from inheriting from it. They don't have vtables on purpose.
Thanks. Sorry if I came off a bit assholish. I edited it down a bit...
C++ and embedded systems don't really work together that well, for the same reason that C++ doesn't work well at the kernel level. I mean, it may be possible to make it work, but it's more trouble than it's worth. Part of the reason is that when dealing with the memory restrictions imposed by those programming environments, the programmer must have absolute control over every single allocation, but this is not the case for C++. Is your vtable allocated in paged or non-paged memory, for example? This is, of course, just one of oodles of issues that can arise, and for that reason it just works better to use C at such a low level. Disclaimer: I'm a C++ programmer, and I hate C.
I can only up-vote you once but I can THANK YOU! many times. THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU! THANK YOU!
Thanks!
I didn't even look at it. Is this yet another placement new blog post? 
just a small typo in the source code in regards to calling a destructor: should be a-&gt;~A() not a-&gt;~a()
You are welcome.
This is a really bad idea. Destructors actually modify the vtable, so subsequent calls to the object will be very weird. For example: #include &lt;stdio.h&gt; struct muh { virtual ~muh() { } virtual void m(void) { printf("Hello Bananas\n"); } }; struct bar : public muh { virtual ~bar() { } virtual void m(void) { printf("Hello Monkeys\n"); } }; struct foo : public bar { virtual ~foo() { } virtual void m(void) { printf("Hello World\n"); } }; int main() { foo *f = new foo(); f-&gt;m(); f-&gt;~foo(); f-&gt;m(); return 0; } When executed, will produce: $ ./a.out Hello World Hello Bananas Probably not what you want.