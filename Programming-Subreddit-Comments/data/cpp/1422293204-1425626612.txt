&gt; Let's hope Microsoft will invest more in the C++ compiler to catch up! In the past I would've agreed with that, kind of since Watcom 10 vc has been the best compiler around. But now days with clang, I no longer thing vc is that good and I wish MSFT would just replace vc with clang and spend all their development effort/budget on improving the llvm project. Won't happen, but I would love it to.
Silly mistakes don't get past the IDE, let alone the compiler.
There's a weird tone which permeates that post, which seems to be implying that Python (or similar) programmers somehow posses an extra "skill" which programmers of statically-typed languages don't. Even the statement &gt;And it’s extra important to write testable code. also implies that programmers who prefer statically-typed are not as good at writing testable code. I don't prefer statically typed language because I can't or don't want to write tests, I prefer them because I don't have to write as many tests.
&gt; I hope they replace it with curly-brace scoping. I would love to see that happen :-)
Python 3 is getting optional type-checking: http://mypy-lang.org/ Still only a draft PEP, but Guido is in-favour of this approach.
&gt; For the case of libraries like GLM, the code must build on old enough compilers because users may rely on those compilers. Happens with every widely-used product, you'll have some very conservative customers - either out of habit or red tape or real reasons. We still have to support customers using Windows XP (and I guess many other shops have to, too). We called off Windows 2000 support less than three years ago. Heck, we carried Windows 98 support for much too long, because our customers were also likeyl to still use another product requiring an ISA bus card, and computers with ISA buses wouldn't run anything but. Having them stuck with an old version is acceptable for the user/customer in my experience, even though it increases maintenance effort. 
If only clang would merge OpenMP support into trunk I would use it (have they done this yet?)
I tried using sublime, but the Clang plugin was out of date. They fixed it?
ClangComplete only works for ST3 and on linux. I not sure what you mean that is out of date.
Need my {} can't stomach white space as block delimited.
This is CPP, so they should read the language specification and don't expect anything beforehand.
it's header only, though. Do you have any reference links for me?
[Looks like you're right](http://article.gmane.org/gmane.comp.compilers.clang.devel/39749). I'm guessing there's no definite date for that yet though.
&gt; it's still your responsibility to make a proper installable package. OP is working for free, there is no responsibility.
We force you to indent, but be damn sure not to hit tab..... Yeah, there is nothing wrong with that logic.
Just because you donate your time you think you are absolved of responsibility?
if he wants it in a distro sure, this is something you'd just pull into your externs for a project there's no reason to do a make install at all. Regardless the Op still has no responsibility to supply this, doing so makes it much easier to pick up and use but a header only library targeted at small projects doesn't take rocket science to use.
Hi. I'm not sure that it's a good solution, but (maybe) you can try something like this: http://rextester.com/VDENV31990. Please, tell me if you want to continue with this issue. Thanks
&gt; they have succesfully reached consensus on that "tabs are evil, use 4-space indents Real pythonistas - yes, but I constantly see python code written by part time python programmers who mix tabs and spaces in python code and they have hard time understanding why that is bad.
Who said i was packaging for a distribution or give a crap about doing so?
Okay, so make tabs a hard error (python has been considering this for a while). Problem solved.
Because if you don't, *nobody* will use your software. Really, it's that simple. You follow some *extremely* simple rules, or you get ignored because you're not worth the effort.
Personally, I don't get why anyone would prefer spaces over tabs. It's easy to be off by one space, but tabs are very clearly lined up or not. Also, it's just one character to move your cursor rather than 4.
Nice, thanks for sharing! What are you planning to use it for? I coded up some computational geometry algos recently (was thinking about PCB design tools) but I never got so far as to package it up. Curious, what was your philosophy for dealing with numerical robustness? In the sense of: http://domino.mpi-inf.mpg.de/internet/reports.nsf/144ba569acc69e99c125647900447126/875a972ddb455a67c1256594004db691/$FILE/ATTC1DYI/MPI-I-98-1-004.ps 
&gt;Personally, I don't get why anyone would prefer spaces over tabs. Because of the literally hundreds, if not thousands of files I've gotten from archives, the internet, online acquaintances, and coworkers over the last 24 years from people who *cannot* if their life depended on it *not* mix tabs and spaces. Also, it *really* upsets me on a fundamental level that one character in my file has a variable with, and every other one sits nicely in the grid. Not long ago I went to rosettacode.org to grab some example code for a demo I was going to do - believe it or not, on tabs vs. spaces. I chose the n-queens problem, and scrolled down and copied the [Lua version](http://rosettacode.org/wiki/N-queens_problem#Lua). I was doing a little demo of converting tabs to spaces and back with regex, and it wasn't working as expected, and then it hit me, and I said out loud "Oh, you've got to be kidding me." In one shot, I'd picked [this mess](http://i.imgur.com/3EoARn5.png) - that's me searching the code in Vim for `\t`. I had a coworker who had code way worse than this all the time. It happened because he used the mouse to drag code around all over the place all day long. He had tons of trailing tabs and spaces, too, because his editor didn't show him trailing spaces (my Vim does), and that is not at all uncommon. This is why I don't like tabs. I want WYSIWYG, everywhere. &gt; It's easy to be off by one space It's almost impossible for me to be off by one space. When I type `def function ():` in a Python file in Vim, and hit enter, it auto-indents me 4 spaces on the next line. This happens for anything requiring indenting. I rarely even use the tab key, because all day long Vim just automatically tabs itself in where appropriate. This works even better in Clojure, but it also works well in Haskell, javascript, Common Lisp, C, C++, Assembly, and hundreds of other languages. I was surprised to find out not that long ago that emacs users hit tab on every new line to get proper indentation. It chooses the right indent level at that point, but hitting tab on every line is crazy to me. &gt; Also, it's just one character to move your cursor rather than 4. When I'm done typing lines at the current indent level, I hit backspace (obviously on the next, blank line), and it jumps back 4 spaces, or 1 indent level. I can keep doing that - jumping back 1 level - until I hit the left edge, or just `ctrl u` to undo all the indentation on the line. If I want to indent a block of code, I do `&gt;ip` to indent (`&gt;`) in-paragraph (`ip`), and unindent works the same way. If I want to indent the line I'm on and the next 3, I do `&gt;3j`. If I want to indent from this line to the end of this block, I hit `&gt;}`. If a line was off by a character during any of these operations (which quite literally never happens - I'd have to selectively delete or add spaces to achieve this), it syncs back up with the rest, because indents - with my settings - always snap to multiples of the current shift width. Obviously those of us who like spaces aren't suffering constantly. We have this stuff worked out quite well.
Type errors, yes, but spelling errors never happen, because of how lazy I am. Once I've typed any longish word in a Python file in Vim, I use ctrl+n (or ctrl+p) to complete every other instance from then on. So if I need to type `ImportError`, I type `Im&lt;c-n&gt;`, and it fills it in. I've gotten super fast with this. I use the outsides of my hands for the control keys, so I barely move to hit ctrl-modified characters. If I need the same line again, I type the first few characters, then ctrl+x ctrl+l (one of the most emacs-like combos in all of Vim), and it fills in the rest of the line for me, or gives me a popup of matches, and I ctrl+n/ctrl+p through those. I've probably only type about 1/2 to 2/3rds of the characters in any given file.
This might make you feel good: I used to disagree with you, and hated types, and declaring anything. Now I've finally had some big projects under my belt, and felt the sting of Python's loosey-goosey type system, and I've started banging around in Haskell, and I've fallen completely in love with types. After a year of playing, Python has become a chore at work, because I can't really trust anything. I realized one day that a large portion of my very carefully built tests were just making sure things handled types well, and even chunks of code asked things like if something was a string, do this, and if a list was passed in, handle that, because that's bad. It's just all over the place. Now that I have experience with types, that all seems like complete craziness. Of course, C/C++ types don't really excite me still - the Haskell stuff is where it's at for me, but at least - "yay, types!" - right? I've noticed in myself (waning, thankfully), and in many programmers I've known a wish to get things done really fast, and either feel good, or impress, or get to market first (which is a thing, I get it), so the speed of dynamic languages for prototyping and banging things out super fast is nice. The tradeoff I feel now is one of immediate gratification vs. long-term investment. Investment pays off - we know this in so many other areas of life. You hold back on the rewards now, and you get much more substantial rewards later. I've seen this in many programming and computing things that took me a while to grok - Linux, git, Vim, Haskell/FP, etc. They took me awhile to push through (and Haskell in particular is ongoing, and will be for a long time), and things were a bit boring and without payoff up front, but now things are so much better, and they're better *from now on*. Now it's constant payoff. Now that I'm in a strongly typed language a lot - Haskell - I'm constantly hitting compile errors, but they're up-front errors. Once I make it through that barrage, once I've dealt with that early fight, I'm way safer for the *rest of time*. Vim was a pain in the beginning, but I'm ridiculously powerful now. Same for Linux. Git has changed how I code - not just how I version, but how I code - and for the better. Not every up-front investment is a winner, but there's something suspect to me now when things are really easy up front. I wonder if I'm trading anything away. Haskell's compile-time fight, and TDD - being about up-front testing, pre code-writing - have also shown me that seeing errors immediately when you make them is in and of itself a huge benefit. It's good training. I don't make a lot of errors now, because I can feel the compile error hovering nearby, or the failing test about to rear its ugly head. The up-front battles are like electroshock conditioning. This is what a good type system seems like to me - it feels like a lot more work to think about types up front than to just say x = 3, okay, now x = "foo", but the rewards later seem pretty great, and have turned me around on the subject. This is why prototypes - IMO - work great in dynamic languages. Prototypes tend not to stick around long enough for the long-term investment to pay off, or for the short-term efforts to cause issues. Anyway, my two cents... at least for now.
He didn't deserve the downvotes
Having it in distros would be lovely, so I'll try to make it happen
Not sure what to do with it, yet :) I guess that's a problem I can't 'fix'. As noted, I templated it. You could use long double for your program. If that's still not precise enough, I fear I can't help.
A second point in favor of using iterators instead of STL containers is that the STL does it itself: [functions in &lt;algorithm&gt; take in iterators instead of containers](http://www.cplusplus.com/reference/algorithm/). However, if [ranges](https://ericniebler.github.io/std/wg21/D4128.html) ever get into the language then those would be even better.
Yeah, because breaking backwards compatibility has worked so well for Python in the past. 
&gt; So please stop complaining or joking about whitespace! Why? Flow control using whitespace is *R E T A R D E D* language design.
&gt; So maybe the C++ programmer who says she can’t write Python is really saying “Writing safer code in a dynamic language is a skill that takes time to learn! I have not yet learned it! I would be too scared to commit to writing a reliable Python program right now!” Or maybe what the C++ programmer means is "I don't like wasting time writing extra tests and using extra tooling to make sure my code is safe when compilers can alleiviate much of the work." Of course anyone who says "I could **never** write *&lt;language&gt;*" is really saying they are inflexible, aren't willing to expand their mind, and are blind to the fact that some tools are better suited for some jobs. (I write short Python scripts all the time. But there are situations where a compiled language is better too. And I must admitt that C++ is da bomb for certain situations.)
In other languages you also use whitespace to make blocks **visible** and therefore **readable**! The braces are only used by the **compiler** - not by the human brain! The only difference is, that python enforces a specific formatting by the programmer, where other languages do not directly. But for every language there are style guides that **demand** a specific formatting. The only difference is therefore the **layer** in the tool chain, that checks for the correctness.
I can take it :D
Yes, completely. Take the time to read the MIT license, it spells this out very clearly.
The main frameworks/libraries are supporting Python 3.x now. Am I missing something here?
That doesn't make the attitude and air of entitlement present in his comments any less toxic. You can be correct and still be an ass.
To be helpful for your testing, in my experience geometry algorithms often fail in cases where geometries almost have a relation. For example, almost coincident, almost intersects, segment length almost 0, coincident with almost equal points, etc... Also, checkout http://www.amazon.com/Computational-Geometry-Applications-Mark-Berg/dp/3540779736/ref=sr_1_1?ie=UTF8&amp;qid=1422374000&amp;sr=8-1&amp;keywords=computational+geometry+algorithms+and+applications. It is absolutely the single best book fior advanced geometry algorithms.
Python 3.0 was released more than 6 years ago. Yet large portion of new code is still written in python 2.7. 
You can use: Point &lt;T&gt; center{}, centerShould{}; Curly brace initialization of POD type with empty braces causes zero-initialization. [Source](http://en.cppreference.com/w/cpp/language/value_initialization): &gt; If T is an non-union class type without any user-provided constructors, then the object is zero-initialized and then the implicitly-declared default constructor is called (unless it's trivial). Plus without a default constructor, you can skip zero-initialization if you know you don't need it, which is sometimes a performance win.
thanks, on it already got most of the other stuff https://github.com/I3ck/lib_2d/commits/master
&gt; I'd argue that the circles can be generated as late as possible. If you only have to move it from time to time, you sure can simply move the center point while not having it generated then. Getting deep into personal opinion, but IMO a geometry library should make users want to put their data into the geometry structures ASAP. I've seen too much code that manually manipulates individual components of `Arc`, `Circle` or even `Point` because the class interface is insufficiently expressive. Or too slow. It doesn't seem like a big deal the first time you write that manual code, but then you find yourself writing the same *slightly* complex code over and over to do something like project a point onto an arc. Since I've been harping on performance, one might bring up StrutureOfArrays vs. ArrayOfStructures as a counterpoint. Yes, sometimes it is better to store components separately like an array of centers and an array of radii. But if `Circle` is POD and super fast, you can construct them on the fly from the array entries when you *do* need to process centers and radii together, and it will get optimized into the same register-using code you'd get if you wrote everything manually. But only if the type is POD and totally fat-free.
No need for a copy-ctor, the default one will do the right thing.
there you go: https://github.com/I3ck/lib_2d/commit/3d1e020590a83f1fb505fc14cad9394868eb8cc5
&gt; That's because it's your code and you're doing it consistently. I am by nature a very consistent person. I even once accidentally rewrote a function I'd written months before - about 20 lines - and when I finally found the other one and compared, they were exactly the same, to the letter, whitespace and all. That's par for the course. I follow every guideline rigidly, and can count on things like searches for words in my code finding every occurrence, because they're all exactly the same, down the capitalization. I find maybe 2 typos per year. A lot of that is me using tools and processes that keep me from hurting myself, though. Vim is a good example - it shows me trailing whitespace and misspellings, autoindents, let's me complete words (so I never mistype them), etc. I find tabs to be an example of something that constantly confuses, and makes it hard to reason about what's actually going on in a file. I didn't know there were tabs in that n-queens file until my regex attempts in Vim were failing. I don't like hidden things. At least the spaces don't lie about where things actually are. You can screw them up, but if you *are* screwing them up, it's 100% on you.
I disagree, simply because it's way too easy to use something like Cilk, OpenMP, or TBB that hides many of the issues that come with explicit threads. There is a huge difference between massively parallel implementations and explicit threading simply due to the nature of why you would use it to begin with, due to performance (overhead) and simplicity.
&gt; I am by nature a very consistent person. I am too; I just chose a different method to use consistently. I find inconsistent spacing atrocious, whether tabs are a part of it or not. &gt; I find tabs to be an example of something that constantly confuses, and makes it hard to reason about what's actually going on in a file. &gt; I didn't know there were tabs in that n-queens file until my regex attempts in Vim were failing. &gt; You can screw them up, but if you are screwing them up, it's 100% on you. But all of these points apply to my perspective as well. Inconsistent indentation hampers readability and is 100% on the writer of the code. I would have written that regex to look for tabs, and if it failed it would be because someone else's code was using spaces where I expected tabs. &gt; I don't like hidden things. At least the spaces don't lie about where things actually are. This seems to be the only difference that isn't just about familiarity with the method. For me, I don't really care where the letters line up relative to a different level of scope; one tab means one level deeper. This fact remains true regardless of what width the editor displays tabs.
The short version of the graph is that Clang and GCC are basically complete, ICC is some what okay, and MSVC is trash.
Competition for the sake of competition is also a bad thing too, it results in fragmentation and wasted/duplicated effort. If Microsoft wants to make a competing compiler with clang, they are more than welcome to, but right now their compiler is holding back the use of many C++11 and C++14 libraries on Windows to the frustration of a lot of developers.
I guess what I'm confused by here is your argument about not liking inconsistent spaces, when I've seen inconsistency so constantly with tabs. I've known a number of developers who switch back and forth between tabs and spaces for indentation, until it looks right (like that n-queens file - I've seen a few hundred like that over the years). Then when I've opened their files in my editor, which treats tabs differently, all has been scattered to the winds, and I have to fiddle with the tab settings until it realigns, then `:retab` in Vim to change the tabs to spaces without moving anything. *Then* I consider it right. Python requires indentation consistency, and I actually knew a guy who would add tabs and spaces based on error messages until the file ran, so visually the file was all over the place, the count of whitespace characters worked for the interpreter. I never figured out how he was keeping track of things, when Python doesn't have other ways of denoting which things are part of which blocks. I also had a guy who used a 2-space wide tab stop, and mine was still at the default 8, so he was adding 4 tabs to simulate 8 spaces, and when he would send me a file, it would open with what appeared to be 32-space wide indent levels. I don't want to deal with all of this. I just want things to be what they are. What I'm looking at should be what's actually there. I cannot stand that "Oh, wait, is this...? Yeah, this is a tab, that's why" feeling. I haven't seen anywhere near the craziness with spaces as I have with tabs over the last 2 decades. I almost want to speak in absolutes - i.e. it's *impossible* for developers to deal with tabs, or *no one* ever gets tabs right. It's been bad enough that that doesn't feel too far off. Maybe it's just my particular field.
&gt; I guess what I'm confused by here is your argument about not liking inconsistent spaces, when I've seen inconsistency so constantly with tabs. Because I've seen it quite often with spaces. It seems we've both had our horror stories of files with inconsistent spacing. You decided "fuck this, I'm going 4 spaces per indent everywhere" and I decided "fuck this, I'm going 1 tab per indent everywhere". You see the tabs as the ones out of place when they're mixed together, and I see the spaces as the ones out of place. &gt; I almost want to speak in absolutes - i.e. it's impossible for developers to deal with tabs, or no one ever gets tabs right. I was feeling the same way about spaces, but you've convinced me that there are people out there who are disciplined enough to do it correctly. And when it comes to tabs, I'm the unicorn you've never seen before. :¬)
My vote is for Eigen3. That library rocks the socks off of every other linear algebra lib I've ever used. (Though I've never tried Blaze) It doesn't compromise usability. I feel like I can almost think in Eigen without needing to think about how to transform it into code.
&gt; I love how the whole "you should follow standards" chain below is downvoted. He recommends using autocrap. That crap should burn and recommending it is good enough reason to downvote the poster. 
thank you for the link
done: https://github.com/I3ck/lib_2d/commit/aa04a3055e3591acb1633a5ac183dfa8f1390242 and https://github.com/I3ck/lib_2d/commit/e3ec69dd53bf2eee61736602947b9d2a07e5afb2
all done besides Arc not being defined by points https://github.com/I3ck/lib_2d/commits/master
Plus Eigen is extremely active. They're also [refactoring their expression evaluator](http://eigen.tuxfamily.org/index.php?title=Working_notes_-_Expression_evaluator).
Ok, I really misremembered things here. There was a Clang-based plugin for ST2, but that plugin was discontinued. I'll try this ClangComplete when I get my next project on linux. Thanks
Awesome, thanks. That's kind of the idea I was left with after finishing the video. This is really cool and interesting stuff
This is wonderful. I am building it now from aur... why does it need the whole llvm? It will take hours :(
Would hidden markov models be within the scope of this project?
Milewski's videos are great.
Is Rust easy to get into from C++? I'll have to give it a go sometime, but I too am not convinced that it will overtake C++ anytime soon to really delve into it. Obviously, I should learn to experiment anyway.
Great article! Solving this issue would be very useful for some people in numerics as well. Eg imagine a mathematical function that evaluates to matrices (could be a time dependent correlation function), that we discretize in its argument. We store it densely in memory for efficiency (cache, etc). Now you want to calculate, say, the point wise product between two such functions. With proxy iterators that give a matrix view on the function's values you can use a generic point-wise-product function that, in turn, just uses the standard transform algorithm to accomplish this task. That would be amazing. No more raw index fiddling. 
Depends. Rust isn't a typical oo language. It has made a few stylistic choices that make it a bit different from your typical c++. I would suggest listening to jim Brandy's talk ([slides and audio can be found here](http://www.reddit.com/r/rust/comments/2trruh/recording_of_jim_blandys_programming_in_rust/co1xd8t)) it gives a good overview of the language.
&gt; All the algorithm signatures in the Palo Alto TR use ValueType in the concept checks in order to constrain the templates. That just seems wrong. When I have constrained functions in the past involving ranges and iterators, I always used the ReferenceType, unless a ValueType was absolutely necessary.(its similiar to people preferring `is_same` over `is_convertible`). Of course it still doesn't solve the issue of proxy iterators. &gt; The language needs to change to better support proxy references (an idea from Sean Parent); I wonder if a library solution could be built for proxy references. 
[This is how it looks like](http://i.imgur.com/Az22Fm7.png) in my current setup under KDE. If you're not using KDE, it'll probably look worse, although better than if you were using a GNOME IDE under KDE. So if you're using something GTK based, this may not do, but if you're under KDE it will perfectly integrate.
Concept *do* allow for convertibility like you want. But in the case of the `zip` view described in the article, the value type `pair&lt;T,U&gt;` is not convertible to the reference type `pair&lt;T&amp;,U&amp;&gt;`. If we go the other way and make the function argument a `pair&lt;T,U&gt;&amp;`, then we find there's no conversion from `pair&lt;T&amp;,U&amp;&gt;`. Zugzwanged!
And to add to this, what people really want to write is: auto&amp; x = *i; or Value_type&lt;I&gt;&amp; x = *i; Because it's obvious that derefencing a pointer gives you a reference. Named `ReferenceType`s and the various wrapper functions we apply to give us this abstraction are just band-aids. And if I know Eric, he's going to apply the band-aid, but will be curmudgeonly about it until somebody gives him a language feature that does what he wants :)
&gt; And if I know Eric, he's going to apply the band-aid, but will be curmudgeonly about it until somebody gives him a language feature that does what he wants :) LOL!
Absolutely brilliant. 
&gt;&gt; The language needs to change to better support proxy references (an idea from Sean Parent); &gt; &gt; Do you have any link about this? No, he shared his idea in a private email. It was an off-the-cuff idea. AFAIK, nobody has given this any serious thought to date.
&gt; `while (height --&gt; 0) {` \*sigh\* &gt; http://.../SadBart-Small. **gif** \*sigh\*
Yeah but llvm is a separate set of executables and libraries. If metashell only patches clang I should not need to recompile the whole llvm.
&gt; The idea is to express relationships between types, not between expressions. But you can't ignore the semantics involved with expressions. If you focus just on the relationship of types then later on it can involve extra metaprogramming to reimplement the semantics of expressions to match user expectations, which many times the metaprogramming is not sufficient to match the semantics. &gt; some algorithms have to make local copies of some elements (e.g., unique_copy on an Input sequence) And that would be an example of "absolutely necessary". If you are making local copies, of course, ValueType will be needed.
I was just arguing that using `is_same` is much worse than using `is_convertible`. For the zip view described in the article `is_convertible` is not enough, but I don't see why we can't define a non-member non-friend `convert&lt;To&gt;(From) -&gt; To` function that does the trick and define the `Convertible` concept in terms of that. 
&gt; And both of those examples are wrong. It should be: &gt; &gt; auto&amp;&amp; x = *i; &gt; &gt; and: &gt; &gt; ReferenceType&lt;I&gt; x = *i; No, Andrew meant what he said, you're just misunderstanding him. He's arguing for a language change, where the `auto` in `auto&amp; v = *i` can be deduced as the value type, *even if `*i` yields a proxy reference.* 
&gt; He's arguing for a language change Ok I see. &gt; where the auto in auto&amp; v = *i can be deduced as the value type, even if *i yields a proxy reference. Please no. There is already confusion with users about how`auto&amp;&amp;` may not be an rvalue reference. Now we are going to say that `auto&amp;` may not be an lvalue reference? Also, how will that work with template type deduction? Also will we add another type deduction that would not succumb to these implicit transformations? It would be much easier if users didn't assume that `*i` returned a reference, and we had real(ie archetype-based) concepts that would help enforce that in generic code.
&gt; Because it's obvious that derefencing a pointer gives you a reference. What about `std::move_iterator&lt;I&gt;`? Its value type may be `I::value_type`, but the reference type is `I::value_type&amp;&amp;`. What's obvious is that dereferencing a pointer of type `I` gives you an `I::reference`. 
If you `#define` `TUPLE_UTILITY_NAMESPACE`, you can put the functions in whatever namespace you want.
You should put that up by the top. It's a real selling point!
I was not trying to be clever. I was trying to use humor.
ok np. I'm happy to clarify my intent. You can laugh now =)
I just finished implementing something similar, but without the c++14 features that make this awesome at compile time. Really great that this is even possible now!
The thing is that if height is unsigned, this is the basically the only way to write a loop that iterates down through all values including zero. And at that point, I am not the only one who thinks that this: height --&gt; 0 // read as 'down-to-operator' is easier to understand at first sight then having to think about what a post-decrement followed by a greater-comparison is supposed to do. (height--) &gt; 0 // can certainly be done but requires more thinking So this really is one of those tricks that confuse you the first time, but can really ease your life after that.
I hope, I'll get back at discussion at pull-request 1300 when I find the time.
Just put a space in, for the rest of us.
People have been proposing type-safe printfs since we got variadic functions, but there's one commonality in all of them that really bothers me: the syntax. printf("%d/%d/%d %s: %s", month, day, year, event.c_str(), note.c_str()); Even just reading the format, it should be obvious that it prints a date and two strings which somehow relates to that. printf("%/%/% %: %", month, day, event, year, note); Now, it looks like just a bunch of random symbols until you look at the variable and try to match it up to its format specifier (if you can still call it that), except that it's actually worse. Notice how I *accidentally* mixed up the event and year? gcc or clang would warn me, with C's printf, that the type I passed in didn't match the format specifier, but version doesn't complain and will, in fact, print "12/30/new year party 15: at -----". Then there's the other variant I've seen with syntax like this: printf(month, '/', day, '/', year, ' ', event, ": ", note); // or: std::cout &lt;&lt; month &lt;&lt; '/' &lt;&lt; day &lt;&lt; '/' &lt;&lt; year ... I dislike this variation because I find it harder to parse the actual format from the data. The one good thing is that it involves no run-time overhead, unlike the format string-parsing version. Since this idea has been played around with so much, and people seem to like it (I do), I think it'd make a good addition to the standard library and sorta wonder why nobody's proposed one yet (or it was rejected?). So I'm kinda curious if anyone has an opinion on this: a type-safe printf, but with a relaxed syntax so that you could write cout.format("%s %i %i %f %f", str, int(), long(), float(), double()); Here, `%i` does not mean "integer of N-bits" but "any sort of integer", `%f` means "any sort of floating point number". Additionally, with compiler support, similar to C's printf, this can be trivially transformed into cout &lt;&lt; str &lt;&lt; ' ' &lt;&lt; int() &lt;&lt; ' ' &lt;&lt; long() ... without requiring any run-time parsing.
I updated my code , i added new features , wanna know your suggestions , i'll also add an api for handling JSON data so what do you think about it ? :)
This idiom doesn't work with pointers or iterators and its not difficult to write using a while loop. http://ideone.com/IUJ0Qx
Since `std::pair` is a very general container that doesn't represent a concrete concept, there isn't a unique definition for every operator that would be correct and useful everywhere (if you define it in an optional namespace it would be less of a problem, of course). Element-wise application would probably usefully cover a majority of use cases, but what should e.g. *std::equal_range(c.begin(), c.end(), 42); do? Element-wise application risks dereferencing an end iterator; it probably makes most sense to return `*p.first`, but there's no way to distinguish this case in the type system, and that itself may be a mistake anyway if `p.first == p.second` (empty range). Of course, that brings up a different problem, which is that the return value of `equal_range` **does** represent a specific concrete concept (a range), but it returns a `std::pair`. With some template metaprogramming you might be able to make the operators safe 99 times out of 100, but the 100th time someone will come along who is using `std::pair` when they should be defining a more specific type and then if you're lucky you'll get a segfault. Regarding the OP example, why not just do int min, max; std::tie(min, max) = std::minmax(c.begin(), c.end()); ? **EDIT:** I believe in Python the equivalent of the OP's C++ code would be: min,max = tuple(*x for x in c) And the C++ equivalent of the Python equivalent would be: int min,max; std::tie(min, max) = fmap(std::minmax(c.begin(), c.end()), [](auto it) { return *it; })); with a suitable definition of `fmap`. The `fmap` version is more general, is explicitly applying an elementwise transformation, and arguably less susceptible to mistakes. The operator version is shorter and prettier.
All the people who moved away or avoided it because of the whole debacle?
You'll be much better off with The C++ Programming Language, 4th edition as a reference. C++ 11/14 has advanced far beyond what is covered in TCPL3. In addition to that, get Stroustrup, A Tour of C++ for a concise overview of the "new" language as a whole.
It's not so much that you understand it the first time you see it. It's that you freak out and get a headache before you understand it.
I don't find pre-incrementation to be premature optimization. I think it's more about using the right tool for the job. If you don't need a copy of the object, you don't post-increment, simple. This also conveys meaning, not "I'm optimizing this." On a sidenote, due to how much more frequent pre-incrementation should be used, I wish the notation were inversed. `++it` is just weirder than `it++` since in C++ every other operation on the object is written after the object. But it's too late to do anything about that now.
So you'd prefer the increment operator to come AFTER the operand to signify PRE-increment and vice versa? Why do you hate people? :)
It seems pretty useless to use postfix increment at all.
Think about it. They are only so named because of their notation, not their function. Nowadays post-increment is synonym with incrementing and returning a copy of the previous value, but I don't see any logical explanation for choosing that to be 'post' rather than 'pre' other than it being an arbitrary choice.
It's reasonable to use either. I always use postincrement when there is a choice. (and before I get lots of flames, I'm only expressing my choice, not specifying correctness) My reasoning: The compiler will optimise out any redundant copies, and the code is more readable (as you say what you are operating on before you say what you are doing to it, which fits more with the general object-oriented pattern). Preincrement (for me) stands out as a bit weird-looking. I also like to think of it as shorthand for: x += 1; Which expands to the more general: object.operation(parameter); In the end though; it's going to come down to personal preference, or more likely, what the coding-standard document says for your project.
Actually, pre-increment can be slower. With pre-increment, the data needs to be incremented near immediately, meaning it usually can't be pipelined away, while with post-increment it can be done at some irrelevant point in the future, thus meaning there's no pipeline stall (and the copy and increment can be done at the same time).
I don't think it matters here. additions are crazy highly optimized in modern CPUs. It is pretty much the fastest operation you can do. On top of that, modern CPUs are pretty good at reordering instructions based on when the data is needed and what the side effects are. But beyond that, they also can do some pretty cool things like starting two instructions and the pushing forward the results of one instruction in the registers of the next instruction (since there is usually a lot of unrelated actions that need to happen.)
&gt; every other operation on the object is written after the object. The other unary operators (*, &amp;, ~, +, -, &amp;, ! and casts) all go in front of their operand. `it++` is the odd man out here.
You beat me to it. Like with many other expressions, there's potential for two different things going on “at once” – the operation itself, and the value it evaluates to as an atom. “Pre” and “post” signify whether the operation itself is placed before or after the evaluation in the processing chain, and I think there's no more fitting mnemonic for the actual operation than the actual operator, hence the placement… at least that's how I learned it.
The compiler will schedule the instructions (dependent of the choosen cpu architecture). Modern compilers do not blindly replicate the original instruction sequence. In addition the CPU is peforming instruction scheduling with "Out of order execution".
&gt; Preferring ++i does not mean writing more complex code in the name of performance before you can prove it’s needed — ++i is not more complex than i++, so it’s not as if you need performance data to justify using it! &gt; Rather, preferring ++i is avoiding *premature pessimization*, which means avoiding writing equivalently complex code that needlessly asks for extra work that it’s just going to ignore anyway. - [Herb Sutter - GotW #2 Solution: Temporary Objects](http://herbsutter.com/2013/05/13/gotw-2-solution-temporary-objects/)
Why don't you change `operator &amp;` to return `bool`? It won't satisfy requirements of 17.5.2.1.3 [bitmask.types] anymore, but who cares?
What? Why? It's the same philosophy as static typing- make the *correct* way to to do something the *only* way to do something. Why would you ever not want to indent properly?
It's just shorthand. It's not the end of the world.
&gt; This is the answer to the question if the compiler can optimize the postfix increment. Sure it can. If you study the implementation (assembler code), you will see that the both functions are implemented with the same instruction set. Well, the article is testing vector::iterator which can be implemented as simply a pointer. A pointer post-increment can be easily optimized, just like the size_t example. I would like to see this test rerun with a complex iterator type such as std::map, std::unordered_map, or possibly something from a math library like an iterator over sparse matrix elements. Then we'll really see how good the optimizers are.
Actually, I was only thinking of validating the number of arguments at compile time; it seems fine that formatting occurs at run-time. Since `count_format_specifiers` is already `constexpr`...
If the value is being used, you can't blindly switch between pre- and post-increment anyway, because they return different values.
&gt; causing a pipeline stall An instruction has to have a runtime of &gt; 1 clock cycle before it can cause any sort of pipeline stall. Addition is one of the few instructions (on intel CPUs) which has a runtime of &lt; 1 clock cycle (because of magic trickery). It is not causing a stall because it is finished before other instructions have even started their fetch/decode phases. It can't even get in the way of the next pipeline stage because it is finishing before the cycle ends.
I think if you want to know which one is faster you should actually test it. My instinct tells me that `it++` means "increment `it` and then return the value of the original variable" which *might* be optimized and executed by the CPU as "return the value of `it` NOW and increment it when you get a chance but before the start of the next iteration." So I'm thinking that (a) making a copy is more expensive than not making a copy, and (b) you need to increment it before the next iteration of the loop and that operation has fixed cost (ie, the operation `it += 1` is performed inside `++it` or `it++` either way so they only differ on the return type). Speaking of that, would `it += 1` be preferable in any case over `++it` or `it++` for performance purposes? It's a negligible difference either way. Also consider that some iterators are less trivial to copy. For them, it's quite clear that `++it` is faster than `it++`. The question is whether the compiler is smart enough, and being run with high enough optimization, to discard the copy and therefore not perform it. But then one should ask: how does `++it` and `it++` differ if the latter doesn't return a copy? Are they not both different ways to write `it += 1` in that case? And if so, wouldn't the "extra function call" from `operator++()` to `operator+=()` add overhead that could be avoided by using `it+=1`?
https://en.wikipedia.org/wiki/Return_value_optimization The following code should trigger RVO on modern compilers for (almost) any complex T: T operator++() { T obj = *this; ++*this; return obj; } If the function is different, then sure, RVO might not be triggered. But it seems to me that you're saying that a complex T may cause the compiler to not perform RVO. This seems rather silly to me since it doesn't matter to the compiler how complex the object is, it knows it's created a temporary of it which isn't used. I suppose it might not optimize it out if the constructor performs operations on external data, such as an object tracker, although even then I suspect it might avoid the additional construction and run that external code separately.
And how well can the compiler tell that the construction and destruction of the temporary doesn't have side effects? If there any sort of run-time assertions, debug statements, etc..., then there are clearly side effects, which is likely why postincrement in debug builds in the article are indeed slower. But complex iterator types may make some calls that are not easily determined to not have side effects. In these cases, the compiler wouldn't be able to optimize it away. And by the way, RVO means that the returned object when returned by value doesn't need to be created again during the return. Construction should still be performed once, assuming of course it doesn't get optimized away (which is what I believe you may be arguing). Also, your operator is incorrect; you should be returning by value, not reference. As it is, you're retuning a reference to a stack variable which will result in UB.
```my_flags &amp; my_mask == expected_flags``` cares
It's not, though. It's a genuinely strange expression, since it calls for temporary state preservation, which nothing else in the language really does. I'm sure that that's perfectly easy to do with ints and pointers, but now that we have iterators that can refer to anything that can be expressed as a range (for instance, the `istream`/`ostream` iterators) and potentially modify the underlying state of that thing (for instance, `operator&gt;&gt;` on an `istream`), the state-preserving `it++` is more of a stumbling block than anything.
As of C++14 the compiler is allowed to drop or combine allocations from new expressions (but not explicit calls to `operator new`), so given sufficient inlining it's theoretically possible to optimize it out. Clang 3.5 doesn't appear to do so even when compiling in C++14 mode, though.
This shouldn't even be a question. Of *course* it's reasonable. If you're not using the result then they mean the exact same thing and anyone who can't cope needs to get off of my ~~lawn~~ code. Using `++it` over `it++` in that context doesn't need to be an optimization at all, it's an arbitrary choice. I happen to prefer the former.
Still not as easy to use as old enum. I don't mean to nitpick your suggestion or suggest that use old enum is better. My point is the purpose of switching to enum class should be to get better scoping and type safety on top of everything old enum offered (nett gain of improvement) but unfortunately this is not the case, we must trade off usability.
&gt; In particular, casting [a char] to int is not correct. Why not? Because char may be unsigned, and char and int may be the same size. For example, on a hypothetical two’s complement system with 32-bit bytes and 32-bit words whose native character set is Unicode (the computer of the future, maybe? well, except for those 32-bit words), a char may hold the value 3,000,000,000 (or 0xB2D05E00) [which would be cast to a negative int] Wow, they seem to really care about the really unrelevant stuff. This blog confirms my decade of experience with C++: You need to have way too much in your head to use it correctly.
Good point, I should have read more closely. I suspected it might be something like that, thus "or. . . extra overhead imposed by the STL implementation". 
But unlike the color of the shed, pre- vs. post-increment actually has different functionality. It's more like do you want the door on the bikeshed to face the street or force everyone to walk around to the back first?
He's testing with iterator range-checking enabled.
This sub isn't for c++ questions, try /r/cpp_questions or /r/learnprogramming. That being said, you're outputting hold's remainder in the if block at the bottom instead of p1 or p2. Besides the output, I'm not seeing any references to hold in the code. The randomly appearing values are a different story. Since you never initialize the variables money or remainder in Person, their starting values are just whatever's floating around in memory at that spot when the person is instantiated. 
It's the correct way to handle bitmasks where some of the enum values have multiple bits set, and many people advocate always doing it since how many bits are set in each enum value should be an implementation detail.
Char is the name of the integer type which is the smallest addressable unit of memory. Therefore, code which operates on raw memory should use char in order to be portable. That may seem pedantic, but I've worked on DSP's that can only address 16-bit integers. Ie, sizeof(int16_t) == 1.
That's incorrect. char16_t and char32_t are guaranteed unsigned, N4296 3.9.1 [basic.fundamental]/5: "Types char16_t and char32_t denote distinct types with the same size, signedness, and alignment as uint_least16_t and uint_least32_t, respectively, in &lt;cstdint&gt;, called the underlying types."
Yes... I'm sorry, but I don't remember where I've found it. There is a little clue in the article[ "How to read an entire file into memory in C++"](http://cpp.indi.frih.net/blog/2014/09/how-to-read-an-entire-file-into-memory-in-cpp/) - maybe someone will find the version written ten years ago with an author name at the bottom ;-).
Instead of using char portable code that wants to address a byte should simply use uint8_t.
Did you have a look at [tiny-cnn](https://github.com/nyanp/tiny-cnn)? I somehow like tiny-cnn's code much more, it's a lot more modern. Also, do you provide the accuracy of your code on e.g. MNIST somewhere? That would kind of help as a baseline for people to see if your code works or not. Also you could e.g. provide a sample implementation of a LeNet-5.
It is unrelevant, because AFAIK there is no compiler where sizeof(int) &lt;= sizeof(char). What does x86 vs PPC/ARM have to do with it? Edit: s/hardware/compiler/ as suggested
This is the C++ reddit. Everything implemented wrt this article is written in C.
s/hardware/compiler/ This motherfucker has sizeof(char) == sizeof(int) == 8bit : http://www.ccsinfo.com/product_info.php?products_id=PCB_full
Interesting project. I will try on MNIST; it should be equal the results reported in the original paper (~94%, Wilamowski, B. M., &amp; Yu, H. (2010). Improved computation for Levenberg-Marquardt training.), as it is exactly the same algorithm.
Why not use C++. C++ has all the advantages of being low-level when you need to be low-level. However, you can put a nice, safe wrapper on it. In addition, in C++ with the STL concepts framework, you can think about the algorithms and data structures in a generic sense. What is the essence of this algorithm? What kind of access does this data structure support? 
What machines can not address 1 byte?
The TI C28x architecture cannot. Its 32-bit address space accesses 8 GB of memory, not 4 GB. It is one of the most popular microcontroller/DSP lines in the field of power electronics.
Very interesting, I've never heard of that. Thank you.
A disappointing consequence of this is that future versions of the C++ standard library (or any generic library) can't add initializer list constructors to existing classes that don't have them without risking breaking old code.
An enhanced, correct write-up is now available.
Thanks for the comments! I didn't consider the Boost license when I was choosing a license (for no particular reason). The main difference seems to be that the Boost license doesn't require to keep the copyright notice in executable distributions. If that's something people want, I'll consider it. Regarding unit tests: You're right, I thought about that too - I just started with gtest because I only read very good things about it. I thought the additional dependency isn't so bad, as gtest is included in the project distribution anyway. On that note, I think my tests are probably quite bad (far too long), it was my first time writing tests. For example, there's certain use-cases in which I want to make sure the optimisation converges and that ended up in quite a bit of code (generating data, etc.).
Modernize your cpp code! Make it in C#!
thats what I got out of it I get why people like to use auto in for loops and non typedef long-variable names/iterators, but I don't understand why scan a whole project to replace and enforce auto in normal variable declarations.
This has to be the ugliest operator overloading I've ever seen. especially because it's hard to even notice that there are operators involved...
I prefer simply named functions..
WTF is with that code? C++ just looks horrible. It's exactly the reason I've learnt/still learning C and C#, skipping the bullshit.
That's just a *little* bit of a hack.
The name of this function is as simple as it gets. And furthermore, using an infix operator here makes a lot of sense. After all, that’s why other languages *do* use an infix operator here (e.g. Python or R). It’s very unconventional in C++, of course, because the technique of defining named infix operators isn’t widely known.
I don't know how I've gone so long not seeing the Fit library. Well done, Paul Fultz II. As always when I see these shiny things, I check to see if it will work in the latest version of Visual C++. The answer in this case is no, it doesn't work with MSVC 12. 
Yes. See the explanation here: https://github.com/klmr/named-operator (also linked above).
Because code is clearer, always use exact types thus you avoid type conversions when you don't need them, less typing, easier refractoring(for example if you change type of variable that you use as source val. in some other variable like this: auto y = z;, then you don't need to manually change type of y. Tons of reasons. And I don't understand this C# remarks. Care to enlighten me? It's hating C++ because it's getting better? Or what? About auto, watch this: https://www.youtube.com/watch?v=xnqTKD8uD64 
By MSVC 12, I meant the version that comes with visual studio 2013. It is the latest stable version. I don't deny that they are behind on standards compliance though.
It aids in refactoring down the road and helps future proof the code.
Ah OK... You may try then on the 2015 technical preview =/
You can still call it as a function as well: if (in(5, numbers)) std::cout &lt;&lt; "Yes" &lt;&lt; std::endl; 
[I coded a version of this that works in C++03](http://ideone.com/tItZgA). Note: I haven't looked at any of Paul's source code, just wrote something that would make the same syntax as in this post work. The main problem is obtaining the return type for an expression like `x &lt;foo&gt; y`. In C++11 it is easy because you can use `decltype` (or `auto` which does the same thing) to specify that your template function returns the same thing as the underlying expression `x + y` or whatever. C++03 doesn't have that, so I had to pollute my code by carrying the return type all through the implementation of `infix`. In C++03 my code will use the left-hand-side as the return type by default, and you can override this for specific "operators" such as `&lt;in&gt;` which I have set to always be `&lt;bool&gt;`. In C++11 it will use `decltype`. 
Not exactly. This is used for improving errors at compile time, so that just the errors at the API boundary is shown(and not the inners of the library). However, it is somewhat similar in concept.
Woe on those who wish to `std::find` two things.
So I installed CPP depends. So far the UX really sucks, for example my project uses Qt5 and CPP depends complains about the $(QTDIR) variable being absent then proceeds to dump a bunch of crap about Qt being too complicated. Sticking to PVS for now.
This is so elegant. Thank you for sharing this !
i would love native support, a hope for the future, a dream. specify affix was maybe a unclear, f.ex instead of a dummy int variable for prefix incerementation: type&amp; prefix operator++(); type suffix operator++(); 
I don't think the said elegance is worth the overhead in comparison with the more simple `std::count()` or `yourContainer.count()`which do quite the same thing.
Don't ever go wandering through Boost.Spirit then.
Or !in to be simpler?
I think the main issue is that it's not an infix operator. It looks like one, but it's a just a hack to add a language feature that doesn't exist. If you want named infix operators, write a proposal - don't hack it in!
`std::any_two_of`! Know your library.
Except that this doesn’t work. `not` is identical to `!` in C++.
&gt; If you want named infix operators, write a proposal - don't hack it in! Why not? The code clearly demonstrates that we *don’t need* to add a language feature, a library is enough. Adding libraries instead of language features has always been the preferred way in C++ standardisation. I honestly don’t think that there’s enough general interest to warrant a language feature proposal. Nevertheless, the specific case shown here of an `&lt;in&gt;` operator works well.
I don’t think this is obviously more readable, even for people who see this `&lt;in&gt;` construct for the first time in their lives. I agree that *for now*, this has no place in production code. But playing with it in a public forum (such as here) may make this idiom familiar enough to enable production usage at some point in the future.
Cruft is a quick-and-dirty piece of code. The code posted here is carefully designed. To be sure, it’s a *hack* – but it’s not *cruft*. Ironically, *your* code qualifies much rather as cruft, since it’s less efficient than using `std::find` (and, consequently, `&lt;in&gt;`).
This Drepper is a damn genius. He has an attitude as far as I know, but damn he's good. 
I agree that libraries should be preferred to changing the language, but I think libraries should add functionality, not language features. This library is pretending to add a new language feature. If you write the same statement as `if (5 &lt; in &gt; numbers)` it suddenly looks very odd. Those less than and greater than operators certainly don't mean less than or more than anymore. If there were some feature of C++ designed for this kind of extension, I'd be fine with it - but there's not.
How come? auto two_things = rng | filter(pred) | take(2);
That would be clever.
&gt; it's an extra dependency to the fit library and a need to read about the fit library It is an extra tool, but it is a very simple tool. Its always good to learn about new tools. Plus, it does a lot more things than just infix operators. &gt; Just adding the fit library as a dependency ensures that if you ever want your code to be cross-platform you will have to remove/rewrite the code (visual studio doesn't compile fit). Fit is cross platform and works on windows, it just doesn't run on visual studio compilers.
(6) I was wrong about this. I put in a [pull request](https://github.com/progschj/ThreadPool/pull/22) and someone kindly showed me the error of my ways. The child threads may call `enqueue` before `join`ing in the destructor of `ThreadPool`. (8a) Yes pretty much the fact that the "do nothing" cases in switch outnumbered the ones that did something. But maybe this one should have gone more in the personal opinion section. (8b) Ah if it's a `double` then a copy of `lambda` will get converted to a `double` then after the expression it will be converted back to a `float` to assign to `lambda`. I'd suggest one of these then, lambda = lambda * static_cast&lt;float&gt;(cv::norm(data)) / numTrainingElements; lambda *= cv::norm(data) / numTrainingElements; lambda *= static_cast&lt;float&gt;(cv::norm(data)) / numTrainingElements; (10) &gt; There are more vector-like containers in heaven and earth, Horatio, &gt; &gt; Than are dreamt of in your standard library. If you add an iterator interface, your code now works seamlessly with these vector-like containers: * `boost::container::vector` * `QVector` * plain array * `eastl::vector` * `std::array` * initialization list * `std::experimental::dynarray` * `wxVector` * `boost::ptr_vector` * `ustl::vector` * ever other container ever! You don't have to `std::move` in this case. But if you do, you'll replace a copy constructor for a move constructor when creating your member variable. This could be expensive if the `std::vector` is large. You don't need to document this because it doesn't make a difference to the caller. If you wanted to you may want to mention that you could move a `std::vector` when constructing your object: std::vector&lt;RegressorType&gt; vec = foo(); // if we also std::move within optimiser's ctor, the below line has no // copy construction of std::vector, only 2 move constructors SupervisedDescentOptimiser(std::move(vec)) optimiser; // note you can no longer use vec other than to // assign to it or destruct it Glad I could help. Good luck with your project! 
I do this. For any libraries like Irrklang or Devil I just throw them into the run directory of Visual Studio. What might work too, if you want to keep the DLL in the project director, is to create a build event that will copy it into the run directly when you build. I do this with assets and it seems to result in a "everything you need to run this" directory automatically.
I remember reading somewhere that the Boost Committee is trying to build some type of dependency manager too, but last time I checked it was still in planning phase. Hopefully the bii guys will let us add libraries. Thanks for posting this btw
It was called Ryppl. It's now quite dead, since it was one of Dave Abraham's projects and he's no longer working on Boost.
Im not shure about that. I know that there a projects on biicode which are also on github. But I think there syncing it by using travis ci. So they push it to their github repo, travis is testing it and if the test succeed, it pushes it to biicode [biicode &amp; travis ci](http://blog.biicode.com/automatically-build-publish-via-travis-ci-github/) I think thats the way how that should be handled
I've definitely experienced the behavior I mentioned - linking then crashing. But that was quite awhile ago, so maybe things have changed since.
You can add as many libraries as you want. Check [biicode docs](http://docs.biicode.com/) . There are also some examples on the [blog](http://blog.biicode.com/upload-biicode-example-5-libuv/). 
I like this too, especially because it solves the %%%% formatting issue with named parameters: "{month}/{day}/{year} {event}: {note}".format( month=month, day=day, ...)
I respectfully disagree. Constructs such as these are bad for both the beginner and the veteran programmer. For beginners, it is bad because they think infix operators make the program more readable. For veterans, it creates a syntax puzzle. C++'s syntax is bad as it is, we don't need more visual puzzles. For everyone, it creates an operator 'less than' or 'greater than' overloading problem. This construct actually doesn't help, because a simple function is_member_of will be equally beautiful in the long run. 
I see. Learn something new every day and your explanation totally makes sense! This will definitely avoid some weird bugs because two libc++ implementations will never behave the exact same.
Yep, Apparently just trying to build boost with CMake is a tough. I tried to build "Bullet Physics" library and got a warning message that you cannot publish libraries over 12 MB... something tells me Boost is currently out of the question. Edit: it calculates the size of all the files in the block including files that are not included in the build (pictures and such).
&gt; Infix operators do make the program more readable. If there is one lesson we can take away from Java’s refusal to allow overloading operators, it’s that experience with Java libraries has undeniably shown that infix operators are much more readable than prefix functions in some cases. Infix operators satisfy only the comprehension of the programmer's intent, not the comprehension of the program's actions. Readability is not what you think. I.e. readability is not only when I am able to tell apart the various symbols. Readability is mostly about understanding what the program does. And that's why languages avoid operator overloading, because after a specific point, operator overloading obscures what the program actually does. &gt; There is no problem, the overloading is completely safe and plays nicely with other overloads. What if I make a public generic overload, like this? bool operator &lt; (T1 t1, T2 t2) { } 
&gt; Auto pointers shouldn't be used, and raw pointers that owns something shouldn't be used too. Never use new/delete, except very isolated, lovlvl classes. But what if they *are* being used? How would I or you know? That's my point. You'd never know. You're maintaining someone else's code. Now you have to inspect the functions because its so poorly done. `auto` has been used pretty poorly here. &gt; Watch this talk that I provided. Most of the time approximate type is logical. And you don't need exact type. You need something like int, or something like real number. Not int64_t every time. I had already seen that specific talk, and even skimmed through it again because I happen to like some of the stuff he goes over about the return by value optimizations. Though, I happen to think he talks a bit much without displaying a benchmark or two, but he directly quotes Bjarne on some things so its better than nothing. I tried to emphasize that I know that auto is still static typing, and isn't exactly the same as dynamic typing that JS employs for example. But, its still in line with some of the reasons people dislike dynamic typing anyway - namely how you read code. I will say, as someone who uses a lot of unsigned numbers, something like auto x = 40ul; Is nice. I actually really like that myself. Same with the long iterator names as I emphasized before.
Note that biicode has travis CI deployment support out of the box.http://docs.travis-ci.com/user/languages/cpp/#Dependency-Management In that docs there's a github project I have written as a working example.
&gt; 10.000 users and we go OPEN SOURCE Just go open source already, I don't want to sign up for yet another thing.
Their behavior is comparable to the "This baby has cancer. Like this page to help him!" things on facebook. 
have you changed your location to blocks/username/repo ? your main.cpp should be saved there, after that It should work. (Tried it on my pc, worked well)
WinSxS, afaik, needs administrative permission and tinkering with the registry, something that not every developer wants to do, as it typically opens an additional can of worms.
Cool! Hope next c++ standard will introduce a way to implement user defined named-operator.
Those libraries are probably distributed as .so files in ELF format which should be compiler/linker independent as long as they comprehend the standard the same way: http://wiki.osdev.org/ELF. The link /u/duuuh provides should go into lots more detail.
Ah, that explains it.
Why don't you write to three temporary files and rename them once all three have properly being written to?
Thanks for the heads up, I didn't realize that. 
I see. I also definitely get your point regarding iterators now! Completely agree. Thanks for the great explanation regarding the `std::move`. I definitely have to brush up on it. Thank you for your time and effort!
Currently, ranges exclude certain type of iterations, such as algorithms for tuples, or segments. Ultimately, it would be nice to see higher-level of algorithms that could support these structures as well. And perhaps with that, you would be able to do more extensive things like database queries. It could be possible to build something like this with monads, fold, and unfold, but it would need to be modified in a way to support random access effeciently.
This is something your filesystem and operating system would need to support. It can't be done in a userland library, and transactional filesystems don't really work similarly enough to each other for a cross-platform wrapper to be useful.
&gt; I'm a big fan of the open source world You should learn the difference between "open source" and "free software". EDIT: That is it sounds like you may sometimes mean free software and not open source. But I could be wrong.
char and wchar_t are also unique types, which permits overloading. (char is bizarre because it has the same range as either signed char or unsigned char, but is not the same type. In contrast, int and signed int are exactly the same type.)
That is forbidden by C and C++ where int must be at least 16 bits.
Qt Creator is a nice C++ environment, freely available for Mac. Aimed at (and written in) Qt but perfectly good for other C++ coding. The native Mac IDE is XCode - free download, lots of nice tools, kinda complex. All the IDEs on mac use the native command line tools to actually compile and link things, so you'll need to download those too (most easily via XCode, but you can download them separately too). You'll probably need to learn some other build environment, whether that be make, qmake, cmake or whatever, as the visual studio build settings are quite different to everything else. If you're writing commandline applications you'll have little problem building them and running them on a Mac. If you're doing GUI applications using the Windows API then you won't (and you'll need to run Windows instead, possibly in a virtual machine on your Mac).
Also, as far as int main() vs int main(int argc, char *argv[]) does it matter which I use at home? The latter is the default in Visual Studio.
&gt; I'm curious what you mean by "segment". Makes me think of geometry. Sometimes it can be more efficient for some algorithms to expose a range as a range of ranges, where the subranges are called segments. &gt; As for tuples, that was kinda my goal with this: But I mean there are extensible parts that just need to be implemented instead of reimplementing all the algorithms once for tuples and again for ranges. &gt; Not sure what you mean, there. Example? As an example, the `cycle` algorithm could be implemented like the following with a monad, where `for_` is monadic bind, and `range` is like the python range function: template&lt;class Range, class N&gt; auto cycle(Range&amp;&amp; r, N n) { return for_(range(n), [&amp;](auto) { return r; }); } Ideally, if the original range `r` is a random access range then using iterators, its trvial to make the cycle range random access as well. Its not so easy to make the above random access. However, the above benefits from more generality because we can apply the above to ranges as well as tuples. Does that makes sense? 
If you aren't using the arguments then `int main()` is good style. 
Hm. At least till recently on SO in [c++] the consensus has been that expressions have a type and a value category, and the type is a non-reference type. See e.g. http://stackoverflow.com/questions/16567044/c11-type-of-variable-in-expression &gt; So although the identifier y has type int&amp;, the expression y has type int. An expression never has reference type, so the type of both of your expressions is int. And: &gt; As I like to think of it, "values never have reference type, only variables do". – Kerrek SB And: &gt; An expression cannot be a reference, although you can bind the result of an expression with an lvalue or rvalue reference. Also: http://stackoverflow.com/questions/20707442/whats-the-type-of-the-function-call-expression &gt; The type of f() is int; its value category is lvalue. So there's at least some difference of opinion here.
what is biicode? their website says absolutely nothing.
Use your operating system's functionality: on Windows you have transactional filesystem functions (doesn't work with FAT), on Linux you have atomic rename of file or directory.
Sorry - do a full sync() instead or fsync each of the 3 file handles after rename. Note however that sync &amp; fsync can often be equivalent in terms of performance &amp; what they do - it can be difficult to impossible to do a per-file flush so a full sync is performed on every fsync.
That is really neat. It also seems drastically simpler to use than boost range, although less flexible. Probably a good intermediary until C++ ranges make their way into the standard. The biicode dependencies feel strange. Also, the macro functions that look like C++ functions are odd, and they're formatted as C++ functions (i.e. indented into a namespace). Additionally, I'm pretty sure that the macros aren't actually necessary &amp; could be accomplished with templates, but I'm not 100% certain. Also, the | operator overload is pessimizing performance - there's no need to std::move something that's an r-value &amp; you're removing the possibility for UNVRO.
I won't sign up to this thing, just based on the frequency "C/C++" is mentioned on the website or blog. There is C, and there is C++. They are two pretty different languages. There is no such thing as C/C++. Seriously, this is annoying as hell. Besides, I'm not entirely sure what kind of problem this is supposed to solve, besides taking away precise control from my build process. What if I want to switch compilers that are not ABI-compatible? What if different projects need different versions of the same library? What if "blocks"(?) hosted on biicode are outdated? Why does anything need to be hosted on/by biicode anyway??? And what's wrong with the good old "know and build your dependencies"?
Better/Alternate/More-portable solution is to use [SRecord's](http://srecord.sourceforge.net/) c-array output format. The source format can be any of the huge list of supported ones, not just binary. *full disclosure: I'm the maintainer.
Better/Alternate/More-portable solution is to use "xxd -i", already available on your platform. But I guess the incbin idea is to avoid the usage of an external tool, and I personally like the idea. I would love to see the same kind of trick to include an external file as a compile-time constant char array, so I could do some beloved-C++1y-template-metaprogramming-compiler-blow-up-thing on it, but it doesn't seem possible right now.
Honestly, why not link directly to the original article? Isocpp.org is seriously starting to get on my nerves. The idea started off nice enough but this constant link-stealing is just incredibly spammy and unprofessional. I thought Reddit frowned on these practices as well (though they probably cannot detect this automatically).
&gt; Sorta, but I think your definition of cycle violates the monad law, m a &gt;&gt;= (a -&gt; b) = m b. Thats not a monad law(plus it looks a like functor not a monad). A monad by definition is `m a &gt;&gt;= (a -&gt; m b) = m b` &gt; So, if r is a vector with the contents {1}, then one would expect cycle(r, 3) to return {1, 1, 1}. But let's say that range(n) returns a vector, {0, 1, ... n}, then for_(range(n), ...) must return a vector, cycle(r, 3) = {{1}, {1}, {1}}, and the lambda should return std::vector&lt;Range&gt;{r}. No, thats not how it works. We don't need to write `std::vector&lt;Range&gt;{r}`, `r` is a range already, so it will "flatten" that range, and so you will have `{ 1, 1, 1 }`. &gt; So, cycle() should return a random access range, but your issue is that "for_" implicitly implies sequential access since, for sequences, it applies the given function member-wise and in order? If we are writing this lazily, we lose the ability to do random access effeciently(even if both ranges are random access), because we can't know the length of each of the subranges. It has nothing to do with order. We can perserve the forward and bidirectional traversal of ranges. 
Static class templates are super powerful abstractions for resource-constrained devices. I implemented it for my ATTiny85 and the assembler output was identical to the standard C-style method, but it *looked* object-oriented and was very expressive. The problem I have though is that for composition/inheritance you need class templates all the way down. Your SPI-bitbanging class needs to be a class template so it can inline your awesome new GPIO templates. Things get verbose, muddy, and almost obfuscated. You *can* use normal objects if you're smart about it. Compare the [Cosa](https://github.com/mikaelpatel/Cosa) project, which is your standard OOP setup for AVR microcontrollers, to [stm32plus](https://github.com/andysworkshop/stm32plus) which heavily leverages static class temples. IMO the former is incredibly more readable, comprehensible, and maintainable... but still performs well and is easy on resources.
&gt; hats not a monad law(plus it looks a like functor not a monad). My mistake. &gt; No, thats not how it works. We don't need to write std::vector&lt;Range&gt;{r}, r is a range already... But didn't you say it should work on tuples and other monads as well? That's the part that really confuses me. Otherwise, using a function like mapConcat would be more explicit. &gt; If we are writing this lazily, we lose the ability to do random access effeciently [...] because we can't know the length of each of the subranges. Ah, now I understand. We don't know the lengths of the subranges because they haven't been computed yet, right? (We could say it's just r.size(), but it might be a bit harder in a more complicated example.) And iterating through the range lazily constructs it? This is why I've taken a liking to Niebler's ranges library and the piping syntax. We can compute lazily and efficiently using iterator/range adaptors. Sure, we lose out on generality and terseness, but I could write a cycle iterator whose index operator is just "(i + n) % size" much more easily and efficiently than a lazy for_. Right now, I'm imagining for_ (rng, f) as returning something like many transform adaptors with pointers to rng, wrapped in a concat adaptor. Even just the increment operator of the concat range would be significantly more expensive than a normal iterator since it'd have to check if it's at the end of the current range. (I haven't yet, but I planned to implement a concatenated ranges adaptor for the project.) Though, I get the idea that I might be totally off compared to how you would implement for_.
There are two parts of libraries: linking them (at the end of compilation) and loading them (at runtime). Linker docs: https://sourceware.org/binutils/docs/ld/Options.html Loader docs: http://man7.org/linux/man-pages/man8/ld.so.8.html But first, you really should say: CPPFLAGS += `pkg-config --cflags-only-I sfml-all` CFLAGS += `pkg-config --cflags-only-other sfml-all` CXXFLAGS += `pkg-config --cflags-only-other sfml-all` LDFLAGS += `pkg-config --libs-only-L --libs-only-other sfml-all` LDLIBS += `pkg-config --libs-only-l sfml-all` (or else replace sfml-all with the specific list of packages you actually use). While specifying the list of `-l` by hand will usually *work* for most libraries on most systems, `pkg-config` takes care of a lot of things you probably *wouldn't* think of until everything broke, such as dependency ordering, installation on nondefault paths or with nondefault settings. And besides it's simpler. An executable is linked like: `${CC or CXX} ${LDFLAGS} $^ ${LDLIBS} -o $@` A library is linked like: `${CC or CXX} -shared -Wl,-soname=$@.${SHORT_NUMBER} ${LDFLAGS} $^ ${LDLIBS} -o $@.${SHORT_NUMBER}.${OTHER_NUMBERS} &amp;&amp; ln -s $@.${SHORT_NUMBER}.${OTHER_NUMBERS} $@.${SHORT_NUMBER} &amp;&amp; ln -s $@.${SHORT_NUMBER} $@`. In the above, it is significant that LDFLAGS is before the list of object files and LDLIBS is after it. ***** Suppose you are writing a `foo` library. You call `gcc -shared -Wl,-soname=libfoo.so.1 foo.o -o libfoo.so.1.0.2 &amp;&amp; ln -s libfoo.so.1.0.2 libfoo.so.1 &amp;&amp; ln -s libfoo.so.1 libfoo.so`. The SONAME, `libfoo.so.1`, is a promise that any future versions of libfoo with the same SONAME can be dropped in place and your program will still load and run; the other numbers are just so you can tell which revision of that SONAME you are using. If you want to use the `foo` library, you call `gcc bar.o -lfoo -o bar`. Gcc forwards the options to `ld`, and ld searches the library path (first any `-L` flags, and then the default directories, but taking to account the sysroot) for the first file named `libfoo.so` or `libfoo.a`. It finds `libfoo.so`, which is a symlink to `libfoo.so.1`, which is a symlink to `libfoo.so.1.0.2`, but ld doesn't care about that, it only cares about the contents of the files. `ld` uses exported symbols (if you are making a library, *please* make sure you pass `-fvisibility=hidden` and only export the symbols you actually use!) to satisfy missing symbols from earlier object files, and adds the SONAME (i.e. `libfoo.so.1`) to the list of dependencies for the executable it emits. Significantly, note that the filename it adds is neither the filename it found in the library path (which is just `libfoo.so`) nor the actual name of the file (which is `libfoo.so.1.0.2`). Also, note that *where* in the library path the library was found is not recorded, so `-L` is forgotten by this point. At runtime, `ld.so` will read the executable and search *its* path (which is different than `ld`'s path and can only be permanently modified by `-rpath`, which is kind of nasty) for the SONAME. It finds the `libfoo.so.1` symlink and loads the file it points to (note: even if `libfoo.so` now points to a newer-but-incompatible `libfoo.so.2`) before calling your global constructors and `main` function. The hard-coded default library path is historically `/lib:/usr/lib` on x86 systems and `/lib64:/usr/lib64` on x86_64 (so that unmodified 32-bit applications can use hard-coded paths to `/lib` or `/usr/lib`), but many distros instead use `/lib32:/usr/lib32` for 32-bit applications on 64-bit systems and sometimes use `/lib:/usr/lib` for native 64-bit applications. However, usually the actual paths are read from `/etc/ld.so.conf` instead, which add things such as `/usr/local/lib` and `/usr/lib/x86_64-linux-gnu`. Also, note that while searching the path, `ld.so` will actually look in subdirectories depending on the runtime set of CPU features. ***** TL;DR linkers do a lot of work behind the scenes, and this post is only a poor approximation (TL;DW).
You can't do this atomically, since the OS only provides single-file-rename. In order to get the *effect* you want, you need to build your own transaction/rollback scheme, which requires everyone who *accesses* the files to have enough information to detect an incomplete transaction and roll it back, and also be sure to lock so that others don't race.
[It also might get removed in the future](https://msdn.microsoft.com/en-us/library/windows/desktop/hh802690%28v=vs.85%29.aspx). I don't know why MS didn't push TxF harder. It seems like it should have been a pretty compelling feature to get people to use NT on servers.
&gt; But didn't you say it should work on tuples and other monads as well? Yes it will. In the case of a sequence(ie tuple), we would need to pass in `n` as an integral constant. So then `range(n)` would be a sequence instead of a range, and `r` would be a sequence as well. I will just use the sequence monad instead of the range monad. &gt; Ah, now I understand. We don't know the lengths of the subranges because they haven't been computed yet, right? Well not only that but the lengths could change each time. &gt; We could say it's just r.size(), but it might be a bit harder in a more complicated example Well thats what I mean by modifing the interface so we could feed it these "hints". Perhaps, instead of writing `for_(range(n), [&amp;](auto) { return r; }` we could write `for_always(range(n), r)`, since we will always return `r` no matter what, and then we could use that to optimize it for random access. &gt; Right now, I'm imagining for_ (rng, f) as returning something like many transform adaptors with pointers to rng, wrapped in a concat adaptor. If I understand what you mean by `concat` then `for_` is just `r | transform(f) | concat`. &gt; Even just the increment operator of the concat range would be significantly more expensive than a normal iterator since it'd have to check if it's at the end of the current range. Well you will need to check if its at the end of the inner range everytime you increment, but if you use a `goto` you will only need to check the outer range when the inner range ends, like this: void operator++() { goto resume; for(;this-&gt;iterator!=this-&gt;last;++this-&gt;iterator) { for(this-&gt;inner_it=begin(*this-&gt;iterator);this-&gt;inner_it!=end(*this-&gt;iterator);++this-&gt;inner_it) { return; resume:; } } } A lot of non-random access iterators already require extra checks during incrementing, so its not a huge deal. However, for random access though it is a huge deal, and so that is where the optimizations need to be.
No, but I thought it almost always was for the last 40 years or so. Apparently on one popular embedded system it is not.
&gt; O(m) where m is the number of ranges zipped together, not O(1). For a given zipped sequence of `m` ranges, `m` is independent of _the length of the sequence_ `N` and `m` doesn't change (it is a constant). So as `N` grows, `m` remains constant, and sort still is `O(NlogN)`. The constant factors infront of that `O(NlogN)` term for a zipped sequence might be larger than those for a non-zipped sequence, but from a Big-Oh point of view, that is irrelevant.
&gt; allow **some form** of operator overloading. Emphasis mine. &gt; The &lt;…&gt; infix case is in no way special. Exactly, and that's why this solution doesn't offer anything substantial. 
IANAL, claiming "this work is public domain" is invalid in many countries, since the creator cannot sign his rights away. Not sure if this makes the whole license invalid.
In fact, the design pattern you propose would be closer to a continuation-like monad if the lib allowed for algorithm chaining with deferred execution: auto v=move()|sort([](...){...})|...; std::cout&lt;&lt;v({1,2,3}); which would enable interesting (or fun, at least) applications.
This has nothing to do with a library and everything to do with whether the underlying system (filesystem, really) supports it. There is no generic solution nor a library. For transactional work, you want transactional software: databases, queueing products, that sort of thing.
What you said is the wrong answer. You can't just say "this variable is fixed for any particular call &amp; thus is a variable" &amp; is thus a constant on my big-O. The way to convince yourself of that, is replace `N` with `m` above in what you wrote &amp; it still makes sense as a statement but is obviously wrong. The standard won't need any changes - the complexity is actually fine for std::sort. The reason is that `O(n log n)` defines the big-O on comparisons &amp; swaps. There is no big-O for std::sort itself as such a thing would be impossible to define since we don't have any guarantees on the big-O of &lt;.
&gt; You can't just say "this variable is fixed for any particular call &amp; thus is a variable" &amp; is thus a constant on my big-O. Actually, you have to remember that f(n) = O(g(n)) is the upper bound of f as n approaches infinity, and that f(n) &lt;= g(n) * C, where C is some constant. The number of containers being sorted isn't what's approaching infinity, it's a constant to the instantiation of the algorithm. &gt; There is no big-O for std::sort itself as such a thing would be impossible to define since we don't have any guarantees on the big-O of &lt;. Section 25.4.1.1, paragraph three: Complexity: O(N log(N)) (where N == last - first) comparisons.
I bet you have tought about "keywords" from the second part of article: virtadd, virtrep, .. As far as I see, all negative comments are about them. Yes of course this looks to be overhead for developer which type faster than thinks. But we have long-term project, 20 years old. And it have very complex areas, where you seat days and thinking, reading. Exactly working in this place - this have come to mind... And was played a little. Strange that people mostly ignore keywords from the first part of article. So far, it seems nobody was very against of virtnew, virtovr, virtimp. And btw, these keywords most probably will not be changed during future refactoring. 
Another point -- if to think at least 5 minutes about: * if IDE can support this? Then I think most will agree: * yes IDE can easy enough support this. Another good question is: * if this keywords help to read code? If you think that not, then you still do not see idea deeply. 
I found a great discussion concerning this. You're right that this license does have issues and supposedly the second paragraph exists to clarify your rights to use the source even if the public domain part is invalidated. http://programmers.stackexchange.com/questions/147111/what-is-wrong-with-the-unlicense
These are the kind of posts that make me think C++ is getting too complicated and a lot of things people end up doing/wanting to do in it uses more time to learn it/use it than would have taken by using orthodox methods.
Perhaps you would prefer a nice, OO framework where everything inherits from `Object`. ;-)
I feel dirty having visited msdn
&gt; Actually, you have to remember that f(n) = O(g(n)) is the upper bound of f as n approaches infinity, and that f(n) &lt;= g(n) * C, where C is some constant. The number of containers being sorted isn't what's approaching infinity, it's a constant to the instantiation of the algorithm. That's only true for single-variable functions. Consider searching for a string of length `m` in a text of length `n`. The complexity of such a function could be O(mn) (or O(m + N) if you use a smarter algorithm) even though for any particular instantiation of the algorithm your search string length is fixed. &gt;&gt; There is no big-O for std::sort itself as such a thing would be impossible to define since we don't have any guarantees on the big-O of &lt;. &gt; Section 25.4.1.1, paragraph three: &gt; &gt; Complexity: O(N log(N)) (where N == last - first) comparisons. You're supporting what I'm saying. sort complexity is defined in terms of the number of comparisons &amp; swaps, not the complexity of std::sort itself. For example, imagine a comparison implementation that itself was `O(n)` for some reason. Then the complexity of a particular call to sort with that comparison would be `O(n^2 log(n))`
No, not really. I would prefer a more complete STL which wouldn't need me to learn new libraries on every project that I enter.
I feel tricked into reading an article about how MSVC does optimization, even though I only have access to clang and gcc (linux user). I would really appreciate if someone did a comparative analysis of how well gcc, clang, and msvc performed specific optimizations.
Me too! That's what I'm working towards, in case that wasn't clear.
&gt; cache locality This was my thought exactly, but adding to that, one version may benefit more from loop unrolling or vectorization than the other, depending on the operation, which may negate even cache locality. The number of iterations may be a big factor, too, in that iterating to 100,000 has different performance characteristics than 100. Though, I do think that the author made one point very, very clear: the two loops are not the same and if one writes an embedded loop, but notices the inner and outer loop can be swapped, it's worth investigating which ordering is more efficient. I think that's more what should be taken away, than anything else.
But prior to the stripping off of the reference qualifier, expressions can have reference type. Ergo expressions can have reference type.
Because most people aren't language lawyers and there's a big difference between an explanation intended for people who like to memorize every detail of the C++ language specification, and people who just want to use C++ to develop actual functional applications. If you intend to develop actual functional applications then it will always be the case that expressions and values never have a reference type. It's much simpler to reason about C++ code in this way, and it provides more explanatory power to think of expressions and values as being composed of a type and a category where the type can never be a reference type. Scott Meyer's isn't unfamiliar with this concept either, having coined the term universal references (now commonly known as forwarding references). In his explanation of universal references he didn't stick to the strict C++ standard technicalities and instead focused more on utility and explanatory power, even when it contradicted sections of the standard. Well in the case of whether expressions can be of reference type, it actually makes even more sense to forgo strict C++ standard legalese in favor of utility and explanatory power, in which case it's best to think of values and expressions as never having reference type. Even the C++ standard itself states that any expression with reference type has that reference type stripped off. Scott Meyer's argument seems to strictly be "Why would the standard say that if the expression didn't, at some point in time, have a reference type?" and the sensible answer is that the standard is written in an informal fashion, often contains ambiguities or just basic language issues, and that for all intent and purposes expressions do not have reference types.
Even though I mostly do my coding in Linux, I found the article to be well written and to contain useful information. All the concepts explained in the article exist in the other compilers as well, so you will not waste your time reading it. At times after reading an article like this one I'll try to remind myself how somebody somewhere more knowledgeable on some issue than me took time to write an article about it for people like me to read, and all for free. I feel privileged.
Could we please stop using those click-bait titles? The things that *every* programmer should know about compiler-optimizations can be written down in a few lines: * They exist and allow to write somewhat suboptimal code without performance-penalty if the better way is totally obvious. * They result in compiled code being often faster than handwritten assembly. * Under normal circumstances, they do not change the observable side-effects of a program. Maybe there are one or two additional points, but really, ***I*** have no need at all, to know about VC-specific pragmas to control their optimization. I admit, I didn't really read the article yet, but I scrolled over it enough to see that it is not what it claims (though it may very well be very interessting).
Because it sounds an interesting project, but video is the absolute worst way to communicate: [Presentation slides](https://meetingcpp.com/tl_files/2014/talks/sqlpp11-meeting-cpp-2014.pdf) as PDF [The docs](https://github.com/rbock/sqlpp11/wiki) [The code](https://github.com/rbock) It seems to support sqlite and mysql right now. 
Seems that it has been fixed in 1.57. Basically there is a proxy (`index_pair`) that acts like `pair&lt;T&amp;,U&amp;&gt;`, and an overload of `iter_swap` that is picked by `std::sort`. Apparently this worked for several years, (ublas was accepted into boost in late 2002), until gcc 4.8 decided to remove `iter_swap`from the STL (at least in C++11), and the code broke. The current fix adds a `swap` overload that takes the proxies by value (and it is probably still wrong). 
Talking about premature optimization you should remember that i++ creates a temporary, while ++i does not. So lets just not do premature optimization, but put our time into learning how to use a profiler. this saved me so much time, much more than those for loops will ever use!
&gt; Why not? The code clearly demonstrates that we don’t need to add a language feature, a library is enough. Is it? What's it like to step through this stuff when debugging?
Does it support any version of Visual Studio?
Well, regarding performance you might be right. I have not performed any profiling. Anyways, there's also the matter of system setup complexity. E.g. if I have a separate database service, it's needs to be installed, configured and run before use. This is not so nice compared to embedded solution. But that's a different measure anyways.
wow - thats actually pretty simple solution to insure immutable package uris and its easy to implement :) thanks for that. I also thought about how to make it very easy to include packages and I have chosen to replace cmake's `find_package()` function which will continue to work the default way but if you enclose your arguments in single quotes it will pass it along to to cmakepp. idea: find_package(Hg) # still works the default way # set vars like HG_FOUND find_package('bitbucket:eigen/eigen') # forwarded to cmakepp # returns unique unchanging package handle for project # and will adhere to the find_package interface by setting # EIGEN_FOUND EIGEN_VERSION,.... etc # packages are downloaded to ${CMAKEPP_DEPENDENCY_DIR } # CMAKEPP_DEPENDENCY_DIR will default to # ${CMAKE_BINARY_DIR}/dependencies # 
The search and retrieval function is one in of 3 steps I plan to take. The first step just finds information/meta-information (e.g. source files/ binaries/ descriptions) and retrieves it locally to any place you want. The second step is to do this recursively (ie create a dependency graph and get all missing dependencies) The third step is what you asked about. So to answer your question - no it does not support IMPORTED targets. But writing handlers for imported targets should be easy (at least at first glance) thank you for your reply - you gave me an idea how to continue:)
I totally agree, sorry for that. It's just that the "share in Reddit" button is so easy to press...
How does this compare to the built in module [ExternalProject](http://www.cmake.org/cmake/help/v3.1/module/ExternalProject.html)? 
I have used embedded databases in the past (in the very long past, the only thing we had was that ;-)). Yes, it is tempting WRT deployment etc. Not so long ago, used BerkeleyDB. You might want to consider it, because if you're considering speed, then you might like the fact that BerkeleyDB has no SQL layer on top of the DB engine, which might/should help.
Honestly, I believe the speed is not so important. At least not now :) I'm about to develop a c++ library which is going to save some data to 3 separate files: text, xml and json. And all i need is transaction mechanism. But I did not want my library users to depend on external database service. Which would complicate the library setup and configuration. I guess these are y main requirements: easy to use. The performance issues just popped into my mind. Maybe prematurelly :) 
I think you're underestimating how valuable iterators that can return proxy references could be. Suddenly, the STL algorithms—and any code you've written to work through similar interfaces—can operate on arbitrary sources and sinks of data without ugly hacks. It could potentially make the STL much more broadly applicable, and reduce the need to rewrite a buggy binary search for the nth time.
Emphasis on 'potentially'. There are a lot of potentials things people want to do in C++. That's the thing. Like I said, I'd be fine if such things were proven to be worthy and actually added to the STL.
I like that you imply I am a programmer who does not think about what I am typing and don't work on projects worth keeping alive for more than brief periods. The project I've been working on for the last 6 years started development over 25 years ago and approaches 1.2 million lines of code. I can assure you I've considered your proposal and don't think the added expressiveness (read: complexity) outweighs my maintenance concerns. Furthermore, what you seem to be missing here is that there are better ways of not only expressing these things (eg, the virtadd/virtrep/virtins/virtwrp), but also *enforcing* them by design. See Herb Sutter's "[Virtuality](http://www.gotw.ca/publications/mill18.htm)" as one good example.
Implying someone has not fully considered your proposal because they disagree with it is insulting.
Although a cute example, you fail to make a point because classes give true value when coding with RAII, which simplifies code; whereas your proposed keywords only add complexity and unnecessarily constrain your implementation. Classes add complexity in some areas to reduce it in other areas. These keywords do nothing but add additional expression to a broken implementation.
This seems really neat. But using Python's "`{}`" syntax instead of the "`%`", used by both `printf` and Boost.Format, would make converting an existing code-base very painful. 
Pretty much impossible to see what's on the slides ...
Hmm. this may be worth exploring. I know we sometimes have to generate text files with millions of lines in them.
Very nice. Been looking for a self contained library. I salute you on your commitment to pre C++11 compilers by emulating variadic templates (a very painful, tedious task). 
How am I supposed to leave a suggestion, if I have to login to your website with a non-existing Wordpress account first to write a comment :-/ Anyways, I just learned about https://www.appveyor.com/ which is basically travis for windows, if I understood correctly. CMake is imho the best build system for c++ currently, but you may have a look at Gradle. Initially just targeting java projects (and Android for a little while now), it seems they're aiming to create a universal build system for just about everything (including c++)
Follow the link to youtube, or download the slides directly: http://meetingcpp.com/index.php/tv14/items/5.html
Thank you very much for your suggestions! You have good points, I'll see how I'll go about them. Regarding namespace name, I've always gone the road of using longer ones (or choose whatever appropriate), and then when someone wants an abbreviation for it, he/she can go and do (or should do) `namespace svd = superviseddescent;`(instead of `using namespace superviseddescent;`). I do the same for boost, `fs` for filesystem, etc. I think two to four character namespaces are not really unique enough. E.g. the term `svd` has a meaning in math that could result in confusion. I guess this is as you already pointed out really a matter of opinion :-)
ExternalProject can handle projects with different build systems, just set the CONFIGURE_COMMAND, BUILD_COMMAND and INSTALL_COMMAND. But the more fine graded control of your solution looks nice.
mostly iostreams and then an sprintf() utility that can be used to set up an iostream which returns a std::string. I'm pretty sure it's not so performant. I'm guessing that "posix.h" can be left out if only using '{}' syntax?
C++ Format is 10-30 times faster than Boost Format on Karma's integer formatting benchmark and generates almost 30 times smaller code on Tinyformat's code bloat benchmarks. In terms of functionality the libraries are probably similar although the format string syntax is different.
I'm not sure what Wordpress website you are talking about, but thanks for the suggestion. Note that the core of the C++ Format consists of only one header file and one source file and it is easy to include it in a project that uses any build system, not necessarily CMake. As of version 1.0, header-only configuration is supported too.
Yes, "posix.h" is an experimental thing that is not really part of the official API.
Unoriginal clickbait. Also, what does this have to do with C++?
`std::rand` (and thus `std::random_shuffle`) is not guaranteed to produce a repeatable sequence between runs even given a constant seed.
Does anyone know why the `std::random_shuffle` can't be implemented in terms of `random_device`? A simple: You're not allowed to use `std::rand()` in the implementation of `std::random_shuffle` anymore (kind of like disallowing CoW for `std::string`). I feel like it achieves the same goal.
Great! What about fixing vector&lt;bool&gt; in C++17 to actually be a vector&lt;bool&gt; &amp; providing a boost::dynamic_bitset that's a drop-in replacement instead for those people that actually need a bitset. http://www.cs.up.ac.za/cs/vpieterse/pub/PieterseEtAl_SAICSIT2010.pdf
Thank you to whoever made the effort to provide a transcript! Some things work well as videos but sometimes it's nice to be able to scan visually.
&gt; mt19937 is such a pseudo RNG, which needs to be initialized with a random value (aka 4). [Nice reference.](https://xkcd.com/221/)
Ugh, a short and hastily scrawled together "article" with terrible grammar and a lot of ads to click on. There's only 5 "advantages" listed.
SORRY but programmers don't care about warning and grammars. just kidding Ozwaldo we try our best to improve . thanks 
Excuse me if I ask something that has no sense: would it be possible to use LLVM's or other compiler's compilation intermediate code (that internal code compiler extracts from actual C++ code, for example, and it uses to perform its optimizations and to generate machine code) as libraries and then, when the program which uses that library gets linked, create the final code from it? Is it not Android doing (or about to do) this for its apps? This would allow C++ libraries to be easily shareable from my understanding.
Judge me by my C++, not my WordPress SO GOOD AND cool like your programming style :) 
What? Why would you do that? How can that happen? If you have an instantiated offset why would this be null? I'm very confused by the premise of this article. Who does that?
Good question. The `fmt::print` and `fmt::printf` functions write to `FILE` once formatting is compete in a similar way that `std::printf` does. So the output should appear in the expected order.
The fun is when you are using callbacks that capture "this" and the referenced object goes out of scope. The solution is something like boost enabled_shared_from_this and passing a weak pointer to ones self. Then if the weak pointer is valid still you can lock it and get the shared pointer. So moral of the story is, don't capture this for an async callback, always copy
I don't understand why this would ever be a good idea. What if this is just a stack allocated object? Even if it is a heap location, it would cause a double free down the road wouldn't it?
Well, I know it's a bad idea, but in some cases one may ressort to bad ideas if the 'proper' way requires writing 100+ lines of code.
&gt; I don't understand why this would ever be a good idea. How else do you delete an (intrusive) reference counted object? See, this, for example: http://www.aristeia.com/BookErrata/M29Source.html 
I love this library so much. You are awesome.
Fair point... I guess a better option would be to say: require `random_shuffle` to use a *strong* PRNG. For example, the replacement suggestion that people point to (seed a Mersenne Twister with a value from `random_device`). My point is that the *interface* of `random_shuffle` isn't broken, only the implications. Requiring certain implementation behavior is not outside of the C++ Standard, so I don't understand why that can't be done here.
Thanks! V8 has already complicated build, it was enough for me :) Actually, v8pp uses template metaprogramming inside, so it has to be header-only. 
Thank you =)
Beware of your implementation of `BasicStringRef`: `c_str` generally assumes a NUL-terminated string of characters, however the constructor `BasicStringRef(const Char *s, std::size_t size)` makes no such assumption.
Looks more like a workaround to some ugly bug.
Prior to C++11: auto v = vector&lt;bool&gt;{true, false}; vector&lt;bool&gt;::reference ref = *begin(v); ref = false; // clear intent assert(v == vector&lt;bool&gt;{false, false}); After C++11: auto v = vector&lt;bool&gt;{true, false}; auto ref = *begin(v); ref = false; // on purpose or error? assert(v == vector&lt;bool&gt;{false, false}); Did the author of the post C++11 code intended to take a reference but knows that vector&lt;bool&gt; returns a proxy and thus it is ok to take it by value, or did he wanted to take a value and the code is broken? The `vector&lt;bool&gt;` issues are old (prior to C++03), Boost.DynamicBitset is very old (2001, prior to C++03). If the fix didn't landed in C++03 and C++11, I doubt that it will land in C++17, 6 years after C++11 introduced the possibility that such a fix would break valid code. Chances of an STL v2 are IMO much higher than that of `vector&lt;bool&gt;` being fixed at any time in the future. It is too late now.
It would break code, see my answer here: http://www.reddit.com/r/cpp/comments/2ux1xa/stdrandom_shuffle_is_deprecated_in_c14/coddcon Having read Eric posts, I don't think that his point is that "standard library algorithms do not have good support for proxies", since in fact they do! Sorting a zipped range and using a random access transform iterator works just fine with standard library algorithms, so _the algorithms do support proxies_. What doesn't support proxies are the standard iterator concepts. For this reason, these _non-conforming_ iterators have to lie (just a little bit) to the STL. With concepts, however, lying won't be possible. And, in my humble opinion, his point is that unless we fix the iterator concepts, useful (non-conforming) iterators _that work fine today_ won't work with a concept-based standard library in C++17.
How do you deal with templates? :)
That's right. C++17 is proposing the existing syntax `= {...}` as the exclusive way to signify an initializer list. You may use that convention right now to reduce future headaches. 
It would break code. Whats worse, it would break the code in a way that makes it difficult to notice - everything still compiles fine, but suddenly you are a lot less space efficient, which you may or may not notice, in that place. These are _terrible_ regressions to do. Making it not compile (somehow) would be much better.
I can't argue for the `RandomFunc` accepting form of `random_shuffle` because it doesn't make a lot of sense (and I agree that there is *no* reason to keep it), so I'll stick to the 2-argument version. To answer your questions, the only thing the C++14 standard says about the PRNG used for `std::random_shuffle` is §25.3.12/4: &gt; The underlying source of random numbers for the first form of the function is implementation-defined. &gt; An implementation may use the `rand` function from the standard C library. An implementation is already free to use an MT, LC or whatever source of "random" data it wants to. The fact that some implementations use `std::rand()` is an unfortunate issue of those implementations, but this is fixable *without* a change to the interface -- just disallow using `std::rand()`as a source of randomness. I suppose I'm looking at the implications in the opposite direction. I have the following code that stops working in C++17: std::random_shuffle(vec.begin(), vec.end()); This seems like a major problem to me when there is a simple workaround for making the code work: template &lt;typename RandomAccessIterator&gt; void random_shuffle(RandomAccessIterator first, RandomAccessIterator last) { // Choose whatever PRNG you want here minstd_rand rng(random_device()()); shuffle(first, last, rng); } I don't think `std::random_shuffle(RandomAccessIterator, RandomAccessIterator)` has a broken interface (as opposed to something like `std::auto_ptr&lt;T&gt;`). The C++ Standard didn't deprecate `std::string` in favor of something like an `std::unique_string`, it simply had the wording changed to disallow a reference-counted `std::string`. If a function `std::allocate_unique&lt;T&gt;` is created where you specify the allocator to use instead of relying on implementation-defined allocation, should the C++ Standard deprecate `std::make_unique&lt;T&gt;`? Of course not! This might be a case of: I understand your argument, but disagree with the conclusion. To rephrase Hariharan Subramanian's quote from the [Discouraging `rand()` in C++14, v2](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3924.pdf) paper for my own purposes: A feature should only be deprecated if there is no point in using it. I certainly see a point in continuing to use `std::random_shuffle` if the wording was changed to require a better PRNG (explicitly disallow the use of `std::rand()`).
&gt; For some classes, this is safe: What about classes that have `const` members?
Also see: http://www.engadget.com/2010/12/29/hackers-obtain-ps3-private-cryptography-key-due-to-epic-programm/
I use this strategy all the time when using binds and lamdas I really wish there was a more elegant way to do this. Perhaps a capture mode that automatically takes a weak pointer to the captured object and doesn't make the call back if the weak pointer expired. 
I've seen `~basic_string` show up in profiles before at maybe ~2% in string-heavy programs. The new `std::string` destructor doesn't have to do the atomic refcount stuff so these programs may get about that much faster. But I've always seen bigger improvements by using `string_ref` in the right places. Apparently Chrome creates *tonnes* of `std::string` instances. Maybe it'll see a benefit.
I have seen realistic scenarios where `std::string` copying and destruction was ~90% of runtime with msvc and clang, but ~10% with gcc, so I could plausibly see it causing significant performance regressions in software that mostly just passes strings around and is only ever compiled with gcc. You probably won't see any meaningful changes to programs which target multiple compilers simply because anywhere that there's a difference worth caring about between SSO and CoW would have been too slow with one of them and thus redesigned to eliminate the difference.
The other fun was fully learning that the lifetime of the shared_ptr is that of the lambda the hard way along with every copy of the lambda increases the ref count. I have some code that wraps the if expired stuff that I use. I had to create a bastard wrapper class for the shared_ptr though that only moves and never copies so that the lifetime really ends at the end of the callback. Another bonus is that at least shared_ptr implicitly converts to weak_ptr so you can just pass it to a method that takes weak_ptr
This looks awesome! I hope you continue working on it! I've been interested in learning how to bind classes into a module for Node.js and this could really help! I'd love to see more examples of that if you have them.
Thank you! I'm going to add a wiki page with examples from Node.js addons documentation but adapted to v8pp.
This is pretty cool. Seems like it's mostly for dynamically allocated objects that manage themselves (independently of the program) once created.
Point taken about Linus :-) although my understanding is that Dirk has taken over much of the development now . I reference the Qt docs all the time and I've made a couple moderate (~ 5-10K lines) apps professionally. I'd say I'm roughly an beginner-intermediate C++/Qt developer (use shared pointers although not with gui elements since qt has its own thing, use boost pretty regularly, getting familiar with practicing scott meyers books). On the other hand jumping into a codebase like KDE might be a bit unwieldy to jump right into. I'd prefer a smaller project that I can wrap my head around.
How is this IDE compared to QtCreator, Code::Blocks or Eclipse?
I'd like to know the reasoning behind the required check when it's noexcept. Why it must exist in the presence of noexcept?
I suspect that is due to noexcept new returning a nullptr on an allocation failure, in which case the constructor should not be called.
Indeed, but for placement new it doesn't make sense =/
I think the STD requires that `new (nullptr) T();` returns nullptr without invoking a constructor. int *p = new (std::nothrow) int[10]; if(p) { for(int i = 0; i &lt; 10; ++i) new (p+i) int(i); // checks if p+i==nullptr ... 10 times // do something with p ... delete [] p; } I simply can't see how such behavior can be useful to anyone, but maybe I am blind. After all it is in the standard.
I believe that is 5.3.4.15 (N4296) &gt;... if the allocation function returns null, initialization shall not be done, the deallocation function shall not be called, and the value of the new-expression shall be null.
yeah sorry for that. Check now
What's your end goal? If you're trying to better understand how to properly use Qt's MVC abstraction, I'd recommend the docs and [this blog post.](http://qt-project.org/doc/qt-4.8/model-view-programming.html) Note that while the URL is similar, it's a different one than has already been linked here. It has a very important takeaway in the best-practices comment at the end. Specifically: &gt; In summary: the take home lesson is that you should look at a QAbstractItemModel as an adaptor to make an already existing data store displayable in an item view. The cases where the data is actually stored inside the model should be the exception, not the rule. From personal experience, I'd say the other big-and-important thing to understand is that the docs aren't kidding when they tell you which virtual functions to override for a particular set of model requirements. I can't find the post any more, but one of the Qt devs once commented on how the MVC abstraction is one of worst ones in the library (relatively). While powerful, it can be non-intuitive, especially when compared with how easy most of the other pieces of functionality are to use. I have seen a lot of confusion arise from the fact that there are many virtual functions that absolutely must be overridden to get correct behavior; unfortunately, they're not pure virtual (an unfortunate but understandable decision), and their default behaviors, e.g. returning false or -1, can leave one confused as to why a given model isn't behaving as expected. Follow the docs carefully in that regard and you should be okay.
A little less feature-rich, smaller in size, and starts a bit faster than most of those in my experience. It's not exactly pretty, either. 
I use it for a big "production software" and I am very satisfied. I started using it primarily for two reasons : - its integration with wxwidgets ( see wxcrafter ) - works well for linux (I recompiled the IDE from source for CentOS) With a small donation , wxcrafter also supports advanced controls , I paid with pleasure.
You don't have an operator- function that takes two Point objects as arguments
Its because you have no overload for `Point`.
This would be a great IDE, if it only could cope with whitespaces in file paths. Edit: T'was my mistake, see the subcomments.
No, that is an overload for Vector.
Qt 4.8, Really? (-_-')
How much better is "so much better" than std::vector?
I am looking for a nice MinGW distro for 32 and 64 bit compilation targets. I want to work with libraries like Boost., Basically I want as good of a C and C++ development I can set working on Windows without needed things like Visual Studio. 
Does it support CMakeLists as project files like QtCreator does?
Thanks Stephan. When you say it is x64-native, you mean the toolchain and not the disabling the ability to -m32 to generate 32bit binaries?
I just rewrote their FindQt4 script and put it in my project's cmake modules folder. "ifdefd" the changes out, so you had to set a variable before you called find_package to get the fixed behavior. With regards to boost, I didn't set anything up just yet, Qt manually set the dependencies so I just used that information to build my own "fixed" qt targets. For some reason I kept getting the properties overwritten with multiple calls to find_package(Qt4), so I just made my own IMPORTED targets. If you go through the hassle of setting up all your targets then you can just list out the dependencies with target_link_libraries for each target and it'll handle it. If you want to walk the dependency tree I remember the code was pretty simple, I just wrote this in sublime, didn't test it as I don't actually have CMake on this machine right now. function DependencyTree_Impl( target ) message( "visiting ${target}" ) get_property( visited_nodes GLOBAL PROPERTY __visited_nodes ) set( visited_nodes ${visited_nodes} ${target} ) set_property( GLOBAL PROPERTY __visited_nodes ${visited_nodes} ) get_property( target_type TARGET ${target} PROPERTY TYPE ) set( libs_to_search ) get_property( link_libs TARGET ${target} PROPERTY INTERFACE_LINK_LIBRARIES ) set( libs_to_search ${libs_to_search} ${link_libs} ) if( TARGET ${target} AND NOT ${target_type} STREQUAL INTERFACE_TARGET ) get_property( link_libs TARGET ${target} PROPERTY LINK_LIBRARIES ) set( libs_to_search ${libs_to_search} ${link_libs} ) endif() foreach( lib ${libs_to_search} ) if( TARGET ${lib} ) list( FIND visited_nodes ${lib} find_index ) if( ${find_index} EQUAL -1 ) DependencyTree_Impl( ${lib} ) endif() endif() endforeach() endfunction() function DependencyTree( target ) if( TARGET ${target} ) set_property( GLOBAL PROPERTY __visited_nodes "" ) DependencyTree_Impl( ${target} ) endif() endfunction() Doing it with the secondary function function lets you do some other stuff like adding a recursion level so you can do "pretty printing" to help debug. You could also skip printing on nodes that aren't real targets (library/exe) and ignore interface targets. because it'll get messy. I also setup the __visited_nodes global property so you don't waste time traversing nodes you've already seen, this is just so you can find the first occurrence of the target you want to debug and figure out where it went wrong, instead of printing out the same target's tree multiple times. I'm more or less abusing the Find scripts to just allow find_package to pass me extra information. I don't actually fill in the variables they say are required (because I want people to use the targets, not ${FOO_LIBRARIES}. They're local to our project and everyone knows how it's setup so they don't expect those variables to be set. The FindConfig does sound like it's a more portable way to do it. Right now I have to write a Find script for every third-party library we bring in, and it'll only work in our project structure. Using helper functions and having a well-structured third party library folder it only takes 10 lines of code for our "Find" scripts. 
Please see my answer above. The point of that code was not to teach OpenGL, but the OP missed that.
Personally i use MSYS2. It comes with two sets of packages: cygwin tools so you can run/build linux apps in a posix environment and mingw-w64 packages for native windows development. The best of all is that they ported pacman from archlinux so you can use it to manage/update your msys2 installation. [MSYS2 Project page](http://sourceforge.net/projects/msys2/) [MSYS2 Installer](https://msys2.github.io/) You can see the packages that they support here: [MSYS2 package list](https://github.com/Alexpux/MSYS2-packages) [MinGW packages](https://github.com/Alexpux/MINGW-packages) As you can see they have lots of packages in their latest version like gcc, Qt, boost, ffmpeg, SDL, etc.
Does that not rely on cygwin1.dll though?
Yep.
Choosing the right MinGW distro is not an easy thing. Qt Project has a wiki page about this: [https://qt-project.org/wiki/MinGW-64-bit](https://qt-project.org/wiki/MinGW-64-bit) I usually get the [Qt MinGW offline package](http://www.qt.io/download-open-source/#). It comes with Qt Creator, MinGW-W64 and with a fully debugger integration. TDM-GCC and nuwen.net don't have python bindings for GDB and I can't use them in Qt Creator. Using GDB from command line is too much for me :) 
Nope, just the standard gcc shared libraries (unless you use -static, then it becomes independant of gcc). Example of a simple c++11 app linked aganist boost::thread. codestation@vm ~ $ cat test.cpp #include &lt;iostream&gt; #include &lt;boost/thread.hpp&gt; int main() { auto th = boost::thread([]{ std::cout &lt;&lt; "hello world" &lt;&lt; std::endl; }); th.join(); return 0; } codestation@vm ~ $ g++ -std=c++11 test.cpp -o test.exe -lboost_system-mt -lboost_thread-mt codestation@vm ~ $ ldd test.exe ntdll.dll =&gt; /c/Windows/SYSTEM32/ntdll.dll (0x7ffeaac50000) KERNEL32.DLL =&gt; /c/Windows/system32/KERNEL32.DLL (0x7ffeaa570000) KERNELBASE.dll =&gt; /c/Windows/system32/KERNELBASE.dll (0x7ffea7ec0000) libboost_system-mt.dll =&gt; /mingw64/bin/libboost_system-mt.dll (0x63400000) libboost_thread-mt.dll =&gt; /mingw64/bin/libboost_thread-mt.dll (0x697c0000) msvcrt.dll =&gt; /c/Windows/system32/msvcrt.dll (0x7ffeaaa70000) USER32.dll =&gt; /c/Windows/system32/USER32.dll (0x7ffea8640000) libgcc_s_seh-1.dll =&gt; /mingw64/bin/libgcc_s_seh-1.dll (0x61440000) libstdc++-6.dll =&gt; /mingw64/bin/libstdc++-6.dll (0x6fc40000) GDI32.dll =&gt; /c/Windows/system32/GDI32.dll (0x7ffeaa7e0000) libwinpthread-1.dll =&gt; /mingw64/bin/libwinpthread-1.dll (0x64940000) IMM32.DLL =&gt; /c/Windows/system32/IMM32.DLL (0x7ffeaa520000) MSCTF.dll =&gt; /c/Windows/system32/MSCTF.dll (0x7ffeaa3c0000) codestation@vm ~ $ g++ -std=c++11 test.cpp -o test.exe -static -lboost_system-mt -lboost_thread-mt codestation@vm ~ $ ldd test.exe ntdll.dll =&gt; /c/Windows/SYSTEM32/ntdll.dll (0x7ffeaac50000) KERNEL32.DLL =&gt; /c/Windows/system32/KERNEL32.DLL (0x7ffeaa570000) KERNELBASE.dll =&gt; /c/Windows/system32/KERNELBASE.dll (0x7ffea7ec0000) msvcrt.dll =&gt; /c/Windows/system32/msvcrt.dll (0x7ffeaaa70000) USER32.dll =&gt; /c/Windows/system32/USER32.dll (0x7ffea8640000) GDI32.dll =&gt; /c/Windows/system32/GDI32.dll (0x7ffeaa7e0000) IMM32.DLL =&gt; /c/Windows/system32/IMM32.DLL (0x7ffeaa520000) MSCTF.dll =&gt; /c/Windows/system32/MSCTF.dll (0x7ffeaa3c0000) codestation@vm ~ $ ./test.exe hello world
You mean the file qt-opensource-windows-x86-mingw491_opengl-5.4.0.exe ?
Yes
This should be everyone's default choice. MSYS2 is essentially a MinGW distro done *right*.
Thanks for posting the link so we can visit the original source instead.
What? I use it, no problem with witespaces. What do you mean?
The reason that Qt's mingw-w64 variant contains a GDB that has Python support is entirely down to the efforts of the core developers of MSYS2. MSYS2 is an offshoot of that work and is closely related to mingw-builds (which is exactly where the Qt-Project gets its toolchains from).
The difference is about 400%-500% on my machine if a lot of placement-new operations are involved. It sounds like much, but take it with a grain of salt, pretty much everything else the usual program does is way more time consuming.
Technically `p+i` could 'wrap around'. My guess is, that the compiler has to account for this. Therefore only the case `i==0` can be optimized away in the placement-new call, because of `if(p)`. In fact my GCC does the following. It unrolls the loop and checks `p+i` 9 (not 10 .. it was an oversight) times. EDIT: Dump of assembler code for function foo(): 0x00000000004007a0 &lt;+0&gt;: sub $0x8,%rsp 0x00000000004007a4 &lt;+4&gt;: mov 0x20084d(%rip),%rsi # 0x600ff8 0x00000000004007ab &lt;+11&gt;: mov $0x28,%edi 0x00000000004007b0 &lt;+16&gt;: callq 0x400650 &lt;_ZnamRKSt9nothrow_t@plt&gt; 0x00000000004007b5 &lt;+21&gt;: test %rax,%rax 0x00000000004007b8 &lt;+24&gt;: je 0x400848 &lt;foo()+168&gt; 0x00000000004007be &lt;+30&gt;: cmp $0xfffffffffffffffc,%rax 0x00000000004007c2 &lt;+34&gt;: movl $0x0,(%rax) 0x00000000004007c8 &lt;+40&gt;: je 0x4007d1 &lt;foo()+49&gt; 0x00000000004007ca &lt;+42&gt;: movl $0x1,0x4(%rax) 0x00000000004007d1 &lt;+49&gt;: cmp $0xfffffffffffffff8,%rax 0x00000000004007d5 &lt;+53&gt;: je 0x4007de &lt;foo()+62&gt; 0x00000000004007d7 &lt;+55&gt;: movl $0x2,0x8(%rax) 0x00000000004007de &lt;+62&gt;: cmp $0xfffffffffffffff4,%rax 0x00000000004007e2 &lt;+66&gt;: je 0x4007eb &lt;foo()+75&gt; 0x00000000004007e4 &lt;+68&gt;: movl $0x3,0xc(%rax) 0x00000000004007eb &lt;+75&gt;: cmp $0xfffffffffffffff0,%rax 0x00000000004007ef &lt;+79&gt;: je 0x4007f8 &lt;foo()+88&gt; 0x00000000004007f1 &lt;+81&gt;: movl $0x4,0x10(%rax) 0x00000000004007f8 &lt;+88&gt;: cmp $0xffffffffffffffec,%rax 0x00000000004007fc &lt;+92&gt;: je 0x400805 &lt;foo()+101&gt; 0x00000000004007fe &lt;+94&gt;: movl $0x5,0x14(%rax) 0x0000000000400805 &lt;+101&gt;: cmp $0xffffffffffffffe8,%rax 0x0000000000400809 &lt;+105&gt;: je 0x400812 &lt;foo()+114&gt; 0x000000000040080b &lt;+107&gt;: movl $0x6,0x18(%rax) 0x0000000000400812 &lt;+114&gt;: cmp $0xffffffffffffffe4,%rax 0x0000000000400816 &lt;+118&gt;: je 0x40081f &lt;foo()+127&gt; 0x0000000000400818 &lt;+120&gt;: movl $0x7,0x1c(%rax) 0x000000000040081f &lt;+127&gt;: cmp $0xffffffffffffffe0,%rax 0x0000000000400823 &lt;+131&gt;: je 0x40082c &lt;foo()+140&gt; 0x0000000000400825 &lt;+133&gt;: movl $0x8,0x20(%rax) 0x000000000040082c &lt;+140&gt;: cmp $0xffffffffffffffdc,%rax 0x0000000000400830 &lt;+144&gt;: je 0x400839 &lt;foo()+153&gt; 0x0000000000400832 &lt;+146&gt;: movl $0x9,0x24(%rax) 0x0000000000400839 &lt;+153&gt;: mov %rax,%rdi 0x000000000040083c &lt;+156&gt;: add $0x8,%rsp 0x0000000000400840 &lt;+160&gt;: jmpq 0x400660 &lt;_ZdaPv@plt&gt; 0x0000000000400845 &lt;+165&gt;: nopl (%rax) 0x0000000000400848 &lt;+168&gt;: add $0x8,%rsp 0x000000000040084c &lt;+172&gt;: retq End of assembler dump. If `p[i]=i` instead of `new(p+i) int(i)`. Dump of assembler code for function foo(): 0x00000000004007a0 &lt;+0&gt;: sub $0x8,%rsp 0x00000000004007a4 &lt;+4&gt;: mov 0x20084d(%rip),%rsi # 0x600ff8 0x00000000004007ab &lt;+11&gt;: mov $0x28,%edi 0x00000000004007b0 &lt;+16&gt;: callq 0x400650 &lt;_ZnamRKSt9nothrow_t@plt&gt; 0x00000000004007b5 &lt;+21&gt;: test %rax,%rax 0x00000000004007b8 &lt;+24&gt;: je 0x400810 &lt;foo()+112&gt; 0x00000000004007ba &lt;+26&gt;: movl $0x0,(%rax) 0x00000000004007c0 &lt;+32&gt;: movl $0x1,0x4(%rax) 0x00000000004007c7 &lt;+39&gt;: mov %rax,%rdi 0x00000000004007ca &lt;+42&gt;: movl $0x2,0x8(%rax) 0x00000000004007d1 &lt;+49&gt;: movl $0x3,0xc(%rax) 0x00000000004007d8 &lt;+56&gt;: movl $0x4,0x10(%rax) 0x00000000004007df &lt;+63&gt;: movl $0x5,0x14(%rax) 0x00000000004007e6 &lt;+70&gt;: movl $0x6,0x18(%rax) 0x00000000004007ed &lt;+77&gt;: movl $0x7,0x1c(%rax) 0x00000000004007f4 &lt;+84&gt;: movl $0x8,0x20(%rax) 0x00000000004007fb &lt;+91&gt;: movl $0x9,0x24(%rax) 0x0000000000400802 &lt;+98&gt;: add $0x8,%rsp 0x0000000000400806 &lt;+102&gt;: jmpq 0x400660 &lt;_ZdaPv@plt&gt; 0x000000000040080b &lt;+107&gt;: nopl 0x0(%rax,%rax,1) 0x0000000000400810 &lt;+112&gt;: add $0x8,%rsp 0x0000000000400814 &lt;+116&gt;: retq End of assembler dump.
I have my MinGW installation under C:\Program Files\MinGW. Codelite cuts the path into "C:\Program", chaos ensues. This is most likely just a windows problem tho, and using symlinks might fix it.
Well, for starters, the C++14 standard (the draft that you can download), is 1354 pages. You can download it here: https://isocpp.org/std/the-standard C11 on the other hard is 683 pages. (http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf)
Heh: "Please note that the standard is not intended to teach how to use C++. Rather, it is an international treaty – a formal, legal, and sometimes mind-numbingly detailed technical document intended primarily for people writing C++ compilers and standard library implementations. "
Looks nice, but any IDE that doesn't turn completely dark (ie. code window and surrounding windows) is an automatic pass for me. My eyes just can't handle light colored themes day after day.
Great work! V8 is changing so fast that other helper libraries (like cvv8, juice...) could not cope with the pace. I hope your libraries will be able to handle that :)
While the C++ language specification is bigger than others that does not mean that C++ programs need to be complex to write. C++ has a potential for complexity, but that those features exist does not mean you have to use them everywhere. sadly, people tend to overthink when using C++, making their programs more complex than necessary. Also, the people teaching C++ often either brag with their knowledge of complex details or teach 1990's C with classes because they never learned modern C++ which can help reducing the complexity. Both ways students get confused by a seemingly unavoidable complexity that is unnecessary. More on that see here: http://arne-mertz.de/why-simplify-cpp/
The c++ language spec is astoundingly detailed. While it is of course nice to describe your language in 130 pages, this is unlikely to get you a full specification of the language and core libraries. If C needs 500 pages, one can wonder how you can describe Python in 130.
Yes, MSYS2 is what you want.
Does the Java standard include its library then?
Thanks! Yes, we used v8-juice in one project, until Stephan Beal [said bye to V8](https://groups.google.com/forum/#!topic/v8-juice-js-dev/MUq5WrC2kcE). At that time we needed a new V8 version, so i had to create my own binding library :) We always been focused on recent V8 versions, that's why v8pp supports them. And I hope v8pp might be helpful for node.js addons since Joyent has finally released version 0.12 a couple of days ago.
C++ is not ~1400 pages if you leave the library specification out, like you are doing for the other languages.
Here's a comparison of the complexity in terms of numbers of concepts in C++ compared with a couple of other languages: http://cpprocks.com/cpp-ruby-coffeescript-language-complexity/ You can quibble about whether different features are equivalent in complexity (for example, it's not obvious from that that C++ templates are a Turing complete language themselves), but it might be a better measure than pages of specification.
A-ha! They're all cheating.
WinRT??? WinRT is dead, not officially, but... http://www.osnews.com/story/28240/Windows_RT_users_won_t_get_Windows_10
Speaking of backward compatibility...
He doesn't write C++. Subsurface is a combination of stuff written in C (Linus) and a GUI written in C++ (by other people).
This is in reference to [Windows Runtime](http://en.wikipedia.org/wiki/Windows_Runtime), the API/framework for Windows Store apps, not [Windows RT](http://en.wikipedia.org/wiki/Windows_RT), the OS.
&gt;As for Java, it's lamentable that the language developers have clung so long to backward compatibility. They made the wrong choice. Also it seems to have some interesting design choices that feel very odd coming from other languages like C++ and C#. Like for instance the fact that allow for anonymous classes yet lacks support for delegates. &gt;All the other languages have been advancing, leaving poor Java in the dust. I don't think Java 8 will go far enough.. They are playing some serious catch up here. The feature set of 8 isn't really that impressive, especially considering lambda expressions have been around in other languages for a long time. I think a lot of the shit people give Java is not deserved, but they are definitely a bit behind other languages. 
One could make similar arguments for some C# features: Allow extension classes, but not free-standing functions..?
There is no way you can fit all of [this](https://docs.python.org/3.4/library/) in 130 pages.
&gt; backward compatibility... Crazy talk. who cares about backward compatibility.
But you're including the library specification. Still, the language definition is bigger than C11's.
From a purely visual standpoint, this sort of functionality seems to be done better by some of the kxstudio apps (eg http://kxstudio.sourceforge.net/Applications:Catia )
Last time I looked, your distribution did not support using std::thread. Is this still the case?
Maybe, but it's certainly uglier.
Huh. TIL about covariant return types and virtual functions. It never would have occurred to me that you can change the return type of a virtual function that way, *especially* with the `override` declarator.
Backward compatibility is fine to a point, but it should always be measured against real, forward progress.
Not an article. I consider this useless spam.
Has an article on Catch and other test frameworks by its creator.
I agree. The programming language should avoid enforcing what it thinks is right upon the programmer - the programmer should always be assumed to know best. C++ fulfills that goal. It's perhaps the reason C++ is the most widely used language in the AAA games industry - other languages don't give the freedom, usually, and game's programmers are smart enough (and have hard performance goals to abide by) so that they are capable of utilizing the power for the better. For example, with Java you can get obnoxious frame skips from whenever the GC decides to run, because you can't be trusted to manage memory yourself.
I confess that I assumed its intention was to implement graphs (the data structure) and associated operations. It was a very confusing 30 seconds of my life.
Isn't visual studio free now?
I use RapidXML in my editor for basic XML config file parsing and it works very well. 
Pugixml is also nice and fast, but it's a pitty that it lacks XML NS support. It is actually not a big deal to implement it as an option to not reduce default performance.
Actually, the *language specification* is [pretty small](https://www.reddit.com/r/cpp/comments/2v98qq/so_how_big_is_the_c_language_specification/cogyzq2). (But I do think it is more complex.)
Are the slides posted anywhere? 
You have their url in the video
I can't see the video...
I can see your point; I think it's a matter of perspective. They're static functions (of a bogus class) that take the object they're decorating as their first argument. If it weren't for the syntactic sugar, they would be very close to free-standing functions. 
Slides: http://meetingcpp.com/index.php/tv14/items/18.html Github: https://github.com/henry-ch/asynchronous
&gt; Where there are two mutex there is a deadlock. I do not agree with this, I managed to deadlock even with one mutex.
Added Node.js addon examples with difference to v8pp: https://github.com/pmed/v8pp/commit/34c6344bdb1bdf7f0b46db6ea15c8ddd20ac3824
Yup, that fixed it. My mistake, sorry Codelite!
I don't want to be a dick but it's Qt, not QT ;-)
&gt;This is a feature of statically typed languages: the compiler informs you about type-system errors — rather than users. Which in my opinion is one of the most important features compared to dynamically typed languages. Having the compiler do this instead of run time checks saves an incredible amount of time on debugging, simply because it eliminates a whole category of bugs. Code that does a good job exploiting this is clearer and is less prone to bugs. I've found that languages like Javascript end up with type checks caked everywhere along with a whole slew of unit tests just because they can't ensure what type something is until runtime.
Pointer addition is not allowed to overflow. The optimizer should thus be safe removing all checks.
It's not really an international treaty or a legal document. ISO is just a private organization, and the US representative, ANSI, is also a private organization.
Yes, but it's not available as a library though. :(
What's the license?
In this case there are of course no heavy constructors involved, it's just integers, but I have seen many people carry the old C tradition of declaring every single variable at the start of the function, only to define them only later when (or even *if*, since the function may be exited before) they need them. (The previous developer of a project I've been maintaining for some time did that, it's horrible and a big reason this bothers me so much) If you do that with an object that has a constructor that does initialization, all the work will have been pointless if you're going to copy- or move-initialize it later with the *real* value, i.e. HeavyObject h; // HeavyObject::HeavyObject() does heavy // stuff, maybe heap allocation? // ... Lots of stuff happening, possibly including an early return from the function ... h = ... // copy- or move-construct, possibly involving // freeing the allocated memory again and // reallocating a new block of another size I'm pretty sure a compiler can optimize that out if it can prove that the initial constructor has no side effects, but even then there are two arguments against it: 1. the readability, having to scroll back up to the beginning of the function scope to see your locals 2. it makes it much easier to forget to initialize a variable.
 std::vector&lt;int&gt; v; std::size_t size = v.size(); I always thought that this is the correct solution. I thought std::size_t will be equal to std::vector&lt;int&gt;::size_type. According to the blog post, I was mistaken. Is that really so? If so, what do we do in a loop? I know that obviously the best solution is for (auto&amp;&amp; e : v) {...} But let's say we need the index variable. So according to the blog post for (std::size_t i = 0; i &lt; v.size(); ++i) {...} is wrong too. The following would be the most correct, right? But it's so verbose: for (std::vector&lt;int&gt;::size_type i = 0; i &lt; v.size(); ++i) {...} I guess this would be possible too but isn't any less verbose and even has unnecessary complexity: for (decltype(v.size()) i = 0; i &lt; v.size(); ++i) {...} So there's no correct &amp; non-verbose solution, is there? Edit: With _wrong_ I mean suboptimal, because type conversion occurs, and we can run into trouble if the vector is large (without noticing it). With _correct_ I mean a solution without these hidden bugs.
Having run into this problem... yes, decltype is the preferred solution. I agree that it's depressingly verbose, but really, the procedure to get the index variable is also unnecessarily verbose. I can't tell you how many times I've wished for Python's enumerate()... EDIT: Maybe I'll write it, as an exercise. EDIT EDIT: [Here](https://gist.github.com/anonymous/47748302ace18a7eff9f). Should be free from undefined behavior. Works with the STL containers and C-style arrays; supports in-place modification. If there are problems, please let me know. 
Herb Sutt has a post on this. I think auto i = 0u; should automatically cast i to an unsigned int? This seems the simplest way, though not the clearest, but auto isn't relaly about clarity.
Gotcha, I get it.
This is something that is going to depend a lot on your personal style. Using either the the decltype solution that you made or the `::size_type` solution are probably the most technical correct way to do it, and will work for any indexed type. HOWEVER. (and someone correct me if I'm wrong here) In your specific case, you just want to be sure that your index doesn't have a maximum less than v.size() to prevent unsigned rollover. According to [cppreference](http://en.cppreference.com/w/cpp/types/size_t), `std::size_t` is the maximum size of any type, including arrays, so it's guaranteed to be as big or bigger than `std::vector&lt;int&gt;::size_type`. So it should be acceptable to use `std::size_t` as a vector index for( std::size_t i = 0; i &lt; v.size(); ++i) Or, if you're like me and made a `std::size_t` user-defined suffix: for( auto i = 0_sz; i &lt; v.size(); ++i ) EDIT: It has been pointed out to me that this is not true for `std::vector&lt;bool&gt;`. It still should be true for other `vector`s though
Forgive me if I'm wrong, but `size_t` should defined to be "big enough to store the size of any type, including an array". Ergo, `sizeof(std::vector&lt;int&gt;::size_type) &lt;= sizeof(std::size_t)` should always be a true statement, and there shouldn't be a problem with using `std::size_t` as a vector index variable.
In all honesty I would just write my loop like this: for(std::size_t i = 0; i &lt; v.size(); ++i) { ... } The simplicity of this code overrides the concerns of a minority of language lawyers for that one obscure case that never happens in practice where I'm iterating over a vector of 4 billion items on 32 bit machine, or a vector with 2^64 items on a 64-bit machine. If my vector does have more than 2^64 elements then chances are there are bigger issues to worry about than bike shedding over what type to use as an index over an array.
It's an interesting solution. I would however not advise using it as anybody that's going to look at that code will think "WTF?" and then they will ponder what your goal here was and if you've made a typo or mistake.
You're right on vector&lt;bool&gt; I completely forgot about that. I was definitely thinking of specifically `std::vector` here, I knew that that trick wouldn't work for `std::deque`. However, I figured it was a useful logic, because the times I have non-bool vectors is much greater than the times I have `std::deque`s and `std::vector&lt;bool&gt;`s.
&gt;facebook &gt;profit Do they generate profits now?
Yeah, this blew my mind when I discovered it. But the *really* nasty issue is that `size_t` and `ptrdiff_t` have no relation to `intptr_t` - they are only defined for differences *within* an allocation. Unfortunately, most implementations are too sane to take advantage of the memory savings.
Small nitpick, you should make it clear to the reader and the compiler that the size of the vector doesn't change during the loop. for (std::size_t i = 0, e = v.size(); i != e; ++i) { ... } 
If C++ wasn't so dedicated to backwards compatibility, they'd probably change it to return an `optional&lt;pos&gt;` instead.
The string methods themselves predate &lt;algorithm&gt; which is a superior interface: not found == end iterator. 
The end iterator is a lot more than just a success boolean, the algorithms in that header compose really nicely because they work on iterators. For example, remove all elements in a vector up until a "12": v.erase(v.begin(), std::find(v.begin(), v.end(), 12)); 
You could do `template&lt;typename T&gt; using VecSize = std::vector&lt;T&gt;::size_type`.
Don't agree with this as it will break if the vector size does change during the loop. Maybe it shows you *intended* the vector to not change during the loop, but then one could argue for refactoring the code if the loop is big enough that it's not obvious whether the vector changes or not.
`uintptr_t` is an optional type and it has nothing to do with array indexing or sizes. What advantage do you see with it over `size_t` ?
You're right, it isn't a cast. I meant to say that auto will type deduce unsigned int. Thanks.
as in beer, yes, as in freedom, no.
You should have: msys2_shell.bat: /usr/{lib,bin} contain 32-bit or 64-bit MSYS2 software depending on which you installed. MSYSTEM env. var is "MSYS" mingw32_shell.bat: /mingw32/{lib,bin} contain 32-bit native Windows software. /mingw32/bin is at the front of your PATH (before /usr/bin). MSYSTEM is "MINGW32" mingw64_shell.bat: /mingw64/{lib,bin} contain 64-bit native Windows software. /mingw64/bin is at the front of your PATH (before /usr/bin). MSYSTEM is "MINGW64" If any of the above is not true, please file a bug report at https://github.com/Alexpux/MSYS2-packages 
Array indexes and sizes will be eventually mapped into memory offsets/addresses. Something like `uintptr_t` is guaranteed to do that all the time, whereas other integer types don't always offer that guarantee. I prefer to use `uintptr_t`, because its name reflects the aforementioned capability. Having said that, you should be safe with `size_t` in that respect, as they are synonymous with `uintptr_t` on most architectures. You might get in trouble with `size_t` on segmented architectures, as they not guaranteed to index arrays spanning multiple segments.
&gt; It would be surprising and flawed if you have a generator that doesn't do this, as it cannot cover all permutations of calls that you should reasonably expect. How does that work? Suppose your seed has a max value n. There can only be n sequences from that generator. But for a sequence of n random values, there are a lot more possible permutations than n. 
You'll have to explain that last bit a bit more; `size_t` is guaranteed to be able to hold the maximum size of an object, and an array is an object. Are you talking about representing an abstract structure which is larger than `size_t` elements? 
Ask not for whom the bone bones. It bones for thee.
If that's your bottleneck, have you considered some other datastructure?
...I'm pretty sure you can't do that. Calling a method through a null pointer would be dereferencing that pointer, which would be undefined behavior. I know some compilers let you do this, but that's the nature of undefined behavior--the compiler can do anything. You should never, ever rely on this.
I don't think that is possible for `vector` (although it is certainly possible for `deque`). The standard requires that vector elements be stored in a single array, accessible by offsetting from `&amp;v[0]` 
This is one more example of useless work done, since you almost never need good random that is not crypto. You either need fast not terrible random(rand()) or crypto... Verbose academic stuff introduced in &lt;random&gt; is neither. 
Did you read the entire article? The author admits that there is a need for a simple rand function and dedicates an entire section explaining alternatives that would be as simple as rand without any of it's downsides.
The issue is more that every time you wrote "random" you meant "unevenly distributed pseudorandom numbers with a low quality seed". In reality, C++, I believe, has plans to implement something very similar to rand but with real number generators behind it.
So is starting with an underscore and I use both conventions anyways because it works and I like it.
&gt; The Standard does not guarantee this, but it suggests that random_device should be such an engine. VC guarantees this. Do you think that there should be either a guarantee that random_device isn't statically seeded PRNG (I am looking at you MinGW) or a boolean member function like `is_random` on `random_device` so that user could check programatically instead of crawling through implementation details? Because currently the random_device -&gt; mt -&gt; distribution idiom for getting random numbers is broken and non portable. (I actually got bitten by this once. Also `enthropy` is currently useless)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Monte Carlo method**](https://en.wikipedia.org/wiki/Monte%20Carlo%20method): [](#sfw) --- &gt; &gt;__Monte Carlo methods__ (or __Monte Carlo experiments__) are a broad class of [computational](https://en.wikipedia.org/wiki/Computation) [algorithms](https://en.wikipedia.org/wiki/Algorithm) that rely on repeated [random](https://en.wikipedia.org/wiki/Random) sampling to obtain numerical results. They are often used in [physical](https://en.wikipedia.org/wiki/Physics) and [mathematical](https://en.wikipedia.org/wiki/Mathematics) problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: [optimization](https://en.wikipedia.org/wiki/Optimization), [numerical integration](https://en.wikipedia.org/wiki/Numerical_integration) and generation of draws from a [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution). &gt;In physics-related problems, Monte Carlo methods are quite useful for simulating systems with many [coupled](https://en.wikipedia.org/wiki/Coupling_(physics\)) degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see [cellular Potts model](https://en.wikipedia.org/wiki/Cellular_Potts_model)). Other examples include modeling phenomena with significant [uncertainty](https://en.wikipedia.org/wiki/Uncertainty) in inputs such as the calculation of [risk](https://en.wikipedia.org/wiki/Risk) in business and, in math, evaluation of multidimensional [definite integrals](https://en.wikipedia.org/wiki/Integral) with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, [cost overruns](https://en.wikipedia.org/wiki/Cost_overrun) and schedule overruns are routinely better than human intuition or alternative "soft" methods. &gt;The modern version of the Monte Carlo method was invented in the late 1940s by [Stanislaw Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam), while he was working on nuclear weapons projects at the [Los Alamos National Laboratory](https://en.wikipedia.org/wiki/Los_Alamos_National_Laboratory). Immediately after Ulam's breakthrough, [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann) understood its importance and programmed the [ENIAC](https://en.wikipedia.org/wiki/ENIAC) computer to carry out Monte Carlo calculations. &gt;==== &gt;[**Image**](https://i.imgur.com/aZnsTq5.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:Rayleigh-Taylor_instability.jpg) --- ^Interesting: [^Quasi-Monte ^Carlo ^method](https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method) ^| [^Dynamic ^Monte ^Carlo ^method](https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method) ^| [^Quantum ^Monte ^Carlo](https://en.wikipedia.org/wiki/Quantum_Monte_Carlo) ^| [^Monte ^Carlo ^methods ^for ^option ^pricing](https://en.wikipedia.org/wiki/Monte_Carlo_methods_for_option_pricing) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cojew9o) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cojew9o)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; Nothing about rand()‘s behaviour is required. rand() could legally return the same value over and over every time you call it. It could have a period in the single digits; or return alternating odd and even numbers (and one implementation actually did this!). There have been some implementations that are legendarily bad. The C11 standard actually says some implementations are known to produce sequences with distressingly non-random low-order bits, and advises against using it for serious purposes. I'd say that if you want any portability, then rand() definitely isn't good enough. If you happen to be sticking to a single platform where you know the implementation of rand() won't change, then maybe rand() will be good enough.
I've been teaching myself C++ as well and I did his recommended google search and found some pretty mediocre links. There's [LearnCpp.com](http://www.learncpp.com/) which has some pretty basic quiz format questions, but when it starts getting into the stuff that could really benefit from some sort of way for being able to test yourself to verify that you have actually grokked the content. There's also [exercism](http://exercism.io/) which requires a github account. I've not used it, but it sounds like something that might give you exactly what you're looking for. If others out there have any first hand experience with sites they have found useful, I would love to hear about them. The advice "Just use Google" is not all that helpful.
Some examples of such sites would be: - [Project Euler](https://projecteuler.net/) - [CodeEval](https://www.codeeval.com/) - [HackerRank](https://www.hackerrank.com/) They're not geared specifically towards C++, but you'll find as you progress up through the problems you'll run into areas where you have to gradually extend you knowledge. These type of challenges will also depend on your knowledge of the standard library/STL just as much as your knowledge of the core language, possibly moreso in some cases. But the standard library is easily searchable online. For general tutorials and howtos I like the content on http://www.cplusplus.com/, but like everything else it hasn't really caught up yet with the latest standards. To be honest I think it's probably better to pick up the older language features first, then build on that and substitute in new techniques and libraries to your code as you learn more.
"To be honest I think it's probably better to pick up the older language features first, then build on that" this is exactly what my friend who loves C++ and has been coding for more than a decade said as well. He actually said he is embarrassed that people pay him what they do for coding, because he loves it so much. 
As someone that does nothing but numerical problems... I haven't really worked with ranges, but why would I want this? The "standard" code given is clean, concise, readable and clearly identifiable as a Newton-Raphson iteration. The replacement code is 10x longer and bears little resemblance to the math that's happening. To me, this: template&lt; typename T , typename F , typename DF &gt; T newton( T x , F f , DF df ) { using std::abs; T y = f(x); while( abs(y) &gt; static_cast&lt; T &gt;( 1.0e-12 ) ) { x = x - y / df(x); y = f(x); } return x; } is vastly preferable to this: template&lt; typename T , typename F , typename DF &gt; class newton_range : public ranges::range_facade&lt; newton_range&lt; T , F , DF &gt; &gt; { friend ranges::range_access; public: newton_range( T x , F f , DF df ) : m_x( std::move( x ) ) , m_f( std::move( f ) ) , m_df( std::move( df ) ) { m_y = m_f( m_x ); } T const&amp; x( void ) const noexcept { return m_x; } T const&amp; y( void ) const noexcept { return m_y; } private: struct cursor { newton_range* m_rng; cursor() = default; explicit cursor( newton_range&amp; rng ) : m_rng( &amp;rng ) {} decltype( auto ) current() const { return m_rng-&gt;m_x; } bool done() const { return false; } void next() { m_rng-&gt;m_x -= m_rng-&gt;m_y / m_rng-&gt;m_df( m_rng-&gt;m_x ); m_rng-&gt;m_y = m_rng-&gt;m_f( m_rng-&gt;m_x ); } }; cursor begin_cursor() { return cursor { *this }; } T m_x , m_y; F m_f; DF m_df; };
Have you read [GotW #94](http://herbsutter.com/2013/08/12/gotw-94-solution-aaa-style-almost-always-auto/)? 
How does implementing those things with a range make things easier/better compared to a traditional class? Asking out of ignorance here, I've never used a range for anything.
Primarily, not screwing up width or signedness.
Picking up the older features first is a good idea. The new stuff isn't really critical. My boss likes to joke about this, saying he codes in C+. 
Despite the rise of StackExchange, programming has never been taught well online. I'm sorry to say it, but if you want to learn you need to buy good books. Otherwise you're wasting your time on the horrible crap people put online for free.
Successful applicants will receive stipends for airfare and lodging this year! If anyone has any questions, feel free to ask - I'm the organizer for this program. -- Bryce
First of all, it is an IDE. it's not made for hello world apps and you can't compare it to a text editor. Also, it's Java so you can set the minimum and maximum amount of memory it can use. 
Ive even seen auto main() -&gt; int Why not simply use int main()
Any feedback, bugs and feature requests would be greatly appreciated. My goal is to have version 1.0 out on March 13th (also a Friday). The code is all on [GitHub](https://github.com/tgockel/json-voorhees) and I am a big fan of using their bug tracking system.
I do like CLion, at least what I've seen already and I cannot wait for it to be ready.
This library looks really good. One thing I noticed though is that the 'Builder DSL' seems to reinvent a lot of what Boost Fusion can provide. Have you looked at adding optional support for it? I'm somewhat in favour of using macros to make this stuff look even cleaner. Here's a DSL-like snippet from some code I use with Boost Property Trees XML backend First the key #defines: #define XML_SUBTREE(TYPE, NAME) (TYPE, TYPE, \ obj.get&lt;TYPE&gt; (NAME), \ obj.set (NAME, val)) #define XML_ATTR(TYPE, NAME) (TYPE, TYPE, \ obj.get&lt;TYPE&gt; ("&lt;xmlattr&gt;." NAME), \ obj.set ("&lt;xmlattr&gt;." NAME, val)) #define XML_TEXT(TYPE) (TYPE, TYPE, \ obj.get&lt;TYPE&gt; ("&lt;xmltext&gt;"), \ obj.set ("&lt;xmltext&gt;", val)) #define XML_SUBTEXT(TYPE, NAME) (TYPE, TYPE, \ obj.get&lt;TYPE&gt; (NAME ".&lt;xmltext&gt;"), \ obj.set ("&lt;xmltext&gt;." NAME, val)) And an example of use: BOOST_FUSION_ADAPT_ADT ( xml_encoded&lt;Description&gt;, XML_ATTR (std::string, "summary") XML_TEXT (std::string) ) BOOST_FUSION_ADAPT_ADT ( xml_encoded&lt;Event&gt;, XML_ATTR (std::string, "name") XML_SUBTREE (std::shared_ptr&lt;Description const&gt;, "description") XML_ATTR (boost::optional&lt;unsigned&gt;, "since") ) Here the 4 macros provide the various structural mappings to the XML DOM, and the xml_encoded template (to which get&lt;T&gt;() and set() belong) contains a property tree reference and provides overloads to handle vectors, optionals, strings, smart pointers. Containers are handled transparently. I never bothered to flesh it out, but the result is I can load and store arbitrarily deep XML, and weird things like loading in to shared_ptr&lt;optional&lt;vector&lt;Employee&gt;&gt;&gt; just work when invoking fusions copy() algorithm.
From going with what /u/Rhomboid mentioned with GotW, I can see a lot of benefits from the article such as the one you mentioned. But even avoiding signedness issues etc; writing code like: auto v = double{}; Doesn't help with anything other than uniformity with the rest of your code. Or am I still wrong?
They're bad if you don't need to do polymorphic deletion (i.e. delete through a pointer to base). Base classes should have a public, virtual destructor or a ~~private~~ protected, non-virtual one.
if you add support for comments and hex/binary/octal literals (and retaining that representation in memory, so that writing it back out to disk does not lose that information!) i'm sold
Good point. On closer reading of the chapter in Scott Meyers' book, there appears that, as you say, there are instances where virtual is a good idea, and instances where it's a bad idea.
The problem here is that there are dozens of build systems and everybody is annoyed if you don't support theirs. Lately I've considered shipping without build files in order to signify 'this project has no special build requirements. If you want to use it, add it to your tree'. Unfortunately, I still would need to ship some sort of build system for the usage examples... It's almost enough to make a person write inexplicably header-only libs (which I'd probably never do because I like my projects to compile fast, and I'd like to use any library I write), or do something crazy like a STB-style build (which I'd probably never do because unlike STB, my projects aren't good enough for anybody to justify using it despite its strange build requirements).
Actually a protected, non-virtual one. Or else even derived classes would have trouble to execute their own destructor, lacking access to the base class dtor ;-)
I'm not opposed to CMake in principle, just practice. When I first ported this to Visual Studio 2015 CTP, CMake did not support project file generation for it, so that was a non-starter for me (they support it now). I also use NuGet in Visual Studio to deal with my build-time dependency on `boost::lexical_cast`, which CMake does not properly support setting up (you can do a prebuild hack, but VS does not seem to like `nuget restore` getting called from under it...that might just be a CTP thing, though). I let VS's NuGet "fiddle with library paths" for me, because I can write one XML file and be done with it. I've heard the "CMake is more flexible than Make" argument before, but I don't understand it. Choosing GNU Make restricts me to platforms and toolchains that support Make, which is pretty much everything. With respect to configuration, with make, you just have to have your Makefile support the conventions that Linux people have been using forever: `make CXX=clang++`. Anyway, now that CMake supports VS2015, I can probably use it.
If nothing else, I'm glad for this post because I just discovered you can get Boost libraries from NuGet. For some reason, that never occurred to me! Edit: Yesterday's [post](http://blogs.msdn.com/b/vcblog/archive/2015/02/13/find-your-favorite-library-for-c-in-nuget.aspx) from the Visual C++ Team Blog covers the prominent C++ libraries that are available from NuGet.
&gt; But to argue that virtual destructors are a flaw is pretty much like arguing that inheritance is a horrible thing. You will find many people who actually argue that and while I personally don't agree, I think inheritance is massively overused.
I've used Boost.Fusion in the past, but I never even thought about using it here. I suppose there is nothing that would prevent you from using Fusion to do your `value` conversions. I am hesitant to add a *requirement* on Boost, I would be happy to add some form of an optional alternative for building `formats`. That said, I really need to think about how that should be done. Does it make sense to have a second "supplementary" project that you can use as some sort of value-add? I haven't thought about any of that just yet :-)
Brace initializers are still broken on this build, shows my code full of false errors for code like this: #include &lt;string&gt; struct foo { std::string bar; }; int main() { foo my_var {"str"}; return 0; } At least they fixed a whole bunch of other false positives, hope they fixes the rest of them for the next build. 
Yeah, that's what I was thinking, or god forbid double indirection. `jsonv::value**`.
I remember Stephanov once mentioned how in an interview they asked him why you would make your destructors virtual. Which he responded that he never makes his destructors virtual and his classes are used by lots of people.
While hex/binary/octal literals are not officially JSON, it does make sense to add *some* of these as [`parse_options`](http://tgockel.github.io/json-voorhees/classjsonv_1_1parse__options.html). I'm somewhat opposed to octal literals because the prefix for them leads to some confusing ambiguity (does `0341213`mean 341213 or 115339?). If I did octal, it would probably be under a different prefix like `0o` (a la Python). As far as "representation-preserving" goes...there's not really a fantastic way to do that. By the time the parser is done, everything is in the AST as a C++ native type -- all of your literal forms will be represented as `std::int64_t`s. With respect to preserving comments, my ultimate plan was to have some form of comment system inside of a `value`, but haven't decided on a few things. * How to comments effect comparison operators? Does `x == y` iff their structure is the same *and* their comments are the same? If comments are discarded in compares, then the `std::hash&lt;jsonv::value&gt;` implementation must also discard them for hashing...are there any negative side effects of that? Probably yes. * Do comments decorate a *document* or any arbitrary `value`? The former implies some tracking system only on `kind::array` and `kind::object`, which is incompatible with `parse`'s ability to parse non-document types (for example, `parse("6 /* yo */")` illegally works). I can't think of how to implement the latter without adding byte overhead to every single `value`. If you don't want to wait around for me to make these decisions, check out [`tokenizer`](http://tgockel.github.io/json-voorhees/classjsonv_1_1tokenizer.html) -- it chops up JSON into tokens so you can easily extract whatever you need (you can even preserve whitespace if you are so inclined). That will let you drop your JSON into your own AST type.
Does anyone know about the license for Fit? I don't see a mention of one.
Indeed, his complaint seems to be about C++ exposing the concept of virtual and how that means programmers have to deal with having a choice. The article is poorly written and the author seems to have a fairly tenuous grasp on how virtual functions operate, apparently gained through experience rather than from knowledge of the formal rules. As a result, it's not surprising that his argument against having to explicitly mark virtual functions is based on an examination of just a couple of superficial side-effects, rather than an examination of the fundamental issues at play.
Its boost license. You can see the license [here](http://pfultz2.github.io/Fit/doc/html/license/).
I'm going to add CMake support before 1.0, but until then: make sudo make install Header-only is problematic here, as JSON Voorhees makes use of globals in a number of places (in a non-evil way).
Not really a fan of CMake but I do see its benefits. If people don't mind it then maybe I'll consider using it in the future. It seems like overkill for small (2-4 file) projects though. Regarding no build system, does that make most people unhappy? I don't mind libraries that say 'include this in your project, it has no build requirements', in fact, I prefer that for anything under a certain size.
I don't really care about syntax, but 0o would work. Alternatively (or additionally) you could have something more generic to denote the base, such as 42_16 for 42 in base 16. Not sure how I'd implement comments, but I think JsonCpp stores them in the value. I'd definitely treat them similar to whitespace though, meaning they don't participate in comparison or hashing.
I love that recursive print.
&gt; I've heard the "CMake is more flexible than Make" Its not CMake vs Make. Make is for building, and CMake is for configuration. &gt; you just have to have your Makefile support the conventions that Linux people have been using forever The convention is to use a configuration tool such as autotools. So you do `CXX=clang++ ./configure`. Using `make CXX=clang++` is quite rare.
That is a somewhat pedantic (albeit entirely correct) distinction. When I use CMake in this context, what I mean is "CMake and the whole infrastructure that comes with it," which generally constitutes generating some form of a project file (in Linux, a Makefile) and then running that. My point here is that since I'm not using the features of CMake (and don't really need them), I might as well skip over the intermediate step and write the Makefile by hand. In any case, now that I'm supporting Visual Studio, there *is* a good use case for a tool like CMake. That FAQ is a bit outdated -- it is from when I only supported reasonable (POSIX) operating systems.
I get where you are coming from but I'm not sure of solution that solves both problems. Providing build scripts and dealing with dependencies (at compile or runtime) are the two biggest issues I have with C++. I've written a few header only libraries for my own use and they work great if they are small utilities not giant beasts like what you'll find in Boost. When you need do more though I'm not sure of an easy solution to the build script explosion.
There is plenty of extra space. The `kind` has 7 possible values, so I only need 3 bits to represent it. Meanwhile, the other component of a `value` is a pointer, so the whole `value` structure gets padded to 16 bytes. If I assume the memory allocator always gives me 8-byte aligned data, I can cram it into the padding around `kind` (which I don't think is a correct assumption in 32-bit mode). However, I was already thinking about using that area for polymorphic allocator support (which is currently an experimental C++ feature, so we'll see).
Just one question: have you benchmarked it against said alternatives? Ease of you is great, of course, but performance matters too (both memory usage and "speed").
Please don't advocate `endl`; a plain `'\n'` is plenty sufficient and likely more efficient (as it does not involve flushing).
I'd take a guaranteed flush over some tiny perf increase any day. Especially when I'm writing valuable debug/logging information.
Terrible title/intro given the nature of what fit is.
oh sure, make is fine, cmake is a bit more flexible that's all. and providing a cmake module, would make including the library in my project easier (as well as error checking and stuff). but yea, nothing inherently wrong with a simple Makefile. about the globals: just don't be thread-unsafe please. that would be a real bummer. by thread-unsafe i mean, using different objects (values let's say) in different threads and having concurrency problems. that should never happen. it's perfectly fine if one object is not safe to be modified at the same time by multiple threads.
&gt; If you know that you really do want a non-virtual destructor, it's annoying that the C++11 "final" keyword wasn't introduced earlier. There are cases where you want to inherit from a class but still have the base class have no virtual destructor. ('final' isn't needed in these cases.) Examples: * Template meta programming. See everything that inherits from std::true_type and std::false_type. (Neither true_type nor false_type have virtual destructors.) * Private base classes or base classes with protected destructors. Destruction through the base class is prevented in these cases. This type of construction is sometimes useful over composition. An example is how std::set and std::map implementations usually inherit from some tree class. But as a user of the STL, you never directly access the tree class.
&gt; some tiny perf increase any day Unless I remember wrong the "tiny" can vary greatly depending on the console/file at the other end of the stream. &gt; Especially when I'm writing valuable debug/logging information. One place where that is the only sane thing vs. many where it can really slow down code without need. 
hey chanz... thanks for this one .. i really appreciate your effort.. and i owe you.. hehe 
I said non-evil! The only non-safe way to use the globals is if the user goes and changes them mid-program. As an example, there is a function called `set_demangle_function`, which is used by the `to_json` and `extract` functions to present the user with cleaner-looking errors. The *intent* is for the user to set this once in `main` and then move on. Almost everything else is non-shared.
&gt; Unless I remember wrong the "tiny" can vary greatly depending on the console/file at the other end of the stream. That's true, but we're talking specifically about `std::cout` which is usually line-buffered anyway (meaning writing `\n` will also cause a flush).
You expected CMake to support a "Community Technology Preview" release of Visual Studio? CMake with a little help from Git Bash allow me to build my cross platform (Linux/Windows) app with the same sequence of commands, without even having to maintain separate project files.
Alexander Stepanov is a funny person to ask about virtual destructors! Virtual destructors are a sign that you are using the language in a way totally differently from what he generally advocates for... Note how little inheritance (and how few virtual functions period) there are in the STL? Alexander Stepanov likes his polymorphism at compile time.... :)
Totally agree about "final"... I didn't know that existed until this article made me wish for it, and then to my happy surprise I found out C++11 has final... :)
Oof. Wrong doesn't even begin to cover this one. The amateurish tone is red flag enough, but: The author doesn't seem to even understand the purpose of virtual destructors at all, then deduces from ignorance that there isn't one... and/or that they should always be present (?). The argument is incoherent and confused... virtual is bad, here's a flood of obvious data that says vtables exist (duh), therefore virtual is bad so everything should be virtual. Sort of. Bonus points for explicitly using POD types as a part of such an argument...
As the comments on that page mention, it's related to the const return value. It never makes sense to return a const value from a function, because returning by value is semantically a copy and copying discards cv-qualifications, which means there's no observable difference from the caller's standpoint between a function returning T and a function returning const T. Const in the return type only makes sense when you're returning a reference. I have no idea how that fits in with the conversion operator, but getting rid of the const in the return value fixes both problems. 
&gt; You expected CMake to support a "Community Technology Preview" release of Visual Studio? No -- it's perfectly fine that they didn't. I was just saying that to point out why I couldn't use CMake originally. CMake 3.1 now supports VS2015 CTP, which is pretty awesome (the CMake team is really good about staying on top of new technology).
Just because it uses a lot of ram for a basic program, that doesn't mean it's going to to grow by leaps and bounds with each file you add. 
...lets ignore the part where apps generally treat std::vector as nearly unbounded storage and typically don't handle the possibility of a platform where std::vector&lt;T&gt;::size_type is type with a lower capacity than is expected. ;)
 #include &lt;string&gt; const std::string Foo() { return ""; } std::string Bar() { return ""; } int main() { std::string s; Foo().swap(s); // does not compile Bar().swap(s); // okay }
Yes, we welcome students from all countries.
Just a note: JSON is good when human consumption is a secondary but not primary need. Otherwise something more pleasant like YAML is a better option.
I understood... none of those words
You're right, I suppose I didn't think it through completely when I said there's no observable difference. I still feel like it's very, very rare that you would want to ever return a const value. 
If you want compact messages, use protocol buffers or msgpack or something like that. But JSON has the advantage of universality, simplicity and readability. Indeed, you can paste JSON right into Python or Javascript code and it's completely legal as a variable definition. For example, I'm working on a server program being used by third-parties. Our "RPC" uses JSON and people write their little JS programs and it all "just works"...
I liked this a lot. It kind of framed the programmer I want to be while making me aware of the programmer I am and the programmer I am becoming, neither of which are the one I want to be. Obvious generalizations, but these types can be seen everywhere in some degree or another.
Sure but you can do that include in the project tree thingy as well with a project that does have CMake. Those methods are not mutually exclusive.
The flexibility argument is easy. CMake can generate makefiles _and_ other project files, so it is in a way a superset of makefiles. If you prefer to work with makefiles, tell CMake to generate them. If someone else doesn't know makefiles (i.e. most windows based coders) they don't have to go through the effort of learning how to setup and tailor their environment of choice to use makefiles, they can just have CMake spit out a CodeBlocks project file, or a Visual studio solution. The bottom line is that people are lazy and are not going to bother fiddling with makefiles just to get a new JSON library working if they don't already know makefiles. There are many other JSON libraries to choose from already, and one that aims for a convenient API that doesn't have a convenient build system is not going to be very attractive to those people. :) But if those are not part of your target audience, that's of course okay.
The ones who understand binary and the ones who don't?
Yes, actually Scott Meyers advised the use of returning `const` value specifically to inhibit `Foo().swap()` or `Foo() = ...` from compiling. This is in line with Stroustrup `const&amp;` temporary binding rules, which do allow simple`&amp;` to bind to temporaries. And honestly, I care for neither. I wish `&amp;` could bind to temporaries and I wish for the ability to assign/tweak a return value if I need to.
Except none of them describe me.
&gt; Implementing it using the ternary operator is just a hack that just doesn't work. You should check out the implementation of `common_reference` in range-v3. In particular, see `detail::builtin_common_t` [here](https://github.com/ericniebler/range-v3/blob/master/include/range/v3/utility/common_type.hpp#L132) to see how the ternary operator *can* be used to implement this, with some smart pre- and post-processing of the arguments and the results.
It's not even `std::result_of_ternary_operator_on_declval_of&lt;A, B&gt;`, if it was `common_type&lt;int&amp;, int&amp;&gt;` would have been `int&amp;`. It is `decay&lt; result_of_ternary_operator &gt; `.
**N4273**: I'm excited. **N4279**: I strongly disagree with the approach. If transitional interfaces are deemed really important, then fine. Add new interfaces but fix emplace too!. That way in 5 years you can deprecate/remove the transitional try_emplace API. For people who can rely on a particular standard-library implementation, they won't need the transitional API in the first place. As for the argument against a pre-processor trait doesn't seem genuine. SG10 is all about feature detection. Define __cpp_lib_emplace_no_move and move on with your life. Even if you didn't, people could check that __cplusplus is &gt;= 2017 anyway. **N4280**: I like it. Would be nice if it came with a std::reserve so I can write generic functions on containers that work efficiently with std::vector, std::list, std::unordered_map etc. **N4282**: I predict this one will get shot down even though I think it would be a good idea (would help with annotating legacy code tremendously). **N4270**: I'm not sure about the proposed searching stuff. This feels like a similar problem to Howard's hash proposal or how the RNG stuff works. Separate the algorithm from the interface. That way you would still have std::search, but your BoyerMoore searcher could be a parameter to it that's templatized with the hash function you need. What happens when you want to you another search algorithm that wants other templates/arguments/state.
Good information, thanks. [Link for openssl on NuGet, current version is 1.0.1.25](http://www.nuget.org/packages/openssl/)
Thanks for this comment -- I'm in complete agreement. My choice to use GNU Make over CMake is the biggest bikeshed of this submission.
6
Scott Meyers would be the first to admit that he's quite often wrong. In this case though, it's the author of the article that is just plain full of crap. They don't even seem to fully grasp the concepts they're talking about.
It's really great to have libraries like SDL work out of the box, without messing around with Include/Library Directories... unless they don't work (I wasn't able to make it work with [pdcurses](https://www.nuget.org/packages/pdcurses_vc120/)) and I don't see a reason to keep a separate copy of Boost for every personal project.
indeed
I pretty much agree with you. But let's say I _use_ your library in my own project and release my project. Then I have to tell each of my users on Windows "please go to NuGet and download tgockel's library". That's not really cool. Of course you could argue I could copy your sources into my project and then use whatever build system I want, but that's not a good solution either as your sources should stay separate for update reasons and others. If you were using CMake, I could just include your library into mine while keeping it separate and have it working on any OS.
I was thinking that too - not sure bundling CMake is a smart idea, as it updates pretty rapidly. Anyway I hope an external CMake can still be used, in that case all is well :-)
NuGet is integrated into Visual Studio quite well, so it is just a matter of adding this to your `packages.config` (or using the UI to perform the same action): &lt;package id="json-voorhees" version="1.0.0" targetFramework="Native" /&gt; If you're packaging your project with NuGet, then you simply add a `dependency` in your package's Nuspec so that it gets pulled in automatically. If you're deploying using the Windows Installer system, then you simply check a box that says to include the packages your project depends on. &gt; "please go to NuGet and download tgockel's library" I think what you're saying here is you don't want to require users to go through the "get the library from NuGet" step (in the event that you're using neither NuGet nor a Windows Installer). So what is CMake saving you here? It doesn't have the ability to fetch packages for you (unless you count `file(DOWNLOAD ...)`, which I don't). Am I missing something obvious here? The *build system* is only used by people who are building the library. I do not understand why someone *using* the library would care what build system the library uses -- they're looking for the result of the build system.
Some time ago i tried Eclipse and Netbeans, and i had the same problem.
I have had a great experience using KDevelop. I find the code completion and CMake integration indispensable.
Coming from visual studio on windows, I found netbeans pretty decent for c++ on linux 
I like it, but three major issues so far: * The response to typing quite often doesn't seem instant. It's like there was a little bit of a lag while typing (maybe 50ms). I think it happens when Clion is looking up completion options and you continue typing. * CPU consumption is huge, my laptop fan spins up all the time, that makes it kind of unusable. When coding in VS (even with VA:X and other extensions) it doesn't ever need to spin up. * memory consumption is humongous Other than that, great to see some IDE competition coming up!
I've had decent luck with both code:: blocks and eclipse for c++.
Qt Creator. I found out it can replace Visual Studio and Visual Assist for me on Linux. Qt Creator has a team of paid developers (in Berlin) which deliver software [constantly](http://qt-project.org/wiki/Releases). Besides their regular releases, they offer fantastic free support on #qt-creator on freenode.net (note the Berlin CET time zone)
Might want to try CLion if you feel like being bleeding edge.
Not to sound like a pretentious hipster, but try emacs. Yes, it started as a CLI, but there's a gui for it now, and it is truly made for efficiency and speed. It's got a steep learning curve, but you can easily make a new project, makefile, build, and run, all without moving your hands from the home position.
I am personally a big advocate of using vim (or whatever your favourite editor is) and command line tools. Especially if you are just starting out with developing C++ on Linux because you'll learn a lot about the eco system. Especially with clang based plugins like YouCompleteMe. But each to their own.
And it also integrates with cmake. The code completion is also decent for Nvidia Cuda projects.
&gt; all without moving your hands from the home position *After* crooked pinky syndrome (also known as emacsoid arthritis) has set in of course. A small price to pay ;)
Remap caps lock to control. Makes things significantly better.
http://stackoverflow.com/questions/24109/c-ide-for-linux That's really solid advice.
Coming from Windows, I found IDE big helpers in productivity. For example: when debugging. Those who use Vim, do they debug with GDB commands in a separate session? I found Visual Studio editing/debugging integration very good and productive. Maybe Qt offers something similar for Linux...?
Although I do actually use vim, I'm so fucking tired of these pretentious hipsters that use every opportunity to masturbate about how elite and minimalist they are by doing things the hard way.
Yes. Even though it's named QtCreator it's a full-featured C++ IDE supporting console mode apps using whatever library you like. It can also use clang if you wish. I find debugging could be improved a little but it works very well. You can't go wrong with QtCreator.
Tried it - great for Qt development, but personally not a fan of the UI - deviates too far from other IDEs I'm used to
I like Code::Blocks, but its good to be familiar with Vim/Emacs (I prefer Vim personally) so you can do stuff over ssh. 
Basically this. I found that a combination of vim and tmux work the best for me. Things like Code::Blocks were far too cluttered. 
I've been using Qt Creator as my main IDE for several years now. It's cross-platform, and does everything I need to do pretty well.
How do you use vim for anything reasonably complex? You need, stuff like code completion, static checking, integrated debugging, and none of them are available without going through the pain of setting up plugins learning their key bindings etc... This effort would be OK if it paid off greatly later -like it does for editing code in dynamically typed languages-, but it just gets you to a minimally featured IDE-like experience.
I've got Syntastic which is decent for tab completion. I get by because I've not spent a whole lot of time in a 'real' IDE. I like how I can use tmux to split my source side-by-side and I also spent a long time learning keyboard shortcuts for VIM and Tmux. If there were a way to do all that encapsulated in a good IDE, I'd actually spend money on it. 
It has really great integration with cmake and you can use it for anything c++ related. 
Yes! Use QtCreator.
&gt; Those who use Vim, do they debug with GDB commands in a separate session? Generally, yes. The GDB integration scripts I've tried don't work very well or are very fragile. On the other hand, I've found it's a benefit to be familiar with using your debugger outside your IDE anyway. In fact, most of the visual frontends to GDB are painful to use. There are also tools like Valgrind, which have very limited visual tools, and you often *must* use them on the command-line - and tools like Valgrind don't really exist on Windows.
Is there a reason you use tmux instead of things like :split?
scons can burn in hell. 
I didn't say it was good. Just that people use it.
Mostly that I've not taken the time to learn it..
I use sublime with ClangComplete and cmake. Its works quite nicely.
Oh, okay. I actually don't know too much about them either, just :split &lt;filename&gt; and :vsplit &lt;filename&gt;, and some other commands to navigate/close/etc.
There are tools you can use outside the editor. Having a scriptable editor is worth it.
It lets you do more than vim splits (multiple pane/window terminals, copy/paste between terminals sans mouse, saved/scriptable sessions, etc...).
Clion, qtcreator, or emacs. All of which have the added benefit of working cross platform and when used with cmake you have a pretty portable setup.
I'm using QtCreator. It have great vim mode implementation. IDE is very simple and easy to use. It uses some kind of simplified language model for C++, it can't show errors in code (only syntactic error) but parsing is very fast and auto-completion works great. 
if you end up around emacs for c/c++ ,what I strongly advice to look at is https://github.com/Andersbakken/rtags it is really powerful and somewhat easy to setup , I had horrible experiences with cedet.
What's stopping me from using vim as an IDE is that I don't know how to handle multiple files at the same time. Usually I have open .. maybe 10 files that I look at. I'm not editing them all at the same time, but like.. looking how I did something earlier in another file or 2 and then using that to write a new header and .cpp file. Or maybe trying to figure out how something works and then going from a click on a button on a widget through 5 different classes to the source of the data on the widget. I've got no idea how to replicate that workflow with vim. Though I'd like to use that. The guides for vim that I have seen concentrate on small features and not the big picture on using lots of small features together, that would someone to use it as an IDE.
I think it was designed before all the silly Ctrl+something shortcuts. 
Here to suggest VisualGDB. This of course is assuming that you can still use an actual Windows box as your dev box. It is a Visual Studio plugin. If you are used to Visual Studio, it is really really awesome. You'll still have to learn the unix tool chain to set up your code/projects sanely, but once that stuff is done, this is a really slick tool. Allows remote building in a super flexible way that populates the output and error/warning lists using GCC or Clang's output. And it allows you to debug (through SSH) as if you were debugging a windows application with its near-seamless integration of GDB into the Visual Studio UI. This is truly the best part for me because I still am not a GDB master. Works great with our CMake, Clang, Ninja, and G++. Of course this isn't the best option for everyone, and really isn't a 'Linux IDE' per se, but it has been immensely helpful to me and my coworkers who have recently had to port and extend our work to Linux, moving from our historically Win32 roots. And we are working on fairly demanding video game stuff, so I can verify that this tool stands up to the needs of some fairly complex development. Sorry if I'm off topic, just wanted to share because I hadn't seen it mentioned. 
yes, this is how I document function prototypes. I also use `/* implicit */` in front of single-argument constructors and conversion operators 
It looks like this was added two months ago, in Qt Creator 3.3. I'm quite glad to hear it. Of course, this wasn't available when I gave it a go (about 6-9 months ago). Hence, [googling it and only finding stylesheet hacks](http://stackoverflow.com/questions/2244774/qt-creator-color-scheme). You should be more charitable when correcting others.
I know. It's a long-standing joke. I use vim because I'm used to it. Emacs is impressive, but I will probably die a vim user at some point.
What you propbably want is Tagbar (opens a side bar which allows you to jump to function declartions and definitions) and the "gd" shortcut (goes to the definition of the word under the cursor). Also ctags is very helpful. [Information on ctags and vim](http://benoithamelin.tumblr.com/post/15101202004/using-vim-exuberant-ctags-easy-source-navigation)
&gt; It might not be true anymore nowadays, but anytime I hear "Java" and "IDE" I just want to run away very fast. I guess I got branded by Eclipse around 10 years ago. I'm using intellij at work, because after 10 years of Eclipse - I can't stand the daily crashes anymore (esp during debugging). I've also used AppCode, CLion, XCode, MonoDevelop and VS (incl VS J++ when it was around) + played around with netbeans and QtCreator - all I can say is that they all have their share of rough corners that are hard to reconcile. Switching to jetbrain's IDE:s has at least conditioned me to accept their family of flaws. Big win in my book. Java-based IDE:s do have some additional overhead however, making them less than ideal for less powerful computers - especially without tweaking.
You also cannot see what the parameter's intent was because it has no name.
It's been there a lot longer than that, for at least several years now.
Further, there are actually compilers around that encode the parameter names into the mangled function name, which means that adding the name later will break ABI.
&gt; code completion, static checking, integrated debugging Code completion shouldn't be necessary - learn to remember code. Static checking - that's what a compiler is for. Integrated debugging - stop making bugs? Maybe a bit arrogant on my side but I don't really miss even VS's awesome debugger. Might be because the majority of my bugs now are on giant codebases with multiple threads and asynchronous completions, on which a debugger isn't that great anyway.
GDB supports a minimal textual interface called [TUI](https://sourceware.org/gdb/onlinedocs/gdb/TUI.html) which makes working with plain GDB a little easier.
Cevelop anyone? https://www.cevelop.com/
Code completion is handy when you're using libraries you haven't spent a few years learning. For example, I started coding for Qt (in Qt Creator, fairly nice IDE by the way) about two weeks ago, so there are still dozens of classes I've never used. Typing out someObj-&gt;set usually gives you the method you want, if you want to change something about an object. Once you've found that, the display for all overloaded parameter possibilities makes it easy to figure out how to use it, without even hitting F1 to show the method's documentation in the side pane. 
Admittedly I'm coming from Arch Linux already, but imo [msys2](http://sourceforge.net/projects/msys2/) is the best tool available for supporting Windows.
whilt that sounds useful, I've found that reading the documentation/help files is *very* helpful. That's how I learn the limitations, corner-cases and assumptions of the thing being called. For an extreme example, I can't imagine code completion of realloc cluing you in to its quirks
Absolutely, it's not a substitute for documentation. Perhaps a better example would have been using a method that you know, but don't fully remember, e.g.: does QPainter::fill take a QColor or a QBrush (or either)? Besides, as mentioned, pressing F1 will open the class docs and scroll down to/highlight the method/overload under the cursor. Very handy, as what I did previously was to alt+tab, open a new browser tab, Google the class name, click, and ctrl+F for the method name. Even then, that doesn't get you to the correct overload, if there are multiple choice. 
Search for "Unix as an IDE" for some ideas - primarily things to explain why vim and emacs aren't quite as feature-less as some would think. For example, as someone said below, vim + tnux can give you a *lot* of things available in just a few keys (help on realloc: ctrl-b 2 man realloc; static analysis: make check (assuming the Makefile has a check target) )
emacs, QtCreator, KDevelop, Anjuta and maybe Gnome Builder
The accepted answer there is mine, and I wouldn’t change it substantially now, seven years later. I do think that IDEs have some advantages (but then, the answer already says that). In particular, I wouldn’t dream of doing GUI work in Vim. And there’s simply a trade-off of features: for instance, as you’ve already mentioned further up, debugger integration in IDEs is hands down better. On the whole, though, the Vim/Emacs workflow is simply much more powerful for the core part of a software project: editing and managing code, and in my experience (having worked extensively both with IDEs and in the terminal) this offsets the disadvantages. How somebody can read this as a “hurr durr” screed or “hipster masturbation” is beyond me.
There are three things that irk me about this: 1. I fail to see a use for a trait that tells me that something is callable but which I cannot call because I don't know how to call it. 2. Have you considered that callables can have overloaded operator()? Or operator() templated on non-type parameters? 3. None of these problems (overloaded and templated op()) is new. Such callables have existed for years.
Lol... so much people without wicked humor here? Come on, what does Visual Studio offer as an IDE for **C++** development? Refactoring? Not really... that will start with 2015 afair. Code completion might be better with 2013 - I just know 2012. But *IntelliSens* even cannot handle lambda expressions properly! Jump from header to implementation? Depends on the mood of *IntelliSense*... often one ist forced to choose from a long list, if a method is polymorphic. To make a clear statement: I believe that is just **disgracefull** for an IDE that exist since so many years! (That is even not **free**) So come on, somebody really like this piece of software? Of course for C# it works much better - but the context here was C++! So perhaps open your mind, and try other IDEs (for and with C++!) before you downvote, because you feel flamed 😉 Perhaps you are happy with VS, because you don't even know other IDEs and cannot guess, what is possible today?
My comment wasn't directed at you or your prior comment.
I think it was: I’m the author of the accepted linked answer.
why does that matter? ctrl-shift-n to open a new terminal, then "make debug" and voila, you could be sitting at a breakpoint wherever the action is going on in your development, if you have your makefile configured right.
Unless I'm confused, tests 9-12 are _not_ intrinsically race-free and therefore aren't legitimate comparisons for the other tests. Otherwise, the results are roughly as expected. I was surprised that Windows std::mutex is quite the performance disaster that it appears to be tho...
Yeah, I have always assumed/been taught that dereferencing a null pointer is bad. What good could possibly come of it? It has never occured to me that some people see it as valid. 
The windows mutex is not slow because it implements recursive behavior. it is slow because it is shareable between processes. This required an user-kernel mode switch each time the mutex is accessed. Source: http://preshing.com/20111124/always-use-a-lightweight-mutex/ I usually don't use std::mutex in windows, but roll my own based on windows critical sections. This requires only a few lines of code. On other platforms a typedef for std::mutex works fine.
I was going to put that on my list, but I couldn't remember the name of it. Shows you how much I use it.
&gt; I just flat out can't use Vim. Then don't. &gt; Why Oh why can't Vim have a sane default for transitioning people, like ctrl-s for save. Because Ctrl-S (and Ctrl-Q) is historically already used by most *nix terminals for XOFF/XON control. Vim (and Vi, Ex, Ed, and its other predecessors) come from a computing environment that is far older than the silly Apple User Interface Guidelines that defined "Cmd-S means Save" (and Windows and DOS programs defined Ctrl- to replace Cmd- because PCs didn't have that key), and the Unix heritage tends to define what Vi/Vim can allow themselves to do. That said, you can freely remap most key combinations in Vim to do almost everything. You can put this in your .vimrc: nnoremap &lt;C-S&gt; :w&lt;CR&gt; inoremap &lt;C-S&gt; &lt;Esc&gt;:w&lt;CR&gt;i And when you press Ctrl-S, it'll save your file. But then, will you really learn to get into the habit of using the `:w` (**write** unconditionally) or `:up` (**update** file if changed) Ex commands? Probably not. There's `vimtutor` and [a fine book](ftp://ftp.vim.org/pub/vim/doc/book/vimbook-OPL.pdf) (PDF over FTP) that can help you learn how to use Vim quite effectively, but it'll never be an overnight switch - it took me a good month of near-constant use to get used to Vim, and even after 15 years, I still learn new features every day.
&gt;Their plugin architecture allows you to get true autocomplete, and to integrate everything you mentioned, while still being focused on the core objective of providing a better editing experience. ... and therein lies the folly of your ways. Because all you have is a hammer, everything looks like a nail. Much of the time, software development is not editing. There's a myriad of other things to do. Sure, you can cram those things into an editor, but you get a bastard child of everything. You get a DE, not an IDE. DE is fine. Saying that a text editor is a DE, however, is... pffffftttt...
One thing is remembering context between sessions. IDE's keep your breakpoints, conditions and watches between debug-session intact. Not sure if you have that as well. Next thing is that you can see changes visually. Say you have a function called from all kind of places and just want to watch variables on each call. In an IDE you will constantly see all your locals (and watches and function parameters) without having to do anything. And most IDE's will show color-hints whenever a value changes. So all you do is set a breakpoint in your function and run to it over and over while watching in realtime how (and which) variable-values change. Next point is speed of looking at things. In IDE's you go over a variable and it shows you the value. In good IDE's it can even do that with STL types (many fail at that...). Another thing is when you try to find stuff in complex class-hierarchies. You simply browse through them like in a file-browser until you get there. No need to type variables over and over until you find the right one. And you are not forced to evaluate pointers manually (thought I know at least one IDE where you also have to do that explicitly - some are a little too thin wrappers around gdb).
I've gotten along fine without an integrated debugger. Using Vim doesn't mean you can't use a shell and an external debugger session. I, too, would like a good integrated debugger, but I doubt that'll happen until NeoVim catches on.
The real problem here is the right side of the if: if ((interface == NULL) || (podhd == NULL)) Because podhd bas been dereferenced, an optimizing compiler might say "well, if pohd is indeed null, dude's code already died on a previous line; well, either that or podhd is not null; yeah, I 'll go with that, no need to generate code for that second check; yay for smaller code!" On a more serious note, yes, the code is wrong, and there is no reason fo it to be. line6 can and should go under the if, problem solved!
&gt; But, I find things like the search '/', find and replace :%s/foo/bar/g works a lot nicer in Emacs with evil-mode than in Vim. Do you mind expanding on this? I'm curious what's nicer.
What other things do you have during software development that cannot be done better in a terminal? Debugging is wonderful with gdb, and everything else should be as automated as possible.
&gt; You should never dereference nullptr under any circumstances! Well its not always dereferencing a null pointer that is undefined. Its the rvalue to lvalue conversion that is undefined. And that is what [issue 232](http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_active.html#232) discusses. The issue is not resolved because they didn't want to add wording to the standard that would essentially allow for null references(eg an empty lvalue). However, other issues such as [issue 315](http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_closed.html#315) which allows calling member function through a null pointer, have been resolved and relies on the rational of issue 232. Of course, neither of these issues are related to the one in the blog. It **is** undefined behavior to access member variables though a null pointer. There is no grey area in this regard.
Except that MSVC's std::mutex doesn't use the Win32 MUTEX (because that would be crazy). If you have VS2013 installed you can just look at the source code of the CRT to see what it's doing. Under the hood it looks like MSVC's std::mutex calls into Concurrency::critical_section (which is part of ConcRT). I've heard that in VC14, MSVC's STL will stop using ConcRT but I'll leave that for /u/STL to confirm/deny that.
It's not always bad. It's not always undefined. C/C++ doesn't make any assumptions about the underlying addressing scheme. On some platforms, the entire address space, including 0 might be addressable and readable. For example on older x86 architecture, if you wanted to read the IDT in real mode you could read address 0000:0000 and it was perfectly valid to do so. If you want nullable pointers and portable-ish code then you should just flag a pointer as null instead of setting it to some value like 0 or -1 which could be a valid address on another platform.
cmake is indeed nice. However, sometimes make will suffice. Also, you should check out [gradle](https://gradle.org/). [Ninja](https://martine.github.io/ninja/) is nice as well.
Being able to apply a tuple is cool, but the version shown here rather obfuscates the problem. I prefer the implementation in [N3915 (std::apply)](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3915.pdf), using std::integer_sequence instead of hand-rolled recursion.
I use CodeLite for a big production software. - Cross platform (win, linux, osx) - Good WxWidgets integration - Very fast - Good integration with versioning system
That is a much nicer solution. In the comments for the blog entry, 'FOONATHAN' links to this nicer solution http://ideone.com/lUbOFA which unfortunately the VS2015 CTP doesn't seem to like (another compiler bug?) but clearly GCC does.
If you are looking for a build system that is more like what you would find in Java land, you are looking for [biicode](http://www.biicode.com/) It's more like maven in that it's a dependency manager and a build system. I actually prefer biicode, which generates cmake files, which I have configured to generate ninja build files. It's still a beta product though.
Of course, but the C standard doesn't completely eliminate the possibility of a conflict. That's up to you, the developer. Which is my point.
I like QtCreator. KDevelop is also really nice, but a little more resource heavy on my laptop.
Shake (done in Haskell) is pretty serious and advanced at what it does. It has a very rigorously conceived logical rule framework/EDSL + leverages Haskell's type system which is very powerful/expressive. And unlike CMake and Waf, **it isn't a Hidden Object State Machine that one has to read the f'ing source of to just understand.** The only caveat is... learning Haskell. Some links: - http://shakebuild.com/ - http://shakebuild.com/manual - https://github.com/jfeltz/shake-cpp Note also, Shake doesn't really provide a set of system lib dependency management tools ala CMake. But if your build isn't public-used then it doesn't matter that much anyway. Plus you can always bolt that on later (it isn't that hard to poll pkg-config etc in Haskell).
Oh, the first example wasn't of the regex... More just the fancy highlighting :P (I think that's basically achievable via :set hlsearch incsearch though). I know none of this is unique to evil-mode. Rather, the fact that these things were enabled by default just made me aware of the usefulness of such features. While I am all for configuration (if I wasn't I would not be an Emacs user of course), I also do believe to some degree in the notion of "sane defaults" which try to be beneficial to most users. I only look for new features when I think I need them, so I admittedly don't get the most out of my tools. But I figure I'm not unique in that regard either, and having the tools be easily "discoverable" was really the difference for me in this case.
If it helps, here's a [link](https://github.com/uxcn/dot-files/blob/master/.tmux.conf) to my `tmux.conf`. I think it's recent.
It is because CEDET tried to solve the problem in large scale, and it blocks Emacs. For small scale stuff (such as parse a file less than 10k lines), it works wonderfully. I created a refactoring tool with it: [Semantic Refactor](https://github.com/tuhdo/semantic-refactor); you can see [the demos](https://github.com/tuhdo/semantic-refactor/blob/master/srefactor-demos/demos.org). In the long term, I planned to add a clang indexer to index large code base and use CEDET's built-in parser for new code typed from the keyboard, and a database engine such as Sqlite for managing a database of tags of a project as a whole.
What makes ConcRT so problematic? Is it just that it choose different semantics than the C++ standard so you have a bunch of awkwardness?
I know that was not the point of the tutorial but it is in complete violation of LSP. Because of tutorials like this we have a lot of bad C++ code (especially from beginners).
&gt; But it's always difficult to find new systems Seriously, making a really cross-platform build tool is not something achieved in the course of the night. Thanks to CMake I build a huge (6 libraries, tons of plug-ins, Max/MSP externals on Mac/Windows, commandline app and desktop app in Qt) codebase on Mac/Windows/Linux/iOS/Android
I never meant to imply that it was. But at the same time, the list of build tools that I can find today is pretty much exactly the same as it was several years ago...
I've ran into enough cases, esp. with bugged external and internal project dependency management, e.g. FindBoost and CTest, to conclude otherwise. But in some limited scenarios, one can just simply read the docs and, no offense intended, muddle through. I'll agree on that.
Whilst I certainly agree with that, my fear is that it could have meant that because we had Autotools that was technically Good Enough, it might have meant that we never got CMake at all. Equally just because we have some build systems now that are Good Enough, and yes getting better all the time, it doesn't necessarily mean there shouldn't be new ones that work in a different way... Having said that - I really need to go and look at CMake again, especially after your links below. In particular, AFAIK the CXX_STANDARD property didn't exist last time I used it properly...
I like scons, so will definitely have a look at cuppa :)
There is also a small and not widely known tool Mxx_ru. It is something like SCons but in Ruby. Mxx_ru allows to define properties of a project in simple Ruby eDSL and Mxx_ru takes the rest. It is distributed like RubyGem and could be installed just like *gem install Mxx_ru*. The project is not activelly developed now but the latest documentation could be found [here](https://sourceforge.net/projects/mxxru/files/Mxx_ru%201.6/)
Thanks. Its not so into macro magic, which where used just to underline SQL resemblance. If you invent more short names for options, add short function from() which will just make tuple from arguments, you can already use it like: select(to_fn([](int x,int y){ return x + y; }), from(ints, ints), to_fn([](int x, int y){ return x+y&lt;10; }), limit(10)); and its possible to get rid of to_fn() also.
I hate macros.
Turns out the reason I'd missed CXX_STANDARD is because it doesn't work on OS X. Support for that is being added to CMake 3.2, but isn't there yet...
Everyone does. Using C++11 template features you can replace a lot of old macro stuff, but, unfortunately, not all. 
why do you use company-mode when you have rtags?
I don't know, it works so I never looked other things after.
Haha fair enough. I had thought it was just a code indexer, so I had only been using it as such. I shall take a closer look after work.
My problem with rtags is that for Make/Automake project, I have to build upfront before using any of its feature, and I have to wrap my compilers inside a shell script, which is undesirable. Making it works on Linux requires a fair amount of configuration, and I'm not sure whether I can make it works on Windows to help Emacs users on Windows.
the documentation part is really true, but in the end I managed to set it up just to see it breaking from time to time and maybe it was totally my fault (cedet has some conflicts with emacs24 default installation, and maybe I complicated the config too much and things was not set up correctly). for clang there are project like https://github.com/Golevka/emacs-clang-complete-async that if I remember correctly worked pretty well, why not using that just only for that purpose and cedet for other things you need?
yes this could be not handy in some situation where you can't use cmake or autotools.
I think its a bad idea to make `SELECT`, `WHERE`, and `FROM` macros. Instead it should just be tokens that are parsed by the preprocessor. That is how it is done in [Linq](https://github.com/pfultz2/Linq). So, I can just write this: auto q = LINQ( from(z, boost::irange(1, std::numerical_limits::max())) from(x, boost::irange(1, z)) from(y, boost::irange(x, z)) where(x*x + y*y == z*z) select(boost::fusion::make_vector(x, y, z)) ); for(auto&amp;&amp; x:q | linq::take(10)) std::cout &lt;&lt; x &lt;&lt; std::endl; And I don't need `SELECT` or `WHERE` macros.
It is already integrated with Projectile. If you use Projectile, the option `(Projectile file)` will be available for you to choose project file to perform refactoring action. For example, you can choose any file in your project to move your semantic unit into, or choose any project file to generate class implementation.
Yes, Paul, you can use LINQ/boost macro, but thats a bit different thing. Starting from the point that select is at the end, instead of the beginning... But main point was that macros are optional in my solution. May be i should edit the post to show it more obvious way... 
Pretty interesting interview -- with some really good in-depth questions! A few points that stuck out for me: - the use of `unsigned int` vs. `int` (models a different concept!) in a functional interface design - new language features (in particular `auto`) vs. basic language features (Alex: "I try to be very conservative in using new language features. Since programs will be read by other people, and many people know only the basic language features, I tend to stick to these basic features.") - regular types (in particular: is the default-constructible requirement truly essential?) vs. non-regular types - memory allocators (Alex: "Allocators were a terrible idea")
ok thanks a lot, that tutorial is really big tho, and has various options for doing the same thing, could you recap your setup now (plugins and what are used for)? Also the ctags db is built automatically or you have to build that manually upfront? that clang autocomplete mode was handy because that was automatic. edit: what's that eval: START that I see in every example before you start giving help commands?
rtags definitely requires some work to get up and running. Have you tried xcscope? *that* is like no work to set up.
I'm planning to reorganize it to be more compact and one main option for doing one thing. As for ctags/gtags, you need to build the database for **navigation** (jump to definition/references, not just completion) in large project. After the tag database is built, it is updated automatically when you save a file if you use ggtags/helm-gtags. Not sure if `rtags` can handle projects with over 10k files or more for navigation?
I was told from a guy that has a really *big* project that rtags was really fast and worked really well, but was pretty demanding in terms of memory resources, but I suppose that the rdm daemon could also be shared in a different place and shared among people working on the same project if that is really big.
A good carpenter will make good table without electricity, sure. But he'll make two with it.
There are several use cases, they just happen to be in advanced library code. Within the STL, we use integer sequences for tuple_cat(), pair's piecewise constructor, and invoke().
It depends on how "big" our definition. To me, big means the project at least the size of LLVM or Linux kernel, and I must be able to jump to any definition/references without any delay, and easy to use. GNU Global with ggtags/helm-gtags works fantastically with these constraints and since the tag database is on disk and is queried when necessary, there's no memory consumption. Since rtags needs to do perform more complicated operations, it needs the data in memory. The primary difference between Rtags and GNU Global is that rtags keeps data in memory and GNU Global keeps it data on disk and retrieve only when necessary. It would be nice if in the future Rtags can generate its own database to cache for subsequent usages to reduce memory consumption and reparsing.
From my experience as both developer and packager (FreeBSD ports committer), CMake is the best build system at the moment. It's simple to use yet powerful, it is actually capable of building most projects out of box in environments different from author's, it respects user-set build settings such as c/c++ compilers and their flags, and even if build script needs to be changed, it's usually fixable with small clean patch (likely upstreamable). Autotools are legacy - though they mostly do their job of software building, they are a real pain to maintain and a nightmare to fix when something goes wrong. Make is fundamental, both CMake and Autotools use it, however standalone it's only usable for very simple projects (e.g. no external depends, no configuration), most people can not write correct Makefile at the first try. SCons is pure evil ([debian](https://wiki.debian.org/UpstreamGuide#SCons) and [gentoo](http://wiki.gentoo.org/wiki/SCons#Why_you_should_NOT_use_SCons_in_your_project) list most shortcomings) and less common systems are not really worth mentioning.
True, I was only considering the simple "you created a class that wasn't meant to be inherited from" case mentioned in the article.
NeoVim is supposed to, so we'll see how that goes.
Yeah... but... SourceForge? Can I get some ads and a stupid shitty downloader along with my code please?
The talk ~~is from a guy of the BiiCode~~ package manager [it isn't]. One thing I learnt, is that it was based in boolean satisfiability, and I made a clunky reasoning: debian's apt was a positive advancement when it appeared, apt is also based in boolean satisfaction for dependency resolution, hence ~~bii~~ [no, ryppl] can share some of apt's cleverness. In fact there is a need of a c++ cousin of maven. EDIT: corrections.
rtags is not bad at all to setup. but emacs in general is a bit more complex to setup for C++ coding than Visual Studio. Except emacs works of course.
Wrapping the compiler is not so bad though, is it? I just do $ cat &gt;my_compiler.sh rc --compile "clang++ $@" clang++ $@ ^D and then: make CC=my_compiler.sh 
Just for example: I regulary download MinGW-w64 distributives from SourceForge. I think the viewing ads for 5 or 10 seconds is nothing in comparison with the usefulness of the downloaded project. Anyway I don't want to discuss hosting and/or VCS used for development of the tool. SF/GitHub/Bitbucket/GoogleCode/etc and Svn/Git/Mercurial/Perforce/etc are just tools. A programmer can choose one it liked the most. And the last but not least: it is a free and OpenSource library. If you don't like something about it just don't use it.
My bad, thanks, I saw Daniel mentioned in a bii blog post and and wrongly took him as from that project. Nice to see work in this area wherever it comes from.
`std::mutex`, IIRC, *cannot* be implemented using `CRITICAL_SECTION` on Windows. ~~I suspect it is either using Semaphores or Mutex objects, both of which mandate syscalls.~~ https://social.msdn.microsoft.com/Forums/vstudio/en-US/3a9387ed-f15f-4b80-944e-180f41980a2c/stdmutex-performance-compared-to-win32-mutex-or-critical-section-apis?forum=vcgeneral It is actually using `Concurrency::critical_section`, which is far more complex than `CRITICAL_SECTION`.
Enjoyed this, thanks for sharing
Wish I could downvote more than once. Dont bother posting this kind of crap, it helps noone. Thanks. Edit: except first sentence
It is interesting that the problem and requirements described in this speak is exactly the same as what lead to the the Wrap dependency manager ([link one](https://groups.google.com/forum/#!topic/mesonbuild/FIbXjMic7kY), [link two](https://groups.google.com/forum/#!topic/mesonbuild/--8HfnKl2fY)) of the [Meson build system](https://github.com/jpakkane/meson) (full disclosure: made by me). Specifically: * works on Linux/OSX/iPhone/Android/Windows/etc * use native packages when possible * automatically download everything so the users don't have to * available today 
What I recall is that you wrote a file describing declaratively your java dependencies, and invoking maven it automagically downloaded all of them and they were transparently incorporated to your eclipse projects.
3.2 release candidate was released last week, so can start testing the feature now. 
What's wrong with 7zip? Seriously asking, I use it every day.
It gives you a constructed defined mechanism to test the pointer and what its pointing to. Allows you to define a platform independent mechanism for handling platform specific behavior or define your own since in this case its undefined. 
So the *downside* of removing the `std::nullptr_t` overload of `value` constructors is there is a `value` constructor that takes a `const char*`, which will be selected if someone says `value(nullptr)`, which causes a `std::logic_error`. More annoyingly, code like `array({ "is this null:", nullptr })` will fail at runtime. The solution I went with is to delete the `nullptr_t` overload...I'm not totally convinced that's the best option, but it's better than doing nothing.
Sure, but not supporting a number of C++11 features isolates my library from being able to work in VS2013. :-)
I think the issue is that C++ got used very widely and there was a lot of variety between early systems. There was no 'standard' environment early on for tools to standardize on the way there was with Java.
[Tup](http://gittup.org/tup/), nothing else.
Much better modern C++ example at http://en.cppreference.com/w/cpp/algorithm/set_union You can actually execute the code. The code uses the standard library algorithms. Also, if you want the implementation, it also gives an example implementation. Also, looked at the website and this is horrible C++. I clicked on the Home button and I saw this code char str[100]; int w=1; cout&lt;&lt;"Please enter a String: "; gets(str); It is because of code like this that C++ has a bad reputation. This is **not** how to do C++ in 2015. If you are the owner of the cppcodes.com website, you are promulgating bad C++. The C++ you were taught in school is bad and outdated. Please read the books from the link on the sidebar (http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and learn Modern C++ before passing on bad information. This is to the website owner: I am happy that you are excited about C++. If you love C++, you will absolutely love C++11/14, because it makes C++ more usable, more teachable, and safer. I would recommend that you forget all that you learned about C++ in college, get Programming: Principles and Practice Using C++ by Bjarne Stroustrup (the inventor of C++) and start at the beginning and work through the book.
This is not how to do C++ in **1998**. I wish people would stop referring to "old as heck C++" as "modern C++".
Thank you so much for this valuable information, I'm C++ developer and we still use C++99, even in big projects. I know you are absolutely right I should read that book but i work for 10 hours in my company and rest of the time I write codes for basic C++ programs for high school programming level. I guess the problem is I'm from India.
I really wish allocators never existed... They must just serve to make our template error messages more complicated...
If it was good enough for Alexandrescu ... :) More seriously, I agree. Using standard library algorithms and containers instead of raw arrays was best practice in C++98. If people would just write good C++98, a lot of security and memory management issues would just disappear. C++11/14 just makes it easier.
*How does it handle unavailability of recipient actors?* Afraid I don't understand this question. Could you explain it, please? *How long are the messages buffered?* Until someone gets them from the queue. *Is there a way to control these QoS settings?* There is no ready-use-tools for that out-of-box. But we have developed a pattern of collector-performer agent pair. Collector collects messages in his local queue and resends them to performer when performer is ready for processing them. But user have to implement this pair by himself. There are few examples in SObjectizer distribution which show how this can be done: [collector_performer_pair](http://svn.code.sf.net/p/sobjectizer/repo/tags/so_5/5.5.3/dev/sample/so_5/collector_performer_pair/main.cpp), [collector_many_performers](http://svn.code.sf.net/p/sobjectizer/repo/tags/so_5/5.5.3/dev/sample/so_5/collector_many_performers/main.cpp) and [simple_message_deadline](http://svn.code.sf.net/p/sobjectizer/repo/tags/so_5/5.5.3/dev/sample/so_5/simple_message_deadline/main.cpp) (this one also shows how application-specific message deadlines can be handled). This samples are not described in the project's Wiki yet so feel free to ask additional questions. (I hope we will add those descriptions in the near future).
*When I mean unavailability I am talking an it an actor going offline.* There is no such thing like "actor going offline". Actor could be deregistered (i.g. removed from the Environment). In this case all the agent's subscriptions will be dropped and new messages will not be sent to the agent anymore. There also could be the case when actor hungs forever (as a consequence of an error, for example). It means that actor doesn't return control to the dispatcher. In this case SObjectizer can't do anything. And messages will be stored in the queue. And queue will grow. *Is there a limit at which it starts dropping any new messages?* No, the current version has no such functionality. There were some discussion in the past about it but we had no found a consensus in this question. So this problem is still open. There are plans to add a possibility to tune types of working queues for agents. It means that user can choose size-limited queue instead of default unlimited one. But we need some clarification on this topic. So I don't say when it will be implemented.
indeed, my SPI, I2C, etc. abstractions are all class templates too. The template parameters are the GPIO pins, but also the timing serivice (which can be busy waiting, timer-based, multithreaded). Writing in this style is surprisingly clear. Such compile-time polymorphism is nice in some situations, but in others run-time objects are better. Think of SPI: the clock and data should be CT-determined for speed, but CS should probably be RT-determined to avoid code duplication. It is not difficult to derive a RT-object from a CT-'object', but writing a SOI libarry that accepts all mixtures of RT and CT object with one syntax is not easy. Fortunately, compilers get better at devirtualization, so I have some hope to find a way of writing library code that accepts both RT and CT objects without duplication.
Do you think we gave you these criticisms/suggestions for our own benefits??
I think everything is very simple: if you want to known something about the tool just ask. If you don't like the hosting, VCS and 7z-files just don't look at this tool at all.
So like... C++ now should be heading towards the museum?
&gt; Each of member variables were properly initialized **before** our constructor was called. To be precise, they are initialized after the constructor is called but before execution reaches the first statement inside the constructor body. It behaves the same as if the initializer appeared in the ctor-initializer list (if the ctor-initializer list doesn't contain an entry for that member already). 
CXX_STANDARD should work with OSX, if the 3.2 Release Candidate doesn't work you should submit a bug report. Edit: The following works for me: project(example CXX) cmake_minimum_required(VERSION 3.1.0 FATAL_ERROR) add_executable(prog main.cxx) set_property(TARGET prog PROPERTY CXX_STANDARD 11) 
No, but it has played a fundamental role in computers for the last 30 years.
I met Stroustrup while he was teaching at A&amp;M. At the time I wish I could have formulated more questions to ask him. His lecture was pretty interesting and his c++ book was a pretty good source for learning and programming.
&gt; There is the added expense of calling the copy constructor to create the returned object so than the calling function can use this copy of the object, but the extra expense is unavoidable. RVO, man. How can you talk about returning objects without mentioning things like RVO, common mistakes that people make (like `return std::move(obj);`) or lengthening lifetime of function-local object (`Foo getFoo(); const Foo&amp; lengthened = getFoo();`)? It's quite deep topic, really.
Ad. "N4314 - Data-Invariant Functions": I would like to see type deduction from constructor invocation, eg.: std::array&lt;auto, 3&gt; a = {1,2,3}; My other wishes: - constexpr specifier on function arguments, ability to write: `foo(1,2);` instead of: `foo&lt;1,2&gt;();` - user defined literals with arguments passed via template arguments or with constexpr specifier - foreach loop over tuple elements: `for(auto e : tuple){}` with its consequences: objects' type mutation - static introspection: `for(auto f : std::get_fields&lt;Foo&gt;()){}` - integer ranges: `for(i : irange(0,N)){}`
Well I'll be... I was all set to prove that it doesn't work, and in the end it turns out that it does... Thanks for clearing that up :) [For reference - I'd typo'd in my CXX_STANDARD in my test and it doesn't error on that. It just ignores it...]
&gt; Picking up the older features first is a good idea. The new stuff isn't really critical. Most of the new stuff was made to address shortcommings or missing features in the old stuff. You should start with the most modern stuff you (feasibly) can. 
Only the pre-C++11 parts. C++ is currently reinventing it self, C++17 will offer Concepts, bringing generic programming to a whole new level. Modules will follow.
&gt; type deduction from constructor invocation Well, they're adding std::make_array which solves your example more elegantly. I don't know if they'll make the change you're talking about, but it wouldn't be in the library papers. &gt; user defined literals with arguments passed via template arguments or with constexpr specifier I'm not sure what you're looking for. You can define a constexpr UDL now. C++17 is, IIRC, going to also relax template constants to include floating-point &amp; strings. &gt; foreach loop over tuple elements: This one is trickier but could be neat. I'm not sure if there's any papers tackling that. &gt; static introspection: I believe this one is being tackled for C++17 as a language feature. n3996 &gt; integer ranges: This is Eric Neibler's effort. It's currently scheduled as a post-C++17 TS to make sure he has time to get it right.
Yes. My plan was to work backwards from VS2015 until the workarounds get too absurd to handle.
That's where vim shines. vim has at least three ways of handling multiple files I use: 1. "windows" - press `Ctrl-W, v` or `Ctrl-W, s` and you get vertical/horizontal split. Each split may be split too, so you get a lot of tiles visible at the same time. Use `Ctrl-W, h/j/k/l` to move focus left/down/up/right or `Ctrl-W, w` to switch quickly between two windows. It is a good way to see any number of files (or even any parts of the same file!) at once. I often miss this in IDEs (though QtCreator can do that, but e.g. Eclipse cannot). Use `:q` or `Ctrl-W, q` to delete a "window" (pane). 2. "tabs" - when you want to have a lot of files, but want to use the whole screen estate for one of them, do a new "tab": `:tabedit yourfile.cpp&lt;CR&gt;` will open a new "tab". Use `gt`/`gT` to switch forth/back. 3. "buffers" - this is the least visual way of handling the content, but also useful sometimes. Each opened file is saved in a list of buffers, that you can see with `:buffers&lt;CR&gt;`. You can choose a buffer by its shown number (e.g. 12) with `:buffer 12&lt;CR&gt;` or (a super convenient feature!) you can search an opened buffer by typing any part of its name after `:buffer ` and pressing `Tab` again and again until the needed is found. So, if I have a lot of opened files and I want to go to one, I can type only a part of its name to search for it. 
&gt; The answer is of course that there's no special rule here That is the very definition of a special rule - init order is mandated to follow declaration order instead of mem-init-list order, since there's only one declaration order but there can be lots of mem-init-lists, and the dtor destroys in reverse declaration order.
I love waf. Pure python, simple API, and good documentation. https://code.google.com/p/waf/ 
addTo is a pointer but it doesn't point to anything, so dereferencing a member will crash. ListElement* addTo = new ListElement();
This line here: ListElement *addTo = { }; should result in addTo being equal to nullptr, so the addTo-&gt;token is an invalid dereference.
&gt;ListElement *addTo = { }; What you mean to say is: create a default ListElement, and get the pointer to it. What happens instead is you are creating a pointer (just the address), and creating a default pointer. This address doesn't have a ListElement there -- it's an address in memory that doesn't have a ListElement there. what you want is: &gt;ListElement *addTo = new ListElement(); Be cautious though, because this is a memory leak. To clean up your leak, you'll need to add a loop in your list's destructor to call 'delete' on every node you created. One of the accepted solutions here is to use a managed pointer -- either std::shared_ptr or std::unique_ptr. Good luck, do your research, and don't give up!
The assignment is d = 4. So d is 4. So the if is if(!4) which is always false. Only !0 is true, any number other than 0 will be false. if it was if(d) any number other than 0 will be true. If(!d) is the opposite so any number other than 0 will be false. 
Having `auto a = { 42 };` or `auto a { 42 };` do anything other than deduce `a` to `int` is just stupid, IMHO. 
Super cool. Thank you!
&gt; Just to recap, SFINAE stands for Signal Failure Is Not An Error. No.
*facepalm* -- right, Substitution. Thanks for pointing that out. Fixed.
Sorry about this tutorial guys - looked at it in more detail and it is quite poor. Wasn't mine, just for the record...
To prevent this kind of mistake, there is [Yoda conditions](https://en.wikipedia.org/wiki/Yoda_conditions). Check this out.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Yoda conditions**](https://en.wikipedia.org/wiki/Yoda%20conditions): [](#sfw) --- &gt; &gt;In [programming](https://en.wikipedia.org/wiki/Computer_programming) [jargon](https://en.wikipedia.org/wiki/Jargon), __Yoda conditions__ (also called *Yoda notation*) is a [programming style](https://en.wikipedia.org/wiki/Programming_style) where the two parts of an expression are reversed from the typical order in a [conditional statement](https://en.wikipedia.org/wiki/Conditional_(computer_programming\)). A yoda condition places the constant portion of the expression on the left side of the conditional statement. The name for this programming style is derived from the *[Star Wars](https://en.wikipedia.org/wiki/Star_Wars)* character named [Yoda](https://en.wikipedia.org/wiki/Yoda), who spoke English in a non-standard syntax. &gt;Yoda conditions are part of the [WordPress](https://en.wikipedia.org/wiki/WordPress) coding standards. &gt; --- ^Interesting: [^Conditional ^\(computer ^programming)](https://en.wikipedia.org/wiki/Conditional_\(computer_programming\)) ^| [^Sansho ^the ^Bailiff](https://en.wikipedia.org/wiki/Sansho_the_Bailiff) ^| [^Mace ^Windu](https://en.wikipedia.org/wiki/Mace_Windu) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cor39qf) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cor39qf)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
How would you get an `initializer_list` with a single element in it, then? And why would `{42}` behaving differently from `{42,7}` make sense? This is exactly the sort of vagarity that braced initializers set out to solve in the first place: `std::vector&lt;int&gt; a{10, 0}; // two elements, "10" and "0"` `std::vector&lt;int&gt; a(10, 0); // ten elements initialized to "0"` And we're already trying to fuck it up. I think N3922 is overly confusing. C++ is complicated enough and has enough dark corners with spike pits in them without having to remember whether to put the braced initializer on one side of the equals or the other.
`std::initializer_list&lt;int&gt;{42}`. And that different behaviour already occurs in many places so we expect it. Actually I'd have `{42, 7}` be illegal in this context. Generally you shouldn't be declaring local initializer lists anyway, they're designed to be used directly in function calls.
Is the Preview still using ConcRT?
I'd have to do source control archaeology, or install Preview in a VM, to check. Pretty sure it's in CTP 5. (Sorry for being lazy, watching the next episode of The Walking Dead sounds more fun right now than logging into corpnet and digging through TFS. :-&gt;) I am constantly aware of what's happening in WCFB01, the compiler/STL branch that I work in. I usually know when stuff flows into WinC, the parent branch for VC. But I generally don't keep track of what happens in Main, the branch for all of VS, much less the various branches that Preview/CTP/etc. releases are built from.
I can do almost everything with templates, but their syntax is ugly, and as a result final code is 'write only'. I guess that's why constexpr functions were introduced. Constexpr is so weak that all functions may be constexpr by default. I need something more. Actually I have to create dummy variables to check if my constexpr function can be really evaluated at compile time. But I still have to use templates if I want to use traits or do some type deduction based on arguments. &gt; I'm not sure what you're looking for. You can define a constexpr UDL now. C++17 is, IIRC, going to also relax template constants to include floating-point &amp; strings. Look at possible declarations at this page: http://en.cppreference.com/w/cpp/language/user_literal and tell me why almost all arguments are passed as runtime values instead of template(really 'constexpr') arguments? Literal passed as variable, really? 
regex_iterator and regex_token_iterator are enormously powerful. However, a word of caution - do not write regexes where different capture groups could match the same text *and* you care which capture group it is. That is the road to insanity, and I say this as a Standard Library maintainer. The ECMAScript standard makes the Necronomicon look like The Cat In The Hat. (In this case, (int|short|long) and [a-zA-Z]+ can match the same text.)
Please provide a self-contained test case.
Make a really small, but functioning, code example.
And here I am wishing I had a bit more time to add features and fixes!
Yes, per /u/Rapptz - my use case is something like: bool something_changed = false; something_changed &amp;&amp;= do_something(); // true if did something something_changed &amp;&amp;= do_something_else(); // ditto return something_changed; You don't want to not do the somethings, just accumulate whether there was any change. Plus, just for symmetry with &amp;=, |=, and indeed, all the other symmetric binary operators.
This is *almost* what I had been doing before, but I'd really like to do things like have "==" be it's own token without having to strip out "=" then "=", notice there's two single "=" tokens and re-turn that into an "=="; it seems a bit wasteful that we would have to do that with "symbols" but not with alphanumeric identifiers. I appreciate the suggestion, and that route is still more or less on the table, but I'm curious if I'm just missing something on the regular expression side that can solve this without having to make more non-regex splits.
Don't push the closing parenthesis.
Just to be sure: SLOC is a terrible, terrible metric for pretty much anything than rough rule-of-thumb measurements (A seems to be about twice as big as B). This is especially true of C++ which allows you to write essentially the same functionality with wildly differing amounts of code. Writing 50k lines of code in two months most likely means that the code contains copypasting and/or not much effort has been made to put it unify common components. A better question (without which the 50k number is meaningless) is how good is the produced code: what is the test coverage, how many bugs does it have, how much resources does it use, what percentage of requirements does it fill? If you ever have a job where you are expected to produce 50k lines of code in two months, the only reasonable choice is to get the hell away from there as fast as possible. In fact you should bail for any LOC requirement because any company that measures programmer productivity by lines of code is irrevocably broken.
They judge us based on how the code performs. I was just noticing he produced a lot, and started looking in to it on my own. I found SLOC on Wikipedia, to be honest.
&gt; It's super bad. It has 1-second granularity, and makes identical seeding (across different processes, computers, etc.) especially likely. random_device is a simpler source of a non-deterministic 32-bit integer, and (with a bit more work) it can be used to fill the whole state of an mt19937. Well, if you're using rand for cryptography, you're going to have a lot more troubles than just a bad seed. And if you're not, you shouldn't probably care. People happily use pseudo-randomness for games and such, without any issues. Does it really matter that if I start a minesweeper game at the same moment as my friend across the ocean, we happen to get the same grid? I don't think so.
No way, man! We don't code like that. There's no documentation anywhere.
A lexer will do what you want, as it takes precendence into account for you. Google lexertl and download the zip (alternatively get the source from github). Under the "extras" directory there is a file cpp.cpp which implements a c++ lexer based on the lexer spec from boost::wave. If you have any questions I am happy to help. #include "lexertl/generator.hpp" #include "lexertl/iterator.hpp" lexertl::rules rules; lexertl::state_machine sm; build_cpp(rules); lexertl::generator::build(rules, sm); // Your example string: std::string s = "long longa=5;"; lexertl::siterator iter(s.begin(), s.end(), sm); lexertl::siterator end; for (; iter != end; ++iter) { std::cout &lt;&lt; "Token: '" &lt;&lt; iter-&gt;str() &lt;&lt; "', ID: " &lt;&lt; iter-&gt;id &lt;&lt; '\n'; }
I do agree with your position on this, `auto`'s type deduction should behave like template's, which won't deduce a type for `std::initializer_list`. The only problem I can see with this is that the wording for `range-based for` would be broken as it is because it is defined as equivalent to a piece of code that depends on `auto` deducing `std::initializer_list`.
Thanks, I'll check it out.
No, disambiguation in the case `=` vs `==` is handled by the leftmost longest rule (normally you would list longer tokens after smaller ones). Rule ordering comes into play when you have keywords and an identifier rule (for example). In that case keywords come first and generalised rules come last.
Hell, who needs functions when you have copy-paste right?
That's more than 1000 lines/day. Most likely some serious copypasta addict, liberal user of vertical space, or some such. If at all true of course. Alternatively, you met a unicorn. (Highly improbable though.) But hey... have a look at guy's commits and see what he does.
best explained [here](http://stackoverflow.com/a/24112395/1505939)
Seems like everything is C++ and he only uses it to access C libraries. 
IIRC neither of those were in Stepanov's STL. I think std::exception might have been, but I'm not sure.
Example 2 of "memory corruption" requires e.g compiler help. Compiler has to add padding around each stack variable, fill that padding with something funny and check that padding stayed funny when leaving the function. Compilers do that. BTW, C++11 is mentioned way too little and is largely unrelated to the rest of the article.
I assume *real* programmers refactor using sed. The rest of us must look like chumps with our encapsulation and our DRY.
Still waiting for it to land in Android Studio, unless Google feels like deprecating the NDK, given the attention span.
See this post for more information: https://www.reddit.com/r/cpp/comments/2wnjyd/recursion/
No one here is going to do your homework for you. Go to /r/cpp_questions or /r/learnprogramming and post the code that you're written so far and ask specific questions. Posting the text of the question and nothing else is not appropriate. 
There were two things keeping me away from CMake, the obtuse syntax, and the fact that it is rather difficult to handle projects with many subdirectories. 
Debug your program by using CTRL + F5
Calling convention can be forced upon a non capturing lambda by converting it to a function pointer.
Look here: https://blogs.msdn.com/b/vcblog/archive/2011/09/12/10209291.aspx &gt; "The closure type for a lambda-expression with no lambda-capture has a public non-virtual non-explicit const conversion function to pointer to function having the same parameter and return types as the closure type’s function call operator. The value returned by this conversion function shall be the address of a function that, when invoked, has the same effect as invoking the closure type’s function call operator."
Yes, in the specific case of converting a lambda to a function pointer. I'm talking about the statement that you can use a cast to affect a function pointer in the general case. 
thx reworded my answer, casting a pointer in this case is of course invalid!
Has to be a win32 console too if I recall??
I am not a windows developer and I am curious.. do people *really* bother about calling convention? Is is it common to achieve better performance by tuning them? Is there any point to use them beside binary compatibility?
`initializer_list&lt;T&gt;` is a thinly veiled wrapper for array that gives it value semantics. 
but it loses the size :(
You're running with debugging on, as others have noted. You can run without debugging (Ctrl+F5), or you can actually use the debugger to do your troubleshooting and set break points, or if it's a console program you can open a console (cmd.exe) and run it from there (as console programs are typically intended). If you search around you'll see people recommend `system("pause")` or `cin.get()` or `getchar()` or some such. While it will keep the window open, I do not recommend getting in the habit of writing code to work around your tools. Learn to use the tools.
Interestingly enough, something came up recently for converting capturing lambdas to a pointer to function: http://codereview.stackexchange.com/questions/79612/c-ifying-a-capturing-lambda
There is __vectorcall, which allows to pass several parameters through the SIMD registers instead of stack memory. It can be a big win in performance. But usually you'll see this calling convention only in code which does some SIMD-accelerated computations. 
&gt;The mersenne twister RNG is known to give very good randomness Sure, but it's far from a cryptographically secure pRNG...
&gt; bii_find_boost(COMPONENTS system coroutine context thread REQUIRED) So it looks like it doesn't directly handle the dependencies of boost yet. It looks like a good start. Perhaps in a newer version it can take advantage of [autolink.hpp](http://www.boost.org/doc/libs/1_56_0/boost/config/auto_link.hpp) to automatically link in the libraries. A [patch](https://svn.boost.org/trac/boost/ticket/1094) has been proposed to this header before to build the pkg-config files. Biicode could patch the files to spit out the cmake targets or components. Being open source makes this more feasible with help from the community.
I should have given context for the quote, because the paragraph containing that quote contained: &gt;I did a bit of fun coding. I'm currently thinking on how to generate random bytes, for example the 16 random bytes for the initialization vector of AES. In which case, I don't necessarily support the use of a mersenne twister.
Oh, hell yes, that context absolutely requires crypto-security.
Are you showcasing a c++ compliation mobile app? 
What is the advantage of that vs e.g. OpenMP 4 `#pragma simd vectorize` ?
Well rounded, except if you want to split a string on a delimiter.
yeah, deleted the AES Context now, as its surely not a good example...
Well, I wanted to know the exact distribution of each bit. bitset::count only gives you the number of bits set to true. You also could then use popcount, which bitset::count is using I guess. And as pointed out above, the standard library is not perfect, &lt;random&gt; does not contain any cryptosecure RNGs.
`__fastcall` is also used.
LOC is a metric for determining how hard it is to find something in our source code and how hard it is to fix it if there is a bug. It does not say anything about quality or productivity. As others here have said, trying to get a negative delta-LOC is a good thing - if you can make your product do a new trick while not being more complicated and hard to understand, that's great. Also consider that bug metrics are typically per LOC. If you write twice as much LOCs of code, you have (up to a point) twice as much bugs. 
I don't think a beginner would ever change this, or even know how. The default setting is fine and doesn't close. But still your info is useful :)
For those interested there is also a `tuple_cat` example in the meta documentation: https://github.com/ericniebler/meta/blob/master/example/tuple_cat.cpp The implementation is ~40 LOC and pretty efficient. Sadly meta has no webpage yet but you can generate the doxygen documentation using `make doc`.
&gt; The idea is very elegant and simple. The core of the concatenation is going to be just one line of code. But it’s an ingenious line of code. Let me show it to you first, so we can bask in its glory: I wonder if I'm going to get credit for this. This was my idea when I rewrote Dinkumware's tuple_cat() in VC 2013, and it took me a weekend to implement. At least the implementation depicted isn't a copy-paste of mine; I used a single big struct to compute the indices and return type simultaneously. (And I didn't use C++14 auto-returns for obvious reasons.) &gt; return make_tuple(get&lt;Jx&gt;(get&lt;Ix&gt;(forward&lt;Tuples&gt;(tuples)))...); And yet, this is not correct. make_tuple() decays and unrefwraps, while tuple_cat() isn't supposed to. For example, concatenating tuple&lt;int&amp;&gt; and tuple&lt;long&amp;&gt; must result in tuple&lt;int&amp;, long&amp;&gt;. Going through make_tuple() will give you tuple&lt;int, long&gt;. My implementation avoids this trap by manually computing the return type, and directly constructing a tuple of that type.
Well, you know, it depends. It really depends on the way your colleague does the coding stuff. If he does some spaghetti coding with much overcoding in or opposite, doing too much OOP (like 75% of the code is around OOP stuff and patterns and factories and etc), then it's pretty obvious. You also must understand that productivity has nothing to do with lines of code / time. It's about implemented features and fixed bugs, okay?
Generally I would include namspace std for this type of program
I'm just trying to be explicit here.
Yeah, but then you'd be able to keep the `std`s away from the `8===D`.
I believe OP whishes it to be repeating. This would do the trick: int c{8}; while (~~c==8) std::cout &lt;&lt; "8===D~~" &lt;&lt; std::endl;
Thanks for the suggestion. However, I was able to fix the problem by 1)setting up the project as Win32 and 2)using the ctrl + F5 command. 
&gt;do people really bother about calling convention? Not exactly. The correct way is to turn optimization dial to 10 and let the compiler deal with the calling convention and inlining. That situation is the same between Windows and other platforms. This stems from following axioms: * I don't know to optimized this better than the compiler * I don't know how to optimize this better than the profiling-guided optimization :-)
I see, thanks.
Not the author, but: "Last post in the std::tuple series, on the two-dimensional tuple_cat inspired by Microsoft's STL" // https://twitter.com/goldshtn/status/569448046972391425 Strange that this isn't mentioned in the article, though... (unless I missed it?).
Well, Stephen Kelly has created [bpm](https://github.com/boostorg/bpm)(Boost Package Manager). It still builds with boost build, but perhaps it can be used to create smaller boost packages in biicode(ie one for each boost module).
At best, that only makes sense for the sake of conformity if everything else uses auto and trailing return types. But even then, that's pretty bad. 
Xcode.
Please don't troll I was being serious.
Alright, ccache is great and in fact, a must-have for a modern C++ developer. A good tool to put in the shed must be a gold linker. Some benchmarks on my PC: gcc + ld = 1 hour 23 min clang + gold = 37 min
Much appreciated.
If there are extra nodes, [distcc](https://code.google.com/p/distcc/) also shaves time off. I don't have access to a cluster at the moment, but obviously speedups generally scale with the number of nodes. It can be combined with ccache as well.
Basically, lock-free data structures don't use mutexes or other synchronisation tools that cause other threads to block. This is usually done by using lower-level atomic primitives (like fences or compare-and-swap instructions) to ensure consistency. The complete story is a bit more complicated than that, but Jeff Preshing has a great series of articles that explains it: http://preshing.com/20120612/an-introduction-to-lock-free-programming/
So they still need hardware support. I wonder if you there is a scheme that works without any hardware support - for example 2 processes running on 2 completely different machines that need to sync their access to a resource via TCP/IP or even UDP.
Well I guess you always rely on hardware to some point even if it's just to be sure integers reads and writes are atomic, which may or may not be the case depending on your system's architecture
Which is also exactly how "hardware-supported" synchronization works. Cores in a shared memory machine are a distributed system. "Pardon me, are any of you other cores trying to write to 0x2ef00068? No? Excellent, carry on then."
&gt;no window.h, which is where I'm really wondering, what does linux use instead? You sure you don't mean Windows.h? In which case I think that depends on your Linux distribution, Ubuntu uses X11. Though I'd say it's a better idea to figure out if any cross platform libraries does what you need.
You absolutely should name your files as .cpp. If you are trying to compile with gcc, your .c files will be compiled as C. If you're writing C++ code, use g++. \#include "iostream" vs. #include &lt;iostream&gt; do different things regardless of platform. Quotes will search in the local directory first, whereas brackets will search in places defined by -I (that's dash eye, not dash elle) as an example, let's say you're in ~/myproject and you have a file named iostream. if you #include "iostream" then that file will be included. If you #include &lt;iostream&gt;, then libstdc++'s iostream (the one you probably want) will be included. libstdc++ files are usually in /usr/include/c++/{g++-version}/ or maybe /usr/local/include/c++/{g++-version}/ and are automatically added as a -I paramter. You can disable the automatic inclusion by providing the -nostdinc++ argument when running g++ A good exercise to make sure you understand this is to compile your code with -nostdinc++ and look at the errors. Then compile it with -nostdinc++ -I /usr/include/c++/{g++-version}/ Windows.h is windows specific. Look at what you are using from that header and find alternatives by googling them.
Haha, this made me laugh. I did actually originally type windows.h but then wasn't sure and decided I made a typo while thinking about windows. Guess not ha. Cheers for letting me know!
Summary: no C++ code analysis.
Right, proper reply. Nice to know I can still use .cpp. I was just taking as what I had seen from other sites really, same for the iostream thing. Thinking about it, I think I got the cli bit wrong, a window does pop up when you run it in vs, it's a text based game so I guess I got a bit confused. I could run on linux though I don't see why not, but I'm not sure what changes would have to be made... Here's my code. https://github.com/Jeklah/combatSimulator Hmm I just realised It's missing the .sln and probably some other files because I didn't port them over as they are visual studio files...I think. I can upload them as well if you want to run it in vs. not that theres much to see.
I'm not really look for cross level, just want to get it working on linux. The libraries I have are just the ones you get from yum groupinstall development tools
yeah doesn't surprise me at all heh. someone above said linux uses posix instead? I'm not sure, this is all new to me.
Yah I'm looking through this and there are multiple mispellings, errors, etc that Visual Studios should have caught. This really went through as is?
Yeah, I think when someone starting out, it's just easier to make a it a blanket rule, especially if they don't understand scope pollution and how translation units are actually compiled.
Yeah I know, but even if you are just targeting linux (or windows or OSX) using a good high level library IS the correct thing to do. This is especially true of linux were most of the libs are very much raw C.
I don't want to be rude but I would seriously recommend you work through a good, recent C++ book. Pick one from [here](https://isocpp.org/get-started). If you want to get to the next level, also watch the Going Native talks. I am pointing this out because there are quite a lot of beginner mistakes in your code (many of them have been pointed out already) and you (and your employer...) would greatly benefit.
That makes no sense. There are significant errors related to spelling, variable names being wrong, improper syntax, etc. I dont see how it could possibly work in vs, are you sure this is the code you tested in vs?
Did biicode ever become open source? I know they had post saying they would but I can't find a link on their website.
I've found it is. Not really sure what (s)he's talking about.
http://linux.die.net/man/3/system In stdlib :)
Nice. What about the pause part? Is there a stock shell command for that?
You could use "read -rsp $'Press enter to continue...\n'" but I've never tried it outside of a shell, so your mileage may vary.
Is this meant as a joke?
Also, though, that is not what I am talking about. The following situation is: class test { int t; }; class other_test { test* test; }; int main() { return 0; } According to [this question](http://stackoverflow.com/questions/15537023/declaration-of-method-changes-meaning-of-symbol), you cannot do the above code, which is modeled after the OP's use of itemTable in the inventory class.
So C++ supports duck-typing now? I'll fetch my coat.
This looks fishy in the first place. Why do you need this code? If it's just so the terminal won't close at the end of `main`, then the correct solution is: Don't do it.
 s.emplace_back(S()); =&gt; s.emplace_back(); 
That's not the point. The point isn't filling the vector with "empty" objects - if you wanted to do that you'd just use std::vector&lt;S&gt; v(3) and be done with it. The point is filling the vector with statically defined elements. They use the empty constructor in the example, but the goal is to investigate trying to create a vector filled with statically defined, non-empty elements. An example would be - say I want a vector of strings filled with the texts "C++", "is", "awesome". So apparently doing std::vector&lt;std::string&gt; texts{"C++", "is", "awesome"}; is the same as doing std::vector&lt;std::string&gt; texts{std::string("C++"), std::string("is"), std::string("awesome")}; (which surprised me) 
How do you know if it looks fishy if you haven't seen the code? In this particular instance, you'd be right. If this were a pause between levels, not so much. The goal here is to help the OP find potential replacements for non-portable code. 
&gt; which surprised me as in "the `string`'s are not constructed in-place"?
I don't really see an issue here. OK sometimes a lot of calls occur, but if your constructor was more realistic (not using std::cout) most of that would just get substituted inline and optimised away, and largely disappear into the call site.
&gt; Variant C3, finally, is better: Although it looks more expensive to pass “wrong” int arguments that first need to be converted into proper S instances, The whole point of `emplace_back` is that you call it with "constructors arguments" to avoid constructing temporaries. The `int` is not "wrong", it's [perfectly forwarded](https://www.justsoftwaresolutions.co.uk/cplusplus/rvalue_references_and_perfect_forwarding.html). &gt; there is just no way to tell emplace_back to instantiate an element directly into the vector through a default constructor call It is possible std::cout &lt;&lt; "C4:\n"; std::vector&lt;S&gt; s; s.reserve(3); s.emplace_back(); s.emplace_back(); s.emplace_back();
If you're always using a converting constructor, you can use the iterator-range vector constructor: static constexpr auto const r = {1, 2, 3}; std::vector&lt;S&gt; s{std::begin(r), std::end(r)}; Even in more complicated cases, you can replace `auto` with `X[]` some type with a conversion function to `S`.
On the contrary - as in they are constructed. In the second option I would expect it to create strings then copy them, so 2 constructors per final string. But in the first option I would expect (by looking at the code) that the char* would remain char* until the string is constructed. I would expect only one string constructor per final string. The only way to have one string constructor per final string is to used what the author called option C3: std::vector&lt;std::string&gt; s; s.reserve(3); s.emplace_back("C++"); s.emplace_back("is"); s.emplace_back("awesome"); This is a pretty ugly way to fill up a vector, and I'm disappointed this is the most "efficient" one. I'm disappointed std::vector&lt;std::string&gt; texts{"C++", "is", "awesome"}; doesn't do the same
Anything that simply says something is 'bad' or 'good' is nothing but useless crap. Those categories are fit for little kids who can't take a shit on their own, not for grown ups looking to write better code.
I do a lot of work on real time applications in an environment without a debugger (it's messy). So I made this to help me watch for changes in variables I want to debug at run time. http://pastebin.com/RG976dT7 Edit: I just realized I forgot to take the 'tag' out of the macro input for this pasted version... oops. Oh well, just remove that (it's un-necessary). I've changed some names and such for this but feel free to suggest improvements.
There are probably built-in functions for these, but I made [a few randomizing functions](https://github.com/eindacor/jep_loot/blob/master/jeploot.cpp) a while ago that I still use quite a bit. Basically it's a few functions for a returning a random `bool` based on a percentage chance, random `int` in a range, random `float` in a range. My personal favorite is the `std::map` overload for the`catRoll()` function. You pass a map with one type for the keys, and `int`s for the values, and it returns one of the keys at random, based on its value's proportion to the rest of the values..... which I'll explain. Let's say you do this: std::map&lt;foo, signed short&gt; options; options[foo_a] = 10; options[foo_b] = 20; options[foo_c] = 40; foo random_selection = catRoll(options); In this example, the odds that the function will return `foo_a` are 10:70, the odds for `foo_b` are 20:70, and the odds for `foo_c` are 40:70. This is useful when you want something done at random, but you want to control the randomness just a little. Like if you're making a game that has different tiers of loot, and you want to control how often these tiers "drop", you could do this: item getItemFromChest() { std::map&lt;int, signed short&gt; options; options[0] = 1000; options[1] = 100; options[2] = 10; options[3] = 1; int selected = catRoll(options); switch(selected) { case 0: return item("common"); case 1: return item("uncommon"); case 2: return item("rare"); case 3: return item("legendary"); } } I've used the function quite a bit, it's handy to be able to mess with a sort of controlled randomness. The linked library has a python version too if anyone is interested. And just a quick disclaimer, I wrote these when I was really new to programming. They're very fragile and easily broken. I can't vouch for how well they work performance-wise but they do the trick for me.
That's really neat. I think if you tossed that into a class that allowed seed manipulation (seed setting/storage, # of iterations of rand(), etc.) on top of it and allowed you to specify the random generator source (ie. so it was easy to use a 3rd party generator in place of rand() ) it could be even better.
That's a great idea. It was always one of those things where I said "one day I'll make this better and more usable", but never got around to doing it. Maybe I'll revisit it when I take a break from one of my pet projects.
&gt; Few projects that large should be. Calm down there, Yoda.
Huh? OP is trying to compile the code right now and have it work as it did on Windows. This'll do the trick just fine. Not sure what you find problematic about it.
Jetbrains IDE's are worth the pricetag they carry. I believe all their IDE's have free versions so if you are only a hobbyist why not just use the free versions?
I think the closest thing to a useful snippet I have lying around, is a template wrapper around COM objects, which takes care of initialization, interface queries and destruction. I used it when writing some SAPI 5.4 code and got annoyed with having to deal with naked pointers and the ComObject class provided by the WinAPI. To be completely honest, my work and experience with the COM is pretty superficial, but the code has withstood every test I've put it through, and makes COM a LOT nicer to work with. The repository can be found here: https://github.com/MathiasPius/comobject
New to a codebase? Have several functions with overloads, but you only want to see where one specific one is called? Add `[[deprecated]]` to the function to have the compiler do the hard work for you! :v 
Alternatively, in older codebases \_\_attribute\_\_((deprecated)) also works for GCC.
Is in a proposal for c++17...
There's nothing problematic about it if he'll "just want it to work". I just pointed out how he could _improve_ the code. He may or may not use the advice :-)
Haven't read the book, but since it's published in 2000, it didn't teach you any modern techniques. It probably gives you a good base, but make sure to learn all the C++11 features too! I'd rather recommend a more recent book.
That is true. My book is dated from 2002, so it's allright to get a basic grasp of what sticks together in the language. After that I'll go over to another book (suggestions are welcome).
I mean it's hard to point you at one that will give you everything. I've learned my C++ through classes (taught with C++03) and general practice through projects. The only book I've found that helps me with specific new features, and keep in mind that I only own two, is Effective Modern C++. The other modern book I own is A Tour of C++, which is very much a general overview for folks who know programming, but not C++. Certainly useful, but I find myself reaching for EMC++ much more often. There's also The C++ Standard Library by Josuttis which has been updated, and I think Stroustrup's reference book, The C++ Programming Language, is supposed to be good for a "complete" reference that isn't just the standard.
It's a good book. Unlike other books about programming languages it doesn't treat you like an idiot who doesn't know what a variable is. I wish there were more books like this, that have the most important features and tips concentrated into a few hundred pages. Sometimes it seems that book authors are forced to fill their books with filler material, to make the look bigger and more important. But not this one. Only useful stuff in it.
This is a couple of functions that have been really useful for me lately while ive been working with the win32 api. http://pastebin.com/wHeMMj1G
I'm currently reading "Programming: Principles and practices using C++" and plan to read "The C++ Programming language" after. After reading this post I think I should also read "Accelerated C++", would you recommend it before or after "The C++ programming language"?
Depends on what you're able to bring with you. Simple questions: Do you know how primitive datastructures (int, doubles, array implementations)? Do you know how objects are work in theory? Can you abstract a complex problem in simpler steps? And even more simple operations? If so, the book is good in the first place. It gives you a general feeling and lookout of what you can do and how you manage problems. It gives enough tasks at the end of a chapter to think about and solve - hut if you lack this, you should go with a more Basic book.
American authors are often paid by page (at least for study books), so that is actually often the case for American books.
Denied cookies, it stole the screen and asked again. Bad etiquette = no read.
compiletime check for equality, which helps determine what the actual values have been - because static_assert( a == b) will only say they are not equal, but not the actual values: template &lt;typename T, T Expected, typename U, U Actual&gt; void check_eq() { static_assert(Expected == Actual, "Equality check failed!"); } #define COMPILE_CHECK_EQ(a, b) \ check_eq&lt;decltype(a), a, decltype(b), b&gt;() int main() { COMPILE_CHECK_EQ(sizeof(void*), 8); COMPILE_CHECK_EQ(sizeof(int), 6); } http://coliru.stacked-crooked.com/a/eb3c4963a54dba00 
Simple macro for 'injecting' inherited class name in derived class (something like java's super): Examples: http://ideone.com/mAB1ek #define SUPER_BASE(clazz) using this_class = clazz #define SUPER(clazz) \ using inherited = this_class; \ using this_class = clazz 
&gt; We all know that **goto is evil** Stopped reading here.
That's too bad, because I guess that most programmers who already know a language would like to read a concentrated introduction to a new language, instead of an overly long and verbose description of the for-loop in a new language...
[Dive into Python 3](http://www.diveintopython3.net/) maybe?
Ah now I understand why at the university, the american books where three times as thick and containd only half the knowledge compared to the course material written by our professors. (And 4 times the price).
What is?
The overhead in creating a new book is quite high compared to the overhead of adding a chapter or a couple paragraphs per chapter.
Oh I see. I'm at chapter 6 currently of Principles. I don't think it's bad, but it's very long, and kinda tedious to read. Which one do you think is better between the two?
You are probably both overestimating the scope of the book (it's a small one, aiming at teaching programming using C++ to non-programmers, not a book aiming to teach C++ in any depth) and underestimating its modernity. A more recent version would take advantage of a few things (using auto and the for each syntax for sure, the lambda syntax would probably not make the cut and I don't see anything else which could be relevant), but it's approach would be fundamentally be the same.
And the demand for a new book is never certain. It depends on the topic being discussed.
Have you read it? Accelerated C++ is certainly unique: Not just because it dares to *shudder* limit its audience to people who have programmed before. Hundreds of *"in 14 days for dummies"* - style books have attempted to treat C++ as *usable without knowing all the details of a particular topic*. This book actually succeeds at this. Instead of presenting the knowledge to become an expert C++ programmer, they use the language as a tool to make stuff work. The authors do have the depth of understanding to make that possible, *and* refrain from the sirens call to teach "everything you need to know about exceptions" at once. ---- This approach is not without problems. For an experienced C++ dev, the sometimes cursory explanations can be scary, and the order of topics can even make students impatient. Yet I give it to the authors that they have much more experience teaching people C++ than I have and despite these seemingly fundamental issues it's a persistent top recommendation. Would it be better if it used C++ 11? Indeed. But it's still better, still more hands on and still more productive for a wide range of recipients than the bulk of more recent write-ups. 
Ah, got it, thanks!
You could also read the [updated GotW atricles](http://herbsutter.com/gotw/). There are only a few, but I hope more are coming soon.
Like other people said, it's dated. I would personally just get the basic concepts and then transition to C++11/14 somehow. 
I'm not impressed unless until you make it work with VLAs (Yes, I know they missed the standard).
Same reason you need the size of fixed-size array even though you already "know" it. template &lt;bool constant&gt; struct is_an_array { /// Used when a constant sized (non-VLA) object is passed in /// only allow arrays past template&lt;class B, size_t n&gt; static _true_ test( B(&amp;)[n] ); }; template &lt;&gt; struct is_an_array&lt;false&gt; { /// This happens only for VLAs; force decay to a pointer to let it work with templates template &lt;class B&gt; static _true_ test(B *n); }; # define ARRAY_SIZEOF(arr) ({ typedef decltype(is_an_array&lt;static_cast&lt;bool&gt;(__builtin_constant_p(sizeof(arr)))&gt;::test(arr)) type; sizeof(arr) / sizeof((arr)[0]); }) 
but you are basically just using the arraysize macro. i think i misunderstood what you were talking about.
Except that this prevents pointers from being passed in.
Maybe you need to go on and read the linked paper? (http://www.cs.cmu.edu/~crary/papers/2015/rmc.pdf, didn't do this myself yet, though)
It should have said "We've all been told...".
Should be titled "four things almost every C++ programmer probably does know about C++".
A wrapper so you can have an easy way to generate random values at your fingertips. [N4316](http://isocpp.org/blog/2014/11/n4316)
Nifty!
The main reason checking is needed is because passing the wrong types as vararg arguments causes undefined behavior. This can't be fixed by runtime checking after the fact. This also means using non-literal format strings with vararg functions is a security vulnerability. That does present a problem with localization, but the answer is probably that one simply should not localize format strings used with printf. That's a real issue with printf and a good reason to use something else. But once one is no longer using varargs there's no need to verify that that the argument for a particular format specifier is of a particular type. Thus a runtime check for matching types doesn't really make sense. Either you should have compile time checks in order to avoid UB, or you can safely pass any type and there's no need for runtime checking. --- I suppose there's once case where runtime checking could make sense: when the API is explicitly a thin wrapper over printf intended to add support for things like localized format strings, in which case runtime checking is needed to make this safe. The only other case I can think of is if one wants to use the same format syntax as printf, but even then there's no need to do checking. Just support the format specifiers but allow them to work with any type. E.g. `%x` with a string will still print the string. Maybe generalize the meaning of the specifiers, so that `%x` with a string would mean to print the string as hexadecimal data.
Clang release notes: http://llvm.org/releases/3.6.0/tools/clang/docs/ReleaseNotes.html
Found it: &gt; **Windows Support:** Clang can now self-host using the `msvc` environment on x86 and x64 Windows. This means that Microsoft C++ ABI is more or less feature-complete, **minus exception support**.
 &gt;I tried implementing an allocator which would use 32-bit integers instead of pointers, but all of standard containers are written with the assumption that `allocator&lt;T&gt;::pointer` is the same as `T*` (it's allowed by the standard). A potentially powerful idea (alternate memory models) flushed down the drain. This is not correct. Both libc++ and libstdc++ support 'fancy' pointers in their containers. The implementation not allowed to assume that `allocator&lt;T&gt;::pointer` is `T*`. 
That looks awesome. It would be awesome if this was added to biicode. &gt; As cursor extends std::iterator, we can use the range-based for Not to be pedantic, but extending `std::iterator` doesn't make a class be usable in range-for(nor does it make the class an [`Iterator`](http://en.cppreference.com/w/cpp/concept/Iterator)). It must be a range(ie have methods `begin` and `end` as member functions or found through ADL) to be used in a `for` loop. Also, a cursor is like an iterator, it doesn't make sense to mix an iterator and range together. It would be better if `col.find` returned a view(ie a range) and that view returned a cursor(ie an iterator) through `begin` and `end`.
It still doesn't work because they use `*p` and `p-&gt;` to dereference "pointers". With such usage, the "pointer" itself has to have additional state to map from index to an address, and you have saved no space. To be able to use indices as pointers in a generic way, an allocator instance `a` must provide `a.dereference(p)`, returning `T*`, and containers must use this way of "dereferencing" consistently.
Yay for rewrites!
Sure but what you originally said is that implementations assume that `allocator&lt;T&gt;::pointer` is a raw pointer type which is not true. 
That is no longer true in C++11. Read the rest of the article. The section you quote references the C++03 standard. 
Btw Qt also has a very good implementation of sockets that takes advantage of its signals and slots mechanism. Though it's primarily thought of as a GUI framework, a lot of people use just QtCore and QtNetwork. 
Love it but for some reason the fonts look terrible on my machine.
&gt; This also means using non-literal format strings with vararg functions is a security vulnerability. That does present a problem with localization, but the answer is probably that one simply should not localize format strings used with printf. Forgive me if I'm wrong, but as long as the format string is coming from a trusted source then I'm not seeing the security vulnerability even if it is non-literal.
Please take a look at boost::asio.
Actually, the Clang and LLVM codebases do NOT use exceptions; and therefore Clang can self-host without being able to use exceptions. That's all.
I think you should be able to get compile-time checking by overloading the function for `constexpr`format strings and then use a `constexpr` check of the format strings versus the type-list. It might NOT be easy, but it would only depend on the compilers implementing the standards and would work with any supported `constexpr` format, not only string literals.
i dont know the real reason but personally im happy it got removed, never liked that construct because it can be too confusing. for(str : strings) screams passed by value, according to the assignment rules we're used to, leaving it like that as a synonym to move assignment was ugly as hell in my opinion. if they really wanted to make it shorter, just remove the "auto" keyword but keep the assignment rules the same. for(str : strings) or for(str&amp;&amp; : strings) or for(str&amp; : strings) would be fine by me.
few comments: 1. you are using c++,why not wrap the socket functionality is a class? 2. you should pass read only reference variables as constants[1] 3. take the pointer to the string[2] outside the loop to avoid calling .c_str() functions repeatedly in the loop. 4. respect the type system,you are assigning an ssize_t object returned by send()[3] to an int object[4].This is bad because in a 64 bit system,"int" takes 4 bytes and "ssize_t" takes 8 bytes and hence that assignment drops 4 bytes of information!!. [1] https://github.com/mikecurry74/sockets/blob/4bb1f5a053e317f7842f30d4e12dbff2fc796634/sockets.cpp#L29 [2] https://github.com/mikecurry74/sockets/blob/4bb1f5a053e317f7842f30d4e12dbff2fc796634/sockets.cpp#L38 [3] http://linux.die.net/man/2/send [4] https://github.com/mikecurry74/sockets/blob/4bb1f5a053e317f7842f30d4e12dbff2fc796634/sockets.cpp#L38
Thanks for the tips! I'll modify the code accordingly. 
5 Do not abuse `#define`. Prefer declaring global `const` variables instead. 6 Modularize. Do not put everything in `main()`. 
No argument there. Thanks for the downvotes though. You were wrong and I never disagreed with these new issues your raising. What containers were you hoping to do this kind of optimization on?
Hey Paul, Thanks for taking the time to comment. In the driver, `col.find` returns a `cursor`, which [actually *is* a range](http://mongodb.github.io/mongo-cxx-driver/cursor_8hpp_source.html). The begin() and end() methods on `cursor` return a `cursor::iterator` which extends `std::iterator`. The error was in my description of the API. I'll have the post updated with a correction. Adam
But if the "external message catalog files" are in same place as executable then they are equally secure. If they're not then they don't qualify as a "trusted source" in the first place.
_Secure Coding in C and C++_ has a chapter on formated output functions. For example, one of the things it has to say: &gt; Attackers must also be prevented from substituting their own message files for the ones normally used. This may be possible by setting search paths, environment variables, or logical names to limit access. (Baroque rules for finding such program coponents are common.) The chapter includes mitigations strategies if you're interested.
Ok, though I'm still not fully convinced, for example on Windows I'm not sure a non-privileged user can affect the environment variables for a privileged process. And anyway, the current directory (for example) of a process is often different from it's installation directory, that's nothing new, all a process has to do is ensure it's loading the file from the correct directory by asking the OS. I guess I just don't believe that the only file safe from an unprivileged user is the exe.
I think you may be confused about how C++11 range-for works. It does not perform assignments. Instead, it performs initializations. Given "for (TYPE elem : range)", each iteration initializes "TYPE elem = *__begin;". Therefore, "auto elem" will result in copies, while "auto&amp; elem" initializes references that are bound to the elements in-place. The proposal's core argument is that programmers expect range loops to observe/manipulate elements in place, since that's what all traditional forms of iteration (*ptr, ptr[idx], *iter, vec[idx]) so. Making "auto&amp; elem" the default would achieve that, except for the headache of proxy iterators, which is solved by "auto&amp;&amp; elem".
&gt; I guess I just don't believe that the only file safe from an unprivileged user is the exe. I certainly wouldn't claim that, and that's not really the way security analysis is done. Instead of positively identifying what is safe, typically security researchers identify potential attacks, and then try to mitigate those risks. There certainly are attacks on exe files or on the memory image of an exe, and for those attacks there are other mitigation techniques.
It's a valid concern about the syntax, I was just unhappy with how it was presented in the full Committee meeting. I have a new idea for syntax, although I am still uncertain as to whether I will write it up.
The bit about std::string not needing to do dynamic allocation is only partly true. It does not do it for libc++ and GCC version 5 or newer (and possibly VS). Current versions of stdlibc++ will always do memory allocation.
That concern was brought up before Urbana as well. E.g.: http://www.reddit.com/r/cpp/comments/1vxgeo/range-based_for-loops_the_next_generation/cex4oyc?context=3 (See the last paragraph where I complain about the syntax not looking like a declaration at all) http://www.reddit.com/r/cpp/comments/2hy94i/why_even_have_the_auto_keyword/ckx87tm?context=3 Perhaps 'simply dismissed' isn't the right phrase. The issues were brought up and you didn't think they had sufficient weight to merit addressing with changes in the proposed syntax.
Ok, I admit I'm out of my depth. What you're saying is interesting and I totally get that one should aim to minimize attack surfaces. I'll read that link you posted, thanks!
Please do. I like the effort &amp; anything that makes unconditional advice on how to write something in C++ possible is always welcome; it makes it easier to interact people with varying skill levels in C++.
By far the best introductory C++ book. 
Right. There was even some discussion about using `constexpr` or other techniques to do compile-time checking in https://github.com/cppformat/cppformat/issues/62. However, there was no work in this direction yet.
Copy the compiler error into Google. This is the most basic compiling issue. Google around a little before asking questions. Good luck.
Sorry, which bit? Formatting functions that return `std::string` do dynamic memory allocation, there is no question about it. But you can avoid it by using `fmt::MemoryWriter`.
If you have hand-rolled all string array memory management then of course your part does not do memory allocation. However what I meant is that if you do std::string foo("bar"); Then you don't get a memory allocation behind the scenes if you use libc++ or stdlibc++&gt;=5. That's because stdlibc++&lt;5 uses refcounted strings and does not provide a small string optimisation. Conversely if you return std::strings that are short enough, std::string doesn't do memory allocation either (well, stdlibc++&lt;5 does, but anyway).
BTW, build times Clang with cmake/ninja GCC 4.9.1 versus Clang 3.9: GCC: 656 seconds Clang: 397 seconds -&gt; Clang needs 0.6 the time of GCC.
Those are indeed strange symptoms. I think rather than asking about Visual Studio you need to engage in normal software troublshooting. Look for errors in the system error log, try installing on a different user account, on a different Windows computer, etc.
Would you consider adding libc++ and libc++abi?
The class chaining looks understandable but cryptic/messy. In practice is this really preferable to the alternative of multiple inheritance? 
Depends. If the compiler can't optimize away multiple empty base classes then multiple inheritance will significantly bloat object sizes. 
Personally i use [json11](https://github.com/dropbox/json11), isn't header only but the source file is a single one so i don't mind. [Here](https://github.com/miloyip/nativejson-benchmark#libraries) are a list of most C++ JSON libraries and you can run a benchmark with them to test their performance.
EDIT: /u/codestation mentioned json11. I just tried it and unless it has some hidden bugs we have a winner!!! Thanks /u/codestation ! ~~I could live with a single cpp file but I agree that many JSON libraries are zillion file affairs where that are shockingly complex for such a simple idea.~~ ~~But what I want is one that generates much cleaner code.~~ ~~Many of them you have to take a peak into the "variable" to see if there is anything there. I don't want code that looks like this:~~ ~~if(bla-&gt;get("IS_POOPY")-&gt;exists()){is_poopy=bla-&gt;getBoolean("IS_POOPY");}~~ ~~I ideally want:~~ ~~is_poopy=bla["IS_POOPY"];~~ ~~or at worst:~~ ~~is_poopy=bla["IS_POOPY"].getBoolean();~~ ~~So if you ask for a nonexistent int you get 0, a nonexistent boolean you get false, a nonexistent string you get "", etc.~~ 
Yes ideally a JSON library would at least let you do that in that it wouldn't squeal if you asked an int to come back as a string, thus handing you "34". Too many libraries that I have so far tested are very happy to throw a fit if you ask for something incorrectly. This is an excellent way to introduce fatal bugs into code. A very common scenario for me is to have a json config file that saves 8 things but then I add a 9th. So anyone with the older config fill won't have that 9th. I want it to ask for the value of the 9th and if it doesn't exist return one of those default values. Then it is more up to my code not to vomit when it gets a default value. But to have the JSON vomit is just being a pedantic asshole library. 
* https://github.com/nlohmann/json -- really nice * https://github.com/kazuho/picojson -- used it for a small project, was OK 
[Boost.property tree](http://www.boost.org/doc/libs/1_57_0/doc/html/property_tree.html) is header-only, I think
You can just load everything as a `map` of `string` which will achieve weak typing. I presume this is exactly what any simple JSON library actually does. Later you can convert strings to int as necessary. For bonus points, use a JSON Schema to validate the types of what is being read. C++ can do anything :)
I had to write one recently for a school assignment it's a little different from what you are asking for but I could change it to be header only/more to your liking? It's written using c++ 11.
Unless templates are being used I really don't get this header only requests. For me what matters is having a good library, regardless how it comes packaged.
Well, by header-only request I understand more something like "easy to use". Sorry, but in 2015, in times of great advance of Java/C# and JavaScript building libraries to use them in your project is one big misunderstanding and relict that must de for any future C++ iteration to become popular.
Thanks, but I don't think I want to give you additional work :)
I wrote [ujson](https://bitbucket.org/awangk/ujson) some time ago. Not header only, but at least no dependencies.
First, this is more limited than Boost.Parameter, since it does only cover named template parameters and not named parameters to function. Personally, I'm not a fan of all the boilerplate code that is necessary with Boost.Parameter for a class template with named parameters, you have to use macros, define the parameters in the template itself, create a signature class and then use special typedefs in the body of the class. I think that the usage of what I've written is easier on the class itself, but this is probably a matter of perspective. This post was mainly here as a teaching purpose, I haven't done a library for this technique, but I'm presenting this "concept" and showing/explaining how to implement it. 
Isn't this what the x32 ABI does at the OS level on Linux ?
&gt;building libraries to use them in your project is one big misunderstanding and relic that must die haha, why? 
Because evolution and natural selection, that's why :) Other languages are fast enough without tons of ancient relics that C++ "offers".
Does anyone know the current state of the modules proposals? 
Calling it a "relic" doesn't mean you're right. It's just a different way of including additional code. Whether it's compiled into a separate library or included into a project directly doesn't make much difference. Is the real problem that you've had difficulty getting libraries to compile...?
This is amazing. Exactly what I imagined, what I wanted. Good to know that there are C++ programmers that can _think_ in newer standards than pre C++11. Thank you so much.
Sorry, I'm not trying to attack you. It just sounds like you've had some trouble getting a library to compile, so now you consider that a *relic*. A complex library will probably be a mess to read through if it's implemented entirely as a collection of header files. You should try to learn a bit more about using the compiler/linker, rather than claiming that external libraries are old/bad. They aren't. (Personally, I code in a variety of languages. I adapt to their methodologies rather than insisting they do things the way I like.)
AFAIK the last paper on modules is [0]. Richard Smith seems to be working hard on Clang's modules implementation [1], but if IIRC clang's approach started from Douglas Gregor's work on ObjectiveC modules, so it differs significantly from [0]. There should be a TS in the works, the mailing list of SG2 seems dead tho. [0] http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4047.pdf [1] http://clang.llvm.org/docs/Modules.html
&gt; &lt;cstdio&gt; is just &lt;stdio.h&gt; with everything put into the std-namespace Oh... no wonder I couldn't remember having to use std::cout when I tried learning C++ in middle school. That threw me for a little bit of a loop when I learned it properly.
And, of course, you can already enjoy all of the benefits of type-safe formatting by using [C++ Format](https://github.com/cppformat/cppformat): fmt::print("I'd rather be {1} than {0}.", "right", "happy");
Nope, that is not exactly the reason: `std::cout` is C++-only-functionality and shouldn't live in the global namespace at all today. It is more likely that you either had a `using namespace std` in some place or that you used a compiler that didn't conform at all. Apparently there was once a breaking change in libstdc++ where they moved all those thinks into it's namespace, meaning that it wasn't there before.
`cout` is part of `iostream`, not `stdio.h`. So it should always require `std::` unless you put `using namespace std;` in, which almost every beginner's tutorial does.
Building something as a library isn't an "obsolete technique", it's just a different way of packaging the code. It compiles the same way in the end, and is most likely easier to read and understand in that format. Honestly if you have trouble building libraries, you spend your time figuring out how to do that, rather than complaining as if it's the wrong way.
Thanks for the information, but that is what I already know as well. Nowadays I barely touch C++, but still care to keep up to date. Last news I had was a statement of Bjarne Stroustrup about those two approaches being discussed. If it stands like this, most likely C++17 won't get them, and then it might no longer matter. Just thinking about the time frame to get C++17 available across compilers and whenever the C++17-Next would be.
Interesting, thanks! Maybe I forgot about the using statement being there because I didn't know what it was at the time. 
&gt; the stream-family is faster for some workloads What workloads would that be? I've never experienced that situation... Full disclosure: I am not a fan of iostreams but mostly due to the god-awful syntax/usability, less so for performance deficiencies (perceived or otherwise). Anyway, I like your idea for persisting ranges -- very nice. 
I've actually created my own system for changing styles of ostream output, that I think works better because it actually *builds on* ostream rather than look to replace it like so many others do. Basically, I have created something that changes the state of an ostream based on a string, similar (but not exactly like) printf strings. So cout &lt;&lt; setprecision(3)&lt;&lt;setw(10)&lt;&lt;pi&lt;&lt;endl; would be cout &lt;&lt; sstate("10.3")&lt;&lt;pi&lt;&lt;endl; It has options to change ALL the ostream parameters, so you can easily use even the more obscure ones. Some of the features: * it only changes the parameters you specify. * works with ostream instead of reimplementing it. This means you can only change things that ostream allows (so no "centering" for example), but it also means it will work with all ostreams * easily turn an ostream into a string state for human-readable easy inspection. Done like that it returns a full string description of the entire state * easily return an ostream to the "default" (clean) state if you don't know what has changed in it example: cout&lt;&lt;sstream(); // returns cout to original state * you can save the current (full) state of the ostream. This is very useful when creating an operator&lt;&lt; that needs to change parameters example: ostream &amp;operator&lt;&lt;(ostream &amp;out,CLASS c){ sstate s(out); return out&lt;&lt;sstate("4x-0")&lt;&lt;c.i&lt;&lt;s; // prints c.i as a length 4 hex number padded by 0, then returns to previous state } there's actually a "sstate_guard" for that as well, for one liners: sstate_guard(out).out&lt;&lt;sstate("B")&lt;&lt;(1==0); // prints a binary as 'true/false' then returns to previous state * I actually have a number of other tools that use this format, so in a way it's "integrated with other tools". On the other hand, so far there are less than 10 people using these tools :) I haven't made this public, as I use C++ mostly for academic research which means a lot of "research style" code, but if people are interested I could add a GPL copyright notice on it and let you have it. 
Awesome work. I've done very custom rendering and modifying the UE4 renderer isn't bad at all. Of course, anything "exotic" would most likely require changing the core renderer. Doing so, however, isn't too bad... especially since the whole source code is provided. It basically comes down to whether the tools provided by the engine are going to help you out with whatever you're doing. Are the benefits worth the cost of modifying the engine for you project...?
Note that the performance issues usually don't manifests itself in the simple cases, it's when do use printf to actually do formatted output, i.e. you give printf a handful of arguments, which printf will handle in 1 function call, and cout &lt;&lt; will incur the overhead of several function calls. All this ofcourse depends more on the actual implementation of printf/cout, I'm not saying printf is any faster , but I am saying that benchmarking the super-naive test isn't that interesting. 
Whatever. This is fantastic news! I'm going to upload it right now!
 template &lt; typename U, std::enable_if_t&lt; Same&lt;Wrapper&lt;T&gt;, std::decay_t&lt;U&gt;&gt; &amp;&amp; CopyCtor&lt;T&gt;, int* &gt; = 0 &gt; Wrapper(const U&amp; rhs); That is just way too complicated. You can write it much simpler like this: template &lt; bool _=true, class=std::enable_if_t&lt;(_ &amp;&amp; std::is_copy_constructible&lt;T&gt;())&gt;&gt; &gt; Wrapper(const Wrapper&amp; rhs); Also, the Tick library already provides a [`TICK_MEMBER_REQUIRES`](http://pfultz2.github.io/Tick/doc/html/requires/#tick_member_requires) macro, you can use instead: TICK_MEMBER_REQUIRES(std::is_copy_constructible&lt;T&gt;()) Wrapper(const Wrapper&amp; rhs); 
Namespaces weren't added to C++ until 1994, so the pre-standardization `iostream.h` obviously didn't put things in namespace `std`.
&gt; deploy to my ipad with 5.1.1 on it That's the iOS equivalent of XP. *You monster.* 
My simple problem is that I have a perfect condition iPad 1. It won't go past 5.1.1 I pretty much only use it to surf the web a bit, do mapping, and mostly watch lectures and netflix. Much like XP was for many people it does everything I want and I see no reason to upgrade. I don't want the camera and so far there is exactly one app that demands iOS 6 that I would like to put on it. Features not worth $500. I would like to test my app on the larger screen with the different ratio. Cocos2d-x works fine on iOS 5.1.1. My iPhone is ios 8.x so for most device testing it works perfectly. 
Maybe paying was mostly for ground floor developers and dedicated hobbyist who really wanted it enough to pay the fee, now they feel its refined enough where they want everyone to tinker around and adopt the engine. 
Still have that on my iPod Touch 4th gen.
They attract students, hobbyists, modders, and anyone else that wants to tinker around with a game engine but not planning on selling a game commerically. Then, some of those people will go on to make more serious games to sell, and they will choose the engine they know how to use already.
Cool! Thanks!
The previous price point of $20 probably wasn't making them any meaningful amount of money and added friction to getting started. A single breakaway success which otherwise wouldn't have used UE can easily make up the difference.
they rarely (if at all) implement them in one place - the skeleton of the algorithm is usually in one file, but almost always has dependencies. take a look at insertion sort from the SO answer you linked: template&lt;class FwdIt, class Compare = std::less&lt;&gt;&gt; void insertion_sort(FwdIt first, FwdIt last, Compare cmp = Compare{}) { for (auto it = first; it != last; ++it) { auto const insertion = std::upper_bound(first, it, *it, cmp); std::rotate(insertion, it, std::next(it)); assert(std::is_sorted(first, std::next(it), cmp)); } } you need `std::upper_bound`, `std::rotate` and `std::next`. in addition, it's templated and uses a custom comparator. it might be easy for you to read, but to 99% of college graduates who have gone through a c++ course, it will look like chinese (unless said students are chinese). in fact, if you refactored the above code by replacing all mention of insertion by something unrelated, most professional developers would struggle to tell you what the code does. all in all, i know everyone on here loves shitting on beginners' use of outdated practices (e.g. raw pointers, rand, etc) as though you magically acquired the experience to write good code. i'm not taking anything away from the article itself, i just don't like the attitude, which, unfortunately seems prevalent in the online community.
At least its not Vista!
Isn't this just an over-complication? Just omit the static_assert &amp; everything works without any extra effort. test.cpp:32:27: error: call to deleted constructor of 'A' ::new (storage()) T(rhs.get_T()); ^ ~~~~~~~~~~~ test.cpp:52:20: note: in instantiation of member function 'Wrapper&lt;A&gt;::Wrapper' requested here Wrapper&lt;A&gt; y = x; ^ test.cpp:7:5: note: 'A' has been explicitly marked deleted here A(const A&amp;) = delete; ^ 1 error generated. https://gist.github.com/vlovich/d36a168e00f11aa7f666 Also the original code doesn't compile - should be reinterpret_cast, not static_cast
And this is why I was behind the rust RFC for checking overflow/underflow and why I am so glad it was accepted and that Rust now checks for them by default in Debug. Of course, in safe Rust this would not create a memory hole.
As an exercise, try writing your own versions of `std::upper_bound` and `std::rotate`. It's harder than you think to get the desired complexity (remember to distinguish bidirectional from random access iterators) or even correctness.
I am currently writing an article explaning of *what* iterators are and how they can be used with the stdlib and I just decided that the last section will contain a link to this article. There are a few tiny things in the details, that I would have done differently, but at this point all I can say is: “Great job, exactly what was needed!”
Traditionally MSCVRT has contained both.
So does this mean users won't have to install redistributables anymore? 
FlatBuffers (http://google.github.io/flatbuffers/) contains a JSON parser at the cost of 2 .h and 1 .cpp file. It is also strongly typed, so as long as you can write a schema for whatever JSON you're loading, you'll end up with faster and less error prone C++ code. It uses way less memory than RapidJSON.
If they're guaranteed to have Win10, and you can guarantee you don't have some old DLL shipping still linked to an old runtime library - then yeah, sounds like it.
msvcr120.dll is 2013's CRT (powering &lt;stdio.h&gt;/etc. for both C and C++ programs). msvcp120.dll is 2013's DLL for the C++ Standard Library (mostly iostreams, some threading/filesystem/etc.). I've usually called them "R" and "P".
They must, for "vcruntime" stuff (think: exception handling, RTTI) and the STL (msvcp140.dll is shipping as usual).
Yes - many algorithms are tricky. Even find() and equal() are.
Interesting. If I were to solve I would have done something different. I would just define the `is_binary_predicate` to optionally take a third parameter to check for both combinations of them, like this: TICK_TRAIT(is_binary_predicate) { // Single version template&lt;class F, class T&gt; auto require(F&amp;&amp; f, T&amp;&amp; x) -&gt; valid&lt; decltype(returns&lt;bool&gt;(f(std::forward&lt;T&gt;(x), std::forward&lt;T&gt;(x)))) &gt;; // Dual version, checks both combinations template&lt;class F, class T, class U&gt; auto require(F&amp;&amp; f, T&amp;&amp; x, U&amp;&amp; y) -&gt; valid&lt; decltype(returns&lt;bool&gt;(f(std::forward&lt;T&gt;(x), std::forward&lt;U&gt;(y)))), decltype(returns&lt;bool&gt;(f(std::forward&lt;U&gt;(y), std::forward&lt;T&gt;(x)))) &gt;; }; Then assuming I had SFINAE-friendly versions of these aliases: template&lt;class Iterator&gt; using iterator_reference = std::iterator_traits&lt;Iterator&gt;::reference; template&lt;class Iterator&gt; using iterator_value = std::iterator_traits&lt;Iterator&gt;::value; Then I would write the constraints something like this(just a rough sketch): template &lt;InputIterator I, WeaklyIncrementable Out, Semiregular R&gt; requires is_binary_predicate&lt;R, iterator_reference&lt;I&gt;, iterator_value&lt;I&gt;&gt;() Out unique_copy(I first, I last, Out result, R comp); It seems simpler and more straightforward, but perhaps I am missing something. 
nlohmann's json is really promising!
Yep, sorry, I missed that part. It makes sense. But then why wouldn't you define `is_binary_predicate` like this then: TICK_TRAIT(is_binary_predicate) { // Single version template&lt;class F, class T&gt; auto require(F&amp;&amp; f, T&amp;&amp; x) -&gt; valid&lt; decltype(returns&lt;bool&gt;(f(std::forward&lt;T&gt;(x), std::forward&lt;T&gt;(x)))) &gt;; // Dual version, checks both combinations template&lt;class F, class T, class U&gt; auto require(F&amp;&amp; f, T&amp;&amp; x, U&amp;&amp; y) -&gt; valid&lt; decltype(this-&gt;require(std::forward&lt;T&gt;(x))), decltype(this-&gt;require(std::forward&lt;U&gt;(y))), decltype(returns&lt;bool&gt;(f(std::forward&lt;T&gt;(x), std::forward&lt;U&gt;(y)))), decltype(returns&lt;bool&gt;(f(std::forward&lt;U&gt;(y), std::forward&lt;T&gt;(x)))) &gt;; }; And isn't that how `EqualityComparable` is defined as well?
&gt; Does [`EqualityComparable`] use `Common` or `CommonReference`? In the Palo Alto TR? It uses `Common`. It does in Range-v3 as well. &gt; But shouldn't Relation&lt;R, ReferenceType&lt;I&gt;, ValueType&lt;I&gt;&gt; check for the cross product of types as well? Yes, in the Palo Alto TR and in Range-v3, `Relation&lt;R, T1, T2&gt;` checks that it can be called as `r(t1, t1)`, `r(t2, t2)`, `r(t1, t2)`, `r(t2, t1)`, and `r(C{t1}, C{t2})`, where `C` is the common type between `T1` and `T2`.
"C++11 threats functions as first-class citizens " **WAT**
`vector&lt;auto&gt;`'s kinda weird, but it did grow on me a bit when reading the example use-cases. I'm not convinced it's worth the complexity, but I do at least like the idea of more flexibility to write just the types that are useful to readers of the code (and `Container foo = doStuff()` sounds really nice).
It depends on what you're doing. For pointer-arithmetic-y stuff -- Cap'n Proto's use case, and arguably the case where overflow detection is most important -- multiplications tend to be by small constants, which work perfectly with `Guarded`. There are only a few places in the Cap'n Proto code where larger numbers are multiplied: specifically, when computing the total size of a struct list, where each element can itself theoretically be fairly large. But this is _exactly_ where Cap'n Proto really needs to do careful checking to avoid vulnerabilities, and in fact the security bug found was a case of a very large list with very large elements overflowing the size computation. Put another way, if you need to check overflow, you need to check overflow, no matter how annoying it might be. `Guarded` just tells you where you need the checks. If you don't care about overflow, by all means, don't use it.
Sorry for the typo. I guess more caffeine is good idea. I'm agree with you when you are talking about functions - what i meant is that using functions as arguments and returning functions as result is no longer problem for modern C++ as it was before. 
How is normal functions not first class in c++?
You can't define a *function* in a nested scope, let alone return it and use it elsewhere. The exception is lambdas, although they have some restrictions if you plan on returning them (no capturing). Lamdas aren't functions, they're function objects, i.e. auto hello = []() { std::cout &lt;&lt; "Hello from a lambda!\n"; } is really something like struct { void operator()() { std::cout &lt;&lt; "Hello from a lambda!\n"; } } hello; An anonymous `struct` with one instantiation of the name you give it that has a single `operator()` implemented.
This is usually done with public inheritance and it is called the Curiously Recurring Template Pattern.
Well, they aren't. I don't know why the title is what it is, but I think a better one would be "Functional Programming in C++ Using Template Metaprogramming Techniques and Lambda Functions".
Is enahnced auto deduction like this part of the standard or is this a clang extension?
&gt; The exception is lambdas, although they have some restrictions if you plan on returning them (no capturing). its allowed to return a capturing lambda. you just have to be careful and make sure the captured variables live past the last point the lambda is used.
Wait for Concepts Lite, if you mean 'full' support for duck typing. 
I think this is explained in the link. vector&lt;auto&gt; y = x() means that the return type has to be a vector, though it can be a vector of any type. auto y = x() means the return type can be any type. It's useful to be more specific, but less specific than providing a concrete type.
How can it be any type? The type is deduced by the right-hand-side expression?
Gotcha, thank you. Didn't know gcc allowed that either.
I guess really the examples should be more like: vector&lt;int&gt; Foo(); vector&lt;auto&gt; v = Foo(); // Helps by enforcing it's a vector, but you don't need to know the type But: auto v = vector&lt;int&gt;(); // We know the type here... no need to spell it out twice
Beautiful
That's horrifying.
This is an implementation of one of the features detailed in the Concepts PDTS [N4377 C++ Extensions for Concepts](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4377.pdf) in the [dcl.spec.auto] section. It's not standard, but it will almost certainly be incorporated into the standard later.
A large number of these bugs could have been found via compiler warnings (that were either turned off or didn't exist). For example `1 &lt;&lt; 32`. 
depends on what you are optimizing for... static link means we (MSFT) can't service a bug in it. Also, for cases where disk size is important (think inexpensive phones) static linking increases binary sizes relative to dynamic linking.
I'm trying to imagine a case where the range of possible integers really explodes and you *don't* want to use some unlimited precision integer library/type...
Modern c++ is great, but the issue is that it's really really easy to blow your legs off if you don't write idiomatic code. Learn the pitfalls, and it's a great language. Also, know when not to use c++; when all you have is a hammer, everything looks like a nail. 