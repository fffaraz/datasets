close() can fail. sorry, posix IO is super painful. don't ever do that if you're writing a databsae
Can I hire you to do my PR reviews? :) 
What makes those discussions often difficult and abstract is that people aren't showing actual error handling in their foo- bar examples, but to make realistic comparisons, you have to write realistic code to begin with. Realistic code means first that the structure of the code even without errors maybe non-trivial and that error handling means not just reporting the error further up the callstack or write a log message and terminate. You have to talk about examples, where the actual handling of an error might happen multiple levels down the callstack (which is much easier with exceptions) and where error handling incorporates e.g. trying a different algorithm or simply retrying an operation a couple of times (which looks horrible with exceptions). What is imho even more problematic is that there isn't an agreement on what actually constitutes an error. What I liked about your original post was that you where taking about disappointments and while errors are certainly disappointing results of a function, not all disappointments are errors. E.g. not finding a cat in an image is certainly disappointing, but can hardly be considered an error. Finally, I agree with matthieum: most of the time the performance is not the most important aspect of different error handling techniques, but what they do for code clarity and how likely you are too screw up when employing one or the other.
Is this VS or VS Code? The only reason I ask is cause I prefer more lightweight IDE's for portable use and not eating up 8GB.
As far as I know, all three implementations provide exactly as much capacity as requested in vector::reserve.
Reviewing code is already half of my day job :-P
What do you mean by that? Compilers only throw away calls to `memset` that they can prove are useless, like setting a local variable right before returning. In their example they're setting whatever `p` is pointing at, so there's not enough context to know whether or not it will be used again. The compiler wouldn't throw away that call to `memset` unless it also threw away the `*p = 1;` line. (By "useless" I mean "unobservable to the C++ abstract machine". You may still want to clear local variables for e.g. security purposes in case of bugs, in which case you've left the realm of the C++ standard and need to use something besides `memset`.)
&gt; I feel like a problem with exceptions, certainly in a large scale codebase of &gt; 500k loc is that they can be a lot of work to implement correctly. As I have no experience with really large code bases: Is it really more work to correctly implement exceptions than to implement the same semantics with return values / ADTs?
Is there anything in the standard that would require different implementations of the distribution code to produce the same sequence?
Nope, nothing at all. For `minstd_rand`, it requires "the 10000th consecutive invocation of a default-constructed object of type minstd_­rand shall produce the value 399268537", but for `uniform_int_distribution`, there are no more specific requirements than the general ones for distributions. In fact, "The algorithms for producing each of the specified distributions are implementation-defined.". 
This is not what "portable" means. 
No, of course not. There are good reasons for tightly specifying the random generators--mostly that in most cases it's hard to specify the exact properties we care about, and people putting them to serious user frequently depend on properties we might have omitted from a description. That's not really true with the distribution classes though: the properties we care about are generally quite easy to describe (e.g., uniform_int_distribution produces a uniform distribution of numbers in a specified range). As such, little (if anything) would be gained by specifying the implementation instead of the interface. Given a reasonable choice in the matter, the C++ standard will normally choose to specify the interface--and that's exactly what they've done here. 
It's like the opposite of obscenity, you know error handling is robust when you don't see them. ^^^^^/s 
Thanks, reduced and filed as VSO#534447 "[15.5 regression] ICE in toil.c involving a generic lambda with a separate default argument". With our current development build, my reduced repro is: C:\Temp&gt;type meow.cpp template &lt;typename T&gt; struct NamedTypeImpl { explicit NamedTypeImpl(const T&amp;) { } }; using Named = NamedTypeImpl&lt;bool&gt;; int main() { auto lambda = [](auto x, Named y = Named(false)) { (void) x; (void) y; }; int i = 0; lambda(i); lambda(i); } C:\Temp&gt;cl /EHsc /nologo /W4 meow.cpp meow.cpp c:\temp\meow.cpp(11): Assertion failed: FATAL_UNREACHABLE, file s:\msvc\src\vctools\compiler\cxxfe\sl\p1\c\toil.c, line 5781 [when compiling meow.cpp] 
Terrible compiler slightly less terrible, film at 11
&gt; As far as I know, all three implementations provide exactly as much capacity as requested in vector::reserve. I checked MSFT and GCC. You are correct. For string both use growth factor, but MSFT does not shrink to fit, so it does not end up with huge number of reallocation. But obviously in theory it can mean it is wasting memory.
Instead of constructing the default initial value, could it use/skip the first value? I've written something like this a few times recently (knowing ahead of time `x` isn't empty): accumulate(next(begin(x)), end(x), *begin(x), op);
It's important to note that throwing away the call can fuck up code that had undefined behaviour, especially if you ended up reusing that part of the stack without initializing it. But this is obviously not something anyone should be relying on.
It might do it if your functions are `noexcept`, but I doubt it would if they aren't.
These take literally minutes to learn because they are very easy, and you aren't likely to forget constexpr if when you suffered through SFINAE for the same thing.
The first rule is make sure you're accessing memory in order and not jumping around, then if you need more performance you can start looking at instrumenting cache misses.
The first rule is make sure you're accessing memory in order and not jumping around, then if you need more performance you can start looking at instrumenting cache misses.
That's one more unit test for the next version I guess.
I always see this but I can't quite figure out how to do this?
You can use godbold to have your answer: https://godbolt.org/g/ao5yxY
What are you supposed to do if `close()` fails though? According to [cppreference](http://en.cppreference.com/w/cpp/io/c/fclose), in the worst case it won't have written to your file correctly.
I actually needed this property in a little project of mine. It was about steganography: it read from / wrote to specific bits based on the given key. Solved the problem by using `boost::random` instead.
If you hit OOM with a small alloc maybe, but you are more likely to hit it with a big spike of memory demand that you can free up when going up the stack and then you have plenty to tell the user that there isn't enough memory. Please don't be like 99% of video games that just CTD when they get OOM.
You can localize them somewhat easily if they use the standard C error codes, but obviously usually for technical errors it isn't so useful.
The easiest way to see is to debug and check the addresses. You can check how much it changes in each iteration, if it changes by the size of your element (so 8 for a `double`), you are probably doing it right. For an image, it's important to know if you store it rows or columns first so you can put your two loops in the right order. In cases where you work on an array of pointers, you usually won't have problems with the pointer themselves (they'll be next to each other in memory), but where the addresses they point to might not be contiguous, creating a lot of cache misses. The best solution here is to use a local allocator or something like that, allowing you to keep allocations close together.
&gt; You can use godbold to have your answer: I dont speak asm. BTW I did not downvote you. 
Thanks for the input! I'll look into local allocator since I don't really know how that would work, I appreciate the clarification :)
&gt; Couldn't we instead just have a std::sorted_associative_container&lt;T&gt; std::flat_map is a container adapter. http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0429r3.pdf
I understand your pain, and thank you for sharing that observation, as it is very valuable. Some people might ask why one would need portable random distributions, but it's very important! If you want to repeat a simulation from a seed, you definitely need all your code to behave the same across platforms. This need arises, for instance, when you're building a networked multi-player game, or a reply game feature. It's also a general mental model issue; you'd normally want to think of a pseudo-random distribution as a pure function from a seed to an infinite sequence, but if it depends on the platform (maybe even compiler version), then you either need to model as an impure function, or a pure function that takes a hidden extra parameter (platform + stdlib version ish, maybe some other stuff too) that you can't change or really introspect.
There were a few allocators presentations during the last CppConf, with comparisons of performance and how to write your own.
STL (the human) uses the word "invariant" to describe this situation, if that works better for you.
lol, i just noticed your handle. not sure if real stl or not.
&gt; was hoping the OP would do some research and realize that he was making an erroneous assumption about the function. OH, I understand the standard says it's implementation defined. It still surprised me and I wanted to share as it may also trip up others. 
Chandler recently said that exceptions cause slower generated code. :) Unfortunately he did not say how much slower... 
I'll try my best, but I'm still learning my ASM. For that constructor, we find that **MSVC Spits out:** MyClass::MyClass, COMDAT PROC push ebp mov ebp, esp push ecx mov DWORD PTR _this$[ebp], ecx lea eax, DWORD PTR _s$[ebp] push eax call ??$move@AAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@std@@YA$QAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@0@AAV10@@Z add esp, 4 push eax mov ecx, DWORD PTR _this$[ebp] call ??0?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@QAE@$QAV01@@Z lea ecx, DWORD PTR _s$[ebp] call std::basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt; &gt;::~basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt; &gt; mov eax, DWORD PTR _this$[ebp] mov esp, ebp pop ebp ret 24 ; 00000018H The big thing I see is the call to a `std::basic_string` constructor followed by a corresponding destructor. Once moved, I don't believe an object should destruct leaving it's scope - as it's somewhere else. The constructor is obfuscated, though, so I'm not entirely sure if it's a move constructor. **Clang gives:** MyClass::MyClass(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;): # @MyClass::MyClass(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) push rbp mov rbp, rsp sub rsp, 16 mov qword ptr [rbp - 8], rdi mov rdi, qword ptr [rbp - 8] mov qword ptr [rbp - 16], rdi # 8-byte Spill mov rdi, rsi call std::remove_reference&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;&amp;&gt;::type&amp;&amp; std::move&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;&amp;&gt;(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;&amp;) mov rdi, qword ptr [rbp - 16] # 8-byte Reload mov rsi, rax call std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;::basic_string(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;&amp;&amp;) add rsp, 16 pop rbp ret Here, we see that an `std::basic_string` constructor is called with the `&amp;&amp;` r-value permutation (by the type signature). There's also no following destructor call, which is as I would expect. So I'm going to assume that MSVC (that's 03, by the way), does not correctly optimize this out. It will treat the parameter as an l-value and keep the reference alive. Clang is smart enough to work that out and move it.
You're just a gift that keeps on giving aren't you? I'll check then out, you're the best
nah, we still use memset for that sometimes, just very carefully through a volatile function pointer. better to use memset_s when it is available though.
Thank you. Do you know what is this call to std::remove_reference? 
Part of the type signature for `std::move`^1 . It takes any l-value `Foo&amp;` or r-value `Foo&amp;&amp;` and returns the base value type `Foo`.^2 This is necessary for generic functions of this nature, to transform the in type to the proper out type. ^1 : http://en.cppreference.com/w/cpp/utility/move ^2 : http://en.cppreference.com/w/cpp/types/remove_reference 
Godbolt is a bit (or not just a bit) outdated on their MSVC version. It looks to just be RTW/RTM, without any toolset updates (15.3 and 15.5).
`std::remove_reference` is not what's called, it's part of the return type. The entity being called appears to be `std::move`.
OK, but it is just a compile time thing, why is there a real `call` in the output asm. I mean std::move is just a cast. "In particular, std::move produces an xvalue expression that identifies its argument t. It is exactly equivalent to a static_cast to an rvalue reference type. "
Ah I think I understand it now... although it is unclear to me why std::move is not inlined. It is just a static_cast.
My problem with exceptions has little to do with performance. Performance just rules out using them for alternative return paths. First, it is because they are unstructured come-from insructions. They are the inverse of goto, where the spot you come from can be in any source file anywhere in your entire project. This makes understanding code flow next to impossible unless you add seriously enforced structure on top of their use. Yes you can "fix" this, but the cost of fixing it is constant vigilance and effort and you get nearly zero help from your tools. My second problem is what happens when you use them incorrectly; your program ceases working. Either you `catch(...)` and have no idea what the error was, you accept the possibility every time you throw you are throwing a hail mary pass to avoid app crashing, or you again enforce manually with zero tools to help an independent flow control system and types etc. If you screw up, boom goes your program. Now for some things, boom goes your program is a decent fallback, like failing to be able to allocate memory. But for almost every other use, it feels like a mixture of basic goto spaghetti hell, python strongly typed data with typeless variables bombs, and raw buffer manipulation explosives. Exceptions: explosive spaghetti type bombs. 
I'm too lazy to get the links right now though but thanks.
&gt; clang optimizes this to very simple code using the fact that your string falls intro SSO It inlines string content into a damn integer if it can... amazing... 
Seems to be an empty function on Clang (optimization off). Just `mov rax, rdi; ret`.
Another use case for the non-exception folks: embedded / kernel developers. Though I'm rarely writing code in environments that can't use exceptions these days, it still flavors my thinking. Having a standard library that you can use in either context is an amazing prospect for me.
Interesting. Can you sum up in a nutshell what this does better than Eigen?
A few issues. First, it looks like both the MSVC and Clang runs were done with the optimizer disabled. In that case it's doubtful whether any transformations are applied. Second, MSVC and Clang are using different parameter passing conventions here. MSVC is using the Windows x64 ABI, which passes the first two arguments in rcx and rdx, while Clang is using a SysV style convention with rdi and rsi. Critically, the Windows ABI specifies that arguments passed by value with non-trivial dtors are destroyed by the callee while SysV has the caller do it, so MSVC is obliged to run the dtor or equivalent on the argument within the function while Clang isn't. Third, the basic_string implementations are different, which can also influence what the compiler is able -- or permitted -- to do. In MSVC's case, it does a straight interpretation of the code, doing copy-ctor to create the argument, then a move construction into the member, then dtor on the argument. With the optimizer on, it all gets inlined into the global initializer. Versions up to VS2017 15.4 generate quite a lot of chaff testing for invalid conditions that are impossible, but 15.5 generates much shorter code: ??__Emc@@YAXXZ (void __cdecl `dynamic initializer for 'mc''(void)): 00000000: 83 EC 18 sub esp,18h 00000003: 68 00 00 00 00 push offset ?lval@@3V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@A 00000008: 8D 4C 24 04 lea ecx,[esp+4] 0000000C: E8 00 00 00 00 call ??0?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@QAE@ABV01@@Z 00000011: 0F 10 04 24 movups xmm0,xmmword ptr [esp] 00000015: 68 00 00 00 00 push offset ??__Fmc@@YAXXZ 0000001A: 0F 11 05 00 00 00 movups xmmword ptr [?mc@@3VMyClass@?A0x01787c46@@A],xmm0 00 00000021: F3 0F 7E 44 24 14 movq xmm0,mmword ptr [esp+14h] 00000027: 66 0F D6 05 10 00 movq mmword ptr [?mc@@3VMyClass@?A0x01787c46@@A+10h],xmm0 00 00 0000002F: E8 00 00 00 00 call _atexit 00000034: 83 C4 1C add esp,1Ch 00000037: C3 ret The code corresponds to a call to string(string const&amp;) and an inlined string(string&amp;&amp;), so it's still not doing the desired optimization, but 15.5 is at least able to recognize that the dtor is a no-op on the moved-from object and optimize it away, and the practical difference in the end is just a handful of extra move instructions. Sadly, I couldn't get it to recognize that 'lval' was a constant and propagate it, which would have potentially reduced the result to zero instructions (pure initialized data). 
Woops! I forgot, `/O3` should be `/O2` for MSVC. With that set correctly, it's optimized out just fine - Just not by default like Clang.
 Got a bunch of emails from MSVC closing bug reports, mostly related to SIMD-- nice job MS. I guess it is worth reporting these issues afterall. 
Yes. This here is about `fclose`. [Possibly interesting discussion here](https://www.reddit.com/r/programming/comments/7gcu42/why_blank_gets_you_root_on_macos/dqjo85l/?utm_content=permalink&amp;utm_medium=user&amp;utm_source=reddit&amp;utm_name=frontpage). For `close`, the [situation is the same](https://linux.die.net/man/3/close) &gt; If close() is interrupted by a signal that is to be caught, it shall return -1 with errno set to [EINTR] and the state of fildes is **unspecified**. If an I/O error occurred while reading from or writing to the file system during close(), it may return -1 with errno set to [EIO]; if this error is returned, the state of fildes is **unspecified**. (Emphasis mine). So this is tricky. If you have read the above linked discussion, you might agree by now that *cleanup, including external resource disposal, must be a no-fail operation* (from the program design standpoint, of course; we can't change POSIX nor many similar cleanup APIs). What can the application do if `close` fails then? Well: * inform the user through some log (if interactive, it can inform to terminal/GUI) and continue; from the above, **nothing** can be done with the problematic file handle anyhow (but see below) * std:: terminate; this is drastic, but can work for simple cases. What the application **must** do, though, is move all can-fail operations outside of the cleanup. So, for example, it must `fflush/fsync` explicitly (but oftentimes, when that fails, whatever larger processing is happening, that larger processing is dead in the water and the only thing to do is abort it). 
I hope we fixed them, not just closed them? LMK if the bugs are still around.
I clarified that response as soon as I saw it. We (I) promised to look at C conformance after C++ is done.
There are a few simple rules to follow that will help. - The less pressure you put on the cache, the more it can keep ready for you to use. I'm not saying to do all sorts of micro-optimisation (stay away from bitfields, in particular, unless you really know what you're doing), but its easy enough to minimize padding by reordering your members. Also, consider specifying the sizes of enums: most will fit in a char, and don't need an int. - Think about the order of member variables in any struct or class. There is a greater chance of the first 64 or so bytes to be in cache than for the next block, so try to allocate the most frequently accessed variables near the front. Also try to keep variables that are frequently accessed together, together. - Do more work on the stack. The top of the stack is always in cache, since so much is happening there. - If you need to store multiple of something, consider using a vector. Vectors are contiguous, so they play nice with the cache. Of course it helps if your access pattern is also in order of increasing offset... And as always, only optimize aggressively if you actually need it. If you don't need it its wasted effort, as well as a future maintenance hazard... 
My new job might actually pertain to micro optimization for maths so these will definitely be useful, thanks!
 Ah ya, they were marked as fixed( i have not downloaded 15.5 yet so haven't verified).
I am unique. I contain `unique()`.
My example, a worker thread in a graph-based program (every node has a thread and implements a virtual Process method to do some processing): while (!end) { T result; try { result = Process(GetItem()); } catch (std::exception&amp; e) { result = MakeErrorReply(); // log exception } SendReply(result); } A lot can happen in Process, yet I have no try/catch there. I just let all failures propagate to the top-level loop. IMO, programming with *anything* that relies on dynamic scoping is hard. With exceptions you "don't know" statically where and whether it is caught, with mutexes you don't know statically whether the mutex is locked when accessing shared data in some lower-level function. In that way, exceptions are like duals of mutexes: exceptions propagate bottom-up; protection scope of a mutex"propagates" top-down, but a single "unlocked" method does not know whether the mutex is locked or not. I tackle the mutex problem by passing a `std::lock_guard&lt;std::mutex&gt;&amp;` to methods whose precondition is that the mutex is locked. The method still has no idea where it was locked, but that doesn't matter. It gets a reference to a lock guard object, so the access is safe. The "problem" with exceptions? Well, I force myself to use RAII consistently so the code ends up having an "onion"-like structure. 90% of my exception handling happens at the 2 outermost layers of the onion.
IIRC you can use the double in duration cast directly.
Hey everyone, I don't want to start extra topic but does anyone else experience super long linking time when building x64 release? All other configurations are built in seconds, but x64 release stays stuck in "Generating code" phase for half an hour or even longer. If I let it finish then I will get correctly working executable.
Try disabling LTCG (link-time code generation) and whole-program optimization.
the idea is nice but supporting most of relational databases leads to lack of custom features that some developers got used to. For example `replace into` query in SQLite or `TOTAL` function
I don't like exception myself. But it makes code more comfortable. Look at `std::vector&lt;T&gt; get_all(...)` function - it returns a container of objects you are asking for. It is comfortable to chain it. If we'd make a lib with no exceptions we had to pass a result as a non-const reference - it would work but it is not comfortable. And why can't you break using no exceptions?
Thanks. It didn't help at first, but then I found that I'm using LTCG version of static libsodium. Because of that the linker was still restarting in LTCG mode. Without LTCG it links almost instantly. I've also found that replacing libsodium with non-LTCG version fixes the issue too and the rest of the project links with LTCG (hopefully).
1. If you do not know where the exception came from, you can just adopt a strategy in your code where each exception holds the name of the function it was thrown from as a data member. 2. Your second problem is "what happens when you use them incorrectly"? Really? What happens when you use any language feature incorrectly? catch (...) basically never triggers, and even if it does, you can at least print an error message to help you debug.
1. If you do not know where the exception came from, you can just adopt a strategy in your code where each exception holds the name of the function it was thrown from as a data member. 2. Your second problem is "what happens when you use them incorrectly"? Really? What happens when you use any language feature incorrectly? catch (...) should never trigger, and that's why you always get an error message if you just inherit from std::exception or if you have your own type that you use consistently to catch.
Yeah my application is a world generator for a game, so I would very much like a platform-independent seed. All fixed now, had to roll my own distribution functions. Note this also includes std::shuffle!
Not in Web Assembly. Exceptions are a really bad idea there (yet). 
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
What's wrong with simple fstream? Why would you want to overcomplicate things? And by the way, pass std::string by reference, it's quite expensive to do this your way.
Great step forward. Hope to see more in new releases ! I see one more interesting optimization in a placement new: __declspec(noinline) void placement_new(int* data) { new(data)int(5); } MSVC 2015 generates: test rcx, rcx je SHORT $LN3@placement_ mov DWORD PTR [rcx], 5 $LN3@placement_: ret 0 MSVC 2017 15.5: mov DWORD PTR [rcx], 5 ret 0 Less branches always better.
Here, works on all platforms where int is 32 bit signed complement, regardless of endianness: #include &lt;cstdint&gt; #include &lt;ostream&gt; #include &lt;istream&gt; #include &lt;vector&gt; void write_i32_binary(std::ostream&amp; out, std::int32_t value) { char array[4]; array[3] = value &amp; 0xf; array[2] = (value &gt;&gt; 8) &amp; 0xf; array[1] = (value &gt;&gt; 16) &amp; 0xf; array[0] = (value &gt;&gt; 24) &amp; 0xf; out.write(array, 4); } void write_binary(std::ostream&amp; out, const std::vector&lt;int&gt;&amp; vec) { static_assert(sizeof(int) &lt;= sizeof(std::int32_t), "fix me for your platform"); write_i32_binary(out, vec.size()); // should probably use different type here for (auto val : vec) write_i32_binary(out, val); } std::int32_t read_i32_binary(std::istream&amp; in) { char array[4]; in.read(array, 4); return std::int32_t(array[3]) | (std::int32_t(array[2]) &lt;&lt; 8) | (std::int32_t(array[1]) &lt;&lt; 16) | (std::int32_t(array[0]) &lt;&lt; 24); } std::vector&lt;int&gt; read_binary(std::istream&amp; in) { auto size = read_i32_binary(in); std::vector&lt;int&gt; result; result.reserve(size); for (auto i = 0; i != size; ++i) result.push_back(read_i32_binary(in)); return result; } (you should probably add some error handling though)
&gt; , I don't believe an object should destruct leaving it's scope Yes it should. The content may have moved, but you still have a live object at the same place.
You asked for random numbers, you got random numbers. Looks fine to me. ☺
clangd is an implementation of [language server protocol specification](https://langserver.org) and I don't see how it can exclude the usage of libclang, and vice-versa. clangd is just a proxy sitting between the client (IDE or an editor) and the implementation of the corresponding feature (i.e. go-to-definition). This project specifically does not use clangd but it sort of implements its own version of it (with literally the same idea behind it). [Feature list of clangd](https://clang.llvm.org/extra/clangd.html) though does not look that complete at its current state.
If you are not using exceptions, chances are you in charge of a large body of exception-free and exception-unsafe code. It is true that adding exceptions in a large system that doesn't have them is going to be a bit of a nightmare, and you'll likely feel like you have completely lost control of everything. However, if a program was written with exceptions in mind from the ground up it is a very different proposition. Bottom line, don't knock it until you tried it. Like with every language feature, exceptions requires a bit of planning to use properly. One tip is to only ever throw things that derive from std::exception; that significantly simplifies catch clauses (since `catch (std::exception &amp;)` will catch everything that could be thrown). Another is to split the program into tasks that either complete or get aborted, and ensure that such tasks are started from try/catch blocks. In practice this comes down to having a try/catch block around your event dispatcher, plus a handful of other relevant places. With that basic framework in place, throwing an exception is not a problem: it _will_ be caught and handled correctly. There's no minefield; every exception returns the application to the status quo, which is that it is ready to process events. Because you use RAII, no resources are left behind in case of an aborted task. The worst thing that could happen is that you get your task granularity wrong, and abort something that could just as easily have been partially completed. In that case you need an extra try/catch block somewhere. I have a 300K line body of source here. It was written with exceptions in mind. Let's have some numbers: - There are 790 places where an exception gets thrown. From a quick glance, the majority of these seem related to input validation in situations where I can at least have an expectation that the input is going to be correct, and where not having that input leaves us stranded. There are also a large number related to unlikely low-level (OS-)errors. - The application mostly uses std::runtime_error. In addition to this, there are only three other types of exception defined, all derived from std::exception (directly or indirectly), and all for situations where handling shouldn't be mixed up with handling for std::runtime_error. - There are 430 try/catch blocks. After searching for five minutes I have yet to find one that has a function that's not 'reporting an error to the MMI'. - Programs may run for months without ever throwing an exception. Or they may end up trying to access a resource that doesn't exist and throw one every few seconds. One subsystem, which has as its goal to calculate lots of values quickly, actually uses an ADT-like approach, with values existing in one of three states: valid, invalid, or non-existent. A calculation with a non-existent value is automatically aborted. In the past the abort used exceptions, but this proved slower than just completing the entire calculation and returning a non-existent value. However, does that mean that I want to rip out exceptions everywhere? Hell no! ADTs are appropriate for the calculation subsystem because it matches the way it works. It is completely inappropriate for the vast majority of the source, where the ability to report an error from twenty levels down without having to manually transport it up the callstack is gold. In summary, exceptions are a phenomenally useful tool, and this drive to completely eradicate them, or to replace them as the default error mechanism in C++, is IMO extremely misguided. In my experience, ADTs should be an _option_ for a small set of problems, not the default for the whole language. 
We use exceptions to gracefully abort on error, present a reasonable error dialog, close resources etc. Doing all that with return codes would cause massive code bloat as you'd have to add return codes everywhere.
I'd like to add one thing here: the game industry is not really known for its robust error handling, is it? An industry that has such a bad track record for robustness in its error handling and recovery should, I feel, not be taken as the standard to which the language should be evolved.
&gt; Beyond the cost of using exceptions versus ADTs, however, there's a reasoning/usability issue. I'm pretty sure that a lot of C++ code doesn't care that much about 1% penalty if it gains significant robustness in exchange. &gt; Therefore, what I'd really like to know, is which of exceptions or ADTs lead to more robust error handling. +1 Real code, real projects, github links.
I use the following guideline for figuring out where exceptions should be thrown: each next statement in the source should have complete confidence that the previous statement completed succesfully. I want the 'productive' path to be clean and obvious, without being cluttered with endless error handling and resource tracking. This is why exceptions (and RAII) work so well: you can be confident that every statement completed succesfully, because if it hadn't, an exception would have been thrown and we would never have reached that point in the source to begin with. 
&gt; With compilation database you need to (remember) to regenerate it every time you add a new source file to the project or change/add an option (say -I). In CMake this can be automated by including a `set(CMAKE_EXPORT_COMPILE_COMMANDS ON)` directive into the build process so you don't have to re-generate the database manually. But still, this approach obviously has a downside of having to trigger the build in order to get up-to-date compilation database. What one might do for example to circumvent this downside is to [monitor the file system events](https://linux.die.net/man/7/inotify) (i.e. file/directory created/deleted) and re-generate the database without having to trigger the build. Re-generating the database should be a low-cost operation. Alternative to the compilation databases would be a buildsystem server sitting in the background which could be queried at any time for a list of source code files which are part of the project, corresponding compiler flags, targets ... basically everything that is part of or influences the source code model. CMake has implemented such thing and it is called [cmake-server](https://cmake.org/cmake/help/v3.10/manual/cmake-server.7.html). I am not sure though if it handles the changes done to the files and/or compiler flags or it still has to be handled by the external component. One thing which is a big downside of JSON compilation database specification and which influences the work of the tools built around it is the fact that it *does not include* any information about the header files. It rather includes information about the translation units only (which from the C++ code model standpoint makes sense). That basically means that every tool that wants to parse the header file will not be able to extract any information about it (i.e. compiler flags) from the compilation database itself and will therefore have to implement some kind of heuristics to find its way through. This may be approached by reusing the compiler flags of corresponding translation unit but problems still exist: (1) C++ source code model does not have a definition of 'corresponding translation unit'. How to define one? Does it have the same basename but has the .cpp/.cxx/.cc extension? C++ source code model does not tell us anything about it and we can only make the best guess how to find the 'corresponding' one. (2) Single header might have implementation spread across different implementation files. (3) Single header might have multiple implementations existing at the same time (i.e. different target configurations). (4) There is no 'corresponding translation unit' at all for the given header? I.e. implementation is completely done in the header file. (a) We might borrow the compiler flags from other existing translation units. (b) But then what about the header-only libraries ... :)
There are macros to detect whether the fma implementation is "fast" or not, see: http://en.cppreference.com/w/cpp/numeric/math/fma 
Eigen runs on the CPU, this run on the GPU.
FYI, `flat_map` *is* an adapter, not a container.
That looks very promising in theory, but in real life the speedup from improved layouts is very moderate. See http://bannalia.blogspot.nl/2015/06/cache-friendly-binary-search.html
One point that I miss in this discursion. If one want that you code run on GPUs and comparable devices then exceptions is not an option, GPUs simple do not support them. 
So is there any good library/code snippet with portable distributions? 
I believe [Boost.Random](http://boost.org/libs/random/) produces the same output across all platforms, though that doesn't seem to be documented directly...
It seems like the code is only half baked for a blog post that could have waited for you to actually figure out if it was worth a blog post before making a blog post. 
Without revealing secrets, can you give some info about the implementation. Eg. does the runtime maintain a thread pool for this?
Cuda directly allows same code to run on device or host ("CPU" and "GPU" respectively). NVCC (Cuda's Compiler) compiles device code it self and forwards compilation of "CPU" code to the host compiler (GCC, Clang, ICC, etc). Decorating a function with __device__ will compile it for device and __host__ will compile for host. __device__ __host__ will compile it for both. There is also __global__ decorator which not only compiles a function for device, but it also makes a kernel (which can be called with &lt;&lt;&lt;&gt;&gt;&gt;() syntax) Now Eigen does use these decorators and provides some code alternatives that gets compiled only in device contexts where needed. Which makes it support Cuda (https://eigen.tuxfamily.org/dox/TopicCUDA.html) In fact with SYCL it even runs on GPU (or any device) via OpenCL as well. I believe there is an ongoing effort to support this workflow as well.
And in the newest VS IIRC
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7i6egp/looking_for_basic_c_help_on_some_coursework/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Why not just generate a file of random numbers?
It doesn't apply. You don't assert on parameters passed by the user, only ones passed internally. 
Should we really care that random numbers are different random numbers depending on the implementation? Same with associative containers - standard says they should have logarithmic complexity, not what tree structure they should have (although all implementations use RB-tree)
&gt; As such, little (if anything) would be gained by specifying the implementation instead of the interface. If the distribution is specified then if you run your program with the same seed, then you should get the same results on any platform and compiler. Repeatability across platforms is very useful, and we have it with the engines, but not the distributions.
Thanks for the hint, now code uses std::chrono::duration&lt;double, std::milli&gt; instead of first count integers in std::microseconds then converting to double.
Why is it important to know where the exception came from?
I'm playing with the new Google Test feature (cool), and I have trouble understanding why this code fails: TEST(Test, random) { std::mt19937 gen; std::vector&lt;uint32_t&gt; v16(1 &lt;&lt; 16); std::uniform_int_distribution&lt;int&gt; rpos(0, 1000); int pos = rpos(gen); v16[pos] += 1; } The last instruction generates a "Access violation writing location". This is the assembly: v16[pos] += 1; 00007FF73B286B06 mov rax,0FFFFFFFC00000000h 00007FF73B286B10 movsxd rcx,r8d 00007FF73B286B13 mov r8,qword ptr [v16] 00007FF73B286B18 add rax,r8 00007FF73B286B1B inc dword ptr [rax+rcx*4] Why that long constant is added to the address of the vector? What I'm doing wrong?
Thanks! I've always wanted to try CUDA and GPU/SIIMD in general out, but never got around to it... so I'm afraid I'm completely ignorant on this matter. Once I get the partitioning fixed on my OS, I'm going to reinstall it and give it a whirl.
Things like that really slows down the adoption of stl features. I also noticed that when creating my own simulations. Now wait for bazillion deterministic distribution libraries.
Dude, Louis wrote you a super nice, not-at-all snarky post with some suggestions. He knows a thing or two about influencing C++, wouldn't you agree? Your response was uncalled for, you're just being rude.
Yep. If you want a portable one, use Boost.Random instead.
I was not aware of this! Wow. That's not confusing at all. Unfortunately, having taken a look at the contents and preview of the two books, C++ Primer _Plus_ looks like an old C++ book that's had a few notes about modern C++ thrown into it, while C++ _Primer_ looks like it teaches _modern C++_. I much prefer the look of the other book, unfortunately. That's not to say that Primer Plus is useless, there's significant overlap between the two, but Primer Plus's information on dynamic memory allocation looks very old-style, and seems to omit the C++ algorithms altogether.
I'm a long-time professional game developer myself, so I guess I'll answer this. I'm not sure why you'd make a blanket statement about the entire game industry's error handling practices. How often do you see a modern console game actually crash? It seems to be pretty rare in my experience. Some poorly-built PC games are occasionally less robust, but I think you have a wider variety of talent combined with a wider variety of hardware and software stability, so that makes sense. But beyond overly-broad generalizations, you have to consider the fundamental differences between a videogame and, say, a document-oriented application when considering what exception handling brings to the table. Within an a normal application, it's desirable to catch and handle most situations gracefully. For instance, if you're loading a document, there may be a thousand unexpected things that could go wrong that could cause the document to fail to load, many of which are out of the application's control. In almost all cases, you'd want to handle this and simply fail to load that document, give the user an error message, while preserving the application state so other documents aren't affected. You certainly don't want the application to crash, and for the user to lose any existing data they've been working on. Moreover, the crash is unlikely to have even affected anything else in the general application state. The event-driven nature means the problem is probably isolated. What happens if a videogame is loading world geometry and loads badly formatted physics data? Nearly every path in a videogame is a critical path, and can tolerate no failures due to its reliance on system-wide state and data sets. As such, it does no good to have some carefully contrived fallback, as there's absolutely nothing the user can do about this, nor is it there even any point for them to continue the game. After all, if that data can't be loaded, the game can't likely can't proceed. And because of the real-time, multi-threaded, highly interdependent nature of a game world and its data, after a critical failure, there very well may be almost nothing you can now rely on. As such, for many types of errors where you'd normally be handling exceptions, it's far better to crash hard, grab stack information via a crash handler, and report the issue back to the developer so it can be fixed. Many game developers, in my experience, will use exceptions where they make sense, such as in our development tools. Personally, I'd never suggest that our own priorities should be considered "normal" in any sense, or dictate the direction of C++ development, except in that we'd just like to be able to turn exceptions off for our own projects, partly since there's still a non-zero cost, and partly for now-legacy reasons (large bodies of non-exception-safe code). But on the other hand, if a game developer makes a blanket statement like "exceptions are evil", I'd just argue they don't have a broad enough perspective. 
You should have a look at Kokkos (https://github.com/kokkos/kokkos), this makes using GPUs a lot easier.
20 years since C++98 roughly. 1 hour/day. Nit enough time.
Because if you don't know where an exception came from, you cannot reason locally about behavior of your code. There are situations where local reasoning is nearly impossible, but they are few and far between. "File name is wrong" is not such a case. Exceptions, by introducing global scope "come from", make huge amounts of code fundamentally non-local. Making code non-local is bad, unless justified by a huge return.
I'm not talking about "knowing where an exception came from" at *runtime*, I'm talking about when reading the code. You see a try/catch, and being able to understand what state the program could be in prior to arriving at the catch requires analyzing your entire code base. You cannot reason locally about the behavior of the try/catch, because you have a come-from instruction. The best you can do is you can generate a series of invariants that your entire code base must maintain. These invariants cannot be statically proven to hold in C++ using the language itself, and as the cases where they are wrong are exceptional and non-local, cannot practically be tested. And yes, what happens when you use something incorrectly *matters*. Off-by-one errors and buffer overflows when working with raw buffers is an error, but *they happen*. This is a reason **not to use raw buffers**. The lesson isn't "that doesn't matter, that only happens when you use them incorrectly", it is "only use them when you absolutely have to, and heavily vet the code in question". If you always inherit from std::exception, how is that different than catch(...)? You can dynamic cast it or extract a string. Still, no static information about what the error was. I think possibly the problem is that we are talking about different kinds of code. 
Would anyone here be able to recommend some prime examples of modern Qt in action that are reasonably easy to access and experience? My knowledge of Qt is limited to the Trolltech days and it looks like a completely new product now. Would love to learn more.
You mean code or examples because the example set that comes with Qt is extensive and pretty much covers most major features. As for real life code I think the most used Qt codebase I know of is the VLC (videolan.org) client and their git is public as well.
I thought i was clever for doing number 1... Guess i'm a little 2 late for be the early bird.
&gt; Dude, Louis wrote you a super nice, not-at-all snarky post with some suggestions. No, it was f off. I do not have the time or the knowledge or the money(I have but I prefer not to burn it on this) to write a proposal that will be ["considered"](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3613.pdf). You could say that every idiot who thinks of adding garbage collection or inline Haskell or should not have the ability to bother experts and waste their time. And I agree. But there is also a downside to this. Why should I waste time and money just for the joy of being told that flat_map that has linear insert and iterator invalidation for erase is a great thing to add to C++? So serious question: If I was to waste a lot of time and a lot of money on writing a paper opposing std::flat_map how would you rate my chances a) nothing changes b) std::flat_map gets better API c) std::flat_map is rejected My personal guess is 100% 0% 0% Also Louis argument about him enjoying teaching people about flat_map is useless. *I* enjoy teaching people about boost::flat_map. So what? People need to be able to ignore their personal preferences and their bubble(Louis does not even closely works with average C++ developers/company). std:: should provide reasonable defaults that average developers can use relatively easily. It is silly to expect that every developer writing in C++ has me or Louis or you to teach them about flat_map. People just quickly google stuff, use intellisense and assume stuff works as they expect. Now you can get all outraged how this is not proper development, but you are just shooting a messenger. World will not change just because you and I think that you should be not able to write code with container if you do not know complexity of all it's operations.
I'm curious about both but I was asking about real life examples. Good to know about VLC, I use that all of the time and didn't realize that -- will check out the code. Thanks!
paging u/AndrewPardoe as I assume this is probably confusing to users so you may want consider adding some warnings in cases like this. I know it is hard since there is no magic number when link is too long, but I am sure that smart MS devs could come up with a reasonable heuristic. :) 
What are the recommended resources for learning Qt these days? Everything I see commonly suggested is from the Qt 4 days, surely there must be newer stuff? 
&gt; There is a combo just under the number of comments, where the sort-type can be changed. which is ignored in contest mode ...
Nice but it is either missing or I skipped, the common initializer list (bug)[https://www.youtube.com/watch?v=rNNnPrMHsAA#t=11m40s].
We're aware of the issues with libsodium. It'll be fixed in 15.6
From [Wiktionary](https://en.wiktionary.org/wiki/robust): &gt; (1) Evincing strength and health &gt; (4) Sensible (of intellect etc.); straightforward, not given to or confused by uncertainty or subtlety &gt; (5) (systems engineering) Designed or evolved in such a way as to be resistant to total failure despite partial damage. &gt; (6) (software engineering) Resistant or impervious to failure regardless of user input or unexpected conditions. So I would say error handling that is conducive to writing robust software, aka software which acts as per specification, even in the presence of rare edge cases.
https://qmlbook.github.io and the official docs
&gt; You cannot reason locally about the behavior of the try/catch, because you have a come-from instruction. Sorry, but it makes no sense to talk in such generalities. When catch executes, you know exactly what failed: whatever was in the try block. If you need some specific information to handle the error, you put that into the exception object. Where exactly the error happened should not matter, what matters is the cause of failure. 
I don't think you have to document arcane stuff like: vector&lt;string&gt; s{{"abc"s, "def"s}}; //will interpret the "abc" and "def" as iterators and crash. vector&lt;int&gt; i((5,10)); // will intiailze vector with capacity 10, instead of 5 elements with value 10 You could also document the o
Qt is used in many products whether they're opensource or not: * [KDE](https://www.kde.org/develop) Desktop environment * Ubuntu's [Unity](https://wiki.ubuntu.com/UnityNextSpec) framework used both for desktop an mobile application (Ubuntu Phone) * [VLC](https://www.videolan.org/developers/) * Skype For Linux (closed source) And many more. There is a list of application built with Qt on [Wikipedia](https://en.wikipedia.org/wiki/Category:Software_that_uses_Qt)
There is this Youtube channel ([Void realm](https://www.youtube.com/watch?v=6KtOzh0StTc&amp;list=PL2D1942A4688E9D63)) where you can find a lot of tutorials for Qt. The first ones date back to 2011 so they might have aged but the latest one was in 2016. They are all about Qt 5 and "modern" usage of Qt.
just go to http://doc.qt.io. Qt has one of the, if not the best, documentations of any library i've ever seen. When i learned C++ i directly dove into Qt and gui programming because i was into 3D animation and wanted to write a 3D application and i could learn how Qt works and how to work with it simply by looking at the docs. It really is that good! Once i started to use other libraries, i was quickly disappointed to find out that that level of documentation is in fact not the standard (looking at you boost and OpenCV, even though i've heard people say, that OpenCVs documentation is supposed to be good *shudder*) For almost any class they have extensive explainations including example code and cross references.
&gt; If you do not know where the exception came from, you can just adopt a strategy in your code where each exception holds the name of the function it was thrown from as a data member. I so love sarcasm; thank you that made me laugh so hard. The idea of code logic taking decisions based on the function name present in the exception is just hilarious! &gt; Your second problem is "what happens when you use them incorrectly"? Actually, it's a generic issue with error handling. Signalling an error is easy. **Recovering** from the error is **hard**. Most people are way too *happy-go-lucky*, catching the exception and continuing like nothing could possibly be wrong, but reality is that it's very likely that there's some mess, somewhere, that was never cleaned-up and will lead to further errors down the road.
I think you should add a part about the problems with std::vector&lt;bool&gt;
Reading the latter site: &gt; The Eytzinger layout is almost never slower and usually faster than a sorted array—even for very small array lengths. This is because the Eytzinger child index calculations 2*i+1 and 2*i+2 are as simple as those done in binary search. It would also be interesting to see how *separate* underlying storage for keys and values affect the speed. Small values are not so bad, but large values definitely "clog" cache lines (whatever the layout), and therefore there's an argument for pushing them off-band so as to keep the core look-up faster.
Also exceptions allow the code to "rewind" across many functions, however in a game there is not really a big need to do so. If an error occurs due to a user activated action, there is no "return point" for an exception to rewind to and reset the state of the program. An error would need to be detected immediately and then stored or partially handled and then the results of which show up in the next frames, for example a user facing message being displayed. Essentially games are constantly forward moving and exceptions effectively rewind the state of the program. Not only that but the amount of these types of errors is quite low, which means that the error paths in your code are very small and contained, very easy to make with a strong error guarantee and don't affect other code. To open up huge parts of your codebase so that you can send exceptions across many functions is just not really necessary. Other more fatal errors like you talked about just crash and don't unnecessarily export their error to the rest of the program. Of course you could still use exceptions in a very contained way if you wanted, I guess that's down to personal preference. I prefer to just cut it all out and not have to think about it. But lets say you do want to throw exceptions across many functions, you have to consider that games have a lot of state, a lot of state which could extremely easily get corrupted when an exception is thrown across functions with only a basic error guarantee. RAII alone will not help you here. Making a lot of functions have a strong guarantee in a game has too high a performance and code cost. There's no point when the whole thing can just be sidestepped.
The only number I have is that I turned one part of my C codebase into a C++ implementation with RAII, exceptions and STL (e.g. switched some qsort to std::sort, used std::vector, not a manually-managed C array) and got marginally smaller code. C code had true/false + errno error reporting, which I didn't change (that is, the exception class was just a number). So it was truly a functionally equivalent code, also very little changes in the internal structure (very little internal structure, too). I did not care about performance and have no numbers on that.
ScopeGuard and friends (unique ptr with a custom deleter) makes writing guard code easier and *exceedingly* succinct. That said, one benefits from these babies *even if no exceptions are used*. From my perspective, lsrge codebases are **exactly** where exceptions do their magic.
we are about to fix another big ol' batch for 15.6p1 and another batch in 15.6p2. yes please continue to file. :) we have to prioritize of course, but we really care about these.
Code logic should not depend on the function name. It's just for logging purposes. You can add additional data for recovery if you need it.
&gt; I'm curious about both but I was asking about real life examples. Blizzard's [Battle.Net](https://i.kinja-img.com/gawker-media/image/upload/s--Im3r8WBB--/c_scale,fl_progressive,q_80,w_800/px2wqtkhkhhzsrhi7ppi.jpg), [Allegorithmic Substance Painter](https://support.allegorithmic.com/documentation/download/attachments/20316164/sp_260.jpg?version=2&amp;modificationDate=1493483369290&amp;api=v2)
 while (!end) SendReply(MakeReply()); T MakeReply() { try { return Process(GetItem()); } catch(exception&amp; e) { Log(e); // noexcept return Error(e); // noexcept } } (Exercise in futility, sorry 😀)
I was poking around in the program files and was like... Wait a second, that's Qt! Another one I use daily us MuseScore.
&gt; Because if you don't know where an exception came from, you cannot reason locally about behavior of your code. What reasoning is needed. The code inside the try block threw something. It shouldn't matter which/where (assuming you used RAII well). It should be like none of that code happened. Assume the first line of the try block threw. This: try { some_depp_callstack(); more(); etc(); } catch (...) { } is as local as try { if (coinflip()) throw x; } catch (...) { } 
Ah, yes :) However, IME, expressions like `SendReply(MakeReply())` tend to be less debuggable with optimizations than with explicit variables. The optimizer should be able to do the same job, but I suspect that debug info is better with more intermediate steps.
Are those done with QtQuick?
So "robust error handling" really means "robust software". ie resistant to error/failure. Whereas "robust error handling" would be resistant to error/failure in the error handling itself". I don't mean to be pedantic, that really was my confusion.
FWIW I don't agree with your parent's statement; I agree it's too broad and unjustified. That said, I still think that saying that there's non-zero cost as an absolute (assuming you're talking about runtime cost, and not binary size) is hard to justify, based on empirical data and based on the fact that error handling of any kind has costs of its own. I think you are also underestimating what exceptions can do; the exceptions I use grab a full backtrace when they are thrown, which can be logged and such. There are situations where a core dump is preferable of course, it just depends. You could imagine a multi-player game, where there is a penalty for disconnecting. Catching an exception could allow the game to spend a special signal for a "legit" disconnect, so the user isn't penalized for their game crashing. Etc. Games are generally reliable, but there are definitely cases where a game crashes and you lose progress. If exceptions are used, it might be reasonable to make an emergency attempt to save game data "on the way out", so to speak. Then on startup a special routine could try to load that file, carefully guarding against the fact that it may be corrupted. These are just examples of course, maybe they are very bad, but I think there are bound to be cases with some merit. It seems to me that there is a fraction of game devs (I honestly don't know what fraction) that are quite hostile to more modern things in C++. Even something like `make_unique` vs `new`. It's either a large minority, or a very vocal one. I have to admit I've never understood that camp (but, maybe, hopefully, they are diminishing now?).
pretty sure for both
Telegram sources on github
Also, a very useful plug-in: integrate gcc.godbolt.org directly in QtCreator ! https://github.com/dobokirisame/CompilerExplorer
I thought about it, but the article was getting too big. I didn't think I could do it justice with just a couple of paragraphs, so I decided to write another article about it if I saw enough interest. Maybe I could add a short warning to this one...
Where/how did you learn C++? I only looked at [this file](https://github.com/VedantParanjape/Chat-Server-and-Client/blob/master/Server/Program%20Codes/RunMeOnlyOnce.cpp) and there are already horrible mistakes
I am not quite sure what you mean by the second. I find that the difficult part of error handling is not the signalling, but the recovery. Anything that leaves behind half-changed state is a pain. In this sense, I see the role of error handling as: - either throwing one's arms up and aborting, - or cleanly recovering from the error condition. Too often I've seen code which discards the error and merrily goes on without pausing to consider whether it's in any state to go on. It seems to me that getting robust software requires: - being aware of the potential errors that may arise, - deal with them appropriately (passing them to the caller may be completely appropriate). Isn't it a failure to not appropriately deal with an error? :D
Diagnostics is a good suggestion, /u/Z01dbrg, thanks. By the way, Terry is on our team. He knows of what he speaks : ) 
Yes, examples that come with framework and [on site are very good and updated mostly](http://doc.qt.io/qt-5/). with good tips too.
I didn't include it. I used operator""s instead of const char[]s. If you use operator""s, the compiler gives you an error, so there won't be a bug. However, I can see how someone could use two const char[]s and run into problems. Thanks for mentioning it.
How are return codes any different? If you call something that returns an error value, that doesn't mean the real error happened just one stack frame down. The function you called could have called another function, which itself called another function, which itself called another function, which itself called another function... which failed... and the error code just got returned all the way up the stack. Which means when you get a return code, you don't know where it originated from.
"robust X" means "deal well with errors about X", so "robust error handling" is "deal well with errors about error handling". ie meta-errors. (or at least that's how my brain was tripped up) Anyhow, "half-changed state" is a pain. So you want strong exception safety. (see https://en.wikipedia.org/wiki/Exception_safety) But that isn't always easy. Basic exception safety is "good enough" in many cases - particularly the "in between cases", but ensure strong guarantees at the doc level. ie Paste in MS Word. During paste, it might format some text, but then need to align an image, etc, ... any of these can fail/throw, and maybe even leave things in a half-way state. That's fine, *if*, for example, non of that in-between state was part of the final document. That might mean the document is duplicated, pasted, then swap if successful. Or some equivalent but less drastic technique. You need to build an undo-able and transaction-based document. Yeah, it probably isn't easy, particularly if you don't start with it. Once you have that in place, new features can be added without fear.
I agree that we should not document every possible way to make an error. I'm not even sure how useful it would be. A couple of notes on your examples: The first one won't compile because of the &lt;a href="http://swdevmastery.com/a-simple-way-to-make-your-c-programs-easier-to-read-now-suffix-for-basic_string-literals-this-little-known-feature-can-be-extremely-valuable/"&gt;operator""s&lt;/a&gt;. The error you are thinking of is with "C strings", I think, like: vector&lt;string&gt; s{{"abc", "def"}}; My compiler, clang-900.0.39.2, gives me a warning on the second one.
Honestly, the only documentation I've seen that's better is MSDN's
Any game which uses Random Procedural Generation depends on the reproducibility of data generation, given an invariant (usually the "seed"), and if different compilers/platforms produce different numbers for the same input, that makes it difficult to guarantee reproducibility. The fact that engines guarantee behavior is good (and the very least I'd expect from any PRNG library) but it would be ideal if we at least had the basic distributions (I'm thinking `std::uniform_int_distribution` and `std::discrete_distribution` should be the minimum, but you could justify stuff like `std::uniform_real_distribution` as well) to guarantee that those behaviors are consistent across compilers and platforms.
Ahh indeed. Reproducing game bugs is harder indeed.
Your website should use HTTPS. &gt; It’s basic purpose Its. &gt; Unlike a static array, a vector automatically grows and shrinks as necessary to make room for new elements inserted or deleted. A vector's size shrinks, but its capacity doesn't (unless you call `shrink_to_fit`). &gt; You can visualize a vector like this: This is kind of a "Bohr model of the atom" visualization. A more accurate one would depict the capacity. (I prefer to depict vector as a triple of pointers instead of a pointer and two integers, but the representations are equivalent.) &gt; for (auto i = 0u; i != v.size(); ++i) { // v is a vector This is a slight mistake because it mishandles 64-bit lengths. `size_t` should be used instead of `unsigned int`. (The distinction between `size_t` and `size_type` is not important in non-library code.) &gt; If you want to have bounds checking, you can use at() This is not a good idea. Exceptions are for recovery, but attempting to recover from bogus indices is a path to misery. Instead, be absolutely confident that your indices are valid. (In debug mode, STL implementations will often check indices in `operator[]`; MSVC does.) &gt; if (e &gt; 0) Should use braces. &gt; // begin is defined in &lt;iterator&gt; This is not actually necessary to include as `&lt;vector&gt;` is guaranteed to drag in these helpers. &gt; // begin points to first element Your article is switching between referring to `v[2]` as the third element, and `begin()` as the first element (instead of zeroth). It is important to be consistent. &gt; std::find(v.cbegin(), end, n); What happens if you mix `begin` and `cend`? This confuses many users. &gt; std::for_each() This is the world's least useful STL algorithm, especially now that we have range-for. (Parallel for_each redeems it, somewhat.) &gt; (auto elem) This copies :-( &gt; Similarly, when deleting an element: This terminology is potentially confusing due to `delete`. I recommend consistently talking about erasing. &gt; are sometimes refer as stack operations. Referred to. &gt; // the state of v becomes undefined Undefined behavior means that the whole program is affected and the Standard no longer says anything about what will happen (even backwards in time). This is different than an unspecified state of a single object. &gt; You may also insert new elements before a specified location: Not especially efficient, though. You should explain the STL's "insert before" convention, and how this works with "before begin" and "before end". &gt; the vector allocates a new chunk of memory of twice the size of its current memory. Untrue. The Standard doesn't guarantee 2x, and MSVC uses 1.5x. &gt; allocate all of the required memory at once with reserve(): This is risky as careless use can trigger quadratic complexity. &gt; generate_n_random_ints(unsigned n) Ditto 64-bit. &gt; In and retain a copy: pass a const lvalue reference, f(const vector&lt;T&gt;&amp;); Not optimal for modifiable rvalue inputs.
&gt; In and retain a copy: pass a const lvalue reference, f(const vector&lt;T&gt;&amp;); Or pass by value `f(std::vector&lt;T&gt; v)`. And then move it into place (wherever you are retaining it). Then `f(g())` can be more efficient. The copy is never made. Also, in "when not to use vector", my rule is: &gt; Remember, when choosing a container, vector is best &gt; Leave a comment to explain if you choose from the rest! ie leave a comment in the code why the container type was needed - if you don't mention what the code depends on (ie "use list because we need stable iterators") someone will later change it to vector and break things. This is just a special case of "comments tell why, not what".
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7i9736/need_help_with_this_c_analysis_problems/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I'm well aware of the capabilities and benefits of exceptions. As I said, game developers often use exceptions in non-runtime code when it makes sense. A lot of my tools are written in C#, which has fairly similar exception capabilities. My comparison of the usefulness of exceptions in games versus document-centric applications was not an abstract theory - it's based on actual experience when creating both. I can't speak for all other videogame developers, but you seem to be trying to convince me of the usefulness of exceptions when I don't really need convincing. I'm already sold, because they're quite useful in my tools. But that doesn't erase the fact that they still don't have a zero cost, and a non-zero cost still doesn't appear to be worth it for many game developers. And due to the nature of how videogames are typically built, exceptions are just one of those things we've done without without. It would be nice if we didn't have to, but for videogames, it's not a terrible loss for reasons I described in detail above. Everything you describe is possible to do without exceptions. For instance, with structured exception handling on Windows, or signals on POSIX-compliant systems, you can still trap and handle almost any error in a completely controlled manner. Game developers aren't necessarily hostile to modern C++, so long as it doesn't come with an unnecessary performance penalty. So, you'll see game devs avoiding the PIMPL idiom, as that creates a penalty in terms of cache coherency and indirection costs. That's not some unfounded dogma, just the practical reality that we need to simulate an entire world in under 17ms for each and every update tick. That relentless focus on the need for real-time efficiency tends to color your judgment, and by necessity, encourages somewhat conservative approaches. So, before you judge game devs too harshly, keep in mind that we live in a world where we even supply our own allocators, because the default allocators are far too inefficient for us, or don't give us enough control or monitoring. We tend to break all kinds of rules for good OOP design to improve cache coherency. We examine the costs and potential drawbacks versus benefits of smart pointers with a microscope, because we live in a world where a fraction of a microsecond could be considered a big performance penalty.
The idea is good in principle, but your implementation isn't portable, because vector iterators aren't guaranteed to be classes you can inherit from. A vector implementation where the iterators are plain pointers would be legal (although I understand this is discouraged for various reasons, and I don't think any of the major implementations do this); even if the iterator is a class, it might be `final`. Most implementations of adaptors like `deref_iterator` hold the underlying iterator as a data member.
&gt; for (auto i = 0u; i != v.size(); ++i) { // v is a vector &gt; &gt; &gt; &gt; This is a slight mistake because it mishandles 64-bit lengths. size_t should be used instead of unsigned int. Screw it, use: for (int i = 0; i != (int)v.size(); ++i) { // v is a vector And add to your explanation "use range-based for to avoid arguments about signed vs unsigned, etc". 
As you may have gathered from my post, I work in HFT. Our performance constraints aren't any less harsh, just different. I dislike PIMPL as well, that's surely not the kind of thing I'm talking about. As I gave a concrete example: I've seen game devs argue against unique_ptr, which has no additional cost over new/delete (I really hope when you wrote, the drawbacks vs benefits of smart pointers, you were not including unique_ptr). I've written a blog post inspired by one of Jon Blow's videos on how to do joint allocations in C++, in a reusable and less error prone way than doing all the arithmetic by hand. That sort of thing. Like I said I don't know how common it is to take it to such an extreme but it does seem to happen. &gt; But that doesn't erase the fact that they still don't have a zero cost, and a non-zero cost still doesn't appear to be worth it for many game developers. Sorry, you still haven't clarified what "costs" you are talking about, but assuming you mean runtime cost: I still don't accept writing this casually. I know many people who are experts in performance, working at far lower timescales than 17ms, who would say that the cost of unthrown exceptions is basically zero. And in particular, typically lower then replacing the exceptions with another error handling mechanism (repeatedly checking return codes up the stack). Also, FYI this attitude of: game devs are special snowflakes because our performance requirements are so crazy, is not appealing. Many people in C++ have crazy performance requirements; not to say that they are all equal, but if they weren't crazy they probably wouldn't be using C++!
And that, kids, is a small taste of what it feels like to have your code/text/proposal run through LWG. :-)
You are, of course, correct :-). Encapsulating the iterator is a more technically correct way to go, but my god, random access iterators have *so much* interface you would have to forward. An amusing way around this that is exactly shareable among all iterator adapters is to write a template that encapsulates an iterator and forwards the entire iterator interface *once*, and do not mark it final. Then, inheriting from that template could be reused by all adapters. That said, if you are not writing hardcore library code, then I would still probably just inherit and test it on my 2-3 compilers of choice.
&gt;&gt;allocate all of the required memory at once with reserve(): &gt;This is risky as careless use can trigger quadratic complexity. Is there any chance you could elaborate on this one? I've always (evidently naively) assumed that if I know at run-time the final size of a vector, I'm best off calling reserve on declaration of the vector. Or am I misunderstanding your comment?
It's harder to find lines, where there is no horrible mistakes.
Do i need their programs to use Qt? I installed it through vcpkg but i'm not sure if i'm missing something
Also GOG and Origin. Pretty much (I'd say 90%, but that's not based on anything besides my experience) every cross-platform GUI program that's written in C++ uses Qt (and that's a huge chunk of cross-platform GUI programs, since the only alternatives I see are either java or js-based).
Well, game bugs are certainly part of it. The example I was thinking of was Minecraft, where the entire map is generated from a single seed value determined when the world is first created, and if the seed value gets altered in the map file, or if the PRNG algorithms change, it causes new terrain to generate haphazardly and nonsensically. Because the C++ Standard Library doesn't guarantee the behavior of the distributions, this means the algorithm can change simply because you compiled for a new platform or for a new version of the compiler, which means seeds aren't portable. The degree to which that can matter is highly dependent on the application itself, but I think it's obvious that having facilities to allow portability are important.
Oh man, i am not a dev, my syllabus doesnot even include vectors, i somehow made by learning from many unofficial places, please dont look at the code as if it's written by a dev sitting in silicon valley. And please DM me the so called "horrible mistakes".
Stop spamming.
Awesome, I was worried that QtQuick's look-n-feel was being targeted for mobile/embedded, but those desktop apps look nice.
Just give me `ssize_t` already. :-D
There are no secrets, all the code is in `&lt;execution&gt;`. We call `CreateThreadpoolWork` / `SubmitThreadpoolWork` / `WaitForThreadpoolWorkCallbacks` / `CloseThreadpoolWork`.
&gt; Is there any chance you could elaborate on this one? I've always (evidently naively) assumed that if I know at run-time the final size of a vector, I'm best off calling reserve on declaration of the vector. Or am I misunderstanding your comment? Me and STL discussed this yesterday. Basically if you do this: `vector&lt;int&gt; vi;` `for (int i=0; i&lt;1000;++i){` `vi.reserve(vi.size()+1); `vi.push_back(i);` `}` This can be bad because for vector actually reserves the exact memory. I find it idiotic but if the vector is of capacity 128 and you request it to reserve 130 it will grow to capacity 130(not 192 or 256). 
If you know the _final_ size, its always better to call reserve. However, if you know the _next desired_ size, you might fall into trap of increasing the size by roughly constant amount each time, instead of exponentially.
&gt; Skype For Linux (closed source) Not any more. New Skype for linux is Electron app.
&gt; Also, in "when not to use vector", my rule is I would consider mentioning why somebody is using hash_map instead of vector comment spam. 
From file /u/snsmac linked: Line 3: [`using namespace std;` is bad.](https://stackoverflow.com/questions/1452721/why-is-using-namespace-std-considered-bad-practice) Line 5: `main` should return `int`. It should not even compile on any proper compiler. Line 7: Why are you using stream for both input and output? `ostream` is suficient here. Line 8: You can specify which file to open in constructor, two step initialization is not needed here. Also lose `ios::out` it is implicitely added. Line 13: not needed. File should be closed automatically in destructor. Manual call to `open` and `close` (except when you are reusing same stream object) are usually redundand. Line 14: WRONG! Deleting something not allocated with `new` is terrible. I do not know how it does not crash instantly for you. And it is unnecessary: object with _automatic_ storage duration are destroyed _automatically_. Line 15: Ditto. Line 16: Well, this line is why line 13 actually does some work. Do not use `exit`without good reason in C++ programs. It does not unwind stack, rendering RAII useless. Use `return 0` here or, better, do nothing and rely on implicit return in `main`. From https://github.com/VedantParanjape/Chat-Server-and-Client/blob/master/Client/Program%20Codes/ChatClient.cpp Similar problems. Line 10: IP-address is read as a string. Max length of IP-address is 15 symbols (4 octets, 3 digits each + three dots) + terminating 0. You have space only for 15 characters. Line 17: no overflow checking. There are much more problems, which I won't write here, because this post is already long enough.
Don't take it bad, see these comments as an opportunity. Try making things better. Find a good tutorial, a good book, upgrade your code, learn from mistakes. This is how you get good and become that programmer at Silicon Valley. BTW, yes those mistake are pretty horrible, but a lot of those are easy to fix.
Sorry if i sound rude, but i think you need to start with the basics. In my opinion the book "C++ Primer" is a good way to learn them. YOu can find it as pdf. Now to the mistakes i saw: 
For reference, with all the mistakes fixed, it'd look roughly like this: #include &lt;fstream&gt; int main() { std::ofstream file("../Logs/RunCounter.txt"); int val = 0; file &lt;&lt; val; }
No offence taken, but whatever mistakes you guys pointed, actually thats the way the high school course teaches us,and the teacher also. But i will still review the corrections, thankyou.
Wireshark
You misspelled ptrdiff_t.
That still mishandles 64 bit lengths.
https://github.com/Mudlet/Mudlet uses Qt 5.6 minimum, that's modern.
This is also true for other distributions, for example the normal distribution where msvc and gcc use the same algorithm but where gcc produces a b c d e f ..., msvc gives b a d c f e ... https://stackoverflow.com/questions/38532927/why-gcc-and-msvc-stdnormal-distribution-are-different Getting the same results with different compilers would be very useful but it could be a pain to specify. When it comes to floating point distributions, it's also not specified what the accuracy requirements are. For example the https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform method for production of normally distributed random variables never produce values more than 6.66 standard deviations away from the mean, making them unsuited for example for some particle physics applications.
Ouch, yes, that would be a terrible thing to do.
Similarly to /u/Z01dbrg that does very much make sense. I always make sure that if I'm reserving on a vector it's because I can calculate, at runtime, the final size of the vector. So hopefully I'm safe... On a slightly different note, though, I do maintain code that has vectors of vectors, though, which are often resized with abandon, and while we've attempted to reduce the performance impact of these, this is mking me feel reinspired to revisit every one of those.
I agree that many C++ programmers are concerned about performance in a broad sense, but I still maintain there's a very big difference between typical event driven applications and real-time systems like the ones you and I work on. I'm used to explaining to people about how game developers are *always* focused on performance, as you see many programmers not even considering the cost of allocations or unnecessary copies (hooray for move semantics, RVO, etc), which is a pretty big indicator that they're not really thinking at that level. So, I hope you'll forgive me if I sounded arrogant or condescending - I didn't mean to come off that way. I'm just trying to describe what working as a game developer is like. I also missed that you were in HFT trading, one of the few areas which has to pay attention to many of the same sorts of micro-performance issues like we do. My bad. Regarding exceptions, keep in mind that many videogame companies are only recently dropping 32-bit support, for which performance differences are *easily* measurable. It's also worth pointing out that we have to run on very modest consumer-grade hardware of various flavors, which is something I'd guess is not an issue for you guys(?). So once we're guaranteed zero-cost exceptions in 64-bit only code on all platforms, attitudes may change regarding this feature. But for the time being, we also have an enormous codebases which were not necessarily written with exception-safety in mind. Even so, I tend to think that eventually game programmers will embrace exceptions as well. I can't even understand why someone would object to unique_ptr, as it optimizes away to the equivalent of a raw pointer. The only objection I could understand is if it slows down debug builds too much, although a more likely explanation is either misunderstanding or simple stubbornness. Honest, game developers aren't all reactionary luddites. Personally, I've embraced new C++ 11/14 features with a vengeance, and they make it feel like a completely new language. I'll be starting some contract work very soon, so I'll have to see how my new employers feel about these new features - have to follow company code standards, after all. The only exception, if you'll pardon the pun, is that I still don't use exception handling in my own engine and game code (written about five years ago), because I wasn't 100% certain I could rely on them being available or zero-cost on all target platforms. 
&gt; This is the world's least useful STL algorithm I disagree. Surprise. :) Even my good friend :P Herb is a fan of std::for_each. His reason is similar to mine(more "what", less "how"). Also he often speaks about impossibility of having a `break;` in std::for_each, making it easier to read the code. IIRC he even once said range based for loop can not have break because he spent so much time talking to people about std::for_each. :) So std::for_each would be quite nice if it worked on containers, especially if you have named function. Ranges will fix this, but obviously I wish we had this earlier. https://godbolt.org/g/F47pQF
Oh I know. Only STL library devs need to worry about that. I don't have that many elements.
Also Team Viewer
Okay, cool, very reasonable response :-). Yeah, I always have to remind myself that often video game developers are targeting, or at least trying to reuse code on, older platforms. So you make a very good point. And yes, you're correct, we're 100% on 64 bit, and typically architectures from the last couple of years, massive amounts of memory, etc. Thanks for the info, very illuminating!
Yes - reserving immediately after initial construction is structurally safe. (If you can just range-construct, that's even better.)
Glad you're not offended. :-) I think a decent takeaway from this is that there are a large number of *very* different use cases for C++, and everyone has their own unique set of priorities. That's definitely a good thing for people to keep in mind when discussing these sorts of issues. One of the fantastic things about C++ is that, for all its issues, it's a very practical language that doesn't really impose any specific programming paradigm on the user, which I believe greatly contributes to its continued popularity among high-performance computing applications.
I hope that is sarcasm
&gt; I do maintain code that has vectors of vectors, though, which are often resized with abandon I am not sure what you mean by resized with abandon, but in case you are having performance problems: a) profile so you know if you have performance problems b) if your vector of vectors is heavily used and is a matrix or approximately a matrix you could use 2D vector aka matrix for better performance and less memory overhead. Problem is that there is no standard matrix. So you need to use some from the other libraries. Please DO NOT WRITE YOUR OWN. Pretty please. c) if your vectors can be compressed it may be faster to compress them and decompress when you use them than to keep them uncompressed. IDK much about this but I know it is a "real" thing done in industry a lot(Google Protobuf for example). https://lemire.me/blog/2017/09/27/stream-vbyte-breaking-new-speed-records-for-integer-compression/ disclaimer) I am not a consultant, I just play one on r/ cpp so you should not blame me if something goes wrong by following my advice 
That was VSO# 367427 'CWG 1748 "don't check placement new's input" is not implemented', should also have been fixed in 2017 15.3.
It's not optimized out by default by Clang. You don't see it because it happens in the callee. That's what xon_xoff was talking about with calling conventions above.
Because inlining never happens with the optimizer disabled.
I'd kill for regular code reviews with that level of attention-to-detail and expertise.
One thing I did not see mentioned is references in callbacks/async stuff. By this I mean passing references to callbacks or starting threads with lambdas that capture by reference. Almost everybody here is a True Believer that will never admit that GC has advantages but this is one place where RAII can not help you since there is no scope in classic sense. My only advice beside sanitizers is to pessimize for safety if you can afford it. Pass strings and vectors by value. 
Can you show the full asm of the function?
&gt; Because inlining never happens with the optimizer disabled. Yeah, I assumed the compared asm is optimized since it is pointless to compare otherwise. :)
&gt; there is no standard matrix. So you need to use some from the other libraries. Please DO NOT WRITE YOUR OWN. Pretty please. Well aware. I use a matrix from boost when I have to :) (Or arrays of arrays if possible, or simply maps if performance isn't an issue.) I came into software development from academia, which suffers a plague of not-invented-here syndrome. Me, I'm very, very happy to take something that someone invented elsewhere, tested thoroughly, and will maintain against future bugs, because it saves me a world of development and maintenance. &gt;&gt; I do maintain code that has vectors of vectors, though, which are often resized with abandon &gt; I am not sure what you mean by resized with abandon, but in case you are having performance problems: Actually these situations are a bit niche, even for us. The vectors of vectors are not my doing and are widespread in a few critical areas of the code. What tends to happen at the minute is that vector one is reserved for the y axis. Then we run over an *unordered set* of quantities, and resize arbitrary x-axes on that y-axis. It's utter lunacy, but there are genuinely reasons we've not been able to back away from this right now. Though I'm going to start benchmarking against simply reserving everything out the outset. Or building it up as a collection of vectors (probably a map of vectors), allocating the space, and then moving them across.
Thanks, that's reassuring :) Normally we have to reserve and then emplace back onto the vector given some operations so range-constructing isn't always an option - so long as that isn't killing us we're hopefully fine.
Good heavens. I was going to ask about that "quadratic complexity" issue as well. Who in the world would even think of doing something like this? I mean, other than beginning-level programmers - in which case such a tutorial should probably not even mention the memory management functions. Sigh... maybe I shouldn't ask that, because I'd guess people have actually seen code like this from supposedly professional programmers. I've seen the Daily WTF, after all. 
&gt; map of vectors If you are talking about std::map please see if sorted vector or unordered_map will work for your use case...
Your problem is that you know stuff. So you do not know/remember how it is to not know the basics. You read somewhere that reserve is good because vector is not resized when you insert into it. You decide to reserve() before push_back. Now your code needs a loop... You do not need to be an idiot to do this, just somebody who is learning.
Hm, after upgrading our enterprise app is crashing on clr memory heap corruption. With help of MDAs we tracked it down to initialization of a mixed mode dll and likely to initialization of vtables. Does anyone else have issues like this? I wonder if i should make a ticket.
LLVM but yeah.
&gt; No, it was f off. I'm sorry if I did not make this clear in my reply, but this was never my intent. This is why I mentioned that you don't _have_ to show up to a meeting (which is a significant barrier to entry), you can also submit an official paper (which is not a significant barrier to entry). I suspect the time difference between writing the article you put together and making it part of an official mailing is around 30 minutes of work on your side. Yet, the benefit is that you know for sure it will be read by your intended audience (Library Evolution members), who may or may not read Reddit. In other words, my advice was targeted towards making your current time investment (writing the article) more worthwhile by making a small additional time investment (making it part of a mailing). &gt; Louis does not even closely works with average C++ developers/company Amazon is a large company, and like any other large company, there's all kinds of developers. I work with a lot of average C++ developers, many of which have strengths in other areas that are not deep C++. But regardless, I understand and respect your opinion, I just don't share it. 
&gt; This is why I mentioned that you don't have to show up to a meeting "First, plan to attend in person and schedule a time with the subgroup chair. For a proposal to make progress, the proposer (or a colleague/representative who is very familiar with the proposal and the domain) needs to attend in person to present it to the subgroup; proposals without presenters are typically not considered." So in other words I guess you are volunteering to present my beautiful proposal. :P Thank you very much. I will send you the first draft soon. :P :P :P Again this policy is not a purely bad idea since it prevents trolls and noobs from spamming, but it is a barrier to entry. &gt;I work with a lot of average C++ developers, many of which have strengths in other areas that are not deep C++. But regardless, I understand and respect your opinion, I just don't share it. If you know basic economics about salaries you may want to check how much Amazon pays it's developers compared to the average in the region where the office is so you would share my opinion. :) 
Yeah.
eh in practice you just have more complicated code and mistakes are easier to miss.
A change to fix this is going through validation builds now (disabling vectorizing `std::reduce` if `/fp:except` is turned on); hope to get it in the next minor update. Thanks for the bug report!
It's shocking that we don't test the libraries with all zillion of off-by-default switches in the compiler? (Our per-commit test matrix is already ~200k+ tests)
I think a lot of this is because we support hardware without `vfmadd132ss` in the libraries, and provide a function you can call to disable use of AVX in the libraries due to bugs in older hardware.
when such a trivial example ends up broken like that, don't you? what about per-release at least?
I'm wondering by the way, do you test the Unit Test framework with itself?
I made this mistake in my personal library long ago. Reserving size+1 is silly, but other forms are more tempting, and that's where I made the mistake - "hey, I'm about to append K elements 3 times over, so let's reserve size + 3 * K".
Ha oh that's interesting, according to the comments they both generate a pair of numbers but each return the pair in a different order.
That makes a bit more sense I guess. It's certainly easier to lose track of the core issue when it's slightly obfuscated by just a bit of additional complexity. And hell, we've all had forehead-slapping moments, no matter our experience level.
&gt; First, plan to attend in person and schedule a time with the subgroup chair. For a proposal to make progress, [...] Your article may not make "progress" in the same sense that a paper presenting a feature might "make progress", but if it's included in the mailing, at least people may read it and form an opinion on it. If they relate to the arguments in the paper, they can bring it up for discussion in the group.
It's a "trivial" example with a compiler flag used by almost nobody.
Not sure how the Unit Test Framework thing actually works; we do not use it. Our tests look like a directory with a .cpp inside, that gets compiled and checked that returned 100.
I see. I know Microsoft has talked a lot about dogfeeding so using the software you're developing when you re developing it, so that would have been one of the cases. Are these tests only compile-time?
&gt; dogfeeding so using the software you're developing There's a reason the test folks added support for Google Test and Boost Test :wink: There are scenarios the standard library supports where building all the tests into a DLL is not OK. &gt; Are these tests only compile-time? Some are compile tests (e.g. the "instantiate every algorithm with every iterator power and with strange difference_types and..." test). Some have complex run-time validation. It's kind of a grab-bag really.
&gt; But loading a file is often not performance critical either To be more precise, the exception overhead is likely smaller the the OS API call to get the file handle unless you're reading from a ramdisk or the like.
I don't see why any AAA game would bother targeting 32 bit now. It limits the game so much with the memory limits and people with a 32 bit machine probably don't have the GPU to run the game correctly in the first place. Also, if you asked gamers if they would rather lose 2FPS in exchange of no CTD, most of them would be quite happy with the tradeoff. 
MSVC will list the result of the return of a function in the auto variables list.
Anything that helps you accomplish what you want with less code does change the way you write for the better, IMO. Structured binding (and maybe if/switch initializers) I can see making an impact, maybe almost as impactful as range based for did. Template deduction could also be a big win for writing succinct code.
I don't know why nobody wants to use the best way for it: for(long long i=-1;i&lt;v.size();) v[++i]=0; If anyone has worse, please tell. I guess you could use `v.end()-v.begin()`
Thanks for the info, it's quite enlightening.
gdb, too.
Agreed about debuggability, sure, we can add a temporary. What I liked less on your side is the *construction and the assignment* of the result thingy, actually.
I don't have so much experience with gdb so I only mention what I know.
Sure, I just wanted to point out that showing a retval is common for debuggers.
I find it (MSDN) more confusing to navigate though. The recent move doesn't help too).
&gt;&gt; test &gt; another Learning, but I'll edit it...
I guess that's a fair critiscism. I haven't used the new website a lot though, I mostly landed on the right page with a google search so I can't really comment on the navigation aspect. On the other hand, the sheer number of examples, in each language they support, plus clear guidelines on how to implement different things (mostly something new I believe) is great to see! But we should'nt congratulate good documentation, we should shun bad ones
Didn't use the `vcpkg` but most likely it comes with everything that's needed. However, you'd probably want a good IDE integration and there are 2 preferred ways to go: use QtCreator - a great cross-platform IDE with built-in support for Qt (as in - visual GUI designer, built-in doc and examples and such), or if you are much more familiar with Visual Studio you can install [their plugin for VS](https://marketplace.visualstudio.com/items?itemName=TheQtCompany.QtVisualStudioTools2015). Anyway, I think the setup process is described pretty clearly in the [docs](http://doc.qt.io/qt-5/gettingstarted.html). 
u/STL, Thanks for reviewing the article and for pointing out the typos. A few comments: &gt; Your website should use HTTPS You are right. I fixed it, but the link in reddit still uses HTTP. &gt;&gt; Unlike a static array, a vector automatically grows and shrinks as necessary to make room for new elements inserted or deleted. &gt; A vector's size shrinks, but its capacity doesn't (unless you call shrink_to_fit). That’s right. I was introducing the subject slowly, and I didn’t want to get into capacity that early in the article. I thought that I had mentioned it later in the capacity section, but I didn’t, so I added an “Edit” note. &gt;&gt; for (auto i = 0u; i != v.size(); ++i) { // v is a vector &gt; This is a slight mistake because it mishandles 64-bit lengths. size_t should be used instead of unsigned int. (The distinction between size_t and size_type is not important in non-library code.) You are right. Considering my audience, beginner and intermediate programmers, I didn’t want to get into std::vector types. I did debate on it for sometime since intermediate developers should probably know it. At the end, I decided to rely on auto, and if there was enough interest, write about it on a sequel, std::vector part 2. &gt; Your article is switching between referring to v[2] as the third element, and begin() as the first element (instead of zeroth). It is important to be consistent. It is consistent: first element: v[0] and begin() returns an iterator that points to it. second: v[1] third: v[2] &gt;&gt; std::find(v.cbegin(), end, n); &gt; What happens if you mix begin and cend? This confuses many users. I debated on this one too. I see how it can be confusing. &gt;&gt; (auto elem) &gt; This copies :-( Yes, but on purpose. elem’s type is int (or will be deduced as int for that particular example) &gt;&gt; the vector allocates a new chunk of memory of twice the size of its current memory. &gt; Untrue. The Standard doesn't guarantee 2x, and MSVC uses 1.5x. I’ll correct it. Thank you. &gt;&gt; In and retain a copy: pass a const lvalue reference, f(const vector&lt;T&gt;&amp;); &gt; Not optimal for modifiable rvalue inputs. You are correct. However, I think it is good advice for a default. If there is proof that it needs to be optimized, or if you are writing a library, then it makes sense to overload with an rvalue reference. The same goes for “in &amp; move from” in my opinion. Thanks again.
I think the 15% is from "I turned on exceptions and RTTI on an unchanged codebase" people. This is complete and utter bollocks, and these people are incompetent enough not to realize the incompetence of their measurement.
Are you talking about runtime errors or bugs in code? I really don't think that "normal" applications try to recover from bugs in code using exceptions (and we all know that, if they try, they fail; UB is a bitch). Or...?
&gt; I guess that's a fair critiscism. I haven't used the new website a lot though, I mostly landed on the right page with a google search so I can't really comment on the navigation aspect. Well yeah. That's probably the only way I manage to navigate it =) And the recent move created a problem, when you get links to the old website and to the new one. And I also have a problem (not bothered enough to investigate) that causes most msdn links to redirect to `404` or to the front page on the 1st try. But yeah, once you've found something, it's great to use. Would be nice if it also worked as qt help in offline mode. And currently if I peek help on some method it usually just opens the class doc, while QtC auto scrolls to the highlighted entity. And I like the layout of qt help a bit more. But that's just nitpicking. It's true that it's hard to find something else as elaborate and complete as Qt docs and MSDN for something so big. Even the basic doxygen-like docs that just list all the methods with occasional 1-2 line comments is a rarity.
It seems to me that the problem is mainly in C++. Look at Python : NumPy has a great doc. Theano has a great doc. Tensorflow has a great doc. And these projects are as popular as most big C++ library. Although, Python standard library documentation is... Poor to say the least (compared to extensive cppreference!)
Except when it won't. And it happens. (See for example whether you get return value of Release() on a COM object.)
I tried with /Ox, doesn't look like the optimization OP wanted is there. I wonder how hard this is for the compiler. There's a sync point there. Depending on the size of the literal, small string optimization might not kick in, so allocation is involved, too etc.
Only once ptrdiff_t is returned from calls to size.
You’re right about first/third, I was jumping at shadows there.
do you fuzz test? that seems like the obvious thing to do if you don't want to write unit tests for each compiler flag (which i still think you should do, but hey)
Especially the changes to Quick Controls 2 in Qt 5.9 and 5.10 bring quite some new features for desktop applications. It's still not possible to get _native_ look and feel, but _modern_ is very easy.
Is recovering from an error easier using ADTs or error return codes? Because in my experience, those make life even harder. Applications that use them tend to drown in a sea of `if (error) { free(); close(); deallocate(); return -1;`. And if you look long enough, you _will_ find paths where resources were missed. 
Skype for Linux is now an Electron app, unfortunately.
Yes, and Skype for Windows is a Delphi app.
&gt; Can you show the full asm of the function? I can do it Monday (there is national holiday in Italy).
So... I'm sorry, but I ask again: how is this different from ADTs or error return codes? You call something that would ordinarily exist in a try/catch block, and find yourself faced with error -1 ("general error in processing"). Now what? How will you be able to reason about it? How do you know what failed, and why? How do you know where the error came from? And if your callstack is deeper than just one subsystem, you get to deal with the joy of having to unify multiple classes of return codes as well. No longer is it just your own error return codes; now you also need to _somehow_ represent all the error return codes that can come from other subsystems. The easiest way out, of course, is to just assign a single code to all the reasons something might fail in another subsystem - and pay the price of not being able to give detailed error messages anymore. I've been told several times by my users that they are particularly happy about the detailed nature of my error messages. It doesn't just say "error -1" - it tells you _what_ failed, _why_ it failed, and if at all possible it will suggest ways to make it work correctly. That works, because the exception that was thrown in response to the failure contains that information. So a typical error message consist of two parts, one being a high-level description of the task that failed (this comes from the catch clause), and the other the low-level reason why it failed (this comes from the exception): "Could not import prediction data from file 'c:\data\temp.dat'. No permission has been granted to read file 'c:\data\temp.dat'." Notice how this is different from: "Could not import prediction data from file 'c:\data\temp.dat'. File 'c:\data\temp.dat' does not exist." And notice how that is different from: "Result: failed." Which is typically what you get from systems that rely on error codes. Which would you prefer, as a user? 
Great. Out of curiosity: Can you share what do they do so crazy that it chokes the linker?
&gt; When asked, why people do code reviews, some of the more frequent and less inspiring answers are “because everyone does it”, “because that guru says so” or “because it’s agile” Really? I never heard anyone say this, in fact, code reviews are one of those things that people tend to recognise being useful on their own. It's easy because just after a few sessions everyone experiences reviewers finding areas for improvement in their code and being a reviewer everyone finds that in others' code. This is unlike unit testing where appreciation comes after unit tests save your ass from introducing a bug. This happens lot less frequently. *Also, not related to the contents of the post, but I feel like we are reaching a point of emoji ridiculousness where in a few years we will look back and compare this to the 90s gif-infected websites of geocities.*
&gt;I'm not sure why you'd make a blanket statement about the entire game industry's error handling practices. *Because games crash*. Quite a bit more than other software, too. I can't remember the last time I saw desktop software (browsers, word processors, dev tools, media players, whatever) crash, but games? Last week I was playing Mass Effect Andromeda. It crashed three times during my play through, which quite frankly didn't bother me because that counts as a pretty painfree experience in gaming. I'm not at all unhappy about the general quality of games, but any playthrough of pretty much any AAA game will get you a handful of crashes. Indies are actually better in that sense; they aren't pushing the performance envelope so much and seem to be more reliable as a result. From statements by people working there, it appears the game industry has many practices that might be considered doubtful. Why _wouldn't_ you stick things in a vector? Allocate it statically, start the game by setting it to a specific size, and use it normally. There is no cost difference wrt. the current model, but if it turns out to be too small, at least it can be resized - at the cost of a one-frame stutter. Mind, that would have been a one-frame crash otherwise, so a stutter seems a low price to pay. Anyway, please don't feel personally attacked; we are discussing exceptions, not people. I just don't feel comfortable using the game industry as a model for how error handling in C++ should be done. 
14.1 is VS 2017. 
Terrible, i always mess up the version numbers!
[removed]
They don't make it easy.
Apparently it was even in 1.64 but somehow removed in the 1.65 release. #if _MSC_VER &gt;=1900 #pragma warning( push ) #pragma warning( disable : 4172 ) #endif template &lt;class Tag, typename Args&gt; inline const typename lookup_named_param_def&lt;Tag, Args, param_not_found&gt;::type&amp; get_param(const Args&amp; p, Tag) { return lookup_named_param_def&lt;Tag, Args, param_not_found&gt;::get(p, param_not_found()); } #if _MSC_VER &gt;= 1900 #pragma warning( pop ) #endif
Code reviews are useful most of the times but unfortunately gets more airtime than its actually worth it. Most people in the organisation do it for the sake of quality gate.. Did it ever happened that code reviews require us to change everything we did earlier.? Most of the code reviews are limited to coding convention and best practices which may increase the visual appearance of the code but not the runtime performance
&gt; We also added support for mnemonics in menus and buttons. Wow, welcome to 2017 ;-) It's an awesome release but very telling of the current sad times when it takes years to implement mnemonics, something that should be there in any UI from the very get-go.
I believe that benchmark you have comparing an Eigen implementation running on CPU vs a plain Cuda kernel. It's not Eigen on CPU vs Eigen on the GPU unless I'm missing something :) AFAIK Eigen employs variety of techniques to speed up calculations. Expression Templates, SIMD (on CPU, when data properly aligned), etc. Now on GPU there is no SIMD to exploit. But using it can still help generation of more efficient code (compared to a naive adhoc implementation) with expression templates. In any case, if you are concerned about speed, Its always best to do your own benchmark with up to date components. 
I've been using Qt 5.7 for the last year, and I'd say there is no better example of modern Qt than Qt Creator. When it comes to Qt Widgets that is... Qt 3D and Qt Quick are a different thing altogether.
What is gcc.godbolt.org exactly? Is it to be able to use gcc directly?
&gt; Did it ever happened that code reviews require us to change everything we did earlier.? Rarely, but that's because of separate dev processes such as architecture design, whiteboarding and prototyping etc... nobody in practice just jumps in and hacks away without thinking about the overall picture. 
the maintenance tool is really slow, taking minutes to just download meta info.
If v[2] is the third element then it should be consistent to say that begin() points to the first. begin()+1 to the second, begin()+2 to the third... So what am I missing? 
It lets you compile c++ code snippets with a huge variety of compilers. You can inspect the compiler's output, for example to see if and how a compiler optimizes certain code.
Same here. Except one use that was then choosing another version of `min` from another namespace used locally, and gave us a hard time finding where these terrain textures were fucking up. :)
a 100 TB file of random numbers?
That might be a bit extreme. But it would help somewhat to overcome the complaint that random numbers aren't predictable.
But PRNGs are entirely predictable, provided you know the seed. The problem comes from PRNG implementations varying. I feel it's likely much easier to provide your own distribution function than messing with potentially huge files. Large periods is pretty important in many games, specially if procedural generation is involved.
&gt;C++ 17 compilers don't quite exist yet. GCC, Clang and MSVC support nearly all new C++ 17 features.
The most important question an intermeditae user has is not answered: When do I use `deque` instead of a `vector`?
&gt; Having a second person who has read (and understood) the code increases the bus factor of the project I've seen this bit work very well, taken to its logical consequence of making the reviewer responsible for supporting the code.
What problem are there?!
&gt; Most of the code reviews are limited to coding convention and best practices which may increase the visual appearance of the code but not the runtime performance Then your company isn't doing it right. There's more to shipping code than making sure it compiles.
&gt; Exceptions are for recovery, but attempting to recover from bogus indices is a path to misery. Instead, be absolutely confident that your indices are valid. Exceptions are for visibility. Telling people not to write bugs is not a feasible solution, because they end up writing bugs anyway.
1) It isn't a `vector` 2) It is not even a `Container` 3) It does not contain `bool`s
Well, using unsigned for bit arrays for decades was the right choice to begin with :-)
So that's why it went unnoticed for so long ...
No include guards and unnecessary macro magic. I don't like it.
FreeCAD 
FreeCAD
You forgot to mention that each compiler supports own subset of C++17. But together thay do support almost everything :)
Now I don't know if you are replacing your fake with gMock because you are mandated to do it or just because you want to move to a gTest/gMock basis. But if I was your tech. lead and your fake works I would just keep the fake for now and only migrate your new code to gMock. Because if your fake has no cpptest dependencies it is really not a problem. gMock and similar things in my opinion exist to help you test code in isolation they are not a purpose in itself and your production code does not because better because you achieve purity in testing. What should really matter to you in this case is to continue working on your production code and provide a good testing basis for them. Maybe if you have spare time in your allotment make it so that you are fully ready for the future. Setup gMock make sure your build supports it, integrate it with your CI loop, stuff like that. But don't spend time on the fake until it becomes a problem or a limitation. It is test code and yes it is important but if you already have a fake don't rewrite it now even if it feels hacky. If this is the only hacky thing that you have to deal with, then you are in a good situation. TLDR: Keep the fake, get ready for hybrid, focus on testing and making working code not rewriting test code.
I have been (and still am) working for huge company where my tasks resolved around: - porting unit tests for 11 argument function - scrolling screens to find where an `if` is ending - finding that 20+ files in the same directory have copy-pasted around 50-100 same lines (which had 8 level depth `if`s) - understanding (or trying to) union in struct in union in struct in struct - understanding (or trying to) member variables `void* null_pointer = NULL;` - getting tasks from guy who had practically no english skills (yes, he wrote code too - with functions copying entire vectors) - places where any comments were forbidden (including documentation) Yet there was very little or no code review. Some of those, who were (only) allowed to do code review did not understand what is `const` or `&amp;&amp;`. I'm deeply surprised how even that large company has people writing garbage.
&gt; Is recovering from an error easier using ADTs or error return codes? No. Which is why I said *from an error* and not *from an exception*. Recovering from half-updated state invariably requires "cleaning up", and as any substractive operation it's inherently difficult not to miss anything. &gt; Because in my experience, those make life even harder. Applications that use them tend to drown in a sea of [...] To be honest, I have seen the same pattern in **Java** recovering from exceptions. It's easy to forget to close a file descriptor, return the connection to the pool, or deregister the connection-associated state from everywhere. Languages with deterministic destruction (C++, Rust, ...) tend to do better in this regard, but destructors are not a silver bullet either. Still, there is an advantage to ADT here: the possibility of error is thrown in your face. So, unlike an exception which you can miss, you have to ignore/pass up ADTs explicitly, and anyone reading the code may realize something's amiss. Emphasis on *may*...
Short and sweet and informative; nice. Of course, this highlights why vector&lt;bool&gt; is dumb. There's nothing wrong with bitsets, but generic code should behave generically!
&gt; But that isn't always easy. Ain't that the truth :( Interestingly, I've also found cases where strong-exception safety induces people in a false sense of security. I remember debugging a case of half-written file, where the original developer swore it wasn't possible because he handled errors. He was indeed deleting the file on catching exceptions (aka "rollback"); what he had NOT anticipated, was that the process could terminate *without* throwing an exception: crashes, sigkills, ... Along the years, I've learned to fear the "undo" strategy: by definition bugs are unforeseen conditions, so attempting to "undo" when in an unforeseen state is playing with fire. Instead, I much prefer either "swap if successful" (as you mentioned) or code *and test* handling of partial updates. The latter has proven very useful in distributed systems, as reliable "undo" is a pipe dream there. 
sorry bro, it's not my company thing, it ubiquitous 
The fake have no dependencies at all, and currently works very well for the tests that use it (which is almost all). One of my main points was to make the fake easy to extend if more of the communication layer is needed. To be honest I'm quite satisfied with it. My main reason not to immediately do this is that if this fake is meant to keep growing as more of the communication layer is needed. Id rather face this problem now than when it has grown to a monster. A not too little part is me wanting to do a good job/impress a little too, as eventually more people will work on this. That said i fully agree with you on priorities. I believe tests should help me code, not the other way around. Thanks for the input! :) just moved the (option 3) experiment to a branch, hopefully to stay there forever 
so code reviews are mainly meant for maintaining coding standard.
Well, yes and no... I’m just saying that there should be separate processes that contribute to the end result.. so by the time you get to the review stage (I prefer OTS, and disagree it should be a audit/tick box exercise) the architectural/design side should have been agreed between developer and reviewer, so what you’re looking for in code review is the adherence to standard/code quality and checking for implementation error (not design issues)... not saying it always ends up that way but that’s how it should work. Couples with a good static analysis tool and test harness... you’re set. 
Well, yes and no... I’m just saying that there should be separate processes that contribute to the end result.. so by the time you get to the review stage (I prefer OTS, and disagree it should be a audit/tick box exercise) the architectural/design side should have been agreed between developer and reviewer, so what you’re looking for in code review is the adherence to standard/code quality and checking for implementation error (not design issues)... not saying it always ends up that way but that’s how it should work. Couples with a good static analysis tool and test harness... you’re set. 
Well that makes more sense then. The moment the "fake" needs engineering and looks like it needs to grow you must treat like any other part of your system that needs refactoring and maintenance. Apply care and evolve in the direction you want would be my suggestion especially if it looks likely to grow. By the way for integration testing of your communication in particular it is often interesting to externalize code like that you have created already and be able to simulate your traffic to a more realistic code setup especially if you experience noise, errors and the like. Seeing the correct reaction with a more integrated set of your code can improve your code beyond what the unit tests give you already and it is very handy if your system has issues in the field and you need to see if you can replicate an issue. Your "fake" may give you valuable code to base such a simulator on.
Having an agreed coding standard in a company is really useful, although this is best enforced using tools like resharper as well as encouraging people to abide by core guidelines if so inclined 😂 It’s implementation detail that is vital, anything that is human understandable but would be overlooked by compiler/static analysis is what’s most important. 
&gt;You are right. I fixed it, but the link in reddit still uses HTTP. Add a (permanent) redirect from HTTP to HTTPS for all requests, preserving the path/query string. This should run as early as possible (before any other redirects). Also look into adding HSTS so future visits never even attempt to try HTTP.
I have a question regarding debugging (no trolling!). Is it planned to improve debugging in QtCreator using CDB? It is extremly slow on larger projects, it sometimes takes minutes to do step over or to show variable values, sometimes they even disappear. Putting a breakpoint in a template before starting the app leads to breaking at random address or causing deadlocks during system's LoadLibrary calls. It is also impossible to choose template specialization in which i want to break. If this slowness is a limitation of CDB, is there any plan to use MSVC debugger (where available)? MSVC debugger is really fast and just works great. We develop out project in Qt Creator, but we are forced to use MSVC for debugging and it is slowing the development down because of switching between two apps, having to open the same files in both apps, ...
I haven't used it yet, but yes, one of the main points of the fake is being able to simulate errors. I have not yet done this though and is one of the reasons i believe the fake will grow. Testing on the actual communication layer would not only be hard (as i would have to set up an external node for sending messages etc) but also very tedious as it takes up to a minute to initialize (for each test...). So far im basing my testing on "This layer has been working for 30 years so i will assume it does its job" and that i can isolate my "new" part as much as possible (to speed up development). Of course in the end there will be some more (manual or automatic) tests to see that it does its intended job in a complete system. But im not taking that into account just yet.
&gt; &gt; &gt; &gt; &gt; So far im basing my testing on "This layer has been working for 30 years so i will assume it does its job" and that i can isolate my "new" part as much as possible (to speed up development). Of course in the end there will be some more (manual or automatic) tests to see that it does its intended job in a complete system. But im not taking that into account just yet. That seems very sensible and it reads like you are aware of the potential that your code might shake bugs out of the old code too (because that can happen especially with 30 year old code used in new ways). 
`ssize_t` isn't returned from calls to size either.
It's a shame that the design of Qt Creator is horrible. 
If it's "configuring" released that sounds like a CMake thing rather than an MSVC thing.
&gt; nobody in practice just jumps in and hacks away without thinking about the overall picture Where do you work and are you hiring?
Unit testing the compiler flag would not help here. The folks who own `/fp:except` do not have a bug in isolation, so unit testing that wouldn't catch this. The folks who own `&lt;numeric&gt;` (which, for this particular change anyway, is my fault) already have unit tests that which try at least 16 C1XX compiler switch cocktails, 2 EDG ones, and 2 Clang ones. There is only a bug when these 2 features are combined, and apparently nobody tried that in Windows/Office/30+ years of accumulated tests/the hundreds of OSS projects we test with.
You mean the UI itself?
Remember that std::vector&lt;bool&gt; is an abomination.
The second part looks like a design flaw. I mean, it's very obscure and there's no way i could predict that behaviour.
I have followed this up via "report a bug", here's the full thread: https://developercommunity.visualstudio.com/content/problem/162361/cmake-configuration-is-unbearably-slow.html# The most interesting part of it was: &gt; Yes msbuild is slower [than ninja], but CMake is also doing the worst possible thing to run it. It creates multiple msbuild projects and runs them all in sequence. They could instead call cl.exe directly to check the same things (but that would require them understanding more of the compiler flags etc). In the ends it looks like it's a bit of fault from everyone. Even with ninja, we're talking about minutes rather than seconds, but I don't have any more exact timings than that. Unfortunately, my end goal of producing UWP builds of libzip via vcpkg does not support using ninja (see https://github.com/Microsoft/vcpkg/issues/625) so I'm stuck. Do you have any suggestions how can this situation be improved?
I don't understand why they didn't go with something similar to lambdas so that you could write `auto [x, y] = ...` if you want values and `auto [&amp;x, &amp;y] = ...` if you want references
If you're at the msbuild / ninja step then it can indeed be the compiler's fault; the original comment was about the configure step which is entirely up to CMake. One thing about vcpkg though is that it actually configures / builds the library you're depending on; not referencing an already built library, so that might take longer. Maybe /u/roschuma will know better.
I think the problem was with life-time extension. I.e. cases like `auto [x, &amp;y, y] = foo()`, could become complicated. 
&gt; the original comment was about the configure step which is entirely up to CMake. Yes, we're still discussing this part. Go to https://ci.appveyor.com/project/janisozaur/vcpkg/build/1.0.15 and hover over log entries for say `Configuring x86-uwp-rel` (line 258-259; this is msbuild generator) and `Configuring x86-windows-static-rel` (line 469-470; this is ninja generator) libzip does [check for a lot of things](https://github.com/nih-at/libzip/blob/5418fc0049049f79f444776c255efe89b8eedced/CMakeLists.txt#L50-L111) but the same thing on my system takes just 4 seconds to process, so there's clearly some `sleep(10)` in one of parts on Windows.
With good RAII-based design and in a single-threaded context, shared_ptr and weak_ptr are not needed for callbacks, capturing raw pointers can be safe. Because destruction of the containing object (to which there may be captured pointers in callbacks) includes destruction of the subobjects from which those callbacks originate, preventing their future invocation. See the example in my proposal for a new libuv feature (Example of Usefulness): https://github.com/ambrop72/leps/blob/c3dd548ff04167c6db38a88791e044db0e3c512e/006-immediate-close.md (speaking of that proposal, for some reason the maintainers are uninterested) I have come to be very fond of these principles and I always cringe at code which just doesn't "get" this. Despite the nontrivial effort to ensure callbacks are safe to do anything, it makes things so much simpler in the long run. Compare that to Qt where you never know when you can actually destruct an object and have to resort to things like deleteLater() which has its own set of problems...
Because despite looking similar, the two syntaxes mean very different things. Structured bindings are _bindings_ into the underlying variable. The intent is very much for them to be aliases. It's this aspect which allows you to just write for (auto const&amp; [key, value] : map) { ... } No copies there. I think if the above syntax made copies, it'd be very surprising.
Also, any IDE that supports CMake can be used with a Qt project pretty easily if you set up a CMakefile for the project.
I'd argue errors due to implicit unsigned conversions are more likely than the signed index type not being able to hold large enough indices
Or the AppVeyor machines are just really slow :). I've seen things that take seconds locally take forever there. OTOH I'm running really fast hardware locally and they're giving away their builders for free.
Had wished that there was better instruction on how to solve real problems with C++ and show the flexibility of language. Will probably be criticized because of my opinion here but say it anyway. It's good that more people learn C ++, but tutorials usually miss out on learning the freedom and what opportunities C++ provides. Typically, there are small examples of poorly written variables, etc. to solve problems that do not come close to how it usually looks when C ++ developers work (those who managed to write good programs). I have been programming C++ for over 30 years. Programmers today know less than they knew 20 years ago.
How are emoji's not related? Nothing says "code smell" in a code review better than the poo emoji 💩😁
I agree with you. However, as a counter point, you can always use something else, I typically go std::vector&lt;char&gt; for a boolean vector and it takes one byte each. If I wanted the super compressed bit thing, it's there though. 
Great post, /u/STL! As an independent, unbiased developer, I fully support your compiler and library removing old features. It's incredibly valuable that you be able to make progress and innovate in both your compiler and the language. 
&gt; The second part looks like a design flaw. I mean, it's very obscure and there's no way i could predict that behaviour. Welcome to C++. :P In their defense they probably knew about this but could not do anything due to existing language rules and grammar... 
One reason is that it actually works with bit fields.
will /permissive- have any effects on deprecations/feature removal?
Good question. Not in the library (as we can't sense the presence of `/permissive-` via macros). According to my understanding [and the documentation](https://docs.microsoft.com/en-us/cpp/build/reference/permissive-standards-conformance), not in the compiler either. `/permissive-` is for eliminating MSVC behavior that contradicts the Standard, not for "conforming extensions" or Standard deprecations/removals. Core Language removals are guarded by `/std:c++17`, like `register`, `bool++`, and `throw(stuff)`.
Literally every version of boost we've used has had some compilation error when compiled under MSVC when you have /Wall and /Wx defined. It has become a normal thing for us to update, find all the required changes, and then make a patch file for the other devs.
btw u/STL I remember asking you many years ago if MSFT STL reserve uses knowledge of malloc block sizes and IIRC you said no. Is that still the case? If it is not clear what I am asking: Even if you do not know bucket sizes when you vector.resize you could get current size up untill which realloc will work without reallocation(with HeapSize), then HeapAlloc exactly to that and set capacity to Heap size. Otherwise that memory is just wasted and in the future you may reallocate when you do not need to. note: I know you must support custom allocators, I am taking about default one. 
auto could have been so much better than it is
Yes, of course. The main thing is the side ribbon. It's pretty dumb and completely inconsistent with respect to other programs on the operating system (whether that be on Linux, windows or macOS). 
/u/STL is correct. While `/permissive-` currently conflicts with non-standard extensions (C++/CLI, C++/CX, and OpenMP) that needs to be fixed before it becomes the default. 
I'm just gonna put [this](https://www.youtube.com/watch?v=YnWhqhNdYyk) here just in case...
The only _actual_ problem in all of that is the name; were it named `dynamic_bitset` instead, the other problems listed are merely documented/known quirks.
It isn't
Yes but this is misleading to people reading your code. There's no real reason to even *have* a `bool` type other than to convey your intention to use it for boolean values only.
That is still the case.
Do you plan to remove your &lt;hash_map&gt;? I asking for a friend ;) who would like to see std::unordered_map deprecated and std::hash_map introduced. :)
The chromium team favours fakes over gmock implementations. In fact, they usually advise to not use mocks, but fakes, unles mocks would result in clearer code. I used to have gmock on most of my tests and they were pretty cryptic. Nowadays I use mocks only for very simple boilerplate.
does permissive- bans that flag that ruins magic statics? If you dont know what flag I am talking about I can try to look it up, but you should know since unfortunately you had to remove magic statics from STL to make it work with this flag...
I added an aggressive #ifndef _SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS #error &lt;hash_map&gt; is deprecated and will be REMOVED. Please use &lt;unordered_map&gt;. You can define \ _SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS to acknowledge that you have received this warning. #endif /* _SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS */ back in VS 2015 (I think; it was a long time ago), and we've outright removed &lt;hash_meow&gt; from the next major binary-incompatible version of the STL.
`/permissive-` is orthogonal to `/Zc:threadSafeInit-`.
There are updates to posix that try to address this. http://austingroupbugs.net/view.php?id=529 Enjoy how HPUX and Linux can't agree on it.
Now I am confused. permissive- should mean (permissive not aka strict), aka ISO compliant. And /Zc:threadSafeInit- means ISO you have no power over me muahahaha.... 
I agree with the side bar being terrible. I can live with it though, it is my prefered IDE when I'm not on Windows by far.
 std::cout &lt;&lt; "Hello, world!"; lol.
Kids, just run away from C++ as far as you possibly can.
I have hopes in that they'll fix it for C++ 20.
So "meow" is the latest, coolest metasyntactic variable? (Why wasn't I informed of this?)
[tie was always evil](https://godbolt.org/g/6wngbx)
Sure. Just allocate your data structure in some scope higher than the co-routines, and literally use placement `new` to initialize the frame on that same pointer.
Yes, contrary to Shakespeare, names are important. For something named `vector` it is a terrible unintuitive bug farm; for something named differently, it is a clever solution with well-documented features.
I rather think, they shouldn't agree, there's no need, unspecified is **fine**. What I think should happen, is: **the system** should do the necessary book-keeping and release handles when possible. **Especially** if it is about a network call (remote volume). Consider a following sequence of events: * close() is called * remote volume closes the file * network drops while the "success" reply travels back `fd` is really closed, but he who asked for it failed to get that information. If the system allows a new call, the remote side needs to keep track of all previously opened handles in order to inform correctly upon the next call. But doing this is effectively a resource leak, not to mention the raise in the complexity of the client code that might to deal with this. 
If you call placement new, you still need to call your destructors if you're dealing with non-plain data. It just becomes your problem. If your types are all plain data then you might want to switch over to a more c-like idiom and call functions on references to plain structs. Placement new is generally for when you have allocated memory that you want to wait to initialize, as in an std::vector with reserved capacity. I'm having difficulty seeing a use case for it eliminating destructor calls that couldn't also be solved by just reusing the same instance (thus eliminating constructors as well)
Ok, so I'ma C dev. I've played around with reimplementing one of my C11 projects in C++11/14/17 whatever. is there a tutorial or something specifically for people like me? that cuts out the basics, and just focuses on the new stuff? BTW I use _Generic functions, I get the idea, it's just the syntax for templates that I don't get. in fact, it's pretty much nothing but the syntax that I don't get.
&gt; All of these are correct, but the idiomatic form is like this: `for (i = 0; i &lt; n; ++i) v[i] = 1.0;` No love for `std::fill` ?
`/permissive-` means _"we remove implicit sources of noncompliance, which you didn't explicitely want (and probably don't even suspect that they exist)."_ `/Zc:threadSafeInit-` means _"I know, what I signing for, and I want this behavior."_
It is an alternative token for `*` wildcard (for people, who prefer English to Perl, in line with || → _or_, ! → _not_, etc.)
&gt; impossibility of having a break; in std::for_each struct Action { void operator()(some_type&amp; obj) { //Do stuff if (condition) throw break_exception; } }; //... try { std::for_each(container.begin(), container.end(), Action()); } except(break_exception&amp;) {}
Speaking of old, obscure C++ features... today I re-learned that `not` is actually a valid C++ keyword (I think I knew about alternate symbols, but had forgotten). You know... because you happen to be using a non-ASCII 7-bit character set, or whatever the totally valid reason was from forty years ago. Visual C++ doesn't seem to complain at all about a variable named "not". Clang and gcc, naturally enough, will refuse to compile. The complexity of the C++ language does indeed have a cost, as I aptly demonstrated today when I broke my macOS and Linux builds of my library with this small error. Old, antiquated features that hang around forever have a small but persistent cost whenever someone stumbles because of that old feature. Deprecating old, largely-unused features also has a cost, but it's at least a fixed, one-time cost. While I do appreciate the robust backwards compatibility that C++ gives us, I certainly don't mind when the committee sweeps away some of the older cruft. And it's nice when compiler and library creators explicitly give developers more control over how and when they want to tackle changes or deprecated features.
I would recommend Scott's [Effective Modern C++](http://shop.oreilly.com/product/0636920033707.do).
&gt; BTW I use _Generic functions I don't know what this means, but every guess I have would indicate you're relying on non-portable behavior...
it's part of the C11 standard. it's for generic functions. you can switch the function being called based on it's type. it's literally a template, but more limited.
Ah, thanks for the clarification. In C++, names beginning with `_X` (where `X1` are reserved for the implementation, so something that should sound klaxons in user code.
Do you have any plans on updating the OpenMP library? AFAIK, MSVC ships version 2.5 and a lot has changed since then. GPU offloading for example. 
&gt; Visual C++ doesn't seem to complain at all about a variable named "not". `/permissive-` to make it. And funny enough, `not` is a keyword I use often. `!` is easy to overlook in constructs like `(!(`
It's the same in C. Usually the reserved names are defined to regular phrases, _Static_assert becoming static_assert is a good example. tbh I'm not sure why they didn't do that for _Generic.
`*` is a bit ambiguous as a trailing wildcard in C++... Looks like a pointer
Interesting. I don't recall ever seeing this used in the wild before. Maybe I've just lived a sheltered life. 
Thanks for writing this, you've clearly thought a lot on the topic! I agree, using the compilation database feels quite a bit like hack for this purpose. The situation with headers gets even worse since a header can be included into two translation units that are compiled differently (e.g., they have different `-D` options that affect the header). I think modules will help a lot in this area since now everything is a translation unit and you can figure out which implementation units correspond to which interface units. Regarding the build system support, in `build2` we are thinking of making the build system core a library so that interested applications can access the functionality programmatically. Though there is still the problem of monitoring for changes. 
This is probably more of a question to the committee, but I'll try anyway: Is there a chance that vector&lt;bool&gt; can get deprecated?
If that happens we'll need an alternative.
That's why, in programming, you are supposed to learn a language, not guess it's behaviour.
The part about futures worries me. I'd hoped they would be fixed by c++14. Now it looks more like c++23 (twelve years, after the original version appeared in the standard). I really don't know how, and really appreciate the work you guys are doing, but c++ standardization needs a way to iterate faster. That aside, I like the Idea of making more of the state public aka, treating the standardization more like an open source of. Can't remember how often I wondered what the state of a particular proposal is.
[Boost::dynamic_bitset](http://www.boost.org/doc/libs/1_65_1/libs/dynamic_bitset/dynamic_bitset.html) seems like an excellent candidate.
I really don't like to be fixing these kind of things, especially because the general consensus is boost is high quality. Having to manually get it compiling feels very wrong
&gt;Because despite looking similar, the two syntaxes mean very different things. Well, that's a bad thing in terms of programming language design then, isn't it ? Else by that logic you could say that esolangs are perfectly fine programming languages for CRUD application development. 
I had no problems with boost.graph and boost 1.66 beta
I like algorithms, but honestly I don't see a reason to use one here. With the current state of affairs a simple range based for loop is just much easier to read. We can revisit this point once the ranges TS and Angebotes Lama Syntax are is in the standard, but currently, using for_each, transform or a hypothetical inplace_teansform in this specific case just adds more boilerplate/noise to the code. One argument is of course that with c++17, you can easily parallelize the algorithmic version, but if you think about it: How much extra work is it really to first write the loop version and switch to the parallel algorithm when an if needed. This sounds much like premature optimization: You uglify code (optimize it for easy refactoring), because sometime in the future, it might gain you a few minutes of shorter refactoring time without even knowing if that will ever be the case. Btw.: This is one of the reasons, why I'd like to see a native string in c++. I'm pretty sure compilers had a much easier time to optimize the transform code, if they actually understood strings.
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7idaef/code_reviews_why_part_1/dqzopoq/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
At least without a ton of exceptions, `/Wall /Wx `is not really a recommended setting in MSVC and IIRC not even the standard library headers will compile with it. (Same for the equivalent `-Weverything -Werror` in clang). I really don't think the boost folks should spend time on supporting this scenario, but I'm sure they (or at least most) will happily accept patches that resolve those warnings.
&gt; As an independent, unbiased developer Wait... aren't you that guy...?
You can also try using std::transform(make_move_iterator(mut_c.begin()), make_move_iterator(mut_c.end()), mut_c.begin(), f); const auto tr_result = benchmark_transform(vs, [](auto&amp;&amp; elem) { elem += kSuffix; return elem; }); for a non-insignificant speedup.
Is it more obscure than `cin &gt;&gt; ch`?
Next you'll complain about: struct foo { foo( foo and )=default; }; 
I believe you, I also feel this way. do you have some real world examples from your experience? in my case (mind that I worked professionally only for 3 years): - tools disappoint in real world. indexers are slow and don't understand everything. debuggers refuse to work and see names. linker errors can hunt you for weeks, until you finally decide to install older Ubuntu on vagrant. profiling is difficult because code compiles slowly with it and takes too much RAM. unit testing is hard, because mocking is too troublesome, etc. - writing classes takes a lot of time. you need snippets or macros to be somehow productive. - 3rd party code. a lot of cool things on GitHub, but only small fraction you can trust their maintainers. e.g. in case of Google's lib, they sometimes start using non open sourced versions and stop maintaining published version. that was the case with their hash map and they say this on cppcon video. in other words, if you use something, you have to be ready to maintain it yourself. that's from top of my head, things that irk me as a developer.
Of course, we have a lot of warnings disabled for things that don't matter 99% of the time (extra padding added between members and so on). The errors in boost almost always come from unused variables void fn(int a) {.. body that doesn't use a } Or narrowing type conversions (size_t to uint32_t). They're never major but it's always something that has to be fixed.
Maya 3d
another nice one is : struct foo { foo(); compl foo(); }; you can read it as "completion of foo()" :p 
Ok, those are a different matter. Afaik, those a are already part of /W4 and here I do think that they should be fixed in boost itself. In fact, I have from time to time opened a bug report or pull requests for those.
The worrying part about futures is if other standard library functions use them as part of their API's. If they're unfit for purpose then this shouldn't happen.
a fun thing to do is to track what /u/STL writes in cppreference by noticing variables named meow :p https://www.google.com/search?q="cppreference"+"meow"
Wait, that works? I always thought that `~` is a part of identifier and cannot be separated with space.
„use std::std::string“ typo 
Is it just me or is this essentially a content-free article? It wishes to find the "most efficient" version of a trivial algorithm yet somehow fails to provide anything that looks like measurements. It also fails to explore the most obvious option of using a range-based for loop, on what appears to be religious grounds. Seriously, WTF? I don't understand this fascination programmers (in general, and C++ in particular) seem to have with choosing the most wordy, complex, slow-to-compile, and possibly also slow-to-run version of anything. The easiest solution is this: for (auto &amp;s : strings) s += "++"; That's everything. Why would you ever want to write something like what Kronikarz is writing down below? Does that in any way make the situation clearer? Does it get any more performance? Why not simply express what needs expressing, and leave it at that? 
If you encounter references to cats in cppreference, it is probably [Cubbi](http://en.cppreference.com/w/User:Cubbi) doing.
std:: types can basically be thought of as native types. The compiler is absolutely allowed to be hard coded internally to treat std:: types specially. 
Trigraphs are already removed. I'm not sure if digraphs meet the same destiny.
No. You're thinking of someone else. 
I'm not sure how to initialize the coroutine frame; it seems to be done "under the covers". Below is a simplified example of how I want to make a vector of coroutines, and be able to restart them without heap allocations. This would be straightforward using old style switch based generators. #include &lt;experimental/generator&gt; using namespace std::experimental; generator&lt;int&gt; coro() { for (int i = 0; ; i++) co_yield i; } // is there a way to do this in a single heap allocation, like using placement new? // heap elision is not an option (even if possible) since in actual application // a coroutine can start on one thread and complete on another. // a global custom allocator is awkward (requires global lock) for the first // vector allocation, but then without placement new, subsequent restarts require // new heap allocations. int main(int argc, char* argv[]) { int n = argc &gt; 1 ? atoi(argv[1]) : 10; // one heap allocation; generator&lt;int&gt; is just a pointer generator&lt;int&gt;* c = new generator&lt;int&gt;[n]; int sum = 0; // one heap allocation for each coroutine for (int i = 0; i != n; i++) c[i] = coro(); for (int k = 0; k != n; k++) { for (int i = 0; i != n; i++) { sum += *c[rand() % n].begin(); // one heap deallocation + allocation for each coroutine c[rand() % n] = coro(); } } delete[] c; return sum; } 
I wrote this gist that compares new coroutines TS to old style switch based coroutine. Old style was 3x faster for me. https://gist.github.com/imoldfella/ea264e146df67884056da0af1e61964f 
If I understand your question right, You can define operator new in coroutine_traits&lt;&gt;::promise_type for your coroutine type. But You need to know, that compiler will not use that new to allocate memory for coroutine state, if coroutine, created in some scope never leak pointer (reference) to it outside the scope. In this case coroutine state will be stored on stack of function created coroutine.
Well I have a lot of examples that that I have issues with. To pick a few ###Longevity One of the strongest part of C++ is it backwards compatibility and that new better compilers will improve old code. Other languages can’t compete in this area. But if the developer don’t understand this very strong advantage he/she may code like coding in other languages. Languages mostly used by consultants which has a deadline when the code has to be shipped. The kickback of doing something good in C++ is huge, code can work for decades. Comparing this with other languages, like scripting languages (javascript) or some other type of languages done for specific tasks or code that depends on huge libraries (developed in C++) is not on the same ballpark. Creating quality software is of course a large area in itself and different programmers may have different opinions on what quality code is. The main part is that developers need to know that code in C++ can be used for a long time and adapt to that. ###Style guides. When I started to code there was a style guide that was very popular called Hungarian notation. Microsoft used this style guide and the programmer understood what was good about this style it could be modified a bit to make code much more readable. If someone mention Hungarian notation today, be prepared to be criticized. What was very good with Hungarian notation was that developers didn’t need to jump around in the code memorizing variables. Compilers then didn’t have all the tools and intelligence that they have today and monitors could be like 640x480. If you have code that is like + 100 000 lines today, all these smart features that the environment should manage stops to work. Jumping to locations etc takes time. The bad thing with hungarian notation was that it defined some standardized abbreviations, it should have focused solely on type. When I code I always code variables in a way that I can understand what it is without jumping around or hover over, debugging code my focus don’t need be on memorizing variables but to understand the code. It could be solved with very simple rules. Integer like int16_t, int32_t, int64_t are prefixed with the letter “i”, like “iVariableName”. Values declared with auto could be prefixed with underscore “_”, like “_VariableName”. Object names with the object or the first letter in each word like MyObject “myobjectVariableName” or “MOVariableName”. I have also different styles for application specific code or general code used in many projects. Teaching how to create readable code for developers (not users) should be included in C++ tutorials. You mentioned that picking 3d party code on github is a problem and I think one main reason for this is "hard to read code". It is easy to write unreadable code with C++. But it is also easy to write readable code with C++. And as you point out, you need to be prepared to manage the code. ###Compile times. As you noted, this is the main disadvantage using C++. It takes time to compile. I always do small side projects adding new code. If I use unit testing (almost never does, better to use the preprocessor) there is where unit testing can be done. Even if large project have a good design where new code don’t need that much compile time it is annoying and testing is often time consuming. The combination of C++ and Lua is what I think unbeatable. With Lua it is easy to test and modify C++ code and it isn't that difficult to embed. Languages done for specific tasks or scripting languages don’t understand this problem because they will not do applications that have that size. ###Is C++ simple or not? C++ is very simple comparing it with code done for other languages that use large prewritten libraries that was probably done in C++ that they take for granted as a part of the language. But C++ developers creates their own libraries too and that code if often complex because it is going to solve complex problems. Badly written code solving complex problems can be completely unmaintainable, Well written code doing complex problems solves many problems. ###Teaching C++ The video posted where Kate Gregory talked about how to teach C++ I think was good. There is no need to learn "hello world" applications, no C++ developer will code like that. If I had done simple tutorials I would have focused first on how to write readable code using all features that C++ has today solving problems that at first seems difficult. And from there digging deeper. Maybe use one project that is extended. When Visual Studio 1.0 shipped, there was one sample project called scribble in the books that you got buying the software. That may not have been the best application to do but it was much better than all these small codesamples that are often used in tutorials because you got a better understanding how to use C++ in normal applications.
Eh, if you cared about speed you wouldn't be converting random values to `std::string`.
I just wanted a performance comparison of string conversion in common use case.
In theory yes, In practice, I don't know any compiler that does that.
I'm looking at that, but it seems awkward at best. All the complaints people normally raise about the std::allocator are certainly a problem here. Since the previous coroutine was just deallocated, the new one could probably be allocated in its place, assuming you used thread local variables to record the most recent deletion it could probably work. I will give it a shot and add it to my gist. If the current TS has no way of explicitly placing the coroutine frame, I suggest that it should. Embedded programmers in particular should be for this. Placement new is also very helpful for parallel programming. I don't think either group would be excited about a 3x performance regression to adopt the pleasant new syntax. Here's one possible syntax to throw darts at (I'm sure there are better ways to do this). generator&lt;int&gt; foo(int a, int b) { co_yield a + b; } // fake syntax - each slot is guaranteed to hold foo frame coroutine&lt;foo&gt; coro_slot[100]; // restart the coroutine in place, note that generator&lt;int&gt; is still a pointer coro_slot[10] = foo(5,6); // destroy the coroutine, leave the space coro_slot[10].destroy(); 
I dont think this is true, what if you pass clang an option that makes it e.g. compile against libc++ instead if libstdc++? It cant hard code all possible versions of the standard library. If the compiler is restricted to only use one version of stdlib or else u have to rebuild compiler, then I guess it could do this. But even then I think its typical that stdlib is developed more or less independently of stdlib. In gcc for example, the compiler needs to be able to build programs that link against ANY previous version of libstdc++. Debian distributes compilers in one package and stdlib in another-- you can install new compiler versions easily, but you cant replace libstdc++ without a dist-upgrade. Otherwise all those compiler versions would be useless together, you cannot simultaneously link 2 versions if libstdc++. So i think in reality the compiler cannot hard-code all the std:: types. There are eg some type traits that have to be implemented using intrinsics, but they still exist in stdlib which calls the intrinsics.
No, if you read and learn from that with some additional online material on move semantics, lambdas and auto then you are ready for the real world. Everything else is just gimmicks added to the language that you can look up when you need them.
thanks man!
You are technically forbidden from opening namespace std:: at all (except in very specific circumstances). So technically, you can not use a different version of the standard library than what your compiler ships with. Of course, in practise this isn't the case, but nothing you have said prevents any kind of hard coding, it just needs to detect what standard library it's compiling against.
Strictly speaking, compiler is defined by Standard as a fairy whch magically transforms your code in something executable. There are no mention of how standard library should be organised (it technically can reside in one single file: headers are not required to be actual filesystem entries), how build happens, should compiler and linker be separate programs, etc. Nothing is standard prohibits opening some file from you compiler installation and modifying it. It is a compiler problem, what it allows and what it disallows. Also quite a few compilers ships without standard library at all, or can use multiple libraries at the same time (clang originally used libstdc++, now it can also use libc++, and MS STL)
I added an allocator based version and it still is 2x slower than hacky coroutines, even with a rather unrealistic bump allocator.
Right, and moreover anytime you're using libraries, especially libraries you *don't* control, you're already in the situation that you can't reason locally about the behavior of the code behind the interface. Some libraries are better than others about this, but all the same sometimes there is a great tension between doing things that are 'friendly' to the app developer and doing the right thing. E.g. I have a library that I'm maintaining based on code written by someone else more than a decade ago. That library has started to be used to parse files to rip out metadata tags, but doing so can be very complicated. *That* has led to security vulnerabilities being reported against it (e.g. "my computer got pwned because my file manager tried to read the title of this file using your stupid library"). So I've started to add error checks, but what do I do when the logic error is only encountered 7 levels deep in the call stack in code that had previously never assumed failure because malicious input wasn't in the threat model? Rearchitecting to add error codes across the entire call graph would take forever and be API/ABI incompatible. The library has previously used the `abort` call for some types of these situations, but that's the exact thing we're talking about: having code you call do things that crash the application and that you can't possibly predict from outside. Exceptions are actually a decent solution, even if I would need to be disciplined to make sure an exception never crosses the library boundary. At least there it's possible to compartmentalize their use and still enforce valid state.
Hey everybody, look at the comment above this one! It's right up there /\ Now I am a unique pointer.
&gt; It wishes to find the "most efficient" version of a trivial algorithm yet somehow fails to provide anything that looks like measurements. If you can not bother to read the article why do bother to comment? If you read the article you would have seem the Problem where your tasks is to "fix" the benchmark code to make it look as if transform is not slow. So code is there. &gt;It also fails to explore the most obvious option of using a range-based for loop for_each is basically range-based for loop. &gt;Seriously, WTF? There is something called a sense of humor. Some people have it. 
Yes, this is a good point.
Just tried it now on short and long strings, does not appear to be helping. I mean it can speed up the things a bit but difference between for each is still huge
&gt; I like algorithms, but honestly I don't see a reason to use one here. I can see your point, but there are plenty of people who hate raw loops in their code. Sean Parent and Edouard from quasardb come to my mind. Also as I mention in the article I also pefer What Code to How Code. transform/inplace_transform tells you what happens, while for for_each/raw loop it is less obvious.
&gt; Why not simply express what needs expressing, and leave it at that? &gt;Why not simply express what needs expressing, and leave it at that? I have answered this somewhere else here, but the point is that a lot of people dislike raw loops and also transform is much more What than How code.
I learned from http://www.cplusplus.com/doc/tutorial/ And I did the exercises from http://www.codeabbey.com/ Pretty much it was just google searches for the tools I needed to complete the exercises. And youtube some videos to get a deeper understanding of polymorphism and design patterns.
Microsoft vcpkg is what I use to install most of my dependencies. Works for GLFW... and glad you can just throw into the include folder and link the include folder in your project options. The bonus of installing with vcpkg is that I never have to deal with setting them up again, unless I want to remove them for a new version.
[?](https://github.com/Dekken/mkn_opengl_sample)
I think you are unfairly comparing C++ with Java. Java cannot run natively. It depends on the Java virtual machine to run. You write code once and anything with the JVM can run it. That JVM handles most of the platform specific stuff and someone already dealt with all the issues you are having when they built that JVM. Most of your highly technical stuff is handled for you. C++ programs run at the level a JVM would run. In fact the JVM is written in C++. So expect more work on a technical level to create a C++ project. Expect to put in a lot of includes to do relatively simple tasks. Expect to have to write multiple versions of code to do the same task for different platforms. Expect it to just overall be more difficult than Java. However there is a light at the end of the tunnel. Once you figure out how things work and how your build environment should be you should you shouldn't have to mess with it all too often later. You will also find and build a nice stock pile of SDKs and collections like Boost to simplify your development as well. You'll be able to start a new project in VS and just code. Stick with it. It's worth it.
&gt;Everything else is just **gimmicks** added to the language that you can look up when **you need them**.
I actually didn't intend to compare with Java at all. Believe me, Java was a pain in the ass too. I know C++ is more work, I've actually done a few projects in it before. But I just can't believe how much shittier the tooling and environments are for C++, at least on Windows. For example, in any other language, there is a list of dependencies that the build system can use to put together the environment. packages.config, requirements.txt, pom.xml... etc. Here it's just "dump it in a libs folder and slap together a MSBuild file/struggle with CMake". Now if I wanted to say move computers, I could just copy the packages.config or whatever, and I would be good. Not with C++. No, I have to download everything myself, match everything up and ensure the paths I slapped together earlier all lined up. Now I know there are historical reasons, and C++ programmers are free spirits who build cabins in the woods and commune with nature or whatever, but my god what an experience for newcomers.
Windows and cmake don't mix so well in my experience. Well VS doesn't mix well with much which is why I only work via CLI on windows. Shameless plug [maiken](https://github.com/Dekken/maiken) is cross platform CLI buildtool with git/dependency integration
Need them because someone tried to look smart and use them in the codebase you are working on.
Yeah, the one thing I've heard is that Windows and C++ programming mix like oil and flesh-eating disease. But I ain't dual-booting for this.
I heartily recommend making that `g_buffer` thread-local, otherwise you are in for a world of pain. Is there any reason you do not implement the same optimization of a global string buffer? --- Regarding `bool` conversion, I would note that there is no overload of `std::to_string` taking a boolean. This means that this boolean is *first* implicitly converted to an `int`, and then the `std::to_string(int)` is called; I guess this explains the relatively poorer performance as the conversion algorithm cannot know that the values are limited to `0` and `1`. --- I must admit being surprised at the 30 integers to string results though. At first I thought this would be due to dynamic allocation, but since you are using `int`, `std::to_string` itself will not cause any memory allocation because libstdc++'s implementation of `string` does not allocate for 15 characters or less. It may still be a memory allocation issue of course; it could simply be that `std::ostringstream` starts of with a bigger buffer or has a more aggressive growth pattern while `std::string` painfully doubles its storage each time it is needed. Have you tried using `reserve` on your `std::string` instance to check this hypothesis? This seems slightly unlikely, as the same pattern should then be observed with `double`. --- The Pause/Resume usage *inside* the loop for `double` makes me uneasy. Pause/Resume is far from free: taking a timestamp can involve a system call, and these operations need to use memory fences, etc... Now, I know that `double` conversion to a decimal representation is fairly heavy-handed, so in this case it may not matter, but I would encourage you to check the assembly code with and without the Pause/Resume around the cast.
Huh, I used it on the code in the article and it gave me about a 30% speedup compared to just `transform`.
yes i agree with you, i just thought it was funny how you worded it.
Languages which have built in packaging systems are the exception rather than the rule. 
Really? Maybe I'm just gravitating towards the ones that do, but off the top of my head, C#, Java, Python, Rust, Ruby, Javascript (though it's more like three here) I am including, by the way, languages that don't have one included but have basically picked one that the whole earth uses, and where nearly every library you download will be guaranteed compatible with it.
I'll look into vcpkg, though the whole "install libraries globally" thing sort of triggers my spidey senses.
You can try Conan, which solves the problem of C++ source dependency management. It has a recipe for GLFW in Bincrafters repository. https://www.conan.io/ https://bintray.com/bincrafters/public-conan/glfw%3Abincrafters
CMake might be a pain sometimes, but personally, I haven't seen a better alternative. My colleague swears by qmake, but my personal experience with it was on the par with CMake: a lot of reading docs, examples, and sometimes even the sources of the build tool. On the other hand, your experience seem strange to me. I set up more complex projects with CMake with less troubles. But maybe that's just experience (I spent a lot of time with CMake already, ever since I began using Allegro game programming library). However, I think that if you just want to program, you should stick to MSVC project system. And if you need any dlls copied to the output folder, do it by hand - it will be much quicker, and in the short run, a perfectly workable solution that lets you "just program", without worrying about settings the build environment right (won't work quite well with very complex libraries like Qt, but your case is much simpler). CMake is not there to make configuring builds easy - it is there to make it even possible at all, across all the different platforms. If you don't care about multi-platform compilation at the moment, you're always better off with platform-specific solutions (MSVC project files, XCode projects... Makefiles on Unix, I guess - though at this point, I actually prefer CMake. I'm stupid and can't write Makefiles without spending a few days (or weeks...) on the simplest things).
C++ has package managers out there. Buckaroo, Hunter, Conan, etc. But I don't think they would of addressed your initial issues with cmake. All of them have their own faults as we. And C++ isn't a consistent experience and can't be like the languages you mentioned where a package manager just works. So an end all or official package manager is likely to never take place. But there are certainly a handful you can find on Google and see if one works for you. But if you are using Visual Studio and plan on targeting Windows as a platform then I would stick with vcpkg as your best option. This best option however would be different if you primarily worked on Linux and targeted that or worked on Mac and targeted that. But overall their is no correct answer or solution to cater to C++ across the board.
This is nothing but an excuse. None of the problems the OP is dealing with have anything at all to do with native code or platform-specific code. As a counterexample, Rust's build system Cargo makes this sort of thing trivial. It runs at exactly the same level as C++, but the build tools are more helpful and less finicky.
I think /u/tcanens slipped in most of those, actually.
Move along, move along!
OK, I should have been more precise. It helps. But difference between raw loop and transform remains large. So lets say instead of 1.8x it is now 1.5x. Big improvement, but still does not "fix transform" 
I am using Visual Studio's integration to actually run CMake for me, so that may be the culprit. That and I did sort of cobble it together. I might give it another try before blowing it up and trying vcpkg.
I only tried VS CMake integration (via File-&gt;CMake-&gt;Open) once, and it didn't work at all (hung during the generation stage). So I can't recommend it. Running CMake separately via cmake-gui works much better.
Not to my knowledge, but this is more of a question for /u/spongo2.
&gt; _Generic functions I don't think that templates are the equivalent in C++. _Generic chooses an already implemented function, templates implement functions. 
Digraphs are still Standard, and C++11 added digraph disambiguation so `vector&lt;::whatever&gt;` doesn’t emit a `&lt;:` digraph.
It’s definitely not me, I’ve never edited cppreference.
 If you want a similar language with a sane built system consider Rust up until the C++ community standardizes a built system, dependency manager, module system and online repository.
What you complain is exactly what Stroustrup said in his keynote at cppcon. C++ does miss a way to quick start and just code. 
_"Here you can see a bunch of C++ developers discussing an important question: Who Said Meow?"_
To me it sounds that the source of your problems had to do with include_directories and target_include_directories. Think of build targets (shared library, static library, and executable) of derived classes of the same class ("target"). Add_executable and add_library are the constructors, and target_link_libraries, target_include_directories, target_compile_options and target_compile_definitions are the member methods to modify the internal attributes. When you do target_link_libraries(A PRIVATE B), A is "consuming" the "public interface" of B. The keywords PUBLIC/PRIVATE/INTERFACE control whether the modified attribute gets exposed to both the target and its consumer, only the target, or only the consumer. As to why it worked a second and it stopped working the next, those things always have an explanation. Cmake on visual studio 2017 runs in server mode and the build scripts are regenerated on the fly when they are modified. 
I wouldn't go so far as to say that transform is "broken", even in quotation marks.
I get the impression a lot of C++ developers haven't done much work in other modern language ecosystems. There's been lots of great talk about improving how C++ is taught to beginners and providing resources for that over the last few years, but the world around the language itself (its tooling, build systems, etc.) are still, in my opinion, very fragmented and esoteric. As someone who doesn't work professionally with C++, instead mostly with JavaScript (NodeJS 8 and ES2016 with Babel), Python, and some occasional others, but spends plenty of time with C++ otherwise, the sheer comfort and "just focus on writing code" that the other ecosystems provide is far and above C++. You just jump in, only need to know a few commands (that are just commonly accepted best practice), and you're building a fully fledged project that anybody else can also get involved with. The feedback loop is immediate and just makes sense. I know they're totally different languages (dynamic, interpreted, etc.), so people will say that's why it's different, but I really don't think that's a decent excuse. This is a user experience issue and not enough people are looking at it with eye of a user experience designer. The JavaScript world has benefited massively from being so closely related to web design. You can see that a lot of user experience effort has gone into the entire ecosystem. I really appreciate all the work that has been done on Conan because they're some of the few actually trying to improve things, but to take it as an example, just look at the [home page](https://www.conan.io/). I'm a fairly experienced developer, but I'm immediately turned off by all the jargon thrown at me straight away. Conan vs. conan.io? Recipes and binary packages? conan-transit Bintray repository? conan-center? You want me to go read more about this? I just want some libraries! Compare to the [npm home page](https://www.npmjs.com/) which just gives you a search box to find packages and a little inspirational message telling you what it's for (okay, I don't like that they have a "Sign Up" button instead of just "Get started", but that's more of a business practices criticism). I know this is just the websites, but all these little UX issues add up. In my view, we haven't really improved the C++ user experience until books and tutorials can just say how to: make a new project; specify its dependencies; version your project and its dependencies; build it for whatever platform with a simple command; package your project. And all this should be able to be done without the user having to decide between a bunch of cryptic options, always cross-platform, no vendor lock-in, etc. And everyone should feel comfortable doing it, knowing they're not pushing any developers away or making it so that only some portion of the C++ community will know how it works. It should just be *the way it is done*. The first law of usability is "don't make me think". When it comes to programming, all the thinking should be directed towards solving problems, being creative, enjoying code. In my opinion, C++ generally doesn't do well here. Of course, I still love C++ though and think this can all get better. It's just a design problem. It can be improved.
Sounds like you mixed something up and there's no issue with C++. You could try using GLEW instead of GLAD, and build with just one target. Take a look at (https://github.com/Dwarfius/VulkanRTSEngine/blob/master/CMakeLists.txt)[my CMakeLists.txt], it creates a project with OpenGL, ALUT and OpenAL, TBB, Vulkan, GLM, GLFW and GLEW. It's really small (50 lines) and doesn't do much, and covers everything that I need.
&gt; I wouldn't go so far as to say that transform is "broken", even in quotation marks. If you read the article I disclaimer that multiple times. But it is true that transform in this case is not 0 overhead as some may expect. I know you can claim that it is stupid to expect transform to be 0 overhead for inplace transform, and I agree, but I guess a lot of people would think trasnform is as fast as raw loop.
:(
What's funny to me is the node/web world is moving towards C++-like complexity. Want to compile your Angular program? First you have to transpile the typescript into your `build` folder. Oh, you have to add this to your `PATH` first and then point your `webpack.config.js` to your dependencies. (Make sure it's your virtual environment and not your global first, though.
What's wrong with converting values to std::string here?
&gt; making that g_buffer thread-local Touching thread_local anything is really expensive, and has hidden costs if anything else in the program makes a thread. I strongly recommend avoiding designs relying on thread_local. &gt;libstdc++'s implementation of string does not allocate for 15 characters or less One of the issues with this blog is the library in use isn't specified. Which will be faster depends a lot on your target. For example, we optimized the snot out of std::to_string for integers in 2015U2, but iostreams haven't really had much perf work done, so I would expect them to lose everywhere on our implementation. But of course benchmark needs to happen and all that.
This is my biggest gripe with programming in C++. Compared to the tooling in go or rust’s cargo the build process and build systems are just painful. 
Is there a good read on thread_local, that explains it in depth?
And once you've gone through the whole book, read **Exceptional C++**, then go back to Accelerated C++ and use a highlighter to mark every bit of sample code that isn't thread safe and sample code that isn't exception safe.
The issue with `make_move_iterator` is that the resulting thing isn't even an input iterator, so which algorithms it works with under which circumstances will vary.
Admittedly, the JS front-end world is more complicated than server-side. I actually quite like webpack as a project and it solves a problem specific to the web, but it can get very complicated once you start mixing in all the transpiling and such as you say. The documentation is also pretty esoteric (or at least was the last time I looked). Although, I think some of these problems will go away as browsers get better modern JS support (in fact, it's pretty good these days and we're considering scrapping Babel) and perhaps HTTP/2 reduces the need for aggressive bundling.
It varies based on platform. On Windows anyway there's ultimately an array of pointers in the TEB backing the thing, but how that interacts with `thread_local` can have some pitfalls. See http://www.nynaeve.net/?p=180 for a good starting point. If you dig around in your sources you can see a few places where we've taken the address of a thread_local thing (like errno) and used that directly to avoid repeated trips through TLS.
CMake really is nice. However, it has one huge problem. The CMake website/documentation doesn't include any sensible introduction that explains how to use it. Sure, there is a API reference, but that's not an introduction, it's not going to teach you CMake. Much like reading a German dictionary isn't going to teach you German. There is a secondary problem that there are a lot of really bad "cmake introductions" on the internet that tell people to use it in ways that are not sensible and lead obviously bad outcomes. The best introduction to cmake I've seen is this C++Now 2017 talk: https://www.youtube.com/watch?v=bsXLMQ6WgIk. It's honestly really good and everyone should watch it. And now that I'm thinking about it, it should probably be on the front page of cmake.org with a blinking banner saying "watch this before you use cmake".
Thanks for the article. I accept the point that ostringstream is faster for this particular case. I am concerned that the way to achieve this makes use of a global without and sort of synching mechanisms.. This could be misleading to a newbie, without discussing the caveats of this case. It raises the point, how does this compare when syncing mechanisms are utilised?
And once you've gone through the whole book, read **Exceptional C++**, then go back to Accelerated C++ and use a highlighter to mark every bit of sample code that isn't thread safe and sample code that isn't exception safe.
Thanks.
Also range-based for loops.
Maybe look at code using Eigen3 with just some cross products/ matrix matrix/vector products/additions and block expressions. The Visual Studio compiler has tremendous problems inlining those.... (even in 15.5). With clang-cl everything is inlined as expected and my code runs 3 to 4 times faster. I tried to added more __forceinline (EIGEN_STRONG_INLINE) to Eigens code base but after encountering a call to std::_&lt;some_template_monster&gt; which was just two instructions long I gave up on the Visual Studio compiler and reverted back. (Used Compiler options /Ox(or/O2) /Ob2 /Oi /Ot /Oy /GL)
You should look into Bazel (bazel.io).
This is why I originally switched to Linux. I tried setting up Windows box for C++ not too long ago and ended up just setting up a cross-compiler from Linux to Windows and then using the actual Windows just to test the result, it really makes you jump too many hoops. 
CMake is pure cancer. It's like someone decided to take the worst part of C++, macros, and make a whole language out of it. 
I know the feeling. If you just want to get to programming try to reduce the number of moving parts. Use visual studios project files and package manager/vcpkg. Good luck!
There's nothing intrinsically wrong with wanting stringifying your values. It's just silly to do it anywhere where the speed of doing so matter. I mean, if you wanted a small, opaque object isomorphic to your `int`, why not just pass around your `int`? The only real use-case for having it in that string format in advance is so you can then go put it in some larger string, but if you're doing that the intermediate string is just a waste of time and you should really be using something like [`std::to_chars`](http://en.cppreference.com/w/cpp/utility/to_chars) instead.
Isn't it amazing that programming, basically solving problems with computers requires a lot of problem solving? xD
I really like Accelerated C++ but i would not spend good money on it these days. For one you now have better online resources that are in fact more up to date. Second one of the authors (Moo) has published, with coauthors, new texts worth yor consideration. I actually think of Accelerated C++ as a classic and very much liked it as an introduction to C++. That was years ago and it might pass as an introduction today, still i think a more comprehensive text is a better use of money if you go that route. Also modern C++ requires a bit of a mind set change to take advantage of new features. It is far better to start out using modern practices in my mind. 
Based on previous discussions on this sub I wouldn't say that's the general consensus :p
There is an incredible amount of C++ materials online that didn't exist when Accelerated C++ first came out. Much of it is more up to date too. This is the primary reason why i see Accelerated C++ as effectively out dated and an expense most don't need. One would be better off spending money on the more advanced texts such as written by Scott Meyers that deal with good practice and a deeper understanding of the language. Even then plenty of stuff online that addresses these things. One issue with online resources though is quality of content!! For a new guy this can be a mine field as there are plenty of crap sites and videos. What is needed is a web site that lists highly recommended options, ideally vetted by people with significant knowledge. So far i don't know of such a site. 
Vcpkg stores the libraries it builds relative to the vcpkg checkout, so it's not really global. You can also look into the cmake Hunter project which basically adds a package manager to cmake. I feel your pain. I too was really happy and productive with tools like Maven before moving back to C++ development and having to endure the grueling salt mines of package management for that language. On the other hand, a lot of these hardships can be laid at least as much at the feet of Microsoft as anyone else.
well, `pacman -S g++ boost cmake qt5` and you're good to go for a *looong* time. Maybe the reason "tooling" is poor is because tooling isn't poor at all for us linux users and thus the incentive to build great cross-platform tooling does not really exist ? 
Heh, interesting exercise
&gt; [So off I go to Visual Studio.](http://i0.kym-cdn.com/photos/images/facebook/001/226/680/53f.png)
&gt; transform/inplace_transform tells you what happens, while for for_each/raw loop it is less obvious. And that is what I heavily disagree with. At some point, adding layers of abstraction doesn't simplify the code anymore but makes it more difficult to read (just because `times_three(x)` is a named function it is not more obvious that the raw `x*3`) . There is nothing difficult to understand about a range based for loop with a body consisting of a single instruction (afaik, this was even mentioned by Sean Parent in his seasoning talk). And due to the iterator interface of standard algorithms, the raw loop is even shorter than using `std::transform`(not to mention if you have to write an inline lambda). 
I had this same problem with c++ until I started using qt for networking. It’s amazing how much of the boilerplate it covers for me, so I can implement the logic I actually wanted to write.
If only there was a standard package management for C++... I hope cppget and bpkg catches on.
&gt; And due to the iterator interface of standard algorithms, the raw loop is even shorter than using std::transform(not to mention if you have to write an inline lambda). I disagree about the rest but I agree with this. Thankfully ranges will save us. :) I recently posted a link on godbolt with ranges example, it is quite nice IMAO.
A note on readability: the fact that I have to click on a bunch of links makes the post way harder to follow. This is especially true on mobile.
To each their own. I very much prefer it's UI to that of any other IDE I have used.
If you are willing to just use keyboard shortcuts to change modes you can turn it off.
Could you please specify what compiler and library were you using and on what platform? Without this information, any benchmark is almost useless. 
Thanks for the pointer. We've been investigating Eigen's extensive use of `EIGEN_STRONG_INLINE` which is defined to `__forceinline` for MSVC. We definitely want to get to a place where the Eigen codebase doesn't use `__forceline` almost 2000 times. But that requires the built-in inliner to work for the codebase. 
I kinda don't think that intermediate string is a waste of time; the results need to be formatted somewhere and the SSO buffer is as good a place as any. `to_chars` might be an interesting solution when it is widely implemented but at least for floats the "exact number of digits" requirement turns out to be very expensive. We'll see.
Thank you for the feedback. There are multiple problems, I will try to explain why they exist: 0) I intentionally did not want to write std::transform near the text of the "puzzle" because of spoilers - it may sound stupid but I personally hate spoilers even in silly problems like this 1) if I inline too much stuff post gets long 2) if I do not use links people can accuse me of inventing stuff. 1)+2) == many links to read If you do not trust me without bothering the every reader with graphs of benchmark and links to code used. This may sound arrogant, but I do not write stuff about performance without benchmarking(it is just that like I said inlining too much stuff turns article into a drag to read article). Obviously this depends on ability of reader to trust the Guy On The Internet. Trust me. :) Seriously for now although most of my writing gets hate :D it is not because of I am technically wrong or my code has UB. It is just that people do not agree with me on certain things that depend on "personal feelings"(for example backwards compatibility and making C++ friendly to noobs). 
It looks like a good project, but at the moment it is pretty unusable, mainly because missing documentation
The idea of "balancing" the work done at compile time versus run time seems like really strange and situational advice. If I'm trying to write maximally optimized code that's going to do a lot of heavy lifting, I'd gladly take a compile time of 10 minutes instead of 5 (2x increase) to get even just a 10% performance improvement on code that will get run for several days. Or if you have applications that have very sensitive real-time requirements, it might be okay to even have *worse* net compile time + run time if it benefits your application. E.g. maybe you're trying to make some scientific instrumentation and the faster you can make your runtime, the more accurate your data gathering can be. Maybe you're designing control software, and making your runtime faster can improve your controller's precision, reducing operation risks. It's probably not terrible advice for a lot of people, but it's heavily situational.
As I said earlier, I ain't dual booting for this.
I don't know if you view it as poorly as dual booting, but I've had great success doing a lot of coding inside a linux VM. I use a full graphical environment with a resource intensive IDE, and it doesn't feel sluggish or anything to me at all.
If you don't care about performance, sure, you might as well have an intermediate and throw it in a `std::string`. But if you do care you shouldn't have an intermediate in the first place, and you *certainly* shouldn't be putting it somewhere as hard for the compiler to produce good code around as the SSO buffer.
I totally agree that it depends on the situation. You should totally optimize the hell out of your hot path, but most of the code is not on it. That's why fmt, for example, generally favors compile times but provides the high performance Write API for cases where it matters.
&gt; Meson is an open source build system meant to be both extremely fast, and, even more importantly, **as user friendly as possible** - [Meson](http://mesonbuild.com)
There is a lot more to quality writing than being technically correct.
&gt; up until the C++ community standardizes a built system, dependency manager, module system and online repository. https://i.imgflip.com/eao9e.jpg
From my understanding you still have to compile on Windows right? Or can you build the windows executable on linux?
IMO even on Linux the process is pretty convoluted compared to building stuff in Go or Rust. You can link additional dependencies into your Go or Rust project by basically adding one line to a file and running a single command. The tools will take care of downloading the deps for you and building them into your project. With C++ you usually have to muck around with include paths, linker flags, defines, etc.
Answers the question but misses the point :P
The point of a cross compiler is that it runs on one system to build binaries for another -- in this case for building on Linux,a nd then running on Windows.
&gt; If you dig around in your sources you can see a few places where we've taken the address of a thread_local thing (like errno) and used that directly to avoid repeated trips through TLS. Can't compiler be potentially taught to do such optimizations itself?
Last time I asked I was that's not exactly how cross compiling works. Can you give me a rundown on your process?
Yes. I wouldn't do that though, it is perfectly usable. I just agree that it is not the best it could be.
Interestingly enough, I got different results for range based for and for_each on msvc - have to investigate this further when I have the time.
Yeah, there is definitely way too much in the way of overhead to project creation. I've been having this problem for years, and every time we get a new developer at work that disdain is rekindled... Much of it has to do with a kind of Stockholm Syndrome developers have about their particular process, mixed with the user-unfriendly mentality "but you only have to do it ONCE!", plus a bit of lack of resources to make it happen sprinkled on top (time, capitol, interest, research, etc...) It's really funny that programmers are problem solvers by nature, yet we tend to overlook our own problems. After my current personal project (which coincidentally involves build systems), I'd undoubtedly be interested in joining any groups investigating this!
not to mention that using thread_local in a shared library has far more worse performance than using it in a monolithic executable
Why would I expect s1 = s1 + s2 to be as efficient as s1+=s2? This has little to do with transform itself but primarily with different string functions being called.
Did you a word? Anyhow, here's some info on how to set up a gcc cross compiler: http://preshing.com/20141119/how-to-build-a-gcc-cross-compiler/ The theory is pretty simple, even if there are complexities in practice. If you have a program that outputs a .jpg file, you can build that program for Linux or Windows. Run the Linux binary on Linux and it'll output a jpeg. Run the Windows binary on Windows and it'll output a jpeg. If you have source code for a program that outputs .exe files rather than jpeg's, you can similarly build it for Linux or Windows. Obviously, that's the compiler. Build that compiler that outputs .exe files as a Linux binary, and you have a cross compiler. Run it on Linux, and Windows binaries pop out when you run it.
&gt; Sounds like you mixed something up and there's no issue with C++ It's an unnecessary barrier to entry, though. There shouldn't need to be these steps to get a project to a decent starting point when we have the means to solve this. It's very overwhelming to newer programmers- or even programmers from other languages- and hurts C++'s user acquisition. I can understand the want for advanced control, but there should still be some form of general quick-start or project setup on top of the control.
I got this(I bumped the size of input to 4096*1024) and string is "Bjarne", 3 runs: transform took 85ms for_each took 34ms caveman took 34ms transform took 84ms for_each took 35ms caveman took 29ms transform took 88ms for_each took 32ms caveman took 36ms Caveman :P is raw loop. x64 release. So you either do not have optimization on, or your CPU works better for some patterns(or compiler emits better code). My desktop is antique AMD FX6300. And it could be just noise... Youtube video buffering in the background just when one of the tests is running. 
&gt; There is a lot more to quality writing than being technically correct. I agreee, but if you would look at the upvotes on my comments and posts you would assume I am invalidating iterators in a vector of raw void* pointers :P 
That is exactly how cross-compiling works. At least on arch linux it boils down to installing any mingw-* libraries you need and then just using Windows as the target for mingw. Most problems are due to CMakelists checking for windows compilers for special cases. Usually these are fixed quite quickly. At least it takes less time than simply installing boost on windows :)
Exactly. But we got used to it so we call it ok (where "ok" is synonymous to Stockholm syndrome) and leave it as it is. I had much fun using build2 though lately.
Did you even try build2 or bazel?
I suspect your comments are mostly being downvoted because of your attitude. &gt; If you can not bother to read the article why do bother to comment? Your article is being downvoted because it's written poorly: * You don't show code in the article; all code is in a massive commentless Compiler Explorer blob. The reading experience should be seamless, with inline code as you explain it. * Your syntax and grammar is poor. If you submitted this article as an undergraduate technical writing assignment, it would fail. Study how to simplify sentences and use punctuation properly. * You don't really say anything new or interesting. The problem statement of avoiding raw loops is arbitrary, and leads to unclean code in this case.
This has definitely been on my radar as something to try out in a personal project. I liked the dev's talk on the design features, but I haven't had time to try it out for an actual project I care about.
&gt; This makes the code much faster to compile and reduces the code bloat at the minor cost of a dispatch on an argument type at runtime. Or in other words a once-off/one-time compile time cost is now being off-loaded to the run-time where its cost will be endured every single time the library's API is invoked. That is definitely one very excellent strategy you got going there. 
Cmake is vile in almost every respect. It's also, unfortunately, better than most of the alternatives.
Is this a real concern for y'all? I've never had compile times that were problematic. I did work at a place that had problematic link times (40 to 50 minutes), but if I can compile my source in a couple minutes or less, who cares?
First, the overhead of this is very small compared to parsing and formatting and even with it, fmt::format easily beats printf (see http://zverovich.net/2013/09/07/integer-to-string-conversion-in-cplusplus.html). Second, compilation is not one-time unfortunately, you do it continuously during development and developer's time is expensive nowadays =). Lastly, if you care about specific formatting function calls you can eliminate this and other overheads entirely.
`is_detected` is mine. `basic_regex` isn't.
True, but he is out here doin' god's work.
Well I guess I should say I appreciate your feedback but I dont. Article is fine. Code is useless in this particular article. I am not indroducing a new way to constrain a template . I am discussing why choosing std::transform for this particular problem is slow. 
Out of curiosity: what don't you like about it and what would be better?
If you're designing a header-based library whose consumer is other developers, this kind of consideration makes a lot more sense than if you're designing a compiled library or an application. It's very situational, and though it's worthwhile to be conscious of the various strategies for trading off between run time and compile time, it's probably not a great idea to make sweeping recommendations. If nothing else, people who work exclusively in fields where the sweeping recommendation is actually harmful (like myself) may get riled up at the perception that they're being told that they're doing something wrong ;)
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7irh0d/need_help_tolower_keep_returning_values/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
PREACH!! Welcome to C++. If you can put up with the craziness, you can enjoy a great career ahead (and maybe help make it better for the learners that come after you)
For lib hearers that will be included everywhere, it's critical. Modifying a single line may take you minutes to hours to recompile.
Hopefully, C++20 will have modules, and GCC and Clang will get them in the interim. But, in the meantime, a lot (and I do mean *a lot*, I daresay *most*) libraries come with a `pkg-config` definition, so there's less worry about your include paths and linker options: Just add `pkg-config --cflags &lt;libraryname&gt;` to your `CFLAGS`/`CXXFLAGS` and `pkg-config --libs &lt;libraryname&gt;` to your LDFLAGS, and you're set. It's what most of CMake's `find_library` definitions use, too. While it's not a 100% solution, (but *is* anything, really?) it does simplify things a bit.
I've had people point me at `waf` and `ninja`, but I agree. So how do we go about building a better `make`? Most of the good ideas have been tried, and that got us SCons and premake, and CMake still seems better.
That's not PIMPL - that's just an abstract class with a static factory method.
I've been using Hunter recently and it seems to just pull in all the libraries/dependencies you need purely through CMake and completely independent of your system configuration/package-manager/filesystem. It might be worth trying If you use their preexisting "Hunterized" libraries it's pretty straightforward. However if you need to modify a library that isn't included it can get messy; you'll need to make Hunter manage *its* dependencies and a lot of projects have horrifyingly terrible CMake configurations
Docker, WSL, and VMs are a more than adequate alternative to dual-booting. WSL is just a bit on the slow side because of Windows' poor file I/O performance when dealing with lots of small files and short-lived processes.
Your article is downvoted by your peers. I wouldn't say "it's fine". You directly reference code in the article: &gt; Example of solution is here. Only change is on line 51. If the code is irrelevant, you shouldn't reference it. Otherwise, you should show us what line 51 is, and what the change is. Real talk: people are not out to get you. I have nothing against you, and am not attacking you personally. You will have a happier and more successful life if you learn to step back and evaluate your work objectively. Being defensive and snarky in the comments is not helping anybody.
Major libraries, yes, but there's a lot of smaller libraries out there which don't have pkg-config and/or don't install system-wide. I'm really looking forward to modules in C++, and I do hope that it will help, but the whole process moves along so incredibly slowly because C++ is already so entrenched. I get the feeling a lot of the more experienced developers are already so entrenched in their ways and not familiar enough with how newer languages do things that they don't see this as a pain point. On the other hand a lot of newer developers are turned away from C++ because of these kinds of issues. I do projets in Go, C++, Rust and Haskell... of those four, only my C++ projects require a 400 line Makefile to build. The other have a simply formatted file listing all the dependencies and the tooling takes care of the rest. It doesn't matter if I'm using a major library or someone's random small project they posted to github. That's where I'd love C++ to get to..
Do note that *ninja* is not really intended to be human-readable [0]. It's a faster alternative to *make*, but its selling point is that it's a good target for build system generators like CMake. [0] I know *make* isn't that readable either.
There can only be "the most sane package management", ISO/IEC 14882 is never known for giving such blessings.
What does posting some lines of text have to do with dual-booting, or which OS you're on?
I was really only addressing the image he posted.
IMHO, faster compile times matter only in relatively fast edit/compile/debug cycle. If you can conditionally enable them in debug configuration, then great. If you can't disable them in Release mode even though nothing stops the run-time optimizations from being there, the bad, bad, bad. Code is generally executed much more times than it's compiled. If it's not, when why use C++? You can just pick any scripting language of your choice and be more productive.
Honestly CMake is miles better than the whole ad-hoc thing I used to do with Visual Studio. But you're right about the docs. They assume you know the structure of the system already and just need a quick check on something.
Well the actual act of dual booting isn't too bad for me. Just another bunch of make work I have to do. Whatever. The real thing is getting used to a whole new OS just so I can approach an ease of use I could have gotten with literally any other modern language. It's really a testament to how powerful C++ is that anyone bothers putting up with this at all.
I didn't mean these recommendations to be "sweeping" =). There are always tradeoffs (compile vs runtime for example) as I mentioned in the post and yes, I'm very biased towards library development (should have mentioned in the post).
Exactly. I had a case where modifying one line resulted in an hour long parallel build on a 48-core machine.
I like [https://gyp.gsrc.io](Google Gyp), the syntax is essentially JSON (actually “python dictionary” format), and its cross platform across Mac / Windows, which is along my interests.
The thing is, C++ is more flexible than Rust, Go or Haskell. I'm not saying C++ can't be better, it can, but it must be said that the standard facilities provided by the other mentioned languages limit its adoption on other platforms because the barrier to entry is very high. It is both a hindrance and an advantage that C++ has an essentially dumb simple build model. For example, for very small projects it's super simple -- just pass all of your files to the complier in a script file. When it gets bigger, you might need a response file. Bigger yet and you'll want to check dependencies and only rebuild what you need, then you'll need a build system. The point is that one is not locked into anything and your build complexities can scale with your project. 
&gt; They're never major but it's always something that has to be fixed. Really? Or just silence the warning? You have to remember that warnings are non-standard and every compiler issues different warnings. Boost supports something like 100 platforms so you can expect that different compilers will issue different warnings, many of which are false alarms. I've even seen cases where fixing a warning on one platform created a warning on another platform so it was essentially irreconcilable. With this in mind, compiling with warnings as errors is bound to cause problems so on gcc style compilers, it's best to include boost via -isystem to avoid this. On MSVC, about the best you can do is disable the warning before including boost. 
I am not defensive. I just wanted to say I am right wrt things you can check. About my tone. A lot of people share same opinions as me but are not in position to say so. For example a lot of people think concepts are sh*t, but they do not want to say that because they either are friends with Bjarne or don't want to bother with wrapping their opinion in a corporate speek. My big advantage is that I am not friends with Bjarne neither I can be fired for calling concepts sh*t. Note that this is example, concepts although not great are not sh*t. If you have problems with my writing where I say something that is factually not true(eg "vector growth factor is 2 as defined in standard") I welcome your feedback(this is not sarcasm). Other than that I do not care about grammar or claims that you disagree(eg "unordered_map should be deprecated") unless you have more facts for me to consider(eg "unordered_map unique API is used so extensively in Chromium/Firefox that changing it to hash_map requires human refactoring"). Oh and for the record: people are not out to get me. They just blindly follow the heard and take any criticisms of CPP as me insulting their god(since people do not know what is sarcasm: I am not being literal). This is partially expected and normal(if STL writes exact same comment as me or you he will get more upovtes because of his "brand", same way if Huaweii sold exact same thing as Apple Watch it could not sell same amount at same price). This is fine, since it is human nature... I like Alexandrescu more than Herb and if Alexandrescu said the same thing as Herb I would probably find it more true and cool than if Herb said it. It is a heuristic that works ok most of the time but not always. Only difference is that I am aware of this, while most people think that any downvote they cast is based on pure logic.
&gt; Here it's just "dump it in a libs folder and slap together a MSBuild file/struggle with CMake". Hmm, thats not really how it works at all, even with cmake. Usually, you install the dependencies after downloading, like this: mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=$PREFIX .. cmake --build . cmake --build . --target install And then configure your project, like this: mkdir build cd build cmake -DCMAKE_PREFIX_PREFIX=$PREFIX .. The `CMAKE_PREFIX_PATH` tells cmake where to find your dependencies. If a library doesn't provide usage requirements with either `find_package` or pkgconfig, you might need to do `find_library` or `find_path` manually, but it should be reported as a bug to that library. Most libraries provide usage requirements, but I have found some to disable it on windows(I don't know why some libraries want to make life more difficult for windows developers). Most package managers(like cget, hunter, or vcpkg) build on top of this workflow. With cget you can even just point it to the url of the source tarball to install it: cget install http://zlib.net/zlib-1.2.11.tar.gz &gt; Now if I wanted to say move computers, I could just copy the packages.config or whatever, and I would be good. Not with C++. No, I have to download everything myself, match everything up and ensure the paths I slapped together earlier all lined up. Using a package manager for C++, its a simple as copying a `requirements.txt` file. Even if you dont use one, you can always put the install steps in a cmake script for easy repeatability.
I'm not sure what's more flexible about C++'s (lack of) build model compared to Rust/Go/Haskell. You're not locked into anything there either, their tools just have better defaults.
Sounds like you don't know c++ and blame cmake for it 
I will admit to not knowing CMake, because I started learning it last night. (Not that I should have to learn a new language to build a project) But, during this ordeal I wrote a grand total of one line of C++. It was "\#include \&lt;blah\&gt;" in a futile attempt to get Intellisense to recognize my include directory. Believe me, this is not a "I don't know what a cout is" kind of problem.
This paradigm is frankly new to me, because coming into this, the prevailing notion, at least on the VS side, was download the libs somewhere, and hook up the include dirs and .libs in the vcxproj. Now coming from C# and Python, where libraries are fundamentally a per-project thing and not a system thing, naturally I dumped them into the project folder and dealt with them via relative paths. When I moved to CMake, naturally I tried to deal with this the same way.
I'm hoping that when modules come, we will all standardize on the New World Order of C++, sort of like PyPi and pip. Sort of a long-shot, but that's what hope is.
No firm plans on this but we've discussed it. Perhaps vote on it on user voice? 
UPDATE: I figured CMake out. It only took me an entire Saturday. For anyone else in the far future stumbling upon this, [here's the GitHub repo.](https://github.com/MadDoctor5813/Hyperbola) Basically, I used to have the root CMakeLists add the individual project, then have it include the libs, then hook everything up with target_link_libraries, in the root CMakeLists For reasons unbeknownst to me this violates the laws of thermodynamics or something, and even worse, it violates it without any sort of info or error message to be found. Instead I have each project include the libraries it needs itself, and do the hooking up in its own CMakeLists file. The root just now includes each project and does nothing else. Unfortunately this necessitates that each project get its own set of libraries, which is too bad, because they'll both use a bunch of the same ones. Join me next week when I complain about the error messages from templates. (Seriously, why does an error in me using operator[] in unordered_map point me to line 1189 in &lt;tuple&gt; in the STL?)
Ninja is more of a backend - I’m using cmake to generate ninja files and it works pretty well. I’ve probably not used _every_ possible build tool for C++ but sometimes it feels like it. waf is excellent until you hit something that’s outside its narrow design space and then you’re screwed. qmake is much, much better than it looks at first site - powerful and flexible, but it’s verbose and ill-documented when you start using it for things outside C++/Qt and the specific tasks (e.g. flex) it has native support for. It also doesn’t support cross platform builds as well as you might like - anything out of the ordinary involves, effectively, passing make rules through (and that won’t work well for, e.g. visual studio project generation). I’ve used it qmake with the make (or nmake, or jom) backend for years, with makefiles and Perl scripts wrapped around it to handle the things it’s no good at. Cmake ... cmake was clearly written from a “be cross platform, support multiple backends” intent. And that’s great, it does succeed at those goals. But the build model appears to have been generated iteratively, with many changes to the user-visible API, leading to it being entirely inscrutable in places. And the docs are terrible, starting with there not being any particular story about what it’s dependency model really *is*. If I were doing a green field build/dependency engine I’d probably start by looking at how cmake is commonly used, and by targeting ninja as a backend (as if you can use ninja, you can probably get by with anything). It’s not a small task, though.
qmake seems less flexible than CMake. 
We compile on MSVC, GCC, and Clang and we the warnings are useful because they find left-overs in code (unused variables) and simply wrong code: you created a variable and meant to use it but didn't and instead used something else. "The best we can do" is fix the warnings in boost every time we update - which we do - and every time it's different things that have to be fixed.
Yet your article isn't relevant to most on r/cpp, nor is it written properly.
You’re not locked into anything with Go and Rust either. The tooling just provides sane defaults that work for 99% of projects, big or small, libraries or apps. You can still customize the process if it doesn’t work for your unique use case. 
Try it. I have used both cmake and meson extensively. Unless you need a lot of IDEs support go for it. Default targets for sanitizing, valgrind and much saner cross compilation. The language is also mich nicer. Highly recommended.
Moving inlined things to the cpp in the development cycle (but not on the build server) is underrated.
If you want to see what it would look like with a well-integrated package manager, build system, and C++ modules, see my [CppCon 2017 lightning talk](https://www.youtube.com/watch?v=PxFrhYAYF3M). Back story: I was chatting to Bjarne after his keynote saying that we *can* do all of that. His said send me the link. So I had to make the link...
True :P
It certainly is not a massive concern for me. In a modify-build-test(debug) cycle, I only modify a handful of files at one time, and only build that, plus whatever is needed to have it running (a test case usually). Add incremental linking and precompiled headers and it's not even minutes, it's tens of seconds at worst. In our build setup, the build server for master also does the incremental build, that takes variable time, but average is around 30min for our platform with most code. But that's for 4 builds (32/64bit, optimized and non-optimised[1]). We also have RC and production builds, those take close to 3hours. So... The difference in scale is seconds to hours for me. Different needs, different builds, of course. **What else?!** [1] retail build, but without optimizations; not `_DEBUG`; we use that one only when we fail to understand the optimized crash dump.
Wow.
You're an idiot.
&gt;What you complain is exactly what Stroustrup said in his keynote at cppcon. C++ does miss a way to quick start and just code. Well, sort of. The basic/intermediate/advanced idea would be more like an extended std library, if I followed it correctly. So it'd help you if it had GL in it, but wouldn't help otherwise. I'm interested in your take on it, though, especially how you think it'd work. 
If your're haunted by the **Ghost of C++ Standard Past** in your codebase, don't procrastinate to **modernize** your code 😉 [Clang Power Tools](www.clangpowertools.com) #clang-tidy #modernize #cpp #clangpowertools #VisualStudio 
You dont have to, vcpkg has a per-project option. There's also NuGet 
Congratz on figuring it out 😀 Good old C++ bracket [] operator for maps. For unknown reasons, it's not a find operation, it's a find or insert new which requires default constructible vales and a non const map(even if the key exists).
Checkout the video here: http://cpptruths.blogspot.com/2017/12/the-video-of-new-tools-for-more.html
I guess it is, but the question is, do you need *all* CMake flexibility, and are you ready to pay for it? It's common for more flexible solutions to be harder to use.
CMake is a completely different beast from build2 and bazel: it's not build system, it's project generator. And personally, in the end I need platform-specific project: Visual Studio or XCode, or some other IDE. Specifically about bazel, I have a deep distrust of Google solutions for developers. Everything they do usually gives me pain, especially on Windows (gyp, NDK and I heard complaints about bazel, too), contains explosive surprises (global static variables? Seriously, protobuf?), or otherwise strange (why the hell do they think I want to include the whole damned Breakpad source tree into my project to use it?!).
Yeah... Modifying some headers for me means a 10 minutes on a 4.3 GHz 8c/16t 6900k. Had some contributors who had to change laptop because it would just be too long for them.
Unfortunately it was [declined](https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/13495731-add-support-for-openmp-4-5-to-vc) last year
Your current code on https://gist.github.com/imoldfella/ea264e146df67884056da0af1e61964f is buggy. c[j].~generator&lt;int&gt;(); c[j] = ints(); The assignment operator needs to have a valid non-destructed variable on the left side.
&gt; All I wanted to do was program. I use what you call the "shady" solution: VS project and post-build events. Works nice, never had a problem, took 1/2 (or less) the time I'd need to do it with cmake. Plus, it's a rather complex solution (multiple DLLs, static libs and EXEs, cross-referencing eachother). Add to that a C# project that implements a COM component called by C++ code. IMO: if you don't care about other platforms, cmake is a waste of time.
You could try Meson. It has a little tutorial
yes CMake sucks
do not use exceptions for something else than exceptional code/error, I think it's the rule...
Yeah, I completely agree. I hope C++20 brings the modules to help with that. 
Well in this case it's not really that simple of case. To get a really basic project going without fancy things like OpenGL and GLAD, you don't need anything - std is enough. If you want to get something more complicated, you start looking at how you can import functionality through libraries - process of adding them to an IDE is straightforward. It's relatively simple until you want to go cross-platform. But CMake helps with tackling the cross-platform building, but it's really not easy to pick up. There are other, better options (vcpkg, Sharpmake, etc) than CMake. The point I was getting at is that in his current problem the issue is with CMake, not how C++ is.
It makes using map a lot less annoying than using it in Java, as you can just do: my_map[key] = value; You can argue, that default value insertion can be surprising, but I think it makes the code easier to read.
well, my bad then! who did start this trend ? :p
&gt; But I guess Python=slow anyway, is the pun. I was referrin how for (foreach) loop is implemented for foo in bar: stuff is equivalent to it = iter(bar): while True: try: foo = next(it) stuff except StopIteration: break Or, in C++: auto it = begin(bar); try { while (true) { auto&amp; foo = *it; //Throws StopIteration if it == end() stuff; ++it; } } catch (StopIteration&amp;) {}
N4659 was the last draft for C++17 (and N4660 is what went to ballot); N4687 was the first draft to incorporate C++2a changes so N4700 isn't useful as a C++17 reference.
OK. So N4659 is to C++17 as N4140 was to C++14 ?
N4140 was an FDIS as N4660 is, which is why both are password-protected; but for the purposes of 'freely-available' N4659 is ideal, yes.
You're off your rocker. The configure scripts in linux are worse than anything you'll find on Windows. What a horrid monstrosity.
STL surely started it, years ago on Channel9; it just took other people to carry the torch onwards to cppreference. :-]
https://github.com/cplusplus/draft/raw/master/papers/n4700.pdf
N4140 isn't protected though? (N4141 is).
Chandler explained it better in his talk of course, but the short version is that the interface of std::unordered_map requires an implementation using linked list buckets. As it turns out however, there are "better" ways to implement a hash table that unfortunately don't support the required interface of std::unordered_map.
As I take it, the heart of the problems with std::future was the lack of specification for exactly how these objects were supposed to run - or rather that the one thread per-future would be a disaster when you need to deal with a million futures?
http://www.open-std.org/jtc1/sc22/wg21/prot/14882fdis/n4140.pdf prompts me for a login (as do n4141 and n4660).
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7isuni/learning_resource_advice_for_picking_up_c_with_a/dr1877p/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
`initializer_list` requires special compiler support.
The current std:: future has two problems I am aware of: 1. There is no continuation (`.then`) 2. Usually futures don't block upon destruction - except when they do (if they are created by a call to std::async) I'm not sure what other problems the committee tries to solve / what additional features they want it to gain. You can have multiple futures for a single thread by the way (manually using std::promise). And maybe that is exactly the problem: They are trying to conflate a relatively low level mechanism to pass data's from one thread to another with the high level idea of a task handle that somehow also serves as an interface to an thread pool, or more generally an executor. Maybe it would be better to split those things up. 
Try this one: https://github.com/cplusplus/draft/blob/master/papers/n4140.pdf
I share your complaints about Google being quite incompetent when it comes to coding and you are right that cmake is fundamentally different. I would argue though that we needed reproducable builds and a packaging system 5 years ago and I welcome any change towards that.
I have the standard proper, but thanks. :-] I wouldn't have bothered with Dropbox had I remembered the github repo; I've surely encountered it before, but it didn't come to mind.
You should try Rust.
how's CMake better than Premake?
&gt; I get the impression a lot of C++ developers haven't done much work in other modern language ecosystems. Pfff. I know few if any C++ programmers who haven't done tons of work in other languages. The issue is simple. Most C++ programmers know that the build systems are terrible; but we simply don't have the spare time to design and build a new one. The problem is complicated by the fact that a real build system would have to handle both Windows and *nix systems, which are fairly different, and doing cross-platform development is always painful.
&gt; Sounds like you mixed something up Come on. You're really blaming the victim. I've been doing C++ for over two decades, and by now I'm used to dealing with build systems, but probably weeks of my life in the past have evaporated dealing with obscure, poorly documented build systems like CMake where it's completely unclear what is mixed up and what isn't. It's not like the CMake documentation explains how to get started, or how to troubleshoot, or really gives you any idea at all how not to "mix things up". Note that in other languages, this simply isn't a problem of such magnitude - and you also get a lot more bangs for your buck out of your make system. In Python, for example, writing a `setup.py` is sort of gnarly, but at least you can debug your build in Python. Much more exciting, `setup.py` doesn't just give you a build; it gives you a way to package and automatically install your program and its dependencies on a third-party system. tl; dr: C++ build systems are poorly documented and filled with obscure footguns, and to shrug and say, "You mixed something up," when someone unfamiliar with it has run into a problem is to be deliberately blind to this annoying lack in the language. 
&gt; To get a really basic project going without fancy things like OpenGL and GLAD, you don't need anything Say, what? How would _that_ work? How does the compiler get called? And the linker? How do you set compile- and link-time options?
I suggest working on your reading comprehension. The very first line makes it absolutely clear that this is a _build system_ issue. &gt; Hi, this is a CMake/C++ build system rant. 
&gt; Touching thread_local anything is really expensive, and has hidden costs if anything else in the program makes a thread. I strongly recommend avoiding designs relying on thread_local. In general, I'd recommend against using any kind of global variable at all. Still, in this case, at least the thread_local version would be *correct*.
When saying basic I'm talking about a console-bound application, didn't have to do anything special to get a simple console dungeon crawler with ASCII graphics going - modern IDEs handle the basic setup configurations for you. Even if something's missing, the internet has plenty of answers for specific problems. If you want to control every aspect of the complicated build system, then sure, getting to know it is a giant hurdle to overcome, but not impossible. Recently, at CPP Con, there was a presentation on VSCode and creating applications fast, with a challenge to get an app written and running in 15-20 minutes. Although the speaker cheated, it shows that once you know the system, you can be fast and efficient with it.
Are yoiu sure? The implementation in libc++ (http://llvm.org/svn/llvm-project/libcxx/trunk/include/initializer_list) looks pretty straight forward to me. That aside. I know that e.g. some type traits are provided by the compiler (https://msdn.microsoft.com/en-us/library/ms177194.aspx, https://gcc.gnu.org/onlinedocs/gcc-7.2.0/gcc/Type-Traits.html) and I think some compiler also provide intrinsics that allow a more (compiletime) efficient implementation of some standard library types (I think msvc has something like that for std::integral_sequence), but my original comment was about std::string.
The root comment in this thread has been deleted, so you're unfortunately missing some context that led to that point of mine. Nonetheless, it's not the only example, otherwise I wouldn't say "a lot". There are two very common arguments that I'm referring to: - C++ is static/low level/high performance/targets specific platforms/&lt;insert property of C++&gt; that makes means making a simple, general purpose tooling ecosystem is not possible (this is what the root comment suggested). - C++ already has great tooling - you just use your system package manager to install your dependencies and away you go! Both of these arguments suggest to me that they don't have enough experience with other ecosystems to know what is possible and what a simple, effective, usable ecosystem looks like. These are the people my point is directed at. Note that I didn't say "most" or "all" C++ developers. But I agree, I'm sure many C++ developers know this and have to just deal with what they've got. Nobody's got the time or money or motivation to throw at solving the problem.
&gt; Come on. You're really blaming the victim. Think there's a misunderstanding - I'm pointing out that the issue for this specific case is the incorrect setup of CMake script, not the C++ build toolchain. Since the author didn't post the script, we can't say anything specific, have to guess, thus the vague "There's something mixed up", and I posted my script. &gt; It's not like the CMake documentation explains how to get started, or how to troubleshoot, or really gives you any idea at all how not to "mix things up". Agreed, I know the basics of it just because I was lucky to have examples of how to set it up. Going completely blind into is hard, but there are a couple tutorials out there, even though a bit outdated, which give a level of basic understanding of what's happening. I'm personally a fan of Gradle, I really liked how it provided a simple interface to manipulate required packages and configure the build solution. It would be great to have something similar for C++, Sharpmake might have enough features to do the same. CMake just seems like an old, a bit crippled tank-of-a-tool, but it works, even though it's hard to pick up.
I would place money on C++2a having a module system + build system with some amount of standardized dependency resolution tooling (e.g. an interface which you can use to describe how your library, if pulled as a dependency, should be versioned and compiled), maybe not in the "final release", but at least as an experimental proposal that pick up a lot of steam and gets included in the next standard.
Another solution that uses DSL? No thanks.
I wonder how much time it will take to get widespread adoption of modules. I mean, Boost still compiles on C++03 compilers because their use is still widespread, 6 years after C++11 was finalized. So, even if we have modules in 3 years, how much time will it take before we can *finally* enjoy pure module programming? I'm lucky to be in a progressive company, so I guess it'll come within a year or so for me, but I'm worried I may be a rare case...
We finally ported our last automake project to CMake. I hate CMake, mind you, but it was still a relief!
I wish we had something like cargo, it's so much nicer, it's what we should be targeting.
&gt; CMake really is nice. Honestly? I mean, it's certainly better than a lot of the competition (Make, automake, ...), but it's still godawful! The first warning is that they had to reinvent their own language. It's not an immediate strike, but in general it spells troubles, and boy did it gets into troubles: - I hope you have a good search function, because variables are global. Define a variable in a script, include it in some fashion, the variable is available. Override it in another script (by design or accident) and the last wins (at least... that's how I understand it). - The language is kinda typed, or kinda not, it's not always clear, and the errors can be rather unhelpful. So, on the one hand I'm glad I have CMake. On the other, I'm glad someone else maintains our CMakeLists.txt.
**couple minutes**? Sometimes I feel like working in C++ for so long, I've developed Stockholm's syndrome, but at least I still shudder when it takes a couple minutes to compile incrementally! :D
Theoretically, maybe. In practice the compiler doesn't know that variable access from any other.
As I mentioned on [r/programming](https://www.reddit.com/r/programming/comments/7iq5v4/improving_compile_times_of_c_code/dr19zv5/), this may actually *improve* performance. --- There is a tendency to think that monomorphization and inlining are the end-all/be-all of optimizations, and they should be applied everywhere and everytime. After all, if the compiler has more information the resulting code will be faster, right? Right? **It doesn't work that way.** There is a trade-off about monomorphization and inlining: code-bloat actually reduces performance. At the CPU level: - code needs be cached (occupying L3, L2, and L1:code space), - code needs be decoded, - branches take space in the branch-predictor, - ... Therefore, more code is a *liability*. It *may* pay off, but only if the actual performance improvement of inlining is sufficient to overcome its cost. Now, there are clear cases where inlining is beneficial: getters/setters for example, as the inlined code takes less space than the function call overhead. However those clear-cut cases are few and far between. And this liability is even worse when the code occurs in a badly placed branch. Logging code is often guarded by a check against a level (typically: error, warn, info, debug). If you turn "info" off, but the code of the branch is *right there* and has to be skipped over to jump to the next executable instruction, then the more code there is, the more the CPU is at risk of stalling.^1 Thus, and it may come as a surprise to inexperienced C++ programmers, sometimes a judicious placed `__attribute__((noinline))` or virtual call actually improves the program's overall performance. Also, it's important to measure the *program* performance, not a micro-benchmark performance. In this specific case, a micro-benchmark is unlikely to (1) saturate the code cache or (2) saturate the branch predictor, and therefore will not show how reducing the pressure caused by inlined formatting code on either improves overall program performance.^2 In this case, given the actual cost of formatting in general, I would not be surprised to see this code improving a program's performance. The actual function call overhead is likely to be negligible compared to the actual formatting taking place, and the savings in resource utilization may actually completely overshadow the initial "cost". ^1 *The `__builtin_expect` intrinsics are very useful to help with appropriate code placement; and can be used effectively outside of the traditional `likely`/`unlikely` macros.* ^2 *Microbenchmarks have their place to quickly iterate over a design, and select a handful of candidate implementations; however the actual decision should be deferred after measuring the performance of those candidates in-situ.*
I don't know about "far worse". Codegen can't assume the 0th TLS slot is the one in use but otherwise not much is different.
Isn't the reason for initializer_list needing compiler support that it "captures" parameters more aggressively then normal parameters
CMake integration in Visual Studio is a bit clunky and broken. It was pretty much non-functional in the 15.4 release. 'cancel cache generation' did nothing and Cmake remained disabled. An error in the build? Restart visual studio. It's fixed now in 15.5. They didn't mention it in the release notes. I've had a lot of my CMake issues magically go away by adding the CMakeSettings.json file and setting the generator to Visual Studio 2017 instead of Ninja. I mean, really, is that such a good idea? A generator (ninja) for a generator (cmake) which finally spits out vcproj files? 
99% of the time you need to put it in an intermediate location. If you want to `fwrite` it, it's got to go to a memory buffer first. If you want to display it to the user, again, needs to be in a buffer first. It's nothing to do with "caring about performance". Formatting an integer requires a bunch of divides. Formatting a float involves bignum manipulations and other such mess. The cost of copying 15-20 characters in the face of that is negligible. At least, it was when I profiled this thing when we optimized the heck out of std::to_chars for integers in 2015U2.
It's the only stdlib type that has its own special syntax.
It's useful for your own code, but has fixing any of these warnings in boost fixed a genuine bug in boost? Probably not, and that's why you disable the warnings for boost code either through -isystem or "pragma disable warning" prior to #include. Otherwise you are creating a maintenance problem for yourself.
And how's that working out in practise?
It works on 99% of Linux, Windows or Mac projects. It's doesn't work at all on any game console, embedded system, or any other platform really besides the big three. And a large reason for that is because the standard facilities are effectively tied to the language. In theory, sure they are decoupled, but in practise they are not because it breaks the ecosystem, and therefore, a large reason for using those languages. 
You should really watch that video I linked to. Variables can be scoped, but you generally don't need them (despite people using them all the time). It's also not typed, but the generally bad documentation on the internet about CMake makes that very unclear. CMakeLists.txt can be very clean. Here is a real world one that's very simple: https://github.com/davisking/dlib/blob/master/examples/CMakeLists.txt. Builds all the example programs for dlib and it's dead simple. The most complex thing in there is some if statements to avoid building certain things in Visual Studio since Visual Studio doesn't support C++11 yet. But that kind of thing isn't CMake's fault.
There is language support for std:: initializer_list in the sense, that the compiler will generate an object of that type under certain circumstances, so the compiler has to know if it's existence, but that doesn't mean, the compiler has to be intrinsically aware of it's implementation (e.g. you can implement it as a pointer+size or as two pointers) or even it's semantics.
Number of times RIIR has been suggested as a solution in this thread thus far: 5.
Why? How does it function then if he doesn't know the variable is `thread_local`? How would it generate the proper code for accessing it?
If by c++2a you mean the next standard version (most likely / hopefully c++20) then I hold this bet. If we are lucky we will get language level modules (what is currently specified in the TS) with a modularized standard library - but I wouldn't even be 100% sure about that. In any case, there will be most likely be 3 different implementations in terms of which files are generated and what information is contained in those files. In terms of build systems I would be pleasently suprised if the situation changes significantly in the next 3 years but I don't believe it and I can almost guarantee that nothing in that direction will be officially standardized in any way (has there even been a single proposal in this regard?). GDR tries really hard to push for a modules system that does more than just help with compiletimes, but the amount of resistance is just astonishing.
If you're doing a personal project, forget all build systems and stick to header-only libraries; you will be programming in minutes and never care about build systems for the life of your project. It is extremely liberating, and very *very* quick to compile. My home project is about 50k LOC, also a little engine written in C++/OpenGL. My build system is a .bat file for Windows and .sh for Linux, which invoke cl.exe and g++/clang++ respectively against a single master.cpp file. This master.cpp file #includes every file in the project. I use GLEW and a few STB libs, all header-only or simple header/source file that just get included into my master.cpp. Despite *all* the code being in a single translation unit, the whole thing takes ~1.5s to compile, full rebuild and link. I actually have the project set up to compile a stub exe that launches (and watches &amp; reloads when changes) a .dll/.so that contains all the engine code, so I can live-load the code as I iterate. Write code, press F7, see updates running in program ~2s later. I wrote the .bat and .sh in 2015 and they have not changed other than to add a few compile/link flags (mostly disabling chatty warnings). Combined they are about 40 lines of script. I use Sublime Text as my editor, and have bound F7 to run a script I named "build_above.bat/sh" which simply recursively searches up the directory tree from the active source file until it finds a build.bat/sh and then runs that, that script is 30 lines. Thus completes my "build system". This is all you need to program, especially for a home project, everything else is massively overkill. It keeps everything in the realms of C++ (#include schematics, master.cpp is your project file), is completely deterministic, fast, reliable and depends on nothing. It is so *so* much better and just lets you get on with programming.
Try linux. Getting all the libraries is far easier in my experience.
Visual Studio by default uses MSBuild. Nothing prevents you from doing it properly there too. You can create arbitrary snippets of declarations and include them into the project (e.g., add a SomeDependency.vsprops file that modifies include paths and link targets and add that props file to the project).
I would add that I think it is unlikely that there will be ever one big build system for C++. Most big companies have their own build system/package manager and will most likely not do the switch. The intermediate level is CMake. (Which is horrible in many ways, the worst offender being the poor documentation and tons of bad examples on the web. It is really difficult do find out how to do things in "Modern CMake".) This leaves nice build systems and package managers for beginners. Who, on the other hand, are not what expert C++ programmers care about. I think what C++ needs would be a de facto standard (standard as in default, not as in C++ standard) build system/package manager that works for small to medium projects and is beginner friendly. It is indeed hard to get into C++ programming and use some dependencies for a beginner. I do not think it would be necessary to achieve the same ease as package mangers for Rust, Python, etc. provide, but things for small projects should be much easer. I do not think that such a system needs to be "industry grade" because I doubt that adoption would be high and I am personally doubtful if such a system is even useful or feasible.
There are industries/applications where C++ is more or less the only sane choice and in huge projects with 50-100 or more programmers working on the same codebase, compilation times really does add up. for example gamedev.
&gt; It's doesn't work at all on any game console, embedded system, or any other platform really besides the big three. Cargo is used to compile *to* all the consoles, many embedded systems, and other platforms than linux/windows/mac. But for many of them, you'd never want to compile on the host anyway.
I see several people who say that something *like* Cargo should exist, but nobody saying that a hypothetical "C++'s build system" should be re-written in Rust. Which comments are you talking about?
Brilliant! The move away is very clever. I initially didn't realize the destructor was even necessary and inserted it as a hack when I realize the old coroutine frame wasn't getting destroyed. I expected that generator&lt;&gt; like string&lt; &gt; would destroy the old frame automatically on a new assignment. There might be a good reason for this behavior but it seems counter to reasonable expectation.
Looks like range-v3, what's the difference?
You are missing the context &gt;Code is generally executed much more times than it's compiled. If it's not, when why use C++? If you are in a such position that you don't really care about performance and anything not abysmally slow would be good enough, then you are much better served by languages other than C++.
Would it be break very much code if it was changed to work like vector&lt;int&gt; and the other types? Beyond just consuming 8 times more memory.
I am not familiar with range, will take a look. One interface that is neither documented and fully explored in tubez is pull interface. This should enable cyclic descriptions I haven't played around.
Nobody `fwrite`s single integers if they care about performance. The only time the performance of formatting matters is if you're writing to a buffer, eg. if you're serializing values to JSON, since if you're not you clearly don't care about performance. Copying large buffers is done with SIMD, so we're talking a cycle or two overhead. And maybe MSVC is super efficient or something, but I use Linux and a simple fast implementation to write an integer is 5-6x the throughput of `std::to_string` on GCC and 7x the throughput on Clang. I suspect someone who spends genuine effort on it would manage a bunch better. Code for demonstration. #include &lt;cstdlib&gt; #include &lt;cstring&gt; #include &lt;chrono&gt; #include &lt;iostream&gt; #include &lt;random&gt; #include &lt;vector&gt; size_t __attribute__ ((noinline)) print_str(char *buffer, int value) { std::string digits = std::to_string(value); strcpy(buffer, digits.c_str()); return digits.size(); } const char DIGIT_PAIRS[] = "00" "01" "02" "03" "04" "05" "06" "07" "08" "09" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30" "31" "32" "33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44" "45" "46" "47" "48" "49" "50" "51" "52" "53" "54" "55" "56" "57" "58" "59" "60" "61" "62" "63" "64" "65" "66" "67" "68" "69" "70" "71" "72" "73" "74" "75" "76" "77" "78" "79" "80" "81" "82" "83" "84" "85" "86" "87" "88" "89" "90" "91" "92" "93" "94" "95" "96" "97" "98" "99"; size_t __attribute__ ((noinline)) print_buf(char *buffer, int value) { bool negative = value &lt; 0; unsigned uvalue = abs(value); if (value == -2147483648) { negative = true; uvalue = 2147483648; } char digits[] = { // - 2 1 4 7 4 8 3 6 4 7 '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '\0','\0','\0','\0','\0','\0','\0','\0','\0','\0','\0' }; char *digit_ptr = &amp;digits[11]; while (uvalue &gt;= 100) { unsigned digit_pair = uvalue % 100; uvalue /= 100; digit_ptr -= 2; digit_ptr[0] = DIGIT_PAIRS[digit_pair * 2 + 0]; digit_ptr[1] = DIGIT_PAIRS[digit_pair * 2 + 1]; } if (uvalue &gt;= 10) { unsigned digit_pair = uvalue; digit_ptr -= 2; digit_ptr[0] = DIGIT_PAIRS[digit_pair * 2 + 0]; digit_ptr[1] = DIGIT_PAIRS[digit_pair * 2 + 1]; } else { digit_ptr -= 1; digit_ptr[0] = '0' + uvalue; } digit_ptr -= negative; memcpy(buffer, digit_ptr, 12); return (&amp;digits[11]) - digit_ptr; } void run_test(std::vector&lt;int&gt; &amp;test_numbers) { char *buffer = new char[100000]; size_t offset = 0; for (size_t i = 0; i &lt; 10; ++i) { for (auto test_number : test_numbers) { // offset += print_str(buffer + offset, test_number); offset += print_buf(buffer + offset, test_number); if (offset + 20 &gt;= 100000) { offset = 0; } } } buffer[100] = '\0'; std::cout &lt;&lt; buffer &lt;&lt; "\n"; delete[] buffer; } int main() { std::mt19937 rng(0); std::uniform_int_distribution&lt;int&gt; int_dist{}; std::vector&lt;int&gt; test_numbers(10000000, 0); for (int &amp;digit : test_numbers) { digit = int_dist(rng); } auto start = std::chrono::system_clock::now(); run_test(test_numbers); auto end = std::chrono::system_clock::now(); std::chrono::duration&lt;double&gt; difference = end - start; std::cout &lt;&lt; difference.count() &lt;&lt; "\n\n"; } 
Generator and string probably work the same here. Both will destroy its current internal memory when new content is move assigned to it. However, you expected in your code with the fugly_allocator that the memory is already returned before you construct a new generator. Having only the move assignment line c[j] = ints3(); You construct the generator first without having the old memory returned. The return of the memory from c[j] in the move assignment happens after your construction of the generator, which is too late for your purpose. 
I recently pulled apart a rats nest of a "cmake like build" that built each component individually using a shell script for each component and then used relative paths to include other project includes and libs. it even cheated with static libs by doing link_directory on other projects object files. First pass was dumping the individual scripts and moving it to a central makefile. Now I can actually create dependencies. Second pass was converting the code base to one cmake project instead of a cluster of cmake projects. I also had to deal with like 13 library depends built from source... All of which were previously "installed" to the build directory so the main project could easily reference them. Oh and this takes an hour to build. I opted for a "install it once" local library directory for all our dependencies and learned how to write cmake find_package scripts. Best investment i ever made. I actually feel like I understand cmake now. Now I can turn a build in 2mins and I know exactly what is being included. Whole thing took me a week. Cmake may have it's warts but I'll take it's bash like language over gnu make lisp and bash integration any day of the week.
Yes
&gt; you'd never want to compile on the host anyway. I did not mean to imply that. I meant "for", not "on". And maybe I shouldn't pick on rust, because it's getting enough attention that this kind of work is being done for less popular platforms. But, regardless, it's work that needs to be done to be able to effectively use the language on those platforms, whereas C++ has a smaller barrier to entry for the vendor, but requires the user to bring their own build and deployment system if the project is of any significant complexity.
I always find hard to understand the fascination for all these build systems, helper libs and so on. You have Visual Studio, you say "all you want to do is program"... start coding then. GLFW? You don't need it, just learn how to open a window and GL context yourself, problem solved. CMake? You don't need it.. VS does eveything you'll need, I can't understand what you could possible need in order to start coding a simple 3D engine. Instead you decide to fight the system and then go on a rant on the internet about it.. makes perfect sense.
Yes this is something I love. I just started using rust a couple of weeks ago and cargo is amazing in this sense. You guys have been doing great work.
https://godbolt.org/g/CXytnJ
A tour of C++ by Bjarne
Windows as a system for C++ development is a pain. unfortunately, modern CMake tries to fix Windows problems and modern CMake guids end make simple things complicated since they want to sell Windows practice as best practice.
There are a couple of ways this could happen. Firstly, the committee could agree on an alternative, say `std::dynamic_bitset` or something like that. Then they could declare the `vector&lt;bool&gt;` specialisation as optional, but deprecated. Implementations would probably continue to provide it by default to avoid ABI breaks, but provide a compiler flag to disable it. Alternatively, the inclusion of concepts and the Ranges TS into the language and library means that at some point we'll probably want to have constrained versions of the standard containers anyway. It's likely these won't be 100% backwards-compatible, so it offers an opportunity to define a `std2::vector` which doesn't have a specialisation for `bool`. This means that old code could continue to work as it always has with `std::vector&lt;bool&gt;`, but new code could use the new container. Note that this doesn't preclude also providing a dedicated `dynamic_bitset` type in namespace `std` or `std2` for people who actually do want a compressed representation and proxy iterators etc. 
But in the grand tradition of build systems provided by Google, it has been dropped and is no longer developed. [GN](https://chromium.googlesource.com/chromium/src/tools/gn/) is the new hotness until they decide to drop that for something else.
Maybe I haven't spent enough time with it, but it seems very much less "batteries included" than CMake, and a premake script, by necessity, is expected to be more conscious of the target build system than CMakeLists are. So, CMake ***seems*** better.
I could learn to do all that window stuff, or I could.call glfwCreateWindow(). As for why CMake, I need some files moved around and processed by tools in the same project. Now I could do that in the tiny tiny post build event box, but u don't want to.
I really wish they had example projects for every man page. Would help me see if the thing I'm reading about actually solved my problem or not. 
I mean, yeah, profile profile profile. When I talk about a 10% improvement in performance, I mean "as indicated by a profiler" or "as measured by running it live", not "according to my intuition".
Seconded. The entire rant only complains about lack of package management. I either use my own tried and true CMake modules and snippets, or run with Conan. Or stick with vcpckg if it's windows only and better matches your tooling. Also CMake does take a while to get a hang of, but it's pretty powerful once you know what you are doing. And if lost, there's always github search for others examples
No worries, I thought that might be what you meant, but wasn’t sure. Some people do care about this kind of thing... Anyway, I agree this problem is hard, and Rust had some structural advantages here. I’m glad there’s smart people working on making this kind of thing easier in the C++ world. 
Thanks!
The fact that maps provide operator[] should be a rant on its own.
&gt; A generator (ninja) for a generator (cmake) which finally spits out vcproj files? Isn't Ninja a build tool/system, not a generator?
Know that Range-v3 is being standarized right now, hopefully it will be added to C++20. It looks like you duplicated a lot of work.
One of the only things holding me back is that I have an educational license to CLion available which I also wanted to pay around with on a personal project( I've liked JetBrains IDEs previously). I have checked and the support doesn't seem likely to get added soon (which makes some sense as CLion leverages CMake heavily). Well that and time as with other languages on my list want a chunk of my not significant spare time. 
Not true, there’s still commits being pushed to the gyp repo. Gyp and GN solve two different problems; gyp creates project files for IDEs similar to CMake, GN generates ninja files, and ninja is a command line only build system.
That greenfield project sounds somewhat like [meson](http://mesonbuild.com/) which already was mentioned below. 
Well, Rust in particular runs on plenty of other platforms, and is only limited on that front by LLVM rather than Cargo. In fact, [Xargo](https://github.com/japaric/xargo) is a great example of how well Cargo handles other platforms, without any need to drop down to writing the whole thing yourself for every project.
&gt; C++ has a smaller barrier to entry for the vendor Not at all. The vendor doesn't have to *touch* Cargo for it to support their platform. Providing an LLVM backend, a target definition, and the equivalent of libc is enough for Cargo to start cross compiling. With Rust, neither the vendor *nor* the user provides the build system.
You colleague is strange. QMake deemed horrible (or at least unsatisfactory) even by Qt devs themselves. They are planning to switch it's just a lot of work and there is no consensus on the new build system (afaik they are choosing between CMake which has the most adoption out of all cross-platform build-tools and bigger user familiarity and Qbs which seems to be much nicer syntax-wise, but doesn't really have any wide adoption and could potentially become second qmake, i.e. the tool that is only used for qt). qmake syntax is nice for simple programs, but as soon as you need to do something complex and interact with 3rd party deps it becomes horrible to manage.
Sometimes wheel needs to be reinvented to learn something :D
It may be a bug, but this is the code from std::experimental::generator which does not destroy an owned coroutine handle generator &amp;operator=(generator &amp;&amp;_Right) { if (this != _STD addressof(_Right)) { _Coro = _Right._Coro; _Right._Coro = nullptr; } return *this; } Looking at the code for string its really apples and oranges, since string doesn't really destroy the memory it owns, it tries to reuse it. A more valid comparison might be to unique_ptr, which does destroy the owned object. unique_ptr&amp; operator=(unique_ptr&amp;&amp; _Right) _NOEXCEPT { // assign by moving _Right if (this != _STD addressof(_Right)) { // different, do the move reset(_Right.release()); this-&gt;get_deleter() = _STD forward&lt;_Dx&gt;(_Right.get_deleter()); } return (*this); } 
Why should I use meson instead of cmake? 
I can't seem to set the compiler to clang-cl on windows. Setting it to clang and clang++ does work however.
Assume that my headers are directly in `src/`. Can I force them to be visible like in different path so in code I can write `#include &lt;myproject/myheader.h&gt;`? 
No, compilers work on directory paths so it's not really possible for a build system to do this (at least portably). You can fake it by having a symlink `myproject` that points to `src` but it is also nonportable and these sorts of hacks tend to break in unexpected nasty ways.
They can be copied to build directory like generated files.
[rxcpp](https://github.com/Reactive-Extensions/RxCpp) exposes a similar interface and implements a "push" mentality (when new data is available at some point of the pipe it triggers the subsequent computations). [ranges-v3](https://github.com/ericniebler/range-v3) (going for c++20 standardization), as already note, also exposes a pipeline interface but implements a "pull" mentality (all computations are lazy, only when someone at the end of the pipe asks for data the computation is triggered). I do not understand where tubez position itself with respect to these two projects.
Meson's DSL for writing build definitions is a lot more productive than CMake's. It's also easier to understand and debug.
How to split library to fancy modules, like boost and Qt, in wrap file?
If you wish to build from source then the dependency must have Meson build definitions (we don't support arbitrary other build systems because it leads to problems). Then you split it up in the usual Meson way, usually with project options. If you wish to use prebuilt libraries, then you can split it up in any method you choose and provide the result [as a prebuilt library dependency](http://mesonbuild.com/Shipping-prebuilt-binaries-as-wraps.html).
I mean that I can depend on Qt using syntax `dependency('qt5', modules: ['Core', 'Gui'])` but I don't see how I can reproduce that syntax in my libraries if I want something like `dependency('mylib', modules: ['foo', 'bar'])`. I can only do `dependency('mylib_foo')` and `dependency('mylib_bar')`.
I had an idea that might encourage people to switch to Meson: have a CMake "backend". If I could generate a CMakeLists.txt from my Meson build, then I wouldn't have to worry much about CMake support. I don't know if people would actually want this or how practical it is, but it's an idea.
Sadly there is no way to do that yet, because there is no agreed-upon way to declare dependency groups. Qt and Boost ones are hardcoded and based only on their naming scheme. If you only care about the case where you build from source (rather than doing pkg-config files) you can do: foo_sp = subproject('foo') mod1_dep = foo_sp.get_variable('mod1_dep') mod2_dep = foo_sp.get_variable('mod2_dep') etc. Assuming you have done the corresponding `declare_dependency`s in the subproject.
That nullptr dereference optimisation is terrifying. Wouldn't the behaviour of the program be defined by the order in which the compiler decides to perform the various optimisations? Is that order the same for all compilers? What if the code was instead Void f(int* i) { If (i) Foo(); Else Bar(); Unused_parameter(*i); }
How about using meson on Windows? Cmake has an installer. Installing meson on Windows seems to involve more steps according to [this post](https://stackoverflow.com/questions/42712896/how-to-run-meson-build-system-on-windows).
The one of the reasons I started Meson was to do all the things CMake could not do. As an example, CMake [still does not do precompiled headers](https://gitlab.kitware.com/cmake/cmake/issues/1260) whereas Meson has had native support for that from almost the beginning. Having a CMake backend would mean not being to able to support these kinds of features. I originally considered doing a CMake backend for exactly the reasons you mention but very quickly realized it would not work.
This is total crap
For what it's worth qbs seems to have pretty decent adoption in the commercial Qt development space. I'm really hoping one of the new competitors like qbs takes off, CMake works, but doesn't feel particularly well designed and is way harder to get into than necessary.
It is better than the ones from the same era, but I'm not convinced that it is better than the recent ones ( such as meson, build2, or qbs.) Personally I like qbs, but it seems like all three are viable and have learned a lot from the CMake era.
By the time the optimizer is running the knowledge that the variable was thread_local is long gone. Also in the case of something like `errno` it's thread_local *in the CRT DLL*, meaning as far as the optimizer is concerned it's an external function call.
Can you expand a bit on your last paragraph? I wasn't aware there are multiple modules proposals.
If you don't want a DSL but do want a modern build system you should look at qbs. It uses QML to define the build, which actually works really well.
What support do you have for custom code generators, i.e. protoc, lex, yacc, etc?
See [sample projects (which are also unit tests) here](https://github.com/mesonbuild/meson/tree/master/test%20cases/frameworks). Protobuf support is a bit plain ATM, improving it is planned for the next release.
You mean when the header is at src/myheader.h? Why not put it at src/myproject/myheader.h?
Question, Instead of creating a new build system from scratch why didn't you just try to improve cmake and add the features you liked to cmake itself ?
 build &gt; mkdir include_root build &gt; cp ../src/*.h include_root/myproject build &gt; $(CXX) -I include_root ../src/main.cpp ?
You can either use the Python one as mentioned in the link or you can use [the MSI installers that we ship](https://github.com/mesonbuild/meson/releases/tag/0.44.0), though note the potential bug in them mentioned in the main text above.
Because the things that are wrong with CMake are the things they will never fix. For example, CMake's only real data type is a string and string typing is the worst of all possible typing strategies. It just can't be made good.
People think because c++ is a language and javascript or python is a language they should get the same experience but it is not the same thing at all. C++ is a box full of wrenches and bolts. Javascript and python are bricks you just glue together, like leggo. C++ lets you build the bricks. Bricks are hard. Bricks are piled in the dirt. Glue/mortar is not quite the same thing. I will cry like a babby when c++ becomes anything like python and/or javascript.
I really dislike this masochistic sort of attitude towards it, where because C++ is used to build great things that it must correspondingly be a pain to use. Is there an actual reason for this beyond the romantic notion of grizzled Real Programmers working in the trenches without package management? I don't think so.
Looks like GCC's implementation of thread_local calls an external function and the optimizer doesn't like that. Makes much less difference on Windows: https://godbolt.org/g/x91tBu
Yes. Sometimes it's not `myheader` but third party library with its own directory structure.
I don't see it as masochistic as much as anarchy. If no one is holding your hand you can do anything at all. It's a philosophic issue rather than a technical one. I like there is at least one language left (other than asm) that let's me do what I want with the machine. 
Just for the record a bit of disassembly (i see similar operations all over the disassembly): 00007FF6512EFD60 lea rdx,[rsp+2178h] 00007FF6512EFD68 lea rcx,[rsp+0EF0h] 00007FF6512EFD70 call std::_Tree_iterator&lt;(more templates) &gt; (07FF6511E7D90h) 00007FF6512EFD75 mov rsi,qword ptr [rax] 00007FF6512EFD78 lea rdx,[rsp+2178h] 00007FF6512EFD80 lea rcx,[rsp+0F00h] 00007FF6512EFD88 call std::_Tree_iterator&lt;(more templates&gt; (07FF6511E7D90h) Goes directly to &lt;tuple&gt; line 877 00007FF6511E7D90 mov qword ptr [rcx],rdx 00007FF6511E7D93 mov rax,rcx 00007FF6511E7D96 ret and thats when I stopped trying to use 15.5 and reverted back to clang-cl. Maybe MSVCs STL needs a few __forceinline itself to deal with the inline issues. 
I see what you mean. OP already gave the best answer then.
keep up the good work!!!!
Yeah I have the diametrically opposite view. What I want is a New World Order for C++. Convert or be destroyed.
I know this is considered bad practice but I really really like using globs in my build system. I feel like the information is already in the filesystem and duplicating it creates unnecessary work. This is the thing currently blocking me from using meson. Is there hope?
don't be scared. I believe in you, even if you don't :)
When I tested this your implementation beats our `std::to_string` by approximately 2x; I don't find this too surprising given that you cut the number of divides in half vs what we're doing. (The perf work we did in 2015U2 was to 1. special case radix == 10 since to_string doesn't let the user change that, and 2. avoid 64 bit divides for long long input, which doesn't apply in your benchmark). The copy out of the string into your temp buffer shows up in the profile but is &lt;10% of the difference (Although we are paying for a branch in std::to_string that we should not be, to ask if the string needs to allocate, since that's never true if the input is int but may be true if the input is long long). I suspect libstdc++/libc++ are calling itoa here which looses the radix == 10 special case which is why they loose by significantly more.
Effective Modern C++ by Scott Meyers is probably your best bet.
There is hope. Eventually you will come around to understand that globbing is unreliable and deceptive. Then you shall be enlightened. (On a more serious note, the people who want to use globbing are those who have not been burned by weird bugs caused by globbing failures. Debugging those is not fun.)
What if someone prove that build definitions are Turing complete? :P
Hm, ok. I just though that the compiler needed to generate a bunch of TEB/TLS Array lookups and/or API calls like TlsGetValue mentioned in the article you've linked, so this knowledge (or at least the notion of some TLS access that could be cached) would be preserved until actual machine code is generated.
The "problem" is that the TS doesn't say anything, about how modules are compiled - for example, it doesn't even require the existance of a bmi-file let alone a specific format (www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0822r0.pdf). And considering that a lot of people are still unhappy with the current specification of modules on a language level (just read some of the modules related papers - e.g. the topic of macros) I find it highly unlikely that the various toolchain communities (and the companies behind them) will be able to agree on a common compilation model, let alone file format specification till 2020. Now, the rest of this post is purely hearsay and modules are a moving target, so I might be completely wrong. My impression is that some (most?) people from the clang community that are involved in modules want to treat the BMI essentially as a glorified pre-compiled header file (containing a dump of some internal compiler datastructure), which is probably how their current module system (which predates the TS by several years) is implemented. Microsoft on the other hand pushes for a standardized file format (or at least an standardized api for parsing it) that does not represent internal datastructures and probably contains the bare minimum of information a tool (like a compiler) needs to understand a cpp file which imports that module. Maybe I've completely missread the current state of modules (I'm not involeved in the process at all, I got that impression just from reading papers, blogs and reddit posts) but until I see concrete progress I hope the best but remain sceptical.
I prefer Tour of C++ for this purpose, EMC++ for someone returning feels like "Look at the sharp pointy bits and how you can cover them up!" without an actual overview of what's changed.
Agree, using variables is a sure sign of code smell in modern CMake. Targets and properties is the way to go. It makes everything straightforward and simple (with a few exceptions of course, there is always exceptions...)
That's reasonably convincing - I retract my suggestion. Read EMC++ later.
I have the impression that to read EMC++ you must already be familiar with a lot of new features.
I read through that other post yesterday. How would it have gone in Meson instead? 
It depends on how was your C++ 5 years ago? Did you use C++11? Have you read a Stroustrup book (The lastest C++ programming Language book, even if it does not cover the lastest of the lastests)? 
&gt; I don't find this too surprising given that you cut the number of divides in half vs what we're doing. Divides by constants are fairly cheap. Witness the following being &lt;10% slower on GCC, ~20% slower on Clang. size_t __attribute__ ((noinline)) print_buf(char *buffer, int value) { bool negative = value &lt; 0; unsigned uvalue = abs(value); if (value == -2147483648) { negative = true; uvalue = 2147483648; } char digits[] = { // - 2 1 4 7 4 8 3 6 4 7 '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '\0','\0','\0','\0','\0','\0','\0','\0','\0','\0','\0' }; char *digit_ptr = &amp;digits[11]; while (uvalue &gt;= 10) { digit_ptr -= 1; digit_ptr[0] = '0' + (uvalue % 10); uvalue /= 10; } digit_ptr -= 1; digit_ptr[0] = '0' + (uvalue % 10); digit_ptr -= negative; memcpy(buffer, digit_ptr, 12); return (&amp;digits[11]) - digit_ptr; } I do have to admit you're at least somewhat right, though. Writing the following only adds a ~60% overhead, so it's entirely feasible that *most* of the slowdown is just an unoptimised stdlib. size_t len = (&amp;digits[11]) - digit_ptr; strcpy(buffer, std::string(digit_ptr, len).c_str()); return len; But still, that's almost half of your time wasted in overhead, and that's only going to grow if one actually spend effort optimising this codepath. It's also only as little overhead as it is because of how much `std::string` is avoided; it exists as a single temporary yet still almost drowns the function! 
I would say you should try the latest version of Creator, and then create bug reports with reproducible examples for the slow debugging behavior, that's the only way the developers can really figure out what's going on, and fix it. Personally I have the same slow performance issue when debugging QtWebEngine / Chromium. The older pre-python-plugin debugger was faster than the new python-based one. So there is code that can be improved. But easy reproducibility is important for that. I'm not sure if MSVC uses something private or not, but creator uses CDB + an extension that uses the cdb Debugger Engine Interface https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/introduction . I guess you could try to debug your application via command line cdb or WinDbg to ascertain the performance difference at least somewhat.
&gt; Even if something's missing, the internet has plenty of answers for specific problems. . &gt; it shows that once you know the system, you can be fast and efficient with it. Yeah, see, these are the exact issues I'm referring to, though, that are entirely excessive for the language's tooling. You shouldn't *need* to be well-versed in a particular build system to get an app started in a few minutes, or *need* to look up information online to add minimal functionality. This is a kind of Stockholm Syndrome that we suffer from wherein we become disillusioned that the current systems are fine and don't need to change because you *can* become proficient in it. It's not the professionals to be concerned with, it's the newer C++ developers who will be overwhelmed. &gt; If you want to control every aspect of the complicated build system, then sure, getting to know it is a giant hurdle to overcome, but not impossible. The point isn't to control every aspect of it, I'm ok with leaving that down to text-editing some XML/JSON/WhatHaveYou structure in the advanced case (for build events, sanitizers, and such), but to enable easier access to project creation for common-case scenarios like cross-platform development and linking external libraries in a friendly way. Even for centralized multi-platform server build systems, there should exist some means for guided, quick setup and extendability. &gt; Well in this case it's not really that simple of case. But why is this not the case? Why can we not have prepackaged modules for OpenGL (or even Vulcan), click a GUI element to add what we need to a project, and have it work on a preconfigured, basic, cross-platform setup right then? I do concede that these systems require resources that most devs don't have, so I'm not saying this is the fault of anyone. Just stating that there exists a need for improvement on the current systems because they are far from user-friendly
I know i'm a little bit late here but I don't agree that exceptions make it any easier to save the game after an uncontinuable error. You can't just save the state at any point in the games execution or you'll just get a corrupt save, and catching an exception with the intent of saving would require the entire code base to be transactional, which is just completely unreasonable imo. If you really wanted to a far more reasonable way to do it would be to just copy the state of the game at the update boundary and then save that later if you really needed to. But arguably neither thing is really necessary if your game has a good robust auto save system, which solves several other problems as well. Also I just don't agree about using exceptions as a long winded way of crashing or exiting the program, and therefore don't agree that they should come with backtraces or crash dumps or any other debug information. If you want to crash, just crash. I really feel like exceptions strength is to rewind from any point and any error back up to a user activated action, and report that error back to the user so they can do something about it. Not to report back a bug or other fatal error to the developer. However like I explained in my other post, games really don't have this situation at all. There's no function or even thread to rewind to, the game has to go on. Those error contexts in games are like 1 or 2 functions deep, it's possible but not necessary to use exceptions to deal with that.
This time of year? http://adventofcode.com for daily problems.
Thanks for your comment. &gt; Have you tried using reserve on your std::string instance to check this hypothesis? I usually keep I will do a more extensive analysis and share a better researched analysis. 
the fact that it exists doesn't devalue your time spent on it! unless you wanted to make something useful in practice. but if it's not the case, don't worry. you developed it without knowing rangesv3, which asserts that there's more differences than similarities anyway. and it's probably smaller = easier to read. I will examine it later, because I've worked with some in-house implementations before and I'm curious of your version.
We haven't looked in Eigen yet, but the fact that it's using `__forceinline` almost a couple thousand times definitely screws with our inliner's heuristics. Adding `__forceinline` to the STL probably isn't the right solution. We'll work on improving the default heuristics to work better on Eigen instead.
Or the people that recognize an obvious violation of the DRY principle in 2017 is stupid busy work. You're never going to convince people that having to list every file by hand is a win, because it isn't. Also Bazel at least claims to correctly handle globs in all situations.
Nice work!
Check out: [LeetCode](https://leetcode.com/)
c++ is the last vestige of freedom in computing (other than asm). personally i hope these campaigns die in a fire and c++ can remain a truly free language where you can manipulate the machine in whatever way you wish without some hand-holding committee deciding what i can and can't do. sory you can't figure out how to build ur shit... maybe npm can help?
because c++ isn't javascript maybe?
I don't have a problem with cmake so I think ur a wuss.
do u work for fox news?
Just curious, are there any changes to C++ modules TS in the recent Visual Studio updates?
http://codewars.com/
How would I know? I'm an independent, unbiased developer who has nothing at all to do with MSVC and nothing to gain from an intelligent, informed, and considerate schedule of old feature removal! But if I did actually have some stake in MSVC I'd say that we've been consistently improving the quality of the modules implementation. So yes, there are significant improvements in 15.5. But it's like peanut butter: it's spread across the feature. Modules is a pretty substantial TS in the way that it's integrated into the language. It's especially impactful in a compiler that might be just oh a little less than standards-conforming and still relies on code that dates back to before Pitfall was released for the Atari 2600. MSVC has a really-close-to-complete implementation but it's got a hefty bug tail. It's ready to try out on your code now, but don't be surprised if it falls over on some nasty templates. We're just now starting build system/IDE integration work. Look for those improvements to pop up in later VS2017 releases. 
CMake: still better than make.
MSBuild is easy to use when you have something simple, but it still has many issues for me. Assuming someone nice made a NuGet package for what you need, you still have to fix the project file if you change the version of the package because it doesn't remove the previous reference correctly. Or how you have to fight with it to make a separate build and source folder so it might be somewhat portable. Or why you can't just have easily a solution-wide .props file that will apply to every project in your solution. Like if you want to add a source path to every project for example.
Compilers should show a warning when you use it, like "are you sure this is what you want?". Especially if you start casting away const because it wasn't const.
Accelerated C++ and Effective Modern C++ are great books. That said, your gimmicks comment is kinda ridiculous. There's tons of things those books will not teach you: details of C++ multi-threading, how to use and write STL allocators, any kind of template programming, useful design patterns, CRTP, etc.
&gt; And if you need any dlls copied to the output folder, do it by hand Good NuGet packages do it for you. Now if only we could have good NuGet support in C++.
It's a dynamic size container that doesn't require heap allocation. It should be pretty clear that it's magical. 
Everyone doing high performance work knows what you're saying. But also I suspect, most people doing high performance work know that you are mostly only going to run into these situations when you really go crazy with templates or even literal code generation. If you go crazy with this stuff you have to make sure it's a win. But if you don't go crazy, a little more of it is almost always a win. 
So I've recently started writing c++ in earnest. One of the earlier things that I learned is that `auto` really should have been named `semiauto` my situation was the inverse, I assumed it would detect a return type was a reference, when in fact it will take a copy -- in my case, later resulting in a destructor being called twice resulting in a double free. A novice mistake, but certainly violates the principle of least surprise (though, in that light, perhaps naming _any_ feature "auto" is a bad idea)
Well of course overall the look/feel of the UI it's subjective. The only thing that is not subjective is the consistency with other applications that belong to the system. I don't see why I'm down voted when I'm providing constructive criticism to the application.
Especially because there are only a few cases where you both need fast formatting and complex formatting (if you don't, you'd be using printf). Suffering through 10 seconds of compiling because you wanted to have a fancy error dump when a rare path is taken is totally not worth it.
Why would I use meson over Bazel? They have similar features: cross platform, multi language, system or source deps, used for real projects, build defections are a DSL, extensions in a Python like language. Main difference I see, Bazel is in Java, supports globs, and many ways is probably more mature.
Thankfully, there is a solution to that. `struct TotallyNotBool{ bool myBool;}` and `std::vector&lt;TotallyNotBool&gt;`
&gt; Here is a real world one that's very simple: it says this is a tutorial in fancy ASCII art at the top..
That's the solution I use. However, it would be nice if we didn't have to teach new C++ programmers about the mess that is std::vector&lt;bool&gt;
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7j0d5h/do_you_have_an_openmp_project/dr2q966/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Not once have we run into any globbing issues in `build2`. On the other hand, a missing header file in the distribution was an every day occurrence until we switched to using wildcard patterns. Originally I was also skeptical about it but now I am a true beliver. Though in `build2` it is well integrated into the buildfile sybntax/semantics so maybe that makes a difference. See the [documentation](https://build2.org/build2/doc/build2-build-system-manual.xhtml#name-patterns) for details.
FWIW, in `build2` it is considered a good practice (see my [reply]( https://www.reddit.com/r/cpp/comments/7iw2ki/meson_build_system_release_0440_is_out/dr2qkju/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=cpp) to /u/jpakkane for details).
a few years ago I created a little quick-start project that I use when I want to mess with OpenGL: https://github.com/seshbot/new-gl-app . Just check it out and go! It also uses google ANGLE so you can write OpenGL ES 2 (which is a fairly common subset, and similar to webGL) code without installing any windows opengl drivers
I understand, it's something that should have had a separate name because it breaks so many assumptions about `std::vector`.
It doesn't own it's storage. Most of the time, it behaves like an array view.
[dcl.init.list]/6: &gt; The array has the same lifetime as any other temporary object, except that initializing an `initializer_list` object from the array extends the lifetime of the array exactly like binding a reference to a temporary.
Only difference between push and pull interface i have found so far is as much not laziness, but who we talk with. If one talks with source/upstream it is pull and if it communicates with drain/downstream it is push. Push seems to be naturally faster, but that is not necessary true, because one can quickly learn that compiler optimizer is crazy creature hard to understand. I still have faith that maybe one can unify both push and pull although knowing all this I am not so sure. 
CMake only makes sense if you're already using it. Otherwise the syntax makes no sense to me and it feels very archaic. Meson is a breath of fresh air. I replaced all my company's firmware projects from CMake to meson in the same time it took to figure out a few simple build settings in CMake. 
Could you elaborate on the "constrained versions of the standard containers" parts? I am not familiar with this idea.
&gt; Can you show the full asm of the function? TEST(Test, random) { 00007FF728E66A40 mov eax,13E8h 00007FF728E66A45 call __chkstk (07FF728EDF810h) 00007FF728E66A4A sub rsp,rax 00007FF728E66A4D mov rax,qword ptr [__security_cookie (07FF728F3A4D0h)] 00007FF728E66A54 xor rax,rsp 00007FF728E66A57 mov qword ptr [rsp+13D0h],rax std::mt19937 gen; 00007FF728E66A5F mov ecx,1571h 00007FF728E66A64 mov dword ptr [rsp+13C4h],0FFFFFFFFh 00007FF728E66A6F mov r8d,1 00007FF728E66A75 mov dword ptr [rsp+44h],ecx } 00007FF728E66A79 mov edx,r8d 00007FF728E66A7C nop dword ptr [rax] 00007FF728E66A80 ?? ?? 00007FF728E66A81 ?? ?? 00007FF728E66A82 ?? ?? 00007FF728E66A83 ?? ?? 00007FF728E66A84 ?? ?? 00007FF728E66A85 ?? ?? 00007FF728E66A86 ?? ?? 00007FF728E66A87 ?? ?? 00007FF728E66A88 ?? ?? 00007FF728E66A89 ?? ?? 00007FF728E66A8A ?? ?? 00007FF728E66A8B ?? ?? 00007FF728E66A8C ?? ?? 00007FF728E66A8D ?? ?? 00007FF728E66A8E ?? ?? std::mt19937 gen; 00007FF728E66A8F enter 0FF41h,0C0h 00007FF728E66A93 mov dword ptr [rsp+rdx*4+44h],ecx 00007FF728E66A97 inc rdx 00007FF728E66A9A cmp rdx,270h 00007FF728E66AA1 jl Test_random_Test::TestBody+40h (07FF728E66A80h) std::vector&lt;uint32_t&gt; v16(1 &lt;&lt; 16); 00007FF728E66AA3 lea r8,[rsp+20h] std::mt19937 gen; 00007FF728E66AA8 mov dword ptr [gen],270h std::vector&lt;uint32_t&gt; v16(1 &lt;&lt; 16); 00007FF728E66AB0 mov edx,10000h 00007FF728E66AB5 lea rcx,[v16] 00007FF728E66ABA call std::vector&lt;unsigned int,std::allocator&lt;unsigned int&gt; &gt;::vector&lt;unsigned int,std::allocator&lt;unsigned int&gt; &gt; (07FF728E6C300h) 00007FF728E66ABF nop std::uniform_int_distribution&lt;int&gt; rpos(0, 1000); int pos = rpos(gen); 00007FF728E66AC0 lea rcx,[gen] 00007FF728E66AC5 call std::mersenne_twister&lt;unsigned int,32,624,397,31,2567483615,11,7,2636928640,15,4022730752,18&gt;::operator() (07FF728E582F0h) 00007FF728E66ACA mov r8d,eax 00007FF728E66ACD mov eax,5E1D27Bh 00007FF728E66AD2 mul eax,r8d 00007FF728E66AD5 mov ecx,r8d 00007FF728E66AD8 sub ecx,edx 00007FF728E66ADA shr ecx,1 00007FF728E66ADC add ecx,edx 00007FF728E66ADE shr ecx,9 00007FF728E66AE1 cmp ecx,417874h 00007FF728E66AE7 jae Test_random_Test::TestBody+80h (07FF728E66AC0h) 00007FF728E66AE9 mov eax,5E1D27Bh 00007FF728E66AEE mul eax,r8d 00007FF728E66AF1 mov eax,r8d 00007FF728E66AF4 sub eax,edx 00007FF728E66AF6 shr eax,1 00007FF728E66AF8 add eax,edx 00007FF728E66AFA shr eax,9 00007FF728E66AFD imul eax,eax,3E9h 00007FF728E66B03 sub r8d,eax v16[pos] += 1; 00007FF728E66B06 mov rax,0FFFFFFFC00000000h 00007FF728E66B10 movsxd rcx,r8d 00007FF728E66B13 mov r8,qword ptr [v16] 00007FF728E66B18 add rax,r8 00007FF728E66B1B inc dword ptr [rax+rcx*4] } 00007FF728E66B1E test r8,r8 00007FF728E66B21 je Test_random_Test::TestBody+140h (07FF728E66B80h) 00007FF728E66B23 mov rcx,qword ptr [rsp+38h] 00007FF728E66B28 mov rdx,3FFFFFFFFFFFFFFFh 00007FF728E66B32 sub rcx,r8 00007FF728E66B35 mov rax,r8 00007FF728E66B38 sar rcx,2 00007FF728E66B3C cmp rcx,rdx 00007FF728E66B3F ja Test_random_Test::TestBody+158h (07FF728E66B98h) 00007FF728E66B41 lea rdx,[rcx*4] 00007FF728E66B49 cmp rdx,1000h 00007FF728E66B50 jb Test_random_Test::TestBody+138h (07FF728E66B78h) 00007FF728E66B52 lea rcx,[rdx+27h] 00007FF728E66B56 cmp rcx,rdx 00007FF728E66B59 jbe Test_random_Test::TestBody+158h (07FF728E66B98h) 00007FF728E66B5B mov rdx,rcx 00007FF728E66B5E test al,1Fh 00007FF728E66B60 jne Test_random_Test::TestBody+158h (07FF728E66B98h) 00007FF728E66B62 mov r8,qword ptr [r8-8] 00007FF728E66B66 cmp r8,rax 00007FF728E66B69 jae Test_random_Test::TestBody+158h (07FF728E66B98h) 00007FF728E66B6B sub rax,r8 00007FF728E66B6E sub rax,8 } 00007FF728E66B72 cmp rax,1Fh 00007FF728E66B76 ja Test_random_Test::TestBody+158h (07FF728E66B98h) 00007FF728E66B78 mov rcx,r8 00007FF728E66B7B call operator delete (07FF728EDF0DCh) 00007FF728E66B80 mov rcx,qword ptr [rsp+13D0h] 00007FF728E66B88 xor rcx,rsp 00007FF728E66B8B call __security_check_cookie (07FF728EDF880h) 00007FF728E66B90 add rsp,13E8h 00007FF728E66B97 ret 00007FF728E66B98 call qword ptr [__imp__invalid_parameter_noinfo_noreturn (07FF728EEF5A8h)] 00007FF728E66B9E int 3 $LN259: 00007FF728E66B9F int 3 
The setup: Windows 10 Visual Studio Community 2017 Version 15.5.0 New Project -&gt; Google Test Default link options (static library Google Test, dynamic c++ library) 
Apologies for beating the old horse; the 15.5 (.1) indeed implements the AVX512VL - confirmed - and it works great, thank you! :) 
How mature is Bazel? Genuine question, I did not use it. In Meson I found from time to time some rough edges but I would say that nowadays is more than usable already.
Even if that is true, I still think it would help adoption. A watered-down CMake with fewer features could be generated. Benefits: - people will start using CMake for compatibility. Especially IDEs such as CLion or VStudio. Eventually they notice that preocompiled headers, out of the box sanitizers, coverage, valgrind and others are supported. At that point they could say: hey, this Meson build system is pretty powerful actually. At that point they notice it is just better :)
I wonder what this _build thing is, I never saw it before: meson _build -Darray_opt=two,three Is that part of meson command line official interface? As long as I understand, mesonconf was replaced by 'meson configure'. Why the example does not look just like this: meson configure -Darray_opt=two,three Would that make sense?
If he worked with it 5 years ago, then EMC++ is the right starting point, I think.
So if I have a file that I add with name1.cpp, later with name2.cpp because I just regret and forget to delete it, the build system can read my mind and I want to discard name1.cpp? I do not get how that would work actually, but this Bazel build system must be extremely smart.
It's misleading, GN is clearly capable of generating Visual Studio, QtCreator which essentially call ninja commands for building but allow all the benefits of having project files. https://chromium.googlesource.com/chromium/src/+/master/tools/gn/docs/reference.md#ide-options
&gt; Here is a real world one that's very simple Not enough XML for Eclipse, VS and Xcode to muck with there. We have to try harder
I really hope modules will be part of C++20 so we don't have ten thousands build systems out there.
Another thing /u/jpakkane did not mention is that the documentation for meson is light years ahead. If you do not know any of them, I mean meson or CMake, meson is going to be so much easier to get going. The cmake documentation is like a reference without examples. Meson has sections properly explained for at least most of the use cases that you will want to handle in every day work: http://mesonbuild.com/Manual.html
When C++20 modules will be out, things will get much easier.
Yes, and then making your program portable get the same issue. Of course developing on Linux is easy, but not releasing for other systems than Linux. 
Well if you think that keeping garbage files outside VCS history is important then I can't agree, it's like commeing large pieces of code and leaving them in case you will need them later.
Well, yes, CLion relies on CMake. I do not think you can do much about it right now. As a build tool I prefer Meson. Also, special mention to the excellent manual: http://mesonbuild.com/Manual.html
What is your proposal? 
Which is a DSL that you just happen to know :)
It is perfectly fine when you exactly know what you are doing. // mymap is a non const lvalue somewhere. mymap["force"] = true; mymap["additionalparams"] = false;
Just because there is Google in the project name is a non-goal to me.
Yeah, I think modules will shake up the C++ build system space quite a bit.
And did you check premake? It was really interesting alternative to CMake.
However, GN doc says: "You cannot generate “real” projects that look like native ones like GYP could." https://chromium.googlesource.com/chromium/src/tools/gn/+/HEAD/docs/faq.md#can-i-generate-xcode-or-visual-studio-projects 
I understand but for end user it makes very little difference. Actually for Visual Studio it's even better because it refuses to parallelize custom build tasks in its native projects.
CMake is bonkers. &gt; `if(&lt;constant&gt;)` &gt; True if the constant is `1`, `ON`, `YES`, `TRUE`, `Y`, or a non-zero number. False if the constant is `0`, `OFF`, `NO`, `FALSE`, `N`, `IGNORE`, `NOTFOUND`, the empty string, or ends in the suffix `-NOTFOUND`. Named boolean constants are case-insensitive. If the argument is not one of these constants, it is treated as a variable. &gt; `if(&lt;variable&gt;)` &gt; True if the variable is defined to a value that is not a false constant. False otherwise. (Note macro arguments are not variables.)
It's perfectly fine when the index is a string literal that practically screams "THIS INSERTS INTO THE MAP". Not so fine when somewhere in the code this exists. if (map[ind] == SomeEnum::State) dosth(); Also IMHO if this results in a segfault, std::vector&lt;int&gt; vec; vec[0]=0; this std::map&lt;int,int&gt; hmap; hmap[0]=0; should too. 
&gt; Just stating that there exists a need for improvement on the current systems because they are far from user-friendly. Fully agree, but just in the current time, when those user-friendly build configuration tools are hard to find, there's not much that can be done other than getting better with what we have.
If you think `xargo` is great, check out `cross`, it's a bit higher level than `xargo`. 
And a ton of unit tests that serve as examples for the features, so if you ever don't understand how something should work there are examples for everything.
&gt; It works on 99% of Linux, Windows or Mac projects. It's doesn't work at all on any game console, embedded system, or any other platform really besides the big three What? We use [`cross`](https://github.com/japaric/cross) for cross-compiling and testing our Rust libraries on x86, x86_64, arm/aarch w/o hf, mips, sparc, powerpc, .... Setting the cross-compilation and testing harness took less than 10 minutes.
Use already existing solutions. Lua, Python, and many more. They have existing rich ecosystems, knowledge base, and programmers that already know them which means they don't have to waste time learning a language which is absolutely useless elsewhere. Also some of them have good language design and/or performance. I don't know a single case where DSL, or even an in-house scripting language, is not a stupid mistake. I don't see a single benefit while there are major downsides. And unless you want more abominations like CMake's DSL, leave it to the professionals. There are many FOSS solutions, there's no excuse to roll your own. For example, CMake to this day can only generate Clang compile commands with makefiles/ninja, while Premake got community made module written in Lua to do it with any generator since 2015.
When I type `pip install meson` on Windows 10 using Python 3.6.3 x64 it doesn't seem to install a `meson.exe` like most python utilities do. I see it has placed `meson.py` in the right directory but without `meson.exe` it's not usable from the command line. I would rather install meson using pip on windows since I've already got Python installed and use it just fine with tools like conan. The broken 64 bit executable doesn't inspire confidence either. Ideally pip installation would work fine on any system.
&gt; I see it has placed meson.py in the right directory but without meson.exe it's not usable from the command line. Just type 'meson.py' instead of 'meson' on the commandline. IIRC that should work. 
My point here was just that you automate something invisible for which you can also make mistakes. Not sure wether it is a terrible thing or not, but happens.
Yeah I tried that and predictably it opened the file in VS Code. I set `python.exe` as the default application for .py files (which I would really rather not do) and it actually starts a new console to run the command instead of running it in the current instance. Also, according to [this post](https://stackoverflow.com/questions/42712896/how-to-run-meson-build-system-on-windows): &gt;Just running meson.py does not work unless you a) have it in your path, and b) have set all .py files to open with python.exe (no thanks). And even then it doesn't seem to pass arguments. Arguments to the command don't work. Like I said most Python utilities ship a small executable that just runs the script properly on Windows. I would love to see meson do the same.
The current futures have more than two problems. We listed them up in our proposal: https://github.com/FelixPetriconi/future_proposal/blob/master/proposal.md
For me this was already common knowledge (and quite a lot of introductions about shared_ptr / make_shared cover this), but it doesn't hurt to raise awareness. tl;dr: don't use make_shared for big objects when using weak_ptr's
&gt; Thanks for writing this, you've clearly thought a lot on the topic! Np. &gt; I think modules will help a lot in this area since now everything is a translation unit and you can figure out which implementation units correspond to which interface units. It will definitely help but it won't make the headers magically disappear so we still have to cope with the problems they bring to us. &gt; Regarding the build system support, in `build2` we are thinking of making the build system core a library so that interested applications can access the functionality programmatically. Though there is still the problem of monitoring for changes. Monitoring the changes shouldn't be that much of the issue I think ... I don't see it being part of the build system but rather as an external component (possibly supporting multiple build systems). However, build system has to anticipate the use-cases of such a component in order to provide enough information for its implementation. 
&gt; Prepare for some apostasy. Your "religion" ("converted to &lt;algorithm&gt;"?!?) is a bit of a straw-man. I for one (and it seems the rest of the commenters here) would just write a for loop and be done with it. &gt; Because modification needs to go through this process: [pass current element to function / assign function return value to current element] No, modification needs to go through this process: for(auto&amp; s: sequence) // 1: get by reference s += "++"; // 2: in-place modification No functions required, no &lt;algorithm&gt; required, no choice between `for_each` and `transform` is necessary (both are wrong choices in this case). 
I'm not necessarily saying Cmake is the answer and I agree it's legacy documentation on the web is a mine of misinformation, but some modern cmake talks Daniel Pfeiffer's original 2014 talk: https://www.youtube.com/watch?v=UnHeOywnj1k&amp;index=1 [Minor] update to his effective Cmake talk (replaces cpp++ Now version) https://www.youtube.com/watch?v=rLopVhns4Zs Steven Kelly - cmake developer and champion of modern cmake... https://www.youtube.com/watch?v=JsjI5xr1jxM Mathieu Roper - his spin on modern cmake usage https://www.youtube.com/watch?v=eC9-iRN2b04 I do wonder if conan + cmake or conan + meson (or bazel?) might be the right answer for now 
"a lot of projects have horrifyingly terrible CMake configurations" As someone who's contributed several libraries to hunter: oh god yes I have nightmares about SDL2s CMakeLists.txt
You should probably read effective modern C++ if you didn't know about this. There may be other things in there you don't know about, too. 
Excuse my ignorance, how will modules replace build systems? I'm genuinely curious.
We recently switched to (modern) cmake and are very happy with it. Massive upgrade from plain Makefiles. Just stick to targets and properties and you're fine, avoid variables as they're a massive code smell.
Very nice talks. Where I can see example projects from that guys where they apply these principles?
Can something be done about the official docs? I feel like they're a missed opportunity.
Hey, you're right!
In my experience Rust and Cargo is only simple because there are sensible defaults that everyone adheres to. In C++ people insists on doing all sorts of crazy stuff. If you stick to standard project structure CMake is just as simple. The difficulty of course is knowing what that is if you're a beginner. In my experience people spend tons of time studying the details of the C++ language, but just wing it when it comes to the toolchain. CMake is just like C++, it's all reasonably straightforward if you take the time to understand your tools.
&gt; In C++ people insists on doing all sorts of crazy stuff. It's not really that people insist. It's just that in C++ there never were a standard build system. All build tools came after the language so there is no way to provide uniform defaults since everyone was using their own "defaults" already. Build tools had to adapt to allow as most use cases as possible.
There are countless resources on the internet about these topics, just follow them and you'll be back in the game: * Move semantics * Lambda expressions * Uniform initialization syntax * The nullptr * Default and delete member functions * Constructor delegation * STL additions: * unordered_map * thread and related stuff * unique_ptr and shared_ptr
Agree, that's we're most of the complexity comes from. But it's not an excuse anymore for new projects.
Are transcripts of these available somewhere?
The point is, good examples aren't existing
True, but most new projects start with CMake or other saner alternative. But they still need to use 3rd party libs most of the time, so their build system still needs to be compatible with their 3rd parties.
Modules require a compiler to have knowledge about said modules, which would/could/should mean that compilers become smarter in the sense that some of the logic we have in build systems now will become part of the compiler toolchain instead.
It depends if you have other good examples, in the case of precompiled headers you can use cotire for instance.
If your goal is to disseminate information, talks are the worst way to do it. If all written down, Googleable information on CMake suggests it's horrible, it may well be.
How about not forcing people into views they do not share? Things like that should always be "recommended usage", not "not going to implement because I **think** its bad". That's not how it works.
Hey, another Rust/JS/Swift/${CURRENT_HIPSTER_LANG} fanboy/girl!
As /u/dodheim said, it extends lifetimes where an array view would dangle, so it's not really the same thing. Put another way: try to figure out a way to pass a variable number of elements to a function without using the heap, or templates, or creating a variable on a separate line (like creating a `std::array` and then passing an `array_view`). It's not possible, so clearly there is some magic in `initializer_list`.
Oh that's nasty
I work on a 6 million line C++ application that has seen 15 years of development. Long compile times are a *huge* problem. I would say your arguments don't have any connection to the real world.
It's a problem for me for instance, with ~100 devs working on the same C++ codebase, keeping up with the latest version of everything means more than 1 hour spent compiling every day. 
Glad it's working for you!
I try to avoid variables as much as I can. The trouble begins when you start to try support multiple dev environments. Sure, generator expressions can be a solution for the simple cases. But they have a tendency to get ridiculous when you are trying to support multiple compilers as well as Win/Linux/Mac. Every combination seems to need their own special treatment with compiler and linker flags, as well as their unique commands to the test framework as well as CI-tool.
Very unlikely. People behind the three main compilers (GCC, Clang, and MSVC) all seem to agree that the compiler should not become the build system. Rather, I believe, the next-generation build systems will be able to handle modules while other will become even more antiquated.
What I wonder is why all that good info is not in the official website. Or some of it at least. The documentation is the worst part of CMake. I say this without hesitation.
Would this application be still chosen to be written in C++ if it were started today? Not necessarily by you or your fellow employees (since you may have a bias towards the language that you've used for so long) but by the company itself? Even then, it doesn't contradict with my statement with being able configure the desired behavior is preferable in this case, since as much as your project probably wouldn't switch to a lib that noticeably increases already bad compile times, countless others won't use it because of the sub-optimal (even if it's really close "sub") performance.
Something that I've had some success with is to set platform specific INTERFACE properties on targets. That way you can propagate them without riddling your CMakeLists.txt files with conditionals or generator expressions. Much fewer variables needed as well.
what exactly is the alternative for new projects? watching talks on modern cmake and implementing advices from them? is this really a complete answer? I don't think "sane defaults" exist even today :( or maybe they are just unpopular, because I don't know where to look. Maybe we need cmake core guidelines?
I prefer to either 1. Precompile and write FindModules, or 2. Use conan.io These two options have the advantage of not relying on cmake compatibility of your thirdparty dependencies.
You don't have an explanation of what this is here and you don't have one on your announcement link. Quick tip for anyone working on a small project: stop assuming people know what it is. 
It's actually worse than that. Programmers are lazy and adding extra busywork discourages them from creating new source files. Existing source files get bigger rather than having decent physical design. And it allows dead code that doesn't even compile to persist in the code base and confuse new team members. This problem only becomes that much more of a problem when the project has four manually-maintained build systems. If you can't do this then your build system is inferior to a basic shell script, and that's saying something.
In theory, all of that already existed 5 years ago. The question is if the author could already use it back then.
Ah, I always posted a link post to the repository main page previous times, which contained all information. I'll edit the announcement to add a description of what it is. Thank you for the feedback.
It would be cool if there was an overload of `new` that allocated two or more adjacent memory blocks with a single allocation. It would have the cost of a single dynamic allocation but it would give you two or more independent pointers to `delete`. This would solve this issue cleanly and maybe some other rare cases, like a dynamically allocated array that can shrink in size in predetermined steps. I don't know if it doesn't exist because it's not feasible or because the problems it solves are so marginal that no on has bothered to propose it. Thoughts?
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7ixd3n/trying_to_get_up_to_date_again_in_c/dr39a95/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Agreed, documentation for Meson is fantastic. I managed to learn how to use it (while struggling to understand cMake for the past 2 days) in about 2 to 3 hours.
I found it actually pretty comprehensive - but its pretty self referential and uses cmake terms everywhere (kinda like trying to learn git) so starting-from-zero is a bit tough try reading this: https://cmake.org/cmake/help/git-master/manual/cmake-buildsystem.7.html and click through anything u don't know. Takes a few rereads for it to a click it really helped me understand the correct workflow - even though I still think its an insane system :) (who thought generators and mashing a build and install system together was a good idea)
I wish their docs would use cookies or something to remember which version you viewed last. Entering the docs from Google always takes you (or at least me) to version 3.0 of the docs, and I have to remember to manually switch to 3.5.2 each time. If I ever forget to do that, I may spend several minutes misinforming myself about the behavior of a function, and then have to start over. Don't get me wrong, I actually think their documentation is fantastic (once you get accustomed to their terminology), but I'd love to see a little quality of life improvement. (...But I'm not a web developer, so I have no idea how feasible that would be.)
The slight performance degradation of the parse lib in the example will in the vast majority of use cases not matter, I would always go with the faster compiling version. If this really is in a critical path then you can still optimize. As for the software: 3D, scientific and cross-platform, so yeah, C++ would still be chosen today.
And there will be much rejoicing. Or rather swearing at cmake.
1st option is hardly portable and manageable if you have lots of 3rd party libs and build for several platforms/OSes (also, not a good idea to put binaries in VCS). 2nd option implies that the lib is already new enough/popular enough that somebody create a package for it and actively maintains it. And also has the same disadvantages as system-wide package managers like on linux: as soon as something isn't there, everything becomes a mess =) All this things make a life a little bit easier but they don't solve the problem completely.
&gt;The slight performance degradation of the parse lib in the example will in the vast majority of use cases not matter, I would always go with the faster compiling version. If this really is in a critical path then you can still optimize. Or it could turn into a death by mosquito bites, when something as common as string formatting/outputting is plastered around all of the codebase and eats up additional 0.001% here and there until it takes a considerable (though maybe still small) time and there is not "hot path" to optimize. Also, all arguments about compile time can be used in the same way regarding run-time. Just change run time to compile time and you get equally meaningful statement.
Hi. I am not a fake account or something :) It was just my first post because I am fan of Qt and I by adding this comment I wanted to say "good work" to author.
&gt; But if you don't go crazy, a little more of it is almost always a win. I agree. I find this specific case all the more interesting because it may well be a prime example of "craziness" given how complicated formatting can be and its common usage in logging.
Have a look at http://www.modernescpp.com/index.php/thread-safe-initialization-of-a-singleton
Keeping the pre-compiled binaries separate from the source-code helps. For linuxes: only have to write a find module. For me the find modules work out quite well for an archive that builds on both linux and windows.
Most high performance allocators use the slab allocator strategy : you keep pools of memory chunk, split into equally sized blocks, for a set of block sizes. Allocating a block of size s consists of taking a block from the pool with the smallest pool that fits. Thus if your compound allocation consists of differents sizes (in different pools), you cannot return two blocks in the same chunk, which is equivalent to performing two independent allocations.
&gt; On the other hand, a missing header file in the distribution was an every day occurrence until we switched to using wildcard patterns. If we're going by personal anecdotes then let me say that I've never had a problem with header files missing in a distribution. Not even once. 
The former is the syntax for running Meson the first time. `_build` stands for the build directory. The latter is the same but for an already existing build dir.
This actually pretty terrible code. Scott Meyer's Singleton is thread-safe (C++11) and the preferred way to implement Singletons. It kinda avoids the SIOF, since the instance is only created when the function is first called. (makes `std::call_once` redundant). You can also use `std::unique_ptr` https://stackoverflow.com/questions/1661529/is-meyers-implementation-of-the-singleton-pattern-thread-safe
Bazel is the result of a monoculture. If your needs are very strongly aligned with Google's, it may be a good choice. If they are not, then you're going to have a tough time convincing them to change. I have not looked at Bazel in detail but from memory the problems it had were lack of proper configure-like functionality, poor Windows support and the like.
That would be considered a bug and fixed.
This is not just a question of "me deciding things for you", there are actual technical reasons for it, too. For example our main backend is Ninja which does not support wild cards of any kind and thus we can't hack support on top of it either.
I did but it is Turing complete so not really an option because I really wanted to create a build system which is not Turing complete.
Open up the YouTube video, click on the three dots, and click on 'Open Transcript'.
So what do you recommend?
Can you point to documentation on how to make Pip do that for you?
Thanks, but I was hoping for something a little more legible than that automated text-to-speach thing about "see make".
C++20 will not have the whole range library. It will just remove the need to use begin() and end().
I understand the Meyer's Singleton is threadsafe, but it doesn't avoid the Destruction Order Fiasco afaik. I think the other benefit of call_once is that it will handle exceptions from constructing... although the `return *I` will segfault in that case so probably no improvement.
I can't speak for glfw, but [here is a sample project in Meson](https://github.com/jpakkane/monocoque/) that does full graphics + sound with SDL on Windows, OSX and Linux. It uses system dependencies when available and downloads and compiles SDL transparently from souce when not.
I'm not at all familiar with packaging Python apps so I just looked at the [conan](https://www.conan.io/) project to see what they are doing. They seem to be using [PyInstaller](http://www.pyinstaller.org/) (which from the description seems to be very similar to what cx_freeze is doing. You can see how they're using it [here](https://github.com/conan-io/conan/blob/develop/pyinstaller.py)). conan like other python utilities that live in the same folder all have a .exe that is like 73kb large and seems to exist just to run the corresponding python script (see: [here](https://i.imgur.com/4K0uqI4.png)). Sorry I can't be more help.
Always nice to see someone recommending code analysis! Additionally, consider following advice in the [C++ Core Guidelines ](https://github.com/isocpp/CppCoreGuidelines). The [C++ Core Guidelines Checker](https://blogs.msdn.microsoft.com/vcblog/tag/cppcorecheck/) included as a plugin to Visual Studio's `/analyze` option is the most complete set of checkers for these guidelines that I'm aware of. 
I'm in the middle of writing a book which is in essence a collection of best practices and an approach to write a better, *modern* CMake: http://effective-cmake.com
No it does not prevent that. Which is why you shouldn’t use singletons at all or make sure they don’t rely on any other singletons (Example: Singleton TextureManager relies on Singleton GraphicsContext -&gt; potential crash) „Exception safety“ Use `std::make_unique`
👍
If you just want an exe to run, then the MSI installer is your best bet (once we get a fix to the 64 bit package from upstream(.
How does Meson produce distributions? In `build2` you have mention everything that will be distrubuted 
If by distribution you mean a source tarball then it zips up everything in your git checkout (plus some extra work for subprojects). In Meson a release is equivalent to a Git checkout, though we support users adding extra things to it if they so choose.
True. It's fine when you only have 2-3 rarely updated deps. If they are much more frequently changed and in bigger number it becomes hard to manage. Also, every OS, architecture (x86, AMD64, etc.), config (SSE/AVX for example) and in some cases compilers (even in linux world ABI breaks are not that rare) multiplies the number of binaries needed which all need maintaining.
Well, that explains why you never missed a header. I hope you at least remove `.git*` stuff. In contrast, in `build2` preparation of the distribution is an operation just like update, test, or install. Which means you can do things like distributing pre-generated source code (which is automatically updated during distribution).
Most have a slides pack. Take a look at the various presenters sites or google
Once I take care of all the real bottle-necks in our code Ill worry about that. No probably not even then. I wonder how much c++ unoptimized string formatting I have to do to arrive at the level of wastefullness of an electron application.
search back through reddit for previous discussions. Several decent examples from the authors of those talks.
Horses for courses. Personally I like the talks and then look through related reddit threads for more tips and info and links to github/presentations etc Adding some other non-talky resources to top post
I think it is google's fault, not for your browser or web-developers in CMake Team. (IIRC Python documentation has a same problems) If you are using firefox, I strongly recommends to use redirector ( https://addons.mozilla.org/en-US/firefox/addon/redirector/?src=search ) addons. You can rewrite URLs using regex, for example, `https://cmake.org/cmake/help/v3.[0-8]/(.*)` to `https://cmake.org/cmake/help/latest/$1`
Every time I find a new DI framework I'm amazed, however in practice I never found a project where using DI would be needed or preferred. 
I believe defining a namespace inside std is undefined behavior. §17.6.4.2.1 &gt; The behavior of a C++ program is undefined if it adds declarations or definitions to namespace std or to a namespace within namespace std unless otherwise specified.
I've used for CMake for 2 years and I know i is not a perfect build system, but other alternatives are sucks(Makefile, bazel, and so on) so we have to use if for next few years. Compared to C++, CMake itself is not so difficult, so I believe that's why there are little good documentations (or blog posting).
That's the "GET OFF MY LAWN" rule.
listdc++ uses `vsnprintf` internally! And it does so by passing a pointer to `vsnprintf` to some function `__to_xstring`, which calls that pointer. This looks like something very difficult for the optimizer. 
This isn't precisely true; the MSFT STL goes out of its way to optimize pointers-to-trivially-copyable-types well. We also have smarts in the STL's contiguous iterators (`string`, `vector`, etc.) so that they can be "unwrapped" into pointers. The actual iterator category for all of those iterator types is still plain vanilla `std::random_access_iterator_tag`.
I have a search engine shortcut, using Chromium but I think other browsers support the same feature: I have `cmake-doc` set to: - http://www.cmake.org/cmake/help/v3.1/search.html?q=%s&amp;check_keywords=yes&amp;area=default And for cppreference.com I have `cpp` set to: - http://en.cppreference.com/mwiki/index.php?title=Special%3ASearch&amp;search=%s&amp;go=Go I use these shortcuts very often. Tutorial with screenshot: http://www.wordreference.com/tools/Chrome-search-shortcut.aspx
&gt; The behavior of a C++ program is undefined if it adds declarations or definitions to namespace std or to a namespace within namespace std unless otherwise specified. is the reason for this that the standard want to reserve the namespace std code that it might add an may use namespaces within the std ? 
I have 2 examples: A toy project: - https://github.com/Sarcasm/slog/ Real world project but that try to handle an older version of CMake gracefully and lacks install targets: - https://github.com/google/cctz 
Future standards as well as STL implementations may clash with your names. Likely, that won't happen for `namespace std { namespace goshs_personal_namespace {`, but it still is undefined behaviour. Just put them in your own `util` or `stdext` namespace and be fine. 
Well, the thing is that it's hard to add a DI framework into an existing project. Sometime the structure or the dependencies of a project are managed in a particular established way that work well enough. Since DI can affect structure and design, I think it's easier implement it into a new or small project. If I really wanted to add DI into an existing project, I would select a subset of that project where I think DI could be beneficial, and mess around with DI there. For example, a place where some singletons are using each other, or a place where a lot of classes need each other to be constructed, or even a place where lot of classes are created then sent into each other with setters.
I like to keep things short, `stdx::`.
We have that. In addition creating a package automatically extracts, builds and runs the test suite. Only if all of those steps pass is it marked as acceptable.
Do a googled in "Qbs"
&gt; The easiest way out, of course, is to just assign a single code to all the reasons something might fail in another subsystem - and pay the price of not being able to give detailed error messages anymore. So, you talk about a poor error message system, and then spend most of your post saying your error message system isn't poor. I'm not sure what that has to do with algebraic error reporting. --- Suppose you have algebriac error reporting in some 3rd party library. You call it. The types of errors it can return are explicitly described and bounded by the API. The code that calls it may run into a problem whereby they cannot translate the error code well. So they *may* take the lazy way out and translate it to "unknown error". This sucks, and the user has a bad experience. The equivalent in an exception library is that the library returns an exception of a completely alien type. Your code goes off and catches all decendents of *your* systems base `std::exception` or `your_base_exception` class. And fails to catch the errors thrown from the 3rd party library. Your application closes itself. Note that the problem here is the same -- an error in the type of exceptions handled. The exception based code is locally clean. It called a function, and like all functions it was presumed to throw. The operation was wrapped in a try/catch block, like all other code. Only because one bit of code far away from the try/catch block failed a specific-project invarient requirement does your code fail. The same local correctness exists in the algebraic code. All functions returning errors are handled. The handler is *poor quality*, as we screwed up and didn't properly deal with the alien error type, but the failure mode is *bad UI* not *application crashes*. --- We back up. There are two possible fixes in both cases. In the exception case, we could add catch(...) blocks at the highest level that also catch these alien exception types. This requires globally auditing the code for the exception types thrown by all code used in the project; easy if the code is all produced specifically for this project under current standards, near impossible otherwise. The other solution is to translate at the point of call. Wrap the library up in try/catch blocks, catch the exceptions it throws, and translate them to your exception system. This requires auditing the API documentation of every library you don't own that throws exceptions. Your IDE and type system gives you zero help here; get it right, or your app crashes. --- Next the algebraic error handling (AEH) based code base calling an AER library. We can augment our algebraic error codes to envelope the ones produced by the library. We can do this in a private header file: Suppose we write an Koenig helper function like this: our_error_type translate_error_to_error_type( their_error_type ) next, we add to our error type a template constructor that lets it implicitly convert from any type that supports `translate_error_to_error_type`. We inject `translate_error_to_error_type` into their error type namespace. We then implicitly convert their error codes to ours. The type system checks if we screwed up. The conversion goes in one spot. We may have to change our error system to be broader to cover their stuff. So long as their library puts its error codes in a namespace, we can locally be certain that the error codes where property translated. --- Next, AEH codebase using exception based library. Here we have to look at the documentation for the exception based library and determine what to catch, wrap all interaction, and basically do the same painful stuff as the exceptoin based codebase has to do with an alien exception based library. AEH based library used by exception based codebase. At the point where you interact with the AER based library, you have to throw the error instead of passing it up. If the types don't match, you have to translate to your own types. This is easier than handling an alien exception based library, because you have the type system on your side. The errors are enumerated by the exception type, instead of merely in documentation. You can locally determine if your error handling code is correct. --- That is what I mean by being able to check *locally* for correctness. Exceptions seem to be a lot like assuming you have a Win32 message pump running and using idle messages to communicate with yourself. If everything exists in the same framework and was written with the same framework in mind you are fine. Because you have *global* assumptions about your code base. Your code is only correct if your *global* exception assumptions hold on your entire code base. Algebraic erorr handling moves the global assumptions into local guarantees. The errors returned? Described by the type system. Barring memory corruption or illegal aliasing, you can reason about the code locally. 
You cannot inject a namespace into `namespace std` legally. The result is an ill-formed program with no diagnostic required. If I was writing later-version code and wanting it in an earlier-version project, I'd use `namespace notstd`. If I was writing utility code, I might use `namespace utility` or `namespace myutility`. 
I doubt that Scott writes a version just for C++17. Maybe C++20 with larger-scale features like metaclasses, ranges, concepts, etc. It would be an addition to the C++11/14 book anyway, so you should read that first 
Which makes the solution obvious. Simply take your code and compile different parts of it with different compilers. Then join the results. Easy. 
I doubt there will be a new version. While Scott Meyers does update errata, he has retired from active C++ development. I'd recommend the book though, anyway. I'd also suggest you take a look at Stroustrup's programming books albeit I don't think any of those have been updated for C++17. The reality is learning C++14 is likely sufficient to be 'modern'. Later you can tackle the C++ 17 goodies
Scott Meyers has retired from active involvement with C++. He said "I'll continue to address errata in my books" but I doubt that includes substantive updates. https://scottmeyers.blogspot.com.au/2015/12/good-to-go.html
I found CMake really hard to get started with. The official docs weren't much use as they are mostly a reference. It was like trying to learn C++ from reading the ISO std doc. It wasn't until I saw some CMake talks on youtube, recorded at cpp confs this year that it finally clicked and I could start writing CMake project files. I'm still not sure if I'm doing it 'right' but I've got things mostly working now, and I find it easier to deal with than the mess of Makefiles that existed before.
The author of hunter has done a writeup of modern cmake style here: https://cgold.readthedocs.io/en/latest/ I haven't read much of it, but it's probably pretty decent
Singletons should always be safe on initialization because it is guaranteed that they are constructed when needed. Destruction order fiasco can be avoided by not destructing at all, aka leaking like you did. If that is acceptable highly depends on such resource your singletons is managing.
I keep all my general stuff into: namespace utility {}
I'll just leave this here: ``` template&lt;class T&gt; struct Immortal { unsigned char space[sizeof(T)] alignas(T); template&lt;class... Args&gt; Immortal(Args&amp;&amp;... args) { ::new(space) T(std::forward&lt;Args&gt;(args)...); } operator T&amp;() &amp; noexcept { return reinterpret_cast&lt;T&amp;&gt;(space); } }; class Singleton { struct key { explicit key() = default; }; ~Singleton() = delete; public: Singleton(key); static Singleton&amp; Instance() { static Immortal&lt;Singleton&gt; instance(key{}); return instance; } }; ```
I'm wondering, does this [std::forward](https://github.com/gracicot/kangaru/blob/master/include/kangaru/autocall.hpp#L35) do anything? Those args are passed by value.
They should be using rel=canonical to tell google which version they want appearing in the search results rather than having it be the most-linked version (which will typically be an old one).
So your argument seems to be that you can forget a try-catch block. How does that matter, though? You have no guarantee that the API won't throw, even in the case of error return codes. You are not (at all) protecting yourself by not having exception handling in your part of the code; quite the opposite, in fact. &gt;Here we have to look at the documentation for the exception based library With error return codes you also have to look at the documentation. &gt;and determine what to catch, wrap all interaction, ... Catching exceptions on a per-function basis is a fundamentally wrong design pattern, pretty much no matter what your problem is. You catch exceptions where you can handle them, not where they are generated. Your idea that you can "check locally" is bogus. The vast majority of APIs do not come with a complete list of possible error codes in the first place, and I have yet to encounter one that uses anything other than #defines to provide a list of numbers. The type safety you talk about doesn't exist. &gt;Exceptions seem to be a lot like assuming you have a Win32 message pump running and using idle messages to communicate with yourself. I'm sure that must have made sense to you when you wrote it... As I try to post this, reddit tells me this: "an error occurred (status: 0)" Do you suppose that's because of someone throwing a zero integer exception, or mis-interpreting an error return code?
I have to admit it's confusing and seem to receive parameters by value, but in fact they can be references too. If you look closely, you'll notice that the type of each arguments are the argument type of another function: [&amp;](detail::function_argument_t&lt;S, typename F::value_type&gt;... args) { The alias `detail::function_argument_t&lt;N, F&gt;` is the type of the `N`th argument of function type `F`. So it can be a reference. Since it's the code related to autocall, the `F` type come from two places. [Here](https://github.com/gracicot/kangaru/blob/master/include/kangaru/detail/autocall_traits.hpp#L27) and [there](https://github.com/gracicot/kangaru/blob/master/include/kangaru/detail/autocall_traits.hpp#L45). As you can see, there are references there. And `[inject_t](https://github.com/gracicot/kangaru/blob/master/include/kangaru/detail/injected.hpp#L112)` always resolve to a rvalue reference.
I would recommend going with namespace sti { }
I'd recommend Discovering Modern C++ by Peter Gottschling. It provides a little more breadth than Tour of C++ and is intended as a quick, but thorough introduction. It covers C++11 and 14 throughout. It's probably a little dense at some points, but a lot of intro texts won't even mention things like SFINAE, RAII, best practices, etc. which this book at least touches on. 
&gt; Original post has been updated to include some links - repeated below. You don't need a separate text post for this. cpp subreddit space is limited and while your other post is good (many upvotes, lots of discussion), we don't need two of them. Removed, as the other post contains this info.
Please don't use `stdext`, that's MSVC's extension namespace.
Reddit doesn't respect triple backticks like GitHub does; you need four-space indents.
I use A LOT of namespaces. Like to keep class and method names short and use namespaces instead. What I was planning on here was only to extend **std** with some minor things that I miss in the lib. Will not add to the **std** namespace area
I use A LOT of namespaces. Like to keep class and method names short and use namespaces instead. What I was planning on here was only to extend **std** with some minor things that I miss in the lib. Will not add to the **std** namespace area
That was true up until C++17. Now we have sized deallocation to take care of this problem.
Seems relevant: [cppreference.](http://en.cppreference.com/w/cpp/language/extending_std) 
`notstd` is exactly the namespace I introduced at work to hold backported standard features.
`wrx::` is a bit more economical for daily use.
Thank you, reddit moderator from across the hall.
thanks
If I wabt to parse and generate code as part of the build process (for instance for reflection) in a cross-platform way without requiring my users to install python or msys2, my only option is to write it in cmake-lang.
Some piece of code that was removed 4000 commits ago can be considered entirely dead in my experience. 
Yuuuuck. And people use it ? 
I personally use Makefiles to compile my projects with mingw on windows and gcc on linux. I personally gave up on CMake the same way I gave up on Maven or gradle. This shit cost me way too much time and didn't increase productivity when it actually worked. Since Eclipse CDT´s analyzer works so well with c++14 I rarely open Visual Studio anymore. I also dropped gigabyte-sized precompiled libraries from my projects. I can get up and running by downloading gnu-tools for windows, mingw and eclipse. Takes 10 minutes on windows, 5 minutes on linux. Use what works for *you*, not what (apparently) works for everyone else 
Anyone up for pronouncing it ku-MAH-kay just to fuck with people?
Btw: Is there a reason, why no iterator category for continuous memory iterators exist?
My guess is that originally it did not exist since it is not very "mathy". I mean RA, BIDI, FWD restrict what operations you can do.... Contiguous is "just" an optimization tag. Additionally AFAIK you can not write Concept for Continuous iterator. Beside what I wrote here there is a nice SO answer regarding this https://stackoverflow.com/questions/42851957/contiguous-iterator-detection
I already said this and got downvoted but C++17 is not that big of a release. Yes it has a lot of nice features, but compared to C++11 that changed idiomatic C++ code hugely it is not that big of a change.
Came for circular references, but all I got was shared block of memory... Other than that comments: u/STL was begging boost people 5? years ago to implement make_shared optimization for a reason: 1 allocation/deallocation is usually faster than 2 even if you hang on to deaded object memory for a while. So there is a reason for this leaky abstraction. "Sometimes I catch myself spending too much time on things that maybe are not super crucial. " I know the feeling. :) 
&gt; Thoughts? Too obscure and could lead to some bizarre patterns. Like you allocate small object and large object pair 1M times and due to adjacent location freeing all large objects does not actually free any memory.
Or in C++ itself! :)
I don't know what's up with all those big heavy CMakeLists.txt. I was able to keep mine pretty straightforward [0]. Or is it that I am not doing it the right way? [0] https://raw.githubusercontent.com/thelostt/cci/master/CMakeLists.txt
My suggestion is something like this // Probably templated for arbitrary number of arguments auto[p1, p2] = new&lt;T1, T2&gt; delete p2; // p1 still valid, p2 destroyed and deallocated If by "does not actually free any memory" you are referring to memory fragmentation then that memory would not be free anyway so it's no worse, but for non-contiguous storage deleting one of the two objects might free usable memory.
As far as text documents, there is [this guide](http://bcm.readthedocs.io/en/latest/src/Building.html) in the boost cmake modules, which discuss how to setup up libraries(based on Pfeiffers guidelines). I have also set up a wiki with notes from two of Pfeiffer's talks on cmake [here](https://github.com/boost-cmake/bcm/wiki/Cmake-best-practices-and-guidelines).
It should be noted that other standard libraries have similar bits to unwrap vector::iterator in their `std::copy`. At least, I know libc++ has `__wrap_iter` or similar; I'm assuming libstdc++'s "normal_iterator" does something similar too.
Wait, so I don't need to add each source file, but can just add subdirectory? 
1. sized deallocation is a C++14, not 17, feature. 2. sized deallocation doesn't let you free *part* of a buffer. It just tells whomever implements operator delete what the size was, which might be helpful when implementing some allocators. The pointer passed to operator delete still needs to be the exact pointer received from operator new. So it actually doesn't help with this scenario at all.
http://goog-perftools.sourceforge.net/doc/tcmalloc.html Assume you need 1000 for large object and 8 for small one. Separate allocation gives you 2 pointers from different buckets. One joined gives you one bucket. You delete 1000 bytes object and allocator still can not reuse that block since those 8 bytes remain there. It could reuse in theory, but it would complicate things for allocator. Ah you need 800 bytes? I will check if ~1000 blocks have a block with unused 800 bytes and give it to you, if not I will give you memory from ~800 bytes block. 
Maybe you should add list of projects that use kangaru to your readme or wiki?
Well... C++03, C++11, C++14, C++17 Of course it isn't. 8 years between 03 and 11, 6 years between 11 and 17, with 14 between them.
I would first need to know what projects uses it ;) I have a couple of projects in which I make use of it, I get clones from time to time but unfortunately I have yet to know other projects using it. If I knew what projects uses it, I would be more than happy to make that list!
Actually It was more like 13 years... since C++03 was very minor release....
Why do you dislike bazel? I never used it but it looks cool from what I read.
Now I know how STL got that stack of gold coins on his desk. :P Also you probably should have pointed out that Google [approves](https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables): 
BTW you should read the SO comments, in particular Jeffery Thomas told you how to do what you want.
Of course those subdirectories have their CMakeLists.txt as well. But they're just as succinct.
I didn't want to reach back to 98 with the 03-11 gap being that big already, but you're right. I'd rather have the releases quickly though, just to get what we can and not have those giant gaps in progress.
Ah, I see. Do you set up your IDE to automatically add .h and .cpp files to these CMakeLists.txt files or do you do it manually? 
I do it manually as I don't use any IDE. I'd suggest you look around the repository if you have any more questions (I've made sure to document everything.)
Will check it out, thanks!
Thanks! I read through the build file and got an idea of how Meson works. How does it know how to automatically download and build SDL if it isn't installed?
Consider two networked clients looking at the same scene of thousands of actors. Normally you'd have to synchronize all of their positions and speeds over the network. If you have a portable PRNG you can just synchronize the seed in order to run the same simulation on both clients. Just one of many useful non-debugging examples. 
C+/C fanboy actually.
Sorry for dredging up an old post but for future time travellers that happen across this I want to take the opportunity to share a lesson I recently learned in Python. "is not" is not != ! So while you said "you can use !=" I would amend that to say "you should probably use !=". The reasoning is succinctly laid out in [this stack overflow answer](https://stackoverflow.com/a/2209781/5432317). Basically, "is not" *checks to see if two objects are the same object*. while "!=" uses class methods to *compare two (possibly different) objects*. Most likely you want the latter. Anyway I doubt people browsing /r/cpp were looking to learn more Python but there you have it.
I see your point but sometimes I feel that deadlines help. 
You should spell out what a DI container is before using the acronym, for people who are unfamiliar with the concept 
Sorry. It stands for dependency injection. I thought the description I added was enough.