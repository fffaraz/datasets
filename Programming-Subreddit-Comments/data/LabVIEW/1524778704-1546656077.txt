Hey there, Universities tend to offer professional licenses for this. My own university has a license for LabVIEW (UoBristol) check with your IT department
So it would be considered illegal to use a student license for any kind of research?
When you say undergraduate research, do you mean a project as part of your course? (Such as a 3rd/4th year project) 
You need the labview 7.1 runtime engine. You can find a download for it on the NI website. Unfortunately, 7.1 doesn't officially support anything after Windows XP, but if you have it running on Windows 7 there's a good chance it will work on Windows 10. I'd download it and try, nothing to lose really
Can I ask which National Lab? The ones I am familiar with have LabVIEW all over the place so a license should be relatively straightforward to obtain. I'm not sure if that will extend to the more esoteric toolkits though.
Having worked as a student and then a staff member at a national lab, my experience is that obtaining licenses is only difficult because you don't know where to start. Once you get the right person to help you, it is trivial. It looks like there is a LabVIEW user group at fermi, so I would try to get in touch with them: http://news.fnal.gov/2015/12/labview-user-group-meeting/ If you're working on a lab computer, you definitely need a lab license. 
As a basic guide: Sensors -&gt; Read by arduino in a loop, processed -&gt; write to serial port -&gt; read serial port on labview -&gt; process and display. I installed the genuino drivers, so I power my arduino via usb which would also act as a virtual serial port so that I could communicate with it with LabVIEW. Good luck!
This answer says it all. Since he isn't familiar with Arduino and/or LabVIEW, let me give him a few tips. *Arduino: You must read the data from your sensors (Analog or Digital Input), then send it through the Serial(USB). *LabVIEW: You have to configure the VISA Serial port used by the Arduino (you can check which on the Arduino IDE); Now you must check if there's data available on the port. If so, read the data and process(or display) as you wish. That's it. 
Lookup LINX on Makerhub. Very simple Arduino integration
I had to do a similar thing and ended up using visa raw. The first thing I did was tried to get it working on a Windows pc that was capturing the usb traffic using wireshark. Once I had that down I moved the code to the cRIO. couldn't run run wireshark but got the usb traffic working.
thank you
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/SFQpnRv.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dy798m5) 
[https://imgur.com/a/IVLZWVX](https://imgur.com/a/IVLZWVX) Monkeymerlot is spot on wit these icons. I posted the help info on URL. Boolean is an indicator and Boolean 2 is same changed to a control. If anyone has any advice on random number generators, case structures, state machines and enums I could really do with some advice. Cheers lads.
Either only add valid numbers to the array, or sort the array after to remove NaN
Or do a quick check such that If array sum =0 then 0 Else array AVG 
I have a VI which needs to increase a value which starts at zero and increases by 0.5 within a loop until it hits a certain numeric input value. This increasing value needs to be used in multiple other calculations within the while loop and once it hits a certain level the loop should stop and give three values. I hope someone can help me as this issue has been a real pain for a long time. I made this sample VI as an example. 
so, what's not working? right now you have IF[(A+15)&gt;B] then stop], and you're exporting the value of A to a local variable. If you're expecting Local Variable A to be updated in sync with other loops, that will not be the case. Within this loop you'd be better off using a shift register to keep track of A.
It is not executing any output values because no matter what I put in for B it never stops the loop. 
How about implementing the loop normally with a shift register and then only assigning the final value to the local variable after the loop finishes? 
you don't need an account for imgur
Could you explain this more or possible upload an example? I tried to play around with shift registers but I need the value of A to continue to be used in two other calculations as it increases until it hits B and then the loop stop and the two calculations are output. 
Just add the control that starts at zero by 0.5 and run the output into a shift register. If it is all in the same loop just run the output wire of the addition to the other terminals that need to use the value. Honestly this is a very very simple problem and this sub will not do other people's homework for them. You have the answer almost there (or acceptable enough) to be correct but it can be made better by using a shift register and adding some timing.
I did this and the stop command still read false when probed. This is the only loop in the subVI so another loop can't be interfering. A was at like 9.0E+9 and B was at 10 and it still read false. 
You said SubVI. Is there another SubVI or is your Main VI also writing to the A local variable? Try recreating exactly what you have above in a brand new VI. You shouldn't see the same behavior.
So I made this example so it was simplified to pinpoint my actual question and I made it in its own VI and it is not stopping. It shows the same behavior as my actual VI within my main VI. 
I am working with shift registers but in the actual VI there is a lot of other stuff going on that I am not sure I can wire all of the outputs through it. Also I am not doing homework, I am building something for my purposes and sorry if the question is below you. I have asked other questions here and people have been very helpful. I made this tiny VI just so I could give a visual representation of the area I wanted to highlight. 
Something isn't right then. To humor me for now try removing your local variable. Alternatively, try adding a wait function with a 100 wired into it in your existing loop.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/lq52s9P.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dylpbi8) 
Can you post a screenshot of the rest of the code. It is hard to figure out what you need.
Wow I wish I would have know that a long time ago! Well [here](https://imgur.com/a/XkuNXlF) is what I did to increment a counter inside a while loop using a shift register.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/u6lRmRI.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
When I remove the local variable it only adds 15 one time and then spit out 15 as A and stops when B is set to 16, so it works but I need it to keep cycling as time goes on until it hits a certain time, that is chosen by the user on the front panel. When I add in a wait function it ignores it and continues cycling A to infinity. 
Something's not adding up (figuratively). [Are you using the run arrow or the run continuously button in LabVIEW](http://physics.oregonstate.edu/~mcintyre/COURSES/ph415/me295/modules/labview/images/labview_0090.jpg)? You should only be using Run.
Sorry, I wanted to be vague because I am not comfortable showing all of my code and I think I've pinpointed the error to this calculation. I will give it a try, I know I tried once but I cant remember if it was just like this. 
The loop that I showed will run in literally a couple micro seconds. IF you are trying to watch something happen, you won't see it. FYI
I suggest you to create a 2D array as input, of course you have others options to do so, it's only a suggestion. I made you a sample code with this suggestion. https://i.imgur.com/ZULoChz.png Each column is a game, each row represents a data from the game, this way you can increase the number of games as you want without having to create a new set of individual numeric values.The For loop is responsible to process every game, and if the OBP for that game is NaN, it stops the loop and won't include this OBP in the results array.
Wow thank you that is amazing. It's my first semester at school using labview and i'm really good at it still.
If you're enjoying it, you could check if your school has access to the Core 1 and 2 courses, you'll learn a lot and maybe become a developer. If you're French, as your VI suggests, there are good opportunists in France to LabVIEW developers. Good luck on your project.
This is an example of using a shift register to perform the same task. You can add code inside the loop that would use the value in the wires coming out of the shift register. https://imgur.com/D5mjvru Unless your code is very complicated, or you're trying to control specific things about the UI, I would recomment against using local variables. They can be very confusing and the way labVIEW code is structured makes understanding them even more dificult.
BTW, next time you post a small snippet of code, try using this tool (or just generally use it): http://www.ni.com/tutorial/7386/en/
I'm from Quebec, but I think there is good opportunities here too! I was not liking it at first, but i'm starting to get the workaround a bit and enjoy it more and more.
If I drag your snippet into LV2016 and press run, [https://imgur.com/z4wLbXg](this is the result.) It seems to work just fine for what you're asking for. If B is set to 16, the loop increments twice, thus making A 30. To be less confusing, [I would rewrite it like this](https://imgur.com/3HcvMeE).
Hi if you click on the link below its a screen shot of the code i made to display the values from the sensors I will be using. I found an example online and I just readjusted it to display the values I need. Im getting close to the point where i can run a test I was just wondering if i needed to change the values of the initial block for ardunio. Because actually using and ardunio mirco instead of an Uno. [https://attachment.outlook.office.net/owa/c.matalda@und.edu/service.svc/s/GetFileAttachment?id=AAMkAGQzZjZlOTU0LWQyNzItNGU3Zi04OGY0LTFhMzgzYmM2YmY5NgBGAAAAAAAtC8b2om0gTbRG1uIXjPXBBwCdvSn88OJATLMskAtaM&amp;#37;2BXCAAAAAAEJAACdvSn88OJATLMskAtaM&amp;#37;2BXCAAJP3OwZAAABEgAQAEXaP2cfBlRPunA&amp;#37;2BE1E58NU&amp;#37;3D&amp;X\-OWA\-CANARY=6Sti\-eYOY02o\_2yca3JcBUDnwkcNtdUYOJQR7gYtJVULMUzQLEi47fw\-jFiGpfXw\_1zFuZB9p70.&amp;token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkJnRDU5blJpQnpmbk5BVGloOFJhZ1l5M3pyZyJ9.eyJ2ZXIiOiJFeGNoYW5nZS5DYWxsYmFjay5WMSIsImFwcGN0eHNlbmRlciI6Ik93YURvd25sb2FkQGVjMzdhMDkxLWI5YTYtNDdlNS05OGQwLTkwM2Q0YTQxOTIwMyIsImFwcGN0eCI6IntcIm1zZXhjaHByb3RcIjpcIm93YVwiLFwicHJpbWFyeXNpZFwiOlwiUy0xLTUtMjEtMTA5NzU5MDY1Mi0zNTEwNzIyNTAxLTcyNTI4NTgyNy0yODgxMzgzNlwiLFwicHVpZFwiOlwiMTE1MzkwNjY2MDkyNjY4ODg2NFwiLFwib2lkXCI6XCI3YWM4MDJhMS0xNmFjLTQ4OTItOTQ5NS00MWNlMGNiZDQ5Y2JcIixcInNjb3BlXCI6XCJPd2FEb3dubG9hZFwifSIsImlzcyI6IjAwMDAwMDAyLTAwMDAtMGZmMS1jZTAwLTAwMDAwMDAwMDAwMEBlYzM3YTA5MS1iOWE2LTQ3ZTUtOThkMC05MDNkNGE0MTkyMDMiLCJhdWQiOiIwMDAwMDAwMi0wMDAwLTBmZjEtY2UwMC0wMDAwMDAwMDAwMDAvYXR0YWNobWVudC5vdXRsb29rLm9mZmljZS5uZXRAZWMzN2EwOTEtYjlhNi00N2U1LTk4ZDAtOTAzZDRhNDE5MjAzIiwiZXhwIjoxNTI1ODAyODgwLCJuYmYiOjE1MjU4MDIyODB9.qep70XT36MmTjsNNmB9JNIKxG9ILQ8SoEPjUGNro7e8vPtVxWEFZe8llpbg0zwViO0rR5gG1wNEeWr4DcZaK64doviFjgMa\_aodyZ1Cnc4HUb5MR\-JMEVz6lWYTLw3uL2BE0HA0C6alhvCtO0sE5FWbiZ1VeUGZ5vjkclIwznUoMJOTLdeoNohUiq7UmIZinR1tj9CnLSyYURbQFVb0eQ2KnJhTFpfHH1JNkocS4\-wkn8WikWQ1iUMQVQacrLTXxI6a8cf6s3Tpyw\-O2\-CekUc505sKq\-i\-z2OBKsdgUHT05uMAPlGm6kuPAU7p7mbsrJACW3KdxT5DnGlzFxiS08A&amp;owa=outlook.office.com&amp;isImagePreview=True](https://attachment.outlook.office.net/owa/c.matalda@und.edu/service.svc/s/GetFileAttachment?id=AAMkAGQzZjZlOTU0LWQyNzItNGU3Zi04OGY0LTFhMzgzYmM2YmY5NgBGAAAAAAAtC8b2om0gTbRG1uIXjPXBBwCdvSn88OJATLMskAtaM%2BXCAAAAAAEJAACdvSn88OJATLMskAtaM%2BXCAAJP3OwZAAABEgAQAEXaP2cfBlRPunA%2BE1E58NU%3D&amp;X-OWA-CANARY=6Sti-eYOY02o_2yca3JcBUDnwkcNtdUYOJQR7gYtJVULMUzQLEi47fw-jFiGpfXw_1zFuZB9p70.&amp;token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkJnRDU5blJpQnpmbk5BVGloOFJhZ1l5M3pyZyJ9.eyJ2ZXIiOiJFeGNoYW5nZS5DYWxsYmFjay5WMSIsImFwcGN0eHNlbmRlciI6Ik93YURvd25sb2FkQGVjMzdhMDkxLWI5YTYtNDdlNS05OGQwLTkwM2Q0YTQxOTIwMyIsImFwcGN0eCI6IntcIm1zZXhjaHByb3RcIjpcIm93YVwiLFwicHJpbWFyeXNpZFwiOlwiUy0xLTUtMjEtMTA5NzU5MDY1Mi0zNTEwNzIyNTAxLTcyNTI4NTgyNy0yODgxMzgzNlwiLFwicHVpZFwiOlwiMTE1MzkwNjY2MDkyNjY4ODg2NFwiLFwib2lkXCI6XCI3YWM4MDJhMS0xNmFjLTQ4OTItOTQ5NS00MWNlMGNiZDQ5Y2JcIixcInNjb3BlXCI6XCJPd2FEb3dubG9hZFwifSIsImlzcyI6IjAwMDAwMDAyLTAwMDAtMGZmMS1jZTAwLTAwMDAwMDAwMDAwMEBlYzM3YTA5MS1iOWE2LTQ3ZTUtOThkMC05MDNkNGE0MTkyMDMiLCJhdWQiOiIwMDAwMDAwMi0wMDAwLTBmZjEtY2UwMC0wMDAwMDAwMDAwMDAvYXR0YWNobWVudC5vdXRsb29rLm9mZmljZS5uZXRAZWMzN2EwOTEtYjlhNi00N2U1LTk4ZDAtOTAzZDRhNDE5MjAzIiwiZXhwIjoxNTI1ODAyODgwLCJuYmYiOjE1MjU4MDIyODB9.qep70XT36MmTjsNNmB9JNIKxG9ILQ8SoEPjUGNro7e8vPtVxWEFZe8llpbg0zwViO0rR5gG1wNEeWr4DcZaK64doviFjgMa_aodyZ1Cnc4HUb5MR-JMEVz6lWYTLw3uL2BE0HA0C6alhvCtO0sE5FWbiZ1VeUGZ5vjkclIwznUoMJOTLdeoNohUiq7UmIZinR1tj9CnLSyYURbQFVb0eQ2KnJhTFpfHH1JNkocS4-wkn8WikWQ1iUMQVQacrLTXxI6a8cf6s3Tpyw-O2-CekUc505sKq-i-z2OBKsdgUHT05uMAPlGm6kuPAU7p7mbsrJACW3KdxT5DnGlzFxiS08A&amp;owa=outlook.office.com&amp;isImagePreview=True)
Hi if you click on the link below its a screen shot of the code i made to display the values from the sensors I will be using. I found an example online and I just readjusted it to display the values I need. Im getting close to the point where i can run a test I was just wondering if i needed to change the values of the initial block for ardunio. Because actually using and ardunio mirco instead of an Uno. [https://attachment.outlook.office.net/owa/c.matalda@und.edu/service.svc/s/GetFileAttachment?id=AAMkAGQzZjZlOTU0LWQyNzItNGU3Zi04OGY0LTFhMzgzYmM2YmY5NgBGAAAAAAAtC8b2om0gTbRG1uIXjPXBBwCdvSn88OJATLMskAtaM\%2BXCAAAAAAEJAACdvSn88OJATLMskAtaM\%2BXCAAJP3OwZAAABEgAQAEXaP2cfBlRPunA\%2BE1E58NU\%3D&amp;X\-OWA\-CANARY=6Sti\-eYOY02o\_2yca3JcBUDnwkcNtdUYOJQR7gYtJVULMUzQLEi47fw\-jFiGpfXw\_1zFuZB9p70.&amp;token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkJnRDU5blJpQnpmbk5BVGloOFJhZ1l5M3pyZyJ9.eyJ2ZXIiOiJFeGNoYW5nZS5DYWxsYmFjay5WMSIsImFwcGN0eHNlbmRlciI6Ik93YURvd25sb2FkQGVjMzdhMDkxLWI5YTYtNDdlNS05OGQwLTkwM2Q0YTQxOTIwMyIsImFwcGN0eCI6IntcIm1zZXhjaHByb3RcIjpcIm93YVwiLFwicHJpbWFyeXNpZFwiOlwiUy0xLTUtMjEtMTA5NzU5MDY1Mi0zNTEwNzIyNTAxLTcyNTI4NTgyNy0yODgxMzgzNlwiLFwicHVpZFwiOlwiMTE1MzkwNjY2MDkyNjY4ODg2NFwiLFwib2lkXCI6XCI3YWM4MDJhMS0xNmFjLTQ4OTItOTQ5NS00MWNlMGNiZDQ5Y2JcIixcInNjb3BlXCI6XCJPd2FEb3dubG9hZFwifSIsImlzcyI6IjAwMDAwMDAyLTAwMDAtMGZmMS1jZTAwLTAwMDAwMDAwMDAwMEBlYzM3YTA5MS1iOWE2LTQ3ZTUtOThkMC05MDNkNGE0MTkyMDMiLCJhdWQiOiIwMDAwMDAwMi0wMDAwLTBmZjEtY2UwMC0wMDAwMDAwMDAwMDAvYXR0YWNobWVudC5vdXRsb29rLm9mZmljZS5uZXRAZWMzN2EwOTEtYjlhNi00N2U1LTk4ZDAtOTAzZDRhNDE5MjAzIiwiZXhwIjoxNTI1ODAyODgwLCJuYmYiOjE1MjU4MDIyODB9.qep70XT36MmTjsNNmB9JNIKxG9ILQ8SoEPjUGNro7e8vPtVxWEFZe8llpbg0zwViO0rR5gG1wNEeWr4DcZaK64doviFjgMa\_aodyZ1Cnc4HUb5MR\-JMEVz6lWYTLw3uL2BE0HA0C6alhvCtO0sE5FWbiZ1VeUGZ5vjkclIwznUoMJOTLdeoNohUiq7UmIZinR1tj9CnLSyYURbQFVb0eQ2KnJhTFpfHH1JNkocS4\-wkn8WikWQ1iUMQVQacrLTXxI6a8cf6s3Tpyw\-O2\-CekUc505sKq\-i\-z2OBKsdgUHT05uMAPlGm6kuPAU7p7mbsrJACW3KdxT5DnGlzFxiS08A&amp;owa=outlook.office.com&amp;isImagePreview=True](https://attachment.outlook.office.net/owa/c.matalda@und.edu/service.svc/s/GetFileAttachment?id=AAMkAGQzZjZlOTU0LWQyNzItNGU3Zi04OGY0LTFhMzgzYmM2YmY5NgBGAAAAAAAtC8b2om0gTbRG1uIXjPXBBwCdvSn88OJATLMskAtaM%2BXCAAAAAAEJAACdvSn88OJATLMskAtaM%2BXCAAJP3OwZAAABEgAQAEXaP2cfBlRPunA%2BE1E58NU%3D&amp;X-OWA-CANARY=6Sti-eYOY02o_2yca3JcBUDnwkcNtdUYOJQR7gYtJVULMUzQLEi47fw-jFiGpfXw_1zFuZB9p70.&amp;token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkJnRDU5blJpQnpmbk5BVGloOFJhZ1l5M3pyZyJ9.eyJ2ZXIiOiJFeGNoYW5nZS5DYWxsYmFjay5WMSIsImFwcGN0eHNlbmRlciI6Ik93YURvd25sb2FkQGVjMzdhMDkxLWI5YTYtNDdlNS05OGQwLTkwM2Q0YTQxOTIwMyIsImFwcGN0eCI6IntcIm1zZXhjaHByb3RcIjpcIm93YVwiLFwicHJpbWFyeXNpZFwiOlwiUy0xLTUtMjEtMTA5NzU5MDY1Mi0zNTEwNzIyNTAxLTcyNTI4NTgyNy0yODgxMzgzNlwiLFwicHVpZFwiOlwiMTE1MzkwNjY2MDkyNjY4ODg2NFwiLFwib2lkXCI6XCI3YWM4MDJhMS0xNmFjLTQ4OTItOTQ5NS00MWNlMGNiZDQ5Y2JcIixcInNjb3BlXCI6XCJPd2FEb3dubG9hZFwifSIsImlzcyI6IjAwMDAwMDAyLTAwMDAtMGZmMS1jZTAwLTAwMDAwMDAwMDAwMEBlYzM3YTA5MS1iOWE2LTQ3ZTUtOThkMC05MDNkNGE0MTkyMDMiLCJhdWQiOiIwMDAwMDAwMi0wMDAwLTBmZjEtY2UwMC0wMDAwMDAwMDAwMDAvYXR0YWNobWVudC5vdXRsb29rLm9mZmljZS5uZXRAZWMzN2EwOTEtYjlhNi00N2U1LTk4ZDAtOTAzZDRhNDE5MjAzIiwiZXhwIjoxNTI1ODAyODgwLCJuYmYiOjE1MjU4MDIyODB9.qep70XT36MmTjsNNmB9JNIKxG9ILQ8SoEPjUGNro7e8vPtVxWEFZe8llpbg0zwViO0rR5gG1wNEeWr4DcZaK64doviFjgMa_aodyZ1Cnc4HUb5MR-JMEVz6lWYTLw3uL2BE0HA0C6alhvCtO0sE5FWbiZ1VeUGZ5vjkclIwznUoMJOTLdeoNohUiq7UmIZinR1tj9CnLSyYURbQFVb0eQ2KnJhTFpfHH1JNkocS4-wkn8WikWQ1iUMQVQacrLTXxI6a8cf6s3Tpyw-O2-CekUc505sKq-i-z2OBKsdgUHT05uMAPlGm6kuPAU7p7mbsrJACW3KdxT5DnGlzFxiS08A&amp;owa=outlook.office.com&amp;isImagePreview=True)
Have you tried running it with highlight execution enabled? Might help you figure out where things are going wrong.
If you provide a screenshot of the for loop, it'll be easier to identify the problem.
Things actually look like they run properly with highlight execution.
Can you provide a screenshot of your VI? Is there a specific reason to use 5 controls and 5 functions?
Sounds like an assignment with specific requirements.
I'd recommend using shift registers to pass info from iteration to iteration of both your for and while loops, such as the arduino connection info and VNA connection info. Maybe even the error wires as well if you have develop something to handle them if they come up. I noticed the VISA resource wire has a use default if unwired terminal, that may be causing some issues. Does the step number indicator increment at all? Have you probed to see if any errors happen with either arduino or VNA?
yes it’s a specific assignment , i don’t have a screenshot right now 
Once you provide us a screenshot, we'll be able to help you, otherwise we would just do your assignment and you would learn almost nothing. ;-)
The assignment brief would also help too, the 5 functions/controls etc. sounds like either a maximum OR that the assesor has a specific idea of the features they want implemented (ability to choose units for example).
I recently bought one on EBay, so I would try there.
If I was up for a move to Seattle, it sounds awesome. 
I'd love to see where this goes as any tutorials found online are very old/outdated. Upvoted!
Thanks! It was a problem when I was looking for LabVIEW help, so I want to do a more complete video series to help out people that are in the position that I was!
Excellent initiative, it's nice to see someone caring about teaching others. I hope you continue doing this, and in the worst case scenario, you'll improve your skills.
Sounds nice, I'm a student in North Texas and I got to a highschool branch off called Career center east and I am experimenting with LabVIEW currently. I will give this a shot as a person in highschool with basic knowledge over it. 
Sounds nice, I'm a student in North Texas and I got to a highschool branch off called Career center east and I am experimenting with LabVIEW currently. I will give this a shot as a person in highschool with basic knowledge over it. 
Sounds nice, I'm a student in North Texas and I got to a highschool branch off called Career center east and I am experimenting with LabVIEW currently. I will give this a shot as a person in highschool with basic knowledge over it. 
Not looking to move to Seattle, but the bringing dogs would be awesome. On the other hand you wouldn't see the same ones from me each time. Very involved with rescue and I just got fosters 22\-30 of this year...
Hey man, I’m cant get to my computer at the moment but I would try going to LabVIEW MakerHub and using the Linx as that’s worked for me before. I think Linx is what the toolkit you’re using turned into. https://www.labviewmakerhub.com/doku.php?id=learn:tutorials:libraries:linx:getting_started Also there are fourms in the MakerHub page as well as LabVIEW forums on NIs website. Hope that helps!
Well, I've never used this approach, I usually communicate with the arduino directly using the VISA resources to send commands through the COM port, but I'll try to help you out. Have you properly uploaded the firmware to the LabVIEW, as described [here](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P8ilSAC)? However, I suggest you to move to [LINX](http://sine.ni.com/nips/cds/view/p/lang/pt/nid/212478), since LINFA is no longer supported. Or, if you have experience, you can create your own interface by programming the arduino to read and process data from the COM port, and on LabVIEW to send the desired data through the COM port.
http://microcontrollerslab.com/program-arduino-labview-example/ https://www.youtube.com/watch?v=-YGyaHW-Ckw
Thank you for your reply. I'm trying to use LINX now.
Thanks for the informations. I'm trying to handle the same thing using LINX now.
Thank you guys. I can light it up using LINX now.
Was going to apply until you said brings dogs to work. What kind of outfit are you running!! 
I would recommend 'Hands-On introduction to LabVIEW' by John Essick. It pretty easy to read and covers alot a the basics, it also has online materials that are free to reference.
Hey, HarwareThrift, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Near the bottom of this page is a whole lot of resources. The MyRIO essentials goose is great with lots of videos and examples. But the best is probably the self paced online training by NI if you have an SSP. https://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495
I'll also add that, if you start to use Labview professionally, a very good resource is Peter Blume's The LabView Style Book. It teaches you how to write readable, well structured labview code. Unfortunately, I think it might be out of print, so you'll only really want it if you can justify the cost of getting a copy.
That's very broad, and very unethical. I don't mind giving advice on things, but I'm not going to do your work for you. Open LabVIEW and check out "Find Examples" - there's *loads* of example programs for virtually everything you could want. So many that it can actually be tricky to find the one you need. What do you mean by work measurement? That's a term I understand to be related to Project Management, and not something I'd ever use LabVIEW for... 
Yeah I felt really bad for asking to be spoonfed the thing. the thing is I don't have time currently or knowledge. WM is a subject in IE
Explaining acronyms with other acronyms isn't helpful, dude. To me, IE means Internet Explorer. I can see from your post history that it's Industrial Engineering though. Stick some terms like those into the Find Examples function though, and see what you get.
http://www.ni.com/examples/
I don't have much experience with the elapsed time express VIs as express VIs are not "best practice" in that you can get the same functionality out of something that is more open, tune-able, and modular. I'm posting a link below to an example for a time FGV that can give you exactly what you want. https://forums.ni.com/t5/Example-Programs/FGV-Timer-SubVI/ta-p/3499484 This being said, I believe it is because you are not resetting the express VI. So when you've reached the elapsed time you desire, you need to feed a true to the reset node on the input side of the express VI. This can most easily be done with a feedback node connecting the output of "Time Elapsed?" and "Reset"
Are you able to share a printscreen of your VI at all? Just happen to be on a PC without LabVIEW currently. You need to find a way to stop and reset this timer when your programme detects it needs to heat again. Sorry if this next point is insulting to your intelligence but are you familiar with State Machines in LabVIEW at all? It is a fantastic technique that is really worth spending a short time reading about. With State Machines you have a case structure in a while loop, and a "case selector" which is an Enumerated Variable, that chooses a case. A list of possible cases for your programme could be "Initialize" "Heatup" "At Temp". Each of these cases can then have code in them, with the possibility of outputting a different case selector value. For example, in your "heatup" case you want it to be able to transition to "At temp" just as you want your "At temp" case to transition back to "heatup" if the temperature drops. You can do this with boolean logic. When your temperature drops, you want to transition back to "heatup" so you could code in at the beginning of "heatup" that it resets the timer, and then when you transition to "At temp" again, the timer will start. Sorry for the blind commentary, just going off what it sounds like, [here is a tutorial for State Machines](http://www.ni.com/tutorial/7595/en/) should you need it, and if you have a printscreen I'll be able to give some better advice :\)
The Elapsed Timer VI is the only one of the Express VIs I use on a regular basis. The rest are bloated garbage and they should be avoided like the plague, IMO. That said, using a feedback node may be the easiest way, but using either a Boolean and a shift register or (ideally) a typedef 'Timing Controls' cluster and a shift register. Either one gives more flexibility and the capability of pausing/resuming timing as well as starting, stopping, and resetting from different states. The first call to the state containing the ET VI should have the Reset? Boolean set to TRUE and pass in the target time, then in the same state he'll need next state selection logic, wherein the Reset? Boolean gets set to FALSE. 
I am normally against these "do my homework" posts. But you sound desperate. [Try this.](https://imgur.com/e3TPcgh)
You're super nice, you even offered suggestions. You're way nicer than me, especially since over half of OP's other posts are asking for homework "help". OP is exactly the type of industrial engineers that makes industrial engineers not look like engineers at all.
Yeah I tried wiring up the reset. Problem is that in this case it would reset as soon as the first cycle ends. Not when the second starts (there is a delay since my device needs to heat up first) I tried making a custom VI similar to the link you send me. However for some reason it only worked when i highlighted execution. When I didnt do that it always showed 0.99 to 1. Thanks for the link, I will take a look. 
Yeah I set the program up kind of like a state machine in the way you describe. I have a heating part that checks if the temperature is to low, then changes the cast structure to heating if not. A second case structure does the timing, it gets activated when the heating reached the planned temperature and is being kept active by a shift register to prevent it from turning off when temperature is not stable. But Labview was so frustrating to work with for me that I deinstalled it just a minute ago in a bit of a ragequit :) I think I will try using python instead, which I have a bit more experience in and which makes a bit more sense to me structuring wise. Thanks a lot though ! :) 
Sounds good to me dude, the way in which we all approach coding is so unique that sometimes things just make more sense to us in different languages; I am a true believer in going with what you know... With LabVIEW it’s not always easy to find the simplest route to a problem that you *know* is straightforward, and it can be a nightmare to self-teach. Best of luck with the work OP :)
Hey /u/Dunkeltrost, your primary problem is that the case containing your elapsed timer VI never runs, at least I didn't see it run. Your logic is overly complex and you need to simplify things. Get to understand how the elapsed timer VI works on its own before you try to integrate it into any other decision logic. 
Haha that's very similar to a work product I was shown when I was hired on at work. It was the "do not do this" example. 
In future can always trying using formula nodes or mathscript nodes, which work more like conventional programming languages. Much simpler to do.
Was just learning formula nodes today! Probably going to use them for my next problem haha as bugs keep being found. 
Looks ready to launch human rated rockets to me 
You can say that again. This whole thread was solved by someone telling me that while loops just act funny with local variables sometimes, like those are things you just learn with experience. 
Oh.. oh no...
Yes, yes it is.
Totally, spaghetti code could only happen in LabVIEW. add_used = 0 # define add def add(a, b): global add_used add_used += 1 return a + b # dont know why this works but it does. def divide(a, b): quotient = 0 c = 0 d = 0 while add(d, b) &lt;= a: c = add(c, 1) d = add(d, b) return c
The good thing about labview is that you can instantly recognize bad code. The bad thing about labview is that your bad code is instantly recognizable. 
Generate an ordered array of numbers (10, 11, 12, ...) and then swap random elements. In other words, pick two number between 0 and 9, and swap the corresponding elements of the array. Do maybe 25 swaps, and you'll have a nicely randomly ordered array. 
i don’t understand this post. is it implying that that awful code isn’t the users fault? it is absolutely the users fault. i don’t particularly love labview or anything, but just like any programming language, if you don’t learn how to properly architect what you’re making and you just start mushing stuff together you’re eventually going to end up with a complete mess. labview is just special because bad code is so obviously immediately visually horrible and obvious. 
Where is the stop in this while loop?
The solution you posted would work if you changed the first 20 constant and changed the value to 10 instead... Right now you're taking a random number between 0\-1 multiplying it by 20 to get a number between 0\-20, and then adding 10 to it, getting a number between 10\-30.
This bothered me enough to go find it... Original \(higher def\) image is here: [http://forums.ni.com/legacyfs/online/35148\_BadLabVIEW.PNG](http://forums.ni.com/legacyfs/online/35148_BadLabVIEW.PNG) The 'stop' is actually a 'run while true' and is above the Terminal labeled "Stop" in the lower left hand corner.
Ha:) found it, couldn't be placed any better!
My main issue with LabVIEW is not the spaghetti code. What really bothers me (from a practical programming perspective) is, that it doesn't force you to name things. You can just pull (unnamed) wires around. Rinse and repeat and eventually you end up with a ratsnets that is very, very hard to understand, even if the overall layout and structure of the data flow is clean and tidy. In text based programming the mere fact that you're more or less forced to name stuff already weeds out a lot of problematic behaviours.
Thats the way i got, took me like 10 mins after the post realize my error, thx guys
I read you posts on NI Forums and I'll try to help you. * 1. From Linx OpenSerial.vi, the "Device Name" is an output, it'll provide you with the name of the device you selected on the specific Serial Port input. * 2. Baud Rate Override is there to, IF you want, try to communicate at a higher rate than the standard 9600. * 3. "Internal 8MHz Osc" is the default clock for the MPU6050, nothing to do with the Linx, LabVIEW or the Arduino, but is related to the module itself. * 4. Again, the Full-Scale range is a characteristic of the MPU6050, it isn't required an input value, if not wired, the default value will be used. you can check the datasheet [here](https://www.invensense.com/wp-content/uploads/2015/02/MPU-6000-Datasheet1.pdf) * 5. The 2 Close.vi you presented there are different. The first one is to close the I2C connection between MPU6050 and the Arduino, and the second is to close the Serial connection between the LabVIEW (Linx) and the Arduino. I hope everything I stated here is correct and will help you with the project.
Vibrations of what? That accelerometer will only work for resolving low frequency vibrations. 
The improve your résumé, I suggest you to become a certified developer. [Here](http://sine.ni.com/nips/cds/view/p/lang/pt/nid/201888) you have the certifications, and since you're aiming at lab assistant job, the CLAD would do just fine. Other than LabVIEW skills, it's imperative that you have knowledge in data acquisition (including theory), both NI and 3rd party hardware when possible, such as DAQ and cRIO. Sensors and Signal conditioning knowledge will help you quite a lot too. FPGA knowledge is a plus, if you have access to a cRIO or myRIO, try to learn the most about the FPGA on it, nothing overly complex, though. 
The wires are the variables, right click the wire, show label, give the label a name. Now you're in control of the variable names if that helps. In text based programming you can easily globally scope a named variable and create a race condition in about 10 lines of code. You can write bad software in any language. Engineering good software takes time and talent, no matter what language you've decided to write it in.
When i took the CLAD early this year it was 89 EUR, but I applied to [students discount](https://lumen.ni.com/nicif/us/academiccladstudentdiscount/content.xhtml) and I actually payed only 26.7 EUR. The CLD is way more expensive, I don't know its current fee but last time I saw, it was US$299. 
Oh. I was thinking of only doing CLAD. That's not too bad of a price. 
&gt; The wires are the variables, right click the wire, show label, give the label a name. Now you're in control of the variable names if that helps. I know. But who does that? The issue is, that labeling wires is completely optional. Properly naming things should be mandatory, though.
Indeed, the CLAD states that you're familiar with the language and can code applications to solve small problems with ease. If the company/lab you work for wishes you to improve, they can always pay for courses and certifications in the future. Good luck!
&gt;1 From Linx [OpenSerial.vi](https://OpenSerial.vi), the "Device Name" is an output, it'll provide you with the name of the device you selected on the specific Serial Port input. So in this case, Device name will be the name of the arduino, is it? &gt;3 \- "Internal 8MHz Osc" is the default clock for the MPU6050, nothing to do with the Linx, LabVIEW or the Arduino, but is related to the module itself. And why the MPU6050 need that clock? I mean, the I2C don't have already a clock to the comunication from the arduino that's the master? &gt;4 \- Again, the Full\-Scale range is a characteristic of the MPU6050, it isn't required an input value, if not wired, the default value will be used. you can check the datasheet here I see the data sheet and where say the "Full\-Scale range" [http://prntscr.com/jk2xz9](http://prntscr.com/jk2xz9) how I can know what of all are the deafult value?, and can you explain what's mean that "Full\-scale range"?, I don't understand so well the importance of set that value. &gt; 6 \- To separate the signals you have to use an [index array function](http://zone.ni.com/reference/en-XX/help/371361L-01/glang/index_array/) and chose the index \(0,1,2 in your case\) to separate each signal. That don't works :s, since it's give me an error of incompability : the type of source is word and the type of sink is a 1D array can you see it here: [http://prntscr.com/jkbfex](http://prntscr.com/jkbfex) . But I fixed it with this implementation, now works :\) I see it in a video, the data from the MPU6050 is a cluster [http://prntscr.com/jkbga2](http://prntscr.com/jkbga2) &gt; 7 \- To remove the DC offset, we usually subtract the mean of the signal from itself. But since you're doing the acquisition on real\-time, I suggest to set a calibration procedure to allow your system to set the zero. In others words, make a few acquisitions on the begging, but only use them to calculate the mean and this will be the value you'll subtract from the others acquisitions. To do this I have to adquiare some data in real\-time for some time is it? , How I can know the sample frecuency of the MPU6050? , I thought in maybe do some while loop for some time, in that way I can take maybe as 1000 data and calculate the mean to then add that value to the signal and graph it with out some offset. Do I have true? if the answer is yes I'm need do this things: 1\) Know the sample frecuencie of the mpu6050 to know how much values take per second **\(I don't how know that D:\)** 2\) use that, to create the while loop for some especif time, and later Convert from dynamic data to a number, in that way I can sum all the resulting values, and divided it with the number of samples \(something like this [http://prntscr.com/jkbl82](http://prntscr.com/jkbl82)\) and indicaded the result value in a indicator **\(I can calculate the mean, like the image, but I don't know how to convert that dynamic data that comes from my MPU6050 to do it \)** 3\) I copy the number from the indicator \(the mean\) and substrac it from the signal that comes from the acceleration Z \(something like [http://prntscr.com/jkbnj7](http://prntscr.com/jkbnj7) \) I thought too maybe in do some program that when take 1000 data from the mpu stop and calculate the mean. But I don't have idea how to do that D: I'll appreciete if you can help me a little likewise I'll looking for my part a solution . i'll try do the other things that you suggest me later that I solved how to fix the offset of my graph, in fact all my gaphs x, y and z, have some offset that I need to fix :s. 
I try to do this proyect, [https://www.youtube.com/watch?v=QSFZSH\-gI9M&amp;t=5s](https://www.youtube.com/watch?v=QSFZSH-gI9M&amp;t=5s) but with a better lectures from the acceleromer, with every acceleration in a separate graph , without offset and showing the FFT graph 
Oh. That's pretty low frequency. That should be okay for a learning project. 
I agree with you. LabVIEW is not the end all, be all and shouldn't be treated as such. Just like Node.JS was only created so that Javascript programmers can call themselves Full-Stack developers and those guys that write Java applets to do spreadsheet control are just fooling themselves. But I'd bet you $100 that there is no language out there that would allow me to write a an interface to control test and measurement instrumentation in as short an amount of time as what I can do in LabVIEW given an appropriate framework. Hell, with the right framework, I can write all my instrumentation code in LV, store all my data in a MongoDB, do all my analysis in MatLAB, and the UI could be a web-app running JS in Chrome. I happen to disagree with you that LabVIEW makes it easier, or that written language 'x' makes it more difficult to write bad code. As a contractor I've walked into many situations and picked up lots of bad code that need help, refactoring, or just plain rewritten of all sorts of languages. There's no wunder-language out there that just magically works and anyone that says there is is probably guilty of writing some pretty horrible stuff in whatever language they think that is.
Very good! Now is LabView ever used outside of lab work or is it never? Should I really seek LabView certification to increase my worth?
Yes, quite a lot. Tesla and SpaceX use LabVIEW extensively. I was at NIDays Paris last year and one of keynote speakers was from SpaceX and showed several number os applications they developed using LabVIEW. The BioTech industry is another field where LabVIEW has some importance, since it's simple to deploy solutions, a significant number of new companies decide to use it and speed things up. If you're aiming at a specific position that requires experience with LabVIEW, yes, otherwise it isn't necessary, although a CLAD certification is cheap and can increase your odds.
&gt;Tesla and SpaceX use LabVIEW We're never getting to Mars
To add on, I'm in Houston and my company has enough LabVIEW work to keep 4 Certified Architects on staff full-time. It's used every where PLCs are.
PLCs are still used? I've only encountered PLCs in technical curriculum but never in my EE curriculum. Would getting a certification in LabView be enough to be hired or do I need other skills?
If you have the commando list from the manufacturer, yes, you can control you power amplifier through LabVIEW. LabVIEW uses the [NI-VISA](http://www.ni.com/tutorial/3702/en/) to communicate with instruments, this includes RS-232 communication.
Not a fan of the poster format for a programming guide (imo) but the RS-232 codes are here: [LINK](http://www.marketing.labgruppen.com/webservices/dh.ashx?t=qv&amp;v=11834) Wouldn't be too hard to implement a basic driver, to be honest if it's just for personal use I wouldn't even bother writing full-functionality. Maybe a basic read and basic write subvi that takes a string input. Good Luck!
I learned to program PLCs at my community college and I learned basics of Labview at my four year. What sort of companies should I apply for a job at? How can I prove to them that I am a good worker? Any tips?
Your code is getting stuck in a loop somewhere and it is not allowing it to continue measuring. You probably want to take turns between acquiring data from one instrument and then the other, sort of like Powermeter 1 GeneratorValue 1Powermeter 2 GeneratorValue 2Powermeter 3 GeneratorValue 3 etc....
If I put the power meter in the while I described before, it work. Also the stop button, which stop the process of the powermeter(PM), don't work (it work fine when PM is alone). That's why I don't think I'm stuck in a loop. Plus other processes still run beside.
hopefully I'll notice your updates, I might use this for training new employees
Use a while loop for each instrument separately as it sounds like the loop isn't executing correctly with everything in one loop. Output the results of each of the loops onto a variable on the front panel, then have another loop pulling all the front panel values together.
Depending on your data stream requirements, you could use MQTT protocal. If I'm not mistaken, [there's a library for MQTT for LabVIEW and Amazon Web Services](http://sine.ni.com/nips/cds/view/p/lang/pt/nid/215508) which allows you to send and retrieve data from the MQTT server. Probably, you'll have others add-ons to others MQTT servers in a more generic way. To display the data in a web page, you can use a websocket, as [u/SpacePiwate](https://www.reddit.com/user/SpacePiwate) proposed.
I am indeed, and I have tried to to follow this persons VI, however when i try to load the VI it gives an error messages because I am missing some of the sub\-VI's
What I can suggest you is to use the library from Adafruit and communicate with LabVIEW through serial (VISA). This is a start-point, but let's say you want the motor to turn forward when a button on LabVIEW is pressed, these are the steps you'll need: 1- The LabVIEW will send a command to the Arduino via serial communication (NI-VISA), lets say it'll send the word 'start'. You can look at the 'Continuos Serial Write and Read' example in the NI Example Finder. 2- On your arduino, create an if statement to perform the action when the command is received through the serial port. Something like: &gt; if (Serial.available() &gt; 0) { // read the incoming byte: incoming = Serial.readString()(); } if(incomingByte == 'start') MOVE THE MOTOR FORWARD 
Thanks, I’ll try this out!
I'd say there are more than three people, maybe 10 or 20, and of those one or two are probably here, but you make a good point. If Chris hasn't already suggested it yet, post over on [LAVA](https://lavag.org/forum/31-job-listings/), or the [Dark Side](https://forums.ni.com/t5/LabVIEW-Job-Openings/bd-p/JobPost) to get more attention.
One man team to do this? Sounds like a contractor to me.
You can use the HTTP Client VIs on RT targets. You may want to check these things first https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019OuKSAU
Make sure windows display scaling is set to the smallest amount, 100%
That has something to do with Windows text-scaling. Display Settings&gt;Change the size of text, apps and other items in Windows 10. You can also set some of that per-program or disable it from the program's properties. If you're able to upgrade, you may want to try using a newer version of LabVIEW.
This is a general requisition intended to be a rather wide net so that we can catch people with LabVIEW knowledge with a little bit of knowledge intersecting one of the many disciplines with which we interact daily.
I fit most of these requirements including missile test and launch conductor experience but CA is a bust for me. Is this position strictly in Long Beach?
This req is for Long Beach. If you’re interested in everything but the location, keep in contact as there will likely be non-CA opportunities in the near future and more operations experience would be big ups for you. 
Hi, will you continue to course ?
Yes, that is the goal, I'm currently traveling but will be trying to get anoyher lesson or two out this week
I'm using git with a tortoisegit plugin for LabVIEW (although the plugin is not completely necessary). Comparing and merging VIs is quite tedious but can be done. I'm a single developer by the way.
Yes, I hear you with respect to Python. Any text based language is going to be easier to merge into a version control system. In terms of my students' employability, the NI toolchain seems to have an edge. Both Word/Excel and labview really seem backwards and out of date when you try to control the version/revision/diffs in code. 
You also might want to check out this post, a lot of FRC teams use labView to control their robots. They have a simple guide to git for their projects. https://www.chiefdelphi.com/forums/showthread.php?t=70958
If you’d like, put it in Dropbox or GDrive and shoot me a link in a private message. Ultimately, you’ll still need to submit an application through the web portal.
Yes.
Do it. It's a pretty simple test. I just got mine like 5 weeks ago after using LabVIEW for 4 months. 
Is it free? If so yes. Otherwise it depends on whether you would want to take the skillset further and in a professional setting (which from the sounds of it is a no).
If I was a recruiter, it would show good initiative to get certified in something like that. 
At a local engineering firm, CLAD certification bumps you up a salary step. 
Good series! I hope to see in the future one video about VISA components, the ones I have found are not very clear
Something is wrong the audio in the second video. Like you have a double audio lines or some other issue. I would fix and re\-upload it.
Thanks for the heads up, ill make sure to check it out!
One common issue with serial communications is hidden characters. Right click your string control or constant and choose to show / codes. See what's really being sent: https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000PAr6SAG
This is the code of the send constant [https://ibb.co/maoJTT](https://ibb.co/maoJTT). It appears the send is good just not the read. [https://ibb.co/jMtfa8](https://ibb.co/jMtfa8)
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019L3mSAE More likely your command was never accepted. Trying deleting the U00\. Almost all serial devices will respond to *IDN?, may want to try that instead for starters.
Even though the exact same command is accepted through a terminal? I will try the idn
the LabVIEW code is waiting for 100 bytes to be at the port, is that desired?
No, I was attempting to just make it large enough to encompass any string at that port \(i guess thats not how it works\). A value of "1" yields the same results. [https://ibb.co/gTsvQ8](https://ibb.co/gTsvQ8)
*IDN? is the normal command. Just trying to keep it simple. There used to be more in depth serial troubleshooting but now I can't find it.
have you tried all of the suggested actions in the link imsoupercereal provided? such as waiting a given number of ms between write and read, or setting the timeout programmatically?
"\*IDN?" in labview yields the same results. [https://ibb.co/cc1aso](https://ibb.co/cc1aso) "\*IDN?" in terminal yields syntax error [https://ibb.co/d2fDdT](https://ibb.co/d2fDdT)
That's getting me something other than an error now. I'll report back after lunch. Thanks for all your help guys.
No problem! Let us know how it goes
Ok, Holy smokes. Got it. Recieving Character termination doesn't seem to matter. My problem was sending the "/n". I was thiking that the term character on the visa serial took care of that. Now my only issue is trying to get the reading byte count to scale 27 and 28 depending on if the analog output is \+/\-10 [https://ibb.co/e1LkQ8](https://ibb.co/e1LkQ8)
you could try to read one byte at a time in a while loop, and stop said while loop when you see the \n. You still may want to put a wait between the write and read, or even before the bytes at port. It may be happening too quickly to register which is why you are seeing 0.
I'm not totally sure you need two. You could just have one and move it left and right a little to see which way had more light and then move in that direction.
The tricky part is how you handle clouds. Easy enough to track the sun on a clear day but how does do you code for cloudy, partly cloudy, etc. 
Looooooooong averaging times.
Light level on the sensor might work.
Why though? The solar array output will be maximized when the path of the sunlight is normal to the surface of the array. This is true regardless of cloud cover. The angle of the sun relative to any point on the earth is well defined as a function of latitude and local time. As long as you know where you are and what time it is, you should know the optimum angle as well. 
Using what you've suggested, I have achieved a working VI. Thanks again. [https://ibb.co/ndoZ58](https://ibb.co/ndoZ58)
awesome. glad to help
You don't need a computer for that. Tie the photodiodes together and use the middle point to attach to the motor (via an amplifier). If the diodes get the same amount of light the middle point will be at 0 volts since the diodes are cancelling each other out. If either diode gets more sun, it will over power the other one and you'll get a voltage. 
How to check if the asynchronous VI is closed: I used a queue. Establish the queue outside of your asynchronous VI, and watch the queue somewhere else in your Main VI. Additionally, if you have multiple asynchronous VIs that you are waiting on to close, they can each send a value to your queue so you know which asynchronous VI is being held up. Abandon not stopping the child VI: I'm not sure sure of the real reason here. If I had to guess? "Abandon" only works on sub-VIs that are being directly called by the Main VI. Bonus Style Points: Instead of using a global variable for stopping the asynchronous VI, you can also use a notifier. 
Dirty solution: 1.) Split out the first column 2.) Search first column in a while loop for all values of 1 3.) When a value of one is found, save that value and the corresponding second column value in a new array. 4.) Loop until no more values of 1 are found Be careful if the arrays get overly large. LabVIEW is bad at memory management. 
Sure, one way to do it is to index the column of 0's and 1's to find what you want, once you have the index, you can add the desired pair to a new array. https://i.imgur.com/dplUhhV.png This is a non-optimized way, was the first thing that came to my mind.
This is a pretty clean option: [https://imgur.com/xginhUv](https://imgur.com/xginhUv)
https://imgur.com/a/uAVWIpE
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/HQmZIPf.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
I suppose I can apply your solution if I call sub\-VI's in a sequence, and not in parallel (what I actually do).
An other elegant way is to keep the reference to your asynchronously running vi and check its status 
If you're already using a QMH, could you use named queues? Pass the name of the queue the parallel process, have it obtain queue and do what you're already doing?
Sorry, QMH is a misnomer, it's just using an array of strings for states, like the JKI state machine. It's not queues. But yeah, that could've been an option if it had been written that way.
Any number of ways, but long story short, as soon as you have any asynchronouss VIs you need to be smarter about your stop methods, and not rely on polling a single variable for a stop condition. Here's a few off the top of my head: Send an event to the VI, that you've registered for in the calling vi, then call that event on exit. Send a notifier(queue) to the VI, and poll the notifier from the calling VI. Store the vi reference and poll the property for the VI state to see when it goes idle. (I don't like this one personalky, because then you don't know if it ended successfully, or if it just ended. Unless you check another item) 
I avoid named queues in most situations. Why not just pass the queue reference? Plus, no need for an event structure with queues. If the queue needs to persist in the async process after the spawning VI stops, you can send a self addressed queue. Basically, create a queue if type queue reference and pass to new process. Then have new process create it's own queue, and pass the reference back to the caller. 
Other options for sharing a single string between two VIs would be some sort of register, or other write-safe data store, such as a SEQ (single element queue) or a notifier.
I don't have an install of LV handy, but I would isolate the first column using index array. Then use Equal To? and wire in a scalar 1 (so you have a 1D array comparing to a number) to get an array of booleans where 1 is True. Lastly, you index the second column from the 2D array (the numbers you want) and use a For loop with a conditional auto indexing terminal to filter the values you want. You can also do this second part with the OpenG Conditional Index VI.
Great suggestions! I would also add that you don't necessarily need to poll anything, as many of these methods (notifier, queue, event) have waiting mechanisms that can save CPU load.
This is a fun question... My gut says that the user event is "better" but I'm really sure that I know why... Best logical answer I can give would be: a) while I the underlying method IS the same, triggering a string VC event seems like a (minor) misuse of the event... b) a perhaps slightly better argument might be that when you call the VC event, you're invoking the UI thread, and (I believe) forcing the FP element to refresh... This would not be the case with the user event. Though now that I'm thinking about it....the string event would definitely have less initialization/shutdown overhead, and I've use a similar technique to stop asynchronous VIs pretty frequently (through a "stop" boolean VC event)... so I guess I that I might not have a strong opinion on this one... ¯\\\_(ツ)\_/¯
This is what I thought. The user event sounds more LabVIEW-like, but, that being said, I couldn't really come up with a good reason not to use the string reference. There is a discussion of future proofing. If you end up having multiple processes that are using this single string reference to send back communications, there is a chance that one or more commands could go unhandled. User Events are guaranteed to be handled. It was that reason that solved the case in our code review, but, if there was no thought towards possibly having multiple 'callers' on that command, seems like both solutions work the same.
Avoid using a hidden string control. It'll work, but it's a wonky way to do things that'll confuse anyone else who has to maintain your code. It also won't work with embedded/real time deployment. Why not a notifier?
Is it possible to go full matlab and do it without a loop? 
Definitely the user event because this is exactly what they are for. * The string control probably has some overhead because of the UI. * The user event is more obvious to read and so the code will be easier to maintain over time. Never underestimate making your code easier to read. It pays off many times over in debugging and maintenance time saved. 
These two first solutions looks good. I'll keep them in mind. I changed the option from 0x80 to 0x100 when I open the ref of sub\-VIs, so it abandonn the when I abandon the main VI who called them. Does it seems a good solution to you too ?
You can use RPi or Arduino for data acquisition without any problem. Just realize that these devices are limited in input range and accuracy compared to a dedicated DAQ board. As an alternative, check out the simple USB DAQ boards from NI like the USB-6008. More expensive, but still less than $200. Easier to work with using the built-in DAQ vi's. Not as "smart", in that the Pi or Arduino can preprocess and store data in a way that the USB-6008 can't, but then you'll need to write separate code for the microprocessor, which can be a whole learning process if you aren't familiar with them. 
Are you sure you want to go down this road? RPi doesn't have any analog to digital pins. You could drive an ADC from a RPi via i2c interface? [https://learn.adafruit.com/raspberry\-pi\-analog\-to\-digital\-converters/overview](https://learn.adafruit.com/raspberry-pi-analog-to-digital-converters/overview) Arduino data connection to Labview (via Linx, [https://www.labviewmakerhub.com/doku.php?id=libraries:linx:start](https://www.labviewmakerhub.com/doku.php?id=libraries:linx:start)) is really really slow. A few years ago I think analog readings via Labview plus Arduino couldn't go faster than \~20Hz If you have the budget, spending \~$200 for dedicated NI daq hardware might make your life more fun. [http://www.ni.com/en\-us/shop/engineering\-education/portable\-student\-devices/analog\-discovery\-2/what\-is\-analog\-discovery\-2.html](http://www.ni.com/en-us/shop/engineering-education/portable-student-devices/analog-discovery-2/what-is-analog-discovery-2.html)
Certainly, you could do it over network with TCP or UDP, or over Serial connection. 
There are simple equations that will tell you exactly where the sun is in relation to your coordinates. No photoresistor required. (azimuth angle, and altitude)
I think that's the first job posting I've seen in a while that doesn't have a minimum experience.
LabJack has really affordable DAQs as well [https://labjack.com/](https://labjack.com/)
I haven’t seen your program yet but I’ll advise you against writing to xls; write to csv instead. Also, don’t iterate over the array to apply calibration coefficients but apply them using array operations—the add and multiply primaries are polymorphic and can operate on arrays or scalars without modification.
I actually write to .lvm
I'm going to need a bigger monitor.
But what about zoom?
Take a look into using tdms files. A binary format. Also look into SQLite for setup and configuration data, there are lots of good 3rd party toolkits for this. Finally consider using reentrant VI s to dynamically call your N channels from the same piece of "template" code
Nah, not for me...interesting thought though.... I am often frazzled, when I view some of my more dated code...looks...hm ;)
haha ya..... lots of scrolling.
Learning sqlite now based on your recommendation. thanks.
This solution has two problems: 1. It doesn't actually produce the right answer. In OP's example it would output a 1D array with the values \[5, 8, 80\]. 2. It allocates 3 extra arrays. 1 for the first column, 1 for the second column, and 1for the boolean array that stores the values. If you're input values are small this doesn't matter. If your array was large this would definitely affect performance. There's a super obvious answer to this that simpler code and will have better performance. [See /u/Traffic\_cone600's solution.](https://www.reddit.com/r/LabVIEW/comments/8p06qn/filter_2darray/e07odoa)
A couple of comments: * Splitting and searching are slow. Avoid them if possible. * Appending to arrays can be slow, avoid when possible ("save that value...") * If you're using arrays you know the amount of times you'll have to iterate, use for loops instead of while loops. For loops can be better optimized by the labview compiler. * LabVIEW is fine at memory management. You don't know the rules. It's hard to win the game when you don't know the rules.
This solution has a couple of problems: 1. You allocate an extra array when you split off the first row. This will affect performance on large inputs 2. Using the insert into array with the shift register will have to perform incremental allocations. Allocating memory is a slow operation, so ideally you allocate memory once up front, then populate it with the correct values. 3. Wiring the loop counter into the array index is kind of unnecessary. You should have just turned auto\-indexing on the 2D array tunnel. This would let you step through each row in the 2D array one by one. Im pretty sure the LabVIEW compiler figures that out (although not 100&amp;#37; sure because of the case structure) for you and compiles the same code for each solution. The indexing tunnel is cleaner to read IMO. [See /u/Traffic\_cone600's solution.](https://www.reddit.com/r/LabVIEW/comments/8p06qn/filter_2darray/e07odoa) 
Yeah, thanks for the feedback, when I wrote this "code" no one had provided a solution yet and, as I said, it's non-optimized. I agree, the /u/Traffic_cone600 solution is the best, but stating it as &gt; a super obvious answer is kinda rude to people took time of our lives who tried to help OP, I see this typical behavior in Stack Exchange, but first time here in reddit. 
Holy shit man this is awful. It's a "bless his heart he's trying" situation though. It's like you didnt know stuff existed so you figured out another way to do it. So Arrays are a thing. It seems like you didnt really embrace that fact. There's a saying "there are 3 numbers in computer science: 0,1 and many". What that means is you shouldn't be doing things like copying and pasting 50 times because you have 50 channels. You should create an array, and that array has 50 elements. Then if you add or remove channels you just add or remove elements. For example, your Wiffle Tree Constants L0\-L31. Instead of creating 32 controls, create an array control with 32 elements in it. You can apply the same ideas to the L100\-104 and Tare measurements. This will also help you with your "Set Data Attirbutes", "[CaliV3.vi](https://CaliV3.vi)", etc. Instead of having to drop a bunch of nodes, you'll just use your arrays with a for loop. Event structures will also help you a lot. Basically you have a thing that sits there idle until the user clicks a button. This eliminates a bunch of your while loops that wait for a button to be clicked. Just a heads up, you typically only have one event loop per VI. So you event structure will handle events for your acquire button click, Tare, and any other button you have.
1. Fair enough, I made the assumption the first column wasn't required since all the values are the same. 2. I'll give you that this probably uses more memory (the initial index likely operates in place, while the boolean array is a new allocation), but not that it's slower. A quick benchmark on my machine shows that it's \~6 times faster than his solution :) [https://imgur.com/a/KDCe3xZ](https://imgur.com/a/KDCe3xZ)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/7I9ugE9.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e0ccc8s) 
Thanks. With all the feedback I'm thinking the best avenue of attack is an event based consumer/producer that utilizes for loops with re\-entrant vi's. SQLite to store calibration constants.
Apple to oranges man. In your benchmark "my" solution is outputting a 2D array, yours is outputting a 1D array. Not fair to compare times. See below. When the benchmark is equal the algorithms are trivially different, but on average mine is faster in both the 2D and 1D versions. [https://imgur.com/a/HhkErIT](https://imgur.com/a/HhkErIT)
I dont think you need re\-entrant VIs. You only need reentrancy if you need things to run at the same time. which you don't need in this application. I'm pretty sure all your doing in your application is reading DAQ values, applying some scales/offset, and outputting the measurements to the front panel and a file. All of that can be done serially no problem. Your debugging will be much easier if you do it that way. Additionally, just because you drop 50 nodes, doesn't mean your executing 50 things at the same time. Your computer can only execute a finite amount of things at the same time, so it will get serialized by the processor. Serial things are much easier to maintain and debug. Don't go out of your way to make things parallel unless you actually need it.
Also... Your DAQ loop will take a little bit of thinking to clean up while maintaining the same business logic. What you're doing now is called loop unrolling. It's probably the easiest way to see what's going on, but it's some unruly code to maintain. Basically each of those "Mean" vis has internal state. If you use a for loop, you wont be able to use that VI like normal. If you did, it'd just average the value of all channels ie. `(chan1\+chan2\+chan3\+...\+chanN) / N` If you wanted to keep things the same, you'd need to keep the rolling buffer on your own. I'd accomplish that like this: https://i.imgur.com/fog5p95.png You currently have a software timed single point acquisition though. Typically this isn't what you want. It looks like you want to acquire at 1kHz, but in general you want to look at data at 25hz (1000/40). You'd make your life a lot easier (and your CPU a lot cooler) if you setup the DAQ to acquire at 1000Hz, but return 40 points at a time. This means your DAQ would output a 2D array instead of a 1D array, and you'd handle things in batches. Each batch would be averaged to create a data point. https://i.imgur.com/GF2sEEi.jpg This would ensure that you're acquiring at using hardware timing (which is more consistent that software timing) and you'd be able to remove the wait from your DAQ loop. The DAQ acquire would block the loop execution until the requested number of samples have been read. Good Luck!
What kind of load cells? Or more specifically, what kind of signal conditioning? 
I'm looking for around 200KG compression/tension load cells. Would be great to have them running over I2C or some serial connection to the Arduino, then use LINX to pick the signal up in LabVIEW
Why bother with LINX? Programming the Arduino using their IDE is very straightforward. If all you want to do is read your load cells and send the data off to a laptop, the Arduino programming would be relatively simple. 
I have a prebuilt system in TestStand that would produce the necessary reporting in the same fashion as the static system I build. However I do see your point and as a first instance build I may very well do that. But the ultimate end goal is to go from load cell --&gt; Arduino --&gt; LabVIEW --&gt; TestStand --&gt; Testing Sequence Report
Converting from a load cell resistivity to I2C would be more of a pain in the ass than it’s worth, unless I don’t know about some magic bridge completion to I2C converter. I would highly recommend buying signal conditioners from the load cell manufacturer which output a scaled 0-5V signal. You can then pipe this directly into the arduino’s AI. You will seriously bottleneck your load cell accuracy if you DIY the signal conditioning (without having prior experience). 
I would still use LabVIEW to connect to the Arduino and collect and process the data through the serial port. I'm only saying that programming the Arduino itself you can do in native Arduino without using LINX. It's pretty easy to learn, as line-code languages go.
A couple of things, the first of which that unless something's changed with LINX, you'll need to get a third-party compiler such as the one from [Aledyne-TSXperts](http://sine.ni.com/nips/cds/view/p/lang/en/nid/213121) if you want to have your setup be truly portable and not require a running LabVIEW application. Secondly, and probably more important, is that you don't need to bother with I2C. Every load cell I've ever used has been paired with a signal conditioner that gave either a 0 - 5V or 0 - 10V output, either of which (IIRC) can be wired directly into an Arduino's analog input pins. Obviously you'll need an external excitation P/S for the signal conditioner, and I'd very strongly recommend going with a linear power supply from Acopian or the like. Switching supplies are way too noisy for load cell applications. So if you want to reach your stated end goal you'll need to either get the third-party compiler or plan on having the LV application connected and running when you're testing. Either way you'll need a signal conditioner and excitation supply.
Depends on what you are going to do with the EE degree. I have a BSEE, a BSOE, and am a Certified LabVIEW Architect.
If you really want to do something with LabVIEW then you really need some projects you can show off, or you need to get your CLAD, CLD, or CLA (depending on where you want to stop). Ideally, you’d have at least a CLD or CLA of those certifications and some good projects to demo. Realistically though, it costs so much to get certified that I would focus on getting a good amount of projects demoing a proficiency with all of the major programming architectures (state machine, consumer/producer, etc.) and good programming style.
Very good. I will consider your advice once I have finished the current tutorials I am watching. 
I would start by creating a new LabVIEW project and choosing the State Machine premade option, it makes the state machine for you and you can just edit your cases to Idle, Trick and Stop. As for exporting data, you can write data to TDMS easily enough. The Write to Measurement File express VI is a decent place to start.
I will take a look at the state machine but The Write to Measurement File express VI cannot export the data as a .txt file which is what i want.
What manner of data is it? What will it be used for in the future? Almost any measurement data will be better off in a .csv or even a .tdms if you're staying in the NI ecosystem. Check out Write to Spreadsheet/Write to Delimited Spreadsheet for csv files. If you're set on using a text file, check out the Write Text File VI.
.csv would also work. It's a two column data file with time and data. The programs that are likely going to be used for data processing are excel, matlab and origin.
Ah lovely - feed your 2D array into the Write to Spreadsheet Vi, and it'll put it into a CSV for you. 
Can it write it while recording? The only way i've exported data before was by using a for-loop that created an array that was stored internally in labview and then when it was done it exported it into an existing file. 
It can - but it'll go easier if you can batch it out. Record it every thousand samples, that kind of thing. What's your sample rate? How long will the recording be going for? There are some clever things to Smith y this kind of process out, but I'd say they're not necessary until you hit a pretty high speed, or are working on a pretty limited computer.
The instrument has a limitation which is about 30 minutes. It will record one data point per second. I would like to run it in a while loop rather than a for-loop because once it starts a for-loop it does not know anything about the program outside the for-loop if that makes sense.
1800 data points shouldn't be a huge problem, you can write that all at the end, as long as you're ok with coming away without any data if the run is interrupted for whatever reason. Is it code you're able to share? I'd be interested to have a look when I get home, see what I can suggest.
I have included a picture of the old export scheme. I can share the code i suppose, but i doubt you have an SR400 stashed away at home :)
If only. If you're only executing that loop once a second, I'd write each new value to a CSV with the Write to Spreadsheet Vi. It's not BEST practice, but it'll work, it's easy, and it'll leave the data in a format your other programs won't struggle with.
You're enabling a termchar '\n', but not sending it in the message. Make sure you show \ codes in the string constant and see that there's a '\n'. This is almost certainly why your device isnt even blinking the LED to show it received the message; LabVIEW is still buffering the message to send until it sees a '\n'. I'm not going to read the command grammar, but are you sure you even need the termchar?
Thanks. That was the problem. The working vi is in the comments.
The easiest way I can think of for a beginner to do this is to use network streams: http://www.ni.com/white-paper/12267/en/ If you're familiar with SW dev in general and just new to LV, rabbitmq is also pretty easy to use for this.
I'm similar...I use by-reference classes nearly exclusively these days, and typically need data to be somewhat generic so I store it in variant attributes instead of as unique class members with prop nodes. Disclosure though...I was a C/C++ guy long before I started working with LV, and I'm only a part-time LV guy.
I use UDP for this, assuming both computers are on the same subnetwork. Sending a message from the "master" computer to xxx.xxx.xxx.255 broadcasts to all devices on the network, using a selected port. The "slave" computer listens for these broadcast messages and when it receives it sends a message back to the master giving its IP address. The master can from the on send messages directly to the slave's IP. These messages can be text (formatted numbers and commands), or binary data flattened to strings. The slave can send back data or acknowledgements. There are some details, of course, like how to open the UDP port and poll for messages. But it is actually pretty simple in principle and very robust for short messages sent back and forth on the local network. 
I think you can use shared network variables.
Most people are suggesting ethernet which is a good option but if the computers were reasonably close, you could also use RS232 which would eliminate some headaches with IP addresses.
They are relatively close together, so I'll look into it. Thanks!
Ok, thanks for the lead. I'll check it out!
I haven't done much in the way of IPs, but heck it would be a good way to learn, so I'll try and give it a shot, thanks!
Yeah, it's pretty simple. On one side you do visa open, put the read in a loop, (filter out)[https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P85KSAS] timeout errors. On the other, you (append)[https://zone.ni.com/reference/en-XX/help/371361H-01/glang/concatenate_strings/] a (carriage return line feed)[http://zone.ni.com/reference/en-XX/help/371361P-01/glang/end_of_line_constant/] to your string, wire that string into VISA write then a close, the text that goes into the write comes out from the read side.
PM me if you need help. 
But then you need cables and adapters (if the PCs don't have built in COM ports. UDP is just a simple to code, no extra cables, and no VISA. 
You can also use the Current Value Table API with the Client Communication add-on - https://forums.ni.com/t5/Reference-Design-Content/LabVIEW-Current-Value-Table-CVT-Library/ta-p/3514251 This is the part of the awesome DCAF - https://forums.ni.com/t5/Distributed-Control-Automation/gp-p/5235
Lots of good answers here, just to add: check out the "Shared Variable.lvproj" example in the example finder (go to the help tab and click "find examples"). This example shows how to communicate data from one computer to another, using a few different methods. Just input the IP address and run the "server" on one computer and any of the client VI's on another computer. 
Just checked it out, and it was really helpful thank you!
Checking it out right now, thanks
have you looked into modbus communication?
Briefly, do I need to say anything up on the PLC end for that? I don't have much access on that end, unfortunately.
Yes, it's possible. Inside NI MAX you can [create an NI-DAQmx Simulated Device](http://www.ni.com/tutorial/3698/en/). And, in the worst case scenario that you can't create the simulated DAQmx, I suggest you to create random signals and use them to simulate your data acquisition. 
Yes, depending on what you want to do you can build your whole diagram, and just place a diconnected daq block. I would strongly reccomend do this before lab as it will free up more time for you to ask questions and learn more about the hardware!
Is this compatible with Labview Robotics 2016 ver16.0? That's the version we have available. Will it be simple to turn the simulated input DAQ into a real DAQ? The ideal solution is one that saves me a few minutes of lab time by having all my settings plugged in a ready to go.
Yeah, it should be ok, you must have NI-DAQmx 7.4 or higher and LabVIEW 7 or higher. It is easy to change from the simulated device to the real one, assuming that you're using NI MAX to create the tasks, you only have to change the task in your block diagram and you're ready to go.
Awesome! thanks!
Thanks! That's what I have until I figure out if I like using the DAQmx the other guy mentioned. 
seems like the PLC is modbus capable http://www.geautomation.com/products/rx3i-cpx-400 i think you should download the modbus library from VIPM and give it a go. there are plenty of examples online as well. If all else fails, opc is definitely a great alternative
seems like the PLC is modbus capable http://www.geautomation.com/products/rx3i-cpx-400 i think you should download the modbus library from VIPM and give it a go. there are plenty of examples online as well. If all else fails, opc is definitely a great alternative
Thanks for the info. I wasnt sure if this capability was something that had to be setup internal to the PLC. Given the link you provided it seems that it is a default, built-in functionality. I'll give it a shot! Thanks!
Using classes would help. Just create a class of Data acquisition and and have one of the children be DAQmx and then just make everything ready. Another child could be some virtual or test that gives the same result as you expect instead.
I'm not sure I've ever tried that, but you for sure can drag it from the icon to anywhere. 
What if the control on the front panel of your typedef itself was a typedef? Should it transfer the host type def or the child type?
that's partially what I'm ranting about... you have to drag the ctl from project or from icon for the type def to follow...
Are you sure you didn't replace the original control with the new type Def. I am pretty sure you can copy from panel to panel. I tend to use the ctrl+drag method, perhaps this is different to copy and pasting.
Without knowing much about the specific instrument I can assure you that your problem will be definitely not a GPIB limitations. I would check the string to number conversion in the subVI that reads the actual value out. Usually it is about decimal separators, SI notation, etc...
Try this. The extension on the URL doesn't matter https://forums.ni.com/t5/Example-Programs/Download-An-Image-From-A-Website-And-Show-It-On-A-LabVIEW/ta-p/3529497 
Thanks, this helped a lot. It didn't work initially, because apparently, while the URL doesn't need .jpg, the output file for the http read node does, and because this vi derrived that filename from the URL, it threw errors. Just had to change the filename to something with a .jpg extension and it worked like a charm :)
What is the purpose of ni-max, ni-visa, and ni-imaq? Are they essential to what I am trying to do
I use these cameras in my research and I can tell you the best thing to do to get used to using them with LabVIEW is to look at the example VIs given by ThorLabs: [https://www.thorlabs.com/thorproduct.cfm?partnumber=DCC1545M](https://www.thorlabs.com/thorproduct.cfm?partnumber=DCC1545M). I believe you can get your hands on them by downloading the software from this link and then looking for the LabVIEW VI files, but it's been a few years since I acquired them so I'm not 100&amp;#37; sure. As to how you achieve your goal, I'd start by trying to use the 'GetActImgMem' VI to output the data to an array in LabVIEW (or output to .csv files and analyse elsewhere). From there you could start doing some math with these arrays to find where your laser is irradiating the camera.
NI-MAX is to control/test settings on instruments you have connected to your pc NI-VISA is usually required to write or read data from an instrument but can be used for other things as well I have never used NI IMAQ however I assume it is the software used to receive a visual signal e.g. your camera
IMAQ is the driver layer for interfacing with cameras for IMage AcQuisition. This layer standardises the way you pull data frame a camera (any camera that is compatible). Once you have the image in LabVIEW you can then process the data in a number of ways. You can process the raw data by hand which is tough going by manipulating data in arrays but requires no additional software libraries. You can pass the data into a LabVIEW pixmap format and process it as a LabVIEW picture which will then get you some extra basic functionality from the LabVIEW pictures library. You can process the images using NI VISION which will do everything you need but is expensive and I doubt you have that. Export the data to CSV or such then do vision analysis using other software platforms such as python which have open source vision libraries available. 
Funny, I'm working on a LabVIEW application with a Thorlabs camera right now. Download the data acquisition software from Thorlabs (ThorCam), it comes with LabVIEW examples for acquiring data with the camera. Take one of those examples and convert it into doing what it is you need to do.
NI-IMAQ provides vision related functions such as creating and manipulating an image, performing image analysis and processing or controlling certain cameras. For the Thorlabs camera, you don't actually need this.
Subtracting one image from another is a great way to detect movement. If the laser is round, then as it tracks across the camera, you should be able to fit a circle to the difference image to obtain the center of the laser as it moves. Alternately, you can fit a 2D gaussian to each image as it is collected, and just obtain the laser position as a function of time. All of these tasks are fairly basic functions that you should be able to identify through the help menus and google searches.
For finding the beam location, processing the raw array will be plenty, no need for NI Vision. I'd just average all the rows and fit the result to a gaussian. Then average all the columns and do the same. If the beam if very dim (low signal to noise), you'd need something fancier, but for a bright beam this will be more than sufficient. 
Define 'as fast as possible' I just purchased a new workstation from HP a few months ago. Most workstations available on US market comes with INTEL processor (E-series/ W-series). But what you really need to understand is this and not make the same mistakes I did: 1. Define the configuration you need for your system; sampling rate, bus speed, write speed etc. In your case what is the color depth of each pixel, are you ready to sacrifice resolution to gain a higher acquisition rate, etc. 2. Contact vendors who sell workstations for example DELL/HP/ACER and ask them to configure a system based on your needs defined in step 1. 3. *This step is important*. Ask them to send you a test PC (DELL usually does this based on your budget and project depth) so that you can try your LabVIEW card on the motherboard/ try your needs in general. 4. Create a Rubric and tick off each criteria based on the PC you reviewed. tl;dr = You need to know what your system should accomplish before you can go ahead and buy stuff
LV actually has a 2D surface fitting tool which will allow him to fit any other features that might come into play. Just fitting rows of gaussians can be a little unreliable.
From my experience, AMD units are cheaper for the same processing power. 
We're going to have it built in-house, so what's available prebuilt doesn't really matter. Also in Canada so there are less options in that regard. Cameras haven't been chosen yet, resolution undetermined. We're doing ion imaging with a 1KHz laser so 1000fps is the cap, our current system can run 100fps with an i7-6700 despite the camera running at 300fps. That's at VGA resolution. As fast as possible within that price range without buying dedicated hardware that isn't useful for other things. An FPGA-type system could probably speed us up a lot, but we'd like to avoid that. Increasing the acquisition rate is very important as the experimental conditions tend to drift. Knowing beforehand precisely what we need the computer to do is going to be impossible.
Thank you for the thoughtful response. I've followed your advice, from thorlabs I've found a folder of sample vi's. However it appears that what comes with my camera I don't have 'GetActImgMem' and I couldn't find it online. [Here](https://imgur.com/a/kgvJrlX) is what I do have. I've played around with the SimpleLive_Memory_Sequence and PixelPeek, I'm not sure if they're going to work as well as the vi you suggested. Also I barely know how to utilize them at this point, but my camera is now reading into labview. Also, when you say you output to .csv and analyze elsewhere, What do you recommend for what program to output to for my purpose? thanks again
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/hyEs60D.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e126jof) 
Would it be a bother for you to email that VI to me? it sounds helpful. I can't find it from a google or a full search of my pc.
You can get pretty good laptops at Walmart, some of them even come with dvd players built in. Try to get one with a big hard drive, that way you can download more movies before you have to buy a newer computer. Windows 11 will probably come out before the holidays, so if you can wait until the black friday sales you can save money and get the better Microsoft. Also save your receipt in case it breaks you can return it for a new one.
Can you describe the use case? Is this a streaming application where youll need to be real-time processing the data for long periods of time, or can you do more of a stream-to-disk, and maybe process every 10th frame for UI updates? also, have you benchmarked core utilization while running the current system? Could you parallelize the processing to improve speeds? Main reason I ask is because you may be able to improve performance dramatically by using all 4 cores if you arent already. 
For this I would probably go for a modern AMD (threadripper anyone!) and try to get as many cores in as you can. I would go for a dedicated high speed network card assuming your going to be using gigevision or similar ( bit out of date in vision) and a lot of fast RAM. Then also a high speed SSD. With something along those lines you should be able to parallel up a lot of the processing. Also of note although I have never used it may be the cuda processing library running on a graphics card....
I do most of my data analysis with MATLAB script nodes within my LabVIEW VIs, mostly just because I'm more familiar working with large arrays in Matlab than LabVIEW. 
I sent you a PM that should hopefully help.
The Ryzen and Threadripper line ups from AMD have better multi-core and hyper threading abilities for workstation tasks, such as labview. They'll out perform comparable Intel CPUs in the price range so I'd suggest going with them. LabVIEW also benefits when paited with more ram to chew on such as 16 or 32 gigs, if you give me a price range I could see what I could throw you together. The other option is to hop on over to r/buildapc and they'll be able to hook you right up with what you need!
Yes another vote for the SSD. A fast drive is often overlooked and makes a huge difference. 
What about multi-threading using LabVIEW? How does it relate to cores ? Can we divide the tasks and overclock 2 cores to carry read/write and leave the rest to do other system tasks ?
Lots of options with regards to multithreading, simplest options would be to have the acquisition in one loop and split the frames into N numbers of loops. Another easy way is to use a parallelized for loop (right click on a for loop). Big thing you want is to be able to have all cores railed to see if you even need a faster machine, or get an idea of how fast a computer you need. Also a good way of checking whether you performance concerns can be alleviated by just writing more optimized software. 
I would like to come up with a configuration for high speed data streaming, 1.25 GS/sec @ 10 bit resolution. I will collecting data for 400 us. So I am confused as well if I should go with Xeon or just i-7 processor. I am pretty sure Graphics card doesn't matter for this purpose. Do you have experience with high speed data acquisition ?
I think you should step back and describe the system in more detail, you keep jumping between different requirements. I thought you were taking imaging frames, not digitizing data, or is this for something else? What is your repetition rate/duty cycle on the digitized data, and do you have to process the data on the fly? 600us of data isnt very much, less than a couple mb, so if you dont have a high duty cycle its not much total data. The digitizer will actually be handling the acquisition, grabbing the samples from the adc then transferring it into its onboard memory, then over to the PCs memory. Nothing in there is processor dependent. I'm a system engineer for NI, so I design a lot of high speed acquisition systems ;) 
I wouldn't rely on the time format when exporting outside of LabVIEW. I would understand yo[ur actual sample rate](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P83OSAS) and ensure that gets recorded into Excel File. The only way to know is to know what sample rate you set. If you were working on a lab, perhaps it specified which sample rate you were supposed to use.
Haven't had a computer without one in over five years :) 
Streaming to disk and processing every 10th frame could work too, I've tried streaming the images to files in a RAM disk or on the hard drive and then having a totally separate VI processing and deleting them too. That doesn't seem to run faster than doing it all within one VI with queuing. The task is actually running in parallel, it queues up the images to process and then runs multiple loops in parallel to reduce the queue size. The problem is that no matter how many parallel loops I put, the queue size always increases faster than they can process it so there's always a saturation point within a few minutes where going higher will utilize too much RAM. I assume parallel loops can utilize different cores properly automatically? 
It wasn't but I spent a while analyzing it and found an Integer dt that made the data produce the intended result. 
When you say drag from front panel to front panel, are you trying to open up the .ctl file and drag the controls on that front panel to another? If so, the problem is, the controls represented on the .ctl front panel are not the typedef themselves. They're the non-typedef (raw?) controls that make up the typedef. So dragging those will exhibit the behavior you're seeing.
[removed]
How do I get an idea as to how "impressive" a program is? I'm a physics graduate student who uses Labview a ton to automate instrument control and full on experiments, and I feel like I could probably figure out how to program something to solve anything I'd encounter... I will certainly keep "extensive labview experience" on my CV but I wonder how much it's worth tidying up some code I've written in order to make it more presentable in a job application. Generally "if it works, don't fix it" holds true in a lab setting where every task only lasts a few years before we completely rebuild the experiment so it's only really instrument control that we tend to care about making "well written." But obviously in industry one needs a different attitude. Of the examples included, can you think of any that would fit under "pretty good," "really good," and "excellent" if they were the tier of program included in a portfolio for a job application. 
It will be of great help if you could take a look at the following links: At the beginning of my system design I was worried about causing a data bottleneck and this discussion was discontinued for whatever reason. Link [here](https://forums.ni.com/t5/PXI/Data-bottleneck-concerns/m-p/3758360/thread-id/17880). After this, I purchased an high end HP workstation Z4 G4 and it dint go very well. I had M.2 SSD (primary) and SSD (secondary drive) configuration. Idea was to stream all data from PXIe 5160 to primary first and then push it to secondary thus having made full use of the high write speed to the M.2 SSD. Because of the issue I faced with the HP workstation, I have all lost all confidence in re-configuring a new one. If you could help me to spec out a new configuration, that would be awesome!
Whenever I've run into similar issues it's because a .dll is either not installed properly, or the dll is calling something that doesn't exist (another .dll in my case). I would make sure you have the correct drivers. And if you can figure out what call LabVIEW is making right when it crashes. 
First, [this DLL is associated with the SSL Web Service](http://ae.natinst.com/public.nsf/web/searchinternal/927987c92452b7b08625764600619e5d?OpenDocument). I would open services.msc and ensure that NI System Web Server and similar services are actually running. Are you actually running Windows 7/10 or are you running Windows Embedded (WES) or another variant? If you're running a variant, it may not have the correct components needed to run. I'd recommend creating a service request with that log and your [https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P9mPSAS](MAX technical report) to further investigate.
I realized that I even can't use Update service. Anything related with network connection just crashes with the same dll error.
What work are you doing with the camera?
Thank you
We are integrating it with a liquid crystal tuneable filter (LCTF) basically, you set the filter to only allow a certain band of light to pass through. The application range is really wide, we have built systems like these for cosmetics to art imaging to steak imaging. Basically someone wants to look at the wavelength response of a target, hopefully being able to identify some kind of feature based on a change in one wavelength compared to another.
Neat. Do you work at a private company or is this research with a school or something? Also which thorlabs camera are you using? I still haven't gotten mine to connect properly to this sample VI i have in labview
Also do you use imaq? My camera picks up in thorlabs but seems unresponsive in labview (nothing happens when i run the VI)
Will you please go into more detail on this 2d surface fitting tool and how it's used? I see curve fitting tools when i google
I work for a private company. We are using the Thorlabs 4050M and 8051M (evaluating between the two). So while MAX and the IMAQ tool pallet can often provide each access to controlling cameras, there are a lot that you cannot. These Thorlabs cameras fall into the pile that cannot. For those cameras that don't use the IMAQ tools, you end up controlling these cameras by making library calls. In the case of the Thorlabs cameras, you are actually making .net property and invoke calls. The best way to figure out how to use these cameras is by downloading the ThorCam software as that comes with a bunch of VIs and examples on how to run the camera.
Oh, of course in 3 we will turn the output of the power supply back on
I think you will be better off using a simple state machine. The thing is that you are suggesting pausing in a loop untill the pause button is released which is a type of blocking code. This isn't a problem in itself but is not the best way to design this. I believe there is a state machine pattern from the "new..." Menu to get you started. Something like the following states would be needed. Start Stop Restart Get time Write file Use shift registers in the state machine to keep track of your time between states.
Hey, SpacePiwate, just a quick heads-up: **untill** is actually spelled **until**. You can remember it by **one l at the end**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I'll check it out
Don't use sequence structures. Stacked sequences do not have functional error checking, and even a flat sequence doesn't do shit to help you avoid race conditions. Control your order of operations with error clusters. Use the flow model that is built into labview. Completely stop using local variables (use property node-&gt;value). Look into functional global variables. 
In general, I keep three parallel queue handlers going in any program: 1. Operations: Handles front panel value changes, toggles switches, etc... 2. Data: Expensive calculations that need a chunk of processor 3. File reading/writing 
Where can I learn more about the LabVIEW programming paradigm??? I am too used to procedural or object-oriented paradigms that dont really seem to translate well to labview. controlling and monitoring the flow of data is difficult for me and I am not even really sure what an error cluster is or how to use them. I've skimmed LabVIEW for Scientists and Engineers by John Essick, but my skimming didn't really give me the chance to write a complex VI before diving right in. Do you have any resources I could use to learn more about writing in LabVIEW?
Here's a pretty good spot, as are youtube tutorials. Learn as soon as you can about how to use objects, state machines, and producer-consumer architectures, and the rest will open up. 
[LabVIEW For Everyone](https://www.amazon.com/LabVIEW-Everyone-Graphical-Programming-Made/dp/0131856723/ref=sr_1_1?ie=UTF8&amp;qid=1530365848&amp;sr=8-1&amp;keywords=labview+for+everyone) is an excellent reference. When you get a little bit more experiece, [The LabVIEW Style Book](https://www.amazon.com/LabVIEW-Style-Book-Peter-Blume/dp/0131458353/ref=sr_1_fkmr0_1?s=books&amp;ie=UTF8&amp;qid=1530365901&amp;sr=1-1-fkmr0&amp;keywords=labview+style+guide) will also be helpful. NI's website is a treasure trove of resources and example code, but be advised that their search engine is a complete donkey show. [The LAVA forums](https://lavag.org/) are another great resource, especially if you have a lot of programming experience. I see a lot of OOP-related topics over there, some of those guys are just doing some next-level stuff. I second the idea of never using sequence structures. Properly designed code should never need them, and you should pretty much never need local or global variables, either. Of course, this is coming from a guy that continually writes code that it literally awash in shift registers, so, there's that.
&gt;Completely stop using local variables (use property node-&gt;value). I definitely wouldn't prefer property nodes over local variables, especially when used in loops. https://forums.ni.com/t5/LabVIEW/local-vs-property-node/td-p/321755
It's a little slower, but it's a million times harder to get all the problems you get with local variables.
I think it occupies a niche that just doesn't warrant the popularity. My experience with it is embedded systems, and that seems to be what it's made for, or at least, the main thing I would use it for. The data flow paradigm can make certain aspects of programming easier but doesn't scale well to complex or general purpose computing where it can easily become an inneficient mess of wires. At least in written code, if all else fails, you can go line by line, stepping into/out of functions as you go, which, in tandem with their general efficiency, justify C and C++ as the two most popular embedded systems languages. Even so, if I feel the need to use a data flow language, i generally can't make a compelling case for LabView over the more familiar, easier to learn and more visually pleasing Simulink. To be perfectly clear, this is just my opinion based on my experiences with these languages, but I also respect that LabView does have it's benefits and use-cases. Honestly, I think most of the hate just comes from it seeming hard because it's a radically new way of thinking about programming for someone (most people) that learned by taking a class in python, matlab, or C/C++. It's easy enough to look up the syntax for an if-then-else command if it's the difference between adding a couple {}s or something, but I know I was a bit lost when I started working with the same commands in LabView. Plus, it's just ugly: it looks and feels clunky and dated from the moment you boot it up.
It’s exceptionally bad for large scale applications. I’ve worked on several distributed applications now that have their core code in LabVIEW and having to wait 10-15 minutes to open a project to make edits or conduct code review is quite cumbersome. As well the LV implementation of OO is crazy unintuitive and only makes project stability worse. I do enjoy LabVIEW but Python is superior in almost every way.
People think scripting and they think Python. I encounter it in my own jobs all the time. The main problem is that LabVIEW has always been marketed to non-computer science engineers (as I said in your other post). So, mechanical and electrical engineers have been the main users for a long time, and unfortunately, they don't engineer software very well. So, when they finally do bring a CSE into the project it's already so deep into the spaghetti stage that there's no way anyone would touch it with a ten foot pole. So they punt and rewrite it in a language they do know, typically Python. Which is a shame, because outside of academia, Python sucks balls. As of last year, which was the last time I used it, I think WxPython was still the most advanced UI toolset for it and it takes pages and pages and pages of code to make the simplest user interfaces. If you thought handling making non-LabVIEW looking UIs is difficult in LabVIEW, you should try writing anything with WxPython. Check out the source for "Hello World" here; [https://www.wxpython.org/pages/overview/#hello-world](https://www.wxpython.org/pages/overview/#hello-world) 
Several good answers already. I don't fully agree that LabVIEW doesn't scale to large projects. Opening large projects can be a pain. Newer versions get better, but even so I've seen massive projects with teams of people working together successfully. I've also never known any other OO than LabVIEW so it doesn't feel weird. It does fell a bit bolted on as an after thought though. I think another reason LabVIEW isn't more popular is it is closed source, and primarily used to drive NI sales up. If I could compile LabVIEW for any random FPGA I could buy it would be amazing. A bare FGPA is on the order of $100, but the cheapest NI FPGA is much more expensive and somewhat tied to a locked down eco system. Having the platform closed, the compiler closed, and the source in a closed file formal, likely make communities not take it serious. NI is behind in the maker movement too since they focus on industries more. They do make an effort to branch out, with the MyDAQ, MyRIO, LINX compiler for the pi, involvement in First robotics, and Lego mindstorm. But many enthusiasts want a cheap micro, and a free language to deploy embedded code to. Even having a home version for $50 is a larger barrier than a free Arduino IDE. I love it I couldn't see using a one dimensional programming language for my career, and intend on retiring in LabVIEW if I can. But honestly that will depend on where NI takes the platform.
LV does NOT become an "inefficient mess of wires" if you follow some simple design principles. It can scale quite nicely. There is nothing inherently limiting about it. As to stepping in and out, probing values during execution, etc., LV has some great tools. I prefer them to the debugging tools in text-based languages. I suppose it is a matter of what you're used to. 
Open source it and make it compatible with Git.
$$$
Granted, I am not an expert in LabView who has worked on many large scale projects, but I worked in LabView as the primary language for my senior project this past year and there were a lot of times that It was harder for me to follow the logic even though the guy who primarily wrote it cleaned it up and made it about as clear as possible. It very well could be that my lack of experience just means it's not as second-nature to me as reading code line-by-line, but there were a lot of moments where It seemed the better option to write it in C/C++ if it wasn't required to write it in LabView (the guy who wrote it expressed a similar, unprompted, sentiment on multiple occasions). That said, it did lend itself well to organizing the state machine we used for the project. To the second point, I don't mean to say that it isn't well equipped with debugging/quality of life tools. I only mean to say that reading code line-by-line is easier (for me personally) to follow because commands are given in order and you can reasonably expect that they will be executed in exactly that order. while the same can be done in LV, the visual approach lends itself way less to an intuitive understand of the order of commands without a thorough knowledge of NIs rules for data flow.
If you become fluent in LabVIEW you’ll start to see that it is very intuitive, and reading code is very easy and straightforward. It’s fine to have a preference but don’t knock LabVIEW out of ignorance. 
Hey! I just downloaded all your CAN stuff off of your blog. Can't wait to integrate the Vector API logging and other file I/O into my stuff at work. Thanks for your work!
I'm not going to get into a philosophical discussion about LabVIEW, you can cite pros and cons of every language all day. I've been a LabVIEW developer for 12 years and as much as I like it, it's not without its flaws. Cost has got to be the biggest barrier to entry for most people. There are so many languages out there that have zero or minimal cost, it's just too easy to find an alternative. Personally, I hate the huge amount number of dependencies between NI s/w products, plus the install/uninstall process takes a month of Sundays. I have six test systems that all need upgrades to the current release, and it will literally take me a week to make it happen. Yech.
I second. And the bigger it gets the worse it is to update or maintain.
Okay, I should probably rephrase and make note of the fact that while my experience in LV is limited, I am not unaccustomed to data flow programming. I learned Simulink long before LV and have used it in multiple classes and in my research regularly as a student. with that in mind, I still found LV difficult and unintuitive. For this reason, as I said before, I can seldom make a compelling case to use LV over Simulink. This isn't to say that LV doesn't have it's use-cases, it absolutely does, but the things I personally need to get done in my work are easier in Simulink or faster in C/C++. Hence my statement that it occupies a niche. 
I think its great for for someone like a mechanical engineer that needs to interface to a simple system like a thermal chamber and vibration table, or even for someone to create a specialized tool that is used by a group of people within your own business. But it doesn't seem reasonable for anything larger than that. 
That’s because the typical ME that’s going to build that interface doesn’t follow the design principles that prevent the tool from becoming an unwieldy tangle that’s impossible to troubleshoot. They build something that probably works ok, but wasn’t designed for maintainability or readability by someone else. 
With the NI Package Manager (not the VIPM!) and SystemLink it will be easier in the future by managing packages and feeds. On the other hand yes, I know the pain looking at my laptop at Sunday 6 pm seeing an installation progress bar at 60%..
By far the biggest reason. I can put python on any computer in the lab, but Labview I only have on my laptop. I don't have the version of Labview on my computer that can deploy to an executable so I have to take the program to another machine in another build make it and put it on the lab machine and hope the change I've made works on the lab computer with the hardware thats too big to move. I would use it more if I could develop on more machines, but my lab isn't covered by the "site licence" it would cost thousands per computer, so I wind up using python for most projects and Labview when it is easier despite the hurdles. 
Depending on the industry and company LabVIEW can be very popular (manufacturing, aerospace) and the standard development environment for test systems or it can be completely unknown (financial services). It is taught or used in most engineering programs around the world. Being propriety proprietary is something that it would get dinged on and creates a barrier to entry. NXG is more open in that the source files are xml based. It may be a matter of time before someone builds an open source editor. I think there should be a free version of LabVIEW, not fifty bucks for home edition, free. See Microsoft Visual Studio Community Edition. 
Interesting. I'm all for anything that simplifies dealing with installations &amp; upgrades as I'm the only person in the company that handles anything NI-related.
So you are making sweeping pronouncements about LabVIEW and its suitability to large projects and ease of debugging when you are an admitted "non expert". And you are spreading FUD when you admit it might be because of a certain "lack of experience". Do yourself a favor as you go out into the real world, preach only about things you are expert in. It will be better for you and better for the world. 
Glad you like it, I've drafted a couple more posts but just don't think I have the time for API writing or examples. Feedback is welcome in any form, LAVA, NI forums, email, etc.
Been talking to some sales reps and app engineers about this recently. Couldn't have said it better myself, and I'm really glad that I'm not alone in this line of thought. Cheers!
Don't waste time on NXG, it has a long ways to go until it becomes viable for professional developers. It's easily three or four versions away from being ready for the prime time. When it gets there, and I'm sure it will, it will be really nice and learning it alongside vanilla LabVIEW is going to be pretty low effort.
I second u/iYougurt. Don’t concentrate on NXG yet, with a small handful of exceptions (such as web VIs). RT and FPGA are not yet supported.
hmmm. Have you tried 2018, which I think looks really nice? Also, the wire mess you talk about is bad coding. That's like writing functions without names. Anyway, maybe it's just me but I find it so much easier to program in LabVIEW (graphically) , simulink doesn't really have the complexity I need but maybe I should look at it again as I haven't used it in ages.
Agree, that seems to be the issue. Inheriting badly written code. My first job in a large corporation several years ago was to convert the mess into readable code. It could be just the way I am but I find it so much easier to remember code written in LabVIEW than text, especially when I taken the time to do a nice subVI icon. IMHO, it would be great if other coding languages became more visual and converting code from LabVIEW to python kind makes me feel sad :-( but still hopefully it'll be an interesting challenge!
Yep, NI recently removed their office from my country!! It's sad really as it's hard to beat feet on the ground. I'm not sure where they are going next but definitely if they opened the FPGA space even with the cost, it might bring in a lot of new users to LabVIEW. However, the may be concerned about RIO sales. Open source LabVIEW would be truly amazing and might change the programming world forever!
It use it on git...
As someone who was the lead on a team of about 13 LabVIEW users, that is simply not true. Amazing things can be done with LabVIEW.
I’m sure. How widely do you distribute those apps? How many copies sold?
LabVIEW being open could change things a lot. But those at NI might ask how making LabVIEW a more general purpose language, or how supporting cheap hardware is going make NI more money. Individuals at NI may love the idea of furthering the programming community but NI themselves likely have money as their driving force. That being said LabVIEW does have a very important place in various industries already. Rapid prototype, automotive, automated test, and engineering fields use it often enough. So NI must be doing something right to at least get into these areas.
I think FPGA is supported in the most recent version, but I agree on the other stuff.
Not supported on NXG 2.1 but supported on 3 beta. 
I think it's good to review NXG before committing to it. Periodic reviews will help you ease into it when it is ready for prime time. 
Sure, you can keep LabVIEW files in Git repositories, but can you easily view diffs between versions and merge in contributions from collaborators? I've seen some 3rd party tools for this but it didn't look like they were quite fully baked.
What is a race condition?
A race condition is when you have two parallel processes running, and the output of your system depends on one of the them finishing first. 
Sounds like you need to simply get in a habit of making a lot more subVIs. A line of subVIs is the same as a sequence structure. To help you, there is a shortcut where you can select a bunch of code from the block diagram and make a subvi from the selection. What is your project? We could help you more if you describe want to create. Things like race conditions are general CS questions, not specific to LabVIEW. 
My project controls a DC power supply voltage according to a profile specified by the user. It also outputs current, voltage, and time measurements to a file. I have a VI written that does exactly what I need it to, but it definitely could be improved. I don't know exactly how I can upload it here but let me know.
Is there a reason you are using LabVIEW rather than Python? LabVIEW has a screenshot tool in the block diagram, see "Vi snippets" on the sidebar. 
The lab I'm working in LabView because that's what the professor at the lab I'm working in wanted me to use. He's vaguely familiar with it compared to python. I also have to take a class next semester thatis centered on learning labview so I thought it would be good to play around with it and get a head start before the class started.
That's a fantastic reason to use LabVIEW; but FWIW for the project you described I would grab python. &gt; I thought it would be good to play around with it and get a head start before the class started. I disagree. Don't get ahead of yourself; just wait for the class and follow the recommendations you learn there. At most maybe pick up a general computer science book and learn about basic data types (learn what U32 is and how it's different from I64, etc) and basic communication. 
Research different design patterns. State machines, queued message handlers, publisher subscriber, and producer consumer. NI has whitepapers on all of these. After that look into more advanced things like actor based designs, actor framework DQMH, and several others for controlling independent parallel loops. Large projects are broken up into smaller parts. Have a single loop doing logging, a single loop talking to a DMM, single loop for UI, etc. Now instead of a loop make it a loop in a VI. Now make that VI into a library, or class. Now individual parts of the application can be unit tested, and verified without the whole application. A person can focus on one task at a time and people can work on separate parts of the application at the same time. Source code control helps keep things organized. 
I have create and supported many complex applications over the years, serving thousands of users. They range from complex real-time image capture and analysis applications (4 USB cameras at 30 fps without skipping a beat) to very elaborate GUI interfaces for data analysis. I've used LabVIEW pretty exclusively for so long that I don't know whether I could have done all this more easily in other IDE's. But I haven't come anywhere close to the limitations of LabVIEW. For me, it is fast, almost self-documenting, and easy to debug. i am all self-taught, but likely somewhere in the CLD range. The LabVIEW Style Book is the best book I've read about LabVIEW. My goodness it's old now (over 10 years), but much of what it talks about is still relevant to the basic design of vi's, and really improved my designs. There's no question that LabVIEW can be very hard to get used to when you are coming from text-based languages. The hardest is realizing that the order of elements on the page is not necessarily the order of execution unless there is data dependency between the elements (beware race conditions). But once you learn LV, it is incredibly powerful and fast for making complex GUIs and controlling complex hardware, whether from NI or not. And LabVIEW is such a rich environment. There are always new things to learn and explore, new ways to do thing, new tools to work with. To me it's more fun than playing video games. LV is absolutely great for making user interfaces. One of my favorite things is making graphs and tables that are active tools for exploring data. The Event Structure and property nodes make this a breeze. Again, it takes some time to learn, but once you do getting things working is quick and easy. 
I guess I see it the other way. Simulink is useful if you want to prototype control schemes or simulate plants. That's its niche. You can do all of that in LabVIEW too...but with LabVIEW you also have a complete programming language and IDE.
Definitely but 11 years ago when I started looking at it, python was nowhere near LabVIEW, the problem I am finding now is people moving away from it instead of toward it. Also, NI seems to becoming much more focused on hardware sales than building the LabVIEW user base. I think NI need to decide, open LabVIEW up or become a hardware company because what I'm seeing in 10 years, LaBVIEW will be legacy. Maybe there is a middle ground, I don't yet know what it is!
It's a little convoluted but : https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019OWSSA2 https://ispring-cloud-45.ispringcloud.com/acc/kc75hpYxMDM3NA/s/10374-N2ZHD-doaJL-kBQpK I'm the only developer in my branch so not a lot of merging/ comparing !
no, the back side of a C-series module is a 9 pin Dsub but it is not communicating via RS232. Theres a 1 slot carrier for this : http://www.ni.com/en-us/support/model.cdaq-9171.html
Good to know. Thanks!
The thing is that you can always write strong backend code in Python, and interact with either a local web application (ala Django), or a C++ Qt application over an encrypted socket. LabVIEW doesn't integrate with other toolkits quite like Python.
First step I guess is to try communicating with your local machine. Run both programs on the same machine and point to ```127.0.0.1:port```
Any reason youre not just reading the number of bytes at the port instead of a specific byte count? Also, if you use the example, do you get similar behavior? What device are you talking to?
Mostly because this device sometimes outputs partial data, which I'm trying to reconstruct. Which is why I have the loop, to consume as much as possible. The device is a proprietary sensor that unfortunately I'm not allowed to talk about outside of the basic serial settings. I've put it onto a logic analyzer, and it definitely isn't dropping the first bytes, so I know it's an issue with my driver. Which example are you referring to?
Are your network settings correct? Does it respond to ICMP, eg ping? Are your firewall ports opened?
yes i can ping it from the CMD. I took the firewall down to make a test as well. No luck
the continuous serial read and write example. 
Okay I'll try with the example, though I don't see a whole lot of difference between that and mine. I like your idea, I'll build something around that concept as well and report back.
Youre probably missing data because your code has some kind of odd logic. You enter the statement when you have enough bytes, then read contunously while ignoring timeouts instead of only calling the read when there are bytes on the port. In general, if youre having to ignore error codes, your code is doing something wrong. 
First off is there a serial termination character? If there is enable termination characters and define it via VISA property nodes. Then read a number of bytes larger than your biggest packet. The serial read will wait untill a term char is received and return your data packet.
Hey, SpacePiwate, just a quick heads-up: **untill** is actually spelled **until**. You can remember it by **one l at the end**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Is it an LXI device? If so add it as a new instrument in MAX and use VISA to communicate with it.
No termchar. Like I mentioned, I've ruled out the basic configuration stuff, as I do have &gt;50% full packet reads, eg no errors.
Dropping bytes is often a symptom of a termination character ending the read between loop iterations. Might be worth double checking your receive data doesn't contain a \n or \r. Other than that it is hard to recommend anything without knowing more about the serial protocol. Good luck!
It actually does! Nice catch, is there a config I can do to change that behavior?
Either change the read to enable term chars and set it to \n or \r (correct way) then put the read up to say 800b and a timeout of say 3000ms. This will then ensure you get a full "packet" of data. You may need to read one or more packets for the full data but it will be much more consistent. Other way is to disable term char and read a set number of bytes(can go wrong quickly if the receive buffer goes out of sync) which is sort of what you are trying to do and is why you are having trouble.
I dont know if it is LXI, but i added it as a VISA Object with MAX. I only got : Possible reason(s): VISA: (Hex 0xBFFF0015) Timeout expired before operation completed. From that
You would need to add it as a TCP/IP device under MAX. Like this. http://www.ni.com/getting-started/set-up-hardware/instrument-control/ethernet-connect If that doesn't work then it might not be via compliant
Just to exist the discussion slightly, I have not tried NXG yet but was thinking it looks like a prime candidate for designing a Web based front end for an SQL database. Anyone have any thoughts on this?
Also, it's a 15 pin connector not a 9 pin.
Good point, forgot it was the VGA connector not a 9pin dsub
If you are using tcp listen, im pretty sure you have to use a specific error handler to ignore the timeout error and retry listen. Try putting the block inside of a while loop until the socket is connected before attempting a read.
I eventually changed the module for a serial module and now it works. I could not figure the problem. Thanks to all of you guys for your help
&gt;I hate the huge amount number of dependencies between NI s/w products NI SW developer here. Can't tell you how much I hate this too. This has been caused by years and years of laziness on our installer team and lack of willpower on management to remove anything unnecessary from our installer ('What if it breaks someone'). The official installer for the product I work on installs somewhere between 700 MB - 1.3 GB of crap depending on which configuration you choose. This is after removing 4-5 products that we didn't need to install but were being installed because we copy/pasted things from a different installer. During development, I literally just need the C++ runtime installed and a few binaries (&lt; 50 MB) and I'm good to develop and test. That should give you an idea of how much crap we shove into our installers.
Ooh! Presuming you've seen http://www.webvi.io then? Can't comment on the SQL stuff but the default UI is miles ahead of how they used to look
Cheers! Will concentrate on classic LabVIEW then
I've got a very similar problem. I have to make some changes to a project that's already written with LabVIEW. I didn't know though that I could do the same with Python. Is it possible to control Keihley SourceMeter this way? How do I execute the code? Is it possible to do it on Windows? Sorry if the question is stupid I'm quite new to this. 
Why not just remove where it's read and replace it with a boolean constant and a comment?
It depends on whether the RIO FPGA was programmed using Scan Engine or custom FPGA code. The Scan Engine has a forcing API in which you can override input or output values (both digital and analog channels). The Forcing API is not accessible through NI-MAX but is available via the NI Distributed Systems Manager. You need to first enable forcing on the chassis as a whole, then on a channel by channel basis. However, IIRC, if you exit Distributed Systems Manager, it will automatically deactivate forcing (I may be wrong on that, it may just be built into the application I created around the Forcing API in LabVIEW). The forcing will deactivate if the chassis is rebooted.
Late to the party and I know you already changed to a serial module, but here is some info for future use. If there is a sample program available, watch the communications by using Wireshark, an Ethernet packet sniffer. It captures all Ethernet communications on you computer, and thus you need to learn its filtering mechanisms, but once you have the packets isolated to where the IP source or IP Destination is your device, it can be fairly straightforward if the protocol is a known one (includes built in decoders for common protocols). Depending upon the protocol the device uses, a TCP Open/Listen/Close is generally not enough. The Listen is used for it to send messages to you. With certain protocols, this may be enough (e.g. your program is acting as a Modbus Slave), as it expects to only respond to messages/commands it receives). However, if the device is more akin to the stereotypical boxed instrument such as an Oscilloscope, once the communication is established by calling the TCP Open, your device will start waiting on commands from you before doing anything. In this sort of instance, you would call TCP Open, TCP Write (i.e. Set Input to Volts), TCP Read (Ack response to Set Input command), and repeat the TCP Write/Reads until you end your session with a TCP Close. If on the other hand you device is one that is just supposed to spit out data once a connection is established, then Wireshark (listed above) would be your best option to determine what might be going wrong.
The porogram is loaded as an executable to the rio i cant really modify it.
Yes. You can do all this. Create an array of clusters with the 4 controls in the cluster. Your + button would increase the number of visible rows in the array (using a property node) until it reaches a the maximum number of rows and the scroll bar appears (also set in the property nodes). You might also want to put a control/indicator in the cluster that you populate with the row number, so the user knows where in the array he is when he scrolls. That way you can hider the Index Display (which is kinda ugly). 
Another tip when doing this: the elements in the cluster will have to be all controls or all indicators. Right click/advanced/disable any controls you want to behave like indicators (such as an index value)
When I wanted something like that in the past, I would represent it as a multi column listbox, or maybe just a normal listbox, and a set of boolean buttons. The MCLB looks like a table and is empty at the start. There is an Add, Configure, and Remove button, but with nothing in the table only Add is enabled the other two are greyed out. Click Add and a dialog comes up asking for the name and configuration. After confirming the dialog an entry is added to the MCLB with the information the user entered. Now you can select that name and click Remove, or Configure. Behind the scenes this information is stored as an array of a cluster where each cluster represents one row in the MCLB. I've taken this a step further and added right click menus to the MCLB, or handling double clicking as a configure.
Can't believe I didn't think of this. Thank you!
Are MCLBs ActiveX controls? I haven't seen them before in the 'normal' palette
No, no ActiveX. It's just a regular FP control, but is very configurable through the property nodes and Properties dialog. It would take less FP space than the cluster of controls that I suggested above. It's a good solution, and only a little more involved to set up. I can help you with details if you get stuck. PM me if you need help. 
Awesome! I'll take a look at both solutions and see which fits my vision the most. Will let you know if I have questions!
By the way, totally unrelated question, but do you know how I should be configuring my classes so that when running code, the whole library doesn't lock up, eg I can make new VIs within a class that has running code?
Create a multicolumn list box with 2 columns. Set it to select entire row for cosmetics. Edit the shortcut menu (Advanced-&gt;Run time Short cut menu) for the control to have a Configure and Remove option. Leave vertical scrollbar as automatic and hide horizontal scrollbar. Resize the two visible columns so that they have enough space for what you need to show. Either add an "Add..." option to the right click menu or create a separate "+" button. Use event structure to capture the right click menu selections.
What is the point on your top loop of putting a 0 on the shift register and then wiring it straight through? Did you mean to do it that way? Or was the tick/inter supposed to be wired to the shift register? 
That’s why it climbs, because the wire is not connected well most probably
Yeah that's my guess, after the subtraction they should wire that to the output shift register. Then it should be what they expect. 
Yes! At this point it also worth noting that in the lower loop the loop timer is not used well. When programming FPGA you always put your loop timer in a flat sequence structure and put the actual code in a separate frame. Otherwise your code will execute twice as fast as the desired rate!
hahaha oh man I'm an idiot. 
Hahaha we've all been there. :) 
Yea I was really confused by the fact that the signal going into this AI didnt look mis-sampled at all... I think I'll post on here instead of calling NI support... 
Why would you wire ticks per iter result to the shift register? Wouldn't you wire the output from Get Tick Count? This ticks minus last ticks is how many ticks this iteration took, right? Really, what you want is what was the tick count before this operation and what was the tick count after this operation. Subtract those and that's how many ticks your operation took. The problem with the attached code, other than being wired wrong, is that is that the Get Tick Count can theoretically occur before the read and write, after the read and write, or between either the read or the write. In my opinion, FPGA is one of the very very very few places where it's okay to use a sequence structure to control order of operations. The above code is the perfect example why this would be a good idea. 
&gt;Why would you wire ticks per iter result to the shift register? Wouldn't you wire the output from Get Tick Count? This ticks minus last ticks is how many ticks this iteration took, right? Yes you would be right. That would be the best approach to get ticks per iteration. To have the new, minus the last ran one. And yes in terms of timing it won't exact, similar to what the other guy replying to me was saying. 
Would you mind explaining this more? I time FPGA loops this way frequently and have never had a problem. 
Loading a library locks it, even if it is in another instance, or context. You need to unload your library to edit it, which means stopping all code that loads it. This also is a pain for things like XControls which get loaded into memory whenever an copy of an XControl is on a VI, even if it isn't running. https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000kFKSSA2 https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P704SAC There is an idea on the idea exchange to improve this but I suspect the effort in redesigning classes to support this would be a large undertaking. https://forums.ni.com/t5/LabVIEW-Idea-Exchange/Classes-should-not-be-locked-when-added-to-multiple-targets/idi-p/2111416
There's several examples of MCLB stuff online. Here are two I did one for resizing control columns... https://forums.ni.com/t5/Community-Documents/Table-MCLB-Tree-Column-Resizing-Utility/ta-p/3737755 and one for performing item selection using the glyphs... https://forums.ni.com/t5/Community-Documents/Listbox-Multicolumn-Listbox-and-Tree-Item-Selection/ta-p/3508098 Neither demonstrate the array of cluster and item selection I described but they show the MCLB control and some basic stuff you can do with it. Oh and a virtual MCLB as shown on LAVA. https://lavag.org/topic/15289-virtual-multicolumn-listbox/
Oof. Definitely a nail in the labview coffin
[Oof indeed!](https://cdn.glitch.com/8ef35b11-8c6c-44bb-afb4-a38b833d3c8b%2Foofd.png?1529987683042) You have oofed 1 time(s). *** ^(I am a bot. Comment ?stop for me to stop responding to your comments.)
See details at the bottom of the page http://zone.ni.com/reference/en-XX/help/371599L-01/lvfpga/loop_timer/
I had no idea this subreddit was this busy, it's fantastic! I made this VI because I have a more complicated one that was not doing what I thought it should be. All this VI (the screenshot one) does is load samples from the AI to the FIFO while the host reads the fifo. When I saw the tick count rising I immediately jumped to the idea that there was some info about the configuration of a fifo or something I had missed... thankfully it turned out to be a real dumb noobie level mistake. I also drop the user led flash loop down because I guess I just get a warm fuzzy feeling that I can glance over and confirm it's running in the event I'm not seeing expected waveforms or action on other instrumentation. it's pretty dope you guys are on here, definitely plan to post up more often and take a look to see what I can contribute...
For the sake of accuracy, it won't run twice as fast, it will just execute twice during the first iteration. Also, a flat sequence **where the loop timer is the first frame** is the remedy, not just a sequence structure.
Have you checked [Monster.com](https://Monster.com)? There are plenty of positions with Labview in the description. Often, these positions have either a test or engineering bent. Before learning labview I did a lot of work in C, Fortran, and Perl. One unusual thing about labview, programming wise, is that execution is implicitly concurrent, or parallel. Like Verilog for FPGA's, Labview's programming paradigm is unusual - certainly not procedural, and will probably expand your view of how execution can run. 
LabVIEW is used in many industries. For example, SpaceX heavily uses LabVIEW for many aspects of their work. Programming environment specific knowledge is specific to an environment/language so there are things you will learn in LabVIEW that does not carry over. However that is true of C, Java, etc... Good programming principles apply across all languages. Learn to be a good SOLID principles developer, and that skill applies in more or less every platform including LabVIEW. The implementation may be different but the principles are the same.
It depends on what you want to do. LabVIEW principles carry over in general, but you will likely have a lot of trouble convincing hiring managers in some fields. I work with a lot of LV devs, and in transferring to other positions they often find that they're viewed as entry-level even with significant LV experience because it is not viewed as highly as C#, Python, etc. in the domains they are applying to. An example of where you might have trouble is in transitioning to a backend dev role. If you have any desire to work with hardware long-term, LV will be helpful. Common places to work with that are hardware manufacturers, defense contractors, and labs.
LabVIEW will ruin you. You will either really like it and only want to use this language or you will not like it and as mentioned you will have a hard time going back because text based people do not regard it.
yea, it's doing it because of the way it is. lol. dumb mistakes are nice though because they have easy fixes!
I'd argue that learning LabVIEW actually makes you a better developer in other languages. Learning to write good, solid G code will greatly deepen your understanding of parallelism, and many of the programming paradigms (Queued Message Handler, Finite State Machine, Event Based FSM, Pub/Sub, Actor) are used the world over in service oriented architectures at the giants eg Google, Amazon, etc. That being said, you need to commit to actually learning how to write good code, as often times people simply get frustrated and make a spaghetti of wires, when they could have written good code with a little foresight. That's my 2c, anyways. For context, my work with LV is controls oriented medical device programming.
Got it working, thanks again for the advice.
This worked like a charm, the user is very happy with this new way of doing things. Thank you!
&gt;it's sad to have to move from something brilliant and intuitive to something less so. Python is about as intuitive and brilliant and beautiful as you can get. If you don't think so, then you should really spend some time doing more traditional programming. If you aren't comfortable with *Python*, of all things, then you really can't call yourself a programmer. It is pretty much executable pseudocode. 
Used to work for NI. I'd definitely say that most people who use LabVIEW are primarily scientists/engineers in their job, but need to use it as a tool. As such, their code isn't always great but it gets the job done. However, for large projects, you need someone who knows what they're doing. Especially when it comes to embedded control, etc. I only know one LabVIEW programmer who's come from a CS background, but get the idea that Alliance Partners (NI Endorsed LabVIEW contractors) like these people. Someone with a strong grasp of programming principles will stand out from your average LabVIEW dabbler.
I call myself Seán ;-) When I look at labVIEW, I can see everything at once, when I look at python I have to scroll up and down etc. Python is fine but I prefer LabVIEW's graphical approach. Maybe that's just me maybe not. Rather than being insulting, tell me how many years you have spent programming in python and LabVIEW and why you think python is more intuitive and brilliant.
You are just not a programmer. Sorry. Go read a Python book and then you can have a more informed opinion. Labview is great for writing something that lets you interface with hardware using a simple UI. Other than that, like writing binary search, or deep learning, or matrix multiplication, or any serious data analysis. You will want to learn a traditional programming language. I strongly recommend you do this so you are not trapped in one extremely idiosyncratic environment for your entire life. Even traditional programmers shouldn't just know C++ or Python. They should know a couple of languages. Good luck I didn't mean to be insulting sorry if I came off that way.
Not to worries, misunderstanding. I can program in C, C++, machine laguage, VHDL, python, php, node.js, etc. Right now I'm doing some bash scripting. When it comes to embedded etc., LabVIEW is completely inappropriate. What I am saying is I like the method of programming using a graphical language, I would prefer to program in python using a graphical frontend instead of text, or at the very least it would eb interesting...
It is never too late when you have something useful to add. Thanks I will definitely try this in the future. 
Maybe you are a very visual thinker. My boss is like that and it drives me crazy. I strongly prefer verbal / text in my thinking etc.. My guess is you would kill me at chess. :)
http://www.ni.com/academic/students/learn/ Further, you may have access to more free training depending on your University's setup and when you bought LabVIEW: http://sine.ni.com/tacs/app/main/p/ap/ov/lang/en/fmid/497 https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P9aiSAC 
Thank you. I have seen almost all of the free instructional videos on the NI website, but I will look into the student licence. 
What type of detector are you using and what is the beam diameter? We have a photodiode beam detector in our lab and there is no need to move it as it can capture the full beam diameter in one shot. It spits out a 2-D array of pixel intensity values, so to get a 1-D beam profile you just need to take a slice through the 2-D data...
Not sure what you mean by "calling a control" or "calling a file". Can you explain a little more?
Sorry, I was meaning like a .ctl file. I have a typedef control saved as a .ctl and I know the file name, and can open a reference to it, but I can’t actually grab the cluster data from the control. I’m basically looking for a way I can use a .ctl the same way you would by copy and pasting or dragging it to the VI, but by opening the file reference somehow. Does that make it any clearer? It’s a little hard to explain lol
LabVIEW is a strictly typed language. Let's say you can some how using the path of a control, call it and get access. What data type would the terminal be in the block diagram? Remember it needs to be known at runtime. This is one reason why you can't do what you want this way. There are things you can do to replicate this type of funtionality but it usually relies on variants and pulling things out based on names or tags. Parts of the structure of the days must be known, or else you'll be coding in interesting ways with recursive parsing of the structure.
So realistically I’d be better off just parsing the data using another XML parser, because there’s no way to dynamically call a control? Thanks! This gives me a great place to start.
https://imgur.com/YC4coD5.jpg
Well I can't say for sure what you are better off doing since I don't have all the information. Maybe you are loading just one of 3 different data types, and then you can try to convert from XML to each of these. Two will generate an error and one will work. Of course at that point we still will be strictly typed and you'll have to work within a case structure where each state is for each of the 3 clusters. I've also seen state machines where you'll have 3 states, one for each of the data types you might load. Each state can load and handle the data differently. But what you can't do is have a single subVI that loads one of the 3 cluster types, then returns a single indicator out that changes depending on what was loaded.
Sorry, I was not paying attention. I am looking to determine the beam size. I'm using the stepper motor to move a razor edge in and out of the beam, to produce an x,P power series that I can fit to a gaussian function and determine beamsize from that. 
Why do you want to use a type def control to parse data from an XML file? What is it that you’re trying to do? Maybe give an example of what you’re trying to accomplish and we can help?
Hi, as a control systems scientist working in a laser institution... I do that a lot :) If you have a choice, I actually recommend you to use a camera for beam profiling. Doesnt give you that accurate data, but a pretty nice quick estimate. Here is code how to do that [https://imgur.com/a/MPIPfcz](https://imgur.com/a/MPIPfcz) (from a new book I am publishing in a couple of weeks) Otherwise I would start with the stepper code. Standa motors are not the most pleasant to use (looks like STANDA, no?) but ok. Use the simple example. (1) Modify the example so the motor drives n steps (should be one movement command). 
Hi, lucky you. Control systems engineer at a laser research facility here :) First of all, consider using a camera for your beam profile. Doesnt give you very strong intensity information, but a good estimate and is much easier to implement. Here is how that would work (screenshot from my new book, which I wiill publish in a couple of weeks: [https://imgur.com/a/ijFE8sS](https://imgur.com/a/ijFE8sS)) Otherwise, you will just have to do it step by step. STANDA is not known to have good software, the problem with that library and the motors is that they keep internal states. So if you don't close a connection properly, remove power unexpectedly etc.. it can get stuck. When you can't communicate, close LabVIEW (the entire process) and their software UI tool. When in doubt, power off the computer. So let's look at the code. What I usually do with "my" scientists, is to write little test programs for their instruments. We identify what we want to do, and write independent components. For your standa motor, you want to do two things: move the motor, and read the status. We can actually get by without the status, but let's roll with it. You are lucky, I actually have code for a STANDA motor in my folder. A snippet that moves a motor by a defined amount of steps looks like this: [https://imgur.com/a/h1WtdXC](https://imgur.com/a/h1WtdXC) Implement this code. Then turn it into a subvi with the "delta" (stepnr) as input. Technically, you could build a loop inside of this code, but as I mentioned, STANDA software sucks. So it's a good idea to have the "connect" / "disconnect" code in a small little unit. If your user later unexpectedly shuts down the program, you won't get stuck because they didn't reach the disconnect vi. Build a similar subvi that reads the position. Do the same for your powermeter. Build a small, little independent subvi that does nothing but return an analog value. Then, it is time to bring those things together. Here is what I would do: 1. Create a new vi 2. Build a loop that moves the motor from position a to b in small define discrete steps (so a for loop with say 200 iterations, and each iteration calls the move-vi we just build and moves the motor.) 3. Create an endless loop that reacts to the press of a button via event structure ([http://www.ni.com/white-paper/3331/en/](http://www.ni.com/white-paper/3331/en/)) Copy the loop from step 2 into that event and remove the "stop". Now you have an endlessly running program that drives a motor from a to b whenever your user clicks a button 4. Now make a button that lets the user drive back 5. Now let's add the powermeter.. You have a subvi that outputs an analogue value, right? Put it into the loop from step 2... That should keep you busy for some time :) Good luck!
This is a classical case for VI scripting. Although .. malleable vis ([http://zone.ni.com/reference/en-XX/help/371361P-01/lvconcepts/malleable\_vis\_intro/](http://zone.ni.com/reference/en-XX/help/371361P-01/lvconcepts/malleable_vis_intro/)) might solve your problem too, but I'm not sure. Would need to understand your usecase.
Thank you so much! I will try and follow your guidance closely. This is very helpful! I did encounter that problem with Standa, took me a good few hours to realize why the program stopped working after turning it off (the wrong way).
I’m not quite sure how this works. GXML requires the exact cluster type that you used to generate the XML file, so how does it get this from the enum?
This is a good idea, but there are going to be so many different cases (I believe there’s already more than 30) that it seems like it would be way too complex. I guess to clarify, I’m trying to use the GXML library to read one of many results file and pull out the data to generate a report. The problem is, it’s going to be a generic VI where a report could be needed for one sub test, or for the complete test. So I’ll need a way to pull out the data for any test (and they will all have different numbers and data types of results inside the XML) but I have to use GXML because that’s how the files are being generated.
Basically, I’m running a test (or piece of a test) on a product. The number of results and the data at types of them change depending on which test or sub test that I run. Each sub test uses the GXML library to generate a XML file with the results data, and I need to read one or multiple of these files to generate a test report. I have to use a type def cluster because that’s what GXML uses to generate the report, and you have to pass that same cluster (as a variant) to the parser to read the result file correct. This returns a variant from which the data has to be extracted. Realistically, I guess I don’t need to use the typedef as I could just use a variant, but I would need to be able to cast the data from the variant that the GXML parser returns to whatever data type it is. The only way I could think to do that is to have the type def cluster and use the Variant to Data function.
I was looking into scripting, but I couldn’t seem to find a way to accomplish what I need. This is the first time I have ever looked into scripting though, so maybe I just haven’t found it yet. Malleable VIs look promising! I’ll have to read more about them and give it a shot. Basically, all I’m trying to do is parse the data from an XML file (created using GXML). The problem is that the GXML parser needs to have a variant containing the cluster of data types (with the correct names associated with them) passed into it, and it returns a variant of the same cluster. Each results file will have a different number of results and the will all be different data types. The only way I could think to do solve this was to pass in the typedef to my VI, then use the Variant to Data function to convert it back to a cluster which I could break apart to use for my report generation.
Check out the help for the search parser VI, the important parts are "...so the order of the elements is not important, only their hierarchy... The XML file is allowed to have extra items not referenced by the type definition"
Oh, sorry! I completely missed that there was a control inside of that case structure. I was hoping to avoid doing something like this because the application is still being developed, so I don’t want to have to constantly update the case structure for new tests. But thanks, if it comes down to it this is probably how I’ll implement it! I’m curious though, instead of using the search parser twice and looking for the enum the first time, why didn’t you just use the enum to select the control cluster initially, then (since it’s a type def) use the quick parser since it’s faster? Or was this just an example thrown together to demonstrate how it could work?
I don't know what the sensor type is before loading the file, so I first search for the type and then load the file using the right typedef
Thanks to everyone for their help. I ended up connecting voltage to it so it think it had the sensor input. Thanks again
Looks nice, but JKI already has a really robust serialization library available on the LabVIEW Tools Network. It does JSON really well (among other things). 
Wow, I literally wrote a VI just like this one yesterday! Also using the OpenG Variant library. This one is definitely seems better though
Scripting and Malleable VIs are only for edit time operations. You aren't going to script code into a running VI. Same with the Malleable VI. You can provide the input to the VIM telling it what type to expect, and it can then return that type, but it only changes operation at edit time, because only at edit time can the type of any wire change. VIMs are fantastic for reuse. Write a VI for one data type and have it support many. But a VIM isn't the solution to your issue here.
Yeah I'd be interested in seeing this.
This sounds like a good use of the [factory method pattern](http://www.ni.com/newsletter/51506/en/). Basically create a parent class 'XML Parser' with a 'Generate Report' method that must be overridden by child classes. Then create child classes that provide the concrete method of interpreting that specific XML file and creating the report. From below, it sounds like you know the specific type of the xml file, that knowledge can be used to dynamically select the child class.
Interesting, this does look like a good use case of that design pattern. I just don’t have much experience with OOP in LabVIEW, so I’ll have to teach myself a little about it before I can attempt this. But this definitely helps, so thank you!
Mhmm. I interpreted it as "spawn different report generators or other sort of typedefs" when creating the original GXML, aka in edit time.. but OK, you got a good point.
Okay! Would you prefer LLB, or just a zip file? I'll upload it to NI's user code website and share the link here
Are you aware of the ones that already exist on VIPM? I'm using MGI's anything to string and vice/versa methods. Someone else mentioned that JKI has a JSON one.
I haven't seen MGIs stuff, my main concern in development was being able to see everything, down to the last SubVI. I started work on it in early 2016, when JKIs solution wasn't around, and the few I saw were password protected. Plus, the ones I've seen offer very little by way of customization, so if you throw a complex data structure at it, your computer may choke. My solution has the best of both worlds!
Came to say this too, JKIs serialisation to JSON I'd extremely robust, quick and can be passed over PSP variables. I use it with cRIOs to remotley update configuration at runtime without stopping the cRIO.
JKIs solution accepts anything, except for a class. However there are others out there that are paid versions that do accept class data
Sounds like OOP would help here. You could pull your data from your XML file, a part of that data points to a class, load your class from file and cast to that class type. You can then override a method in your base class for loading a control, each class you make can point to a different control and every VI that uses that control can be overridden for the specific control type. Take a look at OOP if you haven't, I don't think I could program LabVIEW any other way now.
But does it work in NXG? Asking for a friend.
You can use an event structure and then define key pressed as an event. You can then use a case structure to call a vi for a defined key combination. 
I've started using G Craftsman's JSON Object Serializer and it seems to work very well. It is an extension to JKI's JSON toolkit. It is a paid library, but not too expensive (you can easily burn more than the cost trying to develop something similar). http://www.gcraftsman.com/products.html
Need some more info from you here. Typically a chart is used to show data vs. time, like an old strip chart recorder (an array or cluster input to a chart gives multiple channels on a time scale), while a graph can show X vs. Y. Your X can change dynamically or you can explicitly set it using property nodes - I'm pretty sure there is an X scale value property node.
A few things to clarify: graphs are to show a certain segment of data - with recurring calls it “overwrites” the previous lines thus usually arrays/waveforms are connected to them. You can connect scalars but it will only show a dot all the time. Charts “keep” the previous data and append the new thus we usually connect scalars or arrays. If you build an array from different channels and connect them to a chart it will interpret them as single channel values, in other words it interleaves the channels into a single one. If you want to show multiple channels on a single chart you use clusters to do that. Also scalar practically means that it is not an array. What you are looking for is the formatting of the X axis that can be set with either property nodes or with the right click menu. The “From DDT” VI that converts the dynamic data type to a set one is ignoring samples if you set it to scalar and the source of your data is an array/waveform type. I suggest you read some more about data types in LabVIEW and stop using DDT. DDT was designed to simplify data handling on the other hand many people find its concept more difficult to understand than working directly with various types. Use arrays and waveforms, your life will be much easier.
Take a look into subpanels. These a great for switching to a different subVI from a main VI, whilst leaving all subVIs running and performing their intended function
If you build an Interop assembly DLL you need to make sure that the LabVIEW interop DLLs are placed in the same folder as the DLL you've built or it won't load. The DLL you're looking for is called "NationalInstruments.LabVIEW.Interop.dll" and should be located in C:\\Program Files (x86)\\National Instruments\\LabVIEW xxxx\\resource\\. I normally just copy the one that's in there.
Am I having Deja vu or something? I'm having trouble following why your event structure exists. If you simply want to save your data as an array such as: Sample # | Data ---|--- 1 | Data1 2 | Data2 3 | Data3 You need to build the array (0th element x-axis, 1st element y-axis) and then feed it into a graph - not a chart. As for exporting, a simple array works well in the "write to spreadsheet" subVI. You need to specify some parameters (tab-delimited, comma-delimited, append vs overwrite, etc.) but it just spits out a text file you can import into Excel or your drug of choice. https://zone.ni.com/reference/en-XX/help/371361J-01/lvconcepts/types_of_graphs_and_charts/ 
The event structure runs when the "Start" button is pressed, otherwise it just cycles to the end. I honestly don't know if it's needed or not. Much of this VI is based on an example someone else made for running the stepper, and I added the VNA functionality to it. My understanding of LabView is not great. I've been trying to use a graph, but the graph take in array input which means that I would have to place it out of the for loop. This makes it so the graph doesn't update until all of the iterations have been run through, and I want real time graph updates. Thanks about the spreadsheet idea. :)
Your loop always stops on the first iteration. Maybe the DAQmx VI's need more than one shot to sample the data?
You can build an array inside the for loop and append data to it using shift registers with each iteration. The only reason you're writing an array is that you're using what are called "auto-indexing tunnels" on the border of the for loop, which is building the array for you and spitting it out at the end. http://www.ni.com/getting-started/labview-basics/shift-registers#Shift%20Registers You can start with an empty array outside of the loop, add your data and iteration # each iteration ("Append element"), then feed the subsequent array to both your graph and the right shift register terminal. 
Been a while since I did daqmx but have you tried changing the subvi that says "Counter 32 1 channel 1 sample" to one of the other options? I think it's just getting one data point at the moment.
Look up the CLD practice exams from NI. I think solving those will be great practice. 
What industry and application? 
Manufacturing engineer at a fire protection company. They main use LabVIEW for quality test between the design processes
Alright! Will do! Thanks!
What existing code and architecture do they have? Do they use NI TestStand? Is there a team you will work with? Which source code control do they use? What database is used for DUTs and results? 
Thank you for your help! After lots of hours, I have something I'm pretty darn happy with. Wish I didn't have to go through so much pain to figure out how to do things which SHOULD be simple to do, but it is what it is. Thanks again!
Thx for takin a look! I was so tired when I posted this I didn't even realize I didnt click "save" on the comment I wrote to explain it before closing chrome haha. Turns out, there's nothing wrong with the code (that constant wired to stop was just an edit I made on the example because I only wanted to try to catch one burst of pulses initially). The issue is simply that there's nothing to synchronize the start of the pulses with the acquisition... I dont think it would be possible (please let me know if you think of a way) to acquire each burst including all pulses without another trigger signal (which I later found) - the trouble is that you dont have any way of knowing if the Start Task vi is run while the pulses are already occurring... 
thanks for lookin! see my other comment for why I'm a dangus...
I feel that getting a 3rd party DAQ or other communication protocol device talks up almost a full day getting it to work with labview. After I'm able to connect with it, writing the code seems like the easy part. So I guess what I'm saying is, learn the VISA and COM port setups, because after that, it's smooth sailing.
Yeah, so I figured it out. It was me being clever. Get's me every time. I already had this framework working on PC applications really well with no issues so I stripped out the non-cRIO stuff and plopped it in RT and let 'er rip. To make things really easy on myself, I created a setter/getter template for the internal data of the classes that relied on the front panel indicator for the name of the property and it's type. Front panel indicators don't exist in RT unless you're running it in the IDE or in debug mode. I spent five hours on my stupidity yesterday. Brian Kernighan wrote "Everyone knows that debugging is twice as hard as writing a program in the first place. So if you're as clever as you can be when you write it, how will you ever debug it?" and it rings ridiculously loud today. I just need to get better at automating VI building to handle my templates so I stop trying to take shortcuts there.
Web scraping. No hardware needed. [LabVIEWtest.blogspot.com](http://labviewtest.blogspot.com/2012/04/website-scraping-with-labview.html?m=1) 
When I started I made some easy conversion program since I am a chemist. Just like VIs that could make my lab calculations fast and easy. 
Do you have any hardware? What is your skill level in general? \- If you aren't, make sure you are very familiar with state machines and producer consumer patterns. Maybe functional global variables and OOP. So those CLD type topics. \- If you have the application builder, try to use it. Learn how to make projects and the differences between llbs and lvlibs. \- Some small projects handling configuration files and xml files and generating result files. Look into EasyXML by JKI, it's neat. Try writing Excel-files and TDMS files. \- If you have ANY kind of hardware (even some IoT thing helps): implement something that requires a VISA driver. So open a connection, communicate, parse the communication, close it. Measurement devices with SCPI communication (osci, function generator, power supplies) are also often used. \- If you have no hardware, but 2 computers: Write a chat program (via TCP/IP) between them. \- If you have Vision Development Module: Use your laptops webcam, write a small image processing application. Like a barcode scanner. \- try to interact with some DLL (C dll, .net component).
https://zone.ni.com/reference/en-XX/help/371361H-01/glang/write_characters_to_file/
There is a "write to spreadsheet file" vi that will generate a .CSV file for you. 
Ok thank you
What does it do if you *only* create the queue? Does the VI finish?
You will very likely have a faster and better response over on the NI forums, linked in the sidebar.
I found a Powershell script that will comb your registry for all the ActiveX ProgIDs and put it in text output for easy searching.
http://www.ni.com/academic/students/learn-labview/
http://www.ni.com/getting-started/ If you're academic, you may have access to additional training: https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P9aiSAC If you're with a company, you may have access to training also: http://sine.ni.com/myni/self-paced-training/app/main.xhtml
Refnum Variable , typecast later. 
Depends on what you are doing but it sounds like it would be better to have python as the server. 
Just some simple image processing, trying to get around the vision toolbox. 
How about zeromq? [http://labview-zmq.sourceforge.net/](http://labview-zmq.sourceforge.net/) [https://stackoverflow.com/questions/6596974/send-data-from-labview-to-python-and-get-back](https://stackoverflow.com/questions/6596974/send-data-from-labview-to-python-and-get-back)
I've never tried it but I've heard good things. Let me know how it compares to TCP. 
In looking at it it looks like a wrapper around TCP, so I'm guessing it will just add overhead. In Labview you can use the typecast to convert your 1D array to bytes for free, and in python you can convert it back using `struct`
Wire an empty array constant to the output tunnels? https://imgur.com/EB5zv0G
https://zone.ni.com/reference/en-XX/help/371361J-01/glang/initialize_array/ you can tell it to make a new array every time with this function, or you can wire in an empty array as /u/Muun suggested.
I found this: https://forums.ni.com/t5/LabVIEW/How-can-TDMS-write-generate-error-4/td-p/3757536/page/2 Didn't see much in the way of resolution there. Is this on an embedded target? Saw a few mentions on some other forum posts about seeing this on the Linux RIO targets.... but no solutions... Sorry I couldn't be more help but if I see anything new i'll post back here!
Are you using the tdms file as the buffer? Any reason for choosing that instead of a text or binary file? 
Perhaps this would be a case for a triggered acquisition using an analogue trigger? So once the temperature rises to X, begin acquiring. You could make it a buffered acquisition and process the data while reading it. There will be some good white papers on how to implement it, plus some decent examples (in LabVIEW press Help &gt; Find Examples). Sorry that isn’t more helpful, hopefully it’s at least a nudge in the right direction!
That's a pretty clever use of TDMS files... The ability to concatenate TDMS files is pretty cool, but I've only used it a couple of times (my impression is that it's not a well-known feature in the community). I may actually have to borrow this strategy. I've also found the "in memory" TDMS creation to pretty useful as well. Never used the asynchronous TDMS functionality so I can't speak much to that.
Nice. I work with TDMS files extensively, and really like them. Recently I was frustrated by the fact that there's no "Delete Property" option... but other than that I'm a pretty big fan. Great introductory talk! When combined with DataFinder (and Data Plugins) you can do some very cool stuff as well.
It would be useful to boil your code down to a simple example and post it. While it's OK to open multiple references, I'm not sure why you would. Just lay down the wires, it'll be easier to trace later. I'm guessing your issue is unrelated to the multiple opens. It's probably a library mismatch or something that isn't supported in real time.
I'll try to throw together something to illustrate the issue, but the reasoning behind having it in two places without "laying down the wires" is because the architecture doesn't support "laying down the wires". The architecture is actor-like, and because some deployments might have just acquisition, some might have just control, and some might have both, we broke the acquisition and control into separate actors.
you mentioned in a comment this is within actor stuff on an RT target. good luck! lol. just to double check, does your open fpga reference try to run the bitfile? if both try to run the bitfile, that might cause issues. how are you passing around the fpga reference? is it one of the dynamic references? is it in a typedef control? i could see that causing some deploy issues, as labview really seems to struggle with typedefs inside classes.
NXG uses package manager, not the tools network, and I believe JKI/OpenG/MGI/etc. have their own category in there.
Labview is a really fast way to develop IO for hardware. It can talk to anything. If you wanted to test a hundred hardware configs you would be so thankful for it, at that point many people just say " this is working great! Screw it I'll do the whole thing in labview". It's got effective pre built interfaces for the hardware you want to use. Source- I'm the lab tech at a school that teaches labview. 
Virtual Instruments are just the name given to every code block you write. A VI can be thought of as a function, really. However VIs can also have a front panel component that acts as your UI. LabVIEW can create a text editor, synth, calculator, etc. It's a full-blown programming language. Not sure about making an OS out of it, however. Some things about LabVIEW: - It's a strictly typed language. Type-checking is done at development time. - It's a data flow language. It's highly parallel and handles all the threading for you. The data-flow aspect means that all the inputs to your function must be provided before the function (VI) can run. - It doesn't really have a garbage collector but does manage most memory for you. - It can operate with the .NET CLR and call C DLLs. It can also call python and matlab code. - It's a pass by value language but it does have pointers (data value references). - You can write object-oriented code with it. However, it does not support interfaces or any form of multiple inheritance. I program in javascript and LabVIEW. I can do almost anything faster in LabVIEW. It's great for prototyping. I also haven't seen another programming language that makes graphing so painless. Hope this helps.
This does help. If I understand you, it is another way to create a software tool, but seems to have a lot more streamlined abilities that allows you to create these tools quickly, and integrate these tools with hardware, without having to download the libraries, or packages you would normally have to download and use in a python program? &gt;It's a data flow language. It's highly parallel and **handles all the threading for you**. The data-flow aspect means that all the inputs to your function must be provided before the function (VI) can run. Since I am a new programmer still, that is something you would have to code into Python or C++ or something? How to thread the data coming in from hardware? I may not be understanding your statement. &gt;It can operate with the .NET CLR and call C DLLs. It can also call python and matlab code. Is it able to do this because it has all the libraries associated with DLLs and .NET CLR? Do these come all preinstalled with LabVIEW? &gt;I program in javascript and LabVIEW. I can do almost anything faster in LabVIEW. It's great for prototyping. So you can use it to work with websites? Or, can you do other things besides website with javascript? &gt;I also haven't seen another programming language that makes graphing so painless. Graphing in a mathematical sense (parabolic functions, etc), or plotting data? Again, sorry if these questions are juvenile, I'm still generally new to the computer science world.
It is free for student (not sure how since they've changed a few things) and I think the home version is $50 and can build EXEs. The several thousand dollar versions come with lots of support and online training.
Man, that sounds pretty great. How does it compare with rOS (robot operating system)? VFD? It's so weird that I've never come across LabVIEW at any time while researching robotics...
If you are a student you can get a license from here for 6 months. https://forums.ni.com/t5/Projects-Products/Free-6-Month-Evaluation-of-LabVIEW-Student-Edition-for-at-home/ta-p/3497362 If you are a student when that expires you can get another 6 months until you are no longer a student. Or for $20 you can buy the student edition from studica that doesn't expire. Yes home can make executibles that can be installed and ran on computers that doesn't have the LabVIEW development environment.
It is not used so much by your hobbyist roboticist as it has a high barrier due to the licence cost. However in an industrial environment it is used pretty heavily for automation (robots building things). Also the smaller hobby robots usually have embedded devices running them (arduinos and raspberry pi). LabVIEW is not hobby friendly as the embedded controllers are powerfully and often come with an FPGA but are expensive which is again why you mainly see them in industry. They are getting better for the hobby market (FIRST robotics, LINX etc) but it isn't their primary money spinner for National Instruments. When you see a serious robot it often has LabVIEW running it, I think the latest vehicle that got the record speed for the hyperloop had a National Instruments embedded controller running things. When you're talking robots your talking about sensors (Data Acquisition, Vision inspection), data processing and control (PID, State Space, Kalman filters, AI etc) and actuators (Servo motors, Linear actuators etc). LabVIEW does all of these very well ( except AI which you'd have to roll your own or use a Python Node). The drawback is the cost, which isn't a concern for business but definitely is for hobbyists and people with research budgets.
I write many complex applications in LV for multiple users. It's great for data collection, but I also use it for very rapid development of complex analysis programs, which include some fun UI's. With the Event Structure, you can make your graphs and pictures very active, responding to clicks, double clicks, etc. to show additional data, open additional more detailed windows, zoom/unzoom. It's really fun to make UI's that people like to use. And once you own the Application Builder, you can give you applications to as many users as you like without charge, assuming you don't use the more expensive tools like Vision Builder, etc. It takes a line-coder a good while to adapt to the graphic style of LabVIEW, but once I did, I felt like my productivity went WAY up. I really try to avoid writing line code now. LV is so much more fun and is the secret sauce that allows me to support and build many more tools than I would with C/python/etc.
This is a good list, but there are a few pedantic clarifications that I'd like to make: &gt; It's a pass by value language but it does have pointers (data value references). DVRs aren't quite pointers, in that they're still mutexed. You won't get pointer-like performance out of a DVR. &gt; You can write object-oriented code with it. However, it does not support interfaces or any form of multiple inheritance. Good news! As of LV 17.5, you can use Malleable VIs to create something that works a lot like interfaces in Java: [Using a Malleable VI for Different Classes](http://zone.ni.com/reference/en-XX/help/371361R-01/lvconcepts/vim_adapting_to_input_classes/)
Think of all programming languages as tools. A certain tool might be best suited to a certain job and thus make it much quicker or easier to do, but another tool would be able to reach the same end product given enough time and effort. Labview is a tool that is excellent at communicating with hardware which ranges from ultra-low-cost Arduino microcontrollers, all the way up to very expensive FPGA base National Instrument hardware. If you want some hardware that you can control with a PC, or you have some measurement equipment with USB/GPIB/LAN and you want to log it to a computer, Labview is an obvious choice. You can even fairly quickly automate a whole experiment. So in your case you could make an automated test procedure that takes force measurements, does image capture, measures extension etc etc, and logs all the data in a folder all time stamped together. Having said that I wouldn't use it to analyse my data. It's capable of doing it, but Mathematica/Matlab/R/Python would all be better choices in most cases. 
I did not know that about the malleable VIs. Need to play with it, but it could dramatically change my OOP designs.
Title was supposed to say "Help making code run faster"
I think you will have to include the [2DGaussianModelwithOffset.vi](https://2DGaussianModelwithOffset.vi) also for us to run your VI unless that is part of a toolkit I don't have. When you say very quickly how fast? How fast does the larger array run? If you want to reduce the resolution of your array you should use the Decimate (single shot).vi or the continuous one depending on how your data is coming in. There isn't a whole lot going on here and 1000x1000 isn't that big so this shouldn't be that intensive. What is this running on? I would be curious where most of the extra time is being spent? If it is with the array manipulation stuff we may be able to speed that up a little. But if it is the acutal curve fit itself it may be hard to get much more out of it.
This is great information and good to know. Especially where it is specifically used. Thank you.
Do you mind sharing the specifics of one of those programs? Just to get a better literal grasp of what can be made. Is there a specific 'look' that LV gives to their UIs or can you import your own images from Adobe Illustrator or something and use those as the shell of UIs?
&gt; Labview is a tool that is excellent at communicating with hardware which ranges from ultra-low-cost Arduino microcontrollers, all the way up to very expensive FPGA base National Instrument hardware. Great information, thank you. &gt; So in your case you could make an automated test procedure that takes force measurements, does image capture, measures extension etc etc, and logs all the data in a folder all time stamped together. Got it. &gt; Having said that I wouldn't use it to analyse my data. From what others have said, they would agree. It seems best for automating the collection of data, but better to use something else to process this collected data. Thank you for sharing.
If you're using a loop, you can grab from the iterations terminal - you could also use the Elapsed Time VI to spit out a time value for when the user starts and stops... LabVIEW has some great built-in timing functions.
You can use the value Property Node for your graph, then Array Size on the x-axis data to get number of data points plotted. Also seems unnecessary to switch to a matlab script to do a simple divide?
Or if your y data is a scalar array that is fed into a waveform chat you can just use the "number of array elements" node from the array palette
I have a simpler way of asking my question. The array is so large it takes over 30 seconds for the program to calculate. However, is there a way I can set the program to search for an area of interest in the array first? Perhaps the center? If this is feasible, it would be preferred to manipulation to the actual array. 
So many apps. I do data collection on multiple cameras while tracking objects and measuring motion in real time. I have apps that collect data from custom-made USB interfaces to real-world sensors, apps that do complex and multifarious analyzes and visualization of the data collected. Each app is about 100 vi's or more with many user-visible windows for configuration of the data collection parameters, or visualization of particular analyses. There's an app that present the user with a graphical programming "language" to sequence the occurrence of events in the real world and decide what to do next based on the response. That was a 2-year project and the most fun one to do. So I have been programming LV for 20 years, and have yet to come up against a problem I could not solve with it. Yes, straight LV windows have a particular look if you use the controls and elements native to the IDE. But vi's are infinitely customizable with Illustrator images or any other images you care to add. And there are lots of sets of controls designed by third parties with very different looks to them available for free or for purchase. Or you can design your own controls and edit them in the built in editor that comes with LabVIEW. 
Yes. That's just typical application
Don't forget to mention how easy it is to do multiple tasks in parallel. I don't know of any text based languages that make it as easy to do QMH/event based state machines/actors
Yeah very dumb that this feature is missing. NI's forums still get posts all the time like [this one](https://forums.ni.com/t5/LabVIEW/How-to-do-a-Repair-Install-with-NI-Package-Manager/td-p/3643746) asking the same question. It's possible NI didn't think people used this feature and would save some development by not including it, but I used it plenty of times. Either that or they plan on adding it and just haven't yet.
Because LabVIEW is closed source and so will always be lagging in development.
I'd just like to know WTF was so lacking with VIPM that NI felt is wasn't worth using instead. I mean, it was already fully integrated, tested, and works very well. Sometimes NI's "slap out logo on it and *poof* I made this!" approach just doesn't work well.
I'm assuming by constant you are referring to what's in your control, which you are setting constant on the front panel? What you are doing should work, but it's going to take 65535ms to exit the for loop, and give you an array of 65535 elements, plus your constant array before calling the serial VI. Maybe you could tell me what your end goal is and I could help a bit more, because I doubt that's what you are trying to do? 
Yes, one element is constant, its value is 0.5, and the other one I want to vary 65535 times and then go back to 0 and increment again. My end goal is to have one channel PWM with 50% duty cycle, and the other one to vary from 0 to 100%, I want a ramping command in one channel and the other constant, I am using the iterator but I divide it because it has to be between 0 and 1.
Yeah, that's what I was describing in the edit. Just remove the for loop, and wire a single element into the build array with the constant portion of it. Then use the iterator from the while loop instead of the one from the for loop, as you are doing now. Can't mock it up for you right now, but hopefully that makes sense.
As mentioned by iYogurt, there is the producer consumer loop that would work for this but there is also a neat structure called a tag channel. It similar to a wire but does not enforce data dependency and can cross a while loop edge without issue. http://zone.ni.com/reference/en-XX/help/371361N-01/glang/channel_tag/ Hope this helps!
This is a re-phrasing of "use the producer/consumer architecture" but basically you just need to use a queue. In the synchronization tools panel. Since you're likely producing data faster than you'll be consuming it, you'll want to "enqueue element at opposite end" to always put the latest frame at the front of the queue. Then in your timed loop, read the first element, then flush the rest. Or you could put a timer in the producer loop to only send frames into the queue after 3 seconds have passed on a timer, and run the consumer in a normal while loop. The dequeue element function will just sit there until there is data to pull from it, so the while loop won't run any faster than you're sending data to it. 
I think this is the better answer because it points out data dependency which is a key concept that needs to be understood before someone attempts queues.
Thank you, this worked!
Thank you, this technique seemed to work. It took a bit of trial and error but the array is passed over by using the queue vi's. I appreciate it. 
I was able to get it to work using producer/comsumer loops and passing them using the queue vi's, but I appreciate the advice.
By the way, I've looked into 2d array binning and it seems labview just isn't that great for doing that for some reason
I'm not sure what you're trying to accomplish, but I feel like maybe you should look into using the vision toolkit VIs. Those VIs are designed to do analysis on images and that way you're just carrying around the reference to the image instead of the entire array of values.
I am taking a picture and tracking a beam in the picture to use for imaging under a microscope. The gaussian fit vi I have tracks the beams coordinates. But it's just slow because the array is so large. And honestly, i used this method because its the only way I could think of doing it without getting Ni vision. I had such a hard time getting around NI's website, I downloaded so many versions ni vision modules and they were either virtual machines that expired or didn't have the toolkit, so I just said nevermind and did it this way. It actually works well but just slow, if I use a very small image it tracks quickly. But I don't want to change the resolution of the picture that is acquired, just of the resolution of the array that runs though this portion of the program, if that makes sense.
Can you share a sample picture, by chance?
Maybe this: 2D array into an auto-indexing for loop decimate the 1D arrays take the output (also auto indexing) and rotate the 2D array throw it into a for loop and decimate it again rotate the output and it should be 1/2 the size Is this an option? Also, when it comes to tracking a beam you can have some fun on how you find the center of the beam. You can use statistical methods to attain sub-pixel accuracy. We do it all the time in laser line scan applications.
Yes, this is how it can be done and is probably one of the fastest methods. 
I actually followed yatty33 advice and ended up making it work, thank you iYogurt
Thank you!
there's also a /reinstall flag for overwriting as well... had this save ame a few times... 
And that is why you should be using a good source code control system and having regular check-ins. Personally I like Perforce Helix (formerly known just as Perforce), it is free for a small number of users. I know a lot of LabVIEW developers who like SubVersion, Tortoise SVN, or Git as well. As for recovering your VI: * Try the VI on other computers * Try the VI in a new version of LabVIEW * Try the following: * Go to Tools&gt;&gt;Advanced&gt;&gt;Clear Compiled Object Cache and clear all caches * Close LabVIEW * Reboot the Computer * Open LabVIEW * Verify there is not more than 12 kB in the Compiled Object Cache (back to the first bullet in this list) * Open the VI
Honestly sounds like you should be looking at filtering if you are actually trying to see a changing voltage number. Example would be a butter worth filter. If you really just want to coerce some range of input to 5 volts. I would use the “in range and coerce” then take the Boolean out of that to a selector that if true is 5V if false is either the input signal or 0 or whatever you want
I'm not exactly sure what you're asking, but it sounds like you might be describing a need for a digital input. It sounds like you want to take data that is close to 5 V (5.02, 4.98, etc.) and make them all just 5.0000V? If that is the case, maybe you should just be using a digital input. If you can give more information about what you are trying to do, we might be able to offer more help.
This is a very long-standing problem with LabVIEW. You are correct that the Vision Acquisition software is expensive ($430) currently. And there are not very many good alternatives. I wrote some wrappers for the DLL calls to the SDK of an industrial camera. Or you can use .NET to talk to Basler's pylon software. But there's a lot of effort involved. One way is to look for a camera that comes with software that will save images to a file, which you then open in LabVIEW. Very inelegant. 
NI's vision toolkit is made for industrial machine vision stuff. Robots moving to a position, calibrating based on real world objects, object detection, OCR, pattern matching, etc. It is the kind of thing that people spend their whole careers studying and NI has it in a toolkit that allows non programmers to get stuff done. It ain't perfect, and it ain't cheap but I think that is why NI gets away charging what they do. That being said if you don't want or need all that fancy vision stuff, and really just want to look at a web cam then do the data processing yourself there are a couple options. First you could look into if the camera has a stream that something like VLC can view. I think it was a .Net control someone used to take snapshots of an IP Camera, which then allowed them to do what they wanted. I can't find the thread but it was over on NI's forums. But the easier solution if it works is to just use the free components of the Vision Acquisition Software. Last I knew the [Vision Common Resources](http://digital.ni.com/public.nsf/allkb/5476BD570E87E0BA8625797D0071263C) are free to develop, and deploy with, and handles basic things like getting, reading, and saving images from an IMAQ compatible device. [Here is a thread on LAVA](https://lavag.org/topic/17114-image-roi/?do=findComment&amp;comment=105496) where someone mentions it and how to get and use the free components.
This. The free NI-IMAQdx drivers allow you to connect to USB2 and USB3 cameras: The NI-IMAQdx driver software is needed to use third-party image acquisition devices which use the following buses and standards: * Gigabit Ethernet Cameras Supporting GigE Vision * FireWire IEEE 1394 Cameras * USB 2.0 Cameras Supporting Microsoft DirectShow * USB 3.0 Cameras Supporting USB3 Vision Latest version of IMAQdx is included with this download (I think): [http://www.ni.com/download/ni-vision-acquisition-software-18.0/7552/en/](http://www.ni.com/download/ni-vision-acquisition-software-18.0/7552/en/) &amp;#x200B;
You're totally right about NI charging for stuff that people spend their carrers studying - I still haven' realized that I'm entering a world where stuff like this can be the main part of someones life. And I didn't know about these free components, I'll take a better look trying to make this up then. Thank you for helping me in this hard time.
Ooohh, that's cool. I'll use this link then, and connecting those cameras should be more than enough for the project. Thank you.
An instrumentation guy on my team told me a story about the time he asked this question to a friend of his. His friend said “Don’t do it. That way you won’t have to lie about not knowing LabVIEW.” There’s some truth to this anecdote. Almost all LabVIEW jobs exist because the previous person was horrible at writing LabVIEW code. That said, there’s a lot of folks writing bad LabVIEW code so there’s a big opportunity there for you to become highly valuable. Ultimately it really depends on your interests.
I make a really good living off of other people's bad LabVIEW code.
As someone who writes bad LabVIEW code, you're welcome. &amp;#x200B; BUT, it was great code at first then feature creep happened and my architecture wasn't adequate and I didn't feel like rewriting everything.
I learned LabVIEW basics as part of an autonomous vehicles course. We spent a couple weeks on it and moved on. I would recommend against a semester long LabVIEW specific course and instead use LabVIEW to implement your homework assignments in your classes. I finished my M.S. in electrical and computer engineering this year and my professors were willing to let me turn in either MATLAB or LabVIEW for homework.
NI-IMAQdx still requires paid licensing regardless of what camera bus you are connecting to (USB webcam, GigE Vision, USB3 Vision, IEEE 1394). Only NI-IMAQ (used with older Camera Link frame grabbers) and NI-IMAQ I/O (used for digital I/O line manipulation on certain frame grabbers and controllers) are free within Vision Acquisition Software. http://www.ni.com/product-documentation/53417/en/#toc2
If I was a masters in EE, I wouldn't even think twice about ever learning LabVIEW. 
What kind of course? My college offered the CLAD course, taught by a masters student who had interned at NI. I don't think it even ran the full semester and if you didn't totally screw off in the class you were nearly guaranteed to pass the cert. That was a great call but any more than that and I think I would rather have learned about robots.
Amen. The amount of spaghetti that's come my way has been awesome for job security.
https://i.imgur.com/hvGoJMU.jpg This is from the syllabus. I’m pretty sure the last topic is just informing us about the certification exam and preparing it to take it if we wanted, because nothing was mentioned about actually taking the exam, and I doubt the university would pay for that anyways.
You're singing my tune...
You're describing a digital input. I'm not sure what hardware you are using to acquire the signal, but once it is in LabVIEW, it should be represented as a Boolean datatype. I think what you are wanting is for that signal to "latch," meaning that it starts off FALSE, and when it is TRUE even once, it stays TRUE in your program for the rest of the run? To do that, put the Boolean input into a Shift Register. Use an OR gate on the output of the shift register and the current value read from the device. Wire the output of that into your shift register on the other side. Thus, once, TRUE, it will stay TRUE.
As in you wouldn't bother with it, or you would absolutely do it?
I'd take it. The course that introduced me to labview was by far one of my favourite undergrad courses. I use it a lot in my grad studies. (Physics) 
I have some code somewhere that uses a direct show DLL which may work. I know it did live previews, but not sure about a recording of it. I'll try and dig it out.
PWM based DACs are very imprecise. You gain nothing by using 16 bits. Get a real DAC with a resistor chain. I used [the AD5754R](http://www.analog.com/media/en/technical-documentation/data-sheets/AD5724R_5734R_5754R.pdf) with an ardino recently and I've been very happy with it. 
That's all covered in LabVIEW Core 1 and 2 except MathScript (I used to be an instructor). You can learn all of that in a hard 1 week grind. You're better off just shooting from the hip and learning as you go. There are other better things to learn in grad school than LabVIEW.
I don't really need to record it, just to show for the user - who's seen only the front panel - the images from the webcam so he can see what's going on with the Remote Lab. If you have anything that can help me with that, it would be awesome.
Correct, the big update is malleable VIs. Which are cool and cooler still in 2018 but not required for completing the test.
From what I read they look pretty cool, though I don't think at my level I would get much benefit. The most attractive feature to me by far is the ability to pull objects out of structures and retain the wires lol. Thanks for your input.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/slack] [Post to Slack from LabVIEW (x-post)](https://www.reddit.com/r/Slack/comments/99wfxd/post_to_slack_from_labview_xpost/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I use both labview and text based languages for work. Personally I prefer text based ones more, I find it easier to keep code cleaner and easier to organized, but that's just me. However, when it comes to data acquisition, and testing labview is king. I end up using labview for creating our testing software, and text based languages for everything else. Occasionally I'll also use labview for creating communication DLLs as well, since NI VISA makes it pretty simple to get communication working as well, and then throw that into my text based program. At the end of the day they can all pretty much do the same thing if you spend enough time with them, but for certain tasks one way can be a decent amount faster than the other. It all comes down to the project and what you need it to do. 
Look at the CLD and CLA sample exams. They come with problems you can try and tackle but also have a solution too: CLD: http://www.ni.com/gate/gb/GB_EKITCLDEXMPRP/US CLA: http://www.ni.com/gate/gb/GB_INFOCLAEXMPRP/US LINX is a good toolkit for working with Arduino and LabVIEW.
I'm not aware of any that exist at the moment. However, reading Slack's developer reference for webhooks: https://api.slack.com/incoming-webhooks It looks like the webhook is just another URL that you can POST JSON to. Should be able to make it work with the built in http client.
Thanks for getting back to me, Ill have a play around. Tokens: https://api.slack.com/custom-integrations/legacy-tokens
I've used ethercat with labview but it was with their own slaves so everything was plug and play. I know for sure that most of the CRIOs with dual ethernet ports like the 9068 have support: http://sine.ni.com/psp/app/doc/p/id/psp-1165/lang/en
I have used LV to get data from a Beckhoff PLC with a cRIO via EtherCAT. Do you have questions about it?
\- Can you tell me how many slaves were connected to the Beckoff PLC? \- How many parameters you could read using this method? &amp;#x200B;
No we were just using EtherCAT. This [KB](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019LeiSAE) describes roughly what we were doing. The usual problem people have is that the cRIO [has to be the EtherCAT master](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000P7DmSAK) unless you are using this [card](http://www.ni.com/en-us/support/model.crio-ecat-s.html)
Did you use the cyclic communication (PDO) or the acyclic communication (CoE)? 
PDO
Ok, this answers helped a lot. Much appreciated
About the first application, how did you configure it with labview? Was it a labview library or a beckoff automate? 
I didn't write the application, it was legacy before I came on board and it has been solid with no need for updates/ maintainence. I'm not 100% sure how the interface is, but I'm 75% sure it was a labview library. Sorry not much help, but I did want to share it is doable.
We tried the Ackermann Automation library (Option two) but apparently it's a very limited solution (we can't change the PDO Mapping on the fly and no redunduncy support). About our system, it's about 120 slaves with each slave sending about 10 values each 100 ms (approximatevly 12000 values every second) . Did you use the Ackermann Automation? 
I've used Maxon motion controllers over EtherCAT controlled by a cRIO. I used direct cyclic PDO mapping because I needed the speed that was not available on SoftMotion (LV motion control module). It was a pain to get working due to having to write the controller state machine in LV, but once that was done it worked beautifully. One issue I had was the cycle speed was limited to 1KHz due to the RT Scan Engine. The FPGA can run faster, but you needed another module for the EtherCAT. We were operating 3-6 slaves.
Were the slaves all the same? 
I think that might be overly complicated. You could make it work, I'm sure, but you'd have a whole set of new problems, like blocking the first loop until the macro loop finishes. Why not just create your own library that wraps the Queue functions. I'm guessing the reason you can't use JKI is because your company needs to be able to own 100% of the source IP.... In that case, create a VI that replaces the "macro" call with all of the actual cases you want to execute. Just insert them at the opposite end of the queue, in reverse order. The effect will be that they sort of "replace" the macro case.
You could use two queues, or you could use a single queue with a cluster of { string, \[variant array\] }. Then make functions to add strings to the queue with an optional variant(s). The dequeue would return the string and the array of variants. You could create a few utility vis to turn the variant array into various concrete types (single int, single string, ...) for more complex types cast the variant array right in the states case.
Thank you for the input. I have successfully implement this and will most likely write a small library to do some more as well.
I knew there was a simpler way. Thank you for the input. I will write a quick library to do the operations.
Yes, but configured differently.
[removed]
Are you using the scan engine? Because, there's a decent chance that, yeah, you're not scanning fast enough. I'd highly recommend FPGA mode or hybrid.
Yes when going through the crio wizard for set up, I selected scan interface. Would creating the project and .exe be similar to Scan Interface? I can attempt to look into it tomorrow.
The NI DAQmx driver makes the interface transparent for you - USB, PCIe, WiFi, Ethernet are all the same from a programming point of view. So use the DAQmx shipping examples. One specific thing for cDAQ is using the counters which are in the chassis, these are called internal counters. An other thing specific for enet cDAQ chassis is to Reserve/Unreserve the chassis from MAX. These are well documented just search for these on google. Side note: the Ethernet cDAQ is not really a replacement for FieldPoint as the enet cDAQ doesn’t have a stand-alone functionality. A stand-alone cDAQ or cRIO might be a better choice, consult with your sales rep.
The question is what is the length of the pulse? ScanEngine by default uses 10ms scan rate, however capable of 1ms. Just go the settings of the chassis in the project explorer and change it. On the other hand: when using ScanEngine you can use specialty modes for digital inputs (different modules support different modes). Go to the blue icon of the digital input module in the project explorer and click settings then specialty modes. There you can select counter and practically it will does what your subVI does with one important exception: you will have access to your counter value with the ScanEngine rate but the actual counter input will be way faster so smaller pulses will be counted. If this specialty mode is not available for your module it would still make sense not to place a loop (especially without timing!) in your subVI. Counting edges should be a parallel loop (thread) and use a timer/scheduler to read the values - very similar to how you would implement it in C# for example.
I work for NI so I have tried all of the above. However with the Ackerman solution I just did a short test so do not have much experience. Recently for similar projects we suggest using a larger NI Industrial Controller which has plenty of processor resources and EtherCAT ports. If that’s not an option for you then the section really depends on your specifications. The number of nodes/values is not really a big issue in this case, it is more about determinism and such
What do you mean differently? 
This NI industrial controller works with a library from NI or Ackermann's? 
The NI Industrial Controller (IC) ships with either LabVIEW Linux Real-Time or Windows. The RT version is native NI Ethercat support, the windows version can use the Ackerman solution. It is also important that the LV Linux RealTime is not like the previous LV RealTimes which were based on PharLap or VXWorks. The LV Linux RealTime is different as: you can connect a monitor and have a UI from the front panel of your application, can use standard Linux stuff (security, apps, databases, etc) and can also write some code in non-NI environments. 
I appreciate the link! I don't have a lot of free time but I'm seriously considering working on a LabVIEW Slack API just for fun now.
The multiplication is the conversion from pulses counted to kWh. The confusion caused by cluster and string is on my end. The Thermocouple, Current/Voltage cards are all in the same loop which makes capturing the whole VI very large. The digital input was the only card that I was concerned with as data didn't match other sources. I am in the process of updating the scan engine rate and going to gather more data to validate if that fixed it. I have been attempting to get to it between classes. This is my first LabVIEW project outside of using a USB6211 in laboratory experiments for a class. The cRIO was readily available from an existing project and I understand the ways used are far from efficient as I consider to only know surface portions of LabVIEW
Thanks for the advice!
I don't understand your question but you can switch to FPGA mode if needed: http://zone.ni.com/reference/en-XX/help/372603F-01/riohelprt/crio_chassis_properties_rt/
NI 9403 with DSUB. I checked documentation and couldn't find anything regarding pulse length, so I can not answer the second question.
The virtual license I am using moved to 2018, so I had to spend time reformatting as MAX was giving me issues. I have changed the Scan Engine. Still getting same results. I checked the file type I am writing for time stamps and see that even though the loop is running 15-seconds in the sub vi the time stamps are showing 20 to 21 seconds between. I think the inefficiency of my VIs are causing a 5 second delay from passing my signal counting value through to the database. Would this statement seem accurate?
Thanks for the information on the scan rate. I changed to 1ms and still netting same results. I replied below about missing out on roughly 5 seconds of counting due to VI inefficiency after acquiring data and sending it on through to the database. 
It's a definite possibility. I would say separate the crucial sections of the code: data acquisition, logging, and communications. If these three things are all in their own loops and only passing the important data between them in, it should really cut down on CPU load, execution time, and hopefully clean up your data. Timing is definitely very important as well when it comes to loops to make sure they aren't hogging all of the processor. It may be a little scary if you're a newcomer to use queues or notifiers to pass data between loops, etc, but it is definitely worth learning and there are a lot of resources out there for you to use! Let us know how it goes!
Observe the output of the meter with an oscilloscope to measure the pulse width. I believe that module can't be configured as a counter in the scan engine. If that's true, and your pulse widths are narrow, you'll likely need to use FPGA mode. Alternatively, you could consider a module that was designed for counting like this one: https://www.ni.com/en-us/shop/select/c-series-counter-input-module
I ran my VI without the other shared variables and also without sending any data forward. The card is reading appropriately. I think the rest of the logic is causing issues with timing and losing out on pulses by the time the loop starts again. 
This would be cool, and hopefully it's as easy pushing to Slack from LabVIEW as it is from other languages. Curious -- does anyone use any sort of middleware or platform to do something like this? I can imagine a system like Zapier (https://zapier.com/) but for hardware systems. Instead of pushing a Slack notification directly from LabVIEW, send the data to a database and have the backend system message out to Slack. Has anyone come across anything like this?
He could have found the Peak Detector PyByPt VI, which might explain the real time comment. Depending on your version, you can use Quick Drop (Ctrl+Space), or click the Search button in top-right of the palette to find to array version of Peak Detector. Otherwise, I suspect you'll need to provide an example of how you're using the VI to get more help, or look at the Peak Detection and Display VI in the labview\examples\Signal Processing\Signal Operationdirectory for an example of using the Peak Detector VI. (Copied from the [LabVIEW](http://zone.ni.com/reference/en-XX/help/371361N-01/lvanls/peak_detector/) help)
Don't know why, but with Peak Detector I get only a blank array! 
Sure, but it doesn't work, I don't know exactly why, I put the width to minimum (3) and no threshold, no boolean to inizialize and end of data
Have you set the correct threshold value? It defaults to 0, your peaks may be negative so you could change the value to -1000000 or likewise for valley is your valley is above 0 you need to change the threshold to 100000. (At least this was an issue I ran into. Otherwise you should put a sample picture up so we can see. If you still can't get it
the array is correctly read (i can visualize it on LabVIEW), i suppose the problem is in the parameters of Peak Detector
Added a pic, can someone explain to me what I was doing wrong?
Yep, I can login through the shell and delete them, but can't delete over the web interface, the windows explorer interface, or WebDAV, no matter who I login as. 
Well, don’t read it once.
I have already tried that. Reading the data a second time just gives me a duplicate of the first 4 million data points. I don't seem to be able to continue reading the buffer until empty. 
Can you read the bytes as they become available in the buffer and not try to read a large chunk all at once? Use the Bytes at Port read output to determine how many bytes to read... That's how I would do it on a serial connection - not sure if the same applies to USB.
Thanks a lot, Im going to dig into that.
I just try the exemple and just add my files path. Doesn't work like on they're exemple. Why is labview so complicate for simple things like add data to an excel. Im tired, 1 week im trying that everyday.
Have you looked at the block diagram? It sounds like the controls only get read at the start of the code.
I did, the controls are in the while loop. So I dont think thats the issue.
Could you explain a little bit more about your setup? Also, I would double check all the physical connections and make sure they're as desired. When I am prototyping this is a huge culprit for when things seem off. Making sure the software channel matches which one you are watching is also a minor detail that yields bad results.
Hi, So I just have a 9264 in a cDAQ-9189 chassis. The basic idea is that I want to dynamically modify my frequency/amplitude/dutycycle (if applicable). So, I want to start a waveform at say 10Hz, 1Vpp, sinewave. At some point I then want to modify that waveform to be 20Hz, 2Vpp, sinewave. Currently, when my program (the example) starts, I get the correct output, 10Hz, 1Vpp, sinewave for example. If I modify the parameters to 20Hz, 2Vpp, sinewave, my output doesnt change, it remains at 10Hz, 1Vpp, sinewave. Does that help clarify? This seems super trivial, but I cant get it going. Thanks!
[https://forums.ni.com/t5/LabVIEW/how-to-generate-continuous-wave-generation-using-NI9264/td-p/3608065](https://forums.ni.com/t5/LabVIEW/how-to-generate-continuous-wave-generation-using-NI9264/td-p/3608065) there is an example about two thirds of the way down that a user made (Duffy2007) [http://www.ni.com/example/25370/en/](http://www.ni.com/example/25370/en/) an older example that doesn't have the channel layer
On the front panel of that example the Waveform Settings control is an array of values. If you're using a single channel to check the output, make sure the index is set to 0, otherwise the channel never updates. The example doesn't error because they pre-allocate the phase array at the start of the loop based on the number of channels. Since it auto-indexes the 2 arrays, the shorter of the arrays is the number of times the loop runs for.
Thanks for the response. So If I probe the input of the DAQmx write VI I can see the data change as I modify the front panel control. So this seems like its some sort of DAQmx issue. NI support hasn't had anything for me yet...
They'll likely ask you to try different examples, slots, etc. If you have other modules in the chassis, try removing them. Worst case is they'll ask you to reinstall the driver.
I tried that example on my side with everything vanilla using a 6321. Switching between the different waveform types, I see a fairly long lag (~10 seconds?) between the screen update and the scope update. I'm guessing that's because the board itself has a large buffer it has to run through. If that's the issue, you might be able to mitigate it by limiting how much of the buffer you use. It should be fairly simple to prove that you're not regenerating, set the frequency to be longer than Samples/sample rate. Using the vanilla example, you could set the frequency to 500mHz. You'll see a broken waveform in the scope if it tried to regenerate.
Thanks for looking into this. I will try this one I get back to the hardware tomorrow morning. 
I also have a 9263 I can try but I expect about the same. FYI, I'm using LV2018
I'm using 2018 as well. Apparently the ni support person tried and couldn't replicate the issue. So I'm going to try a different spot in the chassis tomorrow. Not that I think it'll help, but it's worth a shot
It may depend on what equipment did the digitizing. It is not uncommon for wav files to be 32bit floating integer so the values would be between -1:1. You would need to first convert from this range to your bit-depth and then use your sensitivity
So, if I use the default parameters Sine Wave Sample Rate 1000 Number of Samples 1000 and run a frequency of 500mHz, my output is continuous. If I then update to 10Hz for example, I get no physical output change. But the "Output" graph does update.
Before writing to the file, you have a bit of structural work to. But First, tidy your code. Remove the unnecessary "x AND True". Make a SubVI of the part of the code you're confident about. Align the diagram elements in logical patterns. Ugly code has a severe impact on your willingness to work. You are doing this \~ish : [https://imgur.com/xqdLYEG](https://imgur.com/xqdLYEG) &amp;#x200B; \- Avoid using local variables. They break LabVIEW's code conventions, and force you to workaround curtains edge cases that would not exist when using pure dataflow. They are in no way required in any circumstance. Prefer a shift Register, like following : [https://imgur.com/W9rOuGT](https://imgur.com/W9rOuGT) Also notice that doing so created two different datas on the diagram : the one on the shift register, that your software can manipulate, and the one on the Table, that is displayed to the user. &amp;#x200B; \- Now you have a timing issue. You read a Double directly into your loop. This produce a data every iteration, but you only meant to write a few sporadic data. As a workaround, you check a boolean indicating if the data should be handled. As an additional workaround, you had to implement an edge detection to avoid writing too much data if "On Finish" was maintained for too long. Queues are much more suited to this kind of data handler loop. As an additional benefit, the queue wire allow you to see your data path at a glance (if you have LV17, channels are the same, but way better in that regard). [https://imgur.com/a3P8AIo](https://imgur.com/a3P8AIo) Check the "Producer / Consumer Patterns". It's the basis for any sane communication between parallel loops. &amp;#x200B; \- Finaly, keep your data Typed. By turning it immediately into a string, you prevent yourself from easily manipulating it afterward. Concatenating multiples, distinct data in a 2D untyped array is worse. You are passing multiple variable as a common group : "Vitesse", "Tension", "Courant", "Couple". It would be worthwhile to make a (preferably typedef'ed) cluster out of them. The corresponding table holding their value would then be an 1D array of a cluster, much nicer to read &amp; use. [https://imgur.com/gfYBfOK](https://imgur.com/gfYBfOK) Once you need to write the data, format it, then write it. &amp;#x200B; Write the file only from one place at one time. I recommend you make a subVI taking a file path, and the whole data you want to write to the file as a cluster. Then test it by itself with its front panel until it behave the way you expect.
Hey, ElSenorDelgado, just a quick heads-up: **finaly** is actually spelled **finally**. You can remember it by **two ls**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
I'll try and post the results! But it's a good point to consider. Thanks. The device was either 16 bit 0r 24 bit but i do need to change wav file from 32bit to either one and try analyzing it again. Thanks
Thx for the tips and explaination. My problem is still here, I don't know how to find in wich Excel sheet I want to write on
Can you elaborate your problem and show the code you wrote attempting to solve this issue ?
So, NI support actually helped me out on this one. It is due to using an ethernet chassis. The info is dumped to the chassis buffer at a higher rate than its read off. That causes the buffer on the device to grow significantly. Heres the example code: 
Oh, cool, sounds like I was on the right track with point out the buffer monitoring property nodes. &gt; I'm guessing that's because the board itself has a *large buffer* it has to run through. If that's the issue, you might be able to mitigate it by *limiting how much of the buffer you use* or by increasing the sample rate.
? Please elaborate what you're trying to do. 
He's trying to do his homework without studying.
Did you try the homework drop down in the Options menu? No? Well why'd you try it here?
If you're new to LabVIEW, start [here](http://www.ni.com/getting-started/labview-basics/).
i tried bro.. but i can't find it
You've wired a false constant to the exit terminal inside the loop. This means the loop will loop forever so the values never escape it.
There are ways to pass data out of a running loop, but they might be overkill for what you're doing here. What's your project? Might a For loop be a better fit? Could we see a vi? Basically, what are you trying to achieve?
Data flow my friend. A function only returns data once it is finished. 
That's not so different from imperative languages. Take this snippet of C for example: int foo(void) { unsigned a = 0, b, c; scanf("%u %u\n", &amp;b, &amp;c); while( 1 ){ a = b + 5*a; } return a + b + c; } The expression `a + b + c` can be evaluated only after the loop finished, which never happens.
Huntsville?
Near Montgomery 
Can it be done? Sure. Would **I** do it voluntary using LabVIEW. Hell no! If all you want is displaying 3 cameras in a split pane view, then you're probably much better off with using OpenCV and one of its scripting language (e.g. Python) bindings. I've written a couple of video device drivers and bindings for LabVIEW and in my experience doing image and video processing stuff with LabVIEW and IMAQ/-Vision feels like pulling teeth, compared with what's easily done in other environments. Of course if you want to use the video as input to some computer vision stuff, that on the backside talks to DAQmx, then LabVIEW is the obvious choice.
Labview can do that, but you will want to make sure you have the Vison Development Module installed when building your application. 
Oh I will have a look at OpenCV As you can see here [https://i.imgur.com/pZEjVyn.png](https://i.imgur.com/pZEjVyn.png) This is what i want to do. 3 camera and 2 lines. Something really easy and functional. Thanks you so much
&gt; So what other tools have you used do vison applications? Can you still do ocr, template matching and the like easily outside of LabVIEW? Well, I'm an old school Unix hacker, and one of the most important tools in my toolbox is the humble stdio redirection, aka pipes. Actually, any LabVIEW programmer should feel at home with them immediately – they're a data flow after all. So what do I use for these kinds of tasks. For one, if the camera is directly accessible via `/dev/video&lt;n&gt;`, capturing images boils down to *reading* from `/dev/video&lt;n&gt;`, you can then pipe those images through tools that e.g. crop it, perform colorspace conversions, channel extraction, etc. Then you can split the dataflow into multiple receivers (of course at every connections). So you could e.g. build a script like this (with imaginary commands) mkfifo videoinput mkfifo textregion mkfifo cropped_text &lt;/dev/video0 &gt;videoinput detect_text_region &lt;videoinput &gt;textregion crop_video -r &lt;(textregion) &lt;videoinput &gt;cropped_text tesseract-oct &lt;cropped_text However usually it boils down to write a Python program utilizing OpenCV to capture video, detect regions of interests and if I want to talk to an external program, like say, Tesseract-OCR use redirection. The important thing is, that the whole Unix philosophy is founded on the idea of having a toolbox of small, simple tools, each doing one job, and doing it well, and allow for easy composition, by creating a dataflow between them.
Wow, seems like this would be helpful but unfortunately, i'm not IT programmer at all and it seems to be over my intellectual capacity at the moment :)). But thanks for the tips &amp;#x200B; &amp;#x200B;
ahah seems like spaghetti code would be a great solution for me :)
I've found its hard to get labview to handle high camera data rates, like a 12MP USB3 camera at 20hz while streaming to disk, for example. The c# or whatever gui that ships with the camera keeps up fine, but labview gets laggy and can't keep up. Something under the hood not as efficient, I don't know. Some cameras come with labview drivers that essentially call the same dlls and this tend to work better but are harder to customize. 
I programmed Labview and am programming python with Opencv now. I can confirm Opencv and python are well suited for your task, don't even start using Labview. If you are not controlling machines or reading low-level sensors don't use Labview. 
[Here is my poor attempt](https://l.facebook.com/l.php?u=https%3A%2F%2Fimgur.com%2Fgallery%2FlnQ3Hw4&amp;h=AT1lcQoDqHUMflauXnJmbkDE0zqJSUctzQIrVtJF3zcrqvXdbjEF-51HCsHYD-_ChFQ71YSOYHxak_pI4afvJkAuIBi1dR8xXB_HsRtFYxXiNC9zZeqBlZoSwxAeA5h_d9wi)
when you said using a cluster to represent each state, you mean to put boolean values into the cluster or actually put a case structure into a cluster?
Try both and report the results.
I agree with the coding solution above using an array, but might you need pull-down resistors on each pin to help keep the signal from the pins low when they are off?
It's exporting data from the labview program to the Excel sheet. It's also using daqmx drivers 8.9.5 which I assume will have to be updated as well
What error codes are you getting? Yes, you should upgrade DAQmx and any other dependencies to 2018.
I'm not sure what you've tried so far, but make sure you compile the LabVIEW code on the new machine with Office 2016. The office toolkit was notorious in the past for not creating the proper links. 
Thanks for the responses btw. I have upgraded everything to 2018 I haven't messed with the source code (VI) as it's not my original creation. The error I'm getting currently is this Error -2146827284 occurred at exception occurred in Microsoft graph: paste method of range class failedHelp Path is graph10.chm and context 0 in NI_ReportGenerationToolkit.lvlib:Word_Update_Chart.Vi-&gt;DAQ main.vi This error code is undefined. No one has provided a description for this code, or you might have wired a number that is not an error code to the error code input. 
Looks like a fairly generic error with lots of things you can try: https://www.google.com/search?q=2146827284 I found one close hit with no solution: https://forums.ni.com/t5/LabVIEW/Automation-ActiveX-for-msGraph-isnt-working/td-p/67013?db=5
Really vague as you can imagine, what kit do you have access to? Do you have a budget for equipment? What modules do you have installed? Basic LabVIEW doesn't have vision access, but if you have the complete package you can do optical character recognition/webcam things.
Make a space shuttle 
BEng? The B is for Bio?
Yes, and do not forget the Mars Rover.
Bachelor's. As in Bachelor's degree in Engineering.
The concept you're looking for is called hysteresis, it's a fairly common issue in heating systems. This person has the same issue issue that they've resolved https://forums.ni.com/t5/Multifunction-DAQ/hysteresis-bang-bang-control-for-thermostat-in-cDAQ/td-p/1324721
Thank you very much
What kind of engineering? Electrical? Chemical? Structural?
This type of problem is often solved with the pod algorithm. Labview has one but I think it costs extra, and it's pretty simple to make your own. 
Thanks for your answers and the pointers to the correct term 'hysteresis'. With that i was able to find some good tutorials and explanations on some funktions. I solved it by utilising an outer timed loop (1000ms period) and two inner case structures. I also used type definitions for 'heater on' and 'heater off', wich triggered the case structures. Works like a charm, again thanks for the help
If you open up the event structure for the initialize event does it only have that one event listed on the left hand side? 
What happens in the initialize case of the state machine? And definitely combine your event structures, there's no need to have two.
Thank you guys for the quick responses! I already checked each of the event cases and there is no case is calling Initialize. Also, there are no overlaps between the event structures besides the Timeout case. 
I tried to combine the event structures for a while but ran into issues with generating the queue. The 2nd producer loop has the queue generator inside the Timeout case (see below). Do you have any recommendations? &amp;#x200B;
You didn't add a picture. You don't need to regenerate your queue reference in the event structure though. You're already doing it outside the loops (twice, which you also don't need to do). The reference enters the while loop from the left and you can reuse that reference as long as the queue is still alive.
That makes perfect sense. Thank you! I thought that you would lose the reference without the generating queue function. Also, I added the pictures to the original post (sorry I'm still learning reddit and labview). 
I don't think the event structure is triggering the dequeue element. If there's an error on the dequeue, it will return a default value, which is probably the Initialize case and is why you keep seeing that case run. The best thing to do is to wire the error out of the dequeue element call and use probing or highlight execution to debug what is happening.
The consumer loop has the Initialize case also marked as the default case. Thus, any enum values that are not handled as separate cases will trigger the Initialize case. For example, let's say your Enum defined: Initialize, Acquire, Log, and Quit. If you have cases for Initialize, Acquire, and Quit, but have not yet created a case for Log, then anytime a "Log" is enqueued, the initialize case will execute. For this reason, I highly recommend that case structures that have the selector tied to an enum, never have a default case; all cases should be defined. An advantage to this approach is that you move the error finding from run-time to development time. If no default case is defined, and you have not defined one or more cases, the run arrow will break.
Solved: I realized that every time that I clicked SP to edit it, the Initialize button highlighted. I deleted the Initialize button and created another one which solved the problem. I couldn't locate what setting caused the Initialize button to highlight. But thank you all for your help! 
if temp is not changing you can increase loop timing to even 5 minutes instead of 1s intervals if you will have any questions or bigger projects feel free to ask me ;) i accept cryptocurrencies too
Did you buy just the TOF spectrometer, or did the unit come with the necessary instrumentation hardware. Specifically the high speed digitizer card that samples the signal from the ion detector? I know of only a handful of manufacturers of such high speed digitizers. - Gage - Signatec - AlazarTech - SP Devices - Keysight - Spectrum Instrumentation And I've already used one from each vendor. Looking at the pictures in the spectrometer's user manual the digitizer card depicted there looks very much like a card made by Gage.
I am a HUGE fan of bookmarks in comments (#name). I usually have a few key groups of bookmarks in all my code: #revisit #DAQio #ui #fileio things like that. I can use the bookmark manager and absolutely fly through key locations in my VIs. This technique along with the popular numbered step-by-step comments for each SubVI process makes for a very rigorous documentation scheme
Correctly named functions, structures and labels. I don't often use LabVIEW comments unless there is an obtuse trickery or workaround going on. Might be a mistake. Textual (handmade with love) documentation with the high level and the "blackbox" view of the software : * Diagram of the different parallel loops or actors, showing the consumer / producers relations. * Format of the files consumed and created by the software. * Hardware communication protocols, if specific to the application. &amp;#x200B; &gt; I've been toying with the idea of creating some more sophisticated LabVIEW documentation tool (QuickDrop plugin + some generator or similiar, maybe also some guided / walkthrough-thing), so I'm looking for some feedback / info. Could you elaborate ? Do you want to generate an in code documentation or an external one ?
I'm a big believer in self-documentation, so here's what I do: * Ensure that the input/output terminals are named well (self-documenting) * Ensure that any required input terminals are set to 'Required' * Ensure that any optional input terminals are set to 'optional' * Ensure that the VI has an icon that's representative of what it does. If it's part of a bigger library, then it may be color-coded (outline, top bar, main section, and text colors). If it's something simple, then use the stock images. But otherwise text will suffice. * If it's a project, double-check that everything's scoped correctly, into private/public/community * If it's an instrument library, follow the Labview guidelines, categorizing the VI's into action/status, data, etc.. * If it's a project or library, make a VI tree, categorize them into groups. * If I'm doing some mathematical formula type stuff in the block diagram, like converting coordinates from one system to another, I'll typically copy-and-paste in an actual image of the formula that I'm implementing. That's probably my favorite thing about LabVIEW, throwing reference images right into the block diagram for documentation. * Make sure that any implementation details are commented in the block diagram (the 'why', not necessarily the 'what'. Finally, 'Ctrl+I' -&gt; Documentation, and write a bit in the VI description. I use basic HTML tags to make anything important pop out. That's about it. That's as far as I've ever had to go. That's already in better shape than most of the code that I've inherited. Now, if I'm coding in Python or C/C++, I'm using tools like docutils, sphinx w/autodoc, and/or Doxygen to output HTML or PDF documentation. I've never, ever bothered to do that in Labview and probably never will. Different strategies as the languages are quite different.
why should we do a documentation? if less people know how its working its bigger salary for us to stay at work :D
Excellent point. I have not used bookmarks this way, but it certainly act as a good starting point for my idea and fits well itno it. Many thanks
&gt;If I'm doing some mathematical formula type stuff in the block diagram, like converting coordinates from one system to another, I'll typically copy-and-paste in an actual image of the formula that I'm implementing. That's probably my favorite thing about LabVIEW, throwing reference images right into the block diagram for documentation. &amp;#x200B; Oh I love this when doing image processing! I often add sample images of inputs / outputs to individual steps to show future-me/maintainers what I expect to happen. It helps a lot with troubleshooting. I also use formula nodes + expression nodes quite often simply for readability. And I use wire-labels quite a lot to explain the data. Also, datatypes in brackets if I'm dealing with reference types or structures where it is not immediately clear.
Because you don't want to spend your life maintaining boring old code that you wrote 10 years ago, and being called to fix it? Because you will not remember the clever tricks you did 5 projects ago and the domain-specific crap that you had to fight? Because you don't want to explain the same stuff whenever a new person joins your team?
I hate working in teams. And i always ask, if everything is ok. I take money and do not fix anything later. If they ask me after 10 years about something. They have to pay me for my time. If i need 2 weeks to understand what i did they just have to pay me for this. Or if they pay me every month for 10 years for maintanance i think looking once a week in code is enough to leave some stuff, optimize a bit and remember most of tricks.
Thank you. I'd never heard of LV bookmarks!
Oooohhh, ive heard about that. I thought that it was supported by Internet Explorer and Safari. I'll see if it will work with IE
It worked, but now he's not connecting to the Remote Panel, this one i'm going Operate&gt;Connect to Remote Panel, and he's always giving me an error (just says "fatal error"). And the internet Explorer tab is showing:" Downloading Panel. 0.00% of 0 bytes" and doesn't change.
DameWare is an alternative to Remote Desktop on Windows. You just use it to remotely log in to your host computer (presumably in the lab) and run everything from there as if you were in the lab. I think DameWare has a Mac client so you should be able to mix and match platforms, although I am not a Mac user so I can't be sure on it's efficacy on Macs. One challenge I see is that DameWare is not free so if you have a lot of students sharing access to the system it may not be a good fit for you.
Please cover FGV :-) in another lesson oh and invoke nodes would be great :-) &amp;#x200B;
Hey thanks for sharing these! I'll be interested to see where this course goes. Do you have any plans for beyond Lesson 8: Case Structures at this stage?
Fantastic, I'm subscribed now so I look forward to seeing them in due course. :)
Please consider discussing how to work a bit more efficiently than standard "search through panels". Aka QuickDrop, Shift+Right Click to get the tools palette, "Tab" to switch tools if you don't use the AutoTool thingie.. I don't understand how anyone could program without them..
I think you are absolutely correct for the traditional plant. But as more plants are valuing lots of data about their systems and cloud enabled reporting and diagnostics the flexibility of connecting to “anything” is high. Is LabVIEW going to replace the PLC. Never. Could it’s market share (of a very large pie) grow from 1% to 5% absolutely. I think it has value in plants with complexity, need advanced reporting/diagnostics, and are unique (can’t design one program for everything) and need a mashup of solutions. 
I started out programming PLCs in my first engineering job, but I found it extremely boring and moved on, been a LabVIEW developer for 12+ years now. I laughed my ass off when NI came out with their "Programmable Automation Controller" nonsense, which fell flat on its face. There is absolutely NO WAY on earth that NI is ever going to be able to compete with PLCs on a cost-effective basis. None. If you want to come anywhere near the channel count of even a basic PLC system using NI gear, you're looking at a PXI/PXIe system with loads of SCXI extension chassis at a minimum. MASSIVE cost difference. RTOS? Mo' money for NI, PLC has it already. Got your system all set but want to add still more I/O? Get ready to shell out mo' money for another SCXI chassis. Or just keep stacking I/O modules on to your PLC setup. Vision system? HAHAHAHAAA!!! Hope you're loaded or using a PLC platform. The only category in which NI systems beat PLCs hands-down is native processing power, and NI FPGA-based systems are hard to beat. But, that's not really what you generally use PLC systems for, anyways. PLCs stomp NI systems into paste in the industrial controls arena. Cheaper, easier to use, simple to expand, etc., etc. And keep in mind that I HATE PLCs.
If you include the event structure, it'll be really great. 
This is really refreshing to hear. Ive argued before that LabVIEW is not the correct platform for industrial plant and been told to quiet down by "more senior" engineers. I agree that the systems aren't easily (or cheaply) expandable. If honest, I would most likely use LabVIEW in conjunction with a PLC for acquisition and processing data, leaving the plc to drive io and do control.
Yup, you can use LV with the DSC module (which you need for the OPC server) as a front end/GUI and have PLC h/w for everything else. It's still a dicey proposition, though. I did an upgrade project for a bearing manufacturer a few years ago, used a very expensive NI touch panel PC for the HMI. I had to interface the thing with all the existing I/O from the PLC and it was a pain.
I'd suggest using variant attributes as a key-value lookup table. The keys are strings and the values can be any variable type.
The drop down control you want is called an enum. Ideally you would have two while loops. One event handler loop with an event structure inside looking for the enum to change, which then sends a command through a queue to execute the function. The second while loop is a message handler the pulls from the queue and executes the function. Google queued message handler. It's simple enough you could just do everything in the first loop, however, and ignore the queue. 
Post the vi you are working with?
We regularly do automation and industrial automation of plants using LV, but usually we don't use NI hardware, but rather run the control code on a Windows PC and using various remote I/O units (including NI ones in some cases). This has some obvious drawbacks (non-RT system, less reliable hardware, viruses, etc.) and is not perfect, but it has worked quite well for the systems we did and it has some great advantages like ease of communications with different systems (hardware, ERP, etc.), the ability to simulate the system well, the ability to run complex algorithms, etc. We also don't use DSC, but rather use our own code. IIRC, the largest of these in terms of I/O count is around 1400-1500 I/O points.
Can we all agree that automation is a difficult endeavor? I am a systems integrator, and we use LabVIEW (cRIO / PC) and PLC (mainly AB) for a wide range of automated systems. Pretty much every project has some sticking points regardless of the platform. We have had serious issues with PLC controlled servos faulting out and acting strange. We have had LabVIEW related issues with drivers and firmware bugs, etc. Our guideline comes down to picking the platform that best fits the application. Typically we lean toward LabVIEW when there is analog signaling and calculations involved. We also tend to prefer LabVIEW for HMI applications, however we rarely use the NI touch panels. Touchscreen PCs are significantly less expensive and much more powerful. &amp;#x200B; For logic control itself, we are getting heavily invested in DCAF (Distributed Control and Automation Framework). It brings a decent tag-bus architecture to LabVIEW. For distributed I/O, we have found that EtherCAT devices can be highly cost effective. NI even offers some called Remote I/O. We also like the I/O modules from Murr, and you can get valve manifolds from the likes of SMC with EtherCAT functionality. It can save lots of discrete digital I/O points and wiring time. &amp;#x200B; If there is a PLC or robot controller in the system that can act as an Ethernet/IP scanner, we have found LabVIEW is great at being an E/IP device. Yes, there is a toolkit needed for the development system, but it is not expensive compared to DSC. &amp;#x200B; As far as coding the logic, LabVIEW does have an IEC palette on the Real Time platforms. Before that existed, we did code some of the functions in house. Translating ladder to LabVIEW is mostly a matter of recognizing that nodes on the same rung-level are ANDs and ones on parallel rung-branches are ORs.
I might be misunderstanding your intent, but if you have a set number of cases (15 here for you), a type def enum would work best to provide uniformity then you can have a simple drop down menu or if you want to get fancy you can still search by the enum string values. Making it solely string searchable means there needs to be a key for the user or user needs to be intimately familiar with the software such as a the author might be.
You can do more than just check that it appears in MAX. You need to send test commands and verify that you get a correct response (the button is called query I believe). For keithleys it's usually *IDN? if I recall correctly. You'll need to check that the serial configuration of the keithley matches what the pc is sending, so baud rate etc. 
Good idea. If the values are known at program time then a ring control of name/value pairs is a good solution for user friendliness.
You would just take the output from that =0 function (which returns true or false), and wire that into an OR function with the stop button. It would just real as "if Variable 2 is zero (True), or the stop button is pressed (true), then stop the while loop"
If you're going to use a while loop here, at least put a small wait timer in there. Easiest way would be to use a [case structure](http://www.ni.com/getting-started/labview-basics/execution-structures) based on the logic of the 0 function. 
Wire the equal to 0 and or it with stop , place the output to the while loop condition stop.
Yes, I believe the problem is I am leaving Visa sessions open because the program works on the first run but fails any subsequent tries. How do I close the session properly after I run the code?
Yes, I believe the problem is I am leaving Visa sessions open because the program works on the first run but fails any subsequent tries. How do I close the session properly after I run the code?
Ive done this before with a compact DAQ AO module, but that cost major $$$. Its a little hinky because it already comes with the capability baked in, but you could use a PicoScope. They come with some good low level LabVIEW drivers. You could also really challenge yourself and drive a solid state relay with a cheap third party TTL board and just PWM all the outputs
Easily but you can probably only do ~1Hz up to ~40kHz. 
That would actually be just perfect for me, any tutorials or anything, because im clueless how to do it.
I worked on a project for a medical device company a few years ago and we spent a lot of time redesigning the GUI. &amp;#x200B; FWIW, I've been using LabVIEW for over 10 years. It's becoming less supported an more dated. I literally would take NI about a week to release a fresh UI but instead...
According to Google, it's been in decline since 2004. I remember being really excited by LabVIEW in 2008, now...
Thanks for the tip, I'm not seeing DMC in the VIPM repository, do you have a link?
[Here's the link.](https://www.youtube.com/watch?v=dQw4w9WgXcQ)
I did this with an Arduino and a Spi dac chip. The fastest i could get was a 2khz sine wave. Once the Arduino is done you can control it with labview or any other programming language with standard serial commands. 
I don't know what the Google trends are, I do know my experiences. I've had a career in LabVIEW since 2004, and since then every time I've decided to quit a job, I've had competing offers, signing bonuses, and lots of security. My involvement in the NI world has been increasing but all I've seen NI pushing more. They've been investing in the HIL space for almost 10 years, and communication and SDRs. This along with more community things like maker and student projects, first robotics, with MyRIO, MyDAQ, and raspberry pi deployment. NI has had improvements in RT with the Linux RT targets too. All the while developing an entirely new IDE. Something NI has only really don't once 30 years ago. This in addition to traditional test and measurement. I've had the option that NI is actually doing too much and should let things be more community driven because I think they are spread too thin sometimes.
The google trend is here : r/https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=LabVIEW &amp;#x200B; &amp;#x200B; &amp;#x200B;
In Ireland, where I live, the NI representative has been taken out of the country. There is a rep responsible for the country but based remotely. I've had people looking to migrate from LabVIEW to python but not the other way. &amp;#x200B; Personally, I prefer LabVIEW and that is annoying me. There are jobs for sure using LabVIEW but I think it could be a lot more, it's a great language.
Why isn't LabVIEW on the same level as MatLab? &amp;#x200B; [https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=LabVIEW,%2Fm%2F053\_x](https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=LabVIEW,%2Fm%2F053_x)
I can't talk about why Matlab is popular, I don't use it or know what it is good for other than large data manipulation. You have good points to make but are going about it the wrong way. Starting by saying that an obscure strong function is a sign of LabVIEWs decline, and then asking if NI is abandoning it doesn't make much sense. I laid out a list of ways NI has been improving LabVIEW. NI is doing alot of work and it appears that isn't being reflected in popularity.
Hey, hooovahh, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day!
&gt;FWIW, I've been using LabVIEW for over 10 years. IMHO, it's becoming less supported and more dated. It literally would take NI about a week to release a fresh UI but instead... I very, **very** strongly disagree with that statement. Changing the UI system (layout &amp; object structure) to something better is no trivial matter, would probably introduce breaking changes to older VIs, and might go against the "Drop the control, wire it, and run immediately" philosophy of LabVIEW. What kind of woes did you encounter when modifying the existing UI ? One of NXG's goal is to improve on that by allowing the creation of UI in HTLM / CSS / Javascript. It's in the works.
Thanks for all the hard work - very informative 
&gt;One of NXG's goal is to improve on that by allowing the creation of UI in HTLM / CSS / Javascript. It's in the works. Sounds good. NI have two control palettes on VIPM, both pretty nasty. The Ultra light one is good. It can be dropped n and not break anything. The appearance of a control shouldn't affect compatibility.
I did two customer projects using Matlab and I would have preferred to use LabVIEW but c'est la vie. &amp;#x200B; To me the placeholder attribute is a bare minimum on GUI components but appreciate your perspective.
Wow, great resource. Do you plan to cover NXG?
https://www.instructables.com/id/Portable-Function-Generator-on-Arduino/
Sorry I domt have time to read the whole thing but if you hit run it will say error...double click on the error you want to find and it will higlight it. 
Put probes on all the error clusters. The put error Handlers on the VIs that have error outputs that you currently have unwired so that you have other places to put the probes. Remember that LabVIEW is not a left to right language or a one line to the next. LabVIEW can be fully parallel and your breakpoints, even though they are near the far right side of the sequence frame do not guarantee that code "earlier" in the frame (e.g. to the left of the breakpoint) has not executed yet. The error can be coming from at least half a dozen places I can see. You may also want to google the error code in both its decimal and hexadecimal formats. 
LabVIEW is data flow dependent. So what order there is, is determined by data flow requirements. A subVI cannot execute until all data is available. With the left - input, right - ouput standard on most VI's this means that VIs tend to execute from left to right. Beyond that the LabVIEW compiler optimizes code execution and small changes to the code can result in different orders from one version of code to the next. However, if we look inside your case statement that has the instrument initialize; there are also two VIs that are chained together and a local variable write. The only thing we can say about the order in that case statement is that the Clear Window subVI executes after the Open Window subVI. LabVIEW is allowed to optimize execution order of everything else. It may be that we write the local variable, call Open Window, call Init, call Clear Window. It could also be Init, Open, Clear, Write variable. Or Init, Open Window, Write, Clear variable, etc... At the point your breakpoint on the Output of F(c) triggers, we know that F(c) executed, it may well be that nothing else in that sequence frame has executed since there are no data dependencies between F(c) and any of the other subVIS or case structures. The breakpoint on the output of Set Camera is somewhat better as we know Set Camera required inputs from Set IR ad from the acquisition settings input; but StealthGet, Event Spatial, Zeiss Image, the Wavelength settings file, F(c) and the upper case structure may or may not have executed.
Your supposition that it is a grounding issue is reasonable. I have seen several projects where the system is not working and then we connect a scope to troubleshoot only to have it miraculously start working because of the path provided through the scope. It's time to pull out the schematics and check the signal connections. As for the NaN generation, that may be a function of the code when it is getting bad input values, it may be for other reasons. I would start by fixing the inputs and then see if the outputs still have a problem. If so, time to look at the algorithms.
[https://www.dmcinfo.com/services/test-and-measurement-automation/labview-programming/labview-ui-suite](https://www.dmcinfo.com/services/test-and-measurement-automation/labview-programming/labview-ui-suite) &amp;#x200B;
Gotcha, thanks for clarifying. That's the way I thought it operated. What I did in order to eliminate all these subVIs running in parallel (stealth, zeiss, spatial) is that I've put them in Disable structures and ran the code. Since I was still getting the error with all these subprocesses disabled, I'm thinking that the error must me coming from the long chain of events (starting with the wavelength settings files and then going through the camera settings, etc). But then again, I get to the break point after the camera settings without getting any errors, and everything else disabled... so not quite sure about what's happening there.
It's can be somewhat slow depending upon the quantity of subVIs and loops, but have you considered using execution highlighting to step the code?
The first bullet sounds like it might be an impedance mismatch. 
When the "Simple Error Handler" is left unwired on the right, and debugging is enabled, the floating error cluster will raise a window like this one. So look at the places before the error handlers (Set Rec State, LCTFError, Fc, SetCameraSettings.vi) and watch them under highlight execution. Likely one of them has a custom error code in there that LabVIEW doesn't recognize.
Where is the other wire of the photodetector connected?
You should look for a MIDI API for LabVIEW. [This example](http://www.ni.com/example/25718/en/) may be a good starting point if you can’t find one. 
I'm thinking create voltage dividers with a resistor and a potentiometer, apply a common voltage across them (eg 12V or 24V) and measure the voltage across the potentiometer using an analog input channel. Choose resistance values that utilizes your input channel range well, but prevents overvoltage on the input. And try to limit the current that will run through each circuit by going high resistance. Alternatively you utilize the keyboard input natives and map different keys as "+" and "-" for each channel.
Google "USB knob" &amp;#x200B; You'd have to find one that you could write a controller for and something that has the response time that meets spec. But hey, finding stuff that might work is a lot easier than making it work. 
I've played around with MIDI in LabVIEW a fair amount, but honestly the easiest way to get this working would be to: 1. Write a LV program that controls an analog output with your favorite numeric control. 2. Bind the numeric control to two keys on your keyboard 3. Grab any one of the many free MIDI to keystroke programs on the internet, and map the knobs to the keys you picked. This would take less than 5 minutes to get up and running. Not pretty, but functional.
If you have access to the object oriented programming course, I'd recommend going through it. The course if based around a sound player. If you haven't seen it here is the sample: [https://i.imgur.com/jdlhVL5.png](https://i.imgur.com/jdlhVL5.png)
Thanks for the help. I was wondering if this could be some impedance mismatch as you said, but the output impedance of the photodetector is about 50 ohms, while the input impedance of the MyRIO analog input channel is 500 kohms, based in its documentation. The impedance seems matched to me. Is there anything else to consider when dealing with this problem? Thanks again.
There is a BNC-jack connected between the photodetector and the analog input. The BNC side is connected to the photodetector, while the jacks are connected to the analog input channel and analog ground. 
Hello, @SeasDiver. Thanks for the help. I am confused with the fact that the MyRIO has analog and digital ground pins, but the power source connected to the power plug has only two pins. There is no third pin or ground connection. Is there a problem? I may be absolutely wrong, or something is escaping from my mind, but I thought there was the need to provide a physical path to ground pins in the MyRIO device. For now, I am checking the connections, and trying to do tests with the scope attached to the photodetector. But the issue with the NaN generation in the analog output channel still persists.
So do you want to transfer data via usb between computers? As computers are USB hosts this is not possible. Try using Ethernet or if you only have USB ports then can purchase USB Ethernet devices.
I would do it over the network, see "TCP Connection" vi's. Alternatively, you can use a Shared Variable, which LabVIEW handles automagically (though it's quite slow and error prone)
I'd also explore UDP. Much simpler than TCP/IP, very reliable, at least in my experience, though maybe not as professional.
Lovely work ethic you have. 
What parts are you having trouble with? Do you have specific questions?
What have you tried so far...? What specific questions do you have?
Thanks!
Hmm, lets write our "personal project" out in Word and then take a screenshot. That won't look like homework.
There was a pretty cool demo of doing similar with a myRIO on the NIWeek Demo Floor a few years ago. You might be able to find that as a reference. Otherwise, without specific questions, we can't help you.
Ive figured out how to build it project! It took a little bit of time to figure it out, but i got it haha. Thanks for reaching out, i really appreciate it guys. &amp;#x200B; And yeaaaa... its a project for school.. Sorry for trying to claim it as a personal project. Thanks again yall :). If theres a way to attach screenshots, i wanna show you all what i did 
It was actually a snapshot of the assignment itself
I ended up doing a lot of research and watching a few videos on YouTube. I took bits and pieces from everything and figured it out from there. Thank you for your help! 
Network shared variables are an easy way to share with low latency.
£150 isn't too bad. The flights etc are the big cost for me. I'll there in London next week, so will let you know if it's worth the money! S
You can split the signals with array indexing and use 2 graphs - the stereo input is actually an array of 2 elements, each is a waveform. Or set the graph indicator to stack graphs instead of overlapping them
Thank you for the advice, i tried it like this [https://imgur.com/a/vOJNRLy](https://imgur.com/a/vOJNRLy) but im still getting either again on the two the same output or no output when i switch between channels even with different index numbers, any ideas ?
Yes ofcourse sorry, [https://imgur.com/a/tdD7RfJ](https://imgur.com/a/tdD7RfJ) here is a screenshot of my block diagram, and the front pannel [https://imgur.com/a/pwZE0yM](https://imgur.com/a/pwZE0yM)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/WK63zKr.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e94ebbl) 
There are a couple of ways you can do it. There is a function under the signal manipulation palette called split signals where you can take dynamic data and split it into N channels. Another one in the same palette is convert from dynamic data where you can set up the dynamic data to be converted into almost any numeric, such as a 2D double array where the columns are the channels. Signal manipulation is under the express palette
I don't have an actual answer for you but r/FRC or https://www.chiefdelphi.com/forums/portal.php might be a better place to ask or find tutorials. 
Each time you change code if you want that to persist through a reboot you need to right click on your build specification and choose build then right click “run as startup”
any updates?
Yes i managed, to split the stereo audio channels by array indexing them and it worked!
Using = with a time never works out well. Often times it will not register that exact time. Should always use &gt;= or &lt;= or a range. 
Try In Range and Coerce then set the upper limit to 0.11 and the lower limit to 0.09 or something like that. 
Another option is to multiply time by 10 and then convert to integer instead of floating point. Then you can check if the value is equal to 1. There are a lot of solutions here, but ultimately your problem is that the closest time value to 0.1 may be 0.10000000000000000001 which is sooo close, but not equal. 
If you want the concentration and safe bool to update during operation they really need to be in the while loop.
Using a less specific condition like 'greater than' will be your best solution. The reason you are seeing varying behavior based on different inputs is because of how floating point numbers are represented in memory. The 8 bytes of data (for a DBL) are split between a sign, a mantissa (or significand), and a exponent. The usefulness of floating point data is that it can represent a wide range of values and can accomplish computations quickly. However, it has limited precision as a result. The "floating point" refers to how big of an exponent is used, and thus, consecutive binary representations may have differing deltas based on what scale is used. Long story short: floating point uses an approximation, so what is probably happening is you are adding 0.1, to x.900000 and expecting it to be equal. However, the *actual value* is probably something like x.900000001, and thus the equality test fails. [How LabVIEW Stores Data in Memory](https://zone.ni.com/reference/en-XX/help/371361H-01/lvconcepts/how_labview_stores_data_in_memory/) [How Floating Point Math Works](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
the exact wording i used in my resume is "built a labview database to collect and analyze data" which was phrased from help from a subreddit thread on resume help. But yes. the only labview experience i have is simply using it has a tool to gather data points and monitor data. 
LabVIEW is a programming environment. 
That's extremely misleading wording. "Built a LabVIEW" *anything* implies actual programming experience in LabVIEW.
No, but if the same program had been written in Visual Studio and C#, and you just changed out parameters and ran it, I wouldn't say I had experience in Visual Studio or C#. The point is that if you interacted with LabVIEW like you would any other program on your computer, especially if all the hardware was setup, its a stretch to claim experience with it.
Do you have HTML experience because you use a web browser? It isn't exactly the same but saying you have LabVIEW expertise, is like saying you have HTML expertise, and that better mean being able to make things from scratch using it. It is a programming language and putting that on a resume to me means having LabVIEW programming expertise.
Check out the clad certification exam. http://sine.ni.com/nips/cds/view/p/lang/en/nid/14438 Essick's labview book is also good for self study. https://www.amazon.com/Hands-Introduction-LabVIEW-Scientists-Engineers/dp/0199925151
Lego mindstorms is also a fork of labview, but I wouldn't stay with that.
Awesome! I will definitely be checking out your series. Thank you!
Looks great! Not sure I can trust a LV developer who places the terminals as icons though...
In my last interview, the guys wanted me to design a solution for a system they wanted upgraded designed to validate a section of a medical device. That might sound daunting but it was actually pretty straightforward and I got the job. It might be worth looking into the type of hardware that LabVIEW typicallly talks with and especially the type of equipment that the company to which you are applying uses.
Well for learning purposes its a good way to help new students keep track of where all the front panel inputs connect in with the block diagram. I'm glad you enjoyed it!
Thank you very much, works perfectly now. Didn’t realise you could multiply by fixed numbers.🤦‍♂️
[http://zone.ni.com/reference/en-XX/help/371361P-01/glang/flat\_sequence/](http://zone.ni.com/reference/en-XX/help/371361P-01/glang/flat_sequence/)
Yes, an array. You can use the Array Max and Min vi to find which one has the lowest time on it. 
I was going to keep the same number of pumps running if I'm within 2 inHg of the setpoint. If it is less than 2 below for 3 seconds, increase the number of pumps needed. If it is more than 2 above for 10 seconds, decrease the number of pumps needed.
I would like to know the order of all of them so I know which is the first to come online, which is second to come online...etc. &amp;#x200B; The Max and Min function does not sort anything in between the two does it?
That's harder, of course. You can do a couple of things. 1) Sort the array (with the Sort 1D Array vi. Then one by one, take each value in the sorted array and search for it in the unsorted array (Search 1D array). 2) Cluster each time value with its initial order (0, 1, 2, 3). That is, an array of clusters of two elements, in which the time is the first element in the cluster, and the ordinal position is the second element. Now sort the array of clusters. It will sort on the first value (the times), and the ordinal number will come along for the ride. 
This feels an awful lot like a homework assignment, in which case people here won't solve this for you. We can help you along if you are having issues with LabVIEW though - what have you tried? What sort of logic are you trying to implement for the two tries? 
Try counting up when a guess is submitted and stoping the while loop conditionally when the count is greater or equal than 2?
Two hints: 1. Loops have counters and 2. compound logic can be used to stop loops. 
not for homework. started studying case structures and decided to try and make a guessing game. I have tried making a version with a for loop and it only have 2 loop iterations. This did not work
In that case.... This state machine style will work. You have 2 options. You can make a unique state for the first and second guess (Go To First Pick, Go To Second Pick) and have unique cases that handle those. the problem with that approach is that it is very static and would require adding new cases anytime you wanted to expand this to three or more tries. another approach is to have a variable that stores the number of tries that have currently been done. You could use a shift register for this, as that is probably the easiest to implement in this structure. After each guess you increment that value, and compare it vs the number of tries you are allowing. Is that enough to get you going? I don't have access to my laptop right now so I can't really do an example or anything right now.
I will try and implement a counter for the tries. I am new to labview, so i am just trying to figure this all out haha. Thank you very much
Remind me tomorrow if you still need help, I can show you a few ways. Easiest way is to drop an int constant left of the while loop, then run that into a shift register that is only incremented in the guess case, and the exit condition for the loop evaluates the value for that shift register vs. a constant (your desired number of tries). 
So now I have them in order based on their run time. How do I address the Pump number in the sorted array so I know which pump to run first? Can I use a function like unbundle or something?
Yes. First you'll have to inde-array to get a single element, then unbundle to get the pump number and time. 
It works! Thanks for the help!
Great. Thanks for letting me know.
Use Index Array on your starting 2D array, with "2" wired to the Index (col) input. Leave the disabled Index (row) input unwired. That gives you the 3rd column as a 1D array. Now sort, and reverse, and you're done. The thing is that the 1D array that is the output of the Index Array vi is neither a column nor a row. It is just a 1D array. So you won't need to transpose it. If you want to put that sorted 1D array back into a 2D array, then use the Replace Array Subset and wire only the column (or row) you want to replace with your 1D array. When put that 1D array into a row or a column of a 2D array, it BECOMES either a row or a column. But on its own, the 1D array is not defined as a row OR a column. 
Try "index array" and input your original 2D array. It should generate 2 index inputs -- a row index and a (disabled) column index. If you wire a 2 constant to the column index it should enable it and output a 1D array of just that column.
Wow, I can't believe how much I've complicated this one. Thanks a bunch!
Thanks alot!
Hey, JenkeiZed, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Happens all the time. I've been using LV for over 20 years, and I still come across some code of mine where I unnecessarily complicated things. Fortunately, that happen less and less often, and when it does, it's very satisfying to untangle some overly complicated code. 
&amp;#x200B;
Any ideas on the best way to increment the hour counters? I tried to use a for loop that incremented the hour counter indicator every 1000 ms (I didn't want to wait for hours) and the output of the for loop went to a bundle function where the input and output are the same cluster in the unsorted array. Things got weird. I'm not sure what to try next.
I'd have to see your code. Too difficult to know what you're doing from just a text description. 
This isn't a labview problem *per se*, this is a state-machine theory problem. Write down on paper a state diagram for the traffic system as a whole, this will inform your code. As an example; if the system were simple (no amber light) then you could imagine a basic two state implementation: State 1: Vertical lights = green; Horizontal lights = red; State 2: Vertical lights = red; Horizontal lights = green; &amp;#x200B; In your case I assume you want to have in-between states with amber lights and so on. This will require a greater number of states, it is down to you to work out how many. As this is a homework assignment if possible I would include this state machine diagram as it shows you have thought through the problem. As an additional note, comment your code to show what is being done. This makes assessing the work easier and can help recover marks if you fuck up and make a mistake. &amp;#x200B; With regard to syncing, just write all traffic light values within each state. To avoid clutter consider forming a cluster of the three boolean lights and type def-ing it. You can then copy this cluster indicator and write to each light block in the same way.
[Looks like that wasn't a feature until LV 8.6](http://www.ni.com/tutorial/7386/en/)
It didn't exist in that version. But be glad it wasn't LabVIEW 1. In 1 you couldn't move things once they were placed. You had to entirely delete and re-wire it...
Darn, that's what I was thinking. I've seen LV1 in demos but probably will never have the opportunity (or want) to use it. Going from LV2018 to LV6.1 is surprising in the similarities but frustrating in the lack of certain capabilities. Thanks for the info!
Yikes! Thanks for the link and info 👍
These may just save me. Thank you so much! I’ve got a final coming up next week and I’m still really lost with LabVIEW so here’s hoping 
As others have said, didn't exist, but really, if you think the code is bad enough to need the clean-up button, the clean-up button wasn't going to help.
Thanks for the advice, helped quite a lot actually.
Think before you speak.
Undo came in LabVIEW 5. Be glad you didn't have to live without that.
You're more than welcome :) 
Why is this being down voted? Adding a wait timer to slow down free running while loops is good practice and reduces system load. A 50 ms wait timer wouldn't even be noticeable to the user.
My comment was well thought out and I stand by it. The code-cleanup tool has no use and I still long for the days where developers were forced to just write good code. It may be an unpopular opinion, but it's mine. 
That's just silly. Do you still long for the days of roll up windows, non-motor lawn clippers, and washing clothes by hand? The wire cleanup tool has no bearing on "good code", except unnecessarily taking longer to move the wires and just because you enjoy taking longer to move your wires around does not mean the code will be better. In addition to your opinion being self-proclaimed unpopular, it goes directly against what NI teaches. 
We'll have to agree to disagree. I will say, however, that when taking Core 3 at NI in Mountain View our instructor did take a few minutes to discuss clean-up and refactoring and specifically said he was also against the clean-up button. So, it's not exactly going against what NI teaches. 
The exception proves the rule. You're not expected to solely rely on the wire cleanup tool, but it does expedite the cleanup of the wire, thus it's point. The tool doesn't always get it right, that's why there's a human involved.
I think you're missing my point. The wires shouldn't need to be cleaned up. I write code all day long in LabVIEW and many other languages. And I've never used the diagram clean-up button. Ever. And I write code plenty fast enough, I passed my CLD and CLA tests with time to spare. Could you imagine writing a set of functions in .NET with poor variable names and confusing syntax that works but is difficult to follow, and having a button that you press that renames your variables and cleans up syntax to make it more readable? Yes, it would be nice to make sense of poor code, but realistically, when the code is that bad the limits of those tools wouldn't allow much real cleaning to be done. If you want to use the clean-up button, go right ahead. No one is stopping you, least of all my opinion. 
It’s an array constant, when you add it is will be gray so and you will need to drag the right data type into it. 
That is an Array of Clusters, where each Cluster contains two DBLs. Easy way: right click on existing wire, create constant, control, etc. Harder way: put down a Cluster constant from the cluster palette. Drop two DBL constants in it. Drag this finished cluster constant into an empty Array constant. This data type is beginning to get a little complex. You may want to consider making it a Typedef if it is being used in multiple places.
Thank you!
As a heads up, if you have an existing function that's expecting that datatype, you can also just right click on that input and choose 'Create a Constant'.
Ok, thanks
It's a global variable. So this thing lets you connect loops and programs together. So let's say you hsve a loop that only gets the iteration number and saves it to "num1" and displays it. If you right-click that display node, you go down to create global variable, and then drop the node that appears on your mouse. From there, you drop it where you need it, and then you can save a value to that variable on another loop within the same program. If you want to read the value and do something with it, you need to right click the new variable and select "read". I haven't worked with LV in a while so some of the details may be off so just google "gloval variable LabVIEW how to"
Ok thanks!
On a separate note, you should also be able to just "place" a global variable from anywhere, but then you have to select which data you want to pull. Should be straughtforward. You should find it in the "functions" pallets.
This is not a global variable this is a local variable. House on the local variable globe on the global. You can right click on any Ctrl or indicator and choose create&gt;local variable. Be careful with this as you may create a race condition. 
Not to nitpick, but that's a Local variable, not a global variable. Globals are created at the project level but Locals are created and are scoped to the VI level.
It's not nitpicking, you are correct
You're right. It's not nitpicking if I was wrong! Thanks for that. Sorry for the confusion OP
He is wrong. It is a local variable, not a global variable.
Yes I found it thanks 
Por que no los dos?
I havent tinkered with mine much but i thought they had a specific driver pack for MyRIO since it is an educational device. I dont thinknyou have to have the RTM to work with it. I may be totally wrong though
This is correct; the myRIO drivers are not the same as the RealTime Module
Is your student version through a university? They may have an academic site license for RTM. Might be worth calling your local NI d00d to check
How long did this take?
Block diagram options -&gt; uncheck "show terminals as icons' 
This makes me happy 
Nicely done! You can clean up the logic to light up your LEDs with number to boolean array https://zone.ni.com/reference/en-XX/help/371361H-01/glang/number_to_boolean_array/
Why? what does this do?
about 3 hours...
Terminals will be much smaller, the way they where before LabVIEW started to defaulting to those needlessly large terminals.
Nice! This would be a fun refactor expirement where we try to drive it closer and closer to being a beautifully designed piece of code.
Ok, thanks!
What hardware are you using?
I would record the guitar signal into a buffer or array then reference that signal on your output (playback) side. Take the subset or index of the guitar signal based on how much of a delay you want.
My only "hardware" is my guitar and audio interface.
I thought about buffer but I don't really understand how to do your idea.
I forgot to tell: I want to control time of delay, like in guitar effects.
What don't you like about it? How about some constructive criticism of the program and how it could be improved? I've not used it, I am only familiar with normal Labview. 
Well that’s some really helpful and mature feedback.
I thought for a moment to send this to NI devs but yeah there isn't anything constructive here. It has real issues that you could complain about and having an honest discussion here would be valuable. 
At least it’s not both!
I'd say just go ahead and move on over to ".NET C#" now, we don't want complete morons attempting to act as LV advocates.
Look at what this guy says in his other [comments.](https://www.reddit.com/r/programming/comments/a5g3pi/top_3_things_that_ive_learned_in_my_journey_as_a/ebmb95j/)
Combine that with his username
Read Delimited Spreadsheet.vi You want to plot that data? Generating a signal doesn’t seem to have anything to do with reading from a spreadsheet. 
Make sure to post what company's you have worked for, or end up working for in the future. I'm really looking forward to coming behind you and showing them how powerful LabVIEW really is when someone competent uses it. In the meantime, go fuck yourself.
You too moron
If you were depending on NXG to solve a problem, and you’ve *just now* realized it’s not close to being ready for anything, that’s not NI’s fault - it’s yours.
it's 3.0 and still shit. That's it. It's also slow as hell. It builds on Directx 9.0 an obsolete technology (it's windows xp), you can see that from License agreements. It's much more unproductive than classic LabVIEW and changes many things. Also the colours of the IDE are all gray shades it's soooo bad UX principle. It's like Samsung that tries to mock iPhone, copying the single home button, copying the "S", copying the jack removal. NI tries to copy XCode IDE it's so pathetic. Mocking XAML designer. And it's slow as hell.
you are the only clever person here :D
Do you have a schematic for the actuator I could see? What DAQ hardware are you using?
What hardware are you using? You should be feeding in a 2D array or an array of the two waveforms. You might want to post your code (possibly as a snippet) so we can get a better idea of what might be going wrong.
As in the wiring for the actuator ? I can provide the make and model number and a link for the data sheet. The ni hardware is a ni-9264
You cant have 2 tasks of the same type running on a single daq card. If you combine both of those daq channels into 1 daq assistant task with 2 channels you should be fine. 
Ni-9264. I can post a picture when I get back to my pc
I can't seen to work out how to have each signal go to it's own output channel 
just put 2 channels in the daq assistant. The input to the block will just be an array of 2 waveforms/values. 
Yeah, send the link.
Check out the example here: C:\Program Files (x86)\National Instruments\LabVIEW 20xx\examples\DAQmx\Analog Output\Voltage (non-regeneration) - Continuous Output.vi You'll see it uses an array of waveforms. Maybe try setting the amplitude of the second waveform to -1.
Absolutely. A lot of places need developers. Talking to your local sales guy is a great way to get referrals. I had my own business for a while doing consulting. If you are still starting out talking to other local alliance partners (or working for one) is a great way to build your experience on large applications. Do you have an industry that you prefer or have experience in?
Omg this is so encouraging! I don't care about any particular industry over others. My experience is mostly in manufacturing for solar trackers (so developing test automations for mechanical components). I think that will translate pretty well to some other areas.
What hardware do you have experience with? I suggest cRIO for a lot of applications. Do you have experience with any of the application frameworks? I suggest DQMH or if OOP doesn’t scare you then Actor Framework
I write LV for a living now (second career) creating relatively complex applications for a scientific subfield as the owner of my company. The trick is finding your niche. I fell into it almost by chance, so it's hard to give advice on finding one. I also know several people who are freelancers, creating test systems for industry and such. They do very well. Is there a LV Users group in your area. If so, go to their meetings and network.
I'm not one of them, but at NI Week it seems like there are lots of people that freelance. Personally I like working for a larger company and I can focus on what I'm good at, and leave the other parts of the job up to others. I've been working full time doing LabVIEW pretty much daily for 15 years. I'd agree that in the Detroit area demand for someone with a CLD is high. Primary in automotive but plenty of systems integrators too.
I don't have a particular ton of NI hardware experience to write home about. I've configured a couple of cDAQs. Outside of NI, Siemens PLCs and proface HMI. I haven't used DQMH for any real applications but I've practiced with it a bit and I feel reasonably comfortable with it. No OOP yet but it's a goal of mine! Currently working on OOP in .NET and would like to learn LVOOP as well.
Oh this is a good tip! Yes, there is, and I haven't been yet but I have been meaning to start getting involved with them. I will try to leverage that. Thanks!
I wouldn't mine a normal full time job doing LabVIEW, I just wasn't sure that was a thing that existed. I'm currently working a full time job that I *thought* would be mostly LabVIEW development but it's actually 95% project management work which I hate. After this experience I thought I'd have to do freelance work in order to just write LabVIEW all the time, but maybe that's not the case! I wish we could live in Detroit. My husband races road bikes full time so he might kill me if I suggested that 😊
Well maybe look up if there are any alliance partners near you. They generally are a hot bed for all things NI and often looking for talent. That's where I got my start. It isn't all LabVIEW, some hardware designs, quoting potential bids, integration work, onsite work, but it was lots of fun and varying industries.
This is encouraging. My primary job has pissed away two years of promised work and I recently picked up my first freelance job. It's been pretty amazing and has me thinking of going full time freelance if the conditions are right.
I'm a LabVIEW consultant on the side of my full-time career, however, the side business is quickly coming to the forefront. There's a ton of LabVIEW business out there. My recommendation would be to either work for NI or work for a larger integrator or Alliance Partner and get some experience and your name out to the community. Plan on being in Austin at least once a year for NI Week to network and go to every get together there that week that you can. It's totally do-able to earn a great living doing software consulting primarily in LabVIEW. &amp;#x200B; Here's a link to my website: [http://testingsquared.com/](http://testingsquared.com/) if you ever want to talk more about it. 
I laughed when I got to your blog entry, "Going Solo (and why you really shouldn't)" 😊 Eye-opening stuff, and when you describe why you thought it was a good idea in the first place you sound an awful lot like me now. But it does make sense that just having someone else invested in the project would make a huge difference. Whenever I solve a particularly interesting problem I'm always excited and want to tell someone about it, or if my brain just breaks and can't figure out a bug that should be very obvious. Great advice, thank you. There is only 1 alliance partner in my area but I will see what opportunities they have available.
I have been a LabVIEW consultant/freelancer for the last six and a half years. I own my own NI Alliance Partner, but it is just me. I know of several companies that are hiring both CLA and CLD level LabVIEW programmers. I know one in Dallas,Tx that offers full relocation. I know another in Austin, Tx but don't know if they offer relocation. You can PM me for more information. 
Just to add, my experience as a CLA in a manufacturing company is that it's preferable to hire people with EE/mechanical knowledge for our alignment and test systems. I would give my left arm for more CLD level engineers who can appreciate analogue electronics, motion control, industrial automation, physics etc. I prefer to have a bunch of system engineers who can learn to understand the things they're testing (and testing with), and help them become better LabVIEW engineers as their careers progress. (Also, hooovahh is well worth listening to, his reputation precedes him!)
Minus the official CLD, you sound like you're looking for someone like me.
I primarily do it as a side hustle. I have found a good niche in academia helping with research projects. I don't do enough to make a full time job out of it but it lets me buy mountain bikes sometimes, so that's a plus. Shameless plug time! www.GoSandhill.com
Hence 'CLD level' - we can (and do) teach engineers to code in LV, but it's nice not to have to start from scratch. A CLD or CLA (or, heaven forfend, a CLED) tends to provide shorthand for a level of competency that matches this. Having said that, not all CLDs (or CLAs...) are created equal of course :)
The upgrade path from the USB-6008/9 would be the USB-6001/2/3.
myDAQ has a DMM but fewer analog channels
Mmm hmm, mmm hmm, okay. 
What do you intend to use it for?
Ok, not a lot of detail and also not clear why you have posted in r/LabVIEW If you find a modulation scheme suitable for the speed you are trying to measure, you could potentially heterodyne the transmit and receive and use the Doppler effect to get the velocity.
Put IR led on object you need to know the speed of, put the two photodiodes at 1 meter apart, measure the time in between both sensors register a hit, have a speed?
I edited my post. Can you answer my question? Thanks
What do I do?
You need to explain in more detail or no one will be able to help. * what are these 5V pulses and why does counting them help? * what does it mean to say your sensor has "captured" the object? * why are you doing this in LabVIEW and not some other language? * what does you LabVIEW code look like? Post a screenshot if you can. 
It sounds like it might just be a simple light gate (get the time delay between the LED passing PD1 and then PD2, divide distance by it and voila) but it would be nice if OP would say as much. 
NJTech student looking for handouts instead of using brain to solve engineering problem?
Basically I input the output of my photodiodes into my Arduino and when the object passes through my sensor then the Arduino outputs a digital 5v pulse. Otherwise the output is 0v. The digital output from my Arduino goes into the digital input on my NI DAQ. I need to find the time at which the sensor outputs a 5v pulse. The sensor captures the object when it ouputs a 5v pulse. I'm doing this for a project and need Labview to automate velocity calculation. Currently using "2signal edge separation" vi from the Labview examples. Here is what it looks like: https://ni.i.lithium.com/t5/image/serverpage/image-id/203299i393CB9A1957E8554/image-size/original?v=1.0&amp;px=-1
Not a handout! Just help.
The ext. 5V is not meant to be a reference. The specification states it's 5V "typical", with a minimun of 4.85. So that is the type of fluctuations you can expect. If you want something more accurate, use the AO statically set to 5V. BE aware though, that it can supply only 5mA current.
I guess it also depends on where you live :) Although I met LV freelancers from all around the world.
Ah of course, the AO, hadn't thought of that, thanks a lot for your reply!
The PC I'm working from at the moment doesn't have LabVIEW installed so I will have to just describe some suggested additions to your code rather than showing you. I'm also not directly familiar with the "Two Edge Separation" task and it might be that that is a nice and efficient approach to take. My approach would be a little more simplistic and explicit, though. &amp;#x200B; If using NI-DAQmx I would start off just simply monitoring voltage and setting up a conditional that compares that voltage reading to a fixed value. For example, you could get the VI to flash a boolean indicator when the voltage is about 4.5V. In order to get the time information, you can put the \[High Resolution Relative Seconds VI\]([http://zone.ni.com/reference/en-XX/help/371361R-01/glang/high\_res\_rel\_sec/](http://zone.ni.com/reference/en-XX/help/371361R-01/glang/high_res_rel_sec/)) inside a condition box which only fires when the boolean flashes. You can use a shift register to subtract the first result from the second and this will give you the time between the end of the first pulse and the end of the second. &amp;#x200B; If you experiment with these ideas you may find better ways of achieving your target result, but hopefully this is a starting point.
Thanks will look into it
Lab exercises, eg read counts from a Geiger Mueller tube, diy adc via a resistors ladder and a comparator. Nothing faster than kHz and no more than a few voltage lines written within a vi. The last time I taught this classes we used Arduino and the lifa/linx interface. Boy was that ever slow! 
Have you used something like Wireshark to look at network traffic? That could help you find the bottleneck 
Oh, I've never used it before, I'll have a look then Thanks a lot!
Will do my best to speculate here as can’t fully relate to your issue. With the data, in theory should it be near enough instant? Does LabVIEW have to poll for the data? If so, does it only do this occasionally? Is your LabVIEW code running all the time? (i.e. there is no area of code executing really slowly when it starts?) Also, i don’t suppose you have left highlight execution on at all? Have you used it to make sure your code is executing as you would expect?
&gt; With the data, in theory should it be near enough instant? Does LabVIEW have to poll for the data? If so, does it only do this occasionally? &gt; &gt; Is your LabVIEW code running all the time? (i.e. there is no area of code executing really slowly when it starts?) I've been running the Matlab script and entering my values before running the VI (found that it works far better as the VI would time out if I did it the other way round). I'm using the TCP Open block, and to my understanding that it's the TCP Listen block that does polling?
Subscribed to this sub hoping for something like this. Thank you for these! I’m doing an EE degree. My college uses the NTS videos, which move fast and often he doesn’t explain the sub menu he’s pulling something from. And other videos are poor quality or hard to understand. These look solid. Well done! What LabVIEW books/guides do you recommend? It seems like I should probably spend more time learning LabVIEW between classes, but there isn’t a lot of direction or supplements in the courses I’ve taken. 
[removed]
The default timeout on TCP Read is 25 seconds, so that's probably where your delay is coming from. If you haven't received your number of Bytes to Read, the TCP Read function will wait until the timeout occurs.
North logic is likely correct. Can you post an image of your vi? 
Be aware, you'll likely need more than the 5mA though. You could possibly damage the sensor. What sensor model number are you using? Your best bet is to likely sacrifice a wall wart power supply that supplies 5 volts and plenty of current. They are a dime a dozen nowadays. their voltage is regulated pretty reasonably in most of them and you should be able to get enough current.
Hiya thanks for your reply, [this](https://uk.farnell.com/nxp/mpxv7002dp/sensor-pressure-dual-diff-0-02bar/dp/2080499) is the sensor I’m going to use for pressure. So the USB DAQ wouldn’t be able to provide enough current for the sensor? That would be a bit of a pain...
You might be about to use 2 analog outputs at the max draw after reading is only 10 Amps
Hi, many thanks for your help, here you go: https://i.imgur.com/JNL7EZi.png
http://zone.ni.com/reference/en-XX/help/371361R-01/lvcomm/tcp_read/ Look there and you'll see all the settings for the read. How many bytes is Matlab actually sending? Also, doing your DSP in LabVIEW can be much easier depending on what you're trying to do. What's going on in your m script? 
Yep, switch the tcp read mode to "immediate". 
Thanks so much! And keep up the good work on the videos. I’m sure there are a ton of college students who will thank you even if only in their head. 
Looking at the spec: &amp;#x200B; &gt; myDAQ Student Data Acquisition Devices feature eight commonly used plug-and-play computer-based lab instruments based on LabVIEW, including a digital multimeter (DMM), oscilloscope, and function generator. Students can access all the ready-to-run software instruments to perform experiments and exercises with the Bode analyzer, arbitrary waveform generator, dynamic signal analyzer (fast Fourier transform), digital input, and digital output. These affordable devices allow for real engineering and, when combined with LabVIEW and NI Multisim software, give students the power to prototype systems and analyze circuits outside traditional lectures and labs. &amp;#x200B; Lots of cool stuff for students but a bigger price. Maybe students could use the cheaper 6008 and create their own oscilloscope but at the same time it's nice to be able to pull up a working scope. I guess you have to decide based on the course you are giving but personally, if I were the student, in the long run, I'd be glad I paid a bit extra to get the myDAQ. However, as the primary concern of most students is beer and partying I'm not really sure what to say! &amp;#x200B;
So what are you actually trying to accomplish here? Are you just trying to integrate C# and LabVIEW with Minesweeper as an example? Do you just really want to play Minesweeper?
Integrate c# and Labview
Okay, here's some things that might help with that. [https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019Ls1SAE](https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019Ls1SAE) [http://zone.ni.com/reference/en-XX/help/371361B-01/TOC30.htm](http://zone.ni.com/reference/en-XX/help/371361B-01/TOC30.htm)