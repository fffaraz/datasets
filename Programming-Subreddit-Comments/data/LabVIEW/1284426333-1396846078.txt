I don't know of anywhere else on reddit you would. Ask away :)
I am new to LabVIEW and have been given the task of interfacing with a LakeShore temperature controller. My employer wants my program to take data continuously, but only write the data to a file and graph at a certain period. Right now I have the data VI's in a for loop that has a time delay in it, and set the number of iterations as however many seconds or minutes to pass before it writes the data, and then allow it to pass to the write to measurement file VI and the graph before returning to the data for loop. There must be a better way to do this with timed structures or event structures or something. My instinct tends to be to use a bunch of while loops for things, but I can't stop them with a button outside of the while loop. Can anyone help find a better way to go about this?
I've only just found this subreddit - nice to see something recent! I don't actually have access to a copy of LV at the moment, so [here's a representation of roughly what you need](http://i.imgur.com/KOdxi.jpg) in Paint. The outer loop is a while loop, the inner loop is a case structure, where the false case is wired straight across. The big orange boxes are shift registers. In normal operation, the DAQ outputs a value, it is appended to an array and passed through the shift registers, so long as the given number of seconds has not elapsed. (ie, the case is false). When the case becomes true, the array will be written to a file. For future reference, when you have multiple while loops in parallel and want to use one stop button, wire the output of the button to one stop terminal and read local variables from the same button to the stop terminals of the other nodes. EDIT: There's a couple of errors in that diagram - technically the left shift register needs an input case wired to it with an empty 1D array, and in the true case you should have another empty array wired to output tunnel, or it will continue to store data already saved in a file.
I'll give this a try-- Thanks! Good to know I can find LabVIEW help here!
Sorry, didn't see your question until just now... Have you been able to sort out your problem? I would have suggested something similar to Moun's solution..case-structure as a save-trigger...
I ended up just putting the write to file and graph in a case structure with a timer wired to it, which works very well, as I don't need to save every iteration. The case structure set up was key. Thanks for your help!
Sorry for the long absence...
right click 'Disable Indexing'
Thank you much
May I also ask how to compound sections of code in the block diagram into one symbol? I know that this is possible and it would be very nice at the moment
highlight code -&gt; edit -&gt; create subvi
Well, my computer that has LV on it is freaking out right now, so give me a bit and I'll see if I can't come up with something useful for you. Edit: I'm back with something. So, in my experience, I use dynamic event registration when I don't want LV to continuously queue up events once a user has moved on from that page. For example: a testing program and query a DB when a user presses a button (event case). However, after the test starts, the user can still hit the query button. With standard events, LabVIEW will queue these up and execute them the next time the event structure is realized. One solution to the above is dynamic event registration. You create a User Event and register the "query" button for an event. Once the user has moved on from that screen, you unregister that button from events and delete the User Event. Now, if the user presses the Query button while the test is running, or any other time for that matter, LabVIEW will not queue up those events. So, in your case: I'm assuming you're using a standard State Machine architecture. In the state before the one where the user chooses his ROI, create and register events for the ROI stuff. The next frame is the Event Structure. Once the event structure is finished, Unregister and delete the events you created. Here are two examples. The first shows use with the "Generate Event" VI: useful for doing data-based events. The 2nd shows User Interface-based events. [Example 1](http://dl.dropbox.com/u/431026/dynamic%20event%20example.vi) [Example 2](http://dl.dropbox.com/u/431026/dynamic%20event%20example%202.vi) Give me more details on your program and I might be able to help more :-)
For checking when the user is done with the ROI, I usually just check the Image "Mouse Up" event (no need to go dynamic). Is there a dynamic event that's fired? I'm not sure where dynamic events come into that picture. In my experience, dynamic events are useful when you want to send a message to an event structure, or when you want to register for a bunch of events that get handled by one case without hand selecting every object that triggers the case. As for documentation, I would suggest just looking at the examples in the example finder.
Thank you so much! This had got me started in a good direction.
Hmm... Can you give a little more context to what you're trying to accomplish and how you're wanting to do it? Also, an example image of what you'd like to get would help.
Those are droplets that I have isolated but I want to make sure that they have retained their original dimensions. I would love to take the image I gave (actually one frame in a video) convert the white rings to a color and overlay the rings onto the orginal frame.
A very interesting problem. My first inclination is to use picture frames and transparent pixels. Sadly, I have no experience with such. Have you tried the [labview discussion boards](http://forums.ni.com)? There are a few people in there, Altenbach, JackDunaway, and others, who are very helpful and actually have time on their hands. :-P
Do you have the Vision Development Module for LabVIEW?
yes
Yeah, I tried using the discussion boards but they haven't responded to my questions. I think that my focus on image analysis isn't more than a niche field for the users of labview. Unless I happen to be doing something wrong in my posting
If you post in the proper forum (vision), NI engineers should reply within a day or so if no one else answers the question.
Labview isn't very popular on reddit. Head over to NI's [dev zone](http://zone.ni.com/dzhp/app/main) where they have forums and [tutorials](http://zone.ni.com/devzone/cda/tut/p/id/12302).
You can order the coursework from NI directly (or even take the class).
The coursework is very beneficial to learning the software. The class (CORE 1 &amp; 2), however, is geared toward beginning programmers, which means it is long and boring for anyone experienced in programming. I have been told from coworkers that higher classes are much more beneficial.
True, it does start slowly, but Core 2 starts getting more heavy quickly. I wouldn't recommend skipping that class. If you are in psychology and haven't programmed before, Core 1 is a good place to start.
I used [LabVIEW for Everyone](http://www.amazon.com/LabVIEW-Everyone-Graphical-Programming-Made/dp/0131856723/ref=sr_1_2?ie=UTF8&amp;qid=1309361540&amp;sr=8-2) and liked it well enough.
The practice CLD exams are good exercises to build your skill with. 
The best way is to generate the data in a for loop. The output results will be in a dynamically created array
It depends on the application. Let's say you are reading a thermocouple and you want to store all the times the measured temperature goes beyond certain limits. Create an empty 1D array of timestamps outside your main loop and connect it to a shift register. In your main loop poll your thermocouple and if the temperature is beyond your limits append the current time to your array of timestamps using the 'insert into array' vi from the array pallet. 
Or "Build Array" to append it onto the end.
??? More info needed.
happy birthday, dude. sadly, i'll not be at the NI week...:( but most certainly at the VIP2011 in FFB... 
I'd love to go, but alas. I'm really hoping to see some more of the toolkits with 64bit versions.
I'll be in Austin Monday to Friday for NI Week and I am pretty stoked about the factory tour that I saw listed on the final version of the program. Redditor meetup sounds like fun!
Which ones?
Im an NI employee, so most def. Im in charge of one of the demos there (cant disclose details until monday, sorry). 
Sweet, maybe we'll run into each other Monday and not realize it ;) Have you been to previous NI Weeks? I'm curious what to expect beyond what the website says.
This will be my first go-around. If im there when you are, Ill be in the Labview zone by a university of texas project. 
This will be my first go-around. If im there when you are, Ill be in the Labview zone by a university of texas project. 
Excellent, I look forward to seeing your demo. Good luck!
You should just be able to wire up the text line directly to the VISA control. LabVIEW takes care of the data coercion. Can you give an example of the text file you're using to store these VISA names? I'm assuming it looks something like: PowerSupply1 = GPIB0::27::INSTR PowerSupply2 = GPIB0::28::INSTR Another option is to set a VISA aliases in NIMAX. I don't use them myself, but I'm pretty sure they will change how those instruments show up in the VISA control dropdown. [VISA Aliases](http://digital.ni.com/public.nsf/allkb/C063FA3D7C71CE80862569B60074F4A6) EDIT: NIMAX is NI Measurement and Automation Explorer. Just adding this for those that don't know. :-)
I am using NIMAX to get my VISA aliases. It was the only way I found that the power supplies would respond to.
the NI training courses are pretty good if youre at a company/location that can afford it. The [NI forums](http://forums.ni.com/) are also pretty good resource. If you have bought a bunch of equipment, you probably also have a service contract with NI and can call into support for when you get snags. And i wouldnt be worried about IO count. the more difficult thing is ramping up to FPGA and RealTime. 
Don't forget http://lavag.org/. If you're not comfortable using producer/consumer architectures and state machines then I would say you need some more training before you tackle a big project.
As the other guys have said...**NI Forums** and **lavag.org** are good sources of information. Bookwise I can also recommend: * **The LabVIEW Style Book ** by *Peter Blume* [ISBN-13: 978-0131458352] * **Einführung in LabVIEW** by *Wolfgang Georgi* [ISBN-13: 978-3446415607], but it is in German. Best Basic Tutorial, though. Don't worry about the growing complexity of your applications...you'll grow with the task...And **always** check the examples in LabVIEW for ready-made code...make it a habit ;)
At a previous job we used some of these: http://www.seetron.com/sscasd2.html But they were already set up and working when I got there. I never had to mess with them in the 4 years I was there. The only time they didn't work right a simple power cycle fixed it.
Thanks man, that looks like a good option, though, do you know if I could use an Arduino in the same way?
NI has an entire line of motion products.... [NI Motion](http://www.ni.com/motion/)
Thanks for the suggestion, but, from what I've found, NI's servos are far more expensive then hobby servos and there is similarly situation with their USB microcontrolers. Is it because they are perform extremely well? Also, maybe I'm just getting stuck looking at all of NI's newer gear, and the older, perfectly-suitable gear, is just harder to learn about on their website. Can you point me towards anything specific withing their line of motion products?
What is your application? Give me details as to what you are trying to do. 
It's not too complicated of a project. Basically, I'm trying to actuate small maze doors (for mice in an experiment). The acrylic doors slide up 3 inches and since the door isn't heavy, torque isn't a big issue. I'm mainly looking at servos because they could control the speed and position of the doors with much more composure than other options (though, of course, the servo's noise is a downside). The project would only require three or four outputs (plus a handful of inputs for sensors as well). Something on the complexity level of an Arduino Uno would be great (though I guess a question for another time would be how to simply use that as my controller). I was really hoping NI had a device that could work as well.
It sounds to me like you don't really need a motor but some sort of solenoid attached to a door (on/off - no between) would be sufficient. If that is the case, then all you'd need is a cheap USB DAQ device with digital outputs connected to the solenoids.
I was toying with the idea of a solenoid earlier this week, but I'm concerned that it's motion is too fast which would scare the mice. I feel that the quick switching of a solenoid, combined with amplifying it's small movement into a 3 inch movement, will open the door far too quickly. Honestly, though, I'm not too familiar with some of the applications of solenoids and, perhaps... do you know if there are ways to slow down the action of a solenoid?
By the way, thanks a lot for your help
I am not very familiar with solenoids, sorry. That is my easy solution for you. 
No problem, thanks again for the help.
A usb 6501 (99 dollar DIO device) hooked up to some simple relays and some simple motors would also work, and slower. 
It depends what's triggering the events. If it's just checking to see if a control changes then you have to just poll that control and use a feedback node to see if it changed from the last time you read it. If it's other stuff you may have to use queues.
Thanks! &gt;If it's just checking to see if a control changes then you have to just poll that control and use a feedback node to see if it changed from the last time you read it. This is basically it. There are just two buttons; one for running the measurement and one for saving data to a text file. What I did was probably a bit messy: I made a four-frame sequence structure. In the first I placed the two controls in a while loop and used an OR to check their states. If either is true, the while loop ended. In the next frame I enclosed the measurement stuff in a case structure, with the "run measurement" control boolean as input. The false case did nothing. The third frame contained the save data stuff in a similar way, while the fourth used local variables to reset both controls to false. 
If your false case is empty then you can just remove the inner while loop and rely on the outer while loop to check the state of the controls. Put a short wait in the false case so you don't consume excessive CPU.
Use global variables. 
I am trying aarrays, but I will keep that thought in mind
I'm assuming you're trolling. Yes, you can use globals but they are not a best practice. OP, this looks like a homework problem. Try your textbook! Hint: arrays are not the best approach. Edit: Here is NI's [solution](http://zone.ni.com/devzone/cda/tut/p/id/5317); it's simple and better to use over the long run. Globals are generally a bad idea in Labview, because they break the data flow paradigm. I can't recommend even for a simple program because of how things evolve and change over time, it will make debugging more difficult as you add complexity. Edit2: [Here](http://labviewwiki.org/Functional_global_variable) is what you should use.
There's a lot of ways to do this. If you don't need to change things continuously, I might use a config file as a simple option. Save the values of all of the controls and then pull them into the indicators when the program runs. I'm assuming you know of property nodes to change control/indicator values. Another way to do this might be functional global variables, [which are explained pretty well here](http://labviewwiki.org/Functional_global_variable). This would allow you to dynamically change the prices and see them update in the right side panel. Of course, depending on what you're trying to do (and this looks like a homework assignment so I don't assume you have a choice) there might be a better way to architect your code in general to accomplish this.
we don't have a text book... 
Is that a completely separate VI or a subVI for a mainVI? Globals are actually a valid solution here if you are passing between separate VIs here. If it makes arcandor feel better, you can pass the reference to those controls from one VI (via globals), then use that reference to directly edit the values of those indicators via a property node. This would also mean, you would actually only be passing the data for the references between the VIs once. If you are passing to a subVI, a cluster would probably make more sense for you, depending on how you are using it as a subVI. 
As others mentioned, it depends if these VIs are meant to run completely separately, at the same time, or one as a subVI. * If they run at separate times, possibly on different computers: Create a config file (.ini) and use that to store values from your left VI and read values in the right VI. * If they run at the same time as separate processes: Global variables are OK, but make sure to use WORM globals: Write once Read Many (Thanks to... someone on the NI forums. Sorry I can't give credit!). If they run on different computers, you'll need to use network variables. You can also use references from one running VI to the next, but that's a more advanced method and frankly I think it's a pain in the ass. * If one is a SubVI of another, clusters, or clusters of references, are your best bet. References tend to be considered better code, are more powerful, and have the nifty feature of instant-updating. If you want more info I'd be glad to offer it. 
Theres nothing wrong with using global variables for this, and its local variables we try to avoid. 
Ok, no problem. Head over to NI's [developer zone](http://zone.ni.com/dzhp/app/main) where they have lots of resources for learning labview, including forums and articles on specific topics. Also check out [LAVA](http://lavag.org/), where there are good discussion boards that may have answers to your questions already. You are welcome to keep posting here, of course, as well. Good luck!
Could you give a bit more detail about what you are trying to do? The whole approach of using an arbitrarily dimensioned array is certainly a way to accomplish what you're doing, but it might make more sense to store the data in a different way, maybe a 3d array for each measurement type, for example. I can't tell you what would be best without knowing more about the instrumentation and how you get the data from it, however. Edit: looking at your example, you're taking 1 position of data at a time, for an arbitrary number of sensors. The easiest way to do this is to decimate your array from the DAQ device (sensor1, sensor2, sensor3... sensorN) and enter it into a data structure that is separate for each sensor (an array of a cluster of a 3d array might be a nice starting point). That way each sensor's data is all in one group, and should make processing it much easier, and more expandable should you add more sensors in the future.
Hi, thanks for the reply. I think I chose a bad title. There's only 1 sensor, that I am scanning around a 3D space. The example in the NI thread shows each spatial dimension being incremented so that the sensor takes a measurement at points distributed withint a cube. For example, if the all three spatial dimensions were of length 1mm in 1mm steps (so two points along each dimension, 1mm and 2mm), then the positions at which the measurements took place would be: Dimension 1: 1mm, 1mm, 1mm, 1mm, 2mm, 2mm, 2mm, 2mm Dimension 2: 1mm, 1mm, 2mm, 2mm, 1mm, 1mm, 2mm, 2mm Dimension 3: 1mm, 2mm, 1mm, 2mm, 1mm, 2mm, 1mm, 2mm This would be accomplished by the example in the NI thread (3 for loops). However, sometimes I might only chose to scan within a plane (a 2D space), or along a line (a 1D space). Or other times I might chose to also vary another parameter on top of the 3 spacial parameters (4D in total), or another on top of that (5D), etc. I wanted a way to do this without hard coding in a fixed number of for loops for an assumed maximum number of dimensions that would ever be required. I have had some replies on the thread which are all based around flattening all of the parameter values out into the format I displayed the three spatial parameters in above, but for the size of scans I'm doing this is inefficient. The example above could have the same information stored like so: Dimension 1: 1mm, 2mm Dimension 2: 1mm, 2mm Dimension 3: 1mm, 2mm In the mean time I've managed to develop an algorithm that does what I want: given values for N parameters in the second (above) format, it will take a measurement at every point in the N dimensional 'space'. It's pretty difficult to explain like this, but it does it using a for loop which goes through each of the N dimensions, incrementing its position only if the dimension above it has reached its last point, and resetting its position only if the dimension itself has reached its last point. So basically, I've fixed it, but thanks for your reply. Sorry I just made you read all this but I thought it would be rude to just post 'I've fixed it now thanks bye'. I'll probably put the code on the NI thread when I get into the lab tomorrow. 
Someone else came up with a better solution using cartesian product. I've marked it as the solution on the NI thread.
I can kind-of understand that there's no function to join all elements of an array into a string... but why isn't there a function to split a string into an array? That is one of the most basic things in programming! Btw. can anyone check if the split string function I wrote above has any bugs? It seems to work fine...
Why don't you just use Spreadsheet string to Array?
Yep, Labview has the function. Spreadsheet string to array and it's corresponding array to spreadsheet string are exactly what the OP is looking for. Understandable if you're not familiar with the palettes.
I would look into an arduino. I have employed a few in my lab for various things and it works great for controlling simple things like this. In my opinion it is better than a bare DAQ device(depending on your situation, it isnt too great for the aquisition part) since you can program the device and then access it in labview and pass it commands and not interupt your workflow. (i just realized this is 1 month old. sorry!)
Try using "Array to Spreadsheet String" and "Spreadsheet String to Array".
Haha, thanks for the reply. One month old, but I still haven't implemented anything, so it's still relevant. I was thinking about using an arduino, but I'm not familiar with passing commands too and from it. Have any good examples?
No code or anything. One thing I did was use arduino to control a bank of valves. I gave arduino it a few simple commands like "on &lt;valve number&gt;" "off &lt;valve number&gt;" and sent those commands as ascii to the arduino through labview. It just toggles the pin that corresponded to the valve. You just make a serial connection(using visa if you want) to the device. and then do serial.read(). Here is a link to arduino examples of reading serial input http://arduino.cc/en/Serial/Read It was relatively easy. I recommend getting an arduino to play with, you should be able to pick it up pretty quickly to do simple stuff like this.
Ah, that makes more sense that what I thought earlier. So, basically you just send code to the Arduino, which is in a constant Serial Read? I suppose I'll have to learn how to work with a visa, or something similar though.
Use an automation refnum (browse for the correct excel reference) and property/invoke nodes.
My question is why are you developing on 6.1? Its a decade old at this point, and if you ever want to update anything its going to be a pain in the ass.
Because $1249 is not in the budget. However, it is my understanding that Labview 6.1 VI's can be converted to LV 8.x files, but not vice versa. 
Thank you. I have seen that method before, but building it can be a pain in the array. If you know of already built VIs that can save me the time, I would appreciate it. Finding VIs for LV6.1 is difficult. We're hoping to upgrade in the next year or so, but spending the extra money has no cost benifit at this time. 
the forums at ni.com have upconvert and downconvert threads where people will up and backconvert, but going too far is pretty tough. While 1249 may not be in the budget, do realize that carrying on old code and eventually needing to update it once its far too old can ultimately cost a lot more. At 70 dollars an hour (engineering time, facilities, overhead, etc), thats only 2.5 days of work. 
If you want someone to do it for you, your best chance is on the national instruments discussion boards. But why not take a half a day and learn some labview basics? Then you can solve your current problems and hopefully your future problems will become easier to approach as well. NI has tutorials on their site, and the help file (F1) contains lots of information and examples.
I second this, the NI forums are amazing 
On reddit, that is called an upvote =)
Have two!
I very much recommend the forums, but will also say that trying to salvage someones program can often be 10x bigger task than just starting a new one if its for basic functions. It was often started all wrong, so its really trying to polish a turd, instead of just making it well from the start. 
Yeah we have been trying to learn it. Im currently going through a 100 page guide to the basics but so far It hasnt addressed a lot of things in the program. Little widgets that I still have no idea what they do. Thanks for the feedback guys I will definitely check out their site. Im also considering starting from scratch, but Id have to learn a bit more and our boss is on our ass to get it done by friday so I just thought I'd see if anyone here could do it.
What version of LV are you using? You could put some code in a drop box for sharing and see what happens.
ctrl-h and hover over any labview object to see the help text 
It's a USB device that follows RS232 communications protocol, so you can just use a VISA connection to send and receive commands from the device. It's pretty much a dead end on using the vendor supplied DLL unless they get back to you with the function specifications, and even then it might not work properly if it was written using anything other than standard C.
Unless you get their specs with how to communicate with the device, then it's going to be really tough to work with it without a LOT of experimenting. Don't be shy to try forums.ni.com as there may be someone else there who's worked with one.
As long as it's in the same event structure, there's no way for a second event to execute while the code from your first event is still executing. LabVIEW events are very synchronous: the event structure lives inside a loop and only executes one event per loop iteration.
I'm fond of the USB-6008 modules myself. Handy little buggers for data logging.
I was just out playing trivia with ~20 folks from NI, and saw this come up on my phone. Were all trying to track down who it is, whether it be one of the remote-working R&amp;D guys, or one of the power users from the forum, or maybe one of our sales guys. 
Also if you contact NI, they sell training courses. You can also try searching the [NI KnowledgeBase][http://www.ni.com/kb/] for specific errors that you're getting. Also check out the example finder at help » Find Examples... it has alot of working code that you can start from and modify.
They also have engineers moderating them helping to answer questions as well.
If it weren't a sales rep, would someone working at NI be praised or persecuted for having this? I can't imagine being a mid-level programmer at a software company and thinking this was a great idea unless I reeeeaaalllly loved my job
we actually figured out who it was, in fact. It is one of our sales reps up there. I could see a couple of our ultra power users doing something like this, quite honestly. and everyone thought it was awesome. 
The class is all hands on with Labview. Actually, that's why I'm scared to take the test because the class is more about practical compared to how the test is all written and multiple choice. 
As long as the license plate frame didn't have "Sucks" below the plate, I'm confident NI would support it / wouldn't care. Most people who work at NI really enjoy working there. We have a very low attrition rate and a very high return rate for those that leave.
I agree. LabVIEW Certification will not be the only factor in landing a position, but it can definitely help your case. +1 CLA. I really enjoyed the challenge presented with the CLD and CLA exams and I highly recommend taking them!
On using VISA to communicate with the device via USB, check out the following article to get started: http://zone.ni.com/devzone/cda/tut/p/id/4478
I can't recommend this book enough: **The LabVIEW Style Book** by Peter Blume You will learn best practices that easily help and allow you to scale your programs. Pay very close attention to the chapter on design patterns, especially Queued State Machines and Multiple Loop Architecture.
I just started a new job at the beginning of January as a "Systems Developer" using mostly LabVIEW. I have my CLAD exam today and not at all worried. http://www.ni.com/training/ has practice exams that I've passed. I was taught LabVIEW Core 1, 2, and 3 in a week here and was able to pass those practice exams immediately after. I'd say the CLAD exam is worth taking simply because it's not terribly difficult and adds a little "umph" to your resume. Where I work, the CLAD isn't worth much, but the CLD and CLA give 5k/yr raises each. Edit: I guess I should add that I've been programming for 11 years now with most of that in OOP so it was really easy to jump into LabVIEW.
So I passed the CLAD with an 80. I'd say that about half of the questions were repeats from the practice exams and the half were new. Still the same amount of questions per concept though. I'd definitely say that understanding the practice exams will help a lot.
I am working on a project for high school students where you can select an mp3 file and it shows the sound waveform like seen in the picture
A much easier thing would be to use a .wav file, because it doesnt require any processing. I made a very simple demo which pulls in a wav file, graphs the time-domain, then also graphs the frequency domain components and plays everything out on a speaker. Very quick and easy. only a few blocks. If you have something like a simple DAQ board (myDAQ is super easy because it has audio inputs), you can make a very simple DAQ demo which pulls the audio using 2 AI channels broken out from an aux cable. Students can plug in an MP3 player and you can show it. 
If you had access to a piece of C code that could decode the mp3 then it could work. No doubt that the wav file would be easier though.
yes sorry when I was posting this my prof yelled at me to get off reddit! I was just curious if its even possible in LabView and any hints or suggestions if it is, thanks
it can easily be changed to a wav file if needed
Cool. [This](https://decibel.ni.com/content/docs/DOC-14630) example should get you started.
Try the USI Audio Data Plugin... https://decibel.ni.com/content/thread/9457 (Note, requires windows)
We have 93 readers here, and I assume we want this subreddit to grow, so I would encourage posting here as well as over on the NI forums. I typically only go on the NI forums when I have a question I want answered, and I'm on reddit, well, nevermind.
I have two links to post for this, while its not a direct method to importing mp3s into labview the result is still the same. Compiled Lame MP3 encoder libraries found here http://lame1.buanzo.com.ar/#lamewindl Lame library commands here http://lame.cvs.sourceforge.net/viewvc/lame/lame/doc/html/switchs.html#-decode quick run down I'll probably end up building a simple music player or something later but here's so far what I have. command line, could use a system execution with a batch file for this in labview cd c:/program files/lame for audacity/ lame.exe --decode "file path of mp3" In labview Use read simple audio vi with the path of the new wave file created from previous step. Parse about every 45s of data max using index(or similar) input. If your song is too long you will fill up the buffer. I'm running this in parallels so it is probably the internal buffer of parallels for the audio card and may not apply for full windows. Use the play audio vi to play the sample. Then use a for loop to parse the next segment of data waiting (wait (ms) vi) for most of the buffer to clear out. Hope this helps, btw web ui builder allows for playback of mp3s. So you could create your own media server with webui and your mp3 hosted as web services. 
alright thanks for the tip
I work for an Alliance member too so certification is essential for us to go out into industry as a company to our customers. We don't require potential employees are certified as we will put them through the training if employed. Coming with a certification does help and make you a potentially more employable candidate. Being able to demonstrate good coding practice to us is more important though. There is a lot of very bad LabVIEW in the wild that we see from "experienced" Labview programmers. 
You need a while loop. Inside the while loop, wire the temperature measurement to a chart. Use a "Wait Until Next ms Multiple" timer set to 1000 to record 1 measurement per second to make your math easier. If you have LV2010 or greater, you can right click the chart while it's running and select export to excel. If you need to, you can right click the chart in edit mode in increase the chart history length so that it records more points.
Thanks. I'll post the error next time it occurs. Likely tomorrow morning.
Assuming it's a labview error, I'm guessing it could be a memory leak. Grab a trial of the desktop execution trace toolkit or just open up task manager and watch your memory usage as the program runs.
Posting code would help a lot. Could be buffer underrun or overrun, or a million other things. Are you using a producer/consumer architecture?
To start, I hope you're not using the express VIs. The DAQmx API is really simple to use and will make sure that you don't get any buffer errors. My guess is that the error you got was that the sample was no longer in the buffer. Just set up a producer consumer, the template is just about all you need for this. Place the write to file stuff in the bottom loop and maybe have the file close out every once in a while. You could also get a memory error if you write too many samples to a single file. Either start a new one or set up a mySQL DB on the host PC. Write your data to the DB and you won't have to worry about file sizes. Also remember, depending on your sample rate, you'll want to adjust the number of samples that you pull from the buffer. Oh and just as a tip if you don't want to research the DAQmx API. What you'll do is create a virtual channel (this is where you specify the physical channels). Then set your timing with the timing vi (I forget the name, but it looks like a stop watch). Set the sampling to continuous and specify your sampling rate. When it comes to the number of samples per channel, choose something that will account for at least 50ms of data. Now all of these VIs are wired up before the producer loop. Place the read vi inside and wire up the number of samples with the same value as in the timing vi. Wire your output to the enqueue vi and all you have left is to set up your write to file business. As the producer loops, you dot have to worry about timing because the vi will wait (not a busy wait either) until there are enough samples in the buffer. Windows can do everything it wants and you still get your data. 
This subreddit can also be a place to discuss labview technical questions! The official forums are a good place to start, but that doesn't mean that we can't help out here occasionally =)
Every issue filed into the NI forums are tracked by R&amp;D to see where people are having problems. If this is splintered, it means they don't have the real understanding of customer's greatest problems.
It's already splintered, ever heard of LAVA or stackoverflow or openG? Chilling the little discussion there already is on this subreddit really serves no purpose.
Ctrl+h is your friend. "Context Help" will give you information on anything you hold the mouse over. Personally, I have no idea what that thing is.
I think that is Get Terminal Name with Device Prefix VI. But I haven't worked with LabVIEW DAQ, so I don't have any of those drivers installed to confirm. [Source](http://www.ni.com/white-paper/3615/en) If you have DAQ MX installed, you would find it on your measurement I/O pallet.
[Quick drop](http://www.ni.com/white-paper/7423/en) alleviates the problem if you know the function name (and you can create your own shortcuts).
Here's a [8.6 version (ZIP)](http://forums.ni.com/ni/attachments/ni/Certification/294/1/CLD%20Exam%202%20-%20Boiler%20-%2086.zip) if that's more compatible with people. If someone would like to review with an older copy of LabVIEW please let me know. 
You definitely want to use "[Format Into String](http://zone.ni.com/reference/en-XX/help/371361H-01/glang/format_into_string/)." I coded a simple example, but imgur is failing me right now. Look up the help on that function; it should be exactly what you need. 
If you have a support contract, you just need to call 1-866-ASK-MY-NI for the future. Also, support generally can only offer limited support for network configuration problems if you are sure you are correctly publishing your app.
1. You used a single loop instead of separating out the UI. It's a stronger architecture, you won't have your UI and processes stomping on each other. 2. Array of enums is a perfectly fine way of making a queue. To make it work with two loops, however, you're going to have to make an [action engine](http://forums.ni.com/t5/LabVIEW/Community-Nugget-4-08-2007-Action-Engines/td-p/503801) to pass the data between loops. OR you can just use the queue datatype and VI's that are a part of labview. I highly recommend learning both methods. 3. There is only one subvi! Are there any other places where you could modularize the code? I'm not familiar with the specification, but it seems you should be able to use a few more AE's in there to manage button states, etc.
I am not quite sure, what your problem is. * Do you want to save data and recall it in another vi? * Do you want to save a vi and use it in another vi, ie SubVI? Please try to elaborate in a more detailed way, so we can find a solution/help easier for you.
In the NI thread, the last post's attached VI is the best approach unless you have a more complicated requirement. I highly recommend taking a basic labview class, it will help you get up to speed quickly. 
Some tips for the CLD. Read the prep guide on the ni website and watch the video. The presentation has approved architectures. Use event driven producer consumer architecture with a queue Comment everything and I mean everything. If you don't finish the code write what you would have done in the relevant states. The commenting is worth as much as the functionality and style so you will fail if you don't. 
I already have my VI sampling and displaying both the pressure transducer and the rotary encoder, just on separate charts and sampled by time. I have them both set to sample continuously. Yes, it basically functions like a potentiometer. I also have the option to wire it as an SEI bus and read it digitally, which may be more accurate and easier to work with. For now though I'll probably do what you describe and start playing with that DAQmx Timing(SampleClock) VI. I'll start trying to put together the VI to read the incremental voltage changes and make a clock out of it, I might be back again with my tail between my legs to ask you guys how to do that :)
Have you tried to contact your NI rep (in your region)? I have found the guys at NI to be very helpfull and commited, especially in education/unis...so maybe they might have some floating around or can provide you with an internal link to their page to download. The other way would be to search in more dubios/shady places we all know...but, as I said before, in comparison to the helpfullness of NI, I'd try that first. Furthermore, those practices will not be endorsed in this subreddit... Maybe I can find an old dusty copy, I'll have a look. Which region are you in? (just because of shipping)
If you call NI they can send you a copy usually though they might charge something. 
I've had great luck with my NI rep in the past, they're generally very helpful. If you've got access to the license key, he very well may help you out even though you're not the license holder (nobody's wants to "steal" LV7 at this point).
Thanks for this!
They will probably send the replacement disks to your professor if you call. As long as you have the correct information you should be able to talk to an account rep.
Are the two systems running different os? I have found windows 7 does place front panel items differently to xp. Also before the build in the source go to vi options and make sure the advanced appearance options don't have scaled to fit...etc selected. 
Can you post some screen shots?
Figured it out! For anyone else who has the same issue, Labview is well aware of this issue as you may find on their Forums. If you are experiencing font settings that differ on an application build or VI from computer to computer, you must select "Current Font". For more information, got to LabView's website, and search "font issues" under forums. 
Yes. Yes. If you can program at all, the curve isn't steep at all. Use examples. I'm sure there is at least one serial streaming example. The rest will just be parsing the strings you receive and interpreting that. This sounds like a fairly straight-forward application. Yes. Labview is meant for things like this. With the right equipment you could use Labview to define radio to join into your wireless network. Reading a serial stream won't be a problem at all.
if you check out the NI website you can find the application engineer for your area. He or she can help with identifying the hardware you'd need to set this up. It'll be possible to so this in LabVIEW. If you go this route I highly suggest investing in some of the basic training classes. You'll learn to avoid a lot of bad habits that new LabVIEW programmers typically acquire.
He doesn't need any hardware beyond a PC with a serial port. (Although it should probably have capable hardware.)
What kind of information is returned besides the unique node identifier? 
The tutorials on NI's site are excellent. Is there a particular reason you don't want to use those?
Can you give me a bit more information about what you'd like to go in more depth about, maybe specific to your application? Also, what is your personal previous experience? Do you have programming experience outside of LabView? Engineering?
Thanks a ton, unfortunately now i'm having problems installing it on my computer - I get an error. I'm currently trying to fix it.
The best place to learn for a new user is [LabVIEW 101](http://www.ni.com/academic/students/learnlabview/) If you have a service contract with NI (you will if you have a new copy of labview), you also now have access to the Self Paced Online training materials. These go into more depth and breadth than LV101. Example programs, white papers, ni.com forums, and Lavasoft forums are also all great resources. 
On the functions&gt;&gt;measurement&gt;&gt;DAQmx palette are a series of premade VI's that will allow you access to the resources on the card. Using the Help&gt;&gt;Find examples and search for DAQmx will point you in the right direction. Or you can use an ExpressVi found on functions&gt;&gt;Express&gt;&gt;Input&gt;&gt;DAQ Assisst. Once you have set up the task you can right click on the VI and select 'Open front panel' and it will show you the code using the DAQmx palette behined the express vi.
If you open the NI Example Finder you can find an example program to get you started. Look under Hardware Input and Output &gt; DAQmx &gt; Digital Measurements and find the Read Dig Chan.vi Edit: Happy Cake Day
You won't be able to simply transfer file folders from your old computer to the new one. You will have to go through the installation process of an online LV installer at http://www.ni.com/downloads/products/ using your serial number or use your LabVIEW DVDs for installation. If you are new to LabVIEW programming, I would stay on the old computer. Instead, try upgrading the RAM on the old computer for better performance. Hope this helps!
Is this a labview executable program using the runtime engine? Or does the computer have labview's full development system installed? What operating system does the old computer use? What hardware is the old computer talking to? Is the new operating system supported by Labview? (x64 might be a problem) Is the hardware compatible with the new operating system?
Careful using vi's and executables that were created using the older non-DAQmx vi's, as these will not run on 64-bit operating systems -- a lesson learned the hard way.
You can use [VI scripting](https://decibel.ni.com/content/docs/DOC-4973). If you are grading style too, check out the [VI analyzer toolkit](http://sine.ni.com/nips/cds/view/p/lang/en/nid/209042)... I think it's included in the newer versions of labview if you have a development license, not 100% sure though. 
The University of Texas, for one.
Probably anyplace technical. UC San Diego has one, and so does the extension. 
Ah, thanks.
Great. This seems to be the best solution. I was quite curious about the lack of any kind of options in the normal save to file fuction. Seems like I just used the wront tool for the job.
I highly recommend becoming an NI Alliance member and getting to know your local field sales engineer. Alliance members range from 1 guy part time shops up through very large systems integrators, and have the benefit of being officially affiliated with NI. Your local field engineers will know who in the area needs some help, and they typically like to keep a good relationship with all of the alliance members because you can be the missing piece of the puzzle for a company, and they can get you business. http://www.ni.com/alliance/
I know a guy who does this as a career. Free lance LabView. He's very talented, and thus, very successful. 
certifications? location? level of experience? do the sales reps know who you are? do people like you? any talent?
Wouldn't be a bad idea to become a Certified LabVIEW Developer first either.
It's not a bad idea at all. Be selective about the clients/projects you choose, though!
I used that VI and it worked like a charm, as soon as I forced it to use exponential notation with lots of significant digits. I really wonder who at national instruments thought it is a sensible idea to make their %g option cut all but 2.5 digits of doubles in certain situations.
I think you're using the wrong tool for the job. Serial communication is for data transfer, not direct digital I/O, and the LV interface is written as such. Writing data to a serial port is easy as pie with LabVIEW as long as you have the device drivers installed. But if you want to do bit-banging (I think that's what that means) you're barking up the wrong tree. 
Thanks, im just trying to find a simple example of some type of communication as a starting point. Everything im finding is just showing an example. Any tips?
Thanks, im just trying to find a simple example of some type of communication as a starting point. Everything im finding is just showing an example. Any tips?
Thanks, is there a tutorial like your example i can read. Your example makes perfect sense i would love to learn more about it
Open up LabVIEW and navigate to Help &gt; Find Examples. Switch to the Search tab and do a search for VISA. There's many many examples. One of interest to you would be GPIB-Visa. Here's a white paper on VISA as well that will give you some nitty gritty details: http://www.ni.com/white-paper/4058/en 
#rd party scopes can be tricky. You won't (most likely) be able capture too much data. What can normally occur (at least with my experience with tek scopes) is that you will have to manually adjust the scope and triggering on the unit. Once you're satisfied with the waveform, then you can use the provided VIs to request a capture of the waveform data. Also, what driver set are you using? I went to look for the 2701A, but I don't seem to see any drivers for that scope.
The 9214 is in a C-series chassis I presume? You can configure it using NI Measurement &amp; Automation Explorer and add some DAQmx global virtual channels. Once you have done that, open labview, go to 'find examples' and look for a DAQmx read example. You should be able to select your device and channel and see it plot on a chart in labview. You can then modify this example and use it as your program.
eww who buys agilent. btw, what are you measuring with your scope? Is 100 MHz enough bandwidth? I'm guessing you're measuring something kinetic? Also, If you are not familiar with programming, you should pick up the programmers manual for the scope. You'll learn a lot about what is going on "under the hood" of the labview program.
So i went a head a tried something simple. I used the Visa Write and sent the command SOUR : FREQ 5000 which worked great. The frequency changed to 5k bit an error icon poped up on the devices display. Any idea why it would work but still show an error. Sorry about all the questions. I really want to understand this.
Instead of 'usb', try 'serial'
To give a prompt but not very detailed response: USB is hard. It's not like serial where you can just open a connection and send data. There won't be pre-defined functions to do anything for you, which is why you're not finding any in labview. From what I understand, if you can find device drivers (.dll files) and documentation for them, you can wrap them in G to do what you want in the labVIEW environment. But you'll need both, and it'll be difficult programming if you're not experienced, and impossible without **both** documentation and drivers. 
It'd be a lot easier for us to help you if you posted exactly what you need to do (separate from what you *think* you need to do). What is the exact hardware you are communicating with and what do you want it to do? Also, have you tried searching or posting in the NI Forums or contacting NI Support?
Im doing my final project at my school and im using a Thorlabs extinction ration meter to collect data. The program that comes with the product works great (labview executable file.) The only problem is that i need to create a program that sync's up a temperature sensor with the input data. I need a reading once over ten seconds and I need to store the information within a text/excel file. The temperature sensor is the easy part i just need to be able to read the current power from the device and im good to go. http://www.thorlabs.us/newgrouppage9.cfm?objectgroup_id=2962
Everything you need to know is in the manual. Go to: **Section 4.4 - Write Your Own Application** The LabVIEW drivers are (likely) in the folder labeled LabVIEW Container File. Files that end in .llb are library files, which means, if it's for LabVIEW, it contain multiple VIs. (I'm not on a Windows machine or else I'd look at it myself, but if you want, post a screen shot of what it contains after opening it in LabVIEW.)
Thank you sir, all my stuff is at school so i will take a crack at it in the morning. I will keep you updated (if you want that is). Thanks again
I've tried the examples but they do not work. I think I have a triggering issue, but nothing I do seems to correct it.
You don't need to do an external trigger. They way a scope works with auto-triggering is that you have some sort of waveform that is relatively periodic. You use for example channel one as the trigger source and when that waveform comes through, the scope recognizes the waveform and is "triggered" to capture the data and display it. For example if you were to look at a binary signal from a serial cable. You send you data bits and the scope is triggered to capture the signal. After it is captured, depending on how you've set up your scope, the signal will remain displayed until you send another one. Most scopes look like they are always updating because the single being read is always being sent (which is why the serial com example helps best demonstrate how the scope works). The idea of the triggering is that as the waveform is acquired, it is displayed such that the signal is lined up with the previous samples. So if you're reading a 5Hz sine wave, it always looks like a steady 5Hz sine wave where the peaks are in the same position in the display window. So now as for LabVIEW what you'll most likely have to do is work the knobs on the scope until you have "framed-up" the signal that you'd like to capture (adjusted the horizontal and vertical divisions). When the waveform on the screen looks good, then you can use LabVIEW to get the waveform. The data should then return just as shown on the scope (hopefully). Again, I can only speak with my experience with Tektronics scopes. I haven't yet looked at the vi lib that is provided (just got it from NI.com but not yet installed) so I cannot say for sure if that is the right method of capturing the data. What I can say though is that if you want to stream the live data from the scope, you'll need NI scope hardware which gets very expensive very fast.
Disregard my question. I got it to work. Switch computers and the I can now see the usb drive. Thanks
It will be on a com port, it won't be listed by name. Just try making an init, read, close test program and try the different ports (via the visa dropdown control) if you aren't sure which port your device is using.
Congratulations!!! It's always awesome to hear about others obtaining their certifications. Still waiting to hear the results of my CLA exam. 
Thanks. And good luck to you. 
Nice! How'd the CTD exam/prep experience compare to that of the CLD?
Care to post your code? 
you need more then logic gates, you need memory too. You need to build a finite state machine. For a 3 number lock you obviously need 3 inputs, but after a number has been unlocked you need to feed that back as input so the "first number complete" acts as a "ok to start 2nd number check". Your last number doesn't need to be feedback but is your output "lock open". So your FSM has 5 data entry points (3 inputs + 2 loopback) and 3 data exit (2 loopback, 1 output).
wait why not just create an array out of the combo...[number1][number2][number3] and compare that to the entry? maybe i'm misunderstanding the question...
i see how a FSM would help, but i have trouble implementing the FSM. could you educate me on this please? i looked at different videos and articles explaining the FSM, and i get the idea behind it, but i dont understand the implementation.
Exactly. I'm not sure that this is more than 3 minutes of coding if you're using LabVIEW as intended. Remember that LabVIEW is a highly-abstracted language. You don't need to worry about logic gates and memory storage like in other programming languages you may know.
because windows is not that reliable. 
... Anyone else confused?
A little bit.
interesting. i'll investigate this. much appreciated. 
So I've been playing around with priorities (normal vs. high). I can say that high priority *does* make a difference, but not a huge one. It's still not very reliable
The variablily could be because you're using a divide function, which is variably CPU intensive. Also, plotting the wave form is CPU intensive. Try adjusting the loop to remove the divide - ie, use a for loop that is indexed to a constant rather than comparing the elapsed time. Also, move the waveform plot into it's own parallel loop and use a queue to transfer the data. Also, consider using a loop timer instead of a wait function if you want the wait time to actually be constant.
The problem here is variability. I think as other people have pointed out it's a problem with windows not being a real-time operating system. Will play around with the loop timer thanks
3ms is an eternity when you're talking about arithmetic. That's 6000000 cycles on a 2Ghz processor. Divide op takes on the order of 24 cycles. Edit: And yeah -- if you really needed timing info accurate to +/- a couple milliseconds, you'd want to take as much as possible out of the loop that's recording times. No waveform updates, no calculations that can be done before/after, and pre-allocate any arrays. But that's all if you're operating in a real-time environment... which Windows is not.
it won't let me for some reason. "unable to set to realtime"
Thanks! Letting hardware handle timing is what i ended up doing. I'll stick to 100ms refresh rates on the PC for now. 
Hmm, first thing I would try is making sure your .dll for the mathcad activex api is registered. http://www.wikihow.com/Register-a-DLL http://stackoverflow.com/questions/4897685/how-do-i-register-a-dll-on-windows-7-64-bit 
No sir. The "Correct Combination" object at the top is a constant array. The contents of that array are [2,8,9,6]. So an array of 4 numerics. To the right of Lock Button 1-4 is the build array function. I am turning the 4 lock buttons into an array of size 4. Next, I'm using the equals sign in "aggregate mode" to make sure that both arrays are equal at every index. Btw, if you didn't know, the above image is a VI snippet. You can save the image to your desktop and then drag the .png file onto your block diagram to instantly have a copy of the code. This will also preserve the front panel as well so you'll have a basic combo lock UI set up.
Thanks for the input, I am completely new to it and it's not a necessity. Just something I'd like to do as a contribution to the lab I work in. The old computer is ancient, like from '95 ancient. I doubt I'd even be able to find parts for it. Ill try getting a new version from the link via the old serial number and see how far I can go!
I figured it wouldn't be so simple, I'm more of wondering if the file will even be recognizable to a newer LV program, seeing how old this is.
Im an Apply person and the extent of my computing understanding stopped at getting programs online and using serial surfer, so you've lost me on that first question. The os is either 2000 or ME. The hardware is some basic inputs (temp, water flow, gas input) for a thermal spray system. The new OS would be whatever I deem appropriate considering the circumstances so whatever I need I would get. Hardware communicates via USB, so id assume so.
Okie dokie. So, check out "Call Library Function Node" vi. Switch to your block diagram, press ctrl+space to bring up the quick drop menu, and type "Call Library Function Node". Place the node on your block diagram, right-click it, and select "Configure". In this menu you can specify the path to your .dll. Once you do that, it'll fill in the Function Name drop down with the functions it finds in that .dll. Without header information, I have no idea what function does what, and what arguments it takes so I'm unable to help you further in that regard. Once you find the function you want to call, you need to switch to the parameters tab and define the arguments for the function. Again, without header data I have no idea what data types and arguments to use. After that, your node will update with inputs and outputs and you can wire up accordingly.
Alright I will try this tomorrow at school and update you. Thanks a lot, this should definitely help me out. 
You should first check NI's website to see if there are .vi's for your optical power meter that have already been built. http://www.ni.com/downloads/instrument-drivers/ I found a great vi for an older newport optical power meter that i communicate with using GPIB. 
So you're able to read the data, from here you can put this block inside a while structure so it runs continuously. Attach a shift register to the while loop and hook up a numeric array constant to it (or whatever data type your working with), and inside the loop have an append array element block to store new data to the shift register, and pass this out the other side. To control the timing you can use a run every block inside the while loop (can't remember it's proper name, under timing and looks like a metronome). Hook up a stop button to the exit condition of the while loop so you can stop it once you have enough data. As for displaying and printing the data, I'm not too sure, haven't had any experience with it. I'm sure you could export the data to another format like excel and print it there, but someone else here should be able to help. EDIT: [Found this video](http://www.youtube.com/watch?v=RbDLJ1aaG3E) which demonstrates how to use a table control. Basically the same method, replace the random number generators with your data block.
i'd still suggest looking for a "already built" vi that he could modify to do this. The reason i say that is he'll probably need to clear the memory buffer after each reading within the loop before going to the next loop iteration. usually manufactures post a .llb that contain vi's for all functions of the instrument including the initialization, grabbing data from the instrument, clearing the data buffer, and closing communication with the instrument after the vi has run. without clearing the memory or closing com correctly, it's likely that you will error out the instrument or crash the instrument you are using. 
Hello Muun. I went through your steps but got stumped on the parameters part. I don't know what to put in them. (I used Problem Steps Recorder to show you the steps I went through and where I got stumpped on.)[https://www.dropbox.com/sh/2izgi3sbp3bvr17/N5iiTRNpWu] (Here is the pdf with the functions and parameters.)[https://www.dropbox.com/s/rmrjxvuushylapf/X%20Steam%20for%20Matlab.pdf] Hopefully this could help you help me. 
well.... if you are using your crankshaft angle as an "trigger" to then take a pressure reading. I would just put your DAQ Read Event in a case structure, then use a shift register or feedback node to hold your previous crank angle value. When this value is different from your old value, take a pressure reading from your daq. Is this what you are looking for? 
Yeah I was almost sure that I wouldn't get anything on a weekend regardless.
Baud is like chunks per second, but a chunk may or may not be a whole sample. Whether it is or isn't whole, the best case would still be 115.2 kHz using serial. That sensor could work better. You would definitely know when the cylinder hit TDC and you could still get very near 400 kHz on the pressure data (something like 6000 samples a rotation). I feel like there is some way you could make what you have already work, though. See what people have to say on the forums. Those guys know their stuff.
Hey lemendoza, I was unfortunately unable to solve this using my suggestions. However, good news! LabVIEW has support for Matlab functions. Check out this link and see if you can figure it out: http://zone.ni.com/reference/en-XX/help/371361F-01/lvconcepts/calling_udf/ If you don't understand how to get it working from that link, let me know!
Am I? I'm kind of clueless :/
Hello Bertwellius, You mention that you need to create an extension onto a previously made array. You will find that in LabView, there is more than one way to complete a task. For example, you could use the Insert array function, and add your third element and write the output to a local variable of your existing array. There is also build array, in which you may append and element into your array. I am curious to know how you solved your problem. 
Try initializing the array before the for loop. [Example 1](http://imgur.com/eoU77): Appends 4 new random numbers to array every time the VI is run. [Example 2](http://imgur.com/4YhLw): Initializing the array before the for loop avoids appending new values to the array each run.
Thanks for that. Got it taken care of. Appreciate it!
Think of case structures like an If..else statement in a text-based programming language.
I'm not 100% sure what you are trying to accomplish. What type of device are you interfacing to? You could use a while loop with a shift register initialized to 1, increment the shift register every cycle of the while loop, and once the value is greater than 8, set the value to 1. You can change a control to a constant by right clicking the terminal in the block diagram and selecting change to constant. If you still need help give me a bit more information on what you are trying to accomplish.
I think it might be less of an "understanding/getting it" issue, but more of a "does this belong here" issue. The democratic reddit voting system shall decide on this one, and it doesn't seem to bother too many people. 
An influx of meme posts on this sub would probably be A Bad Thing^TM but I think as long as it's LabVIEW-related and still conveys something vaguely interesting about the process of using NI stuff, it's no problem.
What is your stress level like? Do you work 40 hours per week..more or less?
Thanks for the info. I'm not working for a company or anything since it was for a project.
That sounds amazing. So you have a set number of billing hours per project? Or the company? Sounds like you aren't in salary?
No, I'm salary. We charge the client X amount of dollars to complete their project partly based on the amount of hours we estimate it will take to complete. Those are where my billable hours come from.
Hi, i take it, you have used [this](http://forums.jki.net/topic/2043-arduino-package/)? Worked quite well for me, but having it run autonomously on the target without the code running on my pc didn't work yet...but i have only checked the **uno** board for funsies... I'll update you, if I get any new insight...
Let's clarify a few things. When you say oscilloscope, do you mean like a virtual oscilloscope user interface or a real physical oscilloscope? What version of LV are you using? The recent ones let you right click the graphs and you can transfer the data into excel or notepad. A block diagram and front panel screenshot would be useful. What sort of things have you tried to do to fix the issue?
Hi. It's a Virtual Oscilloscope UI. Using LV 2012. I'm been making a lot of changes the past few days, so I'll put up a screenshot once it is in better shape. I'd rather not have screenshots of long sequences across pages. Attempts: First, I was adjusting the Voltage multiplier and added offset to the Ymin and Ymax. (Y+4)xV = Ymax (Y-4)xV= Ymin Y is offset 4 divisions above and below center line V is voltage multiplier The SubVI I was originally working with adjusted both the Voltage multiplier and the offset. This is the only thing that has resembled a solution. The scope's range changed in values (plus and minus) while the signal remained where it was visually. Volts/Division multiplied by 4, then added Yoffset. V/D fed into a For Loop with into a Case Structure. V/D multiplied by -4, then added to Yoffset; followed by a branch into the For Loop via Shift Register adding to above V/D in Case Structure. This adds the divisions to the graph, feeding into YScale.MarkerVals[] in WFGraph(Strict) with WFGraph Refnum. ____ Next few attempts were manipulating the location to add the Yoffset, to no avail. I then tried using the WFGraph element: Y-Scale&gt;Range&gt;Offset and Multiplier&gt;Offset. If I remember correctly, when I did that, as I adjusted the offset, it would simultaneously scale the Y axis. Furthermore, I tried just using both the Multiplier and the Offset from the WFGraph Property Node, but then my signal was just being Auto-fit for some reason (when Auto Size Axes were both off), and likewise, the offset is just changing the numbers at max and min. With this method, I am obviously missing some kind of controls, but I don't know what I need to use to correct this. I'll try to get the pics up tomorrow. Sorry for the lack of clarification. 
I think this is just a math issue. Ymax = NumberOfDivs Ymin = -NumberOfDivs Don't use the offset and multiplier, just the max and the min. We'll manipulate the actual data to put in the offset and multiplier: Y1[time]=(Voltage1[time] + OffsetVoltage1)/ VoltsPerDivision Y2[time]=(Voltage2[time] + OffsetVoltage2)/ VoltsPerDivision Are you using charts or waveforms? Have you tried right clicking the plots and copying the data to notepad or excel? You can use a VI snippet to make a screenshot of the whole block diagram: http://www.ni.com/white-paper/9330/en
Yeah, Windows is the limiting factor. Try running the same test with a virus scanner going on in the background, and you'd be shocked. Realtime OS's do exist. http://www.ni.com/white-paper/3938/en
Yes, if ignoring the canned functions, I've been struggling with the math issue. I've been feeling like an idiot not being able to figure this out, but when I looked back through my notebook, I had what derives into what you suggested above. I'm going back now and trying to play with the signal directly. Also, thank you for the snippet tip! I've linked to it below - goes to a G+ account, which is mine. I hope it works. I know that the SubVI (Voltage and Offset Destroyer) is wrong. Currently it multiplies V/D by the +Divisions and -Divisions, then adds the Offset. From there those values set YMin and YMax. Also YMin is fed into a for loop to add V/D 9 times to set up the YScale.MarkerVals[] Just to clarify, Y[t] is the position of the signal after the scaling. Voltage[t] is the signal going to the graph, which is added to the Offset, and once those are added, diving by V/D. Once I get that value, I have to set the limits and divisions? Finally, I'm unclear as to how to manipulate the signal via the property node. I just don't know exactly which setting to use. Would "Value" be correct? https://plus.google.com/u/0/110508664936801518375/posts/1yEgJDCokrQ
Okay. I've finally gotten through the math issues because it seems to be working re: offsets and multipliers. This last issue now is how to change them in real-time. I change the volt/div and time/div in an Event Sequence, however, when I have attempted to add the change in Y-Offset value, I get no change. In fact, even when I add a constant of 1000, I see no change. When I keep the Offset changes outside the Event Structure, they do adjust the offset correctly, but only after I stop and restart the VI. This is so frustrating to nearly have it, but no matter what I try, I can't seem to figure out how to make it work. :( Posting the Snippets below using Imgur this time. [Imgur](http://i.imgur.com/rfP7T) [Imgur](http://i.imgur.com/6VuZ1) [Imgur](http://i.imgur.com/IJcal) 
The problem is data flow. When your code runs, it's not going to be executing the user event continuously. It's probably going to be running the timeout case of your event structure. I'm guessing you have nothing wired to the right hand side of the event structure in that case which will default the value to 0. That 0 is going to propagate to your offset VI. You can turn on the bulb tool at the top left to see if I'm right. The bulb tool lets you see the data flow. To keep yourself from making a mistake like that you can right click the terminals on the right of your event structure and uncheck the option for using default data for unwired terminals. That will force you to wire the terminals in every case so you're explicit about their value BTW, you shouldn't have a 0 wired to your event structure (at the top left, looks like a small blue square hour glass. You should experiment with different numbers in there (like 1, 50, 100, etc) What you need to do is a bit tricky. You need to create a new event case for when the value of anything in your "control panel destroyer" control changes. The event structure will have a "new value" node that outputs the new value of your "control panel destroyer" control. Right click the pink oscope terminal on the bottom side of your while loop, select the option to make that a shift register. A matching control will appear at the right. Anything you wire into the right will show up on the left in the next iteration. Tee off the wire from the shift register on the left and wire it to the event structure wall. Wire the "new value" from your event structure into the shift register. Go back to your user event case of the event structure and right click the newly created pink terminal from "new value". Select create linked pair. Click the newly created left terminal. This will wire the value from your shift register back into your shift register for every case except the "new value" case.
No, this is not free... sorry. Maybe you can get work to pay for it.
A queued state machine (where you can queue up multiple states to execute in sequence) is basically equivalent to an event structure in a loop with user events. The main advantage of using events is that they are type safe. That is, each user event type knows what type of data is associated with it, and you can't wire the wrong type to it. If you use a queued state machine then you have to use some kind of generic container for the data that goes with each state (usually a variant), and that allows you to easily use the wrong type. You won't figure that out until you enter that state and get an error at run time. A third alternative is to use the command pattern as described [here](https://decibel.ni.com/content/servlet/JiveServlet/downloadBody/18429-102-2-34915/Introduction%20to%20Object-Oriented%20Design%20Patterns%20in%20NI%20LabVIEW.pdf). This is basically an object oriented version of the queued state machine that is both type safe and has lower coupling between the code executing the loop and the code queuing up states. I usually prefer this approach overall, but it does require understanding object oriented programming. 
Oh, another thing to keep in mind is that you should have at least two loops running: one to handle UI events, and one to handle the processing events. If a user presses a button then you should have the UI event loop just queue up the action to take and have that action execute in the other loop. That keeps your UI responsive. 
If I had only one loop in my entire application, that loop would definitely have an event structure in it. Event structures are very powerful but there's certain things you don't want to do with them because you'll lock up your UI. You can put a state inside of your event loop if you find it useful. Just wire a type deffed enum into a shift register on the outside of your loop and you have a state you can keep track of. The "machine" part of your state machine is already handled by the even structure and possibly user events. You only need to add other loops if you have another "job" for your program to do. Don't add another loop just to have another loop.
Ha. I am now. I just looked queues up. This whole time, I was thinking he was referring to the concept of a queue, not an actual LabVIEW construct!
Queues (the construct) are awesome and worth looking into them. They solve many issues (eg blocking). Fortunately, events structures have some built in queueing (the concept) in them [example](https://decibel.ni.com/content/docs/DOC-21277).
As I explained above, having a separate loop for processing tasks is a much better design because it allows the UI to always be responsive. If you only have one loop and that loop is busy then the app won't be responsive to the user. For a small app where you don't especially care about the UI you can simplify things by having just one loop, but if you care at all about the quality of the UI then you should always have at least two loops. 
I'd have to know more about what your loop is busy doing to know what the best path is. For example, it's not too rare that you actually want your UI to lock up to prevent race conditions. For example, if you have a save button and the user clicks it, you don't want the user to be able to go in and change the state that you're trying to save while the "saving" is occurring. If everything except saving is fast in the above example, (for example you have a button that updates a list box, the operation takes 5 ms), it's better to take the 5 ms penalty in responsiveness than to have to deal with other means of locking during the save. Like they say, only the Sith deal in absolutes and a single threaded application can cut down on many headaches.
It's a beautiful thing. I was very worried about race conditions before I learned a little about queues. I'm having a LabVIEW paradigm shift again!
Your program right now has one loop. Aside from the simplest programs, you should have at least two loops (and not more than two without good reason). The first loop handles events (user input) and the second loop takes care of the automatic, non-interactive code, such as monitoring, data collection and control. This second loop is usually a state machine of one flavor or another. How do you communicate between the two loops? Shared variables can work, but are limited. Queues can be good, look into those. Two more concepts can be introduced: [functional global variables](http://labviewwiki.org/Functional_global_variable) and [action engines](http://forums.ni.com/t5/LabVIEW/Community-Nugget-4-08-2007-Action-Engines/td-p/503801). Master those and you are on a fast track to being a good labview developer.
I work with Muun and just stumbled on this post lol. But I agree with him. Stress is pretty low, im only a bit stressed testing a large control system for obvious reasons. With large systems there is a lot of power and energy your code is controlling. Overall very fun though.
NI has a product called Requirements Gateway, is that what you are asking about? Other than that, there is no formal framework for requirements in LabVIEW. What is commonly used is called a Software Requirements Specification, which is a stand-alone document that describes how the program will work. Definition via [wikipedia](http://en.wikipedia.org/wiki/Software_requirements_specification) and how to write one [here](http://home.adelphi.edu/~siegfried/cs480/ReqsDoc.pdf) (pdf). 
Not only do people still use requirements documents, the rule is: Never do a fixed price job without one. Ever. Time and material only without a requirements document. And Requirements Gateway is a good tool with ton's of examples. 
Thank you so much for this helpful reply. With this information and some other tinkering, I believe I have that entire portion of the code up and running! I'd have replied sooner, but I was trying to get everything finished before the holiday, and that didn't happen. Thank you, again.
global variable would be a very easy way to do it. 
have a control in the main program which sets the value of the global variable between true and false. In your subVI which is running have a case structure around everything that reads from the global variable. its not the most hardcore robust way to do this, but it will get the job done very simply. 
why is it a sub vi? I would start by evaluating that first, because the solution to your question is unnecessarily complex
I can't get it to work. I've altered the subVI so it now starts when run, so I only need to send a stop signal to it now. Here is the sub VI. https://www.dropbox.com/s/ieiez7t4ugux6jc/record.vi All i need is for the main program to be able to stop (by sending the signal to the SubVI) and then it outputs the data. Please take a look if you have time, thanks for your help.
Sorry about the messy VI haha. First time writing a program in Labview
I'm using LabView 11. Thanks for the help, I'll try to do it tomorrow (3.30am here). If possible could you download the VI in the comment above (http://www.reddit.com/r/LabVIEW/comments/16ew8m/beginner_question_startstop_subvi_from_parent_vi/c7vewqe) so you can see it a bit more specifically. I still am not sure why I cant just wire a button straight to the terminal of the subVI, the stop button inside the subVI works fine. Thanks for your time.
The waveform data type is essentially a combination of the array of data points along with a t0 (initial time) and dt (time between samples). If you use the waveform version of the DAQ function then you can either just save t0/dt in your file or you can use them to calculate the time for each sample. You can get t0 and dt from the waveform using a function available in the waveform palette. It looks a bit like an unbundle node.
If you take a look at where you place the delineation between the main vi and the sub vi, you can simplify the problem. For instance, if you moved the loop from inside the subvi to outside, called by the main vi, you don't have the problem anymore.
Get waveform components
Okay, thank you. The person I work with that is the relative "Labview Person", I wouldn't dare venture to say expert, changed my initial Waveform Setup to DBL without really explaining why. I then built the rest of it based off of that. I'll make the changes and hopefully won't have to keep begging for help! :) I appreciate all the feedback very much. 
You can create the channels, pass them into the loop, and then close the channels after the loop finishes.
NI actually offers some resources for CLD prep, I'd definitely recommend looking at the sample problems and trying to solve some of them. Here's the NI guide: http://www.ni.com/white-paper/3425/en Documentation is almost as important as actual functionality. The project you turn in should run (no broken wires or broken Run arrow) but as I understand it, it's pretty rare to actually finish the project so in that case you'd want to have placeholder VI's with proper documentation to say "this VI would do &lt;such and such function&gt;" You can run the VI Analyzer to catch some simple mistakes, and I'm not sure but I think they actually use the VI Analyzer to do some initial grading, so that would be a good tool to run through your code. I've heard from others that the state machine architecture is important, so I'd study that. One thing I see a lot of people do poorly with state machines is the shift registers--a proper implementation should have a shift register for the state enum, one for the error, and another one of a cluster that holds all of the state machine's data elements. So if you need to pass three separate pieces of data between states, rather than have three shift registers, bundle them into a cluster, typedef it, and put that into the shift register. It makes your code MUCH cleaner and easier to add new data, as well. I'm in the same boat too, taking the CLD in Feb. I actually work at NI, but all of that info isn't any kind of official statement about it, just what I've heard from people who have taken it. Check out the ni.com link though, it should be a great resource for preparing.
This is some very good advice. I have my CLD (just took the CLA) and also helped co workers prep for the exam. When I took it, I thought I'd use a producer/consumer, however the more I looked at the requirements, a simple state machine was proving to be the best architecture (and almost seems like the test author was wanting you to use one). I can almost guarantee that you WILL use a state machine architecture and that you WILL need to enable and disable/gray controls (you'll need to make a cluster of control refs). Be sure to make all of your clusters and enums type defs and take a look at the pratice exams that are freely available on the NI website. The biggest tip however, is to READ THE ENTIRE EXAM FIRST. You'll have a list of all of the requirements and it is best if you read them all first before starting. I did finish my program and from what I was able to test, it worked as required (if you do not finish, you can still pass). However, I only passed with a 78 because I severely lacked documentation. As a disclaimer, I do not work for NI, I am however an alliance member and CPI. Best of luck.
I took my exam in January so am waiting for the results. As said by everyone else a state machine architecture is most certainly wanted. I found that the actual problem I was given was longer than the practice exams NI supply but no harder - I've found finishing all the practices in time fairly simple but didn't complete the task I was given unfortunately. Another one that has been mentioned several times (CLD prep session and online) is that your VI block diagrams should all fit on screen at 1280 * 768 resolution. A tight squeeze but possible, what I did in exam to make it a bit easier was change the monitor resolution to match this. Finally; DOCUMENTATION, DOCUMENTATION, DOCUMENTATION!
I actually hold my CLD. The guide that DManTech linked is what I used to prepare. Read the guide and watch the video. I would also recommend that you do the practice questions. The architectures are recognisable from the practice questions that you would apply to the different problems you may get in the exam. 
Wow! Thanks to all for your comments! This helps a lot! DManTech, I will folllow your link and also suggestions for documentation. What I am gathering from here, and other places, is that documenting, commenting, and a orderly structure are all very key parts of the exam. 
All the advice you have gotten so far is spot on. One additional comment I'd like to make is that you be familiar with the producer/consumer architecture. Quick guide: https://decibel.ni.com/content/docs/DOC-15453 The idea is that you start the software by obtaining a queue and passing that queue reference to two parallel loops. The top loop enqueues data that the bottom loop dequeues and processes. Don't spend a whole lot of time on studying this, but there are one or two CLD problems that need it. I personally had a state machine exam but I had two co-workers fail because their problem required the producer/consumer architecture and they were not expecting it.
I think the queue based producer consumer state machine architecture is pretty much THE architecture for LabVIEW. The language lends itself to this style so well that I'm yet to come across a mid complexity or higher problem that can't be solved simply and cleanly with this.
Probe the number coming out of your black-box equation when the joystick is at each extreme of its range. Use subtract and maybe absolute value to get it into the range and polarity that you need. If you can post a pic of the block diagram, I can help more.
Hi! I'm an Applications Engineer at NI, so hopefully we can get things going for you. Another great resource for help is our forums (ni.com/forums)...our community is really active, and if your post isn't answered sufficiently by someone from the community, me or another engineer will respond with some ideas as well. Anyway, on to your issue. I'm not sure how familiar you are with LabVIEW, but when you create a new VI (our terminology for a program), you'll have two views, a front panel and a block diagram. The front panel is a visual display of input and output from your program, and the block diagram is your actual code. You can switch between them with Ctrl+E. The DAQ Assistant is code, so it'll be on the block diagram. Once you're seeing that (white screen with no grid, by default), you can right-click and see the "function palette" which has all of the different functions you can use in LabVIEW. The function palette also has a Search button on the upper right. Click that button and type "DAQ Assistant" in the search. Hopefully, this will show you the DAQ Assistant. If so, when you click and place the DAQ Assistant on the block diagram, it should pop up the dialog window you see in the video and allow you to set those options. If you don't see the dialog, double-click the DAQ Assistant block. That should get you going, assuming everything is installed properly. But what if it isn't? The order of installation between LabVIEW and drivers is important--you want to install LabVIEW first and then the drivers. If you didn't do it in this order, a repair install of LabVIEW and then a repair of the drivers will usually fix it. And to be clear, you want DAQmx 9.6, which is the latest version of the driver--you can get that from our site at ni.com/downloads if you need it. Let's start with those steps and see how it goes, and if this doesn't resolve the issue, definitely post back and I'd be happy to help you further, or as I said above, you can also post on our official forums!
Put the function in a case structure, with an invert button controlling it. In one case wire it through, in one, take the negative of it. 
@djodom @DManTech @kmoz @Muun I'll try to screenshot it, it'll be a day or two, the equation is really arbritrarily complex because neither of us (the 2 programmers on the team) are very good with LabView. It doesn't allow for positive/negative inversion because my fellow doesn't keep things organized, and there are already lots of pos/negs in the equation, he might as well re-write it (which I might do later in the season when I don't have any deadlines. How would I put in an "Invert" button?
I'm not exactly sure why the equation needs to be so complex, but I'd recommend looking at the raw data values from the joystick itself. You could possibly invert those rather than the output of the equation. Since this is January and you're talking about joysticks, I'm guessing this is a FIRST issue--good luck to you and your team!
Indeed! I actually inverted the raw data and it turns out it didn't reflect the final output (like you said) so the motor still spun in the right direction! Haha the joys of graphical programming. Thanks for the input guys, is there like a close topic option or something? New to reddit
What do you want to do with LabVIEW? Are you just wanting to work on software programs, or are you looking for something involving DAQ hardware as well? You could try making a calculator application--make it look just like a real calculator, with buttons for all of the functions, as well as a way to enter numbers and an indicator to see results. If that's too easy, make it event-based rather than polling. Then make it write the results to a TDMS file. Then make another program that displays the results by reading from the TDMS file. I'm sure we can all help you more if you can give us a better idea of what you're looking for. Welcome to LabVIEW, it's a really cool and easy to use way of writing programs.
your set up doesn't work because what you want it to do doesn't follow data flow. There are a number of things that you can do. The first is to remove the loop inside your subVI. Make a loop in you parent VI and place the newly loopless subVI within that one. You can then remove the start/stop control from the subVI and use it in your parent VI loop. The second option is to have the subVI open up as a separate window. This can be done by either using a property node, or change the vi properties from file-&gt;vi properties and change the VI to be open when called. You can also create a subPanel on the parent VI and in a more creative way than I'd like to discuss here pass out a ref to the subVI. You can also use global variables to control the start/stop. Like the option above, I don't recommend it. Not to criticize, but these are more advanced options and will most likely cause more trouble that its' worth. I hope you go with the first option, of removing the loop from your subVI. As a general rule of thumb, it's not recommended that you have while loops with controls within subVIs. Although it can be handled, you'll have to break data flow to do so. [edit] I'm a CLD and CPI by the way.
do not ever suggest global variables to anyone. It breaks data flow and is an easy way for developers to get lost with their code. Although yes this is certainly a solution, it is absolutely not best practice.
very true also. A FG is a better option to use, however still for this situation any type of global variable is not the best practice solution. The subVI should just execute the basic function within the loop. The looping process should be handled by the partentVI.
Global Variables should definitely not be used for everything and should be used with some caution, but suggesting to never to do it is just wrong. You just have to be conscious of race conditions. Single writer/multiple reader rules in effect. 
I wouldn't recommend that type of calculator. I asked a new hire to do that once, and we found that to do it right, would be a bit difficult. However, a state machine that takes two numeric dbl control inputs and looks at user input to select the math operation to preform on the two controls, would be an ideal starting point. Might still be a little out reach for someone who's really new.
agreed. I may have over stated to never suggest them. There are good programming techniques that can be done to avoid using them. So I'll reword my statement. Global variables should not be usually be suggested as a first option. It's lazy and they will work as a band-aid in this case. If your code is full of band-aids, you'll have trouble managing it. I will however as a point to your promotion of GVs give an example of when it's best to use one: as a constant. If you have some sort of constant that appears throughout your code, you can use a GV in this case. Since you'll always be reading from the GV (it's not a constant if you write to it), you won't have any race conditions. So for example if you've named a queue. Use a GV here with the default value set before runtime and you'll be sure to use the right name every time. A type def would also be an equal solution to this, but would require the tiniest amount of additional work.
The command pattern has turned into the actor framework, but I still use it presently. If you can lock down the command pattern, you can consider yourself and advanced developer. There are many developers who still don't get LVOOP. Start with the Producer/Consumer (assuming you're not yet familiar) and once you have those "mechanics" well understood, move onto the QDSM (queue driven state machine, same thing as QSM). You can eventually rewrite your code to a command pattern once you understand the other structures well. It would be a good means of reasonable stepping stones.
Well I made virtual yahtzee die in labview where you can roll all and roll them one at a time 
Well I'm looking to be able to use labview for FRC First Robotics Competition and thank you for your input :) 
What problem does this solve? I like to see original content like this, but I don't understand why I would need to use the code as you've presented it, versus any other approach.
Agreed. I just remember at NI Week 11, they did their LVOOP demo with the Command Pattern to demonstrate a plug-in architecture. For some reason in my mind I relate this example with the angry eagles and the cooler fan explanation.
the idea here is to reduce the number of loop iterations to extract certain data points from an array. For example: Let's say that your array is of numeric doubles and you need every value that is greater than 5 (all other values are outliers). You can wire the array up directly to a greater than (top terminal) with a contant of 5 wired in as well (bottom terminal). The output will be a boolean array. Use the new boolean array as your array to search through and return an array of indexes. In the sort portion, pass in the original array and the outliers can be removed. It's beneficial with large data sets; so that you do not have to loop through every single value. I'm sure there might be other approaches and this may not be the very best algorithm. There are other methods such as union-join which may be more beneficial in certain applications for example. This one in particular attempts to search and sort as quickly as possible.
I learned about true false boxes idk what they are called 
It might be more relevant to ask how much LabVIEW experience you had before you passed the exams.
I passed my CLAD when I was 20 and my CLD about a month after that. I really only had 2 months of experience, but I was using LabVIEW a lot within that time.
I passed my CLAD at 18, then my CLD at 20. They have since expired and I have not been motivated to renew it.
Passed my clad in September and my cld yesterday! Cut it far too close but hey, a pass is a pass! Age 24 but have only touched LabVIEW the past 6 months.
If you've prepared like you say you have, you'll likely be fine. 
Run VI analyzer on your code before you submit. 
Somebody told me that was just an add-on! Checked it out, not sure how I haven't seen that before. I will definitely do this to check my documentation.
I still haven't learned about INI files, I'll have to look into that! As for the state machine, I have that down. I don't think I'll use the producer consumer, I much, much prefer the method of having an event structure where I create an array of states, functioning similar to a queue. I still have to build a producer/consumer for my LabVIEW class every once in a while, so I got it down decently. But yeah, the state machine is the premiere template. If you aren't using a state machine, you're probably doing it wrong. Good call on the string manipulation. I'll be practicing it alot, I want to be able to fire off stuff like that without any errors that might burn a solid 15-20 minutes off time, when it should only take 1 with practice. Same goes for the elapsed timer. I had no idea about it until I started looking over the samples, and now I get the necessity. If I see that timing has to do with it at all, which it will probably have a 100% of, I'll probably just code it up in 2 minutes off to the side and grab it later when I need it. I don't really know how much every studies for this exam, but I'm taking it extremely seriously. I would really like to get into the LabVIEW industry after I graduate, and so obviously passing this is really important. 
Also, question. Do they want tip strips/descriptions on every front panel object, even in the SubVI's? I know to fill out VI properties &gt; Documentation everywhere, but I am not sure about the tip strips.
Document document document, its very easy to get full marks in that area which is almost 33%. Don't worry about finishing, instead take your time and do everything properly. I lost marks for things being a little messy (badly placed wires etc) and missing a couple error terminals that should have been wired up. In a prep class during one of the NI Open days the lecturer made a big point of putting an FGV in your code, there's marks to get just by showing you can make one. Simplest option would be a timer.
It couldn't hurt but labelled wires and comments on subvi block diagrams should be done first.
I can't remember. I'd say do it anyways. Documentation is free points. When you only need a 70 to pass, a free 10 or 20 (can't remember how much) points is a big deal. Edit: divadsci says nearly 33%
At least one of your graders will run VI Analyzer and they're likely to use it when assessing your style points.
[This](http://forums.ni.com/t5/Certification/CLD-Tips-you-can-t-have-too-many/td-p/2145314) is a great thread on CLD tips from the NI Forums. And here's the list of possible exam scenarios from the CLD 2012 prep guide: 1) Coffee machine The coffee machine simulates ingredient storage, and performs grinding, brewing and dispensing operations to prepare hot water, coffee and latte. 2) Microwave oven The microwave oven simulates manual or pre-configured staged, recipe based cooking. 3) Security system The multi-zoned security system simulates the arming, disarming, tamper, bypass and alarm functions. 4) Thermostat The thermostat simulates scheduled programmatic heating and cooling control for heating, ventilation and air-conditioning (HVAC) system. 5) Treadmill The treadmill simulates manual or multi-staged program mode that controls speed, incline, time and tracks run time and distance. 6) Vending machine The vending machine simulates the storage, purchase, and dispensing of products using U.S. currency. 
Read the chapter on structures. There is one with a push button to stop. Throw it over your entire vi.
Is their a link to it? Their is nothing in my book on structures... 
There are a few ways you can assign labels to a button. One way would be to use an event structure in a while loop to listen for value change events on the button. Then, based on the event that fired you can output a value. Another way to do it is to assign label names with the value of the button. So for example the label for your button representing 1 would be "1". Then, using property nodes we can get the label text and parse an integer from it. Example: http://imgur.com/AeHxcex By the way, you said you're new to LabVIEW. The above image is called a "VI Snippet". There are special images that have the source code in the image encoded into it in .png format. This means if you take the image and save to your desktop, then drag the the .png to a block diagram, LabVIEW will make place a copy of the source code on your block diagram. Edit: The implicit property node is not shown in the image correctly due to the way LabVIEW creates VI snippets. Instead, what you see is an explicit property node where "Bool Refnum" is a reference to the button labeled "1". One last thing I want to mention, while writing up a reply, I was writing out your entire project solution. I realized once I finished that I didn't want to give you the solution to your project. So, once you finish your project, let me know and I'll post my solution with comments to show you the different features in LabVIEW.
Am I right in assuming you have no programming experience at all? My explanation depends on your response. 
yes you are right, i have no programming experience at all.
I've found a much simpler solution, which I've sketched out [here](http://imgur.com/EM7eTOz). I don't have labview installed, but you should be able to get the general jist of what the code does. This way, you(by which I mean OP) learn general basic programming concepts, as opposed to labview's occasionally obtuse refnums. 
That means the first place you want to start is the basics. Learn what 'for loops' and 'while loops' are, what 'if statements' (labview calls them 'case structures'), what different data types are, what arrays are, and what functions are. Then, you can get into some of the particulars of labview, like how different buttons behave 'mechanically'. It'd take a long time to write up an explanation on this stuff, so I suggest googling, and then posting here when you have specific questions. **EDIT:** Also, labview has a lot of useful tutorials built-in. You can click the question mark in the block diagram to have labview explain every element of code. 
Sorry but i dont understand anything in that picture...except for the equal command
Haha, I wasn't expecting you to. If you did, you'd have the answer to the assignment, and where's the fun in that? I was replying to the guy who was trying to give you what I consider to be a roundabout solution. (Also it's poorly drawn :P) **Edit:** But when you've learned all the stuff from my other post, you should understand ~80% of that diagram. 
so for all i have been able to do is assign each pushbutton a value using this method http://imgur.com/7YqoFAu. I am having trouble coming up with making the 4 digit pass-code, i have to go to work and will work on this when i am back. If you can give me any hints that would be grateful ;). Is their anyway to make the multiple case structer into one? the compare the values within it,to a string? Should have came here earlier~ this is due in 2 days...
thanks a lot i will look into it. the only thing that sucks is that we are only using this program once and then we are done with it. My course is more on autocad and eagle cad. This part of the course my prof is trying to get rid of because their is no material offered at my college that teaches it properly, he says most of you will probably fail this demo. its worth about 15% and i can not bare to lose that lol. Anyway back to work!
You sound like an engineering student. I advise you to find a course in programming, even a basic one, because it really helps you think about problems, and being able to program helps you solve tricky math problems. C might be too comp-sci for you, but matlab or TI-Basic are great math-oriented languages. TI-basic runs on the TI-8x series calculators. 
What are your project requirements and specifications?
As far as I know, it is not possible to create an exe which can run LabVIEW in-place without installing the LabVIEW RTE on the machine. It should be noted, however, that if you install the LabVIEW RTE on a machine once, you can run any number of executables created in LabVIEW directly, with no need to create an installer for those executables.
&gt; Would like: A way to produce a standalone EXE that installs my program This is easy. First you need to [create the application](http://zone.ni.com/reference/en-XX/help/371361H-01/lvhowto/building_a_stand_alone_app/), then you need to [create an installer](http://zone.ni.com/reference/en-XX/help/371361H-01/lvhowto/build_installer/). When configuring the installer, you can set it to run your executable once it is finished. Also, you can select the components of the run time to install so you can only choose the options you need. &gt; But ideally: A way to produce a standalone EXE that installs LabView runtime (if needed) and then runs my program without installing it. If the runtime is already present on the computer, the method I described above would make no changes to the LVRT installation (although you would still have to click through it) and then launch your application.
You could store the trail in one array and then use an input at the front to select the length of a part of the array you want to have (subset or similar is the name, can't look it up at the moment). Or you have a look at property nodes, there you can probably also specify the length of the array and also adjust it while the vi is running. Hope this helps a bit.
Thanks. I am new to LabVIEW, so thanks for the help.
Glad I can help.
I was wondering if you can give this a look. http://i.imgur.com/Yqv32DG.png The particle position is updated after a while loop that updates it's x and y coordinate. Then, I tried to mimic your suggestion. However, it still doesn't seem to work. I wonder if the error in logic is caused by the fact that it is in a while loop as well. Thanks again for getting me started. 
Alright I think I see the issue. The reason I placed a For loop was just to generate a bunch of points. You do not need the For loop, you can just keep the code in the main While loop. That code should run exactly one time each iteration, it is updating your lossy array each time you get a new XY pair. 
[Here is my VI](https://rapidshare.com/#!download|855p1|2723590544|particle.vi|49|0|0)
It's been difficult learning labview coming from other languages. I hope to learn and get better at this. Thanks again. 
Hmm [it won't let me download it](http://i.imgur.com/9sOguXa.png)
I know exactly where you are coming from! I had a number of years of experience with Python and C++, then I kinda just got thrown into LabVIEW. It takes some getting used to, but it is a very powerful and robust language, I've been using it for over 5 years now. If you have any questions as you are going along, the [NI Forums](http://forums.ni.com) have some phenomenally smart people that are very helpful. I'm on there too quite often, I'm [Cory_K](http://forums.ni.com/t5/user/viewprofilepage/user-id/108556)
[Sorry about that](http://rapidshare.com/files/2723590544/particle.vi) 
[Give this a shot](http://rapidshare.com/files/247004094/particle2011.vi) I got rid of the For loop. I moved the array initialization outside of the While loop. I also fixed how you were indexing the X and Y coordinates out of your array "Particle Position". That should do the trick for you. I just tested it, it looks like it is doing what you described. Nice job, you were on the right track, just a few tweaks.
By the way, I just wanted to thank you again of taking time out of your Saturday. If you have any more tips on coding in LabView, that would be terrific. First chance I get, I am giving you gold. :)
No problem, I had a lot of help learning from people on the NI forums, so I figured once I was better at coding in LabVIEW I would pay it forward and help out. Regarding tips, I do have one that is a bit relevant to your current project. It would probably not be worth the trouble to go back and change now, but in the future, you can use the [formula node](http://i.imgur.com/eDRf1cR.png) to take care of math if it is going to get complicated. For example, [these two expressions are identical](http://i.imgur.com/g5GnQB1.png) but you can imagine how for very complex equations the graphical method will get messy. Just my two cents! Good job again, that is a neat program you wrote.
The correct answer is 70.7 volts when T is... I don't see a 60 on your block diagram (between pi and *t in your V(t) equation. Did you mean 2*pi/360. Is 60 your frequency? Also, you multiplied by 2*PI twice. I think you don't want to multiply your frequency by it. It's probably safest to leave the integration method unwired You are dividing by 1/frequency. You should have T wired into the divide after the integration
The 60 is frequency of the signal which is input from the front panel via the component labeled Frequency (Hz). The 2pi/360 changes the cos arguments from degrees to radians which is how LabVIEW manipulates angles. The integration has to stay wired because it is the most important piece of the whole thing. The problem turns out is the 1/frequency that you mentioned. Here is the corrected module that actually works: http://imgur.com/MZKhAkP Turns out T is just the total number of iterations!!! Thanks "infinitenothing" for taking the time to look it over I greatly appreciate that.
You cannot build an exe with the student version. You need the LabVIEW Professional Development version to do this.
You can purchase the Application Builder module to build an exe. If you're at a university, check around because your school may have a license for it already.
Yup, as has been stated, you need the Application Builder to make an executable. If you want to see if you have it, you can open up License Manager (Start &gt; Programs &gt; National Instruments &gt; NI License Manager) and you'd see it under the LabVIEW category.
How hard is the CLAD, I've been using Labview for upwards of 4 years and professionally now for 7 months. I'm nervous about taking the test, any hints or help?
Yes. When you launch LV, it actually comes with a ton of tutorials. Do everything that looks interesting, and then the ones that look almost interesting. If you're an experienced programmer, most of labview should make sense immediately. Code flows left to right, wires are variables, case structures are if statements, sequence structures are mostly useless, flat sequence structures are less awful than sequence structures. If you've never coded before, LabVIEW does a lot of the computer science stuff for you. Read up on some general programming concepts like loops, decision making, functions, basic data types, and complex data types. Read the sidebar (this way -&gt; ) if you haven't already. Context help is useful for all of the standard functions. 
Not sure how you've managed to work in LabVIEW for that long and have been able to do it professionally, without even taking the CLAD. You should certainly be fine. But I would search on NI's website for prep materials anyway, and go through a sample exam. It doesn't hurt to study, and some of those questions can be kinda tricky and bite you in the ass. 
Thanks for the advice.
You can download the full version of LabVIEW to evaluate for 30 days so you can learn outside of your classes. If you want to easily extend the evaluation, install it inside a VM and dump it after 30 days and re-install :) Your university should also have lab computers available to avoid the hassle, I'm not sure of your situation so I won't go further. LabVIEW has heaps of great tutorials, both in program and online on NI's site. BUT the best resource is the example code library! There is a find examples dialog somewhere, open it, and stand in awe. Pretty much everything you could want to do in LabVIEW in working example form. Brilliant. Better yet, anything that isn't an example already has probably been do by someone somewhere. There are a few great LabVIEW code depositories and forums which should will help you out in every imaginable situation. If you get stuck, your not the only one, or the first one. Easily the best one, LAVA. They also have a defacto-standard add-on library which will makes things much easier once you get a bit deeper. http://lavag.org/ Good luck, sorry for my haphazard advice, haven't LabVIEWed for 6 months, but dw there are heaps of resources available for you :)
Virtual Machine. You can use it to install an operation system (windows xp for example) inside of an operating system. A quick google search can help you out with specifics. ^cue ^the ^inception ^jokes
Virtual Machine, its basically like a virtual computer you run on your computer. It's so you can run other operating systems inside a host operating system. They are a bit of a mess around to setup, so if you don't know what they are I wouldn't bother with them. I'd only use them as a desperate work around for the evaluation limitation. But definitely download the evaluation! Now that I think about it the evaluation might have been 45/60 days, even 6 months on some of the modules, it was very generous! https://lumen.ni.com/nicif/us/evallv/content.xhtml Also, don't use a real phone number, you will be harassed by NI sales department.. Very annoying. If you are morally indifferent to pirating software, full versions can easily be downloaded via torrents. If you are tricky you can actually look up your university's academic site license key and use that to unlock your personal evaluation version, I did this but only because I had permission! I was using it for my senior thesis and needed a personal copy so I didn't have to re install propitiatory toolboxes and drivers everytime I needed to program the robot I was using, and this usage was covered under their license.
Inception is the perfect analogy!
Got it. You all have been great. Thanks! 
Example Finder is at Help &gt; Find Examples. www.ni.com/code is the NI community examples page, and has tons of code examples on it. LAVA is also great. The best way to get better at any programming language is to use it. Once you know your way around the environment a bit, try a simple project and solve the issues as you go along. If you get stuck, you can always ask here, or at the NI forums (ni.com/forums). People at both places are happy to help, and if you post in the NI forums, it's monitored by NI's Applications Engineers so you're basically guaranteed to get an answer to your question.
Hey I'm also taking the same course to become a CLAD. I wonder if we go to the same university haha. Edit: totally is the same school. I saw your post history. Welcome to the Sooner labVIEW club! 
Ewww, Sooners? RAIDER POWER! The CLAD exam is...interesting. I've never been a huge fan of multiple choice programming tests, and the CLAD is particularly bad about asking tiny nitpicky things that you could determine in a few seconds with a copy of LabVIEW. But it's definitely a good certification to get--I think the CLD means more if you want to do consulting, but CLAD is the first step to that, and it'll also be a great thing to have on your resume if you apply for a job that would have you using LabVIEW.
Hey it sounds like a similar thing I did while I was in school. It was a temperature sensor that had a power resistor to simulate a heater. You set the upper and lower temperature you want, and the "heater" comes on and off accordingly to keep the sensor at the right temperature. [Here's](http://i.imgur.com/E9M9ftH.png) the block diagram for the main program. If you look to the right of the "Temperature read" comment, you see the DAQmx reading go into a subVI. That subVI converts the DAQ reading into a temperature using a formula provided with the sensor. Then that temperature reading feeds into a waveform chart. [Here's](http://i.imgur.com/inmJCjT.png) the block diagram of that subVI. Hopefully that helps some. It's been a while since I did the project so I'm fuzzy on the details, but I can try to help out more if I can. Also, I never used the DAQ assist as my professors swore it was more beneficial to do it manually. 
Here's some info on how to setup DAQ assistant: http://www.ni.com/white-paper/4656/en Depending on your DAC device, this might be useful for converting voltage to temperature: http://zone.ni.com/reference/en-XX/help/371361J-01/lvinstio/conv_thermcoup_read/ 
Here's a pretty general article from NI's site about thermocouple measurements, maybe it'll help: http://www.ni.com/white-paper/14338/en You can also check out the Example Finder, in LabVIEW under Help &gt; Find Examples. If you look in the Hardware Input and Output section, there should be a few thermocouple samples in there. If there's a specific part you're getting stuck at, let us know and I'm sure we can help!
Whether or not DAQ Assistant is a good idea is fully dependent on your skill level and needs, as follows: A. If you're not very familiar with LabVIEW and want to do something simple, DAQ Assistant is fine and it'll get you up and running quickly. B. If you need to acquire data at a high sampling rate (acquire VI in a loop running very quickly), don't use DAQ Assistant. Also, it's not a good long term approach for complex tasks such as synchronization. I always try and steer people away from DAQ Assistant if they're willing to learn the other way, so kudos to you for doing it that way. The main benefit can be seen in the code you have there, although I'd have added in a DAQmx Start Task before the While loop. If you threw a DAQ Assistant in there, it'd start and stop the task every loop iteration, which is very slow. Splitting it out with the discrete VI's gives you a lot more control over the situation.
I'll have to defer to someone else for more specific knowledge on the audio principles involved, but I feel as though the efficiency of this is going to greatly depend on the quality of the hardware you're using to output the signals, as well as the software that's controlling that output. What hardware device are you using to generate the audio signals, and what VIs are you using to do the generation?
I'm using the generate waveform VIs with inputs for phase, frequency, and amplitude and then merging two of them to form two channels, L&amp;R. First time around, I was using onboard sound card (laptop). I then changed to desktop speakers. I have a feeling that it may have to do with the desktop speakers that I was using, as there was a noticeable difference in volume between the two.
Nah, sorry! Have fun, though ;) Maybe take a quick pic...
Your wish is my command. There's over 100 people here: http://imgur.com/dTDK0Mh
All things labview seem to happen in Austin :) Makes it hard to attend when stuck in Kansas city. Have fun should be worthwhile. 
I was in the back of the room next to Jeff K for Jack Dunaway's session
Holy Crap!
There are some backs-of-heads I recognize there.
Is anyone going to the European CLA Summit in Paris in April? I'll see you there.
Man, shit like this never gets old for me. I just love the idea of, "It would be cool to build this." Then go and god damn build it. Just like that.
It looks like rate is an approximation of your sampling rate. Other documentation lists it as your maximum sample rate. I'm guessing you should put in 150k. Samples is how many samples you want in the waveform. Bigger number –&gt; longer duration waveform. https://decibel.ni.com/content/docs/DOC-11362
Well the weird thing is when I had 150k for the rate and about 1/10th that for the number of samples the encoder was entirely nonresponsive and nothing happened. I started dropping both numbers, maintaining about that ratio, down to 50 Hz and 5 samples and all of a sudden the encoder was very responsive. So I was starting to think that with each pulse of the encoder it was sampling 5 samples at 50 Hz. However, when I dropped down to 1 Hz and 1 sample it did nothing. Clearly there's a relationship here I don't understand. Edit: That's similar to the example I based some of my program off, though I'm running it in continuous mode.
Just wondering, how familiar are you with LabVIEW? I think the simple answer here is a state machine with the following states: 1. Blink LED 1 2. Wait x milliseconds 3. Blink LED 2 4. Blink y milliseconds repeat I'm not near a computer at the moment and depending on your proficiency this may or may not be enough to get you going. I hope it can at least help you organize your thoughts into more of an algorithm mode. Good luck at your regionals!
The number of samples input to the DAQmx timing VI determines your PC buffer size if I'm not mistaken. I think if you don't provide the input, it chooses the buffer size based on the rate of the sample clock. There's a chart of these values in the HW user manual I think. Not really much of a complete answer, but I hope I helped some.
Yeah, I read about that. That's based on the Samples Per Channel input. If it's set for finite samples that declares how many samples to collect. If it's on continuous samples that changes the buffer size based on the table you refer to. You can change that from both DAQmx Read and DAQmx Sample Clock, for some reason. I think I'll set it to -1 (let the daq board decide) and run it on the engine again, not at my lab right now.
I would check the encoder on an o-scope (or if you don't have one you could use your DAQ as a scope) to make sure it is doing what we think it's doing. I'm guessing the rate you set gets multiplied by something and that becomes the rate that your encoder gets sampled at to generate the analog sample clock so keep it big. Here's the continuous sampling example that you are probably using. https://decibel.ni.com/content/docs/DOC-12253 That link says set your rate to at least twice your encoder rate. Are you getting any errors? The error might give us some clues as to why you get nothing sometimes. Try out the finite sample example and see if that makes more sense. Keep the sample size really small at first (like 1000 or even 100) to ensure you don't get a timeout. Benchmark how long that many samples take to grab and scale up from there. 
More of a 'scope question than a labview question. General best bet is to write down the steps to do what you are trying to do manually through the scope's interface, and then translate that to actions using the vendors drivers. More often than not, your line of reasoning leads you to believe you should be able to use one sequence while the manufacturer had in mind another. Be sure you check out the supplied examples, it doesn't sound like what you're doing is too complicated!
I just found the solution. Turned out that the name of the control in the driver is completely unrelated to the name on the scope. For posterity, anybody looking to control the trigger mode should see the "continuous acquisition" Vi. When set to false it sets the scope in normal trigger mode and waits for a single trigger (or however many needed to perform the requested average).
You may already realize this, but the waves will only destructively interfere in certain spots. It may be hard to find these spots for a number of reasons. For example, there will likely be echoes where you are. In addition, there will likely never be perfect cancelation, because unless you have special speakers, they will not act as point sources. This is probably a very hard thing to do, and as was already mentioned, you're probably limited by your hardware. In fact, it seems like there are so many variables (wavelength of given frequency, finite frequency bandwidth of the speakers, etc.) that I'd be amazed if this worked. 
I just took a quick look but I think you should use a sequence structure. [sequence structure] (http://zone.ni.com/reference/en-XX/help/371361H-01/lvconcepts/sequence_structures_concepts/)
My only experience with Labview is with FTC and FLL, so I probably can't help much. However, could you set it up so that one value takes precedence over the other, and just use a set of comparators? For example, in my FTC code this year I asked if one button was true, then if another was true. The second's comparator's false was wired to the first's output, which meant that button one was ignored if button two was true. Could you implement something like that? The next thing I'd test would be to store the boolean value in a set of XOR gates (I forget exactly what that's called, but it's the basics of memory on computers). Then, have a switch that reset the values every x seconds. Probably not the answer you were looking for, but I hope you figure it out. If you do, would you mind posting the answer here so the rest of us can learn? Thanks!
Yeah, although you have to be careful with [sequence structures](http://www.reddit.com/r/LabVIEW/comments/1a6gmz/labview_help_requested/c8uj46r) and should use them sparingly, I think that would be the most straightforward solution. I'm not sure exactly what best practice would be, but my first attempt would probably look something like [this](http://imgur.com/7Tem58x). EDIT: I don't usually complain over downvotes, but this is baffling. Can someone please suggest how I could improve my answer/explain if I've given misleading information? EDIT 2: seems like a lot of people are being downvoted here even though we're all trying to contribute...
Thanks everyone, when I finally get back to my FRC meeting place I'll work with all this advice. Should I post results when I finish? I'm not sure who's advice will work best in this case, but I'll at least let that person know. Upvotes for everyone.
Upvote for ASCII art G code. :)
Glad to help! Which team are you with? 617 uses C++, but I've always wanted to switch us back to labview. It's just so much easier to teach new kids - plus there aren't really that many functions you can't reason your way through. Best of luck figuring it out!
Good advice. Question though, why a property node and not just a local variable?
Either way works. The local variable is slightly more efficient if you're writing a lot of values quickly, but for slow updates it really doesn't matter. 
Updating a property node can take upwards of 500ms. So fairly suitable for a large majority of non time critical operations.
[Here is a simple VI](http://i.imgur.com/oakZp3D.png) I made that will flash Stop and Stop 2 alternately while start flashing is true. The false case has false values wired to the terminals. You could extend this if you wanted a seperate button to stop the flashing once it begins.
Oh yeah, I'm saying 500ms is the upper bound. That's only ever going to be the case if your vi is very heavy on the GUI elements.
=) Didn't have LabView at hand to post a real screenshot.
Friendly improvement suggestions to your answer: Too many nested frames! In fact, the VI you suggest won't work unless it's inside another loop (I understand it's a first sketch, but OP seems to be very novice at LV, your try might mislead him a little). The "button to start" will only get polled once when you start the VI. If the default is the OFF position, the VI won't do anything at all. If the default is the ON position, then you don't need it in the first place... ;) If OP only wants to alternate two LEDs, sequence structure just makes things more complicated. I guess you wanted to provide a solution in which the VI is running and a switch makes the leds blink or not. In that case I'd put that "start" switch inside my main loop along with everything else, and just wire it to the LEDs with AND comparators.
Thanks for actually commenting! As you note, this image isn't intended to be pedagogically accurate, but to give OP an idea of what to put *inside* his mainloop. Maybe I should have clarified that in my comment. Of course you're right about sequence structures being overly complicated.
1) Check your code to make sure that you're handling the punctuation properly. Follow the punctuation step by step with probes until you see it disappear. This is a good programming troubleshooting technique, and is incredibly easy to implement in labVIEW. 2) The details you're giving aren't very clear, so I'll give you some general advice: try bundling the timestamp of each datapoint with the datapoint, and then graphing the resulting array of (datapoint + timestamp) bundles. I almost always found XY graphs to be a pain in the ass, but it seems like the proper tool to use in this case. 
A picture is worth 1000 words. 
First, put them in the right order, which is the opposite order from what you wrote*. Then just use the Type Cast primitive to cast to a DBL. Just wire a DBL constant to the top terminal of the Type Cast primitive. *I can't remember which order the bits are for floating point to begin with. x86 uses little endian order, but I think the Type Cast primitive expects big endian order for legacy reasons.
2 ways to do it: http://imgur.com/TqCgVTt http://imgur.com/B0ghAXM - less technically proper [2nd way is basically did what they talk about in this KB but backwards](http://digital.ni.com/public.nsf/allkb/7EA6073F162B296C86257125006BFFDA)
 [**@eyesonvis**](http://twitter.com/eyesonvis): &gt;[2013-03-20 17:32](https://twitter.com/eyesonvis/status/314429194455093249) &gt;OH: "It's a relatively new feature. I think we added it in LabVIEW 7." (This developer has been on the R&amp;amp;D team since LabVIEW 1.0). ---- [[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1aobnn%0A%0APlease leave above link unaltered.) [[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [[Translate]](http://translate.google.com/#auto/en/OH%3A%20%22It%27s%20a%20relatively%20new%20feature.%20I%20think%20we%20added%20it%20in%20LabVIEW%207.%22%20%28This%20developer%20has%20been%20on%20the%20R%26amp%3BD%20team%20since%20LabVIEW%201.0%29.) [[FAQ]](http://www.reddit.com/r/TweetPoster/comments/13relk/) [[Statistics]](https://www.stathat.com/decks/PJSe8OF5J44Y) 
Version 12. See http://en.wikipedia.org/wiki/Labview#Release_history
Oh, convenient how that lined up.
What was the feature in question? Undo? Autosave? Auto select tool?
For number 2: Try using a waveform data type. It includes a timestamp and LabVIEW has a built in graph that should handle it. In addition, check your graph scaling. It might not be using 'relative time' like you need it to. 
FIRST Robotics. I know its not what you mean, but take a look at past games, figure out what hardwere/sensors you would use, than program it. Its more hardware than softwere, but if you have a cRIO you can build a test bed, and it gets really fun doing autonomus functions. feedback from sencers, image processing, and other complex designes, or you can be simple.
Finally had a chance to test this (No meetings on my team in between competitions and all that jazz.) Thanks.
This just made my day. And I thought I would never gain useful knowledge from this subreddit.
This is the only Harlem Shake video that has made me laugh.
This is the nerdiest thing i have ever seen in my life... 
The counter counts anything up to its timebase limitations. It only reports readings back when requested. If you are doing non-buffered, aka not providing the counter a sample clock it only returns the current when requested from the PC. If you are using buffered, aka providing a sample clock, then it returns the current count every time that clock signal goes high and then stores it in a buffer on the PC to be retrieved by a read call whenever the programmer/code wants. There are shipping examples of both. FYI your DAQpad can't have the counter provide a sample clock to itself, like the analog inputs. This you will have to provide an external clock, maybe generated from another counter or analog task or something else.
Thanks, im not to familiar with buffered vs none buffered. Can you give me some additional info, i would appreciate it
Thank you.
Don't have anything in front of me ATM, but I'd search at NI.com: http://search.ni.com/nisearch/app/main/p/bot/no/ap/global/lang/en/pg/1/q/buffered%20counter/
Yes, DAQmx has built in timers that you can use. There's a million ways to configure the timing. Based on what you've mentioned, is this similar to what you want to do? https://decibel.ni.com/content/docs/DOC-25106
thanks!
You might be looking for something like this: https://decibel.ni.com/content/docs/DOC-25126 The "read memory address" part is all done by that "Counter U32 1 Samp" function
Thank you
I have been learning to use a Daq for the past week. I might be blind but you need to configure the output to a physical analog output pin on the DAQ. 
To do this, click the double arrow (&gt;&gt;) next to details, and you can select a physical channel to map it to.
thanks ill try that
Are you able to share your code? I would suggest using probes to check for any loops that aren't stopping. Also probe the error in and out wires of any dequeue operations to see if anything is waiting on a command with no timeout. Never used the actor framework but my understanding of it, is it uses queue operations to pass notifications between actors. Sounds like something is waiting on a notification that it is not receiving.
The code is very large and would be hard to transfer, as well as under NDA. Thanks for the suggestions!
Mike, try to guess who this is...
Haha, if you found this post through a Google search trying to figure out the problem then that's a really bad sign for me!
Yes, I've found that this is a pretty common occurrence when developing with the actor framework. For me, the most common reason for this behaviour has been that I have overridden Actor Core, and added a parallel loop to it. If you do this, whatever kind of loop/event structure/etc that you've added, you need to make absolutely sure that you've got logic in there to break out. That logic needs to be robust against any funny timing or error states. The stop logic can be implemented either by overriding stop core, or by triggering the action after the "Call Parent Method: Actor Core" node finishes running (since the default Stop Core method causes that to happen anyway). One way to track this down, is to set your Actor Core VI to "Show front panel when called", and then highlight execution when you try to stop it. It will be obvious if somethings not finishing correctly. Remember, you're responsible for cleanly ending any loop (or anything else that might cause a wait --- e.g. something waiting on hardware) that you create. The Actor Framework will not force close any VI for you. Occasionally annoying, but really better for everyone in the end. 
Sorry for pointing out the obvious there! You're obviously past that, it's sometimes hard to know. I've always (eventually) tracked down this issue to an actor core not stopping correctly. If you're really sure that all your actor cores are stopping - are you keeping track of "Last Ack" messages? Perhaps you could find the misbehaving actor by finding which one isn't sending it? You may want to look into tools to help your debugging. I've had some limited success with this [labview task manager](http://lavag.org/topic/14921-labview-task-manager/page-3) Also (and I haven't tried this due to the $1500 price tag) there is the [desktop execution trace toolkit](http://sine.ni.com/nips/cds/view/p/lang/en/nid/209044)
I've never used "Last Ack." My understanding is that I would create a "Handle Last Ack" override for my Controller actor, which would check to see if each nested Actor is sending out a "Last Ack" message (that is, returning its final private data before shutting itself down). Is that right? I can give that a crack, might as well try everything. Let me know if my understanding of how Last Ack works is correct. I did verify that the Stop Cores of all my Actors were triggering (using breakpoints), but haven't gone further than that.
Check out the CLAD exam e-kit. It has some sample exams that you may be able to pull questions from. https://lumen.ni.com/nicif/us/ekitcladexmprp/content.xhtml 
Here are [four sample exams](https://decibel.ni.com/content/docs/DOC-29367) I studied prior to taking the CLAD.
These sample exams (and the ones for CLD and CLD-R) are essential. Many of the sample questions will be found in the actual exam.
I found about 50% overlap in the CLAD (with questions that were almost identical to the practice exams). Others who've taken it have had it even better with 2/3rds or better overlap. I'm actually afraid of taking the CLD-R; I wish they'd just let you take another practical again. I'll be trying for the CLA this year so hopefully I'll just pass and get to put it off another written exam for another couple years.
wow that was helpful, thanks :)
Undo was 5, Auto tool I would guess was 6, Autosave wasn't in 7.0 I'd guess it was added in 8.0 or 8.20.
What's that second queue for? You could use auto indexing on the array terminal instead of the index array in the top for loop. Can you show the part of the code where you are dequeueing?
You only need to create 1 queue (using Obtain Queue( and pass that reference to both loops. There is a template that demonstrates this built right into LabVIEW @ File&gt;New...&gt;VI&gt;From Template&gt;Frameworks&gt;Design Patterns&gt;Producer/Consumer Design Pattern (Data)
The first queue is intended to be character based (single uint8), the second queue was intended to be packet based (uint8 array with control characters stripped off/checksum verified etc). The first queue originated as I was doing a different method of read off the VISA with the wait for event block but that was causing me issues. I could probably do away with it. It's the second that seems to be causing the problem though. The Buffer array seems to fill correctly, and has all the correct values as it's being enqueued in the "Received state" shown above but the item (the array "Buffer") never actually seems to show up on the queue. The part of the dequeueing block never actually executes so I've left it off for now. Here are the other states: Initialize: http://farm9.staticflickr.com/8132/8701109179_447ba81cce_o.png Start of Message: http://farm9.staticflickr.com/8554/8702231396_85721f93db_o.png End of Message: http://farm9.staticflickr.com/8259/8701109189_4f7d046fb7_o.png 
Have you overridden your handle errors? By default, the actor stops on any error. An error 43 means a normal actor stop message, so it is filtered out by the handle error. So what happens is your actor terminates with no errors. Any other error message will cause the actor to terminate WITH errors (check out the last ack of the actor to see the error) If you've overridden the handle errors VI, it's possible for the actor core to basically ignore the stop message. So it will now run forever. Also, if you do not clear your error in this VI and do not set the Stop Actor to TRUE, it's possible to cause the actor core to just spin on no-ops (because an error is wired in to all the actor core VIs). Again, it'll be impossible to stop it.
When the problem first manifested, I was not overriding the Handle Error method. After the problem manifested, I added an override to display any errors that come up. However, I've since "solved" the problem, though it's unclear to me why my solution works. I would like to investigate further, but I wasted so much time on it and the project must march on... check out this [LAVA](http://lavag.org/topic/16774-actors-stay-locked-in-memory-and-labview-produces-catastrophic-crash-on-repeat-calls-videos-of-problem-included/) thread to see the resolution.
If that is publically available information, its typically in the detailed help. 
Is this detailed help usually contained within the manuals? Because they contain nothing about the algorithms behind the functions.
Im talking the Labview Help. They usually talk about the algorithm if it isnt proprietary. Sometimes its a simple description, sometimes its very in depth with all of the math an such. 
Unfortunately, that was the first place I checked &amp; it was extremely basic. Luckily, I just found something: [IMAQ Vision Concept Manual](http://physics.ucsd.edu/neurophysics/Manuals/National%20Instruments/NI%20Vision%20Concepts%20Manual.pdf) This seems to answer everything, thanks for your help anyway!
For any LabVIEW hobbyist or college students working with LabVIEW, I recommend the "Foundational Design Patterns for Moving Beyond One Loop" course. 
I was told the webcasts will be on demand in the next week. I'm expecting an email when they are online.
It would be awesome if you post a link when you hear they are up.
I will try to deliver. They might show up [here](https://decibel.ni.com/content/groups/labview-user-group-leaders).
http://zone.ni.com/reference/en-XX/help/371361H-01/lvhowto/creating_cond_disable_struc/ This is what you're looking for. Checkout the OS symbol. You can make 1 vi that does different stuff based on the OS. Since its a diagram disable, it will not load the other diagrams when not needed (I think)
It might ask you to find the dependencies in the disabled structure but you can just click ignore and you won't get a broken run arrow
Awesome. Thank you. I'll try this out.
Do you mean you're an undergrad or you're in *academia*, as in post-graduate work and beyond?
I am well through my PhD and have helped tons of fellow students and post-docs decipher their messes.
What's your experience been in terms of distribution of LabVIEW skill?
First things first, this is my personal opinion. I am not working off a super huge sample size. I can't find the actual graph, but i've seen NI presentations (at NI week and such) saying something like the following: Of the certifications: 80% are CLAD's, 15% are CLD's, and 5% are CLA's. What the market wants: 25% CLAD, 50% CLD, 25% CLA Again, these numbers are made up, but they're in the right ballpark. From what I've seen: The vast majority of LabVIEW users "figured it out on their own" and therefore don't know what they're doing (Spaghetti code, lots of local variables, huge sequence structures, etc). They can make a rats nest or maybe a simple state machine. That's the blessing and curse of labview. It's really easy to get started, but it's really easy to make a big mess. Fortunately (maybe?) They only need simple DAQ w/ logging type applications, so this is good enough. (These are Non-certified people, CLAD, and some CLDs) I'd say a much smaller amount of people are familiar with a producer/consumer architecture and use it frequently. Some of these people have spot on style, some have no style. (These are CLDs and some CLAs) Now we have the advanced group. These are people who are writing a lot of labview. These are the people who manage messaging between multiple processes, using a variety of architectures. Probably pretty good style, but not a guarantee. (CLAs) Of your list: * regularly deploy state machines : 30% * use standard messaging between processes : 20% * make an effort to keep processes independent and scalable : 5% * keep up proper coding style to maintain code readability : 40% * are even passingly familiar with OO : 10% * are extremely adept at OO / advanced architectures : 5% * regularly use scripting : &lt;1% OO for some reason is deemed an "Advanced" Technique. It should not be. I think 100% of your code should be OO. It is never a con to write something OO. At worst it is no different than a non OO implementation, at best it is a huge check in the pro column. But that's another topic...
I think you produce mostly simple programs. If you can get away with only a few sub VIs as you're suggesting, of course you won't see any benefit from OO. This only describes the simplest of programs though. As the complexity goes up, the benefits of OO go up
proficient, maybe. good? not necessarily
pretty sure that is what i said.
proficient? good. maybe? not necessarily.
As an NI academic field engineer, I feel this is something Im qualified to answer. There are some pretty solid labview programmers in academia, but theyre very few and far between. In general, most things in academia area a bit on the sloppy side, such as manufacturing, wiring, cable routing, source control, etc, so its not something unique to labview. If anything, labview just visually shows you that the code is messy. Its mess is there in almost every environment. Their CAD models are terribly constrained, their experiment kludges 40 years of hardware together, their giant lump of C code is a disaster, etc. After all, the grad students are not professionals at what they do. That said, there are some excellent programmers in academia as well (and it is fantastic to see their work). Altenbach on the forums, for instance, is a researcher at UCLA. some very labview heavy groups, such as the labview-based robotics groups, tend to write some very nice code. Sure, its not alliance partner code, but its on the upper scale of typical skill. 
They are now available on the website posted by OP. Just click on the link and now instead of getting an error message one can watch the webcast.
Sounds like a state machine would be helpful, assuming there is a possible change in the order/number of files read. For instance, if you're repeating the file read step based on some conditions definitely use a state machine. If it's just a straight process that never changes, you might not need any design pattern.
Are you repeatedly accessing these files in a loop, or purely in response to a user selecting a file(s)? 
purely in response to the user selecting file(s)
maybe it is a bit over the top to use design patterns here, but i just learned in a seminar that they exist and i thought it might be a good idea to get familiar with some of them. :)
I have a similar project where the client performs some tests and generates log files. After the test, they then browse for those log files and my software reads and performs analysis on them (they were trying to obtain poisson's ratio by graphing mohr circles). I used a producer/consumer with events design pattern. I highly suggest it if you're only reading those files once in order to display data on a graph. There's a template for this design pattern that ships with LabVIEW that you can use as a start. I recommend having the event structure in the producer loop listen for value change events on your file path control, then enqueue the file path for your consumer loop to dequeue and process.
The state machine does have the added benefit of being more easy to update if your logic needs to change, such as repeating a step some times or inserting a new step.
Elapsed Time express VI is probably a good start...
I think you could do it with a Time Delay VI. If you're having trouble let me know and I'll put together a quick program.
That would be great :'( I need a program :D
Haha okay. I'm headed over to my lab and I'll be firing up Labview anyway. I'll get it to you this afternoon. Want to PM me an email address?
If for some reason you're wanting to not use an express VI another way to do it is just a timed loop with a counter into quotient and remainder then use the remainder as the time lapse counter.
In addition to your bullet points I'd also ask someone about their knowledge with User Events, Quick Drop, and OpenG. Being a master at these things aren't required of you to be a good LabVIEW developer, but being familiar with User Events, Quick Drop, or OpenG tells me something about the developer's coding style, and familiar-ness with newer tools.
A double click on the control finds it, but I definitely like the designated-area idea. Id idea exchange that :) And completely agree on the into structures thing. My least favorite is having to copy paste something into a few structures and then having to fix all the wires. 
there is no way to zoom in/out. I have yet to make a project that can be contained within the bounds of a normal size window. This irks me to no end
They actually recommend you design block diagrams that fit on something like an 800 x 600 px screen for the CLD as well.
Zooming is a terrible idea and I shudder at the thought of the new spaghetti code nightmares I would find were it to be implemented There is a navigation window, which I know if I need it I'm not exactly working in a well designed program
Use cluster/classes, and sub VIs. If you start taking up more than one screen it is usually an indication that you're not doing something right.
I use the auto cleanup function for my block diagrams a lot but I wish it was a little more customizable. Like I could say, I want this while loop over here and this one over here, now make it look pretty. As of now, sometimes I'll hit that and then have a hard time finding anything. Another issue I have is how mixed up wires can get if you're dragging stuff around. Like if you click and drag on a segment of wire accidentally.
The [navigation window](http://zone.ni.com/reference/en-XX/help/371361J-01/lvconcepts/nav_window/) might help you out a little: Ctrl-Shift-N
In the latest versions you can highlight an area and do auto cleanup just for the highlighted area.
I have a simple one: When I search for anything on the block diagram, for example a local variables, why does the search box disappear? This is forcing me to search again when I'm ready to go on to the next instance of the variable.
Without trying to be a jerk about it, there is no reason you can't design code to fit on a reasonable screen. Honestly, I learned to do this by setting a goal for myself with one particular project. You learn some great techniques, and your code will improve. 
I guess the point I'm trying to make is that there are use cases where having a larger 'flat' diagram is beneficial to many layers of subVi's...not that all code should be written this way. I'm also not advocating 'nothing in subs'...just 'only what needs to be in subs'. I do try and code for small diagrams, but I do find there is a point where you start smashing things together, or breaking flow with subs and references to the point that it's harder to follow than if you just gave the components some room to breathe. 
Remember any issues you have with LabVIEW (or any other NI product) can be expressed through the [Idea Exchange](http://forums.ni.com/t5/ideas/v2/ideaexchangepage/blog-id/labviewideas). There was a thread on Lava a while back similar to this one and the big issues were the buggy icon editor, probe window, RunTime Engine issues, and lack of variety with UI controls.
Thanks, I'll try that next time.
I know this is a few days old, but the piece you seem to be missing is called an [action engine](http://forums.ni.com/t5/LabVIEW/Community-Nugget-4-08-2007-Action-Engines/td-p/503801). Whatever architecture you use, you should handle file i/o and data operations within an action engine you create. As far as architecture, a producer/consumer would be adequate where the producer is an event structure loop that fires off events that can then be processed by the consumer loop, and your file processing can happen in the consumer loop. IF your file i/o and data processing is fast, you could simplify and just use a single loop, and break it out into a separate one if/when you add slower or more cpu intensive task.
There is a countdown timer with alarm posted [here](http://forums.ni.com/t5/Counter-Timer/Creating-a-count-down-timer/td-p/268865) on the NI forums. Examining code to see how it works can be useful, even if you don't end up using it.
This is sweeping, but: I wish that the OO implementation were more complete and user-friendly. Right now, if you want to create a "complete" and fully documented class that's friendly to porting, you have to go through a number of unwieldy processes. For example, creating a custom probe is a moderately involved process - and probes don't inherit, so work gets repeated for every child class. Creating a tool palette for your class requires far more clicking and navigating than it should; plus the palette editor itself is horribly antiquated. And Access Scope can be frustrating to set when using LVLIBs to organize classes and messages (because there are two kinds of access scope going on; one within the class and one at the LVLIB level). I use OO every day and the benefits absolutely outweigh the minor hassles. But being complete and diligent in my coding is tough when the development environment stacks irritations and obstacles against me.
Those are also good criteria; I'll make sure to ask about those next time I'm interviewing candidates at work. You're very active on a few LabVIEW forums. What's your experience been in terms of quality of LabVIEW programmers?
Can you describe the entire chain of what is linked up to the pc? Medical device -&gt; NI hw -&gt; Ethernet? What NI hardware are you using? What version of MAX do you have? (It is different from LV version). You could try downloading the newest if you don't have it.
If it's not NI hardware then you don't need to use MAX. To help you further, you'll need to know more about the device and it's software, and how it is communicating with your labview PC. Is there a labview driver provided by the device maker? Is there a communication reference manual? 
It was not developed to interface with LabView. One of the software engineers at the firm who sells this device wrote a simple UDP program to transfer the data from the device to the other computer. Using that program, I can accumulate live data at 10Hz. I'm trying to integrate it into LV because there are other devices we use that work with LV. I've never used LV before, though, so I'm a little lost - not sure if it's even possible.
Ok, so the data is somehow getting to the PC? Is it being written to a file? Is there a program running on the PC? Your two options are: 1) Get the data from the existing program somehow, using DDE or some sort of API (talk to the vendor to see if they provide any way to do this) 2) Write your own program in labview to talk to the windows ME computer over the network and log data. To do this you need a communication reference manual for the device from the vendor. Either way, you are depending heavily on the vendor. Actually, what is the device model number and manufacturer? I'll see if anything turns up. 
Intra Medical is a defunct business? I hope you have access to a communication reference manual, or you may be SOL.
That's bad news. I do have contact with someone who used to work on that machine, though.
I have never used the JKI State Machine in an actual project, however, as an instructor, I've had to teach it in the Advanced Architectures course. Download VI Package Manager (http://jki.net/vipm/download) and install JKI State Machine. You've probably done this, but it helps to make sure we see the same thing on our block diagram pallettes. Open up a VI and switch to the block diagram. Right-click to bring up your palette and navigate to JKI Toolkit &gt; JKI State Machine &gt; String-Based State Machine. Drag that onto your block diagram and it will drop down a tutorial template. It's well commented and will explain what is going on. This is what I see when I do the above steps: http://i.imgur.com/SmcOfMf.png Start by going through the states and reading the comments. The idea behing the JKI State Machine is that you use strings to indicate states (as opposed to the enum). Since we are using strings we can specify multiple states to go to by tokenizing the states with the new line character. Not only can we specify a list of states to go through, but since we are using strings we can encapsulate data between each of the states. Take a look at the "Macro: Initialize" state in the example I mentioned. In this state you can see that it transitions to four different states: 1) Data: Initialize 2) Initialize Core Data 3) UI: Initialize 4) UI: Front Panel State If you take a closer look at the last one, the string they use is "UI: Front Panel State &gt;&gt; Open". Anything after the "&gt;&gt;" is the data being encapsulated to the state string. So the state machine will go to the UI: Front Panel State and will use the Open command to determine that it needs to open the front panel. Alternatively we can send the "Close" command. Another neat feature not shown in the template is the fact that ANYTHING can be represented as a string by using the "Format Into String" VI. If you wanted to, you could encapsulate a LabVIEW Class into a string and pass class data between states. 
You forgot the part about very little experience :P
There is a difference between OO and actor framework. OO should be a part of every project. AF should be part of your project if it makes sense to
.
I'm still waiting on a reply. Thanks for the offer, I will let you know.
&gt; Where can I cash in all my comment karma?
I'm jumping on this a bit late, the LabVIEW UI in general is antiquated: * Import/export layout in XML * Zoom In/Out (this was a particularly annoying point when dealing with older students) * Updated layout manager (Look at Visual Studio/Qt/Swing)... * Automatic Cluster creation. * Backwards type inferencing (this kind of works but not in all situations). I actually have a ton of complaints, but those are high on the list...
Ugh, I don't use OO because new features are just glued on and not properly integrated with the whole system. Honestly, just put the LabVIEW runtime on the CLR and have objects all the way down. Then make it really easy to create and use OO instead of making it an afterthought.
Old people have bad eyes, being able to zoom is helpful for them. Actually, the best option might be adaptive zooming where the area around your mouse gets enlarged (or maybe just the subVI you're hovering over).
You have to save the numeric value to file. https://decibel.ni.com/content/docs/DOC-9258 When the VI starts up, it should read that file and change the control. https://decibel.ni.com/content/docs/DOC-9259
Thanks- this is helpful
The registry is a bit of a scary place for those who aren't used to it. A configuration file in the correct location is better, and also makes the code more portable, both from machine to machine and then from platform to platform (Windows, Mac, RT targets, etc) Another user posted a snippet. In this case, this code maps to the specific user's Applicaiton folder. There are also standard folders for general Application data that can be accessed across all users. When I write a "general" program, I first have the code check the User's AppData for a config file. If that doesn't exist, I check the General/Public folder. If that doesn't exist, I MIGHT have it check the program folder. When those all fail, I have it create the default in the appropriate location. http://www.reddit.com/r/LabVIEW/comments/1fsc2n/how_can_i_get_a_vi_to_remember_values_that_have/cadgbes Other suggestions: Use VIPM to install the OpenG Toolkit. In there you will find a couple VIs that save/read all the data from a front panel or just a cluster into/from a config file. You can use either a standard VI (preferred) or a Global Variable (use with caution) to hold the values. This tends to be a "one step" save/read for configuration info. Once you create a subroutine that works well for you, bind it up into a lvlib and use it again and again.
Where do you store your defaults? Those should belong in the program folder but the program folder is different depending on your environment, project, and individual VI you're running.
So my method is to store all these config variables on one panel (or multiple clusters). When I am ready to release, I set them to defaults within the VI and save them. When you use the OpenG VIs, if the entry does not exist in the INI, the value is left as default. EDIT: Another way would be to programmatically set them when the code starts, but I find that messy.
Doesn't it get hard to read nested cluster/arrays? I've been using JSON for the config file information although I would prefer if NI provided native serialization options.
I meant manually reading/writing the files in a text editor.
Ah. Not really. The OpenG stuff converts standard objects to plain text, and clusters and arrays into multiple plain text items. For example, a cluster called A with a string, a number and an array of strings would get recorded like this: &gt; [globalConfig.vi] &gt; A.Name="ABC" &gt; A.Weight=123.456 &gt; A.Lunch.Size=2 &gt; A.Lunch.0="Spinach" &gt; A.Lunch.1="Cheese" I think JSON is actually a good way to do it, but I have been using this since the pre-JSON days, and it works fine. Also, the JSON LV libs I have found throw exceptions when things are missing from the file (that is, if I have an element, but no entry int he JSON, the error).
I actually prefer the error instead of silently inserting the default. JSON is nice since the format is easily supported by other platforms. Anyways, I'll throw some more LabVIEW hate another day.
No hate! I think the error works fine IFF you want to include a separate defaults specification in some way into your code. With this method, the defaults are handled very simply, and you can still scan for the INI file and raise an exception if it's missing; which is what I think is a reasonable way for a user program to handle itself. My problem with the JSON is this... On one project, I am working on implementing their API in LV. They return JSON when you call specific URLs. I am not sure if this is not standard or not, but their JSON structures skip elements depending on the context. This means that I would need to define a separate type def for every single call in their library- and there's a lot. The other issue is that JSON can have NULL data in all data types, and LV can't handle that.
I have lots of LabVIEW hate: Your issue with JSON is because objects in Javascript are just key value maps. This is actually an issue with many statically typed languages when dealing with JSON. However, this is especially problematic for LabVIEW because there is no native key value store besides variant attributes.
Variant attributes are stored as a binary tree with dynamic typing. This is generally fine (although clumsy) for small scale dynamically structured data. However, read/writes for a binary tree are O(log(n)). This is why key value stores are often implemented as a has table where read/writes are O(1). Also, the dynamic nature of the storage adds another performance and memory degrading layer. Most platform libraries provide easy access to both of these structures. I like JSON because of how easy it is to parse (especially recursively, now that we can FINALLY do that in LabVIEW). Error handling in LabVIEW is also a crapshoot. Many native functions do not return errors even when they should. Handling errors should be easier but since the language hasn't really been updated since the 90s doing proper error handling is quite cumbersome.
This is a good use case for [MGI's read write anything vi's](http://mooregoodideas.com/readwrite-anything-vis/). You could use any sort of config file, binary, ini, etc. But the MGi rw anything are convenient and well thought out.
I don't know if this is the answer you're looking for but matlab has Simulink, which can be used for circuit analysis, I think.
I'll look into that, thank you.
sweet, thanks for the information!
We have the control design and simulation module, as well as Multisim which more than cover the simulink use cases.
No access to LabVIEW right now to mess around but if you want to reset a task simply stop it and start it again.
What's the best way to do that from within the program though? Case structure?
Build an [action engine](http://forums.ni.com/t5/LabVIEW/Community-Nugget-4-08-2007-Action-Engines/td-p/503801) for your DAQ, with init, start, stop, reset and clear as possible actions. Then call it in another VI which will first init the task, then start it and analyze the data in a loop that will end when the data passes.
Are you bottlenecked by performance? If not, I would probably organize the subVI to be easier to read...
So in general, you want to avoid resizing your arrays since this can trigger reallocation and copying. This typically occurs when you're inserting elements, especially in the front. * Is there a reason you have to insert 0s as the first row? * Does the time column really need to be added to the big array since you're storing it anyways? * Your graph data also seems to grow unbounded (will get slower and slower). One tool you can use to see where you might be allocating new memory is under Tools -&gt; Profile -&gt; Show Buffer Allocations. This will highlight functions that might be reallocating/copying memory.
Yeah, 2d arrays and the in place element doesn't seem to let you do what you want. Are you sure this is what's taking the most time on this vi? My guess would be the building of the 2d arrays in your for loop or updating the front panel is where you're spending most of your time. Try using some flat sequences to time the sub sections of this code are to figure out where the bottleneck is
* That's actually deleting the first row (index 0), not inserting 0s. * Nope. You're right, I don't need to reinsert that column at all. I must've been tired when I wrote this! * Yes, but that is pretty much necessary. There is no way for us to know how long the VI will be collecting data. Thanks for the tip. I'll use that profile tool. Will it see all the subVIs, or do I have to Show Buffer Allocations for each VI independently?
It's more about memory than time. I'm trying to just not eat as much RAM. In order to do so, I learned about the In Place Element structure. This was just one place where I thought I might be able to use it. These arrays are very large, so if I can cut down on the number of copies, it'll be helpful.
It looks like you can enable parallel iterations on that for loop. if you're on a multicore system that could provide a nice speed up.
So I just did a benchmark. Deletion from the front is just as slow as insertion to the front. I would avoid this type of operation if I could. Insertion to the back is actually extremely fast (like 10-100000x faster than insertion or deletion of a random row), so I wouldn't worry too much about your graph growing unbounded as long as you insert to the end (just don't run out of memory). It may be a good idea for you to write to a file on disk or a database instead of trying to keep it all in RAM. If you're using LabVIEW 32bit it will run out of memory at around 1.5GB of usage. If there are performance issues trying to write immediately, you can write in batches (Hard drive IO does much better when writing batches compared to a bunch of small writes).
I suspect the bottlenecks are in the operations that reallocate (delete from front and insert in the middle). The for loop only retrieves indices and preallocates output arrays, so you really wouldn't see much improvement over the whole function trying to parallelize that loop (you may even lose performance).
Do they need to be doubles (64 bit)? Can you do with single width floating points (32 bit)? You will gain some speed (mainly in faster copying) and use half the memory.
They aren't mutually exclusive. I find it helpful to use an action engine to encapsulate each device I'm talking to, and then use an overall program structure that is appropriate to the problem. Examples are: Producer/Consumer, State machine, Queued state machine, etc. I see AE's as a smaller building block, not an overall architecture.
Normally, you'd use a shift register to store the data, not a queue. Queues are good for many to one, buffering, and inter-loop communication. If you have plenty of processor power, I'd store an empty array to the shift register in init, append the data in collect with build array, and analyze the shift register in the analyze state. If you find yourself running short on processing power, it should be easy to refactor to use a fixed size buffer instead of the build array. Regarding the architecture, my inclination is towards the state machine. If you force yourself use only shift registers for memory storage, you'll force yourself into using your problem solving skills to keep your application race condition free. I feel like it's easier to debug a state machine too. Your shift register values give you the entire picture and you don't have to worry about how the queue is stacked. 
The way you are caching, there's really nothing you can do. You've chosen to store everything in memory. I guess you could make "graph data" a fixed size array with a circular buffer. Obviously, you'll have to reload the data if you need it later. Have you seen the [Request Deallocation](http://zone.ni.com/reference/en-XX/help/371361J-01/glang/request_dealloc/) VI? I believe it saves memory at the expense of having to reallocate it next time the VI runs. If you're really desperate, yes, you could use a for loop and an in-place for the time conversion at the beginning and preallocate memory+use "replace arrays" for the for loop in the middle. Make sure to turn off debug or use subroutine priority otherwise your for-loops will run very slow 
Hmm, I am moderately concerned about processing power. I'm doing a lot more post processing than just writing to a file. I'm going to build a state machine and see how it goes. I'll report back.
Is negative db unreasonable for your application? Isn't SPL relative to a reference level? Negative db would just mean you're measuring sound less than your reference. What do you mean by change the format of the data you're outputting? Are you using an Express VI to output to Excel?
Thank you for your response, but I'm not trying to filter anything out. I really do appreciate your time, thanks again[.](http://t3.gstatic.com/images?q=tbn:ANd9GcRcnMDG_9fITeCCzkOi_95aAKrrrX8Matmfn56PAX56uYpbFAIs2Q) 
So the first problem sounds like an issue with your equipment/acquisition configuration. What is the data type of your inputs? Seems like you just need to reformat the data a little bit. Can you post screenshots?
Unfortunately, I'm away at the moment (assuming you're asking me about screenshots of the actual settings?). I don't think there's anything wrong with the way I've set up my equipment. The input signal is an analog signal and I've specified it as a sound pressure input. The sensitivity has been specified (happens to be 50 mv/ev (? - i forget the units) given the particular ICP microphone). I could be wrong, but I believe this is something that requires arithmetic, reading/writing to files, etc. Regarding the formatting, try [here](http://i.imgur.com/6sA9h9e.png) The left is what I get, the right is what I would prefer[.](http://www.bubblews.com/assets/images/news/1967606770_1361347959.jpg)
For the negative db. Maybe your cables are just switched? If you measureing in differential mode i could imagine that. Just a stupid idea. Also you may want to calibrate your mic with different frequencies, cause the shift could be different. 
Maybe this will help? https://decibel.ni.com/content/docs/DOC-7332 
For #1, what "function" are you using to write your file? I'm guessing you're using "Write To Spreadsheet File"? If so, based on your example output, it would seem you're calling it 3 times and providing 3 different 2D Arrays. Instead of that, combine them together so that column 1 = frequency, column 2 = iteration 1, column 3 = iteration 2, etc. Then write that 2D array to spreadsheet file. Here's a very quick and dirty example: http://i.imgur.com/vZL3A6X.png For question #2, what is limiting you to multiples of 100? What functions are you using? Where is the 100, 200, etc. coming from?
TLDR; please post the vi, it would probably clear up what you are exactly trying to do. It is not quite clear what data you are trying to collect. Do you have the electromagnet output plugged into the lock in amplifier, or is another signal going into the LIA? If they are separate, your data collection sub vi is not working correctly, or you are not asking it to do the right thing. It should collect more then 14 points if you ask it too. I had to modify the buffer fetch sub vi to check that the number of points recorded matches the expected number. However, the reason is that I am externally triggering the snapshot of the LIA data and the triggering can be unreliable. If you have the signal just going into the LIA then you can use the NI vi's as they largely work properly, in my experience.
I've used a SR830 with LabVIEW before and i didn't have any issues getting a ton of data out of it. You might want to check the settings on your lock-in amp along with if the vi is changing any of the parameters that could cause the lock-in amp to throw an error. 
Hi donblas! Thanks for the response. Despite my search, I had not come across this link. It does seem useful. I'll have to spend some time to see if I can somehow use "write to measurement file". Thanks again[.](http://www.runtogold.com/images/thanks-man.jpg)
[Here's a link to the VI file.](http://dropcanvas.com/#7GfKk8Cc300BWc) Hopefully it will work and open up. I've only been using LabVIEW for about four weeks now, so please excuse any obvious errors or dumb programming haha. You may need to open up the VIs for the [Model 643](http://sine.ni.com/apps/utf8/niid_web_display.download_page?p_id_guid=C2E8EAD8264935E5E0440021287E65E6) and the [SR830](http://sine.ni.com/apps/utf8/niid_web_display.download_page?p_id_guid=E3B19B3E93F3659CE034080020E74861). Thank you to anyone who tries to find out what's wrong :) Edit: The subVIs that need to be opened for my program are (from the Model 643 VI Tree) Initialize, Rate under Configuration, Status and Set I under Action/Status, and Single Reading under Data. 
Also, for a clearer explanation of what I am trying to do. I'm trying to ramp the electromagnet up to a desired current in a certain amount of steps. The steps are determined by the number of samples that want to be collected. It has to be in steps because the LIA has to take some time to integrate and collect data (and requires roughly 3 times as long as the time constant, hence the 1000 ms wait when the time constant is at 300 ms. However, I still need to write it so its 3 times whatever I put in as the time constant). Basically I want it to initialize, ramp up the electromagnet a little bit, collect data from the LIA and the power supply, and then repeat until it reaches the desired current for a specific number of iterations.
Hm okay. You don't need to look too hard into it, but do you have any ideas off the top of your head of parameters that might have caused an error? Thank you! 
Download the .pdf of the manual for the lock-in amp. It does a pretty good job of showing how the time constant, sensitivity, and reserve can affect how many data points the lock in will output. The section on programming the lock-in is good to read even though it's not specific to the LabVIEW vi. That should give you a good base with which you can hunt down the issue(s) that is causing you to only get like 4 data points. 
Outstanding, thank you so much! Will look that over over the weekend and will let you know on Monday if that fixes it. Thank you so much for the suggestion! I appreciate it
no prob. happy to help!
&gt;Explicitly labelling all of the sub requirements will make it a lot quicker to mark, but as mentioned above, the examiner won't ignore something that is not tagged. I don't think this is at all true. My co-worker failed the CLA because he did not correctly label the sub-requirements. Ended up with 30% for functionality. The CLA is going to be extremely similar to the practice exam in terms of requirements. It may be a different topic but expect to have the same general requirements: 1) Multiple processes running simultaneously (hint: focus on inter-process communication). 2) Need for an error handling scheme (I suggest a named queue and handle errors globally). 3) Different screens updating simultaneously and synchronizing data. I ended up using sub-panels to take care of all the display requirements, and user-events for the inter-process communication. If I had to take it again, I'd probably stick more with seperate windows than sub-panels as it's always a headache for me to get them working well. Nearly every post you've linked to stresses one important thing: **Focus on architecture, not on functionality!** For example, there will often be times when you have a case structure tied to an enumeration. Don't fill in the code for each case, just create the case structure and leave comments in the cases. Only your architecture matters. On my CLA I wasted about an hour and a half writing out individual code for my modules before realizing what I was doing. Just write the architecture (processes and inter-process comms). If you're comfortable with the practice exams, don't stress out, you'll do fine. 
Solve the example problem using a few different architectures. Get a good feel for the inter-process communication. OO is definitely not required, use whatever you are most familiar/comfortable with. Once you choose or develop an adequate architecture, you can practice writing it from scratch a few times to improve speed. Then adapt it to the test problem once it's written.
Thanks for all the tips, really appreciate it. I'll keep in mind labeling all the sub-reqs. I was hoping to shave some time off by skipping them, but better safe than sorry. I'm having a ton of trouble getting comfortable with the sample exam. As far as I know, there's only one, right - the ATM? I took a [crack at it here](http://forums.ni.com/t5/Certification/CLA-ATM-Practice-Solution-requesting-review-and-comments-OOP/td-p/2473858). Didn't feel like I was close to a passing grade. I've been trying the practice using Actor Framework, because that's what I use in my professional work whenever I have asynchronous state machines talking to each other. Maybe I should drop it and switch to the much simpler state machines deployed in the sample solution? It's just such a drastic switch from what I normally do that I'm not sure if I'd be substantially quicker with it. I have a month left and figure I have time for about 3 more cracks at the practice exam. I'm wondering if I should invest my time in doing AF over and over again, trying to improve my speed, or just switch gears entirely.
&gt;OO is definitely not required, use whatever you are most familiar/comfortable with. I feel most comfortable coding in OO, but I'm also concerned that I'm losing a lot of time due to the overhead in setting up my classes. On the other hand, if I switch gears to a more task-oriented solution, I might be a little slower just because that's not what I'm used to coding. Actor Framework seems like the natural fit for asynchronous processes. But the overhead... Still debating what to do on the real deal. I threw up a [practice run](http://forums.ni.com/t5/Certification/CLA-ATM-Practice-Solution-requesting-review-and-comments-OOP/td-p/2473858) on the NI Forums. Doesn't feel anywhere near a pass; too many reqs left unaddressed.
Honestly the overhead of using OO in LabVIEW just isn't worth it for an exam project (And since it's a piss poor implementation anyways I'd argue it's not really worth it in real projects either). I agree with arcandor in practicing multiple architectures. Actors are nice but might not be useful in all cases. Why do you claim that there's so much overhead with Actors? You only need 1 queue with a cluster of commands with variant data for each actor you want to create.
That's a tough question to answer. The general idea is, you're filling out the high level architecture for a CLD to come by and fill in the functional code. Like Arcandor and a number of your quotes mention: A Top-down approach is the best way to go for the CLA. 
"Actor Framework" in this case refers to a specific implementation of actors in LabVIEW OOP. Thanks for the input.
Can't you implement it without OOP (or at least a large number of it's features)?
I could try to do something similar conceptually that breaks down to using state machines passing messages around. It's worth attempting in a practice ATM run, at least.
If each different state machine has the same command typedef and variants, you can pass around the queue references just the same as with OOP. If I recall the ATM project correctly, the queues it requires are not that similar, so OOP really wouldn't help that much anyways (it would add an unnecessary level of abstraction). 
But you don't need OOP for that situation since a command/variant cluster is already decoupled from the implementation (except for the command typedef, which is not inheritable anyways which is super lame).
Update: The LIA VI is not working like it should at all. Nothing that I try to change, such as the time constant or sensitivity, is reflected on the actual machine. I'm at a loss and I have no idea what to do haha
Pretty much any time you ask "is it possible in labview" the answer is YES. Labview is a very large and powerful programming language which can handle almost anything computers can do. Sometimes its not the slickest or most efficient way to do something, but it can totally handle anything. Both of your programs are quite simple. I built the first in under 3 minutes including testing. http://imgur.com/yOJtz2f . I know Im not giving you all of the program (settings inside the daq assistant are important!), but it should point you in the right direction. Gotta do your own homework ;) The second basically just requires you to take data as well, and use that as the decision maker instead of a button click. Super easy control system :) Check out [Labview 101](http://www.ni.com/academic/students/learn/) to learn more about labview. And I must ask, where do you go to school? 
Oh no I was not! Let me try that one! I was using this one. http://sine.ni.com/apps/utf8/niid_web_display.download_page?p_id_guid=E3B19B3E93F3659CE034080020E74861
I'm actually a high school student but my professor works at UCI (University of California Irvine)
I was speaking from an application point of view. All languages have limitations to what you can and cant do with them from a CS point of view. And I would very much argue against NI being lazy, or that labview is so dated. You dont see any other general purpose programming languages compiling onto FPGAs (other than C through the XILINX/autoESL toolchain)
How do I find out more about these bootcamps? Like if/when will be coming to my University? A quick Google showed me a bunch of Universities advertizing their own LabVIEW bootcamps, but that doesn't help since I'm not at UCLA. ;)
I've noticed in a few threads that you really dislike numerous aspects of LabVIEW. Are you forced to use it due to legacy code or something like that?
That's probably because the hardcore coders who are using 'general' languages are just going to to write it in Verilog/HDL and not trifle themselves with 'wires'. Oh and, Matlab/simulink can be compiled and run on FPGA's now too. Xilinx and NI are also sort if in bed with one another, so it's not surprising it works so well. In my experience that is the way labview works with most hardware...if it is an official piece of NI hardware with NI provided drivers, it works seamlessly. If you want to interface with something else....you're in for a wild ride
And those groups using vhdl and verilog are going to require a lot of expensive expertise on he team and take multiple times longer to develop than someone using well integrated hardware and software. The simulink FPGA compilation is very very limited in hardware and is far from seamless.
It's tacked on so you don't get objects "all the way down". There's no support for any multiple-inheritance functionality like interfaces.
I have been using it for quite a while. LabVIEW has it's uses, especially with instrumentation. But let us not delude ourselves about it's limitations.
Yeah the other one is supported by NI, so it should work quite a bit better than the one you were using. 
Does save for previous not work for this use case?
Save for previous version should do it. Or post it here: [Version Conversion board](http://forums.ni.com/t5/Version-Conversion/bd-p/VersionConversion)
I'm assuming someone sent him this file, and he doesn't have 2012 to open it. Here you go: http://www.speedyshare.com/ZDwjc/niv1-0-9b.vi
So if we assume that your signal is sinusoidal, I would go about detecting a peak using something like this: for x = startIndex+1 to endIndex-1 if array[x] &gt; array[x-1] and array[x] &gt; array[x+1] then peakFound() Depending on how clean your signal is you may want to modify the check (have it look several elements back and forward to see if it really is a trend in the signal and not just noise) but that is the general idea. *edit, fixed logic error my bad [And here is a link to the block diagram](https://docs.google.com/file/d/0B8lagv4ItwVANVo4U0hoUHVsOEk/edit?usp=sharing)
Your Dad is right! He's giving you a small project and throwing you in the deep end. It's a far better way to learn than the NI standard tutorials (which are Core I, II, and III. And they all suck). I could take a screenshot of a basic program that would get you started, but I won't do that. Maybe someone else will, but I'll just give you some tips to get you started. I've never worked with the USB6353, but the Express DAQ Assistant is your beginning step to talking with LabVIEW. When you place a DAQ Assistant on your block diagram, learn the menus well, each selection is important. Do some test programs first so you can get the hang of it. For example, wire up 5 volts to one of the channels, and read it with LabVIEW. Then toy around. Then try generating 5 volts and reading it with an O'scope or DMM, if you have access to either one of those. If not, well, wire it back into another channel and read it right back! Then maybe you can try generating a custom signal. As for the ADC, I'm not sure whether you mean that he has a separate ADC chip, or he wants you to do it all entirely in LabVIEW. Either way, look it up! No shame in digging around Google or the NI forums to find the thing you're missing.
Entirely in LabVIEW. I've been trying to find stuff with no avail :( Also what do you the menus of the DAQ assistant? I've been reading them up and the only ones I've found use for were the rate(which is basically the frequency)
I'm moving the weekend before so it'll be tough but for now I should be going. I had a blast last year (my first year) but I spent the first day and a half doing CPI training so missed the opening keynote. Tips? Usually they have an NI week website where you can create an account, browse all the seminars and events, and add them to create a schedule for the week. I don't have the site right now but I believe they e-mail that out when you register. If you want to say hi, my company has a booth in the exhibition hall. I'll tell you the company name in a PM if interested.
LabVIEW R&amp;D developer here. I'll be there. :)
Thank you! I apologize for the late response. I was not alerted of any replies[.](http://www.vectorstock.com/i/composite/09,16/thank-you-man-vector-470916.jpg)
Not enough. Probably should have one for the "Parameter", "Property", and "Search and Replace" clusters. Any time you have a cluster it is a good idea to make it a typedef. Or, better yet, a class. 
What youre seeing right now is exactly why you use the NI usb-GPIB device rather than a third party one. I have no idea how they convert their USB-&gt;gpib string. 
So it's more or less impossible without reverse engineering their cable/driver? That's a pain. I'll nag my supervisor to see if he can get me an NI device. Thanks :)
You might be able to modify the driver to use VISA instead. The caveat is that GPIB has more features than serial, and your driver may require those features. I'm not sure why anyone would make a GPIB driver that pretends to be serial anyway. If serial was good enough you would just use serial. The easiest answer would be to just get a better GPIB interface.
All LV objects inherit from the ancestor class Object, just like java. So i don't know what you mean by "all the way down"? Just because there's no Java style interfaces doesn't mean you can't solve many problems with single inheritance. You can even set up "friend" classes to get some of the same functionality. Working with LVOOP even adds additional features like dynamic dispatch (runtime polymorphism).
Call me a noobie, but for the most part OO labview seems extraneous. I can accomplish the same thing with type definitions and data clusters.. I was working on a project at work, and I mused with OO labview for a while, and we ended up deciding on not implementing it because none of our engineers could come up with a reasonable explanation to use it other than it sounds fancy and is a little bit more intuitive to people with CS background. 
I only started using OO regularly in the last six months, but it's definitely made a huge difference in the scalability and maintainability of my code. Most of my code now is written in OO and it's made my projects substantially better. That being said, there is a stiff learning curve and the benefits aren't immediately obvious. I know plenty of great LabVIEW programmers who don't use OO and create very solid code. I also know from my own experiences that it is just as easy (sometimes easier) to write terrible code in OO.
I did some work trying to verify an accelerometer a while ago and learned several things. The first thing is that measuring impacts is really hard. Small, light objects bouncing or hitting a wall produce a huge amount of acceleration in a short period of time, this means you need to sample very quickly to capture the event. Using a large heavy object will lengthen the event time and give you a better shot at capturing the event accurately. Also, if you want to verify the acceleration of the impact you will need to measure or calculate how much the object will deform during the impact. The best way to measure or test out an accelerometer is to use a centrifuge. This will produce a constant acceleration in one axis. In a pinch you could use a fan or a high speed motor (one that you can control speed with voltage would work well), just make sure you have the accelerometer properly secured because that thing could fly off and get damaged or hurt someone.
1) Hang something on a spring from the ceiling, then write a VI that shows the objects displacement from resting using the accelerometer data. 2) Make LabVIEW control another application based on the accelerometer data.
Also, welcome to LabVIEW! You're going to like it here.
Search for examples in help as there are several there. 
Not that I'm aware of, but I haven't really gone looking. How did you get introduced to LabVIEW? Are you using it for research or work?
I'll be there! I'm an intern there and I'll be manning some of the demos as well as taking the whole thing in. I went last year and it was really amazing.
I'm currently using it for a four-week physics project, it's for research into mechanical oscillations and vibrations, really enjoying it
Hi furtivefox, My understanding is that you would like to have an input object instead of the Get Date/Time in Seconds VI. If you just right-click on the output of the Get Date/Time in Seconds VI and select Create/Control then you will have a control object, so you can wire it as you wish and use the connector pane as you've learnt it with the numeric control. You may also find the Timestamp type control directly in the Modern/Numeric palette as "Time Stamp Control". Let me also call your attention that by posting your questions to the NI Developer Zone you might get an answer much sooner ( http://zone.ni.com/dzhp/app/main ) 
You can install the accelerometer and an array of LEDs on a wooden stick. If you have access to fast digital outputs you can correlate the LEDs to the position of the stick, so can write messages in the air (something like a propeller clock)
That did work. The NI forums are currently being blocked by IT at my work. I'm working on getting them unblocked, but the request is still working its way up the chain. Reddit is available however, so thanks!
In addition to what L0ngp1nk wrote I would find local maximums (peaks) by calculating the derivative of the signal, if it is + 0 - , it means you found a peek. To derivate there is a VI but is also easy to do by substracting neighboring values. (so it's the same after all what he wrote). Then you store the indexes of the peaks and if any two neighboring indexes are in the predefined distance you select all the values from your signal in between, then sum them up and multiply with dt. I hope this helps!
I do not suggest either to use the registry as from win7 you have to confirm admin rights to be able to write it. If you use the "This VI" refnum then you can programmatically use the Method that sets the default values. Pity is that you can only use it in Edit mode, it means you have to create a VI that actually calls your original VI and after the original VI had closed it sets values from the outer VI. Using this with the Asynchronous VI Call it is quite comfortable to do.
as for other visual programming languages, there's [vvvv](http://vvvv.org/) and [pure data](http://puredata.info/). they are more similar to max than labview from what i can tell, as i have only seen screenshots of max. there's also audiomulch, and i believe some professional video/graphics software use dataflow approaches. although with vvvv and pure data, i'm not certain that you can create and call sub-patches like you can subVIs.
Thanks! Just what i was looking for. I have pure data but never heard of vvvv, it looks fantastic
That sound so sick!! I very well could be asking one of my professors for a used breakout box and hook up my LED's to a motion sensor camera to really freak out my roomates if they come in my room and they'll know that they have already been caught.
Hey don't know how active you are but is there any chance a bootcamp will be coming to boston?
I'm an undergrad setting up a research project. I learned Labview over the course of about a year. At some points it was the only thing I was working on in my research. I was concurrently learning a lot about how the equipment works and discovering a lot of the major issues. I had no coding experience so it was a pretty steep learning curve. I could describe what I wanted to do quite easily but I quickly learned that it was actually a relatively complex task as far as DAQ goes. I used the forums and this subreddit a lot, tutorials when I could, and emailed with a regional NI engineer a bit. But a lot of the little specific issues like what data types to use or what some error meant I had to figure out on my own through Google-fu and troubleshooting. So now I have a deep but localized understanding of Labview and a pretty complex but good program to do the task I need.
in order 1) NI Training in order Core 1 - kinda skippable Core 2 - good and important Core 3 - good and important Advanced Arch - good DAQ - good and important FPGA and RT - useful if you need to use them 2) CLD and CLA practice exams will teach you a lot by themselves 3) You need to look at professional code and architectures. The NI stuff isn't quite the way it's done in the field. Follow LavaG to get an idea of some of the better way queued state machine and error handlers are done 
Dabbled with it in college. Worked for a consulting company doing mostly LabVIEW for Oil and Gas. They were also a certified teaching center so I was also an instructor. I also do a lot of work with other languagues, general programming guidelines are fairly universal. Be sure to learn them as you're growing with LabVIEW.
Recently, for a number of training modules you have SPOT (self paced online training) available. It works very well if you don't have time dedicated for LV training but trying to catch up when a 30 min suddenly frees up.
Just wire it straight to a chart and set the chart to redraw over itself.
I've been doing that but I've seen some Youtube videos of people's final projects in Labview that are nice scopes with rescaling knobs and saving options. I couldn't find any for download but I thought someone on here might have built one. I just need it for personal use, promise not to redistribute.
Rescaling = changing max/min values on the chart using property nodes Saving = writing the data to a text file You can do this in labview very easily! Take the time to play around with it a little bit.
You may be able to slightly modify the ELVISmx scope VI, which does come with source code that defaults to C:\Users\Public\Documents\National Instruments\NI ELVISmx Source Code after installing elvis mx. Its only configured to be used with the mydaq or elvis boards, but I dont see why you wouldnt be able to change that up. 
I am self taught in LabVIEW, have gone through the Core 1 and 2 classes, and have read the Core 3...other than that, just reading info on forums, and getting my hands dirty with it developing applications. 
I 'played' with LabVIEW in undergrad, not even sure what versions were around circa 2001-2003. At my current job, boss told me I needed to program with a DAQ based on a previous employee's code. The code I was given made no sense, and I'm pretty sure it was based on a "What NOT to do" code I've come across in my texts. While I mostly learned while on the clock at work, I was given no resources, and no one there actually knows LV or had any formal training. There was one Sr. Research Assoc. in another lab that also self-taught, but the advice from that person majorly messed up my code in the long-run. I purchased the LabVIEW for Scientists and Engineers by Essick, and I also use Bloomy's "The LabVIEW Style Book", which, after a firmer understanding of LabVIEW and coding, has been an amazing source. I've been programming with LabVIEW now for a year. While I'm a newly intermediate programmer, I'm still learning more complex aspects of coding each time I sit down to write it. *** I have to mention that without help from a few CLAD and CLA Redditor's, I would have ripped my hair, nails, and teeth out! I received invaluable help in a few short paragraphs from some very outgoing community members of this group that have ultimately influenced my perspective of both LabVIEW and Reddit altogether. This being said, I'm now contemplating spending the $850 on a LabVIEW Programming certification course (out-of-pocket). I've learned a ton from books, but I think it's time I work with professionals so I can stop having to unlearn my previous mistakes. 
This is an old one but you may check this out: http://zone.ni.com/devzone/cda/epd/p/id/1732
Shift register and a xor.
[Here you go](http://imgur.com/uy32uUb) There are a lot of resources on ni.com as well if you are looking to get started with LabVIEW
I think you misread OP's request. He's looking for a case toggle, not a case select.
Thank you, it's not quite what I need. I need it to toggle every time it tests out and it's true. ie: case structure is true and the less? Triggers the toggle so the case structure changes to false, if the the less? triggers the toggle again the case structure changes back to true. If the less? Isn't triggered then it stays as the current case structure. Does that make sense?
Ahh, thanks for the clarification- [updated version](http://imgur.com/JcdyVUk) Is that more what you were looking for? I feel like there may be a more efficient way, but this is what I came up with off the top of my head
sorry for the late reply, that appears to work perfectly, thank you very much for your help!
Ok, I made this example of how to do the time recording, but without knowing what DAQ device you're using I can't get more specific than this. If you are using something like a MyDAQ, you'll hook your buttons up to your device, and then read that channel where I have the boolean buttons right now. Shoot me a reply if you have any questions about this. EDIT: I also don't know what your time scale is. If you're working on a scale of a few minutes or more, you would want to use Get Date/Time in seconds. https://www.dropbox.com/s/5nfkr70g12z5s5u/Time%20Elapsed%20example.zip
Sure thing. Maybe tell me how you are attempting to read the device. Do you have daqmx installed? A quick way to find out is to hit Ctrl+space, and type in DAQ. If that is installed, look for an express VI called DAQ Assistant. When you place that down, it will give you config options, etc that you can set up. Let me know when you get there or if you have trouble and I'll help you out. EDIT: Here's v2. I have it configured for Channel 0 and Channel 1 for a MyDAQ, so you'll likely have to change the configuration for your device. I also suspect you'll have to invert the input from the IR, since they're typically logic level low. You can do that in DAQ Assistant, or with a not gate in LabVIEW. https://www.dropbox.com/s/f7hc9buyjfd868s/Time%20Elapsed%20examplev2.zip 
i'm working on a project that sounds very similar to this one, i only have labview 2012 though, would it be possible for you to save it in a compatible mode so i could see what your VI looked like?
Best place to do marketing
The NI vision toolkit has a neat pattern detection wizard that will actually wire a VI for you to do what you want. Basically, you can grayscale the image, change the contrast around to match ambient lighting and then apply the edge detection algorithm (Laplacian of image, detect the zero crossing, i.e. point of largest change in intensity). After you have image of all edges, you have to detect a pattern, probably a circle for your cups. That will output a cluster for each successful detection including origin point (x,y) + size (x,y). Alternately, you can do this by color kind of like a chroma key. If you subtract the image with every color but the rim of the cup, it will be easier to find those shapes with the pattern detection than using the edge detection VI.
Greetings! On top of what g00gly said, you can alternatively look for the color instead of the pattern of the cup itself - that will only be effective if you know that there will be nothing else of that same color. There are two main kinds of vision algorithms within LabVIEW for pattern matching, by the way - there is the Geometric Pattern Matching and the "regular" pattern matching. The geometric pattern matching can be used to look for the shape of the particular cup that you are looking for because the cup will be deconstructed into a few geometric shapes - or you can use the normal pattern matching algorithm to do something quite similar. The VI that will allow you to do it is the Vision Assistant Express VI if you want to go for the express VI route, or you could look through the example finder to find something for pattern matching as well. There is the following example that you could use to get started: http://zone.ni.com/devzone/cda/epd/p/id/5594 By the way - make sure that your webcam (which I assume is a USB 2 device) is DirectShow compatible. If it isn't, you won't be able to use the Vision Acquisition Software to acquire images and you'll need to depend on some sort of third party software to get the images into LabVIEW. Let me know if you got any other questions or want clarification, and have fun!
Thanks for your answer! I'll try that as soon as I've got time! The pattern detection wizard was something I was looking for!
What you need is a digital edge triggered continuous waveform acquisition and a daq assistant for a digital line input connected to the stop condition of your while loop. I suggest you use the Help/Find examples to search for these, you will find pretty much everything ready to use
First, get the Kinect for Windows SDK 1.8 from here http://www.microsoft.com/en-us/kinectforwindows/ There is a neat open-source project called OpenKinect http://openkinect.org/wiki/Main_Page that has C++ wrappers. You could build a library with it to access through Labview Call-Library nodes if you want to DIY. Alternatively there is a third party labview add-on toolkit the University of Leeds developed that uses Kinect. http://sine.ni.com/nips/cds/view/p/lang/en/nid/210938 After you figure out what route you're taking, you should be able to initialize a Kinect axis like a joystick input using a toolkit. Wire this control into a the value of a dial and you have your answer. 
Does this configure with LABVIEW well? i do have the kinect toolkit installed in my LABVIEW also i also have gesture working well. The problem is i cannot configure my gesture to work to increase the dial.
However I have never used 3DR Radio, I did some search on google: after having the appropriate FTDI drivers installed on the PC your USB Radio module will appear as a regular COM port. After that you can use the NI VISA driver to send/read serial commands. As for the commands, I guess you may find them on these pages (standard AT and special ones): https://code.google.com/p/ardupilot-mega/wiki/CLI https://code.google.com/p/ardupilot-mega/wiki/3DRadio
You can programatically set a value of a control or indicator by using its property node. The property you are looking for is "Value", or "Value (signaling)" if you want to throw an event as well. You can place a property node by right-clicking on the Control object in the Block Diagram and select Create/Property Node So to sum up briefly: if (decreasing gesture happened) { x=NumControl.Value x-- NumControl.Value=x } I could have written NumControl.Value--, but I wanted to represent how you do it with property nodes :) Good luck with your project! 
As I see many people in the LabVIEW subreddit stuck at quite basic LV problems I wanted to share 4 free recorded webcasts on ni.com: Free online LabVIEW training I (53min) http://www.ni.com/webcast/2613/en/ Free online LabVIEW training II (62min) http://www.ni.com/webcast/2614/en/ Data Acquisition Basics (18min) http://www.ni.com/webcast/2926/en/ Introduction to Instrument Control (26min) http://www.ni.com/webcast/3189/en/ Take you time for training yourself as it will save you much more time later!
You probably want the raw values of the position data instead of trying to match gestures then. Look for something like arm to body angle that will output a numeric value. You can then scale this to work with a dial indicator.
Seems very useful! Let's try it out!
You may not even need the mirumod or arduino. Mirumod accomplishes control through the AR Drone's serial port, so in theory you could accomplish controlling the AR Drone using an XBee setup. That being said, if the requirements are to use Mirumod, the only real part of your setup you need to worry about is what transmitter to use, and what signals the AR is expecting. I'm assuming that the Mirumod uses Pulse Position modulation since that's what I've seen with RC Helicopters in the past. If that's the case, you would need to figure out what signals to send and write a LabVIEW program to do that. Perhaps even more useful: http://sine.ni.com/nips/cds/view/p/lang/en/nid/211837
Hey there, firstly thanks for taking your time for replying! :) Sadly, my school project requires me to use both Mirumod and Arduino with the Ar.Drone but not XBee &gt;_&lt; So far i've researched and came across the 3DR Radio can interface with LABVIEW through NI VISA &amp; i'm working on that aside with Mirumod which i've trouble understanding it even though the code is written in C#..
The software costs 2,000 dollar, a little to much for a little project :)
Can you please post the C# you already have?
Hey there, i could only link you to the download as the code exceeds reddit's limitation &gt;_&lt; (http://www.rcgroups.com/forums/showpost.php?p=25650591&amp;postcount=5174). P.S I have to open this code using Arduino for my project Thanks! :)
Ooh, this could be cool! There's potentially a bunch of ways you could do it I think, none of which I have tried, and have no way of knowing how well it could work. I brainstormed a few ideas: * Use the datasocket VI toolkit to open up data from a webpage, such as [this example](http://i.imgur.com/E5GesZv.png) I found. Maybe you can read the code so that you can find some URL that you can scan from that string? Maybe if there's something like currentWeatherIMG = &lt;http://weather.com/kalsdjf/currentradar82721&gt; Then you could open up that URL image in Labview with some clever work. I got a few results by searching for "LabVIEW URL to Image". * Perhaps you can find a way to open up an image of an entire webpage, then toy around with some controls to crop the image down to where the radar always is? * Find some weather program you can download onto your PC, and see if there's a way to communicate with it through LabVIEW. Maybe you could find one with ActiveX support? * Does it need to be displayed constantly? It would be pretty cool to have a little numeric control that you type in your ZIP code, hit a button and it pops up weather.com with the zip code you typed in. 
how can i use this to control the brightness of the LED?
Now i faced this error when i verify the code on Arduino program (http://i41.tinypic.com/2ja97r.png)
If you are talking about a real LED that is connected to a Digital I/O device, then you can generate a PWM signal and set the duty cycle accordingly. If you want to use a virtual LED on your front panel, then you do not have such a parameter like brightness. Instead you can set the color of your LED by using the "Colors [4]" property node. Use the Context Help (Ctrl+H) to see the details for what data this property node expects. The color is in the RRGGBB format so you either create an array of color box constants or you create a formula to calculate the color (it is a number!) upon your kinect input.
I created an example, try this: http://imgur.com/XNR3Yfu
Unfortunately I cannot open the link with the error, on the other hand I'm not sure if we are on the same page. Let me sum up the function of mirumod: The AR Drone can be originally controlled by WiFI. If you do not want to use WiFi then you can install some upgrades on your ArDrone to enable other type of communication channels. 3DR Radio is one of these upgrades, it gives ZigBEE connectivitiy by the X-Bee board. Mirumod is an other type of upgrade, it lets you use a regular RC controller. Having that said you may use 3DR Radio OR Mirumod to control the drone. (it might make some sense to use both in case you need live video feed over the ZigBEE) In case of the 3DR Radio, you can control the ZigBEE transceiver connected to the PC via the RS 232 serial port. In case of the Mirumod you need a regular RC controller. So if you want to use the Mirumod then first you should buy a regular RC controller (for cars, planes, etc..) that fits the requirements: 5+ channel RC TX/RX, 4 channels for sticks, 1+ switch(es) for land/flightmode - according to the mirumod webpage. Some of these controllers have PC connectivity. If you get one that doesn't have that feature than you have to find a way to control the sticks from the PC. It can be done by using an analog output device connected to the RC controller, but it depends on the type. So as you can see you do not really have to interface the arduino with LabVIEW, it is about interfacing an RC controller device.
I have 2.0 running with LabVIEW 2013. You might get better results if you post your question here (https://decibel.ni.com/content/groups/ar-drone-toolkit-for-labview-lvh). There are a few more people there that know the AR toolkit specifically and can help.
Thanks for the input! 
Also, I have the 2010 labview? Do I need 2013? 
Thanks for taking your time to reply so in depth! :) Sorry about the link though, i've uploaded here http://imgur.com/DavDNB0 ! Hmm, i'm only given the options to use the 3DR Radio to transmit &amp; receive to the PC, use Mirumod to interface with 2 ultrasonic sensors by the Arduino codes (where i face the error from the link above), using labview to control the Ar.Drone..therefore i can't use wifi/RC/zigbee &amp; x-bee board as the previous project students have used them :/ Also..I've tried using the old versions of Arduino 0022 &amp; 0023(according to the RCgroup website where they tested Mirumod with this 2 version) but still get the same error message..It seemed like there's duplicate of files in my sketch &amp; the libraries but i can't find any duplication at all when i navigate to the libraries smiley-cry P.S the error message pops out only when i verify it with my modified code here (http://imgur.com/1kLJyvH) 
If you select two channels through the DAQ assistant, you'll get an out put of a 2d-array. Each *ROW* will correspond to a channel. It's just a matter of choosing your device, and then selecting multiple channels (hold shift or ctrl). On a side note, I highly recommend learning the DAQmx API and stop using the assistant for continuous sampling. Although it can be done, it is not recommended (and the API is pretty easy to use once you've seen it done).
You're question is somewhat confusing, but I'll do my best to answer I think what you're saying is you have 2 channels coming into your daq device and you want to read them both at the same time. What you want to do (if using DAQ assistant) is select multiple physical channels when setting up the physical channels of your daq assistant. Use the control or shift key to do this. Now your express vi will output 2 waveforms, one is the x values vs time, the other is the y values vs time. The data coming out of the DAQ assistant will be "dynamic data" type, which is a blue wire. Use the convert from dynamic data bode to convert this to a 2d array of doubles. You can now bundle the pairs of data together as an XY pair that can be plotted using an XY graph
Just want to throw in my support for this advice. This will get the job done, OP.
So tabs look pretty sexy, but can be a pain in the ass. I think you're on the right track in not wanting to duplicate controls code. You can do this with tabs, but it involves a little bit of playing around How about instead of tabs, you give them an enum that is something like "channel 0" "channel 1"..... Now use this enum to as an array index to communicate with the 4 different devices. Honestly, I'd do this OO. It'd get a bit more complicated than I'm willing to type out on a phone. Also, I don't think you need more than 2 loops. 1 being the event handler, the other being a consumer
In this case you have two producers, one for events + one for data and one consumer. A queued state machine is one framework that could work for you. Obtain a queue that you can use to exchange messages between producer and consumer in an initialization VI and share this reference between producer and consumer by wire branch(it's ok to copy). Set up the message contents(through obtain queue vi) to include a cluster of current type def of scope conditions + the current channel either in an enum or another typedef. This can contain any cluster though. You can enqueue messages as events are generated(which you should group together by function) and grabbing current values of controls through local variables. The consumer state machine can poll this queue for a message periodically and dequeue the messages. As each message is dequeued, have your consumer state machine execute the scope functions that were formerly inside each of your event cases. This way, multiple events can generate the same functions by sending the same message.
I haven't tried to interface an arduino with LabVIEW before, but I know that at least on the arduino end you can use [analogwrite(pinNumber, level)](http://arduino.cc/en/Reference/analogWrite) to set a PWM level between 0-255 for a digital pin. On the LabVIEW side, [this video](http://www.youtube.com/watch?v=RGRhIQneO6w) might help.
http://sine.ni.com/nips/cds/view/p/lang/en/nid/209835 This free tool kit allows you to control DIO, AI, PWM, SPI, and I2C from LabVIEW. You basically load a pre-designed sketch from NI then use the LabVIEW tool kit to communicate with the Arduino serially.
Thanks i do already have the toolkit with me. The thread is about the code written on the ARDUINO not on labview. Like a certain code i have to code into arduino program so it can work with labview if i control thru there
My LED is connected to the digital pins. Does this affect the entire process? 
does that mean i do not have to change anything with the lifa base. btw im using kinect gesture to control my brightness. If i do not require to change anything from LIFA BASE that will be wonderful. Im also using wifi codes in arduino
so i can use analog write if my led pin in on the arduino ~ pin?
If the digital pin producing your output has a ~ next to it, this means it is a PWM pin and so yes you should be able to analogWrite to it. On my Arduino Uno the PWM pins are 3, 5, 6, 9, 10 and 11.
Yes, once you load that sketch you can use all the LabVIEW functions to control the DIO, AI, PWM, SPI, and I2C from LabVIEW. No modifications are necessary.
Youd need a simple transceiver to pull the actual transmitted data, and figure out how to interpret the data you got. I had a friend use a 20 dollar transceiver and a webcam (plus image processing) to make an auto-hovering RC helicopter which worked OK. He had to basically build his own driver and figure out the actual communication between the helicopter and RC controller though, which wasnt a very trivial task. 
Hey there, thanks for the reply!I'm guessing using a 3DR telemetry would be possible too? :)
Checkout [ni.com/arduino](www.ni.com/arduino) for the LabVIEW Interface for Arduino (LIFA). LIFA includes a PWM VI as well as an RGB LED VI that allows you to use PWM to control the LED brightness. If you are using just plain arduino you'll want to use a PWM pin to control the brightness. On the uno that mean pin 3, 5, 6, 9, 10 or 11. Use the following code: analogWrite(&lt;PIN&gt;, &lt;VALUE&gt;) And replace &lt;PIN&gt; with the PWM pin number and &lt;VALUE&gt; with a number 0-255 with 0 resulting in off, 128 in half brightness and 255 in full brightness. Post any LIFA questions you have on [ni.com/arduino](www.ni.com/arduino) and I'll do my best to answer them (I developed LIFA). -Sam K
Hey there! Your Labview ArDrone Toolkit is great to use!I'll head over now :) Thanks!
[Try this](http://i.imgur.com/Pv6bblF.png).
It's rounded down to the nearest integer at the moment As it's set up right now it just flips subdiagrams every i value http://i43.tinypic.com/2whmr9k.png 
Alright but I'll need the value to change again at i=2N/3
Basically I just need to use a case structure to plot a chart that's 0 from 0 to N/3-1 then goes from -4 to 6 from N/3 to 2N/3 and then back down to zero from 2N/3+1 to N
It will Never mind, I misunderstood the premise. Give me a few minutes. 
We've never worked with case structures in for loops during class, only with durations in a while loop. Basically I need it to: Output 0 in the first case. Then at N/3 output -4 At 2N/3 output 6 and at 2N/3+1 output 0
[Revised version](http://i.imgur.com/P3YAn4m.png). Top one shows True case, bottom one shows false case.
Do either of these two solutions do what you need? http://i.imgur.com/mneQnZi.png In both cases, I created a boolean array out of your three conditions, converted it to a number, and wired that number to my case structure. Gives you more options than just "True" or "False". If you use a number for a case selector, you need to have either a default case, or cases that cover negative and positive infinity.
Unrelated to the OP but... Instead of using the Boolean Array to number function, just use the 1d array search for a true value. Your case selector values will now be 0, 1, and 2 instead of 1, 2, 4. I find this code to be much more readable. We can argue about performance if you want, but 99.9% of the time I find code readability to be more important than marginal performance gains
The first one almost works, is should be linear between -4 for and 6, not impulses at those times. This http://i.imgur.com/Nd1cyTF.png
Love it. Hadn't considered that. Thanks. I like that it gives sequential numbers. Since readablitiy is a debatable topic, can I ask your opinion on the fact that I put a comment in each case documenting the criteria for that case. Would you consider this acceptable in terms of readability if you were to inherit my code?
Not sure if there's an easy way to format the graph to do what you would like. Hoping someone else chimes in here. My first thought would be to calculate the value of N/3 and 2N/3. Take the difference of those and divide by the difference of -4 and 6. This creates a slope you can use to increment your output gradually, creating your linear line. You would then need to add a condition where i is between N/3 and 2N/3, build that into your array, and create a new case for that condition. You then take your previous output and add your calculated slope to create your new output in that case. See if you can figure out how to implement this on your own. Don't want to do all your classwork for you. :)
We'll, uh, no. USB daq has to be connected to a computer (windows, Mac, or x86 Linux). It cannot connect to an arduino or raspberry pi. Your options as I see them: 1. get a CRIO or standalone daq. This is significantly more expensive. 2. Screw NI. I believe the arduino has a a2d in it right? Program the arduino to do what you need. If all you're doing is logging then I expect this to be fairly simple.
So I looked at some of your other comments. Don't think you really need cases or switching on and off. What you care about is generating an array of N length who's first third is equal to 0, second third is a linear line between -4 and 6, and last third is back to zero. If that's true, you don't need a case structure at all. http://i.imgur.com/igBGL6s.png this code results in this graph http://i.imgur.com/zpptHIj.png If you're dead set on using cases (which you shouldn't be) this is the way to do it. http://i.imgur.com/sR99zZ3.png This code assumes some things (via the coercion dots) about what to do when N isn't divisible by three. It also may be off by 1 in a couple spots. You'll have to take a look at it and decide what you want it to do in these cases
So it's not hard but if you are on Windows you will have to more or less deal with software timing meaning your feedback loop won't be all that great. Helping you would be if you were driving a square wave frequency and not a sine wave. That way you could use a counter output and just command a new frequency with the DAQmx Write function. If you are using a DO or AO you will need to create the buffer then put it on the card which will take more time away from your feedback control.
yeah im on windows, and im also using the timing and transitions measuremnts aimed to the frecuency
Thanks a lot, this works, only change is I need to decrement N/3 by one prior to dividing again.
How much does a myRIO cost, anyway? NI's website doesn't list a price and it looks like an interesting piece of hardware.
Hi Sam, i tried LIFA BASE and its working fine with pin 3 and now i have another issue. It is that can i incorporate WiFi together with the LIFA BASE? im building a project that requires me to use gesture to control the speed of a motor and then wireless connect PC to arduino. I have both the lifa base and wifi code but i cant really seem to put them both together. Your help will be appreciated. Thanks 
I mean the motor speed is controled by labview ( with a train pulse) and the motor is rolling a dent disc that works like the encoder disc. I'ts rolling between the space the opto interrupor works, and depending on the times it "reads" a dent it sends x voltage to labview by using the daq, our profesor need us to get the frecuency the motor is spinning at. After that let's say i dont want the **X** frecuency the motor is currently spinning, i want a **z** frecuency, i need a control that can feed back the current frecuency till it reaches the **z** i desire.
Formula Student teams are work with also use the NI RIO platform. I would encourage you to think one step further and consider using NI RIO for control as well, not just DAQ! Pretty amazing results can be achieved!
Have you tried debugging it with probes and breakpoints? You should be able to figure out at least whether your inputs are what you expect. 
I used probes to debug; I found that my array, which is created and exists only in the subVI, has its correct values. The character that is passed to the subVI is also correct; however, the probe looking at what index value comes after the array/character are passed into the index array function is always -1, no matter what is passed in. I'm super unfamiliar with the nuances of labVIEW since this class is a "teach yourself" type, so troubleshooting errors is hard because I'm not sure how much of my java knowledge (which isn't professional-level by any means, but workable) translates. Thanks for the reply. 
&gt; I was asked to design an oscilloscope for my course (that's it no other help) and to be honest i stitched it from other ones i found online. Welcome to programming in the real world.
Post a VI snippet if you can: http://www.ni.com/white-paper/9330/en/
I just parked to get in to the lab, I'll have it up shortly. Il be in the lab working on this all day, so I can't thank you enough for your help. 
Same here. I had never even heard of LabVIEW when I was first put on the project and I just started reading everything I could get my hands on. I also started watching YouTube videos about LabVIEW and am now finally working towards getting certified. 
are you using the type cast primative? If so, there's your problem. The Keydown event gives you the Char for the key pressed right? If you look at the Char's data type (hold your mouse on the wire and look at the context help window), it is an I16. In labview, a string is a special kind of array of U8s. When you do a typecast to string, you just take those two bytes that makeup an I16 and make them into 2 characters. For example, if I press the A button, I get an I16 from the Char with a value of 0x61. But this is an I16, so it's really 0x0061. This means that if we convert it to an array of u8s it's [0x00,0x16]. Now if we convert that to a string, we have a 2 character string. the first character is 0x00 and the 2nd is 0x61. In LabVIEW's default string control display type, you will not see the 0x00. So it'll just say "a". Right click on a string control and you'll see "Normal display" is checked. Click on "\ codes" or "Hex display" to see the hidden characters. You inputting the 2 character sting "\00a" into the search array function. All of the strings it's searching for are 1 character strings. So it returns -1 meaning it didn't find the string you're looking for. How to fix it? there are a bunch of ways of course, but a simple way is before your typecast to string, convert the I16 character to a u8 character. there is a "to unsigned byte integer" that will do this for your. This is NOT the same as a cast. A type cast will look at the first byte of a I16, and output that (Labview is Big endian). This will result in a 0 in our example. The "to unsigned byte integer" will look at the LSB of the integer and use that. Also, it sounds like your logic has a bug in it. You probably want to take the same action regardless of if they press a or A (notice the case). These will have two separate characters (0x61 and 0x41). Easy fix here is using the "to lowercase" primitive before you search your array. In general. You want to avoid using the typecast for reasons like this. You always end up messing with endianess and things not working like you expected. a pain in the ass IMO. How would I do this? From what i read, here is your problem: Given an alpha character input, i want the number (0 based) in the alphabet for it. See my code below http://i.imgur.com/wsb6nwo.png
OK, so you've basically got a 1 pulse per revolution speed sensor. You need to convert the pulse/rev signal to RPM. Are you familiar with feedback control? I'm not making any assumptions, just asking. It sounds like you are trying to acquire the speed (frequency, to use your terms) of the motor are implement closed-loop control of motor speed. So, you need to convert the feedback signal like I said, then subtract that from your speed setpoint, and there's your error signal. If you can list in detail the hardware and software you're using to attempt this, I can help you more easily. I'm trying to get you to think through this without just giving you the answer. It's hard to do if I don't know exactly what you have at your disposal.
I just started picking it up recently to help mentor a FTC robotics team. I've been working through the FTC labview tutorials in YouTube. Pretty good primer on the basic structures and control flow
http://imgur.com/a/4YT8Z#0 Here is the imgur upload for my project. It is a basic attempt at making an "enigma machine". At this point it is just a basic substitution cypher, but I want to get the logic working first before I go into more advanced methods of cryptography. I think what /u/fuckingpewpew said might be the start of my problem. When I was looking at the example on NI website for the use of keydown, I couldn't for the life of me figure out why they sent it to an unsigned int, and decided to drop it. Clearly thinking I knew more than them was a bad idea, so I'll go in and fix that. Also, I did that to-lowercase to cover the capitals, but I did it inside the subVI, not immediately as the key was pressed - will that cause me a problem? A third question I had was, in their keydown example, they didn't specifically wire in 'string' to the typecast function, but it still knew to spit it out as a string - is this a default setting? Thank you so much for the help. EDIT 1: Also, the "trim whitespace" I put in the subVI was my attempt at troubleshooting; when I dropped probes, it looked like the letters displayed had a leading whitespace character, and in my frustration I thought that might be my problem. I have since taken it out. EDIT 2: Ahhh, it works now. You guys are awesome awesome awesome for all of this help. 
First off, Im an academic field engineer for National Instruments, so I work with people learning labview and teach labview super often, so these are the recommendations I give out on a regular basis. Labview core 1 2 and 3 are awesome in terms learning (its some of the training I went through when I started at NI), and super heavily discounted for students (~90% off in the US) if youre looking for the fastest way to learn. That said, the best free, on demand place to gets started is via the [LabVIEW 101 video tutorial series](http://www.ni.com/academic/students/learn/). There are also some great, [low cost textbooks](http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=labview) on the subject if you prefer to learn that way. I know a ton of people who have read either the Essick or the Bishop book and really liked them. I also hear "Labview for everyone" is good. After you learn some of the basics, try playing around with various things and making some programs. One good exercise is to try solving problems from [Project Euler](http://projecteuler.net/). Best of luck, and let me know if you have any questions! Also- What school are you at? Many large engineering schools also have labview classes taught by Labview Student Ambassadors. They teach a lot of the core 1 and 2 content and give our first level certification exam (CLAD) for free! 
I started using it at work about 10 years ago, but it didn't become the bulk of my job until 6 years ago. Never used it in school. You have to look at NI's site to find out when the Core 1, 2, &amp; 3 classes are offered. They're a good intro, especially if you have little to no LV experience. NI's site has a TON of resources and it's easy to get overwhelmed. I recommend having a specific project that you need to implement in LV. It's not something you can just sit down in front of and "learn", there's just too much to it. LabVIEW For Everyone is indeed an excellent book, you can pick it up from Amazon. Well worth it. Good luck, post questions here!
So I was able to get everything sorted out, and fixed a lot of the errors that were a product of rushing (we should have known better than that). The weirdest part now is that the file plays correctly when I step through the loop piece by piece, but when I just execute the program normally, it only plays one note and then quits. Not quite sure what to make of that, since I've never seen a debugger work properly and then the program not. Thoughts anyone?
When you say "execute the program normally" what do you mean by that? Compiled executable or run arrow in the IDE?
I'm glad you got it working. I'm being nit-picky here because I really hate seeing stacked sequence structures in a dataflow language. You can use the dataflow paradigm to your advantage here by ditching the sequence structure and just wiring your play length directly to your wait.vi, the wait.vi requires that it receive your delay input from your play tone function automatically forcing them to run in order without the use of a sequence structure. http://i.imgur.com/A5oZY9q.png For times when you need a VI that does not require any inputs to run after another VI, you can just drop error in and error out terminals on your VIs and wire the error line to force execution order. 
I got a summer position at a national lab. The AC crapped out and since this is a government institution, it took a month and a half to get a compressor shipped and installed. No AC means tropical temperatures in the laser lab (the postdoc wore an Aloha shirt every day in protest) and tropical temperatures mean unhappy lasers. Instead of doing physics, I was tasked with re-writing a previous researcher's horrible, horrible LabView spaghetti code. I was told she had a cold heart and a mean streak, and it showed in her programming. Anyway, more to your point, I used a copy of LabView for Everyone and googled a million times a day.
There is a "Acquire Sound" Express VI that you could try. Is that what you are looking for? I believe this comes with core LabVIEW.
Hopefully it get's me somewhere, Thanks. I'll see if this works, if not, I'll certainly be back :)
Well, that's what I love about the Express VIs, the configuration that pops up when you drop it down should get you through choosing the inputs. I tried it myself and saw that I could choose "SoundMAX ... Audio Device" or something similar. Just select a duration and hopefully go from there. But yes, the low-level VIs might allow you do things like stream the data and analyze it in real-time instead of having to sample chunks of time and analyzer them? No idea on this one.
Need to convince my boss to pick up one of these. That or an actual cRio. 
All I want for Christmas is an NI Vector Signal Transceiver!
I recently had birthday and even my SO had a hard time finding something I could like. Albums. Music is a pretty good gift imho. Also, an [emotiv](http://www.emotiv.com/).
I'll let Dr. T know. Do you have a color preference? 
The /engineering forum threw a fit when I suggested socks. 
My mom buys me socks every year. I love her for it :)
Try the "Continous Sound Input" example in the Help/Find Example. It's a nice app built from low-level VIs
Blue and grey, of course! Just like all of NI's products :D
Slam dunk. It's pretty alright, for sure. 
I think this link should be what you are looking for http://www.ni.com/white-paper/2710/en/
Using what device of input? You can probably just search FFT in the help or block diagram tools. Not sure about that old version though
Curse my physics-y ways! 
thank you!
10 minutes worth of data in one chunk? Depending on your sampling rate this could be problematic. Your method of attack should be to acquire all data, then process separately. What you don't want to do is have an acquire case where you continuously loop around an array, building it further every iteration. What many people don't know is that Build Array can be a costly function to use. Every time you build an array, the computer has to allocate an entirely new block of memory to 'place' the array. This increases every iteration. It's not a problem, generally, but if your data set could definitely get too large within 10 minutes of acquisition. What I would do is probably stream the acquired data to your disk. Write all the data to a file until you reach your timeout, then go into a separate analysis case where you read all of your data at once from the file. I believe an FFT requires timing information along with the value, so I don't think you'll just be able to read out a DBL (not sure, don't quote me on that in particular, haven't done one in a long time). But if I imagine you have 10 samp/sec * 60 sec * 10 min, that's 6,000 points to read from a file and take an FFT of, which I think should be fine.
That link doesn't work for me. Have you tried emailing or otherwise contacting NI support? Can a prof or students or TA help? I feel like I've helped students with this problem before but can't remember anything useful...
Where are you downloading from?
Depending on your client you may also use Datasocket VIs. It really is a comfortable way of communication. Or if the client is LV the easiest way is to use a Network Shared Variable
Simply download the evaluation version from Ni.com/trylabview (use the See download options to find the Mac version. After having the eval installed you can activate the student edition by using the NI License manager and your student edition serial number. (just make sure you download the LV version your serial number applies to!)
Thanks for the help. Ill let him know. 
No problem. Once he makes a little progress, feel free to ask specific questions, those are much easier to handle than large-scale generalities.
So after much trial and error the both of us managed to do [this](http://i.imgur.com/65vMmgw.png) after looking at some examples on LV. It takes 1 away from 10 but doesn't loop so it finishes at 9. Any tips on how to make it loop? 
Sorry for the late reply needed some sleep. Well, it works! :D I added the shift registers and deleted the case. We put the case in because my brother found a counter example in LV. For the thermometer I did the exact same thing as i did for the tank and just set the rev meter to a constant of 5. Thank you so much for your help we were lost without it. I wish i had enough money to gift you gold.
lol, no worries, I'm just glad it works. I use LV every day so I know the pain it causes when you're not sure how something works. I'm a certified developer and a certified instructor, so don't hesitate to post questions here, I enjoy answering them.
You should convert the dates into a format that can be accepted by a LV. If you use a constant sampling rate then you can create a waveform and display it in a regular graph, otherwise use a XY Graph with Date labels on the X. Can you copy-paste an example line of the date formatting from your original tsv file?
Here's a part of the text I have. The value on the right means depth in cm. 25.2.2013 190 27.2.2013 190 1.3.2013 210 2.3.2013 210 4.3.2013 210 ~~Another problem I also have is when I try to read temperature. If I input depth into a graph it properly shows the values on the Y-axis. But when I enter the temperature data below, the graph just shows every value as 0. Even setting the axis range to 1 through -1 it just shows 0.~~ Realized the temp error. Huge difference between , and .
Try using either the Spreadsheet String To Array or Read From Spreadsheet File VIs. They're located in the String and File I/O palettes, respectively. You can specify a tab or comma delimiter (tab is default, I prefer comma to avoid formatting or parsing difficulties), as well as the format of what you're trying to read. Make sure you check the Example Finder (Help -&gt; Find Examples...), look under Fundamentals (Using Spreadsheet Format.vi, for instance), and you'll also probably want to look at some examples using arrays.
Thanks for the example text. I created a simple VI, it works well on a test txt file. http://imgur.com/qTTLF5e It's a VI Snippet so you can drag and drop it onto your block diagram if saved as the original png format.
http://i.imgur.com/qlOhS3D.jpg If I try a while loop, it will go around the shape very quickly once, then wait the time and then do it again, rather than waiting that time between each point.
Keep in mind that a wait timer only dictates the minimum amount of time for something to happen, it doesn't guarantee it. In this case, you are specifying a half second delay, which sound like it should be plenty. Also, be careful of overuse of local variables. You could be causing race conditions if they are being updated in multiple places at once. Additionally, without seeing the rest of your code, the 2 constants going into Boolean 6 and 7 wouldn't be doing anything effectively.
I am not sure what you are doing there. Why do you have a for loop with only one iteration, you shouldn't need one at all? If you make your screenshot with "Edit-&gt;Create VI snippet from selection" it can help people debug more easily: it makes the screenshot drag and drop-able into labview. Also why are you using local variables for everything? Typically you should use shift registers to store values and write directly to indicators using wiring. Local variables should typically only be used for initializing controls etc. 
Thanks alot for this! It does exactly what I wanted! I do get an error saying the input format is wrong but it still does what I need. You're a hero!
You are trolling me, right? Because I'm YOUR LSA. Boomer Sooner. 
file&gt;new...&gt;select lvclass, save the class with your project, right click on the lvclass in project window, go to properties, then inheritance. in the inhert section you can tell LV what parent the child class has. Now you can make methods in your child class that get run when the equivalent parent method is dropped down.
Just to elaborate on what Dr_Oops said (don't know if that's a play on OOP, as in dr. OOPs :), you got this dialog because you probably double-clicked (i.e., opened) the LocalNode.lvclass:EditOptions.vi, which is a dynamic-dispatch VI. This dialog appears because Line, Machine, VibrationGroup, LocalRoot, and Channel are all child classes of LocalNode that have implemented the EditOptions.vi, so LabVIEW is asking which implementation of this VI you'd like to look at. It's not entirely clear what you're trying to do, but if you're wanting to make your own EditOptions.vi with your own child class of LocalNode, then follow Dr_Oops' advice. You'll create your new class, then select LocalNode.lvclass as your parent class, i.e., the class your class will inherit from. Once you've set the inheritance, you can right-click your class and select "New -&gt; VI for Override…", and you should see EditOptions.vi in the list. If any of this is confusing, just read up on LabVIEW OOP.
&gt;(don't know if that's a play on OOP, as in dr. OOPs :) hahaha I didn't even think of that, I made this UN thinking I could write a solid thesis on how to make mistakes...
Thanks for the elaboration. So THAT'S what a dynamic-dispatch VI looks like. I have always just been using static dispatch. This is partly due to me being the only CLD at my company and not having a whole lot of time to learn new things. Everything I know is from projects I've been thrown in to. I appreciate the help, guys.
When things get stuck it's usually because a resource is blocked. I would guess that one of your DAQ Assistants is running in continuous mode which is blocking your other DAQ Assistant from executing. As for file uploading sites there are a bunch that you can use. Mediafire comes to mind.
no problem. yea, that dialog is somewhat confusing at first, but makes sense after you realize what it's on about. i don't know what the help button brings up in this context. good luck on your project!
Put the files in a project library. This creates a new namespace Or I think you can do a batch "prep end this to all vi names" via a source file distribution. 
Can't you just "Save As..." and then "Duplicate .lvproj file and contents"? I tried it, and when I selected the same directory as my originalit added "1" to all the file names. For example, "main.vi" becomes "main1.vi".
I'd be curious to see your code, but generally speaking DAQ Assistant is more trouble than it's worth. As far as sharing your code goes, check out VI Snippets. Go to "Edit&gt;&gt;Create VI Snippet From Selection", and then send a link to the snippet (From your Dropbox or wherever you can find to upload a relatively small file). The resulting .png can then be viewed by people without LabVIEW installed, and you can drag it on to the block diagram and LabVIEW will import the code.
no, not actually chasing a doctorate... just labviewing it up for work some 40+hrs a week recently (and loving every minute of it, no other language I'd rather work in).
There should be an option to pack and go when your saving the project.
So I knew about the prefix during a source file distribution, but I thought that wouldn't be enough. Now that I think about it, I guess this would solve any problems during deployment if any VI's are forced to shared directories, one would have an added prefix. Thanks!
Is the code in a project? What LV version? If you're lucky enough to have a project, you can create a Source Distribution.
I believe the new one is running continuously, yes. The problem with making it non-continuous is that the VI is ideally going to be running for minutes, perhaps hours on end. Would decreasing the samples per second help, you think?
I will check out whether I can do that from the computer I use for Labview. Regarding the DAQ Assistant being more trouble, can you recommend a simpler way to read the signals from the DAQ?
If you check out the example finder, there are several really good Analog Input examples that would do the trick. Task&gt;&gt;Hardware Input and Output&gt;&gt;DAQmx&gt;&gt;Analog Input You can choose the one that works most like you intend for it to. If you're taking occasional readings, it's probably better to use "Voltage - Finite Input.vi". If you want to check it constantly, try one of the two Continuous Input vi's.
You could set the DAQ Assistants to collect one (or N) sample son demand and put the whole thing inside a while look with a wait delay inside it. This way DAQ Assistant 1 would run, then allow DAQ Assistant 2 to run, wait for a bit and then run again until a stop button (or some other condition) exists the loop. In either case, I would just try changing your sampling mode from continuous to one sample on demand. If your initial problem gets solved then you have a good idea that continuous sampling was the problem.
a source distribution is the best option because it will allow you to deselect anything in &lt;instr.lib&gt;, &lt;vi.lib&gt;, and &lt;user.lib&gt;. it will also allow you to pick a decent file structure for your export, such as placing VIs into folders. you can then zip up the resulting source distribution. if you really want them in one file, you can build a packed project library, but you can't edit these once they're built, as they're basically a LabVIEW shared library. this is likely not what you want though since you're probably just wanting one file for easier file transfers. i've never done the save as and keep hierarchy option, but i imagine this will be pulling things out of LabVIEW's folder unless there are options to ignore VIs there.
sorry, i didn't see your comment about revision control, but i second that suggestion. the working in a project suggestion is also great. i use one in nearly every use case of developing labview code.
what are the similarities? is this a built application that is being deployed to the cRIO? if so, is your cDAQ a standalone cDAQ or a hosted network cDAQ (through USB or Ethernet)? or is your code something that does actual configuring of the cRIO and builds the application? also, as just an architectural suggestion, if you're going to have code be doing very similar things, then that is likely a good use case for using LabVIEW classes. because if you simply duplicate code, it will work now, but if you come back to this later, it will be harder to maintain. only a suggestion though, as i'm obviously not familiar with your project.
doing the prefix at build time won't help though if you are wanting to actively develop both projects which might have shared VI names. adding the VIs to an .lvlib would prevent any development problems of shared VI names. also, why would there be problems for deployment? if one project is for a cRIO and the other for a cDAQ, they wouldn't be sharing directories on the respective targets. or is this mainly a development problem?
Alright, so I have a cDAQ (Windows Embedded, not RT) and a cRIO. The original software is installed on a Windows computer (in this case I'm using the cDAQ) and is then used to configure the cRIO and deploy to it. I want to use the original cRIO configurator software's architecture and some of its classes to make a program to configure the data coming in from the cDAQ modules.
See previous comment about the system setup: http://www.reddit.com/r/LabVIEW/comments/1si98d/renaming_all_project_items/cdz7j1n As far as the architecture, I definitely understand the benefit of making it easy to come back to. At this point, the project is in a rushed state because my boss wants it done by the end of the year, so I don't have the luxury of making it scalable at the moment, unfortunately. I am still using classes, but I'm kinda throwing things together.
I don't have labview in front of me, but the solution should be rather trivial. Store the data in an array. Create a sub array if the last n data points that yiu need. Then sum them. Alternatively, use a circular buffer and constantly take the average of the entire array. 
Thank you very much, the circular buffer worked well
I already implemented the circular buffer found [here](http://www.ni.com/white-paper/7188/en/), but this looks like it would be easier in the future. Thank you!
Thanks for the reply! I assumed the second way is correct due to it working much better so at least I now know that's how other people do it.
Was doing some research to see if my hunch was correct but stumbled on this: http://zone.ni.com/reference/en-XX/help/371361H-01/glang/call_by_reference_node/ &gt;The overhead for calling a VI by reference in the local application instance is negligible compared to the overhead of using a subVI node. LabVIEW requires significantly more overhead to call a VI by reference in a remote application instance. I'm guessing it's a local application instance so there shouldn't be a real difference. My best guess has to do with the nature of the merge sort. Since merge sorts are a more memory intensive sorting algorithm you may be running into delays due to memory allocation. How big are the arrays you're sorting? Someone correct me if I'm wrong, when using call by reference, LabVIEW has to allocate memory for the VI at run-time. Additionally, it has to allocate memory for the arrays as they're broken down. My best guess would be that the memory allocation is bogging down the VI calls. However, I'm not very confident in this guess. I would ALWAYS perform recursion by just dropping the VI within itself. If that's working for you, stick with it and never look back. :) May I ask why you're using a merge sort? Just for fun/testing your LabVIEW abilities? Specific requirement that a merge sort meets? I ask, because the Sort 1D Array VI uses a quicksort algorithm that may be a better option than writing a merge sort. Any chance you'd be willing to share your code? I'd be interested in trying to better understand the performance hit you're seeing.
Hey man thanks for the reply. It's actually for some coursework I have to do with regards to sorting algorithms. I have to implement both insertion-sort and Merge-sort then using the Labview profiler compare the two and explain the differences. I will be comparing them for array sizes ranging from 10 to maybe 100,000. I think you are right about the memory allocation. I notice in the task manager that Labview takes up &gt;1GB when sorting array of 250,000 or so. I also found that the first run takes longer than any subsequent runs. I think that is because after the first run, labview already has memory allocated and it just needs to sort rather than allocate memory. I'll maybe have to err on the side of caution with regards to posting the code in the off chance it violates some coursework submission regulations, although the bottom picture I posted contains pretty much everything as it is the merge-sort VI and the sub VIs are just calling itself, if that makes sense. :)
Good replies here so far. Should you be using option 40 in addition to or instead of 8? I'm working from memory since I'm on my phone. I've always used a string for the vi path, so you might try that. Never tried calling something reentrant like that though. Post some code! I'm curious to try figuring out where the slow down is at.
Hey man I posted a link to the code in a comment :). Please note the disclaimer. I doubt it's necessary but it keeps me safe.
Yeah, you're seriously jumping the gun by saying this is a bug in LabVIEW.
I don't see a reason for this. Spend some time reorganizing the project, getting everything into a single project folder. It'll give you a million file path errors, but you can fix it, and that just may be the way it has to be.
Thanks for your suggestion. I had a look at the buffer allocation tool but surprisingly it didn't indicate any buffers were being created around the for loop. the buffers where being created at the input and outputs to the Mergesort SubVI. I would suppose they would need to be created there though.
This worked! I had some other issues that I had to solve with a colleague but thanks for the recommendation, it worked smoothly. 
Sounds like you're looking at a few copies, some ROI'ing, and some IMAQ Masking. 
I'm ROI'ing, haven't looked into doing and masking. I'll have to look into if when I check in in the morning. I'll get back to you, thanks!
Ppppppppppppppppppplppppppppppppppppppppppppppp
Ok let's try this: * How are you using LabVIEW to interface with your robot? * When you say your servos start spazzing, what are they actually doing? * Do they happen in a certain point in your code? Is this 'spazing' reproducible?
I can take a look for you if you give me some screenshots and description of problem. I don't feel like installing new version just to look.
I figured it out! I started with my initial video stream and created a property node from the original stream called ROI. From there I was able to use ROI to mask to create my mask and overlay the image and the mask with IMAQ mask. Thanks for your help!
http://i.imgur.com/P2HwDGw.png
Thanks. Give me a second to figure this out. Quite a huge block diagram.
I would suggest taking your individual algorithms for scoring, key input and the puck tracking and cut and paste into individual subVIs. That way you don't have to put 30 probes on this block diagram to debug. Secondly, make use of local variables where applicable, or at least label your constants and shift registers with text to clean up the wire mess. Once you complete these style issues, not only will it be easier for another person to look at your code and find out what's wrong with it, but the answer will come to you more clearly as well.
Don't use local variables. Local variables should be used as sparingly as possible. Most of the time they are not needed at all
Local variables can and should be used in some cases, but they are definitely dangerous. They can be very useful for reading/writing to front panel controls. But you have to make sure you are not using them in anyway that allows a race condition. I.e only use them for asynchronous functions. If you use then procedurally one may change before you get to a certain step in a procedure. 
This! As of LabVIEW 2011 or 12, you can right click on wires and choose 'Visible Items'&gt;'Label'. On previous versions you can create a free text block and place it over your wires. Avoid local variables.
thanks. Well I started of from an internet example and worked my way up. That is where the one to five comes from 
Yes. I don't know how you have your code setup, but events are the way to go. When they move the C slider it does one thing, and when they move the F slider, it does another. Lookup basic event tutorials for a start
I think I understand what you are asking: currently you are using a Fahrenheit control to adjust a Celsius indicator. What you want to be able to do is have the same Celsius object adjust the same Fahrenheit object. The way to do this is to make both controls and when one of their values changes, execute the correct conversion code. So let's say you change your Fahrenheit control, you execute the F-&gt;C code and you write the output of that to a property node of the Celsius control's value. Did I understand you or am I totally off? 
Ok, I don't think I'm explaining properly. [Here is a link to a simple application that uses two sliders to adjust Celsius and Fahrenheit](http://www.uploadmb.com/dw.php?id=1387649555) Let me know if this helps.
Put an event structure and add Value Change events for your sliders, then you can read/write from the controls for each case.
You can write to the value of a control using the Value Property. Right click on the control terminal and select create property node.
I found this video: http://www.youtube.com/watch?v=77tSQUbrYmI and the others related to be very helpful as I taught myself how to use labview. It would be helpful to have links for the specific drivers you found to see what you are dealing with. The best way to get a handle on this project of yours is to break it down into bits. Focus on developing code just to read data from the LCR meter. Then work on code to handle the temperature controller. Developing without the hardware handy will be difficult as you won't be able to really test the program until you get back in the lab. You will be able to work on the portion of the code that involves the timing, controlling the temperature setpoints, and some method of data logging. In the end, you'll probably have to pick a specific goal and work toward it, utilizing the forums and this subreddit as you run into more specific problems.
Thanks a ton. Yeah I suppose I can't approach as a single task and try and do it all at once. I appreciate the link and also here is the link to the PTC driver http://www.thinksrs.com/downloads/soft.htm
After reading what you're asking, I'm scared to see your code. "Too many shift registers..." - check out clusters, they help you group data together. That way you can have a lot of different data on 1 shift register http://www.ni.com/video/3233/en/ What I think you're asking is to have multiple loops all pump data into one waveform chart. I'm assuming you mean a multiplot chart, and each loop will be writing into a different plot? Or is each loop writing to a separate chart? What is your supposed to do at a high level? It sounds like you're asking for help on a small section of code, when the real problem is the large section of the code. 
&gt;What I think you're asking is to have multiple loops all pump data into one waveform chart. Yes. &gt;What is your supposed to do at a high level? It sounds like you're asking for help on a small section of code, when the real problem is the large section of the code. The section of code I want right now is a PID controller. The larger code will have something like 6 of these things. A PID controller requires calculating a derivative, which is accomplished by looking back at previous values in the loop. In my case I look at the current value and the value 5 cycles ago (to reduce noise influencing the calculation). So you can imagine having 30 shift registers in a giant while loop would get cumbersome. In any case I seem to have figured it out. I created a waveform chart and a local variable of the waveform chart. I stuck the variable in the loop and the main display out of the loop. Somehow that works...
The best way to transfer data from different parts of a program (independent loops) is to use queues or notifiers. Use Queues if you don't want to miss any data values but be aware that the loop that reads the queued data must execute as fast or faster than the loop generating the data. If the loop that reads the queued data cannot do this use notifiers instead. Notifiers do not buffer data whereas queues do. There are exampled shipped with LabVIEW that you can study to learn how to use both queues and notifiers. As an experienced programmer I use queues in almost every application I write. I very rarely need to use notifiers so my advice is to get a grip with queues. They will serve you well.
In response to your edit regarding loops in subVIs, I know your frustration :) However, I just wanted to point out that LabVIEW makes this type of problem very clear. It's the same as it would be in a normal programming language where you call a function with a while loop in it. The function isn't going to return until the loop stops. As freethinker pointed out, queues are awesome.
With regards to your edit, a loop in a SubVI is no problem as long as you have a way to stop the loop. I don't recommend this, but a functional global is a nice quick and dirty way to get your parallel loops to all shut down. Look into producer/consumer architecture: http://www.ni.com/white-paper/3023/en/ You can set your SubVIs with while loops as producers and then have a single consumer to aggregate all your data. Queues are your best friend. I use them religiously in my software.
Your best bet will be something like this: Step 1: Figure out how to contentiously poll the meter, and display temperature. Step 2: Figure out how to log, and at what speed Step 3: Set up the logic to only log within a range. Step 4: Mash everything together.
Are you using the NI 488.2 communicator in MAX? Take a picture of the status byte on the right side after you attempt the IDN query and post it here for reference. Is COM1 a reference to your serial port? If so, why did you install the NI GPIB drivers? Either use the GPIB bus OR the serial port for remote mode. If it's in GPIB, make sure the instrument is actually in remote mode and listening to the GPIB bus. Also, if it's in a live update, or instantaneous mode, you wont be able to do certain read operations.
http://imgur.com/a/qMX1J
So MAX is what you want to get started. Under Devices and Interfaces, you should see your GPIB card and the device under it. If not, use Scan for Instruments button. You want to use the "Communicate with instrument" button once you find the device and click it. You can use this 488.2 communicator window to send various queries and read or write operations to test the instrument. The VISA address that you use in labview to reference this(the equivalent of COM#) is GPIBA::BB::INSTR with A being the bus # you set for the PC card and BB being the primary GPIB address you set on the instrument itself through the button menu. My first guess that it's not showing up is because you have a primary address of 0(reserved for controller for broadcast msgs). Although, I've had to do weird stuff to get certain Keithley meters to talk with the NI GPIB USB and Ethernet devices like installing older versions of GPIB and VISA. So let me know what you see in MAX. If none of that works, here is a white paper with some additional things to try http://digital.ni.com/public.nsf/allkb/A80DBFCCAC36EBDE862562C80058856E
Have you addressed the images I sent you? And, yah, when I refresh (the equivalent of "Scan for Instruments button" I believe), nothing shows up. So I can't really follow the rest of your advice, though I certainly appreciate it, and it seems like I'm kinda stuck then...
Oh, and no, I'm on address 24.
http://imgur.com/4Lx1qr1 I have been trying to figure that out. Which number on there do you suppose it is. I was given this computer by the lab, so I didn't get to choose the card myself.
I think I found the solution. I'll get back to you if it doesn't work. Thanks for all your help!
Hi idoescompooters, i would say you are going to have better luck on the NI FIRST forums or the ChiefDelphi forums. Most people on the labview reddit are probably not familiar with the FTC specific IDE. If you need help with your FRC code a few years from now, i was actually a mentor so you can hit me up with questions. Here is a white paper on how to set up LVLM though https://decibel.ni.com/content/docs/DOC-17997 
That's what I was thinking. Thanks anyway! Also, I tried RobotC early on, and even while having previous programming experience, I was unable to pick it up. It was very challenging to learn.
That is a reference to a location in memory, so you do not need an array of them. Right click the loop tunnel and select "disable indexing". For instance, if you were to do this for a string datatype, you would get an array with one string for every loop cycle with indexing on, or just the string of the last loop index with indexing off.
it might be right click -&gt; Tunnel Mode -&gt; Last Value if they are using labview 2012 or later.
Keep in mind, OP, that auto-indexing can be a very powerful feature making building and decomposing arrays very simple. By default LabVIEW will automatically apply this behavior to all inputs and outputs for FOR loops, and you can optionally enable for While loops as well. For instance, instead of writing to your file every single time, you could build all of that data into an array (automatically), and perform just 1 write after the loop. This would help improve performance in many cases. However, since you are limiting your loop to a minimum 500ms execution time, this will not affect you.
Thanks for the help! This fixed it.
Labview student edition can be purchased for 20 dollars here: http://www.studica.com/us/en/National-Instruments-students-ni-labview-mydaq/labview-student-edition/779252-02.html Or you can get a 6 month copy for free if youre not using it for any courses: https://decibel.ni.com/content/docs/DOC-30610
I would really advise reading up on some basic architectures before you start your program. Using a good architecture will take a bit more effort at the start but save you time and headache in the long-run.
that price seems more than a little shaky.
i stand corrected. :)
Related: Schools get a discounted licence as well right? Unrelated: It's pretty cool to see NI people on this subreddit, are there many of you on? 
Schools have a lot of ways to buy licenses, including some only available for them like the academic site license. All are heavily discounted, but there are some asterisks in there, such as different pricing for teaching vs research use. 
Excellent! Now add lasers and chainsaws.
FIRST robotics I presume. LabVIEW is a language fast at getting everything up and running quickly, makes it easy to rapidly expand the code to accommodate new hardware, makes more advanced things like vision processing easy, and also runs quick enough. If you aren't already, you should be all over the Chief Delphi forums. Do you have experience programming in any other languages?
I don't he any other programming experience, but I'd like to learn. I'll check those forums out too, thanks :). Yes I'm with FRC as well. 
/r/FRC is a pretty good subreddit for beginner help. You can also just google "LabVIEW with FRC" and there are some guides to help you. Also, some veteran teams have good resources for you to use. I like [Team 358's website](http://team358.org/files/programming/ControlSystem2009-/LabVIEW/). And, of course, the FRC community is eager to help newbies out, so if you ever have more questions, [Chief Delphi](http://www.chiefdelphi.com/forums/portal.php) and /r/FRC will be willing to answer them. 
Your mentor must be totally awesome. He sounds like the type of guy that would spend his time helping out on a Saturday and recommending great subreddits like this one. Plus, I'll bet he's a total badass at BF4. o_O In all seriousness, there are forums, books, documents, websites, etc., etc. aplenty out there. You won't have to look far at all to find help once you've got a specific question. The key to getting to that point is picking a basic task and trying to accomplish it in LabVIEW. Something simple like turning a front panel indicator on and off automatically. Then find a way to control the frequency at which the light flashes. LabVIEW's Example Finder (LabVIEW Splash Screen --&gt; Help--&gt; Find Examples...) gives you access to hundreds of examples of every imaginable type, all grouped by category and searchable. Start with something in the Fundamentals category. Post questions here or ask your awesome mentor. 
Nailed it, except he's horrible at battlefield ;D
Hi, I really enjoy working with Labview, but i do not find that many positions looking specifically for Labview for control systems. Is it just my area (Scandenavia) or am i just looking for the wrong thing? Or is it more like freelance work? Thanks in advance
Cool. Looks like it works pretty well for a week's go at it. Code looks okay, it's kind of strange how you handled the event handling loop. Though I haven't done robotics or vision transfer so I'm not sure how much differently you are supposed to manage that. If it works, it works, though. I don't know if I'm old fashioned, but I always tend to stay away from the express VI's. You'd probably get faster response if you were to make your own driver subVI using native DAQmx.
Great work! Code looked pretty clean. What was something that you learned from taking on the project?
I used the myRIO express VIs because I tested it and you cannot press/release keys without having 10 to 20 ms in between them, which means that the get movement commands loop on the myRIO has a period of at least 10 ms (usually of more than 100 ms), because it waits on a new command (using the Wait on notifier vi) to be received through TCP. Using express VIs also allowed me to avoid doing any FPGA programming, as the myRIO comes with a pre-compiled personality, which has numerous DIO, AI, AO and PWM, which were more than enough for me. I could not have used DAQmx functions, as the myRIO is not a DAQ device.
The main difference between the execution time between the native functions and express VI's is that with express VI's, every time you execute call it, you open a reference, initialize a bunch of options, execute the command, then close the reference. With the native functions and your own driver, you open the reference and initialize when you want, and every time you execute the command, all you're doing is writing or reading, not opening and closing a reference also. In my experience, writing or reading is the easiest part for the computer, it's initializing the hardware that takes the most time. If you configure an express VI, say an analog out, view it's block diagram after converting it to a normal VI, there's just a bunch of shit in there you're doing every iteration that you just don't have to. Ah, I'm guessing you probably already know all of this. And like you said, if it's not entirely time critical or your project's bottleneck, you can get away with it. Still, are you saying that the myRIO express VI's are different than the normal Input/Output express VI's? Because the I/O express VI's are definitely not just consisting of IF statements. I can totally get that if you're doing just a week project, you won't want to spend a bunch of time writing and compiling FPGA code. Can be a real bitch. 
Thank you, I will look into it.
If you want a good YouTube channel to check out, try [Sixclear's VI High series](http://www.youtube.com/user/Sixclear/videos?flow=grid&amp;view=0&amp;sort=da). They are professionally scripted and edited, unlike most tutorials you find on there that are just screen captures of someone's desktop as they improvise their way through it all. Part of my job is to create testing software for lasers using LabView. I had previous programming experience in other text-based languages, but the SixClear videos were all I needed to transfer that over to LabView well enough to start using it professionally.
Ask a specific question and you'll get a specific answer. Ask a generic question and get a generic answer.
There are some good basics here: http://www.ni.com/academic/students/learn-labview/
Thank you for the link, I will start going through all of those now. 
I wasn't sure if anybody would be willing to help so I didn't type out a lengthy question with a lot of specifics to save us both time. 
[Here](http://i.imgur.com/qNSf8k6.jpg) is a link to my current VI (sorry for the mess) and I need to fulfill [these](http://i.imgur.com/8BgiqGe.jpg) requirements. All are met besides the TurnOn in positive and negative directions. I am not sure what they are asking so I don't know what to do. Any advice?
It's not formulated very clearly. My interpretation: * in the forward direction, 0...100% is mapped to TurnOn_Forward...MaxVoltage; * in the backward direction, 0...-100% is mapped to TurnOn_Backward...MaxVoltage. E.g. TurnOn_Forward = 1, TurnOn_Backward = 2, MaxVoltage = 3, then the output tuple (direction, speed) (bool, float) will be * -100%: (False, 3) * -50%: (False, 2.5) * 0%: (False, 2) or (True, 1) * 50% (True, 2) * 100% (True, 3) Unfortunately, this is the kind of thing LabVIEW sucks for. These kind of logic/calculation combinations are typically much easier in a 'calculation' block, where you would use a C-like syntax to describe the behavior.
Okay. Thank you very much for the breakdown of this. The professor ended up posting the solution for people to see b/c there were so many questions. 
This is a simple counter that every time it is accessed increments by one. It outputs true and automatically resets when the limit is reached. It can also be manually reset through the Reset? control.
Just out of interest, do you know if this approach with a case structure uses less overhead than using a select?
Good question, I'd like to think the compiler is smart enough to give the same result for both options. I reckon any difference will be negligible in this case at least.
I don't understand what this has to do with an FPGA?
I think divadsci is right. Darren posted something the other day on the NI forums about how a two case case-structure is quite a bit faster than a multiple case. This isn't a big deal for most of us most of the time, and you shouldn't be nesting two case case-structures in most instances. 
this is implemented in Labview FPGA. It could be a general purpose function in normal labview too, but its pretty cool that the same code could be compiled on a typical PC, embedded RT target, or an FPGA. 
I really don't know anything about LabVIEW but I'm curious. Is there an inherent advantage toward having this flexibility you mention or is it just "cool"? Also, what is the difference between implementing something in LabVIEW vs. doing FPGA with an HDL (Verilog or VHDL) which can also be simulated on a PC? 
I've never touched FPGAs with HDL although it's on my list, so I can't give you much of a comparison I'm afraid. The key difference however is that this is write once run anywhere code. It's not FPGA code that's simulatable on a PC, it's LabVIEW code that can be run on a PC or an FPGA.
I dont have any direct experience with the low level structure (bit file) generated by labview, but I do think that if you code some functions using just verilog and compare it to equivalent code generated by labview I would be very surprised if the generated code could compare regarding efficiency and/or FPGA real estate consumption (space). I would love to learn more about the process of migrating from the labview environment to verilog or vhdl... maybe someone can correct me if my thoughts are wrong here...
It would be a very significant undertaking. Probably a complete re-write Why are you trying to do this? Reduce jitter? Run headless? Expand?
We would like to remove the overhead the windows system brings, make the software more secure (propriety wise) and because our current system has to be on site for data collection, we can reduce the physical size. Currently the data collection unit is sperate from the processing unit. There my be even a future possibility of combining both.
It depends what exactly you are porting. The biggest step up is you need to start moving data off the machine. For that you'll probably want to use TCP/IP. You don't want to start from scratch there. You'll want to borrow concepts from other similar devices. My favorite TCP protocol so far came from [LMI][]()(http://www.lmi3d.com/products/gocator/)](http://www.lmi3d.com/products/gocator/). You could also do something like RPC/SLIP or look at NI's STM libraries. From there, you have to totally decouple your application from the user interface. I don't know how coupled you are so I can't tell you how hard that will be. You'll probably want to improve your code quality. Things that were OK in windows (like dynamic memory allocation) pays a higher penalty in real time. Not sure what hardware drivers you are using. You might have to re-implement them in FPGA. If you're just reading and writing IO once every few hundred ms, that's pretty easy. It sounds like you are doing some high data rates though. You might want to contract that out. It's easy to do if you've done it before. The FPGA provides amazing customizability of data acquisition. Think about your dependencies. You'll have to do some custom TCP stuff if you want to write directly to a database. 
Hey, thanks alot. I forgot to add this, they were also interested into porting to ARM systems . Does LabVIEW support ARM systems well? Would it be easier to port? I went to a NI user group meeting and asked the guy about porting code over and he said a vi can basically be used in any system, its all a LabVIEW thing, maybe a few low level VI's must be changed but otherwise it shouldn't require a massive rework.
A vi can indeed generally be used on any LabVIEW platform. However an FPGA implementation adds limitations (along with its benefits). For instance arrays have to be a fixed size. Your existing code may well (probably) also make use of floating point data types, in order to fit your VI's onto an FPGA you would want to transfer these to fixed point data. Floating point is supported but uses far too much silicon to be particularly useful. Are you sure you will benefit from this? FPGAs really shine when the problem they're solving requires hardware level determinism and massively parallel operations. If those don't matter to you you may well end up putting a lot if effort into something for but a whole lot if gain...
1. "Remove Overhead the windows systems brings": Usually when people say this they're talking about locking the system down, so an operator can't do things like browse the internet on the workstation, or shit it down, whatever. There are a lot of strategies to do this. Other than that, windows brings zero overhead IMO 2. "Make the software more Secure (Propriety wise)": Run a executable, not source. Problem solved. They cannot "de-compile" a LabVIEW exe. 3. "Reduce Physical size": $400 and a random google search gets you a tiny form factor PC. These PCs will be the same size or smaller than a CRIO. 4. "Merging Data and processing": FPGA may be able to help you here, but the fact that you have "some massive code" my guess is no, not much. FPGA is a different beast. A lot of stuff that you take for granted in regular labview doesn't work in FPGA. Remember, FPGA is configuring hardware, pretty different that a high level programming language on a PC If you switch targets to a CRIO, you may be able to just copy paste code to work on the real time part of the code with minimal changes (if you're using any DAQ devices that wont work anymore for example). If you want to do it right, it'd be a complete re-write. The real reasons to consider RT/FPGA: Faster timing (faster than 1ms) or ruggedized headless operation. Obviously I've made a lot of assumptions in this response, but I'd recommend against trying to port it over. It'll cause a headache. Go into it planning on complete re-write.
There was an ARM module for LabVIEW, but as I understand, it was not a lighthearted undertaking. Also, moving a large code base from windows dev to any of these targets would probably be a massive undertaking, requiring re-architecture of many portions of it; it would be a big job for someone familiar with this type of endeavour. It would have a big learning curve for someone unfamiliar.
As a note, a pxi chassis isnt really any different than any other x86/x64 computer outside of form factor. NI also offers a real-time license (Pharlap) for desktops and laptops as well. 
I'm no great fan of them, but Arduino / Arduino clones make cheap DAQ devices. Have you considered using one to talk to the board? It looks like SFE offers [a RFM22 Arduino Shield](https://www.sparkfun.com/products/11018) that might be directly compatible with the air interface on the RFM12B or at least pin-for-pin replaceable with the RFM12 module, and NI has an Arduino LV library. Might speed up development.
No real need to write your own driver. NI has USB to SPI adapters: http://sine.ni.com/nips/cds/view/p/lang/en/nid/210256 Or if you are on a budget: http://www.totalphase.com/products/aardvark-i2cspi/ (has LV drivers available on the website)
Sorry I should mention this is a university project on a very strict budget (£15 left for the RF link + uCs). We use Total Phase adapters a lot at work though and haven't had any problems so far.
I wonder how slow the RF link goes? Could you [bit bang](http://en.wikipedia.org/wiki/Bit_banging) it?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Bit banging**](http://en.wikipedia.org/wiki/Bit%20banging): [](#sfw) --- &gt;**Bit banging** is a technique for [serial communications](http://en.wikipedia.org/wiki/Serial_communications) using software instead of dedicated hardware. Software directly sets and [samples](http://en.wikipedia.org/wiki/Sampling_(signal_processing\)) the state of pins on the [microcontroller](http://en.wikipedia.org/wiki/Microcontroller), and is responsible for all parameters of the signal: timing, levels, synchronization, etc. In contrast to bit banging, dedicated hardware (such as a [modem](http://en.wikipedia.org/wiki/Modem), [UART](http://en.wikipedia.org/wiki/UART), or [shift register](http://en.wikipedia.org/wiki/Shift_register)) handles these parameters and provides a ([buffered](http://en.wikipedia.org/wiki/Data_buffer)) data interface in other systems, so software is not required to perform signal [demodulation](http://en.wikipedia.org/wiki/Demodulation). Bit banging can be implemented at very low cost, and is used in, for example, [embedded systems](http://en.wikipedia.org/wiki/Embedded_systems). &gt; --- ^Interesting: [^I²C](http://en.wikipedia.org/wiki/I%C2%B2C) ^| [^Bit ^manipulation](http://en.wikipedia.org/wiki/Bit_manipulation) ^| [^Serial ^Peripheral ^Interface ^Bus](http://en.wikipedia.org/wiki/Serial_Peripheral_Interface_Bus) ^| [^Serial ^port](http://en.wikipedia.org/wiki/Serial_port) *^\/u/infinitenothing ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cfghm3q) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cfghm3q)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less.* ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 58675:%0Ahttp://www.reddit.com/r/LabVIEW/comments/1xzyaj/interfacing_with_spi_rf_board/cfghly6)
I do ATE software development for an Alliance Partner. Usually when I'm not working I try to force myself to use other languages. Currently I'm learning PHP/HTML5/SQL for web stuff. But sometimes that feels like school. So if I don't feel like work, and I don't feel like school, I usually end up doing junk like this. Overall time spent is ~4 hours or so. Basic architecture is templated so I don't have to do a lot of the grunt work. 
Just want to point out, best practice is to use a shift register there. If for some reason the for loop every had a zero passed to the iteration terminal it would drop the reference without it. 
I totally agree with and endorse this comment if you're working with small data sets. If you working with large, unbounded arrays, be very careful using them in queues. I think it's still a recommended practice to use Functional Globals to accomplish large data set movement. Remember, it may look like LabVIEW is handling memory just fine right up until the point that it crashes. 
The term you are looking for is 'debouncing' your digital input. Very common, especially in button pressing since buttons can sometimes be finicky. It can be done really easily using shift registers like I show [here.](http://i.imgur.com/OW7DYlW.png) Need more samples, just pull down the shift registers. Easier and faster than averaging. 
Could someone convert me a copy to 2011? I'm curious to see how this is implemented. Thanks
i learned OOP, even in labview, from head first java. it's excellent. there are a few white papers as well on ni.com about labview's implementation.
Nice! Would you consider posting the code? 
Woops! Didnt notice you posted the code when I was on mobile! Thanks!
link to code is posted in comments of video. also, I just found this link, so don't credit me with anything here haha
I've never done mindstorm but here's what I'm thinking. It looks odd to me that you have two things writing to port B and C. It's like having two things tell port B and C what to do. They are going to be confused. Instead you have to have a way of negotiating what to do with B and C. That is, you have to make a formula for how to calculate B and C fwd. For example, you could take the average of the top section of code and the bottom section of code and wire that into port B (once). BTW, what's Port B rotation? Is that an input or an output? It sounds like an encoder position in which case I don't understand why it would factor into to calculating a "fwd" amount. Also, you should see if you can use a chart instead of an XY Graph. You can wire the chart straight to rotation. 
The two things wired to port B and C are the two separate motors. For the lab, I do need to have two separate numeric expressions to tell each motor how much power to use. The rotation VIs are something I have not yet explored but I hope to use them in my final program. That portion of the code was the PID controller but it was not quite tuned to my objective. I will keep that in mind when I need to use that function. 
The top portion appears to make sense as a P controller. However, the lower portion appears to write a value to the same outputs in parallel. I don't understand why you would do that. 
(don't have my rig in front of me, so I can't help with the error) The best way to figure it out is to send one command at a time. Create a loop with the write function in it, and pass in an array of the commands, so each is executed separately. That will tell you which ones are throwing the exception (use the probe and light bulb to see which one causes the error). Also, when reporting this stuff, always include the full error message. I can't help because that number means nothing to me. 
Great way to do this. If you use the highlight execution though, be sure to lengthen your visa timeout, as highlight execution slows everything down a ton. Also: This is why its not advised to use an 11 year old version of labview (or any language/ide, really), the driver support is just really rough because its so old, and its going to cause headache in the long run. 
Well the bottom portion was for some code that did something entirely different than what my objective is. I didn't make it, I was just going to try and convert it to do the job I wanted it to do. 
This is a good idea as well so you can control the timing. Some scpi commands will throw an error if you stack them one after the other too quickly.(Youll get errors like query interrupted) What is the actual error the agilent meter is throwing on the front screen? That will be more useful than the labview error.
Port B is a motor and Port C is a different motor right? The VI at the top right is telling motor B to go fwd and VI in the center to the left of Init is also telling port B to go fwd a different amount.
I'm probably going to delete the middle vi and make my own PID
Ah, sorry. The error message in Labview was that there was an error with the Visa Read. The instrument error is "error with remote command".
Haha, don't hate on 7.0. It's still the best version to use on Windows 2000/XP systems.
Do you have access to the PID Toolkit in LabVIEW? It sounds like you do, based on your initial post. I'd say stick with that unless you're very familiar with PID control. How much experience do you have with PID? I'm fairly familiar with Mindstorms components, I'm actually using the motors as a basis for a system to test control algorithms at home. But, I've not had much call to use the Mindstorms VIs. It looks to me like the Rotation VI is the encoder feedback from the Port B motor? First off, you have to calculate each motor power setpoint individually, as you said. So you will need to create a subVI for each. Inside that, I'd recommend using the LabVIEW PID VI, you don't need to reinvent the wheel. Next, you need to figure out how you want to close the loop on the PID control structure. Right now, you're sort of doing it in two ways. One is via the Light sensor, and the other is by means of the Rotation VI. You're mixing measured and controlled variables, and the way your code is written right now means that the execution order is indeterminate. LabVIEW is all about dataflow to control execution order. You need to use the error clusters on the VIs to get them in the order you want. First off, decide what your control structure is going to be. I'd suggest planning on having an outer position control loop and an inner speed control loop. The 'Centered Value' control would act as the position setpoint and be the input to the inner loop. Inside the inner loop (which can be a subVI), you can use the LabVIEW PID VIs to calculate the speed setpoint for each motor, and the motor encoder will serve as the feedback for those. Keep in mind that you'll want to have a limit on how far the motor speeds can differ or things can get a bit unstable. Using the Rate Limiter VI in the PID Toolkit comes to mind. One thing I just noticed is that it looks like you're going to drive in circles, but I'm not sure what the numeric values out of the Light sensor VI will be.
If you can see the functions palette shown on page 3 of the linked document, you have access to the PID VIs. I downloaded and installed the NXT toolkit, so if you have any questions about it or PID control, post them, I'll be glad to help. I'm a licensed Professional Engineer (Control Systems) and a Certified LabVIEW Developer, PID control is my bag.
The document that I linked? I don't think I do. I went back and found a PID I wanted to use from: http://academic.amc.edu.au/~hnguyen/JEE344ACE/tutorial07.pdf I used the PID from figure 4. Here is my snippet: http://imgur.com/AAUv4VA and everything is good except I can't run it yet, theres a problem with the derivate VI.
Fixed the problems using regular arithmetic operations since the NXT can't handle calculus. I got a solid arrow and now I need to see my robot react to the light 
Yes, that PDF document for which you posted the link, the same one you just posted in this reply. Page 3 shows the PID functions sub-palette which, in that case, is under the Control Design and Simulation palette. If your installation of LabVIEW is different, the PID VIs may be under the PID &amp; Fuzzy Logic Toolkit palette. I'll try to take a look at the code in your snippet after work. What's the problem with the derivative VI?
I never installed the PID toolkit, since my professor wanted us to make one by ourselves. I found that example of different PID types to help me get going and found out that the NXT will only work with the VIs under the NXT Robotics palette. So here is my current setup: http://imgur.com/QffvHqL This set up actually works and I am in the process of finding the correct values to put in for each variable.
It looks like the sensor is basically just a resistor that varies based on the force applied to it. All you'll have to do is hook it up to your myDAQ and then do the programming to translate the resistance you get to the force measurement you want.
myDAQ measures voltage (unless you're using the DMM), so you'll need to build a circuit [like this one](http://www.tekscan.com/flexiforce-sample-circuit) and refer to the sensor documentation to correlate the voltage reading with the force applied- start with the "how it works" tab.
Thanks!
It's basically a variable resistor and you need to do a resistance measurement. Here's the design circuit they give you: http://www.tekscan.com/flexible-force-sensors The question is how much accuracy do you need and do you want to make your own circuit. If you don't want to make your own circuit or you need the accuracy, you could use a [DMM](http://sine.ni.com/nips/cds/view/p/lang/en/nid/204061) or a [strain sensor](http://sine.ni.com/nips/cds/view/p/lang/en/nid/204069) would probably work
Actually, I just look at the specs on the MyDAQ, and you can do a resistance measurement directly. You probably don't need the Op Amp at all. http://www.ni.com/pdf/manuals/373060e.pdf
I left this out in the beginning because I didn't think it would come up. I will be measuring the force on people's feet as they walk, which is above the rated load (100lb). But the double asterisk, under the specs tab, says that up to 1000 lb can be measured by using that circuit and lowering the the feedback resistor and drive voltage (though, they don't say what to lower them to). I believe I would need the circuit unless the myDAQ can do all of that too.
Normally, it is easier to measure a voltage than a resistance. The circuit they link to is just a convenient way to turn a resistance into a voltage. Since a DMM can measure a resistance directly, you are good to go and I don't think you need the circuit.
Oh really? So what is the point of that double asterisk note then? If you were measuring force over 100 lb *and* for some reason you also wanted a voltage output; then is that when the circuit may have to be modified?
Yes, using that circuit, without the modifications, you may have some [clipping](http://en.wikipedia.org/wiki/Operational_amplifier#Non-linear_imperfections) when measuring high forces (low resistances). A DMM won't have this issue because it can measure very low resistances.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 10. [**Non-linear imperfections**](http://en.wikipedia.org/wiki/Operational_amplifier#Non-linear_imperfections) of article [**Operational amplifier**](http://en.wikipedia.org/wiki/Operational%20amplifier): [](#sfw) --- &gt; &gt; &gt;* In the case of an op-amp using a bipolar power supply, a voltage gain that produces an output that is more positive or more negative than that maximum or minimum; or &gt;* In the case of an op-amp using a single supply voltage, either a voltage gain that produces an output that is more positive than that maximum, or a signal so close to ground that the amplifier's gain is not sufficient to raise it above the lower threshold. &gt; &gt; --- ^Interesting: [^Operational ^amplifier ^applications](http://en.wikipedia.org/wiki/Operational_amplifier_applications) ^| [^Current-feedback ^operational ^amplifier](http://en.wikipedia.org/wiki/Current-feedback_operational_amplifier) ^| [^Operational ^transconductance ^amplifier](http://en.wikipedia.org/wiki/Operational_transconductance_amplifier) ^| [^Open-loop ^gain](http://en.wikipedia.org/wiki/Open-loop_gain) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cfpwej2) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cfpwej2)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 72805:%0Ahttp://www.reddit.com/r/LabVIEW/comments/1z1wkf/how_do_you_tell_if_a_sensor_can_be_used_with_lab/cfpwe4u)
Well, it doesn't have to be highly accurate, so I'll just try it without the circuit and if it isn't good enough then I'll add the circuit. Thanks a lot for your help.
As long as you measure any voltage between -10V to 10 Volt it mostly doesn't matter what you are measureing. After that use your hopefully given calibration curve/formula to calculate your final values. Also: think about errors. Also a good starting point: http://www.ni.com/white-paper/2835/en/
Would there be any problem measuring resistance with the DMM instead of voltage? The calibration curve will have to made, they have a video showing it.
It's connected by a GPIB port. How do I check the serial settings?
Okay thanks, will check that out!
GPIB is not serial (RS-232 is). So it won't be that. Do you have MAX installed? It's usually in your National Instruments folder. Try using that to send simple queries directly to the device.
For measuring resistance you apply a constanc current and again read the voltage: http://en.wikipedia.org/wiki/Four-terminal_sensing So no problem - search for constant current sources. Look up how this sensor has to be wired - bet they give some information about how it has to be measured. If not - try with this.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Four-terminal sensing**](http://en.wikipedia.org/wiki/Four-terminal%20sensing): [](#sfw) --- &gt;**Four-terminal sensing** (4T sensing), **4-wire sensing**, or **4-point probes method** is an [electrical impedance](http://en.wikipedia.org/wiki/Electrical_impedance) measuring technique that uses separate pairs of [current](http://en.wikipedia.org/wiki/Electric_current)-carrying and [voltage](http://en.wikipedia.org/wiki/Voltage)-sensing electrodes to make more accurate measurements than traditional two-terminal (2T) sensing. 4T sensing is used in some [ohmmeters](http://en.wikipedia.org/wiki/Ohmmeter) and impedance analyzers, and in precision wiring configurations for [strain gauges](http://en.wikipedia.org/wiki/Strain_gauge) and [resistance thermometers](http://en.wikipedia.org/wiki/Resistance_thermometer). 4-point probes are also used to measure [sheet resistance](http://en.wikipedia.org/wiki/Sheet_resistance) of [thin films](http://en.wikipedia.org/wiki/Thin_film). &gt;==== &gt;[**Image from article**](http://i.imgur.com/MYA2lT7.png) [^(i)](http://commons.wikimedia.org/wiki/File:Vierleitermessung.svg) --- ^Interesting: [^Node ^\(circuits)](http://en.wikipedia.org/wiki/Node_\(circuits\)) ^| [^Remote ^sensing](http://en.wikipedia.org/wiki/Remote_sensing) ^| [^Electrode ^array](http://en.wikipedia.org/wiki/Electrode_array) ^| [^Resistance ^thermometer](http://en.wikipedia.org/wiki/Resistance_thermometer) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cft8jcv) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cft8jcv)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Don't forget to mention if you're going to the Americas summit (Austin, March 3-5) or the European summit (Geneva, March 18-20).
The CLA summit is the biggest thing that makes me want to get the certifications. Every year it seems like the most intense and interesting things get discussed there.
It's way worth it. Like others at the CLA, I prefer it to NI Week. Even if the lectures don't cover content you deal with daily (tons of Actor Framework topics this time and I don't really use it), the little tidbits of information you pick-up from questions and off-hand remarks can be amazingly insightful.
Ugghhhh the actor framework.... So good and so bad at the same time. I've given up on for the time being. The actor model is great but the actor framework and the command pattern in general is too tightly coupled for my liking 
Ignore me. With some more googleing I have stumbled on another way to copy things. Hold down ctrl+left click and drag.
I haven't looked at it in quite a while, but I thought one of the examples showed how to do some loose coupling with it? That might have been in the version before it was released with LabVIEW.
a few tips: * ctrl+shift+drag will copy the object at the same horizontal or vertical location, depending on which way you initially drag. * i honestly don't copy and paste much using ctrl+c and then ctrl+v, but it seems as though it's disconnecting the method from the front panel object. if you ever want to do this manually, you can do so by right-clicking on the method tied to a front panel object and there should be an option there (forgot its exact name) to do so. * the method you have on the right will still work, but you'd just have to pass it a reference to the sub-panel. right-click on the sub-panel's block diagram terminal to create such a reference. this is the method you would want to use if you wanted to bundle some of your code into subVIs, as you would need to pass the sub-panel's reference to that subVI since the method on the left won't work in subVIs.
Thanks for the tip. I'll have to dig around and see if i can find it. I'm not a CLA so i doubt i have access to that forum. I could go on and on about the pro/cons of LVOOP. I use it for some things and its great and for other things its SOOO heavy, cumbersome and a pain. For my actor model type stuff i'm actually using Daklu's (Dave?) MHL model and using his lapdog messaging system to pass messages around. 
Ha, I believe Dave had a presentation yesterday on it. Went on a rant about QMHs and discussed his model.
I think LabVIEW would be the wrong tool for this task. Most of the 3D tools LabVIEW has are for graphs, not for general drawing.
Where you ever able to fix it? 
Bear in mind in the end it is the NI guy's job to sell you a product. I've had great experiences with them whenever we've spoken but it is always important to remember that's what it comes down to in the end.
why do you have to do it in labVIEW, and who told you it was possible? there is the 3d picture control with an api built on top of openGL. however, this api is all you get and lower level openGL calls are not possible. this means that if midway development, you find out the api doesn't provide something you need, you'll be up a creek without a paddle. i've messed around with the 3d picture control from time to time, and my initial suspicion is that the interaction you describe would be very difficult if not impossible to do with the exposed api. the only built in manipulation i am aware of is the ability to move the camera. my first guess of moving objects with the mouse would require hacks regarding projections and translation of coordinates between the front panel coordinates and the 3d scene. hopefully, someone with more experience with the 3d picture control can comment.
Awesome, thank you! I'll look into this more. The [Lego Building Academy] (https://www.buildwithchrome.com/buildacademy#) is almost exactly what I'd like to replicated, just in the constrains of what LabView can do and fewer choices (the link is something I think you have to download to interact with, but you might be able to see at least what it does, otherwise, it's not that important). My original plan was to find a way to design a build outside of LabVIEW and bring it in but they keep saying no to it! 
Ok, in response to this and the post below, this wouldn't require any "hacks" to get working. If you use an event-driven state machine, the 3-d drawing tools, and some events this is very possible. The NI guy isn't lying to you. If I were to design this: Figure out how to create and manipulate a single object. -Rotate X -Rotate Y -Rotate Z? (Probably overkill, and could greatly complicate things) -Scale object to simulate depth in frame. -Move object vertically -Move object horizontally Figure out how to create multiple objects and add them to the canvas. -Figure out how to detect object collisions. -Figure out how to switch between active objects. From there, the rest is just UI and control design. I personally think this is a pretty cool idea. Just break the problem in to smaller pieces like above. If senior design at your school like it is at mine, 90% of what they're looking for is that you're able to break a problem up and make incremental progress.
Thank you again!! I think this is exactly what I need. Because it is a senior design project, it's not the end of the world if it doesn't work out perfect, but I really want it to. Most of what I need is just a path to move forward confidently. I really appreciate the time you've taken to think this through! 
hey there. so i decided to open up labview and see for myself. there are three methods for the 3d picture control that will be very useful. they are: * Pick Model * Pick Point * WindowToObjectCoords these were exactly what i was thinking of when i mentioned "projections and translation of coordinates between the front panel coordinates and the 3d scene", so luckily enough these are included in the api. these methods, combined with the user event structure and the ability to manipulate the camera, should allow you to be able to determine which object the user has clicked on and is dragging across the screen. this will still be a challenge, particularly for a student project, at least in my opinion, but it seems doable now. without these methods you would have to be doing all that manually, which was my concern. my worry is not unwarranted, as this is a very typical problem in labview, that an included api provides what you need initially, then you find out halfway through it doesn't do what you thought, doesn't work correctly, or simply doesn't provide what you need. good luck!
This is fantastic help! I am currently trying to run through the training for LabVIEW I've been provided, It seems pretty helpful and I will look for these topics specifically. It's exactly like you're saying though, I don't want to have put all my time and energy into making this work only to find out it just doesn't/can't. Especially when I wasn't able to find anything like it online to give me an approach or even a little hope. Thanks again, yesterday I felt totally lost, today I feel like I've got multiple things to explore to help get me there! 
yea, no problem! i would be very surprised if the training you've been given mentions anything about the 3d picture control. however, paying attention to the user event structure and the producer/consumer design pattern in particular will help you out quite a bit.
The NI LabVIEW forums have knowledgeable people offering answers to questions and issues. Have you tried posting there?
If you are a student in the US, you have free support with NI. Log into ni.com with your .edu email address. also, it looks like youre going to have to basically write your own labview driver wrapper for the device using their DLL on the bottom of this page: http://www.multichannelsystems.com/products/stimulation Its not an ideal kind of project for someone who isnt very familiar with programming, and will definitely teach you why you should use things that have awesome drivers. 
not doomed by any stretch, just have to work hard and use the resources available :) Trust me, its way better than the tons of devices that have zero driver documentation, or even worse: No driver at all. 
You can look at using a .NET constructor node to utilize their driver http://download.multichannelsystems.com/download_data/software/McsNetUsb/McsUsbNet_3.1.20.zip It was written for C# so you should be able to use an example like this https://decibel.ni.com/content/docs/DOC-5921 PM me if you want some direct help. 
The have actual NI people there to serve as a sort of public tech support system. Googling answers 99% of my issues, since the forums are so so active and have been going for years. 
Something you might want to try is LabVIEW's [Instrument Driver Wizard](http://www.ni.com/white-paper/3271/en/). Scroll down to Step 2. I'm taking a look at it, I downloaded the DLL for the STG2008, I'll see what I can make happen. 
It's difficult to debug an entire program with just a few images, especially when you don't seem to be using a standard architecture for your program. Those menu primitives you're using to create the run time menu may be making your life more difficult than it needs to be, Check under the edit pulldown to see if there's an option for Run Time Menu. I'm not sure if that option dates back to version 8.6. If it's there, create your custom run time menu using that. So much easier. Those primitives can be useful if you want to change the menus dynamically, but there's probably no need to get too cute about it. I don't understand that Setup subVI. What is it doing? Are you just reading from it? Loading data from a file? Are you saving data to a file? Do you store the path in that subVI, and try saving to it later? If you use it to save data, do you have to wire in the path each instance?
Is this associated with Cal-Bay? What is it, exactly? It doesn't seem like a company; more like a place that lists freelancers.
Hi Mike, The person responsible for managing VIResource.com and infrastructure is the MD for Cal-Bay Europe Ltd. The purpose of VIResource is really to try and pool together the best talent (in terms of LabVIEW, TestStand developers and project managers) and to provide them with work which will be provided to them by VIresource.com. VIResource.com can provide work for experienced developers who want to develop high quality solutions for our customers. If you are at all interested I would highly recommend sending in your contact details as we are keen to expand this fantastic organisation.
to be completely honest, i would recommend starting over from scratch. debugging that code is going to be a nightmare, even for experienced users. it's breaking almost every best practice there is in labview. * i'm confused why you don't understand that error. it's clearly saying a file wasn't found. you're either trying to read from or write to a file, but labview can't find the file because it doesn't exist. find where you're calling the open file VI (there are multiple ways to do this) and determine what file doesn't exist. it's likely in one of your setup VIs. * if you're at a univeristy, you should check if your university has a newer version of labview that isn't 8 years old. * [before you start over, research labview design patterns.](http://www.ni.com/white-paper/7605/en/). keep your application in mind while reading through these, and pick the one that fits your needs and use case the best. then, be extremely diligent in breaking your problem down into small components BEFORE writing any code. VIs are functions just like in text-based languages. if you're a hardware guy, they are like integrated chips. the primitives of labview (the pieces on the palettes) are similar to individual components like resistors, capacitors, etc., and the VIs are integrated chips that perform certain tasks, possibly in multiple places (i.e., they're reusable and encapsulate behavior). * your code should not have wires going in every direction, and you should be mindful of dataflow. the way you have two event structures inside while loops connected with multiple wires is a huge red flag and an absolute do not do, and so is the stacked sequence structure that contains these. it's basically impossible to understand what direction the data is flowing. you want to have a left to right flow on your block diagrams. again, i cannot emphasize enough to read through the labview design patterns. * you should be putting all your code into a labview project, which should exist for labview 8.6. * [Top 5 LabVIEW Rookie Mistakes](http://www.ni.com/newsletter/51735/en/) * search for "labview best practices" and "labview guidelines" on google. once you've done all that, which should take a day or two of reading, and you have taken a step back from labview and evaluated the components of what you want the code to do, you'll be in much better shape. sketch things out into small, reusable components. there is a lot of repetition on your front panel. if that is indeed needed, then that should be a clue that you should have a lot of common VIs that are reused on your block diagram. if you simply describe on paper like you would a flow diagram or circuit diagram before ever coding, it will make your life much easier when you code. because then, you take the design pattern that meets your project's needs and simply implement your sketched design/solution into the labview design pattern.
no problem, and good luck! i would highly recommend posting in the NI Forums, as there are a lot of people there willing to help. i am very busy at the moment, and so i can't guarantee i'll be available for help.
I can think of a couple of ways to do this. First, a "run-time menu" in LabVIEW is the toolbar at the top of applications (the one with "File", "Edit", "Tools", etc). What it sounds like you want is a configuration screen (usually a floating or dialog window). Two ways of doing it: 1) Create a typedef cluster containing inputs for all the configuration settings you need. Put this cluster into an array (or just put X number of clusters where X is the number of channels to configure). Have your "Apply" or "Ok" button greyed out by default. Use an event structure and the "Value Change" event to validate the input and make sure that every channel has configuration input. If validated, ungrey the Ok/Apply button. 2) Put your knobs down on a front panel and put a string indicator at the top that represents what channel is being configured. Use a state machine with a shift register containing the channel number. Everytime the user hits ok/apply, check to see if you've hit the max number of channels, otherwise go back to the user input state and update the string indicator to the current channel being configured. Store all configuration in an array in a shift register so you can keep building up your configuration array. Once every channel is configured, go to the shutdown states and output the array of configuration to your main UI. Not sure what your LabVIEW expertise is, if you need further help let me know.
First of all, don't close the reference to CMcsUsbListNet right after you call it. Second - did you try to use the device with some other software? Did you verify that it works?
You aren't calling usblist.Initialize which should be an invoke node off the CMcsUsbListNet constructor Are you getting any errors when you run your code?
What youre looking for is an [NI alliance partner](http://www.ni.com/alliance/). They're companies which provide integration assistance anywhere from a bit of labview consulting/programming all the way up to massively complex turnkey solutions. As for your exact system, its hard to say because it could be incredibly simple, or horribly complicated depending on what is there currently, and how complex the tests really are. Id ping an alliance partner to have them check it out. 
Yes, running the wire through is fine. Not sure if that will fix your issue or not. It really might be nice to have example code for using CStg200xDownloadNet. I would definitely look to see if there's a constructor for it available.
Well the company is setup half hardware guys and half software guys. I like the idea of the same people developing the whole package. But there are communication problems and they seem to not understand what we want or sometimes even what there code is doing. They are a fairly large international company so I thought maybe there was an off chance someone had worked with them. The hardest thing for me to under stand is there coding structure its a mess of stacked sequences and messy wires and gigantic clusters that aren't documented. 
Why do you work with NI and LabVIEW over other development languages and hardware?
Not going to give a specific number, but I make 20-30% more than any of my non-doctor friends.
Coolest project you've worked on, and coolest you've built for yourself? Least favorite product to work worth?
Favorite design pattern? Favorite network communication API?
What is your stand on Global Variables. Do you use them to share information between VIs or do you pipe everything? 
you avoid named queues 99% of the time, even on UIs?
yup. You don't need to name the Q, just wire the reference to anybody who needs it. Why would you want to name a Q on a UI? I don't understand where your question is coming from
&gt; i avoid all of these 99% of the time is where my question came from. named queues can be very useful: 1) in complex UIs, it is sometimes nice to simply grab a reference to the queue by name rather than pass the wire around, needlessly cluttering up the diagram since it's a reference anyway. 2) naming the queue helps when using the desktop execution trace toolkit, as queue names appear there. this is very useful for debugging and simply profiling a complex application. 3) named queues are useful in situations other than just UIs as well. for example, i have seen the names used as unique IDs for computation streams. the IDs, i.e. queue names, essentially acted as terminal IDs for connecting the different computations, almost just like how labview connects its dataflow wires to different terminals. they can of course be dangerous. for example, single-element queues are one way to create singleton objects in labview. if the queue was named, code outside the singleton class/object could access the queue reference if they guessed/knew the name. but even in this case, a singleton object is a global variable, but is controlled, similar to a functional global. globals in general are terrible ideas. but functional globals and singleton objects are extremely useful in certain circumstances.
When actor framework came out I tried it and I thought it was too hard to debug. What do you think? I feel like object oriented programming creates too many files that I have to spend managing. What do you think?
NI gives us lots of tools to debug and optimise our code, do you use any of them and if so which ones? What sort of industries do you tend to focus on or get more work in? Any tips for how to get industries or companies aware and interested in your services? 
I'm still very new to LabView, but I enjoy working with it so far and looking forward to gaining proficiency. I do a lot of bench testing and am looking to consolidate and automate most of my processes, as well as general simulation for building prototypes. Since I don't have any good questions, and you probably fix a lot of other peoples mistakes, would you care to simply offer a piece of insightful advice to a novice?
What's your experience in trying to make tests run in parallel? I'm in the middle of a huge project: test station, ~15 different bits of hardware to interface with, ~200 VIs so far and I'm at best half done. All the hardware has yet to arrive so live testing isn't possible yet. It's a laser system and part of the test involves the beam being split 5 different ways, each leading to a different instrument. In order to speed up test times it would be great to do as many tests at once, but as I write the framework for all of those tests to try and do them at the same time, it's getting really involved. Like, I have 4 different queues processing at the same time, 5 different functional global variable VIs, and a global variable file with the information not in danger of race conditions (because I've been able to restrict it so only one VI writes to any given variable in it). However, this test station is replacing an older one that runs way too slow, so fast testing is the main objective, and parallelism is probably the only way I can meet the test time goal.
99% of my debugging is done via highlight execution and probes. Optimizing is done with the profile memory tool and Sequence structures to time sections of code. Industries I work in: Medical, Oil and Gas, DoD. All of my references come from my local NI Sales rep. They work on getting the sale of the hardware, they they say "call this guy for help with the software"
You should seriously consider Actor Framework for this. You would write it once, debug with one system, and then when everything is nailed down, spawn 5 actors. Even if you don't use actors you should really figure out a way to get rid of the 5 copies of the same code. Use arrays or whatever, but you're going down a road that will lead to maintenance nightmares. What happens when you need to add a 6th station? (I'm guessing this would involve copying and pasting a lot of code) what happens when you need to change the logic of the stations? (I'm guessing this means making sure you make the change 5 times). You should be encapsulating each station, when program starts, it just decides I have 5 stations, I need to spawn 5 versions of this Station code. You should use Objects to build a hardware abstraction layer for your hardware. You'll have a parent class with methods like "turn on laser" or "measure value" . Inherit from that class with a Hardware Simulation class and build some Sim code for now to test your core logic (maybe it does nothing when for the turn on laser function, but spits out a random value for the measure value function). Then when your hardware finally comes in, build a Real Hardware class that inherits from hardware that actually implements teh code to talk to the hardware. Now you can test your hardware completely separately from the rest of your code (and you can test your code without hardware). Lookup "hardware abstraction layer in labview" and you should find a lot of good examples. This is one of the reason Objects were added to LabVIEW and is a great way start learning OO.
I'm actually a huge fan of Network Streams. I use it in any project where I need 1 computer to talk to 1 cRIO. The only bug I've ever found was in 2011 when I'd change the data type for the stream endpoint. If the stream reference was in a shift register on a while loop, the register would become corrupt and I'd have to re-write my VI. I haven't seen that problem in 2012/13 though. Other than that, the only other oddity is that enumerations need to be converted to strings or integers.
Thank you, and I value your input. As a student, we used the text "Hands on Introduction to LabView for scientists and engineers" by John Essicks. I found the text useful for helping me gain interest, but it left me with a lot of questions. I like the class recommendations, they sound comprehensive and there are some in my area. I do a lot of research work, so I think proficiency in LabView will be very valuable.
Students get a huge discount on NI courses. It will still cost you a few hundred bucks, but it would give you some real world knowledge. If you got your CLAD and CLD, then you'd have some really good stuff to put on your resume.
I like network streams better than Shared variables. Little problems like what you saw are reason why I don't like these NI supplied libraries. They are supposed to be easy to use and battle tested, but they always seem to break for no reason, and it requires you to do a lot of in depth research or make "pointless" changes to your code to fix them.
Thank you for letting me know about the student rates, as it greatly increases my motivation to follow through with it. I'll look forward to hopefully enrolling this summer. This is exactly the type of info I didn't know I needed! Much appreciated.
Have you looked into TestStand? It makes parallel execution somewhat more structured and fairly easy to debug.
USB connection? Tried a different port?
I've had issues with using the actor framework in the past and present. Mostly that LabVIEW sometime crashes when executing messages. I get invalid memory access errors. Have you seen this before, and if so, how do you handle it? I've switched between my messages being reentrant and not which has cured this in the past, but I'm sure you can imagine it messes up other projects. Also, how do you handle your UIs with AF? I've used sub panels in the past to make replaceable actor segments and just make one actor the main UI. Another AF question (sorry so many questions, I don't come across many other labview developers) do you use a hub design for your actors with a central actor to collect and pass messages, or do you just pass around queue refs to each actor as necessary? More AF question. How do you keep your actor independent of each other? I find the AF to be tightly coupled unless you use a call by object by name. Last AF question. I don't know if you've noticed, but NI has had troubles with supporting the AF. Has this affected your work and do you think AF might be a slowly sinking ship, especially since many LV developers are not able to get it Oh and because why not how about this stumper. In C, a sting occupies memory in consecutive blocks and is terminated with a null byte. How does LV store strings in memory?
What exactly is the problem? does the Init block not let you select the the "COMx" port your arduino was assigned to by your computer. Was your arduino board even assigned a "COMx" port in the device manger? While keeping the device manager window open; when you open labview does the "COMx" port for your arduino go away? Is this how your setup looks like? https://www.youtube.com/watch?v=RGRhIQneO6w
Use VISA Interactive Control to make sure that your computer sees your device correctly. Is your board a Mega or Uno? You have to wire that into the Arduino Init VI.
Sign me up, spaghetti code is my specialty!
Whoa. That was awesome. Thank you for taking the time to go though my laundry list of questions. I hope the AF continues to grow. I like using it and I feel it really embodies the concepts of modular design, but I just get frustrated. Then those frustrations turn into doubt about choosing to use the AF for a given project. I still have a lot of thinking to do on my actor coupling issues. Although my actors are single job, it's the 2-way messaging where I find my actors become dependent. As for the stumper I put at the end, that was an incredible answer. I'm a CPI, and I'm not saying that the string question is a big part of the lessons, but in Connectivity, there is a single slide that shows the difference in how LabVIEW stores strings in memory vs C vs Pascal. You nailed it. Also, going back to the state machine as a sinking ship. I can get behind that. Whenever I teach a Core I&amp;II I always tease the students with the prospect of the P/C design pattern. That they'll find more use with a P/C than a state machine. Again, thank you for the epic answer.
Sounds like grumpy USB ports. It shows up fine in VISA?
&gt; I still have a lot of thinking to do on my actor coupling issues. Although my actors are single job, it's the 2-way messaging where I find my actors become dependent. Abstract messages are how you solve this. Basically you separate the send (and message data) from the Do part (this is where the dependency comes in). The actor only knows about the send part, and at runtime you figure out what specific class to use. Again, there aren't good example online of this, the evaporative cooler uses them, but it's how you do it.
Hi, I hope I'm not too late! I'm looking to reduce the memory space requirements of my VI to as low as possible. Currently I have the space down to that required to store the input and output arrays. I think it could be reduced further to the size of a single array. I was hoping I could take the input array, perform my function on the first row, and then store it back in the input array. This would repeat for all rows. Once the rows are completed, it would be performed on all the columns in a similar fashion. The problem I've encountered is that the In-place structure offered by labview doesn't support working on a row or column, it only supports single elements. Also, it looks like labview will always keep a buffer of the input array stored and not allow that memory space to be overwritten. Thanks if you're able to offer any assistance!
I'll admit, I have never had to worry about reducing memory footprint, so I'm not too confident I'll be able to help. I would have gone with the in place element structure, but you're right. it will not give you just rows/columns. I think I would go with an array index and replace array subset. Like this: http://i.imgur.com/Ec7vJqR.png Replace the +5 with whatever your processing algorithm is. I believe you're correct about the input buffer. I don't think it's impossible to overwrite it. Are you running out of memory? Why do you need to worry about this that much?
Thanks very much for your reply! It's for an assignment I have to do. I had to create a VI that could implement this function, then create a second which optimises for speed, then a third which optimises for memory. I've had good success with optimising for speed, however it was annoying me that theoretically I should be able to reduce the memory cost further by operating on the input array in-place but unfortunately it seems LabVIEW won't allow it. I had tried your suggestion of indexing followed by an array subset replacement but the total cost is still basically the cost of the input and output arrays.
This is awesome.
My boss just gave me a program that was created in labVIEW and told me to figure out how to edit it.. I don't do any sort of programming. Suggestions?
Thanks
Uh, your first two questions seem a bit on the creepy side (or is it just me?), so I'll just say I work for a systems integrator in North Carolina. As far as what I do with LabVIEW, it's a lot of things. Our company has two groups one focusing on projects involving automated test equipment systems (usually NI TestStand-based), and one dealing with control and measurement systems. I'm in the latter group, and the projects we handle vary widely. Some are Windows-based, others are RT/FPGA-based. Most have at least one (usually more) axes of motion, primarily servos. I'm the senior engineer in the company (besides my two bosses) so I do the bulk of the software architecture design (in addition to the vast majority of the electrical design) in our group. I do everything from customizing low-level instrumentation and power supply drivers to creating custom UIs, and everything in between. I've created a lot of control/interface modules for servo drives &amp; controllers (primarily Kollmorgen and Yaskawa), implemented various communication protocol interfaces (various serial comms., MODBUS, TCP/IP, EthernetIP, CAN, CANOpen, GPIB, etc.), developed custom PID control algorithms for projects, done a few FPGA applications, a few headless RT control projects, and on and on and on. I've been doing this for almost seven years, so I've done at least a little bit of everything with LV. One things I was getting quite a bit of for a while was upgrading or modifying someone else's very old (think LV 5.1.1) and very poorly written code. It's always easy to knock code another developer wrote, and I'm sure there were some applications that I just didn't bother thinking about long enough to figure out why something was done a certain way. But, most of the older stuff I've had to modify has been complete crap and I've just wanted to re-write it from scratch. I despise those jobs and fortunately I haven't had one for a long time. I've been working almost exclusively on the same massive project for nearly two years now. Finally make things move around on this one is very satisfying. Are you trying to figure out what you want to do?
Thanks for the reply! Not trying to be creepy, but I see how it could come across that way. My motivation is purely to figure out what companies and what cities would be good for finding a job doing what I like.
I missed the question at the end. I know what I want to do, in that I like working on embedded applications and I have a knack for LabVIEW. I have my CLD and will be starting my 3rd internship programming in LabVIEW this summer, so I feel confident about what I'm interested in. The problem is that I'm definitely in the wrong place geographically. I'm just trying to get a feel for what's out there, and where it is.
I'm in Virginia. My company is probably atypical for a LabVIEW shop in that we're not using any National Instruments hardware. Instead, we have our own custom hardware, FPGA programmed in VHDL instead of LabVIEW, and our software has a LabVIEW front end that calls into our C++-based DLLs. Our GUI is completely LabVIEW, and we have a LabVIEW SDK because a lot of our customers use LabVIEW. We also do a lot of algorithm development in LabVIEW because it's easy to do some math, throw it on a graph, run, tweak things, run again. Ultimately most of that gets rewritten into C++, but for doing the initial algorithm work LabVIEW is great.
Right now, you are running your joystick code in one loop and your motor in another. The motor loop is waiting on the joystick loop to finish, so it never runs until you hit stop on the first loop. You can: Move all of the code in to the same loop. Use parallel loops with your preferred method of passing data between them. If you're new to LabVIEW, start with the first approach. 
In addition to what tttanner wrote, it also looks like you're wiring a boolean into either a cluster input, which is causing that broken wire. It's hard to tell what should go there from just a screenshot though. 