&gt; Actor Framework/LVOOP is literally jesus Actor Framework/LVOOP *really is* Jesus\*. I came across AF a month or so ago and within a few days I was adopting it for my own code because it's just such an awesome thing. It was pretty much a godsend at the time because before then, what I was doing with the top-level VI for each application was creating a state machine - and that's great for handling sequential logic that can be segregated into different states, but what happens if you want to run code in parallel? I went about it by creating another state machine for that in the same VI, and then another state machine for something else that ran in parallel...the code became bloated, incohesive, inflexible... With AF, I could think about all of those parallel logic machines as being *tasks* performed by *actors* which would go about doing their own work concurrently and communicating with each other as needed. It's cohesive - each actor handles one task, and only one task. It's flexible - I can swap out different implementations of actors easily so long as they expose the same interface to other actors. It keeps VIs small because instead of having a bunch of parallel loops in the top-level VI, I can just have a "controller" actor that launches a bunch of nested actors (I assume - I haven't really completed a whole MVC-style application with this new AF approach yet, mainly just smaller projects that are designed to be reused in larger ones). \*All of this at least conceptually. That's the thing about LabVIEW - the *idea* of it and how it all works conceptually is quite nice (barring a few things...lack of generics, invariant return types, functions (VIs) aren't first-class citizens (but they aren't in Java either :\\ ), lack of multiple inheritance like you said (which would be nice for "interface" classes, like in Java) but **Holy Christ does the development environment itself suck**. &gt; ...because of the bulkiness of it... This is it. It's so goddamn bulky and slow to get things done with an OOP approach in LabVIEW relative to OOP in decent programming environments. For instance, let's compare creating class getters/setters in LabVIEW vs. other languages: **LabVIEW** Takes 30-60 seconds per accessor method (maybe longer - I need to time it): 1) Right-click the class. 2) Click "Create VI for data member access". 3) Wait for the sluggish window to appear that let's you choose which class data member and what access type (read/write/read-write). 4) Wait for it to generate the VI. 5) The error in control/error out indicator are both in modern style, whereas I use silver style, so: 5a) Right-click on edge of each. 5b) "Replace...Silver style...Arrays, Clusters, blah...Error in/out". 6) Spend time properly aligning all the front panel objects. 7) Open up the block diagram. Spend a few moments fixing up potential issues here. 8) Open VI icon editing window. **Just opening this window takes goddamn 8-10 seconds alone. I could literally boot up my home desktop in the time that it takes to perform this step a couple times**. 9) Add text to icon to show what it is you're getting or setting. 10) Open VI properties window (a lot quicker thank fuck) and add a short documentation string saying "Returns blah" or "Sets blah". 11) Save the VI, giving it an appropriate name **Python** Takes a few seconds. def getFoo(self): """ Returns foo """ return self.foo **Java** Takes a few seconds. /** * Defensively returns foo. */ public Foo getFoo() { return this.foo.clone(); } **Groovy** 0 seconds; you're already done (Groovy implicitly generates getters/setters for any properties you declare in a Groovy class).
&gt; I'd start by removing classes a bit at a time and retrying the build to see if it works. I'll try that when the time comes. Right now I'm going to carry on as normal. Then at the end, when it's all been built, debugged and tested (LVOOP is way too painful for me to bother with a TDD approach, unfortunately), I'll show it to my boss and be like, "Hey, everything's working exactly how you'd like...but I can't distribute it yet, give me another week". *Then* I'll try to decompose the system into separate modules which can be compiled separately and see if that helps. &gt; My guess is you've either got a corrupted project, which I've seen happen or a corrupted library somewhere. Any idea how this sort of stuff happens? I actually try building the packed library out of paranoia now because when I tried doing it a week or so ago, it kept complaining about this particular VI. I opened up that VI and discovered that both it and another VI had missing block diagrams. I reverted back to the earliest commit (in Git) that had those VIs and even then they were still missing their block diagrams, so this must have happened really soon after I made them. I have no idea how that happened and fortunately most of my VIs are really small and individually would only take minutes to recreate. If I'd spaghetti-coded half the application logic into a single VI and *that* had gone corrupt, I'd have shot myself. Any idea what might cause a VI's block diagram to magically disappear? I didn't deliberately do anything different with these two VIs than any of the others I made. &gt; You are keeping your classes in library's, right? Yeah. For packed library distributions, all the code you write has to be under some library, and in addition I nest libraries inside when a bunch of classes are closely related enough, or a set of VIs are related enough but inappropriate for being combined inside a class.
Hmm, this is definitely possible, but IIRC there are settings you have to configure in either MAX or through the use of property nodes to make it work. I'm home ill and haven't had coffee yet, but I'll see what I can find. I think you have to export the sample clock from the 6320, should be a setting under Advanced Timing, but I can't remember if you have to create a channel on the 6320 or set the property on the channel you're trying to use on another device. Not sure how to connect this to the timing input on the loop in question, let me do some digging.
https://www.reddit.com/r/LabVIEW/comments/3939jy/postlive_cla_practice_exam/
Great post! I think Fab's comment on there hit the nail on the head. &gt; I think you are correct, the issues described by others with regards to QMH have more to do with trying to use them as state machines rather than more like an execution engine.
&gt; Yeah I do something similar and it really bothers me that NI don't provide an implementation of Get Tick Count that have error in/error out terminals so you can get the desired data flow without needing to use flat sequence structures. http://forums.ni.com/t5/LabVIEW-Idea-Exchange/Wait-ms-with-error-pass-through/idi-p/1177437 It was declined on the ideer exchange... I think the openG lib has this though- take a look in VIPM 
Thanks for thinking of me, but there's simply no way I could relocate to LA. It's a damn shame, too, because from the sounds of it I'd really love the job.
Many languages could be used. There is an Instrument Driver available for the E5062A (http://sine.ni.com/apps/utf8/niid_web_display.model_page?p_model_id=6764), so that will help you with getting started. Where should you start? Independent of LabVIEW (or another language if you so desire), I would start with asking yourself what the program is supposed to do? You say you want to automate a set of actions. List out those actions. For example: Step 1: Press Button A Step 2: Set Knob B Step 3: Read value X Step 4: Set Knob C Step 5: Read Value Y Now ask yourself, how much do the above steps vary? Is Knob C always set to the same value in step 4 or does it sometimes change? How do you want to store the information? Are there different sets of steps that the technician performs? Or is there one recipe that the technician always follows or several recipes that they can choose from? Now that you know how you want the program to work, ask yourself how to do that as it related to the instrument interface code (e.g. the functions available via the Instrument Driver). Next, choose a language that supports your hardware (LabVIEW should support all of this quite easily). From a LabVIEW perspective, you can look at quite a few getting started tutorials on the NI website. If your company has funding available, you could hire a LabVIEW Consultant to write the program, or just to help you get started and review your code as you start learning LabVIEW. You can find LabVIEW Consultants on the NI website at http://www.ni.com/alliance/ . I am one such consultant. Feel free to PM me.
&gt;When using a QMH you cannot guarantee that execution will happen in the order that you expect, on the data you expect. I've been using some QMH-like frameworks that have some 'wrappers' around the standard queue vi's, the first one involved DVRing the q, the second actually had a q of type q, which was only one element (I was told this is actually an older idea). If you add a wrapper like this, I think it's a lot easier to manage exec sequencing and whatnot... 
Here is the obligatory links to free training, its at the end. https://decibel.ni.com/content/docs/DOC-40451
Make the output of your squarewave generator latch off when a falling edge is detected. Put that logic in a case determined by your "soft enable". Then you can solve your squarewave generation problem in a more direct fashion.
Try StarUML http://staruml.io/ It's a free program like Visio in a way but geared towards software architecture. You'll have to learn UML to make any sense of it, but it helps keep our projects in order.
I'll take a look. I've been working with XML a lot lately so this may dovetail nicely with that. Although I have to admit, I know zilch about UML. Thanks for the suggestion.
You need to import your array into NumPy from the csv. Usually with something like this import numpy y = genfromtxt('my_file.csv', delimiter=',') rms = numpy.sqrt(sum(y*y)/len(y)) 
Ah thanks! I hadn't thought that Python would have a TDMS package. I've been trying in Scilab.
The "read TDMS" block has two terminals on its too edge labeled "offset" and "count." They control where reading begins and how much is read. I use this to work with my large data files. 
Can't you open the TDSM file in LabVIEW and process it with another VI? This works for me http://i.imgur.com/R1fRoFz.png
This is important. I work with files that have hundreds of millions of data points and I do my processing in batches of 100,000. It makes the process take orders of magnitude less time.
Don't laugh, but seriously: a whiteboard. Work top down. Once all the modules and their messages are identified, take a picture, then create a text document for each module that describes the behavior of that module and how it communicates to the other modules. Erase the whiteboard and draw the state diagram for each module (and take a picture). Granted, you can't edit images easily, but it's a hell of a lot faster to get the first iteration worked out with a marker than it is in software (in my experience). If you have the budget, transfer you diagrams to Visio (or MSPaint or whatever floats your boat). Edit: a 4' x 8' piece of white panel board works as an inexpensive alternative to a whiteboard. 
I hear you, but I don't think it's a problem of not sticking to a process as much as it is not having one defined and not having *any* tools that work for me.
It's quite obvious that the original post is 4 years old and that the author wasn't an experienced LabVIEW programmer. A lot of his complaints have solutions that either already existed (as pointed out by readers) or are handled by things like quick drop or setting the option to load pallets in the background. A few of his points are valid. The alt-tab but and the huge number of windows, but I think everything else is overcome by experience and a solid software design before you start implementing. You can always make space with a subVI and use clusters and OOP to clean up wires. He probably got a poor implementation to start with, and without investing time in becoming a better programmer he's not likely to make it much better. 
I think that he should have used the time he spent writing that to take Core 1 and 2.
Awesome thanks for your help! Definitely something to look at!!!
You're talking about a time-based (vs. size-based) rolling average. Check out [this community post](https://decibel.ni.com/content/docs/DOC-5041) and the comments. It's an example of a size-based rolling average. Shouldn't be too much effort to convert it to time-based, unless you take the point-by-point approach mentioned in the comments.
Push the data into a shift register as you pull it from the DAQ. Every second push the data from the shift register to a calculation loop and reset your shift register to zero and keep gathering data. The calc loop averages your dataset and updates the UI/graph/array. You likely need 2 loops to keep gathering your data and passing to your calculation loop running asynchronously. 
Your chars will map to strings in LV. You may need to write individual controls to the dll library function and keep reading your error output line until you feed the correct data into it. Verify you do not need control characters or anything of that sort in your string. 
Convert the setpoint from dynamic data to a single scalar and bundle it as a cluster.
Is labview decent at data processing? Is it easy enough to code? 
It sounds like you have an architecture in mind, and you're trying to justify to me how the features you have in mind fit into the architecture. In doing so, I've lost track of what you're actually trying to accomplish. So, take a step back. Can you describe to me exactly what your app needs to do (ignoring architecture and such). Tell me how you envision the user interacting with it, what it needs to display to them, and any other outputs it needs (file, DAQ outputs, etc). From there I can help you figure out an architecture that is probably simpler than you nested-state-machines-sending-data-to-producer-loops-which-consume-it-before-sending-it-to-consumer-loops
I'm not really trying to justify as much as figure out if it'll work, I'm definitely open to switching paths if this won't work or there's a better option. But I didn't know of any more promising structures and I needed to start building it to find the issues with it. But a summary: I'm building a program to acquire data, monitor (plot), analyze, and log. It will be built as an EXE and operated by people with no knowledge of LabVIEW. Simple enough but all four tasks need to be changeable on the fly (without stopping and restarting the program entirely). The acquisition and monitor/analyze/log functions need to be relatively independent, as in be able to change the DAQ setup without stopping the program and change the analysis without stopping DAQ. What I'm working on right now is much simpler than how you're describing it, which tells me that I described it poorly. I have 3 loops: an event loop, a producer loop, and a consumer loop. The only complexity is in using two queues. One dequeues in the producer and the other dequeues in the consumer. So the event loop can choose which one to send commands to based on the triggered event. This is so the two parts of the program can operate very much independently. And of course the producer loop also adds data to the queue that dequeues in the consumer loop. Is that any clearer?
Well it depends on the power output of a panel. For the large ones, you will voltage divider in paralel to the output so you can measure very large voltages. You can measure the current by putting very small resistance in series to the output. By measuring voltage on it, you can calculate the current. The power of the panel is just U*I
Do you have a schematic diagram for it ? 
I have. It is schematic for a 3x2 panels in series for my project. But it's all the same. This project also has measurements on the AC side, after inverter [caption](http://imgur.com/luLFtp3.jpg)
Ah, the multi-terminal shift register is the wrong tool to use if you want that many previous values. You'll need to pass an array of values around the loop instead. 50,000 values of double precision shouldn't be too crazy to pass around. Maybe there's a better way of doing it all together depending on the type of data you are trying to average. There's stats blocks in things like the modulation toolkit that might help or possibly an FFT followed by up with a few stats functions to detect center freq of a peak would help - all depends what your end goal is.
[And BBC News](http://www.bbc.co.uk/news/science-environment-33226496)
Link as in, put it on the sidebar or something? Not a bad idea, I suppose. I will always point people towards the NI forums if I'm at a loss trying to help them, and I know there are at least three or four NI employees that post here, but I get what you're saying about response time. That can be slow depending on which forum you post your question on over at NI, as well. I've got a couple that are still hanging out in limbo...
I'm one of those employees you're talking about. If you post over there, your question WILL get answered. I would say it's definitely one of the reasons that this sub isn't more active.
Check how much amperage your coil is pulling. The myDAQ can only source a maximum of 500mW across ALL outputs. If your coil is pulling more than the myDAQ can source, the line will droop.
Youll want a transistor to switch enough power to actually trigger the relay. Most relays need more juice to flip than a DIO line can provide, so you can have the DIO line trigger a transistor, which switches the 5V power supply on the myRIO over to the relay. 
Use this circuit diagram as a reference http://i.stack.imgur.com/NQGkI.gif Most relay bases have the option to add a flyback diode and/or "snubber" which is a flyback diode in parallel with a series RC to curb voltage transients. This is highly recommended to protect your transistor when the relay coil is de-energized. Tune your transistor gate input resistor to the current your myDAQ can provide per channel.
You need a transistor to switch the relay, the digital output can't source enough current to run the relay coil itself
[Thin Long Life Relay](http://www.mcmaster.com) Search for 8299K11 for AC 8299K12 for DC at McMaster-Carr. I use these all the time with digital lines on the DAQ cards. Never used a myDAQ, so I do not know if it would work with it or not. Apparently I suck at linking....
Alright, I've implemented the great suggestions I've been given and have come up with the [following vi](http://imgur.com/SGjTE2e). It works! My only other issue I could ask for help with would be plotting the data in excel. The write to spreadsheet vi now accepts 1d-data because I am reading single points. However, since I am trying to plot Data vs. Time, it ends up plotting [like this](http://imgur.com/3SyZNRy). Any suggestions? Is there a way to make the spreadsheet vi see my data as 2d?
I figured it out! I accidentally left the "Transpose?" option on the 'write to spreadsheet' vi as True, when it should have been false.
Updated with correct info.... 
Late to the thread but you might still benefit from this: - change your default control style to silver to eliminate step 5,6 - step 7 is not needed imho - Get the GOOP suite; tools-&gt;GOOP-&gt;create vi icon is a godsend - VI documentation is redundant to your icon text, imho and could be skipped - You can create all your getters and setters at once using control or shift to select multiple items The biggest pain point for me is you have to manually save each vi after it is created. Hope that helps! 
&gt; I'm assuming I'll have to deep dive into their code to determine what the paths should be so that the DLLs can be installed properly? Pretty much. They are looking for a DLL and can't find it, that's basically what's going wrong. Out of curiosity, are these 32 or 64-bit drivers? You need to use a 32-bit copy of LabView to run a 32-bit DLL (same thing goes with 64-bit). You have to explicitly download the 64-bit version of LabView so if you don't know what version you have you probably have the 32-bit version. &gt; Is there any way to make labview automatically pull the DLLs into it so that it can be installed on other machines? Kinda. When you build an exe you can supply a Data directory. I don't like referencing the data directory because you need to have your application reference the data directory correctly on your development system and that messes up my file structure and that's a pain. When you build an installer you can specify a program to run after installation, so you could write a batch file to move the file for you from the data directory to wherever but that's a pain. Personally I would just find out where the drivers are supposed to be installed and reference the absolute location. That tends to work best for my application but you mileage may very.
The system 32 folder is exactly where their installer puts the DLLs. Doing it manually wasnt my first choice, but running their installer didnt locate the file correctly either. 
Haha, or they think, "Y U no attach code??"
There are much more "fucks" and "idiots" than funny stuff :)
lol "whoever looks at these". I love you OP don't ever change. 
&gt;submit bug report submit curse reel FTFY
'tis a win7 err, I just hate win aero, or any other totally unnecessary UI purdyness...
Can you make these errors programmatically? Or are you a LabVIEW dev?
A sloppy implementation of actor framework with embedded front panels... the crashes are probably my own fault somehow...
The API may not have the ability to download the temp history but you can always set your LV code to read the current temperature in a loop and save the current reading into a database table. 
heres an mp3 reading pallette: https://decibel.ni.com/content/message/94413 then use the fft block or use the spectral analysis vis.
I also don't see much about that on the web, and there's not a driver on NI's IDNet. So in this case, you may be left to writing the DLL call code yourself unless you can find somebody online who's already done that. Do they give you any sort of SDK to interact with?
Yes, that is the approach I have started with. I am currently using RealTerm to try and figure out how to talk with it first, and then I will see if there's any way I can continue on in LabVIEW with the limited knowledge I have. I am new to this, so it's tricky for me to decode what little information actually came with the instrument!
Besides the User Manual that was posted by someone else below, I was able to recieve a *Communication Specifications* package that goes into a little more detail about how to format commands to interact with it. Writing the DLL code by myself may prove to be more of a challenge than I once thought!
If you have to spy on the output from the serial port you are going to have a bad time. I would only do that as an absolute last resort as it could take forever to do. Contact the manufacturer and is if someone in their tech department can help you out and give you the commands. 
Yeah. If the DLLs aren't provided in the SDK, that's not something you'll want to write yourself. Haha Using VISA as described below sounds like your best bet in all this. Let me know if you have any questions!
What does your command look like in real term? I think there a couple ways to send a command to the camera. You could send string commands as hex characters (right click on the string control and you can switch to hex characters) this is how typically communicate with stuff. &lt;STX&gt; = 02 &lt;ETX&gt; = 03. I believe you could also send it as decimal commands using U8 integers (less familiar with this. Look up an ASCII table to see how commands convert. My experience is that manufacturers make this very confusing to do your own communication so you have to buy there software. Hope that helps.
I took the very long-winded way to this: As you can see in the document I posted above (and hopefully won't get sued for distributing) Command "54" should apparently "switch the T-10A to PC connection mode." Sounds like where I would want to start. I ended up typing every part of that command in Hex. Yikes! It looked something like: 0x02 0x30 0x30 0x35 0x34 0x31 0x20 0x20 0x20 ...etc And then I slapped a +CR and a +LF at the end of that. It only returned "NUL" and the instrument itself kind of...restarted? You know that point with an instrument when all the visuals on the screen light up and then it just goes to 0? That's what happens. And it tells me the RXD line breaks. It's frustrating, but at least I know the instrument is hearing *something* from the computer.
They show you exactly what to do in the second posts links. Get the audioplugin usi, and if you're not familiar with how to install a data plugin, you can also read the article linked in there about how to install it.
The computer I am using does not allow me to install the plugin; therefore, I am forgoing converting the .mp3 file to .wav file step and just getting the .wav file from a converter online. How exactly do I use the fft power spectrum analysis VI?
The thing is, I want to upload a .wav file of my choice and then wire that (along with its parameters) to the fft power vi.
So use the file dialog VI to pick your file, then the "sound file read simple" vi to get the data, then spectral measurements VI, and come up with something similar to [this](http://i.imgur.com/E2VL0CY.png) If you google ".wav file spectrum labview" youll come up with dozens of examples. 
Thank you, kmoz! You are an expert at this! Do you use this platform in your job? My professor wanted to see if I could complete the simple task of converting a waveform graph of a sound file with respect to a graph with respect to frequency in order to perform data analysis afterwards. It seems like LabVIEW simplifies things a lot with the expressVIs but I get confused a lot when I am not aware of their required inputs and what they output.
Im an academic field engineer for NI, so you could say I use labview every once in a while ;) Here are some good video tutorial series to help you get started: [Labview 101 Series](http://www.ni.com/academic/students/learn/) [Enable Education Labview Youtube series](https://www.youtube.com/playlist?list=PLdNp0fxltzmPvvK_yjX-XyYgfVW8WK4tu) And you havent even started to scratch the surface of labview :) Pretty much any engineering function you could think of is in there, and you can design everything from the simplest of things up to applications like the SpaceX launch control system. 
I haven't put it into LabVIEW just yet, I will be trying it today. Silly question, but can I send it Hex while in LabVIEW? I would just have to put it through a Format into String block wouldn't I? Also, is it necessary to send the commands in 4's? It seemed to almost work while I was sending then individually. Edit: Ignore my silly questions, I established communication! (Now the hard part of actually making it work in LabVIEW)
Make sure you right click on the string control box and change it to "hex display" before you type in 0230 3035 3120 2020 0330 370D 0A. You can change the diplay back to normal display or codes display after you input it as hex. Not sure what you mean by send commands in 4's. You need to be very specific with serial communication. If the camera expects x bytes and doesn't recieve x bytes it will spit out an error.
When I passed it, I crammed all three practice exams the hour before it. I missed the things not-covered, but made enough that I passed. But don't worry, after you get through that, it's all practical exams. The CLD is not too bad, the CLA sucks.
The daily CLAD forum (not sure if the exact URL but Google finds it) has loads of practice questions. I thought they were a lot harder than the exam actually but there's some useful stuff in there so I recommend checking it out. I did loads of those and the practice papers on the ni website. 
The CLED is a very hard test from my understanding. I havent taken it myself, but its a bit of a beast from what I hear. 
http://download.ni.com/evaluation/certification/cled/cled_exam_prep_guide_english.pdf try this
Hi and welcome back! The BCC is a calculated value to ensure data validity - you should write a subvi to program it automatically for you once the command is otherwise ready to be sent to the device.
Oh, awesome, thanks! I didn't know there were vi's like that which I could directly use. I had assumed I would make all of that from scratch. (if it isn't painfully obvious, I'm still getting the hang of LabVIEW.)
I do need all 48, simply because that covers *all* possibilities of Reading measurements. In reality, I'll probably use 4 of them, but it's better to have something that can handle anything you throw at it! Did you only mess up when trying to calculate it in LabVIEW, or as a whole? I have it calculated on paper correctly, and am about to start working it out in LabVIEW. Edit: [Here's what I did.](http://imgur.com/fpBiUK4) Not pretty, but it works. This excludes the STX. I then tack on that, the STX, and the delimiter, and it works out great.
A LabVIEW meetup at NI Week? Isn't that the purpose of the whole week? Also I think an excellent place to meetup would be the LAVA BBQ. Early bird pricing ends tomorrow. https://lavag.org/topic/19062-lava-bbq-2015-official-information/ I'll be there all week coming in Sunday, through Friday morning. Also here is a similar thread over on NI's forums. http://forums.ni.com/t5/BreakPoint/Attending-NI-Week-2015/td-p/3163319
You're absolutely correct. Thank you! I'm not the best at making things concise,yet.
Thanks for the encouragement! I'm currently trying to find an effective way to display the results now since it sends back more of a code than a nicely readable answer.
We've all been there. Also, try posting in the labview forums at forums.ni.com. Much more active than this sub
What are your application requirements? That's a lot of RAM! You may not need RT, depending on your requirements. If that is the case, you can switch to any other 64-bit Linux distribution that NI supports. If you *need* RT, then your idea of partitioning the system into two parts, one RT part with &lt;4GB ram and one 'data processor' part with 64GB ram isn't terrible, but it adds hardware costs. The last option is to find a NI supported linux distribution and modify the kernel to enable the RT flag. This has the most risk and I wouldn't recommend unless you're in the business of making custom Linux images and supporting them yourself. Again, this all depends on what your program is doing and how fast and precisely timed it needs to be. Please let us know a little bit more about your application so we can further refine our suggestions!
 I can't go, sadly. Are there any rumors about new features in this release? Seems like I've always heard about stuff in the summer leading up to a new version, this year crickets. 
I think the only solution is to use a hypervisor (I guess this is what you mean by using LV RT as a bridge). And do you actually need the 64GB of RAM?
Crosspost http://forums.ni.com/t5/LabVIEW/Specific-2d-array-method-of-iterating/td-p/3169822 Take the free training, look at the example code I posted, and read up on state machines.
Since you are familiar with mysql, this community post may be the way to go: https://decibel.ni.com/content/docs/DOC-10453 Alternatively sqlite is very lightweight and easy to use. There is a great toolkit for it here: https://lavag.org/files/file/212-sqlite-library/ I've used both mysql and sqlite and they can definitely get the job done. More for fellow readers than for the OP, but before implementing a database please read up on the basics of normalization - it can save you and your coworkers lots of headaches down the road! 
Database format != spreadsheet format https://en.wikipedia.org/wiki/Database_normalization Design your schema to minimize repeating data. Work with records (rows), avoid changing columns. 
it wont be on the cloud
I actually don't know what you are trying to make. It sounds like an XY graph where the user gets to define n and X1,X2,...Xn
You need a while loop with the event structure. The event structure has a value change event for when the user adjusts "number of points". When that value change event fires, you want use initialize array to create an array with N points. The user can then fill in the desired value for each point. Then you have a start button that triggers an event which enables your data acquisition to start collecting encoder counts. You multiply the counts by revolutions per count to get revolutions. You poll revolutions at a fixed rate and subtract the new count from the old count divided by the interval to give you RPMs. You monitor RPMs and when oldRPMs &lt; RPMSetting1 &lt; newRPMs you use build array to add the current time to a cluster of two arrays. The first array is the time, the second array is your RPMSetting[] array.
This may be way off base, but I had a similar problem years ago when using windows. I don't know why, but it happened when a USB thumb drive or external disk was also plugged into the computer. Unplug all the thumb drives, or in linux, and no problems.
I don't have it with me, but it's really simple. I did'nt put any nosence inside, just a DAQ assist to write to spreadsheet. The write to spreadsheet is inside a on/off loop with two key : Operator button and an iteration timer. All that is inside a while loop 
Will still try that, it could make sence that the motherboard have issue with to many power going to the USB key and the USB device at the same time ! 
I suspect that it's an issue with a race condition or programming structure issue. What's the purpose of the case structure around the write to spreadsheet? Are you only writing data every nth iteration or if the operator hits 'write'? As a general piece of advice, I highly recommend you use a [producer-consumer architecture](http://www.ni.com/white-paper/3023/en/). Write the producer loop using DAQmx VIs instead of a DAQ Assistant (the examples will have a good template for you to copy). I can almost guarantee this will take care of your issue and run much smoother.
Yes, The loop around write to spreadsheet is to be able to monitore how long and when is the save. It will save only when the button SAVE is press and for the lenght that I will give him. For the other part of your answer, i totally aggree, I have just complet the first course of NI and i still need to understand how the DAQmx VIs function ! Using the DAQ Assistant was just simple at that time 
The DAQ Assistant is just an automatic wrapper around a handful of DAQmx VIs. The problem with this is that every time it runs it initialized an acquisition task, sets it up, runs it, then closes it and deallocates all the resources. So I wonder if maybe something in that process is what's throwing off your data. You'll essentially build the same thing with DAQmx VIs but now you can integrate it into the program better. In this case you'll initialize and set up your data acquisition task outside the while loop (so that just runs once when you start the program) then you'll run the task in the while loop, as many times as you want. Faster and less chance for things to get weird. Take a look at some of the DAQmx examples in the example finder and you'll get a feel for it. I've been building these for a few years and I still start with one of those examples more often than not.
im sorry, i am lost. can you show me?
http://www.filedropper.com/pressuredrop here is the link to my labview program
I'm not familiar with test stand, but it looks like your talking about just a regular EXE right? If so, the top level VI's front panel will always show like you're seeing, there's no way around it. How do you make it not look like shit? Make it a splash screen, not a real VI. All it does is launch your real VI. Checkout the actor framework sample project for a pretty good example of you to do a splash screen (you don't have to know AF to understand most of the splash screen stuff)
To calculate velocity, just the change in distance over the change in time. Average velocity between iteration n-1 and n = (D(n) - D(n-1)) / (T(n)-T(n-1)) Where T(n)-T(n-1) is the time between your measurements (for example a 100Hz polling rate would mean 0.1s) Use a shift register to get the distance at the loop's previous iteration, subtract the new value from the old and divide by the time (in seconds) for each loop iteration (0.005 if that 5 signifies a 5ms polling rate), or you can get the current time and use another shift register to properly find the time between the two measurements.
Thanks for the reply! The AF seems incredibly powerful, and complex... I'm at a point in the project where I rather not have to implement something that im not familiar with. Do you think it would be a bad idea to do the same without using AF?
Ya, I'm saying you should checkout the example for the splash screen section, not the AF part. AF is a pretty big undertaking.
I'm also not familiar with TS, but it's actually a bit surprising to me - your EXE is less than 500K. I would expect it to load quickly and for this not to be visible. Anyway, there are quite a few examples of splash screens around. I generally don't bother with it, because most of my apps stay open for long periods anyway, but I expect the main thing to consider is that in order to speed up the splash screen, you want to load the rest of the code dynamically. That means that you also have to add some code for that in the loader and make sure that the code goes into the EXE. In your case, because the app is small, it might be easiest to simply add a tab control and make the second page the splash screen - set the tab to show that by default and to switch to the main tab after a couple of seconds.
Based on some of the study resources, I'm starting to think they pulled a ton of questions from the CompactRIO Development Guide. Been reading this all day today (test tomorrow!) myself. http://www.ni.com/compactriodevguide/
How well do you know your local NI sales rep? From my understanding, they will be the one giving you a call when they have a client that needs some software help. Depending on your area and your expertise (perceived or otherwise), it could really vary. I'd touch base with NI and see if they had anything suitable you could do to try it out before you quit your job.
Have you used Sqlite with labview? How can I use the sqlite meta commands? (in labview)
I haven't used it in a few years (I used shaunr's library ~2009) but I still remember visiting [this page](https://www.sqlite.org/lang.html) quite often. Dr. Powell's library has a [long discussion thread on LAVA](https://lavag.org/topic/15857-cr-sqlite-library/?hl=sqlite) where there is a bunch of example code and discussion.
Is your Uno being recognized by your OS? Can you see it in device manager?
I assume you've opened the Arduino IDE and compiled and run the Blink test to this same board and it worked fine? If not, try that first. If so, I would suggest posting to the LINX board; https://www.labviewmakerhub.com/forums/ Sam is really good at responding to issues. He may be a little slow this week because I'm fairly certain he's down here getting drunk in Austin with the rest of us.
It is really hard to troubleshoot a picture. And it is even harder to troubleshoot with the information you have given. "Having some problems", maybe you could expand on what the problems are? How about what all the I/O is connected to? If you just want to check if your logic works get rid of your I/O and use controls and indicator. That way you can control exactly what is happening and see if you are getting the expected results. Once you get it working the way you want, then try using real inputs/outputs.
The marketing aspect is a plus, it does not mean jobs come rushing in. There are two aspects to it; having your name on NI's website and your local NI sales rep. Being on the NI website helps, but your mileage may vary depending upon how many similar consultants\integrators are in your geographic region. Likewise, having a good relationship with your NI Sales rep is mandatory (or you may not get accepted into the program), but your mileage will also vary based on what their customer needs are. An integrator will be recommended for different jobs than a consultant. Having a particular specialty that is unique to the Alliance members in your geographic region and the NI Sales rep having customers need that specialty helps them recommend you rather than another one of their local AP's. A huge benefit to the program is the SRL software (it was renamed today and I think is now ASL). This is an NI software bundle specifically for Alliance Members that gives you x seats for most NI software. However, it is a lease, not a purchase. If I bought the current version of LabVIEW, I can use it indefinitely and get upgrades for 1 year. The SRL is purchased for 1 year at a time and times out. I have been an Alliance Partner (consultant) for 3 years and was with an Alliance Partner (integrator) for 9 years prior to that. PM me contact info, and we can talk.
LAVA doesnt let me create a new user :/
Just a qns , my solar cell is connected to the input of myDAQ , i connect the output (line 7 ) to the relay but its unable to turn on the relay when i move the threshold (solar enegry lower limt) Should line 7 have the same voltage level as the solar cell ? when i use a multimeter to check it , it just shows a constant value od 2.1 + volt Thanks 
I've heard it was hard. I'm considering it for next year, although none of my current customers are using RT or FPGA for anything. Any tips?
The best way to write LabVIEW code on a Macbook is to dual boot into Windows and run LabVIEW. My opinion for both LabVIEW for Mac and Linux is a couple of years old now, but back then they were unsupported and buggy at best. 
&gt;Any tips? Study the CompactRIO development guide until you can write it from memory. Sit through the LabVIEW Real-Time 1, LabVIEW Real-Time 2, and LabVIEW FPGA webcasts until you can recite them from memory. I didn't get as far as the practical - I presume you should know the sample exam(s) off by heart for that, and be able to implement anything discussed in the three courses / cRIO developer guide without hesitation. Know the use cases (and underlying reasons) for all modes of network communication and all modes of data transfer, and what errors they can produce. Know how everything works under the hood: block memory, registers, FIFOs, DMA, scan engine, shared variables, etc. You will not be allowed access to any of the reference design libraries discussed in the Real-Time 2 course, (current value table, message handlers, etc.) but nevertheless need to know about them and their use cases. Similarly, you need to know priority levels of all processes, and the limitations, timing, dataflow and use cases of SCTLs, timed loops, and so forth.
Also many questions of the form "select all that apply", so only 1 in 16 chance of a successful guess.
Oh nuuuuu! You mean they want you to know what you're doing?
I feel your pain. My second attempt. Real-time/Embedded is pretty much all I do anymore. The questions are just too ridiculously specific at times. I read the compactrio dev guide TWICE before going in. I guess next time I'll go through the RT1&amp;2/FPGA courses. But, getting pretty tired of the written. The practical is going to be a cake-walk if I can just get the bullshit written out of the way.
Not only the first one :) Now full support of all modules and drivers is, unfortunately, available only on Windows.
should i just dual boot on Mac and download the Windows version?
You need to wire in an array rather than a value. i.e. the "Array min max" function needs to be outside the loop, with the data at the loop exit being auto indexed (see the image on the left http://digital.ni.com/public.nsf/allkb/B85025233861378A86256CE700491E34). I would personally get the data into excel then just use the min/max functions from excel.
Just wondering: has anyone used LabVIEW with Parallels?
are there a lot of jobs for CLED? I can't say I've ever come across anything like that in job descriptions. Just wondering. What was your incentive?
How would that work? Create a diagram instead of normal text language input, and then what is your output in the notebook?
I haven't done this myself, but [this NI forum topic](http://forums.ni.com/t5/LabVIEW/Windows-virtual-machine-with-labview-on-a-Mac/td-p/1787144) might be interesting to you.
Still need to try this. Also planning on running the different cables through an o-scope to get shots of the handshaking process. If the signal's noisy then maybe it's throwing something out of whack? Thanks for the input
I've got a signal booster in line on the cable, gonna be testing to see if it's throwing unnecessary noise into the mix. I hear ya regarding the ethernet, unfortunately the length requirement was a last minute "gotcha," and we'd basically already burnt the majority of our hardware budget. Had to make do... always causes a hiccup.
NI USB is very picky when it comes to powered hubs. I've had many that would not work at all. Of course many were cheap crap, but still I was surprised since it was the first USB device I had used that didn't work in every hub. There are a few threads on NIs forums, but my advice is to buy some local, try them out, find one that works and return the others.
If the unit is permanently damaged I feel like running it under too little voltage is the only likely case. If it's just loosing the unit occasionally then perhaps scoping it out would reveal something.
Nope. I use it in VB all day and use a ton of shortcuts. What OS, and are guest additions properly installed?
i am on a macbook pro and i'm trying to use labVIEW on windows 10. i believe i installed the guest addition already. i also tried installing the 3 updated device drivers but it still wouldn't let me proceed to installing the hardware devices so i just declined.
Hey Infinitenothing, I found a labview program that does almost what i want. The only thing that it is missing is an automatic update of the input array. As you can see from the attached link, i am able to select the number of channels on the blue square. After i select the number of values i can write down their names on the green square section. Although if i select 6 or more channels, the program wont allow me to write names for all of them. it only allows me to write the names of the set 5 channels that came with the program. is there a way to have the number of channels in the blue square update the number of input channel names in the green square. so lets say i select 7 channels on the blue section, the green section will add two more inputs for channels 6 and 7? here is the link to the VI :https://decibel.ni.com/content/docs/DOC-7332
Mishap using your tool from the previous post?
No, I think 2015 does some automatic casting for you now. If you connect an integer to a string indicator it casts it to a string just as if there was an integer to string function there. I believe if you right click the line there's a new option to define the casting process (decimal, binary, hex, etc...)
Running that crashed my dev environment
Yep. That can happen quite easily when you mess with things like that.
That's pretty awesome. Someone mentioned in the Youtube comments: Game of Thrones theme would sound awesome on this.
You should use that as your signature on LAVA ;)
Maybe some sort of zero crossing detection to trigger clearing/logging the current data set? 
It would definitely help to see a code snippet, but it sounds like you're only displaying values based on a maximum-based threshold. If you need to do that for some reason, stuff those values into a cluster and pass it around in a shift register until the conditions are met to update them. Create an additional set of indicators for you current upper/lower/range values and make sure they're wired to the data source PRIOR to whatever logic exists now that's causing your display problems.
Create a cluster that holds two values, Min and Max. Put it in a shift register. Take a measurement, compare the measurement to your stored values. Update your stored values. I posted a snippet of what I think you're going for. http://imgur.com/ubmzpS4 In reality, you'd replace the random number with the measurement code. Also, realize that if you initialize your minimum with the number zero, it will stay your minimum unless your going to measure values below zero. It would be better to take a single measurement, initialize your shift register that way (look at the First Call operator), and than enter your loop of measure and compare. To optimize, create a producer/consumer architecture, the producer makes measurements and passes them to the consumer where the heavy lifting of comparing and display/writing to file/etc... is done. For advanced classes, create a queued state machine so you can initialize and reinitialize your starting values, as well as decouple your UI from your acquisition code. Want a bigger challenge, learn OOP. Put your comparison code into a 'by reference' class, create methods to get the min, max, and range values. Add a fixed array size so you can only look at minimum and maximum values over the last 'n' samples. Yes, the rabbit hole is deep, how deep would you like to go.
I'll try to clean up my code first so it makes sense (I made the mistake of using the wire clean up option and now everything is scrambled).
I'll try to clean up my code first so it makes sense (I made the mistake of using the wire clean up option and now everything is scrambled).
I'm not gonna do your homework for you
http://www.ni.com/academic/students/learn/
This channel is how I started to learn how LV works:http://youtu.be/OjU_CIMjPvY Just watch his videos from 1 to 10 or so, and you'll get a sense of where things are, and how to translate your C knowledge to LV code. 
It's not going to be exactly your stuff, though. But watch them anyways, just to see how stuff works. 
Ah yeah, so the 300 second timeout is because the transmitter only puts out a value every 5 minutes. This is the only way (that I know of) to guarantee that a value is read. The readout is usually ~20 ASCII characters, I should probably change the number of bytes there. The reason the connection gets closed is that I was having issues where the VI would finish, or I would stop it (it used to be inside a while loop/event structure), and the Wixel would show that little "this resource is busy" icon, requiring a full system restart to be able to access it again. If there's a better way to do this, let me know.
So that VISA read + other code inside the error case statement would be inside a statement on "Bytes at port"?
It would be something along the lines of: Bytes at Port &gt; 0 ? True: Read with a 100ms timeout False: do nothing and come back later
Can I wire bytes at port to byte count on VISA read? Or is it better to do the timeout method?
You could do that, but you might end up splitting your input if the Bytes at Port node happens to run right in the middle of a set. Using the Timeout is the best way not to split. Just make sure the timeout you use is larger (by a small margin) than your data takes to come through.
Yes can be a bit hard to get started with it but it's easy once you know a few of the quirks 
Yeah, it's not my project unfortunately. I've submitted a pull request to add that newline char, but he's said that would break his app. It works fine though! I've got a new version that looks like this: http://imgur.com/d6juklX,BvIad8y#0. First image is the subVI in the middle of the second image. The Opening and Closing VIs are the same as yours. And that subVI in the first image is a VI that logs data to a spreadsheet for calibration purposes.
I'm familiar with it but in the video you say there's no PC involved. How are you using it without a PC? Can you deploy to myRIO?
Yes you can deploy to it just like a cRIO or a Real-time PXI
Well I'll be damned, I might have to invest in one of those.
What do you need the update for?
I'm getting a lot of errors while trying to deploy and i taught that the update could help.
Could you post some code? Maybe the errors are not in the device, but in the code that's being deployed to it.
You can use MAX to reformat it. http://digital.ni.com/public.nsf/allkb/6B1343F61905203386257051006573CA
I tried to do it but i got this error after clicking in format http://i.imgur.com/GrUkMVO.png
Can you show the block diagram? What are you trying to do, ultimately? Are you running RTOS?
[CODE SNIPPET](http://i.imgur.com/rtyLDLT.jpg) Decided to start again as I could not decipher the mess my old code turned into. Its nothing flashy but I'm still learning as I go.
I ended up finding an example on youtube that was along the lines of what I was thinking so I followed it :(
Mate you are nothing short of an absolute legend, thank you so much for spending your time on my issue (and giving me heaps of feedback while doing so). This makes a heap of sense, and is much easier for me to work through and understand. So I'm planning on putting in acceleration from a 3 axis sensor and getting velocity from an ultrasonic sensor (which worked great in my old code that I f'd). I'll be aiming to measure these in close (enough) to realtime and plot them as a chart. Only issue I've had previously while doing that is after the application was stopped, the chart data would go away and I could not scroll backwards and see what had been displayed. Just another odd question. With the min and max, would you have any ideas on changing them regardless based on what's happening with either acceleration or velocity? For example: I'm using this to measure positions of a squat. So if rep 1 has a max of 100 and a min of 45 great, but then if rep 2 has a max of 95 and a min of 50, it won't show. But again, thanks heaps for everything mate. I'll hopefully rework my code tomorrow night when my brain is less fried (i think its sleep time).
No problem at all, glad to be of assistance. I asked about the velocity and acceleration because I was wondering if you were planning on trying to get position data from them. It's of course possible via integrating, but if you don't need to go through the additional effort required then by all means don't bother. As far as your question about changing the max/min on the fly, you have a lot of different options. You can append the current data points from each measurement a chart and do something like set a Boolean to TRUE when a new max or min is seen, then set it to FALSE if the next measurement doesn't exceed either parameter. Graphs and charts allow you to due myriad things with the data points through the use of property nodes, and I think this is the approach you may want to take because it sounds like you're not acquiring huge amounts of data at high speeds. Also, make sure that you're using Waveform Chart indicators and not Waveform Graph indicators. The former has data history/retention, the latter does not, and I suspect that's the cause if the loss of data on application stop problem you mentioned. I'll see if I can find some examples for you regarding changing chart appearance through property nodes, and if I can't find anything specific enough I'll knock something together for you to show you what I mean. In the meantime, open up the Example Finder and poke around. Just go to either the LV splash screen or the menu bar of any VI and select Help -&gt; Find Examples... , you'll find tons of useful stuff.
Hmm, here's what I did: Drop a cluster constant on to the block diagram. Drop a DBL numeric constant, make the label visible, and set it to 'Maximum', repeat for Minimum. Move the Maximum constant into the cluster (BTW, I always manually move the labels to look the way they do), then the Minimum. Right-click on the cluster constant border, then select Auto-sizing -&gt; Arrange Vertically. Let me know if that's what you're looking for and sorry if I over-explained anything, just faster than multiple posts.
More is always better in terms of explanations haha and it worked a treat.
Yes, LAVA is active (certainly more than this subreddit). It has always been considerably less active than the NI forums, but that's a good thing. It means the SNR on LAVA tends to be much higher. It still gets the occasional "give me teh codez" thread, but those are relatively easy to filter out and a lot of the content is worth following (again, more than here).
One of the palettes deals with file IO. Checkout some of the functions in it. You can probably use an example in the example finder as a starting point.
It's one long string. My professor didn't give any example but he expects the program to be able to work for any given sequence
Well, I suppose that makes things a bit easier in that you don't have to worry about formatting. Just be aware that GAATTC != gaattc, so when you read the string from the file, you'll want to use the 'To Upper Case' or 'To Lower Case' string functions to ensure that you're making an apples-to-apples comparison. Dealing with strings can be somewhat annoying, but look at the functions available on the String palette and you'll find a lot of options for searching and matching. as /u/fuckingpewpew pointed out, the Example Finder (Help -&gt; Find Examples...) has loads of good stuff to use as a starting point.
You should look at shift registers if you don't want to use that function. 
Do you know how arrays work in general? Or just curious what some of the differences are in LabVIEW? Also, knowing colleges sometimes have old versions of LabVIEW... what version of LabVIEW are you using? Will fine-tune advice to the features present in your version.
/r/LabVIEW, the fastest way to get your homework done.
Wow, thank you so much, that's what I wanted to do but I didn't know how.
You are now banned from using Labview Reason: Looks like a movie bomb
It's a while loop with a 1 second wait each iteration. It's a reference to the kid in Texas that got arrested for making a "bomb" that was really a clock.
There are ways to query an excel file, but are you trying to access it while it is actively being edited by another process? Edit: See if this helps http://www.ni.com/example/28934/en/
yeap! The VI would keep running to compare the actual time with the excel's time and collect the temperature and irradiance value from the excel to calculate out total power to be displayed in labview
https://forums.ni.com/t5/LabVIEW/Obtaining-real-time-value-from-excel/m-p/3192601/highlight/false#M924858 take a look at this, I've included the Excel file
At least he's honest, now we can accuse him of making a bomb instead of just a hoax. 
Check out the VI that is on the page I linked to. It should have an example. I personally avoid using ActiveX, so I'm not sure how to go about doing what you need to do. But, what I linked you to should help you get started.
http://www.ni.com/getting-started/labview-basics/ we won't do your homework for you, but once you get started, we can help answer specific questions. 
I did it but I have no idea if it is right or wrong ! how can I upload it?
The black dashed line is a sign that you are connecting ports that do not fit together. 
And now it's the top post of all time in the sub.
I don't know about Linux, but I just finished working on a project that ran all of it's core code in LabVIEW and all of it's interfaces ran in Chrome using node.js. It's really just about building an exposed API through tcp/ip calls. Use JSON messages or XML messages to make it work with any web-based language, and viola, you're done.
Again not sure about Linux, but you can have an application run a webserver so the user interface can be controlled and monitored from the Web. Last time I used it it required Internet Explorer. Look up Labview Web server or Web interface for more into. 
How often are you using it? The biggest worry with a 12+ week runtime is if your USB or cable get's unplugged or power is lost or something while the port is open it may be harder to recover than just closing it and reopening it. If you're not using it a lot (less than once every few seconds, just open and close it, no harm just extra time for the operation.
There should not be harm in leaving it open for long periods of time when you are continuously using it. However, you should consider including error handling and error recovery code that will close and re-open it in the event the device becomes non-responsive for large periods of time. 
I'll be using it very infrequently more than likely only for initial setup and at test completion. Unplugging is definitely a good thought, but I "should" never have that problem. The test system is completely enclosed and there is no access to the USB connections. I think i'll open and close it per your suggestions.
When making test systems, remember, all those things that 'should' never happen will. Users are amazing creatures. If it can be broken, they will break it.
Ha! Absolutely! Thats why I used "should". 
What do you mean that the direct wire data is fine but the network data isn't. Can you show some pictures of what you're seeing and your code?
This isn't a link to the picture, just imgur's homepage. 
As always, good discussion is ongoing on the LV forums: http://forums.ni.com/t5/LabVIEW/How-do-I-neatly-replace-my-excel-sheet-logging-with-database/m-p/3196810
True it is not a case structure, but I need a stacked sequence because I need to deal with 4 states. Wouldn't changing it to a case structure limit me to only 2?
I don't think you understand what your code is doing. It's currently running through multiple cases in order every single time your loop iterates. If you have a case structure, the code operates based on the index written to the selector. It looks like this is what you were trying to do with the blue wire. You can have each case in the structure output those four booleans.
I see now. I am totally new to labview and I mistakenly thought I had to "replace with stacked sequence" to get multiple cases in a case structure. I see now that adding a numeric to the case structure's input will put it into numeric mode.
Yup, now that should be enough for you to figure out how to output the different Boolean scenarios that you're going for.
 Can I ask your opinion on what the hate against stacked sequence of structures is about? I find them to be a pretty useful way to make a procedural code.
1) They can't be aborted. You're forced into waiting for their execution to be completed. 2) Passing data between two sequences forces you into a right-to-left wiring. In a left to right data flow paradigm, this is horrible and makes for horrid wiring. 3) You can't change the execution order of sequences in run-time. 4) This one's a bit more picky, but having code hidden in other sequences makes it harder to trace. If all the code is laid out, it's much easier and less time consuming to piece together the entire purpose of the code. Sure they're good at forcing execution in a procedural way, however, that's the whole point of dataflow. You can easily force execution just by wiring error in and out wires and avoid the issues listed above.
Well, I just discovered that you can (for some stupid, stupid reason) replace a *case structure* with a *sequence*. I can't for the life of me understand why you'd ever want to do that.
You could just make a separate .vi to log all the default TC signals against the actual calibrations (either by manual input or if you have another analog signal on the RTD or whatever), then fit the data yourself and copypasta into ni-MAX. 
I don't really know his history. I know he's worked for people before, but since I've know him, he's been solo. He is very bright, and very successful. As for me, I work from home mostly because I'm in management. Less development these days. More paperwork. 
I have intermittently worked for a guy who owns his own LabVIEW development company. He travels a LOT, basically every week, and that would drive me crazy. However whenever he's brought me on I've worked from home. Mostly what I've done is support him so I do the grunt work but then he's on site handling the equipment. Either that or the client has some LabVIEW experience so can handle the actual installation. I've been working the last year programming a cRIO located in another state, and my boss is too busy with other projects to be involved at all. Over the life of the project we've experimented with different methods. I'd program locally and then email it to someone to install -- that was a disaster. They shipped the cRIO to me and I programmed it here which was great, but they need to use the equipment as well so it wasn't a long-term solution. I've travelled out there twice but it's not practical either for me to go out there all the time or for them to pay for me to live in a hotel for months. The solution that finally worked for us is my boss bought a laptop, installed TeamViewer and LabVIEW on it, and loaned it to them. They connected it to the cRIO. I can connect to the laptop via TeamViewer, run the remote copy of LabVIEW and program my little heart out. Occasionally I have to find someone to fiddle with the hardware, like when I've managed to lock the cRIO in a reboot cycle and need someone to flip the NO APP switch, but that's pretty rare. When the project is over, he'll take the laptop back and we could use it on the next project. Remote programming certainly requires some new skills, but it's very doable.
It really depends. Labview works best on either an x86/x64 PC or one of their dedicated RIO systems (I don't think you can use it with the raspi, yet), that kind of makes it pricey to build a LabVIEW system. However, your time is also worth a lot and if you are new to programming, LabVIEW will most likely get you results quickly. Also, you might already have an old PC lying around that you can use for the project, so you don't really have to factor price in. Then of course there is the question of GUI. Do you need a technical GUI(toggle buttons, graphs,...) or something fancy and pretty? LabVIEW is great at technical GUI stuff but imho it takes a lot of work to get it to look anything oder than grey and techniky. So, to sum up: Use LabVIEW: single prototype, focus not so much on design Don't use LabVIEW: cheap serial production, very pretty GUI needed
Where should I look in the context help for that ..can you give a little bit more effort in your help please
What do you mean by "gradually"?
How would you do it in a language that's not Labview?
0+0.5=0.5+0.5=1+0.5=1.5 ...answer+0.5 end at 5
Shift registers, while loop (i terminal), add, multiply. Look up all those on the LabVIEW help and you should be able to figure it out.
What effort have you put out? From what we can see, zero. Why not post what you have and ask for constructive criticism on your design. We're not here to do your homework for you. Do some research, try a few things, go ask for help when you can't figure it out yourself and you've exhausted all your options.
No problem, sorry for vagueness, homework and all that jazz. Just as an additional tip, You could also do something like [this](https://drive.google.com/open?id=0B3KkNAmB_2OPZnRqa0daVzItT1E). This utilizes the the i terminal of the while loop to calculate your X value. It also uses the "Indexing Terminal" to automatically output an array. (You could do math on X before it leaves the loop and auto-index your Y &amp; Z as well.
Triple click in the selector and you should select all, then copy and paste into a comment.
What kind of connection will it use? Do you have a comms protocol/datasheet/user guide for these? Sounds like it would be done using VISA reads and writes, so it's very doable.
From my research I saw that xbee can hook up to the my Rio through the UART port and the xbee also comes with a driver for that 
Perfect. Just use open the VISA connection, use a property node to set baud rate, parity, transmission protocol etc, and use a VISA Read to take in the raw data. Data's most likely going to come in ASCII, so the VISA read buffer will be a string of characters you can parse using string to number/string split/regular expression VIs. Check the documentation on the accelerometer to see what to expect. Also, set the XBEEs in Transparent mode, that should make things easiest.
I think I tried this earlier and it just pasted the .. but I will try again tomorrow, thanks!
Doesn't the my rio have an accelerometer in it? What does the 3rd part one have that the myrio doesn't?
If you see the '...' in your case selector than you have strings that are correlated. This can happen by mistake, or sometimes can happen when the compiler optimizes the selector after you have written the values in. Can you paste a screenshot?
You're doing a batch query here. I can't find any documentation on it, but I've never been able to make batch queries return any data. I think it's a deficiency of the ADODB (it's not .NET), not necessarily LabVIEW's database toolkit. If you use the .NET ODBC object instead, I bet your query works. Here's something to test it out that I tested on a MySQL query. When I ran it, my indicator labeled "recordset data (ADODB)" returns an empty array. "Recordset Data (ODBC .NET)" returns the data expected. http://i.imgur.com/rssYIpx.png Edit: Also, you could just do it in 2 queries. First one is the "SELECT MIN(ExperimentID)....." and the second is the updated.
The only reason I'm trying to do it in one query is due to race conditions in the application. The application is an attempt at using LV as a distributed computing platform; I've got to simulate an application that was previously built in LV as a run-once simulator, but now needs to be automatically run for 90,000 permutations of the simulation parameters. The permutations are stored in a table with a primary key of ExpID, and the job status is stored in a table with the same primary key. The idea is for clients to pull jobs from the DB, run them, and commit the results to a set of tables for later analysis. Preliminary runs show that, on occasion, we get clients who get the same job to run, and when the later one tries to commit to the DB, it exits with primary key errors. This could just be handled by making the clients pull another job, but we could avoid the wasted attempt by nipping the problem in the bud at the source. 
I think I got it. Thanks, I'd be really lost if I had to dive into figuring out .NET on my own. edit: yep, this worked wonderfully! Now, how do I integrate this with the other parts of my application? I've been using the Database Connectivity VIs for all the other database related things, and I'd like to not change them if I don't have to. The application prompts you for login credentials at the beginning, which is used to create the DB connection reference. Is there any way to convert this reference to a connection string/reference that I can use with this construction?
Since it has already been mentioned about the actual transmission. What will the purpose be since you didn't mention a GPS.
Heres a pic of the VI. The actual VI is attached to the NI post in the LLB. http://i.imgur.com/sYHl7cl.png 
Sorry, messed up, snippet is below.
They are being passed out to TS and then passed back in once the loop, loops. There are 32 stations, so those arrays would have 32 elements.
So I did another test and this VI isnt not the cause of the issue... I posted another LLB to the ni forum, where I query the PSU for info. That one has a bunch of VISA functions in it, do you think any of those might be causing an issue?
In your PSU SubVIS, move your VISA controls and indicators outside of the case structure. Run it again and let me know if that changes anything.
I have a lot of cRIO's in my shop right now for a project I'm doing for a customer. They're named cRIO1, cRIO2, ..., cRIOn. I'm boring.
Now that you mention it, it is set set to write it to excel so that could definitely be it. Would setting it to another of the available formats help or is it a known issue with the express VI? 
No changes.. Are you familiar with TS at all? Specifically the user interface? I'm wondering if theres something in the UI that I implemented thats causing the leak.
I've got some TS experience, although I've never run into TS interfaces causing memory leaks. My guess is it's in your LabVIEW code somewhere though. The arrays for your green and red lights, how are they defined in TS as variables? Explicitly sized arrays?
The arrays are explicitly sized in TS, 32 elements each. After doing more research, im thinking the way I used UI messages to send data to the UI might be the culprit. http://i.imgur.com/uc37Geo.png I'm sending two 64 element arrays back as a string, using array to spreadsheet string VI. In the UI, I then use the spreadsheet string to array. Maybe this is the cause?
This is a much better way to handle it. I've had applications that had really big datasets (a meg or so of waveform data) for each individual test and used callbacks. I created temporary files in TS and had the LV callback delete them when they were done. The callback was basically just a pointer to the file. Not a problem at all, I definitely prefer helping people that have already tried to do something, rather than the people on here posting homework. Offtopic: but how did the induction go? My wife was induced two years ago last Tuesday, we had an easier go of it than you did though.
Just remove the shift register and build array function and you'll have what you want. The loop output right below the shift register automatically indexes values into an array. 
Close, but not quite. [This](http://imgur.com/QETVhPJ) is what I meant.
I agree with the other commenters that this is mostly a matter of hardware, not software. Of course, LabVIEW would work once you get some kind of signal from your sensor into the environment. Just so I get some kind of impression of what you are trying to achieve, what does ozin refer to? Is it some specific measure for precision with regards to a fixed reference value?
I think he meant oz/in, as in Ounce inches.
Correct
Once you work out the hardware, the next question would be how fast do you want to take the data? And do you want to record it?
Do the math. Your resolution for an analog measurement transducer is determined by its sensitivity and range, the input range of the DAQ device, and the DAQ device's ADC bit depth. So, for example, if you had an analog displacement transducer with a 4" range, that produced 0-5 VDC over that range, that would be 0.8 V/in, or 1.25 in/V. If your DAQ device has a range of +/- 10 V, the corresponding displacement range is 25 inches (1.25 in/V * 20 V). Finally, divide that range by the bit depth - 1 to find the number of discrete steps in the range. For a 16-bit device, our example is 25 in / (2^16 - 1)=0.00038 inches. If the accuracy of your transducer is considerably less than this resolution (noise, hysteresis, repeatability etc. greater than this value) you can get away with a coarser ADC bit depth. A 12 bit device in our example would achieve a resolution of 0.0061 inches. Strain gauge based devices (bridge with excitation) including load cells and pressure transducers, additionally depend on the excitation voltage, and have a proportional sensitivity, with maximum range usually specified by linearity limits.
Are you a student? I took it while still in school, and employers commented on it positively a lot. I know for a fact that I got my first internship as a sophomore because of it. DISCLAIMER: I work for NI now so I'm biased, but it's probably fair to say that my CLAD indirectly led to that too. 
Yes and interesting that employers look for students with that type of knowledge. Didnt think it was that useful.
My co-workers and I tell our boss not to accept interns without a CLAD. Don't quote me on this, but I believe it's free for students to take or at least heavily discounted. If they want to do LabVIEW, there's no reason not to take it. You can take it at any Pearson Vue testing center.
But aren't ounces a unit of volume?
Yup use static VI references. This makes the VI a dependency and will include it in the EXE. What path does it give? Doesn't matter just read it. Also you could choose to use VI Name instead of path, the open can accept a string VI name or path. This allows the open to work on VIs that haven't been saved yet.
You should do something like [this](http://imgur.com/Xpir0nf). There's a few benefits here. Number one, in my opinion it's more readable. You can see all the string outputs right in front of you instead of having to flip through the case structure. Number two, if your bosses (hypothetically) ever ask you to add a category between healthy and overweight called 'Pleasantly Plump' you just add your truth structure in the right place, and add an element to the array. Three, you always know what turns the 'Healthy' or 'Warning' booleans on without having to dig through boolean wires. If the string says Healthy, you're healthy, otherwise, you're not. Simple boolean logic. You could make it more complex, but optimized by doing a nested case structure so that if you're healthy it stops searching, or whatever, but again, you'd be gaining microseconds here and making it much less readable. Remember, readability is key. Someday you'll need to come back and read your code long after you've forgotten what the hell you were trying to do there. Edit: Ooops, forgot to square my height, but you get the picture.
Thank you. This is the first time I've really been messing with the application functions and VI references, so I wasn't clear on how Static references worked, but I was able to adapt your example to my program and it works really well. 
Regardless of language choice, you need to solve the equation for each variable you will be calculating. I'd add a user input to select which variable to solve for In LabVIEW you could use an enum and wire it to a case structure to control which code is executed. For math intensive code I tend to use formula nodes instead of a mess of wires and operation primitives. The syntax is a little tricky (; at the end of each line, x**2 for x squared, etc) but it supports a surprising amount of features. 
[Someone did it 8 years ago](https://decibel.ni.com/content/docs/DOC-1282)
Thank you so much for pointing me to this.
Be careful, the Glang zealots will get after you for text based tomfoolery!
When you say you put the images in by hand, do you mean manually selecting the file path? If so, I recommend turning the file path control into a constant by right clicking and selecting "change to constant" in the block diagram. There are other methods but that one's quick and easy.
FYI pretty sure National Instruments bought DaisyLab years ago. Support is going to be pretty limited on that front.
Pay dirt. I installed a legacy version (circa 2005) to the last dinosaur in this building that's still running XP, and it looks like it's playing nicely with the same model of DAQ board and equipment he was using way back when. More importantly, the files opened! Worst case scenario: I have his entire setup now, so I can at least verify we're doing everything the same way. Best case scenario: I get this stuff to work, then see what I can see. Thanks stranger, you just made my Sunday.
I understand that Windows is non deterministic, but I wouldnt think a factor of 10 spike would result from that would it? I would expect a few seconds, may be 10, but not 150. Thanks for the tips on what to look for, I'll definitly look into the different services that are running! Thanks!
I would look at things like search indexing, anti-virus, windows update, time services, disk maintenance, and screen saver settings.
Nope, nothing of that magnitude. Only timeouts are VISA for LAN instruments, but they are 10 seconds I believe. Also, those VI's are all reentrant, so I think that should help out on that front.
All the kits come with ROBOLAB, but there's a LabVIEW add-on specifically for Mindstorms, so if they already have some coding skill it wouldn't be too bad. 
Do you mean a fast Fourier transform (generally called an fft)? If so, that's a chip, not an algorithm. And if your computer has one (probably) then yes, LabVIEW uses it. 
Yes, it's under signal processing -&gt; time frequency -&gt; transform -&gt; STFT http://zone.ni.com/reference/en-XX/help/371419D-01/lvtimefreqtk/tfa_stft/
Thanks I found it! Let's say I have a .wav sound file and I want to analyze that .wav sound file by looking at its STFT. I noticed that the wire to the input signal (X) is orange. Am I allowed to just wire the .wav file to that input terminal or do I have to perform some sort of conversion first? edit: I don't have the LabVIEW Advanced Signal Processing Toolkit as I only have the Student Edition. The available VI I'm looking at is called STFT Spectogram.VI (http://zone.ni.com/reference/en-XX/help/371361J-01/lvanls/stft_spectrogram_core/) which I assume does the same thing except the terminal for the signal input requires orange data type. 
The input takes an array of doubles that represent a time-domain signal. Check out this link that describes how to open a .wav file and get the data out of it. http://www.ni.com/example/25833/en/ The attached library outputs the file as a 'Waveform' datatype (which is a special datatype that represents a waveform as an array of doubles, a start time, and a delta T between samples). However, by digging through the palettes I'm sure you can find the function that will let you get an array of doubles out of a waveform.
Thanks /u/iYogurt! Here is the corrected VI: http://puu.sh/l8jLp/4cd3dcddb5.png My professor corrected my VI and the main issue was that the output waveform from the Sound File Read Simple VI is outputted as data from multiple channels by default when I only needed one channel (the first) so Index Array was added in. The other issue is that I have two power spectrums because there is no power spectrum displayed from the Auto Power Spectrum VI (I do not know what went wrong there) so I created Power Spectrum 2, which did display the power spectrum. http://puu.sh/l8kc7/14634e764e.png The next task for me is to perform some spectral analysis using the power spectrum and the spectrogram. Since the power spectrum displays sharp amplitude peaks, the professor wants me to create a histogram with frequency bins to hopefully create a smoother curve to actually see at which frequencies those peaks occur (for example, he told me to sum up all the amplitudes for a range of let's say 0-10 Hz, and then for 10-20 Hz, etc). Would you have any suggestion on how I can sum up the amplitude values on LabVIEW? 
There are multiple ways depending on what you want to do. You can right click the control and look at the Data Operations submenu. From there you can copy the data and paste it into another control with compatible data. If you want the data in a control to manipulate then right click the terminal and choose create constant or create control. Or just change it into a constant or control. The data will be prepopulated with the data from the graph. Lastly, you can export the data by right clicking the control on the front panel and choosing one of the options from the Export submenu. 
There are a lot of ways to do that, what's your end goal here?
Let's say I have an amplitude (y-axis) vs frequency (x-axis) graph. I am trying to get the exact values of amplitudes at let's say 1 Hz, 2 Hz, 3 Hz, etc. Then, I want to create a histogram using frequency bins. For example, I'd add up all the amplitude values for frequencies in the range of 0-10 Hz, and the same for 10-20 Hz, and so on to create bars.
I tried your way but I ran into a problem: http://puu.sh/lfKjn/b40c84021f.png This is the description of the error: These cannot be wired together because their data types (numeric, string, array, cluster, etc.) do not match. Show the Context Help window to see what data type is required. The type of the source is cluster of 3 elements. The type of the sink is 1D array of double [64-bit real (~15 digit precision)]. http://puu.sh/lfKYa/58ba2bf78f.png Is there a way I can convert the data type from the property node (value) of the power spectrum (pink) to the data type that is required for the input terminal of the histogram VI (orange)?
So, the professor explained it to me in this way. So we have a power spectrum of values with amplitudes on the y-axis and frequency on the x-axis. The point of making the histogram (or graph, for that matter) is to find out where in the spectrum the peaks in amplitude occur. In order to do that, he wants me to create some sort of graph (not necessarily a histogram) that adds up all the amplitude values from that power spectrum within a certain range of frequencies. He says that would tell us at what range of frequencies the peaks occur in. For example, the graph we end up with would be described as follows. For the frequency bin from 0 to 10 Hz, the sum of amplitudes for that range is some number. For the frequency bin from 10 to 20 Hz, the sum of amplitudes for that range is another number. For the frequency bin from 20 to 30 Hz, the sum of amplitudes for that range is a number that's higher than those 2 previous numbers. So we would know that peak(s) occur at wherever the highest bar is (if we were to make a histogram). It's more like an integration or an average of the amplitudes over a range of frequencies so in that way, it would not be best to create a histogram.
Oh okay, so the histogram is kind of reducing the resolution of the power spectrum. Gotcha. Anyway, that's the gist of the technical side. Let me know how it works for ya.
I abandoned the histogram. Now, I want to get a table of those x and y values from the spectrum and manually add up the y for a range of x values. Would you have any suggestions on how I can do that?
Thank you. I would try to learn this on LabVIEW but I have to report to the professor tomorrow and I don't have time so I figure I could probably do this on Excel.
Yep, I get it. No problem, good luck.
As pointed out in the thread on the NI forums, this is probably because the in the last iteration the stop changes to T, but the bottom wire does not execute because it waits for the timeout from the dequeue which doesn't come. Also, don't cross post. It's considered rude. And a good tool for debugging things like this is execution highlighting.
thanks for the quick reply. some more quick questions: 1. what information is the first circled part passing in into the while loop, just the sampling rate? can I put it inside the loop? 2. Can you elaborate on the shift register? Does that mean that Labview will record the data every time it completes a loop and give it as an output in the displayed waveform? But aren't I already collecting data at a sampling rate of 1/200 second, is the loop results of any significance? 3. how do I determine if I want my loop results to be independent on previous loop? (lame question as I guess this depends on my application, or is this a built in function of while loop?) thanks! 
Daq assistant will do the waiting if its a hardware timed task, so that part is taken care of. 
Sorry i forgot to mention the current program passes references i to subvi through the terminals so that the subvis can use them. In some cases lots of references and values are bundled into a cluster to then be passed i to the subvi in which they are unbundled.
don't mind my third question, it sounds like a stupid question. http://i.imgur.com/TD3nG9n.png do you mind if you can help me recognise the names of the palette I circled? I am struggling real hard to recreate the same block diagram in labview..
Speed and efficiency wise, the difference is minor. However, FGV's and AE's are the way to go over Globals. Ben Raynor's [wonderful post over on the NI forums](http://forums.ni.com/t5/LabVIEW/Community-Nugget-4-08-2007-Action-Engines/td-p/503801) explains the process and benefits in great detail. 
Sure, but this will be easier. Labview has a feature called snippet which is a PNG image that you can convert into VIs. Drag [this image](http://i.imgur.com/DpG1c6b.png) onto your block diagram and it'll convert into VIs.
it doesn't work Q_Q, I am running Labview 14.0 while the snippet is from 15.0 edit: i just realised you put in the effort to create the whole block diagram. thank you kind citizen, whoever you are. 
Ah okay that makes sense. Whoops. Going through your red circles from left to right: 1. Reciprocal 2. Get Date/Time in Seconds 3. Array Constant with a Numeric Constant inside (to make it orange right click the Numeric Constant, go to Representation, then select 'DBL') 4. Convert from Dynamic Data (you'll have to go into the properties for this VI and select the last item: single waveform) 5. Append Waveforms You can find these easily by hitting ctrl+space bar and typing the first few letters of the name.
Highlight execution shows the logical order of dependent code, but drastically affects the timing. Just because code doesn't run in parallel while that is on doesn't mean it doesn't run in parallel. What does the code do when you run it as is?
Also, your two inner loops don't have any sort of execution timing so that could be a problem. I would look in to using a state machine for this, and that would clear up a lot of the issues you are seeing about not being certain what state you're in. 
What do you mean when you say both loops "fail" when run together?
The motors are supposed to either stop or change direction for a certain amount of time when each motor's respective switch is triggered. With only one of the loops it works fine but when they go in parallel the switches have no affect. 
I thought about using nested conditionals but I need the two motors to be acting completely independent of one another. If I have the code wait for 5 seconds for 1 motor, I still need the other switch to be active during that time
http://imgur.com/DHFv3vp This keeps each motor independent in a single loop without nested logic. When the button is pressed, the time 5000ms from now is saved to the shift register. Each loop iteration, the value is compared, and if those 5000ms have passed, the motor is turned off. This is not only expandable for as many motors/outputs as you need, but you can also run other processing logic during each loop cycle for things like PID or more complex I/O processing. Your current code stops all execution when you hit one of your wait blocks, and that's generally considered very bad because it causes the exact problem you're having: a single thread can't do *anything* during that wait. If you were to do something like that for something like a database, you'd have huge processing times for no reason, and depending on the architecture of the rest of the code, you might even stall other operations every time your function runs. Sketches like this are why I love LabVIEW.
Cross posted here - http://forums.ni.com/t5/LabVIEW/private-linker-method/td-p/3222185/jump-to/first-unread-message It's generally considered rude to cross post, certainly if you do it without giving a link to the other thread.
A state machine would be a starting point architecture to having a more responsive UI. Either way, what your asking for really depends on what you need before and after that single value. If you truly only need 1 sample, and no data before or immediately after, you could do an on-demand DAQmx timing setup and then a DAQmx read. If you need data after that initial reading, just start a normal continuous DAQmx timing and read, and pick the first 'Y' array value out of the waveform. If you need data before and after you'll just have to let it run continuous and know that in a non-deterministic environment it won't happen exactly when you push the button. You can mitigate this through a basic state machine, queued state machine or producer consumer architecture. 
Cross post - http://forums.ni.com/t5/LabVIEW/Requesting-single-value-from-DAQ-input-on-button-press/m-p/3222334#M936200 It's generally considered rude to cross post, certainly if you do it without giving a link to the other thread.
Apologies. I hadn't even considered that it would be an issue as they are posted on two separate forums. I was just trying to get help in as many places as possible.
Yeah, sorry i know that. It was not the whole assignment i said before, only the beginning [Imgur](http://i.imgur.com/vKp0V6z.png) this is the latest attempt I did. So basicly I have to use the formula from a capacitor Uct= U*(1-e**(-t/tou), with tou being restistance* capacitor and get this into a graph. The extra part is getting a second plot in the graph wich is max voltage- Uct. There are 2 parts a stuggle with: 1. getting a good way to do the formula until Uct= U max. 2. getting both the voltage-Uct and the UCt in one plot.
so, is that your conditional, (to stop appending the graph when Uct = some number)?
You can add a conditional terminal to the for loop (right click on edge of for loop&gt;&gt;add conditional terminal), or switch to a while loop with the exit condition you mentioned. what does the graph look like currently?
i finally got it working! after some more digging i found a way to get 2 plots in one graph too [this is the result](http://i.imgur.com/DlR93VL.png) thanks for your help
Your While loops don't make sense. You need to understand dataflow before you can figure out your application. LabVIEW programming is based entirely on dataflow and parallelism. This is incredibly powerful and has lead to its success over the years (coupled with the graphical programming), but is usually one of the first things that new developers stumble over. [Here's a simple resource to become more familiar with how it works.](http://www.ni.com/newsletter/51770/en/) The Highlight Execution feature is a great way to watch how your application utilizes dataflow. Your problem is that you have data going from that first loop to the other loops, so those other loops won't even start until the first looped is stopped when you press the Stop button. Why don't you put one loop over the whole thing instead of separate loops?
Right, thanks /u/glorypants. im getting used to visualizing the looping and structures.. i the robot is now responding to the freuency signals..but at uncoordinated times...going to move the daq while loop inside the nxt case structures while loop and add a time delay to the daq to regulate the timing of input hz into the nxt..
There is actually a biomedical toolkit which will do virtually all of the analysis for you for things like ECG signals. Its pretty fantastic and included in the academic site license, if youre a student.
Thank you! I am aware of the biomedical toolkit. However, when I went to use the Heart Rate Variability VI and extracted the statistical measurements out of it, I was getting numbers all over the place, with the heart rate being negative or even in the thousands and even ten thousands at times. I just went and did what /u/arcandor suggested and the heart rate for that was somewhat logical at times. http://imgur.com/szZA0u6 Then, I placed this subVI into my main VI: http://imgur.com/qC5KO4q You have idea why I have question marks on my VI icons? I am suspecting that I don't have the correct drivers downloaded on this computer (different from the one I used to make the VI).
The question marks are from the daqmx driver not being installed. 
You can use a property node to set it's value unless it has the latch when pressed mechanism. If it's set to switch when pressed, you can set it's value with a property node. 
Man. I feel sick just looking at that.
Part of designing and implementing a good framework is having the ability to easily expand to meet possible (probable) scope creep. Projects get bigger, it goes without saying. You give a manager an inch and they're going to try and take a yard. Setting yourself up with a good framework from the start means that you never run into block diagrams that look like this.
Completely understandable sentiment and I can't really argue with it at all. Every language has it's strengths and weaknesses, and one of LabVIEW's biggest weaknesses is it's lack of really skilled devs. All that being said, I have a few frameworks built up into VIPM packages that I'd be happy to share with you that may help take some of the headache out of your LabVIEW applications.
I think you're taking peoples constructive criticisms of your code a little too personally. You posted the picture and attempted to defend it as scope creep. It's not scope creep, it's bad code. Any user of LabVIEW will tell you the same. I'm trying to offer advice on how to avoid block diagrams like you showed because there is no excuse to have working code that looks like that. But take it with a grain of salt. Toss me another five syllable response. Go on back to writing more code like this. Really, please, because code like this is what keeps me employed cleaning up after people that write code like this.
Wrap both loops in the actor framework. Send Actor Messages between the two. I only chose this route because the other three options were already taken.
That looks like a few hours of work at most, maybe it would be better to rewrite from scratch to include the new functionality.
There's no bait to take, and here's a little constructive criticism for you. You've built what is probably a working application that does exactly what it's intended to do. But, what I can tell from looking at your top level navigation window that your code is too big to make for easy troubleshooting. At the bare minimum, wrapping each loop into a sub-vi would make it easier to troubleshoot them individually. The next step would be to create each individual loop as a modular component. I can tell from your top-level navigation window that there's no way to perform a unit test on any of those loops without running them all at the same time. What happens when you need to modify how the high-speed acquisition works? You change it and than run the whole program to see if it works. If it doesn't, you troubleshoot the whole thing. What if, instead, your high-speed acquisition was a module that could be modified and tested outside of this program? I can also tell from this navigation window that re-usability isn't high on your list of priorities. Loop 6 and 7 perform a custom action every N seconds. Why not create a(n) module(object) that performs an action every N seconds, than create child modules(objects) that describe the repeated action. That way when you change how the timing works for repeated actions, you only change it in one place instead of two. I can tell by your high level navigation window that your GUI control is tightly coupled to the rest of your application (by that big stack of references being bundled into that typedef at the far left). What happens when you change the GUI, add or delete an indicator, need to add some functionality, etc... Again, you have to re-test the entire piece of software as a whole. There's a lot you can see from a high level navigation window. I never said you needed to reduce the number of loops. My typical application, at the bare minimum, runs anywhere from 15-20 parallel loops. But, those loops can also run independently as modules and when I change one I can always verify that it works by itself before merging it back into working code. This is very valuable when working with production code in a production environment. The other plus, for my customers, is that my code can be deployed and maintained by someone that's not me very easily. That way they don't have to rely on paying that pretty penny over and over again every time something needs to be changed. Your code would require someone to know each component and how it interacts with one another. There's no way to learn each module by itself, and modifying your code could lead to so-called ripple effects (aka house of cards syndrome). I have no doubt that what you have written works, however, there's a huge difference between writing a piece of working code and engineering a piece of software. That is exactly the point that /u/Jey_Lux was making. My suggestion to you, take it or leave it, would be to post more, talk less, and listen to what people have to say. You sound like someone that thinks they have all the answers. None of us have all the answers and I learn just as much from having a junior level developer participate in my code reviews as they do when I review theirs. I think your post was meant to be a relaxed and joking post about how scope creep can cause well planned code to go sideways. Unfortunately for you it turned into a dog pile. I'm sorry it worked out that way for you, but really, what you posted wasn't a result of scope creep. It was a result of poor engineering, plain and simple.
Well, the thing is we have an Arduino (I do not know how familiar any of you are with that) that has a digital and analog output, we seemingly have some problems with the digital output. So we need to change the analog signal to a digital one (I figured that one out already) but the signal still lasts a little while and it keeps counting during that time.
Just as I thought. Unwilling to accept criticism. I didn't say anything about scope creep. You're the one that keeps saying that scope creep made it look like this. I'm telling you it looks like that because it's badly architected. We'll just have to agree to disagree and I'll know that I never need to take your opinion about LabVIEW ever again.
I have no idea what you're trying to ask. The linked VIs will work to generate a scale and offset to use as calibration values. Then you can use those values to convert a raw measurement from your device into engineering units. 
Yeah it does, but you shouldn't be worried about 1 point throwing everything off. Take a handful of points, find the line of best fit, and roll with it. He line of best fit algorithm is what will take into account noise and other factors and come up with good calibration values. If your hardware is half decent it should be calibrated no problem
I like it. Looks good and seems to work with my webcam just fine. I don't like that it requires Vision Acquisition as that adds $500 to any deployment I make with it, but I also understand that unfortunately, that is the overhead of working with generic camera devices in LabVIEW. One of these days NI is going to figure out that in order to stop being a niche market and start being considered a fully featured programming language that they're going to have to fix nonsense like this.
Well, as an employee... no comment on the forward-moving strategy. Thanks for giving it a try and confirming that it works though!
If I write a .NET application that acquires webcam input I write some classes that inherit from the DirectShow API. An API that's included (for free) in the Windows SDK. If I write a Java application that requires webcam input I write some classes that inherit from the classes in JavaCV. Another free library that uses native Java implementations. If I want to write a LabVIEW application that uses a webcam I'm forced to (like you said) write my own wrapper around OpenCV (which I've done) or the like. Or, pay for the LabVIEW native wrapper called Vision Acquisition. This particular case isn't an issue for me, like I said I've got my own implementation now and it works for what I need, when I need it. However, it's my belief that this is a limiting factor to the adoption of LabVIEW as a fully featured language. Until NI starts producing native LabVIEW wrappers for these common, everyday interfaces they are always going to be fighting an uphill battle for adoption by the mainstream. Which, in my opinion, is a bad thing. I use LabVIEW everyday at work and really enjoy writing code in it. I think the community has come a long way in providing solutions to fill some of the gaps, and with JKI starting to roll out open source projects I think there's a route to more and more of that getting done. I really would love it if the LabVIEW user groups I attend were nearly as popular or packed as the Node.JS or .NET groups that I attend are.
The examples included in labview are a good place to start
You can start by sharing the code that writes the data to the files, so that the community here can have a look and then give suggestions.
This thread may help: https://www.reddit.com/r/LabVIEW/comments/2lzi9v/request_could_someone_point_me_to_a_free_labview/?ref=share&amp;ref_source=link The most simple and comprehensive way to learn LabVIEW from scratch is to take Core 1 and 2 courses from NI, but it's not free: http://www.ni.com/training/labview/ You can also learn LabVIEW by yourself. It's not difficult since it's very friendly to user. The LabVIEW Help System and [The User Manual](http://www.ni.com/pdf/manuals/320999e.pdf) contain all the information you need.
Sorry for the delay in replies, but this is essentially how this toolkit works. If you download the source files, you'll see that there is a camera object with abstract functions that need to be overridden in the child class. You could create a new child class, implement the common functions, and you would then have a new Camera object that uses the interface of your choice. This is purely speculation, but for both IMAQ and IMAQdx, they are optimized for performance and throughput. OOP loads the functions dynamically, so there's a pretty severe performance hit. My guess is that was the design decision behind the two.
Do you want to have a graph showing temperature changes during the experiment, or just a single value added to the data file?
No, not really. There is a company called tsexperts who are writing their own compiler for LabVIEW to target raspberry pi, but only basic operations will be supported at first. The new raspberry pi 2 has an armv7 processor, so in theory it could run NI Linux RT, and then you could compile something using LabVIEW RT to run on it. I'm not aware of anyone who has tried this yet, the solution is full of potential pitfalls, and I doubt it would ever be officially supported by NI.
Another ARM-based CPU. Means you would need to figure out how to get NI Linux-RT to run on it and than treat it like an RT target. Not an easy route.
not the answer I wanted to hear =\ Figured since labview has linux support and supports other ARM devices it wouldn't be a nightmare but guess i was wrong
As suggested, if you want to run code written in LV on the Pi, you would need the TSXperts compiler - http://www.tsxperts.com/labviewforraspberrypi/ And no, there's nothing for compiling LV code to an Android app. NI does have an app called Data Dashboard, but that only allows building UIs which will connect to data on other devices. As for running LV itself - I don't know about the Pi and Linux RT, but there is this on the Edison, which is x86 - https://lavag.org/topic/19312-deploying-labview-to-intel-edison/
Problematic because there was no LabVIEW 2008, it went from 8.6 to 2009 which internally is 9.0. I don't know why it would work. Is there error handling? Any errors generated?
So two questions here: 1) Although you're right that W10 isn't officially supported, there are cases where it does still work fine. Have you tried it and know that it doesn't work? If not, it may be worth trying. 2) If you can give me a job ID of one of the compiles, I'll look into it a little bit and see if I can get any info on why that step is taking longer on the cloud.
I'll second this. I use W10 with Xilinx compile tools without issue. I would recommend tryingt that first if you haven't already.
1) I had tried it previously (back in December, after upgrading to W10) and it failed every time (it would just hang completely on one of the steps ). I tried again just then and it seems to work now. So, yeah, Thanks for prompting me to try that again. I'll probably go back to compiling locally, now. That said, it still irks me that compiling online takes so much longer. To compare two compilations of the same program, it takes 17:10 to compile locally, and 50:03 online (job ID BcPi6EP), most of which is 32:54 generating xilinx (which takes 00:00 locally). Local Compilation Time --------------------------- Date submitted: 1/26/2016 10:28 AM Date results were retrieved: 1/26/2016 10:45 AM Time waiting in queue: 00:11 Time compiling: 17:10 - Generate Xilinx IP: 00:00 - Estimate Resources - PlanAhead: 00:30 - Synthesize - XST: 06:34 - Translate: 03:20 - Map: 02:57 - Place &amp; Route: 02:35 - Generate Programming File: 01:04 Online Compilation Time --------------------------- Date submitted: 1/22/2016 3:19 PM Date results were retrieved: 1/22/2016 4:10 PM Time waiting in queue: 00:15 Time compiling: 50:03 - Generate Xilinx IP: 32:54 - Estimate Resources - PlanAhead: 02:24 - Synthesize - XST: 01:43 - Translate: 03:57 - Map: 03:26 - Place &amp; Route: 02:56 - Generate Programming File: 01:18 
I've worked with 4 GigE Basler Ace cameras at a time before and the issues I ran in to were with the data transfer. I was trying to pull images through a standard Ethernet switch and then through a laptop network card. Sometimes the images would lose packets and it really was not a fun time trying to figure out how to get it to work without having the funds to buy a fancy 10G setup like you have. If your hardware can handle the data rates, then you'll probably be fine. Are you analyzing the videos at all?
It's not ever "LabVIEW" that would have an issue, but your CPU could or the File I/O could. Are you going to be overlaying text on every single frame? Do you have a service subscription with NI by any chance? This is definitely something that the sales reps would have experience with and might know if someone doing exactly what you're trying to do. Even without a service subscription they might still give some input. NI has great customer service.
Good luck! With hardware stuff I've always found the NI reps to be great resources. If it's code related, the NI forums are the best place. If you want the ol' homey family help, Reddit's here too.
My guess is that your biggest bottleneck is going to be at writing to the drive. You may have already done them, but I would do some math and verify that you writing 10 streams at your resolution and rate is somewhere about 50% the write speed of your drive. That way you have a little overhead for it. Labview wise, you should be fine. 
You might consider bonding a couple nics or some other method of redundancy/load balancing for the network layer. What are you using for data storage?
Might be worth trying to write it into a file with an index of the frame number - that way you're not doing image manipulation - just storing it - but still get all of the data. You could also do it in real-time on your PC - but you'll just need more resources to do so (RAM especially). Anywho - good luck with your venture - it is doable! 
A PCI SSD will probably work or a RAID array doing some striping for performance will probably work also. But yes, I would just calculate how much data you're expecting to need to save per second and start there
So, from my research, based on your local machine and the speed of your network, it may actually be faster to compile locally in some cases. Compile cloud really shines when we're looking at situations where we either don't have the space or compatibility to install the compilation tools, when we want to offload the compilation, or we want to kick off multiple compiles without taxing the dev machine. Hope that clears it up a little bit!
So, we did some testing with a single 4K camera, and it didnt seem like the overlay added any load to the processor or RAM. Can you shed a little more light into why it might add load?
The drivers can be optimized for streaming data to disk - but doing in-line processing can add overhead. On your system this may be on a very small order (especially if you over-specced for this application)- so it may be nothing to stress about. 4k cameras aren't THAT much data (in the scheme of things - there are some pretty big sensors out there). You have gone down the right path of testing with one camera to see what the load increase looks like. In some smaller processor applications (like the NI CVS - where ram and CPU are fairly limited) the overlay may take up a much higher percentage of the processor time/ram allocation. If you're having success and seeing negligible increase in system utilization with the overlay - go for it! 
&gt; MyRIO And it can run untethered? Unlike the arduino? That'd be great
What is your application? Are your products/items to inspect moving by the camera? An encoder triggered line-scan camera is likely your most cost effective bet (and can give you one hell of an image). The 8k sensors from Basler are VERY good - but a little pricey (then again - you're only buying 1 camera instead of 4). The images stream in a certain number of horizontal lines (you can set it or have it triggered by an encoder or break beam sensor). http://www.baslerweb.com/en/products/line-scan-cameras/sprint Could be a cool place to start the search! Plus- you're already familiar with Basler - their line usually works extremely well with NI hardware and drivers (they are often the first ones tested since NI resells a few Basler cameras). 
Yes, I know. But labview cant be used on an arduino *untethered*. In other words, theres no easy way to convert labview code into the arduino's C code stuff. Not that I can see. Labview works with arduino, but mostly just to talk to IO pins. Not to run labview code on it, independently. 
Thanks :)
I don't think a line scan will work as the scene is stationary. There are a view different aspects to the project, but the one requiring the super HD image is basically grabbing an image of the ocean floor where everything is essentially static. Those things are insane though! Thanks so much for the help, I really appreciate it!
A) NI embedded hardware (myRIO, sbRIO) using LabVIEW RT B) x86 development board using LabVIEW for Linux A will always be easier and fully supported. 
Be sure to read the fine print, it only supports a subset of LabVIEW functions.
Almost guarantee that you have a local rep. And anyways, just call NI sales they can get you the pricing.
Agreed. I meant it was the closest thing to Arduino, but you're right. It's much much more.
You can use visual studio to build a .dll out of a console app or class library. Set the assembly name and namespace to something besides the default one. When your solution is built, start a new VI that contains a .NET constructor and invoke method located under connectivity in the right click menu. When you place your constructor it should automatically open a menu to browse to your .dll . You should have access to all your class methods through the invoke node when you wire the ref over. Now, that's easy for running C# code in labview, but you need to get the DAQmx data from labview into your C# based GUI and/or automation controller right? Ill leave it to you to figure out the architecture of how to do that.
I've passed data from LabVIEW to C# and vice versa for several automated test systems. If both programs will be running on the same computer then building the LabVIEW code into a dll and calling it in C# should do the trick. Otherwise you can use a TCP socket connection to pass data back and forth. There are also other methods available that may be more appropriate depending on your specific application. This link may be a little more informative as well. http://zone.ni.com/devzone/cda/tut/p/id/10060
This post is off topic for /r/LabVIEW. Please refer to the sidebar for more information.
You could pull a fun trick to learn how the Vision Assistant does this under the hood, then you can tweak it to suit your needs. Use the Vision Assistant Express VI, set up your calibration, then right click and convert to LabVIEW VI. That will break down the Vision Assistant code into all of its sub-VIs. Hope that helps!
I've failed the written exam twice now. The first time, I didn't put a lot of due diligence in. I just took the practice exam and went with it. Nothing from the practice exam was on the actual test... not even the same topics. Second time, I read the cRIO Programmer's guide (like 250 pages) TWICE. Failed again. After talking with my CLED co-workers, all they did was go through the RT and FPGA course materials (we're all CPIs so that's easy to access for us) and take the test. That's my plan when I try again. Just going to run through the course material. The practical will be a breeze at this point in my career so I'm not as worried about it.
We have access to all the RT and FPGA course manuals and exercises.I'll take time to read all those course, but what makes the written exam so difficult?
Licensing as in LabVIEW or licensing on the executable? Once you build an executable, it's all on its own. You can install it anywhere and it will work without needing to pay anyone. The expense when it comes to executables is all up front when you purchase LabVIEW Professional or the add-on alone. There is some extra stuff that needs licensing like if you use add-ons like Vision or the DSC Module, etc. Those add-ons require a run-time license that costs extra.
The easiest way is to use WebDAV or SFTP to access the files on the RIO. Both are super easy. It will be one or the other based on the target. There should be several good Knowledge Base articles on NI.com about this
I highly recommend this approach. WebDAV allows you to map the cRIO's drive like a network drive on your computer. Extremely easy to set up and you can navigate the cRIO like any other folder on your computer.
Hallo! Thank you for your advice. I will try it on monday, when Im at work. I think this is a very good idea. 
put the part that writes to the graph inside of a case structure.
You'll want to read up on Event Structures. By putting an event structure in your while loop it will respond to whatever events you add to it (button presses, etc).
LV3D is pretty rad. Still doesn't zoom, though.
This post is off topic for /r/LabVIEW. Please refer to the sidebar for more information.
Sixclear has a good video on the basics: https://youtu.be/8eO64fo3Pho /u/swegg 
Oops, I think I mean "until released" instead of "while pressed", ~~but it means the same thing~~ and this is where the confusion lies. It's not an undefined behavior, it's just not very useful. I can't think of a use actually, that can't be done better with one of the other options or a good architecture. I was thinking you could use it to test how fast a user can press a button (see if they can press it so fast that the button goes back up before the latch functionality can read it). But that can be done better by just using "switch until released" and timing the change using an event structure.
Latch Until Released, used in a while loop, could be used when you want something to happen only while the user is pressing the button. It could be useful in an industrial control or safety situation. After 20 years using LabVIEW, I still find the mechanical actions a bit arcane. You're post prompted me to set up a little demo VI to show the different behavior. Six rocker switches with the different mechanical actions wired to boolean indicators. Put them in an event structure tied to the value changed event. Duplicate the rocker switches and indicators and put those in a while loop with a 100 ms delay.
Interesting. Mind sharing the VI? Also, happy cake day!
&gt;Latch until releasedChanges the control value when you click it and retains the value until the VI reads it once or you release the mouse button, depending on which one occurs last. [This is from NI's description here.](http://zone.ni.com/reference/en-XX/help/371361H-01/lvhowto/changemechactofboolswitch/) Looks like the confusing part here is that "which one occurs last." This means: when pressed it will remain reading True (or false if reversed default) until released and then will read True one last time just like a latch. Thanks for the correction.
Well you've only included the main VI, which doesn't include "Graph As Chart.vi" That is most likely where you're getting your extra point, but I can't run your VI to confirm your issue. Btw, you aren't trying to graph as a chart, right? It looks to me like you want to just graph Voltage vs Current, so just Bundle those two together and wire to the graph. --- *You should never use the STOP node unless it's for debugging issues or the power plant is about to explode if this VI doesn't stop immediately.* Instead, wire the Stop control through the DAQ Assistant input as well and use the stop terminal on the loop like you already are. This ensures that your VI shuts down properly including all references and communication ports.
Thanks for the advice. I updated the link to include the sub VI. Once you run the program you will see what I am talking about.
Yeah, that's your problem... Why are you using Graph as Chart.vi? Why not just use a chart in the first place? Right-click your graphs and select Replace &gt; Graphs palette &gt; Waveform Chart. This will give you the historical data functionality without that subVI. What's happening is that subVI is being used twice and is set to execute as non-reentrant. This means that the memory space within that VI (those shift registers) is shared between each instance of the VI. So both of your graphs are getting each other's data points. If you *really* want to keep using that VI, you can edit the VI properties to make it execute as preallocated cloned reentrant instances.
Easier than anticipated. See this [VI Snippet](https://imgur.com/3JHNOLN). You should be able to drag&amp;drop that directly in labview to use it.
Thank you very much for your help. Can you give me any links where I can get the information about ASCII, checksum etc. I need? I'm a bit confused with the documentation I have. They say the ASCII sign is R for example (but they also say the equivalent hex symbol is 52) and then they take the sum of the hexcode as a checksum. You said it's a bit more tedious in Labview. Like I said I'm fairly new to Labview and I thought that it would sth work like this: I enter the code from above $R1004C5A and my robot does something whereas the checksum is calculated automatically because of my 5A string. Doesn't it work this way? Do I have to do it a different way? I also couldn't find proper documentation about serial or VISA programming with Labview. I have a basic understanding about the VISA function like open write read etc. but it's still quite abstract for me how I finally control the robot. Do I just write the ASCII commands to it and the robot does what I want him to do? Thanks for your help
ok thank you, I will try it. Do you have any resources for me where I can read in order to get a better understanding of the subject (sending ascii to control a roboto)? Or is it that simple that there is no documentation and I'm overthinking this ? :)
Thanks again. Yes, that's basically what I will do next. I wanted to test some commands in order to see if they work and then I will slowly build a program so I can control the robot via mouse clicks (that's the long time goal). You sure helped me a lot, neither is it your task to do my work nor do I want you to, but sometimes getting a little push into the right direction can help :)
Use [localhost](https://en.wikipedia.org/wiki/Localhost) as the [hostname](https://en.wikipedia.org/wiki/Hostname) 
It works! Thank you!
This might be helpful http://www.ni.com/example/30553/en/
Glad I could help! 
thank you for your help, so is there any tutorials and documentation to do this thing?
No problem. If you have access to LV and have the Vibration Analysis Toolkit installed, you'll find lots of good stuff in the examples that come with LV. Just open LV and select Help -&gt; Find Examples... Beyond that you can search the discussion forums on NI's site, there's lots of helpful folks over there and I'm sure there's example code.
aha, really appreciate your help .
I do something like this this, but instead, I spawn a singleton class register that all classes register with on instantiation. Than I create a server that can interact with the register to speak with registered classes. That server is just a TCP/IP listener, so other executables use clients to connect and interact with the server. They, also, in turn, spawn their own servers, allowing back and forth communication.
Yeah, I was thinking about making some sort of server or daemon to handle hardware communication. Would interprocess communication be faster using shared variables to store queue references and use queues to pass information, rather than sending TCP/IP packets? Can different executables even share queues through the LabVIEW runtime engine?
&gt; Your goal is to essentially lock the RS232 resource and prevent access from multiple executables? Yes and no. What I would like to do is have two different executables be able to send commands and receive data from the same instrument without locking so neither of them has to wait for the other to finish execution. I was thinking something like a hardware server that communicates to the executables and organizes the priority of the calls made from each process. The main problems I have are, how do I make sure only one instance of the hardware server is open at a time and what is the best way to send commands between independent processes using something like a queue? Edit: I was looking at this whitepaper, but I don't think it does what I need it to do. https://decibel.ni.com/content/docs/DOC-21441
Although, you could create a .NET singleton and share it between instances I think. The .NET singleton could contain an array of DVR references as I32's and you could share those between instances? Would that work I wonder? I mean, the latency you see on localhost TCP/IP comms is next to nothing, so I'm not too worried about it, just wondering if the .NET thing would work. I know that .NET singletons will work across instances of LV runtime, I use NLog for logging and it spawns a singleton and is used across all my executables.
If you make the hardware server a separate executable, by default, the LabVIEW RTE does not allow multiple instances of a LabVIEW executable to run simultaneously. If you add "MultipleInstances=True" to the ini for the executable you can get around it. There many ways to send commands between executables. I think using the Asynchronous Messaging Communication (AMC) library might be the easiest for you. It has options for accessing queues in other executables via UDP. Since you're familiar with queues this may have very little learning curve for you. You can download AMC via VI Package Manager. Edit: Here's a relevant white paper on AMC - http://www.ni.com/example/31091/en/ - Talks about sending queued messages over the network. Your IP would just be 127.0.0.1.
There are still issues with this. First, even in .Net the references aren't usable across processes. In fact, .Net has a concept similar to contexts called AppDomains. References usually aren't shareable across AppDomains unless you create proxy objects to cross those boundaries. Second, this still can't work around the fact that the LabVIEW references can't be used across context boundaries. Sending the reference number as an integer doesn't help. The number has no meaning outside the context in which it came. It doesn't matter how you share it. If you cast it back to a reference type then when you try to use it you will get an error because it's an invalid reference. FYI, I worked on the LabVIEW team for 10 years (until last month), and I was the maintainer of the DVR feature so I know this stuff pretty well.
A Coherent Chameleon and a Coherent Verdi.
You already got relevant replies, but for completeness on answering the original question: Technically, you can't create singleton objects in LV, since all objects are by value. I assume you don't mean a single non-reentrant VI which will do all the actions, but rather a class where you use something to lock the actual access to the port (a DVR, a semaphore, a non-reentrant VI which does both write/read, etc.). While others correctly pointed out that references aren't relevant across context boundaries, you can use VI Server to open a reference to a specific VI in the other context and run it using the call by reference node, and it should run in the remote context and maintain its reentrancy behavior correctly. For this to work you would need to configure VI server access on that EXE (using lines in the INI file) and it wouldn't be my preference (I would probably also prefer the TCP server here), but it is an option.
&gt; FYI, I worked on the LabVIEW team for 10 years (until last month) That's a shame. It's always good to have R&amp;D people who interact with the community. Are you still at NI or did you leave completely?
Thank you for your response. Yeah, the solution I'm going with is to write a hardware server that is opened as an executable (if not already open) by the programs that will run experiments. The programs will communicate to the server using the network queues provided in the AMC package (basically queues with UDP communication under the hood).
SAPHIR makes a mySQL API but it's $700 for the license: http://www.saphir.fr/en/produits/gdatabase-for-mysql-7.html You could write your own wrapper for the C API: https://dev.mysql.com/doc/refman/5.7/en/c-api.html Is there a reason you chose mySQL? We use AWS at work and I set-up a Postgres database. Reason being is LAVA has a wrapper for the postgres library titled libpq: https://lavag.org/files/file/240-libpq/ It didn't take much effort for me to write a couple of VIs on top of the wrapper for easy command execution and results retrieval.
You can communicate with mysql over TCP/IP, no third party toolkit required! See https://dev.mysql.com/doc/internals/en/client-server-protocol.html
You can have ODBC on Linux, I was able to set up about 10 clients to connect to my database and run jobs, 4 of them were running Linux. 
I went with mySQL because it was open source, and I figured it would be the easiest to work with. Thanks for the info!
Good to know. I am a chemist and most of my end users will also be chemists, and Macs are big in our world for one reason or another. But this is great if I end up having to stick with ODBC. Thanks!
Correct- hardcoded. Also, I got him to send me a screenshot, and it looks like this was his problem: https://mariadb.com/blog/resolving-error-1918-system-error-code-126-when-installing-mysql-odbc-driver
You could add in this bit of code in place of your connection opening VI: https://www.dropbox.com/s/9bljplmp1792aoe/open%20db%20connection.zip?dl=0 So wherever you have a "ADO Connection Open.vi" you put the "open db connection.vi". It does essentially the same thing, but first it checks a config file in data/database.ini where you can toggle the dialogue box to pop up and set up the connection natively in Windows. The default setting in the ini file is False, so your hardcoded version takes precedence. This makes the code a little more portable; you can deploy it on somebody else's computer, and if the credentials ever change, you just have them set it to use the dialogue until you can redeploy the application to an executable.
Looks like the data isn't in a string format. Did you make an android app to send the sensor data or are you using a third-party one?
Downloaded the app and tried it out. Here's my results: Right click on your string indicator on the front panel and choose hex display. You'll see that you're actually being sent a byte array (U8). You'll notice that it's sending 4 bytes per value. 4 bytes floating point in LabVIEW is called a single. You just need to convert the string to a byte array and typecast the byte array to an array of singles. Here's a VI snippet for getting it to work: http://i.imgur.com/eDgaBr2.png
Ha! I was going to make a comment about how Scott Hannahs will be disappointed that you're no longer around to help with his Mac stuff, but you foiled that. ;)
In addition to /u/fuckingpewpew there is also a "Num to Array" function for converting an integer into an array of booleans representing the integer in bits.
Well I did unsubscribe from Info-LabVIEW so I'm sure he will still be disappointed. :/
Right click indicator &gt; show hex value 
Thanks for your help everyone. it was my fault NOT to mention that it's not only for displaying purposes. 
So then the second part of my reply does the trick?
You could just use "String to Byte Array". It will give you every letter as a U8 integer. What is your end goal? For display purposes everyone else's suggestions will get what you need. For numerical operations, this is what you're looking for.
I want to do the following: I have an ASCII command like the one on top 006072 then I want to convert the ASCII to their respective hex value and add all of these values. I was able to create a string array where I have all my respetice hex values, but I don't know how I'm able to add them because I can't use the add function for a string array. Thanks for your help everyone
Ahh, yea this is what you want (hope I didn't just do your homework for you): http://i.imgur.com/zOuQtP8.png Edit: Actually this is what you want, the above will lose resolution due to the 0-255 limit on U8. http://i.imgur.com/joySdr9.png
Buttmonkey6969? Haha
Did you code it using LabVIEW? That would be pleasing for me 
Sorry, your submission has been automatically removed. Due to spam, AutoModerator has been set-up to remove posts from users with less than 10 comment karma. If you are a legitimate user, please message the mods to approve your post. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/LabVIEW) if you have any questions or concerns.*
Sorry, your submission has been automatically removed. Due to spam, AutoModerator has been set-up to remove posts from users with less than 10 comment karma. If you are a legitimate user, please message the mods to approve your post. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/LabVIEW) if you have any questions or concerns.*
Hi Muun, I tried it today because I didn't have access to LabView during the weekend, but it seems like it doesn't seem to work. I will try to explain it in a better way of what I want to do: Let's take my example ASCII string 006072 each letter translates to hex to the following like in my first post: 0-&gt;30 0-&gt;30 6-&gt;36 0-&gt;30 7-&gt;37 2-&gt;32 Now I want to add the hex values:30+30+36+30+37+32=12F The program you send me doesn't seem to do this for me (or maybe I did an error somewhere which I'm not aware of, but I calculated the sum via my calculator aswell as with the program, but I get different result) Thanks for your help edit: nvm I made a mistake, sorry. I think this works for what I need it.
Simplest logic! Just check if the interval time is a multiple of 5/10 seconds and set the case structure accordingly!
[removed]
At what level would you like to simulate the massspec. That can range from trivial (I.e. plot a prerecorded spectrum) to decades of research (accurate intensities of all fragments in an arbitrary mixture ionized with MALDI).
There is already a program out there that uses prerecord data so we want to go above that. We have been using a pretty detailed system of equations to simulate actual ion fragmentation and since my group specializes in MALDI techniques we have a lot of the tools to do this. Have you come across anything similar?
[removed]
What exactly do you mean by 'record type'?
Sorry, your submission has been automatically removed. Due to spam, AutoModerator has been set-up to remove posts from users with less than 10 comment karma. If you are a legitimate user, please message the mods to approve your post. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/LabVIEW) if you have any questions or concerns.*
I think I get what you're trying to do. [To help with the blinking action I made this blinking LED VI for you.](http://i.imgur.com/WTH7YMU.png) The way to make an LED blink is to have a section of time where it's on and a section when it's off. So I made an Elapsed Time VI, where the input is 1 second. This means every 1 second, the Elapsed Time value will reset. During that 1 second, it is outputting a numeric value from 0 to 1. I then divide that 1 second value by 2, then switch the LED to True after 1/2 second elapsed. If you have LabVIEW 2015, you can drag the image above to your block diagram to import the code. --- If you're trying to adapt my True/False case to your multiple cases, you can have each individual case cause a different LED to blink and set all of the other LEDs to False. Make sure you include the Wait function like I have in my code. This keeps your code from running infinitely fast and hogging all your CPU.
Tried it in my VI and.. it works like a charm! Thanks! I was wondering though, why does your "elapsed time" look like its.. compressed? How do i make it bigger so i know what you connected the 1 to and what your output is? [like so](http://imgur.com/8OA60Cd)
ok thank you. The coordinates in my case are shown as integers. Is there a possibility to get it as a floating point number? edit: I mean that I not only get values like 1;2 or 1;1 but also sth like 1.01,2.49 etc.
Record type is the cluster that defines the data on the front panel. Here's an example where I know the record type: http://i.imgur.com/NMohjQD.png
Thanks for the advice. I'll probably check it out since, at the bare minimum, i'll get to network with other LV users in my company who i haven't met. 
I see. I thought you might have been referring to something like sampling parameters or whatnot. I don't use datalog files, but this problem looks to be very similar to the issue of trying to use LabVIEW to blindly parse XML files. Speaking from firsthand experience, it sucks utterly and I've not had any success with it. You have my sympathies, sorry I can't be of any real help.
Man, you need to get some new minions.
Well, thanks for the gold, kind stranger! I've been a LabVIEW developer for almost 10 years, the bulk of it was with an NI Alliance Partner. I've done a slew of different types of applications, both Windows and RT-based. Process control &amp; measurement, data acquisition, multi-axis motion control, component test systems, etc. I was a CLD for most of the time I was with the Alliance Partner, as well as a CPI (taught LV Core, LV RT &amp; FPGA), but I've let me certs lapse since my current job doesn't require them. I work for a vehicle OEM now, I run the hardware-in-the-loop simulation lab. The heart of my systems is NI Veristand, but I also use LabVIEW, TestStand, and DIAdem. There are LOTS of LabVIEW jobs out there...
It's pretty simple. There is a web article on ni.com on how to set them up. I can try to find that for you later (or a search will find it). 
How far have you gotten so far? What kind of LCD display are you using? (More importantly what interface? USB, RS232, etc.) Do the button presses need to be simultaneous or in a certain sequence? If not, is it any two buttons pressed at any time? Here's a tutorial on state machines to get you going: http://www.ni.com/tutorial/7595/en/
I use a Digilent PmodCLS. I also found that tutorial. Ive problems to bind the the buttons to an analog input of the MyRi I communicate with the UART 
You camera needs to be setup to take a grayscale image. There may be a cast in the acquisition process to handle mismatches.
Find your camera in MAX or in the control software provided with the camera. There should be a setting for the bit depth and grey vs color images capure. Or look in the manual and you can use the camera setup VIs to set the desired parameters in you camera.
Unfortunately I don't have experience with the RIO side of the world. I will point out that that performing a /reinstall is more effective and quicker than uninstall and reinstall because the installer replaces everything as if nothing is on the system: http://digital.ni.com/public.nsf/allkb/ADD22E807D5A12AD862579EC00760F79 Can you also open NI License Manager, take a screenshot and post it here? Expand the collapsible fields if needed. 
Hi, thanks for the reply. The complete reinstall was undertaken on the recommendation of my local NI Academic Field Engineer, in case my original roundabout way of installing (LabVIEW and drivers all separate downloads from NI.com) had missed something that was included in the specific myRIO software bundle. [NI License Manager](https://www.dropbox.com/s/h9ar9aklvlvuyb9/NI_License.JPG?dl=0) screenshot as requested. Thanks for taking the time to try and help me.
No problem, thanks for the suggestions and I'll consider that method in the future. Indeed, I think I have everything installed so I'm a bit baffled. Will wait and see if anyone else has alternative suggestions. Thanks again
I had loads of problems as well and LabVIEW support was no help. Turns out it was my Ethernet drivers. Uninstalled and reinstalled the latest set and got everything working in 2 minutes after about 3 weeks of trying. Might not necessarily be your issue but its another road to look down.
Hey, thanks for the message. That's really interesting; before I give that a go, what motherboard/ethernet ports are you running? If it's killer networking MSI stuff, I may have to try that. Although I'm puzzled as to why that would be the issue with a USB connection.
Yep. It was killer networking. Im using an MSI Apache laptop. I actually had to uninstall the suite and just install the bare drivers. the killer suite assigns weird priorities to network traffic and it was blocking comms with my cRIO. Then again I'm using a networked connection so I'm not sure if it could still be the problem over USB. Quite a coincidence though - probably worth a shot.
Welcome to the wonderful world of LabVIEW RT. Like, I said, I've done tons of RT and RIO dev work and you're seriously dealing with the hardest part. Once you get connected and working, everything seems to work properly. Hopefully the hard part isn't too difficult to deal with.
Installation order matters. My suggestion would be: * LabVIEW 2015 * [LabVIEW Real Time 2015](http://www.ni.com/download/labview-real-time-module-2015/5451/en/) * [Compact Rio Drivers Aug 2015](http://www.ni.com/download/ni-rio-15.0/5363/en/) Your messages make it look like one of those two later items is missing BTW, your MyRIO is running LabVIEW 2014 SP1 software. LabVIEW 2015 wasn't out yet in Feb of 2015 
Hi. I believe I actually did install in that order originally, by pure chance though. I installed LabVIEW first and then when I began having issues, found the specific Real-Time driver you've linked. And then the RIO drivers. That's just from memory though so I'll look when I'm home later. I did however uninstall everything NI and then re-install the myRIO software bundle, so I'm not sure what's still included. The only other thing I can think to mention is that I specified an install location on my hard drive, but program files have still been allocated onto my SSD boot drive (limited space, hence the HDD allocation). I don't know if the split location is relevant; it shouldn't be in my mind but I thought I should mention anyway.
That's okay. Just post the screenshot whenever you get a chance and let's see what's installed right now on your system
Managed to get home a bit earlier so he's the [screenshot](https://www.dropbox.com/s/sqgsi80hg6vywcm/MAXSystemSoftware.JPG?dl=0) of what's *now* installed on my system. Whilst taking this, I noticed the [VISA warning](https://www.dropbox.com/s/b9j2uw73pyrd09s/VISAWarning.JPG?dl=0). Thought I'd post a screenshot in case this was somehow related. EDIT: to clarify, Real-Time is version 15.0.0
Yeah, you definitely want to install the three in the same directory. The latter stuff installs stuff in the folder of the former and the former auto searches that folder when it starts up.
This can easily be done if your camera is fast enough to avoid blurring. I highly recommend using a backlight as your lighting solution. The Vision Development Module has the tools you will need to achieve what you want. Blob analysis and the particle report vis will provide droplet count and size. Linking trajectories together to determine velocity is more difficult and will be much easier if your camera is fast enough that they do not move very far between consecutive frames. Good luck!
You might want to take a look at the book: "Fundamentals of inkjet printing: the science of inkjet and droplets". I want to do something like you and the book helped me a little bit (They had a chapter about calculating droplet volumes). Theres a function (Clamp (Rake)) in LabView with which you can measure along the edges of your droplet in order to get the radius or diameter. The velocity can be calculated if you have to images at different times. My approach would be the following: Getting two pictures at different times. Get the coordinates of the center of mass of your droplet (if it has a spherical shape) and calculate dx/dt. I hope this helps 
I've just been working through reinstalling everything into the same directory. I originally had LabVIEW and all real-time and RIO drivers on my HDD but program files had still been assigned over to my SSD boot drive. As per the below advice by u/infinitenothing I moved everything over to the boot drive. I'll have a look at getting that report sent over to you later today. Thanks!
Okay, it looks like everything was installed in the correct order, except the myRIO Toolkit failed to install. Try doing a "force reinstall" (google) of the myRIO toolkit, make sure it succeeds (you can send me another Tech Report if you're not sure), and then try adding the myRIO to the project again! http://www.ni.com/download/labview-myrio-toolkit-2015/5518/en/
No problem. If you look up the thresholding/blob analysis examples that should help. From the particle reports VI you will get a size and center-of-mass for each blob (droplet). You can then link the xy coordinates from the center-of-mass outputs to form your trajectories and calculate the velocities. In this case the quality of the imaging is important. What camera are you using? Is this a homework project or a real-world project with money to throw at a decent camera?
I suspect you have a bug upstream which sometimes makes the last string in your array empty. That function places commas between array elements, never at the end. 
I also write angry comments on LabVIEW. Glad I'm not the only one. 
Nope, it's a composition pattern tree structure and this is on the return of the name for the composition class which is composed of either compositions or leafs (see my example below). However, there's a special case that the group has added (I stress group because I was against the idea) where the last leaf on a branch might be an empty name to be hidden (why not just call it 'hidden' you ask???), which in that case causes an empty string in the array, rather than a null value. Hence, comma, nothing at the end of the string, sometimes. e.g. I can call 'Get Name' on any composition and get either a single name or a comma-separated list of its name and its childrens' names.
Ahhhhhhhh, my eyes. No seriously, I hate Fourier and everything he's done for us so I'm no help here.
Not to mention regex is slower than what you did.
Can you record a large dataset and play back the actual data?
Well I have to emulate an oil rig so I'm trying to emulate the variables in such a way that there's a chance my lecturer will see some relatively realistic looking out of spec alarms during the 5 minutes he's looking at it. But I wouldn't really be able to use the method you are suggesting in this case I'm afraid. Really appreciate your response though. Thank you.
Can you record a pot into a DAQ input? Or even a simple desk top adjustable power supply would work. Record it high speed into DAQ as a wave form. Play back as needed. Adjust power supply to simulate levels. If you need greater than power supply voltage just do a multiplier. 
What do you mean by first few samples? The spectrums from a time interval? Or the first frequencies for the whole acquisition? From my understanding, if you want to isolate some of that data, you should use array subset and decide if you want to isolate data on the rows or columns an check for peak values.
Not really. I only have access to the software but you and u/AntiGravityBacon have given me an idea. I could record data from a slider in a while loop in labview and build an array that outputs excel. Then I would have all the data I wanted. Do you think this would work?
Yep. That could work too. 
Stuff like this is why i absolutely loath labVIEW sometimes. 
Can you post links to your code and some example images? Template matching will work poorly if the parts are jumbled around and not lying flat, but it is usually pretty robust with rotation of the parts around the viewing axis. As you have alluded to, lighting is always key to these types of things. Although you may not need a backlight, uniform lighting will be extremely helpful.
You may use Sort 1D Array and Search 1D Array functions within a loop to find the students rank. Can someone explain what they mean by this?
Hey! Thanks for your concern. I somehow managed to get decent accuracy on the classification, thanks to over 100 sample images with lighting continually changing 360 degrees across. If I am not wrong, doesn't adding rotation to pattern matching add crazy processing time to the entire process? Geometric match would be much faster and efficient I believe, if only I had the proper lighting required.
The processing burden depends very much on the context of the image. NI's approach to pattern matching has some tricks to make it run reasonably quickly, although what defines 'quickly' is relative to your application. I tend to stick to geometric tools if I can but I have had good results with pattern matching in the past.
Me too. I think this is a perfectly good way to do it, especially if you are communicating with devices with clear initialization and closing phases.
Hey there! I could call them and ask, but I'm sure they'll send me a link to buy the connector from them - as much as I like NI, I'm a poor student and if I can get the same connector elsewhere for a cheaper price, I will :P
Right, but you don't want the connector they sell. You want the PCB version ( if such a beast exisits). I haven't purchased the connectors through NI, I purchased through an electronics distributor.
Yeah that's right, I actually have 3 of them here (already soldered on to a board I mad previously) but I was given them so I don't know the name!
The NI part Number for the MSP connector is 765788-01 Per http://www.ni.com/pdf/manuals/376047a.pdf [Page 20] It is the Same Connector As the myDAQ So you can check out the myDAQ accessories. https://www.sparkfun.com/products/10705 Is A Prototype Board that will work with both. It has the Connector your looking for. In the Comments they Refer To this part http://www.4uconnector.com/online/object/4udrawing/10458.pdf Its a 20 Pin 3.81mm Plugable Screw Terminal Plug Another Option for the part would be http://www.on-shore.com/sites/default/files/manuals/ostv7xx3151_0.pdf 
Most of the similar connectors on the cRIO/cDAQ modules are indeed Phoenix Contact. The one you posted appears to be as well. Look at the connector you have and see if there is a series type molded into the housing, then search for that on your favorite component vendor's website.
Thanks for your reply! I certainly like the idea of neatening up the error wires, I also didn't realise I was losing the error data by not having shift registers. I'm not sure I understand how the rest of the program would look. Would I have three loops, a main simple state machine which has code to execute temperature profiles and monitor user inputs, and then two other loops, one for the temperature controller and one for the voltage reader. I am still unsure on how to act on the instructions from the main loop. I communicate with the temperature controller using visa read/write commands, which I will only be able to send from the temperature controller loop. How do I get the instructions out of the main loop and into the temperature controller loop? Won't I need another queue/global variable etc? 
I'm not sure what you think is dated about the NI forums, but they are certainly far more active than this subreddit. Anyway, even without searching I remember people posting demonstrations of interacting with Google Maps, I assume either by using an embedded .NET browser control or by installing Google Earth or by using the API which I assume it has. Search there and on LAVA. If that doesn't work for you and you have Java code, you can talk to it by implementing a TCP protocol (probably the easiest way). If you can't change the program, you will either need to use whatever API it does provide (assuming it does) or embed it as a child window using the Windows API reparenting functions (something I would suggested avoiding).
For me the biggest issue with flat sequence structures in code written by others is the damage they do to readability. Too many of them and it becomes a serious chore to even figure out what's going on. Although sequence structures in general are undesirable due to there ridged execution order.
I tried, no such luck. The VI just goes "Not responding" and I need to terminate it.
I used the rank and made another array with which students index has the top, then second best, and so on by sorting. Then using the search array function to search the name then write their rank beside their name in a string array.
I prefer to use a User Event for this. I'll see if I can't get you an example when I get home tonight.
I can't believe nobody's mentioned timing yet in this thread. For evaluation of timing, there are no alternatives to sequence structures. I use them in my code to make good diagnostics.
I've attached an example VI. Play around with it, use the Highlight Execution to see what's going on under the hood. Let me know if you have any questions! https://www.dropbox.com/s/nvy7nq04d8352d3/producer%20consumer%20example.vi?dl=0
I've run into these problems before. Reinstalling VISA helps. Also, check that the COM resource isnt being seized by something else.
Can you show use the code you currently use? This is pretty vague, so without more details we can't help you too well.
You could create a data structure that is an array of clusters where each cluster is a "step" in your sequence. A producer consumer design pattern would work best where your producer event structure takes in user interface commands like start,stop, pause and build sequence. Your consumer loop would run a state machine based sequencer with all of your motion controller communication and timing. The two loops should share information by queued messaging and the consumer loop should have a VI to control your current or next state direction based on the incoming messages. If the motion controller needs separate timing for synchronous communication, you will have to create a different consumer loop and message handler for it.
You can use a "flag" that is just a Boolean to store whether or not the previous data point was below 100. Your logic becomes "if the last data point was below 100 AND the current data point is above 100, alert the user." Here is a screenshot: [http://imgur.com/MzCr6Qu](http://imgur.com/MzCr6Qu) Good luck! **edit** The False case of the flag case structure is the same as the True. You could actually get rid of it, I just put it there to demonstrate the flag a little clearer.
Thank you very much! I have been hung up on this for days. Much appreciated! :)
How are you determining depth? 
No need to be stuck for days. There's a community here, a larger one over on Lavag.org, and an even bigger one on forums.ni.com. Ask away, just be sure and provide enough information to make helping you easier.
Yeah OP, this is a much harder problem than you make it out to be.
It certainly can be. You COULD do something like the SICK cameras do and you could make your 2-D image a depth map - which allows you to construct 3D images from it (shadowing being a major problem - if you had an umbrella shape - you couldn't know what's under the umbrella). Guess that's always a problem with a 2-D representation of a 3D space - part of your view will usually be occluded. 
The Kinect uses time of flight pixels and receivers to so just that. It's a lot more complicated than LabVIEW could handle, so good luck with that.
https://imgur.com/a/RP85R Sorry the upload took so long. Its really just a basic program, and unfortunately im not incredibly well versed with labview and its structures, so any help is greatly appreciated.
You could just have a "Move Steps" control, and when there's a value change, it executes the "move 1 step" control in a for loop, using the NewVal to set the number of loop times. edit: although, that' seems like what you already have... do you just want to queue up commands and execute them all at once? have a preprogrammed "Move 20 up, 20 right" type of thing?
What would be the ideal? Just a text box where you put like "u20w5000d20" and click go? Or something else?
Honestly, any scenario which would allow me to queue up the commands is completely acceptable. If its as simple as a text box with an input like the one you mentioned then that would be even better. But im completely open to all solutions
Check my other comment for a parser that does what I said. 
That looks great! I wont be able to test it out until tomorrow as I only have Labview on a different computer. I see where you mentioned to insert the "move stepper motor up VI". Should I save the original program as an entire VI and insert it there, or just insert part of it into the loop?
Just go ahead and PM me a Dropbox (or equivalent) link. Make sure to duplicate the hierarchy else I won't be able to see all the code/fix it properly
What problems are you running into? What have you tried? We can try to help with specific issues, but we aren't going to hand you your project. &gt; This subreddit is not set up/designed to do your homework for you.
At first we attempted to remove every portion of the example having to do with anything but text transmission, simply to get an idea of how to set up the communication between the two radios, that didn't work as the example began to fail. The example also wasn't working on two different computers, I'm starting to think that's because they have different internal clocks, we don't have a MIMO cable to hook the two up. I had looked around on the internet and many people kept mentioning some example that does video streaming and have tried to find it but this video link was/is the closest thing to it. My biggest question at the moment would have to be, is there a block or something within LabVIEW that can get the bits from a video file, my plan is to splice a video into smaller videos, convert those to bits and the bits to packets which I could attempt to stream and reconstruct on the receiving end. Does this seem like a good plan of attack? 
Thank you for your help! I'll be heading to the lab to check these out. I'll look around for those blocks in the meanwhile. 
As soon as you sent it, I found that exact link. When I posted this I had googled it and found all the other LeCroy ones but I had to search within Ni and finally found that exact one. Thank you so much!
Oh my gosh, totally! How can people look at their spaghetti diagrams and not think that there might be a better way? Tidy block diagrams FTW.
To be fair, NI specifically pitches LabVIEW as a solution for people who don't know how to code. The whole idea behind the graphical programming is to make it friendly for someone who is not comfortable with a language like C++. The selling point of LabVIEW is exactly "In no other programming language can you know so little and do so much." 
Funny. I know a lot and yet always seem to not get labview to do what I want. :-( I really prefer written code.
Is it possible to write G code directly and avoid all that block diagram stuff?
G is the name for the graphical programming language you use in the block diagram. There is always the LabWindows/CVI which uses ANSI C.
Fair enough, but I've seen people put on their resume "X Years of LabVIEW experience" who also just used a program someone else wrote.
You can do exactly what you want with a "user event." Simply create a "user event" outside of your three while loops. You'll need to base it on a specific data type of your choice and make sure that data type you input to the user event creation primitive is named. If it isn't named, you'll get a broken run arrow. Register this event with the "register for user events" primitive, and right click on you event structure you so you can display the user events terminal. Wire the output of the register for user events primitive to this terminal, and pass the event itself to the third loop you are trying to create. Inside the third loop, setup your timing, and wire the user event into the "generate user event" primitive. You'll have to feed this primitive data of the same type you used to create the user event. Voila, you'll trigger events in the event structure pragmatically. Be sure to do good cleanup, and unregister the event and destroy it's reference once all three loops finish running. edit1: Any reason you can't have your two event structures exist as separate cases of the same structure? Your two strucutres must exist in different VIs, because as I recall you can't have more than one event structure in a given VI.
It's not that they don't know that there is a better way. It's that learning the better way takes longer than doing it poorly once.
And the pitch of my independent contracting business is that my service exists to fix your 'I didn't know how to code' code and replace it with "now I don't need to know how to code' code. I try to look at a customer that has gotten themselves in over their head and remove the blockages they have created. Typically they are left with software that allows them to modify things without the need to even crack open the IDE. 
This is the same problem with python too. Too easy to do it wrong. Sigh. 
This may have been *somewhat* close to true when LV first came out, but it's in no way remotely accurate now. Not by a long shot. If you honestly believe your opinion to be correct then you really need to start doing some reading. Take a look at the recent developments with the Actor Framework and everything GOOP-related that came before it. There are tons of examples out there proving that while yes, LV is still easy to use for quick-and-dirty DAQ applications (ugly, poorly written ones), there are just as many core programming concepts integral to a properly written application in LV as there are in any other text-based language.
I understand why, but it seems to me that your interpretation of NI's sales pitch is a bit superficial. Just because they're saying that it's *easier* to create a complex application in *less time* than it takes in another programming environment doesn't mean, as you put it, "Anyone can use this software, etc". I'm not trying to defend NI's marketing department, trust me they've annoyed the snot out of me on multiple occasions (incorrect/incomplete hardware specs on the website, anyone?), I'm just eyeballs-deep in it everyday and have been for nearly 10 years, along with teaching it quite a bit. As such, I see their advertising as asking busy engineers and scientists to consider how much more quickly they can develop a complex application to accurately measure data and present it in a visually appealing and understandable way. Oh gawd, I'm utterly smashed on the NI Kool-Aid...
&gt; What I see as the real strength of LabVIEW is quick and easy it is to make functioning data acquisition applications. &gt; If you were tasked to build an application that reads a temperature from a sensor, logs it to file and plots it on a graph on a user interface that might take you maybe 10 minutes to do in LabVIEW. How long is that going to take you in C? Probably longer. I think this answers your own question. The task is much simplier in LV so if you have a user with no experience to start they are more likly to succeed with LV in making that program. They'll need to learn some programming (like loops) but within a couple days and some tutorials they would have that program you described. It would be months before they could do a GUI and instrument comms in C++/Java/python. It wasn't saying someone that has no experience can write that while still having no experience. Someone can write that quickly starting from no experience and *learning on the way*.
Why not just add a case structure that executes every n iterations depending on how long your loop time is? If your loop executes every 10ms then execute the subVi every 50 iterations. edit: Can you post an image of your block diagram?
myRIO libraries aren't compatible with Mac. Or a lot of other stuff for that matter.
I went ahead and e-mailed a contact that I have at NI to see whether he knows anybody there who would be interested in developing something. It's a long shot, but here's hoping we get a viewer in the not-too-distant future.
Um, no I'm not, but you're entitled to your opinion. I don't see how you can speak for "most" LabVIEW users, but the vast majority of the ones I've met, and there have been MANY, do not fall into that category.
What you want to print to the serial line is a new line character (0x0A). In Matlab, you accomplish this by typing "\n". The fprintf function knows that when you type this, you actually want a newline. In Labview, string constants by default are ASCII inputs. this means when you type "A500\n" in will output 6 ASCII characters, the last two are backslash, then a lower case 'n'. To get LabVIEW to use slash codes for specifying special ASCII characters you have to right click on the string control, and click on the "Slash Codes" item. By default it's something like "Normal". Now LabVIEW will interpret the "\n" as a newline character.
Thank you :)
it is just a different way of seeing AND inputting the data. Some views are easier to input things than others. For example, you type: "Hey ho" into a normal string input box, then change it's display format to Slash codes it will show "Hey\r\nho". Notice how it inserted two new characters. If you're in slash codes mode and you type "\\n" into it, then change it to Normal display, it will display "\n". I had to escape the \ character to get it to show up. This type of stuff comes up in other high level programming languages. From the strings article for C# &gt; //Initialize with a regular string literal. string oldPath = "c:\\Program Files\\Microsoft Visual Studio 8.0"; &gt; // Initialize with a verbatim string literal. string newPath = @"c:\Program Files\Microsoft Visual Studio 9.0" 
you can write vi scripting code that exports pictures of block diagrams with not much effort.
&gt; The fprintf function knows that when you type this, you actually want a newline. Actually it's not the `fprint` function, but the source code lexer that parses the string literal and applies escape code conversion. The fprint function never even sees the `\`, it only sees the converted newline character code.
As it turns out, it still doesn't work. I was certain that all it took was assuring the data being sent was the same. As it turns out, there is still something wrong, obviously. Best guess, in how the transmission is actually handled. Here's the full dump of both communications, both by sending the same commands through matlab and labview. labview: http://pastebin.com/TiBSR1Nw matlab: http://pastebin.com/jXmFbfYP As can be seen, the data is correct now, the most striking difference is the fact that the matlab one sends a whole lot of &gt; IOCTL_SERIAL_SET_WAIT_MASK in between each call. Hopefully this sparks something for someone because I'm honestly out of ideas right now. This is incredibly frustrating.
These devices require NI-DAQmx. Currently the only version of NI-DAQmx support on Mac is a reduced subset called NI-DAQmx Base. [As of the previous release of Base,](http://search.ni.com/nisearch/app/main/p/bot/no/ap/tech/lang/en/pg/1/sn/catnav:du,n19:MacOS,n8:3478.41.181,ssnav:ndr/) it does not support the 6001/2/3.
I'm a fan of SVN. Viewpoint Systems makes a [nice toolkit](http://sine.ni.com/nips/cds/view/p/lang/en/nid/211524) that puts the icon overlays right in the project window. Bitnami offers [free VMs and installers for Subversion](https://bitnami.com/stack/subversion) as well as other open source tech stacks and I've found their stuff to be very useful.
You should architect the thing in a way to split it up into small blocks everyone can work on individual blocks. Figure out the messages that should go between the blocks and use some sort of framework like actor framework or aloha
Team development with LabVIEW sucks, there are ways to make it work, but it still sucks compared to what you'd do with a text based language. This is because the files are binary and LabVIEW tries to update dependent files whenever the hell it damn well pleases. I'd recommend subversion or git and use branches and appoint a guru to handle merging. Github and pull requests will make your life the easiest, they offer private account for like 10-15 a month. Always use the LabVIEW diff tool when doing diffs or merging. Subversion with locks will keep merging to a minimum. With git your number of Conflicted files will be based purely on discipline and prowess, or lack of it. 
Yea, I've tried to convince our CTO to let us switch to programming in Java for this very reason. Thanks for your advice.
Definitely going to be looking into subversion. I've been looking for options that I can integrate into LabVIEW, thank you!
Yes this is for FIRST. We program our robot with LabVIEW, and our team leaders (students) take on roles much like a business so I would have to run this by our CTO who is essentially in charge of the programmers. I also share your love for python, but it doesn't seem like the way to go with programming our competition bot. But while you're here, what would you think about switching from programming our robot in LabVIEW to programming in a text based language such as Java? The only reason we haven't switched already is that people believe it'd be much harder to teach new programmers.
The Boolean array is a good idea! Thanks ill give it a shot with the timers! 
[Why not something like this?](http://imgur.com/vOkrw58)
I think that the simplest way to put it is that any problem can look overwhelming if you only ever see the high-level, so, inversely, any problem can be solved if you make it into small enough component problems. For example, you have a daq on your desk and your boss asks you to write a program that measures a value once every five seconds and writes it to a csv file with a timestamp. Once an hour, email that file to all of the board of directors, than delete it if the email is successful and start over. That is a large problem, but can easily be broken into component solutions. How do I measure a value? How do I time a measurement? How do I write it to a csv file? How do I get the timestamp and format it? How do I time an hour? How do I email a csv file? How do I handle if an email fails? If the problems are still too complex, break them down smaller, how do I open a csv, how do I close it, etc... But the real breakthrough came when I stopped thinking about how to solve these problems in LabVIEW and just started thinking about how I'd solve them in general. Before this, I would spend a lot of my time writing something in LabVIEW and than throw it away, start over, repeat. I found that the biggest reason I did this was because I was trying to either solve too large of a problem or trying to solve it in LabVIEW. LabVIEW isn't a solution (neither is C, Python, Java, or any other languages). They are tools. The solution is a generic method to use to solve the problem. The tool is what you use to implement the problem. Let's look at the above example. One of the problems we discovered is how do I write a value to a .csv. The solution is to format the value, open the file, move the file pointer to the right place, append the formatted value, read the file to check it, close the file. I now use my chosen tool to implement the solution. This way my code became much more directed towards using functions to solve problems and less rambling all over the place. I could also write code more modular because as you break problems down into base components you begin to see that so many large problems consist of a lot of the same building blocks (OBJECTS???!?!?). My LabVIEW code got so much cleaner and nicer when I stopped using it as a solution and started using it as a tool to implement solutions. In turn, my job got much easier when I learned that LabVIEW is only one of many tools, all with different things they're good at and things they suck at.
HA. Simon says!
I'm on a team of ~12 developers using LV and C# as primary languages and perforce as our source control. We do get conflicts sometimes, but they're minimized by: * Architecting everything into components that are not tightly coupled so that individuals can work on components without affecting each other; this can be accomplished a number of ways...we're using LVOOP with top level components that are somewhat stable, and have some VIs that wrap dynamic dispatch VIs from other components that have no actual implementation and are filled in by the other components' children...these VIs are what link the components together * Separating our branches so that we have dev, trunk, and exports for example. Once a component is semi-stable, it can go to trunk or an export so that when someone later churns on it and has partial submissions into dev, it doesn't break everyone who depends on it through the trunk or export * Coordinate with others when you are submitting something breaking * Don't submit or lock something that you don't actually need to change * Submit often so that your workspaces don't get out of sync * We have several internal tools that make the process a bit easier LabVIEW is admittedly more painful for this than C#, but we have the same issues with our C# code that we do with LabVIEW...just to a smaller degree. 
This actually the topic I am presenting on at NI Week this year. It's a crap shoot because the merge tool doesn't actually merge anything. We use git internally and this https://github.com/joerg/LabViewGitEnv to integrate the LVCompare and LVMerge tools into git. Additionally, we practice git flow for branch management. https://danielkummer.github.io/git-flow-cheatsheet/ Any APIs we develop are turned into VI Packages for installation through VIPM and we use goPyxis (dropbox alternative) to host all our VI packages so that VIPM auto-detects updates for us. Make sure you have "Separate compiled code from source files" checked in your environment properties. Otherwise, labview is going to touch everything and cause your source control to pick up file changes that don't exist.
Thank you for taking the time to give such an elaborate answer Glorypants. My problem was solved by scrapping the event and while loops and utilising a Feedback node outside the case structure. I will check out the State Machine, it looks like it could be useful in the future.
Evaluation would run fine, I suppose? 
The CLAD is all multiple choice so it's all basic knowledge of what LabVIEW can do and general names of functions. Make sure you know how the different front panel Boolean controls work (latching, switching, etc). Understand basic data flow and the difference between While loops and For loops (For loops can run zero iterations, where While loops always run at least once). Look through the pellets and get a general idea of how some basic functions look so you can name a few if they ask. I've known multiple people to fail the first time, so don't sweat it. It's mostly luck of the draw because sometimes they have tricky questions. Really the best advice would be to take some Core 1 and Core 2 tutorials to get a full scope.
Here are some sample exams: And here is a 
Highly recommend going through all of this content: http://www.ni.com/gate/gb/GB_EKITCLADEXMPRP/US The "most missed concepts" video in there is an absolute must watch. It covers all the tricky little detail crap thats on the CLAD. The clad isnt a test for how much labview you know, nor is it particularly hard, but it is very tricky/detailed and you HAVE to know what to expect on it. People who study pass, not people who necessarily know labview really well. 
Thanks for your reply. It's an interesting perspective, for sure. It's actually not too far off from something called systems engineering. Systems engineering's main concept is to break down a system into its requirement, which then in turn breaks it into components of the system, again, not unlike your solution-based strategy. I'm not sure how familiar with SE. Definitely take a look at it and its standards if you're curious.
Bootcamp + windows + labview 
The way you describe your problem makes not perfectly clear what you aim for. But if your problem is, that you essentially need two loops that run in parallel you you have pass data between the two, you should have a look at *Queues*. One loop can add elements, for example clusters containing commands into the queue and the other loop can dequeue those elements (in order) and execute. http://digital.ni.com/public.nsf/allkb/DD7DBD9B10E3E537862565BC006CC2E4
you do this with loops nested in conditional structures. you might be a long way from kansas though. 
so what we created was nested case structures to determine each type of marble available to us. Once it was identified, we presented a message on screen displaying what it was. But now the task is not to present the user the type of marble,but to determine if it will be used according to the barcode, and if so, how many of each 
I probably should have mentioned it involved Lego NXT earlier
To answer your TL;DR, yes.
Actually I'm not even technically seeing a null, I turned on show slash codes and am still not getting anything out of the buffer
Using Windows API to access windows resources is quite common. You do this using the appropriate DLL for what you're trying to access. [Here's a resource explaining this.](http://digital.ni.com/public.nsf/allkb/09057CC08746FD47862566E800619B12) Next, you will need to find the DLL associated with the accelerometer reading functions. I don't have a surface tablet, so I don't have this DLL on my computer. Maybe searching around your Windows folder or Google, you can find it on your tablet. [Here is a resource for the Windows API functions.](https://msdn.microsoft.com/library/windows/apps/br225687) [Here's a resource for doing that actual implementation of a DLL call in LabVIEW.](http://digital.ni.com/public.nsf/allkb/DCB90714981A1F148625731E00797C33)
Thank you!
A quick a dirty way is to use a tab indicator, hide the tabs and programmatically set the value. The more involved method is to use sub panels. I don't write with them but I work on code with them. Look around on ni forums and in the example finder.
For this, I would either use subpanels or asynchronous calls to GUI VIs, with "call and forget" http://zone.ni.com/reference/en-XX/help/371361H-01/lvconcepts/asynchronous_vi_calls/
Thank you for the reply! I'll look into that some more. Side-question: Would you have any suggestions for writing 2 lists of values obtained from a DAQ (Flowrate and Head) into 2 excel columns? I have both values coming in as doubles, then bundled together, but I can't figure out how to write it to file.
The nuts and bolts have been covered already, and of the three solutions presented, I use two of them on a regular basis. Those two being the tab indicator and sub-panel methods. The first is the easiest, but what was omitted from the explanation was the fact that you'll need to use an Event structure to capture the button clicks. This can be done in a couple of different ways, either single-loop or two-loop. Single-loop is the simplest, if you go with two loops (my preference), you'll minimize your code cohesion and improve your modularity, but you'll also need to use either a queue or notifier to pass data from your Event handling loop to the control loop. There are a bunch of examples for this sort of thing in the Example Finder (Help -&gt; Find Examples...), take a look at those. The single-loop method would dictate that you have a simple state machine with the Event structure in the Idle state. There are plenty of state machine examples in the Example Finder as well. As to your question about writing your Flowrate and Head values to an Excel file, that's very simple, you just need to use a Write To Spreadsheet File function, found on the File I/O palette. Just use array functions to arrange the data to suit, add headers, etc., and then write the resulting 2D array to the spreadsheet file using the aforementioned function.
I just took it without any coursework and passed with an 80. All I did to prepare was review the two sample exams available online. I did have some (non professional) experience from messing around with robotics. Read carefully and use your time wisely 
Oof. LabVIEW is hard. I'll try my best to understand and implement your method. As for the excel issue I was having, I finally figured it out. IIRC, I converted the waveforms to doubles, clustered those bundles, then ran them into a write to file. It works.... usually.
Would using. Xlsx files from the write-to-file wizard be the reason excel would say the file type is corrupt or incompatible. Should I use tab delimited instead? 
Do you mean the "Write to Measurement File Express VI" configuration? In newer releases of LabVIEW, there is an option for .xlsx. If you are selecting that but are seeing corrupted files come out of it, there could be some other problem going on.
You could use Parallels. I have a MacBook that I take with me when I have to go in the field. I have a Windows VM through parallels on it that can do everything a native Windows PC can do.
It works, just strangely. It generates a file, then generates file_2 with the data in it.
That's a simple way to do it, but not very memory friendly. A more rigorous way to to it would be to use an event structure to handle the recording and timestamps to measure the time between each event. Then that could be played back in a consumer loop or something like that.
Agree 100%, however, I tried to make it look close to what OP had already figured out. I thought throwing event loops at them wasn't such a great idea. Me, I'd probably wrap the whole thing in some Playback class.
It can be, at first. But, there are TONS of great examples in the Example Finder, make your life easy and get familiar with it.
I found a VM that had old Mac with LabVIEW 1 a long time ago but not sure what I did with it. While it was a nice nostalgia trip to see where LV started, it was so bad and lacking of the critical features that it was more frustrating than valuable. 
I'm not expecting to do something serious with it, just want to play with it :) I'll keep looking.
It may help to share you code because we can only guess at what you're doing. If you're plotting XY points then you're going to want an XY Graph. A Graph (different from a Chart) does not keep a history of the data. You will need to keep your own buffer of the data and write it to the graph each time. If you are within a While loop, use a shift register to store the data in memory from iteration to iteration.
Yea, I'm not experienced with LabVIEW but I figured Express VI were probably resource hogs and not an efficient way to do thing, just the simple way.
This, but as a general rule, most DAQ cards aren't meant to source much current.l
You're looking for a shift register: https://gyazo.com/3bec2c209e78f43e6e8dc67fcc0e045e The shift register (green block on the outer border of the while loop) keeps track of something. In this case, it's "whether or not x has been greater than 5 in the past". If this is true, OR (thats the little sideways V thing) x is currently greater than 5, we display 1 on the integer. Else, we display 0. The 1000 thing ensures this only executes one time per second.
...wut
Can you post your code?
Thank you, I did a little bit more research and I think that's exactly what I need, but I am having a bad time trying to implement that to my code, after many attempts i can't seem to get it right. I am trying to create a program that does a kind of fatigue cycles, I define a max strength (Fmax), min strength (Fmin) and the number off cycles, and i want it to do: if F is less than Fmin - ligth led 1 (this led simulates an output to a motor or something like that) if F is greater than Fmin and less than F max - keep led 1 on if F is greater than Fmax - ligth led 2 if F is greater than Fmin and less than F max - keep led 2 on it's a sort of strain/release, strain/release, strain/release cycle I would appreciate any help i am really stuck on this one.... 
Of course! There's another free course next semester for that :)
FUCK YEAH!
So what would be my class? One of those two main clusters? Or something more abstract that that? Also, do you have any references on OO with LabVIEW? I've done things almost 100% procedurally to this point because I'm weak on the OO stuff.
Congratulations.
Maybe you're treating it like a chart? That is, adding a few points at a time hoping that the graph remembers previous values? If so, graphs don't work like that. That is a chart behavior. Also, XY graphs by default plot the points in the order that you feed them and draw a line between consecutive points. So you may need to sort your input arrays prior to graphing if you're getting lines all over the place. The other option would be to edit the plot style and turn the line off and just plot points. 
Ah, yeah I figured you were doing analog output. That particular DO module has its own power supply, which is why it is able to source a lot more than it would be able to otherwise. Glad you were able to come up with a solution.
You could create a generic "Function" or "Action" class or something to that effect. It would be the ancestor class for the other functions that you are basically doing: the Pull, User Input, and Hardware Interface. The ancestor class would have either no private data, or only the private data that is common to all three of the child classes. In the ancestor class, you would create methods for things like Initialize, Shutdown, and anything else that they *all* need to do. Then you could override those methods in the individual classes. Doing it this way would allow you to keep some amount of independence in your code for each of the different segments, but it would still allow you to communicate together. All of that code that is duplicated right now would only be needed once - in the methods of the ancestor class.
So my state machine would go away?
What do you mean by an action camera? Is there any reason you can't use a typical GigE machine vision camera on a wireless router? 
One thing I did notice after a google search is that GigE machine vision cameras seems to be a lot more expensive than for example a GoPro. My budget on this project is fairly limited about 2000$ in total, but that should cover all expenses, not just the camera ;)
You can get machine vision cameras for a couple hundred bucks, and they're much better for applications like this than most consumer or hobby grade cameras. 
Ok, thank you. I will have a look around. You have any recommendations for a certain brand?
OK, my apologies, I didn't look at all of your images. The more I look at this, at least through my very narrow GOOP knowledge lens, this really does look like a prime candidate for conversion to an object-oriented approach. If you hadn't made some of the naming choices you have then the tab method would be a snap. You may still be able to use Call By Reference nodes to load your different VIs using a single sub-panel. I can't recall off the top of my head if that method cares about identical names, but I don't think it does as long as the path is different of you're using different strictly typed VI references.
Just looking at the image of your code... this should work... Maybe make a new VI that just uses another Boolean control to show or hide the buttons you're working with? Maybe there's a weird race condition with the buttons property nodes, so wiring the error cluster between the two so they run in series could do the trick? Also, I bet you can do what you're trying to do by just using one button that is either True or False depending on open/closed and just change the button function from latching to switching. If you want it to look different depending on the state, you can mess with the T/F colors and text in the properties. You can also get fancy with images of you really want using the Customize option, but that's a bit advanced for now.
Can you share any code you've tried? There is a property for text color of a string indicator.
Use a color box indicator.
Key thing being using a switch instead of a latch. That should give the OP their intended behavior. 
Normally I dont help with homework, but I wanted to do a bit of coding today, so I coded this up. Took me (5 years LV experience) about 20 mins to go from start to end to make some polished code. First things you need to do are make a way of solving the equation for a given t. Easiest way would probably be to use a for loop (iteration terminal times a timestep value will give you your t values) and a formula node, although its a bit tedious to type that long equation (copy paste away). Out from that for loop you will get an array of values which represents the time domain series for it. I used 10,000 iterations and a step size of .001 (1ms), so i effectively generated 10 seconds of data. Then I split that data, one went to a PSD function, second went to a High pass filter then PSD, the third went to a low pass then PSD. Then I threw them all on some graphs. I wont give you my code (thats the whole point of doing it!), but here are the results you can check your work against. http://imgur.com/23Jz0gp Also, if you were to see my code, youd probably have a pretty hard time reading it with how I built the equations in raw labview without the fomula node. It would also be pretty obvious to your instructor that a new student didnt write it :P 
Hey, thanks for the reply. You're right; this does work, I had just mis-tagged one of the status indicators. I agree that a latching switch to indicate status would be slicker, but the client's insisted that since it's a momentary signal, he doesn't want a toggle switch.
Appreciate the help! 
He means using the addition/subtraction/multiplication/division VIs under the numeric palette as opposed to dropping a formula node and typing the formula out.
Ahhh - thank you
I get an error saying that the number of lines in the channel is 1 and my data line is 100.
Hmm, on the right hand side where you have the data input, delete that wire, right click on 'Data' and go to create &gt; control. Can you tell me or post a screen shot of what type of control it creates? I'd just like to see what type of data it takes. It seems like some sort of mismatch.
Do you also have a link to the driver page? I'll try to download it. I've never used one of those DAQs so I don't know any specifics on how the sub VIs work.
it's a DAQmx USB-6001, it should be on the driver's page I believe
Sadly n, it still gave the same error. I've been having problems making it a continuous signal as well, it's stuck on 1 sample.
It's stuck on one sample because your while loop runs only once. Click on the conditional terminal (the green circle arrow icon) to change it to a red dot.
GOT IT! I realised all of that waveform stuff was outside the loop condition. Now I just have to figure out if it's actually doing what I want it to. Thank you so much for bearing with me!
http://www.ni.com/nisearch/app/main/p/bot/no/ap/tech/lang/en/pg/1/sn/catnav:du,n8:3478.41.181.5495,ssnav:ndr/
Share what you've tried so far 
My Analog in is doing Continuous Samples at 100 samples at a rate of 1kHz, my Digital Out has no specified no. of samples written per channel (or at least, I didn't specify it anywhere). Is it safe to just leave it as the default or should I set the Digital Out value?
You basically need to make sure that your loop runs faster than the 100ms that it takes the DAQ to get the samples. Alternatively, since you already have your acquisition in a loop, you could try using samples on demand (I believe the option is called N samples). That way the buffer will not overflow because you only get samples when you request them.
Since you already did it with the write channels, why not break apart the Express VI and initialize the read channels outside of the while loop? Other than that, /u/Akaitora has a good point.
I had the express VI broken apart for my other VI for testing, but it became really cluttered and I figured it was easier to just use the DAQ assistant. 
Thanks, I'll try to figure out where that setting is. I had it when I used the DAQ assistant but now that it's all separated I'm a little confused as to whee things are. I'll go dig around - thanks for your input!
Easier is not always the correct route. Basically what the DAQ assistant is doing is opening a DAQmx task, reading it, and closing it, every single iteration. Bad idea. Despite it looking a little more cluttered, although, I personally don't think it would, go with the DAQmx route.
Hey, so due to some complications with physical equipment, I'm not able to test my code for another week. I was wondering if anyone could tell me if this setup will work in theory: [Imgur](http://i.imgur.com/txPBbb9.png) I want it to change the color from green to red when the voltage is over 10 (the false case is exactly the same, but green). The control labelled T34 Phase [V] is data socketed to a power meter, so I want the color box to change while running based on that data.
I'm a reddit newb, so I'm not sure if you were already notified, but see my last comment for an example of my code.
You can put the other two daqmx tasks into separate loops with their own timing. Use some sort of scheduling algorithm like Fixed Priority preemptive/round robin or just a fifo to send messages over a queue or notifiers to perform your writes. This way you will not have collisions when trying to allocate your resources. As for your pulse duty cycle just conditional probe it whenever you get outside the data range.
Save it in labview as a tdms file, use this: http://uk.mathworks.com/matlabcentral/fileexchange/44206-converttdms--v10- to then import into matlab.
There are a few ways to do this - but we'd need to know more about your process. 1) you said you have a loop for your DAQ - is it taking other measurements or controlling other output at the same time? 2) What is your general architecture? 3) What does your timing look like now? Is your loop running in (xxx)ms intervals or is it free-running? 4) Is it ok if your process just sits and waits for the time to run out? (This is as simple as a wait (ms) inside a structure)? 5) if your process needs to keep running in parallel - you could use shift registers to hold the value of the output and then switch cases or values based on an 'Time has Elapsed' sub-VI or you could get more tricky and handle things like most people do in the CLD exam. Check out this sample code: http://foolooo.wordpress.com/2011/07/31/pause-and-continue-the-time-with-elapsed-time-vi-in-labview/
Based on your description, I made some assumptions and created [this VI](http://wikisend.com/download/352064/DAQ_ON_Seconds.vi) The idea here is that a time starts counting when you press the button in the front panel and activates a boolean output for 3 seconds, regardless of the state of the button you used to activate it. I used feedback nodes, which are basically shift registers or, in other words, sort of like variables that hold in memory a wire's value from a previous iteration. Edit: Forgot to mention my assumptions! The most important one is of course that you already have a loop running that executes rather frequently (maybe every few milliseconds). How close to 3 seconds the VI gets will depend on how fast your loop executes. The other one is that you will be using this VI as a sub-vi inside your loop and that you are constantly "updating" the state of your digital output.
First question that comes to mind is: are you using the Actor Framework? If not, you really should, specially if you are using LV 2015 or later; NI made it easier to work with in the few latest versions. You can find NI's AF community site [here](https://decibel.ni.com/content/docs/DOC-17193). Also, you can check out this guide a co-worker wrote on how to get started with the Actor Framework [here](http://www.bloomy.com/support/blog/getting-started-actor-framework). As for your question about using different models with your Simulator Class, what you need is inheritance. Your simulator class should be an abstract class and your simulation modes should inherit from the abstract class and override specific methods (in this case your Model.vi). This way you can achieve the "dynamic dispatch" behavior you are looking for.
I'll go check it out, thank you so much!
Yeah, I will try later
Hi! Thank you so much! I'm using National Instrument PXI to generate Signal and arduino will read it and arduino will generate pwm
I'm not really clear what you want from the motor side of things, are you using a motor to generate from the bike? And using that to measure power output of the person. Power out of a motor is voltage x current so you'd need to measure both, if the person is generating then you also need to dump the generated power into something. NI offer hardware that can measure a wide variety of things, but they mostly measure signals - you'll need to decide the sensors you need which then dictates the NI hardware you need. In your case, the majority of you decisions should be based on accuracy that you need.Most bikes measure speed by a single magnet passing a reed switch - would that be accurate enough for your RPM measurement? If so you only need a digital line, if you want really accurate then you could go to an encoder but it'll need more inputs. Temperature could be measured using an RTD or thermocouple module - do you need 5 or is 1 enough? Are you using accelerometers for vibration, if so you can just measure their output voltages? 
CLED, Embedded Developer. I already passed CLD last September.
Thanks for clarifying.
Can you give a bit more context about what you are trying to do with the cluster? It's possible there is existing code you could add to your project to achieve the same effect.
yeah i was going to say, a mouse, a computer...
Property Nodes have much more memory and processing overhead than an indicator, especially when you're setting the value. Also, have you tried timing it using sequence structures? That would allow you to point fingers.
This code was clearly not written by a LV programmer. Instead of creating this weird visibility thing with the property nodes, create yourself a custom control that toggles between 2 colors, one of them transparency / or background color. Datasocket / OPC sucks and is a bloated piece of xxxx, but we should talk milliseconds here. So try to separate your HMI problems from your communication problems and identify which is which. In fact, I cannot totally pinpoint what slows you down SO much (maybe the way you use Datasocket... ). Use Tools-&gt;Profile -&gt;Performace to figure out what is really the problem.
You are probably better off moving to a VM solution for your development and support of legacy products. There's usually a bit of hassle with the hardware integration. If you get the Vmware Workstation Pro or something to that effect you should be able to configure it to find your devices.
Thanks, I'm taking all of these into account. I've been running up against a need for the AF at almost every design step (need spawn an unknown number of sensor acquisition threads at runtime, need to have a class as an element of multiple other classes with same data and no synchronization issues, etc). Thanks for the resources. 
This is a copy-paste from a LAVA.org post by someone else, but you can use these as arguments against using W10 to your company's IT policymaker(s): &gt;OK. So I decided to evaluate Windows 10 - not as a desktop OS, but as a DAQ and automation platform like Windows 7 on PXI racks. &gt; &gt;My conclusion was that it is no longer a viable platform for test/DAQ and automation. &gt; &gt;There were 4 main killers for me. &gt; &gt;1. Updates are forced by Microsoft when connected to the Internet which not only means that we no longer have control over versioning of a deployment but we also have no control over bandwidth usage. The only options are for immediate update or deferred (6 months) update. WE are no longer able to stage deployments with tested targets that are guaranteed to remain stable and prevent mutation of the customers system. &gt;2. Resource usage by background windows processes that cannot be disabled are prohibitive for DAQ and unpredictable when they occur (yes I'm looking at you "Microsoft Common Language runtime native compiler") &gt;3. It is opaque as to exactly what information is being sent to Microsoft and the system is set for maximum disclosure of private data as default. This means it is difficult or even impossible to guarantee a customers privacy. Even if a comprehensive assessment is made of leaked information, there is no confidence that settings will not be reverted or measures circumvented by future updates (see Item 1.) &gt;4. Bandwidth usage is hijacked in order for M$ to deploy updates to nearby machines using "Background Intelligent Transfer" (basically bittorrent). At present this can be disabled in the services but I am not confident that will always be the case. This has caused a couple of mobile applications to exceed limits and result in reduced service speeds and high data usage charges. From [here](https://lavag.org/topic/19183-windows-10-issues/).
No tips I'm afraid, but I'd be keen to hear about your experience of part 1? Tempted to give it a go as soon as it comes to the UK.
The problem is I don't know if it will work or not, just because NI doesn't explicitly tout support does not mean it wont work. And really lack of support is not the end of the world, NI doesn't support these cards any more so I really don't want to either, lack of support for windows 10 would be a good reason to upgrade to something a little more modern like the X series.
I saw the title and was super excited you figured out a way to modify the draw order to place things on top of .NET containers. :( I was working on an XControl that used a .NET container to incorporate .NET charts. At first I wanted to recreate a right-click menu for modifying properties like you do any other charts but couldn't get that working. Then I wanted to put a button down that would go invisible in a run state that could generate the menu. Unfortunately, couldn't get the button to draw on top so I had to do some funky resizing. I ended up abandoning the project because I'm lazy and it needed a ton of work. That, and the main driver was better looking polar charts which I already have an API for. It just uses a million VIs to programmatically modify chart properties. Annoying, but the end result looks great!
Imo Wireshark is the better solution here. 
I'm a little late but you're using the Datasocket read function alone. This means it's opening the datasocket connection, reading it, and closing it. This is what's slowing you down. You'd have to refactor your program but ideally you'd want to keep your datasocket connections opened for the lifespan of your program. Something like this: http://i.imgur.com/xmupDZp.png
Perfect! Thank you so much.
Where do you go to school?
My company definitely paid something for me to take it a while ago, but through a university I believe it is free.
[Have you tried this link?](https://lumen.ni.com/nicif/us/academiccladstudentdiscount/content.xhtml)
I think the discount brought it down to $100. I knew someone who got the student discount for the CLD too but I don't know the final price for them. When I taught at a university we worked out a deal where instead of a final exam they took the CLAD for free. Call up NIs local office and I'm sure they'll work out something.
Wait for an NI event and get it for free. ni.com/events
What device are we talking about here?
That clusterfuck of wires on the right is the navigation window. The lighter rectangle is the size of a 15" laptop screen. On the left we see a small portion of said clusterfuck. My predecessor apparently didn't believe in SubVIs. Nor did he use shift registers. All data had to be entered manually and the whole VI had to be run again to see a new output. Nothing was kept programmatically. Also instead of putting Booleans in an array to be searched for True to drive a case structure, he used the Select you see there, and ORed the results. Not added, but logical OR. Technically had the same functionality, but understandably awful to look at. Even worse to debug. Thankfully this whole thing got scrapped when I came on. 
Honestly, unless you are trying to do this as a project to learn, you are going to save an incredible amount of time (and maybe money) by buying an off the shelf usb [barcode scanner for $20](https://www.amazon.com/Automatic-Barcode-Scanning-Bar-code-Adjustable/dp/B00406YZGK). These scanners work as input devices, like a keyboard, and require no programming to get started. You may want to add a little code to restrict where it can input data in your program, but otherwise it is a very simple application.
There's lots of published algorithms out there. You could try implementing one. Here's one you might find relevant https://users.soe.ucsc.edu/~manduchi/papers/GalloWACV09.pdf
https://pypi.python.org/pypi/PyDAQmx
woah.... 
I've worked on tons of "take my old shitty code written by an intern and make it good" projects that were a total clusterfuck...... but you win. I'm glad, for your sake, that the code was scrapped.
That's joining my folder of bad LV code.
Hmm, I will have to check that out. I tried to find it on the school computers but that module was not downloaded. I will see if I can download Labview at home and then the module. 
I like the top one better. It's like updating code on hard mode.
Adding a single wire branch anywhere leads LabVIEW to freeze up for 2-3mins. Even if it doesn't leave anywhere. And for some reason a single wire branch will convert the datatype from a double to an int.
Here you have all the versions of the VDM: http://search.ni.com/nisearch/app/main/p/bot/no/ap/tech/lang/de/pg/1/sn/ssnav:pdl/q/vision%20development%20module/ And one of the examples is here: http://www.ni.com/example/30533/en/
It definitely should be. It is an test stand for an actuator that runs through different operations to test the product. What baffles me is that it only has like 6 different inputs from the DAQ so why does there need to be so many data types. There are 563 different types pieces (string, numeric, boolean) of data in this thing. Yes, I counted.
As far as terrible code goes though, that doesn't look too bad. Everything seem neat and tidy and spread out in a logical way. It's a mega VI, but I could have gone worse. The local variable probably make it full of race conditions though.
Curious if you're testing appliances, mostly fridges? Looks familiar from an internship long ago. 
Is this navigation window a standard feature? Never seen it before - looks super usefull.
Yes. Press ctrl+ shift+n.
nope. Aerospace products.
May be a dumb question, but have you tried deleting the property node and placing it again? Sometimes property nodes store state information that causes bizarre behavior.
I'm amazed by your support guys! Anyway i spent a some hours with a tester checking the device and it turned out out to be an hardware problem,afer that i manager to get lv to recognize it... honestely i do not recall the exact name ,i will post it tomorrow when i get back into the lab!!
Thanks : )
Instead of indexing several elements individually, you can index them all at the same time, perform a single string to int function and then do your math afterwards. Normally I'd say you should use subVIs to reuse redundant math and/or indexing operations but it's kinda hard to say what the right course of action would be without seeing the other cases.
Good idea. Op said he was new to Labview though, so I don't know if having him learn OOP is the right way to go. 
I don't know, seems like OP grasps basic concepts, can use case structures and LV literals, why not OOP? I get that it's somewhat advanced, but what I've found is that teaching LVOOP to newcomers is easier than those already entrenched in their ways. I almost wish we had a programming language like Java that required the use of Objects. Almost. And, come 2017's LV, we might. :)
I dont shy away from anything care to provide an example or a tread of what you guys describe? 
PM me your email, I'll shoot you an example in the morning as I'm just about to hit the hay.
I'd like you to back up a bit and go into why you are "required" to make a table. What function is it that only a table specifically can provide that forces you to use one? I've been programming in LabVIEW for over two years now and as a general rule I find that the more hard-coded array indexes you use, the less likely it is that the data structure you chose is the best one to use.
Hello, the reason i need to make them is because i work at an automotive company. Before they hired me they used printed out spreadsheets to keep track of things like time it took to make a part, how many parts they have made, how many defects, how many defects per station, etc. They are very set on their ways and every HMI i have to make must keep track of all these things and display them in the same format they had in their printed out spreadsheets. Thus, i have to make tables and spreadsheets to display the data inside the HMI so they can access it when they want. Regards
You are correct, i only display data. This is because of what you mention. All the operations are made outside the tables and just display the end result. Like i answered in a comment above they are mostly there to display times,part numbers, etc. But again this information starts to be more and more as the day goes by. Because they want to breakdown everything down to the hour.
You shouldn't actually use local variables at all. All the code for small application (within 100 vi's) almost never requires local variables to work. If you want to display table A (because user clicks at button A?) should be done in 2 loops. This is the most scalable approach and is being taught on CLD exam. First loop is to get events from front panel, second one to calculate and display results. There should be one cluster with all the data you need, running in the shift register in the second loop and show different tables in different cases. As the application will grow bigger, you'll see benefits of this idea. Read about queues and how to send messages between one loop and the other. http://i.imgur.com/86kREmX.png To close application, add exit button, that will close first loop and send exit message to the second loop.
Since coaxial is a type of cable, that does not narrow down what module you would need. If you could be more detailed on what you are trying to measure, someone may be able to recommend a cDAQ or other solution.
http://www.pcb.com/products.aspx?m=002C10 Thats the part. Single wire 
What are you actually trying to measure? Is it some kind of IEPE device? If so, is it a tachometer or accelerometer? Maybe a microphone? What acquisition rates are needed? Do you need deterministic monitoring? Any control based on this signal?
you have answered my question then. BNC to flying lead / twisted pair.
Here's an idea: use hidden controls or shared variables to store some of these values instead of using specific cells on a table. Then, when you need to populate or update the table you just use that variable instead of reading from specific cells. This will make your code easier to read because instead of indexing a cell to obtain a value, you just read your variable that represents that value. Edit: This would also make your code more adaptable to future changes. What happens if your bosses want you to add or change a row/collum? Because you hard coded a value to a position in the table your code is susceptible to bugs if the format of that table changes. Having the actual data of the table stored behind the scenes and just using the table topics present that data should be the way to go. 
Asking like this is like asking: "Hey what air-freshener do I need for this vehicle to be driven?"
Yes! There are many.
Can you post images of the signal? maybe this is an encoded sensor. Do you have any datasheets?
You do bring a valid point, i have been in the position where they ask for a change and it does take me a while to change a table.
You could easily buy a module that's connectorized correctly but measures a totally incorrect type of signal. You have to tell us what KIND of signal you're measuring. Is it basic analog voltage? A modulated RF signal? Something else entirely? I'm not trying to be an ass, anyone here just needs that information to help you. 
No data sheets since it comes supplied with the bike. No markings or anything on the sensor itself. I did a high sample rate analog read to look at the waveform. It looks pretty good. I was able to sense the signal with an Arduino. I just wrote a simple sketch that sample the state of the sensor and outputs a high or low on a digital pin. I read this signal with my USB-6001 rather than the hall effect switch directly. It works just fine. So, I think that the hall effect signal is just not up to TTL standards and freaks out the counter. 
Maybe you can amplify the signal? use a 741 or a TTL buffer
That's a neatly drawn bad code!
That takes some talent!
I seriously wonder if he believed all people did it like this. It's pretty impressive to behold, from afar. From the next galaxy over, perhaps.
I usually use the VI bundled in the Waveform monitoring palette https://zone.ni.com/reference/en-XX/help/371361J-01/lvwave/waveform_peak_detection/ or this one in the Signal Operation palette https://zone.ni.com/reference/en-XX/help/371361J-01/lvanls/peak_detector/ You could alternatively take a derivative of the signal and find the local maxima when your rate is within a large enough threshold near 0. Whichever method you choose, you should be able to find the index of that peak and get an array subset of the data.
Not sure I understand correctly, but if you're doing Test 1 -&gt; Test 2 -&gt; Test 3 always in that order and you always want the data from the last test could you not store that in a separate array, allowing you to find the max of it? You're finding the max of the array that contains all your data at the moment....
This looks like direct pwm control and the motor deadband is very small. Your output pwm could be between positions. First , adjust your pwm frequency to match the motor and null the offset. After that, run your output signal through a PID control to emulate the functions of a servo driver and tune those variables until it is stable. Other than that make sure the motor can source its full current from the RIOs driver and is the correct voltage.
You could try a switch debounce routine with a sufficiently small interval to work with your entire range of RPMs. If it is going fast enough you can miss a pulse though. Alternatively, you could put a tuned RC filter(snubber) in line with cable signal if the cable is sufficiently short to smooth the counter input.
Thanks very much, if you don't mind I'd like to ask some follow up questions. I've read that most RC controlled servos are PPM not PWM. could that be the problem? I'm going to follow your instructions and report back. I cannot thank you enough, I've worked extensively with Siemens PLCs in logo and graph, FESTO products and C# programming. Unfortunately these have helped little work LabVIEW. Thanks again 
Hi jbone, Your post is welcome here, but you'll probably find more help geared towards beginners on the official NI forums. The two things you want to do are absolutely achievable and LabVIEW is well suited for the task. 1. Normalizing plots. Have you tried the normalize waveform vi? 2. FFT's. Google "labview fft" and check out one of the examples. You need an array of input data of the correct size, and LabVIEW has a built in fast FFT function.
Are you using an XY graph to plot things? It took me the longest time to realize that they are simply a cluster (or array of clusters) which you can put together and pull apart at will using the bundle/unbundle functions. You can then easily normalize them (without having to use a waveform, something I still don't do as I generally work with unevenly spaced data) by dividing them by the integral of your dataset. The waveform approach (as recommended by /u/arcandor) is, however, much simpler if you do have evenly spaced stuff, and of course if you want to do an FFT you'll need to build an evenly spaced set anyway. There's a waveform function to do exactly that as well. But as general advice, I found early on that the "context help" menu was absolutely key to learning anything. Make sure to turn it on, and be open to getting utterly lost on a few generally unguided journeys through the help files. Pretty much nothing you'll come across is useless to any problem, as there are nearly uncountable ways of accomplishing any given task in LabVIEW.
What are you using to read the waveform? When you can "see the signal on a waveform graph" is the data type an array of doubles or is it an actual waveform data type? Sounds like you're going to need to learn about: * While Loops * Shift Registers * Arrays * Array Min/Max * Waveform Data Type (specifically how to get waveform components) * If you feel really frisky, look into using Data Value References to store your waveforms instead of shift registers.
What advantage does a DVR carry over a shift register when you're not trying to access some class instance from multiple areas?
One of the things i have tried but have not found much success is using the MATLAB script, in theory, if you have a working matlab in your pc with a licence and all, you can make programs inside the node as if you are programming in matlab. Not gonna lie, i looked into this node a while back and could not get it to work properly so i scrapped the idea. But, if it works for you i would be glad to hear. Also you could make ActiveX calls to matlab and run a small script that solves the ODE and outputs the value. Ill leave the link to the k-base: http://digital.ni.com/public.nsf/allkb/2F8ED0F588E06BE1862565A90066E9BA
What does the performance need to be? If 5ms isn't fast enough, it's probably time to break out the C dev environment and compile it to a DLL with the functions you need.
I know what you're referring to and I'm excited for the general LabVIEW public to hear about it. :D
I'm giving a technical session this year! Super excited. The only sad part is my session is Thursday morning so I'll have fairly low attendance compared to Tuesday sessions. My company is always on the expo floor and that's always fun. We have some new demos and I even recorded myself completing the ATM CLA practice exam for a timelapse video.
I am really looking forward to your XNode presentation. My co-worker has used XNodes and VI Scripting extensively to eliminate most of the boiler plate code in my particular architecture of choice. I want to get into them as well so I can customize his nodes myself.
Can you post the code, or at least a picture of it?
Cool. Thanks for the input. I'm going to keep getting frustrated for the rest of today and then try to get this data logged the way you suggest tomorrow when my heads not spinning and my blood pressure has had a chance to come down. 
Yeah, I've got a 30 minute slot at the very end of the day, so I'm competing with happy hour. Heck, I know which one I would choose. What's the name of your session?
Yeah, I'm pretty pumped. I think it has potential to really step up the quality and depth of the technical part of NI Week.
Thanks for the tip. I definitely feel like I'm in way over my head. Too much trial and error and not enough guided decision making. 
http://www.ni.com/getting-started/labview-basics/ and if you have a service contract, the full self-paced training is available: http://sine.ni.com/myni/self-paced-training/app/main.xhtml
Do you have any prior programming experience? There are lots of analogous things in LabVIEW to normal programming constructs, you just have to get in the right head space. And if you don't have the service contract, try watching Sixclear's VI high series on YouTube. Good for some basics and pretty entertaining. 
My programming experience is the first lesson of Java through code academy (which has been valuable) and messing with all my android devices using toolkits and a little ADB terminal commands over the years. So, not much at all, but I've got a good aptitude for these kinds of things. I'm definitely doing it the hard way, but I don't feel too daunted by the program. Just need to familiarize myself and learn a few more rules about what it wants to do. I'll check out that channel this evening and see what treasures are to be found. Starting to think my issue with the Express VI has something to do with resetting the VI each iteration, which i thought I had figured out. It works exactly how I want it to, just can't ask it to switch to a new excel file without getting the endless prompts. I know I'm close. 
TS9517 - Efficient LabVIEW Projects: A Team-Based Approach Most of my projects are single developer so it gets complicated fast when trying to deal with multiple developers on a LabVIEW project. We discuss workflow, source control management, the dreaded merge tool, two types of LabVIEW architectures that reduce merge conflicts and separate software tasks into modules, and other topics like team communication and team roles.
Does anyone know when they'll release the NI Week app? Edit: Nevermind. https://play.google.com/store/apps/details?id=com.geniemobile.app5636525615480832&amp;hl=en
Just stopping in to say I got it working. I think it was a combination of latching my control to engage the express VI to a tunnel that carries a "Yes" through to each iteration (outermost structure), as well as removing the initial base path of the express VI, deselecting "ask user to pick file", and placing a File I/O control at the Filename terminal on the express VI. Now to break it all down piece by piece and see what's actually helping and what's redundant. Thanks again for taking the time to give me feedback yesterday. 
No problem. I'm not too active on Reddit but if you have any more questions you should check out the discussion forums. Link in the sidebar. 
Never been before, but gonna try to head to the free Expo this year. Is that worth it? 
[This Knowledgebase article will probably help you.](http://digital.ni.com/public.nsf/allkb/353A696A3F393D9B86256E8B007A2912) There is a property for your application instance that gives you AllVIs. This is all of the VIs in memory and you can use the FP.Open property to check if the front panel is open.
There should also be some examples in the community on this too. Try searching NI.com/examples 
This post is in response to a question I posted [earlier] (https://www.reddit.com/r/LabVIEW/comments/4vsyy9/is_there_a_way_to_get_create_a_list_of_all_vis/). Interested in any thoughts or feedback
Dang IT guys!
If you're working Dev, you can also use the scripting application property node, "ActiveVI". It tells you which VI is currently "Active" aka, has focus. http://imgur.com/a/5p3YX
can you post a screenshot of your steps?
Sure, here it is: http://imgur.com/a/rGIzi Edit: Basically what's happening here is that I have an oscillating signal and I want to run it for an extended period of time until it slows down a certain amount (hence the rise time parameter).
Another thing - It seems like the system is responding to the conditional repeat in some way, because there is a hesitation when I check/uncheck exit if true" as if the code is restarting.
This will probably work in your case, but not all cases. If you're using re-entrant VIs this solution wont work. The "AllVIs" property will only return the "base" vis, not the specific clones. In the past when I needed this info, I wrote some tools that managed the frontmost list for me. For example, when a VI opened it's front panel, it would push it's self onto the list, then when it closed it would pop it's self off. Also, you could have a VI running in the background that watched the "Application: VI Activation" Event [VI Snippet](http://imgur.com/0wiEt9g.png)
That makes sense, thanks!
Off the top of my head, a bit mask might be a more elegant approach. Coupled with an enum your code should be pretty self documenting. You just have to make sure you keep careful track of the way the states map onto integers. http://i.imgur.com/eCEOw96.png
http://imgur.com/jzUye8H True case still use AND but don't invert the bottom input
the bit mask was the way I ended up going with, but I really like the use of the use of the enum in your solution.
Can you post the full python code?
What about making each condition that you're checking in to a subVI, have each subVI throw the appropriate error, and then have an error handler at the end that does whatever you'd like with each? Easy to assemble, looks clean, easy to expand upon, and easy to troubleshoot.
What do you mean? Like did they revamp the diff and merge tools? I don't know about specific changes but I can tell you the new VI format is a .gvi and it seems to be XML instead of binary from the demos I saw. (I can try to confirm this sometime this week). If that is the case, diff and merge will be so much easier. If you're referring to source control integration, I have no idea. I can tell you that for the current LabVIEW IDE, there are many third party tools out there to improve it. We use Git at my company, and I just learned about [git4G](https://decibel.ni.com/content/groups/data-ahead-toolkit-support/blog/2016/06/21/data-ahead-releases-git4g--a-toolkit-to-integrate-git-into-the-development-environment-of-labviewiemens-pl) which integrates git into the IDE.
I'm late to the party, sorry! Here is an example for you: http://imgur.com/GKw54kZ Do you care about why it's not valid? Add some indicators and bolt on your error handling and use the individual wires instead of the compound arithmetic. I left the error 7 and any other errors on the error wire, in practice you may want to clear the errors that you are explicitly checking for rather than pass them downstream. You're right, nested case structures are hard to read and debug past 2 levels, and even at that depth they can be tricky. You can eliminate one case structure by using the 'Select.vi'. The others have been eliminated because I don't necessarily care about *how* the path is invalid, I just need to check if it is valid or not which can be handled with a generic "this path is bad" error that lists the path. As a bonus, I've included my preferred method for handling user selected paths. One compound arithmetic (the small circle is a 'not') takes care of checking "path is valid and not cancelled". 
After a long wait I finally have Labview again, however dropbox link is dead. Any chance you still have it somewhere? I'm still struggling to get my head around it. Many thanks!
I have had a good look and a play with it, thank you very much for making it! The only problem is I need the user events in the top loop to influence the bottom loop, which I can't see a way of doing in this example, unless there was an event structure in the second loop as well, is that possible? I have made a functioning program using the queuing system, however it is messy and I think there is still a better and more robust solution, probably using user events as yours does. This is my code: https://drive.google.com/file/d/0B7G5wNJO6tS4eGtkUTNIa1U3eFE/view?usp=sharing Hopefully it will at least show what I am trying to do if you're willing to have a look, but you have already helped a lot!
AFAIK there is no 2016 (nor 2015 version). LabVIEW Home Edition is a "special" product that is not sold by NI directly, but by resellers (they vary from country to country). If you are a student, you can get the same functionality by buying the Student Edition (also from a reseller: http://sine.ni.com/nips/cds/view/p/lang/pl/nid/210866). And my honest advice is not to install the 64 bit version unless you really need a lot of RAM. Otherwise you can just run into compatibility issues (drivers, toolkits etc.). More info: http://www.ni.com/white-paper/10339/en/
It's not sold by a 3rd party. NI keeps the lid locked down pretty tight on LabVIEW. Also, all installs of LabVIEW are fundamentally the same (LabVIEW itself, not the modules and toolkits). Activating under a particular code is what unlocks certain features that makes it Base, Full, Pro, Eval, Student, etc. Also, here is the link for LabVIEW Home: http://sine.ni.com/nips/cds/view/p/lang/en/nid/213095
Something that works well for us has been to buy a VIPM Pro license. There are a few good options in VIPM Pro that make dependency management easier. 1st, we let VIPM Pro create and manage a repository and we store this repository on a cloud storage like dropbox (we use GoPyxis but no one's ever heard of that). Then we have everyone configure VIPM Pro to subscribe to that shared repository. This way, when someone builds a a VIPackage, they can immediately publish to repository which will then get automatically synced onto everyone's computers allowing VIPM to notify my co-workers of updates to packages as well as letting them see all the packages available to them. Finally, to deal with older versions of dependencies, for every project, we create a .vipc (VI Package Configuration). This is pretty much just a .zip of all the VI Packages we choose to include. VIPM can even scan your project, figure out all the dependencies that have been used, and create the .vipc for you. This .vipc goes into source control and several years from now when someone pulls it, they can run the .vipc and get all the old dependencies installed (and update them if they wish).
Even the link you sent contains information that LV Home edition is not sold directly by NI :) "Please note that the LabVIEW Home Bundle is not sold directly through ni.com. If the product is not available in your region through a distributor, please contact NI." Edit: What you are writing about NI software is true - but no one questions that. LabVIEW Home and Student edition are bundles containing exactly the same: LabVIEW Full, LabVIEW CDS module and MathScript module and both are sold at a fraction of the regular price for commercial customers.
Also 32-bit code can be deployed to 32-bit and 64-bit machines, but 64-bit can only be deployed to 64-bit (without recompiling in LV 32-bit)
I am interested in 2016 64-bit version mainly to be future-proof. I do not want to obtain another license for LabVIEW 2016 (or later) 64-bit version after buying 2014 32-bit. My interest lies in control systems development, for which I mainly use Matlab/Simulink. I had a LabVIEW Student license and I also hold a CLAD certification. However I have since graduated and do not have LabVIEW installed on my PC, so this is my own investment and thats why I am had this question. I see that in LabVIEW 2016 (released this month), both the "Control Design and Simulation" and "Mathscript RT" modules have also been ported to 64-bit. So that is why I am interested in 2016 license instead of 2014 one. Also on a related note is the license different for 32-bit vs 64-bit LabVIEW (for same year version)?
Yep, good catch 
So the big thing I immediately did was make a project. No excuse not to use one when your project is more than ~5 VIs. I made lots of comments, but your queued state machine is very concise and nicely done. I don't have any super major architecture critiques, but I do think you should be using a typedefed cluster for state information instead of local variables. I give examples in MAIN.vi. Also there's a nicer way to do log files, that is also unique and sorts nicely. Plus no need for you to write some path name (although you can use concatenate strings and get rid of the 'log' prefix to maintain that functionality). Otherwise, things look good. You don't really have a need that I can see for something more complex. Let me know if you have questions! https://www.dropbox.com/s/s1m6jh8ff3xfg21/Linkam%20Controller.zip?dl=0
License is the same. I develop control systems in LabVIEW that are deployed at hundreds of sites around the country. I learned the hard way not to develop in 64-bit after dealing with corporate america. Word to the wise: just stick with 32 bit.
So if you wire a VI reference into a subVI, you can address the decorations on the VI's front panel like so: http://i.imgur.com/RhQuf2v.png Decorations don't have labels, so you'll have to settle for picking the one you want via the "top" property. You can calculate this using some simple geometry and an understanding of where it is relatively to the control in question. So make sure both of them are separated by some vertical space or you'll have some issues :) Once you find a reference to them (you could also do this, for example, using a color), you can move them as you want. In terms of where to move them, the tank control has some good properties for you to look at: - Position - Label.Position, Label.Visible? - Caption.Position, Caption.Visible? - Scale.Position, Scale.Visible? These properties can help you figure out, via simple greater or less than checks (or even better, an In Range and Coerce). where exactly the tank itself (not just its outermost bounds), so you can position the decorations exactly as you want them relative to the tank bounds. I don't have time to get the code working for that, so if you have any questions, I can try to explain in more detail.
Head over to [LearnNI](http://www.learnni.com/) and click through the free content. Familiarize yourself with the concepts and basics. In the LabVIEW environment, press F1 to open the help menu. Read the section on fundamentals. Your task sounds like it involves an array of boolean indicators. Those are the terms to look for while you are reading the course/help file. Good luck! 
What kind of 'element' are you referring to? One way you could do it is with a colour box as an output. Then manipulate the RGB values depending on the temperature input. [Example](https://www.dropbox.com/s/lbgod7jvb6pvbxc/color_example.vi?dl=0)
The element itself is not important, should just be something simple, like a square. Your example is exactly the Kind of Thing I'm looking for, just a bit tweaked (19-21 should be pale green, below that from pale to rich blue, above that from pale red to rich red, as you did). I hope I'm able to tweak it the way I Need to, thanks a lot!
Okay, it's a bit hard to interpolate between the different colours. One way may be to work in HSV instead of RGB, then convert to RGB at the end. Check here for a community made tool for this: https://decibel.ni.com/content/docs/DOC-28529
At a bare minimum you'd have to install the LabVIEW Run-Time Engine, but your code doesn't necessarily need an installer. However, you're likely using drivers and more that will require additional installs. Currently the options for going through a browser, etc are fairly limited: https://www.ni.com/mobile/
A remote web page or a exe will still need the Labview runtime environment so you would need to install that (IT needed) 
The system needs the runtime (and all additional software) to run the exe. So once a system has the runtime, they can run any exe. You don't need to build an installer every single time. Just copy over a new exe. 
Hey, just wanted you to know that I got it working thanks to you! I just took your example and adjusted it for every specific color I need, so I have one for red, one for blue and one for green. Then I took all the cases and put it in a single case structure and now it's working, so thanks a lot :)
Agreed. 3 values every 5 seconds is plenty slow for consistent writes.
Thank you for your help! I have got rid of the local variables and replaced with a typedefed cluster. I also think the file system is much better so have used that. Sorry for the late reply but I have been wrangling with getting voltage readings from another source at a much higher rate, think I have a workable solution now though. Thank you so much for all your help!
This is basically what you are looking to do, use a property node to take in the reference of the control and adjust the value. http://imgur.com/a/b1qt6 
National Instruments bought out Digilent in the last couple of years. The home bundle is sold through Digilent. So while you don't purchase it through ni.com, you still can only buy LV home edition from National Instruments, albeit under the Digilent label.
It's not only Digilent that sells LabVIEW Home. It depends on the country. e.g. https://www.conrad.de/de/national-instruments-labview-home-bundle-software-1379643.html 
That's why I said they should be avoided, but it remains an option.
Global variables are never okay to pass data between VIs. Synchronizing loops to stop? Maybe, though a queue is usually more performant. You get more functionality out of implementing an action engine, which has the added benefit of encapsulating its own methods. You also avoid race conditions.
Use a color ramp instead of a color box - this is precisely what they are designed for. Setting them up the way you want is a little tricky to do manually, here's an example that does the config using property nodes. Copy the result into your VI: https://decibel.ni.com/content/docs/DOC-37136
Yea true that. I'm NDA'd on most of it but this is for a pressure control system (the 30th one I've written, very tired of them). 5 identical systems. We have some embedded engineers from NI touring our offices this month so I thought I'd set up all the cRIOs as nicely as possible.
For us, we would use them to drive up our NI sales. It's what gets companies alliance partnerships. edit: I thought you were asking why NI supplies specifically, not DIN mounted. My bad.
damn nda's. spoiling it for all the nerd points we're after after here haha "what do you mean I can't take picture of this incredible machine I'm slaving on to show off to my nerd friends?" 
As someone who has 8 crio's sitting on their desk right now i feel your pain :) 
There seems to be a lot of videos on YouTube. NI does provide training that I think you have to pay for, but honestly I didn't find it all that helpful when I was starting. I've been using LabVIEW for about 6 months now at work and I still struggle with some concepts, especially coming from text based programming. I unfortunately find that NIs documents on the internet are hit or miss in effectiveness. So I'm going out of town on vacation for the next few days. If you have enough time to finish your assignment I would have no problem answering any questions I can for you. Obviously I may not be able to quickly get back to you for the next couple days. The best starting point I can think of is playing with the producer consumer template and trying to find as many examples you can't that explicitly make use of that template. Just to get a feel for how things work. As usual the first couple programs you write in LabVIEW will look horrible to you once you start to understand everything but you have to start somewhere. I know that's a lot and kind of all over the place but I hope it helps a little. 
I program in LabVIEW all day long, and teach high schoolers. I have some slides you can look at if you want. Send me a PM with your email and I'll send you the dropbox link
NI has a couple free options. Search for either LabVIEW 3 hour or 6 hour, there's a couple other beginner tutorials but I'd start with those
I've been thinking about starting a YouTube channel with beginner videos. Might start that tomorrow. What in particular to you want help with?
Congratulations. 1. Yes, put it on the resume, it has value and if nothing else you can always say that it's the first step to committing to achieving your CLA. 2. You should be getting your CLD before your CLAD expires. That should be your next goal. It's not that much harder than the CLAD given proper studying. 3. You should receive an email with your certification number and a link to download the certificate. If not, contact NI. 
Also when you get that e-mail with your certificate, read carefully. I think you can also get a free shirt (I don't remember if the CLAD offers one but the CLD, A, and ED do). Yes CLAD on a resume is good. Finding certified LabVIEW talent can be hard. Of course most companies want CLAs, but when they can't be found a CLAD who is eager to learn can be a great acquisition. In Houston, I know of only one CLA on the job market. Everyone else is taken. When we start looking for a new hire we're going to have to go with a CLAD or CLD.
The default user in my university myRIO (1900) is empty aswell as the pass.
Are you using ActiveX to write to the xlsx or are you actually writing a csv? What's your logging rate and the number of samples being logged? How long are you logging for? (Really I just want to know how big the log files are getting.) I'm guessing your log files are getting pretty big and you're using ActiveX to write to your spreadsheet. If this is the case, try opening the log files yourself and watch how long it takes Excel to open them. You can do a couple of things: 1) Split your log files. Generate a new log file every hour and give it some suffix for grouping. Something like "Log_File_Timestamp_PartX" where X is some incremented number. 2) Write all your data to a temporary binary log file. When the process is complete, read the binary log and generate your xlsx. I'd recommend #1. #2 means you can't view your log files until the end of the process.
Maybe look into not flushing the data to your log file until the final iteration of the program? 
We had a similar problem, switched to TDMS format and save time decreased by a factor of 10. Still opens in Excel (with a TDMS plugin) and has the added value of having indexing on the channels. We write HUGE spreadsheets, and I have done alot of research on prolonged save times with Excel, that seemed to be the best option. You can also set up RAMdisk to save the file to which uses RAM as high speed memory access, but binary file format did most of the heavy lifting for us.
Look through the error window for an error with a red X next to it. That means it's the lowest level vi with an actual error, not just a subVI. Take a screen cap of that and I might be able to help a bit more.
The error traced down to a vi from the NI_Database_API.lvlib. I didn't understand what the red x meant before, or at least I'd never noticed that it only appeared on some errors to indicate the root of the problem. Thanks to you and some help from the NI forums, I think my labview installation is corrupt. I actually just found a coworker with a fresh labview 2016 install to open that subvi and it came up with no errors. Thanks for that tip.
Definitely not local variables. So many ways those can ruin your day. 
very standard. A well organized cluster makes things fairly organized IMO, kind of like a table of contents. And if it gets too big, then think about splitting it into a couple of clusters if theyre being used for different things (such as one for hardware config info, one for calibration info, one for state information, etc). 
Why aren't classes an option exactly? I use them on RT all the time without issue.
Extremely inexperienced dev team. Don't want to have them quit when they have to learn all the OOP stuff.
Yes I agree and if it were my project that's how I would do it. Frankly though, I don't have time to bring these guys up to speed on all the quirks of LabVIEW and how to do OOP. I'm trying to maximize the benefit these guys will get for the time I can invest. 
In my code each separate running code module (actor-like) is a queued message handler (QMH), and if an error happens we go to the error case, enqueing the state. In the error case we can read the error, and the state of the QMH and react accordingly. Were we trying to perform a read when a hardware fault occurred? Maybe we should go to the reinit case and increment a retry counter? This means each QMH can handle errors in ways that are appropriate for that code module. In the error case I also send the error to a separate running QMH dedicated to handling errors in a generic way. This will send the error, what code module generated it (and what instance if it is a clone) what device the error came from (Windows or connected embedded device like cRIO), and what were the last 4 states in the QMH we went to, leading up to this error. This error actor will get errors from all other running QMHs, log it, display it in a parallel running dialog (so the rest of the software continues if no one is there to press Continue) and optionally send the error to a host device if there is one. This is for times when an error on a cRIO or other embedded device, can be sent to the Windows PC so the user can see it. Without this the error will still be logged but the user might not know about it. The error actor also can look at errors and determine severity, and shutdown the test and whole software if an error of some severity is seen. I've also put in code to shut down the test and software if the number of errors seen is too great. If we have 20 errors we probably have bigger problems that a retry or logging isn't going to recover from and there isn't really any need to continue to try to fix it without intervention. This error actor has the history of errors to, so if the software is still running we can go to Help &gt;&gt; Error History and see what errors happened, when, by which hardware, in which QMH, and what states lead up to this error. Of course this information is also in the log file if the target has no UI, or if a crash happened. Then this error actor and the generic functionality can be reused from application to application. All that is needed is to tell the QMH to go to the error case if an error is seen, then call the subVI that sends the error, state information, and QMH information to the error actor. The part that is customized between applications is what each code module should do for specific known errors that can occur.
So lets say I have an with an abstract Sensor, Controller, and System, what I would usually do is make a class for each with overrides for specific things, and set them up in a state machine. Usually the way two pieces talk to each other is via a DVR that's passed to all of them at instantiation for a sort of "Singleton" design (e.g. Everyone can see and modify certain parts of a kind of global object). This gets bad for unit testing as you can imagine. How would AF change that? Sure there would still be an interface for communicating between actors, but how would you be able to communicate between one instance of a System and one instance of a Controller? Do they both get each other's queues? Isn't that coupled inherently?
I see, so it's okay for actors to have two way communication? They just expose public methods for other actors to tell it what to do right? Can you mediate which actors have access to which methods? Mind looking at my current architecture and talking about it? To get more specific to my problem, what I have is a controller for a biological system. [Here's a block diagram](http://imgur.com/xFPBCfZ). The application is composed of a few high level classes: 1. Controller. Only one per application instance. This is PID, MPC, fuzzy logic, AI, you name it. It interfaces with the System via a Sensor, or ideally, an array of Sensors. It doesn't constantly poll sensors, instead opting to make a series of "control recommendations" over a "cycle length" before reading the next sample from the System. 2. System. This is our biological system, of which there is only one in an application instance. It can be upregulated or downregulated via two separate hormones. It is implemented as overrides of an abstract class, so in simulations it can be a set of differential equations, but in practice it can be the actual system. The controller doesn't have direct access to the System, instead it commits its series of commands and the current cycle length to the "Control Queue", which is currently just an array of commands for each hormone, eg "From time 0 to time 5, run substance 1 at rate 2. From time 3 to time 10, run substance 2 at rate 5". The controller has methods to add to this queue, and the system has methods to consume from this queue. The queue is cleared in between cycles to keep things cleaner. 3. Sensor. This one is currently incomplete, as I just have a mathematical model to represent a sensort and how it changes over time. Ideally I would be able to have many sensors acquiring simultaneously and committing to some sort of queue to be processed for consumption by the controller. We do plan to use several sensors of many different types, each with their own separate calibration methods and considerations, so maybe sensors of one type should be lumped together, processed, and the all the processed sensor values could be lumped together for aggregation. The only way I can see this being done in my architecture is the way you warned against; just having an array of class objects and calling individual overrides as needed. 4. SystemInterface. There is only one of these in an application instance. This allows the controller to take samples from the system under different sensor error models. In real life, this is where I would imagine Sensors would commit their data to be calibrated, filtered, and aggregated before being sent as one value to the controller. All of the single-instance classes communicate with each other by having DVRs to the "common interface" of SystemInterface. I did this to make it a little more "encapsulated", eg controllers only know about this one interface and have no way to communicate directly to the system. Do you think I should switch to AF? Sorry if this is way too wordy or abstract. I can post code pictures that are a little more concrete later tomorrow when I get time. 
Thanks for the reply, i've actually been doing this, but i've been a little selective on which errors get written. I guess it really does make sense to write all errors to the log.
Thanks! That sheet will come in handy!
Here's a good article: http://digital.ni.com/public.nsf/allkb/DCB90714981A1F148625731E00797C33 You need to know what functions are in the DLL. Look at the example for passing data mentioned in the article. More in depth one here: http://www.ni.com/tutorial/3009/en/
I second this. However, i would recommend doing a "cluster of clusters". Each one being a type def but split up into a defined area of interest. However, i usually use classes for this sort of thing instead of clusters. I really wouldn't recommend Local Variables unless its a GUI heavy portion of the program and even then they are iffy. If you want a really unpopular answer, you could put the state machine in a library and then add a private global variable to that library. You can get really hacky that way :) Keeps the global data only usable in the state machine, but breaks data flow, which i'm not usually against as long as you follow the standard one writer multiple listeners approach. 
The links I usually give out are the ones at the bottom of this page. https://decibel.ni.com/content/docs/DOC-40451 There's a decent variety, and the myrio guide is a good one for basic sensor, and small electronic information. Beyond that NI does have official classes, but I'd take advantage of the free stuff first.
Take core 1 and 2. Come back with specific questions. 
can you post the front panel so we can see what settings you have going into everything?
This is typically because that samples per channel are too small compared to the sample rate. I would normally set the number of samples to be around the same as the sample rate. So for 50kHz make it 50kS giving a 1 second buffer. Then on the read node, I will read about 10% giving a 10Hz loop rate (so 5000 in my example). This tends to be a good ratio. Too small and you can't achieve the rates to keep up with the hardware. (In actual fact DAQmx will often override this and make it larger anyway -http://zone.ni.com/reference/en-XX/help/370466Y-01/mxcncpts/buffersize/). In the example, you read the whole buffer at once. That means if the acquisition gets to the buffer before you managed to remove a sample then you have the overflow - there is no room for delays!
Ah that tells me exactly the behavior you're seeing, and it makes perfect sense. When you set up your task, you're setting up the daq card's clock to take samples at a specific rate (1000hz in this case), then shove them onto a buffer. You're then pulling them off the buffer in 100 sample chunks, so you need to do that at 10 times per second to keep your buffer from increasing. When you use a breakpoint, you're stopping your program from grabbing points, but it doesn't stop the card from taking those samples and putting them into a buffer (you would miss data if you did, which you don't want in a daq application), so you're filling the buffer up while your breakpoint has the program stopped. This is expected behavior and a fairly common thing to have happen with new users because they're not used to having things hardware clocked and don't realize that when you do, you can't just stop chunks of the code willy nilly.
That makes perfect sense, and confirms my suspicion that it was related to my lack of understanding of how the hardware clocking affects the software process. I primarily use MATLAB and am very used to debugging programs by 'walking' through them using breakpoints. Thank you!
http://i.imgur.com/jvqgysW.png
Awesome man. You're totally going to pass this class.
Looks like you already found a solution. But in the future you might also find it useful to make an Express VI, configure it the way you want, then right click it and Open Front Panel, which will turn it into a normal subVI. You can then dig into the code and see how it works.
If you don't already know about it: Click "Find Examples" under the help menu in LV (either the splash screen or an open vi). Click the search tab and you can find many example programs for various tasks. These examples are canned functions with descriptions and can be very useful for learning basic functionality without having to sift through a mass of code which you'd find in a full blown program.
Thank you. Where would I look for information on doing this tttanner?
What kind of pressure sensors? What do you mean by network them? Why not use analog pressure sensors and wire all the voltage inputs to an analog acquisition module?
Are you connecting to any hardware? If so, http://npoints.com has some pretty good examples to get you started with different transducers. If not, then some of the other folks have offered some good places to start. I'll also say that there are TONS of examples that come preinstalled with LabVIEW that can often help.
[Something like this](http://www.keyence.com/products/process/pressure/ap-c30/index.jsp) We built large machines and we like to network sensors and reduce wiring as much as possible. By network I mean something like CANBus, RS485, IO-Link, etc. Basically a way to daisy chain them all together, such that there is only one home run cable back to the DAQ or master etc.
There are training courses out there. For the CLAD, you would want to take Core 1 and Core 2 courses. There are also practice CLAD exams. If you google "clad sample exam", the first link is to 2 good ones on the NI website.
NI has a couple of practice tests at http://www.ni.com/example/30225/en/ You can also search for "ni clad practice tests" but the few I've found are just the official practice tests with questions in a different order.
The free training materials at the bottom of this link is the stuff I usually pass out. https://decibel.ni.com/content/docs/DOC-40451 But for you I'd also highly recommend checking out the [CLAD Nugget](http://forums.ni.com/t5/Certification/Certification-Nugget-CLAD-Certified-LabVIEW-Associate-Developer/m-p/3171006) 
If the devices are identical. Then I see no reason why controlling two would be twice as hard as controlling one. So I don't even think it even scales linearly let alone exponentially
First of all, you would need some kind of IO device to interface between your computer and the Peltier device. That is going to cost as well, and if the ~$70 price range for these PECs is too much, I would expect the IO device you will need will also be outside your price range. Secondly, it will require some LabVIEW development, and if you're a novice with LabVIEW, it will require some time investment to get up to speed. The question is how much is your time worth? For hardware, looking at something that can support a 2.5A output (like the PECs you listed), you'd be looking at something like this for hardware: [cDAQ 9171](http://sine.ni.com/nips/cds/view/p/lang/en/nid/209817) - $287 [NI 9477](http://sine.ni.com/nips/cds/view/p/lang/en/nid/203524) - $460 Granted, the 9477 is a digital output module, but it is the only way you'll directly be able to provide 1.5A of current without building some external relay circuitry. On top of that, you'll need a power supply, but you're probably already factoring that in. The software required to implement the control won't be too complicated, but if you're just starting out with LabVIEW, you could expect to sink several hours/days of time in getting up to speed. All of that considered, and if your goal is cost savings, I'm not sure NI products and LabVIEW are going to be a great solution for you. 
Do you mean a peltier cooling plate? Yes those are easy to control. The biggest cost will be the power supplies. I used a cheap USB daq that could take a thermocouple input and had enough output to power a solid state relay, to basically make a pulse width controller. I used python, but LabVIEW also has a pid function that can calculate the pulse width based on the temperature. 
Yes order maters, and the number of channels can effect the sampling rate of agrigate sampling hardware. NI also has an article describing signal ghosting, which can be an issue when sampling signals with varying impedance levels. In these cases the read value of the second channel, can be effected by the value read from the first channel. http://digital.ni.com/public.nsf/allkb/73CB0FB296814E2286256FFD00028DDF
Thanks for pointing me there! The free training has been a great introduction!
Thank you for replying! I have checked some of the examples and they are pretty nice! Your advice is really helpful, since I can't really afford the paid courses as a student.
Thanks for the reply! I have checked some of the examples and they have been great!
NI Vision is something I'd like to play with in the future. For this application, licensing is something we want to avoid. The other issue is the time required to port the existing Matlab code which is treated in the industry for this application as the standard. I really can't mess with it and it needs to stay in Matlab as is.
APFI is an analog trigger terminal. Further, it's only for input. You can set a trigger for "start my AO when the signal connected to APFI0 crosses 1.234V". You can't export anything from APFI, though. ao/StartTrigger is a digital trigger. It can be refer to either an input OR an output (or both). It sounds like you want it to be an output. You can export 'ao/StartTrigger" onto PFI0 (no 'A')
So the physical trigger from my PB to my NI card should be PFI# and then the mirror is ao/StartTrigger? Or the opposite?
hmmm. Well our card is being externally triggered (by PB), which I believe is what the mirror is used for. So if the external trigger goes to some PFI# as the source, what would the destiniation be so that I could perform an AO task?
I still don't know what mirror means. That's not a DAQmx term. If you want your AO to be triggered externally, then you will route the other way: source="Dev4/PFI0", destination="Dev4/ao/StartTrigger". And also, you CAN use your APFI for this case, so if you still have it wired that way, then that works Then you can call DAQmx start and it won't actually start until something causes a single pulse on PFI0 (or APFI0). Any additional pulses on PFI0 after the first will be ignored until you stop &amp; start your task again.
Im going to try that. Im not sure what mirror exactly is either (I inherited this software from a non-responsive post doc), but whatever goes under mirror is the destination in that VI. When I do that, I get &gt;DAQmx Connect Terminals.vi:4600001&lt;append&gt; &gt;&lt;B&gt;Source Device: &lt;/B&gt;Dev4 &gt;&lt;B&gt;Source Terminal: &lt;/B&gt;PFI14 &gt;&lt;B&gt;Destination Device: &lt;/B&gt;Dev4 &gt;&lt;B&gt;Destination Terminal: &lt;/B&gt;ao/StartTrigger &gt; &gt;&lt;B&gt;Required Resources in Use by&lt;/B&gt; &gt;&lt;B&gt;Source Device: &lt;/B&gt;Dev4 &gt;&lt;B&gt;Source Terminal: &lt;/B&gt;PFI0 &gt;&lt;B&gt;Destination Device: &lt;/B&gt;Dev4 &gt;&lt;B&gt;Destination Terminal: &lt;/B&gt;ao/StartTrigger &gt; Error code -89136
So we control our entire experiment, which has multiple AO and DO to trigger lasers/aoms/blah blah. The AOs can be set to do various ramps (working in cold atoms so we use lots of microwave ramps and whatnot) We want the external trigger for timing purposes. So we want to write a single point (many times over a ramp) 
Okay, well to get over this hurdle, it sounds like you need a call to DAQmx Trigger.vi. Choose Start-&gt;Digital Edge from the drop-down and wire in the source ("Dev4/PFI0") and optionally the edge (the default is rising edge, so if that's what you want then you dont have to wire it in) If I disappear, you might have better luck on the [NI official DAQ forum](https://forums.ni.com/t5/Multifunction-DAQ/bd-p/250)
Looks like you're getting plenty of help, and this may help too: http://digital.ni.com/public.nsf/allkb/E539D226A643C1CE8625715E007C23C8
Have you written the LabVIEW code yet? Can you post it? If you have a working LabVIEW application in the development environment all you have to do is create an executable and installer in the project. You can find some helpful information here; https://www.youtube.com/watch?v=V_xnJHgct6E and here; https://www.youtube.com/watch?v=V_NsSOVx3wc
What I've done before is use property nodes to get the calibration information programmatically and display a pop up reminder when calibration is coming up/overdue.
One of the stars of the NI forum replied below, functions. measure i/o / ni daqmx / advanced / calibrate I've just finished core 2 building appllications module, the exercise made sense but avoided using any real hardware drivers. So my questions are. 1, Does the target machine need the DAQmx + Max driver installed ? 2, If it doesn't, do I need to bundle the driver in the application build somewhere? 3, If it is either installed or bundled can I still use it to recalibrate the channels inside my compiled VI? Answer: The target machine will need the daqmx drivers and it will need runtime engines. Best way to do this is create an installer, you do it in a similar manner to building an exe. And yes, you can still calibrate your channels at runtime if you build that into your application. Max is also free so you can always include max with your installer and you can manually calibrate if you prefer. Attached is a tutorial very geared towards what your asking. http://www.ni.com/tutorial/5406/en/
Thanks for the reply I've not yet written anything I'm happy with, I'm looking at a master/slave type layout because I only want to log once every 5 sec but I want update the front panel/UI at 10hz. Ideally I would log the average of the front panel readout as I don't want to miss data but the senors don't respond quickly. I've followed your links and that aspect is starting to make sense thankyou. Have you any suggestions: the spreadsheet (TDMS) needs a time base in hh:mm:ss starting at 0 when the experiment starts, plotted against pressures and temps. 
there's probably a couple of ways. you can just return the raw data from the mathscript node and then display it using labview's 3d surface plot control. i would recommend this way. i am using matlab on a current labview project. matlab just does the analysis while labview displays the returned data using its native controls. the mathscript node works via activeX. matlab might allow you to embed activeX controls in labview. i wouldn't recommend this way, even if it's possible. http://zone.ni.com/reference/en-XX/help/371361N-01/lvconcepts/types_of_graphs_and_charts/
Unfortunately my German sucks. I could maybe order food at a restaraunt or ask for directions but couldn't have a real conversation that didn't sound like the lyrics to a Rammstein song.
Hey thank you so much! That got me exactly what I needed. I'm actually working on a project to make a robot that'll basically plot polar functions, so I've taken a look at the polar charts in LabVIEW and I'll completely agree with you on that :P Cheers!
Your application should be an executable that gets a version number when built. There are VI's to grab the executable version number and you can display that on your about dialog.
Instead of using the Get Date/Time in Seconds function, just use the Format Date/Time String function. It saves you a step and the format string is simpler. [Block Diagram](http://i.imgur.com/tjuawMF.png) [Front Panel](http://i.imgur.com/vLcUPVa.png)
I use the one from the MGI toolkit. You can find it on the VI package manager. Alternatively, there is a built in one but it's not on the pallets. I'll try to remember where it is when I'm at work tomorrow and have LabVIEW in front of me Edit - screwed up my reply on mobile, whoops
Dig through your build specification options. Theres a build log file option in there, defaulted to unchecked. Check it, build an exe, and behold. Every vi in the project, revision number, build date, etc.
Vi.lib/platform/fileversioninfo.llb/fileversioninfo.vi
Do you mean in an exe or in a pre build? In an exe there isn't any build spec name. And as for in a pre build you can look at the one I linked to which iterates over all build specs in a for loop. With each reference you can get the name with a property node, selecting name I think. Edit: oh I think the build name is passed into the pre build vi. 
to clarify - why cant i dump an array to a group of connector lines? other than serializing in a for loop... or wait.... shit i think i just ansxwerd my own question... for loop with iteration parallelism???
I dont think you can programatically select what connector index/pin you're writing to though...
yea looks that way, unless someone else wants to chime in here...
ah that trick is awesome... thanks for that
You can use the DIO0:8 IO node and convert the boolean area to a U8. Not at my PC so cant send a screenshot. Will try to get one later.
Don't be afraid of large data types in queues. Queues are absolutely the way to go when streaming large amounts of data. A producer/consumer design pattern with queues sounds like a great way to go here. The acquisition loop would be your producer and the display loop your consumer. If you're truly worried about memory, the DVRs into a queue will help. DVRs are pointers if you didn't already know. They'll keep you from creating several copies of your data in memory. It's not complex either. Take your array of data and throw it into a "New Data Value Reference" vi. Enqueue the reference. Dequeue the reference and wire the DVR reference into a "Delete Data Value Reference". The output will be your original data.
I take data on my loops and control things every ~250 ms. Some loops I have set on a longer duration. Yes it is weird: you can see my post from up top: "The memory will bob around with some noise. You know 134050 kb then 135100 kb then 134500 kb, but at a certain point the code will start reading the same memory over and over. At this point the problem is occurring. If I rush back to the computer, and click the VI, the memory will rocket up instantaneously."
Isn't this what channel wires were designed for? Communicating between two loops running at different speeds? 
Are you using the "run continuously" button? If not, how are you running the loop more than once?
Sorry I've never done the VI snippet, I didn't realize I needed one with each case. Each case is either +1 or -1 to column or row. I did have them inside but I was still getting the same problem. The problem is that when each individual case is selected it deselects the previous case, which I understand why. I am trying to figure out how to compound the results in real time from each successive button hit. So that when I hit North then East it shows the combined coordinates, not just north then east individually. Thanks for the reply!
Yes, but I am still on 2014 on the majority of my systems. Besides channel wires are basically just various types of queues or notifiers with a simplofied implementation and different optimizations. Also, I'm not sure I could make channel wires look clean my architecture.
That's interesting, however, the way in which I've set up the motors is such that one of them moves the position diametrically and the other moves it rotationally, so I guess in this particular application leaving the data in polar coordinates is better no? 
The NI site does a pretty good rundown of what LabVIEW does on Linux: http://www.ni.com/white-paper/11786/en/ Are there specifics you're concerned with? One thing you may need to review is the supported versions of Linux: http://www.ni.com/product-documentation/52786/en/ 
Purely operating under the assumption that the queue ends up creating a copy of the data in memory or has to allocate a new memory space when dequeuing the data. DVRs prevent memory copies and reallocation. If those assumptions are not true then, yes, putting a DVR in a queue gives absolutely no benefit at all as opposed to just enqueuing the data. Now, even if the assumption is true, using DVRs is most likely not going to be very beneficial unless you're dealing with huge amounts of data (several hundred megabytes and greater). Just wanted to help explain how to implement the DVR solution since /u/mogulman31 mentioned it in OP.
The create and delete DVR primitives will do a data copy. The enqueue and dequeue primitives will do a data copy. The memory consumption is the more or less the same.
What? 
Use IMAQ fill to create an all black image of your desired end size. Use imagetoimage to copy the smaller image into part of the bigger image.
LabVIEW for Linux is _not_ just the Windows version running under WINE. It is a native Linux application using X11 directly. Source: I worked on the LabVIEW team for 10 years and was one of the Linux developers from 8.2 (2006) through 2009.
I'm not sure what you want to do is supported. The Tegra is an ARM system. There technically is a LabVIEW for ARM Linux, but it's for the real-time (RT) cRIO devices that NI sells, and it's only supported as a target (i.e., the editor doesn't run on the ARM device). It's possible that you can get LabVIEW to build for a third-party ARM embedded device these days (I haven't kept up with this), but the editor itself only supports x86 and x64 for Linux.
sound advice - I ran into plenty of issues just assuming I could get it to install on ubuntu...
That's a very good source. What do you mean "uses X11 directly"? Do you mean LabVIEW has it's own UX library? That's probably not even true on windows. The LabVIEW interface looks *exactly* like wine. Maybe the backend is native and the GUI uses the wine UX?
Yes, LabVIEW has a home-grown platform abstraction layer that includes a UI framework. Internally most of LabVIEW's code is written against the abstraction layer, and then the abstraction layer is implemented using Win32 on Windows, X11 on Linux, and Cocoa on macOS (ported from Carbon when it was made 64-bit). The Linux implementation directly calls X11 functions. It doesn't use GTK+ or KDE libraries, and definitely not WINE. There is some support for using GTK and KDE libraries to render some native controls, but you can change whether or not it does that (and which one it uses) in the settings. Either way, in the end it's all just X11 calls to get stuff on the screen. 
Interesting. Thanks for taking the time to explain it. I can't find an option to change to GTK, could you elaborate where that is? 
i agree with the above points. I will add, try this tutorial https://www.youtube.com/watch?v=wTJr8beDtvQ i think using the vision assistant would be a good place to start on your problem 
I now have a specific question... I am attempting to run two cameras simultaneously and not having success. I'm running two different while loops in the same VI with different IMAQ Create blocks. I can display two identical images from the same camera side by side, but when I switch cameras, this approach fails.
You're going to need a 3rd party library. Checkout PDF sharp. Look at the c# examples to get an idea how to do it in lab view
Is this python 2 or 3? It sounds like labview is assuming utf encoding and python is using ascii by default. Try: my_bytes = my_str.encode('utf-8') Then you can follow the tips in this post to convert my_bytes into floats: http://stackoverflow.com/questions/1537862/read-bytes-from-string-as-floats
Most of us don't use typecast. Most of us use unflatten from string. Is a human entering the input string? If so, you need to change the [display format on the string to hex](http://zone.ni.com/reference/en-XX/help/371361H-01/lvhowto/hex_display/). That will also make it much easier to post the string on the forum without having to worry about non printable characters. It will also be easier to enter into python. 
Documentation is important, but it's only 10 points. I agree you should document as you go, but don't waste time making nice documentation if you don't have functional code. You cannot pass if your code doesn't work.
Remember this and spend your time accordingly: Style: 15 pts Functionality: 15 pts Documentation: 10 pts. Passing score: 28 pts (70%) The CLD is really mostly focused on seeing you can use and develop using a queued message handler (glorified state machine). Start with the template project in LabVIEW - it will save you time and will have most of what you already need.
To be clear, the code doesn't have to be 100% complete, and you can comment out the parts you can't complete with what you would have done. The parts you do complete should function as directed, though. I got maybe 80% of my functionality completed and still passed easily. 
Edit:Nevermind, found it. File-&gt;Create Project-&gt;Templates I never use the templates but since I can't bring in my own code to start from I'm trying to find the one you're refering to. Are you perhaps talking about the Producer/Consumer Design Pattern (Events) template? I can't see a QMH template.
In the CLD my guess is that you dont want to use the project template, due to having to keep the file structure intact. I would think the best bet is the Producer/Consumer Design Pattern (Events) template.
Thanks. This breakdown is really what I have to keep in my mind. I want to make sure I dont get caught in the minutia.
Awesome. Thanks so much!
1. Get sensor value 2. Interpret sensor value 3. Feed interpreted value into PID controller, which makes control recommendations (eg turn the wheels by so much) 4. Send out control steps to the wheels 5. Go to 1. Without more specifics that's the best we can do.
LabVIEW is correct. Compare what LabVIEW is showing to the ASCII definition ([click here for table](http://www.ascii-code.com/)). You get the following, broken down by byte: Hex| ASCII ---|--- 9D| (invisible character) 80|  66| f FB|  4B| K 09| (invisible character) etc., etc., etc., LabVIEW is correctly mapping the hex values to their ASCII representation and vice versa. In Python, it looks like you are using Unicode instead of ASCII (str). This takes those symbols and encodes them differently than ASCII does. ASCII is 1 byte per character. Unicode uses different formats, but it is more than one byte per character. It looks like the hex result that you generated from Python is UTF-8, not ASCII. [This website lets you input a character string and see the representation in different Unicode formats](http://www.endmemo.com/unicode/unicodeconverter.php), copy and paste your original funky character string into the "Unicode Character" field. Then hit "Convert," and look at the result in the UTF-8 field. The hex matches the hex that you got above. However, it is not ASCII, which is what LabVIEW uses. So, basically you have two different systems for taking hexadecimal data and pairing that up with actual symbols/characters. To fix your problem, you just need to think through what produces the data and what is consuming the data. Typically, what you care about when you use strings like this is the actual data being encoded (the hex data). You don't really care what symbols get paired up with the data. So however your application is designed, you just need to make sure that the actual hex value of the data is preserved between code modules.
You should also consider using a deadband. I wrote a heating control that used it. In my project I would have the thermostat set to 70 with a deadband of +/- 1 degree. If the heat was off, it would wait until the temperature dropped to 69 then it triggered the heat. It would then heat to 71 and turn off. It would wait until it dropped below 69 to enable it again. It probably is similar to the other methods, but I didn't generate a truth table.
Realistically the truth table is overkill, but it's a fun exercise in how to solve the general case and work is slow today.
As a simple sanity check, I would re-point to the DLLs on the new PC, and re-select there methods from the library node configuration window. LabVIEW is very finicky when it comes to loading libraries, so I always try this. If that doesn't fix it, my best guess would be a dependency issue. You can download [this application](http://www.dependencywalker.com/) and check the DLL dependencies on each PC. It can be hard to read at times, but if you post a screen cap of the results on each PC I can try and help you further. 
Do you have a sensor or something that will allow you to know when the pendulum reaches +/- pi?
Awesome! Thank You so much! Works as I wanted! Thank You so much again! :)
That behavior of outputting 10V when idle doesn't sound correct. However, if you're measuring with another DAQ channel, [you may measuring incorrectly due to differential vs single ended, etc \(look for table 1\)](http://www.ni.com/white-paper/3344/en/#toc1). You're going to want to look into power up states or power on states if that doesn't resolve it. 
Not at my computer but can give you some where to start. Under the file pallette there should be a vi for reading a delimited file to a 2D array. You can index the array to pull out your sample count and actual data. Sounds like you need a Fourier transform. LabVIEW has a built in vi for that too. FFT.vi FFT.vi takes sample rate and num samples as inputs. Also string pallette has vis for typecasting strings to numbers.
Great advice but one thing to add is the file IO function you are talking about is a polymorphic VI and can read a text file and return the values as doubles without needing to convert from string. Right click the VI and choose select type and select the appropriate one for what you want it to return. The function name changed in 2015 but I think it is read from spreadsheet file, or read from delimited text file.
Did not know that. I rarely use that VI since it used to not have error in/out. I know that's changed but old habits die hard. Can you control the formatting of the numbers?
I agree about not having error information on the old version. The new one does have formatting and a few other nice features. http://zone.ni.com/reference/en-XX/help/371361N-01/glang/read_delimited_spreadsheet/
I'm not 100% sure, ~~but I think you need DAQmx 12 if you are using LabVIEW 12. I know that I have LabVIEW 15 and 16 and I have to have DAQmx 15 and 16 for them.~~ Edit: I'm wrong, ignore me
I'm sorry but this is incorrect. Think of DAQmx more like hardware drivers for Windows, and you can only have one version of the drivers installed at once. You cannot have DAQmx 15 and 16 installed at once. Installing 16 will remove 15. Luckily support with LabVIEW spans several versions and the read me should make it clear what versions of LabVIEW work with what versions of DAQmx. In this case the 16.1 readme states that it works with LabVIEW 2013 and newer so that is OPs problem. You'll need to uninstall DAQmx, and reinstall a version that works with LabVIEW 2012. http://www.ni.com/download/ni-daqmx-16.1/6424/en/ By the way this scheme of versioning is the same for all other NI products just like how NI-XNet 16 will have support for LabVIEW 2013 or newer.
I don't have a computer to test with at the moment but I'm pretty sure you can only have one version of DMM installed too. Run MAX and see what software it shows as installed software. I'm guessing it only lists one.
[LabVIEW 2012 is not supported by NI-DAQmx 16.0+](http://www.ni.com/download/ni-daqmx-16.1/6424/en/). Make sure you follow the install order, LabVIEW before drivers. [The last version to support LabVIEW 2012 was NI-DAQmx 15.5](http://www.ni.com/download/ni-daqmx-15.5/5901/en/). Also, [your overall NI-DAQmx vs LabVIEW compatibility page for future reference](http://www.ni.com/product-documentation/53326/en/).
Use the "wait" function, for example if you want it to loop every 2 seconds you would put a wait function with a value of 2000 ms in the while loop. It can be anywhere in the loop, I believe it will perform all operations in the loop before waiting the 2 seconds. You can make the wait happen first if you want using the flat sequence structure.
Shit, I can literally just dump a wait function anywhere inside the loop without wiring it up and it will just work?
Well wire a numerical constant to it to tell it how long to wait but yes, that is the general idea. One thing I stated in my previous comment was incorrect, the wait function will happen in parallel with the other operations in the loop unless you use a flat sequence structure to make it happen serially.
http://www.engr.sjsu.edu/bjfurman/courses/ME120/me120pdf/TemperatureRunningAverageExercise.pdf
use search string functions, if the resulting index is greater than or equal to zero, then that letter is present within the whole string. LOGIC: the index feedback is -1 if the letter string input is not found
That's a tough one. I used the Biomedical toolkit to read TDMS files created by a Texas Instrument device and then it would break the heart beat into their portions by determining the wave in the tdms editor (Not the actual name) in the toolkit. I just named them based on their arrhythmia. In fact my method of sorting always involves folders and naming everything. What context are you using he files? 
i could be misremembering, but i think you have to have the DSC module for out of the box search functionality.
DIAdem is the best way, specifically the datafinder back end that ships with it.
Easiest = import to excel
Using the System controls can produce a user interface that looks the same as what you would get from Visual Studio. You could try mocking up a user interface using something like C#, which will show you what a standard out-of-the-box Windows UI would look like, and then try to make your LabVIEW front panel match. Do you have any further information about the requirements? Is there anyone can look at an initial front panel mockup to make sure it meets their needs?
&gt; Is there anyone can look at an initial front panel mockup to make sure it meets their needs? Supposedly there is, but we are still in the requirements gather stage, so I wanted to make sure I was on the right track. Thanks.
Can you use .NET Framework in your project? I did a similar thing more than a year ago using C#, WPF and MahApps library. But it was just a tiny proof-of-concept demo ([here is a short video](http://www.screencast.com/t/inMUUcpp)). I didn't use it in any real projects. The idea was the same as you described, but instead of CIN I've used .NET palette. Here's what I did: * In Visual Studio create a new project - WPF Control Library * Replace UserControl element with Window * Build the UI using the full power of Windows developer tools Microsoft gives you * Expose public properties, methods and events to interact with UI elements * Compile this project into DLL * In LabVIEW use .NET palette to call that DLL. Window will show up when DLL is called. * Configure your VI to hide its front panel as you don't need it anymore. * Use .NET palette to access public properties, methods and events of the DLL. It seems that I've lost that demo project. I can recreate it if you wish. It was an interesting concept but eventually I abandoned that idea, because who needs LabVIEW if you are already writing code in C# and XAML?
If a customer sent me a requirement that said it needed a "windows environment user interface" I'd interpret that as "the UI needs to run on windows". So I'd just write a normal, good looking LV UI If they actually meant what your thinking I'd show them the option of what it'd look like in LV with system controls (change the background color too), and explain to them that this is what you can do for very little money. If they want more windows looking I'd say it will cost them a good bit more for almost no return in investment (unless you have a lot of c#\++ experience). If I HAD to do a "windows" ui, I'd write it in c# and compile my LV code into a .net assembly. alternatively I'd look at writing a .net winform that I could jam into a .net container in LV
Don't make it all at once and don't write it all at once. Use the finite waveform write, then just append some set of new values. Maybe something like 100,000 samples at a time. Then periodically check the buffer, and when it is getting low write another 100,000 values, making sure your buffer doesn't get empty, and that your buffer can hold 200,000 samples at a time. Not at a computer right now so I can't test or post example code sorry. Edit: I just realized you probably do want the continuous output mode, that way the buffer will wrap back around, but you can replace the values in the buffer. Might be a bit of a pain for sure.
A state machine architecture could work, but only if the periodic work is short compared to the period of the continuous acquisition. This has the advantage of staying simple in structure, but allowing for some flexibility and is a great starting point for new users. However, it will break down quickly if the periodic work is active for longer than a fraction of the readout period.
It's worth it just to go pick hooovahh's brain alone.
Aww thanks, but you can do that anytime. Oh and one other thing I forgot to mention. If you aren't an Alliance Partner Monday kinda sucks. They call it Alliance Day and NI has some technical sessions just for them (nothing too spectacular and most of it ends up in the rest of the conference or online) and they get a special keynote where NI discusses adoption rates, 3rd party success stories, and other semi-confidential finance stuff. There is the Academic Forum on Monday but it is mostly just professors talking about how they put NI in the classroom, or seniors talking about their final design project. One other Monday activity that is all day long that I've considered is taking the hands-on with RT and FPGA classes which cost extra but you get to come home with the [RIO Evaluation Kit](http://sine.ni.com/nips/cds/view/p/lang/en/nid/205721) which is an FPGA and ARM embedded device running Real-Time Linux. Oh and if you want to take a certification they are offered at a discounted price. I had a chance to take my CLA there, but decided I didn't want to spend all my time studying, or focusing on that, and then waste half a day of sessions taking it. Still some may find it worth it. You can usually sit in on some of the training prep even if you plan on taking it in the future.
Is Alliance Partner Monday worth it if you are an Alliance Partner? We actually are. The hands on with RT and FPGA seems pretty sweet. That might be worth going. Thanks!!
Yes I've learnt that since (this was indeed my issue), I've now used a queue system to transfer commands like a producer/consumer (thanks lilmaniac2) with good results. Whole things working great now!
Oh well in that case the Express VI Elapsed Time would probably work fine. It will return a true once after the amount of time specified has elapsed. The idea is to call this function over and over in a loop, and then have a case structure wired to the Time Has Elapsed output. In the true case you can take your most current sample, in the false do nothing.
Yea they still suck. One of the maaaany reasons I will never touch LV if I don't have to. Great for small scale rapid time-to-test. Terrible for everything else. Edit: In hindsight it seems a little masochistic that I'm still subscribed to this sub...
That's what LabVIEW is made for
What is N? Can you give an example?
I don't have my LabVIEW PC on me, but off the top of my head my approach would be: 1) use the 'add array elements' function on the integer array e.g. 2+3+5=10 2) feed result into 'initialise array' to make a zero array of this length (use a boolean) 3) make a for loop and wire the N count terminal as 'array size' first element (e.g. 3 rows, use index array element 0) 4) feed the &lt;2,3,5&gt; array in to the loop and set to auto index by right clicking on the tunnel 5) use 'replace array subset' with array input as your zero array, index as the auto indexed value minus 1 (because it is zero based) and new element as boolean 1. Result is the output array. This way of doing it should be nice and scalable. Let me know if you get stuck. The help files for the array functions might come in useful: https://zone.ni.com/reference/en-XX/help/371361J-01/glang/array_functions/
Have you tried double clicking them? Otherwise, no one will be able to help because you provided no details at all. 
From the image you really can't. What is the function that you are trying to achieve with this code?
Unfortunately I do not have a working copy of LV on this computer, so I can't be sure, but you could try using a property node for *this VI* and setting the "locked" property to True just prior to your subVI launching and False just after your subVI returns. I am not positive that this property is accessible through the property node though. You might need to do some searching. You could also launch your sub-VI from within an event structure that responds to pressing some "launch sub-VI" button or something, then setting the "lock front panel until event code is executed" (something like that) property for that event case. That may have some unexpected consequences though, so I would recommend trying something else first.
The easiest way in my opinion would be to set the 'disabled' property to either 'disabled' or 'disabled and grey' (they might be called something slightly different) before the subvi then set it to 'enabled' after the subvi. If you're unfamiliar, just right-click on the icon for the control on the block diagram, select the property node, and pick the one called disabled then change it to write and set the value you want.
The image is showing the general pattern. Think of it as pseudo-code to show you the general approach, not a specific example that can run.
Thanks for the help, I ended up figuring it out
Are you using LINX? I would guess you want [Digital Write N Chans](https://www.labviewmakerhub.com/doku.php?id=learn:libraries:linx:reference:labview:digital-write-n-chans)
So you want something that can repeat your interactions with a UI that you built? Someone made a thing for that, although I must admit I've never used it. You can check it out at: http://forums.ni.com/t5/NI-Labs-Toolkits/LabVIEW-UI-Automation-Tool/ta-p/3521765 
This was the answer. Thanks to all of you that answered the question. The Lock Panel was the solution. Regards
Yes this was the answer to lock the front panel thanks bros
Can you just not record the commands in a text file or array in memory then have a loop which reads them and runs them? 
He means the code to control the robot and if you can get a screen of the error might help as well.
What hardware are you using? The multifunction DAQs have an encoder input that will just give you the position as an integer. Very little coding involved.
Don't really know how to do that. Or where to start with it. + will need the timing also. I think i found a VI that does just this but i couldn't figure out how to add my own toggles as it seems coded for each toggle. 
holy crap this software rocks! obviously not as "customizable" as diadem but way better for simple stuff. The search function seems solid, too. thanks for sharing!
Thanks, will try but not for a couple of weeks 
No problem, glad you like it! Let me know if you have any suggestions.
Did you end up passing!?
I did! Thanks for asking!
Solve the equation analytically and use the math functions in LabVIEW to get a value for x. EDIT: [Link to solution below](https://www.reddit.com/r/LabVIEW/comments/5oasg5/labview_solve_x/dciow1t/)
I don't think there is a closed form for this, aside from using the Lambert W function. Does LV have the W function? If so the solution is: x = W(3 exp(A))/3
Good point, I didn't look at the equation super closely. I don't believe there is a W function in LabVIEW.
For solving, do you have a numerical method that you are looking to implement, and you just don't know how to do that in LabVIEW? Or are you wondering if LabVIEW has a solver function? If the latter, then no, that's not really what LabVIEW is for. But if you have a numerical approximation method to implement, you could do that in LabVIEW just as well as in any other language.
I'm just looking for a way to program this in labview. it doesn't matter how. 
[Here](http://i.imgur.com/Thiz9ms.png). I worked out a simple solution for you using a simple fixed-point iteration method. Here is the theory: You isolate the variable you are seeking on one side of the equation, as a function of itself. Then you can use this to iterate. For example, x=g(x) becomes x_next = g(x_prev). In your equation, isolating x gives: x=(A-log(x))/3 That becomes, x_next = (A-log(x_prev))/3. Take that equation, and feed it into a loop with a shift register, and as long as you don't pick a starting value that causes your approximations to diverge, you'll have your answer. To know when to stop, you just check the approximate error, which is just: Error = |(x_prev - x_next)/x_next|. Set a threshold for how accurate you need your result to be, and use that as the stop condition.
See my answer [here](https://www.reddit.com/r/LabVIEW/comments/5oasg5/labview_solve_x/dciow1t/).
First, explain how you would do it an another language.
http://imgur.com/a/otACd try something like this ^ and you want to select analog&gt;&gt;singlechannel&gt;singlesample&gt;DBL on the polymorphic selector with the daq write vi - and add a wait like in my pic for 1000ms
PS I'm aware that my front panel shows increments of 1V, this was just for proof of concept, you can see why I'm hesitant to put in 200 entries for the values from -10.0 to +10.0 in 0.1 increments.
I'd need to double check when I get to the office tomorrow, but if I understand what you're asking you should be able to use the VISA find resource function.
Hello! I tried doing this like so: http://i.imgur.com/iCXgTQH.png As you can see i got A COM port but no usb, I think it would be important to point out that the PC recognizes it as a Human Interface Device. Not as a USB Device. But on the listing that appears on the driver wizard i am able to see it.
Is the driver installed already? If so, you should be able to see it in MAX and the VISA find should see it too. If you're trying to do this pre-driver, IE it doesn't show up in MAX, I believe you'll need to dig into the windows API to find it.
Thanks for the suggestions y'all, I really do appreciate it, and they worked! I've got it functioning as I'd like, though I decided to go with a for instead of a while. I've attached the VI snippet here in case anyone is curious or needs to use the same. The case structure is because I have another VI that just holds the voltage at a constant value. http://imgur.com/a/0t1mT Thanks again!
Indeed the driver is still no installed, im trying to determine if a keyboard is present or not. But it doesn't show up neither on MAX nor VISA. And the PC Detects it as a Human interface device.
OK, that's a little different. Try looking at this https://forums.ni.com/t5/Example-Programs/Check-if-a-USB-Device-is-Connected-to-the-System-Windows/ta-p/3504405 
I don't have LV in front of me, but I believe you want to right click on the control/indicator on the block diagram and click "View as Icon"
Yes it is an active CAR confirmed by NI support. Thanks for the link - I'll check it out. I only need to monitor which simplifies it a bit. I'll let you know how it goes. 
I've built applications like this in the past using webapps and REST. I don't use Remote Front Panels anymore - I just make it look like what I want it to look like in the php, html, etc, and create methods for LabVIEW to hit.
I'd love to learn how to do that. Is there a walkthrough posted anywhere on the NI or LAVA forums?
Look in to the application Web server (different than the normal Web server) and implement a Web service.
Bootstrap and Javascript are regular old web technologies, so you can learn them pretty much anywhere. Bootstrap is a collection of style sheets and Javascript shims that make up a skeleton of a web page that meets all of the modern requirements that a web page must have, that is, HTML5 compliant, responsive, and modern looking. That gives you more time to work on the content, since all the style and formalities are out of the way. What you're looking for specifically is how to make a simple Bootstrap page, and once you've got that up and running, then look into how to call a rest api method from Javascript function, that you can embed right inside your html document. You will create an object called XMLHttpRequest() and then call the open() and send() methods, which will then return some data back from LabVIEW. That's it. It's extremely straight forward and you're not relying on some custom framework, you're writing it in the most common and widespread manner used in the world today. No vendor lockin, at least on the client side. Hope that makes sense. 
&gt; I would need info on creating the REST API ala LabVIEW web services Super easy. Start a new project. Right click and add a 'Web Service' to the project. Add some Web Resources (URL folder paths, basically) and add some HTTP methods (new VI's under the Web Resources). Start with 'GET' VI's, those are the easiest. Use a random number generator and wire an indicator to an output terminal. Now, that value will be returned when GET is called, along with any other terminal you add (they will be referenced by their terminal names). Now, the trick here is that, by default, it returns XML output when the GET is called. But on the web these days, JSON is much easier to deal with. So go into the properties of the project, and under 'HTTP Method VI Settings', set each and every Web Service VI to use 'JSON' as the Output Type. Then, start the web service by right clicking on it and starting it. You can then right click the HTTP method VI and getting the URL of that VI (every VI under each Web Resource will have its own URL). You can then paste that URL into a web browser and see the JSON output. Now, in your Bootstrap HTML project, you use XMLHttpRequest() to GET that URL, and you write a tiny bit of glue code to look into the JSON data and then store whatever is contained there, into a Javascript variable. Then, using a bit more Javascript, you can display it into some part of the HTML page using the DOM model. That's web development 101, and Labview does the server side pretty well. You can easily compile this into an executable and deploy it, if you'd like. You'll be able to access the output from any web browser or HTTP client that knows how to properly query and display stuff from and to the REST API.
Wow, thanks! I'll check this out.
Newton-Raphson failed a lot for me when trying this. I used Halley's method (see here: https://en.wikipedia.org/wiki/Lambert_W_function#Numerical_evaluation) and got quick, reliable convergence.
You want to make a 7 segment display? The toughest part of this is going to be converting your number into the 7 bits necessary to display the right number. The complicated way to do this would be to solve a karnaugh map for each segment and each bit of the input, like so: Number | Value | Top Segment ------|-----|----------- 001 | 1 | 0 010 | 2 | 1 011 | 3 | 1 100 | 4 | 0 101 | 5 | 1 110 | 6 | 1 That gets nasty though. Much nicer is to just build a lookup table for each segment.
If you're still looking for it. I have made a gamble VI for you. Here it is https://drive.google.com/file/d/0B-PimJ4T_xZSbmQwTHowZkwybmM/view?usp=sharing
Just google 'twitter bootstrap tutorial'. Thousands of videos and tutorials out there.
Thank you..! Could you give me some of your client side example to have a look at? I am curious to see how that JSON from server is transformed into a nice web page.
You are calculating sin(i*freq*2pi + phase) where i runs from 0 to 1000. It should run (presumably) from 0 to 10. Rewrite it to t=i*dt ; sin(t*freq* etc) To get that.
Thanks, I really appreciate your help. I got the code fixed, and it runs perfectly now. http://imgur.com/a/LmOve
Google for a guide called "c Rio Developer Guide". This was instrumental for me when taking over a huge project involving cRio, and all of my previous LV experience was basic use on a pc/daq system as opposed to the real time. 
Accidentally submitted comment early - wanted to add that the guide is from NI, and is just enough to figure out what youre doing without being a super long read. 
I am not sure if the developers guide is really a good start. If you start with cRIO, I think it would be better to first have a look at the basic tutorials (https://www.ni.com/getting-started/install-software/compactrio ; http://www.ni.com/tutorial/11198/en/ ; http://www.ni.com/tutorial/11197/en/ ; http://www.ni.com/pdf/manuals/372596b.pdf )
Don't get me wrong - the dev guide is an awesome document, but for beginners it might be hard to chew.
At least they were kind enough to make it vertical! Right? ...right? Easier to scroll with the mouse wheel than to have to constantly manipulate the horizontal scroll bar. :P
I don't think there's anything inherently wrong with multiple parallel loops. 
Hey, that's at least all in a line! I've opened up legacy code that had loops just strewn about.
This is definitely not one of those "Look at this horrible spaghetti code" posts! The code base has its strengths and weaknesses, along with a rather large block diagram for it's main vi.