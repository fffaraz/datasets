AKA why I don't use LabVIEW anymore for any DAQ/DAC applications beyond prototype stage :/ Edit: I should remember what sub I'm in. It's a good tool people... just not the tool for *every* job 
Ctrl+A, Delete.
&gt; bending over backwards to get LabVIEW to do things that are dead simple in other languages Care to list some examples?
It boils down to modularity at run time, which is not something that LabVIEW excels at. Admittedly the limitations are much more pronounced in my line of work than in other applications of LV/NI. I spent a lot of time working with platinum alliance partners trying to find a way to do what we (and everyone in our industry) needed. It was always more expensive and more time consuming to do a LV solution than something different. 
Aye. I do a lot with packed project libraries, but it's far from common as far as I can tell. For me it was a little up front investment &amp; commitment, which not everyone can justify or afford to do. I hope that NI continues to make them better and more accessible! Would it be helpful (asking the community too) if there were more resources on converting an existing project to using packed project libraries?
Better libraries would certainly help. A big part though is the rigidity of the UI. Things like adding new controls or indicators to a front panel and then binding them to DAQ channels, all during runtime.
As much of a mess as that is, I've seen worse.
It can be done with the more modern frameworks out there nowadays. It just takes some creativity and knowledge of OOP in LabVIEW. I have no idea though how easy something like this can be in other languages.
hey @ Rajdog20 this software is really great. I was wondering, do you know if it's possible to plot XY data in it? For instance, I have two columns/channels, one is x values and the other is y. I'd like to plot them as an X/Y graph.
Could be worse. I just reviewed code from a vendor. The main application was literally just 30 unwired subvis.
You cannot reliably do things every 1ms on a non-deterministic system. While some VIs have inputs delimited in ms, it's not guaranteed. You probably want to look into any of NIs FPGA products. 
It depends a lot on your hardware that you're using. DAQmx is only for NI's DAQ products. You can acquire data at rates faster than 1kHz on a PC - but it is a hardware timed / buffered acquisition (e.g. it acquires data at 20kHz and you read a whole bunch of samples at once). If your hardware supports triggering, then you can use that trigger to start the acquisition (on both devices) and acquire a bunch of data at &gt;1kHz rates (e.g. a fixed number of samples). Where you would need an FPGA or LV RT (e.g. cRIO) is if you wanted to acquire data and then control outputs based on that faster than 1kHz. Windows cannot do anything deterministically so is not good for control, but is fine for measurement applications (providing you have the right hardware so you can take advantage of hardware timing).
Probably easiest to use the System Controls - you can also find the 'System Controls 2.0' in VI Package Manager which adds a bunch of system style controls not natively available (e.g. graphs, buttons with glyphs etc.). Here's a link: http://sine.ni.com/nips/cds/view/p/lang/en/nid/209118
Yes, I do understand that. I'm more so looking for recommendations for manufacturers or specific models.
Well they all output NMEA strings - so you need to find one that meets your specifications / form factor - e.g. for hobby use: https://www.sparkfun.com/categories/17
Yes, if I understand the question correctly. When you click on a channel with data, clicking the "graph" tab next to the file contents will display a graph of the data. The graph is fully customizable with changable x-axis and other features. You can also export the graph and data to excel if you wish using the "export" button. 
You're not going to be able to do this reliably on any non deterministic system, like windows or (non-rt) linux. Your options are then deterministic (real time) operating systems. NI's CompactRIO architecture combines a realtime linux operating system with a FPGA for speed and flexibility. Your LV application in this scenario would be: PC app: provide a front end to talk to the RT app RT app: handles communication with PC app, sends commands to FPGA code, and can contain slower rate calculations or control FPGA code: handles fast input and output control logic You can develop all three in one LabVIEW project, quickly test and deploy. NI offers a great value and developer experience in this space. They offer training specifically for LVRT and FPGA to help you get started, too.
What kind of project are you working on? If you're using NI hardware, you should use the official dev environment. The point of their compiler is to run LV code on non-NI targets. It only supports certain front panel items and block diagram primitives (listed on their website [here](https://github.com/labviewforRaspi/LabVIEWforRasPi/wiki)), which looks like enough to do a lot but it might not have everything you need.
&gt; but it also directly unbundles the large cluster to write to indicators is this bad?
Our idea is to emulate a cDAQ-9132 system with a raspberry pi and a cDAQ-9179 (bypassing the $2500 price increase) for a dedicated data acquisition system that will be running nearly all the time. It is not anything critical, just needs to relay low speed temperature and pressure data to a shared variable that will be accessed by several other systems simultaneously. I did realize this morning that the tsXperts system would not support what we are trying to do, but now I am wondering if there is a way to do it. I saw that the NI Real-Time Linux system supports ARM processors, so it should be relatively easy to port it to the pi unless I am mistaken. Or would it be easier to get an x86 board like the intel edison?
If you're going to go to the effort to make a UI manager class that updates all controls by reference, why would you leave out indicators? It's stopping short.
There is an NI-produced toolkit for building/compiling code for the R-Pi but it's for non-commercial use only. Here's the link: https://www.labviewmakerhub.com/doku.php?id=learn:tutorials:libraries:linx:3-0 Of course, it isn't really 'real-time', doesn't have a user interface and of course it doesn't have much by the way of inputs/outputs, so you'd need to hook it up to external DAQ chips (e.g. via RS-232, SPI, I2C etc.).
&gt; Are you going to run Windows or Linux on it? I could do either, I've not played with LV for Linux before so I am not sure of the capabilities. &gt;What kind of signals are you going to be reading? How many? I'm only acquiring about 8 pressures (4-20 mA) and 4 Temperatures (J-type TCs).
LabVIEW for Linux is full LabVIEW, with various degrees of NI hardware driver support. Here's [NI's official page](http://www.ni.com/product-documentation/52786/en/). What kind of hardware are you planning on using for that DAQ? Be sure that it plays nice with Windows/Linux of your choosing.
Here's an example of using byte arrays. Definitely how I'd prefer to go. http://imgur.com/kXZLikR Communicating with serial devices is one of my favorite things to do in LabVIEW.
Just to get this to compile, you need to replace the bundle node with a string concatenation node. While that will compile, it's almost certainly not what you'll need to get this to work. What your device is probably expecting is 4 bytes with each byte being the value. You're converting the value into ascii, and could possibly be longer than 4 bytes. 
Can you upload the intilization VI somewhere or give me a link to the API MicroFab provides? I'm having trouble locating it on their website.
Looks like your cDAQ-9174 hardware will work with either board. Have fun!
The initialize VI is here: http://imgur.com/a/hmL9y It looks to me like the manufacturer ID, model code and prefix are not entered properly and is giving an error because their input returns false. The manufacturer doesn't really provide any documentation for their labview driver-it includes only an initialize and close function. Instead they offer a sample program for a price. 
Try doing what /u/iYogurt suggested. Set the ID Query and Reset inputs to false and try again. The ID Query is probably not important at all for you. Reset can be important but try without for now and see if you have problems.
There's many ways to go about it. I'd personally build my LabVIEW application as a webservice and modify the game to make webrequests to my webservice to POST the data. Other options: - Create a TCP or UDP connection over 127.0.0.1 and send data to LabVIEW (probably want to use some kind of ASCII packets so you don't have to worry about whether your floating points are 4 bytes or 8 bytes and other BS like that) - OPC - Using the windows API to access the game's memory space and directly reading data from memory addresses (I don't know how to do this and if this is even possible). - Have the game write data to a file and have LabVIEW parse it (probably going to be slow)
Quick and dirty: Open a socket between the two applications and send over UDP. You can hit pretty high data rates on a local network (it'll be going through your router but that's whatever). 
I suspected this! Thanks for letting me know! 
If the simulator and LabVIEW are running on the same computer the traffic wouldn't go through the router. TCP/UDP on localhost is a pretty common method of inter-app communications. If you're curious, you can see what's talking with the "netstat -a -b" command.
Oh interesting to know, thanks! 
Cool, sounds good
For loops are directly analogous to the for loops in the labview structure menu. Conditional statements in C are like case structures in labview. Nest the two for loops and put a case structure in there with a few constants and output indicator and you're good to go.
I like boons as much as the next man... but why post this in /r/LabVIEW ?
Well, no one's going to do your homework for you, but if you have a specific question we might be able to give you some guidance. Just saying you're confused isn't giving anyone much to work with.
That's... Not a question.
Really proud of my team's work on this article. Hopefully you call can find it useful!
This is really well written and provides enough information to understand whats going on without being overwhelming or boring. Probably one of the best Labview architecture descriptions i have read. Thank you for some great ideas for how to implement such an architecture. I will give this a go and see how much it improved my much sought after molecularity. 
Hey, like i said in my original comment, thanks a lot for this article. I have a question regarding figure6. The data is accessed from and written to a local variable... Seeing that you are storing other information globally ( in DVRs for the event notifiers and model data) Im' curious why a local variable was used here. If i have a lot of data that is to be updated via notifiers at a high rate (say a continuous line graph of streaming data ), what is the best mechanism to update the View ? 
When filling in data on the UI, you're limited to three options. Write directly, write to a local variable, or write to it by reference (property nodes). I guess you could also bind to shared variables as well but that isn't an option when you're not using shared variables or data sockets. Writing directly is ideal as it's the fastest from a computation standpoint. Property nodes would be the slowest. In Figure 6, "Downhole" is a cluster indicator and the event returns only one element of that cluster. So a local variable is used to read from that cluster and provide the data type to the "bundle by name" in order to set just the RPM variable. Another local variable is used to set the new data into the "Downhole" indicator. As to why a second local variable is used to write the data? My best guess is some of the other events are updating other elements of the cluster and the indicator terminal can't be everywhere at once, requiring a local variable. As far as streaming, I'd highly recommend a second loop with its own event structure for receiving the streaming event. Since you're going to be spamming events you don't want to bog down the event structure for other events. Another option with this MVVM architecture is to have a separate module handle the data stream, format it into graph data, and fire a separate event for "new graph data" that the UI can then respond to. This seems necessary for XY Graphs.
Thanks for the insight, that makes sense. Usually in the "View" Vi's ( I have have yet to adopt this architecture, just calling it as such to be on the same page), I update a cluster I keep in a shift register to then , as the first option you stated , update the indicator directly . This however requires a separate "update ui" event, which seems nonsensical after seeing how it is done here . On the other hand , this way i can update separate parts of the cluster before "sending" the data to the UI. Would you see an issue in doing it this way , as i would need to have a user event that triggers the event structure it resides in ? Great tip on the streaming data issue. Do i undertand you correctly , that if i was to create a second loop with another event structure , I would have 2 event structures in the same Vi ? I was unaware that this was possible as I thought they may get in each others way. Or would it be that the second loop resides in a Vi on its own (creating another "View" i guess) and embedding this into the main UI via a sub panel for instance ? 
&gt;All software architectures have weaknesses. Due to the global nature of all View Models, it is possible for a developer to access the model data of another module directly. This opens the architecture to race conditions and data corruption. Luckily, this is a development-time issue and can be resolved with proper training. Developers should practice never directly accessing the data model belonging to other modules. Why not resolve this issue by making a generic Model class, and restricting data access that way? Then every other model that you create could inherit from that. It is great that you acknowledged the weaknesses of the architecture, but in my opinion, one hallmark of a great architecture is that the architecture enforces its own rules. It is not sufficient to say that you can just train that part of it away.
You can have multiple event structures in the same VI. For user events, make sure you "Register for Events" separately for each event structure. Meaning, don't branch the "registered events" reference to wire into your dynamic terminal. I don't know off the top of my head if you can have two structures register for the same UI events directly. However, I bet you could get around it by wiring a reference to the control into your "Register For Events". This will make sure each event structure handles its own copy of the event. I don't know the implementation details behind events in LabVIEW but you can think of the "Register For Events" node as initializing a queue for each event. The event structure is then essentially dequeuing events to handle. 
http://forums.ni.com/t5/LabVIEW-Communications-System/Convert-labview-communication-file-in-to-vi-file/m-p/3202580 "currently LabVIEW Communications does not support all of the functionality of LabVIEW, such as Event Structures."
I am reviewing code written by our vendor. The main UI calls various sub-panels by reference. The above image shows a typical section of code they use to load a VI into a sub-panel, and the actions take when the user wishes to return to the main UI screen. This is a test stand an one of the "small changes" we requested was that within a single session the meta data entered in the "Autocycle" sub panel should remain if the user has to go to another section of the program then return. This data is stored within a shift register, but it is not retained. So my question is, if I call a VI by reference, it finishes executing, but the calling VI does not, the reference is not closed, then the same caller makes another call by reference. Is this second call an entirely new instance or the same instance connected to the reference that was never closed?
Thanks for the quick answer I'm got a work around but there is much to be done.. I agree with your rant, this code is poorly written and nearly impossible to follow or upgrade. The main VI is literally 20ish un connected subVIs plopped on the block diagram. 
Excellent article, looking for a project to implement this on.
Hi, Always nice to see a fun idea to help you learn the ropes! You seem to have grasped the idea of data flow, which is important in LabVIEW. The next step however is to start getting the hang of situations where you have to break away from the basic data flow along wires idea, but still keep your data flow under control. The simplest way to solve your problem would be to have a parallel loop, either inside or outside of your main loop that polls the state of your flop &amp; riviere controls &amp; acts accordingly. Things to think about: - How do you want to store the state of your LED so that you know whether to turn it on or off in each loop cycle? - How do you control the timing of your loop? - How do you make sure the blinking light starts at the right time? (Do you monitor the flop state separately or branch off from the output of the flop loop to make a parallel data flow channel?) - How do you make sure the blinking light stops at the right time? (Separate issue to the above) - How do you make sure the loop doesn't hold up your main loop (if you've included it inside)? Or how do you make sure it stops at the same time as the main loop (if outside)? - If you're going to use local variables, how do you make sure you're going to be reading and writing to them in the correct order? (look up race conditions if that's not something you're familiar with) - Are your boolean controls set to latch or switch? How will this impact your polling loop? If you can get his working, then I'd suggest reading up on state-machines (check the labview example finder &amp; google) and then having a go at recreating the same behaviour using that pattern. This would make for a much neater solution and be a good way to learn a simple but incredibly useful design pattern. If you wanted to take it a step further you could then try and do the same with a producer/consumer pattern and then a queued message handler (QMH). Couple of other little notes: - The not gates before the conditional terminals on your loops are unnecessary, just click on the conditional terminal to invert the behaviour. - Don't wire a constant to the conditional terminal of your main loop! I'd switch it to 'stop on true' mode and then wire a latching boolean control to it to act as a stop button for when you want to stop running. This will save you from having to use the abort button each time. Hope this helps! Let us know how you get on.
Not easily. LabVIEW can call functions in DLLs if the servos have drivers available. What you'll need to do is: 1. find a software driver for the servo 1. find some example software using that driver (preferably in C, C++, or C#) 1. transliterate the software example to labview code using the call library vi. Unfortunately there isn't a much more detailed way of helping aside from getting in to the woods with you.
Honestly, the arduino isn't a terrible choice. The other thing that works well is a USB-RS232 converter. There is one embedded in the arduino (which is one of hte reasons they are so easy to use). [FTDI chip](http://www.ftdichip.com/Products/Cables/USBTTLSerial.htm) is a good place to get one of those cables.
no choice, project adviser wants it done without the arduino ¬Ø\\_(„ÉÑ)_/¬Ø thanks again, i'll look into the usb-rs232 
There is [RAW USB](http://digital.ni.com/public.nsf/allkb/E3A2C4FE42D7ED0D86256DB7005C65C9) support in NI VISA. Although for that, you'll need low level documentation on the protocol from the manufacturer. If you control the micro, you're better off using a UART and a USB to Serial device. Those are much easier to develop on both sides. If you don't control that, it's going to be significantly easier if the manufacturer has some external library API that you can call from a call library function node (as another poster mentioned). 
You should write a .Net wrapper around some Java functions that call a Python back end that manages your serial port connection, than export the values to a JSON file, use a little JS and node.JS to translate it to an XML file and read that into MATLAB. /s
Does the stage provide any feedback for its moving status? If not, your only real option is to wait an arbitrary amount of time between sending it new XY coordinates.
If you know the speed of the motors, you could potentially calculate the travel time between two x,y points and set an appropriate wait time.
yeah, there is a 'read response' option-- how would i integrate that into letting the loop know it's ok to move on to the next set of points?
I think Linx has replaced Lifa some years ago. 
Did it work out? 
Yes! After weeks of learning labview, I got it working :)
The NI-VISA drivers and library work pretty well for this. You will need to write a program for the arduino that takes args like direction, on/off, etc. and can basically do "VISA WRITE (args)" based on a combo box/switches to pass a string and control what's happening on the arduino side.
You try this; http://digital.ni.com/public.nsf/allkb/7B5382164D2C12D486257B5800646A6C 
Can you post what you already have and better describe what isn't being done properly? 
Rather than using a different property node, have you considered using a graph instead of a chart?
I will try anything at this point. I'll write you back as soon as I have tried this approach. EDIT: XY Graph doesn't work with DDT (or anything that LV can change DDT into) Making array of clusters with every change of single parameter would take too much time (samples are ~ 80k datapoints) Any other approaches?
The challenge is that a chart is made to inherently handle constantly changing data, and only providing a windowed view of it. I think what you're doing is likely conflicting with the automatic abilities. You may want to tinker with the chart properties to see if you can turn that off. Also XY Graph is not the same as a Waveform Graph as XY expects 2 inputs not in a waveform. Waveform Graph should be able to accept what you're looking for. http://digital.ni.com/public.nsf/allkb/95FEE9F5B252507E862562BA00007657 http://digital.ni.com/public.nsf/allkb/29353B4D8E4CC95C86256B9F007D1AD2 http://forums.ni.com/t5/Example-Programs/Demonstration-Chart-Update-Modes/ta-p/3497512 http://forums.ni.com/t5/Example-Programs/Types-of-Waveform-Charts-and-Graph/ta-p/3499600 
What did the assigned reading that went along with this assignment say? Or, in other words, we're not going to do your homework for you. Why not try something first and ask for assistance once you've attempted the work.
So the 5V isn't connected to anything, which is why you could get rid of it and it won't break the circuit. The PWM is providing the 5V, but it's turning it on and off so fast that it just looks like it's dim. 
ok, change of plans. I will use the zoom tool from graph palette to interact with data. It works fine with selecting part of signal to display. The problem is, that I need to read the number of the most left and most right displayed datapoint. I could make user to enter it manually, but It would have been nicer if program would read it on it's own. Do you know how to read it?
If you like my entry, please think about giving me some Kudos, thanks very much :)
Thanks very much :) I had great fun making it!
I know this isn't super specific, but I had a similar issue and it had to do with the order of installation of LV, Max, and the drivers.
To double check, do you mean that the 'DAQ assistant' Express VI doesn't show up in the LabVIEW functions palette? If you have MAX then i guess you have DAQmx and DAQmx ADE support, but you'll need to make sure you have the right one for your version of LabVIEW. I would recommend you ring your local support hotline during office hours, contact details can be found at ni.com/support. They're a helpful bunch...
I wrote pong in labview once.
Do anything with machine vision. See if you can get a simple USB camera to recognize street signs.
Off the top of my head: - brewing system controller - garden watering system - odbII (CAN) interface for your car - telescope tracking system - home automation system (lights, temps, blinds, etc) - multizone thermostat controller - doorbell camera system You can do all of these with any hardware or software of your choosing. LabVIEW is a solid choice for most for rapid application development, but if you want to make it cheap you will likely be looking at a rewrite on a different platform.
Build array will wrap your single double into an array of length 1.
@havearedpill - 3&amp;4th channel and current/voltage, 1st&amp;2nd are also voltages. Currently all 4 channels use a channel writer to save to tdms. Eventually I would like to include power in the same tdms file. Does 224 mean I'm averaging the power for every 224 samples? Honestly I expected the current/voltage signals to auto index but instead the outgoing power seemed to do so.
Your voltage and current are coming out as 1d waveforms, not as an array of doubles. You could either A) Index out your X elements out of the waveform or B) Change your DAQmx read to instead read out an array of doubles (and remove the for loop). Either option should work - and they would both use the timing of how ever you set up your DAQmx task for timing/synchronization. Right now you're getting the power calculation directly out from the entire waveform (I'm pretty sure that function takes the average AC power) 224 times (so it will always be an array of the same value 224 times). If you had, instead, placed the for-loop around the DAQmx measure too - it would have given you 224 independent measurements, each done on an entire 1d waveform of data (which, again, may or may not be what you're after). 
What he said. It doesn't get any less complicated than that.
Overly complicated. It is a single sample so all they need to do is wire it into a build array function with one input creating an array of length 1. Or if they need an array that is the same length as their other data just take one of their other data sets, pull out the double array using "get waveform components" (Y), use array size to discover the size, wire that and the single data point into an "initialize array" block and then wire the resulting array and the waveform from earlier into a "build waveform" block to sync the timing.
The compiler hopefully is smart enough to realise it needs to instantiate an array of 244 at the start of the loop execution. That may be wishful thinking but it's a pretty simple optimisation for it to do.
Unless he makes it a parallel for loop he will still have to write the same value to an array 244 times in series. Initialize array will do all those writes at once.
Initialize array is a parallel process, it allocates a block of memory 244 elements long then initializes those elements to what ever type and value is wired to the "element" terminal.
My assumption was that every value was the same since he was just wanting to convert a single DBL value to a DBL array. I was assuming he was not going to increase the sample rate.
Some ideas for you: Put the logging code in a case statement Look at timing VI's such as Elapsed Time to determine the case to execute
Yes, the video shows the use of the sequence structure which cannot be aborted which is known (and taught) weakness of that approach. You want to research simple state machines with LabVIEW as a way to sequence events while allowing for an abort following any event.
so ... any suggestion for a new approach? 
My second paragraph - Simple State machines. http://www.ni.com/tutorial/7595/en/ http://www.ni.com/white-paper/2926/en/ 
I've made [this](http://imgur.com/gallery/hsQ7U) Imgur album explaining a few solutions.
Keep it simple. You just need one timed loop, executing every second. It will collect the data and manage the outputs. It'd have an RT fifo command input. The command would be what output to turn on. To manage what outputs to keep you, you just need an array of output states. Each output state will just be an "end timestamp". So you you can have ultra simple logic. If (current time &gt; end timestamp) output = on; else output = off To turn on an output, just get the current time, and add the duration, then save it in the output state array.
Here is a simple project for you to try: * Create a new VI For the front panel: * Create 3 Boolean LED indicators named LED 1; LED 2; LED 3 * Create a Boolean button named STOP For the block diagram: * Create a while loop * Create a case statement inside the while loop * Create an enum constant with 4 items; Init, Led 1 On, Led 2 On, Led 3 On. Note: This is personal preference but can save you headaches as you gain more experience. MAKE THE ENUM A TYPEDEF. * Create a shift register on the while loop * Wire the enum constant (which should be outside the loop to the left) to the shift register terminal (left terminal) * Wire the shift register output (inside the loop) to the case statement selector * Place the LED terminals to the outside right of the case structure * In the Init case, place 3 Boolean false constants and wire them to the 3 LED's, creating output tunnels for each (automatic when you wire from inside the case structure to the outside) * In the init case, place a copy of the enum we created and wire it to the shift register on the right side of the while loop. Set the value to be Led 1 On * Right click on the case structure and choose Remove Empty Cases * Right click on the case structure and choose duplicate case. The new case should be named Led 1 On. * Change the Boolean constant for Led 1 to True. Change the enum constant to Led 2 On * Repeat above 2 steps for cases Led 2 On and Led 3 ON. The enum constant in Led 3 On should be set to Led 1 On. * Wire the stop button to the while loop condition terminal * Add a Wait Until Next ms multiple to the loop and set it for 500 ms * Run the VI * Watch the LEDs Does the LED always remain in the state you want when you press stop? 
Sorry, but it still runs until the loop ends. The Stop button is outside the loop, therefore in the beginning of each loops, the loop will gathers data from Stop Button, if i press its after the beginning, the program only stops after this loop is finished. Here is my Block Diagram http://imgur.com/a/Bv7jC anyway, you are so good, thank you so much ! &gt;....&lt; 
You can move the Wait Until Next ms multiple outside of the case structure; unless you want to change loop timing in different cases. The stop button behavior will vary based on timing. Because there is no data flow dependency inside the loop that tells LabVIEW when to read the Stop button. So, depending upon OS, processors, LabVIEW version, etc... LabVIEW can choose to read the stop button first, then execute the code in the case statement, or may execute the code in the case statement first and then read the stop button. Let's assume a "long" time for the code in each case statement of 5 seconds. If LabVIEW has optimized your code to read the Stop button first, and you press stop in the middle of that 5 second period, then LabVIEW will not detect the change until the next cycle of the loop, the next case will execute, and your code will take ~7.5 more seconds before it stops. However, if LabVIEW had optimized the code to read the stop button after the case, then your code will stop in 2.5 seconds, and will finish on your current case. So, if you want to force the code to always complete on the current case, you need to force LabVIEW to read the Stop button after the case statement has executed. A simple way to do this is to put the Stop button in a sequence structure and wire one of the Boolean outputs from the case to the sequence structure. This forces a data flow dependency and forces execution order. 
You should probably contact MC support. What you're asking to do is trivial, I'm willing to bet they have an example to do it
Do you mean something like this ?: http://imgur.com/a/qAxTi I tried, yes, it is better than the first one, but, the Led still keeps running until the next one lights on. (e.g : I clicked Stop Button after the first second, the program runs until the second second passes) &gt;....&lt; 
Close, but not quite. You should wire the stop button terminal in the sequence frame to the condition terminal. You do not need to use the local variable of stop inside the case structure.
So I'm assuming here that you are using a PC ( or rasp pi or something) to connect to the Arduino via USB. If that is the case you can use the ni visa blocks to communicate with the serial port on the Arduino. You just need to know what com port it is on and the baud rate (9600 is standard for Arduino). I would make it a call and response setup. Labview will use a visa write command to send a request, Arduino will use Serial.read() to receive the request and will send the most recent RPM reading via serial.write(), finally labview will use visa read to receive the rotational speed.
There should also be some serial communication examples in the Labview example finder. Stick to the simple one for now. It will show you how to configure the baud, port and other details to communicate with the arduino. If you open a serial port in LV then you should be able to at least see the data the arduino is spitting out assuming your port settings are correct. After that you can work on parsing the data out and updating an indicator. Next, work on sending a message to the arduino and parsing it there. 
I use semaphores like this all the time. You must be doing something wrong. We'd need to see the code.
thank you ! I understand. Uhmmm But now i have a totally different question. I will post its later. It's hard. 
1. "Starting at .5..." - I bet it takes .5 seconds to initialize the instrument. There is no dataflow dependency between any of the device initialization code and the "get date time in seconds" node (the one thats outside of the while loop). That means they can execute in parallel. So while the device is executing, the "Get Time" is executed, then the loop doesn't do it's first iteration until after the device has initialized. 2. "Graph drifting away" - A waveform in LabVIEW consists of 3 things. t0: the start time, dT: The time (in seconds) between samples, and Y: An array of data values. What is your code doing? It's hardcoding 0 to t0, which only matters if you wanted to find the absolute time (which we dont). Y looks right (The graphs have the same shape). So dT must be wrong. Look at your code. What is dT? Its the time between the start of the loop and the last measurement. This means that if you leave it measuring a long time it dT will be big, if you stop it quickly dT will be small. That's not right. Remember dT is the time between samples, it shouldn't change based on how long you're run the test.
Do you have a signal conditioner as well? 
As /u/fuckingpewpew pointed out, look at this function: http://imgur.com/ZY2xHZF You want to measure the time between loop iterations, right? Sounds like a shift register to me. 
Sounds like a reasonable project - Please post your code or screenshots so we can offer suggestions.
Also optics is more challenging than you might expect. A fascinating area of applied physics in its own right. 
Go help&gt;find examples. There should be plenty of example projects there that would have those elements
On second thought, why not use an xy graph? You can give it the array of elapsed time values vs the array of readings and it will match exactly.
How about a fancy toaster? main vi - toaster.vi... 2 sub-vi's - butter.vi, temp_monitor.vi... etc. If not a toaster, think of any simple machine that you can add complexity/features to, in order to meet all of the requirements of your assignment. There are a lot of elements here. Better get started! Good luck. 
Probably should have paid attention in class bud.
CPU might be the bottleneck here? Using a compressed avi format? Can you save to disk in an uncompressed format? (granted disk speed may not be fast enough as well... might have to reduce quality or resolution) Then you can compress it later, or on a different CPU than your acquisition.
Yes, I am using one of the built-in codecs. I've tried a few different ones and none seem much better than any of the others. I agree that the CPU might be a bottleneck, but im not sure. I'm on a pretty powerful PC, with 8 cores. It appears that none of the cores are fully saturated. I can, but I would prefer not to.
What's your CPU use as you write to file? Writing to AVI can EAT your processor..... you may just be maxing things out. You may have to do something tricky where you buffer into memory to write later (only works on short clips), or maybe acquire a lower resolution? If you aren't railing your CPU - are you perhaps railing the write speed of your storage device? That might cause the system to do some weird stuttering in the back end that could slow down your performance and make it appear as though you aren't able to acquire data fast enough.... How's your memory looking? Watch these things and let us know a few more details about how the system is performing under the strain of both acquisition and the recording. 
x2 on saturating the disk write speed. We ended up having to use two SSD in Raid to get the write speed required to write multiple streams to disk.
Given that the producer consumer loops are isolated, why would a delay in writing to disk delay the acquisition group? Not that I doubt you, just trying to figure it out.
How are you passing images between your loops? Flattening each acquisition and putting it in to a lossless queue? If so, if the Avi write only reads the queue at 15Hz, the enqueue in your producer loop will limit that loop to 15 Hz as well. 
Well, an IMAQ ref is just that, so you're filling a queue with the same ref every time, and you aren't truly passing each image. That's okay if you're cool with dropping frames, but I'm guessing that's not the behavior you're looking for. AVI is really slow, I usually recommend logging the images to disk, then doing post-processing for the AVI. 
Something else to consider. Try building as a debuggable executable and see if the crash occurs. Now that you mention it is inside a PPL, I am wondering if the front panel that comes up for going inside the PPL (and for that matter inside an llb) is getting removed when the exe is built, thus causing the crash. A quick test is to enable debugging in the Advanced tab of your application build specifications. If the debuggable exe does not have the crash, then that is a possible work-around. 
Thank you, this is exactly the approach I want to take. Can you point me to a good resource to learn how to do this? 
I built a debuggable executable and the crash still occurs. In the build spec I made sure under additional exclusions that 'exclude dependent shared libraries' is unchecked. I'm going to try a repair install / compiling on a second computer. This is too strange to have no other support threads. **Update**: I repaired my LV install, nothing. I created a test VI to launch and added it to my executable's main VI, which replicates the issue. Interestingly, I can't browse for files inside LLB's using a file path control either, but it doesn't stop the whole exe like using the file dialog express vi does. This leads me to believe that the exe can't find or isn't including the subfile dialog from vi.lib. I added it to my project and set it to 'always include' but still no luck. I just kicked off a recompile from my main vi to test that as well. 
LabVIEW has a Report Generation Toolkit which should take in a template file as an optional parameter.
Thanks, i was actually running through these numbers and it is an enormous amount of data. Running a disk benchmark software shows write speeds to my SSD of 500MB/s, which "should" be ok. I'm going to try out a few different options today to see if I can make something work. Thanks!!
I'm going to respectfully disagree that Monday isn't worth it. The majority of Monday sessions are presented and developed by Systems Engineers and are generally more technical in nature. There might be a talk on the same topic later in the week, but it will often be less solution focused. Additionally, Systems Engineering is probably the largest source of collaboration with NI for an Alliance Partner, so from a networking point of view it would be a lot more valuable. 
I didn't say it wasn't worth it, I said if you aren't an Alliance Partner it sucks. Out of all the cool technical sessions you've gone to on Monday, how many were in the Academic Forum? Because if you aren't an Alliance Partner that's all you'll be able to go to.
Are you on Windows and using software timing? If so then this task is impossible. Or rather it might work once, or twice but you will have no control over the level of precision. Ever have Windows lock up and have the mouse stop working for a few seconds? What do you think will happen to your program when the OS hiccups like that? Windows is a non-determinist OS and you are at the mercy of the OS, drivers, and kernel. Now if all you need to do is turn on 3 digital lines in 5 ms, and you have hardware that allows for DO buffering then you can put a waveform on the hardware and say 'Play' and it will take care of the timing. But for any more help more information about the requirements are needed.
My hope was to be able to use NI-Switch VIs to trigger: 1 relay, 2 relays and 1 more relay all within 5ms. Each relay controls a different signal. &amp;nbsp; 1. the first relay sends a DC voltage to an input. 2. the second two relays send two opposite phase square waves, each to a separate input. 3. the third relay send a digital logic high to an input. &amp;nbsp; 
please excuxe my inner nerd, but thats really cool! I have not done anything like that so, would you by chance have an example of doing something like this? When set this up with the DO bufffer, what kind of software would you use to do this? Could this be done in LabVIEW or does it need to be in a separate program?
So I built this just for the hell of it to see what you were seeing. The delay I think you are seeing is dependent on when the Stop button is pressed. The loop is going to complete it's current iteration before stopping, so if you press it right after the first LED, you will see the second one still light up before the loop exits. You can test this by using highlight execution on the block diagram and to how the data is flowing
I probably should have mentioned this in the OP, but I'm pretty new to LabVIEW. I have a solid coding base, so I understand the idea of creating an array for my data, but I'm stuck on how to "filter" it out. LabVIEW has no sort of "if" structure that I can find. There is the case structure, and I found an online guide for an "if" equals statement (If input == x...) but what I need is a comparative statement (if input &gt;&gt; constant...).
So to use a Case Structure as an "if" statement, you need to use a comparison. Here is an example of that: http://imgur.com/AKBRgC0 However, since you are trying to look at an array of data, you would need to scan through it. So here is a simple example of how you can do that with a For Loop. You can set For Loops to have a conditional terminal so they will stop executing as soon as the condition is met:http://imgur.com/vXAMiFS The comparison pallet has a lot of diferent options that can account for the range you are looking for (500-1000). Here is an emxaple using the In Range? function: http://imgur.com/LNkpcj2 To make a For Loop have a conditional terminal, right click the For Loop structure and select "Conditonal Terminal" Hope this helps more! Note: these are VI snippets created in LabVIEW 2016 so you can save them and drag/drop them onto the block diagram if you have 2016 or higher
Interesting. Will be good to hear if they reproduce it as well.
Is suspect that the reason the increased number of loops is slowing your motor speed is based on your use of global variables. Globals aren't intrinsically bad, they just have to be used carefully as aside from causing race conditions they can create erratic and hard to debug timing/performance issues like this. The primary benefits of Timed loops are greater execution speeds and the ability to control the execution priority &amp; timing source. They don't possess any magical abilities to enforce execution timing, that's dependent on your code. Remember that each function within the loop takes a finite amount of time to execute, so your total loop iteration time is the sum of those times plus a maximum of the loop dt. Expand the visible loop terminals and you'll see things like 'Last Iteration Duration' and others that can help you assess the exact time for each iteration. Clearly your code in the Timed loop takes less than 1ms to run, as it won't take the full 1ms to complete an iteration. Given that your motor RPM is dependent on your loop timing, it sounds to me like you've got a fundamental flaw in your motor control strategy. Need to see your code to say for sure. If you have parameters that the motor speed calculations need coming from other loops, either use FIFOs or change your code to have all the calculations done in the loop containing the motor speed command functions. Are you controlling the motors with a mix of software and hardware-timed tasks? Those will need to be synchronized to reduce or eliminate the jitter in the execution timing. Post your code when you get a chance.
I kept reading about labview that you should sparingly use global variables because they cause odd behavior, and can cause problems, so a little over a month ago I went back and changed all my global variables to wires wherever possible, but unfortunately there was no noticeable difference in performance. I have a fairly large application and the process of doing that ended up taking most of a day's work, and I regretted doing that. I also used to have my stepper pulses sent to the DIO in each of the loops my control calculations were done, but performance using that method was even worse. Do you have any idea why my timed loops are ignoring the period I set for them?
Do not use MSI blast. It was a very old NI R&amp;D tool that was even well past its useful age 6 years ago when I was in the NI support group. MSI blast will fuck your system up most likely. Just do a force re-install. 
https://i.imgur.com/jx47FpI.png If I run this the loop completes in a little over 100ms, as opposed to the 1000 I would like.
Yes! Each pulse will move to the next item in the scan list. You also can configure continuous repeat, so that the scan list auto-loops: http://zone.ni.com/reference/en-XX/help/375472H-01/switchviref/niswitch_set_continuous_scan/
Whenever you include a VI in your exe, all the dependencies of that VI are also loaded. One of the main dependencies of any VI is other subVIs. This includes any VI that you use off of the palettes. If you open the BD of any of those, you'll notice that there are several subVIs and usually several layers of subVIs. Each and every one of those VIs is a dependency of the caller and will be loaded. If you want to know more about why a particular VI (or other asset) is loaded, go to **Dependencies** in your project. Find the item, right-click it and select **Why is this item in Dependencies?**
Thanks. This is not a program that I wrote, so I'm not actually sure what's in the dependency section of the project. I will look into it next time I'm at work and something isn't on fire. 
Try connecting a control to the number of samples input of the DAQmx read in the while loop called something like "number of elements per iteration" and delete the timing vi :). This will make the DAQmx read function wait for data rather than returning whatever it already has.
The graph doesnt update at all. The VI that writes to the global is pretty large, but I'll upload it soon
You can't. The event occurs for the cluster not the booleans inside the cluster. 
Couldn't you just AND the two arrays and use the search 1D array function instead of the for loop?
You can also just wire them up to the not equal directly and it will produce an array of true and false that you can search. 
That's probably more self explanatory than mine. Basically, "search this 1D array for the item that doesn't match." Your way I wouldn't even put a comment.
Try posting screenshots of the code, the converter configuration, and or the error/unexpected results. Can't help you if we don't know what's wrong üòÅ
Juicy additional details. Seems like the cluster-wrapped arrays in an array are the best solution for my case, at least until the specs change wildly again. I'd totally give gold, but I can't even afford Reddit Silver as a grad school slave. Thank you for your advice. 
I'm sure what you are trying to accomplish can be done, but I'm not sure what you are trying to accomplish. What is the goal of the inner loop (s)? If you are just waiting for the data to drop below 3.9, you could continue using the main DAQmx read to acquire the data instead of the inner loop with a second read function. I can give more detail with a better understanding of the objective.
This is the exact reason that NI created the Producer/Consumer architecture. https://forums.ni.com/t5/Community-Example-Submissions/Using-Producer-Consumer-Architecture-for-DAQmx-Read-and-Write-to/ta-p/3508447 Replace the write to file with your analysis stuff.
Yes, data will be lost unless handled appropriately. One way to manage that would be to recognize the stop condition to stop the producer and perform any remaining cleanup in the consumer using the Flush Queue function. Than dispose of the queue after both loops are stopped. 
Check out [this example](http://digital.ni.com/public.nsf/allkb/B472ABA1362F44328625729C0041A8B1) from NI on counting analog edges. You may be able to leverage the [Analog Edge triggering] (https://zone.ni.com/reference/en-XX/help/370466AC-01/mxcncpts/analogtriggering/) capability in DAQmx as well.
I'm guessing your version of DAQmx doesn't support LabVIEW 7. Or try a MAX database reset. http://www.ni.com/product-documentation/53326/en/ http://digital.ni.com/public.nsf/allkb/2C7480E856987FFF862573AE005AB0D9 
What does the sensor output? 
As an aside, I can't believe a practical LabVIEW exam is focusing on DAQ Assistant. 
The whole thing was a mess... We were instructed to practice for the practical using the University's Virtual Computer Lab, which has LabVIEW14. However, the actual practical has a 50/50 chance you'll use LabVIEW7 or LabVIEW15. For many of us, it's the first time we've used LabVIEW, which only complicated things.
Thanks, everyone, for the replies! I'm following up with my TA and I'll present some of your suggestions and see what he thinks.
Considering the project literally states "No communication with human sources outside the group" I don't you will get much of a response here.
You should start at the beginning and keep going until you get to the end. That's what I would do.
This sounds like a prime use case for analog triggering. I would recommend looking at some of the DAQmx triggering examples in the example finder (Help &gt;&gt; Find Examples). You can set up a buffer with the maximum number of samples, plus some pre trigger samples to ensure you capture the event. What you will read out is an array of just the data you need instead of having to filter out all of those zeros.
Use the LINX software by that u can easily interface LabVIEW with Arduino.
http://www.ni.com/alliance/
Based on what I've seen... * Defense contractors * Universities or research labs * SpaceX * Hardware companies * National Instruments itself 
I'm from Hyderabad,India.
Did you follow this tutorial? http://www.ni.com/tutorial/4478/en/
Definitely. Depending on the resolution, this could be an expensive operation. Just use the pointer directly. 
Like the other guy said, vision wires are completely by reference. If you know the name of the buffer, you can simply wire a string into the image control and get the image. But by that nature, if you edit it in one place, it gets changed anywhere you use it.
I would still try changing the window to be modeless and then see if your computer still crashes. It would allow you to determine if the problem is with the modal window or with another aspect of your code.
You can also implement a circular buffer and get the segment you are looking for. Here is a good circular buffer example http://www.ni.com/tutorial/7188/en/ 
Pid and fuzzy logic palette
You'll need a control platform such as a cRIO, PXIe chassis &amp; controller (most expensive options), or a PCI/PXI/PXIe DAQ card in a PC. You will need to find out if there are existing LabVIEW drivers for your pyrometer (check manufacturer's website or NI's site), or write your own. RS-232 communication is pretty simple, and as long as you know the port settings required and the format of the returned data, it's a snap. Then you will need to determine the control loop rate and to understand that 'real-time temperature control' and 'really fast control' have distinctly different meanings in this context. There are a lot of PID control examples that ship with LabVIEW, you'll likely find one that you can save and modify to suit. Help -&gt; Find Examples... -&gt; search for PID. What version of LabVIEW are you using? Do you have the Control Design &amp; Simulation Module? What hardware do you have? Do you have any experience with PID control?
Hey me too! I got a 9/30 on the practice exam and then 19/30 on the actual exam, which is infuriating. I am scheduled to take it again in a few weeks but I don't really expect to do any better since I don't even know how to study for it.
Study the practice exam! The real exam is not much different at all.
He's right. It can effect performance. I don't know how badly but silver controls do take longer to draw. At the CLA summit in 2013, I sat in on the UI bitchfest lecture that had the UI team at NI listening to concerns. One thing they mentioned was that silver controls on subvis could even effect performance. I was surprised about that myself.
The performance of subVIs should only ever be affected by front panel controls if the subVI's front panel is loaded. Generally a front panel will be loaded if its window is opened, if it had to recompile and has not aged yet (and doesn't store its compiled code separately), or if the code itself requires the front panel to be loaded. The last case means if you use something like a property node to interact with a front panel control then the front panel has to be loaded. Otherwise LabVIEW will try to avoid loading the front panel at all. If the front panel isn't loaded then simply writing data to a front panel terminal won't do any UI work, and thus the type of control will have no effect on performance. Once the front panel is loaded then you may see an effect. The best way to ensure that doesn't happen is to save all and close any subVI front panel windows. 
I would say in most cases if performance is affected by your UI then you should change your code to avoid updating the UI so much. This applies to other languages/UI frameworks as well. The UI is always a bottleneck. Avoid letting that slow down what matters. That means don't write to front panel terminals in a tight loop, for example. An alternative approach is to update a DVR and then use another (throttled) loop to periodically read the DVR and update the UI. The important thing is to keep your processing from being impeded by updating the UI.
That's what I thought until the NI design team said otherwise. Maybe they misunderstood (I'd totally believe that)?
Do I have to track these points (by e-mailing NI)? If not, between presenting at NI week and teaching courses, I've probably hit the 50. Edit: Never mind. According to that pdf, my points expired when my CLA was suspended.
Thanks all! Just took it and passed with 21/30 or 70/100. The bare minimum for passing. That exam sucks.
Wow, what an absolute shit show. 
Hi, thanks for your answer. I contacted the pyrometer manufacturer, and it turns out they sell a control box that is pre-programmed for this type of application, and it's surprisingly affordable.
I used LV with a Kawasaki FS03N to automate some medical device production, but the library I had was pretty bad which made it difficult to work with. Make sure you enclose the work area or use laser curtains.
Your warning made me laugh. I can only imagine the real world accidents that made that a common bit of information for you.
Which library were you using?
ya, a misplaced LV wire is a whole new story when it's a robot twirling hot clubs.
Awesome. Thanks for letting me know!
I have with Staubli, using strings sent via TCP/IP to communicate with their controller. We are looking to migrate to Denso since they claim to have an out of the box LabVIEW toolkit, but I haven't tested it yet.
There are new libraries for fanuc, we just got them but have not yet explored them with our 2000i
Through DigiMetrix or Fanuc directly? I like Fanuc due to their huge product line.
Just keep in mind - when you edit the image in one place all of the other places will get the edits too (which is a bit unexpected for folks that aren't used to wires in LabVIEW being references, like they are for IMAQ). 
Love it! https://i.imgur.com/nZkkSt9.png
Do you have this vi available to download? 
Thanks man, I got caught up in the next step and didn't see the request.
Thanks! 
In the end, LabVIEW is very simple to learn because it's like a flow diagram. The hard part (not really hard, just takes experience) is building large applications with a lot of planning and making sure your program doesn't step on its own toes while juggling a bunch of different functionalities. That's when you need to start learning architectures and advanced communication techniques. Initially, though, for small applications, it'll be pretty easy.
This sounds like an ideal use case for logging to TDMS. With TDMS, you have a hierarchy that looks like file-&gt;channel groups-&gt;channels. Channels can be thought of as collections of data types, so from your description it sounds like you'd use doubles. The channels are only created when you try to log to them, and they can vary in size (e.g., you could have 10 channel 2 readings, 4 channel 3 readings, and nothing else). There is a free excel add-in for viewing the data in excel, and LV also ships with a data viewer for tdms. On your block diagram, look in the file palette and you should have a tdms streaming folder ('File I/O-&gt;TDMS Streaming' in 2015). There's also a pretty comprehensive set of examples that come with LV. 
Holy shit, thanks dude! This sounds like what I need. I'll definitely check it out.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/n93gozQ.png ^[Source](https://github.com/AUTplayed/imguralbumbot) ^| ^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^| ^[Creator](https://np.reddit.com/user/AUTplayed/) ^| ^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^| ^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20di1vw8t) 
I think the images are stored as integer arrays, which (along with everything else) are saved as metadata in the .png file.
You don't see lots of people using TDMS anymore. If I wasn't stuck in a company right now that demands CSV's for integrating into existing DBs I would be using TDMS. The base of TDMS is the HDF5 format, similar to the format used for the LabVIEW waveform files, it's fast, highly indexable, and with some of the new data tools (go fuck yourself DiAdem) looks very promising for post-processing. I'm a huge proponent of TDMS and recommend it for almost anyone doing data acquisition in LabVIEW.
Did you get everything you need?
I downloaded DAQmx, however I'm not sure it'll work with the device I'm using (rbd 9103 Picoammeter). I plugged it in but got no response thru DAQmx. The ammeter has pre-made VIs that I downloaded, though, so those might work instead. Need to do some more testing to determine.
Make sure your signal is grounded correctly.
Use 2 channels for a differential measurement, such as AI 0+ &amp; 0-. An other possible improvement, use shielded cables. 
You may be able to tinker with filtering in software if you think something like 50/60Hz noise is part of the issue. You can also try the other suggestions, but at the end of the day the 6002 isn't meant to take conditioned measurements, especially as low as your output is. You're using a +/- 10V range @ 16 bits [so your code width is](http://www.ni.com/documentation/en/ni-daqmx/latest/measfunds/codewidth/) 20/2^16 = .3 mV. Beyond noise, you can try amplifying your signal to more effectively use the ADC's range. Right now your [MSP-300-100-P-2-N-1](http://www.mouser.com/ProductDetail/Measurement-Specialties/MSP-300-100-P-2-N-1/?qs=Pz4gaf3%252b8zwqSSLuR4ra2A%3D%3D) outputs 0 to 100mV, so you're only using a fraction of your ADC's range. However, devices that are intended to work with sensors like yours also feature extra filtering or "conditioning" onboard to help get a good clean reading. Couple more resources: http://www.ni.com/white-paper/13034/en/ http://www.ni.com/white-paper/14341/en/ 
Use DAQmx programming instead of the DAQ assistant. You can take 5kHz data and average or filter the data. It should smooth it out.
DAQ Assistant is totally fine and appropriate for simple applications like this. You can still acquire data at the same rates as when using the API because the acquisitions are hardware timed and transferred from the HW buffer to the software application in chunks. The issue here has nothing to do with sample rate anyway. It is a noisy physical signal probably resulting in bad grounding.
I assumed the wiring was ground correctly as that is the first thing you should check when you have noise. I also assumed OP was not using any sort of hardware timing since they were using the DAQ assistant.
DAQ Assistant can totally do hardware timed acquisition. When you say you want to acquire some number of samples at a set frequency, the DAQ device actually acquires data at that specified frequency, based on the onboard sample clock. The samples are placed into an onboard buffer until the software application calls the DAQmx Read VI (or the DAQ Assistant iterates again), at which time some number (specified by "samples to read") of samples are transferred from the DAQ device to the software. That is hardware timed acquisition. Software timed acquisition is where you acquire a single point of data only when your software application asks for it.
Look up the producer/consumer design pattern
You're probably using it without realizing it. If you set the acquisition mode to anything other than "On Demand," it's hardware timed. 
No, I don't use the DAQ assistant. I use both hardware timed and software timed acquisition.
ok, I am in front of a computer now. Go back to the DAQ assistant and try setting your timeout to 11 seconds. With the amount of data points and the rate it should take exactly 10 seconds to acquire that data. It may be timing out just before it gets the data.
I was hoping for a way to make it a user input through a switch of some kind, thank you though
So, in short, I must change the bottom For Loop's mumber of iterations?
I'm confused about what situation you could be in where the advantages of a paid for masters degree are up for debate. From the fact that your employer is offering to sponsor people doing this means that they have enough of a need for these skills to try and build up their employees, and if they are trying to cut costs by providing an education and then not paying you what your skills are worth you will have a better resume to look elsewhere with
Sorry, I should have pointed out that I had other programs in mind, it's just this one I was unsure about. I'm an RF test engineer and there was another program that offered a degree that works with me on my off fridays and focuses more on wireless hardware as well as software (in C language). I'm wondering if, someone who is very adapt at LabVIEW, can advise whether I'll get a lot out of the courses that the software engineer degree offers.
LabVIEW is a great work environment, but it suffers greatly from being a "finished product" that is sold by NI. There a lot of cutting-edge coding techniques and strategies that are difficult or impossible to implement efficiently and effectively purely in LabVIEW. By studying other languages, you become aware of these techniques. More specifically, by getting a master's in SE you will be educated up-to-date on what is possible now, and you will gain a glimmer of what will be possible tomorrow. In general, furthering your education in any field is always a no-brainer. If someone is willing to pay for it for you, you really shouldn't even hesitate for a fucking second. 
I agree with each of your words.
Practice helps with LabView.... A Master's Degree helps with your expertise, education, employability, and income. This is a free master's degree... take the dang thing... if it was a Master's in Basket Weaving I would suggest taking it... its something that when you leave your work for bigger and better things you can take it with you...
In short yes assuming that you plan to use LabVIEW for medium to large applications. Software engineering applies to LabVIEW projects as much as any other programming language. There are a few areas where SE tools in LabVIEW are weak but that is improving and most of SE isn't about tools, it's about the soft skills of project management, requirements management, structural design principles and others
Thank you. Everyone gave good advices but I think you gave the best explanation on how much SE is implemented in LV which what I wanted to know. 
Looks like there is a pin on the 2345 which should receive 5V (a digital out connected to a boolean button, perhaps) when on and 0V when off. [Sparkfun's hookup guide](https://learn.sparkfun.com/tutorials/tb6612fng-hookup-guide) tells you which pins and values get you which motor states. For PWM in LabVIEW, [this help page](http://www.ni.com/tutorial/2991/en/) is a decent place to start (though, it does not use the DAQAssist Express VI). You can use either a digital output or an analog waveform to generate a PWM signal.
Try a force reinstall: http://digital.ni.com/public.nsf/allkb/8F3BC98AEDD0F3C88625801E006FAA33
It's not going to be a constant frequency, but both will always be the same frequency. At the highest it's going to be around 5Khz. I haven't thought about building a control circuit to send in a signal that way. 
These kinds of things seem to always clear up for me if I start trying different signal termination style (single ended or differential) and messing with my ground. That is especially noisy for a 1% 100psi pressure transducer. It could be related to a ground loop on your shielding. If your shielding is connected on both ends (your DAQ board/computer chassis and your system end) you may have a ground loop. Try disconnecting it from one of the two ends. Before applying digital filters, I try the various ground and termination configurations that are available (there is only a finite number of things to try regarding this) until I get the best looking signal. Then, I typically do oversample averaging -- sample a 5000 S/s and average down to 50 S/s or 10 S/s. 
I haven't thought too deeply into this, so there may be some glaring errors... The output of the XOR is high when the inputs are not the same. Therefore, you'll have a pulse train of pulses that are 1/4 of your frequency right? So at 5kHz you should have a pulse train of 50uS pulses. If these pulses are not 50uS, then your phase is incorrect. If you know your frequency, then the above should be trivial (assuming I didnt screw it up...) Edit: Actually, you just need to check that the pulse train is consistent. If every pulse of the XOR is XXuS, then you know that your perfectly 90deg out of phase right?
Well, you are somewhat constrained by your hardware (I'm guessing you mean USB-6002?)...but, one thing you could try is using analog acquisition instead. By doing that, you can be sure that your samples will come in on edges of the sample clock running on the USB device. You'd have to include some extra logic to bin your analog values into T and F. Once you have that, you'd have two digital signals with time stamps, and everything after that could be done in software.
Agreed but I guess it depends on the intent and complexity of the code.
Congrats, that's awesome man. I'm gonna start cramming for my second attempt in a couple of weeks. It ain't no cake walk and you should definitely be proud. Grade‚Äã A not a shit post.
Congrats!!!! 
No it's not a ni daq. I'm working on convincing them to buy a better daq, as it will make it pretty trivial to solve, but until then have to work with what's supplied. That's what I did today I built a physical circuit that I fed the inputs into, and just had a digital out I feed to the daq. It worked pretty well, but the precision could be better, had about a 10¬∞ range in the phase. 
I'm ignoring NXG until 2.0. I've been muddling with the beta and I'm psyched for sure, don't get me wrong. But 1.0 is pretty useless for what I do. No app builder, no objects, no PXI, not really serious yet. Once 2.0 rolls out it'll be a different situation. Building web ready interfaces that can attach to existing LV17 code is pretty damn awesome.
Have you tried using the beta for WebVIs? The number of limitations is pretty disappointing. I don't know how different 2.0 will be from the 2.0 beta but I'm still not convinced 2.0 will be very useable. Congrats Muun. I don't think I'll ever try for that certification, I've heard many say it is difficult.
Good for you for trying to come into this with a clear mind. The community (myself included) tend to have a knee-jerk reaction, with the feeling that getting our opinion out there first (or early on) regardless of how thought out the opinion is, is what matters. I think part of what makes this situation different is some people outside of NI have been in the know with alphas, and pre release stuff since 2013. I think I first got an alpha in 2014. So reactions from some users should be taken more seriously than others. Lots of these people have had their opinions already formed over years of usage and are now just able to talk about it publicly. Still I think this diagram I made over on LAVA will help you understand the wave of emotions to expect, which some of us have gone through. http://imgur.com/a/df98n
[removed]
Try playing around with the array subset function. It allows you to set a start index and length. If you know your time Delta, you can take the user input time, divide by Delta, and have the proper index.
Are you asking how to create a graph, customize a graph, or something different? To create a graph, select it from the graph palette on the front panel. To customize it, you can use a variety of right click options, properties window settings, and VI server properties. Hopefully this gives you a good starting point for more investigation. 
Ok! I hope I can figure it out. Thanks for the help! 
How fast do you expect that while loop to run? At first glance I'd move the TDMS create outside of the while loop, but I may not understand your methodology or the read subvi's . 
Hey there, I've made some headway in my project but I've hit a snag and I was wondering if you could help me. I'm trying to get my program to record data from my first device to TDMS Channel 1 and data from the second device to TDMS Channel 2. I have the strings created properly on the "write" VI, but it's only recording data to Channel 1 with no sign of Channel 2. Here's a screencap of my program. Any ideas? Thanks! https://forums.ni.com/ni/attachments/ni/170/1023457/2/Data%20to%20TDMS%20ver%203.png
(tempted to LMGTFY, but I'll be nice) Drivers [here](http://www.programmablepower.com/dc-power-supply/XG/Downloads.htm) and manual [here](http://www.programmablepower.com/dc-power-supply/downloads/XG_R850-1500-1700_Operation-Manual_M370430-01-rvH.pdf).
From page 3-10: &gt;The three output modes of operation are Constant Voltage (CV) mode (see page 3-12), Constant Current (CC) mode (see page 3-12), and Constant Power (CP) mode (version 2.21 and higher; see page 3-12). It looks like this model does not support a voltage sweep, although you could implement code to "sweep" the voltage in discrete steps, say 0.5V every 0.25s.
There should be no need to change the data type in the first place; you can feed an array directly into a chart without any manipulation.
I use a power supply from this line at work. The LabVIEW drivers seem OK enough. It would definitely be easy to whip something like this up.
This got caught in the spam queue and I didn't notice until now. Sorry about that. Usually when I have this issues it's really a timing issue. Since highlight execution slows down the entire execution of the software, I find that some things can have a better wait to initialize items or in other cases there's a race condition that highlight execution slows.
That's fine. It ended up fixing itself the next day anyways. I'll keep your advice in mind for if I have this issue in the future. Thanks for the help :)
If you're new to LabVIEW in general I recommend checking out the links at the bottom of this page for free training. The self paced online training is the best if you have an active SSP (most versions of LabVIEW come with 1 year of SSP) http://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495
For every Open you want to call the Close. The fact that you are calling open a bunch of times and close once is a memory leak and can cause your program to Crash. You want to open once, perform operations on that reference then close once. Next that For loop is doing nothing for you. You say run twice, each time the same input is given, and the same output is generated, so remove it. Then you have the issue that the two samples you are providing go to the same Channel, this is because you don't have interleave on the write function. Here is a demo of what I mean. http://imgur.com/a/6YyXg
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/CzrW2dR.png ^[Source](https://github.com/AUTplayed/imguralbumbot) ^| ^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^| ^[Creator](https://np.reddit.com/user/AUTplayed/) ^| ^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^| ^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dixnmjv) 
I started to post individual links, but it sounds like you should start by working your way through: http://www.ni.com/getting-started/
I'd like to check it out. My company has their own LVOOP test executive for our own internal products but not every client wants it.
All the above
The only one I'm aware of is the PTP Sequencer which came out a few years ago. It had a few good things going for it like a pretty simple editor XControl, with while loops, and dragging and dropping step types. https://www.youtube.com/watch?v=imSbm3-v_u0 I found it a bit too restrictive and the company that made it seemed to be more interested in selling you the service of customizing it. The community would appreciate a free and open alternative. If you are looking for inspiration be sure and check it out as well as common TestStand features, not that you should recreate TestStand, just that people like things like hooks into running something before a step, after a step, when a step fails, etc.
SECTION | CONTENT :--|:-- Title | Demonstration of the arrayed data values feature for PTP Sequencer loops Description | Demonstration of the arrayed data values feature for PTP Sequencer loops Download PTPSequencer today http://www.ptpartners.co.uk/ptp-sequencer/ The toolkit includes a library of functions for creating, managing and executing test sequences, as well as some interactive and reusable control components for seamless integration into a front panel user interface. PTP Sequencer enables a LabVIEW developer to consider building their own custom testing platform, whether simple or complex, a one-ti... Length | 0:03:16 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
How are you determining B is not correct? Do you have the original and the result? Which did you post? Can you post the other? How are you generating and acquiring these signals? Could be a phase issue?
I would like to check it out 
I have been trying since morning to get something similar to what I need but with no luck unfortunately. 
Sure
Try changing the Update Mode on your chart. You can do this through right-click menus. This video gives some idea of what the different modes look like: https://youtu.be/PQXvfIoiUZc
[removed]
Either is fine. Say that my data is 4000 samples long. I would like to hang controls so that the only information graphed is, say, 500 to 1000. It does not matter to me if we discard the other 3500 samples or if we have simply rescaled the graph. Is there a convenient function or node to rescale on the block diagram of "Waveform Graph", or should I look into XY Graph?
What do you need to do with the cRIO for stepper motors exactly? What modules are available for you to use and what type of stepper motor do you have? Beyond this, is there a budget limit or what available stock components are available to you to make it work? Between the gap of my comment to your reply, I'd suggest reading/watching any and all basic electronics 1 &amp; 2 topics since we're dealing with motors. Also, check if there are free online self-paced training courses (e.g. EdX). It's essential to have basic understanding.
Why the for loop? Just use the "Array"‚Üí"Array Subset" block to cut out the desired range from the array.
Hey, sorry for the late response, had a couple of people email me some feedback and I'm implementing a few changes. PM me your email address and I'll email the packages as soon as I'm done.
Shoot me a PM with an email and I'll send the packages. I'm making a few changes to them but will send them out in the next few days.
Sounds great, send me a PM with your email and I'll send it over. This will work well in RT with one small change to the framework. There is one piece of it that's dependent on a .NET library but could be removed with almost no changes.
Sounds good, send me a PM with your email and I'll send the two packages to you.
Yeah, I tried PTP Sequencer and had a similar review to yours. Too restrictive and pretty obviously a gateway to their services. Now, I'd love to have this turn into a little lead generation, but that's not my end goal. I just have had good luck using it and want to make it available. It's kind of the anti-TestStand though, which I know is pretty risky given that TS is the gold-standard in the LV realm. It doesn't have a UI for drag and drop steps, but it has the hooks for one. It also doesn't come with a built-in UI for running, but it has examples on how to build them and a template to build from. It's really a bare framework built around a sequencer. It's also based on a framework that's all by-reference, and I know that can be a little daunting to people. But, I've beat on it some and it seems to have a fairly strong foundation. I just hope people aren't lost in the details. But, I've got templates for creating test applications, test steps within an application, instruments for use in the test steps, and other framework-type objects. The cool thing is that it's got a lot of expandable built-in stuff, like a logger, data display, event logger, system tracker, encryption, etc... and hooks for a lot of other pieces all over. I just need to get some other people looking at it and banging on it to start helping find holes. It's really a case of I use the work-arounds now without realizing it, where a fresh set of eyes would probably say, just do this instead. I'd hesitate to call this a version 1.0, more like 0.5ish.
1. Compactrio is just a platform for the hardware, not specifically what you need to learn to accomplish this. What you really need to understand is how to write LabVIEW fpga code. Here's a link to the NI courses for LabVIEW fpga. (Not sure if these are free or not, I have a license, so they are available to me.) http://sine.ni.com/tacs/app/overview/p/ap/of/lang/en/pg/1/sn/n8:4398,n24:20873/id/3259/ I will try to help with the electrical portion tomorrow I'm lying in bed on my phone. But I'd say that even though any electronics course is going to help you get good background information, you shoukd be able to get by with understanding the sensor/motor specifications and the specifications of your daq modules. I'm not saying to ignore background information, because it will hurt you in the long run, but a simple project can usually just looking at the wiring diagram in your daq module manual.
Is the daq actually measuring a voltage?
yes, how to do it?
alternatively if the value is always 65.5 you could just save that as a numerical constant and subtract it from all the measurements.
From the datasheet of the load cell, it should be possible with a DMM to measure the SIGNAL voltage.
Well to be fair there is LabVIEW and LabVIEW NXG. They both share much of the same compiler. And with scripting it is possible to make your own IDE for current LabVIEW if you wanted. As others have said. G is the language, LabVIEW is the IDE but most use them interchangeably, especially with engineers who don't really care to know the difference.
Do I need a DMM module to read the load cell output into the CompactRio? I have to be able to graph the output. I'm almost done with the block diagram however I'm not sure how to wire the load cell into the CompactRio
Exactly the issue. Red/black is the supply but the other two can be monitored by a DMM or oscilloscope, whichever is available. A fast enough DMM can graph the response for you but oscilloscopes normally have better time resolution. Unless you plan on using a micro-controller w/ analog-to-digital converter? With this method, you can transfer the buffer of data via UART. Unless you're comfortable with your electronics and programming, I wouldn't recommend this.
Well, I'd begin by asking colleagues what we have for the cRIO before looking to buy. Either way, you need some form of voltage monitoring.
Looks like it. I haven't used the module myself so I can't really explain how to use it exactly, but the datasheet seems pretty straight forward. [Figure 2](http://www.ni.com/pdf/manuals/374186a_02.pdf) is a direct match to the load cell wires but verify that it's capable of measuring within the range of the right LSB200 model. Either way, I'd recommend doing a 'bench test' of the load cell to understand how it works. Power it up, push against it, and use a hand-held DMM to monitor the signal pins to validate the math. The cRIO module would only make that measurement easier but it's good practice to try it manually first.
I'll skip messing with that much scripting and just theme my existing IDE instead: https://i.imgur.com/8pdxbqy.png https://i.imgur.com/BWR7SIk.png
Damn my NI 9503 isn't compatible with scan mode unfortunately
Then browse through examples. There should be quite a few for 950X.
So that is pretty neat, and I've never seen anything like that done before. Do you have a link or a post describing what you did to make that happen?
I don't have any sources. I just started going through the resource folder in my LabVIEW directory and finding the VIs that make up the menus. Data Member Access VI is in resource/Framework/Providers/LVClassLibrary for example. resource/dialog has a lot of good ones. QuickDrop is contained there as well as the LabVIEW preferences dialog, new project wizard, bookmark manager, etc. https://i.imgur.com/zCooB8Z.png https://i.imgur.com/JJTEvkA.png https://i.imgur.com/cLmoDVQ.png I really wish I could theme the getting started window but they compiled it into a packed project lib. :( Most everything is password protected so I've had to bypass that frequently. I'm sure you know how to go about doing that. Because of the password bypassing, I'm hesitant to make a blog or tutorial myself.
Also the "CRIO programming guide" is pretty awesome and it will help you to make a proper architecture. You may not need all the logic they implement on it in your prototype, it's very useful to have it thought. 
What signals are you getting, can you post a screenshot like that? What are you measuring, how is it acquired, what DAQmx settings are you using? It's impossible to give you good afvice without more info.
If you have a periodic signal, then it will be near-impossible to catch it exactly at the inflection point, even with triggers and doing some shifting. Is it important that this happens in real-time? If not, you could do something like this: http://forums.ni.com/t5/LabVIEW/Shift-waveform-on-x-axis/td-p/783288
I am struggling to understand why this would be done. This piece of code is called inside of an image acquisition loop. Let's say we want a sequence of 30 images, this piece of code is being called minimum 30 times. It can be called more because quite often the image it returns is null (0x0 pixels) so it is ignored. You can see an image read from the camera 7 times for each image to be acquired. Starting, then stopping and un-configuring between each image seems strange as well. Can anyone shed some light on this? Thanks.
This code looks like it was built using the Vision Assistant. You can do all the grunt work in Assistant, than you can than tell it to compile to LabVIEW and it spits this stuff out and you copy/paste it into your code. Or they saw the IMAQ example code and didn't know what they were doing. Either way, it's likely a memory-hog and quite possibly crashes occasionally. 
I thought so but I wanted to confirm before I ask for any changes.
I have dug into this further, they only create one image buffer in the camera. Could null images result from this if the get image VI is called while the camera is in the middle of writing to that single buffer causing the get image VI to timeout? Null images are a real issue in this program that caused many problems until I discovered the issue and put a check in to ignore them. The provider of the code claims there are "false triggers" which are causing the null images, but that doesn't make much sense to me. If there was a false trigger the image would be dark since the strobe is not synced with the exposure. 
For anyone that is curious, I managed to fix the problem! It turned out that I wasn't using the myRIO version of LabVIEW. When you go to help and click about LabVIEW, a splash screen will come up. Verify that it says LabVIEW nyRIO. If it doesn't, you need to install the myRIO package for LabVIEW.
I remember having an issue about 5 years ago where some example code left a reference open, causing emails to not get sent and also the VI would sometimes hang. So if your code is opening a reference before sending the email and not closing it again, try closing it.
Well to force an email to not go out, unplug from the internet, disconnect your Wifi, and Ethernet. In the past what I've done is keep track of emails that failed to go out, and would retry periodically until the internet came back. I have a facility that has unstable network connections, but I want daily emails to go out to the team members. So at midnight emails are sent out, but everyone that returns an error (on the error wire) will be kept and retried later.
Not sure how new you are but at the bottom of this page is a bunch of free training links, one is targeted to students. http://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495
Could you explain a little more what's going on here?
Sure. The leftmost block with the "IMT" icon is an implementation of the abstract "Controller" class. All "controllers" have some common properties, and common things they have to do, but each one does those things differently, using possible additional properties. So there may be a "make control recommendation" function that the default controller does nothing with, but the IMT controller has some complicated algorithm that it uses to execute. Anyways, the process by which a subclass can implement one of these abstract functions is called "dynamic dispatch", or "overriding". What I'm showing here is that I can turn my IMT subclass into text (XML), which can then be, say, written to a file. After this, I can read said text and interpret it as just a "controller", and then call one of these overridden functions. The expectation is that after being turned into text and back, it would have forgotten it was an "IMT", so the "Controller Name" would just give the default name. Instead, amazingly, it knows that it was not just a controller, but an IMT controller! This is really cool because you can take something really complicated like saving the program state for later resume and make it as simple as writing and reading some text. Neat, right?
Also, before anyone points it out, I know this breaks whenever you change the class name, private data cluster, or inheritance hierarchy, I'm well aware. But for stable code that needs a serializer, it doesn't get much simpler than this!
Ah so it's an LV implementation of serializing objects? Or same idea it sounds like. I've done that plenty in .net but didn't know it was so simple in LV. Cool! 
Yep just serializes to XML, but keeps all the inheritance relationships. 
I believe you can just flatten the class to string and write to file. I believe that method of serialization will be resistant to changes in class private data.
Not changes in the data, it works fine for that, but in the cluster itself, which that method is not safe against either. Adding a numeric control for example. It breaks compatibility with old serializations. One benefit of the XML serialization is that you can easily modify the contents of the serialization to change the state. Not as doable with flatten to string. The flatten to string does win on storage though, if that's a concern. 
I thought you could add to, but not take away from the class. 
I get what you're saying. Have you tried OpenG's INI VIs? I use them all the time and they're very change resistant. Don't think I've broken it yet. Edit: Examples - http://imgur.com/a/mgp25
I recall having issues, but that may be me misremembering. You can't change field names though. 
I love OpenG! I use their cluster/variant VIs to enable easy logging (headers pull directly from the cluster field names, data serialized using string functions). It's awesome. I discovered the INI stuff only after building my own, which I haven't had a need to change yet. But I'm always impressed with the quality and reliability of that code. Thanks for the examples!
That makes sense, effectively deleting one field and adding a new one.
If there was an open position, you had the requisite education and experience, and you interview well. Sure, I'm sure you could get a job at NI with a CLD. If you're asking is a CLD required? No, it's not. If you're asking will passing the CLD automatically qualify you for a job at NI, that answer is also no. Getting a job at NI is the same as getting a job anywhere else.
Listings here: http://www.ni.com/careers/
You should definitely post this to the NI community forums. There is quite a bit of support for questions just like this. Source: I'm an applications engineer at National Instruments. We monitor forum posts as part of our job.
As a rule of thumb, always have a number of samples that is double the frequency you want to observe. This way you don't alias the signal. I'm not sure I can assist on the dB calculation unfortunately.
I'm not familiar with the VIs you're using for the spectral analysis, but I would suggest to you that there is a large near-DC component of your signal. If you don't care about DC offset, try subtracting the mean of your signal and see if that cleans up your low-frequency spectrum a bit. Alternatively, you can high-pass filter your data, cutting off at maybe a couple of 100Hz.
hello. thanks for your response. I have posted on the forum today, first time I do it, could you check if it is ok and if I follow the rules? I also made the questions more specific I think? I am not native english speaker, so sorry for any misunderstanding https://forums.ni.com/t5/LabVIEW/Vi-to-measure-Sound-Pressure-vs-Frequency/m-p/3655143
Just at a glance, I think it's fine. If it isn't, it's not a big deal - someone will let you know if you broke a rule and how to correct it. Just putting your VI up instead of a picture already puts you ahead of most brand new users!
Not enough info here. How are you communicating with the transducer? Through direct serial (RS232, USB, etc.) or through a DAQ device? Post a VI snippet, a circuit diagram, anything to help us figure out what could be going on. In the meantime, NI has a [good white paper](http://www.ni.com/white-paper/6940/en/) on 4-20 mA loops.
Still no luck. I installed the ivi driver. Then the ni-visa package. Labview driver is installed. USB to serial driver from FTDI is also running. I'm lost.
If there is any kind soul who can help me. I have no ideas on how to troubleshoot this after visiting old forum posts and using the owner's manual. I've reached out to Sorensen for help. Thanks!
You have a CRIO. This is a real time controller. The example you linked works with DAQmx. DAQmx is designed to communicate with DAQ devices. The two devices are designed to solve different problems. CRIO -&gt; real time control. DAQ -&gt; continuous data acquisition. You can do continuous acq with a CRIO, you just have to program it yourself. So does the problem you're trying to solve need? If it needs control, stick with the CRIO, and look at articles that talk about scan engine. If it needs DAQ, maybe it's worth buying a CDAQ chassis instead. The amount of time you'll save programming could made the added hardware cost worth it.
Agree. CRIO stuff can get hairy, especially if like me, you don't have a lot of experience with the FPGA. I just did a project with a CDAQ and 9237 and it was _mostly_ straightforward. It's the way to go if you don't need the RT stuff.
Do I need to install the Ni-VISA package for the drivers to work from AMETEK?
Can you share some code? 
https://file.io/x5gsrD
Create an array of buttons. Use search 1d array to get the index of the button that changes.
If you're running on an NI Linux target (cRIOs, sbRIOs, etc.), there is a syslog there (since it's Linux) that you can tap into. There is a really robust framework that NI Systems Engineering puts out called the [Distributed Control and Automation Framework (DCAF)](http://forums.ni.com/t5/Distributed-Control-Automation/gp-p/5235) that already handles logging to syslog for you. Otherwise, I think your QMH approach is good. You should look into the [Delacor Queued Message Handler](http://delacor.com/products/dqmh/) which is a really helpful framework that has already solved many of the issues that you're bringing up. Both of these tools are available for free on the LabVIEW Tools Network or through VI Package Manager.
Ha, just finished it up now. Ah well, it was a learning experience! I did something that somebody suggested on the forums, which is to cache a logger queue reference created by an action engine in all of my reentrant VIs as well as in my QMH. That way, we have one source for the queue, but no bottle neck in the AE.
If it's not an array, you can use the control reference inside of the event structure to get the property "LabelName" and then parse out the button name (assuming the labels have the numbers in it.) If it is an array you can use the last value/ new value nodes to compare the values and determine which index was changed. 
Try using a shift register to hold the last value on the outer most loop that you need to pass it. 
There's an array function called "Threshold 1D Array" and it's inverse function Interpolate. You can use these in loops to find the index, just don't forget to adjust the starting index .
You can use the new(ish) conditional indexing output of a loop to do this handily. You only add something to the output array if a condition (in this case your edge search) is true. Then just pass it the index of the loop (into the terminal itself)and the Boolean output from your check for the true after a false (into the conditional terminal of that output) and Bam, you are done. 
Like this http://zone.ni.com/reference/en-XX/help/371361J-01/lvhowto/condacc_valuesnloops/
Thanks this solution worked very well for me!
It's going to depend on your hardware, but I'm going to guess you probably want to search your preferred IC website (digikey, etc) for a through hole multiplexer
Same thing can be achieved with almost any relay board ([NI USB-6525](http://www.ni.com/pdf/manuals/371818b.pdf) for example) by tying the relay the commons in common.
You may also want to check out Arduino. These microcontrollers are easy to program, cheap and you are also able to connect them through USB if you desire and talk to them via virtual com-port. 
You have a lot of choices. Ultimately it looks like you want to use physical relays (most allow signals up to 250V). You could either use a dedicated multiplexing board (The board is wired such that only one path is connected at a time), or you could use a bank of relays and wire them so that all their common outputs are tied together, and then you just write code that only allows for one connection at a time. What's most important to you, Cost or Hardware quality? _____ If cost is important, go for an arduino + relay board, like this... https://www.amazon.com/LANDZO-8-Channel-Relay-Module-Arduino/dp/B01NBUDHPB/ref=sr_1_1?ie=UTF8&amp;qid=1500396954&amp;sr=8-1-spons&amp;keywords=arduino+relay&amp;psc=1 The cheapest thing to control it would be something like an arduino microcontroller: https://www.amazon.com/Arduino-Uno-R3-Microcontroller-A000066/dp/B008GRTSV6/ref=sr_1_3?ie=UTF8&amp;qid=1500397057&amp;sr=8-3&amp;keywords=arduino+uno You would write you code in LV, which would use a serial message to control the arduino. The arduino has digital outputs that would drive the relay board. The code on this is not terribly complicated, but you would not be using DAQ drivers, but the VISA serial messaging and you would have to deal with a USB com port on the PC. This adds a little complexity and possible failure mode. This is what I would use in my own home for an inexpensive, non-critical application. You can even get cheaper controllers and relays, but we're talking $40 here. Another advantage of the arduino is that you could put a fairly simple protection circuit into the controller code, so that if something goes wrong at the PC level or you generate a bug that the relays can shut down or be more sure that they won't short circuit two paths. _____ To go the other route, where money is no object (or you need a very robust, mission critical system) and want overly simple programming, you could go with an NI relay board on a PXI chassis. That's going to run in the thousands of bucks, but if I was shipping this off to China on a test rig that I never wanted to have to fix, that's what I would use. ____ In a local environment where I could easily support a problem (and a problem wouldn't cost us $50K a day), for an employer that would want a compromise between cost and my time, I would probably use an NI-USB 9174 chassis + 2 9482 relay modules. This will run about $1200, but is reasonably robust and very simple to program and is practically plug-and-play in terms of hardware. Remember that to a reasonably sized employer, you cost about $100/hr considering benefits and such. So if you can save 4-8 hours of learning/programming/debugging/wiring/soldering, it's a wash. _____ Lastly, I recently saw an Industrial Arduino, which is a relatively inexpensive way to get the cost down but beef up the hardware, for about $200. With one of those, I would not use that cheap relay board, but probably find a set of DIN mount discreet, industrial relays, about $20-50 each. If this was the solution, I would spend some time to figure out how to talk to the arduino over some other method of communication, other than USB. It's been my experience that USB can be a source of pain in the ass-ness. Driver updates can break things, plugging something else in can break things, hubs can cause headaches, even certain brands of PC come with shit USB controllers that refuse to play nice once in a while. IF you do use PC + USB in a production environment (which I have done, many times), I suggest buying a dedicated PCI/PCIe USB controller (and get a couple replacements), so that you are not at the mercy of shitty motherboard design or having to deal with driver changes if IT tells you you have to change out the PC. (OK, I'm getting ahead of myself here) _____ Note: For all of these solutions, even the serial programming is simple, so it's really about how stable/robust the hardware is. With the arduino, you are relying on relatively inexpensive non-industrial boards that have a *relatively* high failure rate, along with a bit of your own wiring and a separate code base on the controller. That's not necessarily bad, but I wouldn't send that up on a satellite- and I have actually programmed QA for satellite parts- that's when you spend $10K to get a bank of relays that you don't have to worry about. 
Anyone who knows enough to recognize a CLAD also knows it's pretty meaningless. A decent student could pass a CLAD after a day or two of studying. I would say it isn't worth it unless you plan to get at least a CLD.
CLAD is pretty useless, its just memorizing a bunch of flashcards. 
While I agree the CLAD isn't a super hard test, I'd argue its a pretty big step above all of the people that just slap labview on their resume because they opened it once in school. I think it's worth the time to get considering it doesn't take much time to study for.
&gt; people that just slap labview on their resume because they opened it once in school. Pretty much every spaghetti-filled project I get where the client has source code. "We had this summer intern who wrote this program but we want to add this feature or fix this bug". Also our sales guy had LabVIEW on his resume specifically because he opened it once in school. On his first day at my company, he saw what we were doing with LabVIEW and removed it from his resume.
Counter point: if you say you want a job that requires LabVIEW development and don't have a CLAD, I would assume you want any job, not this job. Also: sure, I'm not going to trust a CLAD to architect something, but it's nice to know I'm not going to have to teach you how to right click. 
My main point is a 5-10 minute discussion in an interview can tell you as much if not more than a CLAD.
I don't disagree. I could tell in 5 minutes. The HR recruiter that the company hires to find engineers can't though. 
sadly, I know of some CLADs who doesn't understand the value of Ctrl+H and its relatives.
I know this is late but your first suggestion worked! Thank you!
No problem. Glad it worked for ya.
Energy harvesting is cool. Vibration, solar, wind, rf...
Welcome to LabVIEW and Arduino. What you are talking about is aboslutely possible. An Arduino will appear as a serial port or COM or RS-232 to a PC that it is plugged into (assuming drivers are found). This in LabVIEW can be read and written to using the VISA drivers, which is usually part of every LabVIEW install, but I think is included in the Device Driver install which takes place after installing LabVIEW. From LabVIEW go to Help &gt;&gt; Find Examples, and search for Simple Serial. It is a basic VI that configures a COM port, writes to it, waits, and reads from it. Your Arduino can perform reading and writing to the serial port and LabVIEW will see it. Writing a sketch that reads from the serial port and acts on it is pretty simple and there are Arduino examples of this. However there are actually two toolkits for LabVIEW that already do this. The [LIFA toolkit](http://digital.ni.com/public.nsf/allkb/A20FBBD36820669086257886004D5F4D?OpenDocument) is older and discontinued but still works fine. [Here is a video](https://www.youtube.com/watch?v=RhdnmFJcFA0) talking about it. It comes with a basic Arduino sketch you upload which already listens for specific commands to set outputs, or read inputs. It has several protocols implemented but also has a section in the sketch where you could add your own custom commands. On the LabVIEW side after you install the toolkit you'll have a palette of functions that send and receive the pre made functions making life much easier. [LINX](https://www.labviewmakerhub.com/doku.php?id=libraries:linx:start) is another Arduino compatible toolkit which is newer and supports more devices. It works on a similar principal as LIFA. I believe both toolkits rely on NI VISA. Then you can get into crazy stuff and actually write LabVIEW code in G, which gets compiled down into code for the Arduino. This is [custom compiler](https://www.tsxperts.com/arduino-compatible-compiler-for-labview/) not made by NI, and I haven't used it personally but love the concept. Oh and for more general free LabVIEW training check out the links at the [end of this page](http://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495).
If you are really going this route I would recommend you only use labview for the GUI. Learn how to program the Arduino using text tools or another language. It will really help you in the future with other platforms such as raspberry pi. Labviews support of these things is really janky.
You can download it for a free 30day trial... A quick Google search should have found this for you. If you to want to learn something in a week you should learn how to google stuff. I realize that comes off as dickish, but if you're trying to be a programmer / engineer, you really need to learn when to Google and when to ask for help.
Googling doesn't quite give me any direct answers. Which is what I was hoping for when I came here.
https://imgur.com/D2l5sJI I got a direct answer from Google. But regardless in this industry you won't always get a direct answer. I'm just trying to say googling LabVIEW, going to the Ni website and looking around would've got you what you needed. Again, I know this sounds dickish, but I'm really just trying to stress to you the importance of being able to find answers for yourself in the industry.
Yeah but clicking on that link, all the "application" options they offer are incomplete versions specializing in specific operations. Was hoping if I asked on the subreddit, there might be a better option out there. I know how to google, but sometimes I prefer talking to people than googling. Because talking to people provides more individualized answers. I realize I wont always get direct answers (sometimes I wont get any), but is there any harm in asking? Don't worry about me in the industry, I am simply seeking help for finding and downloading the program.
http://www.ni.com/download/labview-development-system-2017/6679/en/ leave the serial number blank when you go to install and it'll give you a 30 day trial
We do know where you can find it. Yes, there are trial versions.
And that it's capitalized as LabVIEW. The OP might want to show that attention to detail at the interview, rather than typing its name wrong.
Perfect, thanks for the help
if you have programming experience I don't think it'd be impossible to do some research and try to understand the typical state machine layout in labview, and if you're confident with that go to queued state machine layout...
There are a lot of tutorial exercises that come built in to most LabVIEW installs, I would start by doing these. After this try the start of the CLAD exam preparation syllabus
At the bottom of this page is a ton of good training and videos. http://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495 If you are a student or have an SSP (bought LabVIEW recently) you can do the online training which is pretty good and follows the official courses. The myRIO guide is also good for sensor basics and includes videos and wiring illustrations.
It will slow the launching of LabVIEW just because it has to populate all the palletes. So aside from a tiny bit more RAM for having those extra palletes and things in memory, it shouldn't require any other processing power. Edit: and the slower launch won't be anything significant, I have a lot more modules installed than that and it takes at *most* 10seconds to load.
appreciate it!
Generally the answer is no, but it can depend. Some toolkits like DSC, will add other necessary services that do run in the background. Things like an OPC server, and possibly a few others like NI Network Discovery, NI Service Locator, NI Variable Engine, NI System Web Server. But for the most part, once you have LabVIEW and MAX installed, most other toolkits don't install any extra services that are always running.
Care to give any explanation with the video? If you are trying to draw too much power then it will cause the myRIO to reset/switch off due to volt drop. You can't drive high current loads (like motors/servos) directly from the myRIO and need to use an appropriate power supply.
I know that a lot of the staff I've met at the UK office did a lot of their certifications while working there (I think they get free access to the courses &amp; certs) so I don't think they're essential to get a job there although I'm sure it wouldn't hurt.
TestStand has a few examples of operator interfaces, which includes a custom step list control. It's entirely possible to build your own with the TestStand API, but that is aDEEP rabbit hole. Check out the examples in example finder and see what you think!
Yeah it sounds like an Operator Interface (OI) is what you are looking for. They can get very advanced and be pretty complicated. Start with some basic one, I think several ship with TestStand.
I'd like to take a look at it too. Basically trying to find something I can build upon and not reinvent the wheel. Also don't need anything as onerous as TestStand.
Hi, recently employed by a company that mainly uses labview / NI software. I never received formal training but if you want to work for a company that develops mostly in labview they will likely prepare you for certification, starting with the CLAD, and will likely pay for the exam fee as well. I'd wait until you get a job unless you just want it for your resume, then by all means pursue it
Find the rep for NI at UF, unsure if it's still Austin Smith or not. NI hosts free CLAD courses a couple times a year and that includes free certification. The test is quite difficult, don't think you'll just skate by!!
I have been having a good amount of trouble aha. I hope I could skate by, but that's probably not gonna happen. Do you mean University of Florida? 
The class teaches you everything you need to know for the test. Just be sure to study!! And yeah, completely thought I saw this in the UFL subreddit, still applies: see what NI offers for your campus!
I will probably end up going for it then; it seems like it would be good on a resume!
If I were hiring an engineering student out of college and they had some kind of software certification, that would my mind give them some kind of edge over an identical candidate that doesn't. It shows initiative, and wanting to do software work outside of the classroom. That being said most places hiring a software engineer either aren't going to know what LabVIEW is, will have negative feelings towards it, or know enough about the CLAD to know it is relatively easy to get (multiple choice that answers can be studied). Not trying to discourage you, just trying to make you aware that most industries don't have LabVIEW on their radar. If you are going to take it, talk to NI. Last I remember there was student discounts for taking the exam. But if you wait and get hired somewhere, then I've found good places to work for will pay for the exam, or send you to an NI event that may give discounts or let you take it for free. So I guess what I'm saying is it is up to you, but having a diverse interest, and showing you enjoy working with certain tools, makes you more marketable. Employees will see that you want to work and enjoy it, and hire you over a guy that just wants a paycheck.
I hire LabVIEW programmers out of college. If I see CLAD I would take them more seriously. If I see CLD they're getting hired unless there's something seriously screwy about them.
I'm probably going to go somewhere in aerospace. Is it used a lot there?
When I was at NI week there were alot of SpaceX folks there, as well as nasa. I sat at lunch with some guys who were vaccum testing the new James Webb space telescope. alot of of Aerospace uses it for automated test.
I do quite a bit of work with companies at Cape Canaveral. LabVIEW is all over aerospace.
This makes me a lot more hopeful!
just an additional FYI, the CLAD is pretty useless for someone who actually writes code, you can just memorize the practice tests and pass it from what I understand. As another commentor mentioned, the CLD is much more valuable.
I've not worked in aerospace much, and I'm pretty biased. But as others have said, SpaceX, Blue Origin, NASA, and Virgin Galactic (Orbit) all have had some presence at NI Week for several years. Primarily it sounds like it is used for testing, but several use it for automated controls, and mission status information. No idea how much it is used as a whole company.
I would like to get the CLD eventually! Just taking one step at a time. 
Good idea! Just to give you an idea, SpaceX uses labview for its launch control, you can read about in this AMA https://www.reddit.com/r/IAmA/comments/1853ap/we_are_spacex_software_engineers_we_launch/ 
I like using the MGI read and write anything for INI files. Created a class for just dealing with ini files. 
Congrats sir! Well deserved!
I've got to do the CLD-R next week. I hate multiple choice. Would rather do a practical to recertify.
I worked for NI for 3 years, and I'm a CLD. It's common for a lot of companies to send their people for certification, but plenty of folks get it for themselves, and I think it'd be a great move for you to get a certification while you're still in school. Hardly anyone will have it. And yes, aerospace uses TONS of LabVIEW! Bigger companies will have their own LabVIEW programming teams, although at most places LabVIEW coding will fall under the Test Engineering role, which may be more mechanical engineers than software engineers. Either way, there's no way getting your CLAD could hurt. I say go for it!
This works out a little to well I guess. I really want to be a test engineer too haha
Well in that case you DEFINITELY should get LV certifications!! Lots of big companies like Lockheed, Boeing, Tesla and NASA are LabVIEW houses. It's a great tool to know for test engineering. National Instruments itself also hires a lot of people just out of college as Applications Engineers, that's what I did and how I learned LV. Another cool option for you to look into. Feel free to PM me if you want more info on any of this. 
Calling it multiple places in the same Main VI? Could you try local variables? Typedef with defaults would work if the values never changed, but it sounds like they might depending on your input file. 
I don't think local variables will work because the input to each instance of the subvi will be different. And you are correct, the name/value pairs will change depending on my DUT. 
Do you only have to load the channel mappings once, or once per long period of time? If so, how about making a settings file with the channel mappings for each one, indexed by some identification number. Then write a sub-VI that loads the settings from the file. The only input to the sub-VI then would just be the ID number. 
Great stuff! I love the trend towards data flow and visual programming. Had anyone checked out node-red as well?
I don't see why not. I just implemented a fully FDA 21 CFR Part 11*-compliant pharmaceutical manufacturing system with the software 100% in LabVIEW. I think the choice best choice for you depends on a number of factors: a) How large and complex is the line/facility? b) What capabilities does your team really have? (Is LabVIEW the tool of choice among your colleagues?) c) What do your integrators use? I am guessing you may have some third party partners and suppliers to consider. In my case my recent application was modest in size and only had 3 authors. Implementing LabVIEW for a much larger project will definitely require some forethought in terms or architecture and delegation of roles. Depending on your budget you might want to consider bringing in an established NI partner to help. NI offers the Datalogging and Supervisory Control Module. I bought it but never got around to using it yet - I was able to meet my company's needs without it. *This is the FDA guidance for handling data in pharmaceutical manufacturing systems. It's a bit of a pain in the ass.
Thank you for your reply. This might become our company's standard across 15 factories over 5 states. We are new to deploying automation and are building upon a single simple LabView solution used for a single adhesive dispensing robot. There is a small team of 3 LabView and TestStand devotees. Our integrators do not use LabView and typically we buy one system at a time that run of Allen Bradley PLCs. Do you have any experience using other SCADA software like Wonderware or Ignition? Do you have concerns in documenting the LabView for others to maintain down the road? 
NI's generally pretty good about talking through potential applications and being honest if it's a good fit or not. I'd honestly give your sales rep a call and talk to him about it, especially with it being a potential standardization, it'd be wise to get some insight from experts at NI. Edit: Also, I wouldn't be worried about the long term support. NI offers good basic support and there are plenty of alliance partners to provide more hands on help if needed. You'll always have options for help.
LabVIEW is certainly unusual for us. I am the first real LabVIEW champion at the company and I needed a few things to go my way before I got a LabVIEW-based application into the production environment. In the case of my company, across the globe we use a variety of different platforms and tools. PLCs and associated architecture depend largely on the system vendor's preference. For example in our industry a lot of the major manufacturing system vendors are based in Germany and so they have a preference for Siemens products. Wonderware is used on some of our lines, but I don't work with it directly myself at the moment so can't comment too much there. You are right that documenting LabVIEW presented a challenge - our archaic systems were not set up to record block diagrams and most of my colleagues struggled to adapt to the new paradigm in the beginning. In the end for the purposes of record keeping and qualification activities I had to prepare Word documents that detailed each sub-VI in terms of inputs, outputs and function. Essentially compilations of the VI documentation. Your needs will vary depending on your industry and Quality requirements. Something you might want to consider is that the data collection aspect of factory automation is evolving quite rapidly at the moment, with a lot of new startups focusing on cloud-based data collection and analytics. Some of the big players such as GE are investing heavily in this area. That said these are really data harvesting tools and it sounds like you are probably focusing on the stage before that, gathering the data in the first place.
Thanks for the insight. I am going to ask a lot of questions and if indeed we migrate toward LabView I'll be certain we have a robust documentation process and standards. 
You should check out the Print to HTML property node. It basically creates a nice web page of the VI's documentation. 
Yep, Node red is nice, but it can compile code for energy efficient microcontrollers like on Arduino Uno
Can you say a little more on where you or storing the data on SAP? What t-code?
I really do not know much on that end. I'm the mechanical lead. My concerns are really based on my experience trying to work college LabView projects as a collaborative team and my awareness of COTS SCADA systems. 
Yep, used that one pretty heavily :)
Check out the LabVIEW Distributed Automation Control Framework -DCAF https://forums.ni.com/t5/Distributed-Control-Automation/gp-p/5235 It's pretty awesome and comprehensive, and open source! If you are into purely monitoring that the brand new NI SystemLink is what you may need- send messages, read/write tags, exchange files: http://www.ni.com/hu-hu/landing/systems-management-software.html It's in early access but you can be enrolled very easily, talk to your sales guy!
Try using the Current Value Table, it can store its content to a file and read it back 
I've never used a cFP personally, so I can't answer this 100% because I don't know the exact process for accessing cFP I/O in LabVIEW, but here's my best: First off cFP is RT only and has no FPGA. I believe cFP uses project I/O nodes that you can access similar to a Network Shared Variable, or local variable. cRIO is an RT system that connects to all the cDAQ modules via an FPGA backplane. You can access the I/O a few different ways, all of which are different(somehwat) from the cFP. Here's a very simple list of the usual concepts for this: 1. You can set the cRIO to operate in Scan Engine mode, where the runtime environment deploys a static bitfile down to your FPGA and allows you to access all your I/O through Network Shared Variables. 2. Write custom FPGA and read/write data to the FPGA using the "Read/Write Control" node in LabVIEW to access controls/indicators from your FPGA. 3. Custom FPGA with DMAs to stream data back and forth from the FPGA/RT. All that being said, If all you want to do is access the I/O on the cRIO without any custom FPGA, the only part you should have to change is where you are currently accessing I/O to one of the above methods. Both of the systems are running LabVIEW RT and therefore the code should look identical aside from these parts. The simplest method would be Option 1 in my opinion, and here's a brief intro to the Scan Engine: http://www.ni.com/white-paper/7338/en/. Hopefully that helps, let me know if you have additional questions and I'll help as I can. *If you're looking to port everything to FPGA, that's a whole different concept, and I don't know how to cover it other than, learn how to write LabVIEW FPGA and port your code as you can. 
Wow thank you! This is really helpful.. I‚Äôve never heard of the Scan Engine but I‚Äôll read up on it and see what I can do.. I think I‚Äôll try one of these for now and do a full port later when I have more experience writing LabVIEW FPGA
This seems like something that is going to be very hard to find. If anyone had this information, I imagine it would be NI. You may be able to contact your local sales rep and see if they can point you in the right direction.
Thanks for the reply. Is there a general sense of the rough breakdown?
Why do you want to know? I think it would be very hard to get that information.
You may want to look at analyst companies like Frost and Sullivan, VDC, or Gartner. They tend to have approximations of vendor breakdowns in different industries and applications.
Good idea I'll check them out
I'd agree - people use LabVIEW for an awful lot of things. Would you include the several hundred thousand students with access? Say the figure was 5%, would that make any difference to you than if it were 1%? I'm reading between the lines that you want to know possible limitations of LabVIEW vs other approaches. Ring sales and describe your application, and they'd be happy to let you know if it's a good fit and how it would stack up against other approaches. Or if you've already bought DSC toolkit and have SSP, speak to tech support. Good luck!
C-Series could hit all your points but the budget's going to have to give a little bit.
At the end of the day, LabVIEW is a binary file, and unless NI ends up making LVDiff and LVMerge a bit better merging VIs is always going to suck. However, I use mercurial daily and I primarily write LabVIEW code. If people are writing good Labview code, and understand how to work with multiple people, you can use version control pretty well. I think this is an issue of training and disciplibe. Your programmerd need to be writing a lot of subVIs and not working in the main VIs often. If you are, make sure you aren't all doing it at the same time, which shouldn't really be an issue if your team is divided up well. If you need to work on the same vi, then you should be in different areas anyway, so LabVIEW merge works, regardless of how shitty it is. And if you need to work in the same area, at the same time, go in a room and brogram, cause there's no way you can both be doing it in isolation anyway. My team also has a good code reuse base that we all update in a separate repository and deploy via packages. That helps us out quite a bit when dealing with some of the more commonly used functions. Edit: Just as an aside about your little (no offense) comment... Plenty of LabVIEW programmers are very much real programmers. I write a lot of C# and python as well as having several years of embedded C under my belt. Some of my co-workers are the same, some aren't. We're all engineers that mostly write code, our code is just written to interface with hardware. We can all use the same fundamental programming concepts and be intelligent people that understand source control regardless of language. Just because your LabVIEW programmers are incompetent doesn't mean you should assume a whole subreddit dedicated to them are. (No offense) /rant
Thanks, this is pretty much our usage of it as well. Sorry my attempt at not being offensive failed. My labview guys are very competent at labview, just not terribly familiar with source control (go figure).
I hear ya man, sorry. I'm just touchy cause I've had people actually say that stuff to me before, and I want LabVIEW to be seen as a real language, because it's very capable. It's just generally used by non-programmers, very terribly. :/ But I really feel if you force them to start using the tools, they will learn how to make it work with LabVIEW. We've spent years with it and it had some growing pains for sure, but it really is capable of being handled in a standard version control system.
I am not necessarily looking for official NI-made products, any 3rd party would work if it can meet the price (or quite close to it) and moreover, unfortunately I don't have a cDAQ, so it should work on its own
So most really common devices can be found by googleing, or by going to the [3rd Party Instrumentation Driver Network](http://www.ni.com/downloads/instrument-drivers/). I did a quick search on the Votsch device and it is an environmental chamber. All of the chambers I've talked to that have external communication have had an option for serial of some kind. The majority of the chambers I've talked to have RS-485 and use Modbus to communicate which is a pretty basic standard. The manual for your device will state what registers in modbus are for what settings. You can then use the Modbus drivers, or use the VISA drivers to read and write raw serial commands. There are several examples in the Help &gt;&gt; Find Examples on both of these options. Other chambers will have ethernet, and these too will specify how to talk to it in the manual. Typically you'll use the HTTP Put/Push/Get functions. If the manufacturer has some kind of software to control the chamber, you can use some kind of sniffing program like Wireshark for ethernet traffic, and then also recreate it in LabVIEW using the TCP or UDP functions on the palette. I did some searching on both of your devices and I didn't find a manual that mentioned communications standards. I saw the Binder hardware has Ethernet or RS-422 and some software you could run and then sniff the bus with.
What sample rate do you need? The MCC USB-3101 is the cheapest ao that comes to mind. 
I forgot to include it, I need about 100-200 Hz sample rate
Loop with wait block of 200 ms in it is pretty easy. Being super precise is probably a little more work. 
To make it more precise you could use Wait Until Next ms Multiple (to avoid drift) and also use the Tick Count function. Get the tick count before the loop, and each iteration calculate the difference between the new tick count and the current, use quotient and remainder to get the integer multiple, and that‚Äôs your count. This method won‚Äôt drift over time. If that‚Äôs not important do it the simple way described above. 
Both, thanks for the help! 
You can also used a timed loop instead of a while loop and get it very precise.
edit: ignore
&gt; MCC USB-3101 That doesn't sound bad, probably can't really find anything cheaper
I found a pretty good walkthrough of the GPIB handshake protocol in this document: http://gilbert.myweb.usf.edu/courses/instrumentsystems2/notes/iee488pdf/ieee488.pdf I think instead of trying to hold the bus until the command is complete, you may need to do the communication in separate operations where the master sends the slave the data it needs. Then the slave issues a Service Request and then sends the response when the master is ready.
How long are we talking to process? Usually the controller writes a command, and queries for response. If the cic just issues commands, they will (usually) stack up until buffers fill. The controller is delayed for the length of timeout. Are you sure you need to be blocking during this time (not responding to gpib queries)? If you're not blocking, can you have the CIC query for status before sending more commands? Have you looked at issuing a Service Request SRQ? Have you looked at the status event register and using the operation complete bit? See http://www.ni.com/white-paper/2927/en/ This all requires the master (controller in charge) to operate differently, so hopefully you have access to it. 
I've never had code rewrite itself... If it was a bug before, it's probably still a bug now, the worst race conditions are the rare ones. 
NI is currently undergoing system upgrades this weekend. They're scheduled to have everything up by Sunday (Aug. 27) morning 10am CDT.
[removed]
I have had simillar problem with instrument drivers. What i did was just search for inst.lib inside labview folder And manually copy that folder to the other labview version which was giving errors. In my case it was from 8.6 to 2017. Make a back copy And give it a try
it is completely possible. Do you really need a programmatic event? Are you using DAQmx?
It's possible you don't even need an Event Structure for this. But if you do, make am event case for "Value Change" of whatever you are reading, and then in that case include your code that adds it to your array. You would want this inside a While Loop presumably, and use shift registers to keep track of what entry in the array you are up to. However, this might not be needed at all. Can you tell us more about your system? Are you using DAQmx code?
I don't have a ni card, so not using daqmx. It's an iotech board and using their supplied controls for the card. The physical system is a hand turned wheel, which is why it's a noncontinuous signal. Inside the wheel is a reference encoder, and then from the wheel a sensor sends an analog signal. The goal is that for one full revolution the analog value will be taken on every up tick from the reference. 
Thanks for the background here. Ok so yeah the Event structure sounds like it would still work here, and then in inside the Value Change case, for recording your data I would suggest the "Write to Measurement File" Express VI. This allows you to set up a way to write your data to (for example) a TDMS file which is quite a common way of logging data. I'm not sure what controls you have in LabVIEW for your hardware, but make sure you find out a bit about them. It's possible that they may have a load of useful stuff in them, and that you might not need much other code from LabVIEW itself. tl;dr While loop, event structure inside it. Timeout (default) and Value change cases. Inside Val change case, add code to write your current data value out to a file (make sure it appends the file, not overwrites).
Is having the write to measurement inside the event structure better than using a shift register to save the entire array and then exporting it? Edit: http://imgur.com/a/OEslv This is what I have at the moment. The first while loop just waits for the reference encoder to send the index, so that it doesn't start to collect data mid pulse. The second while loop will run until the index again, to ensure one full revolution, then have the event capture during each pulse of A. Does the event structure need the signal input, to know when to act, or does having it inside the while loop allow it use the data already?
With each case you create with the event structure, you choose an event source (such as a control) and an event type (such as value change or "key down?"), so in your case the event structure already knows about the signal and if/when it changes. So unless you want to perform something with the signal value itself, you don't need to wire the signal into it necessarily. However given that you want to either add the signal value to an array or write it to a file, you will want to have it wired into the event structure. I suspect that having the Write Express VI inside the Event Struc is better, because if the shift register keeps carrying an array around each time, it could become quite memory intensive as the array gets bigger. However, I am positive that you wouldn't notice a performance issue whichever you decide to do, as I am guessing your number of samples isn't massive right? If you ever get into a situation where you're recording tens or hundreds of thousands of samples per second, the array approach might get a bit difficult, as LabVIEW would have to keep assigning larger blocks of memory to keep hold of it. Whereas if you use Write Express VI, you just keep appending a TDMS file each time the Event Structure triggers, so the value gets written and is released. Could I just ask what your stop conditions in both of your loops are? Are they stopping after you read one full sample? Just want to make sure that it wouldn't stop the loops prematurely. Would it be better to have a control and a local variable control the loops, so that you can run the programme, then turn the wheel a number of times, then press stop when finished?
That makes sense about the memory getting larger. The reference encoder has 8,192 pulses per revolution, so only that many total data points. I've changed the VI around some to change the stop conditions. Also, realized I think had the wrong condition on the event selected so changed that as well. http://imgur.com/a/o3hjI The first loop will stop after the index is detected, and then for the second loop it will stop after the correct number of pulses have been detected; rather than just using the index again. The wheel only needs to be turned once, as the data will be the same as the previous revolution. With this VI though, the second loop isn't running at the moment and I'm not exactly sure why. The first one runs until the index is hit and stops, but then the second loop never starts. I'm really not sure why.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/WBUopsc.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dm8oijd) 
Have you tried clicking the Lightbulb at the top bar of the block diagram? This is highlight execution, it goes slowly through how the code executes, and if you click a wire, it brings up a window with "probes" and these probes give you real time values of the data. So a value in your 2nd loop will say "Not Executed". I suspect one of three things. Firstly the golden rule of LabVIEW is **"a function can only execute when it receives all of it's inputs"**. 1/ This could be that the first loop never actually finishes or 2/ The second loop still requires another input, which has not executed or 3/ Everything *is* working, but The Event Structure is not triggering. You should be able to tell what is happening with Highlight Execution, so if the 2nd loop just never activates, try and find what it is waiting for and go from there :)
http://imgur.com/a/IcW93 That is the result from running with the highlight execution. None of the "data tracking pulses" ever went into the event structure. I'm guessing that it means the Event Structure is the issues, which wouldn't surprise me as this is the first time I've tried using one, and have used the other parts of the VI in other test I've done so far. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/pHtpfMO.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dm8rh56) 
What if instead of using the event structure I use the case structure? In the second loop program it to check for a high and when it does use that to trigger the case? 
Ok, yeah event structure is not easy to wrap your head around at first! Took me a while to properly get it. One thing you could do is not have two loops, have a State Machine instead. State Machine is a case struc in a while loop, and you use a shift register to carry over the case selector to initiate a new case. Check an example for this (Click Help at the top, then Example Finder I think, if not google how to find LabVIEW examples) and it will explain in more detail how it works. The tl;dr of the state machine for you would be to have an "Initialize" case where you check your index (first loop) and if this is True, make the case selector change to your "Acquire" case, and then in the acquire case you can code in the parts were you read it and write to the Write Express VI. Now that I am writing this out, it does sound like a neater way of handling this. Usually an "Init" case prepares your code for executing, so it runs some stuff that only needs to run once, and you never go back to it (say you want to set some controls to a specific value, or you want to check that you can communicate with your hardware etc, then move into your *actual* code). Hope this helps :)
Yeah I think that's going to be the best way. Making it detect a high is going to kinda suck though. The way the daq is set up A counts from 0 - 255. On reset B will add 1 to its total up to 35. I'm thinking I'll just pull the A value into a shift register, and then compare it each loop until 255 then have it reset add 1 to B and go on. When those both hit the right number stop the program, and have data being collected everytime A changes. Will be a few days before I get to design and test it, but will let you knew what happens. Thanks for your help. 
Happy to help, hope you get on alright with it :)
When you assign a value to Digital Input Port, use a property node set to Value(signal), that will produce the "value change"event for the event structure to catch. You cannot merely programmatically assign a value to DIP. That may be why the event structure doesn't trigger. (This is from a cursory glance at your code and you saying you never get inside the event structure)
also, using the default value on your incrementer to 8192 is doing to set the value to 0 for any events that fire that don't pass the wire through. you may not have any other events besides the Timeout, but if you ever add one that fires, you're going to constantly be reseting that Shift Register http://imgur.com/UQBvS6N
I don't use JSON all that often so I usually stick with the basic primitives included with LabVIEW. That being said are you aware of the pretty advanced [JSON library](https://lavag.org/files/file/216-json-labview/) by Dr. James Powell? It has been getting regular updates. And he has also been working on a [beta version](https://lavag.org/files/file/294-jsontext/) that works much faster and operates with VIMs and has type adaption in 2017. His code also comes with examples, and with no VI descriptions on your palette items I was a bit confused on what some of the functions do and how to use them. Oh also how does this compare to the other JSON libraries out there like the [i3](https://forums.ni.com/t5/JSON-Toolkit-for-LabVIEW/gp-p/8520), and other various [home brew](https://forums.ni.com/t5/Example-Program-Drafts/Parse-JSON-to-Cluster/ta-p/3499458) or native primitive solutions? EDIT: The inclusion of XNodes was unexpected but appreciated, but again with no description I'm unsure how to use a couple of them.
I am aware of all of those, and many have their strengths (and weaknesses). Dr. James Powell's library looks promising, but 2017 is a high bar to clear right now, and there is no guarantee of backward compatibility for VIMs. I haven't fully benchmarked against this library, but maybe I should... As for the others, they are either more verbose than I hoped or very, very slow. For example, this library will parse a 10 MB file in about 12-15 seconds.The LAVA/Powell library took about a minute. i3 took overnight. I'll definitely take the point of feedback regarding examples and documentation. I've been focused on performance and API definition, but perhaps I'd be better served by getting the documentation up to par before eliciting feedback. Thanks for taking the time to respond!
Also, regarding the native primitives, the reason so many alternatives have sprung up is because they are performant, but completely inflexible.
Oh awesome you've made a state machine. There are lots of examples, and white papers in the subject. Usually this is made with a while loop, case structure, and either a string or enum. http://www.ni.com/tutorial/7595/en/ They're is also a free toolkit that allows drawing the states, and having the code generated for you. https://forums.ni.com/t5/Reference-Design-Content/LabVIEW-State-Diagram-Toolkit/ta-p/3606081
Do you mean you're stuck trying to check if a number is an integer? Here's some forum discussions on doing that. https://forums.ni.com/t5/LabVIEW/how-to-find-whether-a-given-number-is-integer/td-p/841096
State machines are great and glory to our state machine overlords and all that, but I would say it's not necessary for this basic flowchart. This is just some simple comparisons that could fit neatly in a few small nested case structures. If it were to grow to be larger and more complicated, then definitely state machine time.
Are you trying to install "DAQmx Base" or "DAQmx for myDAQ on Mac"? They are two different drivers. What daq device are you wanting to use? 
I have tried both. I will be using the NI USB-6003
Neither of the drivers on Mac support that device. You'll have to use Windows and DAQmx. 
I see your using LVOOP Objects as a parsed representation of the JSON, like LAVA-JSON, rather than the Variants used by I3 or JKI. In JSONtext I went away from using any representation other than JSON text itself. The overhead of creating LVOOP objects or Variant attributes is actually quite high compared to the time it takes to parse along a string. Please add JSONtext to your benchmarks.
The JSONtext example "JSONtext Path examples.vi" extracts items from a JSON array generated by http://www.json-generator.com. If one ups the number of generated elements to about 9000 (a JSON text that is more than 10Mb) then one can look at times. With the latest Dev copy, I get the following: JSON_Path / time $[*].name / 0.16 sec (every name: 9k names) $[\*].friends[*].name / 0.49 sec (every friend's name: 27k names) $[\*].* / 0.84 sec (every item of every array element: 204k items) $[\*].tags[*] / 0.39 sec (every tag: 64k tags) $[9000].name / 0.08 sec (the 9000 element's name; this shows the raw JSON-parsing speed of over 100Mb/sec) $[8,9,10].name / 0.0003 sec (the 8th, 9th and 10th elements' names; note how I don't parse past the requested info) $[\*].[name,email,about] / 0.29 sec (getting multiple items in one parse through: 27k items) This is a somewhat unfair comparison, as I'm not converting to LabVIEW types (parsing a cluster structure is much slower than parsing JSON), but it shows the advantage of not converting to a complex intermediate representation when one is only interested in part of the JSON data (example: metadata from a large number of data files).
Those are impressive times for sure. I'll write the benchmarks to compare and report back, but it could be a day or two.
Okay, I didn't consider yours initially because of the VIMs, and after looking at it I'm not even really sure why you're using them outside of not wanting to see the conversion dots. Are there any that don't use the Variant Info API to determine the datatype, etc.? I had considered implementing something similar to your approach, and even considered a hybrid approach where anything that was never cast to a more specific class would not be given an intermediate type. My parser is set up to allow for me to get the indices to the raw data in the original string, so I considered making it such that on initial parse it creates a generic JSON object with a starting index and length to the raw data. I would still take the overhead hit of creating the object, but not the hit of the typecast. I might play around with it and see what kind of performance boost I get. I also really like the * pathing that you're able to do. Thanks for taking the time to check it out.
At the moment I am only using the VIM feature to hide the ‚ÄúVariant-to-Data‚Äù boilerplate that pollutes the use of Variant-based APIs. But I consider that quite valuable in itself, and I‚Äôm hoping to get some performance improvements once VIMs are more developed. The path stuff tries to follow http://jsonpath.com.
Hmm, interesting question. My first idea would be to make a separate array that records the amplitude value whenever dy/dx = 0 (i.e. peak or trough), interpolating if necessary. Every second value will be a max, so reshape the array and split again to get a 1D array of maxes and a 1d of mins. Then it's a simple 'array max and min' of the two arrays. There might even be a function that does something like this, I'll take a look when I'm next in front of my LabVIEW
Can you find zero crossings of the derivative? I.e. der of signal = velocity, zero velocity is min or max. Direction of sign change tells you post/neg peak. Then just use array max and min to find min pos peak and max neg peak.
So far I've found JKI's JSON Serialization library to be the best for my use. It does a great job of dumping clusters to neat JSON files. The two pitfalls are that there is no support for class objects and reading back data from JSON files is tricky if you don't know the exact format of the JSON file in advance. However, with some slight additions and modifications to the JKI library, I have a set of tools that works quite well for both loading and saving JSON.
[Relevant thread](https://forums.ni.com/t5/LabVIEW/Local-maxima-and-minima-of-an-array/td-p/2241500) - didn't think about whether your sine wave would have noise! The methods suggested in there look better than my suggestion. 
My solution would be to create a while loop that captures one period of your sine wave as a 1D array, use the Array Max and Min function to pull out the max and min, then use shift registers to compare it with the previous max and min.. I‚Äôm not actually using LabVIEW right now though so it might not be as simple as I‚Äôm thinking 
There's several ways you could go about this. My first though: Run the data through a for loop and split it into two arrays using conditional indexing outputs of the for loop, one for the greater than zero, one for the less than. Then run those two arrays through the 1d array min/max. If that doesn't work or doesnt make sense I could mock something up. But basically, I would operate on the greater than zero and less than zero data sets separately, in some way. 
There is already a VI that will do this for you http://zone.ni.com/reference/en-XX/help/371361J-01/lvanls/peak_detector/
In these cases you can also use the 3-points way to estimate derivative in a segment. This works on a constant frequency signal source. Based on the frequency, sampling rate and noise calculate/estimate a distance in samples where a local maximum/minimum must be present, that will be shorter than the half-cycle. When you have that info run that segment through the acquired data, meaning that you index out the first, middle, last samples from the segment and compare those, starting from the first acquired sample and check the relation between those points. If the middle one is higher then the two others it is close to a positive peak, if smaller then it is close to a negative peak. Otherwise start with the next i+n sample from the original signal and put this in a cycle. n can be 1 or you can try with larger number to speed it up. Now that you know the estimated location of the peaks you can look for min/max values. If you have good prior information about the signal/noise you can create a delta range for the expected difference in values (how greater/smaller the middle sample should be) and fine tune the algorithm. Based on what precision you need with this fine tuning you can even not do the max/min search just take the middle sample. This method is very similar to what you did just can be a bit faster if you do it well. 
You mean that's not what everybody's LabVIEW code looks like?
They usually don't offer much on the CLAD as it's pretty straightforward. Read the LV Core 1 &amp; 2 material, take the quizzes at the end of each section, you should be fine.
Thanks!
I just took it about a month ago, and basically all of the problems are snippets of code so you need to be final at with the main functions, their inputs and outputs, and their default values.. It is basically all the same concepts that you‚Äôll see online in all the review documentation but code based instead.. Good luck!
Thanks! How did you feel about the amount of time given for the exam? Some of the older practice exams were very easy to finish within the hour, but the questions in the preparation guide took some thinking. It wasn't even 42 questions, and I felt like I would be crunched on time.
I didn‚Äôt have much problem with the time.. I think I had about 10 minutes left after I completed the exam but I read rather fast and marked a lot of questions for review so I wouldn‚Äôt waste all my time on some of them.. Also, mine was only 40 questions, it didn‚Äôt have the 2 other questions and I‚Äôm not quite sure why.. But either way, it wasn‚Äôt bad! The main thing that got me was that I didn‚Äôt study the hardware functions enough and I didn‚Äôt memorize the array functions.. As long as your familiar with them and other major functions, you should be fine
Try creating an account with your .edu email address. I'm pretty sure that's all it takes (at least it used to be).
it might be easier to find any windows system to use. does your university not provide some windows system on a server somewhere?
As far as I know there is no student edition for the Mac. Last I heard they was no licensing scheme for Mac in general. This means all Mac software is licensed with a specific code built into the software. You can contact an NI sales rep and they can tell you for sure.
Aww man, that's kind of frustrating. I really appreciate your help though, thanks!
Maybe I did something wrong, but the account I made was with my .edu email. :\ I really appreciate the suggestion though, thanks!
As far as I know, none of the computers that are open for public use have labview. I can check them out tomorrow to confirm. The professor never mentioned anything about that, so I didn't even consider that possibility. Thanks for the suggestion!
NI made changes to its academic licensing in the 2017 Spring Academic Site License update package: the Student Install Option is given free of charge to all Universities having active Software Service Program. That means all students (legally students) can install a specific software package (Student Install Option) to their own laptop. This contains way more software than the Student Edition, basically has nearly everything that the Academic Site License has plus it's free for you - and definitely has Mac support! Speak with your teacher to point you to the university software administrator and ask for the installer and appropriate serial number. If they are unaware of this information about the Student Install Option point them to the local NI representative (you can contact NI via the website). On the other hand not all the toolkits and drivers are supported on Mac so do some research and find out if your project will work on a Mac. And a good starting point for your project is ni.com/gettingstarted 
There is a high chance that the contract your school signed with NI (Academic Site License, or similar) includes Mac version. You should contact the ASL administrator (this will be most probably the guy that gave your teacher the serial number for PC). You can also write/call NI support and ask them to give you his name (once you give them the SN, they should cooperate). More info: http://www.ni.com/white-paper/52948/en/#toc4
Try giving them a call. They really will try to help. They want students to get their hands on LabVIEW... 866-275-6964
Unfortunately my school has never purchased any sort of license from NI (or if they have, they sure like keeping it secret!) so it doesn't look like this is the solution like I hoped it would :( I really appreciate how helpful you were though! I'll definitely check out the getting started site. Thanks!
Unfortunately my school has never bought any sort of license as far as I can tell. That's why my teacher wanted us to buy it ourselves. :\ Thanks for the advice, though!
I'm going to call them tomorrow if I can find the chance. I'd just like to thank you once more for how helpful you've been!
I took it not too long ago. The main thing to look out for on the hardware is understanding how sample rate works. An question it likes to ask is how large the array of data will be after X seconds, given a sample and clock rate
You need your CLAD before taking the CLD: http://sine.ni.com/nips/cds/view/p/lang/en/nid/10647
In the public TestStand folder, there are two OI's. One is simple and probably best for deployed code and such. The other is full featured and should do what you are saying. If not, there is a lot of material on editing the OI to fit your needs. As the other commenters have mentioned, that is a deep rabbit hole and is suggested that you a strong understanding of the TestStand API
There were a few of those on the test. Luckily they were too bad. Ended up passing so I'm pretty satisfied!
I'm happy to help! Best of luck to you!
Another approach that you might consider...granted it will cost a little more up front, but it might be helpful in other areas later: Consider getting Parallels, or VMWare Fusion, or something like that where you can run a Windows Virtual Machine on your computer. Then you could install a Windows version of LabVIEW on your Mac via a VM.
The vision licensing is super confusing, and it's been a while since I dealt with it but I believe Imaq (which includes the avi VI's) is free. Imaqdx requires a license but shouldn't be necessary if you only what to save video
I searched for my nickname I use om the internet and I found this!.....
No vision deleopment module requires a paid runtime license.
Good suggestion. I have a query in to NI. They are checking whether the AVI function is in the IMAQ drivers (free) or IMAQdx (requires Runtime license). 
How it works is that there is a Vision Development Module that enables you to develop machine vision applications and there is a Vision Development Module Runtime for that. In your case if you only want to save images you will need a Vision Acquisition Software which is much cheaper than the Vision Development Module. (Btw the Vision Development Module includes the Vision Acquisition Software) The Vision Acquisition Software includes IMAQ and IMAQdx drivers, which are different by what interfaces they support and on what level (IMAQdx is the newer one). Your other option is using various 3rd party dll-s to acquire and encode an image stream. As a hobby project it might make sense to do that. But if it is an industrial project you are working on then the time you can save by the Vision Acquisition Software is significant so might make sense to just buy it.
It's confusing, but it is quite well explained here: http://digital.ni.com/public.nsf/allkb/F1699570F78FECBB86256B5200665134 The main idea is that licensing depends on the hardware you are using. If it's something from NI - the driver will be free.
Edit: /u/litechniks said everything I was trying to say. No point in saying it twice. :) 
Yes.
Thank you!
You already got your answer, but I'd also like to add that if you open a 2013 VI in 2015, and then save it, the VI will be saved in 2015 and can only be opened in 2015 or newer without back saving. 
Ohhh, thank you for the additional info!
https://info-labview.org/LabVIEW%20Version%20Compatibility.pdf just for completeness 
What have you tried so far? What does your code look like?
Literally I have a waveform generator hooked to AIo and AIGND. In the blocks I have the myRIO analog input block defined and have that sending to a waveoform chart. It, semi worked but doesnt dip below zero. So i tried to merge both the positive and negative leads on two seperate AI pins and displayed them on one plot and still didn't work. 
please upload the vi, a vi snippet, or at least a screen shot. &gt;When enquiring for help on your code/program, please provide a VI snippet when possible. VI Snippets are .png images that have your LabVIEW source code embedded in them. For more info on VI Snippets visit: http://www.ni.com/white-paper/9330/en/
Additionally, keep an eye on if you are using 32bit or 64bit and which toolkits you may be using. Some of the NI add-ons are 32bit only. 
Not easily. I have had this issue myself because even after you buy the Vision Development module the AVI toolkit is somewhat limited (and the most recent revision has some bugs). This library is much cheaper but is quite old and I'm not sure what the company's current operational status is: http://www.hytekautomation.com/Products/IVision.html Other than that, as suggested there are open-source libraries written in other languages that you could leverage, although this route is non-trivial if you are beginner. This guy has made LabVIEW wrappers for OpenCV but I am not sure to what extent it covers AVI. Have a look and see: http://download.cnet.com/OpenCV-wrapper-for-LabVIEW/3000-2070_4-75703463.html 
I'd be interested in this. I am a LabVIEW evangelist working in a very large pharmaceutical company. I think I am the only one. When I talk to the engineers that run our manufacturing plants about LabVIEW, I get one of two responses: "never heard of it" or "Oh yeah LabVIEW, I used that once back in college." My perception is that LabVIEW's footprint in the manufacturing space is at best a long, long way short of what NI would like it to be. 
https://imgur.com/0v7SMT3' I switched input to the C-inputs which has negative and positive terminals. I have my function generator set to 1khz and 1Vpk-pk. It is also a ramp function.
Thanks, u/caleyjag. From my own trials, and also those from the support people at NI, it seems like the AVI functions are available without a runtime license. They are present in the Vision Common Resources installer. I'm interested to know that there are bugs in the AVI toolkit in Vision development. Can you say more about that? 
I should look into what is included in the common resources installer. Good to know. Here's my old AVI2 bug request: https://forums.ni.com/t5/LabVIEW/Error-1074395967-Bug-in-IMAQ-AVI2-Read-Frame-vi/m-p/3327986#M975951 I haven't checked the latest version of Vision to see if it's been fixed, since I reverted to the older AVI library for my current projects. 
Use search and replace string: http://zone.ni.com/reference/en-XX/help/371361P-01/glang/search_and_replace_string/ Use \s for the search string, and replace that with an empty string. Connect a true constant to the top inputs to replace multiple occurences and multiple lines :) (You may need to right click on it and choose regular expression).
Thanks. That was really helpful. Other people answered in my question other forums long before me, but for some reason I couldn't figure it out until you sent that link, so thanks.
The default AI access is a polled read, and isn't set up for waveform capture. Look at this example for a better sense of how to capture waveforms: https://forums.ni.com/t5/Example-Program-Drafts/myRIO-Equilizer-How-to-Acquire-and-Process-Audio-Waveforms/ta-p/3513404 You could try running the loop faster. 4VPk-pk means that there is a 4V between the max and min value of the waveform, so you'll see +2V max and -2V min, for a total difference of 4V. Your pictures show readings between +2 and -2 volts, so it does look like you're getting 4VPk-pk :)
Are you using this? http://www.ni.com/tutorial/52864/en/
500kS/s aggregate for all analog inputs on the mux. There are 12, so that bumps it down to 42kHz but that is plenty to measure your signal. Keep in mind that your loop has a 7ms wait time which kind of screws with your sampling - you will get all kinds of aliasing in your samples if you don't measure it at least twice as fast as the dominant signal frequency. As for the peak to peak voltage, your sensor is ac coupled. So even if you use a multimeter to measure your function generator and it is 0V to common and 4V to common, your rio will output -2 to 2. They trend towards summing voltage over time to 0V. 
When I don't have the delay it gets even more distorted. 
I just realized I'm dumb and that's exactly what 4Vpk-pk means lol. 7 years and still making stupid mistakes lol. 
At the sampling speed it should work without this add on lol. 
You should plot the time it takes for the loop to execute. If your loop isn't running at some integer multiple of 1kHz then it will definitely not look like a nice waveform. 1kHz+ signals on the pc and rt are hard to deal with because the base timing options are limited. You could try a timed loop which will allow you to make it iterate reliably at speeds less than 1 ms each. https://en.wikipedia.org/wiki/Aliasing https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem
i'll believe it when i see it. for one, RT and FPGA support is at least one to two years out, so that is the absolute earliest advanced complex systems developers will be able to use NXG. furthermore, advanced users have large portions of code and frameworks in current gen that will not be easily ported to NXG. so advanced users are at a major disadvantage and basically have nothing to look forward to except solving the many new problems that NXG will introduce. i am not looking forward at all to becoming an reluctant expert on NXG AppBuilder and the many quirks that NXG classes will have.
1. Nice to have. Some sort of package management is a requirement in today's world. Almost no software tool chain is without it now-a-days. VIPM has been invaluable to teams thus far and having that tool managed by NI is important when producing work for a customer, so moving VIPM into LabVIEW is nice, even if the underlying code is still just VIPM. 2. Source control management has always been a hack with LabVIEW. We do the best we can, right. Some people have solutions that work better for them, but I'd challenge anyone that whatever they do takes some sort of training to bring new developers up to speed. It would be nice to have a language that just worked with source control. I'll be most interested here. 3. It feels like you're attempting to interrupt my morning coffee. Because loading a large codebase is when I go make my bullet coffee. 4. I'm hesitant to put too much faith into getting excited for this. After struggling through deployments year after year, never really having a solid way to guarantee that I can manage dependencies appropriately I have to say, I've heard it before. Let's call this, believe it when I see it. That being said, yes, yes, yes. Please, make this work and make this easy.
Can they spend a little time optimizing their code? I spend all day working on LV issues where the problem might be in NI-MAX, or I need to install NI-CAN or NI-488. My desk is full of LabVIEW CDs. Each one loads another GB of bloatware. I work with PLC experts that are amazed at how complex the things thing is. 
This is part of the investment described in the blog. The way software is installed and dependencies are detected and resolved is being improved to mitigate users having to keep/find multiple pieces of installation media on hand. 
regarding (1), i have at least tried out the new package manager in NXG and unfortunately, it is nowhere near a proper replacement for the JKI VIPM at the moment. you basically get none of the features in VIPM that allows you to create reusable packages and code. it basically implements copy and paste functionality for a collection of code. i haven't revisited it since i played around with it in NXG 1.0, where i was planning to use it to replace VIPM until i realized there's nothing of substance there.
The development environment are kind of nice (tabs, integrated windows, grid alingment). But the block diagram seems to have been overly softened and flattened. And in all the demos I have seen it seems build to people that have less experiance. And the lack of compatibility seems like a major road block.
Believe it when I see it, absolutely true. For so many years I‚Äôve heard too many promises and revolutionary LabVIEW ideas to blindly believe in the next one.
As others have said, package management is quite immature at the moment. Whenever I have heard NI mentioned package management, the follow up question from others usually is: will NIPM have feature X that VIPM has? Many LabVIEW developers are engineers, and VIPM might be the first package management software they've used and so they naturally only know about that. So for them a package manager in LabVIEW means a way to edit the palette of a package to be installed, scanning for dependencies, and creating offline repositories (VIPCs). NIPM doesn't have any of this yet, but having this 3rd party tool become more 1st party will likely mean better integration with the IDE. And hopefully one day will have even more features than VIPM...someday. Another thing I'd like to touch on is the fact that LabVIEW was first released in 1986, in black and white and only for Mac. Much has changed in 30+ years but a decent amount of the code base, and limitations stem from decisions made early on in development. Did the developers of LabVIEW know they needed to one day support color? Or 4K monitors? Or that web based applications would be so important? Or that object oriented development would be important? Window scaling? Or even project management? My point with this is NXG is an opportunity for NI to learn from the past, but peal back the layers and do some things better, taking some of these features that feel bolted on, and make them an important part of the language. That being said a large migration like this will take time for feature parity. And in a competitive industry where your boss or customer just cares about an end product working, engineers may look elsewhere for tools they feel are more able to help them with their task.
The weird part is that JKIs relationship with NI is well known. Why not just pay JKI a bunch of money and port VIPM to run inside labVIEW, give us a tool that converts VI packages to NI packages, and call it a day.
Well there is some compatibility issues for sure, such as I think that the NIPM is Windows only at the moment (that and RT Linux) while VIPM is Mac and Linux. I think there are likely some compatibility issues with a straight up conversion (like run pre-post install VI). JKI's relationship with NI look quite odd from the outside, and I get the impression they are looking at more at doing contract work rather than for working with NI. Jim once told me the most profit to be made is having yourself positioned as close to the end product as possible. If Ford makes a car, I want to be a supplier for that car. What I don't want to be is a developer selling a software tool, to a developer, of a testing house, contracted to perform testing, for a supplier that produces a piece of a component, that is a small subset of a component, of a car. Again I have no inside information, but it appears that maybe JKI is trying to be closer to that end product, rather than trying to partner with NI to the extent they have in the past.
The lower string control is displayed as an icon, but the upper is not. There is no functional difference between the two. The reason that you get icon string controls is that it's the default behavior of LabVIEW. You can change this in the LabVIEW settings under 'Block Diagram'. You can also change the appearance (icon or not) by right-click on the control and clicking 'View as Icon'. I personally like the more compact version since it looks cleaner in larger programs. 
Just two different ways of displaying the same thing. The larger one is in icon form, for visibility. You can right click on it and deselect "Show as Icon" to have it look like the circled one.
Excellent. Thank you
Thanks for the response!
You can either hide the arrows and number for the display on the front panel or you could run it through another index array block to convert it from an array of strings into a single string. I also think that if you specify both a row and column rather than just a row, it will output a single string although I don't have a seat of LabView in front of me to check.
Those are array indicators. They let you show multiple items per control. The arrow buttons let you choose which element of he array to show. You only care about the first so you really don‚Äôt want an array. The first array on your diagram is a 2D array with all of the rows and columns from the file. You‚Äôre indexing out of that 2D array and selecting the first, second, and third rows of that 2D array. Those rows still have columns so the results are 1D arrays where the first element is what you want (the first column). If you wire the second input below each of your row numbers and give it 0 for each one then you should get just a single string for each output. That is, wire up (0, 0) for the first pair, (1, 0) for the second, and (2, 0) for the third. Then delete and recreate your indicators to get regular string controls. 
Thank you so much for the detailed and clear response!
I didn't know you could hide them! Thanks
Dynamic loading of a VI means it doesn't have to be in memory the entire time your code is running. This is particularly important in large code bases on slow computers. http://zone.ni.com/reference/en-XX/help/371361H-01/lvconcepts/dynamic_loadcall_vis/
Okay, I see. Thanks. Does it call on separate .vi files only when required? Or are all the sub-VI's placed in the main VI file?
All vi's live separately on disk. The only time they are combined into a larger file is creating an llb (an old technology you should not use anymore) or compiling them to exe/dll/lvlibp. If you are working in the development environment, generate a path to the vi and load it with the open vi reference node
Are you saying you have lines like ‚Äúvariable=4503.0‚Äù and you want to get the number? If so, you could just use a split function. Provide the character ‚Äò=‚Äò as the function input and you will be given an array of strings split across that character. If your numbers are always last, for example, then the number will be the last element in the array. Easy enough but not sure it‚Äôs easier than regex. 
In addition to what /u/havearedpill said, you can also call VIs and forget, meaning, you can get one running and then not have to wait around for it to return, like you would for a normal subVI. This can be useful if you want to create background processes that your main application communicates with. I use that approach for communication protocols sometimes. Sometimes I will create a background process to actually handle responses from the instrument - it can format good responses, filter bad responses, etc, and then send them to your application via user event, or notifier, etc. If I were to launch that VI as a regular subVI, one single application would be dependent on it, and vice versa. If I call and forget, many VIs can use it.
Historically, the sample tests are a mixture of retired test questions and actual current test questions. So, the sample tests are really a great thing to study. Source: used to work on the CLAD.
https://imgur.com/NyA1m9g I'm now using the throughput with a .5 ms delay, approximatley half the 1kHz signal im pushing in, and it just seems like random mess.
An example off the top of my head: I had a client with a reasonably complex control system and an equally complex UI. Their UI also had buttons for launching dialogs. At any given time they could have 8-10 floating windows in addition to the main UI. Now, this was back in my beginner LabVIEW days. I was prone to releasing buggy code (no unit testing, no integration testing). Some of these bugs were process breaking and could seriously inhibit their ability to complete testing. Most of the bugs existed in the floating windows they used. On top of that, they might have their UUT under high pressures and high loads that make backing out of the test a serious inconvenience. My solution was to launch the floating windows dynamically by opening a VI from file. Thus, when the user hits the "launch dialog" button, it dynamically calls the VI. When they close the dialog the VI stops and memory cleaned up. The result, while running tests, if they found a bug in a floating window, all I had to do was close the window, overwrite the VI on disk, and run the window again. Not test stoppage. Another term for this process might be a "plug-in architecture".
Assuming the format is variable=value You can use the "Match Regular Expression" VI with regex grouping to get a direct output. Here's a VI snippet example: https://i.imgur.com/AEajIYr.png
The wait that you use coerces the 0.5 floating point number to a 1 integer. So you are forcing it to wait 1 ms.
makes total sense. thank you very much. I guess what I am struggling with is how to make a VI like a function in C. If I have a VI saved on disk, that I'd like to get input from a file path search box in the main VI, then call the VI dynamically and return results to the main VI front panel menu...how do I do it? Only luck I had is with making the VI a Sub VI, then allocating terminals (which doesn't work most of the time, they are all black and don't stay), then making a little icon for it, but this is not calling dynamically. This is a sub VI, right? Got a link to a tutorial or vid showing how to read an input from a main VI, run it through another VI, then display results on main VI? (like pass/fail?)
ah, okay. do you have a link to a tutorial or vid showing how to read an input from a main VI, run it through another VI called dynamically, then display results on main VI? (like pass/fail?)
I understand. Do you have a link to a tutorial or vid showing how to read an input from a main VI, run it through another VI called dynamically, then display results on main VI? (like pass/fail?)
Another reason to load VI's dynamically is this. Say you want to make an application that can use one of two different but equivalent devices, each with its own drivers. Now Let's say you have device A installed, but not device B. Nor have you loaded the drivers for B. If you try to run your application without the drivers, and the VI that controls device B is called in the normal way, your application will break, because when the application first loads, the driver DLL is not available and LV throws an error. This happens even though you never actually try to use that VI. Instead, when the application runs, you can check first to see if the drivers for B are installed on the computer, and only then dynamically call the VI that addresses that hardware. Conversely you can address device B and not A when A is not installed. That said, dynamic calling can be a little painful, especially if you are creating a built application. 
Can you give an example of a frequency rate with a delay that should work?
You just need to use a different loop structure. The ms wait primitive will allow you to control it down to 1 ms (1 kHz). If you want to sample faster, use [the timed loop structure](http://zone.ni.com/reference/en-XX/help/371361H-01/glang/timed_loop/).
No video that I know of, but what you're asking is a question on interprocess communication. There are several modes for this: queues, notifiers, user events, etc. I can try to give you an example if that would be helpful.
This shows how to do it: http://www.ni.com/tutorial/3929/en/ You want to open a dynamic vi with a type specifier (a thing which defines the connector pane you expect the vi on disk to have), then call by reference and you will see the connector pane in the call be reference node and can wire into it and from it.
HEY HYE, getting much closer. any ideas how to wack out them kinks? https://imgur.com/z3tr4Ud
Switch your timing source to MHz and make the dt 250. Then you will sample at 4 kHz which is enough to at least see the structure of your waveform. Feel free to play with that number to adjust your sample rate. Don't be surprised if this nukes your processor usage (don't go below 50 dt). The feedback node outside of the loop makes no sense and you're using it to feed voltage data to a clock-tick counter (???).
Thank you!
thank you for the help. Now, how would i chsngr the dt rate if i was taking a constantly changing signal for output reading? 
That is heavily dependent on the actual signal that you are trying to acquire.
There are specific groups for DQMH support. Try https://forums.ni.com/t5/Delacor-Toolkits-Discussions/bd-p/7120
Yeah you shouldn't need to display the events cluster anywhere. It is private - for a reason. Part of OO Design is the ability to encapsulate data and restrict access to it. What you're seeing here is any example of that. Instead, wife the events cluster straight into a register for events node. Also, pro-tip: instead of having multiple register for events nodes and daisy chaining them together - just use one. You can expand it to include more inputs. This way you can register/unregistered during the application run if you want. It also saves diagram space. I've used DQMH on several projects and I actually worked with the developers of it on their projects using a forerunner of it. It's quality stuff but it does take a minute to wrap your head around it. Feel free to ask me questions. 
Thank you for the reply. I kind of figured that's what the problem was, thanks for confirming. Also I keep forgetting you can expand those nodes!
Not sure what you want to achieve. There is For Each in TS that can be used in these cases. Or can you please write a pseudo code snippet how you would do your task in C for example?
DelayA [ ] DelayB [ ] Finding a gap in delayA value, compare last element of delayB plus last value of A For ( i=0, i&lt;element in A, i++) If( DelayA[I] ¬ª Delay[i-1] + Delay[ last element ] ) Verdict(pass) Else Verdict(fail) 
OK, I think I get it. In TestStand you can use the array[firstElement..lastElement] expression to work on an array subset. More info here https://forums.ni.com/t5/NI-TestStand/Numeric-Array-Subset-in-TestStand/m-p/2946334#M46304 Hope this helps
Thanks, I got it.
Can you post what you have so far? This sub is generally very helpful, but it's also very against doing homework for people. If you have a specific problem and can show you actually tried, we would be happy to help.
Property nodes can be used to control indicator color and blinking. An LED indicator just needs to be fed a true or false, which could also be used to create blinking.
I wouldn't even bother with the true or false. Use property nodes to change both the true and false colors. There is a "blinking" property of many controls that you can set to true when you need it to blink. It may actually default to blinking red.
No, I don't want to do your homework. 
It specifically says not to use the blinking property.
Without seeing the coffee, I can only speculate, but with PWM you want to use a hardware based counter. If somehow you got the PWM running in software, you will see a lot of jitter due to the non deterministic hair off Windows. Can you post some code?
You might want to look at DAQ Output buffer regeneration. Preload a particular signal into the DO buffer with regeneration enabled, then load new buffers in to update the pulse width.
1.) Don't allow the chart to update after the array finishes. You could accomplish this by using a case structure that only allows the chart to update when you have valid data available. However you also need to track when you have valid data. 2.) Slope and offset are properties of the chart I believe. So if you want to remove them, you have to 0 them out in the appropriate property node. 
Plotting zeros: it is more like a programming question than a charts related one. If you are doing offline processing then use a graph on your array not a chart. It will automatically solve your problem. If it is online processing then as the previous comment mentioned do not put any data into your chart. This can be done for example by placing the terminal outside your chart into a case structure. Answering the second question, detrending is a general signal processing question and thus the implementation in LabVIEW depends on how do you want to tackle this task. A basic detrending solution is calculating the dc part of your signal for defined segments and subtract it from the original signal. Also highpass filtering can achieve similar results, or statistics can be used as well (regression for example). It really depends on the information you already have about your signal. So find out what suits you the best and post that algorithm as a specific question so people can help you how to implement it. Btw processing of ECGs is a very well documented topic (trend removals, 13Hz muscle movement frequency removal, noise filtering,etc) so just pick a few options.
You can have both installed! Just install LabVIEW 2016, then the Lego Mindstorms 2016 toolkit after :)
I can't install Labview 16 because I only got one free copy from my University
I'd still try and install 16, you usually can use serial numbers to activate older versions as well. You'll be able to use it in evaluation mode for some time even if you do run into activation issues.
This is true, but you really should install them in order from oldest to newest. Otherwise, you can cause some weird things to happen with shared resources.
Can you make X a constant of 0 or 1?
What is your goal? Do you want something to continuously update indefinitely? In that case, just wire the scalar output to a Waveform Chart. Do you want to generate a finite data set and plot it once? Use a For loop, wire the output out of the loop, make sure it is set to "indexing" so you build an array, and plot it with a Waveform Graph.
The first thing you said is my goal. THanks i'll try that EDIT: It worked perfectly thanks for the help :)
Glad to hear it! 
You can [download](http://www.ni.com/download/labview-development-system-2016/6046/en/) LabVIEW 2016 from National Instruments
Do you have DAQmx installed?
^^^ Go online and install the latest daqmx drivers if you haven't. 
That was it. Awesome! Thank you so much!
Yup did just that. Thanks for your help!
Multiply ‚Äúi‚Äù with the time step and add it to the base timestamp in each iteration. Wire the result of this to the for loop and use indexing in the tunnel. Vois la, there is the timestamp array
So simple but you have saved me much frustration. Thank you!
Can you post an image of your code?
Hi, thanks for replying. Here is my code so far: https://imgur.com/a/ckLwT I'm a beginner at Labview and MyRIO so understand there are probably clear mistakes on what i have here. The problem comes with the addition of the MyRIO 'audio in', as i have this code working just as a simulation in the base LabVIEW. My end goal would be to introduce a simple loopback (Hence why the analog out and in are there). Any help is appreciated Thanks
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/MAQQAAW.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dpdg5uf) 
Looks like the default .net runtime icon. https://www.google.com/imgres?imgurl=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Faspnet%2Fweb-forms%2Foverview%2Fdeployment%2Fvisual-studio-web-deployment%2Fdeploying-to-iis%2F_static%2Fimage2.png&amp;imgrefurl=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Faspnet%2Fweb-forms%2Foverview%2Fdeployment%2Fvisual-studio-web-deployment%2Fdeploying-to-iis&amp;docid=Lh12kNhi2dlYNM&amp;tbnid=-Mu9ciI2nPb6eM%3A&amp;vet=10ahUKEwiI9ZL1y6fXAhUS3GMKHXarBdgQMwhRKAMwAw..i&amp;w=649&amp;h=525&amp;client=ms-android-google&amp;bih=332&amp;biw=684&amp;q=.net%20default%20application%20icon&amp;ved=0ahUKEwiI9ZL1y6fXAhUS3GMKHXarBdgQMwhRKAMwAw&amp;iact=mrc&amp;uact=8
Thank you for that, I believe your correct, I have been sent a slightly better picture and it looks like the application is definitely not Labview. I guess it must have be a text based language that I'm not familiar with. https://imgur.com/myJoWbv 
Excellent picture
If you work a lot in LabVIEW you‚Äôre gonna develop a special relationship with for loops :) 
Writing from my phone so can‚Äôt really figure out what‚Äôs happening in your picture (resolution) but it seems you have an issue because of using dynamic data type (DDT) vs double arrays. Check this link regarding the dynamic data http://zone.ni.com/reference/en-XX/help/371361L-01/lvconcepts/dynamic_data_type/ 
You say ‚Äúequal a word to 00‚Äù. Is your array a string array or numeric? Please share a screenshot of your code to help us better understand. If you‚Äôre outputting an empty array, you can check if the array is empty using an operator. If you‚Äôre outputting zeroes, you can instead output NaN values.
When I say "word" I mean 2 bytes. Sorry for not clarifying. There is a word in the array that I am checking for that will always be "0" if it passes, the other words that output in the array could be anything. So I wouldn't want an output of NaN for "0" if that is what I am looking for. If this makes sense. I can try and make an imgur account real quick and post what is going on. 
Yeah, I think you‚Äôll need to include a snippet of your code. If you‚Äôre outputting a LabVIEW array, you should handle the good zeroes and bad zeroes at the array generation level. If you have no control over that generation then there‚Äôs not much you can do. What I don‚Äôt understand is where the ‚Äúempty‚Äù array part comes in, so maybe the snippet will help.
Check for "not empty array and equal to 00"
I'm assuming this; There is an array of five elements. OP is inspecting the third element and want's it to be NULL, not realizing that every element of an array in LabVIEW is initialized to a value. If that's the case, there is no solution to the problem.
Why the flat sequence structure with the express VIs in it? The VI is broken because the spectral measurements is expecting a waveform data type (special cluster of Y, t0, and dt), but you are feeding it an array of numbers. Express VIs are special bundles that open and close driver references in a loop. so running them in a loop is not necessary because they are essentially special loops How are you reading the data into the MyRIO?
If Lockheed was involved then there is a good chance it was LabVIEW. I worked on a different program that used LabVIEW quite a bit.
We are using MySQL. Now, the thing is that in any case you will need to invest in setting up the backend (designing schemas and procedures, configuring backups, setting up replication if you need it) if you want the database serves you right. Database is not a files system. Unlike with the files where you application is in charge of managing everything , in case of database the right approach will be when all database related logic is located in database. Database can do it much better than the LabVIEW. 
You could just average the signals before you plot them using the basic math functions. Could you post a snippit of your code so we could see what that looks like and make recommendations on where to put the edits?
This right here. We can teach you to fish - we're not here to give you the fish. 
I worked within the Navy as a contractor and they had several applications that were LabVIEW based as well.
There's a package in the JKI package manager with lots of sql functionality. It also includes templates and example code for correctly writing to and reading from SQL databases. I don't remember the name but I'm sure it'll pop up if you search for "sql" in the package manager. If this is the first time you've heard of the package manager, then welcome to a world of plenty :-)
I've used PostGres through Heroku before. Didn't have to do much to set it up. I like PostGres myself because an SDietrich wrote a LabVIEW wrapper around the API so I don't have to work with ODBC or ADO. Wrapper here: https://lavag.org/files/file/240-libpq/ I don't know enough to tell you the difference between PostGres and MYSQL but SQLite is a local database only. It probably won't meet your needs. 
Seconded. Using PostgreSQL as well...Satisfied with performance and usability... 
In the Mathematics palette, under Prob &amp; Stat use the Mean function.
Build a 2D array using the 1D graph arrays, transpose, then feed the signal into a for loop. Inside the for loop, take the average (mean) of each indexed array then wire out of the loop with indexing enabled. The resultant 1D array will be the averaged graph.
Thank you for your response. I added a snippit of my code and tried to explain it as good as i can. 
I already tried that, but I get a single graph that doesnt match the average graph, but is something completely different...
I think their model is based around using NI representatives to get you what you need. So they put more money in to getting their reps what they need than making the site better?
I haven't been able to find much either. It's not any of these accessories perchance? http://sine.ni.com/nips/cds/view/p/lang/en/nid/208992 http://sine.ni.com/nips/cds/view/p/lang/en/nid/208993
It really is awful. I use google to find what I need on their site. Hell, I can't even find the NI Instructors Community anymore. I had to google it. And then, I can't find my list of communities that I belong to anymore so I had to bookmark all of them. Have you tried to build an order using their product build tools? The latency is so horrible. Want to change quantity? That'll be 10 seconds to update. Want to change accessories for a module? Yea, that'll be 20 seconds. I cringe when clients go to NIs website to look up information on the products I've picked out. They always come back complaining about the website. Another thing is how inconsistent everything is. There's like 20 different UI themes across all of the services and they all use different terminology. 
It's annoying, but just call up inside sales...
the recent update, within the oast year or so, was 100% a step backwards. i struggle to even find a zoomed picture for the exact product i want, manuals for the product i am directly looking at (most links only give some subset of the available manuals), and the new table format is terrible to use. it hardly supports widescreens as well. it is mindblowing that it made it through QA as everyone i work with has commented in it and we legitimately struggle to find what we're looking for.
For very basic database stuff I've been using SQLite with this toolkit: https://lavag.org/files/file/212-sqlite-library/ It's simple, pretty portable, and there are a few free database viewers that can view and modify the data.
This ^. Their website used to be fantastic and now it's just a tremendous pain to browse for product specs or find new hardware. 
Agreed. It's really simple to list all their products on a list. Heck, categorize the list instead of Clicking software, then few more things in which it'll just lead to either a package or same thing in different categories. I've been using LabVIEW for several years, the other week I went browsing through their site and saw some details on Diadem. I'm pretty sure I've seen the name here and there using LabVIEW but really never looking into detail of it. Probably would have bought a few licenses a few years ago if I did. Then all the hardware seems to focus on cRIO and PXI, which is great and all but I defiantly could see some use on the smaller USB stuff. 
Seems like every update get's worse. The table is the worst ever, yes, but I bet you good money that whatever they do next is even worse.
Their "wizards" are pretty helpful (e.g. PXI/RIO) if I'm putting together something from scratch, though the interface is slow af... or perhaps it's just my connection.
because nobody respects labview and nobody respects labview developement or developers.
The best part is that it is almost impossible to find what you need unless you already know what you need. For example you need to hook up a legacy SCXI chassis to a M series PCI card. What cable should you use? Well guess who doesn't know, the fucking website. It would be great if there was a "what are you connecting to" wizard that gave you all of the options. I had lunch with my local rep and he says he has to apologize to everyone about how bad the website is. He suggested using Google to find what you need instead of the website. They need to make the website like McMaster Carr's.
[This](https://i.imgur.com/IFaffnB.png) should work.
Thanks alot! That's exactly what I was looking for.
It's not your connection. The assembly wizards are crazy slow. Also, the compare button doesn't work, in Chrome at least. So if you want to compare two relay cards you have to dig into the other part of the site, wasting even more time and probably not finding the information you were after anyway.
Funny, my rep says the same thing. There's obviously no feedback path from everyone in the entire world to whoever designed the website.
Nope, I got the part number of the filters from the last customer I used them for and just called my rep for a quote.
It's never been great, whether searching for products or building a system, but it used to be tolerable. Now it's just bloody awful. I'm really glad that I'm not working as a systems integrator anymore so I don't have to use the site that much. I'm really not pleased with the direction the company seems to be taking overall since Dr. T retired. LabVIEW NXG? OMFG, don't even get me started.
I love how this post has more comments than most of the posts in this subreddit.
We all love LabVIEW, we wouldn't be here if we didn't. But that website really, really, really, really sucks.
I was at the bar with an ex-coworker last night. After a few drinks, unprompted he said "So have you gone to NI's site lately?" Then went into a list of complaints that mirror what we've talked about here. He said in the end he just had to call up NI's sales person and talk about what they wanted since he couldn't get the information from the site he wanted. NI has been made aware of this by complaints, and a few things have improved since February, but it is still pretty terrible.
MySQL seems to have a good active support community and so is probably the friendlier to start with. Similar with MS SQL Server but that costs $$$ I've used postgres as well though and found it to be more powerful than MySQL so personally I would go to that.
btw i have a e3f-ds30c4 IR sensor 
What exactly are you trying to do/modify in the project?
Build a vi with a case structure with the little question mark wired to an outside Boolean or voltage input trigger. The case should be output to motor on in true and nothing but pass all signals through if false. The surround the whole thing by a while loop and add a stop button to it. Don't forget to add a small delay like wait for 2ms. PM me if you'd like a file to start with. I can provide something basic.
Well its a mini project , im going to do a prototype of a lift programmed by labview , if you put an object into the lift an IR sensor sends a signal to the arduino card Which start a DC motor that lifts the object 
Ill try to follow your instructions
Hook up the motor and IR sensor circuits to the arduino. Download the Arduino for Labview package for free. Follow the guide to download the sample code to the arduino using the IDE. Then in LabVIEW initialize the arduino with the correct COM port, and put a case structure inside of a while loop. Wire the case structure with the output of the Read arduino pin vi of the pin the IR is sensed with on the arduino. If the IR pin is sensed, then inside the case structure enable the pins controllng the motor, there will probably be a vi that does this for you. This will be a good starting point. 
i got the hooking and lifa part done , i even got a VI for the dc motor done with linx the only part that's bothering me is the IR sensor part there is nothing to guide me , so i'll try to follow your instructions , can i do this with linx or is it done strictly with lifa , and you please explain a bit more what do i do exactly to enable the sensor 
Not sure I have only used Lifa. Just read in the value of the IR pin and compare to previous value. If different and on, then object detected so enable motor. 
If its not too much ill work up a VI and maybe you can see if my work is right
Haha sure I'll take a look
There was at NI Week. And believe me, they were hammered in it. 
So you need to have your program remember when the light is on so it will stay on. How about a shift register? Have a shift register with a false value (light off) and when the value goes high the light goes on (true). Put that true into your shift register using a case structure or something. Now there will be one more thing you will want. A way to reset the light so it turns off when someone acknowledges it. 
While loop, with a wait and DAQ read, set the wait to be timeout/max iterations, stop the loop if the conditions are met or the max iterations are met.
With this though, if the condition is met before the timeout, it will have to wait until the timeout occurs to check right? I was hoping more for a if the daq is met move on or fail if timeout occurs. 
Place a Get Date Time in Seconds inside and outside the loop. Subtract them inside, the result will be seconds (double) and check if that is greater than the timeout value. Using an OR connect the output of the comparison VI to the Stop Condition. The input of the or should be the DAQ condition check.
Do you know if Berkeley provides the source code or tables/formulas somewhere? If so you could include it as a dll in your project. Otherwise the next best thing would be a web API. Basically accessing the tool programmatically will be a lot easier if Berkeley has built some path for developers to do that vs just using the web GUI. 
thank you for suggestions ;) i have to check them out one by one as i am very new to such these web terminologies
Neat, you should do that. Let us know how it turns out.
Install the ELVISmx driver and you will have a full Bode analyzer Express VI
What‚Äôs your setup like? What kind of I/O etc?
Right now I just have a tank that is controlled by a dial and a greater than or equal to statement for a constant that I can set for the desired set point. 
Is this a real live tank or the tank widget in LabVIEW? What's your code look like now?
Right click on the Icon and check/uncheck View As Icon
Thank you so much. Can I also ask why when I go to make a subVi of a vi with a case structure for T/F it says that it contains and indicator terminal in a case structure and may stop working if I do this. Is there a way I can still do it without it causing the vi to not work anymore. I really appreciate the help. 
An indicator is normally local to a specific VI. So when you try to create a subVI that contains the indicator, one of two things normally happen; the most common is for the value to be passed out as an indicator from the subVI to the indicator in the higher level VI. However, if an indicator is in a "sequenced" object, such as being inside a case or sequence structure, LabVIEW can not simply pass out the value as an indicator without breaking data flow. Thus is this sort of case, it is expected to pass in a control reference and use a property terminal to write to the Value property. 
Thank you so much!!!!
Unfortunately, LabVIEW training materials, like everything else LabVIEW, seem to only be available for a very expensive price. You should be able to find lots of examples under the Help menu which might help you get started.
wow so that's why; looks like A&amp;M is cutting back on their engineering budget
Also, try their forums. You can ask questions and the community is very helpful in answering them. They could point you in the right direction.
NI does have some really basic training online: http://www.ni.com/getting-started/labview-basics/data-structures Googling briefly also turned up this which seems like it might be helpful: http://home.hit.no/~hansha/documents/labview/training/Introduction%20to%20LabVIEW/Introduction%20to%20LabVIEW.pdf It is unfortunate that there is so little available for it because most people end up learning it without learning it properly and it gets out of hand really quickly.
Look up sixclear. His videos are very helpful for learning labview 
[Here are the general training link](https://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495) shared around NI's forums, in the Looking for Free Training section. Notice that there is [self paced training](http://www.ni.com/academic/students/learn/) for students.
They have extremely in depth online training that is very likely available with your site license. It is worth checking. 
Yup looks like its either learn through trial and error or dont learn it all 
Ill check his vids out, and im hoping these booklets are free lol
A&amp;m gave us student editions of labview; how may we access this online training? 
do your school computers have the full version of labview? the computers in the labs
Those are for your personal computer. A&amp;M has a full site license for school computers. Go to your major's computer lab and you'll likely have what you need. 
It's all free haha. It's how I learned labview. 
VI Shots has some tutorial videos for LabVIEW, even going into some advanced concepts too. You can find them [here].(http://vishots.com/category/labview-tutorials/)
I am attending an a&amp;m engineering academy, so we don't have access to college station's campus equipment.
Im at an a&amp;m engineering academy, so we don't have access to college station's campus equipment here.
thats okay. if the school has a site license, you probably have access to the good, detailed educational materials. you just have to figure out who knows, and get in touch with them.
Jim Kring's book is very good although maybe showing its age a bit. I am sure most of it is still pretty good for beginners though: https://www.amazon.com/LabVIEW-Everyone-Graphical-Programming-Made/dp/0131856723/ref=sr_1_1?ie=UTF8&amp;qid=1512157060&amp;sr=8-1&amp;keywords=labview+for+everyone 
You can get a 45 day eval online: http://www.ni.com/en-us/shop/labview/download.html No need to use 2011 :)
Oh I know! I was doing some office cleaning and found these extra giveaways that I had from a class several years ago. In case anyone out there is an aspiring LabVIEW historian.....
I'll take one. For funzies. 
Sure. I'll gladly have one.
I'm in...
Is it cheaper to update if I have 2011 already? I'm not sure it would be useful to me otherwise.
How about this collection? https://imgur.com/VaTbUgl
FWIW you can get a LabVIEW 2014 license for personal use for like 50 bucks.
This is my company's biggest gripe with NI right now. Core 1 and 2 training should be openly available for free so that more people learn proper LabVIEW programming. NI wants to take over the world with their products yet they put up these impossible barriers too entry, making it so the only people who end up learning how to use their products are the people who have no choice.
Can I get one?
I would take that with a smile
I would get one.
Yes, please
sorry for being a bit late was busy with exams , now i'm back on the project and had a problem with my vi i did was you said and used linx i tried to do a distance vi to demonstrate how the sensor works yet it doesn't show the distance but the speed of the object's movement i have a screen of the vi but i don't really now how to post picture in the comments 
Man 2011 was the first version I ever used. Great memories. Probably my second favorite version next to 2013.
The joys of working as an alliance partner. The annoying part is storing all of these versions JUST in case some project comes along that requires LabVIEW 6 or something ancient.
And then when you do get a project written in LabVIEW 6 that needs updating it turns out to be so poorly written that you might as well start from scratch. 
Good question. Try it and see. 
No.
Sampling rate is how fast your device takes samples, number of samples is the size of the chunks of the data being read. For example sampling rate 1kHz and number of samples 10000 means that you take 1000 samples in each second and your program will only read those if 10000 samples are available so your graph will update once in every 10 seconds (that‚Äôs how long to gather 10000 samples @1kHz) What you need is to set the acquisition to Continous mode and select a number of samples that is lower than the acquisition rate and of course put your Reading code (excluding Init and Clear Up) in a loop. For example number of samples = 1000, sampling rate + 10kHz will result your graph updating every 0.1 second by 1000 samples. And if you run your loop N times then you will acquire N*1000 samples in total.
Works exactly like in C
The outer loop will finish its last iteration, hence the inner loop will continue until its stop condition is met.
Can you share some code? Maybe a screenshot? It would be much easier to help you if we could see what you've tried.
Nope. The inner loop needs to complete in order for an outer loop iteration to complete. So the outer loop depends on the inner loop completing. 
To kick off with a few: Drone manufacturing - https://youtu.be/JlzruRmmnWI @6:51 Railgun - https://youtu.be/49H2_Gyws5o @1:05 
Not LabVIEW specifically, but I was pretty impressed that [Chappie's engineer's code AI in DasyLAB](https://imgur.com/VWFJck3).
I think I have seen it used on Mythbusters a couple of times.
YES PLEASE! If you still have one I'd gladly take one!
I don't have a link, but a year or two ago I went to an imax at the museum that was a mini-documentary on james cameron's deep ocean dive. In one scene showing them testing the weight release mechanism, I could clearly see the crappy modern front panel controls and ugly grey background indicative of a labview app.
oh i see 
What is an Arduino VI? What are you trying to accomplish? What tomorrow have you tried? Downloading a VI too an Arduino to be ran without needing a PC is only partially possible with a 3rd party toolkit which isn't free. Running a VI in a PC that talks to an Arduino asking it to take readings or perform actions can be done several ways but LIFA or LINX is free from NI.
There is a thread over on the [NI forums](https://forums.ni.com/t5/BreakPoint/Incongruous-sightings-of-LabVIEW-NI/td-p/2644515) where people discuss seeing LabVIEW on TV. Most of the time I'd say I saw it on Mythbusters, and even had NI come in to help on a couple setups. The last one I saw was on an episode of [The Big Bang Theory](https://forums.ni.com/t5/BreakPoint/Incongruous-sightings-of-LabVIEW-NI/m-p/3690910#M29641).
Is that DasyLAB? I saw that in the movie but assumed it was something like inspired by LabVIEW but not real. [Here is a thread](https://lavag.org/topic/18909-labview-cameo-in-scifi-movie-chappie/) on LAVA, and [one on NI](https://forums.ni.com/t5/BreakPoint/Incongruous-sightings-of-LabVIEW-NI/m-p/3147185#M26952) discussing it.
[SpaceX](https://www.quora.com/What-software-programs-are-used-at-SpaceX)
After seeing those much clearer than mine images I'm gonna have to agree with you on this one, it's LV, but weird. Probably just a shopped static image.
The Arduino VI allows Labview to connect to an Arduino. I've tried using VI Package Manager, but have had no luck. To answer your second question, this is what I'm trying to do: http://www.instructables.com/id/Temperature-Control-using-Arduino-and-LabVIEW/
Using labview with Arduino was difficult when it was supported years ago. So difficult that they quickly stopped supporting it. You'd have an easier time teaching yourself how to do it in the Arduino IDE. If you're the persistent type, be sure to come back and let us know how it goes.
You may need to have a 2014 version of labview, probably the $50 personal edition will work.
[removed]
It will do that automatically
Off of the top of my head try setting the channel count higher on your daq. Else try a bundle or build array node before the "write to measurement file"
wow I am impressed! I can get XCode for 0 bucks, Visual Studio Community 2017 for ZERO bucks (and doing the same thing you do in labview, because I have .NET driver to hardware and DAQ). Are you still using this crap software?
PM me how to get it to you and I'll send one your way. Enjoy!
PM me how to send it to you and it's yours.
If you have 2011 already this is totally useless to you.
Pretty solid game there...my collection is close but you might have me edged out.
PM me how to get it to you and it's yours.
PM me how to get one to you and it's yours.
Send me a PM with how to send it to you and it's yours.
Send me a PM with how to send it to you and it's yours.
I use LabVIEW with Arduino's all the time. But I use the Arduino IDE to develop on the Arduino, and then communicate with the Arduino through the USB port. You can talk through the virtual COM port using VISA, or address the FTDI dll (D2xx) drivers. The Arduino then becomes a cheap, smart, general purpose interface board with A/D, D/A, and digital IO. 
LabVIEW is widely used in industry for a variety of reasons. Among other reasons, creating efficient code that takes advantage of many parallel threads using a producer-consumer architecture for data flow is an infinitely simpler task in LabVIEW than it is in text-based languages. Following from that, for those us us that develop exclusively in LabVIEW at work, it is nice to have the option to work on our personal projects in the same environment for only $50.
You need to build your signals into a 2d array
Yeah that tutorial is using LIFA, one of the toolkits I mentioned earlier which gets installed through VIPM. If you are having troubles with that you should try to resolve that first. Is VIPM not able to connect? What doesn't work in VIPM? [Here is a video](https://www.youtube.com/watch?v=ZeHd7DvbMCk) showing setting up LINX, and communicating to one of the supported microcontrollers. Many of the [MakerHub videos](https://www.youtube.com/channel/UC2JvDufj1vB6GewXYMfpe-Q/videos) are useful and you may want to watch some.
&gt; So difficult that they quickly stopped supporting it. I think this is a little unfair. NI developed LIFA for performing basic functions in LabVIEW by asking the Arduino to do stuff for it and return a reply. They left the firmware open so you could add your own functionality, but it seemed most people didn't want this and just wanted NI to provide a more complete solution. Soon after NI realized this was quite limited, and if a better design has been choose other hardware could be supported. That's where LINX came in supporting all kinds of various hardware and even can deploy VIs directly to a Raspberry Pi or Beagle Bone Black. Their firmware wizard also makes setting up pretty easy. Since 2014 it hasn't seen many improvements, but the last update to LINX was in 2016, so I'd say they are still supporting it.
That's interesting, could you link me to some resources so I can understand this more fully? I've only just started learning labview with basic classes, I don't think I even know what linx is yet.
I have nothing at the moment, however I would probably try and get the latest version of the software I could afford. If having a copy doesnt help with upgrading, its probably better used on someone else.
Oh of course there's lots to learn. For LabVIEW beginners there is the links at the bottom of [this page](https://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495) particularly the online training if you are a student or have an SSP (1 year comes with the purchase of LabVIEW). It is self paced and covers Basics 1 and 2 which is more or less a week to two weeks course. Also the MyRIO essentials is a great resource for electronics and interfacing with IO but is geared toward the MyRIO hardware. Once you have the basics checkout LINX, [here is an intro video](https://www.youtube.com/watch?v=ZeHd7DvbMCk) showing how to setup a microcontroller. It comes with lots of basic examples too. If you like that video checkout any other [MakerHub videos](https://www.youtube.com/channel/UC2JvDufj1vB6GewXYMfpe-Q/videos). The LIFA I mentioned earlier can still be found online but is less mature and involves opening the Arduino source, and downloading it using the Arduino IDE. At that point the functions on the palette (after installing LIFA through VI Package Manager) will function. Again this isn't really supported any more and LINX is intended to replace it.
If you're acquiring the signals from the same DAQ, both signals should be in there same wire, it should take care of it automatically. If they are separate, use the merge signals (maybe combine signals?) vi. 
This actually worked. I didn't realize that I could simply add a channel to the DAQ and read everything through the same VI. Thanks a lot for your answers everyone :)
yes, and you still don't have a clean simple framework (like Cocoa), so that 99% of labview code is just TRASH code. The fact that you can spawn thread with a few clicks (while loop + event handler), doesn't imply that your "producer-consumer" is a good thing. You are talking to a person that uses LV since 6.1, so I know more that all of 90% of labview programmers COMBINED. Labview NGX still suck, they used 4 years of R&amp;D for a flying fuck, and came up with a ridiculous 1.0.
This user has been banned. However, I'm going to leave these comments here. Despite the unnecessary name-calling and insults, there's some potential for discussion regarding /u/TylerFucking1's criticisms of LabVIEW, NI, and LabVIEW developers.
If you right click on the string input on the front panel, you can change the display to hex representation. At the end of the day, ascii characters are still just bits, no different than hex. Just a different way to look at 1's and 0's
Are there any errors? Wire some error indicators and see if the errors give you any indication. Also, is the device capable of 100,000 Hz?
it's done i just extracted the formula
can someone please tell me why the hell labview 2017 spring student version is over 100GB?? what the hell happened?
Check "Using Loops to Build Arrays" from http://zone.ni.com/reference/en-XX/help/371361G-01/lvconcepts/for_loop_and_while_loop_structures/ You'll obviously need to ID the even numbers. Adding the elements of an array is trivial with a function you can find in the numeric palette. I'll give you a few guesses as to what it is named :)
You shouldn't need to id the even numbers, if you incremented the loop index and doubled it, you'd have the i'th even number. 
Some things to notice about for loops: 1) If you wire a variable inside the for loop to outside the for loop, it will either auto-index or you have the option to set the node to indexing. This causes it to build an array with size = # of for loop iterations. 2) The top left "N" node, can be set (though not required) to the number of iterations you want the for loop to run. 3) In the bottom left, inside the for loop, there is a little "i" constant. This is the iteration index. It will output the current iteration count. This starts at 0 for your first iteration. These information should be enough for you to get your homework done.
Why wouldn't you just create a formula that calculates this sum without going into loops etc. The formula for sum is (x0 + xn)*n/2. First element is 2, last element is your N*2. Amount of elements=N For N=2 you will have (2 + 2*2)*2/2 = 6 For N=3 you will have (2 + 3*2)*3/2 = 12 Problem? ;)
Nice! I wonder if the professor required the use of a for loop though? Seems like teaching loops in LabVIEW would be the goal of the homework instead of "can you come up with an equation for this?".
Yea, since its a LabVIEW class it sounds like its more about learning about loops and shift registers.
This is pretty neat! Our prof recommended we use loops but there's not too many hard and fast rules, I'll bring up this method in class as having a standard formula is a pretty simple transfer function to troubleshoot. I ended up using shift registers to track previous values and add them into an array, then sum all elements of the array for the final output. You guys are awesome, thank you so much!!
&gt; transfer function Oh god we're all EE here aren't we...
Please never trust me with anything connected to a power supply :). Second Last semester, ready to utilize absolutely everything I've learned as I'm sure it's all relevant in the industry!
I would just use a formula, but if you must use loops a shift register will perform similarly to the global variable you mentioned in java. Just be sure to initialize it to zero before the loop.
google "labview shift registers". http://zone.ni.com/reference/en-XX/help/371361G-01/lvconcepts/shift_registers_feedback_node/
Are you dealing with any ODBC configuration? It may be an issue of 32-bit vs. 64-bit. I'm thinking, running from (x86) is telling Windows to treat the app as a 32-bit application and thus use 32-bit ODBC drivers. This would be a problem if you've only configured 64-bit ODBC drivers. I believe there are actually two ODBC configuration executables. One for 32-bit and one for 64-bit.
So, i've tried this a couple different ways. Initially I was just configuring the UDL to use &gt;"Microsoft OLE DB Provider for SQL Server" Then I tried creating a DSN, using the 32bit ODBC drivers you mentioned. I also tried launching the UDL as a 32bit UDL using: &gt;C:\Windows\syswow64\rundll32.exe "C:\Program Files (x86)\Common Files\System\Ole DB\oledb32.dll",OpenDSLFile C:\test.udl Both of the last two approaches throw a new error: &gt;Possible reason(s): ADO Error: 0x800300FB Exception occured in Microsoft OLE DB Service Components: The file is not a valid compound file. 
Sorry I haven't responded. I'm just not sure where to go from here as I usually build wrappers around the C APIs for the different SQL databases that avoids ADO and ODBC. This is comcast tech-support level helpful but, maybe review the process you're using to create "test.udl" and see if any configuration is wrong?
No worries, I should have replied, but I ended up figuring it out. For some reason, the configuration wasn't holding in the UDL. I only figured this out by opening the UDL in notepad. I then edited it in Notepad, and that ended up working out. Thanks!
You've got several options here. The simplest in my opinion would be to feed an analog input trigger internally to a frequency measurement counter task, but this requires some particular hardware features that your DAQ might not have (analog trigger and x series DAQ I believe). Sounds like that may be overkill though if the FFT approach was working previously. I think this VI should work for a software based trigger as long as you are taking records or parsing them in intervals shorter than the signal period http://zone.ni.com/reference/en-XX/help/371361G-01/lvwave/basic_level_trigger_detect/ You might also consider using this if you want to pull multiple triggers from a single record https://forums.ni.com/t5/Example-Programs/Threshold-Detector-with-Hysteresis-vi/ta-p/3492427
[removed]
Why can you not use Google?
"The problem **isn't** that I can't use google"....."but that the word "waveform" never came to mind"
So a coercion dot doesn't necessarily mean data is lost, or that there is a loss in precision, but that is often the case. It really depends on what the data types of the input are. I think in modern versions of LabVIEW you can open the context help window, and it will show what the uncoerced and coerced data types are and understand why there is a coercion dot and if it is a sign of lost data. In this case I'd ask for details on the data types to help understand why, and the impact. Clusters are similar to struct types in other languages. It is a way of collecting some set amount of data into a single data type for conviencince. Often clusters are turned into type def clusters, which is super handy. Using a type def means that you can use an instance of that cluster over and over, and then when you make an update you change it in one place (adding new data, renaming controls, etc) and this change is then pushed to all the places you used that cluster. You can sorta think of it as a class, without any methods, and the data is all public by value. It has its uses but more hardcore developers will say a class should often replace a cluster, since it adds other OO functionality and scales better if needed.
Can you provide a little more information? What are the values that are going into the subVI with the coercion dots? What are they being coerced to? If the coercion is not the source of the error, you may still want to get rid of these dots by explicitly casting the input data to the correct type - this is just good practice as it signifies to the maintainer that you are aware that a cast is necessary.
There are a LOT of questions you'll need to answer in order to get detailed info, but in general you'll need: * A control platform (PC, etc.) * A source for your controlled motor variable (position, speed, etc) signal * A sink for your motor feedback signal * Power electronics to provide drive currents/motor phase signals to your servo or stepper * Wiring, cabling, &amp; interconnect components for the whole works You really need to take a hard look at both your budget and what you have on hand to start. Take a look at this [simple case study](http://sine.ni.com/cs/app/doc/p/id/cs-14833) on NI's site.
First of all, thank you so much for your reply! Unfortunately, I do not have a background in electronics so please bear with me. * For the control platform, I already have a PC with Labview installed on it * For the variable source, I am interested in position. I was under the impression that a servo comes with an encoder. Could this encoder be used to determine the position? * What do you mean by a sink for the feedback signal? * For the power electronics, do you mean something like a DC power supply? * There is no concern for the budge I have looked at the case study and looks interesting! gonna read it thoroughly later. Again, thank you for your reply.
Sounds like you might be better off going with an integrator to do this for you. 
Ideally you would just convert an extension/compression tester. I wrote some software for an old Hounsfield which does everything you need, but is expensive. A much cheaper way is to grab a load cell, HX711 and an Arduino. There's a library for it and you can send the results over serial to Labview. The motor could also be controlled by Arduino, or through national instruments hardware. 
Yeah, based on your response I'm going to agree with /u/Aviator07 here and say that you're going to want to get some help with this. Not trying to be rude, just honest, but you're in over your head.
what version of labview are you using? there are json flatten/unflatten primitive VIs available along with a handful of other toolkits/addons created by community. check out www.lavag.org and vi package manager
Is this something you have to write yourself? DrPowell has a certified JSON parser on LavaG: https://lavag.org/topic/20236-cr-jsontext/ If this is something you have to or want to write yourself, you can try doing what Dr. Powell did, and implement your own JSONPath expression interpreter. http://goessner.net/articles/JsonPath/index.html#e2 I'm not very experienced with JSON parsing, I'm sorry I'm unable to help further.
LabVIEW has JSON parsing built in. Do Help, Find Examples, and search for JSON. You're looking for "Unflatten From JSON.vi".
thank you! Had no idea it had it built it
Had no idea, I have 2017 and I see that it is in fact supported. Thank you!
LabVIEW has a built in HTTP client API that you can use. It's under Data Communication &gt; Protocols &gt; HTTP Client. Use the HTTP Get VI and wire the url you provided to the URL input then check the "body" output for the JSON contents. Heck, if you want, you can even specify an output file that the VI will dump it to.
Wow that is awesome. Seems like I should do it in one step as opposed to saving the file. If I can just convert the JSON file to a string, then all my code I already wrote will work. But I want to try the JSON specific module to make it more robust. thanks!
Hey guys, Thanks for the replies. I decided to cancel the motor and will rely on manual height adjustments for the sake of simplicity. So, I will only need a compression load sensor to get my data. I appreciate any advice from you.
Thanks for the reply. This sounds manageable to do. I looked at some articles describing the same method you mentioned but HX711 was connected to an another part. Can you tell the name of that missing part?
It's a microcontroller. Arduino is an easy to program board programmed in a form of C++. There's a library for the HX711 which makes it easy to take measurements and do what you want with them. Switch relays, light LEDs, send to a computer. Whatever you program it to. 
What you need is the Application Builder which can be purchased as a separate module or a default part of the LabVIEW Professional (Full doesn‚Äôt have it)
You mentioned that you are using a mydaq. Is your project an academic or research one?
Academic. It's part of an [earthquake tower](https://www.pitsco.com/myQuake-miniSystem) unit at my high school that uses myDaq to record from accelerometers. But the software provided, written with labview, only works with windows. It would be really helpful to port it over, or write a new program, that works with macs because so many students have macbooks.
What is your hardware setup? You using an SDR?
Have you had a look at the white paper? You can find it here: http://www.ni.com/white-paper/52524/en/ There's a few sections on DCI that you may find helpful starting at 2.2.2.5.3.
Thanks for your reply. I went through the white paper but what I actually require is : Calculate the expected throughput at LTE Host UE and feedback to eNodeB. Do you have any idea on how this can be done? 
Using the quick drop box, try looking up this VI: http://zone.ni.com/reference/en-XX/help/371361H-01/lvanls/peak_detector/ Seems like it will do what you are asking for.
Yes I'm using two NI USRP RIO 2954R (160MHz). One for eNodeB and one for UE. Framework : LTE application framework 2.0
i am pretty for sure i saw labview in a video about the naval rail gun. don‚Äôt remember the exact video though.
You should be able to run the 32-bit version of LabVIEW 2016 on your Win 10 system. Your license covers both 32 and 64-bit versions. Unfortunately a number of National Instruments add-ons are still 32-bit only. Unless you are dealing with especially large datasets the 32bit version should probably meet your needs okay. 
Yeah, unless you have a very specific and fringe use-case, the 32-bit version should be adequate. And, as you said, often the modules are only compatible with the 32-bit versions.
thank you all. I'm trying the 32bit version now. Is there any where I can check what modules are available/compatible with labVIEW 2016(x32) ?
Here is the Windows 10 x64 compatibility. http://www.ni.com/white-paper/52818/en/ Some toolkits were deprecated, and some rely on 3rd party told that aren't 64 bit so don't expect to see every thing become 64 bit. Also NXG is apparently 64 bit only but not feature parity yet.
thanks
First app of LABVIEW TUTORIALS in playstore.
Well, this instrument has a LabVIEW driver already. I would grab that and see what it gets you. What signal are you reading with the myDaq?
Make sure the dll is in the 'always included' area of your build spec. Also make sure the dll is in either the same directory as the application or the data directory.
Here is the problem with that. I can't include the dll in "always include" because when I try it gives me a little yellow triangle that says "The selected file was not saved" and I can't include it in the build. I've tried manually adding it into the same directory as the build, and I've tried adding it to the data folder. 
Under source file settings in your build spec, make sure the dll is marked to be included in the data directory. 
While not doing that, I did manually change the dll calls to point to a copy of the dll within my project directory. Doing this allows me to add it to the build along with it being included within the data directory but for some reason it still crashes. If I don't turn the hardware on the it throws errors about not being able to start so I'm starting to believe it sees the dll and may be an issue somewhere else.
There are a few ways to handle this. One is to change the mechanical action of the button (right click on the button, mechanical action, then switch between toggle when released and seitch when released to see the two behaviors). The other is to use an event structure in a different loop to capture button presses, and use a queue or similar data structure to pass the press to the case structure loop for processing. Search for basic producer consumer architecture in labview in Google for examples. If you need more help just let me know!
Does the folder name start with an underscore? User.lib ignores these finders for the auto populate of the palette.
they did, but I wound up editing the the palet to manually add them
It sounds like you should make this into a package with [VIPM](https://forums.ni.com/t5/Developer-Center-Resources/VI-Package-Manager-An-Introduction/ta-p/3535883). Here you take a collection of files and turn them into a single file that VIPM can install. A package can be installed anywhere including vi.lib or user.lib. Then in VIPM you make the palette in a nice drag and drop editor. You can setup things like a package depending on another package, and VIPM will resolve the dependencies when installing it. Admittedly it is a bit of work up front. But the result is a single file that any other developer can double click and install in any version of LabVIEW that meets the requirement of the package (usually that version of LabVIEW or newer). Then in the future there isn't any manual file copying, or palette editing to use it again. The free version of VIPM does just about everything you'd want it to.
You‚Äôre probably missing some dependencies. 
dependencies on the .dll or in labview? Nothing else is listed in labview and the .dll is the only one the Scilab manual references. 
Does you .exe contain the scilab VIs, and do they know how to find the .dll?
Hi! Usually a question like this is an indicator of not really understanding the graphical programming as a concept. I mean no offense, my point is that let‚Äôs reconsider the question itself. What you need to do is in fact saving the data which is in the wire connected to the indicators. For that, you can use VIs in the File palette. The easiest way is to use the Write Measurement File express VI. Search for an example and it will be very easy!
im such a dummie, its doing scientific then concatenating that 6 to the end of the string... i'll have to find a new approach
Yes, this seems to be some sort of issue with the gateway itself, because it compiles in 32bit LV but not 64 bit. 
I don't think the manufacturer exists anymore.
If I'm understanding your set up right, you have a USB-Serial adapter, and you are trying to use this to communicate via RS-232 to the motor controller? Most of those cables use an FTDI chipset, and you need to make sure you have the right drivers installed for it to work on Mac. You probably do, if you're seeing it show up as an available COM port. It might not hurt to verify that you are actually sending data via the serial port. To do that, try a loopback test (short the Rx and Tx pins together). Assuming all of that is going fine though, the problem is likely in your command formatting. A carriage return is \r, and a linefeed is \n. The manual says it wants carriage returns and NOT line feeds, so don't use the \n. A further note on that - in your command formatting, make sure you have \ codes enabled. The code \r stands for a hex value of 0x0D. And the reason a code is necessary is because it is an invisible character. If you are simply typing "\" and "r" as a normal string, then you are sending two very different characters, instead of the one invisible character represented by "\r."
I love you. Thank you so much for your response. I'll try that out!
I did a very similar project. Had a piece of emissions sampling equipment set up for remote control with RS232. I built an Arduino based controller to interface with the machine, set all the settings, etc, then send the data out onto a CAN bus. /u/Aviator07 has some good suggestions. I would also start with an RS232 terminal program and take Labview out of the loop (weird advice for the Labview subreddit...) I think the one I used for that project was [RealTerm](https://sourceforge.net/projects/realterm/). If you have a USB-serial adapter that interfaces with RealTerm then you should be able to step through one of their sample programs by typing each line into the terminal and make sure everything is working right. Then you can move over to a more automated program written in Labview.
What motor driver are you using? We need some more info to be helpful. Labview can easily generate waveforms that are that fast (20kHz) but you need a controller that can both interface with LabVIEW and is able to provide that clock rate on the output pins.
You're not going to be able to accomplish that kind if timing resolution (or requisite determinism) with software timing. In other words, you need a hardware timed solution.
Search for Vision Development Module 2017 in google and just download it. It will run in eval without a valid serial number. Also 2017 can be replaced with your preferred version, to a few versions back.
So let‚Äôs start for example from here: http://www.ni.com/download/vision-development-module-2017/6640/en/ Do not select the evaluation but the already a user option. It does the trick for me ( still have to login but that‚Äôs all)
No comments? 10 upvotes? Hm... Any feedback regarding usefulness? I recon this might be self-promotion (again)...?
It kind of depends on what you mean by "crunching" very large matrices. I've written code in LabVIEW to process large arrays of data (something like 1000 x 100,000) and given what I was doing and what the performance requirements were for my application, LabVIEW worked just fine. If you have access to LabVIEW and lots of experience with it, I would think that it would be useful to do a quick mockup of what you expect to be the slowest operation and do some quick profiling to see how performant it is.
If you‚Äôre trying to put a large amount of data into an indicator that is very likely to cause performance problems. Sending data to the UI requires copying it twice, and depending on how you wrote the VI you may be introducing additional copies (hint: avoid local variables and property nodes). You could also be forcing code to run on the UI thread, which can cause performance problems. Basically, don‚Äôt judge performance using code that tries to use the UI. That‚Äôs hard to make performant in any language, and LabVIEW makes a lot of things implicit in ways that make it unintuitive how to keep performance high while interacting with the UI. There have been many presentations and white papers on that topic. I‚Äôve personally given some of those presentations. It can be done, but the best advice for good performance is to stay away from the UI as much as possible. 
I've already made kind of a Benchmark with Labview and Fortran, I've used them for solve linear systems and labview went better (faster and less processor usage), but my Fortran code wasn't optimized (neither my labview code). I believe you can your computation in labview, there are some libraries that maybe can help you, like sparse matrix methods and GPU analysis (if you have a Nvidia board), I suggest you to look on labview package manager for other libraries. And sorry for my rusty english :)
Thank you for your reply. I did mangage to create my desired file, by using the Write Measurement File. I selected Excel as the created file. I connected 3 signals to an array, before connecting the array and the "Write to Measurement File" block. 1. When opening the excel sheet, the columns are labeled Untitled, Untitled1 and Untitled2. Is it possible to have these labeled in the code? 2. I selected One Column only for timestamp, and I get the date and time for the different time samples. Is it possible to add another column, where the first row shows time = 0, and last data will show the elapsed time for the run time? Thanks in advance
First robotics competition Would a drive robot until left and right motor hit X number of spins And we use encoder from: first choice by andymark
Sounds like English isn‚Äôt your first language. Maybe if you include your country in the post someone may come along that can help you more directly. 
I judged typed the way I thought it. I will make a edit so it can be easy to understand, not right now because typing with iPod is a pain
I have not used these compiler products yet. However I have used both RPs and arduinos extensively without LV, and I am a CLD. The raspberry pi is non-determinate. Code running on it will have significant jitter in any timing you set up. That is, if you want a code loop to execute every 100ms, you'll find it executes ROUGHLY every 100ms, but sometimes with extreme delays. This is not good for precise data logging or control systems that require tight timing. Code running on an arduino, conversely, is rigidly timed- possibly down to the very timer cycle. So if you need to generate a PWM wave, or sample data every 30us, use an arduino. But the arduino has limited capability. No simple display, the network is very rudimentary, storage, etc. So... How about splitting the code up on both? Basically write 90% of your app to run on the Pi with all the bells and whistles... But any hardware outputs are pushed down to the arduino. Essentially the pi sends a message to the arduino. "start pulsing every 15ms... NOW". "generate a 50% 500Hz PWM". Theres a couple ways to implement the communication, but essentially treat them like two loops that communicate via messaging. I am not sure what features of LV work on each. Fwiw, it sounds like rigid timing is not important for you. So you could get away with a raspberry pi + sensor kit. Again, not sure how LabVIEW handles that kind of IO on the pi. It may be that any analog data has to go through an arduino though... So you'll have to read up. Good luck! 
I see your points. Answering them: 1. if you doubleclick the Analog Inn Express VIs then you can do the configuration. There you should find the untitled text. If not, then you can use the Set Dynamic Data Attributes vi (Ctrl+Space to search) to modify a certain signal's name. 2. Regarding the relative time this is a simple solution: https://imgur.com/EJf9yGI 
It is viable if you aren't interested in 100% reliability IMO. Porting a moderately complex project to the compiler is a challenge to do with the limited blocks it supports. However, if you are starting a new project not relying on previous LabView this would be fairly easy (only use their toolbox). Take a look at some of the posts on the support forum ([link](https://www.tsxperts.com/forums/forum/raspberrypiforlabviewforum/raspberry-pi-compatible-compiler-for-labview-support-forum/)) to see the kind of issues people face. 
For academic use-cases another option is the [LabVIEW MakerHub Linx toolkit](https://www.labviewmakerhub.com/doku.php?id=learn:tutorials:libraries:linx:3-0) which supports running LabVIEW on BeagleBone Black and Raspberry Pi 2/3. It has some good step-by-step set-up tutorials.
Hi again. Thank you for the help. I have now been able to log the time spent running the code, and also naming the headers using Set Dynamic Data Attributes. I could not name the signals through Analog Inn VIs. Double clicking it didnt get me the Untitled names. Do I have to create one Set Dynamic Data Attributes per column of data in excel, or is it possible to use only one? Again, thank you very much for your time.
It is an interesting question! My doing the math you can see that transfer functions in series result the overall transfer function as the multiplication of individual transfer functions. So by looking from this point of view it will be a 4th order one. On the other hand, because of the numerical solving method I have a gut feeling that it will be not really the case and will be closer to the 2nd degree version
I‚Äôm glad it worked out for you! The Set Dynamic Data Attributes can accept a single index so it means you have to do it channel by channel
Hey, I'd say that it should work out to be a 4th order filter. If you look at a specific frequency on how much the signal is going to be lowered (for example -20db) the first output will be lowered by -20db and the second by -40db. 
Random times 25, round up. Now you have an integer between 1 and 25. Now indeed a 1D array at that index minus one.
Cast to int after multiplying and save the subtraction?
Okay first I made a mistake. It is actually round down, then add one. If you just cast to an integer then the values won't be eventually distributed. https://forums.ni.com/t5/LabVIEW/How-do-I-generate-a-random-integer-from-1-5/td-p/1056529
Good point
Round down to -Infinity is the same as casting to integer for non-negative numbers. Casting to an integer will truncate the decimal part, which is what you want. 
Just go over all of the prep. They have old stuff too. Keep them fresh in your head. That's like 60%. It was pretty easy after those practice exams.
Ah, this might be why the controller isn't reacting as I expected. I will have a look at that. Thank you!
Not teststand, but whenever I have issues with the application builder reporting broken VIs that arent actually broken, a fresh recompile of the top level VI (Shift + Run) seems to solve it. Takes a while though.
Thanks for your comments! I believe the math does, in fact, check out. When zero phase shifting (via recursive filtering as seen here), the order is squared. Y‚Äôall rock! 
Thank you for the reply, I apologize for the late response. I completely forgot I had posted to this page about the project. I'm taking voltage from photomultiplier attached to the detector into the myDaq. I have the library for the monochromator downloaded and have been using it in labVIEW to do simple things like check if it is on, set the grating to a certain wavelength, ect. I've managed to figure out how to get the monochromator to do a scan by putting a set and end wavelength constant, then adding .5 to it every iteration of a loop. I'm really struggling trying to understand how I'm going to combine the voltage response with the current wavelength into a table or array and then construct a waveform out of it. If you have any insight on that it would be much appreciated. Once again, thanks for reply!
Most hardware have an over voltage protection, so throwing in 12V or 14V into the input likely wouldn't have fried the channel, but the reading will rail at 10.0V. Providing too much voltage will degrade the hardware and cause it to fail early. If that voltage is too high that hardware will fail instantly. You should use some kind of signal conditioner to scale the voltage down to what can be read easier. Some devices can isolate it and scale it down by 1/2 so reading 5V on the DAQ card is like 10V. Beyond that a voltage divider can be used but has its draw backs.
Set up a voltage divider with two equal value resistors and measure it that way. 
Could you post a screenshot of your block diagram? What sampling method are you using? Are you using the DAQ Assistant? I've used that method before with some success. LabVIEW also has the "split signals" and "select signals" functions which preserve the dynamic data type through them. I'm wondering if your index array function is converting your vibration data to integers or something funny like that.
Vibration data is usually measured in terms of frequency content, and the magnitude of that content. What you (probably) want is FFT (fast fourier transform) 
Hmm interesting. I tried looking at freq content but it wasn't giving anything back. I can screen shot a picture of the graph sometime tomorrow but what freq roughly w0uld you expect? 100hz? 1khz?
Expected frequency is totaly depends on your setup and what you want to achieve. Did you consider Nyquist frequency and aliasing effect?
You're missing some fundamental knowledge necessary to do this: *What nyquist frequency is and how it affects your data *how fft analysis works *what range of frequencies you expect from your experiment. This depends on the size of the thing you've instrumented, among other things 
I am familiar with all if the above except how the size of the device affects it. I am able to pull out high freq data on the accel with other tests so I know I am way above the nyquist rate. I just figured I was using a vi incorrectly
In that case, it's kind of impossible to help if you don't post a code snippet . . . RMS of the accel data would give you amplitude information, but nothing about frequency content. Do you care about the frequency content? If I was doing this I would just record the raw voltages using LabVIEW, then use Matlab/Python/whatever to analyze the data. I am not very good with LabVIEW though.
Not looking to apply (sorry), but if you haven't already I suggest posting over on [LAVA](https://lavag.org/forum/31-job-listings/) or [NI forums](https://forums.ni.com/t5/LabVIEW-Job-Openings/bd-p/JobPost), or even the local user group for Seattle.
When you acquire data for one channel of analog input, it's typically going to be an array of values. The time between each sampled values is the inverse of the sampling rate you used for the data acquisition. Now, what you want to do with that data depends a lot on what kind of phenomenon you're trying to measure. You can just extract RMS or peak-to-peak for general vibration information or you can transform that data into frequency-domain (using a Fast Fourier Transform) and learn additional information. For that you'd need to specify a bit further what you're looking for with your measurements
Also, it's important that you check if you have the right hardware for the type of sensor you're using. If you're using an IEPE accelerometer, for example, you need the right kind of module to read the signal correctly, otherwise you'll only get noise
I see a few problems with the vi: 1. You need to initialize the feedback node with an array that is not empty. My guess is that array is the user input from your step 1. 2. Your vi is not picking a random element from the array, the knob control is selecting which item gets deleted. You'd need to use a random number multiplied by the size of the array and rounded down to get that random selection. 
From user manual p391 from the 2400 series(2200 should be similar) [ :SENSe[1]] :CURRent[:DC] Path to configure current: ‚úì :PROTection Path to configure current compliance: ‚úì [:LEVel] &lt;n&gt; Specify current limit for V-Source. Go into instr.lib and program your own VI with the SCPI command and add it to the library. Then you could talk with the collaborators and update the NI instrument library.
Decimal String to Number is for Integers (which is why you're getting a rounded number). Try Fractional String to Number - should give you what you're after. 
An alternative that‚Äôs provides a bit more flexibility is the Scan From String primitive http://zone.ni.com/reference/en-XX/help/371361N-01/glang/scan_from_string/
The simplest thing I can think to do is put a string control on the front panel. This will be your comment field. Concatenate that with your data text and you're done. Leave it blank if you don't want a comment. It's rudimentary, but might give you some ideas to make it more sophisticated if you need it to be. 
That's what I normally do. Sometimes I set up a momentary switch which will use a select function or switch a case structure, if the program is incrementally writing data to a file, so that it uses one iteration of a loop to write comments.
What's your budget? NI makes it pretty dang easy if you can afford a 4 slot USB cDAQ chassis, and two cards. With this you will be able to measure three phase voltage and current with simultanous sampling cards and even had some free toolkits for calculating power factor and other things. For single phase you can probably get away with a single card, and a signal conditioner. http://www.ni.com/white-paper/8198/en/ At one point the productized the software but if you can find an older version it has less features but is free. https://forums.ni.com/t5/Community-Documents/Electrical-Power-Measurements-with-LabVIEW-and-CompactDAQ/ta-p/3515033
[These units](https://www.amazon.com/Sonoff-Wireless-Consumption-Measurement-Appliances/dp/B06XSD6PD6) have an ESP8266 inside that can be programmed using the Arduino libraries for it, you could pull data from that over HTTP or UDP or something.
Be ready to ask questions. Take advantage of exercise time that you finish early to try the challenge problems (when provided) or ask additional questions to the instructor.
You don't need to prepare. If you have any programming experience, it will be piece of cake.
If you have a project in mind, work or personal, that can focus you in on asking the best questions. This link has good starter videos: http://www.ni.com/academic/students/learn/ 
I have some C from years back, and have done a lot of PHP stuff in the more recent years. 
Working on a basic test stand control interface for a work project now. But nothing too complex other than UI and flipping some relays.
Should be good to go, I always like to push past the standard exercises and build on them.
My main goal in this class is to learn good habits and standard procedures, as I'm coming from regular programming as well.
I think we need a little more details than that but I assume you will want to use the limit switch to trigger a digital input in a DAQ and use that channel as a Boolean in your code to move through a state machine. 
We a using a vertical linear slide to pick up power cubes for the FRC tournament and needs to stop so it doesn't strip the brass
Dataflow is a topic that self taught LabVIEW programmers tend to overlook. The class may feel slow since it doesn't assume any programming knowledge. The help has style guidelines. Most things that can be done in other languages can be done using LabVIEW but in a different way. The class won't talk about pointers (DVRs) and interfacing to external code such as DLLs or .NET. Optimization is also not discussed too much because they want you to get going on your application. Most of the course topics exist as papers on NI's website. Their forum is a good resource, people tend to be quite responsive. Good luck. 
Thanks for that response. I feel like I'm in a good shape heading in, and I'll bring some problems I've been working through with me. What I'm most looking forward is a full week of being immersed in labview and zero distractions.
I wound up running the two vi's separately but this is more elegant. Thanks!
Right click the build spec in the project and select "run as startup" and the right click again and select "deploy." From then on, when you turn the cRIO on or reboot it, your app will automatically run. It may take a few seconds to start but it'll go. 
This is just a theory that I haven't tested, but if you are looking to be able to "hot swap" rtexes, it may be possible to have some sort of configuration host application that FTP or webdav the correct rtexe to the startup folder then restarts the cRIO. An easier way might be to have all of the possible rtexes on the cRIO constantly and have the configuration app simply move around the files so that everything is happy
This is what I was thinking I would have to do, I was just hoping there was an interface similar to the one provided to load FPGA bitfiles to the device
You could use the RTAD tool to swap images on the controller. It's a free tool on NIs website. You can also start messing with the sysconfig API, it has some tools for tasks like this, but is not entirely user friendly. 
We do offer paid internships typically during the summer. Send me your resume and make sure you include a contact phone number. Send examples of work, and portfolio if available.
Whether you're programming a digital or counter task, the answer is yes. The answer will vary on which you're using. Try using Context Help on whichever function you're using to learn more about it. 
If it is what I think it is - you know that stuff is infringing patents and basically illegal, right? :)
Explain.
Well this could just be some random spammer trying to get interest in their mini PC. But since this was posted on the LabVIEW subreddit one might think that this is intended on being an RT target for LabVIEW code. We've seen a few Chinese knock offs over on [LAVA](https://lavag.org/topic/19912-world-first-3rd-party-product-supports-labview-fpga-from-mangotree-tech/) primarily from MangoTree who have an FPGA that LabVIEW can discover and add as a target, as well as some kind of RT platform. 
Language-independent update just landed! For those curious, the node was creating some property nodes and setting the selected properties by the property's name, which varies from one language to another. We just needed to update it so that they were set by the property's unique ID.
If you don't want to replace the whole image of the cRIO, you just have to replace the .rtexe file using FTP or WebDAV. Depending on the OS running on your cRIO, the folder for the rtexe will be different. Take a look at this article for help: https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000PAM8SAO 
Would it better for me to develop the program on a different monitor, transfer it over, then run it on the smaller display?
Yes, the runtime PC resolution doesn't really matter as long as you design around it
Thank you.
What part of Raleigh? Can you PM me the company name?
What part of Raleigh? Can you PM me the company name?
How competitive is a graduate degree in experimental physics with a massive portfolio of "done in a rush because we needed something that instant" Labview code? 
Suppose I should add a picture. https://imgur.com/wIhJSRc
It‚Äôs difficult to tell what‚Äôs going on in several parts of your block diagram. Typically subVI‚Äôs acquiring data will have a rate and # of samples to acquire. Adding some comments on the block diagram would be helpful. You mentioned you‚Äôre new, did you take Core 1/2? Seems like some of this might be cleared up with a better understanding of the fundamental VI structure and best practices. 
I thought about that. Would I then have to add a breaker of some sort to make sure it doesn‚Äôt pull to many amps?
The RIO will only pull as much current as it needs to operate. So if it is operating at 12V most that your DC-DC converter would need to be able to supply is ~1.2A
I think there may be some problems with the data acquisition on your VI. From what I understand, the Linx VIs provide with a single voltage sample coming from your Beaglebone board that is read in a specified time interval (the sampling rate). This time interval is the value you should use for dt when you build your waveform before applying the FFT. You should also check if this sampling rate is stable for this time of data acquisition and if the frequency you're looking for will be within the bandwidth
It is possible for you to learn and develop in cRIO. You are not screwed. You may or may not need to program FPGA. There are places where programming LabVIEW RT and regular LabVIEW differ, but if you have some basic LabVIEW skills you can develop the knowledge. The question is will the timeline allow you to do so, and the complexities of the project will determine that. On the IO side, how fast do the DI, DO, and AI tasks need to be? If it is all relatively slow (Hz), then you don't necessarily need FPGA and can use the Scan Engine Interface. The TLDR version of Scan Engine is a way to access (most) cRIO modules without knowing FPGA programming. Very simple interface, but speed/determinism mostly go out the door. On the cRIO vs computer side, the easiest way to think is that will be developing 2 programs not 1. You will develop the computer side program, which contains the GUI to the user. You will also develop the cRIO side program. The cRIO side will have your data acquisition, analysis, and output. Since it is 2 programs, you will need a communications layer to allow them to talk to each other. Since you have the online training, take advantage of it. Start with LabVIEW Core 1-3 (as a refresher), then LabVIEW RT 1 &amp; 2, and the Embedded Control and Monitoring course. 
Search for and read the cRIO developer guide. Shouldn't take more than a hour or two to read and will give you a great overview of everything you asked about. You can focus effort from there. 
Farm it out to a contractor. I might know one... &lt;wink wink&gt; Naw, I'm just kidding, listen to /u/SeasDiver, he's not steering you wrong. You can do this, it'll be hard and you may stumble but you'll figure it out in the end. It's not that hard.
Take some time to get familiar with the cRIO developer guide. NI published it years ago, and I consistently suggested it to new users who were just getting started with the platform. Also, call into NI support to see if you are eligible for the NSCS (new service customer support). If you are, you can get more proactive support from the Applications Engineering team. Take the time to take the LabVIEW Core 1 and 2 courses and then Core 3 after you have some LV experience. There are a lot of pitfalls with RT and FPGA development - so do research, ask for help, and plan on unexpected pitfalls. Lastly, enjoy the chance to be creative! Source: Former Applications Engineer turned LabVIEW Developer.
Thank you, /u/george8762 for you advices. It is very nice and exciting to receive these advice from people with so much experience such you guys! 
So you have a number of questions to consider: * How does the robot know where it is? * How does the robot know where to go? * How does the robot move itself? * How does the robot move its arm to grab the object? * How does the robot know where the object is? Answer those, and then you can start getting into more details. LabVIEW is somewhat irrelevant until you know more about the bigger picture...
MyRIO essentials guide has lots of examples, and YouTube videos. http://www.ni.com/tutorial/14621/en/ And for more basic LabVIEW training check out the links at the bottom of this page. https://forums.ni.com/t5/Community-Documents/Unofficial-Forum-Rules-and-Guidelines/ta-p/3536495
Terminology gets a bit confusing here, so I'll use the following: * audio clips = your sound files * pitch = perceived 'note' of the audio clip * samples = underlying digital audio samples that make up the clips * sample rate = samples per second, sampling frequency If your aim is simply to change the pitch of your clips without changing the *sample rate*, you can re-sample the clips using interpolation. This will change the number of samples in a clip, so that when you play it at your original sample rate, the pitch will be different. However, this will also change the duration of the clip. If your aim is to change the pitch of the clips without changing their *duration*, then you'll need to use methods like those on the Wikipedia page you linked to. The crudest method would be to re-pitch a clip as above, and then use something like granular synthesis to change it length back to its original length - cut the clip up into tiny, overlapping, windowed 'grains', and then change the time delta between each grain, lengthening or shortening the clip without changing its pitch. LabVIEW is not the best tool for this though.
http://www.ni.com/tutorial/7900/en/
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z000000PA2SSAW According to this the Compare tool is included in the Base as well. Maybe it‚Äôs not licensing but a technical problem. I‚Äôm sure tech support could help you
Take it somewhere else, buddy. This is a LabVIEW forum.
Unfortunately this has always been a professional only feature: http://zone.ni.com/reference/en-XX/help/371361P-01/lvhowto/configlvcomp_thirdparty/ 
As an NXG developer on a throwaway account, I can say we're definitely throwing a lot of developer hours at the package manager and making package distribution from within NXG work. I'm not on either of those teams but I have attended their workflow audits and while a lot of it is slideware right now (UX made some pretty pictures with what they want it to look like), the tech leads/product owners for both teams are extremely down to earth and understand what gap their features are actually trying to fill. As someone also skeptical of NI's new software strategy, I'd give the package manager and distributions within NXG another chance in a few years' time.
https://play.google.com/store/apps/details?id=com.thunkable.android.ajaysai397.LabVIEW_Tutorial&amp;hl=en LabVIEW tutorials app in the Google Playstore. 
You can do this with any device capable of digital I/O. A normal DAQ card can do this. Look up digital input and output and that should get you started. 
look at the manuals of the motors or speak with someone how has electrical knowledge to determine how the motors are actuated. Posibillities: * just a digital Output (On or Off) * PWM Signal, DutyCycle is Speed in % * Motors with own ECU and communication * frequency converter connected to motor 
i wrote this library when i worked at national instruments: https://forums.ni.com/t5/Example-Program-Drafts/RT-Management-A-library-for-managing-an-RT-Target-with/ta-p/3526783 it provides the ability to easily transfer and set, programmatically, a startup application. note, however, that this does not take care of programmatically deploying chassis settings, shared variables, etc. that is a more complex beast. i have written code for that as well, but it isn't available. to programmatically deploy such things, you need to use VI Server to interact with the project and libraries to deploy libraries and the startup application. if you are only needing to simply transfer or interchange startup executable, then the RT Management library will work fine. after i left, NI also includes it in the VIPM tools network now, but they removed some stuff like the palettes. you can see that it handles both FTP and Webdav and provides the ability to extend to a new file transfer method. at the time it was written, a lot of work was done to make it work seamlessly across many, many types of cRIO targets (non-trivial). however, i am sure NI has done their best to introduce new use cases it isn't handling with newer targets. they should be, hopefully, able to be fixed though for your specific target.
To measure current with a voltage reading you put a known resistor into the circuit then measure the voltage across it. https://electronics.stackexchange.com/questions/51602/simple-way-to-measure-amps-with-a-volt-meter To measure resistance you need to source a current. You could do this with the AO and a known resistor. https://forums.ni.com/t5/Multifunction-DAQ/measurement-of-resistance-by-USB-6008/td-p/3268731
Check out the example problems for the CLD on NI.com. They're fun little projects and a good way to practice. Plus it gives you a sense of direction.
Here's an idea he'd be pleased with: find some publicly available data source in CSV format, like weather or satellite almanac data. Use the HTTP GET VI to fetch the page, parse it out, then display the results on a graph. If a student of mine did that on their own I'd be surprised as hell. Because that's actually useful. And it requires no hardware. 
If you want to interact with the real world, one inexpensive way to do that is through an Arduino. You can can connect to the Arduino easily from LabVIEW through a virtual serial port. It adds a little extra work because you have to program the arduino to report on its sensors. But that's not very difficult. And you can get inexpensive sensors for it from places like Smartfun or Adafruit. 
absolute value
Perfect, thank you!
How are the axis different? The graph after the read will have the x-axis with no scale. It will just be points since it has no idea what it is reading. For example if you have 5 seconds of data at 1000Hz, the x-axis will just show 5000 points.
I'm at school right now, but when I get home I will see if I can help you. I use LabVIEW with cRIO FPGAs, but I think it should loosely translate to Arduino. We'll see.
I did a project controlling arduino with LabView but did not use Arduino interface because I was told it isn't really good, rather I did it from scratch with VISA and writing my own code for Arduino so I can send you those later if you are interested doing it that way. Since serial takes quite some time to send data to Arduino and since that time can vary by even 50ms I would advise against commanding each step with LabVIEW. Any quick work should be done by Arduino itself and LabVIEW should only update it (by interrupting) and do more complex processing that would take Arduino too much time. If you want help around your code best you send some screenshots and also send Arduino code.
Check LINX, it is very simple and comes with great examples. You can pretty much call any analog and digital read/write function and also write your own functions and call them in Labview. Get started with [LINX](https://www.labviewmakerhub.com/doku.php?id=learn:tutorials:libraries:linx:getting_started) 
Can you post a screenshot of your code? The while loop will stop when the input Boolean to the "stop" terminal is true. You should be using a "less than or equal" node to compare the two values and feeding the output Boolean of that node to the stop node of the whole loop.
What is it that you are hoping to accomplish exactly? If you want to have cases that behave differently based on what inputs there are, you should consider using an Enum control instead of a String control. That way, you can control what the options for input are. If you want to get string information, you can use the Format Into String function. https://imgur.com/a/vEQkj
Thank you for your answer. As a basic program, I wanted to convert from celsius into either fahrenheit or kelvin according to user input and I thought that it would be more meaningful to write F or K than 1 or 2. I didn't do any different thing than the pictures I posted but now it works for strings. 
What inputs were you using? Something like 34F or 19K? If so - then the string doesn't match - because you're looking for "F" and it's finding "34F" You'd have to strip the number off before you passed it into the case structure.
That's the beauty of an Enum control - it functions like a string for the user. Regarding why yours works now - string case structures are case sensitive, and the strings must match exactly. So you may have had an accidental space in there or the wrong case or something when you tried it before. 
I mean if it's experience you want check whether your university allows you to access the core 1 and 2 online training. The way you interface with equipment will change depending on the application but learning how to effectively (and legibaly) implement a basic architecture (eg. event driven state machine) is something that will help setting up future VIs.
There may be an easier way than what I'm going to say but my first thought in at least getting the coordinates is as follows. First, programmatically determine the location and dimensions of the map using a property node. This will let you determine where and how big the map is on the front panel regardless of window/map resizing. Then use mouse coordinates from initialize mouse and acquire input data VIs to determine where you are within the map bounds. Using a percentage based calculation you could then probably figure out how far NSEW you are. This could then be mapped to the data you would have read from the data file and written to a couple indicators. As far as clicking and adding a point, I can't think of a method off the top of my head to do so. 
Hey thanks for taking the time to help me out. I think I was a little unclear in the post. I would like to add a static global map to the front panel, just a basic one with the curves of the land shown, something like [this](https://imgur.com/a/2lvXc). I already have the coords from the CSV in an array. I need to figure out to configure the map to GPS coords, then just put a marker (like a dot) on the world map, when I run the VI. 
&gt; you'd have to pre make several indicators and then programmatically change their positions I had to do something very similar to what OP is describing - display lightening strikes in real time on a map, pulling the data from NOAA. Messed around with different ways for a while, but ended up doing pretty much what you described. I created I think 30 or so indicators and hid them, then un-hid/placed as needed. It's not 'modular' per say, but you can choose an arbitrarily large number so that the staticness of it isn't an issue.
There is a robotics toolkit for LabVIEW which does exactly the same. You can build a robot frame + virtual robot according to your mechanical setup and after using the frame the only inputs you have to control are speed and angles of turning. You can also do simulation in a virtual environment - there is a 3D model and environment builder in LV which is pretty neat
That‚Äôs the toolkit I have but I‚Äôm confused by the frames setup and how this works - do you know of any examples? I assume I‚Äôm able to control the acceleration and braking profiles through this? The key issue I‚Äôm having is that my motors have no braking so the robot will roll a fair distance after turning them off - I need to ramp the acceleration down. 
You could create an X-Y graph, and then make all of its various parts transparent. Lay it over your map and size the graph portion (exclusive of the surrounding frame) to be the same size as the map. Use the Cursor Legend of the graph to create some cursors. Then use a property node for the graph to select a cursor and make it vislble/invisible (or transparent/colored) and set its coordinates. When you want to place a point, use a property node to make one of the graphs cursors visible and located correctly. 
This is a great description. Just as a sidenote, if OP did want to click and add new points, perhaps something with an Event Struc "Mouse Down" event could be a good starting point, then in that event frame have it append the CSV file with the new point. Perhaps if the whole thing was a state machine, it could have a "Read" case, where it just plots all the points on the map, and a "Write" case with the event structure in it, allowing the user to click and add new points. The initialize frame of the case structure could read from the same CSV, but then initialize a new file and append only to that one, so the original data is preserved.
You're right. I totally forgot about the event structure and took it for granted since I tend to just build all of my VIs with producer- consumer / state machine mentality with event structures. I used the mouse down event recently along with a method incredibly similar to the link you added along with the concepts I mentioned above when I wanted to add the ability to click within a certain area in an array to select a channel to calibrate. Honestly, OP, you might be a step ahead just looking up queued state-machines, event structures, and producer consumer parallel loops to get a feel for architecture. All of these concepts will help you better understand how to best apply some of the methods we have brought up. 
https://knowledge.ni.com/KnowledgeArticleDetails?id=kA00Z0000019KoDSAU &gt; LabVIEW for Linux is not currently compatible with Fedora. The document on Software Support for Linux Distributions describes in detail which Linux distributions are supported with NI software. I apologize, I don't know enough about linux to know if there's work arounds, but it appears to not be officially supported by NI.
[Image of code I am working on](https://i.imgur.com/vrm2y6m.jpg)
I know that it's not officialy supported, thanks anyway. I was just thinking if there is a workaround or any other method to use it, because Redhat and Centos distributions are supported, and Fedora is similar so i still hope there is a workaround
If fits your requirements, you can use the Delete From Array Function. It seems that you have no other parameter to look inside the elements you want to delete other than its index, right? If so, this is the way to go.
For what I understood, you can investigate the spikes in a while loop and verify if the conditions are met before updating the blink value. https://i.imgur.com/6UhBYc3.png You have two conditions to update (+1) the value: 1. First spike (i.e. no previous blink and detection of a spike) 2. Another spike within 3s after the first. (i.e. you must have a blink =1, check the elapsed time, and, of course, detecte a spike). I hope this will help you.
Haha oh man how I wish you were 30 minutes earlier. I was trying to figure it out that way but gave up.
Save yourself a whole lot of trouble and run it in a virtual machine.
Unfortunately I‚Äôve only run it on Windows. I don‚Äôt know what your resource limitations are but that will get you the most functional release. Can‚Äôt back this up but I‚Äôve always felt like LabVIEW on Linux was an afterthought for NI. I don‚Äôt work with LabVIEW on a daily basis like I used to though so that may have changed in the past few years. That all being said...CentOS is the way to go for Linux. 
Even better than mine, I was focused on the 'delete' part, and didn't think of that option. However, OP said he solved the issue in more rough way. I assume he's using a small VI, so it works nicely.
oh wow. If you cant tell im new to labview and everytime I try to do something it just blows my mind how easy it is. 
If you right click on my computer and select create an actor a library will be created. If you right click on a library and select create actor, the new actor will be added to the selected library.
Dude, you're a legend.
On an unrelated note, I love the actor framework but I find it completely unusable in large projects. LabVIEW cannot handle large amounts of classes to save its life. I have a huge project I wrote in actor framework with an PC and RT component. Both sides written in actor framework. I had to separate each target into its own project to keep LabVIEW from crashing. It takes 30 minutes for me to get my project opened and ready for development. If I have to switch between targets (switch between projects) there's an hour killed just waiting on LabVIEW to open the damn thing. Anytime I change class private data, the LabVIEW compiler screws up and generates bizarre and obscure compiler errors. Something I'd never seen prior to working with the Actor Framework. The solution is to go through all my member VIs and find the unbundle that didn't correctly reset to the right value. Often times, the compiler corrupts my builds (builds that take 20-30 minutes) so there's another huge time loss. I've tried every suggestion I can find on lavag and the LabVIEW forums to speed up LabVIEW's performance and nothing works. I really wish NI would address compiler performance with classes because I really want to use the Actor Framework.
That's disturbing. We've got a 20 year old LAbvIEW project with 6000 old school vi's in it. We've been converting peices to actor framework. I hope we don't run into this.
It's the message classes, they're the killers. I've written some actor-ish style frameworks that are way more lightweight than the AF and have them working in projects with well over 300 classes.
Is it just the number of them or is there something about their design?
For the long load times it's the number of them, if you separate your compiled code, it's that much longer. But the corruption, I never figured out what was causing that. Personally I don't like to use property node accessors, they've always been problematic (although some say that's fixed), and the more objects you have using property node accessors the worse it gets, than tied to the fact that even simple Actor Framework applications are going to have a ton of classes because of all the message classes you need, and it just gets nasty in there. I really dislike the message as a class mindset. I prefer to just use simple JSON messages to do communications, when needed. I think it's just as flexible and you then open up the door to expanding the code outside of LabVIEW if necessary. 
Not sure if there‚Äôs a higher level solution, but you could always try using the NI-VISA USB raw mode: http://zone.ni.com/reference/en-XX/help/370131S-01/ni-visa/usingnivisatocommunicatewithyourusbdevice/
This might be of help https://forums.ni.com/t5/Community-Example-Submissions/Get-Mouse-Coordinates/ta-p/3511576 
Thank you! I‚Äôve messaged that Group. Wasn‚Äôt sure if they‚Äôd get any interest on eBay but I may post and see. Cheers for the help.
Quick &amp; dirty method: Property nodes Better method: Create an initialized shift register (requires a loop), perform whatever processing actions on the array contents you need to, then push the updated array contents back into the shift register. 
Can you a give a simple example of those two? 
There are similar companies as well. I know Artisan because I see them at NI Week and have sold a couple items to them. There may be ones that are located in the U.K.
Connectivity Palette &gt; Input Device Control. https://imgur.com/A6H5t76
Not in front of my computer to confirm, but it's the "read from spreadsheet" function. It's used to read data from can/tsv- formatted files into arrays in LabVIEW.
thats it thanks so much! 
This is something that annoys me with LabVIEW. Pallette icons looking different when placed on the block diagram. When I started out it really confused me, especially with arrays
Press ctrl-h to bring up the context help. 
Once you do, hover the cursor over the vi you are puzzled about. There is often a link in the context help window that brings up the full LV help system with a full description of the vi and its inputs and function. 
It's somewhat necessary for functions that apply to both 1D and 2D arrays because the extra dimensionality requires additional inputs to properly specify the behavior of the function.
Press Ctrl+h and then hover over any element for a full description. 
The sooner you make it a habit to use the context help window (ctrl-h) the better your LV development experience will be.
+1. There are a lot of nodes, you'll have to use the context help window eventually...
To be honest, just knowing that you are able to run it successfuly is very good news. I wasn't sure I had made the correct choice, and the money is already spent. I've tried tweaking resolution configs but no success yet. 
I think the second is "array subset" and the third is index array, as you said. Similar but not quite the same. 
I thought so too at first but the second picture has an index input for the second input. Array subset has a length input for the second input instead of an index input.
What's the issue with using control H? Do you want more info on how these come together? 
It really depends on what level of security you're looking for and what exactly you're trying to hide. If you're just trying to prevent curious eyes from casually glancing at the screen, then you could hardcode the password in a string constant on the block diagram, create a dialog that prompts for the password, and only displays certain content if the password was correct. Then password-protect the block diagram of the VI. If you're looking for a robust solution that allows for multiple configurable accounts and would stop a LabVIEW CLA from being able to see the screen given an infinite amount of time with your code, then things get more complicated.
Create a file with login credentials (how you secure the file is up to you), and once a given user logs in, load their name into a shift register. Use an event structure to capture button clicks that call a screen change state (easily done with a typedef tab indicator), and check the password against the requested screen in the screen change state. If the user has the appropriate access, allow the change. If not, either pop up an ACCESS DENIED dialog or (more flexible option) enqueue a call to an operator message state and pass the ACCESS DENIED message to it as variant data. Sorted.
Typically I store passwords in a configuration file in a hashed form. You can use the MD5 hash that's in the OpenG toolkit. For a little added security you should "salt" the password with some extra text. I make a dialog VI that prompts for the password. When the user enters one, hash it and compare it to what is in the configuration file. If you have a match then proceed to navigate to the protected screen or launch the protected VI. Make one such protected screen or VI that has a field for the user to enter a new password. If they enter a new password and click a "Save" or "Change Password" button, hash what they entered and replace the value in the configuration file. Beyond that, you could implement a whole user authentication scheme with usernames and passwords.
On mobile so I'm just going to link to places this had been asked before. Basically I will rely on others systems whenever possible. I don't want to work about how often a password expired, or rules or whenever. So I use the DSC toolkit from NI and then enable or disable buttons based on the user credentials. Or even better use Windows. Then you can even have the domain control what features of the software each user has. https://lavag.org/topic/16072-password-security-in-labview/ https://lavag.org/topic/15260-labview-and-windows-authentication/
If you‚Äôre just looking for something simple, you can open the VI &gt; File &gt; VI Properties &gt; Protection &gt; Password-protected and that will prevent the VI to be edited and the block diagram from being viewed without the correct password. I‚Äôve never used it before though, I just know it‚Äôs there (in 2010 so I assume later versions).