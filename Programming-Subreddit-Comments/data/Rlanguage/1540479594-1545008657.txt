The panda package in Python is pretty similar to R if you are comfortable with Python. That could be a good start for your situation. 
You can also check out RStudio, which is a GUI interface that runs on top of your R session. 
I read this title as "Thinking about switching FROM R TO EXCEL" and my heart fluttered. You still won't be able to insert new rows with clicks in R, since it is command line driven for r-base-core. That said, R will guide you in the right direction and will most likely be a faster workflow for you. It will probably force you into a better strategy. [You can paste directly into an R dataframe](https://www.dropbox.com/s/ydxwc9rrtudnpte/R-paste-df.mp4?dl=0), but it is not recommended. R will let you copy, rename, and move excel files. It will also let you run an analysis and write back to an excel file, separating results into tabs. If you can get the information in the same way each time, you should be able to run everything programmatically. 
This. As an analyst, I use both R and Excel. I am not a big fan of Excel, so I do as much work as I can in R and make it presentable in Excel. R is just too powerful to not use, and automating the boring stuff saves me a lot of time. 
I think you are close to having a fundamental epiphany regarding how to think about data. "Fat finger" typos resulting from manual entry of data or formulae into cells in Excel have wreaked havoc in the fields of finance, economics, and all areas of academia, among others. Some people think these kinds of errors may have significantly contributed to the 2008 recession! In R (and other programming languages like Python), you should only ever "touch" your data with code. Data import, manipulation, and analysis can (and should) be saved to script, so that at all times you have a reliable record of everything that has happened to the data from the moment that you imported it. Sure, typos can still happen, but they are easier to detect and fix. Furthermore, you can recreate the entire analysis any time you want, by re-running the script. You can also far more easily re-use code chunks and functions that you've written in the past. Things like inserting new rows (although obviously still possible) often become less important, because your mindset has shifted away from the "enter by hand" mentality. In Excel, on the other hand (as you know), often the various steps in analysis are difficult to re-create exactly. Everything is kind of "on the fly"--you can spend many hours entering data by hand, tweaking it (adding rows, for example), writing formulae into cells (the whole time potentially making mistakes or typos which are almost impossible to find later on), and if you later on want to go back and fundamentally change certain aspects of the analysis, or re-use your approach on a future project, unless it's similar enough to be able to be able to use your spreadsheet as a template, you're out of luck. In short, code-based analysis is "safer"--you're much less likely to make costly mistakes!
You don't need to copy and paste rows in R. You can reference rows by position, which is better! In Excel, if you want Row 5321, you have to find it, click it, copy it, and paste it. In R (with the dplyr library), you can just do dat %&gt;% slice(5231) to access the row, then do whatever you want with it!
You will figure out really quickly how badass R is, and how much safer it is to use than excel. If you’re really serious you should but a datacamp subscription and take as many r classs as you can. That’s far and away the best way to learn, as you are coding as you go. Even for experienced r users it’s a hugely positive experience. 
Also this, I used to spend 4+hours a week just generating reports that barely anyone pays attention to. That same work now only takes me 5 minutes with me firing up my laptop, logging in, clicking my batch file, chatting with coworkers, and when I come back it's done. All I need to do then is make graphs in Excel. Creating these reports is also how I got to learn r fairly well, I'm no expert by any means but I know enough to do what I need to do.
I’m curious about your reports. Are they markdown? Did you used to have to put them into a Word doc?
In your ggplot call you subset the data by a specific site. So if you facet it based on site later on there isn't going to be any other values in site to facet on. I don't have rstudio handy, but remove your subset call in ggplot and then try your facet call
Thanks for the response. I tried that as well, I should have mentioned that in my post. Subsetting the data by site still screws up the plot. If I don't include Site whatsoever then I get a really nice figure. As soon as I add in Site the actual boxplots go away (including the fill) and I'm left with a sold black line representing the mean. This is the case whenever I try to add in a third factor, so I don't think the issue is with the Site column
If you only have one value per variable combination you won't be able to draw a boxplot. It looks like that is the case from the head(). Without having a reproducible example we probably can't help you more than that.
I've used `R` for over a decade, and I never knew about the `example()` function until now. I guess there is always something to learn! Thanks for posting!
Hmm, i think I see what you're saying. I have 4 sites, with 3 treatments at 3 days at each site. What I showed in #head(mydata) are all levels for the first site. So you're saying I don't have enough replication to make individual boxplots for each site?
Thanks for the input. But I hope to get a general R command, for cases not limited to just plot(… , pch = … )
In general such a function *does* exist: `args`; You can say `args(functionname)` to get a list of the function’s arguments. Unfortunately this won’t work well in the specific case of `plot` because it just accepts arbitrary arguments via `...` and forwards them to the `par` function (which, in turn, also accepts arbitrary arguments). That said, you can run `par()` directly. This will show you the *currently set* graphical options. But unfortunately this isn’t a complete list. You will, after all, need to consult the documentation to find all its possible values.
I’d generally agree but Lisp famously has both. There’s definitely a place for facilitating computations on the language (which is what macros allow in the general case) in addition to computations on values. It’s just that, in the case of R, functions *can* actually do that too.
Lisp and its various dialects are essentially the only (mainstream) functions that have both functions and macros.
How does lisp differentiate between a function and a macro?
Syntactically you define a function via `defun` and a macro via `defmacro`. Semantically macros don’t evaluate their arguments, so you can perform computations on their expressions (which is easy in R since everything is an S-expression). R does something similar if you pass function parameters to `substitute` (or equivalent). If you write an R function where you `substitute` all its arguments, you essentially have a Lisp macro. This works because function arguments in R are *promises*: they are only evaluated once used, and carry with them both their actual expression at call site, as well as the scope of that expression.
Thanks!! That makes sense. 
I get the error that R does not recognise || so I am asking what the equivalent of | in R is.
huh...is `abs(sin(x))` not working for some reason?
In interest of helping you figure it out, what would |x| normally represent in math?
Absolute value. You can do it in R by the function `abs()`
Just put `abs(sin(x))` See that the | | is just the absolute value.
I was asking OP to try to help them figure it out on their own, but seems like people just gave the answer outright :P
Is `plot(eq(seq(-3,3,by=0.001)), type='l')` what you're looking for. I hope this isn't a troll given the resulting figure :) 
Thanks! nope not a troll just going to send it to my work colleague!
You can also do `curve(eq, -3, 3)`
Anyway to combine multiple functions to make some sort of bukakke?
I won't say this is the best way, but it's one way. i &lt;- sample(LETTERS[1:5], 1) df &lt;- droplevels(df[df$parts != i,]) df.l &lt;- split(df, df$parts) df.l &lt;- lapply(df.l, function(x)x[-sample(nrow(x),1),]) df.l$make.row.names &lt;- FALSE do.call(rbind, df.l) &amp;#x200B;
You can use the `axis` function with the `pos` argument to move the axis to the middle of the graph. Then you can manually add arrows using the `arrows` function. It's a pain to get the arrows to extend beyond the axis lines, but here's one way to do it. n &lt;- 100 x &lt;- seq(-20, 20, length.out = n) y &lt;- seq(-.0002, 0.0002, length.out = n) plot(x, y, type = "n", ylab = "", xlab = "", axes = F) axis(side = 2, pos = c(0, 0), labels = NA, tcl = -0.2) axis(side = 1, pos = c(0, 0), labels = NA, tcl = -0.2) arrows(0,min(y),0,max(y) + diff(y, lag = 4)[1], code = 2, length = 0.1) arrows(min(x),0, max(x) + diff(x, lag = 2)[1],0, code = 2, length = 0.1) lines(x = x[x&lt;0], y = 1/x[x&lt;0]^5, col = "blue") lines(x = x[x&gt;0], y = 1/x[x&gt;0]^5, col = "blue") &amp;#x200B; &amp;#x200B;
Use the duplicated function. It will return a logical vector where TRUE are duplicates. You can then subset the original list. Something like: duplicate &lt;- m_numbers[duplicated(m_numbers)] Should do the trick. Generally you want to avoid for loops in R, and take advantage of its vectorized nature.
Thank you! 
Here's one possible way with dplyr: library(dplyr) df %&gt;% filter(parts != sample(parts, 1)) %&gt;% group_by(parts) %&gt;% filter(row_number() %in% sample(n(), n() - 1))
No, the reports are generated from IT generated reports from my company. I used to have to manually click through those reports with a gillion steps to get what I needed from each one. Now I have R do the work for me and export the numbers I want to an excel spreadsheet and I copy that data into weekly reports I send out with graphs and such. I am nowhere near markdown yet.
Ah, so if I follow your company generates many different files, possibly excel format. You need various bits of data from many of these files to make a more customs report for your department. Manually cutting and pasting. If I’m right that is yucky. 
You are correct, it is much better now :-)
Here's a step by step walk-through of base R code that accomplishes exactly what you asked for. http://rpubs.com/thaufas/433944
I found your code to be remarkably succinct and elegant. The idea to use `split()` didn't even occur to me. After walking through your code, I realize that I misunderstood the OP's original request. After OP dropped all rows corresponding to one factor, I thought OP only wanted to drop 1 row from the full data set, meaning that if there were 20 rows (5 rows x 4 factors), OP wanted to randomly drop 1 row, for a total of 19 rows remaining. After walking through your code, I made some interesting observations, and I have a question about your code. Listed below is a full set of working code that contains OP's original code, your code (with some embellishment), and some additional code from me. The block labeled `# Option 1` is the heart of your original solution. *Can you help me understand how the* `df.l3a$make.row.names &lt;- FALSE` *works?* Although this elegant way of reassembling the data did not occur to me, after studying it, I still don't understand how it works. The `do.call()` function is one of those R super-functions that I just never think to use. After reviewing the documentation for it, the way I would have worked in the `make.row.names` parameter is illustrated in the code block labeled `# Option 2`. If this code doing the same thing as your code? Incidentally, after I saw you use the `split()` function, I wondered why you didn't use the `unsplit()` function to reassemble the data. After trying to do just that, I came up with the monstrosity in the code block labeled `# Option 3`. The way you used `do.call()` with `rbind` is so much more compact, easier to read, and maintainable! ### Example Code # OP's original code set.seed(1) parts = c("A", "B", "C", "D", "E") stuff = c( rep("A",5), rep("B", 5), rep("C",5),rep("D",5),rep("E",5) ) observations1 = c(rep(1,5), rep(2,5), rep(3,5), rep(4,5), rep(5,5)) df = data.frame(parts = stuff, obs = observations1) df set.seed(2^5) i &lt;- sample(LETTERS[1:5], 1) df &lt;- droplevels(df[df$parts != i,]) df.l1 &lt;- split(df, df$parts) df.l2 &lt;- lapply(df.l1, function(x)x[-sample(nrow(x),1),]) # Option 1 df.l3a &lt;- df.l2 df.l3a$make.row.names &lt;- FALSE df.l3a do.call(rbind, df.l3a) # Option 2 df.l3b &lt;- df.l2 do.call(rbind, c(df.l3b, make.row.names = FALSE)) # Option 3 factorSplits &lt;- as.vector(unlist(lapply(df.l3b, function(x) { as.character(x$parts) }))) factorSplits unsplit(df.l3b, factorSplits)
What you’ve done here is make a 4 by 4 matrix of df$col2. df$col2 is c3 which is the numbers 1:16 in order. If you want something different, you need to change that order first. But the way you’re doing this, is very complicated. What don’t you just make a matrix then select the rows and columns in the order you want?
&gt; *Can you help me understand how the* df.l3a$make.row.names &lt;- FALSE *works?* It just adds make.row.names = FALSE as an element to the list object, because [do.call](https://do.call) requires a list of arguments. I should have just done what you did in Option 2! I see now it's mentioned in the example in the man pages for [do.call](https://do.call). That does the exact same thing as my code. 
&gt; What don’t you just make a matrix then select the rows and columns in the order you want? I would like to - but I need to be able to perform anova with the data in the ordering of the matrices
So make the matrix then pull the data back out with `as.array()`? Check out this [blog post](https://www.r-bloggers.com/latin-squares-design-in-r/amp/) which I think is doing what you want. 
I didn't realise it would be so hard :&lt;
You create a Latin square from the factor levels of your treatment, not your data. Then you use the Latin square design to run your experiment and collect the data. The agricolae package has a function for generating Latin squares. Example: &gt; library(agricolae) &gt; trt &lt;- c("a","b","c","d") # 4 levels of trt &gt; lsd &lt;- design.lsd(trt) &gt; lsd$sketch [,1] [,2] [,3] [,4] [1,] "c" "d" "a" "b" [2,] "d" "a" "b" "c" [3,] "b" "c" "d" "a" [4,] "a" "b" "c" "d" &gt; lsd$book plots row col trt 1 101 1 1 c 2 102 1 2 d 3 103 1 3 a 4 104 1 4 b 5 201 2 1 d 6 202 2 2 a 7 203 2 3 b 8 204 2 4 c 9 301 3 1 b 10 302 3 2 c 11 303 3 3 d 12 304 3 4 a 13 401 4 1 a 14 402 4 2 b 15 403 4 3 c 16 404 4 4 d Running `design.lsd` repeatedly without setting the `seed` argument will churn out random latin square designs. &amp;#x200B; &amp;#x200B; &amp;#x200B;
&gt; OP, I am not clear what you are trying to achieve here understandable , I've edited the original post, hopefully it's more clear what I'm aiming for? The final code example is perhaps the most illustrative 
Thank you. This (leveraging a library) is probably the most sensible approach by a long shot. I have a question about laying things out though. [From the docs]( https://cran.r-project.org/web/packages/agricolae/agricolae.pdf ) I can set randomisation to false, which gives me a standard latin square &gt; lsd &lt;- design.lsd(trt, randomization = F) &gt; lsd$sketch [,1] [,2] [,3] [,4] [1,] "a" "b" "c" "d" [2,] "b" "c" "d" "a" [3,] "c" "d" "a" "b" [4,] "d" "a" "b" "c" &gt; I would like to display that in the format that has `treatment ( observation ) `, is there an easy approach to this? Or does it require a custom function of sorts? Is it possible to randomise the rows, *then* randomise the columns? And to display the process of doing so. I can see the example that you've given, but I'm not sure how to go about putting my own observations into this function. &gt; lsd$obs = seq(1:16) &gt; lsd$book plots row col trt 1 101 1 1 a 2 102 1 2 b 3 103 1 3 c 4 104 1 4 d 5 201 2 1 b 6 202 2 2 c 7 203 2 3 d 8 204 2 4 a 9 301 3 1 c 10 302 3 2 d 11 303 3 3 a 12 304 3 4 b 13 401 4 1 d 14 402 4 2 a 15 403 4 3 b 16 404 4 4 c &gt; 
That's really helpful. Thank you for the explanation.
https://uqkdhanj.wordpress.com/2015/02/10/dijkstras-shortest-pathway-algorithm/
I know it is not elegant but is there not a way to append at the end (bottom) of your matrix a column (line) full of random values, then sort it by those variables and finally remove them from the data.frame?
If I understand your data correctly, you should be able to ‘rbind’ your year dataframes and then create new variables using ‘case_when’ in ‘dplyr’ combined_df = rbind(year1, year2, etc) combined_df$newvar = case_when(x1 = 1 ~ “one”, etc) 
[https://cran.r-project.org/web/packages/xtable/vignettes/xtableGallery.pdf](https://cran.r-project.org/web/packages/xtable/vignettes/xtableGallery.pdf)
I can also recommend (stargazer)[https://www.r-statistics.com/2013/01/stargazer-package-for-beautiful-latex-tables-from-r-statistical-models-output/]
 print(xtable(A, type = "latex"), file = "output.tex") where A is the matrix. This is how I get my basic matrix output in LaTeX and then I make the small changes afterwards. If you want to get rid of the decimal places, use print(xtable(A, type = "latex", digits=0), file = "output.tex") or you can specify a vector that contains the length of each column.
I wish I had known about this package before. This is so much easier than xtable.
If it doesn't work with ggplot then I DONT WANT TO USE iT. 
This is really cool. I'm not sure what I'd do with it because I've forced myself to solve all my problems with 2D graphs. But if this was available, I think I would start to use it, maybe to replace side-by-side graphs.
I like python's matplotlib and would love to see some sort of a port or mimic. Go for it!
Good Amit link!
We did something similar trying to find a path, albeit a little less efficient. Our data was not a matrix, rather it was "a set of possible values each station could take." ex. station 1 possible values: 1,2,3,4,5 station 2 possible values: 2,5,3,7,4 etc... we solved this by using an expand.grid on the whole thing (creating a matrix like yours) and then pruning "impossible paths" (ours couldn't be more than 3 entries away from its current row, yours seems that it can't be more than 1 entry away from its current row), and then just testing each path to see if it's the lowest. It's extremely easy to implement, and for small data runs pretty fast. I don't know how big your matrix is, ours was 43*11. If this seems like something that may work for you, let me know, and I'll mock up some code (it's from work, so I have to de-identify it a little)
It's a long shot but you could always try to contact the maintainer (rstudio) they might want to share some insights.
Archive.org might have some interesting websites cached... something like this: https://archive.org/search.php?query=r%20shiny
Hmm interesting. I remember doing some crazy formatting stuff with it years ago to make a special case work, but other than that I had good experiences with it. I'll try the linked alternatives in the future, however I'm not using too much latex these days.
Yes. I use both and R has made it possible to do much more complex tasks. I use excel to share/pretty up the analysis
Yes rstudio is a must also checkout rmarkdown
You can always ask Joe Cheng, who developed Shiny. He still works for RStudio. https://twitter.com/jcheng
What do you mean by the normality of the data?
Hi, thanks for your reply ! So I have done a Normal Q-Q plot and it's not really clear, so I would like to find the kernel density estimator to see if the data is normally distributed or skewed. So is it not possible to see if the data is skewed or not with the KDE ?
The need to check residuals is to verify that there are no systematic patterns in the distribution of Y "after" taking X into account. If there are any pattern that means there are other factors at work that have not been accounted for by the model, it's a poor model. &amp;#x200B; Residual is typically expected to have a gaussian/normal distribution
I mean go ahead and plot the KDE. Normally distributed vs skewed aren't your only two options, by the way. I don't know what purpose you have for verifying normality - how sensitive the methods you care about are to departures from normality and what type of departures you care about. But any plotting method (QQ plot or plotting histograms or densities) is only going to be an "ocular pat down" of your data (well, there are some visual methods of testing but this isn't helpful to you). 
or, in base, with data in a vector called `x`: d &lt;- density(x) plot(x)
For a graphic: `plot(density(residuals))` For a hypothesis test: `shapiro.test(residuals)` Just leave kernel density (and `y`) alone. 
That's sort of what sparked the idea. For one problem on the assignment, I was asked to make a simple points() plot of a function of one variable at four levels of another variable. I thought making a continuous contour-like plot would be much more interesting. 
I've never used ggplot but it sounds like it's better suited to plotting sets of data points rather than continuous functions. At some point I want to enable my script to generate fitted curves to plot data points with a continuous visual representation, but I'm not there yet...
UPDATE: I've finished an auto-calibration feature for the z scale that seems to work pretty well and can be enabled with the argument "calibrate = TRUE". I think I've polished out all of the bugs I can find for everything I've included so far, and retooled a few of the functions to be more reliable and efficient in the process. In addition to the calibrate feature, I've also included a "by.z" option that, if TRUE, generates the image by z level rather than by x-coordinate. This is not of much use in most applications, since the majority of the processing time is consumed by generating the z-matrix. However, it will save significant time if you only have, say, 100 levels of z, and plan on generating multiple very high resolution plots from the same function. Up next are the graph-y graphical parameters, which should only take a work-day at most to write and debug. Once those are finished, I'll consider packing everything up into a shareable package for people to try out. There are a number of smaller functions included with this project that I haven't mentioned, and may be useful on their own for specific applications. For example, one of the simplest yet most important functions is something I named adj() which adjusts the values in an object to span from 1 to z, and can be configured to leave the values as decimals or round them to integers. The function I made to scale the z axis, sampler(), is also something that might have surprising niche uses. I'll make an updated post in a few days with more details. On a different note, another class of mine has inspired me to add another level of complexity to my project by creating a new host of more advanced features that I dont think exist in any program I've heard of yet. In short, my ultimate goal with this will be to generate a 3D contour plot given only a few images of graphs. Once I've determined it's actually possible using the approach I have in mind, I'll make a separate post for that. 
do your own homework
We can help you, but you have to tell us what approaches you've tried already. No one wants to help you if you haven't even attempted the problem.
Tried finding the difference of consecutive values and framed a regression model, but the test data isn't matching well. 
Probably recode the NA's to a binary or two level factor. ARIMA/Logistic.
apply works on arrays that have as many dimensions as you like, so you can just use apply.
So I could say apply(x, 3, sum) to refer to the third dimension? 
Yep &gt; observations &lt;- array(1:60, dim=c(3,4,5)) &gt; observations[1,1,1] [1] 1 &gt; observations[1,1,2] [1] 13 &gt; observations[1,1,3] [1] 25 &gt; observations[1,1,4] [1] 37 &gt; observations[1,1,5] [1] 49 &gt; result &lt;- apply(observations,3,function(x){ + return(x[1,1]) + }) &gt; result [1] 1 13 25 37 49
I would think that you would use the function to generate points then use ggplot. Ggplot can already make your graph easily, so I think you are trying build a package for a problem that has already been solved. 
How do you know that the premise is true - that you even can predict their dropout chance based on GPA? 
The dataset has an NA more likely when their marks drop. Likely in the sense 95%
True. I built this knowing well that there might already be a function just like this out there. This has all always been a personal project, just a personal project gone far enough to want to share with people. 
How would recoding NA to a binary help? These are scores. Recoding then to 0 or 1 or any arbitrary n level factor would distort the data if it is done at an observation level and transforming the variable (col) to binary (i.e. gpa available or not) will lead to a loss in data. I’d suggest dropping sem columns where a lot of values are NA (say 30% or more is missing) and impute the NAs row-wise. Also, considering that drop out rate is generally smaller (more 0s than 1), Poisson or Negative Binomial are also suitable choice models. This would have to be validated in the data as this doesn’t seem to be a real world data set. Who can study for 15 years! Check out the auto.arima function which is pretty cool for ARIMA models If OP didn’t have so many independent variables, I’d suggest transforming it into panel data and use interactions of first differenced variables(gpa) as predictors which is more in line with the hypothesis.
you do have to be a little careful about the shape of the final output, but, yeah. colSums and rowSums also work but it's a little tricky, you have to read the documentation and experiment.
 library(ggplot2) library(dplyr) x = seq(-100, 100, by = 1) y = seq(-100, 100, by = 1) df = expand.grid(x = x, y = y) df = df %&gt;% mutate(z = y^2 - abs(x^3) * y^2 + abs(y)^.5) df %&gt;% ggplot(aes(x = x, y = y)) + geom_raster(aes(fill = z), interpolate = TRUE) + scale_fill_continuous(type = "viridis") + theme_classic() 
library(ggplot2) library(dplyr) x = seq(-100, 100, by = 1) y = seq(-100, 100, by = 1) df = expand.grid(x = x, y = y) df = df %&gt;% mutate(z = y^2 - abs(x^3) * y^2 + abs(y)^.5) df %&gt;% ggplot(aes(x = x, y = y)) + geom_raster(aes(fill = z), interpolate = TRUE) + scale_fill_continuous(type = "viridis") + theme_classic() 
You should add that as a comment on the actual post so anybody else who doesn't know can see it. My goal with this project isn't to plot a particular dataset, it's a personal challenge to write a function for this myself. But that might be useful for anyone who came to this thread thinking that something like this didn't already exist. That said, if you have any ideas for features I can add that would make it an improvement over the ggplot version, please do suggest them. Part of my goal is to make this function as versatile and customizable as possible while still being intuitive and easy to use. 
How do you know that? You should plot the distribution of the drop that occurs before the NAs and the distribution of the drop that occurs before other points to compare. If it's separable, then you should be able to use logistic regression and get a solid result.
Use all() or any() inside the if statement.
It's good to challenge yourself. You had asked for feedback regarding the usefulness of your functions: I am providing you with a benchmark to compare your work against. The simple answer is that no one will use your package, and your time would be better spent learning R programming before you try to reinvent it. 
That's not quite what I was asking for. I asked if anyone was interested in it, and then if anyone would want to use it. To another user, and to myself in the future, I can already see why I'd rather use my function over the script you shared. It's just a lot simpler and more intuitive. It's a single line of code, just one function and as little as three arguments. The majority of students in science and math programs don't know anything about how to use R, and normally wouldn't care enough to learn, but this is simple enough for just about anyone to use. For that reason I have hope that my creation might be used by someone else somewhere. As for the second sentence, and I don't mean to be rude, but I really don't think it's your place to tell me what to do with my time. The best way I learn is by doing, and even setting aside the fact that I'd have no reason to want to learn more R if I hadn't started this project, I'm learning a lot more by working on this than I would be if I was just flipping through a textbook or two weeks into a course. To be honest, I'm actually quite confused as to why you seem so adamant that I shouldn't be doing this. Why, and for what reason? I asked for feedback about whether or not anybody would be interested in what I was working on, not for criticism of my decision making. And while "nobody cares" does answer my question, it's not really an appropriate response in this context. I do appreciate the insight into ggplot, though. I may use it for other things later, and/or as a benchmark to compare my product to. 
I started having the same problem with my code. Var &lt;- c(1,2,3) I then use Var in a sprintf function holding SQL code: Qry &lt;- Sprintf(“select * from blah where blah in (‘%s)”, Var) When I run Qry through sqlquery the results I get are only for 1 and not 1,2, and 3. It was driving me mad today because it was working other times I’ve done it but today it didn’t.
 ggplot(df, aes(x=group, y=probability, fill=result)) + geom_bar(width = 1, stat = "identity")+ scale_fill_manual(values=c("grey", "red")) The values correspond to the order of levels that would be created out of your `fill` variable. 
https://dplyr.tidyverse.org/reference/lead-lag.html
Thanks for the reply! Is it possible to lead() or lag() on a yearmon type column? 
Here is some code that works, let me know if you have any questions. &amp;#x200B; sample$price\_diff &lt;- NA &amp;#x200B; foods &lt;- unique(food\_type) #get all possible food values &amp;#x200B; for (i in 1:length(foods)) { int\_food &lt;- foods\[i\] # look at the first food ind &lt;- grep(int\_food, sample$food\_type) # find all occurrences of that food type diff &lt;- diff(sample$price\[ind\]) #diff find the difference between the value and the next one and prints in a vector for (j in 1:length(diff)) { # now lets fill those differences in... ind &lt;- ind\[-1\] #remove the first instance of that food occurance from our initial index sample$price\_diff\[ind\]&lt;- diff\[j\] # fill in the rest of the diff values into their spots in the df } } &amp;#x200B;
This works, may not be the most elegant solution. Let me know if you have questions. &amp;#x200B; sample$price\_diff &lt;- NA #initiate new column &amp;#x200B; foods &lt;- unique(food\_type) # find all foods &amp;#x200B; for (i in 1:length(foods)) { int\_food &lt;- foods\[i\] #get first food ind &lt;- grep(int\_food, sample$food\_type) #find all rows of df that are this food diff &lt;- diff(sample$price\[ind\]) #take a difference of the vector of these food prices. This returns 1 less value than the prices ind &lt;- ind\[-1\] #remove our first index for the instance of that food sample$price\_diff\[ind\]&lt;- diff #now fill in our differences with the modified index }
Why do people ask these kinds of questions here? You’ll get a better answer faster on Staxkoverflow.
This sounds like you should use an event history model. You could reshape to three columns (ID, GPA, and whether dropped in next semester) and do a logit but this wouldn't capture individual effects. That is, it would treat different semester observations of the same individual as different people. An event history model would give you a time-to-failure model for each ID where failure is dropping out.
You might find [this discussion](https://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio) useful.
Your odds ratios are just *e* to the power of your coefficients. So you can easily get the OR for all coefficients with: `exp(coef(x))` Or even this for both: `exp(cbind(coef(x), confint(x)))`
Just what I needed, I knew I was over-complicating it. Thanks!!
Thank you
Yep! Just copy and paste the following anytime you need the ORs quick. #extract OR and CI ors &lt;- exp(coef(model1))[2:3] cis &lt;- exp(confint.default(model1))[2:3,] cbind(ors, cis) Where \[2:3, \] refers to however many coefficients you need to get the OR for. So, if you had more terms, it might be \[2:6, \] or something!
No reason to exclude the intercept, really.
If I'm understanding your problem correctly, my solution would just to use the hist function. If you have the "edges" of the quantiles, you could just do hist(data,breaks = &lt;edges&gt;,plot = FALSE)$counts 
u/mvhcmaniac has a good suggestion of using `hist`. As an alternative, if you wanted to continue with your function: 1. You are cutting `vec` based on the quantiles of `vec$cumRet`. You probably want to cut `vec` based on the quantiles of `vec` itself. 2. Using the `table()` command will count up the number of observations of each category. 3. Your `aggregate` call is a bit excessive. You are defining a function that calls your new function. So something more like: someFunc = function(vec){ cats &lt;- cut(vec, breaks=c(quantile( vec, probs = seq(0, 1, by = 0.1))), labels=c("0-10","10-20","20-30","30-40","40-50","50-60","60-70","70-80","80-90","90-100")) table( cats ) } # Check it gets what we want someFunc( df_old$cumRet ) aggregate( cumRet ~ Month , df_old, FUN=someFunc )
This is great, and I am glad people are looking into creative ways to enhance Shiny. Environments are powerful.
Nothing to add about the article. I ended up moving away from Shiny for some bigger projects and just used R as a JSON endpoint essentially. I ended up creating a hash of the request and saving the response under that [hash].json (either in memory or as a file). Not sure if that will help you on any projects, but it helped me a lot. 
&gt; cats &lt;- cut(vec, breaks=c(quantile( vec, probs = seq(0, 1, by = 0.1))), &gt; labels=c("0-10","10-20","20-30","30-40","40-50", &gt; "50-60","60-70","70-80","80-90","90-100")) &gt; table( cats ) Hey, thanks for the suggestion. Why do we call someFunc(df_old$cumRet) before the aggregate call? 
Ahh, I meant to have a comment there. When I'm developing functions like that, I tend to have a test case to make sure the output is what I want it to be - double-checking that it's working on a single case before then applying it to a more complex one. 
Oh, that makes sense. Also, can you please tell me a bit more about how hist() can be used here? I am trying to do so by — hist(df_old, breaks=quantile(df_old$cumRet, seq(0.1,1,0.1)) but it returns a blank histogram? 
Here's the problem, I want my for loop to generate sqlite queries and send those queries to the database and which would yield a result. I intend to store the result in a variable called "total". However, from the output I observe, the query statement that is being generated in the for loop isn't syntactically right. How do I fix this? 
I found this as well that might be useful. [https://www.rdocumentation.org/packages/questionr/versions/0.6.3/topics/odds.ratio](https://www.rdocumentation.org/packages/questionr/versions/0.6.3/topics/odds.ratio)
&gt; hist(data,breaks = &lt;edges&gt;,plot = FALSE)$counts Hi, thanks for the response! I tried this by doing — hist(group_by(df_old, Month),breaks = quantile(df_old$cumRet, probs = seq(0.1, 1, 0.1), type = 5),plot = FALSE)$counts I did group_by because I want to group by Months, but it says "x must be a numeric", which seems odd because shouldn't group_by(.data, ...) give me a numeric/vector? 
Looks like its to do with the Dbsendquery. I haven't used that package so I can't help with that. 
What database package is that? Does it require you to commit the query? Most do... Also, unrelated but there is a `paste0()` function that doesn’t require you to say `sep=“”`. 
&gt; Edit: Also noticed that the sum of the values in the row of the output from cut/table approach don't add up to the correct value? For example in this output... I just noticed that as well. The `cut()` function has an `include.lowest` argument you can use to correct that. As for `hist`, something like: hist(df_old, breaks=quantile(df_old$cumRet, seq(0.1,1,0.1))$counts will get your counts as well.
How does this differ from the base R `lazyload` https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/lazyLoad
How do you get that? Anyway, Stackoverflow has an answer: https://stackoverflow.com/questions/21583382/r-how-to-lazyload-variables-from-inst-extdata-in-r-package
R is open sourced, here is the source for lazyLoad function https://github.com/wch/r-source/blob/5a156a0865362bb8381dcd69ac335f5174a4f60c/src/library/base/R/lazyload.R
Why don't you just group by egressGW?
I don't want to give the whole answer because this looks a lot like homework but I would start by building a for loop that ends at length(a). Then you can use an iterative variable to set the exponent values based on position.
Close but you're third argument is wrong in the paste call. a is a vector of coefficients so you should be raising to the index of a - 1. Try this paste(a\[a!=0\], "\*x\^",(1:length(a))\[a!=0\]-1, sep = "", collapse = " + ") Notice this doesn't quite work that as you will need to invoke a special method for first input of since it will appear as 2\*x\^0 instead of two. Also I removed the polyPrint calls since the above should be used within that function(you want the function to do it all for you. (Hint consider storing the the last n-1 elements of a into to a vector b and running the above with a slight modification and the adding on the first element.) 
Why not store the results in a data frame or vector? good &lt;- c() bad &lt;- c() for(region in regions) { # your code good &lt;- c(good, good_percent) bad &lt;- c(bad, bad_percent) } results &lt;- data.frame(region=regions, good=good, bad=bad) results &amp;#x200B;
Good call.
Thanks. Now, it works almost perfectly. I managed to remove the \^1 and the \*x\^0 from the result. The only problem is that I still cannot remove the 1\* from it. Could you please help me with that? My code now looks like this: `polyPrint &lt;- function(a) {` `b &lt;- tail(a, -2)` `c &lt;- paste(a[1])` `d &lt;- paste(" + ", b[b!=0], "*x^",(1:length(b))[b!=0]+1, sep = "", collapse = "")` `e &lt;- a[2]` `f &lt;- paste(" + ", e[e!=0], "*x"[e!=0], sep = "")` `g &lt;- paste(c, f[e!=0], d, sep = "")` `return(g)` `}`
my best guess would be to replace the 1's with "". Something like a[a==1] &lt;- "". I'm not positive that will work but let me know! Also you're solution is pretty slick I jumped straight to for loops and if statements, so nice job.
Read the documentation on optim()
https://www.r-bloggers.com/implementing-the-gradient-descent-algorithm-in-r/
It's relatively straightforward with `dplyr` and `ggplot2`: library(dplyr) library(ggplot2) df &lt;- &lt;your dataframe&gt; df %&gt;% group_by(Year) %&gt;% summarize(tot_val = sum(Value, na.rm = TRUE) %&gt;% ggplot(aes(x = Year, y = tot_val)) + geom_line()
Well this sure puts the fun in function! This is pretty cool, thank you.
The key to solving your problem is `dplyr()`. You could also solve it rather easily with `base::tapply()`, but `dplyr()` is actually easier to learn and so much more powerful. Let me know what you think. library(tibble) library(purrr) library(dplyr) library(ggplot2) # Generate the individual values set.seed(2^8) yearVals &lt;- 1920:2020 set.seed(2^8) t1 &lt;- tibble(yr = rep(yearVals, 100), val = purrr::map_dbl(yr, ~ rnorm(n = 1, mean = .x, sd = 0.2 * .x))) t1 #Plot the individual values p1 &lt;- ggplot() + geom_point(aes(x = yr, y = val, color = as.factor(yr)), data = t1, show.legend = FALSE) + labs(title = "Sample Plot - All 'Values", y = "Value", x = "Year") + geom_smooth(aes(x = yr, y = val), data = t1, method = "loess", formula = y ~ x) p1 # Sum the values by year t2 &lt;- t1 %&gt;% group_by(yr) %&gt;% summarize(sum_val = sum(val)) t2 # Plot the values summed by year p2 &lt;- ggplot() + geom_point(aes(x = yr, y = sum_val, color = as.factor(yr)), data = t2, show.legend = FALSE) + labs(title = "Sample Plot - Values Summmed by Year", y = "Summed Value by Year", x = "Year") + geom_smooth(aes(x = yr, y = sum_val), data = t2, method = "loess", formula = y ~ x) p2 [Figure 1](https://i.imgur.com/0sGxCWV.png) [Figure 2](https://i.imgur.com/a08HbiR.png)
Of course!! I’ve begun using stat_summary much more often than group_by and summarize. 
Exactly, a and b are no defined column names in your dataframe that is why you get the error message. &amp;#x200B; I also suggest using *aes\_string* instead of *aes* when passing a column name as an argument to a function. [https://stackoverflow.com/questions/22309285/how-to-use-a-variable-to-specify-column-name-in-ggplot](https://stackoverflow.com/questions/22309285/how-to-use-a-variable-to-specify-column-name-in-ggplot) &amp;#x200B;
I think latest version of ggplot2 works differently with non standard evaluation. 
I want to be able to pass different parameters of a data frame and plot them &gt; p &lt;- ggplot(data=data_f, aes(x = xi, &gt; y = yi)) + So a,b were part of the data frame, df$a, df$b
But they are not based on the code you shared. You need to look at how ggplot2 works with non standard evaluation. 
Look at the section on tidy evaluation https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/
I asked a similar question over here a while back: https://old.reddit.com/r/statistics/comments/89ug0l/using_monte_carlo_simulation_to_demonstrate_the/ You might find some of the responses helpful.
using aes_string didn't work http://vpaste.net/pKToc
```plot_box &lt;- function(data_f, xi, yi, xax, yax, tit) { # take in a data frame, the xi/yi data to plot. The yax/xax axis information, # and the title x_var &lt;- rlang::enquo(xi) y_var &lt;- rlang::enquo(yi) ggplot(data = data_f, aes(x = !!x_var, y = !!y_var)) + geom_boxplot(fill = "grey") + labs(title = tit, x = xax, y = yax) + theme_light() } plot_box(df, week, part, "Operator", "%P/T", "%P/T for operator")```
&gt; You need to look at how ggplot2 works with non standard evaluation is this something that can be done without the tidy verse? 
I'm trying to get this to work, what are the `!!` for? &gt; a = 3 &gt; !!a [1] TRUE &gt; Seems that they're logic? I'm not sure why they're needed here though. Also I get the following error from this code : Error in !x_var : invalid argument type here's the paste : http://vpaste.net/TNVQU Here's the section plot_box &lt;- function(data_f, xi, yi, xax, yax, tit) { # take in a data frame, the xi/yi data to plot. The yax/xax axis information, # and the title x_var &lt;- rlang::enquo(xi) y_var &lt;- rlang::enquo(yi) ggplot(data = data_f, aes(x = !!x_var, y = !!y_var)) + geom_boxplot(fill = "grey") + labs(title = tit, x = xax, y = yax) + theme_light() } # p1 = plot_box(df, week, part, "Operator", "%P/T", "%P/T for operator") p1
ggplot2 is in the tidyverse.
To be completely honest, I always have to just try different versions of quo, enquo, and !! every time I want to do this until it works. :)
&gt; Please research non-standard evaluation in R Thanks - I really don't know where to look for this kind of stuff it's quite new, so sorry if It comes across that I'm asking you to do everything. &gt; I'm not sure why it doesn't work for you. It works for me yeah, I've no idea :&lt; I think I'll just leave it for now and have a lot more verbose code, I'll try to bear this in mind for later though. thanks
Update ggplot2.
I downloaded the text file version, cleaned it to my preferences , then saved it as a .rda file. I imagine importing from Stata, SPSS, etc., might cause extra problems.
Won’t they be categorical variables? I’d have converted them to factors with their appropriate code assigned to levels()
I think the argument should be row.names=FALSE
Invalid [row.name](https://row.name) specification
Have you tried read.csv instead of read.table?
Yes, I use read.csv but in the error it says read.table &amp;#x200B;
Can you paste the full command and maybe the head of the file?
I generally have better luck with this package for reading files https://readr.tidyverse.org/articles/readr.html
Warning message: In rbind(names(probs), probs\_f) : number of columns of result is not a multiple of vector length (arg 1) &amp;#x200B; If i use NULL it doesn´t give me an error but all the cols are as one now
Cant help more without what your data looks like. 
\&gt; data=read.csv("mustela lutreola gbif/0018020-181003121212138.csv", header=TRUE, sep=",", na.strings="") Error in read.table(file = file, header = header, sep = sep, quote = quote, : duplicate 'row.names' are not allowed &amp;#x200B; This are the cols names that are creating the problem in my opinion: &amp;#x200B; &amp;#x200B; gbifID datasetKey occurrenceID kingdom phylum class order family genus species infraspecificEpithet taxonRank scientificName countryCode locality publishingOrgKey decimalLatitude decimalLongitude coordinateUncertaintyInMeters coordinatePrecision elevation elevationAccuracy depth depthAccuracy eventDate day month year taxonKey speciesKey basisOfRecord institutionCode collectionCode catalogNumber recordNumber identifiedBy dateIdentified license rightsHolder recordedBy typeStatus establishmentMeans lastInterpreted mediaType
Yes, I tried to read it doing a .txt file and this was the error there: &amp;#x200B; \&gt; read.table("mustela lutreola gbif/Mustela lutreola gbif.txt") Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 2 did not have 45 elements
The extension doesnt matter. No one can help you without seeing a sample of what's in the file.
But what line could i send to you to help me. I pasted in other message the col names 
Something like this should work: pisaScores[,3] &lt;- sub("..", "", pisaScores[,3])
What does this do? pisaScores[!grepl("..",pisaScores[["X2015..YR2015."]]),]
That will remove any occurrence of two *arbitrary* values since the regular expression `'.'` matches any character. Either escape the characters (`'\\.\\.'`) or pass the `fixed = TRUE` argument to `sub`.
Your approach is correct but `grepl` and other methods use [*regular expressions*](https://www.regular-expressions.info/) for search patterns. And the pattern `'.'` matches *any* character. You either need to escape it, o pass `fixed = TRUE` to `grepl`. Furthermore, see my comment further down about how to ensure that the *entire* string consists solely of two dots.
pisaScores %&gt;% magrittr::set_colnames(c(“Name”,”Code”,”Year_2015”)) %&gt;% dplyr::mutate(Year_2015 = Year_2015 %&gt;% as.character() %&gt;% stringr::str_replace_all(“[..]”, “”) %&gt;% as.numeric())
O wao我也在这个行业。我想我可以从你那里学到很多东西。我在 [https://www.magicessay.org/](https://www.magicessay.org/) 在线网页上看到了你网页的广告，我真的非常喜欢它。 
That s true. Sorry my bad.
Instructions are a bit unclear. But these two should cover what you probably want: ``` library(stringr) library(dplyr) df &lt;- data.frame(a = c(1:5), b = c("Words are Here", "Okay", "Be Good Now", "WTF are you Doing", "it's good")) # Concatenate elements within specific rows of a column df %&gt;% rowwise() %&gt;% mutate(capital_letters = str_extract_all(string = b, pattern = "[A-Z]") %&gt;% unlist() %&gt;% paste0(collapse = "") ) # Concatenate all elements of a column str_extract_all(string = df$b, pattern = "[A-Z]") %&gt;% unlist() %&gt;% paste0(collapse = "") ```
Yes, that helps to make it a character matrix that I would then have to concatenate all the cols of. Haven’t figured out how to do that.
gsub("[^A-Z]","", df$b) seems to work. Thanks.
`par(mfrow=c(2,2))` should come between `png(...)` and `plot(a1)`
ack, thanks
Just replace the ".." with NA: pisaScores[pisaScores==".."] &lt;- NA You should also probably convert the column to numeric instead of character: pisaScores$X2015 &lt;- as.numeric(pisaScores$X2015) &amp;#x200B;
Can you elaborate what you mean "unlayered"? &amp;#x200B; In general layers mean that if a pixel is plotted (say a background color) and another on top (say the line of a plot going over that background) that the pixel is storing information that first it is color A (the bottom layer), then color B (the top layer). An example of an image file that LAYERS is .png, .jpg would only tell you the FINAL layering (top layer). &amp;#x200B; By definition, a pdf is layered, because in stead of saying pixel A is a color, it stores everything as VECTOR! This means the first layer (background color) is a vector, and the line is its own vector. &amp;#x200B; If you want to have it be UNLAYERED, than a PDF is not the right format. If you want, just save it as a pdf, and then export that pdf as an image file (a .jpg) for example. &amp;#x200B; Does this make sense? &amp;#x200B;
This solved it, thank you!
So I didn’t even realize that there was a .txt version available, which, in retrospect, was dumb on my part. Now it’s all figured out lol
You should specify `[:graph:]` or `[:alnum:]` instead of `[:alpha:]`. The first is the same as alphanumeric + punctuation and the second just alphanumeric. `[:alpha:]` is just A-z (upper and lower case letters). Your second problem is that these should be inside a second set of brackets to treat it as a letter group: `[[:graph:]]+` matches more than one alphanumeric or punctuation character. To match a specific number of characters, use curly braces: `[[:alpha:]]{3}` matches three alphabetic characters. The last character should be `$` to match the end of the line. Otherwise, you could match more than three characters after the period. Finally, a period is actually a special character. It matches any single character. What you want is an actual period so you have to type an escape in the pattern string: `\\.` To return the emails matching the pattern: pattern &lt;- "^[[:graph:]]+@[[:alnum:]]+\\.[[:alpha:]]{3}$" emails[grep(pattern, emails)]
Try ifelse function, ifelse(values &gt;= 20, TRUE, FALSE)
In base R you could use the ifelse funciton: position &lt;- ifelse(values &gt;= 20, values, NA) This will give you a list with the actual values, and NAs for any value that didn't meet the condition, from there dropping the NAs should be simple. There is probably also a dplyr way to do this.
position &lt;- data[values &gt;= 20] position this should work.
That makes sense, thanks. 
Definitely check out the [lubridate](https://lubridate.tidyverse.org) package. It makes this kind of thing much easier.
Thank for for sharing...I feel your pain with a boss that clings to SPSS.
also thank mr skeltal for good bones and calcium[^*](https://www.reddit.com/r/tmsbmeta/)
I know, seriously. 
I dont really get: &gt;dat\[\[1\]\]\[2\] why are there 2 brackets around the 1 and only one around 2, what is the difference? also how would I iterate over the links?
Here's one way to do it in ggplot without reshaping the data. library(ggplot2) ggplot(convultions, aes(x = Dose_standard, y = Y.N.Standard)) + geom_point(aes(color = "standard")) + geom_point(data = convultions, mapping = aes(x = Dose_special, y = Y.N.Special, color = "special")) + labs(x = "dose", y = "Y.N.") + scale_color_discrete("Type") &amp;#x200B;
ah ok, that's ace, thanks
Something like gsub(“^https”,”http”,variable) 
cool I wanted to remove the s because I hough it would solve this problem: Error in if (substr(file.name, 1, 5) != "http:") {: argument is of length zero Traceback: 1. getIncome(1400000, 1) 2. getFinancial(income.descriptions, symbol, year) # at line 27 of file &lt;text&gt; 3. getInstFile(inst.url) # at line 66 of file &lt;text&gt; 4. XBRL::xbrlDoAll(url, cache.dir = "XBRLcache", prefix.out = "out", . verbose = FALSE) # at line 53 of file &lt;text&gt; 5. xbrl$processSchema(xbrl$getSchemaName()) 6. which(discovered.files == file) 7. xbrl$getSchemaName() 8. fixFileName(dname.inst, .Call("xbrlGetSchemaName", doc.inst, . PACKAGE = "XBRL")) but I am still getting the error, any idea why this is happening??
Hard to tell without the code, my guess would be that `file.name` is `NULL` for some reason.
&gt;reason can I post it here, or should I open a new question?
Note that this'll mess up if the site is something like https://https.com. Better to use `^https` to make sure you match the one at the beginning. 
`apt-get` isn’t supported on macs, just Linux. [here’s](https://github.com/tensorflow/serving/issues/493) a page providing a couple of work arounds. First, tho, I would suggest updating R, Xcode, and xquartz (and gcc, but I doubt that’s causing this issue).
[https://twitter.com/hadleywickham/status/643381054758363136?lang=en](https://twitter.com/hadleywickham/status/643381054758363136?lang=en)
Check the structure of the data, str(data.file). This often happens when the data is in the wrong format for analysis and plotting.
Do all the columns you're trying to transform start with pht? Or are there some like pht45 that you don't want to transform?
Try mutate\_at(). It lets you select which columns to map the values for with pattern matching. Assuming you want to manipulate a column if and only if it's name starts with "pht": data2 %&lt;&gt;% mutate_at( vars(matches("pht")), funs(mapvalues(., c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 888, 99, 999 ), c(30, 40, 60, 85, 125, 175, 225, 275, 350, 450, 625, 875, 1250, 1750, 2500, 4000, 7500, 12500, 17500, 20000, 0, NA, NA))) ) You might need to load magrittr for the `%&lt;&gt;%` but this is the same as `data2 &lt;- data2 %&gt;% mutate_at(...)`
For future reference, you can create a function and apply it to the data: data &lt;- c("high and low", "high and high", "moderate and low", "moderate and high", "low and high", "slight and moderate", "low and slight")) replaceValues &lt;- function(v) { high &lt;- grepl("high",v,ignore.case=T) moderate &lt;- grepl("moderate",v,ignore.case=T) slight &lt;- grepl("slight",v,ignore.case=T) low &lt;- grepl("low",v,ignore.case=T) if(high) { return(3) } else if(moderate &amp; !high) { return(2) } else { return(1) } } newdata &lt;- sapply(data, replaceValues) &amp;#x200B;
Plot the function as a line in ggplot2 and then again as a geom_area in the part you want colored in
Why `gsub`? It's only ever replacing a single instance.
If I say it is a remarkable effort then it would not be wrong. I think your [college paper writing service](https://uksuperiorspaper.com/) tips made many ordinary writers, extraordinary authors of this time. A good job is done. 
The plots are very nice. I guess I don't understand though, why was this needed? How different is it from ggplot's geom_tile, except that for ggplot you have to pre-calculate the response variable?
Yes. Rasters are 3 dimensional data layers. Essentially the x and y become coordinates and the z dimension is shown with different colors. You should compare your function to raster plotting to know how what you do is unique. Also, DEMs will be good datasets to test your plotting function with.
&gt; they are not related to this homework directly. You are probably never going to have a problem in which a perfectly matched example/solution exists in some forum. Part of learning R, or any language for that matter, is learning to seek out and find solutions to your problem. I think it is best to break your issue into smaller parts and look for solutions to each of those separately. From what I can see you need to: * Read data from a .txt file * Plot the data (I'm assuming an x, y scatter) * Find the average * Find the standard deviation * Add vertical lines to plots * Sort values * Plot a histogram * etc. It is very easy to find help with *all* of these items with quick google searches. Try searching "reading txt files with r" and you will find countless examples. Nobody is going to do your homework for you.
Oh cool, thanks for the tip! That sounds like a step I really should take once I have time to work on this kind of thing again. 
I think I explained myself wrong. Sorry for that. I don't want someone to do my homework. I just couldn't understand the logic of r graphs. What do we put on x and y? and we are calculating the average of x or y? If I understand these I will be able to understand the sources I found on internet and do the homework myself.
What does a table of the two choices look like? If you do `table(choice1, choice2)` it will show you the cross-tabluated totals. This will tell you if people who have A as the first choice tend to have a different second choice from those who have B as a first choice. I'd normally suggest a chi-square test of independence but I assume that people don't select the same value for their first and second choice. Having an all-zero diagonal would definitely favor a higher chi-square statistic. Correspondence analysis might be appropriate given your interest in factions.
Oof that is untidy...
 Given the following dataset, I cannot seem to get: &amp;#x200B; 1) How would I get for each country independently the sum for all Series.Names that contain the string “Male” ? This means. For each country have a sum of mathematics.Male + reading.Male + science.Male? &amp;#x200B; 2) How would I get the middle 5 countries for “reading.Female”, given that I already have a data frame that only holds data for “female” rows? For the top 5 and bottom 5 I managed to do it like this; bestReadingFemale &lt;- femaleReading\[order(femaleReading$z, decreasing = T)\[1:5\], \] (And turning decreasing to False for getting the bottom 5.) &amp;#x200B;
Look into the follow packages: tidyr - this will rearrange your dataset columns and rows so that values are in cells and parameters are column headers, functions like gather() and spread() stringr - this is great for character manipulation and filtering, many different functions to achieve this, simply google stringr and what you want to do and it will certainly come up dplyr - this one is perfect for filtering and creating new columns with adding other columns, filter() and mutate() is what you're after, perhaps filter() by gender then mutate() the sums into new columns?
apologies my base R is rusty, but with dplyr something like this should work: &amp;#x200B; `library(dplyr)` &amp;#x200B; `data %&gt;%` `select(``country.name``,series.name,x2015yr2015) %&gt;% ## this selects the relevent columns` `filter(grepl("Male",series.code) %&gt;% ## this filters out the rows for male` `select(-series.name%&gt;% ## removes this columns because you want sum by country` `group_by(``country.name``) %&gt;% ## prepares a grouping by country` `summarise(value = sum(x2015yr2015)) ## sums up the column by country group`
`gsub("'",'"',"[{'id': 7, 'name': 'Funny', 'count': 19645},{'id': 2, 'name': 'Awesome', 'count': 19645}]")` gsub(pattern, replacement, thingy) here your patten is a single quote, so i'm putting it in double quotes. your replacement is a double quote and so i'm putting it in single quotes, you could also use escape characters to instead of switching between the different kinds of quotes
Agreed. I work with data all day long and won’t do it without the tidyverse packages. (Actually I’m super hooked on data.table, but tidyr and dplyr are much easier to learn.)
Thanks bunches!
You can do it interactively with the locator() function inside of text(). For example, if you run the following: text(x = locator(n = 2), c("2x", "3x + 1")) You get the option to click in the plot where you want "2x" and then "3x + 1" to appear.
This is a case where the dark arts of *regular expression* must be invoked. The above answer should do the trick. Check out the "stringr" package for a tidyverse alternative.
Tidyverse: the jQuery of R. Sure, you can write R without it, but why?
I think UN looking data like this is literally the example of untidy data in the tidyverse book. 
How good is government data in general though? Incredible abominations.
I'm working with government data right now and the columns themselves are not that bad, but rows are appended columns-wise. It takes me 35 minutes to undo whatever had been done to it. 
Oof.
I wouldn't advice iterating over rows with a loop. In R you'd want to use vectorised functions for these kind of tasks. But I'm no expert, anyway here's a working example: &amp;#x200B; `## load dplyr` `library(dplyr)` `## create a sample dataset where the 2nd row is all NA` `test &lt;- data.frame("row" = c("a",NA,NA,"z"),"age" = c(1,NA,2,"c"),"status"=c(NA,NA,"c","b"))` &amp;#x200B; `## create a function to get the sum of NAs per row` `count_na &lt;- function(x) sum(`[`is.na`](https://is.na)`(x))` `## get the count of columns in the dataset` `col_count &lt;- dim(test)[1]` &amp;#x200B; `## create a new column with the count of the NAs per row and then remove the rows with all NA in them` `result &lt;- test %&gt;%` `mutate(na_count = apply(., 1, count_na)) %&gt;%` `filter(na_count != col_count)` &amp;#x200B; &amp;#x200B; For further information on the dplyr package please refer to the [vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
I think it's paste throwing the error because you didn't explicitly state collapse=NULL. As for your question, using apply is much faster na.indices = sapply(1:nrow(log_inc),funxtion (i){ if(sum(is.na(log_inc[i,]))== ncol(log&amp;inc)){ return(i)} })
It can be a lot easier. No need to make a function. ## load dplyr library(dplyr) ## create a sample dataset where the 2nd row is all NA test &lt;- data.frame("row" = c("a",NA,NA,"z"),"age" = c(1,NA,2,"c"),"status"=c(NA,NA,"c","b")) #make a vector giving the NA counts per row na_counts &lt;- rowSums(is.na(test)) #remove the rows where the NA count equals the number of columns (ie where the row is all NAs). col_count &lt;- ncol(test) result &lt;- test[na_counts != col_count, ]
 x[!apply(is.na(x), 1L, all),]
 From here: [https://stackoverflow.com/questions/33135060/read-csv-file-hosted-on-google-drive](https://stackoverflow.com/questions/33135060/read-csv-file-hosted-on-google-drive) id &lt;- "0B-wuZ2XMFIBUd09Ob0pKVkRzQTA" # google file ID read.csv(sprintf("https://docs.google.com/uc?id=%s&amp;export=download", id))
first result on google https://stackoverflow.com/questions/33135060/read-csv-file-hosted-on-google-drive
Find the queens. See if there is a line-of-sight between them. If there is, see if there are any other pieces in that path. If there are not, there is a threat.
Are all the key value pairs in 1 cell? Or multiple?
They would be in one column separated by a comma
Regular expressions and `grep()` or `stringr::str_extract_all()`. There are websites that will help you build the regex. Google for "regex builder"
 txt &lt;- "orderid params 1134 color=blue,size=medium 1135 color=red,width=wide,height=tall" data &lt;- read.table(text=txt, header=TRUE, row.names=1) keyvalueframe &lt;- function(x) { mat &lt;- do.call(rbind, strsplit(x,"=")) as.data.frame(setNames(as.list(mat[,2]), mat[,1])) } &gt; plyr::ldply(strsplit(data$params, ","), keyvalueframe, .id="orderid") color size width height 1 blue medium &lt;NA&gt; &lt;NA&gt; 2 red &lt;NA&gt; wide tall 
I've never done anything like this before in R and I don't know chess, but this is my best shot. Purely conceptual code here because I'm away from my computer. I'll follow up with better code later if I have some free time. Is the chessboard numbered squares? If you had a kind of coordinate system using tuples (or pseudo tuples - this Stack thread may be relevant - https://stackoverflow.com/questions/7799591/creating-tuples-from-two-vectors), you could create distances and paths that would correspond to particular moves on the chessboard. I don't know anything about chess, so I don't know what those could be or even if the movements are fixed length for each piece, but I think that is how I would try to go about it. If you could record these distances/paths correctly, you could get the range of movement for any piece on the map depending on its type and position. Queens, in whatever way they move, would have a certain legal range of motion and movement pattern. Same goes for Knights, Pawns, Kings, what have you. Using this information, you could then find the overlap in legal movements, using geometry to calculate the area or going with the up/down coordinate system depending on the rules of the game. I would think the combinatorics on the possible movements from any one position and any single piece would be batshit crazy to record in a static array. I would use functions to generate the range of movement dynamically depending on the input string with the piece's type (i.e. If "Queen", Queen_func. If "Pawn", run pawn_func"). You would run those if statements across the rows using for loops or one of the apply family functions. So, in sum: 1. Convert chessboard to coordinate system if not already formatted as one. 2. Write functions that generate an area of legal movement for each kind of chess piece based on the input coordinates. 3. Write another function to check for area overlap(or something more complicated that fits chess rules). 
I saw that, and initially thought it wasn't the answer I was looking for because of the reference to downloading, but there's no download involved and it worked. Thanks!
This is great!. Thanks for your help. btw... I was getting error: Error in strsplit(setNames(table$params, table$orderid), ",") : non-character argument &amp;#x200B; This fixed it: ldply(strsplit(setNames(**as.character(**table$params), table$orderid), ","), keyvalueframe, .id="orderid") &amp;#x200B;
This is awesome. Thanks a ton.
Sorry, `read.table(..., as.is=TRUE)` will also do it.
Let me know if this helps. library(tibble) library(dplyr) NUM_VALS_PER_GROUP &lt;- 50 # Set a seed so that we'll get the same data set each time and build the data set set.seed(2^10 + 3) t1 &lt;- tibble(a = factor(LETTERS[sample(1:26, NUM_VALS_PER_GROUP, replace = TRUE)]), b = factor(LETTERS[sample(1:26, NUM_VALS_PER_GROUP, replace = TRUE)]), val = runif(NUM_VALS_PER_GROUP, 0, 100)) t1 # Which factors are present in a, but not in b? sort(setdiff(t1$a, t1$b)) # Which factors are present in b, but not in a? sort(setdiff(t1$b, t1$a)) # Examine cross tabulation table(t1$a, t1$b) # View all pairs of combinations and how often they occur t1 %&gt;% group_by(a, b) %&gt;% summarize(n = n()) # Get mean(val) grouped by a &amp; b t1 %&gt;% group_by(a, b) %&gt;% summarize(mean_val = mean(val)) # Get mean(val) for all rows where a is not in b t1 %&gt;% filter(!(a %in% b)) %&gt;% group_by(a, b) %&gt;% summarize(mean_val_a_not_in_b = mean(val)) # Get mean(val) for all rows where b is not in a t1 %&gt;% filter(!(b %in% a)) %&gt;% group_by(a, b) %&gt;% summarize(mean_val_a_not_in_b = mean(val)) %&gt;% arrange(b, a) 
Do you mean a linear regression model instead of correlation? The easiest way to create a linear model is `lm(y ~ x + controls, data)`.
I don't understand what you are trying to accomplish. What do you mean by non_combo_value? All of your values will be accounted for by way of the nature of how the group_by works.
So for example, you might want 4 columns: id, type, in_group_mean, out_group_mean? (Where in group mean is the avg of all values in that group and our group mean is the avg of all values for all observations that do not belong in that group?
I'll check that out. Thanks for the link. 
I think this is the plan of attack I'm gonna go for . Thanks for the help. 
I'm not quite sure what you're trying to do here. I mean, it kind of sounds like you want to know if any value is equal to 1 and not necessarily where those values are in the matrix. Also if you've searched in one direction, I'm not sure why you would search in another as you've already checked each value. any() would tell you if there's any value in the matrix equal to what you're looking for, if there are, get the length of the matches. &amp;#x200B; &amp;#x200B;
I have a 8 x 8 matrix where some values equalling 1 are pre-set by my instructor. Those values are essentially the queen in chess. They can move horizontally, diagonally and vertically. I need to go through that matrix and return the number of 1 values which are threatened or in the line of attack of another 1 value.
It's slightly different. Here I'm asking exactly how to write the code, there I was asking what method I should use. 
The line that's rounding is: extract(r1,shp) &amp;#x200B;
 Winter has arrived and Shetland Wool Crewneck Sweater is a really nice one. I have tagged this title on [college paper writing service](https://uksuperiorspaper.com/) online web forum. I got to read some reviews as well. Thanks 
How many cities do you have?
 plot.new() plot(1, type="n", xlab="", ylab="", xlim=c(-10, 10), ylim=c(-10, 10)) abline(1, -1) abline(h=0) # if you want the x axis on the plot itself abline(v=0) # ditto for y Initialize the plot and then add the line you want.
Sorry, it was meant for ggplot
Completeness and accuracy: two things you generally will never find in real world data.
Despite the poor English and overly broad claims of this article, it had some good points about how to clean and cross verify data before doing an analysis to reduce issues.
There is a package called Rchess that gives you possible moves for a given board position. However in this particular case it might not work as it might get confused if it's not a possible game state without a king
These palettes are really good. 
That's so true! The idea for this article came after having to work with a real, no academic, dataset. 
[https://imgur.com/a/mWwbMzS](https://imgur.com/a/mWwbMzS)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/9bnB7pO.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Ok, thanks. So the reason your select call isn't doing much the first time around is that you aren't assigning the returned data frame to any environment variable. Here's how to correct that: # Create new data frame, nw_metrics, by selecting key variables from nw nw_metrics &lt;- nw %&gt;% select(Player, FG, FGA, X3P, FT, FTA, TRB, AST, STL, BLK, PTS) # Assign % variables to field goals and free throws nw_metrics$FGX &lt;- with(nw_metrics, FG/FGA) nw_metrics$FTX &lt;- with(nw_metrics, FT/FTA) Alternatively, you could do this in a single step, with dplyr's mutate function: nw_metrics &lt;- nw %&gt;% select(Player, FG, FGA, X3P, FT, FTA, TRB, AST, STL, BLK, PTS) %&gt;% mutate( FGX = FG / FGA, FTX = FT / FTA ) Hope this clears things up!
Thanks!
I think that's found in aov() http://www.sthda.com/english/wiki/one-way-anova-test-in-r#compute-one-way-anova-test
Really? It works for me. I added some axis and more ticks to verify: ggplot(data=data.frame(x=c(-10,10),y=c(-10,10)),aes(x=x,y=y)) + geom_blank() + geom_abline(slope=-1, intercept=1) + scale_x_continuous(breaks = seq(-10,10,1)) + scale_y_continuous(breaks = seq(-10,10,1)) + geom_hline(yintercept=0) + geom_vline(xintercept=0) This is what I get: [https://imgur.com/a/dPw6jHA](https://imgur.com/a/dPw6jHA)
This thread answers your question and more: [https://stackoverflow.com/questions/11099272/r-standard-error-output-from-lm-object](https://stackoverflow.com/questions/11099272/r-standard-error-output-from-lm-object) &amp;#x200B; I wanted to say - tslm() might be a better choice for this exercise.
Thanks, it's working. Sorry about that confusion on my part
What do you mean by “background points”? Are you saying that you want all of the random points within your polygon? If so, this will return that: points_in_poly &lt;- points[polygon,]
I have 3800 points in Europe and I need to get 500 random points in the same map for doing the semiausence points. I need to do the first thing u write?
Thank you! Is the R squared statistic the correlation in the lm model?
Yeah I think you’re o onto something. You could search through both the City and Country column instead of city_country to try and differentiate between city names and state names. You’d need to scrape a list of state names too though... Like I said before, I’m sure there’s a smarter way to do it with just regex but I’m bad at regex. 
A few pointers: - the rnorm() function you're using to simulate BMI takes an argument n. `?rnorm` - In order for the calculation of Z to make sense, you first need to put those simulated BMIs together in one vector. Something like this should work, as long as you make sure your M dummy matches up with how you built BMI: BMI = c(BMIF, BMIM) M=c(rep(0, length(BMIF)), rep(1, length(BMIM))) Z= BMI - mean(BMI) - The error of 300 you were given is the magnitude of the "noise", aka the normal error (usually normal at least). So you'll probably want `error = rnorm(n, 0, 300)` instead of what you currently have. - The description asks for using `lm` rather than lsfit to fit your model. Looking at `lsfit`, this is probably because a) lsfit only supports one predictor (and you have two), and b) lm() is much more widely used 
Thank you for the advice! I've modified my code a bit but I'm getting an error at the 'for' loops: "longer object length is not a multiple of shorter object length Error in model.frame.default(formula = model, drop.unused.levels = TRUE) : variable lengths differ (found for 'mu')" b2=c(0,0.2,0.3,0.5) N=c(30,50,100,200,500,1000,3000) nRep=5000 significance=0.05 mat &lt;- matrix(nrow=7, ncol=4, 0) rownames(mat) = c("N=30", "N=50", "N=100", "N=200", "N=500", "N=1000", "N=3000") colnames(mat) = c("R2=0", "R2=0.2", "R2=0.3", "R2=0.5") BMIF=rnorm(n=N/2,26.5,30) BMIM=rnorm(n=N/2,27.4,16.7) BMIMEAN=(BMIF+BMIM)/2 BMI=c(BMIF, BMIM) error=rnorm(n=N/2, 0, 300) mu=120 b1=-3 for(h in 1:length(b2)){ message("R-sq=",b2\[h\]) for (i in 1:length(N)) { message("N=", N\[i\]) for(j in 1:nRep){ M=c(rep(0, length(BMIF)), rep(1, length(BMIM))) Z=BMI - mean(BMI) SBP=mu+M\*b1+Z\*b2+error model='SBP\~mu+M\*b1+Z\*b2+error' fm=lm(model) reject=(ls.print(fm,[print.it](https://print.it) = F)$coef\[\[1\]\]\[2,4\]&lt;significance) mat\[h,i\]=mat\[h,i\]+reject &amp;#x200B; } } } &amp;#x200B; Any ideas?
did you try running install.packages("Rcpp"). Then library("Rcpp"). Then the install command for the package you want again.
Yes. I tried it. And I don’t get the error when installing the wanted package. It’s when I try and use one of its functions 
Use grepl() or stringr::str_detect(). Don't worry about the details of the regular expression, in this case the plain string will work.
No worries! I understand your question. So yes, I understand what I am trying to do, but truthfully, my department is notorious for not explaining things very well and just sending us off into researching and trial/error to figure things out for ourselves. We are simulating BMI data based on two different distributions, one for males and one for females, using monte carlo replicates. The BMI data is then used to calculate a response (SBP). The response is to be modeled using BMI (Z) and Gender(M) and the number of rejections (rejection of a null hypothesis) are tallied in the matrix (mat). We then use the number of rejections to ultimately plot a power curve where sample size (N vector) is along the x-axis and effect size is along the y-axis. I've included updated code and used your pointers to make modifications, but I am getting an error at the "mat\[h,i\]" step: "Error in mat\[i,h\] &lt;- mat\[i,h\] + reject : number of items to replace is not a multiple of replacement length" b2=c(0,0.2,0.3,0.5) N=c(30,50,100,200,500,1000,3000) nRep=5000 significance=0.05 mat &lt;- matrix(nrow=7, ncol=4, 0) rownames(mat) = c("N=30", "N=50", "N=100", "N=200", "N=500", "N=1000", "N=3000") colnames(mat) = c("R2=0", "R2=0.2", "R2=0.3", "R2=0.5") mu=120 b1=-3 for(h in 1:length(b2)){ message("R-sq=",b2\[h\]) for (i in 1:length(N)) { message("N=", N\[i\]) for(j in 1:nRep){ BMIF=rnorm(n=N\[i\],26.5,30) BMIM=rnorm(n=N\[i\],27.4,16.7) BMIMEAN=(BMIF+BMIM)/2 BMI=c(BMIF, BMIM) M=c(rep(0, length(BMIF)), rep(1, length(BMIM))) Z=BMI - BMIMEAN error=rnorm(N\[i\], BMIMEAN, 300) signal=mu+M\*b1+Z\*b2\[h\]+error SBP=signal+error fm=lm(SBP\~Z+M) reject=coef(summary(fm))&lt;significance mat\[i,h\]=mat\[i,h\]+reject &amp;#x200B; } } } &amp;#x200B;
So if youre' searching through a character vector, for entries that have "atl" in them, use grep(). If you want to find the positions of "atl" in a string, use regexpr(). Use fixed=T to see if it works, else remove the constraint. 
Yes, yourself. Packages should never be a “run me and I’ll do everything for you.” User input is critical.
The following builds on the previous comment and assumes the data has these columns: time, well name, chemical, and concentration `chemicals &lt;- unique(my_dataset$chemical)` `plots &lt;- list()` `for (i in chemicals) {` `plots[[i]] &lt;- ggplot(data = dplyr::filter(my_dataset, chemical == i), aes(x = time, y = concentration)) +` `geom_line(aes(group = chemical)) + facet_grid(well ~ .)` `}`
 reject=coef(summary(fm))[2,4]&lt;significance 
I don't see how it could get any easier than plot(n_visits ~ years_active, data=users)
Thank you! Code ran through, and power curves plotted. One last question if you have time: Can you please help me understand why the `[2,4]` portion of the code mitigated the error I was receiving?
Yes there's a ready made package called Excel.
You were comparing the whole table of estimates coef(summary(fm)) to the alpha level, which gave you a whole table of TRUE/FALSE values that you then tried to add to `mat`. What you actually wanted, and similar to your original code `reject=(ls.print(fm,print.it = F)$coef[[1]][2,4]&lt;significance) `, was to compare just the p value for BMI (which is at row 2, column 4 in the table of coefficients).
It looks like there was an issue installing the git2r package. Then installing the 'usethis' package failed because it depends on git2r and then 'devtools' failed because it depends on both of these. Usually, there would be a step in the output above where it would show messages relating to installing git2r. And there's be some error message specific to that package. Alternately, you could try installing git2r manually by running `install.packages("git2r")` and maybe you'll get a more specific error message. My guess here is that you don't have `git` installed and that's the issue. Not sure the best way to install it on a Mac (maybe it comes with Xcode?).
I have no experience with this, but thought this thread might provide some insight. https://www.researchgate.net/post/Which_R_package_would_you_recommend_to_analyse_geolocation_data
I happened to see an [example of a project](https://gist.github.com/psychemedia/9737637) using 'ggmap' the other day that could interest you. It was used in the context of a Shiny web app, but the concept was to find the distance between two locations. I don't know whether you would still have the same limitations as you had with gmapdistance. I'm sorry if this isn't of any use to you. 
Depends. What have you tried so far? 
lm(y \~ x), then predict(), but this makes no sense, because it will draw E(x) I think, which is the median of y.
I believe that this thread is the first one on ResearchGate that I have ever found useful. I've experimented with almost all of the packages mentioned there. That's a very helpful thread. Thanks for posting!
The median of all y’s IS a simple model do this relationship. lm() is giving you the conditional mean of y. Your data looks like a time series so you could build a model based off of that (ie your features can be traditional time series features)
[https://rickpackblog.wordpress.com/2017/08/17/bing-maps-api-for-driving-distance-in-r/](https://rickpackblog.wordpress.com/2017/08/17/bing-maps-api-for-driving-distance-in-r/) &amp;#x200B; click on the *earlier post* link to get more context.
Try this package: https://github.com/dreamRs/esquisse Is super simple.
For a real world working example, here's code I wrote to scrape a similar website run by the FCC where you also have to manually input data. The website is: https://www.fcc.gov/media/engineering/dtvmaps cities &lt;- read.csv("../top100cities.csv", stringsAsFactors = F) rD &lt;- rsDriver(browser="firefox") remDr &lt;- rD[["client"]] cities.df &lt;- data.frame(city = cities$City) cities_scrape &lt;- function(city){ remDr$navigate("https://www.fcc.gov/media/engineering/dtvmaps") Sys.sleep(1) remDr$findElement("id", "startpoint")$sendKeysToElement(list(city)) remDr$findElements("id", "btnSub")[[1]]$clickElement() Sys.sleep(2) html &lt;- remDr$getPageSource()[[1]] signals &lt;- read_html(html) %&gt;% html_nodes("table.tbl_mapReception") %&gt;% .[3] %&gt;% .[[1]] %&gt;% html_table(fill=T) names(signals) &lt;- c("rm", "callsign", "network", "ch_num", "band", "rm2") signals &lt;- signals %&gt;% slice(2:n()) %&gt;% filter(callsign != "") %&gt;% select(callsign:band) strength &lt;- read_html(html) %&gt;% html_nodes("table.tbl_mapReception:nth-child(3) .ae-img") %&gt;% html_attr("src") signals &lt;- cbind(signals, strength) signals &lt;- mutate(signals, strength = strength %&gt;% str_extract("strength.")) return(signals) Sys.sleep(runif(1, 1, 3)) } scrape_safe &lt;- function(url){ result &lt;- try(cities_scrape(url)) if (class(result) == "try-error") { cat("Error encountered for url:", url, "\n") return(data.frame()) Sys.sleep(runif(1, 1, 3)) } else { return(result) } } cities.df &lt;- cities.df %&gt;% group_by(city) %&gt;% do(scrape_safe(.$city)) The key here is that I have all of the cities I want to search for in a properly formatted way.
Both of these posts are incredibly helpful. Thank you so much. I'll have to brush up on using functions but I think I can give it a go - I'm pretty novice with R.
No problem. This second post is essentially exactly what you want to do, just change the relevant parts of the code to fit your application.
I've done a bunch of the R beginner/intermediate stuff, and I am halfway through the Python Data Scientist course. I think it's worth the price and I would pay for it myself if my company wasn't willing to pay for it for me. There are other free courses that are okay, but I think the DataCamp platform is well worth the cost because it streamlines learning so much. 
I've done it. In fact, I've done the majority of courses on Data Camp - R, Python, SQL, even Spreadsheets. I really enjoyed it. You already know the format, so you know how it works. They do push package usage more (like dplyr) as opposed to doing things in base R but that shouldn't be a problem. 
Doing data science in R without Tidyverse seems infeasible to me. 
I did the Python one and found it very valuable
Package Tidyverse with RStudio when?
Your question is not concrete. The probability of y &gt; 10 is simply the number of ys &gt; 10 (2) over the number of observations (if we assume this data is the population). So 2 / 8 = 0.25. This is Pr(y &gt; 10). Now what you're really probably interested in is Pr(y &gt; 10 | x). This is a much different question. There are a bunch of interesting ways to do this. But which one is most important depends on the nature of your data. For example, if we think y is roughly normally distributed, we can generate a new variable equal to one when y &gt; 10 and estimate a probit model. The estimated function is a model for the probability y &gt; 10 given x.
Agreed. I'm way too used to the pipe now so it's weird to me when some, say, Stack Exchange approaches don't use it.
I completed the Data Analyst track and having used many other sites, EdX.org, Udemy etc... DataCamp is by far the best for learning to code for data science. That being said... I recommend waiting for a sale. They have 40-50% off at times. I bought a one year subscription for $189 which breaks down to about $15.75/mo instead of the standard $25. 
This. They regularly do the 50% off.
see `?printCoefmat`. This is the function the `print.summary.glm` function uses to print the coefficient matrix. By default it formats test statistics to have one fewer significant digits than coefficients. It has an argument called `dig.tst` that you can use to manually set the minimum number of significant digits for test statistics. The following would result in both coefficients and test statistics having 3 significant digits print(summary(m), digits = 3, dig.tst = 3) &amp;#x200B;
I think this should do the trick: polls &lt;- webpage %&gt;% html_nodes("div#polling-data-full a.normal_pollster_name") %&gt;% html_text() Polls$Poll[-1] &lt;- polls The first part extracts the normal\_pollster\_name values in the table with id=polling-data-full. The next line writes them to the data frame, minus the first entry, which is the RCP Average.
Can anyone provide a non-trivial example of where kind of stuff would be useful? I use `data.table` with `get` to pass in strings that are evaluated in the context of the table. This has never failed to provide me with anything I need for "dynamic" programming, but my needs are pretty specific. Why did Hadley develop this framework? 
I did it and found it very useful. I wanted to be able to use R for all of my data analysis and now I do. I don't have everything memorized, but after finishing it I feel fluent enough to look up and read the man pages easily and answer questions from my coworkers. The curriculum was updated earlier this year and now includes some SQL courses which are really useful for better understanding the dplyr.
How about dynamically transforming a dataset and then creating many different plots?
I'm running v3.5 on Pop!_OS 18.04 LTS and it runs flawlessly. 
This looks like a case where you must use a "for loop", since you are accessing 2 different indices of `lxp` at the same time. Moreover, you are growing a vector - the `apply` family of functions work on existing data structures, not in thin air.
I'm not sure I understand what you're trying to do, but here's my best guess using base R with magrittr pipes. &gt; library(magrittr) &gt; # generate similar data &gt; set.seed(1) &gt; prod_cat &lt;- sample(c("A","B","C","D"), size = 500, replace = TRUE) &gt; Transaction_Level &lt;- factor(sample(c("Low","Medium","High"), + size = 500, replace = T), + levels = c("Low","Medium","High")) &gt; payment_method &lt;- sample(c("CC","DC"), size = 500, replace = T) &gt; dat &lt;- data.frame(prod_cat, Transaction_Level, payment_method) &gt; head(dat) prod_cat Transaction_Level payment_method 1 B Medium DC 2 B High DC 3 C Medium CC 4 D Medium DC 5 A Medium CC 6 D High CC &gt; &gt; # proportion of CC by Transaction_Level &gt; t1 &lt;- xtabs(~ Transaction_Level + payment_method, data = dat) %&gt;% + prop.table(margin = 1) %&gt;% + extract(,1,drop=F) &gt; &gt; # proportion of prod_cat by Transaction_Level &gt; t2 &lt;- xtabs(~ Transaction_Level + prod_cat, data = dat) %&gt;% + prop.table(margin = 1) &gt; &gt; # combine into one table, convert to percent &gt; cbind(t1,t2) %&gt;% + multiply_by(100) %&gt;% + round(2) CC A B C D Low 52.56 23.08 30.77 23.72 22.44 Medium 52.98 26.79 32.14 15.48 25.60 High 55.68 22.73 26.70 26.14 24.43 &amp;#x200B; &amp;#x200B;
R 3.4.4, RStudio 1.2.1114 on Ubuntu 18.04 with NVIDIA GeForce 960M support. TensorFlow, ggplot, tidyverse, XGBoost, caret. Perfect
If you can calculate the proportion for the whole data set can't you just do the same thing using summarize to do it on a group? This doesn't matter but your if statement could be... ifelse(amount &gt;= 1000, "High", ifelse(amount &gt;= 100, "Medium", "Low"))
I don't think you need a loop or an apply. Looks like you're just doing a cumprod. I get the same result as you with the following: cumprod(c(100, 22:30)) &amp;#x200B;
That's exactly the output I'm looking for and neat solution. Thank you. &amp;#x200B; I was just wondering if there was a simpler way to do it with dplyr and the summarize function, but that works. Thanks agian.
You can perform the CC percentage and the prod\_cat percentages separately and left\_join them like so: &amp;#x200B; `left_join(df %&gt;%` `group_by(transaction_level, prod_cat) %&gt;%` `summarise(prop = n()) %&gt;%` `mutate(prop = prop / sum(prop, na.rm = T)) %&gt;%` `spread(prod_cat, prop),` `df %&gt;%` `group_by(transaction_level, payment_method) %&gt;%` `summarise(prop = n()) %&gt;%` `mutate(prop = prop / sum(prop, na.rm = T)) %&gt;%` `spread(payment_method, prop) %&gt;%` `select(1:2)` `) %&gt;%` `select(transaction_level,` `perc_CC = CC,` `everything()` `)` &amp;#x200B; You can build the sample data frame I used like this: &amp;#x200B; `df &lt;- tibble(customer_id = sample(c(10000567:10000582),` `2000,` `replace = T` `),` `trans_dt = sample(c("2018-01-01", "2018-01-02", "2018-01-03"),` `2000,` `replace = T` `) %&gt;% ymd,` `prod_cat = sample(c(LETTERS[1:4]),` `2000,` `replace = T` `),` `amount = rnorm(2000, 100, 35) %&gt;% round(2) %&gt;% abs,` `payment_method = sample(c("CC", "DC"),` `2000,` `replace = T,` `prob = c(.8,.2)` `),` `transaction_level = sample(c("High", "Medium", "Low"),` `2000,` `replace = T,` `prob = c(.1, .6, .3)` `)` `) %&gt;%` `arrange(trans_dt, customer_id)`
It depends on your definition of dynamic, but imagine you want to get summary statistics of various attributes by year from some data.table dt. # sum by year dt[Year &gt; 2012, mean(Price), by = Year] # fully generic version value &lt;- 'Price' key &lt;- 'Year' min_year &lt;- 2012 func &lt;- function(x) { mean(x) } dt[get(key) &gt; min_year, func(get(value)), by = get(key)] See how dead simple using `get()` is? `melt` and `dcast` are basically the gather/spread equivalents and already work directly off strings. ggplot can be a little trickier, but it's a ggplot kind of trickiness and not the non standard eval kind. I'm probably just not using this stuff at the level that requires much scrutiny, I know for example that using `get()` like in my example is not super safe, it searches the enclosing environment then up the frames. So if you forgot you don't have a column 'Year', but you have a vector named Year outside of the dt, it will erroneously return that object as expected without indicating any issue. But this is not a deal breaker in the sort of psuedo-production code that I deal with and is not that difficult to live with. To be clear I'm not knocking this at all, I'm sure if Hadley went through the trouble to create this framework it has it's uses. I just don't understand when/if I would use this. 
ggplot is a mostly great use case for this, I didn't mean to imply it wasn't. What I meant was with ggplot is it gets tricky to guarantee "good" plots the more dynamic you want them to be. In the past I've used ggplot to generate long pdf summary reports with multiple charts per page. Writing the code to dynamically pass in what I wanted to plot was easy. What took more time was making things robust to edge cases that broke the plots, like data being missing, at irregular scales, getting output dpi right, etc. That's what I meant by "ggplot trickiness". 
I spent 5 hours on this. I love you. 
This is sweet!
Thanks, time to do some googling. I understand the %&gt;% html\_text() part now too.
Sure. How do you capture user provided transformations? I.e. How do you write a function where one of the arguments is an expression (potentially including column names) to be performed on your data.table? For instance, how would you implement the following function on top of data.table? With dplyr and tidy evaluation is trivial. yearly_return = grouped_summary(data, group = year, sum(dividend) - sum(expenses)) This function doesn't do very much (it could be replaced by a single line data.table query) but I can't come up with a better simple example now. Please feel free to adapt the function signature to be idiomatic for data.table usage. The return type should either be a named list or a data.table with two columns.
Thanks for the update OP
Aww, thanks for the love! `div#polling-data-full` identifies div tags with id=polling-data-full. (There's only one.) `a.normal_pollster_name` identifies the a tags with class=normal\_pollster\_name. Placing `a.normal_pollster_name` immediately after `div#polling-data-full` (with a space), says select all a tags with class=normal\_pollster\_name *inside* the div tag with id=polling-data-full. FWIW, I found the tutorial at [http://flukeout.github.io/](http://flukeout.github.io/) to be very useful for learning how to use css selectors. 
Not sure why you got downvoted. OP asked how to do it with dplyr and you did it. Very impressive!
I'm not sure I understand your intention for the example. Because the `j` argument for data.table is designed to accept user expressions, including column names, the super simple replacement would just be: dt[, sum(dividend) - sum(expenses), year] If I wanted to make the `[` call fully general, I could create a function and pass in arbitrary columns as parameters. f &lt;- function(x, y){ sum(x) - sum(y) } dt[, f(dividend, expenses), year] which is also identical to dt[, f(get('dividend'), get('expenses')), year] which is also identical to dt[, { f &lt;- function(x, y){ sum(get(x)) - sum(get(y)) }; f('dividend', 'expenses') }, year] You could also do this using `as.call`, `as.name`, `call`, `eval`, etc... that would craft the expression `sum(dividend) - sum(expenses`to be called in the `j` argument using `eval`. I could see the use for this if you are forced to go that direction, because crafting the expression using strings and the call/name/eval functions would get impossibly complicated fast. Check out the final answer [here](https://stackoverflow.com/questions/37404931/fast-data-table-assign-of-multiple-columns-by-group-from-lookup) for what I'm talking about. `batch.lookup` there is a beast of a function. But also note there turned out to be a much, much simpler solution using `Map` and `mget`!
I see your point, thanks! The "answer" for data.table here would probably be a hideous mess of as.call/as.name/call/eval. I say the "answer", because my practical solution would be to never create a function that allows the user to input an expression like this :) Probably I would avoid it by constructing a function to pass to `j`. Here is my quick and dirty attempt to get similar functionality: f &lt;- function(x, y){ sum(x) - sum(y) } grouped_summary &lt;- function(data, group, func, ...) { data[, { arguments &lt;- as.list(mget(c(...))); names(arguments) &lt;- NULL; do.call(f, arguments)}, get(group)] } grouped_summary(dt, group = 'gear', func = f, 'disp', 'hp') 
Regarding his last example: http://www.thinkingondata.com/wp-content/uploads/2018/09/final.png What can you do to align those charts? So that the main charts would be aligned from left (here not a problem, but it can be in general) and right and the legend too.
Can you provide a minimal reproducible example of what you're tried so far?
One idea would be to post the code you ran. Google has lots of APIs.
I'm running R 3.5 on Debian Stable using [CRAN repositories for Debian](https://cran.r-project.org/bin/linux/debian/). These are maintained by the same people who maintain R in Debian itself, so they work very well.
Nice overview of ggplot customization. I don’t agree with removing the x axis label in this example though. 
Was thinking the same thing, can’t think of a situation where that’s a better idea.
Pretty sure they are calculated by string width. This is a horrible hack, but you COULD do something like this max &lt;- 14 p1$labels$colour %&lt;&gt;% (20 - .) %&gt;% nchar p1$labels$colour %&lt;&gt;% append(rep("", max - nchar(p1$labels$colour)) %&gt;% paste(collapse = " ")) %&gt;% paste(collapse = "") p2$labels$colour &lt;- "*Different" p2$labels$colour %&lt;&gt;% append(rep("", max - nchar(p2$labels$colour)) %&gt;% paste(collapse = " ")) %&gt;% paste(collapse = "") #-------------------------- &gt; p1$labels$colour [1] "year " &gt; p2$labels$colour [1] "*Different "
[Fundamentals of Data Visulaisation](https://serialmentor.com/dataviz/) by Claus O. Wilke is a good resource for data visualisation. 
Well, the idea came from this visualization, because I think that x-axis label is redundant. But for that moment, I didn't know how to do that. http://www.thinkingondata.com/wp-content/uploads/2018/07/Age_AllTeams.png 
Patchwork is even better than cowplot.
Debian is good for stability by default which means older more tested/mature software. You could force it to go more up to date versions but at that point you should try mint or Ubuntu or if you want bleeding edge / totally custom you could look to arch (not for beginners). 
Makes sense! I need to remember to indicate the matrix position I am asking R to consider. Thanks again for all your help! I greatly appreciate it.
Sure, with categorical data where axis values are a bit more descriptive this makes a bit more sense (depending on title and presentation format) but having an unlabeled scale on the x is pretty confusing. It isn't clear on the website that \`carat\` is being displayed on that axis.
To access the CI of a t.test, use: results &lt;- t.test(RR_1940,conf.level = 0.95) low &lt;- results$conf.int[1] high &lt;- results$conf.int[2] It's not terribly surprising that your CI is so thin given the large number of samples. The interval tells us the population mean will be between the low and high ends of the CI 95% of the time. A large N shrinks the size of the interval.
This might help. # https://www.reddit.com/r/Rlanguage/comments/9x7jz7/95_confidence_regions_on_a_histogram/ NDP &lt;- 200 MEAN &lt;- 50 SD &lt;- 5 CL &lt;- 0.95 ci95 &lt;- c( qnorm( (1 - CL) / 2, mean = MEAN, sd = SD), qnorm( CL + (1 - CL) / 2, mean = MEAN, sd = SD) ) # Theoretical Plot plot(curve(dnorm(x, mean = MEAN, sd = SD), from = MEAN - 4 * SD, to = MEAN + 4 * SD), type = "l", main = "Theoretical Histogram", xlab = "x", ylab = "probability density") # Draw theoretical confidence interval abline(v = ci95, lty = 2, col = "red", lwd = 2) # Create a sampled distribution set.seed(251) myDist &lt;- rnorm(NDP, MEAN, SD) # Draw the sampled histogram hist(myDist, breaks = 11, main = "Sample Histogram", xlab = "var") # Draw theoretical confidence interval abline(v = ci95, lty = 2, col = "red", lwd = 2) # Calculate empirical confidence limits ndf &lt;- length(myDist) - 1 obsMean &lt;- mean(myDist) obsSD &lt;- sd(myDist) stdError &lt;- obsSD / sqrt(ndf) t_crit &lt;- qt((1 + CL) / 2, ndf) cl_lower &lt;- obsMean - stdError * t_crit cl_upper &lt;- obsMean + stdError * t_crit # Draw empirical confidence limits abline(v = c(cl_lower, cl_upper), col = "blue", lty = 2, lwd = 2) [Theoretical Probability Distribution](https://i.imgur.com/unurpG6.png) [Empical Distribution with 95% Confidence Limits](https://i.imgur.com/OF35kEw.png)
Here, you're showing a confidence interval as opposed to a confidence limit, but the mechanics of plotting either of them are the same. I think you've figured out what you need, but I posted code below to illustrate plotting both kinds of values and how to calculate a confidence limit directly from sampled data.
Is this what you're looking for? my_table %&gt;% group_by(Month, prod_cat) %&gt;% summarize(Total_Prod_Sales = sum(amount)) %&gt;% arrange(Total_Prod_Sales) %&gt;% head(1) After your `summarize` call, sort the month-categories by sale price and pull the top value.
If you are talking about Google analytics. An idea is to reduce the number of dimensions and get the extraction day by day. If this don't work you will need to buy the 360 version. 
This may work: my_table %&gt;% group_by(Month, prod_cat) %&gt;% summarize(Total_Prod_Sales = sum(amount)) %&gt;% # within group filter for the product with the highest Total_Prod_Sales per month group_by(Month) %&gt;% filter(Total_Prod_Sales == max(Total_Prod_Sales)) Two caveats for the above solution: 1. It assumes that there is no missing data, otherwise you would need to add a "na.rm = TRUE" to the max function. 2. If there is a tie between two or more products having the max Total\_Prod\_Sales for the same month, then all of them will be selected.
Nice try but you won’t convince me flobs is a real world and not some rick and morty reference 
You can also use `top_n(1)` which both arranges and selects rows. It sorts by the right-most column in the table, unless specified otherwise. 
I normally use ggpubr, which (as I understand it) basically uses cowplot for the arranging. I was looking at patchwork and it looks good, but why do you think it is better than cowplot? 
Thanks!
The function format is used to format is used to "format" a R object for pretty printing meaning it returns a character string. Don't call format if you don't want character strings.
Claus Wilke, the creator of cowplot, himself admitted that [cowplot is not the cleanest package under the hood](https://twitter.com/ClausWilke/status/953289312296226816). Also, I find the API for Patchwork very elegant, and it's much easier to control how you want the charts laid out.
Pro tip: don’t put files into databases.
I updated the article and aligned those chart using patchwork: http://www.thinkingondata.com/wp-content/uploads/2018/09/plot_layout_theme_bw.png
I'll play around with it a bit and see. Thanks,I had never heard of it
Thanks for the update. Yes, the `patchwork` package looks nicer both syntactically and output-wise.
Does cowplot still think it's a good idea to silently change your ggplot theme when you import it for a function?
It is a matrix algebra rule. If you are multiplying two matrices, let's say A and B, A times B only works if the number of columns in A is the same as the number of rows in B. 
unique(c(x,y)) unique(data[,c(x,y)])
 NDP &lt;- 50 set.seed(1029) df1 &lt;- data.frame( a = sample(letters[1:26], NDP, replace = TRUE), b = round(rnorm(NDP), digits = 1), c = sample(1:5, NDP, replace = TRUE), d = sample(1:4, NDP, replace = TRUE) ) # Get unique values for all columns lapply(df1, unique) # Summarize unique values by count lapply(df1, function(x) { tapply(X = x, INDEX = as.factor(x), FUN = length) }) # Option 1: Get unique values for all columns except the 1st and 2nd columns (ie "a" &amp; "b") lapply(df1[c(-1, -2)], unique) # Option 2: Get unique values for only the 3rd and 4th columns (ie "c" &amp; "d") lapply(df1[c("c", "d")], unique) 
Clarify please? 
Change this: filter(isd,date==c(20160615,20160616... to this: filter(isd,date %in% c(20160615,20160616... because this `==` means exactly equal to*. there is no date that is exactly equal to 20160615,20160616,20160617... `%in%` just means if the first thing is somewhere in the second thing keep it. *pretty much
Okay, that cleared up a problem I didn't know I had. Now the graph looks like hell. I want to show a continuous trend connecting lines between sequential points, not whatever this is doing.
i think he means that a single value 'x' can't really be compared to a list of values: (x1, x2, x3) &amp;#x200B; if you want to know if your value is in that list you can try the %in% operator, although i've never used it. &amp;#x200B; I don't know how it compares with the other method but i use any(x1, x2, x3 == x) to see if a value is in a vector
have you tried geom\_smooth?
Good idea. It didn't like that. 
Here's some tips. Try to keep your code clear and use the data structures R gives you to make things easier on yourself. library(rnoaa) library(lubridate) library(dplyr) library(readr) library(tidyr) library(ggplot2) stations_raw &lt;- isd_stations() %&gt;% filter(ctry == "US", # keep line widths under control state == "WV", end &gt; 20160715) ##Since Huntington's coordinates are approx. 38N 82W, # use Tri-State/ M.J. Ferguson Field Airport## ##USAF=724250, # WBAN=03860## #Station ID is GHCND:USW00003860# isd_df &lt;- isd(usaf="724250", wban="03860", year=2016) %&gt;% mutate(date_time = paste(date, time), date_time = ymd_hm(date_time), # There's no seconds date = ymd(date), AA1_depth = as.numeric(AA1_depth)) %&gt;% # Sort out your types early filter(between(date, ymd("20160615"), ymd("20160715"))) %&gt;% # Here's where you used in, but since dates are essentially integers we can use a numeric filter instead # Check your environment in Rstudio to make sure your filter did what you think it did! select(date_time, AA1_depth) isd_df %&gt;% ggplot(aes(x = date_time, y = AA1_depth)) + geom_point(show.legend=FALSE) + geom_line() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) If I understand correctly, these are periodic rainfall measurements. Because this is not strictly a continuous measurement I recommend either aggregating your data or making a histogram instead.
You're a lifesaver. Now, my little problem: Can I make it show more than 4 dates on the x axis? 
Instead of geom_line try geom_smooth, and read it's help page.
Where did you come across that phrase? The brms package is a high-level interface to [Stan](http://mc-stan.org/), a probabilistic programming language/Bayesian model fitting software package. So, brms can be used to fit a wide variety of statistical models, and it has a lot of flexibility with respect to the distributional assumptions one makes about the data.
Hi, so that produced this table, but I need a column that just has the highest grossing category per month. So for it would look like Month | Product Category January D February D March B... So, only the highest selling product category each month. I'm not sure how to extract that from the resulting table that your code produces. Any ideas? Thank you! &amp;#x200B; Output from your code: Month prod\_cat Total\_Prod\_Sales &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; 1 1 A 50028. 2 1 B 71534. 3 1 C 500382. 4 1 D 582371. 5 2 A 50105. 6 2 B 75646. 7 2 C 575561. 8 2 D 641376. 9 3 A 51738. 10 3 B 78409.
I find R to be great for plots, if you use the ggplot package and the larger tidyverse set of packages. Within ggplot, you can use geom\_point() to display your points, then you can assign groups or use color to distinguish your group. If the points overlap too much on specific values, you can use geom\_jitter, which randomly pushes each point around slightly so they're not all overlapping at one spot. For your regression line, you can run geom\_fit() with a linear option, or other various options if you're looking for non-linear fits. 
There are at least two ways, one easier and one more versatile: 1) The easier way: ggplot has a geom\_smooth() function that will draw regression lines for you. As long as you add your categorical predictor as a grouping variable, it will draw separate lines for you. Something like this: ggplot(df, aes(x=x, y=y, group=birth_month, color=birth_month)) + geom_point() + geom_smooth(method='lm', formula=y~x, fullrange=TRUE) (In the above I am assuming your data set is called \`df\`.) Of course, because you have more than one categorical predictor, this might be tricky. The line is going to get drawn for whatever reference group you are using for gender (which you can check and/or set with the contrasts() function). You could, if you wish, use facet\_wrap() to create separate plots for both genders (e.g., add `+ facet_wrap(vars(gender))` to the end of the statement). 2) The more versatile option is to basically calculate the lines yourself. Luckily, there are some functions that make this not so difficult. First, the expand.grid() and predict() functions let you easily create a set of points to predict from your model, and then get the predicted y-values. Like so: grid &lt;- expand.grid(x=seq(0, 100, 5), gender=levels(df$gender), birth_month=levels(df$birth_month)) predicted &lt;- predict(lm_fit, newdata=grid, se.fit=TRUE) grid$y &lt;- predicted$fit grid$se.hi &lt;- predicted$fit + predicted$se.fit grid$se.lo &lt;- predicted$fit - predicted$se.fit This just gets the predicted values for x from 0 to 100, across all levels of gender and birth\_month. You can also get the standard errors for these predictions if desired, which I've done above. Then, you can simply feed that grid into ggplot: ggplot(grid, aes(x=x, y=y, group=birth_month, color=birth_month)) + geom_point(data=df, aes(x=x, y=y, group=birth_month, color=birth_month)) + geom_line() + geom_ribbon(aes(ymin=se.lo, ymax=se.hi)) Again, you could add facet\_wrap() to this if you wanted to add gender. Note that in this case, the scatterplot portion is using the raw data, and the rest of it is using the predicted values from the \`grid\` object we created. Like I said, this second approach is a bit more complicated, but it does allow more versatility if you only want to predict across some birth months, or if you decide to add non-linear effects, etc. Hope that helps. Sounds like you've found ggplot to be a challenge, and it can be! But it is also very powerful, and once you get the hang of it you can do some pretty fantastic things with it. I'd recommend taking the time to sit down and learn about the idea behind how ggplot structures plots, as it will really help you understand it. This page has some links to some good introductions: [https://ggplot2.tidyverse.org/](https://ggplot2.tidyverse.org/)
I think geom\_smooth() would work fine. But... &amp;#x200B; stat\_smooth(mapping = NULL, data = NULL, geom = "smooth", position = "identity", ..., method = "auto", formula = y \~ x, se = TRUE, n = 80, span = 0.75, fullrange = FALSE, level = 0.95, method.args = list(), na.rm = FALSE, show.legend = NA, inherit.aes = TRUE) &amp;#x200B; where in all of that can I put categorical variables? 
Why don't you read [the vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html)? It's pretty straight forward. But basically, it's about conditioning (ie fitting a model on) more than just the mean. For your usual regression model, you have an equation that looks like `mu = x0 + x1 * b1 + error`, where `error ~ normal(0, sigma)`. With brms you can also put a model on the error, to for instance model that the error is greater in one condition than the other: `sigma = sigma_0 + sigma_1 * b1`. This is what the first example ("A simple distributional model") in the vignette does. 
Why would you expect that to work? A plot function takes variable names or expressions for its named arguments. 
Pipe operator, at the core, is a function. `a %&gt;% b(...)` is basically equal to `b(a, ...)`. It takes a left-hand-side and puts it as the first argument to the function on the right-hand-side. Now, that means that your desired function (the one you're piping to) should take the input data as its first argument. Which means that piping with `gsub()` function is not as easy as with tidyverse functions which are specifically designed to be easily pipeable.
The stuff in stringr should work better with piping in general. https://stringr.tidyverse.org/reference/str_replace.html
 library(stringr) corpusclean &lt;- corpus %&gt;% str_sub("\W", " ") %&gt;% str_sub("\d", " ") %&gt;% str_to_lower %&gt;% removeWords(stopwords("english")) %&gt;% str_sub("\b[A-z]\b{1}", " ") remember, piping works like this: for a given function f(x, y): x %&gt;% f(y) # or y %&gt;% f(x, .) Where the `.` signifies that's where you passing in the previous value.
Basically what everyone else said, pipes work better when used with other functions in the tidyverse. https://www.tidytextmining.com/ With that being said, Julia Silge and David Robinson have a great guide on that exact topic. I'd buy the book because it's nice to support them.
Such a great resource
i think η is supposed to be notation for the function, ie logit(zi)=log(zi / 1−zi)= η(zi) &amp;#x200B; zi is in \[0,1\] &amp;#x200B; zi / 1−zi is in \[0,inf) (plug in the bounds for zi) &amp;#x200B; logit(zi)=log(zi / 1−zi)= η(zi) is in (-inf,inf)
Well, first off you're displaying too many options on there. My code has very few options displayed in general, it makes it much more readable. For instance, if you use tidyverse, you generally just pipe the data directly in with your.dataframe %&gt;% geom_smoooth() and it knows what the data is supposed to be, no "data=" needed. GGplot is finnicky when it comes to factor variables, those are categorical variables with different levels like gender. You may have to specify as.factor(gender). Also, it depends how you want it set up. Let's say you want the points as different colors, plus a line for each different colored point/group. You could use color= as.factor (gender) in your geom_point, then for geom_smooth() you wouldn't put the group in the formula itself. Check out this resource: https://r4ds.had.co.nz/data-visualisation.html#geometric-objects Before my first R job, I read that book a few times over, just to give myself a basic grounding in the tidyverse. It's a nice way to code, though these days I am much more confident in base R as well as in data.table, which is another package that's useful for very large datasets. Right now, I'm waiting on a data pull of some 100 million observations, so eventually you'll get past tidyverse. But for now, tidyverse it up and reference that book whenever you need to. 
Tidytext is a great package that works well with piping. There is a free e-book that has great examples on how to use the package too
Try adjusting the tick number to something larger than 10?
Wow, thanks a lot for this detailed reply. I will bookmark this and reference this every time I have to use ggplot.
You mean nticks? Doesn't do anything unfortunately.
sapply(dataframe$column,function(x){x[[length(x)]]}) might work?
Use `dplyr` and `purrr`, something like: df %&gt;% mutate(y=purrr::map(mouse.y, function(x) x[length(x)]) %&gt;% unnest(y)
 a &lt;- list( autotick = FALSE, ticks = "outside", tick0 = 0, dtick = 10, # adjust this ticklen = 5, tickwidth = 2, tickcolor = toRGB("blue") ) s &lt;- seq(1, 4, by = 0.25) p &lt;- plot_ly(x = ~s, y = ~s) %&gt;% layout(xaxis = a, yaxis = a) &amp;#x200B;
&gt;I think geom\_smooth() would work fine. But... what does this function does? A categorical predictor is example: ''male'' ''women''...so why does OP needs more than one regression line, or even a smooth line?
Thank you! I have tried it (i need last timestamp, so i used this instead): library(purrr) library(dplyr) mouse_exp1 &lt;- mouse_exp %&gt;% mutate(y=purrr::map(mouse.time, function(x) x[length(x)])) %&gt;% unnest(time) 
It worked, but not in the way I wanted it to [link](https://www.dropbox.com/s/ze7bkw9faajgn0i/Screenshot%202018-11-16%2017.09.40.png?dl=0) 
Start up Rstudio and create a new R project in the cloned directory. Open up the project, edit the files as you wish, and when you want to test it press 'Build and Install' in the upper right panel. Here's a useful reference: (http://r-pkgs.had.co.nz/)
&gt;Why would you expect that to work? A plot function takes variable names or expressions for its named arguments. what does this function does? A categorical predictor is example: ''male'' ''women''...so why does OP needs more than one regression line, or even a smooth line?
Note that even if it's for your personal use, you might find it easier to have a Github clone of your own so that you can work in any machine without having to carry things over and install with install_github without having to have a local copy of the package in each machine. 
&gt;Grouping goes in aes() for the main call. and what does this main call make?
Yes, unnest might be found in the `tidyr` package instead. 
I am using anaconda environment. Does that mean that I need to move the cloned package into that environment? 
&gt; Are you asking what ggplot() does? It creates the ggplot object. Aesthetics are then passed to the object. And what are those ''objects'' in pratice? I am new to R, sorry
X axis and y axis
The aes function creates the "aesthetic mapping" between the data and visual representation. So if you want some variable in your data to manifest in the graph you need to set it to a visual property like, x, y, color, fill, whatever. A common mistake I've made a lot, let's say you want your points to be green, just because. You would not do that in the aes function. The points arn't green because of your data, they are just green because you set them to be green. If you wanted some of your points to be green because they are related to some categorical variable, then you would set the color equal to that categorical variable in the aes function. 
Should not matter.
Note the difference between '==' and '='. Using assignment in this context doesn't do anything unless the left hand side is a named argument of the function.
It stands for "aesthetic mappings". Usually it is used to set variables on the graph, such a X and Y coordinates on the plot, or fill colours as a potential third variable. It is a special type of object that maps colours and dimentions to variables. For more information, you can at an R prompt type "?aes" and hit enter, which will show you a help screen for it. &amp;#x200B;
This should help get you started https://r4ds.had.co.nz/data-visualisation.html
Well, the error itself is pretty common. Try: `1 + "1"` Trying to add a number to a non-number just doesn't work. &amp;#x200B; Is this raster::extract? I'm not really familiar with it, but basically something you're passing in is unexpected. Are variable.wm and data0cols in the correct positions? If not, I assume you've figured that out by now. But, the issue is either the position within the function or that one of those things is not what the function expects. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
The key phrase to google for is “moving average.” [Here is a tutorial with examples](http://uc-r.github.io/ts_moving_averages).
Use `sample()` to randomly select records and you can easily create new variables by matching the string of the country variable. Say the data is called wines: s &lt;- sample(1:nrow(wines), 500) wines.sample &lt;- wines[s,] # rows in the sample wines.not.sample &lt;- wines[-s,] # rows not in the sample It's a little trickier to get 100 per 5 reviewers, but you can use a for loop. This splits the data by reviewer and chooses 100 rows: wines.sample &lt;- NULL for(i in unique(wines$reviewer)) { wines.subset &lt;- wines[wines$reviewer==i,] s &lt;- sample(1:nrow(wines.subset), 100) wines.sample &lt;- rbind(wines.sample, wines.subset[s,]) } To create binary variables, ifelse is probably the easiest: wines$isUS &lt;- ifelse(wines$country=="US",1,0) # 1 if US, 0 otherwise wines$isFrance &lt;- ifelse(wines$country=="France",1,0) &amp;#x200B;
&gt; this is done to map features predictions on the real numbers (-inf,inf) to the interval of probability [0,1] could you please explain this in simpler words... I havent' understood. ''mapping the features/preditcors'' what does mean? 
Thanks but still doesn't work. What I'm trying to do is replicate this plot. You can see on the y axis that it goes from zero to 38 with no values between.
Can you trying to add row number to the data frame? I think you just do this: mtcars %&gt;% mutate(row_num = row_number()) Also I'm puzzled why c() is being used here: c(1:nrow(statement.skeleton))
Adjusting some of the things in the code I pasted above will likely get you what you need - mess around with it some
Without more information, it's hard to guess what is going wrong. You used "income" as the example variable. Does your data, perhaps, have "$"s in it? That is, instead of "100" it's "$100"?
If it's one character that needs to be removed this might help: `df$numbers &lt;- as.numeric(gsub(",","",df$numbers))` The other thing I can think of is removing white space which can be done with `trimws(x)`
 stringer::str_extract_all(df$income, “[:digit:]*”)
I might be misunderstanding the problem here.. if the data is in long format could you just filter the data frame like this. df %&gt;% filter(year %in% c(2014, 2018, slider$from, slider$to))?
Correct. I have that part functioning. However, how do I redisplay it to be in the wide format for shiny display? Like, can I do additional formatting to the data frame after my reactive function? If so, how? 
or it could be easier to just filter the wide format on it's column names. df[df %in% c('Category', 'SubCategory', '2018', as.character(c(slider$from:slider$to)), '2014')]
Good luck :)
Thank you very much. Would you like to be credited in my report?
Yes you can plot the interaction line if you know the intercepts and both slopes, statistically an interaction is really just multiplying both lines together. However, this doesn't really tell you anything if you don't have a valid data set. If I'm understanding you correctly and you're making up points to fit this graph you can't make any inferences about the data (can't tell how well it fits for example)
Using $ coerces the result to a vector, your package probably only takes vectors for that argument, while the other one takes more complex data structures.
Nah, it's all good.
Do you use the tidyverse set of packages? If so, blob might help you: https://github.com/tidyverse/blob
Can you provide an example of what the dataset looks like? I'd guess there's a number of different ways you can graphically represent this 
Lists don't have columns, so I can't see a scenario in which your first example would work. Do you have an example of a list to illustrate what you would like to select? You can find more information on subsetting [here](http://adv-r.had.co.nz/Subsetting.html).
Yes. Also, what do you mean by list? R uses matrix notation so list[x:y] will error if you're subsetting an object with 2 dimensions (e.g. A data.frame), because you're only giving it one, so it doesn't know what you want it to do. Similarly list[, x:y] will error if your object only has 1 dimension (e.g. A vector) as you're giving it 2 (you're asking it to give you all the rows, and columns x to y. Leaving it blank is the same as putting 1:nrow(list)) If you genuinely mean a list (a hierarchical object containing other objects) then both of those commands will error, as lists need double square brackets to determine what you want R to do (otherwise it'd get confused as to whether you're on about a list or not) Good luck
Data frames are lists
Yes, but it's heavily discouraged as doing it like that is really annoying and confusing to read, the objects returned are not identical, it uses more memory, and it's also slower (I think because it also creates rownames for the df[1] way of doing it). An experiment to prove my point: #make an example data.frame() df &lt;- data.frame( first = c(1,2,3), second = c(1,2,3), third = c(1,2,3) ) #df[3] returns a 2d object with rownames (and makes you think it's a vector when QCing someone's code, which is infuriating): df[3] #should return: #third #1 1 #2 2 #3 3 #on my machine it takes about double the time of df[,3] when scaled to only 1000 its system.time( for (i in 1:1000) { df[3] } ) #my machine returns: # user system elapsed # 0.037 0.000 0.037 #on the other hand df[,3] returns a 1d object (vector) df[,3] #should return this: [1] 1 2 3 #It's about twice as fast as doing df[3] system.time( for (i in 1:1000) { df[,3] } ) #On my machine # user system elapsed # 0.018 0.000 0.017 
Well one takes longer because, as you said, they do different things. 
yep, you guys are totally right. However, if you set up the data as anything else (e.g. a matrix) then it won't give you what you want (just playing with that now). This is why it annoys me. apologies! df &lt;- matrix(nrow = 3, c(1,2,3,1,2,3,1,2,3) ) df[,3] df[3] now return different things. 
Isn’t [,x:y] indexing the columns and the other defaults to rows since that is the first argument?
Is your R version up to date?
Lol, this is what I should've said, but I started an argument instead :|
Stupid question but I have to ask, is your computer connected to the internet?
That's pretty up to date. What is the output when running the install.packages command? It should tell you at the end if it fails. Usually it will say something like "installing package &lt;blah&gt; had a non-zero exit status" that tells you which package failed and so you can either scroll WAY up through all the compilation messages and find the error specific to that package or you can try installing it separately (e.g., `install.packages(&lt;blah&gt;)`) and see what pops up. This will help you diagnose the problem - likely some system dependency (e.g., a package you install with `sudo apt-get`) that is missing.
Um, I'm posting this on Reddit using my computer. Last I checked, Reddit uses the internet.
Nevermind if you're going to be sarcastic, I take the stance to never assume you could have been using R on a different computer.
 ERROR: configuration failed for package ‘rgeos’ * removing ‘/usr/local/lib/R/site-library/rgeos’ ERROR: dependency ‘stringi’ is not available for package ‘stringr’ * removing ‘/usr/local/lib/R/site-library/stringr’ ERROR: dependencies ‘curl’, ‘openssl’ are not available for package ‘httr’ * removing ‘/usr/local/lib/R/site-library/httr’ ERROR: dependency ‘httpuv’ is not available for package ‘shiny’ * removing ‘/usr/local/lib/R/site-library/shiny’ ERROR: dependency ‘units’ is not available for package ‘sf’ * removing ‘/usr/local/lib/R/site-library/sf’ ERROR: dependency ‘gdtools’ is not available for package ‘svglite’ * removing ‘/usr/local/lib/R/site-library/svglite’ ERROR: dependency ‘httr’ is not available for package ‘gh’ * removing ‘/usr/local/lib/R/site-library/gh’ ERROR: dependency ‘stringr’ is not available for package ‘reshape2’ * removing ‘/usr/local/lib/R/site-library/reshape2’ ERROR: dependency ‘httr’ is not available for package ‘covr’ * removing ‘/usr/local/lib/R/site-library/covr’ ERROR: dependency ‘stringr’ is not available for package ‘knitr’ * removing ‘/usr/local/lib/R/site-library/knitr’ ERROR: dependencies ‘curl’, ‘fs’, ‘gh’, ‘git2r’ are not available for package ‘usethis’ * removing ‘/usr/local/lib/R/site-library/usethis’ ERROR: dependencies ‘stringr’, ‘knitr’ are not available for package ‘htmlTable’ * removing ‘/usr/local/lib/R/site-library/htmlTable’ ERROR: dependencies ‘knitr’, ‘stringr’ are not available for package ‘rmarkdown’ * removing ‘/usr/local/lib/R/site-library/rmarkdown’ ERROR: dependency ‘reshape2’ is not available for package ‘ggplot2’ * removing ‘/usr/local/lib/R/site-library/ggplot2’ ERROR: dependency ‘ggplot2’ is not available for package ‘viridis’ * removing ‘/usr/local/lib/R/site-library/viridis’ ERROR: dependencies ‘git2r’, ‘httr’, ‘usethis’ are not available for package ‘devtools’ * removing ‘/usr/local/lib/R/site-library/devtools’ ERROR: dependencies ‘ggplot2’, ‘htmlTable’, ‘viridis’ are not available for package ‘Hmisc’ * removing ‘/usr/local/lib/R/site-library/Hmisc’ ERROR: dependencies ‘devtools’, ‘gdtools’, ‘shiny’, ‘svglite’, ‘xml2’ are not available for package ‘vdiffr’ * removing ‘/usr/local/lib/R/site-library/vdiffr’
Sorry, I actually never thought of that.
Don't have my laptop to check at the minute but I think this would give you something different with a single length index. I.e df[,1] vs df[1] the first gives you a vector, the second a dataframe. If I remember rightly. the reason would be is that a dataframe is actually a list of the columns but with some additional structure. 
Going by your username, I take it you use Linux? There's two ways to install dependences in Linux: * Without Root access... it will download to some Temp directory. Open a terminal, navigate to that directory, and do `R CMD INSTALL &lt;packages&gt;`. This may take a long time. * With Root access... Do `sudo R`and run `libPaths()`. Follow it up with `install.packages("tidyverse", lib="/path/to/library")` (tidyverse is a much more versatile package which contains ggplot2) **Further reading:** * https://linuxconfig.org/how-to-install-and-use-packages-in-gnu-r * https://clas.uiowa.edu/linux/help/applications/rpackage
You'll probably want to first split them into columns and then 'gather' (https://r4ds.had.co.nz/tidy-data.html) them into rows per group, so it looks like: show_id | genre ---|--- 1 | horror 1 | comedy 2 | romance 2 | comedy 3 | horror 3 | thriller 3 | comedy then you have a lot of options that depend on your exact application. you could do something like group_by(show_id) %&gt;% filter(any(genre=="horror" | genre=="comedy"))
The problem is those folders require root access, which could be problematic if you're not running as root. See [my comment below](https://www.reddit.com/r/Rlanguage/comments/9yjxyl/having_trouble_installing_ggplot2/ea1x10u/) about how to install these packages and their dependencies.
For future reference, I have no idea if a poster is behind a corporate firewall or proxy that could be inadvertently blocking access to CRAN or running R on an embedded device not connected to the internet and posting a question on Reddit using their personal mobile device (like I'm doing now). So I have to ask seemingly stupid questions because sometimes posters overlook obvious solutions to problems. So next time just hold off on the sarcasm as it doesn't help solve any problems and since you are the one requesting help you really shouldn't be disrespectful to people who are trying to help you.
And dplyr::separate is great for string splitting into columns.
OK, I won't make fun of people for that again.
Sorry about taking so long. I followed your instructions and it is still running.
No problem. Let me know how it pans out. First time installing is a doozy.
How long does `install.packages("tidyverse", lib="/path/to/library")` usually take, and is it mostly processing or network activity? It is still running.
It can take a while. On Linux, the default is to download all packages as source packages. A lot of packages have C++ code that needs to be compiled into .so files and most of the installation time (and all the messages you are probably seeing) are the steps involved in the compilation. For something like the Tidyverse - there are many packages and each has dependencies and those dependencies can have dependencies....so it will take a while. The network time is usually pretty small as source is pretty lightweight and I think CRAN has specific size limits for packages (a few MB at most) and so even packages with data are not so large.
It's mostly processing the information. About the time it takes to brew a cup of tea and browse /r/all/top. Depends on the computer. Also hopefully you didn't do a literal `"/path/to/library"` and instead opted for one of the directories that were present when you ran `.libPaths()`...
I have not done a literal `"/path/to/library"`, but i guess it isn't helping that my computer is eight years old.
&gt; my computer is eight years old. We're going to need a bigger cup of tea. My install took 15-30 minutes on a 4-year old computer. Back-calculating Moore's law it should take around 1-2 hours.
This won't help your issue with installing packages, but I'd highly recommend you download and install RStudio to use instead of the base R terminal.
That's the spirit. Come for the cats, stay for the ridiculously long install.
I would suggest you use a JOIN function from tidyverse: [https://dplyr.tidyverse.org/reference/join.html](https://dplyr.tidyverse.org/reference/join.html) There are different types of joins depending on what you are trying to accomplish with your data.
Yep, that's a good excuse. And it just finished compiling. I still get `Error in library("ggplot2") : there is no package called ‘ggplot2’`. ERROR: dependencies ‘knitr’, ‘stringr’ are not available for package ‘rmarkdown’ * removing ‘/usr/lib/R/library/rmarkdown’ ERROR: dependencies ‘reshape2’, ‘stringr’, ‘tidyr’ are not available for package ‘broom’ * removing ‘/usr/lib/R/library/broom’ ERROR: dependency ‘reshape2’ is not available for package ‘ggplot2’ * removing ‘/usr/lib/R/library/ggplot2’ ERROR: dependencies ‘xml2’, ‘httr’, ‘selectr’ are not available for package ‘rvest’ * removing ‘/usr/lib/R/library/rvest’ ERROR: dependencies ‘broom’, ‘tidyr’ are not available for package ‘modelr’ * removing ‘/usr/lib/R/library/modelr’ ERROR: dependencies ‘fs’, ‘rmarkdown’ are not available for package ‘reprex’ * removing ‘/usr/lib/R/library/reprex’ ERROR: dependencies ‘broom’, ‘ggplot2’, ‘httr’, ‘lubridate’, ‘modelr’, ‘reprex’, ‘rvest’, ‘stringr’, ‘tidyr’, ‘xml2’ are not available for package ‘tidyverse’ * removing ‘/usr/lib/R/library/tidyverse’
OK, I'll consider RStudio.
Unusual, but I guess you need a different flag. Make sure you're still in `sudo R`, and then try `install.packages("ggplot2", lib="/path/to/library/", dep=TRUE)` (I've never personally had to do the `dep=TRUE` flag, but if it works it works). If that works, move up to the tidyverse package. If you want, test it out on a smaller package like `broom`.
Doesn't work. ERROR: configuration failed for package ‘rgeos’ * removing ‘/usr/lib/R/library/rgeos’ ERROR: dependency ‘stringi’ is not available for package ‘stringr’ * removing ‘/usr/lib/R/library/stringr’ ERROR: dependencies ‘curl’, ‘openssl’ are not available for package ‘httr’ * removing ‘/usr/lib/R/library/httr’ ERROR: dependency ‘httpuv’ is not available for package ‘shiny’ * removing ‘/usr/lib/R/library/shiny’ ERROR: dependency ‘units’ is not available for package ‘sf’ * removing ‘/usr/lib/R/library/sf’ ERROR: dependency ‘gdtools’ is not available for package ‘svglite’ * removing ‘/usr/lib/R/library/svglite’ ERROR: dependency ‘httr’ is not available for package ‘gh’ * removing ‘/usr/lib/R/library/gh’ ERROR: dependency ‘stringr’ is not available for package ‘reshape2’ * removing ‘/usr/lib/R/library/reshape2’ ERROR: dependency ‘httr’ is not available for package ‘covr’ * removing ‘/usr/lib/R/library/covr’ ERROR: dependency ‘stringr’ is not available for package ‘knitr’ * removing ‘/usr/lib/R/library/knitr’ ERROR: dependencies ‘curl’, ‘fs’, ‘gh’, ‘git2r’ are not available for package ‘usethis’ * removing ‘/usr/lib/R/library/usethis’ ERROR: dependencies ‘stringr’, ‘knitr’ are not available for package ‘htmlTable’ * removing ‘/usr/lib/R/library/htmlTable’ ERROR: dependencies ‘knitr’, ‘stringr’ are not available for package ‘rmarkdown’ * removing ‘/usr/lib/R/library/rmarkdown’ ERROR: dependency ‘reshape2’ is not available for package ‘ggplot2’ * removing ‘/usr/lib/R/library/ggplot2’ ERROR: dependency ‘ggplot2’ is not available for package ‘viridis’ * removing ‘/usr/lib/R/library/viridis’ ERROR: dependencies ‘git2r’, ‘httr’, ‘usethis’ are not available for package ‘devtools’ * removing ‘/usr/lib/R/library/devtools’ ERROR: dependencies ‘ggplot2’, ‘htmlTable’, ‘viridis’ are not available for package ‘Hmisc’ * removing ‘/usr/lib/R/library/Hmisc’ ERROR: dependencies ‘devtools’, ‘gdtools’, ‘shiny’, ‘svglite’, ‘xml2’ are not available for package ‘vdiffr’ * removing ‘/usr/lib/R/library/vdiffr’
&gt; ERROR: dependency ‘stringi’ is not available for package ‘stringr’ There should be a compile error just above the stringi (or any of the other dependencies). Can you paste the text just above this line?
It won't fit in a Reddit comment, so you will have to go to https://pastebin.com/GuYJW3bP to see it.
Also some fancy autocomplete and lookup stuff for writing Stan models!
Just as I suspected. You need to download external dependencies as it states within the error messages. Exit R and execute the following command: `sudo apt-get install libcurl4-openssl-dev libssl-dev libgit2-dev libcairo2-dev libxml2-dev libudunits2-dev` Then re-run `sudo R` then `install.packages("ggplot2", lib="/path/to/library")` Sadly you'll have to repeat this if you run into any other dependency issues.
That fixed it. Why did they make it so hard?
¯\\\_(ツ)_/¯
OK. At least it should be better documented. The O'Rielly book and every tutorial I could find simply said to do `install.packages("ggplot2")` and it would work.
StackOverflow also told me to simply do `install.packages("ggplot2")`. I think they said that if it didn't work to try the dep=TRUE flag. I know pretty much nothing about R, so it would be hard for me to do trial and error. Thanks for the help.
Yeah gotta watch out for the external dependencies. Something I ran into and then a light bulb went off when I read the error.
I'll try to remember that.
Glad it works. Cheers.
Now that it works, I'm going to get off Reddit.
Is there a reason you can’t use a condition like `StartYear &lt; 2000 | EndYear &gt; 2000` to filter for employees who started before 2000 or stopped after 2000? It seems simpler. If you really have to create another column representing the full range of time, you may want to think about using package `lubridate`, specifically `lubridate::interval()`, and put interval objects in that column (I think you may have to make it a list column). That will make it easier to do more complicated comparisons with time intervals, if that’s what you need to do. 
The other responses don’t consider the fact that you can’t easily separate the column since it has a variable number of responses in it. However, tidyr::separate_rows() will do this for you. 
Hello! I have now added an example of the some of the data to the original post. I've just included the first replicate in each block for each set of experimental conditions though. I hope this little bit of data can show why I am confused as to what to do, as there just seem to be a lot of variables and I don't know how to account for them all. Using my extremely limited R knowledge, the only thing I can come up with is to produce many different subsets of data, and then produce box plots of the distributions within these subsets. However I don't know if this is right, and as this is for a report I think need to keep graphs to a limit and so am looking for a more succinct way of doing this.
`xlev` and `data` (or something else) is of two different lengths. How many rows does data have? How many numbers does xlev have?
Thanks! I figured it out just now. I'm using R for forecasting where I test for seasonality at a frequency of 12 instances ...my data ended September rather than December so the last 'year' of 9 months must've screwed it up. Your "how many rows does the data have?" made me think about it. 
And custom themes!
... Or you can use the geom that ggolot has for this exact purpose
Thanks for the recommendation. I'll look into that. How hard and time intensive is this project for someone who's proficient in R?
They're pretty different ecosystems. Excel shows you the numbers and hides the methods. R does the opposite. It takes a while to get used to that, and learn the useful functions in R. I think it took me about six months to feel fluent enough after switching from Excel. Ymmv, I was still doing my work for my full-time job but learning R on the side through datacamp. For your project you could probably use dplyr's group_by and summarize functions to make the two tables and join them by customer. From there you could maybe turn it into a heat map plot with labels to make it look like an Excel sheet? All of what you want to do is certainly possible, but making it like Excel is probably not the easiest. You might have an easier time just plotting the data through more commonly used methods. Maybe try looking up some ggplot2 cookbook examples and seeing if any might work for displaying your data in a format that's easiest for your colleagues? 
I'd consider using a data visualization tool like power bi. 
To make a shinyapp that displays this table (given the table is already cleaned!) and host it to a webservice so others can use it? I would say 2h. &amp;#x200B;
This. After tidying up your data, with columns 'Customer, Year, Q, Value', the rest is easily done with a few `group_by` and `summarize` calls.
Also, you can’t utilize filter by year in OP’s example. Great suggestion. 
Thanks for the time line information. I'm going to adjust my expectations and take my time to become comfortable. I'll also look into more display options. Maybe I'll find something that's easier and better.
Hey! I've been trying to figure this out recently and have hit a roadblock that all my google searching can't seem to solve. Hoping you might be able to help. I have my list of names in a vector called "lastname" but when I try to do sendKeysToElement(lastname) I get error: "unknown error: keys should be a string." I've tried a lot of different things (including using paste(), c(), and data.frame() to make the vector, adding quotation marks to the original data set and then reading it in that way but a bunch of slashes appear in the names and i'm not sure why). Here's my code so far: library(RSelenium) library(rvest) rD &lt;- rsDriver() remDr &lt;- rD[["client"]] race_scrape &lt;- function(lastname){ names &lt;- read.csv(file = "C:/Users/natha/School/5 Grad School/CHNA/names.csv", header = TRUE, stringsAsFactors = F) lastname &lt;- paste(names$LastName) remDr$open() remDr$navigate("https://mdocweb.state.mi.us/otis2/otis2.aspx") Sys.sleep(1) remDr$findElement(using = "id", "txtboxLName")$sendKeysToElement(list(lastname)) remDr$findElement("id", "btnSearch")$clickElement() Sys.sleep(2) html &lt;- remDr$getPageSource()[[1]] page &lt;- read_html(html) %&gt;% race_list_html &lt;- html_nodes(page,'.offenderRow .searchCol6') race_list &lt;- html_text(race_list_html) } race_scrape() I'm a super novice at this stuff so please bear with me.
You'll probably pick it up faster than me. I spent the first couple of months wading through base R before the tidyverse got its name, and that really clicked for me. By nine months I felt like I had broken even in terms of my invested time to learn and how much time I had now saved by using R instead of Excel. Now I really appreciate the versatility of R. Besides data analysis, I've used it to scrape webpages, make maps, do a little machine learning, learn some finance, get into data viz, and do text mining. The R notebooks are also an amazing feature worth checking out if you haven't done so already. If this is something you might be serious about, I'll reiterate what others have said and check out datacamp. They now have a tidyverse track that's curated by some of the top developers in R. I tried to spend 30-60 minutes a day going through 2-3 courses at a time slowly, taking notes on the lectures, and using their practice modules often. Last year they had a black Friday sale, so check it out soon if you can (or see if your work won't pay for a license like I got mine to do).
Tot.withinss is the total within cluster sum of squares, and you want this as low as possible. In terms of optimizating the number of clusters, there are several methods for doing this. https://uc-r.github.io/kmeans_clustering
I'd recommend this video to anyone looking into doing this type of analysis. It completely changed the way I used R. https://m.youtube.com/watch?v=qvPDE4ppAns
Going to make another comment so that it pings you. Watch this video and reproduce it. Wish I did when I first started https://m.youtube.com/watch?v=qvPDE4ppAns
Wow, that's long. Definitely will work through this. Thanks a ton!
Stop using setwd(). https://www.tidyverse.org/articles/2017/12/workflow-vs-script/ 
I love you.
I used to use autohotkey. You can have a shortcut that that pastes by replacing all slashes whenever I typed rpaste. Not sure if the code is still around but it was a one liner so it shouldn't be too difficult. 
Came here to post this and was so happy to see it already here. 
One question I have about the here package. I always use the Project feature in RStudio, and I've never used the here::here() function. Am I missing anything, or is it just an alternative to using the Project feature?
Doesn't "\\" work on windows? So If you save the path to a string variable you can replace all "/" with "\\". Not sure if this works as I'm not in front of my computer but if my memory is correct it should work.
Great reference. 
Lol I typed double slashes but Reddit changed it to a single
Haha same thing happened to me
So, if I understand this post correctly, if there are other users of my R script I should store the files that I need to access locally in the same folder as the code that I am working on?
Oh my god. Thank you. 
You could use readline() to initialise IO then paste the path as is. The result will auto escape the path into an R friendly string 
 --- title: "..." author: "..." date: "`r Sys.Date()`" output: html_document: self_contained: no --- at the begin of .Rmd file should help :)
Thank you so much! It worked like a charm! I still find it so awesome how reddit is such a good platform for R :) 
That's how I work. Everything I need is in a project folder (except maybe some master data files that are stored elsewhere, but those paths never change). In order to call something into R all I have to do is `read_csv ("file.csv")` , or `read_csv ("data/file.csv")` 
Alright, so, in a way I've been doing industry best practice on accident all the files I read into R are located in a share drive which can be located from any computer that had access to that share drive.
Whoever wrote this survey is seriously out of touch. 
Not at my computer, but try wrapping your data in "data.frame()"
Thanks! 
Thanks for sharing this. But due to the name I thought it was related to this jekyll template: https://github.com/jekyller/vitae I guess the idea is a popular one!
library(readr) library(purrr) map_df(list.files("/path/to", full.names = TRUE), read_csv) You’re welcome ;)
Not home now but if this works: I love you. If it doesnt work: Thanks! :D
I use variations of this all the time so feel free to follow up if you’re trying to do anything fancier. For example, including the name of each constituent file as a field, etc.
`?add_predictions` takes the model as the second argument. The error is telling you that `citation.grid` is a data_frame rather than a `lm` (or any other model that has a `predict()` method.
&gt; Note the difference between '==' and '='. Using assignment in this context doesn't do anything unless the left hand side is a named argument of the function. what is a ''named argument'' of function? basically what?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/econometrics] [Symmetric double sum in R](https://www.reddit.com/r/econometrics/comments/9zxsrr/symmetric_double_sum_in_r/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Yeah sure, t=1 so the outer sum dosnt sum over anything
So basically I can just sum the residuals and then square the sum? And the second sum is not relevant?
You can try using the `countrycodes` package to standardize country names.
sum(sapply(1:t, function(i) sum(res[1:i]) ^ 2)) Assuming n is 200 and u_hat is res.
I think they just had inspired by the tidyverse logoy
Now you're going to want to wrap it in a function like the post from earlier: library(RSelenium) library(rvest) rD &lt;- rsDriver() remDr &lt;- rD[["client"]] names &lt;- read.csv(file = "C:/Users/natha/School/5 Grad School/CHNA/names.csv", header = TRUE, stringsAsFactors = F) names$LastName &lt;- as.character(names$LastName) names$FirstName &lt;- as.character (names$FirstName) function &lt;- race_scrape(lastname, firstname){ # think of this as proof of concept #lastname &lt;- names$LastName[1] #firstname &lt;- names$FirstName[1] remDr$open() remDr$navigate("https://mdocweb.state.mi.us/otis2/otis2.aspx") Sys.sleep(1) remDr$findElement("id", "txtboxLName")$sendKeysToElement(list(lastname)) remDr$findElement("id", "txtboxFName")$sendKeysToElement(list(firstname)) remDr$findElement("id", "btnSearch")$clickElement() Sys.sleep(2) html &lt;- remDr$getPageSource()[[1]] race &lt;- read_html(html) %&gt;% html_nodes('.offenderRow .searchCol6') %&gt;% html_text() return(data.frame(race)) } scrape_safe &lt;- function(lastname, firstname){ result &lt;- try(race_scrape(lastname, firstname)) if (class(result) == "try-error") { cat("Error encountered for name:", lastname, "\n") return(data.frame()) Sys.sleep(runif(1, 1, 3)) } else { return(result) } } names.df &lt;- data.frame(names$FirstName, names$LastName) names.df &lt;- names.df %&gt;% group_by(LastName, FirstName) %&gt;% do(scrape_safe(.$LastName, .$FirstName)) This will return a dataframe where each row is a last name, first name and race. The scrape safe function will prevent this from failing if a page times out or some other error occurs when searching for a name by moving on to the next one and returning a blank dataframe. 
It's a good point. I updated the example showing how the data looks like after and before. http://www.thinkingondata.com/how-to-create-a-heatmap/ I will try to implement that in future examples. 
Thanks!
Not enough info to help. Please post a sample of your data and the code you have tried already
Sounds like your problem is that dob isn't at date. Like you said it's a factor and for all R knows it's just a factor of numbers. You have to convert dob to an actual date format first. [This may help you get started] (https://www.google.com/amp/s/www.r-bloggers.com/date-formats-in-r/amp/)
**Direct link**: https://www.r-bloggers.com/date-formats-in-r/ --- ^^I'm&amp;#32;a&amp;#32;bot&amp;#32;-&amp;#32;[Why?](https://np.reddit.com/user/amp-is-watching-you/comments/970p7j/why_did_i_build_this_bot/)&amp;#32;-&amp;#32;[Ignore&amp;#32;me](https://np.reddit.com/message/compose/?to=amp-is-watching-you&amp;subject=ignore&amp;message=If%20you%20click%20%27send%27%20below%2C%20the%20following%20action%20will%20be%20taken%3A%0A%0A%2A%20The%20bot%20will%20ignore%20you%0A%0AYou%20will%20receive%20a%20confirmation%20in%20reply.)&amp;#32;-&amp;#32;[Source&amp;#32;code](https://github.com/bvanrijn/aiwy)
You are doing two completely different things in the two examples. (and you also shouldn't be calling a function and a variable by the same name 'roll') &amp;#x200B; The problem is that roll\[i\] for i &gt; 1 returns NA, something you could easily find out yourself, and if you look at roll you should understand why.
This. Another issue I see is you'll be overwriting `G_than` and `L_than` with each iteration of the loop.
Datacamp is best its 100% worth it but wait for sale. Dplry course is really good though me why this is do good package
You could use sapply(1:100, roll)
This is a simple check, but i always get caught in this trap. Check the type of variables you're using with str(df) just to make sure R is reading the data frame the way you think it should, such that Y is numeric and X is character.
Try geom_bar(stat="identity") Also make sure you match the names of your fields - it looks like you have X not x in your table
&gt; Aes represents aesthetics, you could assign your x axis,y axis, color or group , for example ggplot(dataset, aes(x=xvalfromdataset, y=yvalfromdataset,color= any discrete variable from dataset) any discrete variable from dataset as colourfor x and y?
&gt; Where did you come across that phrase? The brms package is a high-level interface to Stan, a probabilistic programming language/Bayesian model fitting software package. So, brms can be used to fit a wide variety of statistical models, and it has a lot of flexibility with respect to the distributional assumptions one makes about the data. so bascially it can fit regression even on data non normal distribution?
This is a pretty specific error and you should probably post some example code/part of the problem column
Correct, the default stat is count() to make a histogram. You’ll need to set the stat to identity. 
One option is to try and remove any special characters after executing the SQL query / before writing the dataframe to csv. This example using dplyr/stringr would remove any non alphanumeric/punctuation/space characters: `df &lt;- df %&gt;% mutate(column_name = str_replace_all(column_name, "[^[:print:]]", " "))` &amp;#x200B;
Or `geom_col` which is just a shorthand.
The bulk_read (or read_bulk, I can never remember which way round it goes) package will do this.
In this example, for this dataset, it is not posible to use geom_tile, because the way that geom_tile works is to divide all the data into smaller rectangles or squares. Each of the smaller rectangles or squares is called a tile. There is no parameter to consider different scales, for columns or rows, because geom_tile assumes all the dataset is expressed in the same unit. In this example we have variables expressed in different units like goals, performance, points, and there is no relationship between them. On the other hand, heatmap allow you to use the parameter scale, and for this case we are using scale= "column", indicating that the values should be scaled for each column. 
Yes.
 library(tidyverse) df &lt;- data_frame(unique_id = c("John", "Luke", "Sally", "Andy", "Kyle")) ab &lt;- df %&gt;% select(unique_id) %&gt;% mutate(seed = runif(nrow(.), 0 ,2)) %&gt;% mutate(group = case_when( seed &gt; 1 ~ "A", seed &lt;= 1 ~ "B" )) %&gt;% select(unique_id, group) df &lt;- df %&gt;% left_join(ab, by = "unique_id") 
 df &lt;- data.frame(IDs = c("blah", "glob", "blah"), info = c(1, 2, 1)) id &lt;- unique(df$IDs) factor &lt;- rep(NA, nrow(df)) a &lt;- NA for (i in seq_len(unique(df$IDs)) { if (df4info[df$id == id[i]][1] &gt; 1) { a &lt;- "A"} else { a &lt;- "B"} factor[df$id == id[i]] &lt;- a }
If you include both male and and female in the same regression you are going to experience perfect multicollinearity. Remove one gender variable from the regression, in this case the male variable. Interpret the output of your new regression as the excluded variable (male) being your base. Ie: your regression should show what the effect of being female is compared to a male baseline.
This is hilarious. I never would have put two and two together. Two weeks notice and an urgency to learn R for medical research.
Very good post. a lot of impressive information is here... Thanks keep posting [web design pakistan](https://www.hubsol.com/)
To expand on this a little, when you have a malformed csv, read it in as text using readLines(), implement the fix and write it back to disk. Only after the offending character is gone should you try to parse the file.
Output missing. 
Oh you're right. Sorry! Here it is: https://imgur.com/a/c5J9Oi8 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/2zaE0tc.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Sorry buddy. Don't know. 
No worries, mate. Thanks though =) 
I don't know exactly, or what it is you need, but roughly it'll be a geom_line for the data + a geom_smooth for the regression and the same for the residuals. And then a second/dual y axis for the residuals, with sec.axis. if you google a bit on this you should be able to find a tutorial. :)
This is probably just base R. It’s a time series plot and someone added the lines manually with `lines`. Then they added a second axis. It could also be a `zoo` plot. You can do multiple time series automatically. Then you would add the the second line and second axis manually. The tricky part would be calculating the secondary scale. You’d need to translate the second data vector to the scale of the first, and the scale of the second axis. 
Thanks, I'll try generating a similar output using lines. I'll let you know what the result looks like
need residuals and regression model for the predict better
 df &lt;- df[-sample(which(df$CreditStanding=="Good"),200),]
Thank you, will try it when I get home. 
 dplyr::lead() dplyr::lag()
Thanks for the reply, unfortunately, that's not exactly what I'm talking about. 
 Thanks for the reply, unfortunately, that's not exactly what I'm talking about. Those don't give the previous value of the column I am trying to calculate. What I am trying to do could be thought of as a running total.
one way to get a simple version of this (assuming by "SE boxes" you mean what I would call error bars): library(ggplot2) Dummy$category &lt;- c('A','B','C') ggplot(Dummy, aes(x=category)) + geom_bar(aes(y = Percentage_of_x), stat = 'identity', position='dodge')+ geom_errorbar(aes(ymin = Percentage_of_x - x_SE, ymax = Percentage_of_x + x_SE), position = 'dodge', width = .33) &amp;#x200B;
You’re trying to make the first dataframe look like the second dataframe?
Yes. With the second table in excel, column E (Start Date ADJ), would have equation **E2**=IF(A2=A1,MAX(**E1**\+C1,B2),B2) such that, it's looking at the previous value in column E.
ggplot does not have a direct way to do this by simply giving it an angle. However, you calculate the start an end coordinates of each arrow that you want to draw and then supply that data to `geom_segment` in order to create lines on top of your points. Here is an example. arrow.length &lt;- 1 #aka the hypotenuse data.frame(x = 1:10) %&gt;% #sample x data mutate(y = -x, #sample y data angle = seq(0, 359, by = 36), #sample angle data radians = angle*(pi/180), #calculating radians from degrees since R trig functions use radians xEnd = (cos(radians)*arrow.length) + x, #cos(theta)=x/hypotenuse yEnd = (sin(radians)*arrow.length) + y) %&gt;% #sin(theta)=y/hypotenuse print() %&gt;% #to see the output ggplot(aes(x = x, y = y)) + geom_point() + geom_segment(aes(xend = xEnd, yend = yEnd), arrow = arrow()) [This is what the output looks like](https://imgur.com/a/zGjbtXd) 
You could still use suse segments(...) and specify your end coordinates as the sum of your start coordinate + the respective sine and cosine function of the angle. Using made up data to try and recreate your goal &amp;#x200B; `df&lt;-data.frame(x=c(1,2,3,4),y=c(5,3.5,2,1), angle=c(80,45,200,90))` `# sin() and cos() use angles expressed in radians, so convert them to degrees using this function` `degrees &lt;- function(deg) {(deg * pi) / (180)}` `# plotting using base graphics with aspect ratio 1/1 (otherwise your segments are not all equally large):` `plot( x = df$x, y = df$y, asp= 1)` `segments(x0 = df$x, y0 = df$y, x1 = df$x+cos(degrees(df$angle)), y1 = df$y+sin(degrees(df$angle)))`
thank you
thank you
It’s coming out soon-ish...The publisher has hired technical reviewers and they are looking for completion ASAP. 
I would give you a billion upvotes if I could. Thank you for explaining this! I did a bunch of trial runs (not with 10k samples) to really get a good understanding of how this works. What you did and your explanation is perfect. Thank you so much!
Try running ```Sys.getlocale()``` on both builds. My guess is one of the two doesn't properly identify your locale settings.
Thanks! I looked and they were different. Running `Sys.setlocale("LC_COLLATE", "en_US.UTF-8")` on the first build seems to have fixed the issue.
Merhaba :) the best way to learn what you need for data science work is to go to job posting sites for data scientists and see what they request, then try to learn the most commonly requested things. It may differ a bit from market to market. Basically, reverse engineer your skill set ! Iyi sanslar!
Thank you it is clever 👍🏻 thanks for Turkish words too :)
If your aim is to get out, try to find a grad school outside. Depending on the school you might get paid a living wage which gives time to improve and meet people without any cost. Some schools encourage stats departments to actively collaborate with other departments. If an opportunity arises, take that so you can get some domain specific experience. You'll probably end up in conferences which often have industry people on them too which you can use for networking and try to land find a job before graduation.
Thanks for your thinks :) the community is real helpful 
Isn’t SQL considered the language of data science though?
This mindset is present in a lot of governments and corporate organizations, but from a science or university perspective it's usually not the case. Even in university Data Science offerings (I've taken quite a few from a wide variety), they largely teach R and might brush on some SQL or graph databases. The problem with SQL is that it is in fact so structured that you have to have the structure ironed out before you can even generate any data whatsoever. This is very useful for some places, such as ecommerce, HR, warehousing, and other business oriented things, but for most scientific tasks it's pretty useless. Even with regard to web analytics it's not used as often because of the overhead required and the centralized and transactional/ACID nature of relational databases requires an insert to complete over on X server before Y server can insert a new record. This is why NoSQL was invented, which led to key/value based data stores that didn't need to be up-to-date all the time and could simply gather information and merge them together later (such as the clicks you do online, searches on Google, etc, that sort of analytics over distributed computing). But again, from a research and science perspective, you are usually doing one of two things. One you could be merging large datasets together that have never been merged before (so they would largely not be on the same systems and have the same SQL access, for instance) so you can't really use SQL to join two tables across systems that aren't allowed to connect to each other. The other common thing you could be doing is that you are generating or cleaning up data from a source that isn't concretely defined and will be figured out later or is based on simple readings from individual scientists making notes in journal form. So unless you are in marketing or business analytics for an individual organization, or work for a governmental intelligence or statistical agency (which even they don't always use SQL), SQL isn't much use to data science. It's also heavy on requiring access to the centralized server, and harder to morph the data into something new that you might want to do, or to port it around on a laptop for instance. That can be useful if you are trying to secure certain data, but you'll get into major fights when servers get slammed by a data scientists' inefficient queries (which most data scientists know nothing about efficiently writing code) and take out the warehouse inventory systems, for instance.
I'd suggest you look into the plot.ly package: [https://plot.ly/r/sliders/](https://plot.ly/r/sliders/)
Thanks for the reply. I’m currently using a PostgreSQL db for environmental science research but I’ll admit that I’ve found that interfacing with R via RPostgres has been my default workflow. I basically just use the db for storage and manipulate data mostly in R. There are definitely still some efficiency gains from performing as much data transformation in SQL as possible. Sometimes my data is so large before a query that I don’t have enough memory to read it into R!
SQL is always going to perform better. For your purposes, SQL is probably the best bet, actually. Just like an individual organization would benefit as I was describing, you are benefiting as an individual as well. This is probably going to be the case especially if software is designed that gathers to an SQL database for you. Really it's a matter of which is the best tool for the job, but in large scale, in-memory solutions tend to be where people go. Generally there are large Hadoop infrastructures behind this, though, for large shared RAM pools that are terabytes upon terabytes large. This is why so many data-centers have gotten so big. That being said, R or other "in-memory" solutions can benefit from the same techniques that SQL is performing internally. I remember taking a class once where they said "minimum requirements are 16GB RAM in your computer, so use our 32GB RAM shared server or a decent laptop". I had a laptop with 4GB of RAM, and didn't use the server at all. Most people, since it was shared, were trying to fight over resources and the server crashing all the time. What I did instead was pull into memory only small bits at a time, do miniaturized versions of the calculations, then put them all together at the end of the code. Basically breaking the math down into smaller problems on smaller datasets. What resulted was me getting everything done faster despite not having enough RAM to perform the task since the servers kept going down. Most people didn't even complete that assignment. So there are tricks that can be done to not use all of the data at the same time, essentially, and minimize your RAM footprint much like SQL does internally, but if you don't have to do it SQL is a better option which will be wayyy more optimized than anything you write. Problem is, most datasets that are published are CSVs (SQL databases are hard to publish), and most things kicked around between organizations have to be flat files for similar reasons.
That's certainly one way to do it (assuming that 30:39 are the admission IDs that correspond to emergencies). You could also do this with the tidyverse: ``` fake_hospital_records %&gt;% select(fake_admission %in% 30:39 ``` The second line of code doesn't work because you aren't specifying any columns for the dataframe. You would need to write: ``` fake_hospital_records[fake_hospital_records$fake_admission %in% c(30, 31, 32, 33, 34, 35, 36, 38, 39), ] ```
Thanks! That worked :) 
For the original method and not the tidyverse method you can also use seq(30,39) or the colon syntax as well. fake_hospital_records[fake_hospital_records$fake_admission %in% seq(30,39),] This also has the benefit of looking cleaner than the colon format IMHO.
ggplot2 and gganimate packages might help you (I have used ggplot2 myself, but I have only seen other people using gganimate, with some nice results… so I can’t guide you further on gganimate, but if you have doubts on anything else, feel free to ask) https://github.com/thomasp85/gganimate
that is, if what you want is an animated video or gif… if you want an interactive plot, the best solution would be using a shiny app, as suggested
I’m sorry, what? SQL is used all the time across more than just government organizations or whatever. Do you have any evidence for that assumption? It is literally the backbone of any company that has data. How else do you query a database?
I do not have any evidence for that assumption because it is an assumption you invented for me. I never made that assumption. I literally said SQL is for production IT systems. For computer science and programming, SQL is essential. We are talking here about data science, which is a completely different field. That being said, anymore data science is just interpreted as "web analytics" and "business analytics" and even "computer science", but it used to be a completely different field. It used to mean econometrics, bioinformatics, and other actual sciency things. As for government stuff, I was referring again to governmental statistical analysis, not production DMV systems. This is /r/Rlanguage... the R programming language is largely geared toward sciency data science, not web analytics and business analytics/BI stuff.
That did the trick! Thank you!
 df &lt;- data.frame(season = 1:4) for (i in 1:4){ text &lt;- paste("Season", toString(i), sep = " ") df[[text]] &lt;- ifelse(df$season == i, 1, 0) } df
You might want to post sample code so people can give you a hand with it.
\`model.matrix\` is a useful function for one-hot encoding as well. Let's assume your season field is attached to the classic iris data. &amp;#x200B; \# create some data my\_iris &lt;- iris my\_iris$season\_ &lt;- as.character([sample.int](https://sample.int)(4, size=nrow(my\_iris), replace=T)) head(my\_iris) one\_hot\_iris &lt;- as.data.frame(model.matrix(\~ . -1, data=my\_iris))
Do you mean " all the area under 0.4 on the **x**\-axis"? Anyway, the `DescTools` package provides the `Shade` function that I think does what you're asking for. library(DescTools) curve(dchisq(x,3), xlim=c(0,20),type="n", las=1, ylab="") Shade(dchisq(x,3), breaks=c(0,0.4,5, 10,20),col = c("blue", "black"),density=c(50, 0)) See `?Shade` for how to tweak the colors
You could create every combination of the 4 letters using `expand.grid` and then subset for the rows that have no duplicates. out &lt;- expand.grid(c("C","R","A","N"),c("C","R","A","N"),c("C","R","A","N"),c("C","R","A","N")) k &lt;- apply(out, 1, function(x)anyDuplicated(x))==0 as.matrix(out[k,], rownames.force = F) See also the `Permn` function in the `DescTools` package which will do this for you. If this is a homework problem and you have to write a loop, then look at the source code for `Permn` for some hints by entering `Permn` in the console without parentheses. &amp;#x200B;
Creating your plot with ggplot and then calling ggplotly on it may be all you need to do. Example with toy data: x &lt;- rnorm(100) Fn &lt;- ecdf(x) d &lt;- data.frame(x = sort(x), y = sort(Fn(x))) library(ggplot2) ggplot(d, aes(x, y)) + geom_line() library(plotly) ggplotly() # creates interactive plot After calling `ggplotly()` you should be able to hover your mouse pointer over the line to see coordinates. That sounds like what you're trying to do.
I recommend long format for most work as well. I figured they knew what they were doing when posting the question, maybe for feature building on a learning model or something. 
I think you're a bit confused about how the polygon function works. The first argument to polygon should be a vector of x values, the second should be a vector of y values, and then after that you can specify colour. You can get most of the y-values from the output of dchisq, but you'd need to specify to more corners to the polygon with y-values of 0. I'd implement this with something like: xseq=seq(0,20,0.01) plot(dchisq(xseq,3)~xseq,t="l") polygon(c(5,xseq[xseq&gt;=5 &amp; xseq&lt;=10],10),c(0,dchisq(xseq[xseq&gt;=5 &amp; xseq&lt;=10],3),0),col="red")
[This link](https://rstudio-pubs-static.s3.amazonaws.com/84527_6b8334fd3d9348579681b24d156e7e9d.html) is similar to what you might be asking, not sure what the year range is but I presume you could start by separating each metric by year 
Thanks for the response, the link provided looks like it could be helpful. I am not 100% sure on how to do this, and had a couple questions. I know that I can subset the data frame for a specific year, but how would I group events by year? Thanks again for all your help! storm_trend[storm_trend$Year == "1964",] &amp;#x200B;
[This blog post](https://juliasilge.com/blog/tidy-word-vectors/) has a great method for calculating skip-gram probabilities. Those might be more interesting than just adjacent words.
Make a long/tidy dataframe. Make an `index` column for x by observation number (just a sequence of the positions in the vector). Then another `variable` column for the parameter saved in that row (y_test, ymle_predict, ymap_predict). Then a final `value` column for you y or value (the numbers inside your vectors). With this you can use ggplot, where x = index, y = value, color = variable. All 3 values will be shown along the x axis lined up and colored differently. You might be able to use base plots in wide (3 columns in a dataframe) but I honestly am not sure. 
Pretty difficult to understand what you want but here's a Tidyverse solution: With ggplot2, you should put things in a dataframe/tibble then just map the columns to the aesthetics you want: library(tidyverse) mydata &lt;- tibble(ytest, ymle_predict, ymap_predict, i) %&gt;% gather(method, value, ytest:ymap_predict) ggplot(mydata, aes(x = i, y = value, color = method)) + geom_point() + geom_line() https://i.imgur.com/mxp8bZB.png
Interesting! Yeah that's what I'm looking for, finding something on either side of the word. I was thinking maybe there's a way to create a bunch of mini buckets of words, like 5 words before and 5 after each mention of the EU, and then compare those all to each other? (Forgive the non-technical explanations, I'm still getting the hang of all this.)
Thank you so much! I was struggling a lot but your recommendations really helped. And yeah it’s homework but I looked at the source code and figured out how to do it so thank you again!
With your help and a bunch of tweaking on my end, I finally got it to work! Thank you SO much for all your help and guidance. If there's any way I can repay you don't hesitate to ask!
This is awesome! Thank you!
There is a package called reshape2 you can look into. Here is an example using your data: library(reshape2) df &lt;- data.frame( name = c("Jake","Jake","Jake","Alex","Alex","Dan","Dan"), year = c(2015,2016,2017,2016,2017,2017,2015), var1 = c(5,7,8,5,2,13,7), var2 = c(3.2,12,4,3.2,6,20,3), var3 = c(7,2,4,7,9.1,6.5,2), var4 = c(2,1,5,2,6,7,6) ) dcast(melt(df,id.vars = c("name","year")),formula = name ~ year + variable) 
Stand on the shoulders of giants!
This definitely works for the scenario I laid out, and I don't see why it wouldn't be scalable so that's perfect. Thanks a lot! 
This is very cool. Thanks for sharing!
Do your factors have non-numeric symbols in them, or locale specific ones like currency signs or commas?
How do I estimate "season 1"? This is quarterly data &amp;#x200B;
Yes, they have currency signs. $
And commas
And commas
Thousands 
That worked! Thank you so much.
Make a prediction for it or estimate its effect? One of them has to be the baseline so the other seasons are interpreted in terms of compared to season 1 is the way I interpret it.
Thank you! 
I use vaply to generate tables with different statistics for different variable. As yzpaul said integer(1) refers to the class and length of rows you will have. If you have 5 lists in log and you are only using one function you will have one output per list. Otherwise if you have two functions you have to use integer(2) and so on 
Coercing factors to numeric directly will give you the index of the factor level rather than the factor label, even if that label is a number.
Are good and bad the only options? I prefer to think of binary encoding 1/0 as True/False or Present of/Absence of. In this case. 1 = presence of good 0 = absence of good "absence of good" could be bad or it could be another response depending on how the data was collected.
Yes, good and bad loans. 
Will there only ever be a single match between datasetA\[,4\] and datasetB\[,1\]? Right now if there is more than one match it will take the info from the last match in datasetB.
That's a good point! Luckily for these datasets there is only one match.
Are you asking how to aggregate the data and compute group means of frequencies? Or are you asking how to format it so it looks like the way you stated? Have you also considered specific software made to analyze text and frequencies of words and phrases?
Yeah, I used the code you suggested with some minor tweaks to work on a much bigger dataset and it worked exactly like I needed. :)
Not really, no; things like `lm` can handle them just fine, with the added benefit of a nice label. It also makes plots a bit nicer since if you are choosing the colour based on the level of the factor then you get nice labels on the plot.
Hey, There’s a few packages out there for this. A popular one is caret() and it has some cross-validation functions in it. You can read about them in the links below. I also recommend the book “Introduction to Machine Learning with R” by O’Reilly publishers. [Caret()](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) [Cross-Validating](https://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package/) [Introduction to Machine Learning with R](https://m.barnesandnoble.com/w/introduction-to-machine-learning-with-r-scott-v-burger/1127035141?ean=9781491976449&amp;gclid=Cj0KCQiA3IPgBRCAARIsABb-iGLhETWcKSD_s5y90Q_grMZ6hWzYE-NO2x_P1etqD_2xZiLCh5xxyGwaAgKKEALw_wcB) 
[removed]
That did it! &amp;#x200B;
I get similar errors. \`ggmap\` is really going to be deprecated soon in favor of leaflet, I think? The new update to \`ggplot2\` has a lot of mapping built in with \`sf\`. I don't get the sense they're still working on it. &amp;#x200B; to actually address your question though: sometimes when I get this error, I change the map source. Google only allows you to download so many maps per time period, so if you're doing it repeatedly to get it 'just right' you may have overstressed it. Try the osm option that goes with \`get\_map()\` and see if that works better.
Thanks for the response. It is kinda frustrating. I found this page on github addressing the issue and it seems like a lot of people are struggling with the same issue: [https://stackoverflow.com/questions/51481913/mapping-in-ggmap-with-api-key](https://stackoverflow.com/questions/51481913/mapping-in-ggmap-with-api-key) It doesn't seem to be able to connect to the internet for some reason. This is the first time I am trying to run the code but to no avail. As for trying the "osm option" do you know about any documentation on it that I could read up? I am very much a newbie to the programming/R aspects of Data Science and am learning things on the go as I work on projects. Probably should take a dataquest clcass on this or something lol
I just went to googlecloud to enable my geomap and geocode API(s), I am getting one less warning than before but it still doesn't work :/ I get the same error and when I paste the link ([https://maps.googleapis.com/maps/api/staticmap?center=Europe&amp;zoom=4&amp;size=640x640&amp;scale=2&amp;maptype=terrain&amp;language=en-EN&amp;key=xxx](https://maps.googleapis.com/maps/api/staticmap?center=Europe&amp;zoom=4&amp;size=640x640&amp;scale=2&amp;maptype=terrain&amp;language=en-EN&amp;key=xxx)) on to a browser I see the error message: " The Google Maps Platform server rejected your request. The provided API key is invalid. "
this used to work: \`\`\` map &lt;- get\_map(location = "Europe", zoom = 4, source = "osm") \`\`\` &amp;#x200B; but doesn't today. I think google somehow changed its API in a way that ggmap hasn't caught up with. This explains why my old shiny app with ggmap doesn't work at all anymore. &amp;#x200B; I guess you're gonna have to learn how to use \`sf\` with \`ggplot2\`. Sorry. &amp;#x200B; As for reference, there's a great ggmap reference document: [http://stat405.had.co.nz/ggmap.pdf](http://stat405.had.co.nz/ggmap.pdf) &amp;#x200B; but if you can't download maps anymore, it doesn't matter how good the guide is. 
Seriously just try viā sf. 
So I started following this tutorial on sf and ggplot 2 for getting maps ([https://cfss.uchicago.edu/geoviz\_plot.html](https://cfss.uchicago.edu/geoviz_plot.html)) and I tried the following code (after installing the packages ofc): library(tidyverse) library(sf) options(digits = 3) set.seed(1234) theme_set(theme_minimal()) usa &lt;- st_read("data/census_bureau/cb_2013_us_state_20m/cb_2013_us_state_20m.shp") And then I get this error: "Error in CPL\_read\_ogr(dsn, layer, query, as.character(options), quiet, : Open failed."
Compared to any other season
Do your own homework
You should avoid for loops, r got lot of ways to avoid it. I would merge datasets then filter and then make new columns with replaced data. With new columns you can check how many records was changed etc.
Did you even try? Or just throwing the homework questions out there? 
I try out, but my output is showing error
Please post error and maybe we can help 
do you have the source data? Also have you tried loging the data?
The problem is that your feeding in vectors rather than allowing the nse in ggplot2 to do the work for you. Try this: ggplot(df, aes(gene, value, fill = direction)) + geom_bar(stat = “identity”)
&gt; A popular one is caret() and it has some cross-validation functions in it which kind of K-cross validation would you suggets me in order to get the best randomized results? When I say random it means that the choosing of training and testing data is taken in a random order (first 6 data, then the last 3 .... and so on) Finally an istogram with the values it's plotted to give me which cross validation tecnique has been more reliable. Any tool for this? 
If I was interviewing someone for a data science role and they said that they didn’t know SQL, then that would be the end of the interview. 
I think I might be missing something, this code does not seem to order any thing and stacks the duplicate "a" values on top of each other library(ggplot2) vals &lt;- c(6,3,9,8,1,4,4,5,2,6) directions &lt;- c(rep("up", 3), rep("down", 2), rep("up", 2), rep("down", 3)) genes &lt;- c("a", "b", "c", "d", "e", "f", "g", "h", "i", "a") x &lt;- data.frame(gene = genes, direction = directions, val = vals) ggplot(x, aes(gene, val, fill=direction)) + geom_bar(stat="identity") here is that plot: https://imgur.com/a/SjUVSW4 what I am looking try to achieve is this, but with the "j" tick being and "a":' https://imgur.com/a/SjUVSW4 Do you know how to go about this? 
This code was just showing you to fix the “a” redundant issue. Add position = “dodge” To your geom_bar() to change the stacking issue. This code assumed your dataframe “df” was already ordered the way you want it. Just order it the way you want it with dplyr::arrange() before plotting. 
&gt;do you have the source data? Also have you tried loging the data? &amp;#x200B; &amp;#x200B; &gt; can I ask you what is random in this image? What does it refers to? &amp;#x200B; [https://imgur.com/a/WsiX79U](https://imgur.com/a/WsiX79U) &amp;#x200B; And can I ask you what is the meaning of ''adding seasonal dummies to the model'' ? What are seasonal dummies? &amp;#x200B; And in the second image what are all that 0 and 1? [https://imgur.com/a/rRFKPFG](https://imgur.com/a/rRFKPFG) Thank you &amp;#x200B; &amp;#x200B;
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/fhlRkBk.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eavrn5q) 
I have done a lot of work with transcriptomics data, so I think I understand what you're trying to accomplish. I believe that one of these plots will meet your needs. However, note that fold changes alone aren't that useful. You should consider a *volcano plot* because it will incorporate both fold change and variance, and because it incorporates logarithms, you can quickly tell if a fold change was up or down, and you won't need to color by up/down. Let me know if you want a sample of a volcano plot or need more help. # Sample Code library(tidyverse) vals &lt;- c(6,3,9,8,1,4,4,5,2,6) directions &lt;- c(rep("up", 3), rep("down", 2), rep("up", 2), rep("down", 3)) genes &lt;- c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j") df1 &lt;- tibble(genes, directions, vals) df1$genes &lt;- factor(df1$genes, levels = letters[1:10], ordered = FALSE) df1$directions &lt;- factor(df1$directions, levels = c("up", "down"), ordered = FALSE) # Option 1 p1 &lt;- ggplot(df1) + geom_col(aes(x = genes, y = vals, fill = directions)) + ggtitle("Option 1") p1 # Option 2 p2 &lt;- p1 + facet_wrap(. ~ directions) + ggtitle("Option 2") p2 # Option 3 - Create a new data frame where the signed magnitude corresponds to "directions" df2 &lt;- df1 %&gt;% mutate(vals = if_else(directions == "up", vals, -1 * vals)) p3 &lt;- ggplot(df2) + geom_col(aes(x = genes, y = vals, fill = directions)) + ggtitle("Option 3") p3 # Option 4 - Reorder the "genes" sorted by fold change - using factors p4 &lt;- ggplot(df2, aes(x = factor(genes, levels = genes[order(vals)]))) + geom_col(aes(y = vals, fill = directions)) + ggtitle("Option 4") p4 Figure 1: [Option 1](https://i.imgur.com/G5iWQ0r.png) Figure 2: [Option 2](https://i.imgur.com/lcuPn07.png) Figure 3: [Option 3](https://i.imgur.com/VkvJa8J.png) Figure 4: [Option 4](https://i.imgur.com/wUPHz59.png)
&gt; library(tidyverse) &gt; &gt; vals &lt;- c(6,3,9,8,1,4,4,5,2,6) &gt; directions &lt;- c(rep("up", 3), rep("down", 2), rep("up", 2), rep("down", 3)) &gt; genes &lt;- c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j") &gt; df1 &lt;- tibble(genes, directions, vals) &gt; df1$genes &lt;- factor(df1$genes, levels = letters[1:10], ordered = FALSE) &gt; df1$directions &lt;- factor(df1$directions, levels = c("up", "down"), ordered = FALSE) &gt; &gt; # Option 1 &gt; p1 &lt;- ggplot(df1) + &gt; geom_col(aes(x = genes, y = vals, fill = directions)) + &gt; ggtitle("Option 1") &gt; p1 &gt; &gt; # Option 2 &gt; p2 &lt;- p1 + &gt; facet_wrap(. ~ directions) + &gt; ggtitle("Option 2") &gt; p2 &gt; &gt; # Option 3 - Create a new data frame where the signed magnitude corresponds to "directions" &gt; df2 &lt;- df1 %&gt;% mutate(vals = if_else(directions == "up", vals, -1 * vals)) &gt; p3 &lt;- ggplot(df2) + &gt; geom_col(aes(x = genes, y = vals, fill = directions)) + &gt; ggtitle("Option 3") &gt; p3 &gt; &gt; # Option 4 - Reorder the "genes" sorted by fold change - using factors &gt; p4 &lt;- ggplot(df2, aes(x = factor(genes, levels = genes[order(vals)]))) + &gt; geom_col(aes(y = vals, fill = directions)) + &gt; ggtitle("Option 4") &gt; p4 Thanks for the time you put into this, I am going to use some of these tricks. I'm actually looking at the upregulated and downregulated KEGG pathways (genes in my example), vals is actually the -log(p Value) associated with these pathways. My main issue was this case: vals &lt;- c(6,3,9,8,1,4,4,5,2,6) directions &lt;- c(rep("up", 3), rep("down", 2), rep("up", 2), rep("down", 3)) genes &lt;- c("a", "b", "c", "d", "e", "f", "g", "h", "i", "a") x &lt;- data.frame(gene = genes, direction = directions, val = vals) Where I had an instance of the same pathway being upregulated and downregulated. I would try this to sort before plotting: x$gene &lt;- factor(x$gene, levels = x$gene[order(x$direction,x$val)]) I kept running into this error: Error in `levels&lt;-`(`*tmp*`, value = as.character(levels)) : factor level [9] is duplicated I ended up with this solution with makes a helper column that is a paste of gene and direction that is unique that can be used as a factor for sorting: vals &lt;- c(6,3,9,8,1,4,4,5,2,6) directions &lt;- c(rep("up", 3), rep("down", 2), rep("up", 2), rep("down", 3)) genes &lt;- c("a", "b", "c", "d", "e", "f", "g", "h", "i", "a") x &lt;- data.frame(gene = genes, direction = directions, val = vals) x$gene_dir &lt;- paste(x$gene, x$direction, sep="_") x$gene_dir &lt;- factor(x$gene_dir, levels = x$gene_dir[order(x$direction,x$val)]) x$gene &lt;- x$gene[order(x$gene_dir)] ggplot(x, aes(gene_dir, val, fill=direction)) + geom_col() + scale_x_discrete(labels=x$gene) Only took me 8 hours :) 
Ahh, now I understand your question better. Creating a helper column with the correct sort order followed by a special plotting function to print the labels is precisely what I'd have done. 
[https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/data.frame](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/data.frame)
LOL, it's a spam account. The question was rhetorical.
The source data: [https://pastebin.com/mNjcnGp5](https://pastebin.com/mNjcnGp5) The data is relatively stable in level over time (showing mild quadratic trend), so with no exponential trend is there a reason to log it?: [https://imgur.com/a/LVcuwXH](https://imgur.com/a/LVcuwXH) &amp;#x200B;
The source data: [https://pastebin.com/mNjcnGp5](https://pastebin.com/mNjcnGp5) The data is relatively stable in level over time (showing mild quadratic trend), so with no exponential trend is there a reason to log it?: [https://imgur.com/a/LVcuwXH](https://imgur.com/a/LVcuwXH) The random here refers to the remainder components: those that cannot be described by trend or seasonal components. The 0s and 1s in the first and third image are seasonal dummies to describe the seasonal factor, giving the component to help with forecasting: [https://imgur.com/a/0rmdPMT](https://imgur.com/a/0rmdPMT) &amp;#x200B;
Oh I'm sorry. Let see if I can explain myself more clearly. The idea is: we have a big corpus on the debates of the parliament. We want to identify 100 instances in which the debate (between two politicians) is about one particular topic (the territorial debate), so we want to search for some particular expressions and terms and their frequency of occurrence in the whole corpus. Then, we need two things: 1) A method which allows us to cut/isolate particular debates in virtue of the occurrence of the desired expression and terms. In other words, to isolate every debate between two (or more) politicians on the subject we want. Once this is done, we need 2) A method to select, from among all the exchanges between two politicians (i.e. concrete debates), 100 instances chosen randomly. Here is an explanation/picture just in case it helps to understand my problem, lol: [https://imgur.com/a/ZfraTiQ](https://imgur.com/a/ZfraTiQ) Thank you very much for your attention!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/8C7Q75F.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eax5k10) 
&gt;Or are you asking how to format it so it looks like the way you stated? Yeah that's exactly what I asked (not clearly enough) for. Please, see the answer I gave above this one. Let see if I'm being more clear. We are using RStudio btw! Thank you very much for your response!
You never know...and I want to contribute! lol
Unfortunately a few of these subs are plagued by people spamming links to their blogs. I'd be okay with it if they didn't use such clickbaity titles that dupe people into thinking they need help. It would be helpful if the mods added some kind of flair that could be added to the post so you know which ones are people just advertising their sites.
Do I fit into that category too? I have made some posts [here](https://www.reddit.com/r/Rlanguage/comments/a1lj6y/create_3d_county_maps_using_density_as_zaxis/) and [here](https://www.reddit.com/r/Rlanguage/comments/9j973x/tutorial_for_scraping_tables_example_shown_for/) that link to tutorials on my blog.
See that's fine. You titled your post so that it's clear that you're providing an explanation. 
Will do for next time, thanks for the input. 
&gt;Unfortunately a few of these subs are plagued by people spamming links to their blogs. I'd be okay with it if they didn't use such clickbaity titles that dupe people into thinking they need help. It would be helpful if the mods added some kind of flair that could be added to the post so you know which ones are people just advertising their sites. a data frame has nothing to do with R, so why does reddit let those kind of posts? I mean if, the admin of the sub doesn't take action does the post remain?
Mutate(region = ifelse()) 
in my opinion you shouldn't code the classification especially with so many value. &amp;#x200B; my preferred method would be to create a csv with the state region mapping, reading it in, and then joining it as necessary.
Isnt that even more work?
That worked! Is there a different oneliner I can use if I am using the tidyverse package?
Try case_when
Mutate is part of dplyr which is part of tidyverse (which is a bunch of packages like ggplot2 etc bundled together), I find wickham's methods really easy to remember and for reviewers to read, so I massively prefer them to base R. Hope that helps
if you need to change something you change the data and not the code. 
Thaaannkkk you for advise 👍🏻 usefull
You could use diff. However, if your data is a time series I'd convert it using the ts function first &lt;ts(data col, seasonal length)&gt; and then use the decompose function which splits the data into simple seasonal components, trends, and noise.
Hey - I created this series: noseason &lt;- (UE-decompose(UE)$seasonal); UE is my TS-data. The ACF seems very chaotic, would you propose a differencing of first order? =)
[https://ibb.co/25hrgY5](https://ibb.co/25hrgY5) &amp;#x200B;
Plot the decomposition and see what it looks like. But I'd try either a Box Cox transform or 1st order differencing also just for comparisons. It really depends on what your final goal is to be honest.
the final goal is definitely white noise &amp;#x200B;
Not quite what I meant but if you are just trying to model the data in a way where the residuals are noise put the ts in either auto.arima() or tbats() in the forecast package and they'll work great. They both decide the appropriate differencing for you so you don't have to guess and check.
Thank you very much, I will try that out 
Note he says this before that code : " In the following example, there are many columns of monthly data in a csv file with the first column containing the month of observation (beginning with April 1982)" &amp;#x200B; &amp;#x200B;
R in *nix vs R in Windows operates with a completely different paradigm. If I remember correctly, the nix versions leverage existing libraries within the OS, whereas on Windows everything has to be bundled for the package and things that require Java can be problematic. I have been out of the Windows game for ages though.
Wouldn't that make the syntax for the startdate retail &lt;- ts(retail\[,-1\], start=c (1982,4),frequency =12 ) I guess I have just never seen anyone express April as +3/12 or does +3/12 have some other significance
January would be the beginning of 1982 so if you specify start=1982 then it's assumed to be January. Since each month is 1/12 of the year each additional month you want to add 1/12 if you're going to use the single number interface. So 1982 + (1/12) would be February 1982, 1982 + (2/12) would be March, and 1982 + (3/12) would be April. &amp;#x200B; Your way of specifying it is how I would do it - it's more intuitive but oh well.
Ohhh I see! Looking into the output I realized that it was 24 months of forecasts /facepalm, I am going to try out forecasting my skus and simply reinserting my sku vector at the end for colnames, and forecast periods as rownames, and see how it pans out. Thanks for the clarification, i was really confused. 
&gt; such clickbaity titles that dupe people into thinking they need help. Writing titles as questions is a common mistake. They've been teaching high schoolers not to do that for decades maybe centuries. It's just bad style, not intentional clickbait. The oilshell guy is serious. He isn't selling something. He's doing a big project and blogging about it for a few years now. As part of his project, he's implemented a Python interpreter, and through that learned about Tidyverse, which in turn inspired his own API. The content of most of his blog posts about other people's technology is often pretty simple, but it's always very clear and he often makes some non-obvious connections. 
Are you using nix now? Kinda wondering what people find works best. So I've been running into the same errors with Fedora 29/RHEL 7.5 and CentOS. They're all pretty similar, so maybe I need to go further afield.
Ubuntu 16.04.5 LTS (Xenial Xerus). I run that on both of my servers and it works great. I chose that just to simplify things, especially with the package source lists. The biggest things is to make sure you have the `apt-get install` stuff sorted. Usually when a library fails it is because of a package dependency. If you keep follow the failed dependency you get helpful information telling you what to install. Java, curl, openssl, etc have been culprits for me. I recommend using portainer, and installing the tidyverse docker container. * https://hub.docker.com/r/rocker/tidyverse/ * https://portainer.io/ 
&gt;Thanks for the reply! &gt; &gt;Wouldn't that make the syntax for the startdateretail &lt;- ts(retail\[,-1\], start=c (1982,4),frequency =12 ) &gt; &gt;I guess I have just never seen anyone express April as +3/12 &gt; &gt;or does +3/12 have some other significance Can I ask you why are you making batch? what is the final purpose?
&gt;Ohhh I see! &gt; &gt;Looking into the output I realized that it was 24 months of forecasts /facepalm,I am going to try out forecasting my skus and simply reinserting my sku vector at the end for colnames, and forecast periods as rownames, and see how it pans out. &gt; &gt;Thanks for the clarification, i was really confuse &amp;#x200B; What is the purpose of batch forecasting? sorry? &amp;#x200B;
Can you share some sample data and the code you're using?
So many tutorials online. How can people help if you don't post what you've tried 
Thanks very much. I will give this a try!
Share your code!
Any reason you can’t leverage Leaflet instead? It’s really intuitive 
Use sample(): s &lt;- sample(1:nrow(movies.df), round(.1 * nrow(movies.df))) test.set &lt;- movies.df[s,] prediction.set &lt;- movies.df[-s,] &amp;#x200B;
I've thought about using the sample function or createDataPartition function in the caret package, but the part that I'm stuck on is making sure that the test set contains every userid and movieid in the training set. I know that I can subset the 10% further, but that's not ideal
I'm a beginner so I can't give you code, but it sounds like maybe you need a stratified sample. Does this help? link: https://www.rdocumentation.org/packages/fifer/versions/1.0/topics/stratified https://en.wikipedia.org/wiki/Stratified_sampling
**Stratified sampling** In statistics, stratified sampling is a method of sampling from a population. In statistical surveys, when subpopulations within an overall population vary, it could be advantageous to sample each subpopulation (stratum) independently. Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should be mutually exclusive: every element in the population must be assigned to only one stratum. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
You would never get a truly random sample that way. What about bootstrapping the partition on your model?
I apologize for the late response, but the goal is to create a script that we can feed large amounts of sales data into then preform a profitability analysis (calculating holding/financing costs/COGSvs revenue) to see which products are worth selling more aggressively/increasing marketing dollars spent 
Is that a package?
library (tidyverse) df %&gt;% group_by (webPublicationDate) %&gt;% count () %&gt;% ggplot (aes (x = Date, y = n) + geom_col ()
That worked great, thank you for your response! I did it by day, which was a bit too spread out, so I tried to group by month with this somewhat janky solution. It worked well in terms of grouping, except the labels of the x-axis look ridiculous haha. [https://imgur.com/a/17NvHca](https://imgur.com/a/17NvHca) Think I'm on the right path though! mutateddf &lt;- df %&gt;% mutate(months = format(webPublicationDate, "%y-%m")) &amp;#x200B; &amp;#x200B;
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/s4ZgVRc.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
I'm not sure if it really matters that the sample is truly random? I want to set aside 10% of data for a final validation/test. I'll split the remaining 90% into training and test sets (which I know how to do) to train and fine tune my model. The trouble I'm having is making sure the 10% validation set contains every userid and movieid in the other 90%.
Yep! Check out this resource: [https://rstudio.github.io/leaflet/](https://rstudio.github.io/leaflet/)
Read up on the `lubridate` package. Really helpful for working with time/date data. Also, if you add `coord_flip ()` to the chart it will put dates on the y-axs. Easier to read. 
Thanks for the advice, and for the help!
Try making sure the length of your xreg and y are the same.
when I add a d-component,ARIMA(p,1,q), it alters the lengths it probably cuts the intercept or something, I don't know what it is. It works just fine, if I leave out the d-component 
Do you take first difference of both series? 
Use the auto.arima function instead before you diff as it will diff for you if it helps the model.
I already used it beforehand 
Yes sir
[https://imgur.com/a/rrZ7rut](https://imgur.com/a/rrZ7rut)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/jRo57Fy.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eb3vawp) 
[https://imgur.com/a/iS8Xm76](https://imgur.com/a/iS8Xm76)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/wv2Tde5.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eb3vdhp) 
You need to do library(nycflights13) to tell it to use the package.
I saw it in my local bookstore today and on Amazon.
Suggest using %in% or named vector indexed by state # Do this to modify in place without nested ifelse pkdata$region[pkdata$state %in% c('CA','NV','AZ')] &lt;- 'West' pkdata$region[pkdata$state %in% c('NJ','NY','CT')] &lt;- 'East' # Or do this to take advantage of named vector index pkdata$region &lt;- c( 'NJ' = 'East', 'AZ' = 'West', 'SC' = 'South' )[pkdata$state] &amp;#x200B; &amp;#x200B;
This looks so much cleaner. Thanks!
 # dplyr &amp; tidyverse are the best library('dplyr') my_dataframe %&gt;% group_by(user, movie) %&gt;% sample_frac(.1) &amp;#x200B;
fyi, you'll have to take the quotes out of tidyverse. Quotes are for installing, not loading(the library function)
either should work
Til 
What’s the error message? 
As far as I can see you are modelling dy with arima(4.1.4). And then regressing with the other series. Dy is already first differenced, so you exclude the first observation. By using Arima integrated of order 1, you difference it again and exclude another observation. The other series is only differenced onced, thus have one observation less. You should also note that second order differencing of a series that is only first difference stationary creates a unit root in the series. 
For the dataset you are using (which I am assuming is an external file), it looks like part of the column header is being included as column values. You don't want "(g/km)" or "CITY (L/100 km)" to be read as part of the columns, because I think it'll force all the data to be character or factor. Those units should either be part of the column name or removed entirely.
How would I do this? I've already tried removing the first row: auto&lt;-auto\[-c(1),\] It doesn't seem to solve the problem.
Read.csv("file location", headers = TRUE) 
Do you have Rtools installed and in the right place? It's been a long time, but I seem to recall having that problem when trying to install odbc in the past.
When I read the file using read.csv, all columns are factors, not numbers, except for CYLINDERS and X.3. If you change them it will work. df$CO2.EMISSIONS &lt;- as.numeric(df$CO2.EMISSIONS) df$ENGINE.SIZE &lt;- as.numeric(df$ENGINE.SIZE) df$FUEL.CONSUMPTION &lt;- as.numeric(df$FUEL.CONSUMPTION) m1 &lt;- lm(CO2.EMISSIONS~ENGINE.SIZE+CYLINDERS+FUEL.CONSUMPTION, data = df) summary(m1)
You should probably include the OS for this question. 
Code and excerpt from the data?
The only thing I can think of would be to read the file in and then write it out at the new location. I've had success with the *readxl* and *WriteXLS* packages.
Would it be easier to send you the data through message? I'm using ggplot so I'm wondering if the formatting of my data is the thing causing issues.
Not OP but if it's AWS it's probably Amazon Linux 2012 which is a fork of RHEL/CentOS 
https://cran.r-project.org/web/packages/odbc/readme/README.html At the bottom you will see &gt;odbc and it’s dependencies use C++11 features. Therefore you need gcc 4.8, clang 3.3 or Rtools 3.3 or later. Those dependencies may require you to have gcc/rtools like the other person mentioned. There is also this note for Linux installation &gt;Linux - Debian / Ubuntu apt-get can be used to easily install database drivers on Linux distributions which support it, such as Debian and Ubuntu. UnixODBC - Required for all databases # Install the unixODBC library apt-get install unixodbc unixodbc-dev Common DB drivers # SQL Server ODBC Drivers (Free TDS) apt-get install tdsodbc # PostgreSQL ODBC ODBC Drivers apt-get install odbc-postgresql # MySQL ODBC Drivers apt-get install libmyodbc # SQLite ODBC Drivers apt-get install libsqliteodbc
This did it.... Thank you. Good learning experience, I'll need to install packages and then add them to the library
I don't use powerBI so I can't test if it work but have you tried the xlsx library? Also is xlsx format your only option? R can export to other formats like the older xls format using the readXL library.
just head 
I think I ran into a similar problem once, and it was either the character encoding on one of the sheets or the naming being too long/having bad encoding on one of the sheet names. Have you tried using the [development version sourced directly from GitHub](https://github.com/awalker89/openxlsx)? Could just be some weird UTF-8 issue. Is there a trace log you can post? Have you looked into saving as a tsv instead of csv?
I don’t know why you’d say that, they offer several OS options. For example I usually go with an amazon Ubuntu AMI. You do have one upvote, so maybe you’re right, but your response surprised me. 
Use library(tidyverse) and spread(). Otherwise does the reshape() function work very nice as well. 
Wide to long is ```gather()```.
Check out the gather function from the tidyr package! gather(data, key, value), where key is the name of your new variable that hold your column names, and value is the name of the new column with your data. 
Of course. Read it the other way around
Gather has been mentioned, but I would also like to throw in `melt` http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/
SUMLEV STATE COUNTY STNAME CTYNAME YEAR TOT_POP TOT_MALE TOT_FEMALE &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 50 38 7 North… Billin… Apr-… 888 471 417 2 50 38 7 North… Billin… Jul-… 876 464 412 3 50 38 7 North… Billin… 1-Jul 877 471 406 4 50 38 7 North… Billin… 2-Jul 854 459 395 5 50 38 7 North… Billin… 3-Jul 820 447 373 6 50 38 7 North… Billin… 4-Jul 810 440 370 I'm trying to get Year on the X-axis and have a bar for each year that equates to Tot_Pop, but shows how much of that is men and women(which come from the other two columns
As previous commentators have said spread() and gather() read all about it here [https://r4ds.had.co.nz/tidy-data.html#spreading-and-gathering](https://r4ds.had.co.nz/tidy-data.html#spreading-and-gathering)
I said "probably" in the sense "let's provide answers assuming this until OP says otherwise" as opposed to waiting for a response. 
I use gather when I have few columns which I don't want gathered, and melt when I have many columns I don't want gathered. 
Oh would that I could. My org is so locked down, github is blocked. I could try TSV, though I think CSV actually works fine. Upon further inspection the variable I thought was pulled in wrong just had conflicted values because of changes to the survey in the past. Though the identical CSVs are significantly larger in file size which still makes me not like them but I think I'll switch back because the CSV datetime comes in properly and the XLSX datetimes all come in wrong and it's tedious to fix them. I don't see a tracelog in PowerBI.
Thank you for your reply. Apologies for being so bad at R, I'm very new to the program. Entering in the commands you've shown I receive "Error in df$FUEL.CONSUMPTION : object of type 'closure' is not subsettable" I don't entirely understand how the df$ function works. The information I've found online appears to show that it's the result of a created subset? I'm guessing it would only work after the information has been subsetted but I'm not clear on how to do that. How did you create the subset?
Oh no worries. It’s just that I read and saved the data file as an object named df. Then what comes after the $ is the name of the column in that data set. How did you retrieve your data? I couldn’t tell from your example. 
Ah, I see. Thanks. I just used: \&gt; autos&lt;-read.csv("fuell.csv") \&gt; autos&lt;-autos\[-c(1104:1366),\] To read the file and delete the blank space rows in the dataset. So then, for me it would be autos$FUEL.CONSUMPTION, thanks.
Will check tmrw ;)
Without being able to see the data, try using read_csv() instead of read.csv(). If that doesn’t work, please provide a glimpse() or str() of your data set. 
The [Mahalanobis distance](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/mahalanobis.html) does not require any knowledge of groups. I don't understand your question.
Also, maybe check that your version of dplyr is up to date? It's strange that `filter` is calling a function that is deprecated to the point of throwing an error...
Using the pairwise mahalanobis in PAST specifically, it does. In R, it will figure out the groups for you if unspecified. Regular Mahalanobis distance doesn't need groups, but unfortunately I need to use the pairwise distance instead. 
have you loaded the dplyr library?
I also don't know about powerBI but have you tried the XLConnect package?
Not at this job. I'll try it. Since I had to set everything up again I forgot about a lot of the packages my personal computer has.
OK. I'm not familiar with this specific distance measure, but I found the [documentation for the pairwise.hahalanobis\(\) function](https://www.rdocumentation.org/packages/HDMD/versions/1.2/topics/pairwise.mahalanobis), and you are correct: it does require you to specify the groupings. I don't know what to tell you because I don't understand your specific problem domain. For example, in problems where I don't know specific groupings and I have high dimensional data, I'll often start by computing principal components (PCA) to see of there are inherent groupings in the data. I have no idea if such an approach would be valuable for you. If you could make your data set available, I'll try to help you.
Perhaps try calling filter directly from the dplyr library. Use dplyr::filter instead of filter. I get an error when I use just filter. I think another package might be loaded on my machine that uses filter as a function name, and R doesn't call dplyr by default. I'll fix it one of these days. 
if(condition) { functioncall1() } else { functioncall2() }
This also isn't my speciality, but I thought you needed two or more groups when doing pairwise comparisons with mahalanobis (generally these functions are set up assuming you want to do a cluster / discriminate analysis; assessing distance between groups)? Another way mahalanobis is commonly used is to screen for multivariate outliers (instead of comparing two or more groups, you are comparing a data point to the cloud of all other data points). This would tell you how far apart each data point is relative to all the others, but I'm not sure this is really what you want to do (based on my understand of your Q).
This, I run into problems with filter, specifically, all the time but using dplyr::filter() always seems to do the trick.
Firstly, this is a subreddit for R as a programming language, not for answering statistics questions directly. That said, your question is too vague to possibly answer. You didn't provide any data, just started naming attributes.
you can do sth like: ggplot(data,aes(pop, time, group= sex, fill = sex))+ geom_bar(stat = "identity")
Thanks for the help, I was able to install and then use the package "odbc" by installing # Install the unixODBC library apt-get install unixodbc unixodbc-dev --install-suggests
Thanks for the help, I was able to install and then use the package "odbc" by installing # Install the unixODBC library apt-get install unixodbc unixodbc-dev --install-suggests
Love the thorough explanation of the cleaning process. Data integrity is rarely 100% there in the real world, and most tutorials (understandably, for conciseness) gloss over cleaning.
Thank you! I really appreciate your feedback! 
[One year ago](https://minimaxir.com/2017/06/r-notebooks/) jupyter devs seemed somehow into checking with r-studio on how to collaborate for that. 
I think I figured out how to do it using PAST actually - it has a similarity/dissimilarity index function that seems to produce the right output - but thank you for your thoughts all the same!
Yeah, you do need at least two groups to do pairwise with mahalanobis - I have two different time periods from the same cemetery that I could use, but was really hoping to analyze them separately. I believe I've worked it out in another program though, so fingers crossed. 
You can also just pass the function in as an argument. do_this &lt;- function(val, f) { f(val) } &gt; do_this(c(1, 10), mean) [1] 5.5
Thanks! For some reason y = datasetB\[, c(2,4)\] doesn't work for me. What I was able to do was to merge the whole B dataset and then drop the columns that I didn't need. Yeah, it takes less than a second now.
I have not. Thanks! That worked great.
Do you know what the difference would be, in my example, between a merge and a left\_join (if there's any)? I've run both and it seems the output is the same although they put the columns in a different order.
Do you know what the difference would be, in my example, between a merge and a left\_join (if there's any)? I've run both and it seems the output is the same although they put the columns in a different order.
Yes we use plumber in production and have no complaints so far. We only started building them maybe six months ago and use them to supply graphs and data to shiny applications. We also have an api as a validation check that is triggered after a weekly batch job to ensure the output data is good. The only aspect that I am worried about in the near future is that each api is single threaded currently, this requires an upgrade which is in the plumber docs but just haven't gotten around to it yet. 
I’m interested to hear why you use plumbeR endpoints in a Shiny app over output$xyz&lt;- . Are the endpoints more generalized and used in numerous places?
This is great, I use it all the time!
&gt; left Merge with all.x=TRUE should be the same as a SQL left join. :) 
get(paste0(“matrix.”,i)) should do it.
If the matrices are the only objects in your environment that match the `matrix.[0-9]` pattern, this will work: list(mget(ls(pattern = 'matrix.[0-9]'))) 
I believe it was used for this https://opensource.t-mobile.com/blog/posts/r-tensorflow-api/
Yes, this is a better option. Nice and clean, and though it probably won't make a difference unless these matrices are pretty huge, it runs faster, too. I'll add one minor tweak, though - mget returns a named list so the list() wrapper actually dumps the list of matrices into another list. Can drop the list() altogether, OP, unless you wanted this behavior. Thanks /u/ReimannOne. I always forget about mget and the reminder is probably going to save me some time in the near future. &amp;#x200B;
Thanks. You're right about the list() call, it isn't necessary. The regex-ish expression probably isn't as good as the one in the response from /u/shujaa-g in the [other thread](https://www.reddit.com/r/rstats/comments/a427ru/connecting_matrices_into_list/ebayiwu/).
I'm not certain if you're describing a data frame or standalone vectors. Always try to share reproducible code. Try something like x &lt;- c(x, rep(NA, 50-length(x))
I recently had to do something like that and inserted the following line in my for-loop. It's not very neat but it worked. temp[(nrow(temp)+1):50,] &lt;- NA
Thanks for finding and mentioning. I really despise crossposting without mentioning/linking it. Wastes people's time and unnecessarily fragments discussions.
Thank you for your comment!
Have you tried anything?
salary[salary$Club != "Manchester United", ] will return your dataframe with all clubs except man utd.
Make sure your factor variable is converted to character
This did the trick. thank you. salary &lt;- salary\[salary$Club != "Major League Soccer L.L.C",\] salary$Club &lt;- factor(salary$Club)
have you tried \`if\` statement then replace the whole value?
You mentioned dplyr: salary %&gt;% filter(
I get this error: the condition has length &gt; 1 and only the first element will be used
[Check this out](http://www.sthda.com/english/wiki/ggplot2-error-bars-quick-start-guide-r-software-and-data-visualization)
Tidyverse. mutate. case_when 
Exactly that. We want as much of the logic as possible in the api itself. Then instead of copying and pasting code into each application we can just work on making the api's really general and intelligent. It also means that we update the data in the api rather than in each application. We want to expose these views in other types of applications eventually. 
Maybe you can first identify which items don't match. I created example data frames based on yours: td &lt;- data.frame(list = c("Pine Sol", "Pinesol", "WD40", "WD4o", "WD-40")) pd &lt;- data.frame(master = c("Pinesol", "WD-40")) td$match &lt;- td$list %in% levels(pd$master) It will probably help to arrange the list by whether it's is a match or not and the product names in alphabetical order: library(dplyr) td &lt;- arrange(td, match, list) You can find out what kinds of names there are that don't match the master list: unique(td[td$match==FALSE, "list"]) Variations of WD-40 in this example are: WD40, WD4o &amp;#x200B; Then you could do something like this to replace the unwanted names with the correct one. The function says: in td$list, find and replace WD40 or WD4o, with WD-40. library(stringr) str_replace_all(td$list, c("WD40|WD4o"), "WD-40")
This is what I’ve tried testDate&lt;-match("6/16/1986",rolling.dates[,10]) #####THIS IS MY TEST DATE r.squared[idxDate,2] #This should give us the rsquared for our dates fit.2YR.3YR&lt;-lm(Output1~USGG2YR+USGG3YR, data=AssignmentData) anova(fit.2YR.3YR)
Any idea on how I put non-numerical variables on the x-axis? I have two classes of four measurements, but I can't seem to plot them on the x-axis.
Try making them factor variables as.factor() 
So there is no function to preform a “fuzzy” match?
testDate&lt;-as.Date("1986-06-16") #####THIS IS MY TEST DATE r.squared\[testDate,2\] #This should give us the rsquared for our dates part4&lt;-lm(Output1\~USGG3M+USGG5YR+USGG30YR, data=AssignmentData) summary(part4) ##this Gives us R-Squared anova(part4) **#I THINK THIS IS IN THE RIGHT DIRECTION** R2&lt;- **#We get from summary above** cfnts&lt;- **#NOT SURE WHERE WE GET THIS YET** pvals&lt;- **#I think we get from summary above** R2&lt;- **#We get from summary above** testDate&lt;- **#No damn clue** &gt;**NO CLUE ON HOW TO FIT THE RESPONSE AROUND testDate** &amp;#x200B;
I'd call it either a categorial scatterplot or a jitterless jitterplot. In base plot you could plot the points with the tilde operator. A$A ~ A$B. Then you could add individual lines of SD with lines. Ithink the x-coordinates then wpuld be 1 and 2.
Thanks for asking! The 2nd edition will be released sometime in 2019. J.D. Long is bringing the *R Cookbook* up-to-date and, I might add, doing a great job.
Could you present the structure of the data using `str` This post gives a good idea why the warning occurs &lt;[https://stackoverflow.com/questions/14170778/interpreting-condition-has-length-1-warning-from-if-function](https://stackoverflow.com/questions/14170778/interpreting-condition-has-length-1-warning-from-if-function)\&gt;, which should help you get around. &amp;#x200B;
https://stackoverflow.com/questions/43877172/split-comma-delimited-string
Just worked when I tried it. Try running this code. install.packages("ISLR") library(ISLR) \#To verify it loaded head(ISLR) fix(ISLR) /#Note, I don't know how to format in Reddit well so apologies for that. 
when I type head(ISLR) I get: &gt;Error in head(ISLR) : object 'ISLR' not found I looked over something previously though. When I type library(ISLR) I get a warning message that it was built under R version 3.4.4 whereas I am using 3.4.0. Could that be it?
Sorry was a typo, I fixed in less than a minute but apparently not in time. Try to copy/paste code again.
Well that's weird. It works perfectly now. Thanks. Maybe I needed to exit out of R after installing the package and re-open it? 
Well I typed head(ISLR) instead of head(Carseats) the first time and you can't see the first 6 lines of ISLR because it is a package and not a table. I don't know why you were getting the error though. I suspect you might have not capitalized Carseats, but I'm not sure.
It is sad that Google broke ggmap. But try leaflet-- esp. the stamen layer. Easy to learn, easy to share, though it does not have the analytical strength of ggmap or other packages.
You’ll probably want it to accept three arguments. Include a df argument as well. Then rewrite the logic to point to that internal function. The main thing you’re missing is a return at the end of the function though. return(df)
I tried that, but still no Bueno. :( # Functions cleanteam &lt;- function(df, x, y) { df$Club &lt;- gsub(x, y, df$Club) return(df) } cleanteam(salary,"Seattle Sounders FC", "Seattle Sounders")
Does gsub need to go into apply? If you run that line outside the function what happens? 
Net of any other variables, if your coefficient is -0.68256 in log odds, that means being male (iv=1) decreases the log odds of y=1 by .68. In odds ratio, e\^-.68256 = .505. This means that being male multiplies the odds of y=1 by a factor of .505, a decrease of 49.5%. Are you trying to get predicted probability?
It is sad and whilst leaflet is great it can’t produced pdf output as nice as ggmap. For anyone else struggling with ggmap but who is happy to hand over billing information, I followed [this issue on Stack Overflow](https://stackoverflow.com/questions/52743142/is-something-wrong-with-the-ggmap-and-mapview-packages-in-r/52826485#52826485?) to get my code to work. I think I also had to first install the dev version of ggmap: ```devtools::install_github("dkahle/ggmap")``` 
The problem is that the assignment to `salary$Club` occurs within the function and so doesn't make any changes to the actual salary dataset in the wider enviroment. There are a couple of options to fix: Someone mentioned refactoring it so it takes a dataframe as an argument and then returns the modified dataframe. You could also return the modified salary$Club. A simple hack that keeps your code almost identical would be the double headed arrow which does "superassignment". This does assignment in the global environment outside the function / closure. More info: [manual](https://stat.ethz.ch/R-manual/R-devel/library/base/html/assignOps.html) entry; [stackoverflow](https://stackoverflow.com/questions/2628621/how-do-you-use-scoping-assignment-in-r) question. So you just change: salary$Club &lt;- gsub(x, y, salary$Club) to: salary$Club &lt;&lt;- gsub(x, y, salary$Club) 
Hey, blozenge, just a quick heads-up: **enviroment** is actually spelled **environment**. You can remember it by **n before the m**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: I learnt quite a lot from the bot. Though it's mnemonics are useless, and 'one lot' is it's most useful one, it's just here to help. This is like screaming at someone for trying to rescue kittens, because they annoyed you while doing that. (But really CMB get some quiality mnemonics) I do agree with your idea of holding reddit for hostage by spambots though, while it might be a bit ineffective. Have a nice day!
hEy, BlOzEnGe, JuSt a qUiCk hEaDs-uP: **eNvIrOmEnT** Is aCtUaLlY SpElLeD **EnViRoNmEnT**. yOu cAn rEmEmBeR It bY **N BeFoRe tHe m**. HaVe a nIcE DaY! ^^^^ThE ^^^^PaReNt ^^^^cOmMeNtEr ^^^^cAn ^^^^rEpLy ^^^^wItH ^^^^'dElEtE' ^^^^tO ^^^^DeLeTe ^^^^tHiS ^^^^CoMmEnT.
Don't even think about it.
dOn't eVeN ThInK AbOuT It.
Yes. Thank you so much! 
I haven't but maybe something like [Shiny dashboard](https://rstudio.github.io/shinydashboard/) would be enough? Depending on how complex this website should be.
Your idea seems impractical if not impossible. You need to run the shiny server to host your apps. I think you'd be better off learning a framework like Flask or Django and embedding your shiny apps. The benefit here is that your skills with those frameworks would be transferrable to other full-stack positions.
Unless you are using Shiny Server Pro or some heavy caching you are going to run into problems if multiple users give it a go. If you still want to go the route, look into load balancing and docker containers to run multiple Shiny servers. * https://www.bjoern-hartmann.de/post/learn-how-to-dockerize-a-shinyapp-in-7-steps/ * http://nginx.org/en/docs/http/load_balancing.html
Shiny dashboard is great but won't accomplish all the backend stuff you would need. OP could use ShinyProxy for reasonable multi-user performance on a well equipped server. Still has serious limitations though.
Depends what you mean by a "completely shiny" website. Dealing with multiple users is less of a problem with the introduction of async programming capabilities. Here's some of my work: * [neuroexpresso.org](https://neuroexpresso.org) * [http://oganm.com/shiny/taracyc](http://oganm.com/shiny/taracyc/) * [oganm.github.io/5eInteractiveSheet](https://oganm.github.io/5eInteractiveSheet) The last two are hosted in the smallest digitalocean machine possible which has been enough so far. Async programming makes serving multiple users at the same time less of an issue.
I guess it depends what you want the website to do; I think it’s doable, though others make good points about balancing, etc. I do an alternative setup; I host a personal site on digitalocean using the ghost blogging platform to maintain a simple updates / landing page. On this same server, there is a section for shiny apps, with each being linked to in a scrolling page. One thing that would help with your endeavors; try to implement shiny modules. The major issue with Shiny is how easily things become spaghetti, modules can help to reuse code and keep sections separate. Honestly, it seems easier to just create smaller apps and have an html page they are linked to. If you have a large app that really needs a lot of sections to pass data through, that is one thing, but I think most apps serve more niche purposes.
if you have to use shiny, you might want to use blogdown for the static part of your site, and then embed the shiny stuff as necessary.
There's a company that does health economics outcomes research (HEOR) called delta hat. Their website is totally in R. Having a look should at least give you an indication. 
The skills and knowledge required by **data analyst**(4 steps) **1, data acquisition；2. data processing：**The processing of data requires an efficient tool：Excel and high-end skills**；**[**FineReport**；](http://www.finereport.com/en/)Oracle and SQL sever;**3. Analyze the data：SPSS series；SAS；R；Various BI tools:**[Tableau、](https://www.tableau.com/)[FineBI；](http://www.finereport.com/en/business-intelligence)**4.** [data visualization](https://www.finereport.com/en/product-features/data-visualization.html):FineReport. In my view, you can use FineReport instead of Excel.Professional reporting tool, a daily report design can be used as a template, as long as you can write SQL to get started. Compared with excel reporting, the development of technical requirements is less, can quickly develop regular reports, dynamic reports, and can be placed on the mobile and large screen [viewing.You](https://viewing.you) can refer to this article:[http://www.finereport.com/en/about-finereport/best-reporting-tool-a-template-is-better-than-hundreds-of-excel.html](http://www.finereport.com/en/about-finereport/best-reporting-tool-a-template-is-better-than-hundreds-of-excel.html)
I **love** your blog format! Not only is the content great, but the actual formatting is very easy on the eyes. I like to use a *dark theme* in the evenings, and other than the white backgrounds of your images (which is hard to avoid), my browser plug-in for switching on dark mode works beautifully with your site.
I'm afraid you are going to need to post information. Is there some sort of unique ID in each of the datsets you can use to connect them? How are you defining "term"? How are set 1/row 27 and set2/row2 associated with one another? I'm struggling to understand what you are trying to accomplish.
My website runs entirely on Shiny, both the static parts and the executable parts. You can see the static parts here: [http://quantdevel.com/public/](http://quantdevel.com/public/). (I'm sorry, but I cannot open the executable parts to the general public). Also, engti's suggestion to incorporate blogdown is very good. My employer uses that technique for posting materials to the internal RStudio server. &amp;#x200B;
I'm sorry. I have to train myself to give more informative and clearer explanations. Let see. I want to do a linguistic research and I'm trying out some ways to analyze the relation of some terms (if at all). I'm just using "term" here as a synonym of "noun". I have a large text block (a political debate) and I want to analyze the relation between two terms they usually use in their interventions (this is not the "linguistic research" ofc). Lets say I want to analyze the expressions "it's false" and "law". I just extract the positions (i.e. number of line) where an "it's false" appears. That's SET 1, a .txt file where I can look for the positions of that expression in the debate. Same applies to SET 2 with "law". So, the thing is sometimes these expressions are together and sometimes they aren't. I was wondering if I can know the times they are closer to know if there is any kind of relation between the expressions. That's why I said "We just want to say to Rstudio "hey, give me the position of the term "x" and "y" where there is no more than 1000 (for example) lines between them". I hope I have been clearer this time. I'm sorry for the inconveniences. Thank you very much!
You can write a function that finds all the values in set 2 that are “close” to a single value in set 1 and then apply it to every value in set 1. You’ll want to use lapply since the input isn’t a matrix and the output will probable be different lengths for each term.
You can do this with the `outer` function. If you have two named vectors of word positions, you can try the suggestion posted [here](https://stackoverflow.com/questions/15627958/r-function-for-doing-all-pairwise-comparisons-for-two-vectors?rq=1). Once you've generated your vector of pairwise differences, you can subset it to keep only those with distances no more than 1000. Good luck!
How did you try to do it? This shouldn’t be a problem, ggplot2 automatically does the right thing if your variable is categorical.
Try this: for(i in names(df)){ assign(i, df[i])) }
Generally, this is a bad approach. It would be better to make each data frame an element of a list, or to keep it as one dataframe in long format.
Thank you for your comment. I'll give a try to look for a way to change the background of my images, so the blog could be look good in both formats. I love using dark themes during evenings.
Building on this, using a loop, you could do: mylist = list() for (name in cols){ mylist[[name]] = df[,name] } then later you access your desired dataframe from mylist by name.
What limitations do you mean? 
Mobile cancer
??? Doesn't it render properly on mobile?
It has shitty pop ups. 
Rather than using a for-loop, which isn't very idiomatic R, I would use \`split\` ([man](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/split)) as in [this dplyr/purrr example](http://www.stat.wisc.edu/~yandell/R_for_data_sciences/curate/tidyverse.html#split-apply-combine-using-split-purrr-and-tidyr).
This seems common enough question that one might suspect it has been asked hundreds of times before on the internet. So instead of answering your question I went ahead and googled "dplyr keep only maximum". Top result: https://stackoverflow.com/questions/24237399/how-to-select-the-rows-with-maximum-values-in-each-group-with-dplyr Use the solution with `slice()` if you absolutely wish to keep only 1 row per year. Otherwise go with the top answer.
You sweet little thing, you!
ShinyProxy spins up one R instance per user. So if you have even 10 users you're gonna be using a lot of resources to accomodate them.
Hey, mosskin-woast, just a quick heads-up: **accomodate** is actually spelled **accommodate**. You can remember it by **two cs, two ms**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: I learnt quite a lot from the bot. Though it's mnemonics are useless, and 'one lot' is it's most useful one, it's just here to help. This is like screaming at someone for trying to rescue kittens, because they annoyed you while doing that. (But really CMB get some quiality mnemonics) I do agree with your idea of holding reddit for hostage by spambots though, while it might be a bit ineffective. Have a nice day!
Delete
hEy, MoSsKiN-WoAsT, jUsT A QuIcK HeAdS-Up: **AcCoMoDaTe** iS AcTuAlLy sPeLlEd **aCcOmMoDaTe**. YoU CaN ReMeMbEr iT By **tWo cS, tWo mS**. hAvE A NiCe dAy! ^^^^tHe ^^^^pArEnT ^^^^CoMmEnTeR ^^^^CaN ^^^^RePlY ^^^^WiTh ^^^^'DeLeTe' ^^^^To ^^^^dElEtE ^^^^ThIs ^^^^cOmMeNt.
Don't even think about it.
dOn't eVeN ThInK AbOuT It.
What are you reading from? CSV? Can you show us a snippet of the relevant text file (if it's a text file) as well as the command you're using to read it?
haha I was super dumb, it was that ggplot syntax was bad, it handles it fine &amp;#x200B; It was a big clue when I got the same error for plain numeric data :)
I can delete if it's better etiquette, and post this as a new q...I had some other q's based on this weird format. So, I'm trying to predict the hms/difftime data, that's my response var. And I can't use any other time value to predict it, I have only text features. Time as response is weird enough, but bc I have no other time, I can't use duration/distance-of-time/survival analysis stuff. &amp;#x200B; Would you keep as hms/difftime? Change to posixct? Change to categorical, and bin (this is where I was headed but I'm real dumb), or somehow discretize into numeric? I was hoping it would be like the nycflights data, where time is just numeric (5:30a = 530).
So to be clear, your dependent variable is a difference of times? Maybe coerce it into an integer and then convert your predicted values back into difftimes? Sorry if I'm not understanding correctly
no, it's a time of day for a starting time R just reads that as a time difference by default the question of how to treat this data if it is the response affects whether I coerce it to some other type on import, I guess.
IMHO the cheatsheet is better... 
I want to predict start\_time, but I can't use end\_time as a feature. description/snippet/title are text features
Well, a posixtime is just an integer as I understand it, so converting it to an integer shouldn't hurt anything. I wouldn't make any destructive changes to your data though :) Are you running a regression?
for me, that q seems inseparable from the q of how to encode/group the response. Integer here = something like, seconds after midnight?
is it just inherently weird to keep the response in time-series format, especially if there are no ts predictors?
I was hoping each value was only to the nearest half-hour, which would make me feel better about binning...but no.
I don't really get what's going on, but have you tried parsing with `lubridate` (e.g. `lubridate::ymd()`)
if classification plays more nicely with the text features, then I bin categories of times. if regression works fine with text features, after they are tf/idf or bow or word2vec encoded (and I'd like to include some binary predictors too), then let's go w/continuous.
I think I am just psyching myself out. &amp;#x200B; I repeat steps prior to fitting glmnet on each text predictor. Because they have the same number of rows (length) for all of them, the output is the same length. Join them as a matrix, and this is the "x" argument.
 Your data collection approach is very interesting and this topic is very informative for me.I got to read some blogs links on [rushmyessay.com prices](https://www.rushmyessays.org/) forums which were very worth-reading. 
All of the R cheatsheets are great 
Your issue is that you're making a temporary variable "cat" inside the loop, and you change "cat" but not the column. You want something like: for (row in 1:nrow(mtcars)) { milespergallon &lt;- mtcars[row, "mpg"] if(milespergallon &lt; 20) { mtcars[row, "car_cat"] = 1 } } If I were doing this from scratch, I would avoid the for loop altogether: data(mtcars) mtcars$car_cat = ifelse(mtcars$mpg &lt; 20, 1, NA)
Thank you very much, that explains it. 
not your question, but you can simplify the code to not require for or if. vector operations and subseting are pretty nice. 
dplyr version: data(mtcars) mtcars &lt;- mtcars %&gt;% mutate(car_cat = ifelse(mpg &lt; 20, 1, NA))
Would be great if you could show!
There's lots of ways to do this without building your own loop. Here's one way in `dplyr`: library(dplyr) data(mtcars) new_mtcars &lt;- mtcars %&gt;% mutate(car_cat = ifelse(mpg &lt; 20, 1, NA)) head(new_mtcars) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb car_cat #&gt; 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 NA #&gt; 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 NA #&gt; 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 NA #&gt; 4 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 NA #&gt; 5 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 1 #&gt; 6 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 1 You can also use `switch` or `case_when` or `recode`, but I think `ifelse` is the easiest one to read and understand. You _very rarely_ need to roll your own loop in R, it's quite a convenient vectorised language in that regard.
You need to use the same vocabulary in your train and test datasets. Words that appear in your training dataset but not in your test dataset should get a column of all 0s. Any word that appears only in your test dataset will probably have to be excluded, though that depends a little on your model. e.g. if you're building a logistic regression model, a column of all zeros in the fitting will cause the algorithm to fail to converge.
This can't be right. How is it possible to do any nlp in R? It simply can't be done on any unknown words/unseen in training set? You sure?
The frameworks that you have used previously may have abstracted this away from whatever you were doing--encoding all unknown tokens as "unk" is basically the same as throwing them away--but, yes, I'm sure. In an NLP model tokens (words, letters, n-grams, whatever) are features. You train your model on what those features represent. By definition, it can't know what to do with a feature it wasn't trained on.
Thanks. I'd love to get someone else's opinion. 
Ok. I'd recommend making your question a little more specific as well. What's your task, what model are you trying to build, what framework / package(s) are you using, etc... If you don't get any bites here maybe try out community.rstudio.com which has a pretty great community. Best of luck
my approach would be similar to the ifelse(...) version: mtcars$car_cat &lt;- NA mtcars$car_cat[mtcars$mpg &lt; 20] &lt;- 1
Once you have your look up table, the operation you are looking for is a join, which in base R can be done with `merge` (and some options), or more straightforwardly with `left_join(combo, codes, by = c("curr" = "Currency"))` from the `tidyverse`. This will add the AlphabeticCode from the lookup as appropriate when the fuller name currency columns match up.
Do you have a [CRAN mirror set](https://www.r-bloggers.com/permanently-setting-the-cran-repository/)? I just ran `install.packages("fasttime")` and it worked fine: install.packages("fasttime") trying URL 'http://cran.cnr.berkeley.edu/bin/macosx/el-capitan/contrib/3.5/fasttime_1.0-2.tgz' Content type 'application/x-gzip' length 14821 bytes (14 KB) ================================================== downloaded 14 KB The downloaded binary packages are in /var/folders/l_/4mhb025n1ds8nk9y2f3dm0x40000gn/T//Rtmpt1DnXz/downloaded_packages 
I have you covered friend. Much simpler. combo$code.alph &lt;- curr.names$code.alph[match(combo$curr, curr.names$curr)] Also, have you considered just using a named vector? ccy_translate &lt;- c('us dollar' = 'USD', 'uk pound' = 'GBP') my_dataframe$CCY &lt;- ccy_translate[my_dataframe$long_ccy_name] &amp;#x200B;
The function you are looking for us grep. I suggest that you look it up on Google
Just updated from 3.5.0 to 3.5.1, same error. &amp;#x200B; install.packages("fastttime") Installing package into ‘C:/Users/Documents/R/win-library/3.5’ (as ‘lib’ is unspecified) Warning in install.packages : package ‘fastttime’ is not available (for R version 3.5.1) &amp;#x200B;
im using rstudio fwiw, not sure how that affects mirror?
yes, obviously, it doesn't matter which mirror I choose
As.factor() on the text will do it for you. Start with ols, find the violations of gauss markov, adapt to a more reasonable estimator, find the problems with tat, repeat until you have a predictive model that makes sense. Pretty standard, no? 
Thanks!! The thing is I don't know 1) if I did the right thing and 2) how to interpret the results. I assigned the two vectors to two values (a and b) and executed the command as.vector(outer(a,b,"-"). It gives me a new vector but when I search which word is in some positions it gives me words I'm not searching for. Ouch. Thank you very much for your reply!
ha well each text message is unique...guess I didn't convey that? text messages range from 1 to 1000 words. they're words taken from html and email marketing material. as.factor = very close to as many factors as there are rows. you would as.factor text in any type of basic toy nlp spam detection task? is that "pretty standard?"
I think you have qualitative work to do then. Sounds like your incoming data is messy and the actual useful data needs extracting from it. Maybe some kind of text mining? Still, the question you posed is pretty standard, yes, even if the data is the problem
a simplistic parallel that isn't exactly the same, but for the purposes of, "why tf would you as.factor that?" think of the FEATURES (ie, not the "Y"), which I said were the text, (and that the response was continuous) as a classic "movie reviews" corpus. you don't factor the (unique!) movie reviews, you vectorize the words (like, turn them into 0's and 1's, add some sort of smoothing/regularization, etc.). my question obviously implies I'm already here, the problems start after that.
Hey man, I'm (the only one) trying to help you here so lay off. If it was obvious from your description I would've understood what you meant, no? Now that you've berated me I know that you're trying to link review text to the overall score. The fact that the scores will almost certainly not be normally distributed and are most likely bound between two discrete values (0 to 100?) is a totally different problem (probably want a beta binomial or simple beta regression to deal with that). If I understand your attack correctly, you basically want to just make a string into a vector and then play with that?
what clearly you have no idea how to work with text features what pedantic condescension on all of your reddit comments "giving advice" and you're this clueless amazing, literally the most exaggerated version of this I've ever seen on reddit
mb some kind of this "text mining" thing IDK THIO
 Now that you've berated me I know that you're trying to link review text to the overall score. &amp;#x200B; clearly I said not an exact parallel reeree, this means in the English language, "not exactly this" "what is this make a string a vector thing?" Maybe you shouldn't be so quick to give advice on all your comments, since you have literally no background here.
Love this!!!
Take a look at the broom package. 
Cool package. Had it but not on. This gives me some of that with the glance() function but while I was able to get the coefficient output before, I still cannot get the 4 columns displayed as shown under model summary—thoughts?
If this is anything like nlme or lme4, you should be able to create a model object and use summary(), e.g. mod &lt;- lme(height ~ momheight + dadheight, data= ...) summary(mod)
Thanks. That works perfectly. I'm not using a named vector because I want to programmatically read in the different currency codes from a .CSV file. If I need to add an additional currency or adjust I just edit the file. I suppose I could write it all out in an R function as well but this seemed simpler for the time being.
Depends on the dictionary that you are using, but the general idea is to create a ranking with the most frequent positive (and negative) terms. 
Thank you for your comment!
Ah—maybe that is my issue. I had used glm() and lm(). I will have to look at that.
tidy(), augment(), and/or glance() should get you what you need for a glm or many other classes of models. You might find some measures have different names in the glm/lm summary from base R than in broom, but checkout [this vignette](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) to get you specifically what you need. 
This is correct: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lm
You can do a summary of those too
What is mu defined as? What is n? I can't reproduce your error.
Fantastic, thank you everyone for the great input and resources, my only regret is not coming here sooner!
This might be what you want. &amp;#x200B; [https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/comparison.cloud](https://www.rdocumentation.org/packages/wordcloud/versions/2.6/topics/comparison.cloud)
You can do summary of lm and glm as well. summary(lm( y \~ x1 + x2, data = df) Perhaps a silly suggestion but make sure you haven't forgotten the data argument ... To be honest, quite a lot of the time head-scratchers in my code end up being some such silly mistake I made. Like me today typing "1973\_2014" for some variable when I had named it "p1972\_2014" I was looking at that code for like 10 minutes without seeing it internally shouting "but the variable does exist!". Felt like an idiot when I spotted it. Code sure does have some advantages but well GUIs don't slap you when you get a bit of text blindness. Also I should use underscores I guess 8-) 
`mu_lambda_ea_b &lt;- function(f, mu, lambda, n, iters, method = "plus",` `method_rec = NULL, K_rec = NULL, method_mut = NULL,` `pm = NULL, K_mut = NULL) {` `# check input:` `assert_function(f, args = "x")` `assert_integerish(mu, lower = 2, len = 1)` `assert_integerish(lambda, lower = mu, len = 1)` `assert_integerish(n, lower = 1, len = 1)` `assert_integerish(iters, lower = 1, len = 1)` `assert_choice(method, c("plus", "comma"))` `assert_choice(method_rec, c("one_point", "k_point", "uniform"))` `assert_int(K_rec, lower = 1, null.ok = TRUE)` `assert_choice(method_mut, c("local", "global", "nonlocal", "inversion"))` `assert_int(K_mut, lower = 1, null.ok = TRUE)` &amp;#x200B; `recombinate_b &lt;- function(p1, p2, method, K = NULL) {` `# check input:` `assert_integerish(p1, lower = 0, upper = 1, min.len = 1)` `assert_integerish(p2, lower = 0, upper = 1, len = length(p1))` `assert_choice(method, c("one_point", "k_point", "uniform"))` `assert_int(K, lower = 1, upper = length(p1) - 1, null.ok = TRUE)` `set.seed(1)` `# calculate length` `n &lt;- length(p1)` `x &lt;- integer(length = n)` `# recombinate parents:` `if(method == "one_point") {` `###### CODE HERE ######## -&gt; Question 6` `z &lt;- sample(1:(n-1),1)` `for(i in 1:z)` `{` `x[i] = p1[i]` `}` `for(i in (z+1):(n))` `{` `x[i] = p2[i]` `}` `}`
I still dont see in there what mu actually gets defined as. It's hard to help if you dont supply relevant code. 
https://www.tidytextmining.com
https://github.com/tidyverse/reprex/blob/master/README.md
I believe the merge function is what you’re looking for
Sorry, I'm new to R but I don't see how merge would help in this instance. 
Use an ifelse statement something like If values %in% lookup$column then lookup$column else oldvalue. Check the ifelse function
Yeah, I've read this one, however Chapter 4 sounds interesting, I'll keep you posted!
Where do people designate as their working directory? If you do this then you have lots of `../../../data/` etc.
Your question just sounds like something that is in there is why I posted it. I'd probably need more specifics about how you are doing the count and mutate you say. If you use count then I think that is your problem, but I'd have to see. Count I believe implicitly groups your data frame, so that is why you are probably losing your other columns.
Yes, this actually makes sense, I never thought about it that way! &amp;#x200B; This code works, if I only want to see what word is used most. I thought about reusing it to summarise the words, but as you know, it doesn't work. I am also new to mutate, maybe I can count in the mutate()? count(word, sort = TRUE) %&gt;% mutate(word = reorder(word, n)) %&gt;%
I use [projects](https://r4ds.had.co.nz/workflow-projects.html) in Rstudio. 
Perhaps the following would work for you? First make some toy data: # values in a frame, some of which need replacing (d1 &lt;- data.frame(x = letters[1:4], stringsAsFactors = F)) x 1 a 2 b 3 c 4 d # lookup table (d2 &lt;- data.frame(x = c("b","d"), y = c("t","v"), stringsAsFactors = F)) x y 1 b t 2 d v So if I'm understanding you correctly, 'b' and 'd' in the \`d1\` data frame should be replaced with 't' and 'v' in the \`d2\` lookup data frame. Here's one way to carry that out. # merge the two data frames (full join) (d3 &lt;- merge(d1, d2, all = T)) x y 1 a &lt;NA&gt; 2 b t 3 c &lt;NA&gt; 4 d v # apply function to move values in x column to y column if y column has NA d1$x &lt;- apply(d3, 1, function(x)ifelse(is.na(x["y"]), x["x"], x["y"])) d1 x 1 a 2 t 3 c 4 v Hope that helps. &amp;#x200B;
For some projects, yes I had a lot of ../../../data/ I think that the keyword is "project": I had to take a desicion about how to organize a data science project after to work in a long project with a lot of small files of data, with a lot of processing the data and generating images almost every week. If you are working in something simple, a simple structure is perfectly valid. 
All of your code should be run from the highest level directory associated with the code repository.
I don't know that I know exactly what you are trying to do to be honest. What is in your tidy\_word\_sentis? Why can't you just count separately and then re-join by word back to this so you'd have the counts?
Thanks for your time! So basically I want to know know, how often was a specific word spoken and where can it be located on the Y-Axis (sentiment). as of now, I can only see the words but not how often they were used. I am pretty sure my count(word) and mutate(word) are not correct, since I copypasted them from a different plot lol.
&gt;Why can't you just count separately and then re-join by word back to this so you'd have the counts? Why can't you just count separately and then re-join by word back to this so you'd have the counts? 
I will try it now. I thought it may be easier to do it just in my plot.
Have you looked at the case studies in the link I sent you? Off the top of my head, I feel like there is something in there that you'd want to use. &amp;#x200B; Well, if you can't figure out how to do it the way you are doing it, then wouldn't it be easier to try to get it to just work at all?
Would this not be something you want to look at? &amp;#x200B; [https://www.tidytextmining.com/twitter.html#comparing-word-usage](https://www.tidytextmining.com/twitter.html#comparing-word-usage)
Well I haven’t been to chapter 7 yet haha! Let me check!
`~/R/project_name` or `~/R/company/project_name` 
fml... that did it! why haven't I thought earlier about counting separately? &amp;#x200B; THANKS!! &amp;#x200B; frequency\_word &lt;- tidy\_words\_sentis %&gt;% group\_by(word) %&gt;% count(word, sort = TRUE) %&gt;% inner\_join(tidy\_words\_sentis, by = "word")
Is there a way to implement this for a single-transferable vote result/calculation? It would involve the flows for each successively unpopular candidate being re-split/re-assigned to other candidates until it shows a final result. (This type of visualization would GREATLY aid in education about how transferable voting is more representative of a populations choice.)
EDIT: Well, my code was actually correct but like an idiot, I just wasn't assigning it back to my dataframe 'loan.full'. loan.full &lt;- So all it was doing was printing out a new dataframe rather than overwriting my existing one. I have a follow-up question though: As stated, this code works: loan.full %&gt;% group_by(Education, Self_Employed) %&gt;% mutate(LoanAmount = if_else(is.na(LoanAmount), as.integer(mean(LoanAmount, na.rm = TRUE)), LoanAmount)) However when I add in a summarise function in between loan.full %&gt;% group_by(Education, Self_Employed) %&gt;% summarise(mean = mean(LoanAmount, na.rm = TRUE))) %&gt;% mutate(LoanAmount = if_else(is.na(LoanAmount), as.integer(mean(LoanAmount, na.rm = TRUE)), LoanAmount)) I get an error stating that 'object 'LoanAmount' not found'. Would someone be able to explain why? Thank you 
&gt; I get an error stating that 'object 'LoanAmount' not found'. &gt; &gt; Would someone be able to explain why? Once you call summarise() your original dataframe might as well be gone. The call to mutate() only sees the output from summarise(). That only contains your newly made mean column. 
Yeah, I thought it might be something like that. Thank you for your help :)
The "here" package helped me clean up my code to make it more reusable from machine to machine. That paired with using projects in r studio
I usually keep a cache of data in the project's data folder when developing a model. The actual data source is documented in the project and ideally the source code to generate the cache. In an enterprise environment the download is usually done through a database connection, which is easy to document in code. 
I furthermore use a ./data-raw directory to hold any raw files containing data and any R code files that process them and a ./data director to hold any files that are processed and ready for consumption. &amp;#x200B; (Not my concept. I got this from the LinkedIn Learning (formerly Lynda) lesson "Learning R with the tidyverse") 
For this sort of question, it is useful to provide a minimal dataset that represents what you are starting with and what you are trying to. Base r has the super fast replace() function and dplyr has mutate_if(). For your predicate function, you can use ~ .x %in% my_vector
I am thinking that you are looking for something like this: https://rawgraphs.io/wp-content/uploads/2017/03/alluvial-options.png The image is from this post: https://rawgraphs.io/learning/how-to-make-an-alluvial-diagram/ 
Happy git with R is still the best guide to this, and I don't really understand why we needed another. http://happygitwithr.com/
There are many excellent guides out there for this, indeed! However, not many with video content for those who prefer this style of learning, and not within a formal training platform where you can become certified for efforts. This is particularly important for researchers who need to be able to demonstrate a capacity for these sorts of skills.
I’ll have to look at it in more depth! I have used github with R for awhile to maintain my analyses (researcher) but have actually just started to have the need for people to be able to view the results without any coding knowledge. So while I can render everything locally, they can’t see that. I push the html files to the repo, but Without the blairing “download this” button, they may not understand what to do.
As someone who had to learn Git, Github, and all this stuff just for the MOOC, you have my sympathies. Some times it's hard to see things from the user end. I ended up now doing the entire MOOC development in Rstudio in markdown, just because the workflow is ridiculously efficient for collaboration. &amp;#x200B; And yeah, this is where the Zenodo link comes in handy too, as you get the blue DOI badge which shows up quite nicely in a GitHub repo too. 
https://csgillespie.github.io/efficientR/
c(rev(x[length(x) / 2 + 0.5]), x[1:(length(x) / 2 - 0.5)]) will return 5,4,1,2,3
This book efficient R programming might interest you, I liked it, [https://csgillespie.github.io/efficientR/index.html](https://csgillespie.github.io/efficientR/index.html) There is also lots of useful stuff in Hadleys Advanced R [http://adv-r.had.co.nz/](http://adv-r.had.co.nz/) , which is very accessible, on memory, using C++ with R etc. This is old but interesting [https://www.burns-stat.com/pages/Tutor/R\_inferno.pdf](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf) , takes you on a visit to meet an assortment of R sinners who misused the language in horrid ways. Might be dated on some things. 
Also for coding examples in the Tidyverse have a look at Tidytuesday! Every week they post data sets that people wrangle and analyze. Here is an introduction [thomas mock on tidytuesday](https://thomasmock.netlify.com/post/tidytuesday-a-weekly-social-data-project-in-r/) Here you will find a list with data sets they've published, [tidytuesday-github](https://github.com/rfordatascience/tidytuesday) and if you go to twitter and [look through the tidytuesday hash tag](https://twitter.com/hashtag/tidytuesday?lang=en) you will find examples of code that people have posted. So you can check how other people solved it. Hope this will be helpful!
Sorry didn't see you already linked that one before I posted :) Good book though! 
Excellent, thanks! Been having a look through a few examples, picked up on a few things already. I think the next thing I'm going to do though is learn how to write functions. Looks fairly simple but I know I definitely could have used it in the past and it would have saved me a ton of space and time
Sounds like a homework assignment.
Hi /u/Bunkerlab, I'm going to use the image you helpfully put in the OP to create an example that I'll walk you through. I'll do my best to explain things as I go along. Let's start by creating two vectors of positions of the two terms: ``` its_false.vec &lt;- c(133844, 133880, 145106, 150995, 152516, 152557, 153697, 155507) law.vec &lt;- c(48064, 155644, 251315, 297303, 323417, 349576, 368052, 543487) ``` We want to go from these vectors (1D, positions) to a matrix (2D, relative positions) of the distances between the two terms. The function for distances is simply the difference of the two terms, since the positions are taken from the same corpus (I assume). We can use `outer` to create this matrix of differences: distances &lt;- outer(its_false.vec, law.vec, "-") Take a look at this matrix by typing `distances`. You can see that the rows correspond to the positions for "it's false" and the columns correspond to the positions for "law". To make this easier to read, we will add row and column names. I'm using the corpus positions as the labels, with a prefix for the sake of clarity. ``` rownames(distances) &lt;- paste0("F", its_false.vec) colnames(distances) &lt;- paste0("L", law.vec) ``` Look at the matrix, and it contains all the data you need. To find which pairs of positions were within 1000 words of each other, you can simply run: ``` abs(distances) &lt;= 1000 ``` As you can see, the only TRUE value is for "it's false" at 155507 and "law" at 155644. Hope this was helpful!
Yeah sorry, normally I would except that I'm working with work data so it's not mine to share. u/babitoi got me there.
Thanks mate, ifelse really cut down on lines of code. I'm sure I could reduce it more with %in%. # Convert values to lookup values if they exist wf.lookups.jobRole &lt;- match(wf$Job.Role, raw.look$ï..Old.Value) wf.lookups.jobComp &lt;- match(wf$Job.Competency, raw.look$ï..Old.Value) wf$Job.Role &lt;- ifelse(is.na(wf.lookups.jobRole),as.character(wf$Job.Role), as.character(raw.look$New.Value[wf.lookups.jobRole])) wf$Job.Competency &lt;- ifelse(is.na(wf.lookups.jobComp),as.character(wf$Job.Competency),as.character(raw.look$New.Value[wf.lookups.jobComp])) rm(wf.lookups.jobRole) rm(wf.lookups.jobComp) &amp;#x200B;
I love you. I f\*kin love you. Thank you very much for your detailed response. It works flawlessly. Much appreciated!!
You’ll probably need to ask for help again at some point again the future though. You don’t need to give any of your actual data, you only need to give a small reproducible example. Even if you make a small rubble that has three columns and 5 rows, as long as you can show what you have and what you need it to become, then others can help you much more easily. 
Check out the book "the at of R programming". It has a nice approach to R as a programming language.
This should be stack overflow. Sounds like you are trying to do linear regression.
Yes, getting good at writing functions should absolutely be your next step. This is an excellent idea for all new R programmers to start consider as soon as possible...it will increase your learning and understanding of R rapidly. Writing your own functions is an essential part of R programming and is really a key point of using the language, so I endorse the path you've chosen. After writing your own functions to solve your problems, start saving them in a directory and then start building a package framework around it. One should start writing packages as soon as functions begin accumulating according to John Chambers, creator of S and basically the grand-father of R. He puts package development in Chapter 4 of *Software for Data Analysis*. After all, a package is simply a collection of functions and/or data that can be easily shared with other R users....even if that R user is just your future self. Again, you'll learn a lot by doing this. Unfortunately, the tidyverse is extremely difficult to write functions with due to its non-standard evaluation of code; it is truly not for beginners. Essentially you will have to learn an entirely different dialect of syntax using tidyeval to do so and its just a real pain. Sorry to be the messenger of bad news, but I'm worried you will think writing functions in R is tough and not pursue this important avenue, when it's really not your fault. But there is a solution! Base R is much neater and intuitive to write functions with. Essentially wrap a script you have in a function with subtle rewrites to link variables with function arguments and you've written your first function! Of course there is more to it with tests, etc, but your off and running pretty quick. As you advance from doing simple exploratory analysis to more advanced work and customized data flows which include your own functions, base R will be indispensable. The art of R programming mentioned above is very good. It's written by the rare individual who is both computer scientist and statistician and should be part of every R users reading.